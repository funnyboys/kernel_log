commit bd35c77e32e4359580207891c0f7a438ad4b42df
Author: Krzysztof Piecuch <piecuch@protonmail.com>
Date:   Thu Jan 23 16:09:26 2020 +0000

    x86/tsc: Add tsc_early_khz command line parameter
    
    Changing base clock frequency directly impacts TSC Hz but not CPUID.16h
    value. An overclocked CPU supporting CPUID.16h and with partial CPUID.15h
    support will set TSC KHZ according to "best guess" given by CPUID.16h
    relying on tsc_refine_calibration_work to give better numbers later.
    tsc_refine_calibration_work will refuse to do its work when the outcome is
    off the early TSC KHZ value by more than 1% which is certain to happen on
    an overclocked system.
    
    Fix this by adding a tsc_early_khz command line parameter that makes the
    kernel skip early TSC calibration and use the given value instead.
    
    This allows the user to provide the expected TSC frequency that is closer
    to reality than the one reported by the hardware, enabling
    tsc_refine_calibration_work to do meaningful error checking.
    
    [ tglx: Made the variable __initdata as it's only used on init and
            removed the error checking in the argument parser because
            kstrto*() only stores to the variable if the string is valid ]
    
    Signed-off-by: Krzysztof Piecuch <piecuch@protonmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/O2CpIOrqLZHgNRkfjRpz_LGqnc1ix_seNIiOCvHY4RHoulOVRo6kMXKuLOfBVTi0SMMevg6Go1uZ_cL9fLYtYdTRNH78ChaFaZyG3VAyYz8=@protonmail.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index fdd4c1078632..49d925043171 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -41,6 +41,7 @@ EXPORT_SYMBOL(tsc_khz);
  * TSC can be unstable due to cpufreq or due to unsynced TSCs
  */
 static int __read_mostly tsc_unstable;
+static unsigned int __initdata tsc_early_khz;
 
 static DEFINE_STATIC_KEY_FALSE(__use_tsc);
 
@@ -59,6 +60,12 @@ struct cyc2ns {
 
 static DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
 
+static int __init tsc_early_khz_setup(char *buf)
+{
+	return kstrtouint(buf, 0, &tsc_early_khz);
+}
+early_param("tsc_early_khz", tsc_early_khz_setup);
+
 __always_inline void cyc2ns_read_begin(struct cyc2ns_data *data)
 {
 	int seq, idx;
@@ -1412,7 +1419,10 @@ static bool __init determine_cpu_tsc_frequencies(bool early)
 
 	if (early) {
 		cpu_khz = x86_platform.calibrate_cpu();
-		tsc_khz = x86_platform.calibrate_tsc();
+		if (tsc_early_khz)
+			tsc_khz = tsc_early_khz;
+		else
+			tsc_khz = x86_platform.calibrate_tsc();
 	} else {
 		/* We should not be here with non-native cpu calibration */
 		WARN_ON(x86_platform.calibrate_cpu != native_calibrate_cpu);

commit fdf5563a720004199324371c08071b8ea27bd994
Merge: 97cddfc34549 a2150327250e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 31 11:04:05 2020 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "This topic tree contains more commits than usual:
    
       - most of it are uaccess cleanups/reorganization by Al
    
       - there's a bunch of prototype declaration (--Wmissing-prototypes)
         cleanups
    
       - misc other cleanups all around the map"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (36 commits)
      x86/mm/set_memory: Fix -Wmissing-prototypes warnings
      x86/efi: Add a prototype for efi_arch_mem_reserve()
      x86/mm: Mark setup_emu2phys_nid() static
      x86/jump_label: Move 'inline' keyword placement
      x86/platform/uv: Add a missing prototype for uv_bau_message_interrupt()
      kill uaccess_try()
      x86: unsafe_put-style macro for sigmask
      x86: x32_setup_rt_frame(): consolidate uaccess areas
      x86: __setup_rt_frame(): consolidate uaccess areas
      x86: __setup_frame(): consolidate uaccess areas
      x86: setup_sigcontext(): list user_access_{begin,end}() into callers
      x86: get rid of put_user_try in __setup_rt_frame() (both 32bit and 64bit)
      x86: ia32_setup_rt_frame(): consolidate uaccess areas
      x86: ia32_setup_frame(): consolidate uaccess areas
      x86: ia32_setup_sigcontext(): lift user_access_{begin,end}() into the callers
      x86/alternatives: Mark text_poke_loc_init() static
      x86/cpu: Fix a -Wmissing-prototypes warning for init_ia32_feat_ctl()
      x86/mm: Drop pud_mknotpresent()
      x86: Replace setup_irq() by request_irq()
      x86/configs: Slightly reduce defconfigs
      ...

commit b95a8a27c300d1a39a4e36f63a518ef36e4b966c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 7 13:38:56 2020 +0100

    x86/vdso: Use generic VDSO clock mode storage
    
    Switch to the generic VDSO clock mode storage.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com> (VDSO parts)
    Acked-by: Juergen Gross <jgross@suse.com> (Xen parts)
    Acked-by: Paolo Bonzini <pbonzini@redhat.com> (KVM parts)
    Link: https://lkml.kernel.org/r/20200207124403.152039903@linutronix.de

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 742da141a30a..971d6f0216df 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1110,7 +1110,7 @@ static void tsc_cs_tick_stable(struct clocksource *cs)
 
 static int tsc_cs_enable(struct clocksource *cs)
 {
-	vclocks_set_used(VCLOCK_TSC);
+	vclocks_set_used(VDSO_CLOCKMODE_TSC);
 	return 0;
 }
 
@@ -1124,7 +1124,7 @@ static struct clocksource clocksource_tsc_early = {
 	.mask			= CLOCKSOURCE_MASK(64),
 	.flags			= CLOCK_SOURCE_IS_CONTINUOUS |
 				  CLOCK_SOURCE_MUST_VERIFY,
-	.archdata		= { .vclock_mode = VCLOCK_TSC },
+	.vdso_clock_mode	= VDSO_CLOCKMODE_TSC,
 	.enable			= tsc_cs_enable,
 	.resume			= tsc_resume,
 	.mark_unstable		= tsc_cs_mark_unstable,
@@ -1145,7 +1145,7 @@ static struct clocksource clocksource_tsc = {
 	.flags			= CLOCK_SOURCE_IS_CONTINUOUS |
 				  CLOCK_SOURCE_VALID_FOR_HRES |
 				  CLOCK_SOURCE_MUST_VERIFY,
-	.archdata		= { .vclock_mode = VCLOCK_TSC },
+	.vdso_clock_mode	= VDSO_CLOCKMODE_TSC,
 	.enable			= tsc_cs_enable,
 	.resume			= tsc_resume,
 	.mark_unstable		= tsc_cs_mark_unstable,

commit eec399dd862762b9594df3659f15839a4e12f17a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 7 13:38:54 2020 +0100

    x86/vdso: Move VDSO clocksource state tracking to callback
    
    All architectures which use the generic VDSO code have their own storage
    for the VDSO clock mode. That's pointless and just requires duplicate code.
    
    X86 abuses the function which retrieves the architecture specific clock
    mode storage to mark the clocksource as used in the VDSO. That's silly
    because this is invoked on every tick when the VDSO data is updated.
    
    Move this functionality to the clocksource::enable() callback so it gets
    invoked once when the clocksource is installed. This allows to make the
    clock mode storage generic.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>  (Hyper-V parts)
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com> (VDSO parts)
    Acked-by: Juergen Gross <jgross@suse.com> (Xen parts)
    Link: https://lkml.kernel.org/r/20200207124402.934519777@linutronix.de

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 7e322e2daaf5..742da141a30a 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1108,17 +1108,24 @@ static void tsc_cs_tick_stable(struct clocksource *cs)
 		sched_clock_tick_stable();
 }
 
+static int tsc_cs_enable(struct clocksource *cs)
+{
+	vclocks_set_used(VCLOCK_TSC);
+	return 0;
+}
+
 /*
  * .mask MUST be CLOCKSOURCE_MASK(64). See comment above read_tsc()
  */
 static struct clocksource clocksource_tsc_early = {
-	.name                   = "tsc-early",
-	.rating                 = 299,
-	.read                   = read_tsc,
-	.mask                   = CLOCKSOURCE_MASK(64),
-	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
+	.name			= "tsc-early",
+	.rating			= 299,
+	.read			= read_tsc,
+	.mask			= CLOCKSOURCE_MASK(64),
+	.flags			= CLOCK_SOURCE_IS_CONTINUOUS |
 				  CLOCK_SOURCE_MUST_VERIFY,
-	.archdata               = { .vclock_mode = VCLOCK_TSC },
+	.archdata		= { .vclock_mode = VCLOCK_TSC },
+	.enable			= tsc_cs_enable,
 	.resume			= tsc_resume,
 	.mark_unstable		= tsc_cs_mark_unstable,
 	.tick_stable		= tsc_cs_tick_stable,
@@ -1131,14 +1138,15 @@ static struct clocksource clocksource_tsc_early = {
  * been found good.
  */
 static struct clocksource clocksource_tsc = {
-	.name                   = "tsc",
-	.rating                 = 300,
-	.read                   = read_tsc,
-	.mask                   = CLOCKSOURCE_MASK(64),
-	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
+	.name			= "tsc",
+	.rating			= 300,
+	.read			= read_tsc,
+	.mask			= CLOCKSOURCE_MASK(64),
+	.flags			= CLOCK_SOURCE_IS_CONTINUOUS |
 				  CLOCK_SOURCE_VALID_FOR_HRES |
 				  CLOCK_SOURCE_MUST_VERIFY,
-	.archdata               = { .vclock_mode = VCLOCK_TSC },
+	.archdata		= { .vclock_mode = VCLOCK_TSC },
+	.enable			= tsc_cs_enable,
 	.resume			= tsc_resume,
 	.mark_unstable		= tsc_cs_mark_unstable,
 	.tick_stable		= tsc_cs_tick_stable,

commit 4d1d0977a2156a1dafe8f1cd890ab918c803485b
Author: Martin Molnar <martin.molnar.programming@gmail.com>
Date:   Sun Feb 16 16:17:39 2020 +0100

    x86: Fix a handful of typos
    
    Fix a couple of typos in code comments.
    
     [ bp: While at it: s/IRQ's/IRQs/. ]
    
    Signed-off-by: Martin Molnar <martin.molnar.programming@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Randy Dunlap <rdunlap@infradead.org>
    Link: https://lkml.kernel.org/r/0819a044-c360-44a4-f0b6-3f5bafe2d35c@gmail.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 7e322e2daaf5..0109b9d485c6 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -477,7 +477,7 @@ static unsigned long pit_calibrate_tsc(u32 latch, unsigned long ms, int loopmin)
  * transition from one expected value to another with a fairly
  * high accuracy, and we didn't miss any events. We can thus
  * use the TSC value at the transitions to calculate a pretty
- * good value for the TSC frequencty.
+ * good value for the TSC frequency.
  */
 static inline int pit_verify_msb(unsigned char val)
 {

commit 63ec58b44fcc05efd1542045abd7faf056ac27d9
Author: Michael Zhivich <mzhivich@akamai.com>
Date:   Thu Oct 24 13:59:45 2019 -0400

    x86/tsc: Respect tsc command line paraemeter for clocksource_tsc_early
    
    The introduction of clocksource_tsc_early broke the functionality of
    "tsc=reliable" and "tsc=nowatchdog" command line parameters, since
    clocksource_tsc_early is unconditionally registered with
    CLOCK_SOURCE_MUST_VERIFY and thus put on the watchdog list.
    
    This can cause the TSC to be declared unstable during boot:
    
      clocksource: timekeeping watchdog on CPU0: Marking clocksource
                   'tsc-early' as unstable because the skew is too large:
      clocksource: 'refined-jiffies' wd_now: fffb7018 wd_last: fffb6e9d
                   mask: ffffffff
      clocksource: 'tsc-early' cs_now: 68a6a7070f6a0 cs_last: 68a69ab6f74d6
                   mask: ffffffffffffffff
      tsc: Marking TSC unstable due to clocksource watchdog
    
    The corresponding elapsed times are cs_nsec=1224152026 and wd_nsec=378942392, so
    the watchdog differs from TSC by 0.84 seconds.
    
    This happens when HPET is not available and jiffies are used as the TSC
    watchdog instead and the jiffies update is not happening due to lost timer
    interrupts in periodic mode, which can happen e.g. with expensive debug
    mechanisms enabled or under massive overload conditions in virtualized
    environments.
    
    Before the introduction of the early TSC clocksource the command line
    parameters "tsc=reliable" and "tsc=nowatchdog" could be used to work around
    this issue.
    
    Restore the behaviour by disabling the watchdog if requested on the kernel
    command line.
    
    [ tglx: Clarify changelog ]
    
    Fixes: aa83c45762a24 ("x86/tsc: Introduce early tsc clocksource")
    Signed-off-by: Michael Zhivich <mzhivich@akamai.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20191024175945.14338-1-mzhivich@akamai.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index c59454c382fd..7e322e2daaf5 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1505,6 +1505,9 @@ void __init tsc_init(void)
 		return;
 	}
 
+	if (tsc_clocksource_reliable || no_tsc_watchdog)
+		clocksource_tsc_early.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
+
 	clocksource_register_khz(&clocksource_tsc_early, tsc_khz);
 	detect_art();
 }

commit 5ebb34edbefa8ea6a7e109179d5fc7b3529dbeba
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 27 21:48:24 2019 +0200

    x86/intel: Aggregate microserver naming
    
    Currently big microservers have _XEON_D while small microservers have
    _X, Make it uniformly: _D.
    
    for i in `git grep -l "\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*_\(X\|XEON_D\)"`
    do
            sed -i -e 's/\(\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*ATOM.*\)_X/\1_D/g' \
                   -e 's/\(\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*\)_XEON_D/\1_D/g' ${i}
    done
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Cc: x86@kernel.org
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: https://lkml.kernel.org/r/20190827195122.677152989@infradead.org

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 57d87f79558f..c59454c382fd 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -638,7 +638,7 @@ unsigned long native_calibrate_tsc(void)
 	 * clock.
 	 */
 	if (crystal_khz == 0 &&
-			boot_cpu_data.x86_model == INTEL_FAM6_ATOM_GOLDMONT_X)
+			boot_cpu_data.x86_model == INTEL_FAM6_ATOM_GOLDMONT_D)
 		crystal_khz = 25000;
 
 	/*

commit 5b7a2095232d026d4537c4be54040c0f10525b5b
Merge: 6cfcdad7630d 53b7607382b0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 17:27:24 2019 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc small cleanups: removal of superfluous code and coding style
      cleanups mostly"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/kexec: Make variable static and config dependent
      x86/defconfigs: Remove useless UEVENT_HELPER_PATH
      x86/amd_nb: Make hygon_nb_misc_ids static
      x86/tsc: Move inline keyword to the beginning of function declarations
      x86/io_delay: Define IO_DELAY macros in C instead of Kconfig
      x86/io_delay: Break instead of fallthrough in switch statement

commit 0902d5011cfaabd6a09326299ef77e1c8735fb89
Merge: 927ba67a63c7 f8a8fe61fec8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 11:22:57 2019 -0700

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x96 apic updates from Thomas Gleixner:
     "Updates for the x86 APIC interrupt handling and APIC timer:
    
       - Fix a long standing issue with spurious interrupts which was caused
         by the big vector management rework a few years ago. Robert Hodaszi
         provided finally enough debug data and an excellent initial failure
         analysis which allowed to understand the underlying issues.
    
         This contains a change to the core interrupt management code which
         is required to handle this correctly for the APIC/IO_APIC. The core
         changes are NOOPs for most architectures except ARM64. ARM64 is not
         impacted by the change as confirmed by Marc Zyngier.
    
       - Newer systems allow to disable the PIT clock for power saving
         causing panic in the timer interrupt delivery check of the IO/APIC
         when the HPET timer is not enabled either. While the clock could be
         turned on this would cause an endless whack a mole game to chase
         the proper register in each affected chipset.
    
         These systems provide the relevant frequencies for TSC, CPU and the
         local APIC timer via CPUID and/or MSRs, which allows to avoid the
         PIT/HPET based calibration. As the calibration code is the only
         usage of the legacy timers on modern systems and is skipped anyway
         when the frequencies are known already, there is no point in
         setting up the PIT and actually checking for the interrupt delivery
         via IO/APIC.
    
         To achieve this on a wide variety of platforms, the CPUID/MSR based
         frequency readout has been made more robust, which also allowed to
         remove quite some workarounds which turned out to be not longer
         required. Thanks to Daniel Drake for analysis, patches and
         verification"
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/irq: Seperate unused system vectors from spurious entry again
      x86/irq: Handle spurious interrupt after shutdown gracefully
      x86/ioapic: Implement irq_get_irqchip_state() callback
      genirq: Add optional hardware synchronization for shutdown
      genirq: Fix misleading synchronize_irq() documentation
      genirq: Delay deactivation in free_irq()
      x86/timer: Skip PIT initialization on modern chipsets
      x86/apic: Use non-atomic operations when possible
      x86/apic: Make apic_bsp_setup() static
      x86/tsc: Set LAPIC timer period to crystal clock frequency
      x86/apic: Rename 'lapic_timer_frequency' to 'lapic_timer_period'
      x86/tsc: Use CPUID.0x16 to calculate missing crystal frequency

commit 83e837269e87436fda1cbf82214a5494fb6b35b1
Author: Mathieu Malaterre <malat@debian.org>
Date:   Fri May 24 12:32:51 2019 +0200

    x86/tsc: Move inline keyword to the beginning of function declarations
    
    The inline keyword was not at the beginning of the function declarations.
    Fix the following warnings triggered when using W=1:
    
      arch/x86/kernel/tsc.c:62:1: warning: 'inline' is not at beginning of declaration [-Wold-style-declaration]
      arch/x86/kernel/tsc.c:79:1: warning: 'inline' is not at beginning of declaration [-Wold-style-declaration]
    
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: trivial@kernel.org
    Cc: kernel-janitors@vger.kernel.org
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/20190524103252.28575-1-malat@debian.org

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 0b29e58f288e..75a41bddbc9d 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -59,7 +59,7 @@ struct cyc2ns {
 
 static DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
 
-void __always_inline cyc2ns_read_begin(struct cyc2ns_data *data)
+__always_inline void cyc2ns_read_begin(struct cyc2ns_data *data)
 {
 	int seq, idx;
 
@@ -76,7 +76,7 @@ void __always_inline cyc2ns_read_begin(struct cyc2ns_data *data)
 	} while (unlikely(seq != this_cpu_read(cyc2ns.seq.sequence)));
 }
 
-void __always_inline cyc2ns_read_end(void)
+__always_inline void cyc2ns_read_end(void)
 {
 	preempt_enable_notrace();
 }

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 356dfc555a27..0b29e58f288e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
 #include <linux/kernel.h>

commit df24014abe3694e7c34ce5e50248611b7a93fe83
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Apr 29 15:03:58 2019 +0530

    cpufreq: Call transition notifier only once for each policy
    
    Currently, the notifiers are called once for each CPU of the policy->cpus
    cpumask. It would be more optimal if the notifier can be called only
    once and all the relevant information be provided to it. Out of the 23
    drivers that register for the transition notifiers today, only 4 of them
    do per-cpu updates and the callback for the rest can be called only once
    for the policy without any impact.
    
    This would also avoid multiple function calls to the notifier callbacks
    and reduce multiple iterations of notifier core's code (which does
    locking as well).
    
    This patch adds pointer to the cpufreq policy to the struct
    cpufreq_freqs, so the notifier callback has all the information
    available to it with a single call. The five drivers which perform
    per-cpu updates are updated to use the cpufreq policy. The freqs->cpu
    field is redundant now and is removed.
    
    Acked-by: David S. Miller <davem@davemloft.net> (sparc)
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 15b5e98a86f9..356dfc555a27 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -979,7 +979,7 @@ static int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
 		if (!(freq->flags & CPUFREQ_CONST_LOOPS))
 			mark_tsc_unstable("cpufreq changes");
 
-		set_cyc2ns_scale(tsc_khz, freq->cpu, rdtsc());
+		set_cyc2ns_scale(tsc_khz, freq->policy->cpu, rdtsc());
 	}
 
 	return 0;

commit 2420a0b1798d7a78d1f9b395f09f3c80d92cc588
Author: Daniel Drake <drake@endlessm.com>
Date:   Thu May 9 13:54:17 2019 +0800

    x86/tsc: Set LAPIC timer period to crystal clock frequency
    
    The APIC timer calibration (calibrate_APIC_timer()) can be skipped
    in cases where we know the APIC timer frequency. On Intel SoCs,
    we believe that the APIC is fed by the crystal clock; this would make
    sense, and the crystal clock frequency has been verified against the
    APIC timer calibration result on ApolloLake, GeminiLake, Kabylake,
    CoffeeLake, WhiskeyLake and AmberLake.
    
    Set lapic_timer_period based on the crystal clock frequency
    accordingly.
    
    APIC timer calibration would normally be skipped on modern CPUs
    by nature of the TSC deadline timer being used instead,
    however this change is still potentially useful, e.g. if the
    TSC deadline timer has been disabled with a kernel parameter.
    calibrate_APIC_timer() uses the legacy timer, but we are seeing
    new platforms that omit such legacy functionality, so avoiding
    such codepaths is becoming more important.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Daniel Drake <drake@endlessm.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: len.brown@intel.com
    Cc: linux@endlessm.com
    Cc: rafael.j.wysocki@intel.com
    Link: http://lkml.kernel.org/r/20190509055417.13152-3-drake@endlessm.com
    Link: https://lkml.kernel.org/r/20190419083533.32388-1-drake@endlessm.com
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1904031206440.1967@nanos.tec.linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 6e6d933fb99c..8f47c4862c56 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -671,6 +671,16 @@ unsigned long native_calibrate_tsc(void)
 	if (boot_cpu_data.x86_model == INTEL_FAM6_ATOM_GOLDMONT)
 		setup_force_cpu_cap(X86_FEATURE_TSC_RELIABLE);
 
+#ifdef CONFIG_X86_LOCAL_APIC
+	/*
+	 * The local APIC appears to be fed by the core crystal clock
+	 * (which sounds entirely sensible). We can set the global
+	 * lapic_timer_period here to avoid having to calibrate the APIC
+	 * timer later.
+	 */
+	lapic_timer_period = crystal_khz * 1000 / HZ;
+#endif
+
 	return crystal_khz * ebx_numerator / eax_denominator;
 }
 

commit 604dc9170f2435d27da5039a3efd757dceadc684
Author: Daniel Drake <drake@endlessm.com>
Date:   Thu May 9 13:54:15 2019 +0800

    x86/tsc: Use CPUID.0x16 to calculate missing crystal frequency
    
    native_calibrate_tsc() had a data mapping Intel CPU families
    and crystal clock speed, but hardcoded tables are not ideal, and this
    approach was already problematic at least in the Skylake X case, as
    seen in commit:
    
      b51120309348 ("x86/tsc: Fix erroneous TSC rate on Skylake Xeon")
    
    By examining CPUID data from http://instlatx64.atw.hu/ and units
    in the lab, we have found that 3 different scenarios need to be dealt
    with, and we can eliminate most of the hardcoded data using an approach a
    little more advanced than before:
    
     1. ApolloLake, GeminiLake, CannonLake (and presumably all new chipsets
        from this point) report the crystal frequency directly via CPUID.0x15.
        That's definitive data that we can rely upon.
    
     2. Skylake, Kabylake and all variants of those two chipsets report a
        crystal frequency of zero, however we can calculate the crystal clock
        speed by condidering data from CPUID.0x16.
    
        This method correctly distinguishes between the two crystal clock
        frequencies present on different Skylake X variants that caused
        headaches before.
    
        As the calculations do not quite match the previously-hardcoded values
        in some cases (e.g. 23913043Hz instead of 24MHz), TSC refinement is
        enabled on all platforms where we had to calculate the crystal
        frequency in this way.
    
     3. Denverton (GOLDMONT_X) reports a crystal frequency of zero and does
        not support CPUID.0x16, so we leave this entry hardcoded.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Daniel Drake <drake@endlessm.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: len.brown@intel.com
    Cc: linux@endlessm.com
    Cc: rafael.j.wysocki@intel.com
    Link: http://lkml.kernel.org/r/20190509055417.13152-1-drake@endlessm.com
    Link: https://lkml.kernel.org/r/20190419083533.32388-1-drake@endlessm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 15b5e98a86f9..6e6d933fb99c 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -631,31 +631,38 @@ unsigned long native_calibrate_tsc(void)
 
 	crystal_khz = ecx_hz / 1000;
 
-	if (crystal_khz == 0) {
-		switch (boot_cpu_data.x86_model) {
-		case INTEL_FAM6_SKYLAKE_MOBILE:
-		case INTEL_FAM6_SKYLAKE_DESKTOP:
-		case INTEL_FAM6_KABYLAKE_MOBILE:
-		case INTEL_FAM6_KABYLAKE_DESKTOP:
-			crystal_khz = 24000;	/* 24.0 MHz */
-			break;
-		case INTEL_FAM6_ATOM_GOLDMONT_X:
-			crystal_khz = 25000;	/* 25.0 MHz */
-			break;
-		case INTEL_FAM6_ATOM_GOLDMONT:
-			crystal_khz = 19200;	/* 19.2 MHz */
-			break;
-		}
-	}
+	/*
+	 * Denverton SoCs don't report crystal clock, and also don't support
+	 * CPUID.0x16 for the calculation below, so hardcode the 25MHz crystal
+	 * clock.
+	 */
+	if (crystal_khz == 0 &&
+			boot_cpu_data.x86_model == INTEL_FAM6_ATOM_GOLDMONT_X)
+		crystal_khz = 25000;
 
-	if (crystal_khz == 0)
-		return 0;
 	/*
-	 * TSC frequency determined by CPUID is a "hardware reported"
+	 * TSC frequency reported directly by CPUID is a "hardware reported"
 	 * frequency and is the most accurate one so far we have. This
 	 * is considered a known frequency.
 	 */
-	setup_force_cpu_cap(X86_FEATURE_TSC_KNOWN_FREQ);
+	if (crystal_khz != 0)
+		setup_force_cpu_cap(X86_FEATURE_TSC_KNOWN_FREQ);
+
+	/*
+	 * Some Intel SoCs like Skylake and Kabylake don't report the crystal
+	 * clock, but we can easily calculate it to a high degree of accuracy
+	 * by considering the crystal ratio and the CPU speed.
+	 */
+	if (crystal_khz == 0 && boot_cpu_data.cpuid_level >= 0x16) {
+		unsigned int eax_base_mhz, ebx, ecx, edx;
+
+		cpuid(0x16, &eax_base_mhz, &ebx, &ecx, &edx);
+		crystal_khz = eax_base_mhz * 1000 *
+			eax_denominator / ebx_numerator;
+	}
+
+	if (crystal_khz == 0)
+		return 0;
 
 	/*
 	 * For Atom SoCs TSC is the only reliable clocksource.

commit 8f5e823f9131a430b12f73e9436d7486e20c16f5
Merge: 59df1c2bdecb e07095c9bbcd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 19:40:31 2019 -0700

    Merge tag 'pm-5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "These fix the (Intel-specific) Performance and Energy Bias Hint (EPB)
      handling and expose it to user space via sysfs, fix and clean up
      several cpufreq drivers, add support for two new chips to the qoriq
      cpufreq driver, fix, simplify and clean up the cpufreq core and the
      schedutil governor, add support for "CPU" domains to the generic power
      domains (genpd) framework and provide low-level PSCI firmware support
      for that feature, fix the exynos cpuidle driver and fix a couple of
      issues in the devfreq subsystem and clean it up.
    
      Specifics:
    
       - Fix the handling of Performance and Energy Bias Hint (EPB) on Intel
         processors and expose it to user space via sysfs to avoid having to
         access it through the generic MSR I/F (Rafael Wysocki).
    
       - Improve the handling of global turbo changes made by the platform
         firmware in the intel_pstate driver (Rafael Wysocki).
    
       - Convert some slow-path static_cpu_has() callers to boot_cpu_has()
         in cpufreq (Borislav Petkov).
    
       - Fix the frequency calculation loop in the armada-37xx cpufreq
         driver (Gregory CLEMENT).
    
       - Fix possible object reference leaks in multuple cpufreq drivers
         (Wen Yang).
    
       - Fix kerneldoc comment in the centrino cpufreq driver (dongjian).
    
       - Clean up the ACPI and maple cpufreq drivers (Viresh Kumar, Mohan
         Kumar).
    
       - Add support for lx2160a and ls1028a to the qoriq cpufreq driver
         (Vabhav Sharma, Yuantian Tang).
    
       - Fix kobject memory leak in the cpufreq core (Viresh Kumar).
    
       - Simplify the IOwait boosting in the schedutil cpufreq governor and
         rework the TSC cpufreq notifier on x86 (Rafael Wysocki).
    
       - Clean up the cpufreq core and statistics code (Yue Hu, Kyle Lin).
    
       - Improve the cpufreq documentation, add SPDX license tags to some PM
         documentation files and unify copyright notices in them (Rafael
         Wysocki).
    
       - Add support for "CPU" domains to the generic power domains (genpd)
         framework and provide low-level PSCI firmware support for that
         feature (Ulf Hansson).
    
       - Rearrange the PSCI firmware support code and add support for
         SYSTEM_RESET2 to it (Ulf Hansson, Sudeep Holla).
    
       - Improve genpd support for devices in multiple power domains (Ulf
         Hansson).
    
       - Unify target residency for the AFTR and coupled AFTR states in the
         exynos cpuidle driver (Marek Szyprowski).
    
       - Introduce new helper routine in the operating performance points
         (OPP) framework (Andrew-sh.Cheng).
    
       - Add support for passing on-die termination (ODT) and auto power
         down parameters from the kernel to Trusted Firmware-A (TF-A) to the
         rk3399_dmc devfreq driver (Enric Balletbo i Serra).
    
       - Add tracing to devfreq (Lukasz Luba).
    
       - Make the exynos-bus devfreq driver suspend all devices on system
         shutdown (Marek Szyprowski).
    
       - Fix a few minor issues in the devfreq subsystem and clean it up
         somewhat (Enric Balletbo i Serra, MyungJoo Ham, Rob Herring,
         Saravana Kannan, Yangtao Li).
    
       - Improve system wakeup diagnostics (Stephen Boyd).
    
       - Rework filesystem sync messages emitted during system suspend and
         hibernation (Harry Pan)"
    
    * tag 'pm-5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (72 commits)
      cpufreq: Fix kobject memleak
      cpufreq: armada-37xx: fix frequency calculation for opp
      cpufreq: centrino: Fix centrino_setpolicy() kerneldoc comment
      cpufreq: qoriq: add support for lx2160a
      x86: tsc: Rework time_cpufreq_notifier()
      PM / Domains: Allow to attach a CPU via genpd_dev_pm_attach_by_id|name()
      PM / Domains: Search for the CPU device outside the genpd lock
      PM / Domains: Drop unused in-parameter to some genpd functions
      PM / Domains: Use the base device for driver_deferred_probe_check_state()
      cpufreq: qoriq: Add ls1028a chip support
      PM / Domains: Enable genpd_dev_pm_attach_by_id|name() for single PM domain
      PM / Domains: Allow OF lookup for multi PM domain case from ->attach_dev()
      PM / Domains: Don't kfree() the virtual device in the error path
      cpufreq: Move ->get callback check outside of __cpufreq_get()
      PM / Domains: remove unnecessary unlikely()
      cpufreq: Remove needless bios_limit check in show_bios_limit()
      drivers/cpufreq/acpi-cpufreq.c: This fixes the following checkpatch warning
      firmware/psci: add support for SYSTEM_RESET2
      PM / devfreq: add tracing for scheduling work
      trace: events: add devfreq trace event file
      ...

commit c208ac8f8f862dba7b01eb54557f4803b3c17296
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Apr 18 16:11:37 2019 +0200

    x86: tsc: Rework time_cpufreq_notifier()
    
    There are problems with running time_cpufreq_notifier() on SMP
    systems.
    
    First off, the rdtsc() called from there runs on the CPU executing
    that code and not necessarily on the CPU whose sched_clock() rate is
    updated which is questionable at best.
    
    Second, in the cases when the frequencies of all CPUs in an SMP
    system are always in sync, it is not sufficient to update just
    one of them or the set associated with a given cpufreq policy on
    frequency changes - all CPUs in the system should be updated and
    that would require more than a simple transition notifier.
    
    Note, however, that the underlying issue (the TSC rate depending on
    the CPU frequency) has not been present in hardware shipping for the
    last few years and in quite a few relevant cases (acpi-cpufreq in
    particular) running time_cpufreq_notifier() will cause the TSC to
    be marked as unstable anyway.
    
    For this reason, make time_cpufreq_notifier() simply mark the TSC
    as unstable and give up when run on SMP and only try to carry out
    any adjustments otherwise.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Viresh Kumar <viresh.kumar@linaro.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3fae23834069..cc6df5c6d7b3 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -185,8 +185,7 @@ static void __init cyc2ns_init_boot_cpu(void)
 /*
  * Secondary CPUs do not run through tsc_init(), so set up
  * all the scale factors for all CPUs, assuming the same
- * speed as the bootup CPU. (cpufreq notifiers will fix this
- * up if their speed diverges)
+ * speed as the bootup CPU.
  */
 static void __init cyc2ns_init_secondary_cpus(void)
 {
@@ -937,12 +936,12 @@ void tsc_restore_sched_clock_state(void)
 }
 
 #ifdef CONFIG_CPU_FREQ
-/* Frequency scaling support. Adjust the TSC based timer when the cpu frequency
+/*
+ * Frequency scaling support. Adjust the TSC based timer when the CPU frequency
  * changes.
  *
- * RED-PEN: On SMP we assume all CPUs run with the same frequency.  It's
- * not that important because current Opteron setups do not support
- * scaling on SMP anyroads.
+ * NOTE: On SMP the situation is not fixable in general, so simply mark the TSC
+ * as unstable and give up in those cases.
  *
  * Should fix up last_tsc too. Currently gettimeofday in the
  * first tick after the change will be slightly wrong.
@@ -956,22 +955,22 @@ static int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
 				void *data)
 {
 	struct cpufreq_freqs *freq = data;
-	unsigned long *lpj;
 
-	lpj = &boot_cpu_data.loops_per_jiffy;
-#ifdef CONFIG_SMP
-	if (!(freq->flags & CPUFREQ_CONST_LOOPS))
-		lpj = &cpu_data(freq->cpu).loops_per_jiffy;
-#endif
+	if (num_online_cpus() > 1) {
+		mark_tsc_unstable("cpufreq changes on SMP");
+		return 0;
+	}
 
 	if (!ref_freq) {
 		ref_freq = freq->old;
-		loops_per_jiffy_ref = *lpj;
+		loops_per_jiffy_ref = boot_cpu_data.loops_per_jiffy;
 		tsc_khz_ref = tsc_khz;
 	}
+
 	if ((val == CPUFREQ_PRECHANGE  && freq->old < freq->new) ||
-			(val == CPUFREQ_POSTCHANGE && freq->old > freq->new)) {
-		*lpj = cpufreq_scale(loops_per_jiffy_ref, ref_freq, freq->new);
+	    (val == CPUFREQ_POSTCHANGE && freq->old > freq->new)) {
+		boot_cpu_data.loops_per_jiffy =
+			cpufreq_scale(loops_per_jiffy_ref, ref_freq, freq->new);
 
 		tsc_khz = cpufreq_scale(tsc_khz_ref, ref_freq, freq->new);
 		if (!(freq->flags & CPUFREQ_CONST_LOOPS))

commit 0f0b7e1cc7abf8e1a8b301f2868379d611d05ae2
Author: Juri Lelli <juri.lelli@redhat.com>
Date:   Thu Mar 7 13:09:13 2019 +0100

    x86/tsc: Add option to disable tsc clocksource watchdog
    
    Clocksource watchdog has been found responsible for generating latency
    spikes (in the 10-20 us range) when woken up to check for TSC stability.
    
    Add an option to disable it at boot.
    
    Signed-off-by: Juri Lelli <juri.lelli@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: bigeasy@linutronix.de
    Cc: linux-rt-users@vger.kernel.org
    Cc: peterz@infradead.org
    Cc: bristot@redhat.com
    Cc: williams@redhat.com
    Link: https://lkml.kernel.org/r/20190307120913.13168-1-juri.lelli@redhat.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3fae23834069..aab0c82e0a0d 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -283,6 +283,7 @@ int __init notsc_setup(char *str)
 __setup("notsc", notsc_setup);
 
 static int no_sched_irq_time;
+static int no_tsc_watchdog;
 
 static int __init tsc_setup(char *str)
 {
@@ -292,6 +293,8 @@ static int __init tsc_setup(char *str)
 		no_sched_irq_time = 1;
 	if (!strcmp(str, "unstable"))
 		mark_tsc_unstable("boot parameter");
+	if (!strcmp(str, "nowatchdog"))
+		no_tsc_watchdog = 1;
 	return 1;
 }
 
@@ -1349,7 +1352,7 @@ static int __init init_tsc_clocksource(void)
 	if (tsc_unstable)
 		goto unreg;
 
-	if (tsc_clocksource_reliable)
+	if (tsc_clocksource_reliable || no_tsc_watchdog)
 		clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
 
 	if (boot_cpu_has(X86_FEATURE_NONSTOP_TSC_S3))

commit a786ef152cdcfebc923a67f63c7815806eefcf81
Author: Daniel Vacek <neelx@redhat.com>
Date:   Mon Nov 5 18:10:40 2018 +0100

    x86/tsc: Make calibration refinement more robust
    
    The threshold in tsc_read_refs() is constant which may favor slower CPUs
    but may not be optimal for simple reading of reference on faster ones.
    
    Hence make it proportional to tsc_khz when available to compensate for
    this. The threshold guards against any disturbance like IRQs, NMIs, SMIs
    or CPU stealing by host on guest systems so rename it accordingly and
    fix comments as well.
    
    Also on some systems there is noticeable DMI bus contention at some point
    during boot keeping the readout failing (observed with about one in ~300
    boots when testing). In that case retry also the second readout instead of
    simply bailing out unrefined. Usually the next second the readout returns
    fast just fine without any issues.
    
    Signed-off-by: Daniel Vacek <neelx@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/1541437840-29293-1-git-send-email-neelx@redhat.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index e9f777bfed40..3fae23834069 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -297,15 +297,16 @@ static int __init tsc_setup(char *str)
 
 __setup("tsc=", tsc_setup);
 
-#define MAX_RETRIES     5
-#define SMI_TRESHOLD    50000
+#define MAX_RETRIES		5
+#define TSC_DEFAULT_THRESHOLD	0x20000
 
 /*
- * Read TSC and the reference counters. Take care of SMI disturbance
+ * Read TSC and the reference counters. Take care of any disturbances
  */
 static u64 tsc_read_refs(u64 *p, int hpet)
 {
 	u64 t1, t2;
+	u64 thresh = tsc_khz ? tsc_khz >> 5 : TSC_DEFAULT_THRESHOLD;
 	int i;
 
 	for (i = 0; i < MAX_RETRIES; i++) {
@@ -315,7 +316,7 @@ static u64 tsc_read_refs(u64 *p, int hpet)
 		else
 			*p = acpi_pm_read_early();
 		t2 = get_cycles();
-		if ((t2 - t1) < SMI_TRESHOLD)
+		if ((t2 - t1) < thresh)
 			return t2;
 	}
 	return ULLONG_MAX;
@@ -703,15 +704,15 @@ static unsigned long pit_hpet_ptimer_calibrate_cpu(void)
 	 * zero. In each wait loop iteration we read the TSC and check
 	 * the delta to the previous read. We keep track of the min
 	 * and max values of that delta. The delta is mostly defined
-	 * by the IO time of the PIT access, so we can detect when a
-	 * SMI/SMM disturbance happened between the two reads. If the
+	 * by the IO time of the PIT access, so we can detect when
+	 * any disturbance happened between the two reads. If the
 	 * maximum time is significantly larger than the minimum time,
 	 * then we discard the result and have another try.
 	 *
 	 * 2) Reference counter. If available we use the HPET or the
 	 * PMTIMER as a reference to check the sanity of that value.
 	 * We use separate TSC readouts and check inside of the
-	 * reference read for a SMI/SMM disturbance. We dicard
+	 * reference read for any possible disturbance. We dicard
 	 * disturbed values here as well. We do that around the PIT
 	 * calibration delay loop as we have to wait for a certain
 	 * amount of time anyway.
@@ -744,7 +745,7 @@ static unsigned long pit_hpet_ptimer_calibrate_cpu(void)
 		if (ref1 == ref2)
 			continue;
 
-		/* Check, whether the sampling was disturbed by an SMI */
+		/* Check, whether the sampling was disturbed */
 		if (tsc1 == ULLONG_MAX || tsc2 == ULLONG_MAX)
 			continue;
 
@@ -1268,7 +1269,7 @@ static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
  */
 static void tsc_refine_calibration_work(struct work_struct *work)
 {
-	static u64 tsc_start = -1, ref_start;
+	static u64 tsc_start = ULLONG_MAX, ref_start;
 	static int hpet;
 	u64 tsc_stop, ref_stop, delta;
 	unsigned long freq;
@@ -1283,14 +1284,15 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 	 * delayed the first time we expire. So set the workqueue
 	 * again once we know timers are working.
 	 */
-	if (tsc_start == -1) {
+	if (tsc_start == ULLONG_MAX) {
+restart:
 		/*
 		 * Only set hpet once, to avoid mixing hardware
 		 * if the hpet becomes enabled later.
 		 */
 		hpet = is_hpet_enabled();
-		schedule_delayed_work(&tsc_irqwork, HZ);
 		tsc_start = tsc_read_refs(&ref_start, hpet);
+		schedule_delayed_work(&tsc_irqwork, HZ);
 		return;
 	}
 
@@ -1300,9 +1302,9 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 	if (ref_start == ref_stop)
 		goto out;
 
-	/* Check, whether the sampling was disturbed by an SMI */
-	if (tsc_start == ULLONG_MAX || tsc_stop == ULLONG_MAX)
-		goto out;
+	/* Check, whether the sampling was disturbed */
+	if (tsc_stop == ULLONG_MAX)
+		goto restart;
 
 	delta = tsc_stop - tsc_start;
 	delta *= 1000000LL;

commit f682a7920baf7b721d01dd317f3b532265357cbb
Merge: 99792e0cea1e 3a025de64bf8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 17:54:58 2018 +0100

    Merge branch 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 paravirt updates from Ingo Molnar:
     "Two main changes:
    
       - Remove no longer used parts of the paravirt infrastructure and put
         large quantities of paravirt ops under a new config option
         PARAVIRT_XXL=y, which is selected by XEN_PV only. (Joergen Gross)
    
       - Enable PV spinlocks on Hyperv (Yi Sun)"
    
    * 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/hyperv: Enable PV qspinlock for Hyper-V
      x86/hyperv: Add GUEST_IDLE_MSR support
      x86/paravirt: Clean up native_patch()
      x86/paravirt: Prevent redefinition of SAVE_FLAGS macro
      x86/xen: Make xen_reservation_lock static
      x86/paravirt: Remove unneeded mmu related paravirt ops bits
      x86/paravirt: Move the Xen-only pv_mmu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the pv_irq_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the Xen-only pv_cpu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move items in pv_info under PARAVIRT_XXL umbrella
      x86/paravirt: Introduce new config option PARAVIRT_XXL
      x86/paravirt: Remove unused paravirt bits
      x86/paravirt: Use a single ops structure
      x86/paravirt: Remove clobbers from struct paravirt_patch_site
      x86/paravirt: Remove clobbers parameter from paravirt patch functions
      x86/paravirt: Make paravirt_patch_call() and paravirt_patch_jmp() static
      x86/xen: Add SPDX identifier in arch/x86/xen files
      x86/xen: Link platform-pci-unplug.o only if CONFIG_XEN_PVHVM
      x86/xen: Move pv specific parts of arch/x86/xen/mmu.c to mmu_pv.c
      x86/xen: Move pv irq related functions under CONFIG_XEN_PV umbrella

commit c05f3642f4304dd081876e77a68555b6aba4483f
Merge: 0200fbdd4315 dda93b45389f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 13:32:18 2018 +0100

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf updates from Ingo Molnar:
     "The main updates in this cycle were:
    
       - Lots of perf tooling changes too voluminous to list (big perf trace
         and perf stat improvements, lots of libtraceevent reorganization,
         etc.), so I'll list the authors and refer to the changelog for
         details:
    
           Benjamin Peterson, Jérémie Galarneau, Kim Phillips, Peter
           Zijlstra, Ravi Bangoria, Sangwon Hong, Sean V Kelley, Steven
           Rostedt, Thomas Gleixner, Ding Xiang, Eduardo Habkost, Thomas
           Richter, Andi Kleen, Sanskriti Sharma, Adrian Hunter, Tzvetomir
           Stoyanov, Arnaldo Carvalho de Melo, Jiri Olsa.
    
         ... with the bulk of the changes written by Jiri Olsa, Tzvetomir
         Stoyanov and Arnaldo Carvalho de Melo.
    
       - Continued intel_rdt work with a focus on playing well with perf
         events. This also imported some non-perf RDT work due to
         dependencies. (Reinette Chatre)
    
       - Implement counter freezing for Arch Perfmon v4 (Skylake and newer).
         This allows to speed up the PMI handler by avoiding unnecessary MSR
         writes and make it more accurate. (Andi Kleen)
    
       - kprobes cleanups and simplification (Masami Hiramatsu)
    
       - Intel Goldmont PMU updates (Kan Liang)
    
       - ... plus misc other fixes and updates"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (155 commits)
      kprobes/x86: Use preempt_enable() in optimized_callback()
      x86/intel_rdt: Prevent pseudo-locking from using stale pointers
      kprobes, x86/ptrace.h: Make regs_get_kernel_stack_nth() not fault on bad stack
      perf/x86/intel: Export mem events only if there's PEBS support
      x86/cpu: Drop pointless static qualifier in punit_dev_state_show()
      x86/intel_rdt: Fix initial allocation to consider CDP
      x86/intel_rdt: CBM overlap should also check for overlap with CDP peer
      x86/intel_rdt: Introduce utility to obtain CDP peer
      tools lib traceevent, perf tools: Move struct tep_handler definition in a local header file
      tools lib traceevent: Separate out tep_strerror() for strerror_r() issues
      perf python: More portable way to make CFLAGS work with clang
      perf python: Make clang_has_option() work on Python 3
      perf tools: Free temporary 'sys' string in read_event_files()
      perf tools: Avoid double free in read_event_file()
      perf tools: Free 'printk' string in parse_ftrace_printk()
      perf tools: Cleanup trace-event-info 'tdata' leak
      perf strbuf: Match va_{add,copy} with va_end
      perf test: S390 does not support watchpoints in test 22
      perf auxtrace: Include missing asm/bitsperlong.h to get BITS_PER_LONG
      tools include: Adopt linux/bits.h
      ...

commit dda93b45389f025fd3422d22cc31cc1ea6040305
Merge: 2e62024c265a b61b8bba18fe
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Oct 23 12:30:19 2018 +0200

    Merge branch 'x86/cache' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 4907c68abd3f60f650f98d5a69d4ec77c0bde44f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 11 12:38:26 2018 +0200

    x86/tsc: Force inlining of cyc2ns bits
    
    Looking at the asm for native_sched_clock() I noticed we don't inline
    enough. Mostly caused by sharing code with cyc2ns_read_begin(), which
    we didn't used to do. So mark all that __force_inline to make it DTRT.
    
    Fixes: 59eaef78bfea ("x86/tsc: Remodel cyc2ns to use seqcount_latch()")
    Reported-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: hpa@zytor.com
    Cc: eric.dumazet@gmail.com
    Cc: bp@alien8.de
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181011104019.695196158@infradead.org

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index b52bd2b6cdb4..6d5dc5dabfd7 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -58,7 +58,7 @@ struct cyc2ns {
 
 static DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
 
-void cyc2ns_read_begin(struct cyc2ns_data *data)
+void __always_inline cyc2ns_read_begin(struct cyc2ns_data *data)
 {
 	int seq, idx;
 
@@ -75,7 +75,7 @@ void cyc2ns_read_begin(struct cyc2ns_data *data)
 	} while (unlikely(seq != this_cpu_read(cyc2ns.seq.sequence)));
 }
 
-void cyc2ns_read_end(void)
+void __always_inline cyc2ns_read_end(void)
 {
 	preempt_enable_notrace();
 }
@@ -104,7 +104,7 @@ void cyc2ns_read_end(void)
  *                      -johnstul@us.ibm.com "math is hard, lets go shopping!"
  */
 
-static inline unsigned long long cycles_2_ns(unsigned long long cyc)
+static __always_inline unsigned long long cycles_2_ns(unsigned long long cyc)
 {
 	struct cyc2ns_data data;
 	unsigned long long ns;

commit 2647c43c7f3ba4b752bfce261d53b16e2f5bc9e3
Author: Mike Travis <mike.travis@hpe.com>
Date:   Tue Oct 2 13:01:46 2018 -0500

    x86/tsc: Fix UV TSC initialization
    
    The recent rework of the TSC calibration code introduced a regression on UV
    systems as it added a call to tsc_early_init() which initializes the TSC
    ADJUST values before acpi_boot_table_init().  In the case of UV systems,
    that is a necessary step that calls uv_system_init().  This informs
    tsc_sanitize_first_cpu() that the kernel runs on a platform with async TSC
    resets as documented in commit 341102c3ef29 ("x86/tsc: Add option that TSC
    on Socket 0 being non-zero is valid")
    
    Fix it by skipping the early tsc initialization on UV systems and let TSC
    init tests take place later in tsc_init().
    
    Fixes: cf7a63ef4e02 ("x86/tsc: Calibrate tsc only once")
    Suggested-by: Hedi Berriche <hedi.berriche@hpe.com>
    Signed-off-by: Mike Travis <mike.travis@hpe.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Russ Anderson <rja@hpe.com>
    Reviewed-by: Dimitri Sivanich <sivanich@hpe.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Russ Anderson <russ.anderson@hpe.com>
    Cc: Dimitri Sivanich <dimitri.sivanich@hpe.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Cc: Xiaoming Gao <gxm.linux.kernel@gmail.com>
    Cc: Rajvi Jingar <rajvi.jingar@intel.com>
    Link: https://lkml.kernel.org/r/20181002180144.923579706@stormcage.americas.sgi.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 6490f618e096..b52bd2b6cdb4 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -26,6 +26,7 @@
 #include <asm/apic.h>
 #include <asm/intel-family.h>
 #include <asm/i8259.h>
+#include <asm/uv/uv.h>
 
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
@@ -1433,6 +1434,9 @@ void __init tsc_early_init(void)
 {
 	if (!boot_cpu_has(X86_FEATURE_TSC))
 		return;
+	/* Don't change UV TSC multi-chassis synchronization */
+	if (is_early_uv_system())
+		return;
 	if (!determine_cpu_tsc_frequencies(true))
 		return;
 	loops_per_jiffy = get_loops_per_jiffy();

commit f2c4db1bd80720cd8cb2a5aa220d9bc9f374f04e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 7 10:17:27 2018 -0700

    x86/cpu: Sanitize FAM6_ATOM naming
    
    Going primarily by:
    
      https://en.wikipedia.org/wiki/List_of_Intel_Atom_microprocessors
    
    with additional information gleaned from other related pages; notably:
    
     - Bonnell shrink was called Saltwell
     - Moorefield is the Merriefield refresh which makes it Airmont
    
    The general naming scheme is: FAM6_ATOM_UARCH_SOCTYPE
    
      for i in `git grep -l FAM6_ATOM` ; do
            sed -i  -e 's/ATOM_PINEVIEW/ATOM_BONNELL/g'             \
                    -e 's/ATOM_LINCROFT/ATOM_BONNELL_MID/'          \
                    -e 's/ATOM_PENWELL/ATOM_SALTWELL_MID/g'         \
                    -e 's/ATOM_CLOVERVIEW/ATOM_SALTWELL_TABLET/g'   \
                    -e 's/ATOM_CEDARVIEW/ATOM_SALTWELL/g'           \
                    -e 's/ATOM_SILVERMONT1/ATOM_SILVERMONT/g'       \
                    -e 's/ATOM_SILVERMONT2/ATOM_SILVERMONT_X/g'     \
                    -e 's/ATOM_MERRIFIELD/ATOM_SILVERMONT_MID/g'    \
                    -e 's/ATOM_MOOREFIELD/ATOM_AIRMONT_MID/g'       \
                    -e 's/ATOM_DENVERTON/ATOM_GOLDMONT_X/g'         \
                    -e 's/ATOM_GEMINI_LAKE/ATOM_GOLDMONT_PLUS/g' ${i}
      done
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: dave.hansen@linux.intel.com
    Cc: len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 6490f618e096..dd6b564f65e3 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -635,7 +635,7 @@ unsigned long native_calibrate_tsc(void)
 		case INTEL_FAM6_KABYLAKE_DESKTOP:
 			crystal_khz = 24000;	/* 24.0 MHz */
 			break;
-		case INTEL_FAM6_ATOM_DENVERTON:
+		case INTEL_FAM6_ATOM_GOLDMONT_X:
 			crystal_khz = 25000;	/* 25.0 MHz */
 			break;
 		case INTEL_FAM6_ATOM_GOLDMONT:

commit 17f6bac2249356c795339e03a0742cd79be3cab8
Author: Chuanhua Lei <chuanhua.lei@linux.intel.com>
Date:   Thu Sep 6 18:03:23 2018 +0800

    x86/tsc: Prevent result truncation on 32bit
    
    Loops per jiffy is calculated by multiplying tsc_khz with 1e3 and then
    dividing it by HZ.
    
    Both tsc_khz and the temporary variable holding the multiplication result
    are of type unsigned long, so on 32bit the result is truncated to the lower
    32bit.
    
    Use u64 as type for the temporary variable and cast tsc_khz to it before
    multiplying.
    
    [ tglx: Massaged changelog and removed pointless braces ]
    
    Fixes: cf7a63ef4e02 ("x86/tsc: Calibrate tsc only once")
    Signed-off-by: Chuanhua Lei <chuanhua.lei@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: yixin.zhu@linux.intel.com
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Pavel Tatashin <pasha.tatashin@microsoft.com>
    Cc: Rajvi Jingar <rajvi.jingar@intel.com>
    Cc: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Link: https://lkml.kernel.org/r/1536228203-18701-1-git-send-email-chuanhua.lei@linux.intel.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 1463468ba9a0..6490f618e096 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1415,7 +1415,7 @@ static bool __init determine_cpu_tsc_frequencies(bool early)
 
 static unsigned long __init get_loops_per_jiffy(void)
 {
-	unsigned long lpj = tsc_khz * KHZ;
+	u64 lpj = (u64)tsc_khz * KHZ;
 
 	do_div(lpj, HZ);
 	return lpj;

commit 5c83511bdb9832c86be20fb86b783356e2f58062
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Aug 28 09:40:19 2018 +0200

    x86/paravirt: Use a single ops structure
    
    Instead of using six globally visible paravirt ops structures combine
    them in a single structure, keeping the original structures as
    sub-structures.
    
    This avoids the need to assemble struct paravirt_patch_template at
    runtime on the stack each time apply_paravirt() is being called (i.e.
    when loading a module).
    
    [ tglx: Made the struct and the initializer tabular for readability sake ]
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel@lists.xenproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: boris.ostrovsky@oracle.com
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/20180828074026.820-9-jgross@suse.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 1463468ba9a0..9044aa5e2389 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -246,7 +246,7 @@ unsigned long long sched_clock(void)
 
 bool using_native_sched_clock(void)
 {
-	return pv_time_ops.sched_clock == native_sched_clock;
+	return pv_ops.time.sched_clock == native_sched_clock;
 }
 #else
 unsigned long long

commit 608008a45798fe9e2aee04f99b5270ea57c1376f
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Mon Jul 30 15:54:20 2018 +0800

    x86/tsc: Consolidate init code
    
    Split out suplicated code from tsc_early_init() and tsc_init() into a
    common helper and fixup some comment typos.
    
    [ tglx: Massaged changelog and renamed function ]
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: <hpa@zytor.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/20180730075421.22830-2-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 02e416b87ac1..1463468ba9a0 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -182,7 +182,7 @@ static void __init cyc2ns_init_boot_cpu(void)
 }
 
 /*
- * Secondary CPUs do not run through cyc2ns_init(), so set up
+ * Secondary CPUs do not run through tsc_init(), so set up
  * all the scale factors for all CPUs, assuming the same
  * speed as the bootup CPU. (cpufreq notifiers will fix this
  * up if their speed diverges)
@@ -1389,7 +1389,7 @@ static bool __init determine_cpu_tsc_frequencies(bool early)
 	}
 
 	/*
-	 * Trust non-zero tsc_khz as authorative,
+	 * Trust non-zero tsc_khz as authoritative,
 	 * and use it to sanity check cpu_khz,
 	 * which will be off if system timer is off.
 	 */
@@ -1421,6 +1421,14 @@ static unsigned long __init get_loops_per_jiffy(void)
 	return lpj;
 }
 
+static void __init tsc_enable_sched_clock(void)
+{
+	/* Sanitize TSC ADJUST before cyc2ns gets initialized */
+	tsc_store_and_check_tsc_adjust(true);
+	cyc2ns_init_boot_cpu();
+	static_branch_enable(&__use_tsc);
+}
+
 void __init tsc_early_init(void)
 {
 	if (!boot_cpu_has(X86_FEATURE_TSC))
@@ -1429,10 +1437,7 @@ void __init tsc_early_init(void)
 		return;
 	loops_per_jiffy = get_loops_per_jiffy();
 
-	/* Sanitize TSC ADJUST before cyc2ns gets initialized */
-	tsc_store_and_check_tsc_adjust(true);
-	cyc2ns_init_boot_cpu();
-	static_branch_enable(&__use_tsc);
+	tsc_enable_sched_clock();
 }
 
 void __init tsc_init(void)
@@ -1456,13 +1461,10 @@ void __init tsc_init(void)
 			setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
 			return;
 		}
-		/* Sanitize TSC ADJUST before cyc2ns gets initialized */
-		tsc_store_and_check_tsc_adjust(true);
-		cyc2ns_init_boot_cpu();
+		tsc_enable_sched_clock();
 	}
 
 	cyc2ns_init_secondary_cpus();
-	static_branch_enable(&__use_tsc);
 
 	if (!no_sched_irq_time)
 		enable_sched_clock_irqtime();

commit 8dbe438589f373544a1af8b4a859e4da853c0f90
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:45 2018 -0400

    x86/tsc: Make use of tsc_calibrate_cpu_early()
    
    During early boot enable tsc_calibrate_cpu_early() and switch to
    tsc_calibrate_cpu() only later. Do this unconditionally, because it is
    unknown what methods other cpus will use to calibrate once they are
    onlined.
    
    If by the time tsc_init() is called tsc frequency is still unknown do only
    pit_hpet_ptimer_calibrate_cpu() to calibrate, as this function contains the
    only methods wich have not been called and tried earlier.
    
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-27-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 60586779b02c..02e416b87ac1 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -854,7 +854,7 @@ unsigned long native_calibrate_cpu_early(void)
 /**
  * native_calibrate_cpu - calibrate the cpu
  */
-unsigned long native_calibrate_cpu(void)
+static unsigned long native_calibrate_cpu(void)
 {
 	unsigned long tsc_freq = native_calibrate_cpu_early();
 
@@ -1374,13 +1374,19 @@ static int __init init_tsc_clocksource(void)
  */
 device_initcall(init_tsc_clocksource);
 
-static bool __init determine_cpu_tsc_frequencies(void)
+static bool __init determine_cpu_tsc_frequencies(bool early)
 {
 	/* Make sure that cpu and tsc are not already calibrated */
 	WARN_ON(cpu_khz || tsc_khz);
 
-	cpu_khz = x86_platform.calibrate_cpu();
-	tsc_khz = x86_platform.calibrate_tsc();
+	if (early) {
+		cpu_khz = x86_platform.calibrate_cpu();
+		tsc_khz = x86_platform.calibrate_tsc();
+	} else {
+		/* We should not be here with non-native cpu calibration */
+		WARN_ON(x86_platform.calibrate_cpu != native_calibrate_cpu);
+		cpu_khz = pit_hpet_ptimer_calibrate_cpu();
+	}
 
 	/*
 	 * Trust non-zero tsc_khz as authorative,
@@ -1419,7 +1425,7 @@ void __init tsc_early_init(void)
 {
 	if (!boot_cpu_has(X86_FEATURE_TSC))
 		return;
-	if (!determine_cpu_tsc_frequencies())
+	if (!determine_cpu_tsc_frequencies(true))
 		return;
 	loops_per_jiffy = get_loops_per_jiffy();
 
@@ -1431,6 +1437,13 @@ void __init tsc_early_init(void)
 
 void __init tsc_init(void)
 {
+	/*
+	 * native_calibrate_cpu_early can only calibrate using methods that are
+	 * available early in boot.
+	 */
+	if (x86_platform.calibrate_cpu == native_calibrate_cpu_early)
+		x86_platform.calibrate_cpu = native_calibrate_cpu;
+
 	if (!boot_cpu_has(X86_FEATURE_TSC)) {
 		setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
 		return;
@@ -1438,7 +1451,7 @@ void __init tsc_init(void)
 
 	if (!tsc_khz) {
 		/* We failed to determine frequencies earlier, try again */
-		if (!determine_cpu_tsc_frequencies()) {
+		if (!determine_cpu_tsc_frequencies(false)) {
 			mark_tsc_unstable("could not calculate TSC khz");
 			setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
 			return;

commit 03821f451d2d2d7599061244734245be139014ea
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:44 2018 -0400

    x86/tsc: Split native_calibrate_cpu() into early and late parts
    
    During early boot TSC and CPU frequency can be calibrated using MSR, CPUID,
    and quick PIT calibration methods. The other methods PIT/HPET/PMTIMER are
    available only after ACPI is initialized.
    
    Split native_calibrate_cpu() into early and late parts so they can be
    called separately during early and late tsc calibration.
    
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-26-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 9277ae9b68b3..60586779b02c 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -680,30 +680,17 @@ static unsigned long cpu_khz_from_cpuid(void)
 	return eax_base_mhz * 1000;
 }
 
-/**
- * native_calibrate_cpu - calibrate the cpu on boot
+/*
+ * calibrate cpu using pit, hpet, and ptimer methods. They are available
+ * later in boot after acpi is initialized.
  */
-unsigned long native_calibrate_cpu(void)
+static unsigned long pit_hpet_ptimer_calibrate_cpu(void)
 {
 	u64 tsc1, tsc2, delta, ref1, ref2;
 	unsigned long tsc_pit_min = ULONG_MAX, tsc_ref_min = ULONG_MAX;
-	unsigned long flags, latch, ms, fast_calibrate;
+	unsigned long flags, latch, ms;
 	int hpet = is_hpet_enabled(), i, loopmin;
 
-	fast_calibrate = cpu_khz_from_cpuid();
-	if (fast_calibrate)
-		return fast_calibrate;
-
-	fast_calibrate = cpu_khz_from_msr();
-	if (fast_calibrate)
-		return fast_calibrate;
-
-	local_irq_save(flags);
-	fast_calibrate = quick_pit_calibrate();
-	local_irq_restore(flags);
-	if (fast_calibrate)
-		return fast_calibrate;
-
 	/*
 	 * Run 5 calibration loops to get the lowest frequency value
 	 * (the best estimate). We use two different calibration modes
@@ -846,6 +833,37 @@ unsigned long native_calibrate_cpu(void)
 	return tsc_pit_min;
 }
 
+/**
+ * native_calibrate_cpu_early - can calibrate the cpu early in boot
+ */
+unsigned long native_calibrate_cpu_early(void)
+{
+	unsigned long flags, fast_calibrate = cpu_khz_from_cpuid();
+
+	if (!fast_calibrate)
+		fast_calibrate = cpu_khz_from_msr();
+	if (!fast_calibrate) {
+		local_irq_save(flags);
+		fast_calibrate = quick_pit_calibrate();
+		local_irq_restore(flags);
+	}
+	return fast_calibrate;
+}
+
+
+/**
+ * native_calibrate_cpu - calibrate the cpu
+ */
+unsigned long native_calibrate_cpu(void)
+{
+	unsigned long tsc_freq = native_calibrate_cpu_early();
+
+	if (!tsc_freq)
+		tsc_freq = pit_hpet_ptimer_calibrate_cpu();
+
+	return tsc_freq;
+}
+
 void recalibrate_cpu_khz(void)
 {
 #ifndef CONFIG_SMP

commit 4763f03d3d186ce8a1125844790152d76804ad60
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:40 2018 -0400

    x86/tsc: Use TSC as sched clock early
    
    All prerequesites for enabling TSC as sched clock early in the boot
    process are available now:
    
     - Early attempt of TSC calibration
    
     - Early availablity of static branch patching
    
    If TSC frequency can be established in the early calibration, enable the
    static key which switches sched clock to use TSC.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-22-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 7ea0718a4c75..9277ae9b68b3 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1408,6 +1408,7 @@ void __init tsc_early_init(void)
 	/* Sanitize TSC ADJUST before cyc2ns gets initialized */
 	tsc_store_and_check_tsc_adjust(true);
 	cyc2ns_init_boot_cpu();
+	static_branch_enable(&__use_tsc);
 }
 
 void __init tsc_init(void)

commit e2a9ca29b5edc89da2fddeae30e1070b272395c5
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:39 2018 -0400

    x86/tsc: Initialize cyc2ns when tsc frequency is determined
    
    cyc2ns converts tsc to nanoseconds, and it is handled in a per-cpu data
    structure.
    
    Currently, the setup code for c2ns data for every possible CPU goes through
    the same sequence of calculations as for the boot CPU, but is based on the
    same tsc frequency as the boot CPU, and thus this is not necessary.
    
    Initialize the boot cpu when tsc frequency is determined. Copy the
    calculated data from the boot CPU to the other CPUs in tsc_init().
    
    In addition do the following:
    
     - Remove unnecessary zeroing of c2ns data by removing cyc2ns_data_init()
    
     - Split set_cyc2ns_scale() into two functions, so set_cyc2ns_scale() can be
       called when system is up, and wraps around __set_cyc2ns_scale() that can
       be called directly when system is booting but avoids saving restoring
       IRQs and going and waking up from idle.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-21-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 4cab2236169e..7ea0718a4c75 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -103,23 +103,6 @@ void cyc2ns_read_end(void)
  *                      -johnstul@us.ibm.com "math is hard, lets go shopping!"
  */
 
-static void cyc2ns_data_init(struct cyc2ns_data *data)
-{
-	data->cyc2ns_mul = 0;
-	data->cyc2ns_shift = 0;
-	data->cyc2ns_offset = 0;
-}
-
-static void __init cyc2ns_init(int cpu)
-{
-	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
-
-	cyc2ns_data_init(&c2n->data[0]);
-	cyc2ns_data_init(&c2n->data[1]);
-
-	seqcount_init(&c2n->seq);
-}
-
 static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 {
 	struct cyc2ns_data data;
@@ -135,18 +118,11 @@ static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 	return ns;
 }
 
-static void set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)
+static void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)
 {
 	unsigned long long ns_now;
 	struct cyc2ns_data data;
 	struct cyc2ns *c2n;
-	unsigned long flags;
-
-	local_irq_save(flags);
-	sched_clock_idle_sleep_event();
-
-	if (!khz)
-		goto done;
 
 	ns_now = cycles_2_ns(tsc_now);
 
@@ -178,12 +154,55 @@ static void set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_
 	c2n->data[0] = data;
 	raw_write_seqcount_latch(&c2n->seq);
 	c2n->data[1] = data;
+}
+
+static void set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	sched_clock_idle_sleep_event();
+
+	if (khz)
+		__set_cyc2ns_scale(khz, cpu, tsc_now);
 
-done:
 	sched_clock_idle_wakeup_event();
 	local_irq_restore(flags);
 }
 
+/*
+ * Initialize cyc2ns for boot cpu
+ */
+static void __init cyc2ns_init_boot_cpu(void)
+{
+	struct cyc2ns *c2n = this_cpu_ptr(&cyc2ns);
+
+	seqcount_init(&c2n->seq);
+	__set_cyc2ns_scale(tsc_khz, smp_processor_id(), rdtsc());
+}
+
+/*
+ * Secondary CPUs do not run through cyc2ns_init(), so set up
+ * all the scale factors for all CPUs, assuming the same
+ * speed as the bootup CPU. (cpufreq notifiers will fix this
+ * up if their speed diverges)
+ */
+static void __init cyc2ns_init_secondary_cpus(void)
+{
+	unsigned int cpu, this_cpu = smp_processor_id();
+	struct cyc2ns *c2n = this_cpu_ptr(&cyc2ns);
+	struct cyc2ns_data *data = c2n->data;
+
+	for_each_possible_cpu(cpu) {
+		if (cpu != this_cpu) {
+			seqcount_init(&c2n->seq);
+			c2n = per_cpu_ptr(&cyc2ns, cpu);
+			c2n->data[0] = data[0];
+			c2n->data[1] = data[1];
+		}
+	}
+}
+
 /*
  * Scheduler clock - returns current time in nanosec units.
  */
@@ -1385,6 +1404,10 @@ void __init tsc_early_init(void)
 	if (!determine_cpu_tsc_frequencies())
 		return;
 	loops_per_jiffy = get_loops_per_jiffy();
+
+	/* Sanitize TSC ADJUST before cyc2ns gets initialized */
+	tsc_store_and_check_tsc_adjust(true);
+	cyc2ns_init_boot_cpu();
 }
 
 void __init tsc_init(void)
@@ -1401,23 +1424,12 @@ void __init tsc_init(void)
 			setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
 			return;
 		}
+		/* Sanitize TSC ADJUST before cyc2ns gets initialized */
+		tsc_store_and_check_tsc_adjust(true);
+		cyc2ns_init_boot_cpu();
 	}
 
-	/* Sanitize TSC ADJUST before cyc2ns gets initialized */
-	tsc_store_and_check_tsc_adjust(true);
-
-	/*
-	 * Secondary CPUs do not run through tsc_init(), so set up
-	 * all the scale factors for all CPUs, assuming the same
-	 * speed as the bootup CPU. (cpufreq notifiers will fix this
-	 * up if their speed diverges)
-	 */
-	cyc = rdtsc();
-	for_each_possible_cpu(cpu) {
-		cyc2ns_init(cpu);
-		set_cyc2ns_scale(tsc_khz, cpu, cyc);
-	}
-
+	cyc2ns_init_secondary_cpus();
 	static_branch_enable(&__use_tsc);
 
 	if (!no_sched_irq_time)

commit cf7a63ef4e0203f6f33284c69e8188d91422de83
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:38 2018 -0400

    x86/tsc: Calibrate tsc only once
    
    During boot tsc is calibrated twice: once in tsc_early_delay_calibrate(),
    and the second time in tsc_init().
    
    Rename tsc_early_delay_calibrate() to tsc_early_init(), and rework it so
    the calibration is done only early, and make tsc_init() to use the values
    already determined in tsc_early_init().
    
    Sometimes it is not possible to determine tsc early, as the subsystem that
    is required is not yet initialized, in such case try again later in
    tsc_init().
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-20-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 186395041725..4cab2236169e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -33,6 +33,8 @@ EXPORT_SYMBOL(cpu_khz);
 unsigned int __read_mostly tsc_khz;
 EXPORT_SYMBOL(tsc_khz);
 
+#define KHZ	1000
+
 /*
  * TSC can be unstable due to cpufreq or due to unsynced TSCs
  */
@@ -1335,34 +1337,10 @@ static int __init init_tsc_clocksource(void)
  */
 device_initcall(init_tsc_clocksource);
 
-void __init tsc_early_delay_calibrate(void)
-{
-	unsigned long lpj;
-
-	if (!boot_cpu_has(X86_FEATURE_TSC))
-		return;
-
-	cpu_khz = x86_platform.calibrate_cpu();
-	tsc_khz = x86_platform.calibrate_tsc();
-
-	tsc_khz = tsc_khz ? : cpu_khz;
-	if (!tsc_khz)
-		return;
-
-	lpj = tsc_khz * 1000;
-	do_div(lpj, HZ);
-	loops_per_jiffy = lpj;
-}
-
-void __init tsc_init(void)
+static bool __init determine_cpu_tsc_frequencies(void)
 {
-	u64 lpj, cyc;
-	int cpu;
-
-	if (!boot_cpu_has(X86_FEATURE_TSC)) {
-		setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
-		return;
-	}
+	/* Make sure that cpu and tsc are not already calibrated */
+	WARN_ON(cpu_khz || tsc_khz);
 
 	cpu_khz = x86_platform.calibrate_cpu();
 	tsc_khz = x86_platform.calibrate_tsc();
@@ -1377,20 +1355,52 @@ void __init tsc_init(void)
 	else if (abs(cpu_khz - tsc_khz) * 10 > tsc_khz)
 		cpu_khz = tsc_khz;
 
-	if (!tsc_khz) {
-		mark_tsc_unstable("could not calculate TSC khz");
-		setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
-		return;
-	}
+	if (tsc_khz == 0)
+		return false;
 
 	pr_info("Detected %lu.%03lu MHz processor\n",
-		(unsigned long)cpu_khz / 1000,
-		(unsigned long)cpu_khz % 1000);
+		(unsigned long)cpu_khz / KHZ,
+		(unsigned long)cpu_khz % KHZ);
 
 	if (cpu_khz != tsc_khz) {
 		pr_info("Detected %lu.%03lu MHz TSC",
-			(unsigned long)tsc_khz / 1000,
-			(unsigned long)tsc_khz % 1000);
+			(unsigned long)tsc_khz / KHZ,
+			(unsigned long)tsc_khz % KHZ);
+	}
+	return true;
+}
+
+static unsigned long __init get_loops_per_jiffy(void)
+{
+	unsigned long lpj = tsc_khz * KHZ;
+
+	do_div(lpj, HZ);
+	return lpj;
+}
+
+void __init tsc_early_init(void)
+{
+	if (!boot_cpu_has(X86_FEATURE_TSC))
+		return;
+	if (!determine_cpu_tsc_frequencies())
+		return;
+	loops_per_jiffy = get_loops_per_jiffy();
+}
+
+void __init tsc_init(void)
+{
+	if (!boot_cpu_has(X86_FEATURE_TSC)) {
+		setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
+		return;
+	}
+
+	if (!tsc_khz) {
+		/* We failed to determine frequencies earlier, try again */
+		if (!determine_cpu_tsc_frequencies()) {
+			mark_tsc_unstable("could not calculate TSC khz");
+			setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
+			return;
+		}
 	}
 
 	/* Sanitize TSC ADJUST before cyc2ns gets initialized */
@@ -1413,10 +1423,7 @@ void __init tsc_init(void)
 	if (!no_sched_irq_time)
 		enable_sched_clock_irqtime();
 
-	lpj = ((u64)tsc_khz * 1000);
-	do_div(lpj, HZ);
-	lpj_fine = lpj;
-
+	lpj_fine = get_loops_per_jiffy();
 	use_tsc_delay();
 
 	check_system_tsc_reliable();

commit fe9af81e524e8a86bdd59c0cc0d9e2b0ccaf840f
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:30 2018 -0400

    x86/tsc: Redefine notsc to behave as tsc=unstable
    
    Currently, the notsc kernel parameter disables the use of the TSC by
    sched_clock(). However, this parameter does not prevent the kernel from
    accessing tsc in other places.
    
    The only rationale to boot with notsc is to avoid timing discrepancies on
    multi-socket systems where TSC are not properly synchronized, and thus
    exclude TSC from being used for time keeping. But that prevents using TSC
    as sched_clock() as well, which is not necessary as the core sched_clock()
    implementation can handle non synchronized TSC based sched clocks just
    fine.
    
    However, there is another method to solve the above problem: booting with
    tsc=unstable parameter. This parameter allows sched_clock() to use TSC and
    just excludes it from timekeeping.
    
    So there is no real reason to keep notsc, but for compatibility reasons the
    parameter has to stay. Make it behave like 'tsc=unstable' instead.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-12-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 74392d9d51e0..186395041725 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -38,11 +38,6 @@ EXPORT_SYMBOL(tsc_khz);
  */
 static int __read_mostly tsc_unstable;
 
-/* native_sched_clock() is called before tsc_init(), so
-   we must start with the TSC soft disabled to prevent
-   erroneous rdtsc usage on !boot_cpu_has(X86_FEATURE_TSC) processors */
-static int __read_mostly tsc_disabled = -1;
-
 static DEFINE_STATIC_KEY_FALSE(__use_tsc);
 
 int tsc_clocksource_reliable;
@@ -248,8 +243,7 @@ EXPORT_SYMBOL_GPL(check_tsc_unstable);
 #ifdef CONFIG_X86_TSC
 int __init notsc_setup(char *str)
 {
-	pr_warn("Kernel compiled with CONFIG_X86_TSC, cannot disable TSC completely\n");
-	tsc_disabled = 1;
+	mark_tsc_unstable("boot parameter notsc");
 	return 1;
 }
 #else
@@ -1307,7 +1301,7 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 
 static int __init init_tsc_clocksource(void)
 {
-	if (!boot_cpu_has(X86_FEATURE_TSC) || tsc_disabled > 0 || !tsc_khz)
+	if (!boot_cpu_has(X86_FEATURE_TSC) || !tsc_khz)
 		return 0;
 
 	if (tsc_unstable)
@@ -1414,12 +1408,6 @@ void __init tsc_init(void)
 		set_cyc2ns_scale(tsc_khz, cpu, cyc);
 	}
 
-	if (tsc_disabled > 0)
-		return;
-
-	/* now allow native_sched_clock() to use rdtsc */
-
-	tsc_disabled = 0;
 	static_branch_enable(&__use_tsc);
 
 	if (!no_sched_irq_time)
@@ -1455,7 +1443,7 @@ unsigned long calibrate_delay_is_known(void)
 	int constant_tsc = cpu_has(&cpu_data(cpu), X86_FEATURE_CONSTANT_TSC);
 	const struct cpumask *mask = topology_core_cpumask(cpu);
 
-	if (tsc_disabled || !constant_tsc || !mask)
+	if (!constant_tsc || !mask)
 		return 0;
 
 	sibling = cpumask_any_but(mask, cpu);

commit e3b4f79025e0a4eb7e2a2c7d24dadfa1e38893b0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Apr 30 12:00:12 2018 +0200

    x86/tsc: Fix mark_tsc_unstable()
    
    mark_tsc_unstable() also needs to affect tsc_early, Now that
    clocksource_mark_unstable() can be used on a clocksource irrespective of
    its registration state, use it on both tsc_early and tsc.
    
    This does however require cs->list to be initialized empty, otherwise it
    cannot tell the registation state before registation.
    
    Fixes: aa83c45762a2 ("x86/tsc: Introduce early tsc clocksource")
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Diego Viola <diego.viola@gmail.com>
    Reviewed-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: len.brown@intel.com
    Cc: rjw@rjwysocki.net
    Cc: rui.zhang@intel.com
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180430100344.533326547@infradead.org

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 23f36bc236ba..74392d9d51e0 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1067,6 +1067,7 @@ static struct clocksource clocksource_tsc_early = {
 	.resume			= tsc_resume,
 	.mark_unstable		= tsc_cs_mark_unstable,
 	.tick_stable		= tsc_cs_tick_stable,
+	.list			= LIST_HEAD_INIT(clocksource_tsc_early.list),
 };
 
 /*
@@ -1086,6 +1087,7 @@ static struct clocksource clocksource_tsc = {
 	.resume			= tsc_resume,
 	.mark_unstable		= tsc_cs_mark_unstable,
 	.tick_stable		= tsc_cs_tick_stable,
+	.list			= LIST_HEAD_INIT(clocksource_tsc.list),
 };
 
 void mark_tsc_unstable(char *reason)
@@ -1098,13 +1100,9 @@ void mark_tsc_unstable(char *reason)
 		clear_sched_clock_stable();
 	disable_sched_clock_irqtime();
 	pr_info("Marking TSC unstable due to %s\n", reason);
-	/* Change only the rating, when not registered */
-	if (clocksource_tsc.mult) {
-		clocksource_mark_unstable(&clocksource_tsc);
-	} else {
-		clocksource_tsc.flags |= CLOCK_SOURCE_UNSTABLE;
-		clocksource_tsc.rating = 0;
-	}
+
+	clocksource_mark_unstable(&clocksource_tsc_early);
+	clocksource_mark_unstable(&clocksource_tsc);
 }
 
 EXPORT_SYMBOL_GPL(mark_tsc_unstable);

commit e9088adda13cd23249d4b0abb97ff8a81bf5573a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Apr 30 12:00:09 2018 +0200

    x86/tsc: Always unregister clocksource_tsc_early
    
    Don't leave the tsc-early clocksource registered if it errors out
    early.
    
    This was reported by Diego, who on his Core2 era machine got TSC
    invalidated while it was running with tsc-early (due to C-states).
    This results in keeping tsc-early with very bad effects.
    
    Reported-and-Tested-by: Diego Viola <diego.viola@gmail.com>
    Fixes: aa83c45762a2 ("x86/tsc: Introduce early tsc clocksource")
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: len.brown@intel.com
    Cc: rjw@rjwysocki.net
    Cc: diego.viola@gmail.com
    Cc: rui.zhang@intel.com
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180430100344.350507853@infradead.org

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 91e6da48cbb6..23f36bc236ba 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1244,7 +1244,7 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 
 	/* Don't bother refining TSC on unstable systems */
 	if (tsc_unstable)
-		return;
+		goto unreg;
 
 	/*
 	 * Since the work is started early in boot, we may be
@@ -1297,11 +1297,12 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 
 out:
 	if (tsc_unstable)
-		return;
+		goto unreg;
 
 	if (boot_cpu_has(X86_FEATURE_ART))
 		art_related_clocksource = &clocksource_tsc;
 	clocksource_register_khz(&clocksource_tsc, tsc_khz);
+unreg:
 	clocksource_unregister(&clocksource_tsc_early);
 }
 
@@ -1311,8 +1312,8 @@ static int __init init_tsc_clocksource(void)
 	if (!boot_cpu_has(X86_FEATURE_TSC) || tsc_disabled > 0 || !tsc_khz)
 		return 0;
 
-	if (check_tsc_unstable())
-		return 0;
+	if (tsc_unstable)
+		goto unreg;
 
 	if (tsc_clocksource_reliable)
 		clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
@@ -1328,6 +1329,7 @@ static int __init init_tsc_clocksource(void)
 		if (boot_cpu_has(X86_FEATURE_ART))
 			art_related_clocksource = &clocksource_tsc;
 		clocksource_register_khz(&clocksource_tsc, tsc_khz);
+unreg:
 		clocksource_unregister(&clocksource_tsc_early);
 		return 0;
 	}

commit d3878e164dcd3925a237a20e879432400e369172
Author: Xiaoming Gao <gxm.linux.kernel@gmail.com>
Date:   Fri Apr 13 17:48:08 2018 +0800

    x86/tsc: Prevent 32bit truncation in calc_hpet_ref()
    
    The TSC calibration code uses HPET as reference. The conversion normalizes
    the delta of two HPET timestamps:
    
        hpetref = ((tshpet1 - tshpet2) * HPET_PERIOD) / 1e6
    
    and then divides the normalized delta of the corresponding TSC timestamps
    by the result to calulate the TSC frequency.
    
        tscfreq = ((tstsc1 - tstsc2 ) * 1e6) / hpetref
    
    This uses do_div() which takes an u32 as the divisor, which worked so far
    because the HPET frequency was low enough that 'hpetref' never exceeded
    32bit.
    
    On Skylake machines the HPET frequency increased so 'hpetref' can exceed
    32bit. do_div() truncates the divisor, which causes the calibration to
    fail.
    
    Use div64_u64() to avoid the problem.
    
    [ tglx: Fixes whitespace mangled patch and rewrote changelog ]
    
    Signed-off-by: Xiaoming Gao <newtongao@tencent.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Cc: peterz@infradead.org
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/38894564-4fc9-b8ec-353f-de702839e44e@gmail.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index ef32297ff17e..91e6da48cbb6 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -317,7 +317,7 @@ static unsigned long calc_hpet_ref(u64 deltatsc, u64 hpet1, u64 hpet2)
 	hpet2 -= hpet1;
 	tmp = ((u64)hpet2 * hpet_readl(HPET_PERIOD));
 	do_div(tmp, 1000000);
-	do_div(deltatsc, tmp);
+	deltatsc = div64_u64(deltatsc, tmp);
 
 	return (unsigned long) deltatsc;
 }

commit fc804f65d46236c211f530174904c1ed70db5888
Author: Rajvi Jingar <rajvi.jingar@intel.com>
Date:   Thu Mar 8 09:28:36 2018 -0800

    x86/tsc: Convert ART in nanoseconds to TSC
    
    Device drivers use get_device_system_crosststamp() to produce precise
    system/device cross-timestamps. The PHC clock and ALSA interfaces, for
    example, make the cross-timestamps available to user applications.  On
    Intel platforms, get_device_system_crosststamp() requires a TSC value
    derived from ART (Always Running Timer) to compute the monotonic raw and
    realtime system timestamps.
    
    Starting with Intel Goldmont platforms, the PCIe root complex supports the
    PTM time sync protocol. PTM requires all timestamps to be in units of
    nanoseconds. The Intel root complex hardware propagates system time derived
    from ART in units of nanoseconds performing the conversion as follows:
    
         ART_NS = ART * 1e9 / <crystal frequency>
    
    When user software requests a cross-timestamp, the system timestamps
    (generally read from device registers) must be converted to TSC by the
    driver software as follows:
    
        TSC = ART_NS * TSC_KHZ / 1e6
    
    This is valid when CPU feature flag X86_FEATURE_TSC_KNOWN_FREQ is set
    indicating that tsc_khz is derived from CPUID[15H]. Drivers should check
    whether this flag is set before conversion to TSC is attempted.
    
    Suggested-by: Christopher S. Hall <christopher.s.hall@intel.com>
    Signed-off-by: Rajvi Jingar <rajvi.jingar@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: peterz@infradead.org
    Link: https://lkml.kernel.org/r/1520530116-4925-1-git-send-email-rajvi.jingar@intel.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index fb4302738410..ef32297ff17e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1179,6 +1179,45 @@ struct system_counterval_t convert_art_to_tsc(u64 art)
 }
 EXPORT_SYMBOL(convert_art_to_tsc);
 
+/**
+ * convert_art_ns_to_tsc() - Convert ART in nanoseconds to TSC.
+ * @art_ns: ART (Always Running Timer) in unit of nanoseconds
+ *
+ * PTM requires all timestamps to be in units of nanoseconds. When user
+ * software requests a cross-timestamp, this function converts system timestamp
+ * to TSC.
+ *
+ * This is valid when CPU feature flag X86_FEATURE_TSC_KNOWN_FREQ is set
+ * indicating the tsc_khz is derived from CPUID[15H]. Drivers should check
+ * that this flag is set before conversion to TSC is attempted.
+ *
+ * Return:
+ * struct system_counterval_t - system counter value with the pointer to the
+ *	corresponding clocksource
+ *	@cycles:	System counter value
+ *	@cs:		Clocksource corresponding to system counter value. Used
+ *			by timekeeping code to verify comparibility of two cycle
+ *			values.
+ */
+
+struct system_counterval_t convert_art_ns_to_tsc(u64 art_ns)
+{
+	u64 tmp, res, rem;
+
+	rem = do_div(art_ns, USEC_PER_SEC);
+
+	res = art_ns * tsc_khz;
+	tmp = rem * tsc_khz;
+
+	do_div(tmp, USEC_PER_SEC);
+	res += tmp;
+
+	return (struct system_counterval_t) { .cs = art_related_clocksource,
+					      .cycles = res};
+}
+EXPORT_SYMBOL(convert_art_ns_to_tsc);
+
+
 static void tsc_refine_calibration_work(struct work_struct *work);
 static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
 /**

commit aa83c45762a242acce9b35020363225a7b59d7c9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Dec 22 10:20:13 2017 +0100

    x86/tsc: Introduce early tsc clocksource
    
    Without TSC_KNOWN_FREQ the TSC clocksource is registered so late that the
    kernel first switches to the HPET. Using HPET on large CPU count machines is
    undesirable.
    
    Therefore register a tsc-early clocksource using the preliminary tsc_khz
    from quick calibration. Then when the final TSC calibration is done, it
    can switch to the tuned frequency.
    
    The only notably problem is that the real tsc clocksource must be marked
    with CLOCK_SOURCE_VALID_FOR_HRES, otherwise it will not be selected when
    unregistering tsc-early. tsc-early cannot be left registered, because then
    the clocksource code would fall back to it when we tsc clocksource is
    marked unstable later.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: len.brown@intel.com
    Cc: rui.zhang@intel.com
    Cc: Len Brown <lenb@kernel.org>
    Link: https://lkml.kernel.org/r/20171222092243.431585460@infradead.org

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a2c9dd8bfc6f..fb4302738410 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1006,8 +1006,6 @@ static void __init detect_art(void)
 
 /* clocksource code */
 
-static struct clocksource clocksource_tsc;
-
 static void tsc_resume(struct clocksource *cs)
 {
 	tsc_verify_tsc_adjust(true);
@@ -1058,12 +1056,31 @@ static void tsc_cs_tick_stable(struct clocksource *cs)
 /*
  * .mask MUST be CLOCKSOURCE_MASK(64). See comment above read_tsc()
  */
+static struct clocksource clocksource_tsc_early = {
+	.name                   = "tsc-early",
+	.rating                 = 299,
+	.read                   = read_tsc,
+	.mask                   = CLOCKSOURCE_MASK(64),
+	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
+				  CLOCK_SOURCE_MUST_VERIFY,
+	.archdata               = { .vclock_mode = VCLOCK_TSC },
+	.resume			= tsc_resume,
+	.mark_unstable		= tsc_cs_mark_unstable,
+	.tick_stable		= tsc_cs_tick_stable,
+};
+
+/*
+ * Must mark VALID_FOR_HRES early such that when we unregister tsc_early
+ * this one will immediately take over. We will only register if TSC has
+ * been found good.
+ */
 static struct clocksource clocksource_tsc = {
 	.name                   = "tsc",
 	.rating                 = 300,
 	.read                   = read_tsc,
 	.mask                   = CLOCKSOURCE_MASK(64),
 	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
+				  CLOCK_SOURCE_VALID_FOR_HRES |
 				  CLOCK_SOURCE_MUST_VERIFY,
 	.archdata               = { .vclock_mode = VCLOCK_TSC },
 	.resume			= tsc_resume,
@@ -1187,8 +1204,8 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 	int cpu;
 
 	/* Don't bother refining TSC on unstable systems */
-	if (check_tsc_unstable())
-		goto out;
+	if (tsc_unstable)
+		return;
 
 	/*
 	 * Since the work is started early in boot, we may be
@@ -1240,9 +1257,13 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 		set_cyc2ns_scale(tsc_khz, cpu, tsc_stop);
 
 out:
+	if (tsc_unstable)
+		return;
+
 	if (boot_cpu_has(X86_FEATURE_ART))
 		art_related_clocksource = &clocksource_tsc;
 	clocksource_register_khz(&clocksource_tsc, tsc_khz);
+	clocksource_unregister(&clocksource_tsc_early);
 }
 
 
@@ -1251,13 +1272,11 @@ static int __init init_tsc_clocksource(void)
 	if (!boot_cpu_has(X86_FEATURE_TSC) || tsc_disabled > 0 || !tsc_khz)
 		return 0;
 
+	if (check_tsc_unstable())
+		return 0;
+
 	if (tsc_clocksource_reliable)
 		clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
-	/* lower the rating if we already know its unstable: */
-	if (check_tsc_unstable()) {
-		clocksource_tsc.rating = 0;
-		clocksource_tsc.flags &= ~CLOCK_SOURCE_IS_CONTINUOUS;
-	}
 
 	if (boot_cpu_has(X86_FEATURE_NONSTOP_TSC_S3))
 		clocksource_tsc.flags |= CLOCK_SOURCE_SUSPEND_NONSTOP;
@@ -1270,6 +1289,7 @@ static int __init init_tsc_clocksource(void)
 		if (boot_cpu_has(X86_FEATURE_ART))
 			art_related_clocksource = &clocksource_tsc;
 		clocksource_register_khz(&clocksource_tsc, tsc_khz);
+		clocksource_unregister(&clocksource_tsc_early);
 		return 0;
 	}
 
@@ -1374,9 +1394,12 @@ void __init tsc_init(void)
 
 	check_system_tsc_reliable();
 
-	if (unsynchronized_tsc())
+	if (unsynchronized_tsc()) {
 		mark_tsc_unstable("TSCs unsynchronized");
+		return;
+	}
 
+	clocksource_register_khz(&clocksource_tsc_early, tsc_khz);
 	detect_art();
 }
 

commit 30c7e5b123673d5e570e238dbada2fb68a87212c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Dec 22 10:20:11 2017 +0100

    x86/tsc: Allow TSC calibration without PIT
    
    Zhang Rui reported that a Surface Pro 4 will fail to boot with
    lapic=notscdeadline. Part of the problem is that that machine doesn't have
    a PIT.
    
    If, for some reason, the TSC init has to fall back to TSC calibration, it
    relies on the PIT to be present.
    
    Allow TSC calibration to reliably fall back to HPET.
    
    The below results in an accurate TSC measurement when forced on a IVB:
    
      tsc: Unable to calibrate against PIT
      tsc: No reference (HPET/PMTIMER) available
      tsc: Unable to calibrate against PIT
      tsc: using HPET reference calibration
      tsc: Detected 2792.451 MHz processor
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: len.brown@intel.com
    Cc: rui.zhang@intel.com
    Link: https://lkml.kernel.org/r/20171222092243.333145937@infradead.org

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index e169e85db434..a2c9dd8bfc6f 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -25,6 +25,7 @@
 #include <asm/geode.h>
 #include <asm/apic.h>
 #include <asm/intel-family.h>
+#include <asm/i8259.h>
 
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
@@ -363,6 +364,20 @@ static unsigned long pit_calibrate_tsc(u32 latch, unsigned long ms, int loopmin)
 	unsigned long tscmin, tscmax;
 	int pitcnt;
 
+	if (!has_legacy_pic()) {
+		/*
+		 * Relies on tsc_early_delay_calibrate() to have given us semi
+		 * usable udelay(), wait for the same 50ms we would have with
+		 * the PIT loop below.
+		 */
+		udelay(10 * USEC_PER_MSEC);
+		udelay(10 * USEC_PER_MSEC);
+		udelay(10 * USEC_PER_MSEC);
+		udelay(10 * USEC_PER_MSEC);
+		udelay(10 * USEC_PER_MSEC);
+		return ULONG_MAX;
+	}
+
 	/* Set the Gate high, disable speaker */
 	outb((inb(0x61) & ~0x02) | 0x01, 0x61);
 
@@ -487,6 +502,9 @@ static unsigned long quick_pit_calibrate(void)
 	u64 tsc, delta;
 	unsigned long d1, d2;
 
+	if (!has_legacy_pic())
+		return 0;
+
 	/* Set the Gate high, disable speaker */
 	outb((inb(0x61) & ~0x02) | 0x01, 0x61);
 

commit 4b5b2127238e689ee18aa6752959751dd61c4c73
Author: Len Brown <len.brown@intel.com>
Date:   Fri Dec 22 00:27:56 2017 -0500

    x86/tsc: Print tsc_khz, when it differs from cpu_khz
    
    If CPU and TSC frequency are the same the printout of the CPU frequency is
    valid for the TSC as well:
    
          tsc: Detected 2900.000 MHz processor
    
    If the TSC frequency is different there is no information in dmesg. Add a
    conditional printout:
    
      tsc: Detected 2904.000 MHz TSC
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: peterz@infradead.org
    Link: https://lkml.kernel.org/r/537b342debcd8e8aebc8d631015dcdf9f9ba8a26.1513920414.git.len.brown@intel.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3bf4df7f52d7..e169e85db434 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1316,6 +1316,12 @@ void __init tsc_init(void)
 		(unsigned long)cpu_khz / 1000,
 		(unsigned long)cpu_khz % 1000);
 
+	if (cpu_khz != tsc_khz) {
+		pr_info("Detected %lu.%03lu MHz TSC",
+			(unsigned long)tsc_khz / 1000,
+			(unsigned long)tsc_khz % 1000);
+	}
+
 	/* Sanitize TSC ADJUST before cyc2ns gets initialized */
 	tsc_store_and_check_tsc_adjust(true);
 

commit b511203093489eb1829cb4de86e8214752205ac6
Author: Len Brown <len.brown@intel.com>
Date:   Fri Dec 22 00:27:55 2017 -0500

    x86/tsc: Fix erroneous TSC rate on Skylake Xeon
    
    The INTEL_FAM6_SKYLAKE_X hardcoded crystal_khz value of 25MHZ is
    problematic:
    
     - SKX workstations (with same model # as server variants) use a 24 MHz
       crystal.  This results in a -4.0% time drift rate on SKX workstations.
    
     - SKX servers subject the crystal to an EMI reduction circuit that reduces its
       actual frequency by (approximately) -0.25%.  This results in -1 second per
       10 minute time drift as compared to network time.
    
    This issue can also trigger a timer and power problem, on configurations
    that use the LAPIC timer (versus the TSC deadline timer).  Clock ticks
    scheduled with the LAPIC timer arrive a few usec before the time they are
    expected (according to the slow TSC).  This causes Linux to poll-idle, when
    it should be in an idle power saving state.  The idle and clock code do not
    graciously recover from this error, sometimes resulting in significant
    polling and measurable power impact.
    
    Stop using native_calibrate_tsc() for INTEL_FAM6_SKYLAKE_X.
    native_calibrate_tsc() will return 0, boot will run with tsc_khz = cpu_khz,
    and the TSC refined calibration will update tsc_khz to correct for the
    difference.
    
    [ tglx: Sanitized change log ]
    
    Fixes: 6baf3d61821f ("x86/tsc: Add additional Intel CPU models to the crystal quirk list")
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: peterz@infradead.org
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/ff6dcea166e8ff8f2f6a03c17beab2cb436aa779.1513920414.git.len.brown@intel.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index ce4b71119c36..3bf4df7f52d7 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -602,7 +602,6 @@ unsigned long native_calibrate_tsc(void)
 		case INTEL_FAM6_KABYLAKE_DESKTOP:
 			crystal_khz = 24000;	/* 24.0 MHz */
 			break;
-		case INTEL_FAM6_SKYLAKE_X:
 		case INTEL_FAM6_ATOM_DENVERTON:
 			crystal_khz = 25000;	/* 25.0 MHz */
 			break;

commit da4ae6c4a0b8dee5a5377a385545d2250fa8cddb
Author: Len Brown <len.brown@intel.com>
Date:   Fri Dec 22 00:27:54 2017 -0500

    x86/tsc: Future-proof native_calibrate_tsc()
    
    If the crystal frequency cannot be determined via CPUID(15).crystal_khz or
    the built-in table then native_calibrate_tsc() will still set the
    X86_FEATURE_TSC_KNOWN_FREQ flag which prevents the refined TSC calibration.
    
    As a consequence such systems use cpu_khz for the TSC frequency which is
    incorrect when cpu_khz != tsc_khz resulting in time drift.
    
    Return early when the crystal frequency cannot be retrieved without setting
    the X86_FEATURE_TSC_KNOWN_FREQ flag. This ensures that the refined TSC
    calibration is invoked.
    
    [ tglx: Steam-blastered changelog. Sigh ]
    
    Fixes: 4ca4df0b7eb0 ("x86/tsc: Mark TSC frequency determined by CPUID as known")
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: peterz@infradead.org
    Cc: Bin Gao <bin.gao@intel.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/0fe2503aa7d7fc69137141fc705541a78101d2b9.1513920414.git.len.brown@intel.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 8ea117f8142e..ce4b71119c36 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -612,6 +612,8 @@ unsigned long native_calibrate_tsc(void)
 		}
 	}
 
+	if (crystal_khz == 0)
+		return 0;
 	/*
 	 * TSC frequency determined by CPUID is a "hardware reported"
 	 * frequency and is the most accurate one so far we have. This

commit 99306dfc067e6098365d395168b6fd5db3095292
Merge: 3643b7e05b16 120fc3fbb778
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 19:07:38 2017 -0800

    Merge branch 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 timer updates from Thomas Gleixner:
     "These updates are related to TSC handling:
    
       - Support platforms which have synchronized TSCs but the boot CPU has
         a non zero TSC_ADJUST value, which is considered a firmware bug on
         normal systems.
    
         This applies to HPE/SGI UV platforms where the platform firmware
         uses TSC_ADJUST to ensure TSC synchronization across a huge number
         of sockets, but due to power on timings the boot CPU cannot be
         guaranteed to have a zero TSC_ADJUST register value.
    
       - Fix the ordering of udelay calibration and kvmclock_init()
    
       - Cleanup the udelay and calibration code"
    
    * 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/tsc: Mark cyc2ns_init() and detect_art() __init
      x86/platform/UV: Mark tsc_check_sync as an init function
      x86/tsc: Make CONFIG_X86_TSC=n build work again
      x86/platform/UV: Add check of TSC state set by UV BIOS
      x86/tsc: Provide a means to disable TSC ART
      x86/tsc: Drastically reduce the number of firmware bug warnings
      x86/tsc: Skip TSC test and error messages if already unstable
      x86/tsc: Add option that TSC on Socket 0 being non-zero is valid
      x86/timers: Move simple_udelay_calibration() past kvmclock_init()
      x86/timers: Make recalibrate_cpu_khz() void
      x86/timers: Move the simple udelay calibration to tsc.h

commit 120fc3fbb7787fb70240190cc9c113d1f6523c42
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Wed Nov 8 18:09:52 2017 +0800

    x86/tsc: Mark cyc2ns_init() and detect_art() __init
    
    These two functions are only called by tsc_init(), which is an __init
    function during boot time, so mark them __init as well.
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1510135792-17429-1-git-send-email-douly.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index f1326c0422c1..416de29fe862 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -112,7 +112,7 @@ static void cyc2ns_data_init(struct cyc2ns_data *data)
 	data->cyc2ns_offset = 0;
 }
 
-static void cyc2ns_init(int cpu)
+static void __init cyc2ns_init(int cpu)
 {
 	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
 
@@ -955,7 +955,7 @@ core_initcall(cpufreq_register_tsc_scaling);
 /*
  * If ART is present detect the numerator:denominator to convert to TSC
  */
-static void detect_art(void)
+static void __init detect_art(void)
 {
 	unsigned int unused[2];
 

commit 76ce7cfe35ef58f34e6ba85327afb5fbf6c3ff9b
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Fri Oct 27 20:11:00 2017 -0400

    x86/smpboot: Make optimization of delay calibration work correctly
    
    If the TSC has constant frequency then the delay calibration can be skipped
    when it has been calibrated for a package already. This is checked in
    calibrate_delay_is_known(), but that function is buggy in two aspects:
    
    It returns 'false' if
    
      (!tsc_disabled && !cpu_has(&cpu_data(cpu), X86_FEATURE_CONSTANT_TSC)
    
    which is obviously the reverse of the intended check and the check for the
    sibling mask cannot work either because the topology links have not been
    set up yet.
    
    Correct the condition and move the call to set_cpu_sibling_map() before
    invoking calibrate_delay() so the sibling check works correctly.
    
    [ tglx: Rewrote changelong ]
    
    Fixes: c25323c07345 ("x86/tsc: Use topology functions")
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: peterz@infradead.org
    Cc: bob.picco@oracle.com
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20171028001100.26603-1-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 796d96bb0821..ad2b925a808e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1346,12 +1346,10 @@ void __init tsc_init(void)
 unsigned long calibrate_delay_is_known(void)
 {
 	int sibling, cpu = smp_processor_id();
-	struct cpumask *mask = topology_core_cpumask(cpu);
+	int constant_tsc = cpu_has(&cpu_data(cpu), X86_FEATURE_CONSTANT_TSC);
+	const struct cpumask *mask = topology_core_cpumask(cpu);
 
-	if (!tsc_disabled && !cpu_has(&cpu_data(cpu), X86_FEATURE_CONSTANT_TSC))
-		return 0;
-
-	if (!mask)
+	if (tsc_disabled || !constant_tsc || !mask)
 		return 0;
 
 	sibling = cpumask_any_but(mask, cpu);

commit 6c66350d0a482892793b888b07c1177fc6d4b344
Author: mike.travis@hpe.com <mike.travis@hpe.com>
Date:   Thu Oct 12 11:32:05 2017 -0500

    x86/tsc: Provide a means to disable TSC ART
    
    On systems where multiple chassis are reset asynchronously, and thus
    the TSC counters are started asynchronously, the offset needed to
    convert to TSC to ART would be different.  Disable ART in that case
    and rely on the TSC counters to supply the accurate time.
    
    Signed-off-by: Mike Travis <mike.travis@hpe.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Dimitri Sivanich <dimitri.sivanich@hpe.com>
    Cc: Russ Anderson <russ.anderson@hpe.com>
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Bin Gao <bin.gao@linux.intel.com>
    Link: https://lkml.kernel.org/r/20171012163202.289397994@stormcage.americas.sgi.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 896dbe31b407..f1326c0422c1 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -962,10 +962,14 @@ static void detect_art(void)
 	if (boot_cpu_data.cpuid_level < ART_CPUID_LEAF)
 		return;
 
-	/* Don't enable ART in a VM, non-stop TSC and TSC_ADJUST required */
+	/*
+	 * Don't enable ART in a VM, non-stop TSC and TSC_ADJUST required,
+	 * and the TSC counter resets must not occur asynchronously.
+	 */
 	if (boot_cpu_has(X86_FEATURE_HYPERVISOR) ||
 	    !boot_cpu_has(X86_FEATURE_NONSTOP_TSC) ||
-	    !boot_cpu_has(X86_FEATURE_TSC_ADJUST))
+	    !boot_cpu_has(X86_FEATURE_TSC_ADJUST) ||
+	    tsc_async_resets)
 		return;
 
 	cpuid(ART_CPUID_LEAF, &art_to_tsc_denominator,

commit af5768507c051ceb9fe12bee59202bd83115c073
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Fri Jul 14 11:34:07 2017 +0800

    x86/timers: Make recalibrate_cpu_khz() void
    
    recalibrate_cpu_khz() is called from powernow K7 and Pentium 4/Xeon
    CPU freq driver. It recalibrates cpu frequency in case of SMP = n
    and doesn't need to return anything.
    
    Mark it void, also remove the #else branch.
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/1500003247-17368-2-git-send-email-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index c173dcef1f46..896dbe31b407 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -812,13 +812,13 @@ unsigned long native_calibrate_cpu(void)
 	return tsc_pit_min;
 }
 
-int recalibrate_cpu_khz(void)
+void recalibrate_cpu_khz(void)
 {
 #ifndef CONFIG_SMP
 	unsigned long cpu_khz_old = cpu_khz;
 
 	if (!boot_cpu_has(X86_FEATURE_TSC))
-		return -ENODEV;
+		return;
 
 	cpu_khz = x86_platform.calibrate_cpu();
 	tsc_khz = x86_platform.calibrate_tsc();
@@ -828,10 +828,6 @@ int recalibrate_cpu_khz(void)
 		cpu_khz = tsc_khz;
 	cpu_data(0).loops_per_jiffy = cpufreq_scale(cpu_data(0).loops_per_jiffy,
 						    cpu_khz_old, cpu_khz);
-
-	return 0;
-#else
-	return -ENODEV;
 #endif
 }
 

commit eb496063c9904ce682253ee445b9acb9b6257581
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Fri Jul 14 11:34:06 2017 +0800

    x86/timers: Move the simple udelay calibration to tsc.h
    
    Commit dd759d93f4dd ("x86/timers: Add simple udelay calibration") adds
    an static function in x86 boot-time initializations.
    
    But, this function is actually related to TSC, so it should be maintained
    in tsc.c, not in setup.c.
    
    Move simple_udelay_calibration() from setup.c to tsc.c and rename it to
    tsc_early_delay_calibrate for more readability.
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/1500003247-17368-1-git-send-email-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 796d96bb0821..c173dcef1f46 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1263,6 +1263,25 @@ static int __init init_tsc_clocksource(void)
  */
 device_initcall(init_tsc_clocksource);
 
+void __init tsc_early_delay_calibrate(void)
+{
+	unsigned long lpj;
+
+	if (!boot_cpu_has(X86_FEATURE_TSC))
+		return;
+
+	cpu_khz = x86_platform.calibrate_cpu();
+	tsc_khz = x86_platform.calibrate_tsc();
+
+	tsc_khz = tsc_khz ? : cpu_khz;
+	if (!tsc_khz)
+		return;
+
+	lpj = tsc_khz * 1000;
+	do_div(lpj, HZ);
+	loops_per_jiffy = lpj;
+}
+
 void __init tsc_init(void)
 {
 	u64 lpj, cyc;

commit 3ad918e65d6926490c8f18a157cea25bf29ecd3a
Merge: 8c073517a992 a1272dd5531b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 3 18:01:50 2017 -0700

    Merge branch 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 timers updates from Thomas Gleixner:
     "This update contains:
    
       - The solution for the TSC deadline timer borkage, which is caused by
         a hardware problem in the TSC_ADJUST/TSC_DEADLINE_TIMER logic.
    
         The problem is documented now and fixed with a microcode update, so
         we can remove the workaround and just check for the microcode version.
    
         If the microcode is not up to date, then the TSC deadline timer is
         disabled. If the borkage is fixed by the proper microcode version,
         then the deadline timer can be used. In both cases the restrictions
         to the range of the TSC_ADJUST value, which were added as
         workarounds, are removed.
    
      - A few simple fixes and updates to the timer related x86 code"
    
    * 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/tsc: Call check_system_tsc_reliable() before unsynchronized_tsc()
      x86/hpet: Do not use smp_processor_id() in preemptible code
      x86/time: Make setup_default_timer_irq() static
      x86/tsc: Remove the TSC_ADJUST clamp
      x86/apic: Add TSC_DEADLINE quirk due to errata
      x86/apic: Change the lapic name in deadline mode

commit a1272dd5531b259bf7313ac7597a67362698038c
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Wed Jun 21 01:23:37 2017 -0700

    x86/tsc: Call check_system_tsc_reliable() before unsynchronized_tsc()
    
    tsc_clocksource_reliable is initialized in check_system_tsc_reliable(), but
    it is checked in unsynchronized_tsc() which is called before the
    initialization.
    
    In practice that's not an issue because systems which mark the TSC
    reliable have X86_FEATURE_CONSTANT_TSC set as well, which is evaluated
    in unsynchronized_tsc() before tsc_clocksource_reliable.
    
    Reorder the calls so initialization happens before usage.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/b1532ef7-cd9f-45f7-9f49-48dd2a5c2495@default

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 714dfba6a1e7..a316bdd42a37 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1412,11 +1412,11 @@ void __init tsc_init(void)
 
 	use_tsc_delay();
 
+	check_system_tsc_reliable();
+
 	if (unsynchronized_tsc())
 		mark_tsc_unstable("TSCs unsynchronized");
 
-	check_system_tsc_reliable();
-
 	detect_art();
 }
 

commit 5c3c2ea6887176c5ae812c9f0350ff65b10e9485
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed May 17 22:39:24 2017 +0200

    x86/tsc: Fold set_cyc2ns_scale() into caller
    
    The newly introduced wrapper function only has one caller,
    and this one is conditional, causing a harmless warning when
    CONFIG_CPU_FREQ is disabled:
    
      arch/x86/kernel/tsc.c:189:13: error: 'set_cyc2ns_scale' defined but not used [-Werror=unused-function]
    
    My first idea was to move the wrapper inside of that #ifdef,
    but on second thought it seemed nicer to remove it completely
    again and rename __set_cyc2ns_scale back to set_cyc2ns_scale,
    but leaving the extra argument.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 615cd03373a0 ("x86/tsc: Fix sched_clock() sync")
    Link: http://lkml.kernel.org/r/20170517203949.2052220-1-arnd@arndb.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a3b544264360..5270fc0c2df6 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -137,7 +137,7 @@ static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 	return ns;
 }
 
-static void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)
+static void set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)
 {
 	unsigned long long ns_now;
 	struct cyc2ns_data data;
@@ -186,11 +186,6 @@ static void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long ts
 	local_irq_restore(flags);
 }
 
-static void set_cyc2ns_scale(unsigned long khz, int cpu)
-{
-	__set_cyc2ns_scale(khz, cpu, rdtsc());
-}
-
 /*
  * Scheduler clock - returns current time in nanosec units.
  */
@@ -892,7 +887,6 @@ void tsc_restore_sched_clock_state(void)
 }
 
 #ifdef CONFIG_CPU_FREQ
-
 /* Frequency scaling support. Adjust the TSC based timer when the cpu frequency
  * changes.
  *
@@ -933,7 +927,7 @@ static int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
 		if (!(freq->flags & CPUFREQ_CONST_LOOPS))
 			mark_tsc_unstable("cpufreq changes");
 
-		set_cyc2ns_scale(tsc_khz, freq->cpu);
+		set_cyc2ns_scale(tsc_khz, freq->cpu, rdtsc());
 	}
 
 	return 0;
@@ -1224,7 +1218,7 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 
 	/* Update the sched_clock() rate to match the clocksource one */
 	for_each_possible_cpu(cpu)
-		__set_cyc2ns_scale(tsc_khz, cpu, tsc_stop);
+		set_cyc2ns_scale(tsc_khz, cpu, tsc_stop);
 
 out:
 	if (boot_cpu_has(X86_FEATURE_ART))
@@ -1314,7 +1308,7 @@ void __init tsc_init(void)
 	cyc = rdtsc();
 	for_each_possible_cpu(cpu) {
 		cyc2ns_init(cpu);
-		__set_cyc2ns_scale(tsc_khz, cpu, cyc);
+		set_cyc2ns_scale(tsc_khz, cpu, cyc);
 	}
 
 	if (tsc_disabled > 0)

commit ac1e843f0900bea92fcb47f6205e1f9ffb0d469c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 21 12:26:23 2017 +0200

    sched/clock: Remove unused argument to sched_clock_idle_wakeup_event()
    
    The argument to sched_clock_idle_wakeup_event() has not been used in a
    long time. Remove it.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index c1b16b328abe..a3b544264360 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -182,7 +182,7 @@ static void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long ts
 	c2n->data[1] = data;
 
 done:
-	sched_clock_idle_wakeup_event(0);
+	sched_clock_idle_wakeup_event();
 	local_irq_restore(flags);
 }
 

commit b421b22b00b0011f6a2ce3561176c4e79e640c49
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 21 12:14:13 2017 +0200

    x86/tsc, sched/clock, clocksource: Use clocksource watchdog to provide stable sync points
    
    Currently we keep sched_clock_tick() active for stable TSC in order to
    keep the per-CPU state semi up-to-date. The (obvious) problem is that
    by the time we detect TSC is borked, our per-CPU state is also borked.
    
    So hook into the clocksource watchdog and call a method after we've
    found it to still be stable.
    
    There's the obvious race where the TSC goes wonky between finding it
    stable and us running the callback, but closing that is too much work
    and not really worth it, since we're already detecting TSC wobbles
    after the fact, so we cannot, per definition, fully avoid funny clock
    values.
    
    And since the watchdog runs less often than the tick, this is also an
    optimization.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 66015195bd18..c1b16b328abe 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1033,6 +1033,15 @@ static void tsc_cs_mark_unstable(struct clocksource *cs)
 	pr_info("Marking TSC unstable due to clocksource watchdog\n");
 }
 
+static void tsc_cs_tick_stable(struct clocksource *cs)
+{
+	if (tsc_unstable)
+		return;
+
+	if (using_native_sched_clock())
+		sched_clock_tick_stable();
+}
+
 /*
  * .mask MUST be CLOCKSOURCE_MASK(64). See comment above read_tsc()
  */
@@ -1046,6 +1055,7 @@ static struct clocksource clocksource_tsc = {
 	.archdata               = { .vclock_mode = VCLOCK_TSC },
 	.resume			= tsc_resume,
 	.mark_unstable		= tsc_cs_mark_unstable,
+	.tick_stable		= tsc_cs_tick_stable,
 };
 
 void mark_tsc_unstable(char *reason)

commit aa7b630ea023d2d70b34af18cea92f31429c1db0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 21 11:32:46 2017 +0200

    x86/tsc: Feed refined TSC calibration into sched_clock()
    
    For the (older) CPUs that still need the refined TSC calibration, also
    update the sched_clock() rate.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index dda964c6387d..66015195bd18 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1161,6 +1161,7 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 	static int hpet;
 	u64 tsc_stop, ref_stop, delta;
 	unsigned long freq;
+	int cpu;
 
 	/* Don't bother refining TSC on unstable systems */
 	if (check_tsc_unstable())
@@ -1211,6 +1212,10 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 	/* Inform the TSC deadline clockevent devices about the recalibration */
 	lapic_update_tsc_freq();
 
+	/* Update the sched_clock() rate to match the clocksource one */
+	for_each_possible_cpu(cpu)
+		__set_cyc2ns_scale(tsc_khz, cpu, tsc_stop);
+
 out:
 	if (boot_cpu_has(X86_FEATURE_ART))
 		art_related_clocksource = &clocksource_tsc;

commit 615cd03373a02a17b82b253ff5e02e3a83b224f8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 5 09:55:01 2017 +0200

    x86/tsc: Fix sched_clock() sync
    
    While looking through the code I noticed that we initialize the cyc2ns
    fields with a different cycle value for each CPU, resulting in a
    slightly different 0 point for each CPU.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 23ce8b9a5d1f..dda964c6387d 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -137,9 +137,9 @@ static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 	return ns;
 }
 
-static void set_cyc2ns_scale(unsigned long khz, int cpu)
+static void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)
 {
-	unsigned long long tsc_now, ns_now;
+	unsigned long long ns_now;
 	struct cyc2ns_data data;
 	struct cyc2ns *c2n;
 	unsigned long flags;
@@ -150,7 +150,6 @@ static void set_cyc2ns_scale(unsigned long khz, int cpu)
 	if (!khz)
 		goto done;
 
-	tsc_now = rdtsc();
 	ns_now = cycles_2_ns(tsc_now);
 
 	/*
@@ -186,6 +185,12 @@ static void set_cyc2ns_scale(unsigned long khz, int cpu)
 	sched_clock_idle_wakeup_event(0);
 	local_irq_restore(flags);
 }
+
+static void set_cyc2ns_scale(unsigned long khz, int cpu)
+{
+	__set_cyc2ns_scale(khz, cpu, rdtsc());
+}
+
 /*
  * Scheduler clock - returns current time in nanosec units.
  */
@@ -1251,7 +1256,7 @@ device_initcall(init_tsc_clocksource);
 
 void __init tsc_init(void)
 {
-	u64 lpj;
+	u64 lpj, cyc;
 	int cpu;
 
 	if (!boot_cpu_has(X86_FEATURE_TSC)) {
@@ -1291,9 +1296,10 @@ void __init tsc_init(void)
 	 * speed as the bootup CPU. (cpufreq notifiers will fix this
 	 * up if their speed diverges)
 	 */
+	cyc = rdtsc();
 	for_each_possible_cpu(cpu) {
 		cyc2ns_init(cpu);
-		set_cyc2ns_scale(tsc_khz, cpu);
+		__set_cyc2ns_scale(tsc_khz, cpu, cyc);
 	}
 
 	if (tsc_disabled > 0)

commit 59eaef78bfea88fcbbd7b9b48ccf513aae1522c3
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 2 13:22:07 2017 +0200

    x86/tsc: Remodel cyc2ns to use seqcount_latch()
    
    Replace the custom multi-value scheme with the more regular
    seqcount_latch() scheme. Along with scrapping a lot of lines, the latch
    scheme is better documented and used in more places.
    
    The immediate benefit however is not being limited on the update side.
    The current code has a limit where the writers block which is hit by
    future changes.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 8ab883a4293e..23ce8b9a5d1f 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -51,115 +51,34 @@ static u32 art_to_tsc_denominator;
 static u64 art_to_tsc_offset;
 struct clocksource *art_related_clocksource;
 
-/*
- * Use a ring-buffer like data structure, where a writer advances the head by
- * writing a new data entry and a reader advances the tail when it observes a
- * new entry.
- *
- * Writers are made to wait on readers until there's space to write a new
- * entry.
- *
- * This means that we can always use an {offset, mul} pair to compute a ns
- * value that is 'roughly' in the right direction, even if we're writing a new
- * {offset, mul} pair during the clock read.
- *
- * The down-side is that we can no longer guarantee strict monotonicity anymore
- * (assuming the TSC was that to begin with), because while we compute the
- * intersection point of the two clock slopes and make sure the time is
- * continuous at the point of switching; we can no longer guarantee a reader is
- * strictly before or after the switch point.
- *
- * It does mean a reader no longer needs to disable IRQs in order to avoid
- * CPU-Freq updates messing with his times, and similarly an NMI reader will
- * no longer run the risk of hitting half-written state.
- */
-
 struct cyc2ns {
-	struct cyc2ns_data data[2];	/*  0 + 2*24 = 48 */
-	struct cyc2ns_data *head;	/* 48 + 8    = 56 */
-	struct cyc2ns_data *tail;	/* 56 + 8    = 64 */
-}; /* exactly fits one cacheline */
+	struct cyc2ns_data data[2];	/*  0 + 2*16 = 32 */
+	seqcount_t	   seq;		/* 32 + 4    = 36 */
 
-static DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
-
-struct cyc2ns_data *cyc2ns_read_begin(void)
-{
-	struct cyc2ns_data *head;
+}; /* fits one cacheline */
 
-	preempt_disable();
-
-	head = this_cpu_read(cyc2ns.head);
-	/*
-	 * Ensure we observe the entry when we observe the pointer to it.
-	 * matches the wmb from cyc2ns_write_end().
-	 */
-	smp_read_barrier_depends();
-	head->__count++;
-	barrier();
-
-	return head;
-}
+static DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
 
-void cyc2ns_read_end(struct cyc2ns_data *head)
+void cyc2ns_read_begin(struct cyc2ns_data *data)
 {
-	barrier();
-	/*
-	 * If we're the outer most nested read; update the tail pointer
-	 * when we're done. This notifies possible pending writers
-	 * that we've observed the head pointer and that the other
-	 * entry is now free.
-	 */
-	if (!--head->__count) {
-		/*
-		 * x86-TSO does not reorder writes with older reads;
-		 * therefore once this write becomes visible to another
-		 * cpu, we must be finished reading the cyc2ns_data.
-		 *
-		 * matches with cyc2ns_write_begin().
-		 */
-		this_cpu_write(cyc2ns.tail, head);
-	}
-	preempt_enable();
-}
+	int seq, idx;
 
-/*
- * Begin writing a new @data entry for @cpu.
- *
- * Assumes some sort of write side lock; currently 'provided' by the assumption
- * that cpufreq will call its notifiers sequentially.
- */
-static struct cyc2ns_data *cyc2ns_write_begin(int cpu)
-{
-	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
-	struct cyc2ns_data *data = c2n->data;
-
-	if (data == c2n->head)
-		data++;
+	preempt_disable_notrace();
 
-	/* XXX send an IPI to @cpu in order to guarantee a read? */
+	do {
+		seq = this_cpu_read(cyc2ns.seq.sequence);
+		idx = seq & 1;
 
-	/*
-	 * When we observe the tail write from cyc2ns_read_end(),
-	 * the cpu must be done with that entry and its safe
-	 * to start writing to it.
-	 */
-	while (c2n->tail == data)
-		cpu_relax();
+		data->cyc2ns_offset = this_cpu_read(cyc2ns.data[idx].cyc2ns_offset);
+		data->cyc2ns_mul    = this_cpu_read(cyc2ns.data[idx].cyc2ns_mul);
+		data->cyc2ns_shift  = this_cpu_read(cyc2ns.data[idx].cyc2ns_shift);
 
-	return data;
+	} while (unlikely(seq != this_cpu_read(cyc2ns.seq.sequence)));
 }
 
-static void cyc2ns_write_end(int cpu, struct cyc2ns_data *data)
+void cyc2ns_read_end(void)
 {
-	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
-
-	/*
-	 * Ensure the @data writes are visible before we publish the
-	 * entry. Matches the data-depencency in cyc2ns_read_begin().
-	 */
-	smp_wmb();
-
-	ACCESS_ONCE(c2n->head) = data;
+	preempt_enable_notrace();
 }
 
 /*
@@ -191,7 +110,6 @@ static void cyc2ns_data_init(struct cyc2ns_data *data)
 	data->cyc2ns_mul = 0;
 	data->cyc2ns_shift = 0;
 	data->cyc2ns_offset = 0;
-	data->__count = 0;
 }
 
 static void cyc2ns_init(int cpu)
@@ -201,43 +119,20 @@ static void cyc2ns_init(int cpu)
 	cyc2ns_data_init(&c2n->data[0]);
 	cyc2ns_data_init(&c2n->data[1]);
 
-	c2n->head = c2n->data;
-	c2n->tail = c2n->data;
+	seqcount_init(&c2n->seq);
 }
 
 static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 {
-	struct cyc2ns_data *data, *tail;
+	struct cyc2ns_data data;
 	unsigned long long ns;
 
-	/*
-	 * See cyc2ns_read_*() for details; replicated in order to avoid
-	 * an extra few instructions that came with the abstraction.
-	 * Notable, it allows us to only do the __count and tail update
-	 * dance when its actually needed.
-	 */
-
-	preempt_disable_notrace();
-	data = this_cpu_read(cyc2ns.head);
-	tail = this_cpu_read(cyc2ns.tail);
-
-	if (likely(data == tail)) {
-		ns = data->cyc2ns_offset;
-		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, data->cyc2ns_shift);
-	} else {
-		data->__count++;
+	cyc2ns_read_begin(&data);
 
-		barrier();
+	ns = data.cyc2ns_offset;
+	ns += mul_u64_u32_shr(cyc, data.cyc2ns_mul, data.cyc2ns_shift);
 
-		ns = data->cyc2ns_offset;
-		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, data->cyc2ns_shift);
-
-		barrier();
-
-		if (!--data->__count)
-			this_cpu_write(cyc2ns.tail, data);
-	}
-	preempt_enable_notrace();
+	cyc2ns_read_end();
 
 	return ns;
 }
@@ -245,7 +140,8 @@ static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 static void set_cyc2ns_scale(unsigned long khz, int cpu)
 {
 	unsigned long long tsc_now, ns_now;
-	struct cyc2ns_data *data;
+	struct cyc2ns_data data;
+	struct cyc2ns *c2n;
 	unsigned long flags;
 
 	local_irq_save(flags);
@@ -254,8 +150,6 @@ static void set_cyc2ns_scale(unsigned long khz, int cpu)
 	if (!khz)
 		goto done;
 
-	data = cyc2ns_write_begin(cpu);
-
 	tsc_now = rdtsc();
 	ns_now = cycles_2_ns(tsc_now);
 
@@ -264,7 +158,7 @@ static void set_cyc2ns_scale(unsigned long khz, int cpu)
 	 * time function is continuous; see the comment near struct
 	 * cyc2ns_data.
 	 */
-	clocks_calc_mult_shift(&data->cyc2ns_mul, &data->cyc2ns_shift, khz,
+	clocks_calc_mult_shift(&data.cyc2ns_mul, &data.cyc2ns_shift, khz,
 			       NSEC_PER_MSEC, 0);
 
 	/*
@@ -273,15 +167,20 @@ static void set_cyc2ns_scale(unsigned long khz, int cpu)
 	 * conversion algorithm shifting a 32-bit value (now specifies a 64-bit
 	 * value) - refer perf_event_mmap_page documentation in perf_event.h.
 	 */
-	if (data->cyc2ns_shift == 32) {
-		data->cyc2ns_shift = 31;
-		data->cyc2ns_mul >>= 1;
+	if (data.cyc2ns_shift == 32) {
+		data.cyc2ns_shift = 31;
+		data.cyc2ns_mul >>= 1;
 	}
 
-	data->cyc2ns_offset = ns_now -
-		mul_u64_u32_shr(tsc_now, data->cyc2ns_mul, data->cyc2ns_shift);
+	data.cyc2ns_offset = ns_now -
+		mul_u64_u32_shr(tsc_now, data.cyc2ns_mul, data.cyc2ns_shift);
+
+	c2n = per_cpu_ptr(&cyc2ns, cpu);
 
-	cyc2ns_write_end(cpu, data);
+	raw_write_seqcount_latch(&c2n->seq);
+	c2n->data[0] = data;
+	raw_write_seqcount_latch(&c2n->seq);
+	c2n->data[1] = data;
 
 done:
 	sched_clock_idle_wakeup_event(0);

commit 8309f86cd41e8714526867177facf7a316d9be53
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Apr 13 14:56:44 2017 +0200

    x86/tsc: Provide 'tsc=unstable' boot parameter
    
    Since the clocksource watchdog will only detect broken TSC after the
    fact, all TSC based clocks will likely have observed non-continuous
    values before/when switching away from TSC.
    
    Therefore only thing to fully avoid random clock movement when your
    BIOS randomly mucks with TSC values from SMI handlers is reporting the
    TSC as unstable at boot.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 714dfba6a1e7..8ab883a4293e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -374,6 +374,8 @@ static int __init tsc_setup(char *str)
 		tsc_clocksource_reliable = 1;
 	if (!strncmp(str, "noirqtime", 9))
 		no_sched_irq_time = 1;
+	if (!strcmp(str, "unstable"))
+		mark_tsc_unstable("boot parameter");
 	return 1;
 }
 

commit 698eff6355f735d46d1b7113df8b422874cd7988
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Mar 17 12:48:18 2017 +0100

    sched/clock, x86/perf: Fix "perf test tsc"
    
    People reported that commit:
    
      5680d8094ffa ("sched/clock: Provide better clock continuity")
    
    broke "perf test tsc".
    
    That commit added another offset to the reported clock value; so
    take that into account when computing the provided offset values.
    
    Reported-by: Adrian Hunter <adrian.hunter@intel.com>
    Reported-by: Arnaldo Carvalho de Melo <acme@kernel.org>
    Tested-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 5680d8094ffa ("sched/clock: Provide better clock continuity")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index c73a7f9e881a..714dfba6a1e7 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -328,7 +328,7 @@ unsigned long long sched_clock(void)
 	return paravirt_sched_clock();
 }
 
-static inline bool using_native_sched_clock(void)
+bool using_native_sched_clock(void)
 {
 	return pv_time_ops.sched_clock == native_sched_clock;
 }
@@ -336,7 +336,7 @@ static inline bool using_native_sched_clock(void)
 unsigned long long
 sched_clock(void) __attribute__((alias("native_sched_clock")));
 
-static inline bool using_native_sched_clock(void) { return true; }
+bool using_native_sched_clock(void) { return true; }
 #endif
 
 int check_tsc_unstable(void)

commit 44fee88cea43d3c2cac962e0439cb10a3cabff6d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Mar 13 15:57:12 2017 +0100

    x86/tsc: Fix ART for TSC_KNOWN_FREQ
    
    Subhransu reported that convert_art_to_tsc() isn't working for him.
    
    The ART to TSC relation is only set up for systems which use the refined
    TSC calibration. Systems with known TSC frequency (available via CPUID 15)
    are not using the refined calibration and therefor the ART to TSC relation
    is never established.
    
    Add the setup to the known frequency init path which skips ART
    calibration. The init code needs to be duplicated as for systems which use
    refined calibration the ART setup must be delayed until calibration has
    been done.
    
    The problem has been there since the ART support was introdduced, but only
    detected now because Subhransu tested the first time on hardware which has
    TSC frequency enumerated via CPUID 15.
    
    Note for stable: The conditional has changed from TSC_RELIABLE to
                     TSC_KNOWN_FREQUENCY.
    
    [ tglx: Rewrote changelog and identified the proper 'Fixes' commit ]
    
    Fixes: f9677e0f8308 ("x86/tsc: Always Running Timer (ART) correlated clocksource")
    Reported-by: "Prusty, Subhransu S" <subhransu.s.prusty@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: stable@vger.kernel.org
    Cc: christopher.s.hall@intel.com
    Cc: kevin.b.stanton@intel.com
    Cc: john.stultz@linaro.org
    Cc: akataria@vmware.com
    Link: http://lkml.kernel.org/r/20170313145712.GI3312@twins.programming.kicks-ass.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 4f7a9833d8e5..c73a7f9e881a 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1333,6 +1333,8 @@ static int __init init_tsc_clocksource(void)
 	 * the refined calibration and directly register it as a clocksource.
 	 */
 	if (boot_cpu_has(X86_FEATURE_TSC_KNOWN_FREQ)) {
+		if (boot_cpu_has(X86_FEATURE_ART))
+			art_related_clocksource = &clocksource_tsc;
 		clocksource_register_khz(&clocksource_tsc, tsc_khz);
 		return 0;
 	}

commit 609b07b72d3caaa8eed3a238886467946b78fa5e
Merge: c3abcabe813b f94c8d116997
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 7 14:42:34 2017 -0800

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "A fix for KVM's scheduler clock which (erroneously) was always marked
      unstable, a fix for RT/DL load balancing, plus latency fixes"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/clock, x86/tsc: Rework the x86 'unstable' sched_clock() interface
      sched/core: Fix pick_next_task() for RT,DL
      sched/fair: Make select_idle_cpu() more aggressive

commit f94c8d116997597fc00f0812b0ab9256e7b0c58f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 1 15:53:38 2017 +0100

    sched/clock, x86/tsc: Rework the x86 'unstable' sched_clock() interface
    
    Wanpeng Li reported that since the following commit:
    
      acb04058de49 ("sched/clock: Fix hotplug crash")
    
    ... KVM always runs with unstable sched-clock even though KVM's
    kvm_clock _is_ stable.
    
    The problem is that we've tied clear_sched_clock_stable() to the TSC
    state, and overlooked that sched_clock() is a paravirt function.
    
    Solve this by doing two things:
    
     - tie the sched_clock() stable state more clearly to the TSC stable
       state for the normal (!paravirt) case.
    
     - only call clear_sched_clock_stable() when we mark TSC unstable
       when we use native_sched_clock().
    
    The first means we can actually run with stable sched_clock in more
    situations then before, which is good. And since commit:
    
      12907fbb1a69 ("sched/clock, clocksource: Add optional cs::mark_unstable() method")
    
    ... this should be reliable. Since any detection of TSC fail now results
    in marking the TSC unstable.
    
    Reported-by: Wanpeng Li <kernellwp@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Fixes: acb04058de49 ("sched/clock: Fix hotplug crash")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 2724dc82f992..911129fda2f9 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -326,9 +326,16 @@ unsigned long long sched_clock(void)
 {
 	return paravirt_sched_clock();
 }
+
+static inline bool using_native_sched_clock(void)
+{
+	return pv_time_ops.sched_clock == native_sched_clock;
+}
 #else
 unsigned long long
 sched_clock(void) __attribute__((alias("native_sched_clock")));
+
+static inline bool using_native_sched_clock(void) { return true; }
 #endif
 
 int check_tsc_unstable(void)
@@ -1111,8 +1118,10 @@ static void tsc_cs_mark_unstable(struct clocksource *cs)
 {
 	if (tsc_unstable)
 		return;
+
 	tsc_unstable = 1;
-	clear_sched_clock_stable();
+	if (using_native_sched_clock())
+		clear_sched_clock_stable();
 	disable_sched_clock_irqtime();
 	pr_info("Marking TSC unstable due to clocksource watchdog\n");
 }
@@ -1134,18 +1143,20 @@ static struct clocksource clocksource_tsc = {
 
 void mark_tsc_unstable(char *reason)
 {
-	if (!tsc_unstable) {
-		tsc_unstable = 1;
+	if (tsc_unstable)
+		return;
+
+	tsc_unstable = 1;
+	if (using_native_sched_clock())
 		clear_sched_clock_stable();
-		disable_sched_clock_irqtime();
-		pr_info("Marking TSC unstable due to %s\n", reason);
-		/* Change only the rating, when not registered */
-		if (clocksource_tsc.mult)
-			clocksource_mark_unstable(&clocksource_tsc);
-		else {
-			clocksource_tsc.flags |= CLOCK_SOURCE_UNSTABLE;
-			clocksource_tsc.rating = 0;
-		}
+	disable_sched_clock_irqtime();
+	pr_info("Marking TSC unstable due to %s\n", reason);
+	/* Change only the rating, when not registered */
+	if (clocksource_tsc.mult) {
+		clocksource_mark_unstable(&clocksource_tsc);
+	} else {
+		clocksource_tsc.flags |= CLOCK_SOURCE_UNSTABLE;
+		clocksource_tsc.rating = 0;
 	}
 }
 

commit e601757102cfd3eeae068f53b3bc1234f3a2b2e9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/clock.h>
    
    We are going to split <linux/sched/clock.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and .c files.
    
    Create a trivial placeholder <linux/sched/clock.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 2724dc82f992..46bcda4cb1c2 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -2,6 +2,7 @@
 
 #include <linux/kernel.h>
 #include <linux/sched.h>
+#include <linux/sched/clock.h>
 #include <linux/init.h>
 #include <linux/export.h>
 #include <linux/timer.h>

commit 828cad8ea05d194d8a9452e0793261c2024c23a2
Merge: 60c906bab124 bb3bac2ca9a3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 12:52:55 2017 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this (fairly busy) cycle were:
    
       - There was a class of scheduler bugs related to forgetting to update
         the rq-clock timestamp which can cause weird and hard to debug
         problems, so there's a new debug facility for this: which uncovered
         a whole lot of bugs which convinced us that we want to keep the
         debug facility.
    
         (Peter Zijlstra, Matt Fleming)
    
       - Various cputime related updates: eliminate cputime and use u64
         nanoseconds directly, simplify and improve the arch interfaces,
         implement delayed accounting more widely, etc. - (Frederic
         Weisbecker)
    
       - Move code around for better structure plus cleanups (Ingo Molnar)
    
       - Move IO schedule accounting deeper into the scheduler plus related
         changes to improve the situation (Tejun Heo)
    
       - ... plus a round of sched/rt and sched/deadline fixes, plus other
         fixes, updats and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (85 commits)
      sched/core: Remove unlikely() annotation from sched_move_task()
      sched/autogroup: Rename auto_group.[ch] to autogroup.[ch]
      sched/topology: Split out scheduler topology code from core.c into topology.c
      sched/core: Remove unnecessary #include headers
      sched/rq_clock: Consolidate the ordering of the rq_clock methods
      delayacct: Include <uapi/linux/taskstats.h>
      sched/core: Clean up comments
      sched/rt: Show the 'sched_rr_timeslice' SCHED_RR timeslice tuning knob in milliseconds
      sched/clock: Add dummy clear_sched_clock_stable() stub function
      sched/cputime: Remove generic asm headers
      sched/cputime: Remove unused nsec_to_cputime()
      s390, sched/cputime: Remove unused cputime definitions
      powerpc, sched/cputime: Remove unused cputime definitions
      s390, sched/cputime: Make arch_cpu_idle_time() to return nsecs
      ia64, sched/cputime: Remove unused cputime definitions
      ia64: Convert vtime to use nsec units directly
      ia64, sched/cputime: Move the nsecs based cputime headers to the last arch using it
      sched/cputime: Remove jiffies based cputime
      sched/cputime, vtime: Return nsecs instead of cputime_t to account
      sched/cputime: Complete nsec conversion of tick based accounting
      ...

commit f2e04214ef7f7e49d1e06109ad1b2718155dab25
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 9 16:08:41 2017 +0100

    x86/tsc: Avoid the large time jump when sanitizing TSC ADJUST
    
    Olof reported that on a machine which has a BIOS wreckaged TSC the
    timestamps in dmesg are making a large jump because the TSC value is
    jumping forward after resetting the TSC ADJUST register to a sane value.
    
    This can be avoided by calling the TSC ADJUST saniziting function before
    initializing the per cpu sched clock machinery. That takes the offset into
    account and avoid the time jump.
    
    What cannot be avoided is that the 'Firmware Bug' warnings on the secondary
    CPUs are printed with the large time offsets because it would be too much
    effort and ugly hackery to print those warnings into a buffer and emit them
    after the adjustemt on the starting CPUs. It's a firmware bug and should be
    fixed in firmware. The weird timestamps are collateral damage and just
    illustrate the sillyness of the BIOS folks:
    
    [    0.397445] smp: Bringing up secondary CPUs ...
    [    0.402100] x86: Booting SMP configuration:
    [    0.406343] .... node  #0, CPUs:      #1
    [1265776479.930667] [Firmware Bug]: TSC ADJUST differs: Reference CPU0: -2978888639075328 CPU1: -2978888639183101
    [1265776479.944664] TSC ADJUST synchronize: Reference CPU0: 0 CPU1: -2978888639183101
    [    0.508119]  #2
    [1265776480.032346] [Firmware Bug]: TSC ADJUST differs: Reference CPU0: -2978888639075328 CPU2: -2978888639183677
    [1265776480.044192] TSC ADJUST synchronize: Reference CPU0: 0 CPU2: -2978888639183677
    [    0.607643]  #3
    [1265776480.131874] [Firmware Bug]: TSC ADJUST differs: Reference CPU0: -2978888639075328 CPU3: -2978888639184530
    [1265776480.143720] TSC ADJUST synchronize: Reference CPU0: 0 CPU3: -2978888639184530
    [    0.707108] smp: Brought up 1 node, 4 CPUs
    [    0.711271] smpboot: Total of 4 processors activated (21698.88 BogoMIPS)
    
    Reported-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20170209151231.411460506@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index e41af597aed8..37e7cf544e51 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1356,6 +1356,9 @@ void __init tsc_init(void)
 		(unsigned long)cpu_khz / 1000,
 		(unsigned long)cpu_khz % 1000);
 
+	/* Sanitize TSC ADJUST before cyc2ns gets initialized */
+	tsc_store_and_check_tsc_adjust(true);
+
 	/*
 	 * Secondary CPUs do not run through tsc_init(), so set up
 	 * all the scale factors for all CPUs, assuming the same
@@ -1386,8 +1389,6 @@ void __init tsc_init(void)
 
 	if (unsynchronized_tsc())
 		mark_tsc_unstable("TSCs unsynchronized");
-	else
-		tsc_store_and_check_tsc_adjust(true);
 
 	check_system_tsc_reliable();
 

commit ed5c8c854f2b990dfa4d85c4995d115768a05d3c
Merge: 619bd4a71874 a2ca3d617944
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 09:12:25 2017 +0100

    Merge branch 'linus' into sched/core, to pick up fixes and refresh the branch
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 12907fbb1a691807bb0420a27126e15934cb7954
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 15 11:44:28 2016 +0100

    sched/clock, clocksource: Add optional cs::mark_unstable() method
    
    PeterZ reported that we'd fail to mark the TSC unstable when the
    clocksource watchdog finds it unsuitable.
    
    Allow a clocksource to run a custom action when its being marked
    unstable and hook up the TSC unstable code.
    
    Reported-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index be3a49ee0356..c8174c815d83 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1106,6 +1106,16 @@ static u64 read_tsc(struct clocksource *cs)
 	return (u64)rdtsc_ordered();
 }
 
+static void tsc_cs_mark_unstable(struct clocksource *cs)
+{
+	if (tsc_unstable)
+		return;
+	tsc_unstable = 1;
+	clear_sched_clock_stable();
+	disable_sched_clock_irqtime();
+	pr_info("Marking TSC unstable due to clocksource watchdog\n");
+}
+
 /*
  * .mask MUST be CLOCKSOURCE_MASK(64). See comment above read_tsc()
  */
@@ -1118,6 +1128,7 @@ static struct clocksource clocksource_tsc = {
 				  CLOCK_SOURCE_MUST_VERIFY,
 	.archdata               = { .vclock_mode = VCLOCK_TSC },
 	.resume			= tsc_resume,
+	.mark_unstable		= tsc_cs_mark_unstable,
 };
 
 void mark_tsc_unstable(char *reason)

commit 695085b4bc7603551db0b3da897b8bf9893ca218
Author: Len Brown <len.brown@intel.com>
Date:   Fri Jan 13 01:11:18 2017 -0500

    x86/tsc: Add the Intel Denverton Processor to native_calibrate_tsc()
    
    The Intel Denverton microserver uses a 25 MHz TSC crystal,
    so we can derive its exact [*] TSC frequency
    using CPUID and some arithmetic, eg.:
    
      TSC: 1800 MHz (25000000 Hz * 216 / 3 / 1000000)
    
    [*] 'exact' is only as good as the crystal, which should be +/- 20ppm
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/306899f94804aece6d8fa8b4223ede3b48dbb59c.1484287748.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index be3a49ee0356..e41af597aed8 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -694,6 +694,7 @@ unsigned long native_calibrate_tsc(void)
 			crystal_khz = 24000;	/* 24.0 MHz */
 			break;
 		case INTEL_FAM6_SKYLAKE_X:
+		case INTEL_FAM6_ATOM_DENVERTON:
 			crystal_khz = 25000;	/* 25.0 MHz */
 			break;
 		case INTEL_FAM6_ATOM_GOLDMONT:

commit a5a1d1c2914b5316924c7893eb683a5420ebd3be
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:32:01 2016 +0100

    clocksource: Use a plain u64 instead of cycle_t
    
    There is no point in having an extra type for extra confusion. u64 is
    unambiguous.
    
    Conversion was done with the following coccinelle script:
    
    @rem@
    @@
    -typedef u64 cycle_t;
    
    @fix@
    typedef cycle_t;
    @@
    -cycle_t
    +u64
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 0aed75a1e31b..be3a49ee0356 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1101,9 +1101,9 @@ static void tsc_resume(struct clocksource *cs)
  * checking the result of read_tsc() - cycle_last for being negative.
  * That works because CLOCKSOURCE_MASK(64) does not mask out any bit.
  */
-static cycle_t read_tsc(struct clocksource *cs)
+static u64 read_tsc(struct clocksource *cs)
 {
-	return (cycle_t)rdtsc_ordered();
+	return (u64)rdtsc_ordered();
 }
 
 /*
@@ -1192,7 +1192,7 @@ int unsynchronized_tsc(void)
 /*
  * Convert ART to TSC given numerator/denominator found in detect_art()
  */
-struct system_counterval_t convert_art_to_tsc(cycle_t art)
+struct system_counterval_t convert_art_to_tsc(u64 art)
 {
 	u64 tmp, res, rem;
 

commit 5bae156241e05d25171b18ee43e49f103c3f8097
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 13 13:14:17 2016 +0000

    x86/tsc: Force TSC_ADJUST register to value >= zero
    
    Roland reported that his DELL T5810 sports a value add BIOS which
    completely wreckages the TSC. The squirmware [(TM) Ingo Molnar] boots with
    random negative TSC_ADJUST values, different on all CPUs. That renders the
    TSC useless because the sycnchronization check fails.
    
    Roland tested the new TSC_ADJUST mechanism. While it manages to readjust
    the TSCs he needs to disable the TSC deadline timer, otherwise the machine
    just stops booting.
    
    Deeper investigation unearthed that the TSC deadline timer is sensitive to
    the TSC_ADJUST value. Writing TSC_ADJUST to a negative value results in an
    interrupt storm caused by the TSC deadline timer.
    
    This does not make any sense and it's hard to imagine what kind of hardware
    wreckage is behind that misfeature, but it's reliably reproducible on other
    systems which have TSC_ADJUST and TSC deadline timer.
    
    While it would be understandable that a big enough negative value which
    moves the resulting TSC readout into the negative space could have the
    described effect, this happens even with a adjust value of -1, which keeps
    the TSC readout definitely in the positive space. The compare register for
    the TSC deadline timer is set to a positive value larger than the TSC, but
    despite not having reached the deadline the interrupt is raised
    immediately. If this happens on the boot CPU, then the machine dies
    silently because this setup happens before the NMI watchdog is armed.
    
    Further experiments showed that any other adjustment of TSC_ADJUST works as
    expected as long as it stays in the positive range. The direction of the
    adjustment has no influence either. See the lkml link for further analysis.
    
    Yet another proof for the theory that timers are designed by janitors and
    the underlying (obviously undocumented) mechanisms which allow BIOSes to
    wreckage them are considered a feature. Well done Intel - NOT!
    
    To address this wreckage add the following sanity measures:
    
    - If the TSC_ADJUST value on the boot cpu is not 0, set it to 0
    
    - If the TSC_ADJUST value on any cpu is negative, set it to 0
    
    - Prevent the cross package synchronization mechanism from setting negative
      TSC_ADJUST values.
    
    Reported-and-tested-by: Roland Scheidegger <rscheidegger_lists@hispeed.ch>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bruce Schlobohm <bruce.schlobohm@intel.com>
    Cc: Kevin Stanton <kevin.b.stanton@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Allen Hung <allen_hung@dell.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161213131211.397588033@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index bfb541a5bb48..0aed75a1e31b 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1386,7 +1386,7 @@ void __init tsc_init(void)
 	if (unsynchronized_tsc())
 		mark_tsc_unstable("TSCs unsynchronized");
 	else
-		tsc_store_and_check_tsc_adjust();
+		tsc_store_and_check_tsc_adjust(true);
 
 	check_system_tsc_reliable();
 

commit 6a369583178d0b89c2c3919c4456ee22fee0f249
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 13 13:14:17 2016 +0000

    x86/tsc: Validate TSC_ADJUST after resume
    
    Some 'feature' BIOSes fiddle with the TSC_ADJUST register during
    suspend/resume which renders the TSC unusable.
    
    Add sanity checks into the resume path and restore the
    original value if it was adjusted.
    
    Reported-and-tested-by: Roland Scheidegger <rscheidegger_lists@hispeed.ch>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bruce Schlobohm <bruce.schlobohm@intel.com>
    Cc: Kevin Stanton <kevin.b.stanton@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Allen Hung <allen_hung@dell.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161213131211.317654500@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 2bb8de4f3b39..bfb541a5bb48 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1080,6 +1080,11 @@ static void detect_art(void)
 
 static struct clocksource clocksource_tsc;
 
+static void tsc_resume(struct clocksource *cs)
+{
+	tsc_verify_tsc_adjust(true);
+}
+
 /*
  * We used to compare the TSC to the cycle_last value in the clocksource
  * structure to avoid a nasty time-warp. This can be observed in a
@@ -1112,6 +1117,7 @@ static struct clocksource clocksource_tsc = {
 	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
 				  CLOCK_SOURCE_MUST_VERIFY,
 	.archdata               = { .vclock_mode = VCLOCK_TSC },
+	.resume			= tsc_resume,
 };
 
 void mark_tsc_unstable(char *reason)

commit 8b223bc7abe0e30e8d297a24ee6c6c07ef8d0bb9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Nov 19 13:47:36 2016 +0000

    x86/tsc: Store and check TSC ADJUST MSR
    
    The TSC_ADJUST MSR shows whether the TSC has been modified. This is helpful
    in a two aspects:
    
    1) It allows to detect BIOS wreckage, where SMM code tries to 'hide' the
       cycles spent by storing the TSC value at SMM entry and restoring it at
       SMM exit. On affected machines the TSCs run slowly out of sync up to the
       point where the clocksource watchdog (if available) detects it.
    
       The TSC_ADJUST MSR allows to detect the TSC modification before that and
       eventually restore it. This is also important for SoCs which have no
       watchdog clocksource and therefore TSC wreckage cannot be detected and
       acted upon.
    
    2) All threads in a package are required to have the same TSC_ADJUST
       value. Broken BIOSes break that and as a result the TSC synchronization
       check fails.
    
       The TSC_ADJUST MSR allows to detect the deviation when a CPU comes
       online. If detected set it to the value of an already online CPU in the
       same package. This also allows to reduce the number of sync tests
       because with that in place the test is only required for the first CPU
       in a package.
    
       In principle all CPUs in a system should have the same TSC_ADJUST value
       even across packages, but with physical CPU hotplug this assumption is
       not true because the TSC starts with power on, so physical hotplug has
       to do some trickery to bring the TSC into sync with already running
       packages, which requires to use an TSC_ADJUST value different from CPUs
       which got powered earlier.
    
       A final enhancement is the opportunity to compensate for unsynced TSCs
       accross nodes at boot time and make the TSC usable that way. It won't
       help for TSCs which run apart due to frequency skew between packages,
       but this gets detected by the clocksource watchdog later.
    
    The first step toward this is to store the TSC_ADJUST value of a starting
    CPU and compare it with the value of an already online CPU in the same
    package. If they differ, emit a warning and adjust it to the reference
    value. The !SMP version just stores the boot value for later verification.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161119134017.655323776@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 2b27c5ae9d1f..2bb8de4f3b39 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1379,6 +1379,8 @@ void __init tsc_init(void)
 
 	if (unsynchronized_tsc())
 		mark_tsc_unstable("TSCs unsynchronized");
+	else
+		tsc_store_and_check_tsc_adjust();
 
 	check_system_tsc_reliable();
 

commit 7b3d2f6e08ed5eb6bcf6912938f7a542405f8e8e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Nov 19 13:47:33 2016 +0000

    x86/tsc: Use X86_FEATURE_TSC_ADJUST in detect_art()
    
    The art detection uses rdmsrl_safe() to detect the availablity of the
    TSC_ADJUST MSR.
    
    That's pointless because we have a feature bit for this. Use it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161119134017.483561692@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 0ff1ec61d1e4..2b27c5ae9d1f 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1057,18 +1057,20 @@ static void detect_art(void)
 	if (boot_cpu_data.cpuid_level < ART_CPUID_LEAF)
 		return;
 
-	cpuid(ART_CPUID_LEAF, &art_to_tsc_denominator,
-	      &art_to_tsc_numerator, unused, unused+1);
-
-	/* Don't enable ART in a VM, non-stop TSC required */
+	/* Don't enable ART in a VM, non-stop TSC and TSC_ADJUST required */
 	if (boot_cpu_has(X86_FEATURE_HYPERVISOR) ||
 	    !boot_cpu_has(X86_FEATURE_NONSTOP_TSC) ||
-	    art_to_tsc_denominator < ART_MIN_DENOMINATOR)
+	    !boot_cpu_has(X86_FEATURE_TSC_ADJUST))
 		return;
 
-	if (rdmsrl_safe(MSR_IA32_TSC_ADJUST, &art_to_tsc_offset))
+	cpuid(ART_CPUID_LEAF, &art_to_tsc_denominator,
+	      &art_to_tsc_numerator, unused, unused+1);
+
+	if (art_to_tsc_denominator < ART_MIN_DENOMINATOR)
 		return;
 
+	rdmsrl(MSR_IA32_TSC_ADJUST, art_to_tsc_offset);
+
 	/* Make this sticky over multiple CPU init calls */
 	setup_force_cpu_cap(X86_FEATURE_ART);
 }

commit 984fecebda3b9c8e3d75f8492593da71c58972b3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Nov 18 10:38:09 2016 +0100

    x86/tsc: Finalize the split of the TSC_RELIABLE flag
    
    All places which used the TSC_RELIABLE to skip the delayed calibration
    have been converted to use the TSC_KNOWN_FREQ flag.
    
    Make the immeditate clocksource registration, which skips the long term
    calibration, solely depend on TSC_KNOWN_FREQ.
    
    The TSC_RELIABLE now merily removes the requirement for a watchdog
    clocksource.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bin Gao <bin.gao@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index f4dfdaa6633c..0ff1ec61d1e4 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1299,13 +1299,8 @@ static int __init init_tsc_clocksource(void)
 	/*
 	 * When TSC frequency is known (retrieved via MSR or CPUID), we skip
 	 * the refined calibration and directly register it as a clocksource.
-	 *
-	 * We still keep the TSC_RELIABLE flag here to avoid regressions -
-	 * it will be removed after all the conversion for other code paths
-	 * connected to this flag is done.
 	 */
-	if (boot_cpu_has(X86_FEATURE_TSC_RELIABLE) ||
-		boot_cpu_has(X86_FEATURE_TSC_KNOWN_FREQ)) {
+	if (boot_cpu_has(X86_FEATURE_TSC_KNOWN_FREQ)) {
 		clocksource_register_khz(&clocksource_tsc, tsc_khz);
 		return 0;
 	}

commit 4635fdc696a8e89eead3ea1712ae6ada38538d40
Author: Bin Gao <bin.gao@linux.intel.com>
Date:   Tue Nov 15 12:27:23 2016 -0800

    x86/tsc: Mark Intel ATOM_GOLDMONT TSC reliable
    
    On Intel GOLDMONT Atom SoC TSC is the only available clocksource, so there
    is no way to do software calibration or have a watchdog clocksource for it.
    Software calibration is already disabled via the TSC_KNOWN_FREQ flag, but
    the watchdog requirement still persists, so such systems cannot switch to
    high resolution/nohz mode.
    
    Mark it reliable, so it becomes usable. Hardware teams confirmed that this
    is safe on that SoC.
    
    Signed-off-by: Bin Gao <bin.gao@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1479241644-234277-4-git-send-email-bin.gao@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index e58c31959666..f4dfdaa6633c 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -709,6 +709,13 @@ unsigned long native_calibrate_tsc(void)
 	 */
 	setup_force_cpu_cap(X86_FEATURE_TSC_KNOWN_FREQ);
 
+	/*
+	 * For Atom SoCs TSC is the only reliable clocksource.
+	 * Mark TSC reliable so no watchdog on it.
+	 */
+	if (boot_cpu_data.x86_model == INTEL_FAM6_ATOM_GOLDMONT)
+		setup_force_cpu_cap(X86_FEATURE_TSC_RELIABLE);
+
 	return crystal_khz * ebx_numerator / eax_denominator;
 }
 

commit 4ca4df0b7eb06df264b2919759957f6d6ea1822e
Author: Bin Gao <bin.gao@linux.intel.com>
Date:   Tue Nov 15 12:27:22 2016 -0800

    x86/tsc: Mark TSC frequency determined by CPUID as known
    
    CPUs/SoCs with CPUID leaf 0x15 come with a known frequency and will report
    the frequency to software via CPUID instruction. This hardware provided
    frequency is the "real" frequency of TSC.
    
    Set the X86_FEATURE_TSC_KNOWN_FREQ flag for such systems to skip the
    software calibration process.
    
    A 24 hours test on one of the CPUID 0x15 capable platforms was
    conducted. PIT calibrated frequency resulted in more than 3 seconds drift
    whereas the CPUID determined frequency showed less than 0.5 second
    drift.
    
    Signed-off-by: Bin Gao <bin.gao@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1479241644-234277-3-git-send-email-bin.gao@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index d2c4ee4e4866..e58c31959666 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -702,6 +702,13 @@ unsigned long native_calibrate_tsc(void)
 		}
 	}
 
+	/*
+	 * TSC frequency determined by CPUID is a "hardware reported"
+	 * frequency and is the most accurate one so far we have. This
+	 * is considered a known frequency.
+	 */
+	setup_force_cpu_cap(X86_FEATURE_TSC_KNOWN_FREQ);
+
 	return crystal_khz * ebx_numerator / eax_denominator;
 }
 

commit 47c95a46d0fae07762f0a38aa3709ae63f307048
Author: Bin Gao <bin.gao@linux.intel.com>
Date:   Tue Nov 15 12:27:21 2016 -0800

    x86/tsc: Add X86_FEATURE_TSC_KNOWN_FREQ flag
    
    The X86_FEATURE_TSC_RELIABLE flag in Linux kernel implies both reliable
    (at runtime) and trustable (at calibration). But reliable running and
    trustable calibration independent of each other.
    
    Add a new flag X86_FEATURE_TSC_KNOWN_FREQ, which denotes that the frequency
    is known (via MSR/CPUID). This flag is only meant to skip the long term
    calibration on systems which have a known frequency.
    
    Add X86_FEATURE_TSC_KNOWN_FREQ to the skip the delayed calibration and
    leave X86_FEATURE_TSC_RELIABLE in place.
    
    After converting the existing users of X86_FEATURE_TSC_RELIABLE to use
    either both flags or just X86_FEATURE_TSC_KNOWN_FREQ we can seperate the
    functionality.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Bin Gao <bin.gao@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1479241644-234277-2-git-send-email-bin.gao@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 46b2f41f8b05..d2c4ee4e4866 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1283,10 +1283,15 @@ static int __init init_tsc_clocksource(void)
 		clocksource_tsc.flags |= CLOCK_SOURCE_SUSPEND_NONSTOP;
 
 	/*
-	 * Trust the results of the earlier calibration on systems
-	 * exporting a reliable TSC.
+	 * When TSC frequency is known (retrieved via MSR or CPUID), we skip
+	 * the refined calibration and directly register it as a clocksource.
+	 *
+	 * We still keep the TSC_RELIABLE flag here to avoid regressions -
+	 * it will be removed after all the conversion for other code paths
+	 * connected to this flag is done.
 	 */
-	if (boot_cpu_has(X86_FEATURE_TSC_RELIABLE)) {
+	if (boot_cpu_has(X86_FEATURE_TSC_RELIABLE) ||
+		boot_cpu_has(X86_FEATURE_TSC_KNOWN_FREQ)) {
 		clocksource_register_khz(&clocksource_tsc, tsc_khz);
 		return 0;
 	}

commit 6baf3d61821f5b38f27b4e9f044ad4d1e8f3d14f
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Mon Sep 19 08:51:41 2016 -0400

    x86/tsc: Add additional Intel CPU models to the crystal quirk list
    
    commit aa297292d708 ("x86/tsc: Enumerate SKL cpu_khz and tsc_khz via
    CPUID") added code to retrieve the crystal and TSC frequency from CPUID
    leaves. If the crystal freqency is enumerated as 0,the resulting TSC
    frequency is 0 as well. For CPUs with a known fixed crystal frequency a
    quirk list is available to set the frequency,
    
    Kabylake and SkylakeX CPUs are missing in the list of CPUs which need this
    quirk. Add them so the TSC frequency can be calculated correctly.
    
    [ tglx: Removed the silly default case as the switch() is only invoked when
            cpu_khz is 0. Massaged changelog. ]
    
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Cc: "Peter Zijlstra (Intel)" <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Link: http://lkml.kernel.org/r/1474289501-31717-3-git-send-email-prarit@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 2344758ba8a3..46b2f41f8b05 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -689,8 +689,13 @@ unsigned long native_calibrate_tsc(void)
 		switch (boot_cpu_data.x86_model) {
 		case INTEL_FAM6_SKYLAKE_MOBILE:
 		case INTEL_FAM6_SKYLAKE_DESKTOP:
+		case INTEL_FAM6_KABYLAKE_MOBILE:
+		case INTEL_FAM6_KABYLAKE_DESKTOP:
 			crystal_khz = 24000;	/* 24.0 MHz */
 			break;
+		case INTEL_FAM6_SKYLAKE_X:
+			crystal_khz = 25000;	/* 25.0 MHz */
+			break;
 		case INTEL_FAM6_ATOM_GOLDMONT:
 			crystal_khz = 19200;	/* 19.2 MHz */
 			break;

commit 655e52d2b62458032fc67ff7daaa664af6f36fb5
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Mon Sep 19 08:51:40 2016 -0400

    x86/tsc: Use cpu id defines instead of hex constants
    
    asm/intel-family.h contains defines for cpu ids which should be used
    instead of hex constants. Convert the switch case in native_calibrate_tsc()
    to use the defines before adding more cpu models.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Cc: "Peter Zijlstra (Intel)" <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Link: http://lkml.kernel.org/r/1474289501-31717-2-git-send-email-prarit@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 78b9cb5a26af..2344758ba8a3 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -23,6 +23,7 @@
 #include <asm/x86_init.h>
 #include <asm/geode.h>
 #include <asm/apic.h>
+#include <asm/intel-family.h>
 
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
@@ -686,11 +687,11 @@ unsigned long native_calibrate_tsc(void)
 
 	if (crystal_khz == 0) {
 		switch (boot_cpu_data.x86_model) {
-		case 0x4E:	/* SKL */
-		case 0x5E:	/* SKL */
+		case INTEL_FAM6_SKYLAKE_MOBILE:
+		case INTEL_FAM6_SKYLAKE_DESKTOP:
 			crystal_khz = 24000;	/* 24.0 MHz */
 			break;
-		case 0x5C:	/* BXT */
+		case INTEL_FAM6_ATOM_GOLDMONT:
 			crystal_khz = 19200;	/* 19.2 MHz */
 			break;
 		}

commit fdbdfefbabefcdf3f57560163b43fdc4cf95eb2f
Merge: 6731b0d611a1 a0cba2179ea4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Aug 10 14:36:23 2016 +0200

    Merge branch 'linus' into timers/urgent, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6731b0d611a1274f9e785fa0189ac2aeeabd0591
Author: Nicolai Stange <nicstange@gmail.com>
Date:   Thu Jul 14 17:22:55 2016 +0200

    x86/timers/apic: Inform TSC deadline clockevent device about recalibration
    
    This patch eliminates a source of imprecise APIC timer interrupts,
    which imprecision may result in double interrupts or even late
    interrupts.
    
    The TSC deadline clockevent devices' configuration and registration
    happens before the TSC frequency calibration is refined in
    tsc_refine_calibration_work().
    
    This results in the TSC clocksource and the TSC deadline clockevent
    devices being configured with slightly different frequencies: the former
    gets the refined one and the latter are configured with the inaccurate
    frequency detected earlier by means of the "Fast TSC calibration using PIT".
    
    Within the APIC code, introduce the notifier function
    lapic_update_tsc_freq() which reconfigures all per-CPU TSC deadline
    clockevent devices with the current tsc_khz.
    
    Call it from the TSC code after TSC calibration refinement has happened.
    
    Signed-off-by: Nicolai Stange <nicstange@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Christopher S. Hall <christopher.s.hall@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: http://lkml.kernel.org/r/20160714152255.18295-3-nicstange@gmail.com
    [ Pushed #ifdef CONFIG_X86_LOCAL_APIC into header, improved changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a804b5ab32d0..8fb4b6abac0e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -22,6 +22,7 @@
 #include <asm/nmi.h>
 #include <asm/x86_init.h>
 #include <asm/geode.h>
+#include <asm/apic.h>
 
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
@@ -1249,6 +1250,9 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 		(unsigned long)tsc_khz / 1000,
 		(unsigned long)tsc_khz % 1000);
 
+	/* Inform the TSC deadline clockevent devices about the recalibration */
+	lapic_update_tsc_freq();
+
 out:
 	if (boot_cpu_has(X86_FEATURE_ART))
 		art_related_clocksource = &clocksource_tsc;

commit aeb35d6b74174ed08daab84e232b456bbd89d1d9
Merge: 7a66ecfd319a a47177d360a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 1 14:23:42 2016 -0400

    Merge branch 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 header cleanups from Ingo Molnar:
     "This tree is a cleanup of the x86 tree reducing spurious uses of
      module.h - which should improve build performance a bit"
    
    * 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, crypto: Restore MODULE_LICENSE() to glue_helper.c so it loads
      x86/apic: Remove duplicated include from probe_64.c
      x86/ce4100: Remove duplicated include from ce4100.c
      x86/headers: Include spinlock_types.h in x8664_ksyms_64.c for missing spinlock_t
      x86/platform: Delete extraneous MODULE_* tags fromm ts5500
      x86: Audit and remove any remaining unnecessary uses of module.h
      x86/kvm: Audit and remove any unnecessary uses of module.h
      x86/xen: Audit and remove any unnecessary uses of module.h
      x86/platform: Audit and remove any unnecessary uses of module.h
      x86/lib: Audit and remove any unnecessary uses of module.h
      x86/kernel: Audit and remove any unnecessary uses of module.h
      x86/mm: Audit and remove any unnecessary uses of module.h
      x86: Don't use module.h just for AUTHOR / LICENSE tags

commit c48ec42d6eae08f55685ab660f0743ed33b9f22a
Author: Wei Jiangang <weijg.fnst@cn.fujitsu.com>
Date:   Fri Jul 15 16:12:10 2016 +0800

    x86/tsc: Remove the unused check_tsc_disabled()
    
    check_tsc_disabled() was introduced by commit:
    
      c73deb6aecda ("perf/x86: Add ability to calculate TSC from perf sample timestamps")
    
    The only caller was arch_perf_update_userpage(), which had been refactored
    by commit:
    
      d8b11a0cbd1c ("perf/x86: Clean up cap_user_time* setting")
    
    ... so no need keep and export it any more.
    
    Signed-off-by: Wei Jiangang <weijg.fnst@cn.fujitsu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: a.p.zijlstra@chello.nl
    Cc: adrian.hunter@intel.com
    Cc: bp@suse.de
    Link: http://lkml.kernel.org/r/1468570330-25810-1-git-send-email-weijg.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 2a952fcb1516..a804b5ab32d0 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -335,12 +335,6 @@ int check_tsc_unstable(void)
 }
 EXPORT_SYMBOL_GPL(check_tsc_unstable);
 
-int check_tsc_disabled(void)
-{
-	return tsc_disabled;
-}
-EXPORT_SYMBOL_GPL(check_tsc_disabled);
-
 #ifdef CONFIG_X86_TSC
 int __init notsc_setup(char *str)
 {

commit 186f43608a5c827f8284fe4559225b4dccaa49ef
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:56 2016 -0400

    x86/kernel: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed some implicit header usage that was fixed up accordingly.
    
    Note that some bool/obj-y instances remain since module.h is
    the header for some exception table entry stuff, and for things
    like __init_or_module (code that is tossed when MODULES=n).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-4-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 38ba6de56ede..443568761066 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -3,7 +3,7 @@
 #include <linux/kernel.h>
 #include <linux/sched.h>
 #include <linux/init.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/timer.h>
 #include <linux/acpi_pmtmr.h>
 #include <linux/cpufreq.h>

commit ff4c86635ee12461fd3bd911d7d5253394da8f9d
Author: Len Brown <len.brown@intel.com>
Date:   Fri Jun 17 01:22:52 2016 -0400

    x86/tsc: Enumerate BXT tsc_khz via CPUID
    
    Hard code the BXT crystal clock (aka ART - Always Running Timer)
    to 19.200 MHz, and use CPUID leaf 0x15 to determine the BXT TSC frequency.
    
    Use tsc_khz to sanity check BXT cpu_khz,
    which can be erroneous in some configurations.
    
    (I simplified the original patch from Bin Gao.)
    
    Original-From: Bin Gao <bin.gao@intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/bf4e7c175acd6d09719c47c319b10ff1f0627ff8.1466138954.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index e1496b79c28a..2a952fcb1516 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -693,7 +693,11 @@ unsigned long native_calibrate_tsc(void)
 		switch (boot_cpu_data.x86_model) {
 		case 0x4E:	/* SKL */
 		case 0x5E:	/* SKL */
-			crystal_khz = 24000;	/* 24 MHz */
+			crystal_khz = 24000;	/* 24.0 MHz */
+			break;
+		case 0x5C:	/* BXT */
+			crystal_khz = 19200;	/* 19.2 MHz */
+			break;
 		}
 	}
 
@@ -895,6 +899,8 @@ int recalibrate_cpu_khz(void)
 	tsc_khz = x86_platform.calibrate_tsc();
 	if (tsc_khz == 0)
 		tsc_khz = cpu_khz;
+	else if (abs(cpu_khz - tsc_khz) * 10 > tsc_khz)
+		cpu_khz = tsc_khz;
 	cpu_data(0).loops_per_jiffy = cpufreq_scale(cpu_data(0).loops_per_jiffy,
 						    cpu_khz_old, cpu_khz);
 
@@ -1302,8 +1308,16 @@ void __init tsc_init(void)
 
 	cpu_khz = x86_platform.calibrate_cpu();
 	tsc_khz = x86_platform.calibrate_tsc();
+
+	/*
+	 * Trust non-zero tsc_khz as authorative,
+	 * and use it to sanity check cpu_khz,
+	 * which will be off if system timer is off.
+	 */
 	if (tsc_khz == 0)
 		tsc_khz = cpu_khz;
+	else if (abs(cpu_khz - tsc_khz) * 10 > tsc_khz)
+		cpu_khz = tsc_khz;
 
 	if (!tsc_khz) {
 		mark_tsc_unstable("could not calculate TSC khz");

commit aa297292d708e89773b3b2cdcaf33f01bfa095d8
Author: Len Brown <len.brown@intel.com>
Date:   Fri Jun 17 01:22:51 2016 -0400

    x86/tsc: Enumerate SKL cpu_khz and tsc_khz via CPUID
    
    Skylake CPU base-frequency and TSC frequency may differ
    by up to 2%.
    
    Enumerate CPU and TSC frequencies separately, allowing
    cpu_khz and tsc_khz to differ.
    
    The existing CPU frequency calibration mechanism is unchanged.
    However, CPUID extensions are preferred, when available.
    
    CPUID.0x16 is preferred over MSR and timer calibration
    for CPU frequency discovery.
    
    CPUID.0x15 takes precedence over CPU-frequency
    for TSC frequency discovery.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/b27ec289fd005833b27d694d9c2dbb716c5cdff7.1466138954.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 35a3976c19cc..e1496b79c28a 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -239,7 +239,7 @@ static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 	return ns;
 }
 
-static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
+static void set_cyc2ns_scale(unsigned long khz, int cpu)
 {
 	unsigned long long tsc_now, ns_now;
 	struct cyc2ns_data *data;
@@ -248,7 +248,7 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 	local_irq_save(flags);
 	sched_clock_idle_sleep_event();
 
-	if (!cpu_khz)
+	if (!khz)
 		goto done;
 
 	data = cyc2ns_write_begin(cpu);
@@ -261,7 +261,7 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 	 * time function is continuous; see the comment near struct
 	 * cyc2ns_data.
 	 */
-	clocks_calc_mult_shift(&data->cyc2ns_mul, &data->cyc2ns_shift, cpu_khz,
+	clocks_calc_mult_shift(&data->cyc2ns_mul, &data->cyc2ns_shift, khz,
 			       NSEC_PER_MSEC, 0);
 
 	/*
@@ -665,15 +665,72 @@ static unsigned long quick_pit_calibrate(void)
 }
 
 /**
- * native_calibrate_tsc - calibrate the tsc on boot
+ * native_calibrate_tsc
+ * Determine TSC frequency via CPUID, else return 0.
  */
 unsigned long native_calibrate_tsc(void)
+{
+	unsigned int eax_denominator, ebx_numerator, ecx_hz, edx;
+	unsigned int crystal_khz;
+
+	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)
+		return 0;
+
+	if (boot_cpu_data.cpuid_level < 0x15)
+		return 0;
+
+	eax_denominator = ebx_numerator = ecx_hz = edx = 0;
+
+	/* CPUID 15H TSC/Crystal ratio, plus optionally Crystal Hz */
+	cpuid(0x15, &eax_denominator, &ebx_numerator, &ecx_hz, &edx);
+
+	if (ebx_numerator == 0 || eax_denominator == 0)
+		return 0;
+
+	crystal_khz = ecx_hz / 1000;
+
+	if (crystal_khz == 0) {
+		switch (boot_cpu_data.x86_model) {
+		case 0x4E:	/* SKL */
+		case 0x5E:	/* SKL */
+			crystal_khz = 24000;	/* 24 MHz */
+		}
+	}
+
+	return crystal_khz * ebx_numerator / eax_denominator;
+}
+
+static unsigned long cpu_khz_from_cpuid(void)
+{
+	unsigned int eax_base_mhz, ebx_max_mhz, ecx_bus_mhz, edx;
+
+	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)
+		return 0;
+
+	if (boot_cpu_data.cpuid_level < 0x16)
+		return 0;
+
+	eax_base_mhz = ebx_max_mhz = ecx_bus_mhz = edx = 0;
+
+	cpuid(0x16, &eax_base_mhz, &ebx_max_mhz, &ecx_bus_mhz, &edx);
+
+	return eax_base_mhz * 1000;
+}
+
+/**
+ * native_calibrate_cpu - calibrate the cpu on boot
+ */
+unsigned long native_calibrate_cpu(void)
 {
 	u64 tsc1, tsc2, delta, ref1, ref2;
 	unsigned long tsc_pit_min = ULONG_MAX, tsc_ref_min = ULONG_MAX;
 	unsigned long flags, latch, ms, fast_calibrate;
 	int hpet = is_hpet_enabled(), i, loopmin;
 
+	fast_calibrate = cpu_khz_from_cpuid();
+	if (fast_calibrate)
+		return fast_calibrate;
+
 	fast_calibrate = cpu_khz_from_msr();
 	if (fast_calibrate)
 		return fast_calibrate;
@@ -834,8 +891,10 @@ int recalibrate_cpu_khz(void)
 	if (!boot_cpu_has(X86_FEATURE_TSC))
 		return -ENODEV;
 
+	cpu_khz = x86_platform.calibrate_cpu();
 	tsc_khz = x86_platform.calibrate_tsc();
-	cpu_khz = tsc_khz;
+	if (tsc_khz == 0)
+		tsc_khz = cpu_khz;
 	cpu_data(0).loops_per_jiffy = cpufreq_scale(cpu_data(0).loops_per_jiffy,
 						    cpu_khz_old, cpu_khz);
 
@@ -1241,8 +1300,10 @@ void __init tsc_init(void)
 		return;
 	}
 
+	cpu_khz = x86_platform.calibrate_cpu();
 	tsc_khz = x86_platform.calibrate_tsc();
-	cpu_khz = tsc_khz;
+	if (tsc_khz == 0)
+		tsc_khz = cpu_khz;
 
 	if (!tsc_khz) {
 		mark_tsc_unstable("could not calculate TSC khz");
@@ -1262,7 +1323,7 @@ void __init tsc_init(void)
 	 */
 	for_each_possible_cpu(cpu) {
 		cyc2ns_init(cpu);
-		set_cyc2ns_scale(cpu_khz, cpu);
+		set_cyc2ns_scale(tsc_khz, cpu);
 	}
 
 	if (tsc_disabled > 0)

commit 02c0cd2dcf7fdc47d054b855b148ea8b82dbb7eb
Author: Len Brown <len.brown@intel.com>
Date:   Fri Jun 17 01:22:50 2016 -0400

    x86/tsc_msr: Remove irqoff around MSR-based TSC enumeration
    
    Remove the irqoff/irqon around MSR-based TSC enumeration,
    as it is not necessary.
    
    Also rename: try_msr_calibrate_tsc() to cpu_khz_from_msr(),
    as that better describes what the routine does.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/a6b5c3ecd3b068175d2309599ab28163fc34215e.1466138954.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 38ba6de56ede..35a3976c19cc 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -674,10 +674,7 @@ unsigned long native_calibrate_tsc(void)
 	unsigned long flags, latch, ms, fast_calibrate;
 	int hpet = is_hpet_enabled(), i, loopmin;
 
-	/* Calibrate TSC using MSR for Intel Atom SoCs */
-	local_irq_save(flags);
-	fast_calibrate = try_msr_calibrate_tsc();
-	local_irq_restore(flags);
+	fast_calibrate = cpu_khz_from_msr();
 	if (fast_calibrate)
 		return fast_calibrate;
 

commit eff4677e9fb9b680d1d5f6ba079116548d072b7e
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Apr 5 08:29:53 2016 +0200

    x86/tsc: Save an indentation level in recalibrate_cpu_khz()
    
    ... by flipping the check.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1459837795-2588-5-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 5bb702c77e8f..38ba6de56ede 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -834,15 +834,15 @@ int recalibrate_cpu_khz(void)
 #ifndef CONFIG_SMP
 	unsigned long cpu_khz_old = cpu_khz;
 
-	if (boot_cpu_has(X86_FEATURE_TSC)) {
-		tsc_khz = x86_platform.calibrate_tsc();
-		cpu_khz = tsc_khz;
-		cpu_data(0).loops_per_jiffy =
-			cpufreq_scale(cpu_data(0).loops_per_jiffy,
-					cpu_khz_old, cpu_khz);
-		return 0;
-	} else
+	if (!boot_cpu_has(X86_FEATURE_TSC))
 		return -ENODEV;
+
+	tsc_khz = x86_platform.calibrate_tsc();
+	cpu_khz = tsc_khz;
+	cpu_data(0).loops_per_jiffy = cpufreq_scale(cpu_data(0).loops_per_jiffy,
+						    cpu_khz_old, cpu_khz);
+
+	return 0;
 #else
 	return -ENODEV;
 #endif

commit a841cca74ea7612508aee161c89987b2646ed769
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Apr 5 08:29:52 2016 +0200

    x86/tsc: Do not check X86_FEATURE_CONSTANT_TSC in notifier call
    
    ... because the notifier-registering routine already does that. Also,
    rename cpufreq_tsc() init call to something more telling.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1459837795-2588-4-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a0346bc51833..5bb702c77e8f 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -922,9 +922,6 @@ static int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
 	struct cpufreq_freqs *freq = data;
 	unsigned long *lpj;
 
-	if (cpu_has(&cpu_data(freq->cpu), X86_FEATURE_CONSTANT_TSC))
-		return 0;
-
 	lpj = &boot_cpu_data.loops_per_jiffy;
 #ifdef CONFIG_SMP
 	if (!(freq->flags & CPUFREQ_CONST_LOOPS))
@@ -954,7 +951,7 @@ static struct notifier_block time_cpufreq_notifier_block = {
 	.notifier_call  = time_cpufreq_notifier
 };
 
-static int __init cpufreq_tsc(void)
+static int __init cpufreq_register_tsc_scaling(void)
 {
 	if (!boot_cpu_has(X86_FEATURE_TSC))
 		return 0;
@@ -965,7 +962,7 @@ static int __init cpufreq_tsc(void)
 	return 0;
 }
 
-core_initcall(cpufreq_tsc);
+core_initcall(cpufreq_register_tsc_scaling);
 
 #endif /* CONFIG_CPU_FREQ */
 

commit 59e21e3d00e6bc23186763c3e0bf11baf8924124
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Apr 4 22:24:59 2016 +0200

    x86/cpufeature: Replace cpu_has_tsc with boot_cpu_has() usage
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Sailer <t.sailer@alumni.ethz.ch>
    Link: http://lkml.kernel.org/r/1459801503-15600-7-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index c9c4c7ce3eb2..a0346bc51833 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -36,7 +36,7 @@ static int __read_mostly tsc_unstable;
 
 /* native_sched_clock() is called before tsc_init(), so
    we must start with the TSC soft disabled to prevent
-   erroneous rdtsc usage on !cpu_has_tsc processors */
+   erroneous rdtsc usage on !boot_cpu_has(X86_FEATURE_TSC) processors */
 static int __read_mostly tsc_disabled = -1;
 
 static DEFINE_STATIC_KEY_FALSE(__use_tsc);
@@ -834,7 +834,7 @@ int recalibrate_cpu_khz(void)
 #ifndef CONFIG_SMP
 	unsigned long cpu_khz_old = cpu_khz;
 
-	if (cpu_has_tsc) {
+	if (boot_cpu_has(X86_FEATURE_TSC)) {
 		tsc_khz = x86_platform.calibrate_tsc();
 		cpu_khz = tsc_khz;
 		cpu_data(0).loops_per_jiffy =
@@ -956,7 +956,7 @@ static struct notifier_block time_cpufreq_notifier_block = {
 
 static int __init cpufreq_tsc(void)
 {
-	if (!cpu_has_tsc)
+	if (!boot_cpu_has(X86_FEATURE_TSC))
 		return 0;
 	if (boot_cpu_has(X86_FEATURE_CONSTANT_TSC))
 		return 0;
@@ -1081,7 +1081,7 @@ static void __init check_system_tsc_reliable(void)
  */
 int unsynchronized_tsc(void)
 {
-	if (!cpu_has_tsc || tsc_unstable)
+	if (!boot_cpu_has(X86_FEATURE_TSC) || tsc_unstable)
 		return 1;
 
 #ifdef CONFIG_SMP
@@ -1205,7 +1205,7 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 
 static int __init init_tsc_clocksource(void)
 {
-	if (!cpu_has_tsc || tsc_disabled > 0 || !tsc_khz)
+	if (!boot_cpu_has(X86_FEATURE_TSC) || tsc_disabled > 0 || !tsc_khz)
 		return 0;
 
 	if (tsc_clocksource_reliable)
@@ -1242,7 +1242,7 @@ void __init tsc_init(void)
 	u64 lpj;
 	int cpu;
 
-	if (!cpu_has_tsc) {
+	if (!boot_cpu_has(X86_FEATURE_TSC)) {
 		setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
 		return;
 	}

commit f508a5ba7a4570418df6cfd68fe663ffdef2be63
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 18 08:35:29 2016 +0100

    x86/tsc: Prevent NULL pointer deref in calibrate_delay_is_known()
    
    The topology_core_cpumask is used to find a neighbour cpu in
    calibrate_delay_is_known(). It might not be allocated at the first invocation
    of that function on the boot cpu, when CONFIG_CPUMASK_OFFSTACK is set.
    
    The mask is allocated later in native_smp_prepare_cpus. As a consequence the
    underlying find_next_bit() call dereferences a NULL pointer.
    
    Add a proper check to prevent this.
    
    Fixes: c25323c07345 "x86/tsc: Use topology functions"
    Reported-and-tested-by: Richard W.M. Jones <rjones@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Josh Boyer <jwboyer@fedoraproject.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1603180843270.3978@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 5e19d2587cc5..c9c4c7ce3eb2 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1306,11 +1306,15 @@ void __init tsc_init(void)
 unsigned long calibrate_delay_is_known(void)
 {
 	int sibling, cpu = smp_processor_id();
+	struct cpumask *mask = topology_core_cpumask(cpu);
 
 	if (!tsc_disabled && !cpu_has(&cpu_data(cpu), X86_FEATURE_CONSTANT_TSC))
 		return 0;
 
-	sibling = cpumask_any_but(topology_core_cpumask(cpu), cpu);
+	if (!mask)
+		return 0;
+
+	sibling = cpumask_any_but(mask, cpu);
 	if (sibling < nr_cpu_ids)
 		return cpu_data(sibling).loops_per_jiffy;
 	return 0;

commit 00f526850151e91fdad0896a1436341687ad2582
Merge: cbf8b5a2b649 d89abe2a1f0c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Mar 17 09:44:57 2016 +0100

    Merge branch 'x86/cleanups' into x86/urgent
    
    Pull in some merge window leftovers.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 8a284c062ec923c924c79e3b1b5199b8d72904fc
Merge: 208de2147767 6436257b491c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 12:13:56 2016 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "The timer department delivers this time:
    
       - Support for cross clock domain timestamps in the core code plus a
         first user.  That allows more precise timestamping for PTP and
         later for audio and other peripherals.
    
         The ptp/e1000e patches have been acked by the relevant maintainers
         and are carried in the timer tree to avoid merge ordering issues.
    
       - Support for unregistering the current clocksource watchdog.  That
         lifts a limitation for switching clocksources which has been there
         from day 1
    
       - The usual pile of fixes and updates to the core and the drivers.
         Nothing outstanding and exciting"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (26 commits)
      time/timekeeping: Work around false positive GCC warning
      e1000e: Adds hardware supported cross timestamp on e1000e nic
      ptp: Add PTP_SYS_OFFSET_PRECISE for driver crosstimestamping
      x86/tsc: Always Running Timer (ART) correlated clocksource
      hrtimer: Revert CLOCK_MONOTONIC_RAW support
      time: Add history to cross timestamp interface supporting slower devices
      time: Add driver cross timestamp interface for higher precision time synchronization
      time: Remove duplicated code in ktime_get_raw_and_real()
      time: Add timekeeping snapshot code capturing system time and counter
      time: Add cycles to nanoseconds translation
      jiffies: Use CLOCKSOURCE_MASK instead of constant
      clocksource: Introduce clocksource_freq2mult()
      clockevents/drivers/exynos_mct: Implement ->set_state_oneshot_stopped()
      clockevents/drivers/arm_global_timer: Implement ->set_state_oneshot_stopped()
      clockevents/drivers/arm_arch_timer: Implement ->set_state_oneshot_stopped()
      clocksource/drivers/arm_global_timer: Register delay timer
      clocksource/drivers/lpc32xx: Support timer-based ARM delay
      clocksource/drivers/lpc32xx: Support periodic mode
      clocksource/drivers/lpc32xx: Don't use the prescaler counter for clockevents
      clocksource/drivers/rockchip: Add err handle for rk_timer_init
      ...

commit f9677e0f83080bb4186865868c359e72e1fac1ea
Author: Christopher S. Hall <christopher.s.hall@intel.com>
Date:   Mon Feb 29 06:33:47 2016 -0800

    x86/tsc: Always Running Timer (ART) correlated clocksource
    
    On modern Intel systems TSC is derived from the new Always Running Timer
    (ART). ART can be captured simultaneous to the capture of
    audio and network device clocks, allowing a correlation between timebases
    to be constructed. Upon capture, the driver converts the captured ART
    value to the appropriate system clock using the correlated clocksource
    mechanism.
    
    On systems that support ART a new CPUID leaf (0x15) returns parameters
    “m” and “n” such that:
    
    TSC_value = (ART_value * m) / n + k [n >= 1]
    
    [k is an offset that can adjusted by a privileged agent. The
    IA32_TSC_ADJUST MSR is an example of an interface to adjust k.
    See 17.14.4 of the Intel SDM for more details]
    
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: kevin.b.stanton@intel.com
    Cc: kevin.j.clarke@intel.com
    Cc: hpa@zytor.com
    Cc: jeffrey.t.kirsher@intel.com
    Cc: netdev@vger.kernel.org
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Christopher S. Hall <christopher.s.hall@intel.com>
    [jstultz: Tweaked to fix build issue, also reworked math for
    64bit division on 32bit systems, as well as !CONFIG_CPU_FREQ build
    fixes]
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3d743da828d3..80d761e420c5 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -43,6 +43,11 @@ static DEFINE_STATIC_KEY_FALSE(__use_tsc);
 
 int tsc_clocksource_reliable;
 
+static u32 art_to_tsc_numerator;
+static u32 art_to_tsc_denominator;
+static u64 art_to_tsc_offset;
+struct clocksource *art_related_clocksource;
+
 /*
  * Use a ring-buffer like data structure, where a writer advances the head by
  * writing a new data entry and a reader advances the tail when it observes a
@@ -964,6 +969,37 @@ core_initcall(cpufreq_tsc);
 
 #endif /* CONFIG_CPU_FREQ */
 
+#define ART_CPUID_LEAF (0x15)
+#define ART_MIN_DENOMINATOR (1)
+
+
+/*
+ * If ART is present detect the numerator:denominator to convert to TSC
+ */
+static void detect_art(void)
+{
+	unsigned int unused[2];
+
+	if (boot_cpu_data.cpuid_level < ART_CPUID_LEAF)
+		return;
+
+	cpuid(ART_CPUID_LEAF, &art_to_tsc_denominator,
+	      &art_to_tsc_numerator, unused, unused+1);
+
+	/* Don't enable ART in a VM, non-stop TSC required */
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR) ||
+	    !boot_cpu_has(X86_FEATURE_NONSTOP_TSC) ||
+	    art_to_tsc_denominator < ART_MIN_DENOMINATOR)
+		return;
+
+	if (rdmsrl_safe(MSR_IA32_TSC_ADJUST, &art_to_tsc_offset))
+		return;
+
+	/* Make this sticky over multiple CPU init calls */
+	setup_force_cpu_cap(X86_FEATURE_ART);
+}
+
+
 /* clocksource code */
 
 static struct clocksource clocksource_tsc;
@@ -1071,6 +1107,25 @@ int unsynchronized_tsc(void)
 	return 0;
 }
 
+/*
+ * Convert ART to TSC given numerator/denominator found in detect_art()
+ */
+struct system_counterval_t convert_art_to_tsc(cycle_t art)
+{
+	u64 tmp, res, rem;
+
+	rem = do_div(art, art_to_tsc_denominator);
+
+	res = art * art_to_tsc_numerator;
+	tmp = rem * art_to_tsc_numerator;
+
+	do_div(tmp, art_to_tsc_denominator);
+	res += tmp + art_to_tsc_offset;
+
+	return (struct system_counterval_t) {.cs = art_related_clocksource,
+			.cycles = res};
+}
+EXPORT_SYMBOL(convert_art_to_tsc);
 
 static void tsc_refine_calibration_work(struct work_struct *work);
 static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
@@ -1142,6 +1197,8 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 		(unsigned long)tsc_khz % 1000);
 
 out:
+	if (boot_cpu_has(X86_FEATURE_ART))
+		art_related_clocksource = &clocksource_tsc;
 	clocksource_register_khz(&clocksource_tsc, tsc_khz);
 }
 
@@ -1235,6 +1292,8 @@ void __init tsc_init(void)
 		mark_tsc_unstable("TSCs unsynchronized");
 
 	check_system_tsc_reliable();
+
+	detect_art();
 }
 
 #ifdef CONFIG_SMP

commit 6a6256f9e0ebaabf7ded1fef8977a4352dbe7784
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Tue Feb 23 15:34:30 2016 -0800

    x86: Fix misspellings in comments
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: trivial@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3d743da828d3..acec49b302d1 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -876,7 +876,7 @@ void tsc_restore_sched_clock_state(void)
 	local_irq_save(flags);
 
 	/*
-	 * We're comming out of suspend, there's no concurrency yet; don't
+	 * We're coming out of suspend, there's no concurrency yet; don't
 	 * bother being nice about the RCU stuff, just write to both
 	 * data fields.
 	 */

commit c25323c07345a843a56a294047b130dfd9250fad
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 18 20:53:43 2016 +0100

    x86/tsc: Use topology functions
    
    It's simpler to look at the topology mask than iterating over all online cpus
    to find a cpu on the same package.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3d743da828d3..5a6cb4684e0f 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1246,14 +1246,14 @@ void __init tsc_init(void)
  */
 unsigned long calibrate_delay_is_known(void)
 {
-	int i, cpu = smp_processor_id();
+	int sibling, cpu = smp_processor_id();
 
 	if (!tsc_disabled && !cpu_has(&cpu_data(cpu), X86_FEATURE_CONSTANT_TSC))
 		return 0;
 
-	for_each_online_cpu(i)
-		if (cpu_data(i).phys_proc_id == cpu_data(cpu).phys_proc_id)
-			return cpu_data(i).loops_per_jiffy;
+	sibling = cpumask_any_but(topology_core_cpumask(cpu), cpu);
+	if (sibling < nr_cpu_ids)
+		return cpu_data(sibling).loops_per_jiffy;
 	return 0;
 }
 #endif

commit 2f7a3f8e871eb0713d23c533bd5e44a544e43eb8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 17 15:09:46 2015 +0100

    x86/tsc: Remove unused tsc_pre_init() hook
    
    No more users. Remove it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index c7c4d9c51e99..3d743da828d3 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1185,8 +1185,6 @@ void __init tsc_init(void)
 	u64 lpj;
 	int cpu;
 
-	x86_init.timers.tsc_pre_init();
-
 	if (!cpu_has_tsc) {
 		setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
 		return;

commit b9511cd761faafca7a1acc059e792c1399f9d7c6
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Fri Oct 16 16:24:05 2015 +0300

    perf/x86: Fix time_shift in perf_event_mmap_page
    
    Commit:
    
      b20112edeadf ("perf/x86: Improve accuracy of perf/sched clock")
    
    allowed the time_shift value in perf_event_mmap_page to be as much
    as 32.  Unfortunately the documented algorithms for using time_shift
    have it shifting an integer, whereas to work correctly with the value
    32, the type must be u64.
    
    In the case of perf tools, Intel PT decodes correctly but the timestamps
    that are output (for example by perf script) have lost 32-bits of
    granularity so they look like they are not changing at all.
    
    Fix by limiting the shift to 31 and adjusting the multiplier accordingly.
    
    Also update the documentation of perf_event_mmap_page so that new code
    based on it will be more future-proof.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Fixes: b20112edeadf ("perf/x86: Improve accuracy of perf/sched clock")
    Link: http://lkml.kernel.org/r/1445001845-13688-2-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 69b84a26ea17..c7c4d9c51e99 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -259,6 +259,17 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 	clocks_calc_mult_shift(&data->cyc2ns_mul, &data->cyc2ns_shift, cpu_khz,
 			       NSEC_PER_MSEC, 0);
 
+	/*
+	 * cyc2ns_shift is exported via arch_perf_update_userpage() where it is
+	 * not expected to be greater than 31 due to the original published
+	 * conversion algorithm shifting a 32-bit value (now specifies a 64-bit
+	 * value) - refer perf_event_mmap_page documentation in perf_event.h.
+	 */
+	if (data->cyc2ns_shift == 32) {
+		data->cyc2ns_shift = 31;
+		data->cyc2ns_mul >>= 1;
+	}
+
 	data->cyc2ns_offset = ns_now -
 		mul_u64_u32_shr(tsc_now, data->cyc2ns_mul, data->cyc2ns_shift);
 

commit 02386c356af0ce5bbee11ed9b23c312ca60298f0
Merge: d71b0ad8d309 f73e22ab4501
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Sep 18 09:24:01 2015 +0200

    Merge branch 'perf/urgent' into perf/core, to pick up fixes before applying new changes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 42dc2a3048247109b0a5ee6345226cbd3e4f6410
Merge: 1345df21ac54 03da3ff1cfcd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 17 11:01:34 2015 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     - misc fixes all around the map
     - block non-root vm86(old) if mmap_min_addr != 0
     - two small debuggability improvements
     - removal of obsolete paravirt op
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/platform: Fix Geode LX timekeeping in the generic x86 build
      x86/apic: Serialize LVTT and TSC_DEADLINE writes
      x86/ioapic: Force affinity setting in setup_ioapic_dest()
      x86/paravirt: Remove the unused pv_time_ops::get_tsc_khz method
      x86/ldt: Fix small LDT allocation for Xen
      x86/vm86: Fix the misleading CONFIG_VM86 Kconfig help text
      x86/cpu: Print family/model/stepping in hex
      x86/vm86: Block non-root vm86(old) if mmap_min_addr != 0
      x86/alternatives: Make optimize_nops() interrupt safe and synced
      x86/mm/srat: Print non-volatile flag in SRAT
      x86/cpufeatures: Enable cpuid for Intel SHA extensions

commit 03da3ff1cfcd7774c8780d2547ba0d995f7dc03d
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Sep 16 14:10:03 2015 +0100

    x86/platform: Fix Geode LX timekeeping in the generic x86 build
    
    In 2007, commit 07190a08eef36 ("Mark TSC on GeodeLX reliable")
    bypassed verification of the TSC on Geode LX. However, this code
    (now in the check_system_tsc_reliable() function in
    arch/x86/kernel/tsc.c) was only present if CONFIG_MGEODE_LX was
    set.
    
    OpenWRT has recently started building its generic Geode target
    for Geode GX, not LX, to include support for additional
    platforms. This broke the timekeeping on LX-based devices,
    because the TSC wasn't marked as reliable:
    https://dev.openwrt.org/ticket/20531
    
    By adding a runtime check on is_geode_lx(), we can also include
    the fix if CONFIG_MGEODEGX1 or CONFIG_X86_GENERIC are set, thus
    fixing the problem.
    
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>
    Cc: Andres Salomon <dilinger@queued.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Marcelo Tosatti <marcelo@kvack.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/1442409003.131189.87.camel@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 79055cf2c497..51e62d6afd9a 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -21,6 +21,7 @@
 #include <asm/hypervisor.h>
 #include <asm/nmi.h>
 #include <asm/x86_init.h>
+#include <asm/geode.h>
 
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
@@ -1015,15 +1016,17 @@ EXPORT_SYMBOL_GPL(mark_tsc_unstable);
 
 static void __init check_system_tsc_reliable(void)
 {
-#ifdef CONFIG_MGEODE_LX
-	/* RTSC counts during suspend */
+#if defined(CONFIG_MGEODEGX1) || defined(CONFIG_MGEODE_LX) || defined(CONFIG_X86_GENERIC)
+	if (is_geode_lx()) {
+		/* RTSC counts during suspend */
 #define RTSC_SUSP 0x100
-	unsigned long res_low, res_high;
+		unsigned long res_low, res_high;
 
-	rdmsr_safe(MSR_GEODE_BUSCONT_CONF0, &res_low, &res_high);
-	/* Geode_LX - the OLPC CPU has a very reliable TSC */
-	if (res_low & RTSC_SUSP)
-		tsc_clocksource_reliable = 1;
+		rdmsr_safe(MSR_GEODE_BUSCONT_CONF0, &res_low, &res_high);
+		/* Geode_LX - the OLPC CPU has a very reliable TSC */
+		if (res_low & RTSC_SUSP)
+			tsc_clocksource_reliable = 1;
+	}
 #endif
 	if (boot_cpu_has(X86_FEATURE_TSC_RELIABLE))
 		tsc_clocksource_reliable = 1;

commit b20112edeadf0b8a1416de061caa4beb11539902
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Fri Aug 21 12:05:18 2015 +0300

    perf/x86: Improve accuracy of perf/sched clock
    
    When TSC is stable perf/sched clock is based on it.
    However the conversion from cycles to nanoseconds
    is not as accurate as it could be.  Because
    CYC2NS_SCALE_FACTOR is 10, the accuracy is +/- 1/2048
    
    The change is to calculate the maximum shift that
    results in a multiplier that is still a 32-bit number.
    For example all frequencies over 1 GHz will have
    a shift of 32, making the accuracy of the conversion
    +/- 1/(2^33).  That is achieved by using the
    'clocks_calc_mult_shift()' function.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/1440147918-22250-1-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index c8d52cb4cb6e..39ec3d07affd 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -167,21 +167,20 @@ static void cyc2ns_write_end(int cpu, struct cyc2ns_data *data)
  *              ns = cycles * cyc2ns_scale / SC
  *
  *      And since SC is a constant power of two, we can convert the div
- *  into a shift.
+ *  into a shift. The larger SC is, the more accurate the conversion, but
+ *  cyc2ns_scale needs to be a 32-bit value so that 32-bit multiplication
+ *  (64-bit result) can be used.
  *
- *  We can use khz divisor instead of mhz to keep a better precision, since
- *  cyc2ns_scale is limited to 10^6 * 2^10, which fits in 32 bits.
+ *  We can use khz divisor instead of mhz to keep a better precision.
  *  (mathieu.desnoyers@polymtl.ca)
  *
  *                      -johnstul@us.ibm.com "math is hard, lets go shopping!"
  */
 
-#define CYC2NS_SCALE_FACTOR 10 /* 2^10, carefully chosen */
-
 static void cyc2ns_data_init(struct cyc2ns_data *data)
 {
 	data->cyc2ns_mul = 0;
-	data->cyc2ns_shift = CYC2NS_SCALE_FACTOR;
+	data->cyc2ns_shift = 0;
 	data->cyc2ns_offset = 0;
 	data->__count = 0;
 }
@@ -215,14 +214,14 @@ static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 
 	if (likely(data == tail)) {
 		ns = data->cyc2ns_offset;
-		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, CYC2NS_SCALE_FACTOR);
+		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, data->cyc2ns_shift);
 	} else {
 		data->__count++;
 
 		barrier();
 
 		ns = data->cyc2ns_offset;
-		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, CYC2NS_SCALE_FACTOR);
+		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, data->cyc2ns_shift);
 
 		barrier();
 
@@ -256,12 +255,11 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 	 * time function is continuous; see the comment near struct
 	 * cyc2ns_data.
 	 */
-	data->cyc2ns_mul =
-		DIV_ROUND_CLOSEST(NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR,
-				  cpu_khz);
-	data->cyc2ns_shift = CYC2NS_SCALE_FACTOR;
+	clocks_calc_mult_shift(&data->cyc2ns_mul, &data->cyc2ns_shift, cpu_khz,
+			       NSEC_PER_MSEC, 0);
+
 	data->cyc2ns_offset = ns_now -
-		mul_u64_u32_shr(tsc_now, data->cyc2ns_mul, CYC2NS_SCALE_FACTOR);
+		mul_u64_u32_shr(tsc_now, data->cyc2ns_mul, data->cyc2ns_shift);
 
 	cyc2ns_write_end(cpu, data);
 

commit ca520cab25e0e8da717c596ccaa2c2b3650cfa09
Merge: 4c12ab7e5e2e d420acd816c0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 3 15:46:07 2015 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking and atomic updates from Ingo Molnar:
     "Main changes in this cycle are:
    
       - Extend atomic primitives with coherent logic op primitives
         (atomic_{or,and,xor}()) and deprecate the old partial APIs
         (atomic_{set,clear}_mask())
    
         The old ops were incoherent with incompatible signatures across
         architectures and with incomplete support.  Now every architecture
         supports the primitives consistently (by Peter Zijlstra)
    
       - Generic support for 'relaxed atomics':
    
           - _acquire/release/relaxed() flavours of xchg(), cmpxchg() and {add,sub}_return()
           - atomic_read_acquire()
           - atomic_set_release()
    
         This came out of porting qwrlock code to arm64 (by Will Deacon)
    
       - Clean up the fragile static_key APIs that were causing repeat bugs,
         by introducing a new one:
    
           DEFINE_STATIC_KEY_TRUE(name);
           DEFINE_STATIC_KEY_FALSE(name);
    
         which define a key of different types with an initial true/false
         value.
    
         Then allow:
    
           static_branch_likely()
           static_branch_unlikely()
    
         to take a key of either type and emit the right instruction for the
         case.  To be able to know the 'type' of the static key we encode it
         in the jump entry (by Peter Zijlstra)
    
       - Static key self-tests (by Jason Baron)
    
       - qrwlock optimizations (by Waiman Long)
    
       - small futex enhancements (by Davidlohr Bueso)
    
       - ... and misc other changes"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (63 commits)
      jump_label/x86: Work around asm build bug on older/backported GCCs
      locking, ARM, atomics: Define our SMP atomics in terms of _relaxed() operations
      locking, include/llist: Use linux/atomic.h instead of asm/cmpxchg.h
      locking/qrwlock: Make use of _{acquire|release|relaxed}() atomics
      locking/qrwlock: Implement queue_write_unlock() using smp_store_release()
      locking/lockref: Remove homebrew cmpxchg64_relaxed() macro definition
      locking, asm-generic: Add _{relaxed|acquire|release}() variants for 'atomic_long_t'
      locking, asm-generic: Rework atomic-long.h to avoid bulk code duplication
      locking/atomics: Add _{acquire|release|relaxed}() variants of some atomic operations
      locking, compiler.h: Cast away attributes in the WRITE_ONCE() magic
      locking/static_keys: Make verify_keys() static
      jump label, locking/static_keys: Update docs
      locking/static_keys: Provide a selftest
      jump_label: Provide a self-test
      s390/uaccess, locking/static_keys: employ static_branch_likely()
      x86, tsc, locking/static_keys: Employ static_branch_likely()
      locking/static_keys: Add selftest
      locking/static_keys: Add a new static_key interface
      locking/static_keys: Rework update logic
      locking/static_keys: Add static_key_{en,dis}able() helpers
      ...

commit 5778077d03cb25aac9b6a428e18970642fc019e3
Merge: 65a99597f044 7e01ebffffed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 1 08:40:25 2015 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm changes from Ingo Molnar:
     "The biggest changes in this cycle were:
    
       - Revamp, simplify (and in some cases fix) Time Stamp Counter (TSC)
         primitives.  (Andy Lutomirski)
    
       - Add new, comprehensible entry and exit handlers written in C.
         (Andy Lutomirski)
    
       - vm86 mode cleanups and fixes.  (Brian Gerst)
    
       - 32-bit compat code cleanups.  (Brian Gerst)
    
      The amount of simplification in low level assembly code is already
      palpable:
    
         arch/x86/entry/entry_32.S                          | 130 +----
         arch/x86/entry/entry_64.S                          | 197 ++-----
    
      but more simplifications are planned.
    
      There's also the usual laudry mix of low level changes - see the
      changelog for details"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (83 commits)
      x86/asm: Drop repeated macro of X86_EFLAGS_AC definition
      x86/asm/msr: Make wrmsrl() a function
      x86/asm/delay: Introduce an MWAITX-based delay with a configurable timer
      x86/asm: Add MONITORX/MWAITX instruction support
      x86/traps: Weaken context tracking entry assertions
      x86/asm/tsc: Add rdtscll() merge helper
      selftests/x86: Add syscall_nt selftest
      selftests/x86: Disable sigreturn_64
      x86/vdso: Emit a GNU hash
      x86/entry: Remove do_notify_resume(), syscall_trace_leave(), and their TIF masks
      x86/entry/32: Migrate to C exit path
      x86/entry/32: Remove 32-bit syscall audit optimizations
      x86/vm86: Rename vm86->v86flags and v86mask
      x86/vm86: Rename vm86->vm86_info to user_vm86
      x86/vm86: Clean up vm86.h includes
      x86/vm86: Move the vm86 IRQ definitions to vm86.h
      x86/vm86: Use the normal pt_regs area for vm86
      x86/vm86: Eliminate 'struct kernel_vm86_struct'
      x86/vm86: Move fields from 'struct kernel_vm86_struct' to 'struct vm86'
      x86/vm86: Move vm86 fields out of 'thread_struct'
      ...

commit a94cab2376cb35f236be14e2833cef63a8762a31
Author: Andi Kleen <ak@linux.intel.com>
Date:   Sun May 10 12:22:39 2015 -0700

    perf/x86: Add a native_perf_sched_clock_from_tsc()
    
    PEBSv3 has a raw TSC time stamp in its memory buffer that
    later needs to to be converted to perf_clock.
    
    Add a native_sched_clock_from_tsc() that works the same
    as native_sched_clock(), but starts with an already given
    TSC value.
    
    Paravirt is ignored, it will just get the native clock.
    But there isn't a para virtualized PEBS anyway.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: eranian@google.com
    Link: http://lkml.kernel.org/r/1431285767-27027-2-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 7437b41f6a47..88e9a38c71a5 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -296,6 +296,14 @@ u64 native_sched_clock(void)
 	return cycles_2_ns(tsc_now);
 }
 
+/*
+ * Generate a sched_clock if you already have a TSC value.
+ */
+u64 native_sched_clock_from_tsc(u64 tsc)
+{
+	return cycles_2_ns(tsc);
+}
+
 /* We need to define a real function for sched_clock, to override the
    weak default version */
 #ifdef CONFIG_PARAVIRT

commit 3bbfafb77a06327fa1bc9f19bc55b5c558475091
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jul 24 16:34:32 2015 +0200

    x86, tsc, locking/static_keys: Employ static_branch_likely()
    
    Because of the static_key restrictions we had to take an unconditional
    jump for the most likely case, causing $I bloat.
    
    Rewrite to use the new primitives.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 1bb8bab1b3cb..b9cfd462f7e7 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -38,7 +38,7 @@ static int __read_mostly tsc_unstable;
    erroneous rdtsc usage on !cpu_has_tsc processors */
 static int __read_mostly tsc_disabled = -1;
 
-static struct static_key __use_tsc = STATIC_KEY_INIT;
+static DEFINE_STATIC_KEY_FALSE(__use_tsc);
 
 int tsc_clocksource_reliable;
 
@@ -274,7 +274,12 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
  */
 u64 native_sched_clock(void)
 {
-	u64 tsc_now;
+	if (static_branch_likely(&__use_tsc)) {
+		u64 tsc_now = rdtsc();
+
+		/* return the value in ns */
+		return cycles_2_ns(tsc_now);
+	}
 
 	/*
 	 * Fall back to jiffies if there's no TSC available:
@@ -284,16 +289,9 @@ u64 native_sched_clock(void)
 	 *   very important for it to be as fast as the platform
 	 *   can achieve it. )
 	 */
-	if (!static_key_false(&__use_tsc)) {
-		/* No locking but a rare wrong value is not a big deal: */
-		return (jiffies_64 - INITIAL_JIFFIES) * (1000000000 / HZ);
-	}
-
-	/* read the Time Stamp Counter: */
-	tsc_now = rdtsc();
 
-	/* return the value in ns */
-	return cycles_2_ns(tsc_now);
+	/* No locking but a rare wrong value is not a big deal: */
+	return (jiffies_64 - INITIAL_JIFFIES) * (1000000000 / HZ);
 }
 
 /* We need to define a real function for sched_clock, to override the
@@ -1204,7 +1202,7 @@ void __init tsc_init(void)
 	/* now allow native_sched_clock() to use rdtsc */
 
 	tsc_disabled = 0;
-	static_key_slow_inc(&__use_tsc);
+	static_branch_enable(&__use_tsc);
 
 	if (!no_sched_irq_time)
 		enable_sched_clock_irqtime();

commit 5b929bd11df23922daf1be5d52731cc3900c1d79
Merge: b2c51106c758 37868fe113ff
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jul 31 10:23:35 2015 +0200

    Merge branch 'x86/urgent' into x86/asm, before applying dependent patches
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 27c634054a3155e1d9a02f0e362e4f4ff8d28ee7
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:44:10 2015 +0200

    x86/asm/tsc: Use rdtsc_ordered() in read_tsc() instead of get_cycles()
    
    There are two logical changes here.  First, this removes a check
    for cpu_has_tsc.  That check is unnecessary, as we don't
    register the TSC as a clocksource on systems that have no TSC.
    
    Second, it adds a barrier, thus preventing observable
    non-monotonicity.
    
    I suspect that the missing barrier was never a problem in
    practice because system calls themselves were heavy enough
    barriers to prevent user code from observing time warps due to
    speculation. (Without the corresponding barrier in the vDSO,
    however, non-monotonicity is easy to detect.)
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/c6ff621a053127a65b70f175443578db7a0711be.1434501121.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 21d6e04e3e82..451bade0d320 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -961,7 +961,7 @@ static struct clocksource clocksource_tsc;
  */
 static cycle_t read_tsc(struct clocksource *cs)
 {
-	return (cycle_t)get_cycles();
+	return (cycle_t)rdtsc_ordered();
 }
 
 /*

commit 4ea1636b04dbd66536fa387bae2eea463efc705b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:44:07 2015 +0200

    x86/asm/tsc: Rename native_read_tsc() to rdtsc()
    
    Now that there is no paravirt TSC, the "native" is
    inappropriate. The function does RDTSC, so give it the obvious
    name: rdtsc().
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/fd43e16281991f096c1e4d21574d9e1402c62d39.1434501121.git.luto@kernel.org
    [ Ported it to v4.2-rc1. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index e66f5dcaeb63..21d6e04e3e82 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -248,7 +248,7 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 
 	data = cyc2ns_write_begin(cpu);
 
-	tsc_now = native_read_tsc();
+	tsc_now = rdtsc();
 	ns_now = cycles_2_ns(tsc_now);
 
 	/*
@@ -290,7 +290,7 @@ u64 native_sched_clock(void)
 	}
 
 	/* read the Time Stamp Counter: */
-	tsc_now = native_read_tsc();
+	tsc_now = rdtsc();
 
 	/* return the value in ns */
 	return cycles_2_ns(tsc_now);

commit 87be28aaf1458445d5f648688c2eec0f13b8f3b9
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:43:58 2015 +0200

    x86/asm/tsc: Replace rdtscll() with native_read_tsc()
    
    Now that the ->read_tsc() paravirt hook is gone, rdtscll() is
    just a wrapper around native_read_tsc(). Unwrap it.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/d2449ae62c1b1fb90195bcfb19ef4a35883a04dc.1434501121.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index e7710cd7ba00..e66f5dcaeb63 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -248,7 +248,7 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 
 	data = cyc2ns_write_begin(cpu);
 
-	rdtscll(tsc_now);
+	tsc_now = native_read_tsc();
 	ns_now = cycles_2_ns(tsc_now);
 
 	/*
@@ -290,7 +290,7 @@ u64 native_sched_clock(void)
 	}
 
 	/* read the Time Stamp Counter: */
-	rdtscll(tsc_now);
+	tsc_now = native_read_tsc();
 
 	/* return the value in ns */
 	return cycles_2_ns(tsc_now);

commit c6e5ca35c4685cd920b1d5279dbc9f4483d7dfd4
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:43:55 2015 +0200

    x86/asm/tsc: Inline native_read_tsc() and remove __native_read_tsc()
    
    In the following commit:
    
      cdc7957d1954 ("x86: move native_read_tsc() offline")
    
    ... native_read_tsc() was moved out of line, presumably for some
    now-obsolete vDSO-related reason. Undo it.
    
    The entire rdtsc, shl, or sequence is only 11 bytes, and calls
    via rdtscl() and similar helpers were already inlined.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/d05ffe2aaf8468ca475ebc00efad7b2fa174af19.1434501121.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 505449700e0c..e7710cd7ba00 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -308,12 +308,6 @@ unsigned long long
 sched_clock(void) __attribute__((alias("native_sched_clock")));
 #endif
 
-unsigned long long native_read_tsc(void)
-{
-	return __native_read_tsc();
-}
-EXPORT_SYMBOL(native_read_tsc);
-
 int check_tsc_unstable(void)
 {
 	return tsc_unstable;

commit 5aac644a9944bea93b4f05ced1883a902a2535f6
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Wed Jun 3 10:39:46 2015 +0300

    x86/tsc: Let high latency PIT fail fast in quick_pit_calibrate()
    
    If it takes longer than 12us to read the PIT counter lsb/msb,
    then the error margin will never fall below 500ppm within 50ms,
    and Fast TSC calibration will always fail.
    
    This patch detects when that will happen and fails fast. Note
    the failure message is not printed in that case because:
    1. it will always happen on that class of hardware
    2. the absence of the message is more informative than its
    presence
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/556EB717.9070607@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 505449700e0c..7437b41f6a47 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -598,10 +598,19 @@ static unsigned long quick_pit_calibrate(void)
 			if (!pit_expect_msb(0xff-i, &delta, &d2))
 				break;
 
+			delta -= tsc;
+
+			/*
+			 * Extrapolate the error and fail fast if the error will
+			 * never be below 500 ppm.
+			 */
+			if (i == 1 &&
+			    d1 + d2 >= (delta * MAX_QUICK_PIT_ITERATIONS) >> 11)
+				return 0;
+
 			/*
 			 * Iterate until the error is less than 500 ppm
 			 */
-			delta -= tsc;
 			if (d1+d2 >= delta >> 11)
 				continue;
 

commit 520452172e6b318f3a8bd9d4fe1e25066393de25
Author: Alexandre Demers <alexandre.f.demers@gmail.com>
Date:   Tue Dec 9 01:27:50 2014 -0500

    x86/tsc: Change Fast TSC calibration failed from error to info
    
    Many users see this message when booting without knowning that it is
    of no importance and that TSC calibration may have succeeded by
    another way.
    
    As explained by Paul Bolle in
    http://lkml.kernel.org/r/1348488259.1436.22.camel@x61.thuisdomein
    
      "Fast TSC calibration failed" should not be considered as an error
      since other calibration methods are being tried afterward. At most,
      those send a warning if they fail (not an error). So let's change
      the message from error to warning.
    
    [ tglx: Make if pr_info. It's really not important at all ]
    
    Fixes: c767a54ba065 x86/debug: Add KERN_<LEVEL> to bare printks, convert printks to pr_<level>
    Signed-off-by: Alexandre Demers <alexandre.f.demers@gmail.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/1418106470-6906-1-git-send-email-alexandre.f.demers@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index b7e50bba3bbb..505449700e0c 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -617,7 +617,7 @@ static unsigned long quick_pit_calibrate(void)
 			goto success;
 		}
 	}
-	pr_err("Fast TSC calibration failed\n");
+	pr_info("Fast TSC calibration failed\n");
 	return 0;
 
 success:

commit b47dcbdc5161d3d5756f430191e2840d9b855492
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Wed Oct 15 10:12:07 2014 -0700

    x86, apic: Handle a bad TSC more gracefully
    
    If the TSC is unusable or disabled, then this patch fixes:
    
     - Confusion while trying to clear old APIC interrupts.
     - Division by zero and incorrect programming of the TSC deadline
       timer.
    
    This fixes boot if the CPU has a TSC deadline timer but a missing or
    broken TSC.  The failure to boot can be observed with qemu using
    -cpu qemu64,-tsc,+tsc-deadline
    
    This also happens to me in nested KVM for unknown reasons.
    With this patch, I can boot cleanly (although without a TSC).
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Bandan Das <bsd@redhat.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/e2fa274e498c33988efac0ba8b7e3120f7f92d78.1413393027.git.luto@amacapital.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index b6025f9e36c6..b7e50bba3bbb 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1166,14 +1166,17 @@ void __init tsc_init(void)
 
 	x86_init.timers.tsc_pre_init();
 
-	if (!cpu_has_tsc)
+	if (!cpu_has_tsc) {
+		setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
 		return;
+	}
 
 	tsc_khz = x86_platform.calibrate_tsc();
 	cpu_khz = tsc_khz;
 
 	if (!tsc_khz) {
 		mark_tsc_unstable("could not calculate TSC khz");
+		setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
 		return;
 	}
 

commit e7fda6c4c3c1a7d6996dd75fd84670fa0b5d448f
Merge: 08d69a257144 953dec21aed4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 5 17:46:42 2014 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer and time updates from Thomas Gleixner:
     "A rather large update of timers, timekeeping & co
    
       - Core timekeeping code is year-2038 safe now for 32bit machines.
         Now we just need to fix all in kernel users and the gazillion of
         user space interfaces which rely on timespec/timeval :)
    
       - Better cache layout for the timekeeping internal data structures.
    
       - Proper nanosecond based interfaces for in kernel users.
    
       - Tree wide cleanup of code which wants nanoseconds but does hoops
         and loops to convert back and forth from timespecs.  Some of it
         definitely belongs into the ugly code museum.
    
       - Consolidation of the timekeeping interface zoo.
    
       - A fast NMI safe accessor to clock monotonic for tracing.  This is a
         long standing request to support correlated user/kernel space
         traces.  With proper NTP frequency correction it's also suitable
         for correlation of traces accross separate machines.
    
       - Checkpoint/restart support for timerfd.
    
       - A few NOHZ[_FULL] improvements in the [hr]timer code.
    
       - Code move from kernel to kernel/time of all time* related code.
    
       - New clocksource/event drivers from the ARM universe.  I'm really
         impressed that despite an architected timer in the newer chips SoC
         manufacturers insist on inventing new and differently broken SoC
         specific timers.
    
    [ Ed. "Impressed"? I don't think that word means what you think it means ]
    
       - Another round of code move from arch to drivers.  Looks like most
         of the legacy mess in ARM regarding timers is sorted out except for
         a few obnoxious strongholds.
    
       - The usual updates and fixlets all over the place"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (114 commits)
      timekeeping: Fixup typo in update_vsyscall_old definition
      clocksource: document some basic timekeeping concepts
      timekeeping: Use cached ntp_tick_length when accumulating error
      timekeeping: Rework frequency adjustments to work better w/ nohz
      timekeeping: Minor fixup for timespec64->timespec assignment
      ftrace: Provide trace clocks monotonic
      timekeeping: Provide fast and NMI safe access to CLOCK_MONOTONIC
      seqcount: Add raw_write_seqcount_latch()
      seqcount: Provide raw_read_seqcount()
      timekeeping: Use tk_read_base as argument for timekeeping_get_ns()
      timekeeping: Create struct tk_read_base and use it in struct timekeeper
      timekeeping: Restructure the timekeeper some more
      clocksource: Get rid of cycle_last
      clocksource: Move cycle_last validation to core code
      clocksource: Make delta calculation a function
      wireless: ath9k: Get rid of timespec conversions
      drm: vmwgfx: Use nsec based interfaces
      drm: i915: Use nsec based interfaces
      timekeeping: Provide ktime_get_raw()
      hangcheck-timer: Use ktime_get_ns()
      ...

commit 19d402c1e75077e2bcfe17f7fe5bcfc8deb74991
Merge: 98959948a7ba a9cfccee6604 b08ee5f7e413 3bab13b015a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 16:56:16 2014 -0700

    Merge branches 'x86-build-for-linus', 'x86-cleanups-for-linus' and 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 build/cleanup/debug updates from Ingo Molnar:
     "Robustify the build process with a quirk to avoid GCC reordering
      related bugs.
    
      Two code cleanups.
    
      Simplify entry_64.S CFI annotations, by Jan Beulich"
    
    * 'x86-build-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, build: Change code16gcc.h from a C header to an assembly header
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Simplify __HAVE_ARCH_CMPXCHG tests
      x86/tsc: Get rid of custom DIV_ROUND() macro
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/debug: Drop several unnecessary CFI annotations

commit 09ec54429c6d10f87d1f084de53ae2c1c3a81108
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 16 21:05:12 2014 +0000

    clocksource: Move cycle_last validation to core code
    
    The only user of the cycle_last validation is the x86 TSC. In order to
    provide NMI safe accessor functions for clock monotonic and
    monotonic_raw we need to do that in the core.
    
    We can't do the TSC specific
    
        if (now < cycle_last)
                now = cycle_last;
    
    for the other wrapping around clocksources, but TSC has
    CLOCKSOURCE_MASK(64) which actually does not mask out anything so if
    now is less than cycle_last the subtraction will give a negative
    result. So we can check for that in clocksource_delta() and return 0
    for that case.
    
    Implement and enable it for x86
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 57e5ce126d5a..456c0e660c43 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -951,7 +951,7 @@ core_initcall(cpufreq_tsc);
 static struct clocksource clocksource_tsc;
 
 /*
- * We compare the TSC to the cycle_last value in the clocksource
+ * We used to compare the TSC to the cycle_last value in the clocksource
  * structure to avoid a nasty time-warp. This can be observed in a
  * very small window right after one CPU updated cycle_last under
  * xtime/vsyscall_gtod lock and the other CPU reads a TSC value which
@@ -961,26 +961,23 @@ static struct clocksource clocksource_tsc;
  * due to the unsigned delta calculation of the time keeping core
  * code, which is necessary to support wrapping clocksources like pm
  * timer.
+ *
+ * This sanity check is now done in the core timekeeping code.
+ * checking the result of read_tsc() - cycle_last for being negative.
+ * That works because CLOCKSOURCE_MASK(64) does not mask out any bit.
  */
 static cycle_t read_tsc(struct clocksource *cs)
 {
-	cycle_t ret = (cycle_t)get_cycles();
-
-	return ret >= clocksource_tsc.cycle_last ?
-		ret : clocksource_tsc.cycle_last;
-}
-
-static void resume_tsc(struct clocksource *cs)
-{
-	if (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC_S3))
-		clocksource_tsc.cycle_last = 0;
+	return (cycle_t)get_cycles();
 }
 
+/*
+ * .mask MUST be CLOCKSOURCE_MASK(64). See comment above read_tsc()
+ */
 static struct clocksource clocksource_tsc = {
 	.name                   = "tsc",
 	.rating                 = 300,
 	.read                   = read_tsc,
-	.resume			= resume_tsc,
 	.mask                   = CLOCKSOURCE_MASK(64),
 	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
 				  CLOCK_SOURCE_MUST_VERIFY,

commit 3896c329df8092661dac80f55a8c3f60136fd61a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 24 14:48:19 2014 +0200

    x86, tsc: Fix cpufreq lockup
    
    Mauro reported that his AMD X2 using the powernow-k8 cpufreq driver
    locked up when doing cpu hotplug.
    
    Because we called set_cyc2ns_scale() from the time_cpufreq_notifier()
    unconditionally, it gets called multiple times for each freq change,
    instead of only the once, when the tsc_khz value actually changes.
    
    Because it gets called more than once, we run out of cyc2ns data slots
    and stall, waiting for a free one, but because we're half way offline,
    there's no consumers to free slots.
    
    By placing the call inside the condition that actually changes tsc_khz
    we avoid superfluous calls and avoid the problem.
    
    Reported-by: Mauro <registosites@hotmail.com>
    Tested-by: Mauro <registosites@hotmail.com>
    Fixes: 20d1c86a5776 ("sched/clock, x86: Rewrite cyc2ns() to avoid the need to disable IRQs")
    Cc: <stable@vger.kernel.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Bin Gao <bin.gao@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mika Westerberg <mika.westerberg@linux.intel.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Stefani Seibold <stefani@seibold.net>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 57e5ce126d5a..ea030319b321 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -920,9 +920,9 @@ static int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
 		tsc_khz = cpufreq_scale(tsc_khz_ref, ref_freq, freq->new);
 		if (!(freq->flags & CPUFREQ_CONST_LOOPS))
 			mark_tsc_unstable("cpufreq changes");
-	}
 
-	set_cyc2ns_scale(tsc_khz, freq->cpu);
+		set_cyc2ns_scale(tsc_khz, freq->cpu);
+	}
 
 	return 0;
 }

commit 891715793f0451e5114d200be932ac14ce8521a3
Author: Michal Nazarewicz <mina86@mina86.com>
Date:   Thu Jun 19 03:58:36 2014 +0200

    x86/tsc: Get rid of custom DIV_ROUND() macro
    
    When invoced for positive values, DIV_ROUND macro defined in
    arch/x86/kernel/tsc.c behaves exactly like DIV_ROUND_CLOSEST from
    include/linux/kernel.h file, so remove the custom macro in favour
    of the shared one.
    
    [ hpa: changed line breaks ]
    
    Signed-off-by: Michal Nazarewicz <mina86@mina86.com>
    Link: http://lkml.kernel.org/r/1403143116-21755-1-git-send-email-mina86@mina86.com
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 57e5ce126d5a..8764232bf0f1 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -234,9 +234,6 @@ static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 	return ns;
 }
 
-/* XXX surely we already have this someplace in the kernel?! */
-#define DIV_ROUND(n, d) (((n) + ((d) / 2)) / (d))
-
 static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 {
 	unsigned long long tsc_now, ns_now;
@@ -259,7 +256,9 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 	 * time function is continuous; see the comment near struct
 	 * cyc2ns_data.
 	 */
-	data->cyc2ns_mul = DIV_ROUND(NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR, cpu_khz);
+	data->cyc2ns_mul =
+		DIV_ROUND_CLOSEST(NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR,
+				  cpu_khz);
 	data->cyc2ns_shift = CYC2NS_SCALE_FACTOR;
 	data->cyc2ns_offset = ns_now -
 		mul_u64_u32_shr(tsc_now, data->cyc2ns_mul, CYC2NS_SCALE_FACTOR);

commit c6f21243ce1e8d81ad8361da4d2eaa5947b667c4
Merge: 9447dc43941c 37c975545ec6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 2 12:26:43 2014 -0700

    Merge branch 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 vdso changes from Peter Anvin:
     "This is the revamp of the 32-bit vdso and the associated cleanups.
    
      This adds timekeeping support to the 32-bit vdso that we already have
      in the 64-bit vdso.  Although 32-bit x86 is legacy, it is likely to
      remain in the embedded space for a very long time to come.
    
      This removes the traditional COMPAT_VDSO support; the configuration
      variable is reused for simply removing the 32-bit vdso, which will
      produce correct results but obviously suffer a performance penalty.
      Only one beta version of glibc was affected, but that version was
      unfortunately included in one OpenSUSE release.
    
      This is not the end of the vdso cleanups.  Stefani and Andy have
      agreed to continue work for the next kernel cycle; in fact Andy has
      already produced another set of cleanups that came too late for this
      cycle.
    
      An incidental, but arguably important, change is that this ensures
      that unused space in the VVAR page is properly zeroed.  It wasn't
      before, and would contain whatever garbage was left in memory by BIOS
      or the bootloader.  Since the VVAR page is accessible to user space
      this had the potential of information leaks"
    
    * 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      x86, vdso: Fix the symbol versions on the 32-bit vDSO
      x86, vdso, build: Don't rebuild 32-bit vdsos on every make
      x86, vdso: Actually discard the .discard sections
      x86, vdso: Fix size of get_unmapped_area()
      x86, vdso: Finish removing VDSO32_PRELINK
      x86, vdso: Move more vdso definitions into vdso.h
      x86: Load the 32-bit vdso in place, just like the 64-bit vdsos
      x86, vdso32: handle 32 bit vDSO larger one page
      x86, vdso32: Disable stack protector, adjust optimizations
      x86, vdso: Zero-pad the VVAR page
      x86, vdso: Add 32 bit VDSO time support for 64 bit kernel
      x86, vdso: Add 32 bit VDSO time support for 32 bit kernel
      x86, vdso: Patch alternatives in the 32-bit VDSO
      x86, vdso: Introduce VVAR marco for vdso32
      x86, vdso: Cleanup __vdso_gettimeofday()
      x86, vdso: Replace VVAR(vsyscall_gtod_data) by gtod macro
      x86, vdso: __vdso_clock_gettime() cleanup
      x86, vdso: Revamp vclock_gettime.c
      mm: Add new func _install_special_mapping() to mmap.c
      x86, vdso: Make vsyscall_gtod_data handling x86 generic
      ...

commit 0b443ead714f0cba797a7f2476dd756f22b5421e
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Mar 19 11:24:58 2014 +0530

    cpufreq: remove unused notifier: CPUFREQ_{SUSPENDCHANGE|RESUMECHANGE}
    
    Two cpufreq notifiers CPUFREQ_RESUMECHANGE and CPUFREQ_SUSPENDCHANGE have
    not been used for some time, so remove them to clean up code a bit.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    [rjw: Changelog]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index cfbe99f88830..7a9296ab8834 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -914,8 +914,7 @@ static int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
 		tsc_khz_ref = tsc_khz;
 	}
 	if ((val == CPUFREQ_PRECHANGE  && freq->old < freq->new) ||
-			(val == CPUFREQ_POSTCHANGE && freq->old > freq->new) ||
-			(val == CPUFREQ_RESUMECHANGE)) {
+			(val == CPUFREQ_POSTCHANGE && freq->old > freq->new)) {
 		*lpj = cpufreq_scale(loops_per_jiffy_ref, ref_freq, freq->new);
 
 		tsc_khz = cpufreq_scale(tsc_khz_ref, ref_freq, freq->new);

commit d2312e3379d581d2c3603357a0181046448e1de3
Author: Stefani Seibold <stefani@seibold.net>
Date:   Mon Mar 17 23:22:01 2014 +0100

    x86, vdso: Make vsyscall_gtod_data handling x86 generic
    
    This patch move the vsyscall_gtod_data handling out of vsyscall_64.c
    into an additonal file vsyscall_gtod.c to make the functionality
    available for x86 32 bit kernel.
    
    It also adds a new vsyscall_32.c which setup the VVAR page.
    
    Reviewed-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Stefani Seibold <stefani@seibold.net>
    Link: http://lkml.kernel.org/r/1395094933-14252-2-git-send-email-stefani@seibold.net
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index cfbe99f88830..227dcfc0e5e7 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -985,9 +985,7 @@ static struct clocksource clocksource_tsc = {
 	.mask                   = CLOCKSOURCE_MASK(64),
 	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
 				  CLOCK_SOURCE_MUST_VERIFY,
-#ifdef CONFIG_X86_64
 	.archdata               = { .vclock_mode = VCLOCK_TSC },
-#endif
 };
 
 void mark_tsc_unstable(char *reason)

commit 5f0e030930d715920be4de638084aaf8653867e8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 19 13:52:29 2014 +0200

    x86, tsc: Fallback to normal calibration if fast MSR calibration fails
    
    If we cannot calibrate TSC via MSR based calibration
    try_msr_calibrate_tsc() stores zero to fast_calibrate and returns that
    to the caller. This value gets then propagated further to clockevents
    code resulting division by zero oops like the one below:
    
     divide error: 0000 [#1] PREEMPT SMP
     Modules linked in:
     CPU: 0 PID: 1 Comm: swapper/0 Tainted: G        W    3.13.0+ #47
     task: ffff880075508000 ti: ffff880075506000 task.ti: ffff880075506000
     RIP: 0010:[<ffffffff810aec14>]  [<ffffffff810aec14>] clockevents_config.part.3+0x24/0xa0
     RSP: 0000:ffff880075507e58  EFLAGS: 00010246
     RAX: ffffffffffffffff RBX: ffff880079c0cd80 RCX: 0000000000000000
     RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffffffffffffffff
     RBP: ffff880075507e70 R08: 0000000000000001 R09: 00000000000000be
     R10: 00000000000000bd R11: 0000000000000003 R12: 000000000000b008
     R13: 0000000000000008 R14: 000000000000b010 R15: 0000000000000000
     FS:  0000000000000000(0000) GS:ffff880079c00000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
     CR2: ffff880079fff000 CR3: 0000000001c0b000 CR4: 00000000001006f0
     Stack:
      ffff880079c0cd80 000000000000b008 0000000000000008 ffff880075507e88
      ffffffff810aecb0 ffff880079c0cd80 ffff880075507e98 ffffffff81030168
      ffff880075507ed8 ffffffff81d1104f 00000000000000c3 0000000000000000
     Call Trace:
      [<ffffffff810aecb0>] clockevents_config_and_register+0x20/0x30
      [<ffffffff81030168>] setup_APIC_timer+0xc8/0xd0
      [<ffffffff81d1104f>] setup_boot_APIC_clock+0x4cc/0x4d8
      [<ffffffff81d0f5de>] native_smp_prepare_cpus+0x3dd/0x3f0
      [<ffffffff81d02ee9>] kernel_init_freeable+0xc3/0x205
      [<ffffffff8177c910>] ? rest_init+0x90/0x90
      [<ffffffff8177c91e>] kernel_init+0xe/0x120
      [<ffffffff8178deec>] ret_from_fork+0x7c/0xb0
      [<ffffffff8177c910>] ? rest_init+0x90/0x90
    
    Prevent this from happening by:
     1) Modifying try_msr_calibrate_tsc() to return calibration value or zero
        if it fails.
     2) Check this return value in native_calibrate_tsc() and in case of zero
        fallback to use normal non-MSR based calibration.
    
    [mw: Added subject and changelog]
    
    Reported-and-tested-by: Mika Westerberg <mika.westerberg@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bin Gao <bin.gao@linux.intel.com>
    Cc: One Thousand Gnomes <gnomes@lxorguk.ukuu.org.uk>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1392810750-18660-1-git-send-email-mika.westerberg@linux.intel.com
    Signed-off-by: Mika Westerberg <mika.westerberg@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index acb3b606613e..cfbe99f88830 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -653,13 +653,10 @@ unsigned long native_calibrate_tsc(void)
 
 	/* Calibrate TSC using MSR for Intel Atom SoCs */
 	local_irq_save(flags);
-	i = try_msr_calibrate_tsc(&fast_calibrate);
+	fast_calibrate = try_msr_calibrate_tsc();
 	local_irq_restore(flags);
-	if (i >= 0) {
-		if (i == 0)
-			pr_warn("Fast TSC calibration using MSR failed\n");
+	if (fast_calibrate)
 		return fast_calibrate;
-	}
 
 	local_irq_save(flags);
 	fast_calibrate = quick_pit_calibrate();

commit 569d6557ab957d6ae7e97a46ae669174be4189e6
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Feb 4 14:13:15 2014 -0500

    x86: Use preempt_disable_notrace() in cycles_2_ns()
    
    When debug preempt is enabled, preempt_disable() can be traced by
    function and function graph tracing.
    
    There's a place in the function graph tracer that calls trace_clock()
    which eventually calls cycles_2_ns() outside of the recursion
    protection. When cycles_2_ns() calls preempt_disable() it gets traced
    and the graph tracer will go into a recursive loop causing a crash or
    worse, a triple fault.
    
    Simple fix is to use preempt_disable_notrace() in cycles_2_ns, which
    makes sense because the preempt_disable() tracing may use that code
    too, and it tracing it, even with recursion protection is rather
    pointless.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20140204141315.2a968a72@gandalf.local.home
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 19e5adb49a27..acb3b606613e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -209,7 +209,7 @@ static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 	 * dance when its actually needed.
 	 */
 
-	preempt_disable();
+	preempt_disable_notrace();
 	data = this_cpu_read(cyc2ns.head);
 	tail = this_cpu_read(cyc2ns.tail);
 
@@ -229,7 +229,7 @@ static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 		if (!--data->__count)
 			this_cpu_write(cyc2ns.tail, data);
 	}
-	preempt_enable();
+	preempt_enable_notrace();
 
 	return ns;
 }

commit 5e3c1afd4587e70c201bf7224b51f747c9a3dfa8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jan 22 22:08:14 2014 +0100

    sched/x86/tsc: Initialize multiplier to 0
    
    Since we keep the clock value linearly continuous on frequency change,
    make sure the initial multiplier is 0, such that our initial value is 0.
    Without this we compute the initial value at whatever the TSC has
    managed to reach since power-on.
    
    Reported-and-Tested-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Fixes: 20d1c86a57762 ("sched/clock, x86: Rewrite cyc2ns() to avoid the need to disable IRQs")
    Cc: lenb@kernel.org
    Cc: rjw@rjwysocki.net
    Cc: Eliezer Tamir <eliezer.tamir@linux.intel.com>
    Cc: rui.zhang@intel.com
    Cc: jacob.jun.pan@linux.intel.com
    Cc: Mike Galbraith <bitbucket@online.de>
    Cc: hpa@zytor.com
    Cc: paulmck@linux.vnet.ibm.com
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: dyoung@redhat.com
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140123094804.GP30183@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a3acbac2ee72..19e5adb49a27 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -180,7 +180,7 @@ static void cyc2ns_write_end(int cpu, struct cyc2ns_data *data)
 
 static void cyc2ns_data_init(struct cyc2ns_data *data)
 {
-	data->cyc2ns_mul = 1U << CYC2NS_SCALE_FACTOR;
+	data->cyc2ns_mul = 0;
 	data->cyc2ns_shift = CYC2NS_SCALE_FACTOR;
 	data->cyc2ns_offset = 0;
 	data->__count = 0;

commit 74e8ee8262c3f93bbc41804037b43f07b95897bb
Merge: 8bd6964cbd17 ca1e631c3acf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 12:09:31 2014 -0800

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull Intel SoC changes from Ingo Molnar:
     "Improved Intel SoC platform support"
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, tsc, apic: Unbreak static (MSR) calibration when CONFIG_X86_LOCAL_APIC=n
      x86, tsc: Add static (MSR) TSC calibration on Intel Atom SoCs
      arch: x86: New MailBox support driver for Intel SOC's

commit 7da7c1561366ba8adb7275464ab44e84e1faa7e0
Author: Bin Gao <bin.gao@intel.com>
Date:   Mon Oct 21 09:16:33 2013 -0700

    x86, tsc: Add static (MSR) TSC calibration on Intel Atom SoCs
    
    On SoCs that have the calibration MSRs available, either there is no
    PIT, HPET or PMTIMER to calibrate against, or the PIT/HPET/PMTIMER is
    driven from the same clock as the TSC, so calibration is redundant and
    just slows down the boot.
    
    TSC rate is caculated by this formula:
    <maximum core-clock to bus-clock ratio> * <maximum resolved frequency>
    The ratio and the resolved frequency ID can be obtained from MSR.
    See Intel 64 and IA-32 System Programming Guid section 16.12 and 30.11.5
    for details.
    
    Signed-off-by: Bin Gao <bin.gao@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/n/tip-rgm7xmg7k6qnjlw3ynkcjsmh@git.kernel.org

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 930e5d48f560..e5747167da83 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -419,6 +419,16 @@ unsigned long native_calibrate_tsc(void)
 	unsigned long flags, latch, ms, fast_calibrate;
 	int hpet = is_hpet_enabled(), i, loopmin;
 
+	/* Calibrate TSC using MSR for Intel Atom SoCs */
+	local_irq_save(flags);
+	i = try_msr_calibrate_tsc(&fast_calibrate);
+	local_irq_restore(flags);
+	if (i >= 0) {
+		if (i == 0)
+			pr_warn("Fast TSC calibration using MSR failed\n");
+		return fast_calibrate;
+	}
+
 	local_irq_save(flags);
 	fast_calibrate = quick_pit_calibrate();
 	local_irq_restore(flags);

commit 10b033d434c25a6c9e0f4f4dc2418af1b8236c63
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 28 19:01:40 2013 +0100

    sched/clock, x86: Avoid a runtime condition in native_sched_clock()
    
    Use a static_key to avoid touching tsc_disabled and a runtime
    condition in native_sched_clock() -- less cachelines touched is always
    better.
    
                            MAINLINE   PRE       POST
    
        sched_clock_stable: 1          1         1
        (cold) sched_clock: 329841     215295    213039
        (cold) local_clock: 301773     220773    216084
        (warm) sched_clock: 38375      25659     25231
        (warm) local_clock: 100371     27242     27601
        (warm) rdtsc:       27340      24208     24203
        sched_clock_stable: 0          0         0
        (cold) sched_clock: 382634     237019    240055
        (cold) local_clock: 396890     294819    299942
        (warm) sched_clock: 38194      25609     25276
        (warm) local_clock: 143452     71232     73232
        (warm) rdtsc:       27345      24243     24244
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-hrz87bo37qke25bty6pnfy4b@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 53c123537245..6377fb28b958 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -11,6 +11,7 @@
 #include <linux/clocksource.h>
 #include <linux/percpu.h>
 #include <linux/timex.h>
+#include <linux/static_key.h>
 
 #include <asm/hpet.h>
 #include <asm/timer.h>
@@ -37,6 +38,8 @@ static int __read_mostly tsc_unstable;
    erroneous rdtsc usage on !cpu_has_tsc processors */
 static int __read_mostly tsc_disabled = -1;
 
+static struct static_key __use_tsc = STATIC_KEY_INIT;
+
 int tsc_clocksource_reliable;
 
 /*
@@ -282,7 +285,7 @@ u64 native_sched_clock(void)
 	 *   very important for it to be as fast as the platform
 	 *   can achieve it. )
 	 */
-	if (unlikely(tsc_disabled)) {
+	if (!static_key_false(&__use_tsc)) {
 		/* No locking but a rare wrong value is not a big deal: */
 		return (jiffies_64 - INITIAL_JIFFIES) * (1000000000 / HZ);
 	}
@@ -1193,7 +1196,9 @@ void __init tsc_init(void)
 		return;
 
 	/* now allow native_sched_clock() to use rdtsc */
+
 	tsc_disabled = 0;
+	static_key_slow_inc(&__use_tsc);
 
 	if (!no_sched_irq_time)
 		enable_sched_clock_irqtime();

commit 35af99e646c7f7ea46dc2977601e9e71a51dadd5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 28 19:38:42 2013 +0100

    sched/clock, x86: Use a static_key for sched_clock_stable
    
    In order to avoid the runtime condition and variable load turn
    sched_clock_stable into a static_key.
    
    Also provide a shorter implementation of local_clock() and
    cpu_clock(int) when sched_clock_stable==1.
    
                            MAINLINE   PRE       POST
    
        sched_clock_stable: 1          1         1
        (cold) sched_clock: 329841     221876    215295
        (cold) local_clock: 301773     234692    220773
        (warm) sched_clock: 38375      25602     25659
        (warm) local_clock: 100371     33265     27242
        (warm) rdtsc:       27340      24214     24208
        sched_clock_stable: 0          0         0
        (cold) sched_clock: 382634     235941    237019
        (cold) local_clock: 396890     297017    294819
        (warm) sched_clock: 38194      25233     25609
        (warm) local_clock: 143452     71234     71232
        (warm) rdtsc:       27345      24245     24243
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-eummbdechzz37mwmpags1gjr@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 92b090b2b79e..53c123537245 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -822,7 +822,7 @@ static unsigned long long cyc2ns_suspend;
 
 void tsc_save_sched_clock_state(void)
 {
-	if (!sched_clock_stable)
+	if (!sched_clock_stable())
 		return;
 
 	cyc2ns_suspend = sched_clock();
@@ -842,7 +842,7 @@ void tsc_restore_sched_clock_state(void)
 	unsigned long flags;
 	int cpu;
 
-	if (!sched_clock_stable)
+	if (!sched_clock_stable())
 		return;
 
 	local_irq_save(flags);
@@ -984,7 +984,7 @@ void mark_tsc_unstable(char *reason)
 {
 	if (!tsc_unstable) {
 		tsc_unstable = 1;
-		sched_clock_stable = 0;
+		clear_sched_clock_stable();
 		disable_sched_clock_irqtime();
 		pr_info("Marking TSC unstable due to %s\n", reason);
 		/* Change only the rating, when not registered */

commit 20d1c86a57762f0a33a78988e3fc8818316badd4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 29 15:40:29 2013 +0100

    sched/clock, x86: Rewrite cyc2ns() to avoid the need to disable IRQs
    
    Use a ring-buffer like multi-version object structure which allows
    always having a coherent object; we use this to avoid having to
    disable IRQs while reading sched_clock() and avoids a problem when
    getting an NMI while changing the cyc2ns data.
    
                            MAINLINE   PRE        POST
    
        sched_clock_stable: 1          1          1
        (cold) sched_clock: 329841     331312     257223
        (cold) local_clock: 301773     310296     309889
        (warm) sched_clock: 38375      38247      25280
        (warm) local_clock: 100371     102713     85268
        (warm) rdtsc:       27340      27289      24247
        sched_clock_stable: 0          0          0
        (cold) sched_clock: 382634     372706     301224
        (cold) local_clock: 396890     399275     399870
        (warm) sched_clock: 38194      38124      25630
        (warm) local_clock: 143452     148698     129629
        (warm) rdtsc:       27345      27365      24307
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-s567in1e5ekq2nlyhn8f987r@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index b4a04ac1d7aa..92b090b2b79e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -39,7 +39,119 @@ static int __read_mostly tsc_disabled = -1;
 
 int tsc_clocksource_reliable;
 
-/* Accelerators for sched_clock()
+/*
+ * Use a ring-buffer like data structure, where a writer advances the head by
+ * writing a new data entry and a reader advances the tail when it observes a
+ * new entry.
+ *
+ * Writers are made to wait on readers until there's space to write a new
+ * entry.
+ *
+ * This means that we can always use an {offset, mul} pair to compute a ns
+ * value that is 'roughly' in the right direction, even if we're writing a new
+ * {offset, mul} pair during the clock read.
+ *
+ * The down-side is that we can no longer guarantee strict monotonicity anymore
+ * (assuming the TSC was that to begin with), because while we compute the
+ * intersection point of the two clock slopes and make sure the time is
+ * continuous at the point of switching; we can no longer guarantee a reader is
+ * strictly before or after the switch point.
+ *
+ * It does mean a reader no longer needs to disable IRQs in order to avoid
+ * CPU-Freq updates messing with his times, and similarly an NMI reader will
+ * no longer run the risk of hitting half-written state.
+ */
+
+struct cyc2ns {
+	struct cyc2ns_data data[2];	/*  0 + 2*24 = 48 */
+	struct cyc2ns_data *head;	/* 48 + 8    = 56 */
+	struct cyc2ns_data *tail;	/* 56 + 8    = 64 */
+}; /* exactly fits one cacheline */
+
+static DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
+
+struct cyc2ns_data *cyc2ns_read_begin(void)
+{
+	struct cyc2ns_data *head;
+
+	preempt_disable();
+
+	head = this_cpu_read(cyc2ns.head);
+	/*
+	 * Ensure we observe the entry when we observe the pointer to it.
+	 * matches the wmb from cyc2ns_write_end().
+	 */
+	smp_read_barrier_depends();
+	head->__count++;
+	barrier();
+
+	return head;
+}
+
+void cyc2ns_read_end(struct cyc2ns_data *head)
+{
+	barrier();
+	/*
+	 * If we're the outer most nested read; update the tail pointer
+	 * when we're done. This notifies possible pending writers
+	 * that we've observed the head pointer and that the other
+	 * entry is now free.
+	 */
+	if (!--head->__count) {
+		/*
+		 * x86-TSO does not reorder writes with older reads;
+		 * therefore once this write becomes visible to another
+		 * cpu, we must be finished reading the cyc2ns_data.
+		 *
+		 * matches with cyc2ns_write_begin().
+		 */
+		this_cpu_write(cyc2ns.tail, head);
+	}
+	preempt_enable();
+}
+
+/*
+ * Begin writing a new @data entry for @cpu.
+ *
+ * Assumes some sort of write side lock; currently 'provided' by the assumption
+ * that cpufreq will call its notifiers sequentially.
+ */
+static struct cyc2ns_data *cyc2ns_write_begin(int cpu)
+{
+	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
+	struct cyc2ns_data *data = c2n->data;
+
+	if (data == c2n->head)
+		data++;
+
+	/* XXX send an IPI to @cpu in order to guarantee a read? */
+
+	/*
+	 * When we observe the tail write from cyc2ns_read_end(),
+	 * the cpu must be done with that entry and its safe
+	 * to start writing to it.
+	 */
+	while (c2n->tail == data)
+		cpu_relax();
+
+	return data;
+}
+
+static void cyc2ns_write_end(int cpu, struct cyc2ns_data *data)
+{
+	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
+
+	/*
+	 * Ensure the @data writes are visible before we publish the
+	 * entry. Matches the data-depencency in cyc2ns_read_begin().
+	 */
+	smp_wmb();
+
+	ACCESS_ONCE(c2n->head) = data;
+}
+
+/*
+ * Accelerators for sched_clock()
  * convert from cycles(64bits) => nanoseconds (64bits)
  *  basic equation:
  *              ns = cycles / (freq / ns_per_sec)
@@ -61,49 +173,106 @@ int tsc_clocksource_reliable;
  *                      -johnstul@us.ibm.com "math is hard, lets go shopping!"
  */
 
-DEFINE_PER_CPU(unsigned long, cyc2ns);
-DEFINE_PER_CPU(unsigned long long, cyc2ns_offset);
-
 #define CYC2NS_SCALE_FACTOR 10 /* 2^10, carefully chosen */
 
+static void cyc2ns_data_init(struct cyc2ns_data *data)
+{
+	data->cyc2ns_mul = 1U << CYC2NS_SCALE_FACTOR;
+	data->cyc2ns_shift = CYC2NS_SCALE_FACTOR;
+	data->cyc2ns_offset = 0;
+	data->__count = 0;
+}
+
+static void cyc2ns_init(int cpu)
+{
+	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
+
+	cyc2ns_data_init(&c2n->data[0]);
+	cyc2ns_data_init(&c2n->data[1]);
+
+	c2n->head = c2n->data;
+	c2n->tail = c2n->data;
+}
+
 static inline unsigned long long cycles_2_ns(unsigned long long cyc)
 {
-	unsigned long long ns = this_cpu_read(cyc2ns_offset);
-	ns += mul_u64_u32_shr(cyc, this_cpu_read(cyc2ns), CYC2NS_SCALE_FACTOR);
+	struct cyc2ns_data *data, *tail;
+	unsigned long long ns;
+
+	/*
+	 * See cyc2ns_read_*() for details; replicated in order to avoid
+	 * an extra few instructions that came with the abstraction.
+	 * Notable, it allows us to only do the __count and tail update
+	 * dance when its actually needed.
+	 */
+
+	preempt_disable();
+	data = this_cpu_read(cyc2ns.head);
+	tail = this_cpu_read(cyc2ns.tail);
+
+	if (likely(data == tail)) {
+		ns = data->cyc2ns_offset;
+		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, CYC2NS_SCALE_FACTOR);
+	} else {
+		data->__count++;
+
+		barrier();
+
+		ns = data->cyc2ns_offset;
+		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, CYC2NS_SCALE_FACTOR);
+
+		barrier();
+
+		if (!--data->__count)
+			this_cpu_write(cyc2ns.tail, data);
+	}
+	preempt_enable();
+
 	return ns;
 }
 
+/* XXX surely we already have this someplace in the kernel?! */
+#define DIV_ROUND(n, d) (((n) + ((d) / 2)) / (d))
+
 static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 {
-	unsigned long long tsc_now, ns_now, *offset;
-	unsigned long flags, *scale;
+	unsigned long long tsc_now, ns_now;
+	struct cyc2ns_data *data;
+	unsigned long flags;
 
 	local_irq_save(flags);
 	sched_clock_idle_sleep_event();
 
-	scale = &per_cpu(cyc2ns, cpu);
-	offset = &per_cpu(cyc2ns_offset, cpu);
+	if (!cpu_khz)
+		goto done;
+
+	data = cyc2ns_write_begin(cpu);
 
 	rdtscll(tsc_now);
 	ns_now = cycles_2_ns(tsc_now);
 
-	if (cpu_khz) {
-		*scale = ((NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR) +
-				cpu_khz / 2) / cpu_khz;
-		*offset = ns_now - mult_frac(tsc_now, *scale,
-					     (1UL << CYC2NS_SCALE_FACTOR));
-	}
+	/*
+	 * Compute a new multiplier as per the above comment and ensure our
+	 * time function is continuous; see the comment near struct
+	 * cyc2ns_data.
+	 */
+	data->cyc2ns_mul = DIV_ROUND(NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR, cpu_khz);
+	data->cyc2ns_shift = CYC2NS_SCALE_FACTOR;
+	data->cyc2ns_offset = ns_now -
+		mul_u64_u32_shr(tsc_now, data->cyc2ns_mul, CYC2NS_SCALE_FACTOR);
+
+	cyc2ns_write_end(cpu, data);
 
+done:
 	sched_clock_idle_wakeup_event(0);
 	local_irq_restore(flags);
 }
-
 /*
  * Scheduler clock - returns current time in nanosec units.
  */
 u64 native_sched_clock(void)
 {
-	u64 this_offset;
+	u64 tsc_now;
 
 	/*
 	 * Fall back to jiffies if there's no TSC available:
@@ -119,10 +288,10 @@ u64 native_sched_clock(void)
 	}
 
 	/* read the Time Stamp Counter: */
-	rdtscll(this_offset);
+	rdtscll(tsc_now);
 
 	/* return the value in ns */
-	return cycles_2_ns(this_offset);
+	return cycles_2_ns(tsc_now);
 }
 
 /* We need to define a real function for sched_clock, to override the
@@ -678,11 +847,21 @@ void tsc_restore_sched_clock_state(void)
 
 	local_irq_save(flags);
 
-	__this_cpu_write(cyc2ns_offset, 0);
+	/*
+	 * We're comming out of suspend, there's no concurrency yet; don't
+	 * bother being nice about the RCU stuff, just write to both
+	 * data fields.
+	 */
+
+	this_cpu_write(cyc2ns.data[0].cyc2ns_offset, 0);
+	this_cpu_write(cyc2ns.data[1].cyc2ns_offset, 0);
+
 	offset = cyc2ns_suspend - sched_clock();
 
-	for_each_possible_cpu(cpu)
-		per_cpu(cyc2ns_offset, cpu) = offset;
+	for_each_possible_cpu(cpu) {
+		per_cpu(cyc2ns.data[0].cyc2ns_offset, cpu) = offset;
+		per_cpu(cyc2ns.data[1].cyc2ns_offset, cpu) = offset;
+	}
 
 	local_irq_restore(flags);
 }
@@ -1005,8 +1184,10 @@ void __init tsc_init(void)
 	 * speed as the bootup CPU. (cpufreq notifiers will fix this
 	 * up if their speed diverges)
 	 */
-	for_each_possible_cpu(cpu)
+	for_each_possible_cpu(cpu) {
+		cyc2ns_init(cpu);
 		set_cyc2ns_scale(cpu_khz, cpu);
+	}
 
 	if (tsc_disabled > 0)
 		return;

commit 57c67da274f3fab38e08d2c9edf08b89e1d9c71d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 29 15:39:25 2013 +0100

    sched/clock, x86: Move some cyc2ns() code around
    
    There are no __cycles_2_ns() users outside of arch/x86/kernel/tsc.c,
    so move it there.
    
    There are no cycles_2_ns() users.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-01lslnavfgo3kmbo4532zlcj@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 930e5d48f560..b4a04ac1d7aa 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -38,6 +38,66 @@ static int __read_mostly tsc_unstable;
 static int __read_mostly tsc_disabled = -1;
 
 int tsc_clocksource_reliable;
+
+/* Accelerators for sched_clock()
+ * convert from cycles(64bits) => nanoseconds (64bits)
+ *  basic equation:
+ *              ns = cycles / (freq / ns_per_sec)
+ *              ns = cycles * (ns_per_sec / freq)
+ *              ns = cycles * (10^9 / (cpu_khz * 10^3))
+ *              ns = cycles * (10^6 / cpu_khz)
+ *
+ *      Then we use scaling math (suggested by george@mvista.com) to get:
+ *              ns = cycles * (10^6 * SC / cpu_khz) / SC
+ *              ns = cycles * cyc2ns_scale / SC
+ *
+ *      And since SC is a constant power of two, we can convert the div
+ *  into a shift.
+ *
+ *  We can use khz divisor instead of mhz to keep a better precision, since
+ *  cyc2ns_scale is limited to 10^6 * 2^10, which fits in 32 bits.
+ *  (mathieu.desnoyers@polymtl.ca)
+ *
+ *                      -johnstul@us.ibm.com "math is hard, lets go shopping!"
+ */
+
+DEFINE_PER_CPU(unsigned long, cyc2ns);
+DEFINE_PER_CPU(unsigned long long, cyc2ns_offset);
+
+#define CYC2NS_SCALE_FACTOR 10 /* 2^10, carefully chosen */
+
+static inline unsigned long long cycles_2_ns(unsigned long long cyc)
+{
+	unsigned long long ns = this_cpu_read(cyc2ns_offset);
+	ns += mul_u64_u32_shr(cyc, this_cpu_read(cyc2ns), CYC2NS_SCALE_FACTOR);
+	return ns;
+}
+
+static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
+{
+	unsigned long long tsc_now, ns_now, *offset;
+	unsigned long flags, *scale;
+
+	local_irq_save(flags);
+	sched_clock_idle_sleep_event();
+
+	scale = &per_cpu(cyc2ns, cpu);
+	offset = &per_cpu(cyc2ns_offset, cpu);
+
+	rdtscll(tsc_now);
+	ns_now = cycles_2_ns(tsc_now);
+
+	if (cpu_khz) {
+		*scale = ((NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR) +
+				cpu_khz / 2) / cpu_khz;
+		*offset = ns_now - mult_frac(tsc_now, *scale,
+					     (1UL << CYC2NS_SCALE_FACTOR));
+	}
+
+	sched_clock_idle_wakeup_event(0);
+	local_irq_restore(flags);
+}
+
 /*
  * Scheduler clock - returns current time in nanosec units.
  */
@@ -62,7 +122,7 @@ u64 native_sched_clock(void)
 	rdtscll(this_offset);
 
 	/* return the value in ns */
-	return __cycles_2_ns(this_offset);
+	return cycles_2_ns(this_offset);
 }
 
 /* We need to define a real function for sched_clock, to override the
@@ -589,56 +649,6 @@ int recalibrate_cpu_khz(void)
 EXPORT_SYMBOL(recalibrate_cpu_khz);
 
 
-/* Accelerators for sched_clock()
- * convert from cycles(64bits) => nanoseconds (64bits)
- *  basic equation:
- *              ns = cycles / (freq / ns_per_sec)
- *              ns = cycles * (ns_per_sec / freq)
- *              ns = cycles * (10^9 / (cpu_khz * 10^3))
- *              ns = cycles * (10^6 / cpu_khz)
- *
- *      Then we use scaling math (suggested by george@mvista.com) to get:
- *              ns = cycles * (10^6 * SC / cpu_khz) / SC
- *              ns = cycles * cyc2ns_scale / SC
- *
- *      And since SC is a constant power of two, we can convert the div
- *  into a shift.
- *
- *  We can use khz divisor instead of mhz to keep a better precision, since
- *  cyc2ns_scale is limited to 10^6 * 2^10, which fits in 32 bits.
- *  (mathieu.desnoyers@polymtl.ca)
- *
- *                      -johnstul@us.ibm.com "math is hard, lets go shopping!"
- */
-
-DEFINE_PER_CPU(unsigned long, cyc2ns);
-DEFINE_PER_CPU(unsigned long long, cyc2ns_offset);
-
-static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
-{
-	unsigned long long tsc_now, ns_now, *offset;
-	unsigned long flags, *scale;
-
-	local_irq_save(flags);
-	sched_clock_idle_sleep_event();
-
-	scale = &per_cpu(cyc2ns, cpu);
-	offset = &per_cpu(cyc2ns_offset, cpu);
-
-	rdtscll(tsc_now);
-	ns_now = __cycles_2_ns(tsc_now);
-
-	if (cpu_khz) {
-		*scale = ((NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR) +
-				cpu_khz / 2) / cpu_khz;
-		*offset = ns_now - mult_frac(tsc_now, *scale,
-					     (1UL << CYC2NS_SCALE_FACTOR));
-	}
-
-	sched_clock_idle_wakeup_event(0);
-	local_irq_restore(flags);
-}
-
 static unsigned long long cyc2ns_suspend;
 
 void tsc_save_sched_clock_state(void)

commit c73deb6aecda2955716f31572516f09d930ef450
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Fri Jun 28 16:22:18 2013 +0300

    perf/x86: Add ability to calculate TSC from perf sample timestamps
    
    For modern CPUs, perf clock is directly related to TSC.  TSC
    can be calculated from perf clock and vice versa using a simple
    calculation.  Two of the three componenets of that calculation
    are already exported in struct perf_event_mmap_page.  This patch
    exports the third.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1372425741-1676-3-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 6ff49247edf8..930e5d48f560 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -89,6 +89,12 @@ int check_tsc_unstable(void)
 }
 EXPORT_SYMBOL_GPL(check_tsc_unstable);
 
+int check_tsc_disabled(void)
+{
+	return tsc_disabled;
+}
+EXPORT_SYMBOL_GPL(check_tsc_disabled);
+
 #ifdef CONFIG_X86_TSC
 int __init notsc_setup(char *str)
 {

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 098b3cfda72e..6ff49247edf8 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -824,7 +824,7 @@ static void __init check_system_tsc_reliable(void)
  * Make an educated guess if the TSC is trustworthy and synchronized
  * over all CPUs.
  */
-__cpuinit int unsynchronized_tsc(void)
+int unsynchronized_tsc(void)
 {
 	if (!cpu_has_tsc || tsc_unstable)
 		return 1;
@@ -1020,7 +1020,7 @@ void __init tsc_init(void)
  * been calibrated. This assumes that CONSTANT_TSC applies to all
  * cpus in the socket - this should be a safe assumption.
  */
-unsigned long __cpuinit calibrate_delay_is_known(void)
+unsigned long calibrate_delay_is_known(void)
 {
 	int i, cpu = smp_processor_id();
 

commit 82f9c080b22a5b859ae2b50822dfb6b812898fdb
Author: Feng Tang <feng.tang@intel.com>
Date:   Tue Mar 12 11:56:47 2013 +0800

    x86: tsc: Add support for new S3_NONSTOP feature
    
    Add support for new S3_NONSTOP feature
    
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 4b9ea101fe3b..098b3cfda72e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -768,7 +768,8 @@ static cycle_t read_tsc(struct clocksource *cs)
 
 static void resume_tsc(struct clocksource *cs)
 {
-	clocksource_tsc.cycle_last = 0;
+	if (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC_S3))
+		clocksource_tsc.cycle_last = 0;
 }
 
 static struct clocksource clocksource_tsc = {
@@ -939,6 +940,9 @@ static int __init init_tsc_clocksource(void)
 		clocksource_tsc.flags &= ~CLOCK_SOURCE_IS_CONTINUOUS;
 	}
 
+	if (boot_cpu_has(X86_FEATURE_NONSTOP_TSC_S3))
+		clocksource_tsc.flags |= CLOCK_SOURCE_SUSPEND_NONSTOP;
+
 	/*
 	 * Trust the results of the earlier calibration on systems
 	 * exporting a reliable TSC.

commit 90889a635a9b5488624bccce3ff6b2eec68c007b
Merge: a9037430c6c7 6f16eebe1ff8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 4 11:03:03 2013 +0100

    Merge branch 'fortglx/3.9/time' of git://git.linaro.org/people/jstultz/linux into timers/core
    
    Trivial conflict in arch/x86/Kconfig
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 2353b47bffe4e6ab39042f470c55d41bb3ff3846
Author: Bernd Faust <berndfaust@gmail.com>
Date:   Wed Dec 5 15:16:49 2012 +0100

    Round the calculated scale factor in set_cyc2ns_scale()
    
    During some experiments with an external clock (in a FPGA), we saw that
    the TSC clock drifted approx. 2.5ms per second.
    
    This drift was caused by the current way of calculating the scale.
    In our case cpu_khz had a value of 3292725. This resulted in a scale
    value of 310. But when doing the calculation by hand it shows that the
    actual value is 310.9886188491, so a value of 311 would be more precise.
    
    With this change the value is rounded.
    
    Signed-off-by: Bernd Faust <berndfaust@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index cfa5d4f7ca56..8ed085733773 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -617,7 +617,8 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 	ns_now = __cycles_2_ns(tsc_now);
 
 	if (cpu_khz) {
-		*scale = (NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR)/cpu_khz;
+		*scale = ((NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR) +
+				cpu_khz / 2) / cpu_khz;
 		*offset = ns_now - mult_frac(tsc_now, *scale,
 					     (1UL << CYC2NS_SCALE_FACTOR));
 	}

commit ce37f400336a34bb6e72c4700f9dcc2a41ff7163
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Oct 8 13:07:30 2012 +0100

    x86: Allow tracing of functions in arch/x86/kernel/rtc.c
    
    Move native_read_tsc() to tsc.c to allow profiling to be
    re-enabled for rtc.c.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1349698050-6560-1-git-send-email-david.vrabel@citrix.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index cfa5d4f7ca56..06ccb5073a3f 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -77,6 +77,12 @@ unsigned long long
 sched_clock(void) __attribute__((alias("native_sched_clock")));
 #endif
 
+unsigned long long native_read_tsc(void)
+{
+	return __native_read_tsc();
+}
+EXPORT_SYMBOL(native_read_tsc);
+
 int check_tsc_unstable(void)
 {
 	return tsc_unstable;

commit c767a54ba0657e52e6edaa97cbe0b0a8bf1c1655
Author: Joe Perches <joe@perches.com>
Date:   Mon May 21 19:50:07 2012 -0700

    x86/debug: Add KERN_<LEVEL> to bare printks, convert printks to pr_<level>
    
    Use a more current logging style:
    
     - Bare printks should have a KERN_<LEVEL> for consistency's sake
     - Add pr_fmt where appropriate
     - Neaten some macro definitions
     - Convert some Ok output to OK
     - Use "%s: ", __func__ in pr_fmt for summit
     - Convert some printks to pr_<level>
    
    Message output is not identical in all cases.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: levinsasha928@gmail.com
    Link: http://lkml.kernel.org/r/1337655007.24226.10.camel@joe2Laptop
    [ merged two similar patches, tidied up the changelog ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index fc0a147e3727..cfa5d4f7ca56 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1,3 +1,5 @@
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
 #include <linux/kernel.h>
 #include <linux/sched.h>
 #include <linux/init.h>
@@ -84,8 +86,7 @@ EXPORT_SYMBOL_GPL(check_tsc_unstable);
 #ifdef CONFIG_X86_TSC
 int __init notsc_setup(char *str)
 {
-	printk(KERN_WARNING "notsc: Kernel compiled with CONFIG_X86_TSC, "
-			"cannot disable TSC completely.\n");
+	pr_warn("Kernel compiled with CONFIG_X86_TSC, cannot disable TSC completely\n");
 	tsc_disabled = 1;
 	return 1;
 }
@@ -373,7 +374,7 @@ static unsigned long quick_pit_calibrate(void)
 			goto success;
 		}
 	}
-	printk("Fast TSC calibration failed\n");
+	pr_err("Fast TSC calibration failed\n");
 	return 0;
 
 success:
@@ -392,7 +393,7 @@ static unsigned long quick_pit_calibrate(void)
 	 */
 	delta *= PIT_TICK_RATE;
 	do_div(delta, i*256*1000);
-	printk("Fast TSC calibration using PIT\n");
+	pr_info("Fast TSC calibration using PIT\n");
 	return delta;
 }
 
@@ -487,9 +488,8 @@ unsigned long native_calibrate_tsc(void)
 		 * use the reference value, as it is more precise.
 		 */
 		if (delta >= 90 && delta <= 110) {
-			printk(KERN_INFO
-			       "TSC: PIT calibration matches %s. %d loops\n",
-			       hpet ? "HPET" : "PMTIMER", i + 1);
+			pr_info("PIT calibration matches %s. %d loops\n",
+				hpet ? "HPET" : "PMTIMER", i + 1);
 			return tsc_ref_min;
 		}
 
@@ -511,38 +511,36 @@ unsigned long native_calibrate_tsc(void)
 	 */
 	if (tsc_pit_min == ULONG_MAX) {
 		/* PIT gave no useful value */
-		printk(KERN_WARNING "TSC: Unable to calibrate against PIT\n");
+		pr_warn("Unable to calibrate against PIT\n");
 
 		/* We don't have an alternative source, disable TSC */
 		if (!hpet && !ref1 && !ref2) {
-			printk("TSC: No reference (HPET/PMTIMER) available\n");
+			pr_notice("No reference (HPET/PMTIMER) available\n");
 			return 0;
 		}
 
 		/* The alternative source failed as well, disable TSC */
 		if (tsc_ref_min == ULONG_MAX) {
-			printk(KERN_WARNING "TSC: HPET/PMTIMER calibration "
-			       "failed.\n");
+			pr_warn("HPET/PMTIMER calibration failed\n");
 			return 0;
 		}
 
 		/* Use the alternative source */
-		printk(KERN_INFO "TSC: using %s reference calibration\n",
-		       hpet ? "HPET" : "PMTIMER");
+		pr_info("using %s reference calibration\n",
+			hpet ? "HPET" : "PMTIMER");
 
 		return tsc_ref_min;
 	}
 
 	/* We don't have an alternative source, use the PIT calibration value */
 	if (!hpet && !ref1 && !ref2) {
-		printk(KERN_INFO "TSC: Using PIT calibration value\n");
+		pr_info("Using PIT calibration value\n");
 		return tsc_pit_min;
 	}
 
 	/* The alternative source failed, use the PIT calibration value */
 	if (tsc_ref_min == ULONG_MAX) {
-		printk(KERN_WARNING "TSC: HPET/PMTIMER calibration failed. "
-		       "Using PIT calibration\n");
+		pr_warn("HPET/PMTIMER calibration failed. Using PIT calibration.\n");
 		return tsc_pit_min;
 	}
 
@@ -551,9 +549,9 @@ unsigned long native_calibrate_tsc(void)
 	 * the PIT value as we know that there are PMTIMERs around
 	 * running at double speed. At least we let the user know:
 	 */
-	printk(KERN_WARNING "TSC: PIT calibration deviates from %s: %lu %lu.\n",
-	       hpet ? "HPET" : "PMTIMER", tsc_pit_min, tsc_ref_min);
-	printk(KERN_INFO "TSC: Using PIT calibration value\n");
+	pr_warn("PIT calibration deviates from %s: %lu %lu\n",
+		hpet ? "HPET" : "PMTIMER", tsc_pit_min, tsc_ref_min);
+	pr_info("Using PIT calibration value\n");
 	return tsc_pit_min;
 }
 
@@ -785,7 +783,7 @@ void mark_tsc_unstable(char *reason)
 		tsc_unstable = 1;
 		sched_clock_stable = 0;
 		disable_sched_clock_irqtime();
-		printk(KERN_INFO "Marking TSC unstable due to %s\n", reason);
+		pr_info("Marking TSC unstable due to %s\n", reason);
 		/* Change only the rating, when not registered */
 		if (clocksource_tsc.mult)
 			clocksource_mark_unstable(&clocksource_tsc);
@@ -912,9 +910,9 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 		goto out;
 
 	tsc_khz = freq;
-	printk(KERN_INFO "Refined TSC clocksource calibration: "
-		"%lu.%03lu MHz.\n", (unsigned long)tsc_khz / 1000,
-					(unsigned long)tsc_khz % 1000);
+	pr_info("Refined TSC clocksource calibration: %lu.%03lu MHz\n",
+		(unsigned long)tsc_khz / 1000,
+		(unsigned long)tsc_khz % 1000);
 
 out:
 	clocksource_register_khz(&clocksource_tsc, tsc_khz);
@@ -970,9 +968,9 @@ void __init tsc_init(void)
 		return;
 	}
 
-	printk("Detected %lu.%03lu MHz processor.\n",
-			(unsigned long)cpu_khz / 1000,
-			(unsigned long)cpu_khz % 1000);
+	pr_info("Detected %lu.%03lu MHz processor\n",
+		(unsigned long)cpu_khz / 1000,
+		(unsigned long)cpu_khz % 1000);
 
 	/*
 	 * Secondary CPUs do not run through tsc_init(), so set up

commit bcd550745fc54f789c14e7526e0633222c505faa
Merge: 93f378883cec 646783a38982
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 14:16:48 2012 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer core updates from Thomas Gleixner.
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      ia64: vsyscall: Add missing paranthesis
      alarmtimer: Don't call rtc_timer_init() when CONFIG_RTC_CLASS=n
      x86: vdso: Put declaration before code
      x86-64: Inline vdso clock_gettime helpers
      x86-64: Simplify and optimize vdso clock_gettime monotonic variants
      kernel-time: fix s/then/than/ spelling errors
      time: remove no_sync_cmos_clock
      time: Avoid scary backtraces when warning of > 11% adj
      alarmtimer: Make sure we initialize the rtctimer
      ntp: Fix leap-second hrtimer livelock
      x86, tsc: Skip refined tsc calibration on systems with reliable TSC
      rtc: Provide flag for rtc devices that don't support UIE
      ia64: vsyscall: Use seqcount instead of seqlock
      x86: vdso: Use seqcount instead of seqlock
      x86: vdso: Remove bogus locking in update_vsyscall_tz()
      time: Remove bogus comments
      time: Fix change_clocksource locking
      time: x86: Fix race switching from vsyscall to non-vsyscall clock

commit 2e7580b0e75d771d93e24e681031a165b1d31071
Merge: d25413efa953 cf9eeac46350
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 14:35:31 2012 -0700

    Merge branch 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Avi Kivity:
     "Changes include timekeeping improvements, support for assigning host
      PCI devices that share interrupt lines, s390 user-controlled guests, a
      large ppc update, and random fixes."
    
    This is with the sign-off's fixed, hopefully next merge window we won't
    have rebased commits.
    
    * 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (130 commits)
      KVM: Convert intx_mask_lock to spin lock
      KVM: x86: fix kvm_write_tsc() TSC matching thinko
      x86: kvmclock: abstract save/restore sched_clock_state
      KVM: nVMX: Fix erroneous exception bitmap check
      KVM: Ignore the writes to MSR_K7_HWCR(3)
      KVM: MMU: make use of ->root_level in reset_rsvds_bits_mask
      KVM: PMU: add proper support for fixed counter 2
      KVM: PMU: Fix raw event check
      KVM: PMU: warn when pin control is set in eventsel msr
      KVM: VMX: Fix delayed load of shared MSRs
      KVM: use correct tlbs dirty type in cmpxchg
      KVM: Allow host IRQ sharing for assigned PCI 2.3 devices
      KVM: Ensure all vcpus are consistent with in-kernel irqchip settings
      KVM: x86 emulator: Allow PM/VM86 switch during task switch
      KVM: SVM: Fix CPL updates
      KVM: x86 emulator: VM86 segments must have DPL 3
      KVM: x86 emulator: Fix task switch privilege checks
      arch/powerpc/kvm/book3s_hv.c: included linux/sched.h twice
      KVM: x86 emulator: correctly mask pmc index bits in RDPMC instruction emulation
      KVM: mmu_notifier: Flush TLBs before releasing mmu_lock
      ...

commit b74f05d61b73af584d0c39121980171389ecfaaa
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Feb 13 11:07:27 2012 -0200

    x86: kvmclock: abstract save/restore sched_clock_state
    
    Upon resume from hibernation, CPU 0's hvclock area contains the old
    values for system_time and tsc_timestamp. It is necessary for the
    hypervisor to update these values with uptodate ones before the CPU uses
    them.
    
    Abstract TSC's save/restore sched_clock_state functions and use
    restore_state to write to KVM_SYSTEM_TIME MSR, forcing an update.
    
    Also move restore_sched_clock_state before __restore_processor_state,
    since the later calls CONFIG_LOCK_STAT's lockstat_clock (also for TSC).
    Thanks to Igor Mammedov for tracking it down.
    
    Fixes suspend-to-disk with kvmclock.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a62c201c97ec..aed2aa1088f1 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -629,7 +629,7 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 
 static unsigned long long cyc2ns_suspend;
 
-void save_sched_clock_state(void)
+void tsc_save_sched_clock_state(void)
 {
 	if (!sched_clock_stable)
 		return;
@@ -645,7 +645,7 @@ void save_sched_clock_state(void)
  * that sched_clock() continues from the point where it was left off during
  * suspend.
  */
-void restore_sched_clock_state(void)
+void tsc_restore_sched_clock_state(void)
 {
 	unsigned long long offset;
 	unsigned long flags;

commit 57779dc2b3b75bee05ef5d1ada47f615f7a13932
Author: Alok Kataria <akataria@vmware.com>
Date:   Tue Feb 21 18:19:55 2012 -0800

    x86, tsc: Skip refined tsc calibration on systems with reliable TSC
    
    While running the latest Linux as guest under VMware in highly
    over-committed situations, we have seen cases when the refined TSC
    algorithm fails to get a valid tsc_start value in
    tsc_refine_calibration_work from multiple attempts. As a result the
    kernel keeps on scheduling the tsc_irqwork task for later. Subsequently
    after several attempts when it gets a valid start value it goes through
    the refined calibration and either bails out or uses the new results.
    Given that the kernel originally read the TSC frequency from the
    platform, which is the best it can get, I don't think there is much
    value in refining it.
    
    So  for systems which get the TSC frequency from the platform we
    should skip the refined tsc algorithm.
    
    We can use the TSC_RELIABLE cpu cap flag to detect this, right now it is
    set only on VMware and for Moorestown Penwell both of which have there
    own TSC calibration methods.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Cc: John Stultz <johnstul@us.ibm.com>
    Cc: Dirk Brandewie <dirk.brandewie@gmail.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: stable@kernel.org
    [jstultz: Reworked to simply not schedule the refining work,
    rather then scheduling the work and bombing out later]
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a62c201c97ec..6fcfcb3865c2 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -932,6 +932,16 @@ static int __init init_tsc_clocksource(void)
 		clocksource_tsc.rating = 0;
 		clocksource_tsc.flags &= ~CLOCK_SOURCE_IS_CONTINUOUS;
 	}
+
+	/*
+	 * Trust the results of the earlier calibration on systems
+	 * exporting a reliable TSC.
+	 */
+	if (boot_cpu_has(X86_FEATURE_TSC_RELIABLE)) {
+		clocksource_register_khz(&clocksource_tsc, tsc_khz);
+		return 0;
+	}
+
 	schedule_delayed_work(&tsc_irqwork, 0);
 	return 0;
 }

commit 9993bc635d01a6ee7f6b833b4ee65ce7c06350b1
Author: Salman Qazi <sqazi@google.com>
Date:   Fri Mar 9 16:41:01 2012 -0800

    sched/x86: Fix overflow in cyc2ns_offset
    
    When a machine boots up, the TSC generally gets reset.  However,
    when kexec is used to boot into a kernel, the TSC value would be
    carried over from the previous kernel.  The computation of
    cycns_offset in set_cyc2ns_scale is prone to an overflow, if the
    machine has been up more than 208 days prior to the kexec.  The
    overflow happens when we multiply *scale, even though there is
    enough room to store the final answer.
    
    We fix this issue by decomposing tsc_now into the quotient and
    remainder of division by CYC2NS_SCALE_FACTOR and then performing
    the multiplication separately on the two components.
    
    Refactor code to share the calculation with the previous
    fix in __cycles_2_ns().
    
    Signed-off-by: Salman Qazi <sqazi@google.com>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Turner <pjt@google.com>
    Cc: john stultz <johnstul@us.ibm.com>
    Link: http://lkml.kernel.org/r/20120310004027.19291.88460.stgit@dungbeetle.mtv.corp.google.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a62c201c97ec..183c5925a9fe 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -620,7 +620,8 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 
 	if (cpu_khz) {
 		*scale = (NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR)/cpu_khz;
-		*offset = ns_now - (tsc_now * *scale >> CYC2NS_SCALE_FACTOR);
+		*offset = ns_now - mult_frac(tsc_now, *scale,
+					     (1UL << CYC2NS_SCALE_FACTOR));
 	}
 
 	sched_clock_idle_wakeup_event(0);

commit 282f445a779ed76fca9884fe377bf56a3088b208
Merge: 68f30fbee19c 90a4c0f51e8e
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Jan 19 12:56:50 2012 -0800

    Merge remote-tracking branch 'linus/master' into x86/urgent

commit 68f30fbee19cc67849b9fa8e153ede70758afe81
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 17 15:35:37 2012 -0800

    x86, tsc: Fix SMI induced variation in quick_pit_calibrate()
    
    pit_expect_msb() returns success wrongly in the below SMI scenario:
    
    a. pit_verify_msb() has not yet seen the MSB transition.
    
    b. we are close to the MSB transition though and got a SMI immediately after
       returning from pit_verify_msb() which didn't see the MSB transition. PIT MSB
       transition has happened somewhere during SMI execution.
    
    c. returned from SMI and we noted down the 'tsc', saw the pit MSB change now and
       exited the loop to calculate 'deltatsc'. Instead of noting the TSC at the MSB
       transition, we are way off because of the SMI.  And as the SMI happened
       between the pit_verify_msb() and before the 'tsc' is recorded in the
       for loop, 'delattsc' (d1/d2 in quick_pit_calibrate()) will be small and
       quick_pit_calibrate() will not notice this error.
    
    Depending on whether SMI disturbance happens while computing d1 or d2, we will
    see the TSC calibrated value smaller or bigger than the expected value. As a
    result, in a cluster we were seeing a variation of approximately +/- 20MHz in
    the calibrated values, resulting in NTP failures.
    
      [ As far as the SMI source is concerned, this is a periodic SMI that gets
        disabled after ACPI is enabled by the OS. But the TSC calibration happens
        before the ACPI is enabled. ]
    
    To address this, change pit_expect_msb() so that
    
     - the 'tsc' is the TSC in between the two reads that read the MSB
    change from the PIT (same as before)
    
     - the 'delta' is the difference in TSC from *before* the MSB changed
    to *after* the MSB changed.
    
    Now the delta is twice as big as before (it covers four PIT accesses,
    roughly 4us) and quick_pit_calibrate() will loop a bit longer to get
    the calibrated value with in the 500ppm precision. As the delta (d1/d2)
    covers four PIT accesses, actual calibrated result might be closer to
    250ppm precision.
    
    As the loop now takes longer to stabilize, double MAX_QUICK_PIT_MS to 50.
    
    SMI disturbance will showup as much larger delta's and the loop will take
    longer than usual for the result to be with in the accepted precision. Or will
    fallback to slow PIT calibration if it takes more than 50msec.
    
    Also while we are at this, remove the calibration correction that aims to
    get the result to the middle of the error bars. We really don't know which
    direction to correct into, so remove it.
    
    Reported-and-tested-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1326843337.5291.4.camel@sbsiddha-mobl2
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 2c9cf0fd78f5..f54694611172 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -290,14 +290,15 @@ static inline int pit_verify_msb(unsigned char val)
 static inline int pit_expect_msb(unsigned char val, u64 *tscp, unsigned long *deltap)
 {
 	int count;
-	u64 tsc = 0;
+	u64 tsc = 0, prev_tsc = 0;
 
 	for (count = 0; count < 50000; count++) {
 		if (!pit_verify_msb(val))
 			break;
+		prev_tsc = tsc;
 		tsc = get_cycles();
 	}
-	*deltap = get_cycles() - tsc;
+	*deltap = get_cycles() - prev_tsc;
 	*tscp = tsc;
 
 	/*
@@ -311,9 +312,9 @@ static inline int pit_expect_msb(unsigned char val, u64 *tscp, unsigned long *de
  * How many MSB values do we want to see? We aim for
  * a maximum error rate of 500ppm (in practice the
  * real error is much smaller), but refuse to spend
- * more than 25ms on it.
+ * more than 50ms on it.
  */
-#define MAX_QUICK_PIT_MS 25
+#define MAX_QUICK_PIT_MS 50
 #define MAX_QUICK_PIT_ITERATIONS (MAX_QUICK_PIT_MS * PIT_TICK_RATE / 1000 / 256)
 
 static unsigned long quick_pit_calibrate(void)
@@ -383,15 +384,12 @@ static unsigned long quick_pit_calibrate(void)
 	 *
 	 * As a result, we can depend on there not being
 	 * any odd delays anywhere, and the TSC reads are
-	 * reliable (within the error). We also adjust the
-	 * delta to the middle of the error bars, just
-	 * because it looks nicer.
+	 * reliable (within the error).
 	 *
 	 * kHz = ticks / time-in-seconds / 1000;
 	 * kHz = (t2 - t1) / (I * 256 / PIT_TICK_RATE) / 1000
 	 * kHz = ((t2 - t1) * PIT_TICK_RATE) / (I * 256 * 1000)
 	 */
-	delta += (long)(d2 - d1)/2;
 	delta *= PIT_TICK_RATE;
 	do_div(delta, i*256*1000);
 	printk("Fast TSC calibration using PIT\n");

commit 9fc5c3e3237e02a94f41cd1d2b4291593d29791d
Merge: 541048a1d313 7c9c3a1e5fc8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 11 19:13:40 2012 -0800

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/intel config: Fix the APB_TIMER selection
      x86/mrst: Add additional debug prints for pb_keys
      x86/intel config: Revamp configuration to allow for Moorestown and Medfield
      x86/intel/scu/ipc: Match the changes in the x86 configuration
      x86/apb: Fix configuration constraints
      x86: Fix INTEL_MID silly
      x86/Kconfig: Cyclone-timer depends on x86-summit
      x86: Reduce clock calibration time during slave cpu startup
      x86/config: Revamp configuration for MID devices
      x86/sfi: Kill the IRQ as id hack

commit 0518469d0a32be1e6dd8850ff274d52d72cdb52d
Merge: 28a00184be26 f5a54dd7952e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Dec 5 22:13:49 2011 +0100

    Merge branch 'fortglx/3.3/tip/timers/core' of git://git.linaro.org/people/jstultz/linux into timers/core

commit 28a00184be261e3dc152ba0d664a067bbe235b6a
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Nov 4 15:42:17 2011 -0700

    x86, tsc: Skip TSC synchronization checks for tsc=reliable
    
    tsc=reliable boot parameter is supposed to skip all the TSC
    stablility checks during boot time.
    
    On a 8-socket system where we want to run an experiment with the
    "tsc=reliable" boot option, TSC synchronization checks are not
    getting skipped and marking the TSC as not stable.
    
    Check for tsc_clocksource_reliable (which is set via
    tsc=reliable or for platforms supporting synthetic TSC_RELIABLE
    feature bit etc) and when set, skip the TSC synchronization
    tests during boot.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: John Stultz <johnstul@us.ibm.com>
    Tested-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1320446537.15071.14.camel@sbsiddha-desk.sc.intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index db483369f10b..eee465109e16 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -35,7 +35,7 @@ static int __read_mostly tsc_unstable;
    erroneous rdtsc usage on !cpu_has_tsc processors */
 static int __read_mostly tsc_disabled = -1;
 
-static int tsc_clocksource_reliable;
+int tsc_clocksource_reliable;
 /*
  * Scheduler clock - returns current time in nanosec units.
  */

commit b565201cf75210614903ef2ae5917b4379681647
Author: Jack Steiner <steiner@sgi.com>
Date:   Tue Nov 15 15:33:56 2011 -0800

    x86: Reduce clock calibration time during slave cpu startup
    
    Reduce the startup time for slave cpus.
    
    Adds hooks for an arch-specific function for clock calibration.
    These hooks are used on x86.  If a newly started cpu has the
    same phys_proc_id as a core already active, uses the TSC for the
    delay loop and has a CONSTANT_TSC, use the already-calculated
    value of loops_per_jiffy.
    
    This patch reduces the time required to start slave cpus on a
    4096 cpu system from: 465 sec OLD 62 sec NEW
    
    This reduces boot time on a 4096p system by almost 7 minutes.
    Nice...
    
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: John Stultz <john.stultz@linaro.org>
    [fix CONFIG_SMP=n build]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index db483369f10b..490fb330be87 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -995,3 +995,23 @@ void __init tsc_init(void)
 	check_system_tsc_reliable();
 }
 
+#ifdef CONFIG_SMP
+/*
+ * If we have a constant TSC and are using the TSC for the delay loop,
+ * we can skip clock calibration if another cpu in the same socket has already
+ * been calibrated. This assumes that CONSTANT_TSC applies to all
+ * cpus in the socket - this should be a safe assumption.
+ */
+unsigned long __cpuinit calibrate_delay_is_known(void)
+{
+	int i, cpu = smp_processor_id();
+
+	if (!tsc_disabled && !cpu_has(&cpu_data(cpu), X86_FEATURE_CONSTANT_TSC))
+		return 0;
+
+	for_each_online_cpu(i)
+		if (cpu_data(i).phys_proc_id == cpu_data(cpu).phys_proc_id)
+			return cpu_data(i).loops_per_jiffy;
+	return 0;
+}
+#endif

commit b7743970b054a08acf6445cc6d10838e60cdb639
Author: Deepak Saxena <dsaxena@linaro.org>
Date:   Tue Nov 1 14:25:07 2011 -0700

    time: x86: Remove CLOCK_TICK_RATE from tsc code
    
    The tsc code uses CLOCK_TICK_RATE which on x86
    is defined to just be the same as PIT_TICK_RATE.
    This patch updates the code use the later
    as we want to depecrate and remove the global
    CLOCK_TICK_RATE symbol.
    
    Signed-off-by: Deepak Saxena <dsaxena@linaro.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index db483369f10b..1e88c8ef6d6d 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -178,11 +178,11 @@ static unsigned long calc_pmtimer_ref(u64 deltatsc, u64 pm1, u64 pm2)
 }
 
 #define CAL_MS		10
-#define CAL_LATCH	(CLOCK_TICK_RATE / (1000 / CAL_MS))
+#define CAL_LATCH	(PIT_TICK_RATE / (1000 / CAL_MS))
 #define CAL_PIT_LOOPS	1000
 
 #define CAL2_MS		50
-#define CAL2_LATCH	(CLOCK_TICK_RATE / (1000 / CAL2_MS))
+#define CAL2_LATCH	(PIT_TICK_RATE / (1000 / CAL2_MS))
 #define CAL2_PIT_LOOPS	5000
 
 

commit b4db920c7f524b2cd0f5ae7efbbbbfd2c76a27da
Merge: 7080d306762f 24a42bae6852 3e7cf5b00dd5 050438ed5a05 43605ef188cd 38175051f8e7 df049672dddd 14cb6dcf0a02
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 23 10:38:21 2011 -0700

    Merge branches 'x86-detect-hyper-for-linus', 'x86-fpu-for-linus', 'x86-kexec-for-linus', 'x86-platform-for-linus', 'x86-quirks-for-linus', 'x86-tsc-for-linus' and 'x86-smpboot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-detect-hyper-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, hyper: Change hypervisor detection order
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86-32, fpu: Fix DNA exception during check_fpu()
    
    * 'x86-kexec-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      kexec, x86: Fix incorrect jump back address if not preserving context
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, config: Introduce an INTEL_MID configuration
    
    * 'x86-quirks-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, quirks: Use pci_dev->revision
    
    * 'x86-tsc-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: tsc: Remove unneeded DMI-based blacklisting
    
    * 'x86-smpboot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, boot: Wait for boot cpu to show up if nr_cpus limit is about to hit

commit 98d0ac38ca7b1b7a552c9a2359174ff84decb600
Author: Andy Lutomirski <luto@mit.edu>
Date:   Thu Jul 14 06:47:22 2011 -0400

    x86-64: Move vread_tsc and vread_hpet into the vDSO
    
    The vsyscall page now consists entirely of trap instructions.
    
    Cc: John Stultz <johnstul@us.ibm.com>
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Link: http://lkml.kernel.org/r/637648f303f2ef93af93bae25186e9a1bea093f5.1310639973.git.luto@mit.edu
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index e7a74b889ab3..56c633a5db72 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -777,7 +777,7 @@ static struct clocksource clocksource_tsc = {
 	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
 				  CLOCK_SOURCE_MUST_VERIFY,
 #ifdef CONFIG_X86_64
-	.archdata               = { .vread = vread_tsc },
+	.archdata               = { .vclock_mode = VCLOCK_TSC },
 #endif
 };
 

commit 433bd805e5fd2c731b3a9025b034f066272d336e
Author: Andy Lutomirski <luto@mit.edu>
Date:   Wed Jul 13 09:24:13 2011 -0400

    clocksource: Replace vread with generic arch data
    
    The vread field was bloating struct clocksource everywhere except
    x86_64, and I want to change the way this works on x86_64, so let's
    split it out into per-arch data.
    
    Cc: x86@kernel.org
    Cc: Clemens Ladisch <clemens@ladisch.de>
    Cc: linux-ia64@vger.kernel.org
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: John Stultz <johnstul@us.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Link: http://lkml.kernel.org/r/3ae5ec76a168eaaae63f08a2a1060b91aa0b7759.1310563276.git.luto@mit.edu
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 6cc6922262af..e7a74b889ab3 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -777,7 +777,7 @@ static struct clocksource clocksource_tsc = {
 	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
 				  CLOCK_SOURCE_MUST_VERIFY,
 #ifdef CONFIG_X86_64
-	.vread                  = vread_tsc,
+	.archdata               = { .vread = vread_tsc },
 #endif
 };
 

commit df049672dddde4a2fdacf63fb32eb80146e26841
Author: Tero Roponen <tero.roponen@gmail.com>
Date:   Tue May 31 10:24:39 2011 +0300

    x86: tsc: Remove unneeded DMI-based blacklisting
    
    The blacklist was added in response to my bug report
    (http://lkml.org/lkml/2006/1/19/362) and has never
    contained more than the one entry describing my old
    now dead ThinkPad 380XD laptop. As found out later
    (http://lkml.org/lkml/2007/11/29/50), this special
    treatment has been unnecessary for a long time, so
    it can be removed.
    
    Signed-off-by: Tero Roponen <tero.roponen@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 6cc6922262af..5c45c62cce5d 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -5,7 +5,6 @@
 #include <linux/timer.h>
 #include <linux/acpi_pmtmr.h>
 #include <linux/cpufreq.h>
-#include <linux/dmi.h>
 #include <linux/delay.h>
 #include <linux/clocksource.h>
 #include <linux/percpu.h>
@@ -800,27 +799,6 @@ void mark_tsc_unstable(char *reason)
 
 EXPORT_SYMBOL_GPL(mark_tsc_unstable);
 
-static int __init dmi_mark_tsc_unstable(const struct dmi_system_id *d)
-{
-	printk(KERN_NOTICE "%s detected: marking TSC unstable.\n",
-			d->ident);
-	tsc_unstable = 1;
-	return 0;
-}
-
-/* List of systems that have known TSC problems */
-static struct dmi_system_id __initdata bad_tsc_dmi_table[] = {
-	{
-		.callback = dmi_mark_tsc_unstable,
-		.ident = "IBM Thinkpad 380XD",
-		.matches = {
-			DMI_MATCH(DMI_BOARD_VENDOR, "IBM"),
-			DMI_MATCH(DMI_BOARD_NAME, "2635FA0"),
-		},
-	},
-	{}
-};
-
 static void __init check_system_tsc_reliable(void)
 {
 #ifdef CONFIG_MGEODE_LX
@@ -1010,8 +988,6 @@ void __init tsc_init(void)
 	lpj_fine = lpj;
 
 	use_tsc_delay();
-	/* Check and install the TSC clocksource */
-	dmi_check_system(bad_tsc_dmi_table);
 
 	if (unsynchronized_tsc())
 		mark_tsc_unstable("TSCs unsynchronized");

commit 44259b1abfaa8bb819d25d41d71e8e33e25dd36a
Author: Andy Lutomirski <luto@MIT.EDU>
Date:   Mon May 23 09:31:28 2011 -0400

    x86-64: Move vread_tsc into a new file with sensible options
    
    vread_tsc is short and hot, and it's userspace code so the usual
    reasons to enable -pg and turn off sibling calls don't apply.
    
    (OK, turning off sibling calls has no effect.  But it might
    someday...)
    
    As an added benefit, tsc.c is profilable now.
    
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Borislav Petkov <bp@amd64.org>
    Link: http://lkml.kernel.org/r/%3C99c6d7f5efa3ccb65b4ac6eb443e1ab7bad47d7b.1306156808.git.luto%40mit.edu%3E
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 24249a5360b6..6cc6922262af 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -763,40 +763,6 @@ static cycle_t read_tsc(struct clocksource *cs)
 		ret : clocksource_tsc.cycle_last;
 }
 
-#ifdef CONFIG_X86_64
-static cycle_t __vsyscall_fn vread_tsc(void)
-{
-	cycle_t ret;
-	u64 last;
-
-	/*
-	 * Empirically, a fence (of type that depends on the CPU)
-	 * before rdtsc is enough to ensure that rdtsc is ordered
-	 * with respect to loads.  The various CPU manuals are unclear
-	 * as to whether rdtsc can be reordered with later loads,
-	 * but no one has ever seen it happen.
-	 */
-	rdtsc_barrier();
-	ret = (cycle_t)vget_cycles();
-
-	last = VVAR(vsyscall_gtod_data).clock.cycle_last;
-
-	if (likely(ret >= last))
-		return ret;
-
-	/*
-	 * GCC likes to generate cmov here, but this branch is extremely
-	 * predictable (it's just a funciton of time and the likely is
-	 * very likely) and there's a data dependence, so force GCC
-	 * to generate a branch instead.  I don't barrier() because
-	 * we don't actually need a barrier, and if this function
-	 * ever gets inlined it will generate worse code.
-	 */
-	asm volatile ("");
-	return last;
-}
-#endif
-
 static void resume_tsc(struct clocksource *cs)
 {
 	clocksource_tsc.cycle_last = 0;

commit 3729db5ca2b2000c660e5a5d0eb68b1053212cab
Author: Andy Lutomirski <luto@MIT.EDU>
Date:   Mon May 23 09:31:26 2011 -0400

    x86-64: Don't generate cmov in vread_tsc
    
    vread_tsc checks whether rdtsc returns something less than
    cycle_last, which is an extremely predictable branch.  GCC likes
    to generate a cmov anyway, which is several cycles slower than
    a predicted branch.  This saves a couple of nanoseconds.
    
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Borislav Petkov <bp@amd64.org>
    Link: http://lkml.kernel.org/r/%3C561280649519de41352fcb620684dfb22bad6bac.1306156808.git.luto%40mit.edu%3E
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 1e6244202612..24249a5360b6 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -767,6 +767,7 @@ static cycle_t read_tsc(struct clocksource *cs)
 static cycle_t __vsyscall_fn vread_tsc(void)
 {
 	cycle_t ret;
+	u64 last;
 
 	/*
 	 * Empirically, a fence (of type that depends on the CPU)
@@ -778,8 +779,21 @@ static cycle_t __vsyscall_fn vread_tsc(void)
 	rdtsc_barrier();
 	ret = (cycle_t)vget_cycles();
 
-	return ret >= VVAR(vsyscall_gtod_data).clock.cycle_last ?
-		ret : VVAR(vsyscall_gtod_data).clock.cycle_last;
+	last = VVAR(vsyscall_gtod_data).clock.cycle_last;
+
+	if (likely(ret >= last))
+		return ret;
+
+	/*
+	 * GCC likes to generate cmov here, but this branch is extremely
+	 * predictable (it's just a funciton of time and the likely is
+	 * very likely) and there's a data dependence, so force GCC
+	 * to generate a branch instead.  I don't barrier() because
+	 * we don't actually need a barrier, and if this function
+	 * ever gets inlined it will generate worse code.
+	 */
+	asm volatile ("");
+	return last;
 }
 #endif
 

commit 057e6a8c660e95c3f4e7162e00e2fee1fc90c50d
Author: Andy Lutomirski <luto@MIT.EDU>
Date:   Mon May 23 09:31:25 2011 -0400

    x86-64: Remove unnecessary barrier in vread_tsc
    
    RDTSC is completely unordered on modern Intel and AMD CPUs.  The
    Intel manual says that lfence;rdtsc causes all previous instructions
    to complete before the tsc is read, and the AMD manual says to use
    mfence;rdtsc to do the same thing.
    
    From a decent amount of testing [1] this is enough to make rdtsc
    be ordered with respect to subsequent loads across a wide variety
    of CPUs.
    
    On Sandy Bridge (i7-2600), this improves a loop of
    clock_gettime(CLOCK_MONOTONIC) by more than 5 ns/iter.
    
    [1] https://lkml.org/lkml/2011/4/18/350
    
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Borislav Petkov <bp@amd64.org>
    Link: http://lkml.kernel.org/r/%3C1c158b9d74338aa5361f96dd473d0e6a58235302.1306156808.git.luto%40mit.edu%3E
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index db697b81b8b1..1e6244202612 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -769,13 +769,14 @@ static cycle_t __vsyscall_fn vread_tsc(void)
 	cycle_t ret;
 
 	/*
-	 * Surround the RDTSC by barriers, to make sure it's not
-	 * speculated to outside the seqlock critical section and
-	 * does not cause time warps:
+	 * Empirically, a fence (of type that depends on the CPU)
+	 * before rdtsc is enough to ensure that rdtsc is ordered
+	 * with respect to loads.  The various CPU manuals are unclear
+	 * as to whether rdtsc can be reordered with later loads,
+	 * but no one has ever seen it happen.
 	 */
 	rdtsc_barrier();
 	ret = (cycle_t)vget_cycles();
-	rdtsc_barrier();
 
 	return ret >= VVAR(vsyscall_gtod_data).clock.cycle_last ?
 		ret : VVAR(vsyscall_gtod_data).clock.cycle_last;

commit 8c49d9a74bac5ea3f18480307057241b808fcc0c
Author: Andy Lutomirski <luto@MIT.EDU>
Date:   Mon May 23 09:31:24 2011 -0400

    x86-64: Clean up vdso/kernel shared variables
    
    Variables that are shared between the vdso and the kernel are
    currently a bit of a mess.  They are each defined with their own
    magic, they are accessed differently in the kernel, the vsyscall page,
    and the vdso, and one of them (vsyscall_clock) doesn't even really
    exist.
    
    This changes them all to use a common mechanism.  All of them are
    delcared in vvar.h with a fixed address (validated by the linker
    script).  In the kernel (as before), they look like ordinary
    read-write variables.  In the vsyscall page and the vdso, they are
    accessed through a new macro VVAR, which gives read-only access.
    
    The vdso is now loaded verbatim into memory without any fixups.  As a
    side bonus, access from the vdso is faster because a level of
    indirection is removed.
    
    While we're at it, pack jiffies and vgetcpu_mode into the same
    cacheline.
    
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Borislav Petkov <bp@amd64.org>
    Link: http://lkml.kernel.org/r/%3C7357882fbb51fa30491636a7b6528747301b7ee9.1306156808.git.luto%40mit.edu%3E
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 9335bf7dd2e7..db697b81b8b1 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -777,8 +777,8 @@ static cycle_t __vsyscall_fn vread_tsc(void)
 	ret = (cycle_t)vget_cycles();
 	rdtsc_barrier();
 
-	return ret >= __vsyscall_gtod_data.clock.cycle_last ?
-		ret : __vsyscall_gtod_data.clock.cycle_last;
+	return ret >= VVAR(vsyscall_gtod_data).clock.cycle_last ?
+		ret : VVAR(vsyscall_gtod_data).clock.cycle_last;
 }
 #endif
 

commit 0d2eb44f631d9d0a826efa3156f157477fdaecf4
Author: Lucas De Marchi <lucas.de.marchi@gmail.com>
Date:   Thu Mar 17 16:24:16 2011 -0300

    x86: Fix common misspellings
    
    They were generated by 'codespell' and then manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>
    Cc: trivial@kernel.org
    LKML-Reference: <1300389856-1099-3-git-send-email-lucas.demarchi@profusion.mobi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index ffe5755caa8b..9335bf7dd2e7 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -427,7 +427,7 @@ unsigned long native_calibrate_tsc(void)
 	 * the delta to the previous read. We keep track of the min
 	 * and max values of that delta. The delta is mostly defined
 	 * by the IO time of the PIT access, so we can detect when a
-	 * SMI/SMM disturbance happend between the two reads. If the
+	 * SMI/SMM disturbance happened between the two reads. If the
 	 * maximum time is significantly larger than the minimum time,
 	 * then we discard the result and have another try.
 	 *
@@ -900,7 +900,7 @@ static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
  * timer based, instead of loop based, we don't block the boot
  * process while this longer calibration is done.
  *
- * If there are any calibration anomolies (too many SMIs, etc),
+ * If there are any calibration anomalies (too many SMIs, etc),
  * or the refined calibration is off by 1% of the fast early
  * calibration, we throw out the new calibration and use the
  * early calibration.

commit f9ee7f60d6f37ae0184812b4c59b3869f875768b
Merge: 16c102036208 1161ec944916 76d1f7bfcd58 afa14e7c553e 7c46d8da09df
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 15 12:45:00 2011 -0800

    Merge branches 'core-fixes-for-linus', 'x86-fixes-for-linus', 'timers-fixes-for-linus' and 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      rcu: avoid pointless blocked-task warnings
      rcu: demote SRCU_SYNCHRONIZE_DELAY from kernel-parameter status
      rtmutex: Fix comment about why new_owner can be NULL in wake_futex_pi()
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, olpc: Add missing Kconfig dependencies
      x86, mrst: Set correct APB timer IRQ affinity for secondary cpu
      x86: tsc: Fix calibration refinement conditionals to avoid divide by zero
      x86, ia64, acpi: Clean up x86-ism in drivers/acpi/numa.c
    
    * 'timers-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      timekeeping: Make local variables static
      time: Rename misnamed minsec argument of clocks_calc_mult_shift()
    
    * 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      tracing: Remove syscall_exit_fields
      tracing: Only process module tracepoints once
      perf record: Add "nodelay" mode, disabled by default
      perf sched: Fix list of events, dropping unsupported ':r' modifier
      Revert "perf tools: Emit clearer message for sys_perf_event_open ENOENT return"
      perf top: Fix annotate segv
      perf evsel: Fix order of event list deletion

commit 62627bec8a601c5679bf3d20a2096a1206d61b71
Author: John Stultz <johnstul@us.ibm.com>
Date:   Fri Jan 14 09:06:28 2011 -0800

    x86: tsc: Fix calibration refinement conditionals to avoid divide by zero
    
    Konrad Wilk reported that the new delayed calibration crashes with a
    divide by zero on Xen. The reason is that Xen sets the pmtimer
    address, but reading from it returns 0xffffff. That results in the
    ref_start and ref_stop value being the same, so the delta is zero
    which causes the divide by zero later in the calculation.
    
    The conditional (!hpet && !ref_start && !ref_stop) which sanity checks
    the calibration reference values doesn't really make sense. If the
    refs are null, but hpet is on, we still want to break out.
    
    The div by zero would be possible to trigger by chance if both reads
    from the hardware provided the exact same value (due to hardware
    wrapping).
    
    So checking if both the ref values are the same should handle if we
    don't have hardware (both null) or if they are the same value (either by
    invalid hardware, or by chance), avoiding the div by zero issue.
    
    [ tglx: Applied the same fix to native_calibrate_tsc() where this
            check was copied from ]
    
    Reported-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    LKML-Reference: <1295024788-15619-1-git-send-email-johnstul@us.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 463901efdba4..ae09f970e626 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -464,7 +464,7 @@ unsigned long native_calibrate_tsc(void)
 		tsc_pit_min = min(tsc_pit_min, tsc_pit_khz);
 
 		/* hpet or pmtimer available ? */
-		if (!hpet && !ref1 && !ref2)
+		if (ref1 == ref2)
 			continue;
 
 		/* Check, whether the sampling was disturbed by an SMI */
@@ -935,7 +935,7 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 	tsc_stop = tsc_read_refs(&ref_stop, hpet);
 
 	/* hpet or pmtimer available ? */
-	if (!hpet && !ref_start && !ref_stop)
+	if (ref_start == ref_stop)
 		goto out;
 
 	/* Check, whether the sampling was disturbed by an SMI */

commit 16ee8db6a93ffbc021132599f33288613f042c3d
Merge: 5943a268002f fa36e956c502
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 11 11:11:46 2011 -0800

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Fix Moorestown VRTC fixmap placement
      x86/gpio: Implement x86 gpio_to_irq convert function
      x86, UV: Fix APICID shift for Westmere processors
      x86: Use PCI method for enabling AMD extended config space before MSR method
      x86: tsc: Prevent delayed init if initial tsc calibration failed
      x86, lapic-timer: Increase the max_delta to 31 bits
      x86: Fix sparse non-ANSI function warnings in smpboot.c
      x86, numa: Fix CONFIG_DEBUG_PER_CPU_MAPS without NUMA emulation
      x86, AMD, PCI: Add AMD northbridge PCI device id for CPU families 12h and 14h
      x86, numa: Fix cpu to node mapping for sparse node ids
      x86, numa: Fake node-to-cpumask for NUMA emulation
      x86, numa: Fake apicid and pxm mappings for NUMA emulation
      x86, numa: Avoid compiling NUMA emulation functions without CONFIG_NUMA_EMU
      x86, numa: Reduce minimum fake node size to 32M
    
    Fix up trivial conflict in arch/x86/kernel/apic/x2apic_uv_x.c

commit 29fe359ca20326e57b25e8545c49ed9ff5e830c7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jan 11 11:40:48 2011 +0100

    x86: tsc: Prevent delayed init if initial tsc calibration failed
    
    commit a8760ec (x86: Check tsc available/disabled in the delayed init
    function) missed to prevent the setup of the delayed init function in
    case the initial tsc calibration failed. This results in the same
    divide by zero bug as we have seen without the tsc disabled check.
    
    Skip the delayed work setup when tsc_khz (the initial calibration
    value) is 0.
    
    Bisected-and-tested-by: Kirill A. Shutemov <kas@openvz.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 356a0d455cf9..463901efdba4 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -965,7 +965,7 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 
 static int __init init_tsc_clocksource(void)
 {
-	if (!cpu_has_tsc || tsc_disabled > 0)
+	if (!cpu_has_tsc || tsc_disabled > 0 || !tsc_khz)
 		return 0;
 
 	if (tsc_clocksource_reliable)

commit 72eb6a791459c87a0340318840bb3bd9252b627b
Merge: 23d69b09b78c 55ee4ef30241
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 7 17:02:58 2011 -0800

    Merge branch 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (30 commits)
      gameport: use this_cpu_read instead of lookup
      x86: udelay: Use this_cpu_read to avoid address calculation
      x86: Use this_cpu_inc_return for nmi counter
      x86: Replace uses of current_cpu_data with this_cpu ops
      x86: Use this_cpu_ops to optimize code
      vmstat: User per cpu atomics to avoid interrupt disable / enable
      irq_work: Use per cpu atomics instead of regular atomics
      cpuops: Use cmpxchg for xchg to avoid lock semantics
      x86: this_cpu_cmpxchg and this_cpu_xchg operations
      percpu: Generic this_cpu_cmpxchg() and this_cpu_xchg support
      percpu,x86: relocate this_cpu_add_return() and friends
      connector: Use this_cpu operations
      xen: Use this_cpu_inc_return
      taskstats: Use this_cpu_ops
      random: Use this_cpu_inc_return
      fs: Use this_cpu_inc_return in buffer.c
      highmem: Use this_cpu_xx_return() operations
      vmstat: Use this_cpu_inc_return for vm statistics
      x86: Support for this_cpu_add, sub, dec, inc_return
      percpu: Generic support for this_cpu_add, sub, dec, inc_return
      ...
    
    Fixed up conflicts: in arch/x86/kernel/{apic/nmi.c, apic/x2apic_uv_x.c, process.c}
    as per Tejun.

commit 0a3aee0da4402aa19b66e458038533c896fb80c6
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Dec 18 16:28:55 2010 +0100

    x86: Use this_cpu_ops to optimize code
    
    Go through x86 code and replace __get_cpu_var and get_cpu_var
    instances that refer to a scalar and are not used for address
    determinations.
    
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 0c40d8b72416..acb08dd7bb57 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -659,7 +659,7 @@ void restore_sched_clock_state(void)
 
 	local_irq_save(flags);
 
-	__get_cpu_var(cyc2ns_offset) = 0;
+	__this_cpu_write(cyc2ns_offset, 0);
 	offset = cyc2ns_suspend - sched_clock();
 
 	for_each_possible_cpu(cpu)

commit a8760eca6cf60ed303ad494ef45901f63165d2c8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Dec 13 11:28:02 2010 +0100

    x86: Check tsc available/disabled in the delayed init function
    
    The delayed TSC init function does not check whether the system has no
    TSC or TSC is disabled at the kernel command line, which results in a
    crash in the work queue based extended calibration due to division by
    zero because the basic calibration never happened.
    
    Add the missing checks and do not touch TSC when not available or
    disabled.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <johnstul@us.ibm.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index dc1393e7cbfb..356a0d455cf9 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -965,6 +965,9 @@ static void tsc_refine_calibration_work(struct work_struct *work)
 
 static int __init init_tsc_clocksource(void)
 {
+	if (!cpu_has_tsc || tsc_disabled > 0)
+		return 0;
+
 	if (tsc_clocksource_reliable)
 		clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
 	/* lower the rating if we already know its unstable: */

commit 08ec0c58fb8a05d3191d5cb6f5d6f81adb419798
Author: John Stultz <johnstul@us.ibm.com>
Date:   Tue Jul 27 17:00:00 2010 -0700

    x86: Improve TSC calibration using a delayed workqueue
    
    Boot to boot the TSC calibration may vary by quite a large amount.
    
    While normal variance of 50-100ppm can easily be seen, the quick
    calibration code only requires 500ppm accuracy, which is the limit
    of what NTP can correct for.
    
    This can cause problems for systems being used as NTP servers, as
    every time they reboot it can take hours for them to calculate the
    new drift error caused by the calibration.
    
    The classic trade-off here is calibration accuracy vs slow boot times,
    as during the calibration nothing else can run.
    
    This patch uses a delayed workqueue  to calibrate the TSC over the
    period of a second. This allows very accurate calibration (in my
    tests only varying by 1khz or 0.4ppm boot to boot). Additionally this
    refined calibration step does not block the boot process, and only
    delays the TSC clocksoure registration by a few seconds in early boot.
    If the refined calibration strays 1% from the early boot calibration
    value, the system will fall back to already calculated early boot
    calibration.
    
    Credit to Andi Kleen who suggested using a timer quite awhile back,
    but I dismissed it thinking the timer calibration would be done after
    the clocksource was registered (which would break things). Forgive
    me for my short-sightedness.
    
    This patch has worked very well in my testing, but TSC hardware is
    quite varied so it would probably be good to get some extended
    testing, possibly pushing inclusion out to 2.6.39.
    
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    LKML-Reference: <1289003985-29060-1-git-send-email-johnstul@us.ibm.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: Ingo Molnar <mingo@elte.hu>
    CC: Martin Schwidefsky <schwidefsky@de.ibm.com>
    CC: Clark Williams <williams@redhat.com>
    CC: Andi Kleen <andi@firstfloor.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index bb64beb301d9..dc1393e7cbfb 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -888,7 +888,82 @@ __cpuinit int unsynchronized_tsc(void)
 	return 0;
 }
 
-static void __init init_tsc_clocksource(void)
+
+static void tsc_refine_calibration_work(struct work_struct *work);
+static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
+/**
+ * tsc_refine_calibration_work - Further refine tsc freq calibration
+ * @work - ignored.
+ *
+ * This functions uses delayed work over a period of a
+ * second to further refine the TSC freq value. Since this is
+ * timer based, instead of loop based, we don't block the boot
+ * process while this longer calibration is done.
+ *
+ * If there are any calibration anomolies (too many SMIs, etc),
+ * or the refined calibration is off by 1% of the fast early
+ * calibration, we throw out the new calibration and use the
+ * early calibration.
+ */
+static void tsc_refine_calibration_work(struct work_struct *work)
+{
+	static u64 tsc_start = -1, ref_start;
+	static int hpet;
+	u64 tsc_stop, ref_stop, delta;
+	unsigned long freq;
+
+	/* Don't bother refining TSC on unstable systems */
+	if (check_tsc_unstable())
+		goto out;
+
+	/*
+	 * Since the work is started early in boot, we may be
+	 * delayed the first time we expire. So set the workqueue
+	 * again once we know timers are working.
+	 */
+	if (tsc_start == -1) {
+		/*
+		 * Only set hpet once, to avoid mixing hardware
+		 * if the hpet becomes enabled later.
+		 */
+		hpet = is_hpet_enabled();
+		schedule_delayed_work(&tsc_irqwork, HZ);
+		tsc_start = tsc_read_refs(&ref_start, hpet);
+		return;
+	}
+
+	tsc_stop = tsc_read_refs(&ref_stop, hpet);
+
+	/* hpet or pmtimer available ? */
+	if (!hpet && !ref_start && !ref_stop)
+		goto out;
+
+	/* Check, whether the sampling was disturbed by an SMI */
+	if (tsc_start == ULLONG_MAX || tsc_stop == ULLONG_MAX)
+		goto out;
+
+	delta = tsc_stop - tsc_start;
+	delta *= 1000000LL;
+	if (hpet)
+		freq = calc_hpet_ref(delta, ref_start, ref_stop);
+	else
+		freq = calc_pmtimer_ref(delta, ref_start, ref_stop);
+
+	/* Make sure we're within 1% */
+	if (abs(tsc_khz - freq) > tsc_khz/100)
+		goto out;
+
+	tsc_khz = freq;
+	printk(KERN_INFO "Refined TSC clocksource calibration: "
+		"%lu.%03lu MHz.\n", (unsigned long)tsc_khz / 1000,
+					(unsigned long)tsc_khz % 1000);
+
+out:
+	clocksource_register_khz(&clocksource_tsc, tsc_khz);
+}
+
+
+static int __init init_tsc_clocksource(void)
 {
 	if (tsc_clocksource_reliable)
 		clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
@@ -897,8 +972,14 @@ static void __init init_tsc_clocksource(void)
 		clocksource_tsc.rating = 0;
 		clocksource_tsc.flags &= ~CLOCK_SOURCE_IS_CONTINUOUS;
 	}
-	clocksource_register_khz(&clocksource_tsc, tsc_khz);
+	schedule_delayed_work(&tsc_irqwork, 0);
+	return 0;
 }
+/*
+ * We use device_initcall here, to ensure we run after the hpet
+ * is fully initialized, which may occur at fs_initcall time.
+ */
+device_initcall(init_tsc_clocksource);
 
 void __init tsc_init(void)
 {
@@ -952,6 +1033,5 @@ void __init tsc_init(void)
 		mark_tsc_unstable("TSCs unsynchronized");
 
 	check_system_tsc_reliable();
-	init_tsc_clocksource();
 }
 

commit b0f969009f647cd473c5e559aeec9c4229d12f87
Merge: 3561d43fd289 d3b8f889a220
Author: John Stultz <john.stultz@linaro.org>
Date:   Thu Dec 2 16:47:52 2010 -0800

    Merge remote branch 'tip/x86/tsc' into fortglx/2.6.38/tip/x86/tsc
    
    Conflicts:
            Documentation/kernel-parameters.txt

commit 2f0384e5fc4766ad909597547d0e2b716c036755
Merge: bc4016f48161 5c80cc78de46
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:01:08 2010 -0700

    Merge branch 'x86-amd-nb-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-amd-nb-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, amd_nb: Enable GART support for AMD family 0x15 CPUs
      x86, amd: Use compute unit information to determine thread siblings
      x86, amd: Extract compute unit information for AMD CPUs
      x86, amd: Add support for CPUID topology extension of AMD CPUs
      x86, nmi: Support NMI watchdog on newer AMD CPU families
      x86, mtrr: Assume SYS_CFG[Tom2ForceMemTypeWB] exists on all future AMD CPUs
      x86, k8: Rename k8.[ch] to amd_nb.[ch] and CONFIG_K8_NB to CONFIG_AMD_NB
      x86, k8-gart: Decouple handling of garts and northbridges
      x86, cacheinfo: Fix dependency of AMD L3 CID
      x86, kvm: add new AMD SVM feature bits
      x86, cpu: Fix allowed CPUID bits for KVM guests
      x86, cpu: Update AMD CPUID feature bits
      x86, cpu: Fix renamed, not-yet-shipping AMD CPUID feature bit
      x86, AMD: Remove needless CPU family check (for L3 cache info)
      x86, tsc: Remove CPU frequency calibration on AMD

commit e82b8e4ea4f3dffe6e7939f90e78da675fcc450e
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Mon Oct 4 17:03:20 2010 -0700

    x86: Add IRQ_TIME_ACCOUNTING
    
    This patch adds IRQ_TIME_ACCOUNTING option on x86 and runtime enables it
    when TSC is enabled.
    
    This change just enables fine grained irq time accounting, isn't used yet.
    Following patches use it for different purposes.
    
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1286237003-12406-6-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 26a863a9c2a8..a1c2cd768538 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -104,10 +104,14 @@ int __init notsc_setup(char *str)
 
 __setup("notsc", notsc_setup);
 
+static int no_sched_irq_time;
+
 static int __init tsc_setup(char *str)
 {
 	if (!strcmp(str, "reliable"))
 		tsc_clocksource_reliable = 1;
+	if (!strncmp(str, "noirqtime", 9))
+		no_sched_irq_time = 1;
 	return 1;
 }
 
@@ -801,6 +805,7 @@ void mark_tsc_unstable(char *reason)
 	if (!tsc_unstable) {
 		tsc_unstable = 1;
 		sched_clock_stable = 0;
+		disable_sched_clock_irqtime();
 		printk(KERN_INFO "Marking TSC unstable due to %s\n", reason);
 		/* Change only the rating, when not registered */
 		if (clocksource_tsc.mult)
@@ -987,6 +992,9 @@ void __init tsc_init(void)
 	/* now allow native_sched_clock() to use rdtsc */
 	tsc_disabled = 0;
 
+	if (!no_sched_irq_time)
+		enable_sched_clock_irqtime();
+
 	lpj = ((u64)tsc_khz * 1000);
 	do_div(lpj, HZ);
 	lpj_fine = lpj;

commit 86ffb08519d6fe1e9c4fe5a110ffa5fcb868cb1c
Merge: 23ac4ae827e6 aeb9c7d61826
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Oct 1 16:18:11 2010 -0700

    Merge remote branch 'origin/x86/cpu' into x86/amd-nb

commit 5ee5e97ee9bca919af11c562beeaf61741ad33f1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 10 22:32:53 2010 +0200

    x86, tsc: Fix a preemption leak in restore_sched_clock_state()
    
    A real life genuine preemption leak..
    
    Reported-and-tested-by: Jeff Chua <jeff.chua.linux@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index d632934cb638..26a863a9c2a8 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -655,7 +655,7 @@ void restore_sched_clock_state(void)
 
 	local_irq_save(flags);
 
-	get_cpu_var(cyc2ns_offset) = 0;
+	__get_cpu_var(cyc2ns_offset) = 0;
 	offset = cyc2ns_suspend - sched_clock();
 
 	for_each_possible_cpu(cpu)

commit acf01734b1747b1ec4be6f159aff579ea5f7f8e2
Author: Borislav Petkov <bp@amd64.org>
Date:   Wed Aug 25 18:28:23 2010 +0200

    x86, tsc: Remove CPU frequency calibration on AMD
    
    6b37f5a20c0e5c334c010a587058354215433e92 introduced the CPU frequency
    calibration code for AMD CPUs whose TSCs didn't increment with the
    core's P0 frequency. From F10h, revB onward, however, the TSC increment
    rate is denoted by MSRC001_0015[24] and when this bit is set (which
    should be done by the BIOS) the TSC increments with the P0 frequency
    so the calibration is not needed and booting can be a couple of mcecs
    faster on those machines.
    
    Besides, there should be virtually no machines out there which don't
    have this bit set, therefore this calibration can be safely removed. It
    is a shaky hack anyway since it assumes implicitly that the core is in
    P0 when BIOS hands off to the OS, which might not always be the case.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <20100825162823.GE26438@aftab>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index ce8e50239332..13b6a6cc77f2 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -854,60 +854,6 @@ static void __init init_tsc_clocksource(void)
 	clocksource_register_khz(&clocksource_tsc, tsc_khz);
 }
 
-#ifdef CONFIG_X86_64
-/*
- * calibrate_cpu is used on systems with fixed rate TSCs to determine
- * processor frequency
- */
-#define TICK_COUNT 100000000
-static unsigned long __init calibrate_cpu(void)
-{
-	int tsc_start, tsc_now;
-	int i, no_ctr_free;
-	unsigned long evntsel3 = 0, pmc3 = 0, pmc_now = 0;
-	unsigned long flags;
-
-	for (i = 0; i < 4; i++)
-		if (avail_to_resrv_perfctr_nmi_bit(i))
-			break;
-	no_ctr_free = (i == 4);
-	if (no_ctr_free) {
-		WARN(1, KERN_WARNING "Warning: AMD perfctrs busy ... "
-		     "cpu_khz value may be incorrect.\n");
-		i = 3;
-		rdmsrl(MSR_K7_EVNTSEL3, evntsel3);
-		wrmsrl(MSR_K7_EVNTSEL3, 0);
-		rdmsrl(MSR_K7_PERFCTR3, pmc3);
-	} else {
-		reserve_perfctr_nmi(MSR_K7_PERFCTR0 + i);
-		reserve_evntsel_nmi(MSR_K7_EVNTSEL0 + i);
-	}
-	local_irq_save(flags);
-	/* start measuring cycles, incrementing from 0 */
-	wrmsrl(MSR_K7_PERFCTR0 + i, 0);
-	wrmsrl(MSR_K7_EVNTSEL0 + i, 1 << 22 | 3 << 16 | 0x76);
-	rdtscl(tsc_start);
-	do {
-		rdmsrl(MSR_K7_PERFCTR0 + i, pmc_now);
-		tsc_now = get_cycles();
-	} while ((tsc_now - tsc_start) < TICK_COUNT);
-
-	local_irq_restore(flags);
-	if (no_ctr_free) {
-		wrmsrl(MSR_K7_EVNTSEL3, 0);
-		wrmsrl(MSR_K7_PERFCTR3, pmc3);
-		wrmsrl(MSR_K7_EVNTSEL3, evntsel3);
-	} else {
-		release_perfctr_nmi(MSR_K7_PERFCTR0 + i);
-		release_evntsel_nmi(MSR_K7_EVNTSEL0 + i);
-	}
-
-	return pmc_now * tsc_khz / (tsc_now - tsc_start);
-}
-#else
-static inline unsigned long calibrate_cpu(void) { return cpu_khz; }
-#endif
-
 void __init tsc_init(void)
 {
 	u64 lpj;
@@ -926,10 +872,6 @@ void __init tsc_init(void)
 		return;
 	}
 
-	if (cpu_has(&boot_cpu_data, X86_FEATURE_CONSTANT_TSC) &&
-			(boot_cpu_data.x86_vendor == X86_VENDOR_AMD))
-		cpu_khz = calibrate_cpu();
-
 	printk("Detected %lu.%03lu MHz processor.\n",
 			(unsigned long)cpu_khz / 1000,
 			(unsigned long)cpu_khz % 1000);

commit cd7240c0b900eb6d690ccee088a6c9b46dae815a
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Aug 19 17:03:38 2010 -0700

    x86, tsc, sched: Recompute cyc2ns_offset's during resume from sleep states
    
    TSC's get reset after suspend/resume (even on cpu's with invariant TSC
    which runs at a constant rate across ACPI P-, C- and T-states). And in
    some systems BIOS seem to reinit TSC to arbitrary large value (still
    sync'd across cpu's) during resume.
    
    This leads to a scenario of scheduler rq->clock (sched_clock_cpu()) less
    than rq->age_stamp (introduced in 2.6.32). This leads to a big value
    returned by scale_rt_power() and the resulting big group power set by the
    update_group_power() is causing improper load balancing between busy and
    idle cpu's after suspend/resume.
    
    This resulted in multi-threaded workloads (like kernel-compilation) go
    slower after suspend/resume cycle on core i5 laptops.
    
    Fix this by recomputing cyc2ns_offset's during resume, so that
    sched_clock() continues from the point where it was left off during
    suspend.
    
    Reported-by: Florian Pritz <flo@xssn.at>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: <stable@kernel.org> # [v2.6.32+]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1282262618.2675.24.camel@sbsiddha-MOBL3.sc.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index ce8e50239332..d632934cb638 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -626,6 +626,44 @@ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 	local_irq_restore(flags);
 }
 
+static unsigned long long cyc2ns_suspend;
+
+void save_sched_clock_state(void)
+{
+	if (!sched_clock_stable)
+		return;
+
+	cyc2ns_suspend = sched_clock();
+}
+
+/*
+ * Even on processors with invariant TSC, TSC gets reset in some the
+ * ACPI system sleep states. And in some systems BIOS seem to reinit TSC to
+ * arbitrary value (still sync'd across cpu's) during resume from such sleep
+ * states. To cope up with this, recompute the cyc2ns_offset for each cpu so
+ * that sched_clock() continues from the point where it was left off during
+ * suspend.
+ */
+void restore_sched_clock_state(void)
+{
+	unsigned long long offset;
+	unsigned long flags;
+	int cpu;
+
+	if (!sched_clock_stable)
+		return;
+
+	local_irq_save(flags);
+
+	get_cpu_var(cyc2ns_offset) = 0;
+	offset = cyc2ns_suspend - sched_clock();
+
+	for_each_possible_cpu(cpu)
+		per_cpu(cyc2ns_offset, cpu) = offset;
+
+	local_irq_restore(flags);
+}
+
 #ifdef CONFIG_CPU_FREQ
 
 /* Frequency scaling support. Adjust the TSC based timer when the cpu frequency

commit f12a15be63d1de9a35971f35f06b73088fa25c3a
Author: John Stultz <johnstul@us.ibm.com>
Date:   Tue Jul 13 17:56:27 2010 -0700

    x86: Convert common clocksources to use clocksource_register_hz/khz
    
    This converts the most common of the x86 clocksources over to use
    clocksource_register_hz/khz.
    
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    LKML-Reference: <1279068988-21864-11-git-send-email-johnstul@us.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 9faf91ae1841..ce8e50239332 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -751,7 +751,6 @@ static struct clocksource clocksource_tsc = {
 	.read                   = read_tsc,
 	.resume			= resume_tsc,
 	.mask                   = CLOCKSOURCE_MASK(64),
-	.shift                  = 22,
 	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
 				  CLOCK_SOURCE_MUST_VERIFY,
 #ifdef CONFIG_X86_64
@@ -845,8 +844,6 @@ __cpuinit int unsynchronized_tsc(void)
 
 static void __init init_tsc_clocksource(void)
 {
-	clocksource_tsc.mult = clocksource_khz2mult(tsc_khz,
-			clocksource_tsc.shift);
 	if (tsc_clocksource_reliable)
 		clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
 	/* lower the rating if we already know its unstable: */
@@ -854,7 +851,7 @@ static void __init init_tsc_clocksource(void)
 		clocksource_tsc.rating = 0;
 		clocksource_tsc.flags &= ~CLOCK_SOURCE_IS_CONTINUOUS;
 	}
-	clocksource_register(&clocksource_tsc);
+	clocksource_register_khz(&clocksource_tsc, tsc_khz);
 }
 
 #ifdef CONFIG_X86_64

commit 318ae2edc3b29216abd8a2510f3f80b764f06858
Merge: 25cf84cf377c 3e58974027b0
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Mar 8 16:55:37 2010 +0100

    Merge branch 'for-next' into for-linus
    
    Conflicts:
            Documentation/filesystems/proc.txt
            arch/arm/mach-u300/include/mach/debug-macro.S
            drivers/net/qlge/qlge_ethtool.c
            drivers/net/qlge/qlge_main.c
            drivers/net/typhoon.c

commit e56425b135a8892d1e71ad5bb605d12c10efeb32
Merge: 786f8ba2e944 6622e670b26f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 1 08:48:25 2010 -0800

    Merge branch 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      posix-timers.c: Don't export local functions
      clocksource: start CMT at clocksource resume
      clocksource: add suspend callback
      clocksource: add argument to resume callback
      ntp: Cleanup xtime references in ntp.c
      ntp: Make time_esterror and time_maxerror static

commit 3ad2f3fbb961429d2aa627465ae4829758bc7e07
Author: Daniel Mack <daniel@caiaq.de>
Date:   Wed Feb 3 08:01:28 2010 +0800

    tree-wide: Assorted spelling fixes
    
    In particular, several occurances of funny versions of 'success',
    'unknown', 'therefore', 'acknowledge', 'argument', 'achieve', 'address',
    'beginning', 'desirable', 'separate' and 'necessary' are fixed.
    
    Signed-off-by: Daniel Mack <daniel@caiaq.de>
    Cc: Joe Perches <joe@perches.com>
    Cc: Junio C Hamano <gitster@pobox.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 597683aa5ba0..dec8f68e3eda 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -50,7 +50,7 @@ u64 native_sched_clock(void)
 	 *   unstable. We do this because unlike Time Of Day,
 	 *   the scheduler clock tolerates small errors and it's
 	 *   very important for it to be as fast as the platform
-	 *   can achive it. )
+	 *   can achieve it. )
 	 */
 	if (unlikely(tsc_disabled)) {
 		/* No locking but a rare wrong value is not a big deal: */

commit 17622339af2536b32cf29699ddd4ba0fe79a61d5
Author: Magnus Damm <damm@opensource.se>
Date:   Tue Feb 2 14:41:39 2010 -0800

    clocksource: add argument to resume callback
    
    Pass the clocksource as an argument to the clocksource resume callback.
    Needed so we can point out which CMT channel the sh_cmt.c driver shall
    resume.
    
    Signed-off-by: Magnus Damm <damm@opensource.se>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 597683aa5ba0..9eeb9be26aa4 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -740,7 +740,7 @@ static cycle_t __vsyscall_fn vread_tsc(void)
 }
 #endif
 
-static void resume_tsc(void)
+static void resume_tsc(struct clocksource *cs)
 {
 	clocksource_tsc.cycle_last = 0;
 }

commit 00097c4fdf117d9845d772f571a987ae95523f8c
Author: Thadeu Lima de Souza Cascardo <cascardo@holoscopio.com>
Date:   Sun Jan 17 19:44:44 2010 -0200

    x86, trivial: Fix grammo in tsc comment about Geode TSC reliability
    
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@holoscopio.com>
    Cc: marcelo@kvack.org
    Cc: dilinger@collabora.co.uk
    Cc: trivial@kernel.org
    LKML-Reference: <1263764685-9871-1-git-send-email-cascardo@holoscopio.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 597683aa5ba0..23066ecf12fa 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -806,7 +806,7 @@ static void __init check_system_tsc_reliable(void)
 	unsigned long res_low, res_high;
 
 	rdmsr_safe(MSR_GEODE_BUSCONT_CONF0, &res_low, &res_high);
-	/* Geode_LX - the OLPC CPU has a possibly a very reliable TSC */
+	/* Geode_LX - the OLPC CPU has a very reliable TSC */
 	if (res_low & RTSC_SUSP)
 		tsc_clocksource_reliable = 1;
 #endif

commit 6c56ccecf05fafe100ab4ea94f6fccbf5ff00db7
Author: Pallipadi, Venkatesh <venkatesh.pallipadi@intel.com>
Date:   Thu Dec 17 12:27:02 2009 -0800

    x86: Reenable TSC sync check at boot, even with NONSTOP_TSC
    
    Commit 83ce4009 did the following change
    If the TSC is constant and non-stop, also set it reliable.
    
    But, there seems to be few systems that will end up with TSC warp across
    sockets, depending on how the cpus come out of reset. Skipping TSC sync
    test on such systems may result in time inconsistency later.
    
    So, reenable TSC sync test even on constant and non-stop TSC systems.
    Set, sched_clock_stable to 1 by default and reset it in
    mark_tsc_unstable, if TSC sync fails.
    
    This change still gives perf benefit mentioned in 83ce4009 for systems
    where TSC is reliable.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <20091217202702.GA18015@linux-os.sc.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index cd982f48e23e..597683aa5ba0 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -763,6 +763,7 @@ void mark_tsc_unstable(char *reason)
 {
 	if (!tsc_unstable) {
 		tsc_unstable = 1;
+		sched_clock_stable = 0;
 		printk(KERN_INFO "Marking TSC unstable due to %s\n", reason);
 		/* Change only the rating, when not registered */
 		if (clocksource_tsc.mult)

commit bfefb7a0c6e08736f2d5917c468467f134bf28bb
Merge: 8d0cc631f6dd 78f28b7c5553
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Sep 20 20:24:58 2009 +0200

    Merge branch 'linus' into x86/urgent
    
    Merge reason: Bring in changes that the next patch will depend on.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 878f4f533e5b4498215e67e0f886b0fc81417f5e
Author: Felipe Contreras <felipe.contreras@gmail.com>
Date:   Thu Sep 17 00:38:38 2009 +0300

    x86: Trivial whitespace cleanups
    
    Signed-off-by: Felipe Contreras <felipe.contreras@gmail.com>
    Cc: Vegard Nossum <vegardno@ifi.uio.no>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Alok N Kataria <akataria@vmware.com>
    Cc: "Tan Wei Chong" <wei.chong.tan@intel.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Bob Moore <robert.moore@intel.com>
    LKML-Reference: <1253137123-18047-2-git-send-email-felipe.contreras@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 71f4368b357e..8a2fc1123690 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -670,7 +670,7 @@ static int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
 	if ((val == CPUFREQ_PRECHANGE  && freq->old < freq->new) ||
 			(val == CPUFREQ_POSTCHANGE && freq->old > freq->new) ||
 			(val == CPUFREQ_RESUMECHANGE)) {
-		*lpj = 	cpufreq_scale(loops_per_jiffy_ref, ref_freq, freq->new);
+		*lpj = cpufreq_scale(loops_per_jiffy_ref, ref_freq, freq->new);
 
 		tsc_khz = cpufreq_scale(tsc_khz_ref, ref_freq, freq->new);
 		if (!(freq->flags & CPUFREQ_CONST_LOOPS))

commit 78f28b7c555359c67c2a0d23f7436e915329421e
Merge: 3240a77b515f 7bd867dfb4e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 18 14:05:47 2009 -0700

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (38 commits)
      x86: Move get/set_wallclock to x86_platform_ops
      x86: platform: Fix section annotations
      x86: apic namespace cleanup
      x86: Distangle ioapic and i8259
      x86: Add Moorestown early detection
      x86: Add hardware_subarch ID for Moorestown
      x86: Add early platform detection
      x86: Move tsc_init to late_time_init
      x86: Move tsc_calibration to x86_init_ops
      x86: Replace the now identical time_32/64.c by time.c
      x86: time_32/64.c unify profile_pc
      x86: Move calibrate_cpu to tsc.c
      x86: Make timer setup and global variables the same in time_32/64.c
      x86: Remove mca bus ifdef from timer interrupt
      x86: Simplify timer_ack magic in time_32.c
      x86: Prepare unification of time_32/64.c
      x86: Remove do_timer hook
      x86: Add timer_init to x86_init_ops
      x86: Move percpu clockevents setup to x86_init_ops
      x86: Move xen_post_allocator_init into xen_pagetable_setup_done
      ...
    
    Fix up conflicts in arch/x86/include/asm/io_apic.h

commit 2d826404f0bdcac2a4dd7e3c446b70d6a3b63b78
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 17:06:25 2009 +0200

    x86: Move tsc_calibration to x86_init_ops
    
    TSC calibration is modified by the vmware hypervisor and paravirt by
    separate means. Moorestown wants to add its own calibration routine as
    well. So make calibrate_tsc a proper x86_init_ops function and
    override it by paravirt or by the early setup of the vmware
    hypervisor.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 97a0bcbad100..9917632a8b49 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -18,6 +18,7 @@
 #include <asm/delay.h>
 #include <asm/hypervisor.h>
 #include <asm/nmi.h>
+#include <asm/x86_init.h>
 
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
@@ -401,15 +402,9 @@ unsigned long native_calibrate_tsc(void)
 {
 	u64 tsc1, tsc2, delta, ref1, ref2;
 	unsigned long tsc_pit_min = ULONG_MAX, tsc_ref_min = ULONG_MAX;
-	unsigned long flags, latch, ms, fast_calibrate, hv_tsc_khz;
+	unsigned long flags, latch, ms, fast_calibrate;
 	int hpet = is_hpet_enabled(), i, loopmin;
 
-	hv_tsc_khz = get_hypervisor_tsc_freq();
-	if (hv_tsc_khz) {
-		printk(KERN_INFO "TSC: Frequency read from the hypervisor\n");
-		return hv_tsc_khz;
-	}
-
 	local_irq_save(flags);
 	fast_calibrate = quick_pit_calibrate();
 	local_irq_restore(flags);
@@ -567,7 +562,7 @@ int recalibrate_cpu_khz(void)
 	unsigned long cpu_khz_old = cpu_khz;
 
 	if (cpu_has_tsc) {
-		tsc_khz = calibrate_tsc();
+		tsc_khz = x86_platform.calibrate_tsc();
 		cpu_khz = tsc_khz;
 		cpu_data(0).loops_per_jiffy =
 			cpufreq_scale(cpu_data(0).loops_per_jiffy,
@@ -917,7 +912,7 @@ void __init tsc_init(void)
 	if (!cpu_has_tsc)
 		return;
 
-	tsc_khz = calibrate_tsc();
+	tsc_khz = x86_platform.calibrate_tsc();
 	cpu_khz = tsc_khz;
 
 	if (!tsc_khz) {

commit 08047c4f1740c7cee75d58e2919d48c09f951649
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 16:27:41 2009 +0200

    x86: Move calibrate_cpu to tsc.c
    
    Move the code where it's only user is. Also we need to look whether
    this hardwired hackery might interfere with perfcounters.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 652bc214eebf..97a0bcbad100 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -17,6 +17,7 @@
 #include <asm/time.h>
 #include <asm/delay.h>
 #include <asm/hypervisor.h>
+#include <asm/nmi.h>
 
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
@@ -852,6 +853,60 @@ static void __init init_tsc_clocksource(void)
 	clocksource_register(&clocksource_tsc);
 }
 
+#ifdef CONFIG_X86_64
+/*
+ * calibrate_cpu is used on systems with fixed rate TSCs to determine
+ * processor frequency
+ */
+#define TICK_COUNT 100000000
+static unsigned long __init calibrate_cpu(void)
+{
+	int tsc_start, tsc_now;
+	int i, no_ctr_free;
+	unsigned long evntsel3 = 0, pmc3 = 0, pmc_now = 0;
+	unsigned long flags;
+
+	for (i = 0; i < 4; i++)
+		if (avail_to_resrv_perfctr_nmi_bit(i))
+			break;
+	no_ctr_free = (i == 4);
+	if (no_ctr_free) {
+		WARN(1, KERN_WARNING "Warning: AMD perfctrs busy ... "
+		     "cpu_khz value may be incorrect.\n");
+		i = 3;
+		rdmsrl(MSR_K7_EVNTSEL3, evntsel3);
+		wrmsrl(MSR_K7_EVNTSEL3, 0);
+		rdmsrl(MSR_K7_PERFCTR3, pmc3);
+	} else {
+		reserve_perfctr_nmi(MSR_K7_PERFCTR0 + i);
+		reserve_evntsel_nmi(MSR_K7_EVNTSEL0 + i);
+	}
+	local_irq_save(flags);
+	/* start measuring cycles, incrementing from 0 */
+	wrmsrl(MSR_K7_PERFCTR0 + i, 0);
+	wrmsrl(MSR_K7_EVNTSEL0 + i, 1 << 22 | 3 << 16 | 0x76);
+	rdtscl(tsc_start);
+	do {
+		rdmsrl(MSR_K7_PERFCTR0 + i, pmc_now);
+		tsc_now = get_cycles();
+	} while ((tsc_now - tsc_start) < TICK_COUNT);
+
+	local_irq_restore(flags);
+	if (no_ctr_free) {
+		wrmsrl(MSR_K7_EVNTSEL3, 0);
+		wrmsrl(MSR_K7_PERFCTR3, pmc3);
+		wrmsrl(MSR_K7_EVNTSEL3, evntsel3);
+	} else {
+		release_perfctr_nmi(MSR_K7_PERFCTR0 + i);
+		release_evntsel_nmi(MSR_K7_EVNTSEL0 + i);
+	}
+
+	return pmc_now * tsc_khz / (tsc_now - tsc_start);
+}
+#else
+static inline unsigned long calibrate_cpu(void) { return cpu_khz; }
+#endif
+
 void __init tsc_init(void)
 {
 	u64 lpj;
@@ -870,11 +925,9 @@ void __init tsc_init(void)
 		return;
 	}
 
-#ifdef CONFIG_X86_64
 	if (cpu_has(&boot_cpu_data, X86_FEATURE_CONSTANT_TSC) &&
 			(boot_cpu_data.x86_vendor == X86_VENDOR_AMD))
 		cpu_khz = calibrate_cpu();
-#endif
 
 	printk("Detected %lu.%03lu MHz processor.\n",
 			(unsigned long)cpu_khz / 1000,

commit 845b3944bbdf9e9247849bf037f27ff3a3f26d87
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 19 15:37:03 2009 +0200

    x86: Add timer_init to x86_init_ops
    
    The timer init code is convoluted with several quirks and the paravirt
    timer chooser. Figuring out which code path is actually taken is not
    for the faint hearted.
    
    Move the numaq TSC quirk to tsc_pre_init x86_init_ops function and
    replace the paravirt time chooser and the remaining x86 quirk with a
    simple x86_init_ops function.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 71f4368b357e..652bc214eebf 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -857,6 +857,8 @@ void __init tsc_init(void)
 	u64 lpj;
 	int cpu;
 
+	x86_init.timers.tsc_pre_init();
+
 	if (!cpu_has_tsc)
 		return;
 

commit d3b8f889a220aed825accc28eb64ce283a0d51ac
Author: john stultz <johnstul@us.ibm.com>
Date:   Mon Aug 17 16:40:47 2009 -0700

    x86: Make tsc=reliable override boot time stability checks
    
    This patch makes the tsc=reliable option disable the boot time
    stability checks. Currently the option only disables the runtime
    watchdog checks. This change allows folks who want to override the
    boot time TSC stability checks and use the TSC when the system would
    otherwise disqualify it.
    
    There still are some situations that the TSC will be disqualified,
    such as cpufreq scaling. But these are situations where the box will
    hang if allowed.
    
    Patch also includes a fix for an issue found by Thomas Gleixner, where
    the TSC disqualification message wouldn't be printed after a call to
    unsynchronized_tsc().
    
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: akataria@vmware.com
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    LKML-Reference: <1250552447.7212.92.camel@localhost.localdomain>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 71f4368b357e..648fb269e5d1 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -825,6 +825,9 @@ __cpuinit int unsynchronized_tsc(void)
 
 	if (boot_cpu_has(X86_FEATURE_CONSTANT_TSC))
 		return 0;
+
+	if (tsc_clocksource_reliable)
+		return 0;
 	/*
 	 * Intel systems are normally all synchronized.
 	 * Exceptions must mark TSC as unstable:
@@ -832,10 +835,10 @@ __cpuinit int unsynchronized_tsc(void)
 	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL) {
 		/* assume multi socket systems are not synchronized: */
 		if (num_possible_cpus() > 1)
-			tsc_unstable = 1;
+			return 1;
 	}
 
-	return tsc_unstable;
+	return 0;
 }
 
 static void __init init_tsc_clocksource(void)

commit 7285dd7fd375763bfb8ab1ac9cf3f1206f503c16
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Aug 28 20:25:24 2009 +0200

    clocksource: Resolve cpu hotplug dead lock with TSC unstable
    
    Martin Schwidefsky analyzed it:
    To register a clocksource the clocksource_mutex is acquired and if
    necessary timekeeping_notify is called to install the clocksource as
    the timekeeper clock. timekeeping_notify uses stop_machine which needs
    to take cpu_add_remove_lock mutex.
    Starting a new cpu is done with the cpu_add_remove_lock mutex held.
    native_cpu_up checks the tsc of the new cpu and if the tsc is no good
    clocksource_change_rating is called. Which needs the clocksource_mutex
    and the deadlock is complete.
    
    The solution is to replace the TSC via the clocksource watchdog
    mechanism. Mark the TSC as unstable and schedule the watchdog work so
    it gets removed in the watchdog thread context.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <new-submission>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: John Stultz <johnstul@us.ibm.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 968425422c46..fc3672a303d6 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -767,12 +767,14 @@ void mark_tsc_unstable(char *reason)
 {
 	if (!tsc_unstable) {
 		tsc_unstable = 1;
-		printk("Marking TSC unstable due to %s\n", reason);
+		printk(KERN_INFO "Marking TSC unstable due to %s\n", reason);
 		/* Change only the rating, when not registered */
 		if (clocksource_tsc.mult)
-			clocksource_change_rating(&clocksource_tsc, 0);
-		else
+			clocksource_mark_unstable(&clocksource_tsc);
+		else {
+			clocksource_tsc.flags |= CLOCK_SOURCE_UNSTABLE;
 			clocksource_tsc.rating = 0;
+		}
 	}
 }
 

commit 1be396794897f80bfc8774719ba60309a9e3d374
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Aug 14 15:47:20 2009 +0200

    timekeeping: Move reset of cycle_last for tsc clocksource to tsc
    
    change_clocksource resets the cycle_last value to zero then sets it to
    a value read from the clocksource. The reset to zero is required only
    for the TSC clocksource to make the read_tsc function work after a
    resume. The reason is that the TSC read function uses cycle_last to
    detect backwards going TSCs. In the resume case cycle_last contains
    the TSC value from the last update before the suspend. On resume the
    TSC starts counting from 0 again and would trip over the cycle_last
    comparison.
    
    This is subtle and surprising. Move the reset to a resume function in
    the tsc code.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: John Stultz <johnstul@us.ibm.com>
    Cc: Daniel Walker <dwalker@fifo99.com>
    LKML-Reference: <20090814134808.142191175@de.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 71f4368b357e..968425422c46 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -744,10 +744,16 @@ static cycle_t __vsyscall_fn vread_tsc(void)
 }
 #endif
 
+static void resume_tsc(void)
+{
+	clocksource_tsc.cycle_last = 0;
+}
+
 static struct clocksource clocksource_tsc = {
 	.name                   = "tsc",
 	.rating                 = 300,
 	.read                   = read_tsc,
+	.resume			= resume_tsc,
 	.mask                   = CLOCKSOURCE_MASK(64),
 	.shift                  = 22,
 	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |

commit b6e61eef4f9f94714ac3ee4a5c96862d9bcd1836
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 31 12:45:41 2009 -0700

    x86: Fix serialization in pit_expect_msb()
    
    Wei Chong Tan reported a fast-PIT-calibration corner-case:
    
    | pit_expect_msb() is vulnerable to SMI disturbance corner case
    | in some platforms which causes /proc/cpuinfo to show wrong
    | CPU MHz value when quick_pit_calibrate() jumps to success
    | section.
    
    I think that the real issue isn't even an SMI - but the fact
    that in the very last iteration of the loop, there's no
    serializing instruction _after_ the last 'rdtsc'. So even in
    the absense of SMI's, we do have a situation where the cycle
    counter was read without proper serialization.
    
    The last check should be done outside the outer loop, since
    _inside_ the outer loop, we'll be testing that the PIT has
    the right MSB value has the right value in the next iteration.
    
    So only the _last_ iteration is special, because that's the one
    that will not check the PIT MSB value any more, and because the
    final 'get_cycles()' isn't serialized.
    
    In other words:
    
     - I'd like to move the PIT MSB check to after the last
       iteration, rather than in every iteration
    
     - I think we should comment on the fact that it's also a
       serializing instruction and so 'fences in' the TSC read.
    
    Here's a suggested replacement.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: "Tan, Wei Chong" <wei.chong.tan@intel.com>
    Tested-by: "Tan, Wei Chong" <wei.chong.tan@intel.com>
    LKML-Reference: <B28277FD4E0F9247A3D55704C440A140D5D683F3@pgsmsx504.gar.corp.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 6e1a368d21d4..71f4368b357e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -275,15 +275,20 @@ static unsigned long pit_calibrate_tsc(u32 latch, unsigned long ms, int loopmin)
  * use the TSC value at the transitions to calculate a pretty
  * good value for the TSC frequencty.
  */
+static inline int pit_verify_msb(unsigned char val)
+{
+	/* Ignore LSB */
+	inb(0x42);
+	return inb(0x42) == val;
+}
+
 static inline int pit_expect_msb(unsigned char val, u64 *tscp, unsigned long *deltap)
 {
 	int count;
 	u64 tsc = 0;
 
 	for (count = 0; count < 50000; count++) {
-		/* Ignore LSB */
-		inb(0x42);
-		if (inb(0x42) != val)
+		if (!pit_verify_msb(val))
 			break;
 		tsc = get_cycles();
 	}
@@ -336,8 +341,7 @@ static unsigned long quick_pit_calibrate(void)
 	 * to do that is to just read back the 16-bit counter
 	 * once from the PIT.
 	 */
-	inb(0x42);
-	inb(0x42);
+	pit_verify_msb(0);
 
 	if (pit_expect_msb(0xff, &tsc, &d1)) {
 		for (i = 1; i <= MAX_QUICK_PIT_ITERATIONS; i++) {
@@ -348,8 +352,19 @@ static unsigned long quick_pit_calibrate(void)
 			 * Iterate until the error is less than 500 ppm
 			 */
 			delta -= tsc;
-			if (d1+d2 < delta >> 11)
-				goto success;
+			if (d1+d2 >= delta >> 11)
+				continue;
+
+			/*
+			 * Check the PIT one more time to verify that
+			 * all TSC reads were stable wrt the PIT.
+			 *
+			 * This also guarantees serialization of the
+			 * last cycle read ('d2') in pit_expect_msb.
+			 */
+			if (!pit_verify_msb(0xfe - i))
+				break;
+			goto success;
 		}
 	}
 	printk("Fast TSC calibration failed\n");

commit 1eb51c33b21ffa3fceb634d1d6bcd6488c79bc26
Merge: b0b7065b64fe 3104bf03a923
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 20 10:57:40 2009 -0700

    Merge branch 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sched: Fix out of scope variable access in sched_slice()
      sched: Hide runqueues from direct refer at source code level
      sched: Remove unneeded __ref tag
      sched, x86: Fix cpufreq + sched_clock() TSC scaling

commit c30938d59e7468259855da91a885b19e8044b5f4
Merge: aa2638a210ab 8e7c25971b15
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 17 09:51:50 2009 -0700

    Merge branch 'fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/davej/cpufreq
    
    * 'fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/davej/cpufreq:
      [CPUFREQ] cpumask: new cpumask operators for arch/x86/kernel/cpu/cpufreq/powernow-k8.c
      [CPUFREQ] cpumask: avoid playing with cpus_allowed in powernow-k8.c
      [CPUFREQ] cpumask: avoid cpumask games in arch/x86/kernel/cpu/cpufreq/speedstep-centrino.c
      [CPUFREQ] cpumask: avoid playing with cpus_allowed in speedstep-ich.c
      [CPUFREQ] powernow-k8: get drv data for correct CPU
      [CPUFREQ] powernow-k8: read P-state from HW
      [CPUFREQ] reduce scope of ACPI_PSS_BIOS_BUG_MSG[]
      [CPUFREQ] Clean up convoluted code in arch/x86/kernel/tsc.c:time_cpufreq_notifier()
      [CPUFREQ] minor correction to cpu-freq documentation
      [CPUFREQ] powernow-k8.c: mess cleanup
      [CPUFREQ] Only set sampling_rate_max deprecated, sampling_rate_min is useful
      [CPUFREQ] powernow-k8: Set transition latency to 1 if ACPI tables export 0
      [CPUFREQ] ondemand: Uncouple minimal sampling rate from HZ in NO_HZ case

commit 84599f8a59e77699f18f06948cea171a349a3f0f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 16 12:34:17 2009 -0700

    sched, x86: Fix cpufreq + sched_clock() TSC scaling
    
    For freqency dependent TSCs we only scale the cycles, we do not account
    for the discrepancy in absolute value.
    
    Our current formula is: time = cycles * mult
    
    (where mult is a function of the cpu-speed on variable tsc machines)
    
    Suppose our current cycle count is 10, and we have a multiplier of 5,
    then our time value would end up being 50.
    
    Now cpufreq comes along and changes the multiplier to say 3 or 7,
    which would result in our time being resp. 30 or 70.
    
    That means that we can observe random jumps in the time value due to
    frequency changes in both fwd and bwd direction.
    
    So what this patch does is change the formula to:
    
      time = cycles * frequency + offset
    
    And we calculate offset so that time_before == time_after, thereby
    ridding us of these jumps in time.
    
    [ Impact: fix/reduce sched_clock() jumps across frequency changing events ]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Chucked-on-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3e1c057e98fe..ef4dac50143f 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -589,22 +589,26 @@ EXPORT_SYMBOL(recalibrate_cpu_khz);
  */
 
 DEFINE_PER_CPU(unsigned long, cyc2ns);
+DEFINE_PER_CPU(unsigned long long, cyc2ns_offset);
 
 static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 {
-	unsigned long long tsc_now, ns_now;
+	unsigned long long tsc_now, ns_now, *offset;
 	unsigned long flags, *scale;
 
 	local_irq_save(flags);
 	sched_clock_idle_sleep_event();
 
 	scale = &per_cpu(cyc2ns, cpu);
+	offset = &per_cpu(cyc2ns_offset, cpu);
 
 	rdtscll(tsc_now);
 	ns_now = __cycles_2_ns(tsc_now);
 
-	if (cpu_khz)
+	if (cpu_khz) {
 		*scale = (NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR)/cpu_khz;
+		*offset = ns_now - (tsc_now * *scale >> CYC2NS_SCALE_FACTOR);
+	}
 
 	sched_clock_idle_wakeup_event(0);
 	local_irq_restore(flags);

commit 08604bd9935dc98fb62ef61d5b7baa7ccc10f8c2
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Jun 16 15:31:12 2009 -0700

    time: move PIT_TICK_RATE to linux/timex.h
    
    PIT_TICK_RATE is currently defined in four architectures, but in three
    different places.  While linux/timex.h is not the perfect place for it, it
    is still a reasonable replacement for those drivers that traditionally use
    asm/timex.h to get CLOCK_TICK_RATE and expect it to be the PIT frequency.
    
    Note that for Alpha, the actual value changed from 1193182UL to 1193180UL.
     This is unlikely to make a difference, and probably can only improve
    accuracy.  There was a discussion on the correct value of CLOCK_TICK_RATE
    a few years ago, after which every existing instance was getting changed
    to 1193182.  According to the specification, it should be
    1193181.818181...
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Dmitry Torokhov <dtor@mail.ru>
    Cc: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3e1c057e98fe..ae3180c506a6 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -9,6 +9,7 @@
 #include <linux/delay.h>
 #include <linux/clocksource.h>
 #include <linux/percpu.h>
+#include <linux/timex.h>
 
 #include <asm/hpet.h>
 #include <asm/timer.h>

commit 931db6a32dbfaad627e89d0524979ce9cb894691
Author: Dave Jones <davej@redhat.com>
Date:   Mon Jun 1 12:29:55 2009 -0400

    [CPUFREQ] Clean up convoluted code in arch/x86/kernel/tsc.c:time_cpufreq_notifier()
    
    Christoph Hellwig noticed the following potential uninitialised use:
    
     > arch/x86/kernel/tsc.c: In function 'time_cpufreq_notifier':
     > arch/x86/kernel/tsc.c:634: warning: 'dummy' may be used uninitialized in this function
     >
     > where we do have CONFIG_SMP set, freq->flags & CPUFREQ_CONST_LOOPS is
     > true and ref_freq is false.
    
    It seems plausable, though the circumstances for hitting it are really low.
    Nearly all SMP capable cpufreq drivers set CPUFREQ_CONST_LOOPS.
    powernow-k8 is really the only exception. The older CPUs were typically
    only ever UP. (powernow-k7 never supported SMP for eg)
    
    It's worth fixing regardless, as it cleans up the code.
    
    Fix possible uninitialized use of dummy, by just removing it,
    and making the setting of lpj more obvious.
    
    Signed-off-by: Dave Jones <davej@redhat.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3e1c057e98fe..3fbd3206eccf 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -631,17 +631,15 @@ static int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
 				void *data)
 {
 	struct cpufreq_freqs *freq = data;
-	unsigned long *lpj, dummy;
+	unsigned long *lpj;
 
 	if (cpu_has(&cpu_data(freq->cpu), X86_FEATURE_CONSTANT_TSC))
 		return 0;
 
-	lpj = &dummy;
-	if (!(freq->flags & CPUFREQ_CONST_LOOPS))
+	lpj = &boot_cpu_data.loops_per_jiffy;
 #ifdef CONFIG_SMP
+	if (!(freq->flags & CPUFREQ_CONST_LOOPS))
 		lpj = &cpu_data(freq->cpu).loops_per_jiffy;
-#else
-	lpj = &boot_cpu_data.loops_per_jiffy;
 #endif
 
 	if (!ref_freq) {

commit 595dc54a1da91408a52c4b962f3deeb1109aaca0
Merge: 9b29e8228a5c 7d96fd41cadc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 16:15:59 2009 -0700

    Merge branch 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: move rdtsc_barrier() into the TSC vread method

commit 7dc3ca39cb1e22eedbf1207ff9ac7bf682fc0f6d
Merge: aa98936e4f42 a4046f8d299e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 15:49:36 2009 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, nmi: Use predefined numbers instead of hardcoded one
      x86: asm/processor.h: remove double declaration
      x86, mtrr: replace MTRRdefType_MSR with msr-index's MSR_MTRRdefType
      x86, mtrr: replace MTRRfix4K_C0000_MSR with msr-index's MSR_MTRRfix4K_C0000
      x86, mtrr: remove mtrr MSRs double declaration
      x86, mtrr: replace MTRRfix16K_80000_MSR with msr-index's MSR_MTRRfix16K_80000
      x86, mtrr: replace MTRRfix64K_00000_MSR with msr-index's MSR_MTRRfix64K_00000
      x86, mtrr: replace MTRRcap_MSR with msr-index's MSR_MTRRcap
      x86: mce: remove duplicated #include
      x86: msr-index.h remove duplicate MSR C001_0015 declaration
      x86: clean up arch/x86/kernel/tsc_sync.c a bit
      x86: use symbolic name for VM86_SIGNAL when used as vm86 default return
      x86: added 'ifndef _ASM_X86_IOMAP_H' to iomap.h
      x86: avoid multiple declaration of kstack_depth_to_print
      x86: vdso/vma.c declare vdso_enabled and arch_setup_additional_pages before they get used
      x86: clean up declarations and variables
      x86: apic/x2apic_cluster.c x86_cpu_to_logical_apicid should be static
      x86 early quirks: eliminate unused function

commit 7d96fd41cadc55f4e00231c8c71b8e25c779f122
Author: Petr Tesarik <ptesarik@suse.cz>
Date:   Mon May 25 11:02:02 2009 +0200

    x86: move rdtsc_barrier() into the TSC vread method
    
    The *fence instructions were moved to vsyscall_64.c by commit
    cb9e35dce94a1b9c59d46224e8a94377d673e204.  But this breaks the
    vDSO, because vread methods are also called from there.
    
    Besides, the synchronization might be unnecessary for other
    time sources than TSC.
    
    [ Impact: fix potential time warp in VDSO ]
    
    Signed-off-by: Petr Tesarik <ptesarik@suse.cz>
    LKML-Reference: <9d0ea9ea0f866bdc1f4d76831221ae117f11ea67.1243241859.git.ptesarik@suse.cz>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: <stable@kernel.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index d57de05dc430..cf8611d991e0 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -710,7 +710,16 @@ static cycle_t read_tsc(struct clocksource *cs)
 #ifdef CONFIG_X86_64
 static cycle_t __vsyscall_fn vread_tsc(void)
 {
-	cycle_t ret = (cycle_t)vget_cycles();
+	cycle_t ret;
+
+	/*
+	 * Surround the RDTSC by barriers, to make sure it's not
+	 * speculated to outside the seqlock critical section and
+	 * does not cause time warps:
+	 */
+	rdtsc_barrier();
+	ret = (cycle_t)vget_cycles();
+	rdtsc_barrier();
 
 	return ret >= __vsyscall_gtod_data.clock.cycle_last ?
 		ret : __vsyscall_gtod_data.clock.cycle_last;

commit 8e19608e8b5c001e4a66ce482edc474f05fb7355
Author: Magnus Damm <damm@igel.co.jp>
Date:   Tue Apr 21 12:24:00 2009 -0700

    clocksource: pass clocksource to read() callback
    
    Pass clocksource pointer to the read() callback for clocksources.  This
    allows us to share the callback between multiple instances.
    
    [hugh@veritas.com: fix powerpc build of clocksource pass clocksource mods]
    [akpm@linux-foundation.org: cleanup]
    Signed-off-by: Magnus Damm <damm@igel.co.jp>
    Acked-by: John Stultz <johnstul@us.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 7a567ebe6361..d57de05dc430 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -699,7 +699,7 @@ static struct clocksource clocksource_tsc;
  * code, which is necessary to support wrapping clocksources like pm
  * timer.
  */
-static cycle_t read_tsc(void)
+static cycle_t read_tsc(struct clocksource *cs)
 {
 	cycle_t ret = (cycle_t)get_cycles();
 

commit 2c1b284e4fa260fd922b9a65c99169e2630c6862
Author: Jaswinder Singh Rajput <jaswinder@kernel.org>
Date:   Sat Apr 11 00:03:10 2009 +0530

    x86: clean up declarations and variables
    
    Impact: cleanup, no code changed
    
     - syscalls.h       update declarations due to unifications
     - irq.c            declare smp_generic_interrupt() before it gets used
     - process.c        declare sys_fork() and sys_vfork() before they get used
     - tsc.c            rename tsc_khz shadowed variable
     - apic/probe_32.c  declare apic_default before it gets used
     - apic/nmi.c       prev_nmi_count should be unsigned
     - apic/io_apic.c   declare smp_irq_move_cleanup_interrupt() before it gets used
     - mm/init.c        declare direct_gbpages and free_initrd_mem before they get used
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinder@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 7a567ebe6361..a8dc0d00b830 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -384,13 +384,13 @@ unsigned long native_calibrate_tsc(void)
 {
 	u64 tsc1, tsc2, delta, ref1, ref2;
 	unsigned long tsc_pit_min = ULONG_MAX, tsc_ref_min = ULONG_MAX;
-	unsigned long flags, latch, ms, fast_calibrate, tsc_khz;
+	unsigned long flags, latch, ms, fast_calibrate, hv_tsc_khz;
 	int hpet = is_hpet_enabled(), i, loopmin;
 
-	tsc_khz = get_hypervisor_tsc_freq();
-	if (tsc_khz) {
+	hv_tsc_khz = get_hypervisor_tsc_freq();
+	if (hv_tsc_khz) {
 		printk(KERN_INFO "TSC: Frequency read from the hypervisor\n");
-		return tsc_khz;
+		return hv_tsc_khz;
 	}
 
 	local_irq_save(flags);

commit 6e15cf04860074ad032e88c306bea656bbdd0f22
Merge: be0ea69674ed 60db56422043
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 26 21:39:17 2009 +0100

    Merge branch 'core/percpu' into percpu-cpumask-x86-for-linus-2
    
    Conflicts:
            arch/parisc/kernel/irq.c
            arch/x86/include/asm/fixmap_64.h
            arch/x86/include/asm/setup.h
            kernel/irq/handle.c
    
    Semantic merge:
            arch/x86/include/asm/fixmap.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 831576fe40f4175e0767623cffa4aeb28157943a
Merge: 21cdbc1378e8 66fef08f7d52
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 26 16:05:01 2009 -0700

    Merge branch 'sched-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (46 commits)
      sched: Add comments to find_busiest_group() function
      sched: Refactor the power savings balance code
      sched: Optimize the !power_savings_balance during fbg()
      sched: Create a helper function to calculate imbalance
      sched: Create helper to calculate small_imbalance in fbg()
      sched: Create a helper function to calculate sched_domain stats for fbg()
      sched: Define structure to store the sched_domain statistics for fbg()
      sched: Create a helper function to calculate sched_group stats for fbg()
      sched: Define structure to store the sched_group statistics for fbg()
      sched: Fix indentations in find_busiest_group() using gotos
      sched: Simple helper functions for find_busiest_group()
      sched: remove unused fields from struct rq
      sched: jiffies not printed per CPU
      sched: small optimisation of can_migrate_task()
      sched: fix typos in documentation
      sched: add avg_overlap decay
      x86, sched_clock(): mark variables read-mostly
      sched: optimize ttwu vs group scheduling
      sched: TIF_NEED_RESCHED -> need_reshed() cleanup
      sched: don't rebalance if attached on NULL domain
      ...

commit ada19a31a90b4f46c040c25ef4ef8ffc203c7fc6
Merge: 8d80ce80e1d5 36e8abf3edcd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 26 11:04:08 2009 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/davej/cpufreq
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/davej/cpufreq: (35 commits)
      [CPUFREQ] Prevent p4-clockmod from auto-binding to the ondemand governor.
      [CPUFREQ] Make cpufreq-nforce2 less obnoxious
      [CPUFREQ] p4-clockmod reports wrong frequency.
      [CPUFREQ] powernow-k8: Use a common exit path.
      [CPUFREQ] Change link order of x86 cpufreq modules
      [CPUFREQ] conservative: remove 10x from def_sampling_rate
      [CPUFREQ] conservative: fixup governor to function more like ondemand logic
      [CPUFREQ] conservative: fix dbs_cpufreq_notifier so freq is not locked
      [CPUFREQ] conservative: amend author's email address
      [CPUFREQ] Use swap() in longhaul.c
      [CPUFREQ] checkpatch cleanups for acpi-cpufreq
      [CPUFREQ] powernow-k8: Only print error message once, not per core.
      [CPUFREQ] ondemand/conservative: sanitize sampling_rate restrictions
      [CPUFREQ] ondemand/conservative: deprecate sampling_rate{min,max}
      [CPUFREQ] powernow-k8: Always compile powernow-k8 driver with ACPI support
      [CPUFREQ] Introduce /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_transition_latency
      [CPUFREQ] checkpatch cleanups for powernow-k8
      [CPUFREQ] checkpatch cleanups for ondemand governor.
      [CPUFREQ] checkpatch cleanups for powernow-k7
      [CPUFREQ] checkpatch cleanups for speedstep related drivers.
      ...

commit 37ba317c9ed19eb126e40bbf563f2711e191a636
Merge: 708dc5125309 ee568b25ee9e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Mar 18 09:57:02 2009 +0100

    Merge branches 'sched/cleanups' and 'linus' into sched/core

commit 9e8912e04e612b43897b4b722205408b92f423e5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 17 08:13:17 2009 -0700

    Fast TSC calibration: calculate proper frequency error bounds
    
    In order for ntpd to correctly synchronize the clocks, the frequency of
    the system clock must not be off by more than 500 ppm (or, put another
    way, 1:2000), or ntpd will end up giving up on trying to synchronize
    properly, and ends up reseting the clock in jumps instead.
    
    The fast TSC PIT calibration sometimes failed this test - it was
    assuming that the PIT reads always took about one microsecond each (2us
    for the two reads to get a 16-bit timer), and that calibrating TSC to
    the PIT over 15ms should thus be sufficient to get much closer than
    500ppm (max 2us error on both sides giving 4us over 15ms: a 270 ppm
    error value).
    
    However, that assumption does not always hold: apparently some hardware
    is either very much slower at reading the PIT registers, or there was
    other noise causing at least one machine to get 700+ ppm errors.
    
    So instead of using a fixed 15ms timing loop, this changes the fast PIT
    calibration to read the TSC delta over the individual PIT timer reads,
    and use the result to calculate the error bars on the PIT read timing
    properly.  We then successfully calibrate the TSC only if the maximum
    error bars fall below 500ppm.
    
    In the process, we also relax the timing to allow up to 25ms for the
    calibration, although it can happen much faster depending on hardware.
    
    Reported-and-tested-by: Jesper Krogh <jesper@krogh.cc>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 9e80207c96a2..d5cebb52d45b 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -273,30 +273,43 @@ static unsigned long pit_calibrate_tsc(u32 latch, unsigned long ms, int loopmin)
  * use the TSC value at the transitions to calculate a pretty
  * good value for the TSC frequencty.
  */
-static inline int pit_expect_msb(unsigned char val)
+static inline int pit_expect_msb(unsigned char val, u64 *tscp, unsigned long *deltap)
 {
-	int count = 0;
+	int count;
+	u64 tsc = 0;
 
 	for (count = 0; count < 50000; count++) {
 		/* Ignore LSB */
 		inb(0x42);
 		if (inb(0x42) != val)
 			break;
+		tsc = get_cycles();
 	}
-	return count > 50;
+	*deltap = get_cycles() - tsc;
+	*tscp = tsc;
+
+	/*
+	 * We require _some_ success, but the quality control
+	 * will be based on the error terms on the TSC values.
+	 */
+	return count > 5;
 }
 
 /*
- * How many MSB values do we want to see? We aim for a
- * 15ms calibration, which assuming a 2us counter read
- * error should give us roughly 150 ppm precision for
- * the calibration.
+ * How many MSB values do we want to see? We aim for
+ * a maximum error rate of 500ppm (in practice the
+ * real error is much smaller), but refuse to spend
+ * more than 25ms on it.
  */
-#define QUICK_PIT_MS 15
-#define QUICK_PIT_ITERATIONS (QUICK_PIT_MS * PIT_TICK_RATE / 1000 / 256)
+#define MAX_QUICK_PIT_MS 25
+#define MAX_QUICK_PIT_ITERATIONS (MAX_QUICK_PIT_MS * PIT_TICK_RATE / 1000 / 256)
 
 static unsigned long quick_pit_calibrate(void)
 {
+	int i;
+	u64 tsc, delta;
+	unsigned long d1, d2;
+
 	/* Set the Gate high, disable speaker */
 	outb((inb(0x61) & ~0x02) | 0x01, 0x61);
 
@@ -324,45 +337,43 @@ static unsigned long quick_pit_calibrate(void)
 	inb(0x42);
 	inb(0x42);
 
-	if (pit_expect_msb(0xff)) {
-		int i;
-		u64 t1, t2, delta;
-		unsigned char expect = 0xfe;
-
-		t1 = get_cycles();
-		for (i = 0; i < QUICK_PIT_ITERATIONS; i++, expect--) {
-			if (!pit_expect_msb(expect))
-				goto failed;
+	if (pit_expect_msb(0xff, &tsc, &d1)) {
+		for (i = 1; i <= MAX_QUICK_PIT_ITERATIONS; i++) {
+			if (!pit_expect_msb(0xff-i, &delta, &d2))
+				break;
+
+			/*
+			 * Iterate until the error is less than 500 ppm
+			 */
+			delta -= tsc;
+			if (d1+d2 < delta >> 11)
+				goto success;
 		}
-		t2 = get_cycles();
-
-		/*
-		 * Make sure we can rely on the second TSC timestamp:
-		 */
-		if (!pit_expect_msb(expect))
-			goto failed;
-
-		/*
-		 * Ok, if we get here, then we've seen the
-		 * MSB of the PIT decrement QUICK_PIT_ITERATIONS
-		 * times, and each MSB had many hits, so we never
-		 * had any sudden jumps.
-		 *
-		 * As a result, we can depend on there not being
-		 * any odd delays anywhere, and the TSC reads are
-		 * reliable.
-		 *
-		 * kHz = ticks / time-in-seconds / 1000;
-		 * kHz = (t2 - t1) / (QPI * 256 / PIT_TICK_RATE) / 1000
-		 * kHz = ((t2 - t1) * PIT_TICK_RATE) / (QPI * 256 * 1000)
-		 */
-		delta = (t2 - t1)*PIT_TICK_RATE;
-		do_div(delta, QUICK_PIT_ITERATIONS*256*1000);
-		printk("Fast TSC calibration using PIT\n");
-		return delta;
 	}
-failed:
+	printk("Fast TSC calibration failed\n");
 	return 0;
+
+success:
+	/*
+	 * Ok, if we get here, then we've seen the
+	 * MSB of the PIT decrement 'i' times, and the
+	 * error has shrunk to less than 500 ppm.
+	 *
+	 * As a result, we can depend on there not being
+	 * any odd delays anywhere, and the TSC reads are
+	 * reliable (within the error). We also adjust the
+	 * delta to the middle of the error bars, just
+	 * because it looks nicer.
+	 *
+	 * kHz = ticks / time-in-seconds / 1000;
+	 * kHz = (t2 - t1) / (I * 256 / PIT_TICK_RATE) / 1000
+	 * kHz = ((t2 - t1) * PIT_TICK_RATE) / (I * 256 * 1000)
+	 */
+	delta += (long)(d2 - d1)/2;
+	delta *= PIT_TICK_RATE;
+	do_div(delta, i*256*1000);
+	printk("Fast TSC calibration using PIT\n");
+	return delta;
 }
 
 /**

commit a6a80e1d8cf82b46a69f88e659da02749231eb36
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 17 07:58:26 2009 -0700

    Fix potential fast PIT TSC calibration startup glitch
    
    During bootup, when we reprogram the PIT (programmable interval timer)
    to start counting down from 0xffff in order to use it for the fast TSC
    calibration, we should also make sure to delay a bit afterwards to allow
    the PIT hardware to actually start counting with the new value.
    
    That will happens at the next CLK pulse (1.193182 MHz), so the easiest
    way to do that is to just wait at least one microsecond after
    programming the new PIT counter value.  We do that by just reading the
    counter value back once - which will take about 2us on PC hardware.
    
    Reported-and-tested-by: john stultz <johnstul@us.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 599e58168631..9e80207c96a2 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -315,6 +315,15 @@ static unsigned long quick_pit_calibrate(void)
 	outb(0xff, 0x42);
 	outb(0xff, 0x42);
 
+	/*
+	 * The PIT starts counting at the next edge, so we
+	 * need to delay for a microsecond. The easiest way
+	 * to do that is to just read back the 16-bit counter
+	 * once from the PIT.
+	 */
+	inb(0x42);
+	inb(0x42);
+
 	if (pit_expect_msb(0xff)) {
 		int i;
 		u64 t1, t2, delta;

commit f24ade3a3332811a512ed3b6c6aa69486719b1d8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 10 19:02:30 2009 +0100

    x86, sched_clock(): mark variables read-mostly
    
    Impact: micro-optimization
    
    There's a number of variables in the sched_clock() path that are
    in .data/.bss - but not marked __read_mostly. This creates the
    danger of accidental false cacheline sharing with some other,
    write-often variable.
    
    So mark them __read_mostly.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 599e58168631..9c8b71531ca8 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -17,20 +17,21 @@
 #include <asm/delay.h>
 #include <asm/hypervisor.h>
 
-unsigned int cpu_khz;           /* TSC clocks / usec, not used here */
+unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
-unsigned int tsc_khz;
+
+unsigned int __read_mostly tsc_khz;
 EXPORT_SYMBOL(tsc_khz);
 
 /*
  * TSC can be unstable due to cpufreq or due to unsynced TSCs
  */
-static int tsc_unstable;
+static int __read_mostly tsc_unstable;
 
 /* native_sched_clock() is called before tsc_init(), so
    we must start with the TSC soft disabled to prevent
    erroneous rdtsc usage on !cpu_has_tsc processors */
-static int tsc_disabled = -1;
+static int __read_mostly tsc_disabled = -1;
 
 static int tsc_clocksource_reliable;
 /*

commit 199785eac892a1fa1b71cc22bec58e8b156d9311
Author: Matthias-Christian Ott <ott@mirix.org>
Date:   Fri Feb 20 20:52:17 2009 -0500

    [CPUFREQ] p4-clockmod reports wrong frequency.
    
    http://bugzilla.kernel.org/show_bug.cgi?id=10968
    
    [ Updated for current tree, and fixed compile failure
      when p4-clockmod was built modular -- davej]
    
    From: Matthias-Christian Ott <ott@mirix.org>
    Signed-off-by: Dominik Brodowski <linux@brodo.de>
    Signed-off-by: Dave Jones <davej@redhat.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 599e58168631..5ad22f8f5f3a 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -523,8 +523,6 @@ unsigned long native_calibrate_tsc(void)
 	return tsc_pit_min;
 }
 
-#ifdef CONFIG_X86_32
-/* Only called from the Powernow K7 cpu freq driver */
 int recalibrate_cpu_khz(void)
 {
 #ifndef CONFIG_SMP
@@ -546,7 +544,6 @@ int recalibrate_cpu_khz(void)
 
 EXPORT_SYMBOL(recalibrate_cpu_khz);
 
-#endif /* CONFIG_X86_32 */
 
 /* Accelerators for sched_clock()
  * convert from cycles(64bits) => nanoseconds (64bits)

commit 3e5095d15276efd14a45393666b1bb7536bf179f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 27 17:07:08 2009 +0100

    x86: replace CONFIG_X86_SMP with CONFIG_SMP
    
    The x86/Voyager subarch used to have this distinction between
     'x86 SMP support' and 'Voyager SMP support':
    
     config X86_SMP
            bool
            depends on SMP && ((X86_32 && !X86_VOYAGER) || X86_64)
    
    This is a pointless distinction - Voyager can (and already does) use
    smp_ops to implement various SMP quirks it has - and it can be extended
    more to cover all the specialities of Voyager.
    
    So remove this complication in the Kconfig space.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 599e58168631..83d53ce5d4c4 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -773,7 +773,7 @@ __cpuinit int unsynchronized_tsc(void)
 	if (!cpu_has_tsc || tsc_unstable)
 		return 1;
 
-#ifdef CONFIG_X86_SMP
+#ifdef CONFIG_SMP
 	if (apic_is_clustered_box())
 		return 1;
 #endif

commit fa623d1b0222adbe8f822e53c08003b9679a410c
Merge: 3d44cc3e01ee 1ccedb7cdba6 34945ede3107 d43779740621 c415b3dce30d beeb4195cbc8 f269b07e862c 4e42ebd57b2e e1286f2c686f 878719e831d9 fd28a5b58ddd adf77bac052b 8f2466f45f75 93093d099e5d bb5574608a83 f34a10bd9f8c b6fd6f26733e 30604bb410b5 5b9a0e14eb4b 67bac792cd0c 7a9787e1eba9 f4166c54bfe0 69b88afa8d11 8daa19051e1c 3e1e9002aa8b 8403295e0fa4 4db646b1af8f 205516c12dbb c8182f0016fb ecbf29cdb399
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 23 16:27:23 2008 +0100

    Merge branches 'x86/apic', 'x86/cleanups', 'x86/cpufeature', 'x86/crashdump', 'x86/debug', 'x86/defconfig', 'x86/detect-hyper', 'x86/doc', 'x86/dumpstack', 'x86/early-printk', 'x86/fpu', 'x86/idle', 'x86/io', 'x86/memory-corruption-check', 'x86/microcode', 'x86/mm', 'x86/mtrr', 'x86/nmi-watchdog', 'x86/pat2', 'x86/pci-ioapic-boot-irq-quirks', 'x86/ptrace', 'x86/quirks', 'x86/reboot', 'x86/setup-memory', 'x86/signal', 'x86/sparse-fixes', 'x86/time', 'x86/uv' and 'x86/xen' into x86/core

commit 7cbaef9c83e58bbd4bdd534b09052b6c5ec457d5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Nov 8 17:05:38 2008 +0100

    sched: optimize sched_clock() a bit
    
    sched_clock() uses cycles_2_ns() needlessly - which is an irq-disabling
    variant of __cycles_2_ns().
    
    Most of the time sched_clock() is called with irqs disabled already.
    The few places that call it with irqs enabled need to be updated.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 2ef80e301925..424093b157d3 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -55,7 +55,7 @@ u64 native_sched_clock(void)
 	rdtscll(this_offset);
 
 	/* return the value in ns */
-	return cycles_2_ns(this_offset);
+	return __cycles_2_ns(this_offset);
 }
 
 /* We need to define a real function for sched_clock, to override the

commit 70de9a97049e0ba79dc040868564408d5ce697f9
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Nov 3 11:18:47 2008 -0800

    x86: don't use tsc_khz to calculate lpj if notsc is passed
    
    Impact: fix udelay when "notsc" boot parameter is passed
    
    With notsc passed on commandline, tsc may not be used for
    udelays, make sure that we do not use tsc_khz to calculate
    the lpj value in such cases.
    
    Reported-by: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 62348e4fd8d1..2ef80e301925 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -813,10 +813,6 @@ void __init tsc_init(void)
 		cpu_khz = calibrate_cpu();
 #endif
 
-	lpj = ((u64)tsc_khz * 1000);
-	do_div(lpj, HZ);
-	lpj_fine = lpj;
-
 	printk("Detected %lu.%03lu MHz processor.\n",
 			(unsigned long)cpu_khz / 1000,
 			(unsigned long)cpu_khz % 1000);
@@ -836,6 +832,10 @@ void __init tsc_init(void)
 	/* now allow native_sched_clock() to use rdtsc */
 	tsc_disabled = 0;
 
+	lpj = ((u64)tsc_khz * 1000);
+	do_div(lpj, HZ);
+	lpj_fine = lpj;
+
 	use_tsc_delay();
 	/* Check and install the TSC clocksource */
 	dmi_check_system(bad_tsc_dmi_table);

commit 395628ef4ea12ff0748099f145363b5e33c69acb
Author: Alok Kataria <akataria@vmware.com>
Date:   Fri Oct 24 17:22:01 2008 -0700

    x86: Skip verification by the watchdog for TSC clocksource.
    
    Impact: Changes timekeeping on Vmware (or with tsc=reliable).
    
    This is achieved by resetting the CLOCKSOURCE_MUST_VERIFY flag.
    
    We add a tsc=reliable commandline option to enable this.
    This enables legacy hardware without HPET, LAPIC, or ACPI timers
    to enter high-resolution timer mode.
    
    Along with that have extended this to be used in virtualization environement
    too. Now we also set this flag if the X86_FEATURE_TSC_RELIABLE bit is set.
    
    This is important since there is a wrap-around problem with the acpi_pm timer.
    The acpi_pm counter is just 24bits and this can overflow in ~4 seconds. With
    the NO_HZ kernels in virtualized environment, there can be situations when
    the guest is descheduled for longer duration, as a result we may miss the wrap
    of the acpi counter. When TSC is used as a clocksource and acpi_pm timer is
    being used as the watchdog clocksource this error in acpi_pm results in TSC
    being marked as unstable, and essentially results in time dropping in chunks
    of 4 seconds whenever this wrap is missed. Since the virtualized TSC is
    reliable on VMware, we should always use the TSCs clocksource on VMware, so
    we skip the verfication at runtime, by checking for the feature bit.
    
    Since we reset the flag for mgeode systems too, i have combined
    the mgeode case with the feature bit check.
    
    Signed-off-by: Jeff Hansen <jhansen@cardaccess-inc.com>
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 6dbf0bcb44a8..ee01cd96b5e1 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -32,6 +32,7 @@ static int tsc_unstable;
    erroneous rdtsc usage on !cpu_has_tsc processors */
 static int tsc_disabled = -1;
 
+static int tsc_clocksource_reliable;
 /*
  * Scheduler clock - returns current time in nanosec units.
  */
@@ -99,6 +100,15 @@ int __init notsc_setup(char *str)
 
 __setup("notsc", notsc_setup);
 
+static int __init tsc_setup(char *str)
+{
+	if (!strcmp(str, "reliable"))
+		tsc_clocksource_reliable = 1;
+	return 1;
+}
+
+__setup("tsc=", tsc_setup);
+
 #define MAX_RETRIES     5
 #define SMI_TRESHOLD    50000
 
@@ -738,24 +748,21 @@ static struct dmi_system_id __initdata bad_tsc_dmi_table[] = {
 	{}
 };
 
-/*
- * Geode_LX - the OLPC CPU has a possibly a very reliable TSC
- */
+static void __init check_system_tsc_reliable(void)
+{
 #ifdef CONFIG_MGEODE_LX
-/* RTSC counts during suspend */
+	/* RTSC counts during suspend */
 #define RTSC_SUSP 0x100
-
-static void __init check_geode_tsc_reliable(void)
-{
 	unsigned long res_low, res_high;
 
 	rdmsr_safe(MSR_GEODE_BUSCONT_CONF0, &res_low, &res_high);
+	/* Geode_LX - the OLPC CPU has a possibly a very reliable TSC */
 	if (res_low & RTSC_SUSP)
-		clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
-}
-#else
-static inline void check_geode_tsc_reliable(void) { }
+		tsc_clocksource_reliable = 1;
 #endif
+	if (boot_cpu_has(X86_FEATURE_TSC_RELIABLE))
+		tsc_clocksource_reliable = 1;
+}
 
 /*
  * Make an educated guess if the TSC is trustworthy and synchronized
@@ -790,6 +797,8 @@ static void __init init_tsc_clocksource(void)
 {
 	clocksource_tsc.mult = clocksource_khz2mult(tsc_khz,
 			clocksource_tsc.shift);
+	if (tsc_clocksource_reliable)
+		clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
 	/* lower the rating if we already know its unstable: */
 	if (check_tsc_unstable()) {
 		clocksource_tsc.rating = 0;
@@ -850,7 +859,7 @@ void __init tsc_init(void)
 	if (unsynchronized_tsc())
 		mark_tsc_unstable("TSCs unsynchronized");
 
-	check_geode_tsc_reliable();
+	check_system_tsc_reliable();
 	init_tsc_clocksource();
 }
 

commit 88b094fb8d4fe43b7025ea8d487059e8813e02cd
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Oct 27 10:41:46 2008 -0700

    x86: Hypervisor detection and get tsc_freq from hypervisor
    
    Impact: Changes timebase calibration on Vmware.
    
    v3->v2 : Abstract the hypervisor detection and feature (tsc_freq) request
             behind a hypervisor.c file
    v2->v1 : Add a x86_hyper_vendor field to the cpuinfo_x86 structure.
             This avoids multiple calls to the hypervisor detection function.
    
    This patch adds function to detect if we are running under VMware.
    The current way to check if we are on VMware is following,
    #  check if "hypervisor present bit" is set, if so read the 0x40000000
       cpuid leaf and check for "VMwareVMware" signature.
    #  if the above fails, check the DMI vendors name for "VMware" string
       if we find one we query the VMware hypervisor port to check if we are
       under VMware.
    
    The DMI + "VMware hypervisor port check" is needed for older VMware products,
    which don't implement the hypervisor signature cpuid leaf.
    Also note that since we are checking for the DMI signature the hypervisor
    port should never be accessed on native hardware.
    
    This patch also adds a hypervisor_get_tsc_freq function, instead of
    calibrating the frequency which can be error prone in virtualized
    environment, we ask the hypervisor for it. We get the frequency from
    the hypervisor by accessing the hypervisor port if we are running on VMware.
    Other hypervisors too can add code to the generic routine to get frequency on
    their platform.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 62348e4fd8d1..6dbf0bcb44a8 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -15,6 +15,7 @@
 #include <asm/vgtod.h>
 #include <asm/time.h>
 #include <asm/delay.h>
+#include <asm/hypervisor.h>
 
 unsigned int cpu_khz;           /* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
@@ -352,9 +353,15 @@ unsigned long native_calibrate_tsc(void)
 {
 	u64 tsc1, tsc2, delta, ref1, ref2;
 	unsigned long tsc_pit_min = ULONG_MAX, tsc_ref_min = ULONG_MAX;
-	unsigned long flags, latch, ms, fast_calibrate;
+	unsigned long flags, latch, ms, fast_calibrate, tsc_khz;
 	int hpet = is_hpet_enabled(), i, loopmin;
 
+	tsc_khz = get_hypervisor_tsc_freq();
+	if (tsc_khz) {
+		printk(KERN_INFO "TSC: Frequency read from the hypervisor\n");
+		return tsc_khz;
+	}
+
 	local_irq_save(flags);
 	fast_calibrate = quick_pit_calibrate();
 	local_irq_restore(flags);

commit 017d9d20d88cacb0a6a29f343b23c95e203f6645
Author: James Bottomley <James.Bottomley@HansenPartnership.com>
Date:   Thu Oct 30 16:05:39 2008 -0500

    x86: use CONFIG_X86_SMP instead of CONFIG_SMP
    
    Impact: fix x86/Voyager boot
    
    CONFIG_SMP is used for features which work on *all* x86 boxes.
    CONFIG_X86_SMP is used for standard PC like x86 boxes (for things like
    multi core and apics)
    
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 161bb850fc47..62348e4fd8d1 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -759,7 +759,7 @@ __cpuinit int unsynchronized_tsc(void)
 	if (!cpu_has_tsc || tsc_unstable)
 		return 1;
 
-#ifdef CONFIG_SMP
+#ifdef CONFIG_X86_SMP
 	if (apic_is_clustered_box())
 		return 1;
 #endif

commit e496e3d645c93206faf61ff6005995ebd08cc39c
Merge: b159d7a989e5 5bbd4c372400 175e438f7a2d 516cbf3730c4 af2d237bf574 9b1568458a3e 5b7e41ff3726 1befdefcf476 a03352d2c1dc 7b22ff5344fd 2c7e9fd4c6cb 91030ca1e739 dd5523552c28 b3e15bdef689 20211e4d3447 efd327a2d412 c7ffa6c26277 e51a1ac2dfca 5df455155124 d99e90164e6c e621bd18958e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 6 18:17:07 2008 +0200

    Merge branches 'x86/alternatives', 'x86/cleanups', 'x86/commandline', 'x86/crashdump', 'x86/debug', 'x86/defconfig', 'x86/doc', 'x86/exports', 'x86/fpu', 'x86/gart', 'x86/idle', 'x86/mm', 'x86/mtrr', 'x86/nmi-watchdog', 'x86/oprofile', 'x86/paravirt', 'x86/reboot', 'x86/sparse-fixes', 'x86/tsc', 'x86/urgent' and 'x86/vmalloc' into x86-v28-for-linus-phase1

commit 5df45515512436a808d3476a90e83f2efb022422
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Sep 6 23:55:40 2008 +0200

    x86, tsc calibration: fix
    
    my brown paperbag day ...
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 6dab90f68515..4847a9280505 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -319,7 +319,7 @@ static unsigned long quick_pit_calibrate(void)
 		/*
 		 * Make sure we can rely on the second TSC timestamp:
 		 */
-		if (!pit_expect_msb(--expect))
+		if (!pit_expect_msb(expect))
 			goto failed;
 
 		/*

commit 4156e9a8ef5b521185f451213d33fa661f38512e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Sep 4 22:47:47 2008 +0200

    x86: quick TSC calibration, improve
    
    - make sure the final TSC timestamp is reliable too
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 839070ba8465..6dab90f68515 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -316,6 +316,12 @@ static unsigned long quick_pit_calibrate(void)
 		}
 		t2 = get_cycles();
 
+		/*
+		 * Make sure we can rely on the second TSC timestamp:
+		 */
+		if (!pit_expect_msb(--expect))
+			goto failed;
+
 		/*
 		 * Ok, if we get here, then we've seen the
 		 * MSB of the PIT decrement QUICK_PIT_ITERATIONS

commit 6ac40ed0413ef4096720f966e11c7cdf259eee3f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 4 10:41:22 2008 -0700

    x86: quick TSC calibration
    
    Introduce a fast TSC-calibration method on sane hardware.
    
    It only uses 17920 PIT timer ticks to calibrate the TSC, plus 256 ticks on
    each side to make sure the TSC values were very close to the tick, so the
    whole calibration takes 15ms. Yet, despite only takign 15ms,
    we can actually give pretty stringent guarantees of accuracy:
    
     - the code requires that we hit each 256-counter block at least 50 times,
       so the TSC error is basically at *MOST* just a few PIT cycles off in
       any direction. In practice, it's going to be about one microseconds
       off (which is how long it takes to read the counter)
    
     - so over 17920 PIT cycles, we can pretty much guarantee that the
       calibration error is less than one half of a percent.
    
    My testing bears this out: on my machine, the quick-calibration reports
    2934.085kHz, while the slow one reports 2933.415.
    
    Yes, the slower calibration is still more precise. For me, the slow
    calibration is stable to within about one hundreth of a percent, so it's
    (at a guess) roughly an order-and-a-half of magnitude more precise. The
    longer you wait, the more precise you can be.
    
    However, the nice thing about the fast TSC PIT synchronization is that
    it's pretty much _guaranteed_ to give that 0.5% precision, and fail
    gracefully (and very quickly) if it doesn't get it. And it really is
    fairly simple (even if there's a lot of _details_ there, and I didn't get
    all of those right ont he first try or even the second ;)
    
    The patch says "110 insertions", but 63 of those new lines are actually
    comments.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    ---
     arch/x86/kernel/tsc.c |  111 ++++++++++++++++++++++++++++++++++++++++++++++++-
     1 files changed, 110 insertions(+), 1 deletions(-)

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index da033b5b3e19..839070ba8465 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -227,6 +227,117 @@ static unsigned long pit_calibrate_tsc(u32 latch, unsigned long ms, int loopmin)
 	return delta;
 }
 
+/*
+ * This reads the current MSB of the PIT counter, and
+ * checks if we are running on sufficiently fast and
+ * non-virtualized hardware.
+ *
+ * Our expectations are:
+ *
+ *  - the PIT is running at roughly 1.19MHz
+ *
+ *  - each IO is going to take about 1us on real hardware,
+ *    but we allow it to be much faster (by a factor of 10) or
+ *    _slightly_ slower (ie we allow up to a 2us read+counter
+ *    update - anything else implies a unacceptably slow CPU
+ *    or PIT for the fast calibration to work.
+ *
+ *  - with 256 PIT ticks to read the value, we have 214us to
+ *    see the same MSB (and overhead like doing a single TSC
+ *    read per MSB value etc).
+ *
+ *  - We're doing 2 reads per loop (LSB, MSB), and we expect
+ *    them each to take about a microsecond on real hardware.
+ *    So we expect a count value of around 100. But we'll be
+ *    generous, and accept anything over 50.
+ *
+ *  - if the PIT is stuck, and we see *many* more reads, we
+ *    return early (and the next caller of pit_expect_msb()
+ *    then consider it a failure when they don't see the
+ *    next expected value).
+ *
+ * These expectations mean that we know that we have seen the
+ * transition from one expected value to another with a fairly
+ * high accuracy, and we didn't miss any events. We can thus
+ * use the TSC value at the transitions to calculate a pretty
+ * good value for the TSC frequencty.
+ */
+static inline int pit_expect_msb(unsigned char val)
+{
+	int count = 0;
+
+	for (count = 0; count < 50000; count++) {
+		/* Ignore LSB */
+		inb(0x42);
+		if (inb(0x42) != val)
+			break;
+	}
+	return count > 50;
+}
+
+/*
+ * How many MSB values do we want to see? We aim for a
+ * 15ms calibration, which assuming a 2us counter read
+ * error should give us roughly 150 ppm precision for
+ * the calibration.
+ */
+#define QUICK_PIT_MS 15
+#define QUICK_PIT_ITERATIONS (QUICK_PIT_MS * PIT_TICK_RATE / 1000 / 256)
+
+static unsigned long quick_pit_calibrate(void)
+{
+	/* Set the Gate high, disable speaker */
+	outb((inb(0x61) & ~0x02) | 0x01, 0x61);
+
+	/*
+	 * Counter 2, mode 0 (one-shot), binary count
+	 *
+	 * NOTE! Mode 2 decrements by two (and then the
+	 * output is flipped each time, giving the same
+	 * final output frequency as a decrement-by-one),
+	 * so mode 0 is much better when looking at the
+	 * individual counts.
+	 */
+	outb(0xb0, 0x43);
+
+	/* Start at 0xffff */
+	outb(0xff, 0x42);
+	outb(0xff, 0x42);
+
+	if (pit_expect_msb(0xff)) {
+		int i;
+		u64 t1, t2, delta;
+		unsigned char expect = 0xfe;
+
+		t1 = get_cycles();
+		for (i = 0; i < QUICK_PIT_ITERATIONS; i++, expect--) {
+			if (!pit_expect_msb(expect))
+				goto failed;
+		}
+		t2 = get_cycles();
+
+		/*
+		 * Ok, if we get here, then we've seen the
+		 * MSB of the PIT decrement QUICK_PIT_ITERATIONS
+		 * times, and each MSB had many hits, so we never
+		 * had any sudden jumps.
+		 *
+		 * As a result, we can depend on there not being
+		 * any odd delays anywhere, and the TSC reads are
+		 * reliable.
+		 *
+		 * kHz = ticks / time-in-seconds / 1000;
+		 * kHz = (t2 - t1) / (QPI * 256 / PIT_TICK_RATE) / 1000
+		 * kHz = ((t2 - t1) * PIT_TICK_RATE) / (QPI * 256 * 1000)
+		 */
+		delta = (t2 - t1)*PIT_TICK_RATE;
+		do_div(delta, QUICK_PIT_ITERATIONS*256*1000);
+		printk("Fast TSC calibration using PIT\n");
+		return delta;
+	}
+failed:
+	return 0;
+}
 
 /**
  * native_calibrate_tsc - calibrate the tsc on boot
@@ -235,9 +346,15 @@ unsigned long native_calibrate_tsc(void)
 {
 	u64 tsc1, tsc2, delta, ref1, ref2;
 	unsigned long tsc_pit_min = ULONG_MAX, tsc_ref_min = ULONG_MAX;
-	unsigned long flags, latch, ms;
+	unsigned long flags, latch, ms, fast_calibrate;
 	int hpet = is_hpet_enabled(), i, loopmin;
 
+	local_irq_save(flags);
+	fast_calibrate = quick_pit_calibrate();
+	local_irq_restore(flags);
+	if (fast_calibrate)
+		return fast_calibrate;
+
 	/*
 	 * Run 5 calibration loops to get the lowest frequency value
 	 * (the best estimate). We use two different calibration modes

commit a977c400957451f3bd92b9ed6022f5fe8a6cbbf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 4 15:18:59 2008 +0000

    x86: TSC make the calibration loop smarter
    
    The last changes made the calibration loop 250ms long which is far
    too much. Try to do that more clever.
    
    Experiments have shown that using a 10ms delay for the PIT based calibration
    gives us a good enough value. If we have a reference (HPET/PMTIMER) and the
    result of the PIT and the reference is close enough, then we can break out of
    the calibration loop on a match right away and use the reference value.
    
    Otherwise we just loop 3 times and decide then, which value to take.
    
    One caveat is that for virtualized environments the PIT calibration often does
    not work at all and I found out that 10us is a bit too short as well for the
    reference to give a sane result. The solution here is to make the last loop
    longer when the first two PIT calibrations failed.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 52284d31fc9c..da033b5b3e19 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -159,9 +159,14 @@ static unsigned long calc_pmtimer_ref(u64 deltatsc, u64 pm1, u64 pm2)
 	return (unsigned long) deltatsc;
 }
 
-#define CAL_MS		50
+#define CAL_MS		10
 #define CAL_LATCH	(CLOCK_TICK_RATE / (1000 / CAL_MS))
-#define CAL_PIT_LOOPS	5000
+#define CAL_PIT_LOOPS	1000
+
+#define CAL2_MS		50
+#define CAL2_LATCH	(CLOCK_TICK_RATE / (1000 / CAL2_MS))
+#define CAL2_PIT_LOOPS	5000
+
 
 /*
  * Try to calibrate the TSC against the Programmable
@@ -170,7 +175,7 @@ static unsigned long calc_pmtimer_ref(u64 deltatsc, u64 pm1, u64 pm2)
  *
  * Return ULONG_MAX on failure to calibrate.
  */
-static unsigned long pit_calibrate_tsc(void)
+static unsigned long pit_calibrate_tsc(u32 latch, unsigned long ms, int loopmin)
 {
 	u64 tsc, t1, t2, delta;
 	unsigned long tscmin, tscmax;
@@ -185,8 +190,8 @@ static unsigned long pit_calibrate_tsc(void)
 	 * (LSB then MSB) to begin countdown.
 	 */
 	outb(0xb0, 0x43);
-	outb(CAL_LATCH & 0xff, 0x42);
-	outb(CAL_LATCH >> 8, 0x42);
+	outb(latch & 0xff, 0x42);
+	outb(latch >> 8, 0x42);
 
 	tsc = t1 = t2 = get_cycles();
 
@@ -207,18 +212,18 @@ static unsigned long pit_calibrate_tsc(void)
 	/*
 	 * Sanity checks:
 	 *
-	 * If we were not able to read the PIT more than PIT_MIN_LOOPS
+	 * If we were not able to read the PIT more than loopmin
 	 * times, then we have been hit by a massive SMI
 	 *
 	 * If the maximum is 10 times larger than the minimum,
 	 * then we got hit by an SMI as well.
 	 */
-	if (pitcnt < CAL_PIT_LOOPS || tscmax > 10 * tscmin)
+	if (pitcnt < loopmin || tscmax > 10 * tscmin)
 		return ULONG_MAX;
 
 	/* Calculate the PIT value */
 	delta = t2 - t1;
-	do_div(delta, CAL_MS);
+	do_div(delta, ms);
 	return delta;
 }
 
@@ -230,8 +235,8 @@ unsigned long native_calibrate_tsc(void)
 {
 	u64 tsc1, tsc2, delta, ref1, ref2;
 	unsigned long tsc_pit_min = ULONG_MAX, tsc_ref_min = ULONG_MAX;
-	unsigned long flags;
-	int hpet = is_hpet_enabled(), i;
+	unsigned long flags, latch, ms;
+	int hpet = is_hpet_enabled(), i, loopmin;
 
 	/*
 	 * Run 5 calibration loops to get the lowest frequency value
@@ -257,7 +262,13 @@ unsigned long native_calibrate_tsc(void)
 	 * calibration delay loop as we have to wait for a certain
 	 * amount of time anyway.
 	 */
-	for (i = 0; i < 5; i++) {
+
+	/* Preset PIT loop values */
+	latch = CAL_LATCH;
+	ms = CAL_MS;
+	loopmin = CAL_PIT_LOOPS;
+
+	for (i = 0; i < 3; i++) {
 		unsigned long tsc_pit_khz;
 
 		/*
@@ -268,7 +279,7 @@ unsigned long native_calibrate_tsc(void)
 		 */
 		local_irq_save(flags);
 		tsc1 = tsc_read_refs(&ref1, hpet);
-		tsc_pit_khz = pit_calibrate_tsc();
+		tsc_pit_khz = pit_calibrate_tsc(latch, ms, loopmin);
 		tsc2 = tsc_read_refs(&ref2, hpet);
 		local_irq_restore(flags);
 
@@ -290,6 +301,35 @@ unsigned long native_calibrate_tsc(void)
 			tsc2 = calc_pmtimer_ref(tsc2, ref1, ref2);
 
 		tsc_ref_min = min(tsc_ref_min, (unsigned long) tsc2);
+
+		/* Check the reference deviation */
+		delta = ((u64) tsc_pit_min) * 100;
+		do_div(delta, tsc_ref_min);
+
+		/*
+		 * If both calibration results are inside a 10% window
+		 * then we can be sure, that the calibration
+		 * succeeded. We break out of the loop right away. We
+		 * use the reference value, as it is more precise.
+		 */
+		if (delta >= 90 && delta <= 110) {
+			printk(KERN_INFO
+			       "TSC: PIT calibration matches %s. %d loops\n",
+			       hpet ? "HPET" : "PMTIMER", i + 1);
+			return tsc_ref_min;
+		}
+
+		/*
+		 * Check whether PIT failed more than once. This
+		 * happens in virtualized environments. We need to
+		 * give the virtual PC a slightly longer timeframe for
+		 * the HPET/PMTIMER to make the result precise.
+		 */
+		if (i == 1 && tsc_pit_min == ULONG_MAX) {
+			latch = CAL2_LATCH;
+			ms = CAL2_MS;
+			loopmin = CAL2_PIT_LOOPS;
+		}
 	}
 
 	/*
@@ -309,7 +349,7 @@ unsigned long native_calibrate_tsc(void)
 		/* The alternative source failed as well, disable TSC */
 		if (tsc_ref_min == ULONG_MAX) {
 			printk(KERN_WARNING "TSC: HPET/PMTIMER calibration "
-			       "failed due to SMI disturbance.\n");
+			       "failed.\n");
 			return 0;
 		}
 
@@ -328,37 +368,18 @@ unsigned long native_calibrate_tsc(void)
 
 	/* The alternative source failed, use the PIT calibration value */
 	if (tsc_ref_min == ULONG_MAX) {
-		printk(KERN_WARNING "TSC: HPET/PMTIMER calibration failed due "
-		       "to SMI disturbance. Using PIT calibration\n");
+		printk(KERN_WARNING "TSC: HPET/PMTIMER calibration failed. "
+		       "Using PIT calibration\n");
 		return tsc_pit_min;
 	}
 
-	/* Check the reference deviation */
-	delta = ((u64) tsc_pit_min) * 100;
-	do_div(delta, tsc_ref_min);
-
-	/*
-	 * If both calibration results are inside a 5% window, the we
-	 * use the lower frequency of those as it is probably the
-	 * closest estimate.
-	 */
-	if (delta >= 95 && delta <= 105) {
-		printk(KERN_INFO "TSC: PIT calibration confirmed by %s.\n",
-		       hpet ? "HPET" : "PMTIMER");
-		printk(KERN_INFO "TSC: using %s calibration value\n",
-		       tsc_pit_min <= tsc_ref_min ? "PIT" :
-		       hpet ? "HPET" : "PMTIMER");
-		return tsc_pit_min <= tsc_ref_min ? tsc_pit_min : tsc_ref_min;
-	}
-
-	printk(KERN_WARNING "TSC: PIT calibration deviates from %s: %lu %lu.\n",
-	       hpet ? "HPET" : "PMTIMER", tsc_pit_min, tsc_ref_min);
-
 	/*
 	 * The calibration values differ too much. In doubt, we use
 	 * the PIT value as we know that there are PMTIMERs around
-	 * running at double speed.
+	 * running at double speed. At least we let the user know:
 	 */
+	printk(KERN_WARNING "TSC: PIT calibration deviates from %s: %lu %lu.\n",
+	       hpet ? "HPET" : "PMTIMER", tsc_pit_min, tsc_ref_min);
 	printk(KERN_INFO "TSC: Using PIT calibration value\n");
 	return tsc_pit_min;
 }

commit 827014be05e4515fa0dfc32e3100c4dab2070a98
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 4 15:18:53 2008 +0000

    x86: TSC: use one set of reference variables
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index ebb9bf824a07..52284d31fc9c 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -104,7 +104,7 @@ __setup("notsc", notsc_setup);
 /*
  * Read TSC and the reference counters. Take care of SMI disturbance
  */
-static u64 tsc_read_refs(u64 *pm, u64 *hpet)
+static u64 tsc_read_refs(u64 *p, int hpet)
 {
 	u64 t1, t2;
 	int i;
@@ -112,9 +112,9 @@ static u64 tsc_read_refs(u64 *pm, u64 *hpet)
 	for (i = 0; i < MAX_RETRIES; i++) {
 		t1 = get_cycles();
 		if (hpet)
-			*hpet = hpet_readl(HPET_COUNTER) & 0xFFFFFFFF;
+			*p = hpet_readl(HPET_COUNTER) & 0xFFFFFFFF;
 		else
-			*pm = acpi_pm_read_early();
+			*p = acpi_pm_read_early();
 		t2 = get_cycles();
 		if ((t2 - t1) < SMI_TRESHOLD)
 			return t2;
@@ -228,7 +228,7 @@ static unsigned long pit_calibrate_tsc(void)
  */
 unsigned long native_calibrate_tsc(void)
 {
-	u64 tsc1, tsc2, delta, pm1, pm2, hpet1, hpet2;
+	u64 tsc1, tsc2, delta, ref1, ref2;
 	unsigned long tsc_pit_min = ULONG_MAX, tsc_ref_min = ULONG_MAX;
 	unsigned long flags;
 	int hpet = is_hpet_enabled(), i;
@@ -267,16 +267,16 @@ unsigned long native_calibrate_tsc(void)
 		 * read the end value.
 		 */
 		local_irq_save(flags);
-		tsc1 = tsc_read_refs(&pm1, hpet ? &hpet1 : NULL);
+		tsc1 = tsc_read_refs(&ref1, hpet);
 		tsc_pit_khz = pit_calibrate_tsc();
-		tsc2 = tsc_read_refs(&pm2, hpet ? &hpet2 : NULL);
+		tsc2 = tsc_read_refs(&ref2, hpet);
 		local_irq_restore(flags);
 
 		/* Pick the lowest PIT TSC calibration so far */
 		tsc_pit_min = min(tsc_pit_min, tsc_pit_khz);
 
 		/* hpet or pmtimer available ? */
-		if (!hpet && !pm1 && !pm2)
+		if (!hpet && !ref1 && !ref2)
 			continue;
 
 		/* Check, whether the sampling was disturbed by an SMI */
@@ -285,9 +285,9 @@ unsigned long native_calibrate_tsc(void)
 
 		tsc2 = (tsc2 - tsc1) * 1000000LL;
 		if (hpet)
-			tsc2 = calc_hpet_ref(tsc2, hpet1, hpet2);
+			tsc2 = calc_hpet_ref(tsc2, ref1, ref2);
 		else
-			tsc2 = calc_pmtimer_ref(tsc2, pm1, pm2);
+			tsc2 = calc_pmtimer_ref(tsc2, ref1, ref2);
 
 		tsc_ref_min = min(tsc_ref_min, (unsigned long) tsc2);
 	}
@@ -301,7 +301,7 @@ unsigned long native_calibrate_tsc(void)
 		       "SMI disturbance.\n");
 
 		/* We don't have an alternative source, disable TSC */
-		if (!hpet && !pm1 && !pm2) {
+		if (!hpet && !ref1 && !ref2) {
 			printk("TSC: No reference (HPET/PMTIMER) available\n");
 			return 0;
 		}
@@ -321,7 +321,7 @@ unsigned long native_calibrate_tsc(void)
 	}
 
 	/* We don't have an alternative source, use the PIT calibration value */
-	if (!hpet && !pm1 && !pm2) {
+	if (!hpet && !ref1 && !ref2) {
 		printk(KERN_INFO "TSC: Using PIT calibration value\n");
 		return tsc_pit_min;
 	}

commit d683ef7afe8b6dbac6a3c681cef8a908357793ca
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 4 15:18:48 2008 +0000

    x86: TSC: separate hpet/pmtimer calculation out
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index aa11413e7c1d..ebb9bf824a07 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -122,6 +122,43 @@ static u64 tsc_read_refs(u64 *pm, u64 *hpet)
 	return ULLONG_MAX;
 }
 
+/*
+ * Calculate the TSC frequency from HPET reference
+ */
+static unsigned long calc_hpet_ref(u64 deltatsc, u64 hpet1, u64 hpet2)
+{
+	u64 tmp;
+
+	if (hpet2 < hpet1)
+		hpet2 += 0x100000000ULL;
+	hpet2 -= hpet1;
+	tmp = ((u64)hpet2 * hpet_readl(HPET_PERIOD));
+	do_div(tmp, 1000000);
+	do_div(deltatsc, tmp);
+
+	return (unsigned long) deltatsc;
+}
+
+/*
+ * Calculate the TSC frequency from PMTimer reference
+ */
+static unsigned long calc_pmtimer_ref(u64 deltatsc, u64 pm1, u64 pm2)
+{
+	u64 tmp;
+
+	if (!pm1 && !pm2)
+		return ULONG_MAX;
+
+	if (pm2 < pm1)
+		pm2 += (u64)ACPI_PM_OVRRUN;
+	pm2 -= pm1;
+	tmp = pm2 * 1000000000LL;
+	do_div(tmp, PMTMR_TICKS_PER_SEC);
+	do_div(deltatsc, tmp);
+
+	return (unsigned long) deltatsc;
+}
+
 #define CAL_MS		50
 #define CAL_LATCH	(CLOCK_TICK_RATE / (1000 / CAL_MS))
 #define CAL_PIT_LOOPS	5000
@@ -247,22 +284,11 @@ unsigned long native_calibrate_tsc(void)
 			continue;
 
 		tsc2 = (tsc2 - tsc1) * 1000000LL;
+		if (hpet)
+			tsc2 = calc_hpet_ref(tsc2, hpet1, hpet2);
+		else
+			tsc2 = calc_pmtimer_ref(tsc2, pm1, pm2);
 
-		if (hpet) {
-			if (hpet2 < hpet1)
-				hpet2 += 0x100000000ULL;
-			hpet2 -= hpet1;
-			tsc1 = ((u64)hpet2 * hpet_readl(HPET_PERIOD));
-			do_div(tsc1, 1000000);
-		} else {
-			if (pm2 < pm1)
-				pm2 += (u64)ACPI_PM_OVRRUN;
-			pm2 -= pm1;
-			tsc1 = pm2 * 1000000000LL;
-			do_div(tsc1, PMTMR_TICKS_PER_SEC);
-		}
-
-		do_div(tsc2, tsc1);
 		tsc_ref_min = min(tsc_ref_min, (unsigned long) tsc2);
 	}
 

commit cce3e057242d3d46fea07b9eb3910b0076419be5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 4 15:18:44 2008 +0000

    x86: TSC: define the PIT latch value separate
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 346cae5ac423..aa11413e7c1d 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -122,6 +122,10 @@ static u64 tsc_read_refs(u64 *pm, u64 *hpet)
 	return ULLONG_MAX;
 }
 
+#define CAL_MS		50
+#define CAL_LATCH	(CLOCK_TICK_RATE / (1000 / CAL_MS))
+#define CAL_PIT_LOOPS	5000
+
 /*
  * Try to calibrate the TSC against the Programmable
  * Interrupt Timer and return the frequency of the TSC
@@ -144,8 +148,8 @@ static unsigned long pit_calibrate_tsc(void)
 	 * (LSB then MSB) to begin countdown.
 	 */
 	outb(0xb0, 0x43);
-	outb((CLOCK_TICK_RATE / (1000 / 50)) & 0xff, 0x42);
-	outb((CLOCK_TICK_RATE / (1000 / 50)) >> 8, 0x42);
+	outb(CAL_LATCH & 0xff, 0x42);
+	outb(CAL_LATCH >> 8, 0x42);
 
 	tsc = t1 = t2 = get_cycles();
 
@@ -166,18 +170,18 @@ static unsigned long pit_calibrate_tsc(void)
 	/*
 	 * Sanity checks:
 	 *
-	 * If we were not able to read the PIT more than 5000
+	 * If we were not able to read the PIT more than PIT_MIN_LOOPS
 	 * times, then we have been hit by a massive SMI
 	 *
 	 * If the maximum is 10 times larger than the minimum,
 	 * then we got hit by an SMI as well.
 	 */
-	if (pitcnt < 5000 || tscmax > 10 * tscmin)
+	if (pitcnt < CAL_PIT_LOOPS || tscmax > 10 * tscmin)
 		return ULONG_MAX;
 
 	/* Calculate the PIT value */
 	delta = t2 - t1;
-	do_div(delta, 50);
+	do_div(delta, CAL_MS);
 	return delta;
 }
 

commit de014d617636d6a6bd5aef3b3d1f7f9a35669057
Author: Alok N Kataria <akataria@vmware.com>
Date:   Wed Sep 3 18:18:01 2008 -0700

    x86: Change warning message in TSC calibration.
    
    When calibration against PIT fails, the warning that we print is misleading.
    In a virtualized environment the VM may get descheduled while calibration
    or, the check in PIT calibration may fail due to other virtualization
    overheads.
    
    The warning message explicitly assumes that calibration failed due to SMI's
    which may not be the case. Change that to something proper.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 346cae5ac423..8f98e9de1b82 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -267,8 +267,7 @@ unsigned long native_calibrate_tsc(void)
 	 */
 	if (tsc_pit_min == ULONG_MAX) {
 		/* PIT gave no useful value */
-		printk(KERN_WARNING "TSC: PIT calibration failed due to "
-		       "SMI disturbance.\n");
+		printk(KERN_WARNING "TSC: Unable to calibrate against PIT\n");
 
 		/* We don't have an alternative source, disable TSC */
 		if (!hpet && !pm1 && !pm2) {

commit ec0c15afb41fd9ad45b53468b60db50170e22346
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 3 07:30:13 2008 -0700

    Split up PIT part of TSC calibration from native_calibrate_tsc
    
    The TSC calibration function is still very complicated, but this makes
    it at least a little bit less so by moving the PIT part out into a
    helper function of its own.
    
    Tested-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-of-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index ac79bd143da8..346cae5ac423 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -122,15 +122,75 @@ static u64 tsc_read_refs(u64 *pm, u64 *hpet)
 	return ULLONG_MAX;
 }
 
+/*
+ * Try to calibrate the TSC against the Programmable
+ * Interrupt Timer and return the frequency of the TSC
+ * in kHz.
+ *
+ * Return ULONG_MAX on failure to calibrate.
+ */
+static unsigned long pit_calibrate_tsc(void)
+{
+	u64 tsc, t1, t2, delta;
+	unsigned long tscmin, tscmax;
+	int pitcnt;
+
+	/* Set the Gate high, disable speaker */
+	outb((inb(0x61) & ~0x02) | 0x01, 0x61);
+
+	/*
+	 * Setup CTC channel 2* for mode 0, (interrupt on terminal
+	 * count mode), binary count. Set the latch register to 50ms
+	 * (LSB then MSB) to begin countdown.
+	 */
+	outb(0xb0, 0x43);
+	outb((CLOCK_TICK_RATE / (1000 / 50)) & 0xff, 0x42);
+	outb((CLOCK_TICK_RATE / (1000 / 50)) >> 8, 0x42);
+
+	tsc = t1 = t2 = get_cycles();
+
+	pitcnt = 0;
+	tscmax = 0;
+	tscmin = ULONG_MAX;
+	while ((inb(0x61) & 0x20) == 0) {
+		t2 = get_cycles();
+		delta = t2 - tsc;
+		tsc = t2;
+		if ((unsigned long) delta < tscmin)
+			tscmin = (unsigned int) delta;
+		if ((unsigned long) delta > tscmax)
+			tscmax = (unsigned int) delta;
+		pitcnt++;
+	}
+
+	/*
+	 * Sanity checks:
+	 *
+	 * If we were not able to read the PIT more than 5000
+	 * times, then we have been hit by a massive SMI
+	 *
+	 * If the maximum is 10 times larger than the minimum,
+	 * then we got hit by an SMI as well.
+	 */
+	if (pitcnt < 5000 || tscmax > 10 * tscmin)
+		return ULONG_MAX;
+
+	/* Calculate the PIT value */
+	delta = t2 - t1;
+	do_div(delta, 50);
+	return delta;
+}
+
+
 /**
  * native_calibrate_tsc - calibrate the tsc on boot
  */
 unsigned long native_calibrate_tsc(void)
 {
-	u64 tsc1, tsc2, tr1, tr2, tsc, delta, pm1, pm2, hpet1, hpet2;
+	u64 tsc1, tsc2, delta, pm1, pm2, hpet1, hpet2;
 	unsigned long tsc_pit_min = ULONG_MAX, tsc_ref_min = ULONG_MAX;
-	unsigned long flags, tscmin, tscmax;
-	int hpet = is_hpet_enabled(), pitcnt, i;
+	unsigned long flags;
+	int hpet = is_hpet_enabled(), i;
 
 	/*
 	 * Run 5 calibration loops to get the lowest frequency value
@@ -157,72 +217,22 @@ unsigned long native_calibrate_tsc(void)
 	 * amount of time anyway.
 	 */
 	for (i = 0; i < 5; i++) {
-
-		tscmin = ULONG_MAX;
-		tscmax = 0;
-		pitcnt = 0;
-
-		local_irq_save(flags);
+		unsigned long tsc_pit_khz;
 
 		/*
 		 * Read the start value and the reference count of
-		 * hpet/pmtimer when available:
+		 * hpet/pmtimer when available. Then do the PIT
+		 * calibration, which will take at least 50ms, and
+		 * read the end value.
 		 */
+		local_irq_save(flags);
 		tsc1 = tsc_read_refs(&pm1, hpet ? &hpet1 : NULL);
-
-		/* Set the Gate high, disable speaker */
-		outb((inb(0x61) & ~0x02) | 0x01, 0x61);
-
-		/*
-		 * Setup CTC channel 2* for mode 0, (interrupt on terminal
-		 * count mode), binary count. Set the latch register to 50ms
-		 * (LSB then MSB) to begin countdown.
-		 *
-		 * Some devices need a delay here.
-		 */
-		outb(0xb0, 0x43);
-		outb((CLOCK_TICK_RATE / (1000 / 50)) & 0xff, 0x42);
-		outb((CLOCK_TICK_RATE / (1000 / 50)) >> 8, 0x42);
-
-		tsc = tr1 = tr2 = get_cycles();
-
-		while ((inb(0x61) & 0x20) == 0) {
-			tr2 = get_cycles();
-			delta = tr2 - tsc;
-			tsc = tr2;
-			if ((unsigned int) delta < tscmin)
-				tscmin = (unsigned int) delta;
-			if ((unsigned int) delta > tscmax)
-				tscmax = (unsigned int) delta;
-			pitcnt++;
-		}
-
-		/*
-		 * We waited at least 50ms above. Now read
-		 * pmtimer/hpet reference again
-		 */
+		tsc_pit_khz = pit_calibrate_tsc();
 		tsc2 = tsc_read_refs(&pm2, hpet ? &hpet2 : NULL);
-
 		local_irq_restore(flags);
 
-		/*
-		 * Sanity checks:
-		 *
-		 * If we were not able to read the PIT more than 5000
-		 * times, then we have been hit by a massive SMI
-		 *
-		 * If the maximum is 10 times larger than the minimum,
-		 * then we got hit by an SMI as well.
-		 */
-		if (pitcnt > 5000 && tscmax < 10 * tscmin) {
-
-			/* Calculate the PIT value */
-			delta = tr2 - tr1;
-			do_div(delta, 50);
-
-			/* We take the smallest value into account */
-			tsc_pit_min = min(tsc_pit_min, (unsigned long) delta);
-		}
+		/* Pick the lowest PIT TSC calibration so far */
+		tsc_pit_min = min(tsc_pit_min, tsc_pit_khz);
 
 		/* hpet or pmtimer available ? */
 		if (!hpet && !pm1 && !pm2)

commit fbb16e243887332dd5754e48ffe5b963378f3cd2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 3 00:54:47 2008 +0200

    [x86] Fix TSC calibration issues
    
    Larry Finger reported at http://lkml.org/lkml/2008/9/1/90:
    An ancient laptop of mine started throwing errors from b43legacy when
    I started using 2.6.27 on it. This has been bisected to commit bfc0f59
    "x86: merge tsc calibration".
    
    The unification of the TSC code adopted mostly the 64bit code, which
    prefers PMTIMER/HPET over the PIT calibration.
    
    Larrys system has an AMD K6 CPU. Such systems are known to have
    PMTIMER incarnations which run at double speed. This results in a
    miscalibration of the TSC by factor 0.5. So the resulting calibrated
    CPU/TSC speed is half of the real CPU speed, which means that the TSC
    based delay loop will run half the time it should run. That might
    explain why the b43legacy driver went berserk.
    
    On the other hand we know about systems, where the PIT based
    calibration results in random crap due to heavy SMI/SMM
    disturbance. On those systems the PMTIMER/HPET based calibration logic
    with SMI detection shows better results.
    
    According to Alok also virtualized systems suffer from the PIT
    calibration method.
    
    The solution is to use a more wreckage aware aproach than the current
    either/or decision.
    
    1) reimplement the retry loop which was dropped from the 32bit code
    during the merge. It repeats the calibration and selects the lowest
    frequency value as this is probably the closest estimate to the real
    frequency
    
    2) Monitor the delta of the TSC values in the delay loop which waits
    for the PIT counter to reach zero. If the maximum value is
    significantly different from the minimum, then we have a pretty safe
    indicator that the loop was disturbed by an SMI.
    
    3) keep the pmtimer/hpet reference as a backup solution for systems
    where the SMI disturbance is a permanent point of failure for PIT
    based calibration
    
    4) do the loop iteration for both methods, record the lowest value and
    decide after all iterations finished.
    
    5) Set a clear preference to PIT based calibration when the result
    makes sense.
    
    The implementation does the reference calibration based on
    HPET/PMTIMER around the delay, which is necessary for the PIT anyway,
    but keeps separate TSC values to ensure the "independency" of the
    resulting calibration values.
    
    Tested on various 32bit/64bit machines including Geode 266Mhz, AMD K6
    (affected machine with a double speed pmtimer which I grabbed out of
    the dump), Pentium class machines and AMD/Intel 64 bit boxen.
    
    Bisected-by:  Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 8e786b0d665a..ac79bd143da8 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -127,75 +127,202 @@ static u64 tsc_read_refs(u64 *pm, u64 *hpet)
  */
 unsigned long native_calibrate_tsc(void)
 {
-	unsigned long flags;
-	u64 tsc1, tsc2, tr1, tr2, delta, pm1, pm2, hpet1, hpet2;
-	int hpet = is_hpet_enabled();
-	unsigned int tsc_khz_val = 0;
+	u64 tsc1, tsc2, tr1, tr2, tsc, delta, pm1, pm2, hpet1, hpet2;
+	unsigned long tsc_pit_min = ULONG_MAX, tsc_ref_min = ULONG_MAX;
+	unsigned long flags, tscmin, tscmax;
+	int hpet = is_hpet_enabled(), pitcnt, i;
 
-	local_irq_save(flags);
-
-	tsc1 = tsc_read_refs(&pm1, hpet ? &hpet1 : NULL);
-
-	outb((inb(0x61) & ~0x02) | 0x01, 0x61);
-
-	outb(0xb0, 0x43);
-	outb((CLOCK_TICK_RATE / (1000 / 50)) & 0xff, 0x42);
-	outb((CLOCK_TICK_RATE / (1000 / 50)) >> 8, 0x42);
-	tr1 = get_cycles();
-	while ((inb(0x61) & 0x20) == 0);
-	tr2 = get_cycles();
-
-	tsc2 = tsc_read_refs(&pm2, hpet ? &hpet2 : NULL);
-
-	local_irq_restore(flags);
+	/*
+	 * Run 5 calibration loops to get the lowest frequency value
+	 * (the best estimate). We use two different calibration modes
+	 * here:
+	 *
+	 * 1) PIT loop. We set the PIT Channel 2 to oneshot mode and
+	 * load a timeout of 50ms. We read the time right after we
+	 * started the timer and wait until the PIT count down reaches
+	 * zero. In each wait loop iteration we read the TSC and check
+	 * the delta to the previous read. We keep track of the min
+	 * and max values of that delta. The delta is mostly defined
+	 * by the IO time of the PIT access, so we can detect when a
+	 * SMI/SMM disturbance happend between the two reads. If the
+	 * maximum time is significantly larger than the minimum time,
+	 * then we discard the result and have another try.
+	 *
+	 * 2) Reference counter. If available we use the HPET or the
+	 * PMTIMER as a reference to check the sanity of that value.
+	 * We use separate TSC readouts and check inside of the
+	 * reference read for a SMI/SMM disturbance. We dicard
+	 * disturbed values here as well. We do that around the PIT
+	 * calibration delay loop as we have to wait for a certain
+	 * amount of time anyway.
+	 */
+	for (i = 0; i < 5; i++) {
+
+		tscmin = ULONG_MAX;
+		tscmax = 0;
+		pitcnt = 0;
+
+		local_irq_save(flags);
+
+		/*
+		 * Read the start value and the reference count of
+		 * hpet/pmtimer when available:
+		 */
+		tsc1 = tsc_read_refs(&pm1, hpet ? &hpet1 : NULL);
+
+		/* Set the Gate high, disable speaker */
+		outb((inb(0x61) & ~0x02) | 0x01, 0x61);
+
+		/*
+		 * Setup CTC channel 2* for mode 0, (interrupt on terminal
+		 * count mode), binary count. Set the latch register to 50ms
+		 * (LSB then MSB) to begin countdown.
+		 *
+		 * Some devices need a delay here.
+		 */
+		outb(0xb0, 0x43);
+		outb((CLOCK_TICK_RATE / (1000 / 50)) & 0xff, 0x42);
+		outb((CLOCK_TICK_RATE / (1000 / 50)) >> 8, 0x42);
+
+		tsc = tr1 = tr2 = get_cycles();
+
+		while ((inb(0x61) & 0x20) == 0) {
+			tr2 = get_cycles();
+			delta = tr2 - tsc;
+			tsc = tr2;
+			if ((unsigned int) delta < tscmin)
+				tscmin = (unsigned int) delta;
+			if ((unsigned int) delta > tscmax)
+				tscmax = (unsigned int) delta;
+			pitcnt++;
+		}
+
+		/*
+		 * We waited at least 50ms above. Now read
+		 * pmtimer/hpet reference again
+		 */
+		tsc2 = tsc_read_refs(&pm2, hpet ? &hpet2 : NULL);
+
+		local_irq_restore(flags);
+
+		/*
+		 * Sanity checks:
+		 *
+		 * If we were not able to read the PIT more than 5000
+		 * times, then we have been hit by a massive SMI
+		 *
+		 * If the maximum is 10 times larger than the minimum,
+		 * then we got hit by an SMI as well.
+		 */
+		if (pitcnt > 5000 && tscmax < 10 * tscmin) {
+
+			/* Calculate the PIT value */
+			delta = tr2 - tr1;
+			do_div(delta, 50);
+
+			/* We take the smallest value into account */
+			tsc_pit_min = min(tsc_pit_min, (unsigned long) delta);
+		}
+
+		/* hpet or pmtimer available ? */
+		if (!hpet && !pm1 && !pm2)
+			continue;
+
+		/* Check, whether the sampling was disturbed by an SMI */
+		if (tsc1 == ULLONG_MAX || tsc2 == ULLONG_MAX)
+			continue;
+
+		tsc2 = (tsc2 - tsc1) * 1000000LL;
+
+		if (hpet) {
+			if (hpet2 < hpet1)
+				hpet2 += 0x100000000ULL;
+			hpet2 -= hpet1;
+			tsc1 = ((u64)hpet2 * hpet_readl(HPET_PERIOD));
+			do_div(tsc1, 1000000);
+		} else {
+			if (pm2 < pm1)
+				pm2 += (u64)ACPI_PM_OVRRUN;
+			pm2 -= pm1;
+			tsc1 = pm2 * 1000000000LL;
+			do_div(tsc1, PMTMR_TICKS_PER_SEC);
+		}
+
+		do_div(tsc2, tsc1);
+		tsc_ref_min = min(tsc_ref_min, (unsigned long) tsc2);
+	}
 
 	/*
-	 * Preset the result with the raw and inaccurate PIT
-	 * calibration value
+	 * Now check the results.
 	 */
-	delta = (tr2 - tr1);
-	do_div(delta, 50);
-	tsc_khz_val = delta;
+	if (tsc_pit_min == ULONG_MAX) {
+		/* PIT gave no useful value */
+		printk(KERN_WARNING "TSC: PIT calibration failed due to "
+		       "SMI disturbance.\n");
+
+		/* We don't have an alternative source, disable TSC */
+		if (!hpet && !pm1 && !pm2) {
+			printk("TSC: No reference (HPET/PMTIMER) available\n");
+			return 0;
+		}
+
+		/* The alternative source failed as well, disable TSC */
+		if (tsc_ref_min == ULONG_MAX) {
+			printk(KERN_WARNING "TSC: HPET/PMTIMER calibration "
+			       "failed due to SMI disturbance.\n");
+			return 0;
+		}
+
+		/* Use the alternative source */
+		printk(KERN_INFO "TSC: using %s reference calibration\n",
+		       hpet ? "HPET" : "PMTIMER");
+
+		return tsc_ref_min;
+	}
 
-	/* hpet or pmtimer available ? */
+	/* We don't have an alternative source, use the PIT calibration value */
 	if (!hpet && !pm1 && !pm2) {
-		printk(KERN_INFO "TSC calibrated against PIT\n");
-		goto out;
+		printk(KERN_INFO "TSC: Using PIT calibration value\n");
+		return tsc_pit_min;
 	}
 
-	/* Check, whether the sampling was disturbed by an SMI */
-	if (tsc1 == ULLONG_MAX || tsc2 == ULLONG_MAX) {
-		printk(KERN_WARNING "TSC calibration disturbed by SMI, "
-				"using PIT calibration result\n");
-		goto out;
+	/* The alternative source failed, use the PIT calibration value */
+	if (tsc_ref_min == ULONG_MAX) {
+		printk(KERN_WARNING "TSC: HPET/PMTIMER calibration failed due "
+		       "to SMI disturbance. Using PIT calibration\n");
+		return tsc_pit_min;
 	}
 
-	tsc2 = (tsc2 - tsc1) * 1000000LL;
-
-	if (hpet) {
-		printk(KERN_INFO "TSC calibrated against HPET\n");
-		if (hpet2 < hpet1)
-			hpet2 += 0x100000000ULL;
-		hpet2 -= hpet1;
-		tsc1 = ((u64)hpet2 * hpet_readl(HPET_PERIOD));
-		do_div(tsc1, 1000000);
-	} else {
-		printk(KERN_INFO "TSC calibrated against PM_TIMER\n");
-		if (pm2 < pm1)
-			pm2 += (u64)ACPI_PM_OVRRUN;
-		pm2 -= pm1;
-		tsc1 = pm2 * 1000000000LL;
-		do_div(tsc1, PMTMR_TICKS_PER_SEC);
+	/* Check the reference deviation */
+	delta = ((u64) tsc_pit_min) * 100;
+	do_div(delta, tsc_ref_min);
+
+	/*
+	 * If both calibration results are inside a 5% window, the we
+	 * use the lower frequency of those as it is probably the
+	 * closest estimate.
+	 */
+	if (delta >= 95 && delta <= 105) {
+		printk(KERN_INFO "TSC: PIT calibration confirmed by %s.\n",
+		       hpet ? "HPET" : "PMTIMER");
+		printk(KERN_INFO "TSC: using %s calibration value\n",
+		       tsc_pit_min <= tsc_ref_min ? "PIT" :
+		       hpet ? "HPET" : "PMTIMER");
+		return tsc_pit_min <= tsc_ref_min ? tsc_pit_min : tsc_ref_min;
 	}
 
-	do_div(tsc2, tsc1);
-	tsc_khz_val = tsc2;
+	printk(KERN_WARNING "TSC: PIT calibration deviates from %s: %lu %lu.\n",
+	       hpet ? "HPET" : "PMTIMER", tsc_pit_min, tsc_ref_min);
 
-out:
-	return tsc_khz_val;
+	/*
+	 * The calibration values differ too much. In doubt, we use
+	 * the PIT value as we know that there are PMTIMERs around
+	 * running at double speed.
+	 */
+	printk(KERN_INFO "TSC: Using PIT calibration value\n");
+	return tsc_pit_min;
 }
 
-
 #ifdef CONFIG_X86_32
 /* Only called from the Powernow K7 cpu freq driver */
 int recalibrate_cpu_khz(void)

commit 52a8968ce95da8469ba0a9b3e4010fe31caf77a3
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Aug 25 13:35:06 2008 +0200

    x86: fix cpufreq + sched_clock() regression
    
    I noticed that my sched_clock() was slow on a number of machine, so I
    started looking at cpufreq.
    
    The below seems to fix the problem for me.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 9bed5cae4bdc..8e786b0d665a 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -314,7 +314,7 @@ static int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
 			mark_tsc_unstable("cpufreq changes");
 	}
 
-	set_cyc2ns_scale(tsc_khz_ref, freq->cpu);
+	set_cyc2ns_scale(tsc_khz, freq->cpu);
 
 	return 0;
 }

commit 060700b571717c997a2ea5e2049b848fa248ee13
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Aug 24 11:52:06 2008 -0700

    x86: do not enable TSC notifier if we don't need it
    
    Impact: crash on non-TSC-equipped CPUs
    
    Don't enable the TSC notifier if we *either*:
    
    1. don't have a CPU, or
    2. have a CPU with constant TSC.
    
    In either of those cases, the notifier is either damaging (1) or useless(2).
    
    From: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 46af71676738..9bed5cae4bdc 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -325,6 +325,10 @@ static struct notifier_block time_cpufreq_notifier_block = {
 
 static int __init cpufreq_tsc(void)
 {
+	if (!cpu_has_tsc)
+		return 0;
+	if (boot_cpu_has(X86_FEATURE_CONSTANT_TSC))
+		return 0;
 	cpufreq_register_notifier(&time_cpufreq_notifier_block,
 				CPUFREQ_TRANSITION_NOTIFIER);
 	return 0;

commit d554d9a4295dd0595d12eeccbc55d1f495b15176
Author: Marcin Slusarz <marcin.slusarz@gmail.com>
Date:   Mon Aug 11 00:07:44 2008 +0200

    x86, tsc: fix section mismatch warning
    
    WARNING: vmlinux.o(.text+0x7950): Section mismatch in reference from the function native_calibrate_tsc() to the function .init.text:tsc_read_refs()
    The function native_calibrate_tsc() references
    the function __init tsc_read_refs().
    This is often because native_calibrate_tsc lacks a __init
    annotation or the annotation of tsc_read_refs is wrong.
    
    tsc_read_refs is called from native_calibrate_tsc which is not __init
    and native_calibrate_tsc cannot be marked __init
    
    Signed-off-by: Marcin Slusarz <marcin.slusarz@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 7603c0553909..46af71676738 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -104,7 +104,7 @@ __setup("notsc", notsc_setup);
 /*
  * Read TSC and the reference counters. Take care of SMI disturbance
  */
-static u64 __init tsc_read_refs(u64 *pm, u64 *hpet)
+static u64 tsc_read_refs(u64 *pm, u64 *hpet)
 {
 	u64 t1, t2;
 	int i;

commit 431ceb83f703a343bdd14350480a2224fa4bfedf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 15 22:08:04 2008 +0200

    x86: fix TSC build error on 32bit
    
    Dave Hansen reported a build error on 32bit which went unnoticed
    as newer gcc versions seem to optimize unused static functions
    away before compiling them.
    
    Make vread_tsc() depend on CONFIG_X86_64
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3c36f92160c9..7603c0553909 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -358,6 +358,7 @@ static cycle_t read_tsc(void)
 		ret : clocksource_tsc.cycle_last;
 }
 
+#ifdef CONFIG_X86_64
 static cycle_t __vsyscall_fn vread_tsc(void)
 {
 	cycle_t ret = (cycle_t)vget_cycles();
@@ -365,6 +366,7 @@ static cycle_t __vsyscall_fn vread_tsc(void)
 	return ret >= __vsyscall_gtod_data.clock.cycle_last ?
 		ret : __vsyscall_gtod_data.clock.cycle_last;
 }
+#endif
 
 static struct clocksource clocksource_tsc = {
 	.name                   = "tsc",

commit e54afe38630e3b577968428f48ed8ef1e13a2a15
Author: Glauber Costa <gcosta@redhat.com>
Date:   Thu Jul 10 14:01:47 2008 -0300

    x86: remove duplicate call to use_tsc_delay
    
    Integration generated a duplicate call to use_tsc_delay.
    Particularly, the one that is done before we check for general
    tsc usability seems wrong.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 4a775d001957..3c36f92160c9 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -513,7 +513,6 @@ void __init tsc_init(void)
 	 */
 	for_each_possible_cpu(cpu)
 		set_cyc2ns_scale(cpu_khz, cpu);
-	use_tsc_delay();
 
 	if (tsc_disabled > 0)
 		return;

commit 0a4d8a472f645d99f86303db1462b64e371b090d
Author: Glauber Costa <gcosta@redhat.com>
Date:   Tue Jun 24 09:34:08 2008 -0300

    x86: provide delay loop for x86_64.
    
    This is for consistency with i386. We call use_tsc_delay()
    at tsc initialization for x86_64, so we'll be always using it.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3c36f92160c9..4a775d001957 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -513,6 +513,7 @@ void __init tsc_init(void)
 	 */
 	for_each_possible_cpu(cpu)
 		set_cyc2ns_scale(cpu_khz, cpu);
+	use_tsc_delay();
 
 	if (tsc_disabled > 0)
 		return;

commit e93ef949fd9a3f237aedfb8e64414b28980530b8
Author: Alok Kataria <akataria@vmware.com>
Date:   Tue Jul 1 11:43:36 2008 -0700

    x86: rename paravirtualized TSC functions
    
    Rename the paravirtualized calculate_cpu_khz to calibrate_tsc.
    In all cases, we actually calibrate_tsc and use that as the cpu_khz value.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Cc: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 94c16bdd5696..3c36f92160c9 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -123,9 +123,9 @@ static u64 __init tsc_read_refs(u64 *pm, u64 *hpet)
 }
 
 /**
- * tsc_calibrate - calibrate the tsc on boot
+ * native_calibrate_tsc - calibrate the tsc on boot
  */
-static unsigned int __init tsc_calibrate(void)
+unsigned long native_calibrate_tsc(void)
 {
 	unsigned long flags;
 	u64 tsc1, tsc2, tr1, tr2, delta, pm1, pm2, hpet1, hpet2;
@@ -195,10 +195,6 @@ static unsigned int __init tsc_calibrate(void)
 	return tsc_khz_val;
 }
 
-unsigned long native_calculate_cpu_khz(void)
-{
-	return tsc_calibrate();
-}
 
 #ifdef CONFIG_X86_32
 /* Only called from the Powernow K7 cpu freq driver */
@@ -208,8 +204,8 @@ int recalibrate_cpu_khz(void)
 	unsigned long cpu_khz_old = cpu_khz;
 
 	if (cpu_has_tsc) {
-		cpu_khz = calculate_cpu_khz();
-		tsc_khz = cpu_khz;
+		tsc_khz = calibrate_tsc();
+		cpu_khz = tsc_khz;
 		cpu_data(0).loops_per_jiffy =
 			cpufreq_scale(cpu_data(0).loops_per_jiffy,
 					cpu_khz_old, cpu_khz);
@@ -487,10 +483,10 @@ void __init tsc_init(void)
 	if (!cpu_has_tsc)
 		return;
 
-	cpu_khz = calculate_cpu_khz();
-	tsc_khz = cpu_khz;
+	tsc_khz = calibrate_tsc();
+	cpu_khz = tsc_khz;
 
-	if (!cpu_khz) {
+	if (!tsc_khz) {
 		mark_tsc_unstable("could not calculate TSC khz");
 		return;
 	}

commit 8fbbc4b45ce3e4c0eeb15004c79c72b6896a79c2
Author: Alok Kataria <akataria@vmware.com>
Date:   Tue Jul 1 11:43:34 2008 -0700

    x86: merge tsc_init and clocksource code
    
    Unify the clocksource code.
    Unify the tsc_init code.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Cc: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 595f78a22212..94c16bdd5696 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -5,8 +5,16 @@
 #include <linux/timer.h>
 #include <linux/acpi_pmtmr.h>
 #include <linux/cpufreq.h>
+#include <linux/dmi.h>
+#include <linux/delay.h>
+#include <linux/clocksource.h>
+#include <linux/percpu.h>
 
 #include <asm/hpet.h>
+#include <asm/timer.h>
+#include <asm/vgtod.h>
+#include <asm/time.h>
+#include <asm/delay.h>
 
 unsigned int cpu_khz;           /* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
@@ -16,12 +24,12 @@ EXPORT_SYMBOL(tsc_khz);
 /*
  * TSC can be unstable due to cpufreq or due to unsynced TSCs
  */
-int tsc_unstable;
+static int tsc_unstable;
 
 /* native_sched_clock() is called before tsc_init(), so
    we must start with the TSC soft disabled to prevent
    erroneous rdtsc usage on !cpu_has_tsc processors */
-int tsc_disabled = -1;
+static int tsc_disabled = -1;
 
 /*
  * Scheduler clock - returns current time in nanosec units.
@@ -241,7 +249,7 @@ EXPORT_SYMBOL(recalibrate_cpu_khz);
 
 DEFINE_PER_CPU(unsigned long, cyc2ns);
 
-void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
+static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
 {
 	unsigned long long tsc_now, ns_now;
 	unsigned long flags, *scale;
@@ -329,3 +337,201 @@ static int __init cpufreq_tsc(void)
 core_initcall(cpufreq_tsc);
 
 #endif /* CONFIG_CPU_FREQ */
+
+/* clocksource code */
+
+static struct clocksource clocksource_tsc;
+
+/*
+ * We compare the TSC to the cycle_last value in the clocksource
+ * structure to avoid a nasty time-warp. This can be observed in a
+ * very small window right after one CPU updated cycle_last under
+ * xtime/vsyscall_gtod lock and the other CPU reads a TSC value which
+ * is smaller than the cycle_last reference value due to a TSC which
+ * is slighty behind. This delta is nowhere else observable, but in
+ * that case it results in a forward time jump in the range of hours
+ * due to the unsigned delta calculation of the time keeping core
+ * code, which is necessary to support wrapping clocksources like pm
+ * timer.
+ */
+static cycle_t read_tsc(void)
+{
+	cycle_t ret = (cycle_t)get_cycles();
+
+	return ret >= clocksource_tsc.cycle_last ?
+		ret : clocksource_tsc.cycle_last;
+}
+
+static cycle_t __vsyscall_fn vread_tsc(void)
+{
+	cycle_t ret = (cycle_t)vget_cycles();
+
+	return ret >= __vsyscall_gtod_data.clock.cycle_last ?
+		ret : __vsyscall_gtod_data.clock.cycle_last;
+}
+
+static struct clocksource clocksource_tsc = {
+	.name                   = "tsc",
+	.rating                 = 300,
+	.read                   = read_tsc,
+	.mask                   = CLOCKSOURCE_MASK(64),
+	.shift                  = 22,
+	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
+				  CLOCK_SOURCE_MUST_VERIFY,
+#ifdef CONFIG_X86_64
+	.vread                  = vread_tsc,
+#endif
+};
+
+void mark_tsc_unstable(char *reason)
+{
+	if (!tsc_unstable) {
+		tsc_unstable = 1;
+		printk("Marking TSC unstable due to %s\n", reason);
+		/* Change only the rating, when not registered */
+		if (clocksource_tsc.mult)
+			clocksource_change_rating(&clocksource_tsc, 0);
+		else
+			clocksource_tsc.rating = 0;
+	}
+}
+
+EXPORT_SYMBOL_GPL(mark_tsc_unstable);
+
+static int __init dmi_mark_tsc_unstable(const struct dmi_system_id *d)
+{
+	printk(KERN_NOTICE "%s detected: marking TSC unstable.\n",
+			d->ident);
+	tsc_unstable = 1;
+	return 0;
+}
+
+/* List of systems that have known TSC problems */
+static struct dmi_system_id __initdata bad_tsc_dmi_table[] = {
+	{
+		.callback = dmi_mark_tsc_unstable,
+		.ident = "IBM Thinkpad 380XD",
+		.matches = {
+			DMI_MATCH(DMI_BOARD_VENDOR, "IBM"),
+			DMI_MATCH(DMI_BOARD_NAME, "2635FA0"),
+		},
+	},
+	{}
+};
+
+/*
+ * Geode_LX - the OLPC CPU has a possibly a very reliable TSC
+ */
+#ifdef CONFIG_MGEODE_LX
+/* RTSC counts during suspend */
+#define RTSC_SUSP 0x100
+
+static void __init check_geode_tsc_reliable(void)
+{
+	unsigned long res_low, res_high;
+
+	rdmsr_safe(MSR_GEODE_BUSCONT_CONF0, &res_low, &res_high);
+	if (res_low & RTSC_SUSP)
+		clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
+}
+#else
+static inline void check_geode_tsc_reliable(void) { }
+#endif
+
+/*
+ * Make an educated guess if the TSC is trustworthy and synchronized
+ * over all CPUs.
+ */
+__cpuinit int unsynchronized_tsc(void)
+{
+	if (!cpu_has_tsc || tsc_unstable)
+		return 1;
+
+#ifdef CONFIG_SMP
+	if (apic_is_clustered_box())
+		return 1;
+#endif
+
+	if (boot_cpu_has(X86_FEATURE_CONSTANT_TSC))
+		return 0;
+	/*
+	 * Intel systems are normally all synchronized.
+	 * Exceptions must mark TSC as unstable:
+	 */
+	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL) {
+		/* assume multi socket systems are not synchronized: */
+		if (num_possible_cpus() > 1)
+			tsc_unstable = 1;
+	}
+
+	return tsc_unstable;
+}
+
+static void __init init_tsc_clocksource(void)
+{
+	clocksource_tsc.mult = clocksource_khz2mult(tsc_khz,
+			clocksource_tsc.shift);
+	/* lower the rating if we already know its unstable: */
+	if (check_tsc_unstable()) {
+		clocksource_tsc.rating = 0;
+		clocksource_tsc.flags &= ~CLOCK_SOURCE_IS_CONTINUOUS;
+	}
+	clocksource_register(&clocksource_tsc);
+}
+
+void __init tsc_init(void)
+{
+	u64 lpj;
+	int cpu;
+
+	if (!cpu_has_tsc)
+		return;
+
+	cpu_khz = calculate_cpu_khz();
+	tsc_khz = cpu_khz;
+
+	if (!cpu_khz) {
+		mark_tsc_unstable("could not calculate TSC khz");
+		return;
+	}
+
+#ifdef CONFIG_X86_64
+	if (cpu_has(&boot_cpu_data, X86_FEATURE_CONSTANT_TSC) &&
+			(boot_cpu_data.x86_vendor == X86_VENDOR_AMD))
+		cpu_khz = calibrate_cpu();
+#endif
+
+	lpj = ((u64)tsc_khz * 1000);
+	do_div(lpj, HZ);
+	lpj_fine = lpj;
+
+	printk("Detected %lu.%03lu MHz processor.\n",
+			(unsigned long)cpu_khz / 1000,
+			(unsigned long)cpu_khz % 1000);
+
+	/*
+	 * Secondary CPUs do not run through tsc_init(), so set up
+	 * all the scale factors for all CPUs, assuming the same
+	 * speed as the bootup CPU. (cpufreq notifiers will fix this
+	 * up if their speed diverges)
+	 */
+	for_each_possible_cpu(cpu)
+		set_cyc2ns_scale(cpu_khz, cpu);
+
+	if (tsc_disabled > 0)
+		return;
+
+	/* now allow native_sched_clock() to use rdtsc */
+	tsc_disabled = 0;
+
+	use_tsc_delay();
+	/* Check and install the TSC clocksource */
+	dmi_check_system(bad_tsc_dmi_table);
+
+	if (unsynchronized_tsc())
+		mark_tsc_unstable("TSCs unsynchronized");
+
+	check_geode_tsc_reliable();
+	init_tsc_clocksource();
+}
+

commit 2dbe06faf37b39f9ecffc054dd173b2a1dc2adcd
Author: Alok Kataria <akataria@vmware.com>
Date:   Tue Jul 1 11:43:31 2008 -0700

    x86: merge the TSC cpu-freq code
    
    Unify the TSC cpufreq code.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Cc: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index e6ee14533c75..595f78a22212 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -4,6 +4,7 @@
 #include <linux/module.h>
 #include <linux/timer.h>
 #include <linux/acpi_pmtmr.h>
+#include <linux/cpufreq.h>
 
 #include <asm/hpet.h>
 
@@ -215,3 +216,116 @@ int recalibrate_cpu_khz(void)
 EXPORT_SYMBOL(recalibrate_cpu_khz);
 
 #endif /* CONFIG_X86_32 */
+
+/* Accelerators for sched_clock()
+ * convert from cycles(64bits) => nanoseconds (64bits)
+ *  basic equation:
+ *              ns = cycles / (freq / ns_per_sec)
+ *              ns = cycles * (ns_per_sec / freq)
+ *              ns = cycles * (10^9 / (cpu_khz * 10^3))
+ *              ns = cycles * (10^6 / cpu_khz)
+ *
+ *      Then we use scaling math (suggested by george@mvista.com) to get:
+ *              ns = cycles * (10^6 * SC / cpu_khz) / SC
+ *              ns = cycles * cyc2ns_scale / SC
+ *
+ *      And since SC is a constant power of two, we can convert the div
+ *  into a shift.
+ *
+ *  We can use khz divisor instead of mhz to keep a better precision, since
+ *  cyc2ns_scale is limited to 10^6 * 2^10, which fits in 32 bits.
+ *  (mathieu.desnoyers@polymtl.ca)
+ *
+ *                      -johnstul@us.ibm.com "math is hard, lets go shopping!"
+ */
+
+DEFINE_PER_CPU(unsigned long, cyc2ns);
+
+void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
+{
+	unsigned long long tsc_now, ns_now;
+	unsigned long flags, *scale;
+
+	local_irq_save(flags);
+	sched_clock_idle_sleep_event();
+
+	scale = &per_cpu(cyc2ns, cpu);
+
+	rdtscll(tsc_now);
+	ns_now = __cycles_2_ns(tsc_now);
+
+	if (cpu_khz)
+		*scale = (NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR)/cpu_khz;
+
+	sched_clock_idle_wakeup_event(0);
+	local_irq_restore(flags);
+}
+
+#ifdef CONFIG_CPU_FREQ
+
+/* Frequency scaling support. Adjust the TSC based timer when the cpu frequency
+ * changes.
+ *
+ * RED-PEN: On SMP we assume all CPUs run with the same frequency.  It's
+ * not that important because current Opteron setups do not support
+ * scaling on SMP anyroads.
+ *
+ * Should fix up last_tsc too. Currently gettimeofday in the
+ * first tick after the change will be slightly wrong.
+ */
+
+static unsigned int  ref_freq;
+static unsigned long loops_per_jiffy_ref;
+static unsigned long tsc_khz_ref;
+
+static int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
+				void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	unsigned long *lpj, dummy;
+
+	if (cpu_has(&cpu_data(freq->cpu), X86_FEATURE_CONSTANT_TSC))
+		return 0;
+
+	lpj = &dummy;
+	if (!(freq->flags & CPUFREQ_CONST_LOOPS))
+#ifdef CONFIG_SMP
+		lpj = &cpu_data(freq->cpu).loops_per_jiffy;
+#else
+	lpj = &boot_cpu_data.loops_per_jiffy;
+#endif
+
+	if (!ref_freq) {
+		ref_freq = freq->old;
+		loops_per_jiffy_ref = *lpj;
+		tsc_khz_ref = tsc_khz;
+	}
+	if ((val == CPUFREQ_PRECHANGE  && freq->old < freq->new) ||
+			(val == CPUFREQ_POSTCHANGE && freq->old > freq->new) ||
+			(val == CPUFREQ_RESUMECHANGE)) {
+		*lpj = 	cpufreq_scale(loops_per_jiffy_ref, ref_freq, freq->new);
+
+		tsc_khz = cpufreq_scale(tsc_khz_ref, ref_freq, freq->new);
+		if (!(freq->flags & CPUFREQ_CONST_LOOPS))
+			mark_tsc_unstable("cpufreq changes");
+	}
+
+	set_cyc2ns_scale(tsc_khz_ref, freq->cpu);
+
+	return 0;
+}
+
+static struct notifier_block time_cpufreq_notifier_block = {
+	.notifier_call  = time_cpufreq_notifier
+};
+
+static int __init cpufreq_tsc(void)
+{
+	cpufreq_register_notifier(&time_cpufreq_notifier_block,
+				CPUFREQ_TRANSITION_NOTIFIER);
+	return 0;
+}
+
+core_initcall(cpufreq_tsc);
+
+#endif /* CONFIG_CPU_FREQ */

commit bfc0f5947afa5e3a13e55867f4478c8a92c11dca
Author: Alok Kataria <akataria@vmware.com>
Date:   Tue Jul 1 11:43:24 2008 -0700

    x86: merge tsc calibration
    
    Merge the tsc calibration code for the 32bit and 64bit kernel.
    The paravirtualized calculate_cpu_khz for 64bit now points to the correct
    tsc_calibrate code as in 32bit.
    Original native_calculate_cpu_khz for 64 bit is now called as calibrate_cpu.
    
    Also moved the recalibrate_cpu_khz function in the common file.
    Note that this function is called only from powernow K7 cpu freq driver.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Cc: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 5d0be778fadd..e6ee14533c75 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1,7 +1,11 @@
+#include <linux/kernel.h>
 #include <linux/sched.h>
 #include <linux/init.h>
 #include <linux/module.h>
 #include <linux/timer.h>
+#include <linux/acpi_pmtmr.h>
+
+#include <asm/hpet.h>
 
 unsigned int cpu_khz;           /* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
@@ -84,3 +88,130 @@ int __init notsc_setup(char *str)
 #endif
 
 __setup("notsc", notsc_setup);
+
+#define MAX_RETRIES     5
+#define SMI_TRESHOLD    50000
+
+/*
+ * Read TSC and the reference counters. Take care of SMI disturbance
+ */
+static u64 __init tsc_read_refs(u64 *pm, u64 *hpet)
+{
+	u64 t1, t2;
+	int i;
+
+	for (i = 0; i < MAX_RETRIES; i++) {
+		t1 = get_cycles();
+		if (hpet)
+			*hpet = hpet_readl(HPET_COUNTER) & 0xFFFFFFFF;
+		else
+			*pm = acpi_pm_read_early();
+		t2 = get_cycles();
+		if ((t2 - t1) < SMI_TRESHOLD)
+			return t2;
+	}
+	return ULLONG_MAX;
+}
+
+/**
+ * tsc_calibrate - calibrate the tsc on boot
+ */
+static unsigned int __init tsc_calibrate(void)
+{
+	unsigned long flags;
+	u64 tsc1, tsc2, tr1, tr2, delta, pm1, pm2, hpet1, hpet2;
+	int hpet = is_hpet_enabled();
+	unsigned int tsc_khz_val = 0;
+
+	local_irq_save(flags);
+
+	tsc1 = tsc_read_refs(&pm1, hpet ? &hpet1 : NULL);
+
+	outb((inb(0x61) & ~0x02) | 0x01, 0x61);
+
+	outb(0xb0, 0x43);
+	outb((CLOCK_TICK_RATE / (1000 / 50)) & 0xff, 0x42);
+	outb((CLOCK_TICK_RATE / (1000 / 50)) >> 8, 0x42);
+	tr1 = get_cycles();
+	while ((inb(0x61) & 0x20) == 0);
+	tr2 = get_cycles();
+
+	tsc2 = tsc_read_refs(&pm2, hpet ? &hpet2 : NULL);
+
+	local_irq_restore(flags);
+
+	/*
+	 * Preset the result with the raw and inaccurate PIT
+	 * calibration value
+	 */
+	delta = (tr2 - tr1);
+	do_div(delta, 50);
+	tsc_khz_val = delta;
+
+	/* hpet or pmtimer available ? */
+	if (!hpet && !pm1 && !pm2) {
+		printk(KERN_INFO "TSC calibrated against PIT\n");
+		goto out;
+	}
+
+	/* Check, whether the sampling was disturbed by an SMI */
+	if (tsc1 == ULLONG_MAX || tsc2 == ULLONG_MAX) {
+		printk(KERN_WARNING "TSC calibration disturbed by SMI, "
+				"using PIT calibration result\n");
+		goto out;
+	}
+
+	tsc2 = (tsc2 - tsc1) * 1000000LL;
+
+	if (hpet) {
+		printk(KERN_INFO "TSC calibrated against HPET\n");
+		if (hpet2 < hpet1)
+			hpet2 += 0x100000000ULL;
+		hpet2 -= hpet1;
+		tsc1 = ((u64)hpet2 * hpet_readl(HPET_PERIOD));
+		do_div(tsc1, 1000000);
+	} else {
+		printk(KERN_INFO "TSC calibrated against PM_TIMER\n");
+		if (pm2 < pm1)
+			pm2 += (u64)ACPI_PM_OVRRUN;
+		pm2 -= pm1;
+		tsc1 = pm2 * 1000000000LL;
+		do_div(tsc1, PMTMR_TICKS_PER_SEC);
+	}
+
+	do_div(tsc2, tsc1);
+	tsc_khz_val = tsc2;
+
+out:
+	return tsc_khz_val;
+}
+
+unsigned long native_calculate_cpu_khz(void)
+{
+	return tsc_calibrate();
+}
+
+#ifdef CONFIG_X86_32
+/* Only called from the Powernow K7 cpu freq driver */
+int recalibrate_cpu_khz(void)
+{
+#ifndef CONFIG_SMP
+	unsigned long cpu_khz_old = cpu_khz;
+
+	if (cpu_has_tsc) {
+		cpu_khz = calculate_cpu_khz();
+		tsc_khz = cpu_khz;
+		cpu_data(0).loops_per_jiffy =
+			cpufreq_scale(cpu_data(0).loops_per_jiffy,
+					cpu_khz_old, cpu_khz);
+		return 0;
+	} else
+		return -ENODEV;
+#else
+	return -ENODEV;
+#endif
+}
+
+EXPORT_SYMBOL(recalibrate_cpu_khz);
+
+#endif /* CONFIG_X86_32 */

commit 0ef95533326a7b37d16025af9edc0c18e644b346
Author: Alok Kataria <akataria@vmware.com>
Date:   Tue Jul 1 11:43:18 2008 -0700

    x86: merge sched_clock handling
    
    Move the basic global variable definitions and sched_clock handling in the
    common "tsc.c" file.
    
     - Unify notsc kernel command line handling for 32 bit and 64bit.
     - Functional changes for 64bit.
            - "tsc_disabled" is updated if "notsc" is passed at boottime.
            - Fallback to jiffies for sched_clock, incase notsc is passed on
              commandline.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Cc: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
new file mode 100644
index 000000000000..5d0be778fadd
--- /dev/null
+++ b/arch/x86/kernel/tsc.c
@@ -0,0 +1,86 @@
+#include <linux/sched.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/timer.h>
+
+unsigned int cpu_khz;           /* TSC clocks / usec, not used here */
+EXPORT_SYMBOL(cpu_khz);
+unsigned int tsc_khz;
+EXPORT_SYMBOL(tsc_khz);
+
+/*
+ * TSC can be unstable due to cpufreq or due to unsynced TSCs
+ */
+int tsc_unstable;
+
+/* native_sched_clock() is called before tsc_init(), so
+   we must start with the TSC soft disabled to prevent
+   erroneous rdtsc usage on !cpu_has_tsc processors */
+int tsc_disabled = -1;
+
+/*
+ * Scheduler clock - returns current time in nanosec units.
+ */
+u64 native_sched_clock(void)
+{
+	u64 this_offset;
+
+	/*
+	 * Fall back to jiffies if there's no TSC available:
+	 * ( But note that we still use it if the TSC is marked
+	 *   unstable. We do this because unlike Time Of Day,
+	 *   the scheduler clock tolerates small errors and it's
+	 *   very important for it to be as fast as the platform
+	 *   can achive it. )
+	 */
+	if (unlikely(tsc_disabled)) {
+		/* No locking but a rare wrong value is not a big deal: */
+		return (jiffies_64 - INITIAL_JIFFIES) * (1000000000 / HZ);
+	}
+
+	/* read the Time Stamp Counter: */
+	rdtscll(this_offset);
+
+	/* return the value in ns */
+	return cycles_2_ns(this_offset);
+}
+
+/* We need to define a real function for sched_clock, to override the
+   weak default version */
+#ifdef CONFIG_PARAVIRT
+unsigned long long sched_clock(void)
+{
+	return paravirt_sched_clock();
+}
+#else
+unsigned long long
+sched_clock(void) __attribute__((alias("native_sched_clock")));
+#endif
+
+int check_tsc_unstable(void)
+{
+	return tsc_unstable;
+}
+EXPORT_SYMBOL_GPL(check_tsc_unstable);
+
+#ifdef CONFIG_X86_TSC
+int __init notsc_setup(char *str)
+{
+	printk(KERN_WARNING "notsc: Kernel compiled with CONFIG_X86_TSC, "
+			"cannot disable TSC completely.\n");
+	tsc_disabled = 1;
+	return 1;
+}
+#else
+/*
+ * disable flag for tsc. Takes effect by clearing the TSC cpu flag
+ * in cpu/common.c
+ */
+int __init notsc_setup(char *str)
+{
+	setup_clear_cpu_cap(X86_FEATURE_TSC);
+	return 1;
+}
+#endif
+
+__setup("notsc", notsc_setup);
