commit 61790d5bbba7f6c8dd22db6effa6fa984089cbde
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 29 16:58:00 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 346
    
    Based on 1 normalized pattern(s):
    
      use of this code is subject to the terms and conditions of the gnu
      general public license version 2 see copying or http www gnu org
      licenses gpl html
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 1 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Armijn Hemel <armijn@tjaldur.nl>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190530000437.611918838@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 891a75dbc131..796cfaa46bfa 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -1,11 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * vSMPowered(tm) systems specific initialization
  * Copyright (C) 2005 ScaleMP Inc.
  *
- * Use of this code is subject to the terms and conditions of the
- * GNU general public license version 2. See "COPYING" or
- * http://www.gnu.org/licenses/gpl.html
- *
  * Ravikiran Thirumalai <kiran@scalemp.com>,
  * Shai Fultheim <shai@scalemp.com>
  * Paravirt ops integration: Glauber de Oliveira Costa <gcosta@redhat.com>,

commit a48777fdda7d13179979a889e1fb87655a783cc0
Author: Eial Czerwacki <eial@scalemp.com>
Date:   Mon Nov 5 19:31:54 2018 +0200

    x86/vsmp: Remove dependency on pv_irq_ops
    
    vSMP dependency on pv_irq_ops has been removed some years ago, but the code
    still deals with pv_irq_ops.
    
    In short, "cap & ctl & (1 << 4)" is always returning 0, so all
    PARAVIRT/PARAVIRT_XXL code related to that can be removed.
    
    However, the rest of the code depends on CONFIG_PCI, so fix it accordingly.
    
    Rename set_vsmp_pv_ops to set_vsmp_ctl as the original name does not make
    sense anymore.
    
    Signed-off-by: Eial Czerwacki <eial@scalemp.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Shai Fultheim <shai@scalemp.com>
    Cc: Juergen Gross <jgross@suse.com>
    Link: https://lkml.kernel.org/r/1541439114-28297-1-git-send-email-eial@scalemp.com

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 1eae5af491c2..891a75dbc131 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -26,65 +26,8 @@
 
 #define TOPOLOGY_REGISTER_OFFSET 0x10
 
-#if defined CONFIG_PCI && defined CONFIG_PARAVIRT_XXL
-/*
- * Interrupt control on vSMPowered systems:
- * ~AC is a shadow of IF.  If IF is 'on' AC should be 'off'
- * and vice versa.
- */
-
-asmlinkage __visible unsigned long vsmp_save_fl(void)
-{
-	unsigned long flags = native_save_fl();
-
-	if (!(flags & X86_EFLAGS_IF) || (flags & X86_EFLAGS_AC))
-		flags &= ~X86_EFLAGS_IF;
-	return flags;
-}
-PV_CALLEE_SAVE_REGS_THUNK(vsmp_save_fl);
-
-__visible void vsmp_restore_fl(unsigned long flags)
-{
-	if (flags & X86_EFLAGS_IF)
-		flags &= ~X86_EFLAGS_AC;
-	else
-		flags |= X86_EFLAGS_AC;
-	native_restore_fl(flags);
-}
-PV_CALLEE_SAVE_REGS_THUNK(vsmp_restore_fl);
-
-asmlinkage __visible void vsmp_irq_disable(void)
-{
-	unsigned long flags = native_save_fl();
-
-	native_restore_fl((flags & ~X86_EFLAGS_IF) | X86_EFLAGS_AC);
-}
-PV_CALLEE_SAVE_REGS_THUNK(vsmp_irq_disable);
-
-asmlinkage __visible void vsmp_irq_enable(void)
-{
-	unsigned long flags = native_save_fl();
-
-	native_restore_fl((flags | X86_EFLAGS_IF) & (~X86_EFLAGS_AC));
-}
-PV_CALLEE_SAVE_REGS_THUNK(vsmp_irq_enable);
-
-static unsigned __init vsmp_patch(u8 type, void *ibuf,
-				  unsigned long addr, unsigned len)
-{
-	switch (type) {
-	case PARAVIRT_PATCH(irq.irq_enable):
-	case PARAVIRT_PATCH(irq.irq_disable):
-	case PARAVIRT_PATCH(irq.save_fl):
-	case PARAVIRT_PATCH(irq.restore_fl):
-		return paravirt_patch_default(type, ibuf, addr, len);
-	default:
-		return native_patch(type, ibuf, addr, len);
-	}
-
-}
-
-static void __init set_vsmp_pv_ops(void)
+#ifdef CONFIG_PCI
+static void __init set_vsmp_ctl(void)
 {
 	void __iomem *address;
 	unsigned int cap, ctl, cfg;
@@ -109,28 +52,12 @@ static void __init set_vsmp_pv_ops(void)
 	}
 #endif
 
-	if (cap & ctl & (1 << 4)) {
-		/* Setup irq ops and turn on vSMP  IRQ fastpath handling */
-		pv_ops.irq.irq_disable = PV_CALLEE_SAVE(vsmp_irq_disable);
-		pv_ops.irq.irq_enable = PV_CALLEE_SAVE(vsmp_irq_enable);
-		pv_ops.irq.save_fl = PV_CALLEE_SAVE(vsmp_save_fl);
-		pv_ops.irq.restore_fl = PV_CALLEE_SAVE(vsmp_restore_fl);
-		pv_ops.init.patch = vsmp_patch;
-		ctl &= ~(1 << 4);
-	}
 	writel(ctl, address + 4);
 	ctl = readl(address + 4);
 	pr_info("vSMP CTL: control set to:0x%08x\n", ctl);
 
 	early_iounmap(address, 8);
 }
-#else
-static void __init set_vsmp_pv_ops(void)
-{
-}
-#endif
-
-#ifdef CONFIG_PCI
 static int is_vsmp = -1;
 
 static void __init detect_vsmp_box(void)
@@ -164,11 +91,14 @@ static int is_vsmp_box(void)
 {
 	return 0;
 }
+static void __init set_vsmp_ctl(void)
+{
+}
 #endif
 
 static void __init vsmp_cap_cpus(void)
 {
-#if !defined(CONFIG_X86_VSMP) && defined(CONFIG_SMP)
+#if !defined(CONFIG_X86_VSMP) && defined(CONFIG_SMP) && defined(CONFIG_PCI)
 	void __iomem *address;
 	unsigned int cfg, topology, node_shift, maxcpus;
 
@@ -221,6 +151,6 @@ void __init vsmp_init(void)
 
 	vsmp_cap_cpus();
 
-	set_vsmp_pv_ops();
+	set_vsmp_ctl();
 	return;
 }

commit 6da63eb241a05b0e676d68975e793c0521387141
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Aug 28 09:40:24 2018 +0200

    x86/paravirt: Move the pv_irq_ops under the PARAVIRT_XXL umbrella
    
    All of the paravirt ops defined in pv_irq_ops are for Xen PV guests
    or VSMP only. Define them only if CONFIG_PARAVIRT_XXL is set.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel@lists.xenproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: boris.ostrovsky@oracle.com
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/20180828074026.820-14-jgross@suse.com

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 789918d78697..1eae5af491c2 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -26,7 +26,7 @@
 
 #define TOPOLOGY_REGISTER_OFFSET 0x10
 
-#if defined CONFIG_PCI && defined CONFIG_PARAVIRT
+#if defined CONFIG_PCI && defined CONFIG_PARAVIRT_XXL
 /*
  * Interrupt control on vSMPowered systems:
  * ~AC is a shadow of IF.  If IF is 'on' AC should be 'off'

commit 5c83511bdb9832c86be20fb86b783356e2f58062
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Aug 28 09:40:19 2018 +0200

    x86/paravirt: Use a single ops structure
    
    Instead of using six globally visible paravirt ops structures combine
    them in a single structure, keeping the original structures as
    sub-structures.
    
    This avoids the need to assemble struct paravirt_patch_template at
    runtime on the stack each time apply_paravirt() is being called (i.e.
    when loading a module).
    
    [ tglx: Made the struct and the initializer tabular for readability sake ]
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel@lists.xenproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: boris.ostrovsky@oracle.com
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/20180828074026.820-9-jgross@suse.com

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index f194e5e1e95c..789918d78697 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -73,10 +73,10 @@ static unsigned __init vsmp_patch(u8 type, void *ibuf,
 				  unsigned long addr, unsigned len)
 {
 	switch (type) {
-	case PARAVIRT_PATCH(pv_irq_ops.irq_enable):
-	case PARAVIRT_PATCH(pv_irq_ops.irq_disable):
-	case PARAVIRT_PATCH(pv_irq_ops.save_fl):
-	case PARAVIRT_PATCH(pv_irq_ops.restore_fl):
+	case PARAVIRT_PATCH(irq.irq_enable):
+	case PARAVIRT_PATCH(irq.irq_disable):
+	case PARAVIRT_PATCH(irq.save_fl):
+	case PARAVIRT_PATCH(irq.restore_fl):
 		return paravirt_patch_default(type, ibuf, addr, len);
 	default:
 		return native_patch(type, ibuf, addr, len);
@@ -111,11 +111,11 @@ static void __init set_vsmp_pv_ops(void)
 
 	if (cap & ctl & (1 << 4)) {
 		/* Setup irq ops and turn on vSMP  IRQ fastpath handling */
-		pv_irq_ops.irq_disable = PV_CALLEE_SAVE(vsmp_irq_disable);
-		pv_irq_ops.irq_enable  = PV_CALLEE_SAVE(vsmp_irq_enable);
-		pv_irq_ops.save_fl  = PV_CALLEE_SAVE(vsmp_save_fl);
-		pv_irq_ops.restore_fl  = PV_CALLEE_SAVE(vsmp_restore_fl);
-		pv_init_ops.patch = vsmp_patch;
+		pv_ops.irq.irq_disable = PV_CALLEE_SAVE(vsmp_irq_disable);
+		pv_ops.irq.irq_enable = PV_CALLEE_SAVE(vsmp_irq_enable);
+		pv_ops.irq.save_fl = PV_CALLEE_SAVE(vsmp_save_fl);
+		pv_ops.irq.restore_fl = PV_CALLEE_SAVE(vsmp_restore_fl);
+		pv_ops.init.patch = vsmp_patch;
 		ctl &= ~(1 << 4);
 	}
 	writel(ctl, address + 4);

commit abc745f85c1193d2a052addf0031d59b4436c246
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Aug 28 09:40:17 2018 +0200

    x86/paravirt: Remove clobbers parameter from paravirt patch functions
    
    The clobbers parameter from paravirt_patch_default() et al isn't used
    any longer. Remove it.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel@lists.xenproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: boris.ostrovsky@oracle.com
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/20180828074026.820-7-jgross@suse.com

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 44685fb2a192..f194e5e1e95c 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -69,7 +69,7 @@ asmlinkage __visible void vsmp_irq_enable(void)
 }
 PV_CALLEE_SAVE_REGS_THUNK(vsmp_irq_enable);
 
-static unsigned __init vsmp_patch(u8 type, u16 clobbers, void *ibuf,
+static unsigned __init vsmp_patch(u8 type, void *ibuf,
 				  unsigned long addr, unsigned len)
 {
 	switch (type) {
@@ -77,9 +77,9 @@ static unsigned __init vsmp_patch(u8 type, u16 clobbers, void *ibuf,
 	case PARAVIRT_PATCH(pv_irq_ops.irq_disable):
 	case PARAVIRT_PATCH(pv_irq_ops.save_fl):
 	case PARAVIRT_PATCH(pv_irq_ops.restore_fl):
-		return paravirt_patch_default(type, clobbers, ibuf, addr, len);
+		return paravirt_patch_default(type, ibuf, addr, len);
 	default:
-		return native_patch(type, clobbers, ibuf, addr, len);
+		return native_patch(type, ibuf, addr, len);
 	}
 
 }

commit baab1e84b1124bfd3e40ef6c8e05b2a15136e3d5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 13 23:29:43 2017 +0200

    x86/apic: Remove unused callbacks
    
    Now that the old allocator is gone, these apic functions are unused. Remove
    them.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Juergen Gross <jgross@suse.com>
    Tested-by: Yu Chen <yu.c.chen@intel.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Link: https://lkml.kernel.org/r/20170913213155.524662349@linutronix.de

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index b034b1b14b9c..44685fb2a192 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -26,9 +26,6 @@
 
 #define TOPOLOGY_REGISTER_OFFSET 0x10
 
-/* Flag below is initialized once during vSMP PCI initialization. */
-static int irq_routing_comply = 1;
-
 #if defined CONFIG_PCI && defined CONFIG_PARAVIRT
 /*
  * Interrupt control on vSMPowered systems:
@@ -105,9 +102,6 @@ static void __init set_vsmp_pv_ops(void)
 	if (cap & ctl & BIT(8)) {
 		ctl &= ~BIT(8);
 
-		/* Interrupt routing set to ignore */
-		irq_routing_comply = 0;
-
 #ifdef CONFIG_PROC_FS
 		/* Don't let users change irq affinity via procfs */
 		no_irq_affinity = 1;
@@ -211,23 +205,10 @@ static int apicid_phys_pkg_id(int initial_apic_id, int index_msb)
 	return hard_smp_processor_id() >> index_msb;
 }
 
-/*
- * In vSMP, all cpus should be capable of handling interrupts, regardless of
- * the APIC used.
- */
-static void fill_vector_allocation_domain(int cpu, struct cpumask *retmask,
-					  const struct cpumask *mask)
-{
-	cpumask_setall(retmask);
-}
-
 static void vsmp_apic_post_init(void)
 {
 	/* need to update phys_pkg_id */
 	apic->phys_pkg_id = apicid_phys_pkg_id;
-
-	if (!irq_routing_comply)
-		apic->vector_allocation_domain = fill_vector_allocation_domain;
 }
 
 void __init vsmp_init(void)

commit 70c4f78b23c69013c908222d55a07c96fea4bba1
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri May 1 20:13:42 2015 -0400

    x86: replace __init_or_module with __init in non-modular vsmp_64.c
    
    The __init_or_module is from commit 05e12e1c4c09cd35ac9f4e6af1e
    ("x86: fix 27-rc crash on vsmp due to paravirt during module load").
    
    But as of commit 70511134f61bd6e5eed19f767381f9fb3e762d49
    ("Revert "x86: don't compile vsmp_64 for 32bit") this file became
    obj-y and hence is now only for built-in.  That makes any
    "_or_module" support no longer necessary.
    
    We need to distinguish between the two in order to do some header
    reorganization between init.h and module.h and we don't want to
    be including module.h in non-modular code.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index ee22c1d93ae5..b034b1b14b9c 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -72,7 +72,7 @@ asmlinkage __visible void vsmp_irq_enable(void)
 }
 PV_CALLEE_SAVE_REGS_THUNK(vsmp_irq_enable);
 
-static unsigned __init_or_module vsmp_patch(u8 type, u16 clobbers, void *ibuf,
+static unsigned __init vsmp_patch(u8 type, u16 clobbers, void *ibuf,
 				  unsigned long addr, unsigned len)
 {
 	switch (type) {

commit 5e3bf215f4f2efc0af89e6dbc5da789744aeb5d7
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Aug 1 14:47:56 2014 -0700

    x86/apic/vsmp: Make is_vsmp_box() static
    
    Since checkin
    
    411cf9ee2946 x86, vsmp: Remove is_vsmp_box() from apic_is_clustered_box()
    
    ... is_vsmp_box() is only used in vsmp_64.c and does not have any
    header file declaring it, so make it explicitly static.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Cc: Shai Fultheim <shai@scalemp.com>
    Cc: Oren Twaig <oren@scalemp.com>
    Link: http://lkml.kernel.org/r/1404036068-11674-1-git-send-email-oren@scalemp.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index b99b9ad8540c..ee22c1d93ae5 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -152,7 +152,7 @@ static void __init detect_vsmp_box(void)
 		is_vsmp = 1;
 }
 
-int is_vsmp_box(void)
+static int is_vsmp_box(void)
 {
 	if (is_vsmp != -1)
 		return is_vsmp;
@@ -166,7 +166,7 @@ int is_vsmp_box(void)
 static void __init detect_vsmp_box(void)
 {
 }
-int is_vsmp_box(void)
+static int is_vsmp_box(void)
 {
 	return 0;
 }

commit 2605fc216fa492f9e7c488bdc7f687cd6dcc703b
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri May 2 00:44:37 2014 +0200

    asmlinkage, x86: Add explicit __visible to arch/x86/*
    
    As requested by Linus add explicit __visible to the asmlinkage users.
    This marks all functions visible to assembler.
    
    Tree sweep for arch/x86/*
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1398984278-29319-3-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 5edc34b5b951..b99b9ad8540c 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -36,7 +36,7 @@ static int irq_routing_comply = 1;
  * and vice versa.
  */
 
-asmlinkage unsigned long vsmp_save_fl(void)
+asmlinkage __visible unsigned long vsmp_save_fl(void)
 {
 	unsigned long flags = native_save_fl();
 
@@ -56,7 +56,7 @@ __visible void vsmp_restore_fl(unsigned long flags)
 }
 PV_CALLEE_SAVE_REGS_THUNK(vsmp_restore_fl);
 
-asmlinkage void vsmp_irq_disable(void)
+asmlinkage __visible void vsmp_irq_disable(void)
 {
 	unsigned long flags = native_save_fl();
 
@@ -64,7 +64,7 @@ asmlinkage void vsmp_irq_disable(void)
 }
 PV_CALLEE_SAVE_REGS_THUNK(vsmp_irq_disable);
 
-asmlinkage void vsmp_irq_enable(void)
+asmlinkage __visible void vsmp_irq_enable(void)
 {
 	unsigned long flags = native_save_fl();
 

commit 39025ba38278f3003ee538409f7c98970620ef49
Author: Oren Twaig <oren@scalemp.com>
Date:   Mon Apr 28 10:21:37 2014 +0300

    x86/vsmp: Fix irq routing
    
    Correct IRQ routing in case a vSMP box is detected
    but the  Interrupt Routing Comply (IRC) value is set to
    "comply", which leads to incorrect IRQ routing.
    
    Before the patch:
    
    When a vSMP box was detected and IRC was set to "comply",
    users (and the kernel) couldn't effectively set the
    destination of the IRQs. This is because the hook inside
    vsmp_64.c always setup all CPUs as the IRQ destination using
    cpumask_setall() as the return value for IRQ allocation mask.
    Later, this "overrided" mask caused the kernel to set the IRQ
    destination to the lowest online CPU in the mask (CPU0 usually).
    
    After the patch:
    
    When the IRC is set to "comply", users (and the kernel) can control
    the destination of the IRQs as we will not be changing the
    default "apic->vector_allocation_domain".
    
    Signed-off-by: Oren Twaig <oren@scalemp.com>
    Acked-by: Shai Fultheim <shai@scalemp.com>
    Link: http://lkml.kernel.org/r/1398669697-2123-1-git-send-email-oren@scalemp.com
    [ Minor readability edits. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index f6584a90aba3..5edc34b5b951 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -26,6 +26,9 @@
 
 #define TOPOLOGY_REGISTER_OFFSET 0x10
 
+/* Flag below is initialized once during vSMP PCI initialization. */
+static int irq_routing_comply = 1;
+
 #if defined CONFIG_PCI && defined CONFIG_PARAVIRT
 /*
  * Interrupt control on vSMPowered systems:
@@ -101,6 +104,10 @@ static void __init set_vsmp_pv_ops(void)
 #ifdef CONFIG_SMP
 	if (cap & ctl & BIT(8)) {
 		ctl &= ~BIT(8);
+
+		/* Interrupt routing set to ignore */
+		irq_routing_comply = 0;
+
 #ifdef CONFIG_PROC_FS
 		/* Don't let users change irq affinity via procfs */
 		no_irq_affinity = 1;
@@ -218,7 +225,9 @@ static void vsmp_apic_post_init(void)
 {
 	/* need to update phys_pkg_id */
 	apic->phys_pkg_id = apicid_phys_pkg_id;
-	apic->vector_allocation_domain = fill_vector_allocation_domain;
+
+	if (!irq_routing_comply)
+		apic->vector_allocation_domain = fill_vector_allocation_domain;
 }
 
 void __init vsmp_init(void)

commit a2e7f0e3a4f0f23fe4cd8cc22da547872f0170bb
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Oct 22 09:07:56 2013 -0700

    x86, asmlinkage, paravirt: Make paravirt thunks global
    
    The paravirt thunks use a hack of using a static reference to a static
    function to reference that function from the top level statement.
    
    This assumes that gcc always generates static function names in a specific
    format, which is not necessarily true.
    
    Simply make these functions global and asmlinkage or __visible. This way the
    static __used variables are not needed and everything works.
    
    Functions with arguments are __visible to keep the register calling
    convention on 32bit.
    
    Changed in paravirt and in all users (Xen and vsmp)
    
    v2: Use __visible for functions with arguments
    
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Ido Yariv <ido@wizery.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1382458079-24450-5-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 992f890283e9..f6584a90aba3 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -33,7 +33,7 @@
  * and vice versa.
  */
 
-static unsigned long vsmp_save_fl(void)
+asmlinkage unsigned long vsmp_save_fl(void)
 {
 	unsigned long flags = native_save_fl();
 
@@ -43,7 +43,7 @@ static unsigned long vsmp_save_fl(void)
 }
 PV_CALLEE_SAVE_REGS_THUNK(vsmp_save_fl);
 
-static void vsmp_restore_fl(unsigned long flags)
+__visible void vsmp_restore_fl(unsigned long flags)
 {
 	if (flags & X86_EFLAGS_IF)
 		flags &= ~X86_EFLAGS_AC;
@@ -53,7 +53,7 @@ static void vsmp_restore_fl(unsigned long flags)
 }
 PV_CALLEE_SAVE_REGS_THUNK(vsmp_restore_fl);
 
-static void vsmp_irq_disable(void)
+asmlinkage void vsmp_irq_disable(void)
 {
 	unsigned long flags = native_save_fl();
 
@@ -61,7 +61,7 @@ static void vsmp_irq_disable(void)
 }
 PV_CALLEE_SAVE_REGS_THUNK(vsmp_irq_disable);
 
-static void vsmp_irq_enable(void)
+asmlinkage void vsmp_irq_enable(void)
 {
 	unsigned long flags = native_save_fl();
 

commit 1ac322d0b169c95ce34d55b3ed6d40ce1a5f3a02
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Jun 25 13:38:28 2012 -0700

    x86/apic/x2apic: Limit the vector reservation to the user specified mask
    
    For the x2apic cluster mode, vector for an interrupt is
    currently reserved on all the cpu's that are part of the x2apic
    cluster. But the interrupts will be routed only to the cluster
    (derived from the first cpu in the mask) members specified in
    the mask. So there is no need to reserve the vector in the
    unused cluster members.
    
    Modify __assign_irq_vector() to reserve the vectors based on the
    user specified irq destination mask. If the new mask is a proper
    subset of the currently used mask, cleanup the vector allocation
    on the unused cpu members.
    
    Also, allow the apic driver to tune the vector domain based on
    the affinity mask (which in most cases is the user-specified
    mask).
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Yinghai Lu <yinghai@kernel.org>
    Acked-by: Alexander Gordeev <agordeev@redhat.com>
    Acked-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Link: http://lkml.kernel.org/r/1340656709-11423-3-git-send-email-suresh.b.siddha@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 3f0285ac00fa..992f890283e9 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -208,7 +208,8 @@ static int apicid_phys_pkg_id(int initial_apic_id, int index_msb)
  * In vSMP, all cpus should be capable of handling interrupts, regardless of
  * the APIC used.
  */
-static void fill_vector_allocation_domain(int cpu, struct cpumask *retmask)
+static void fill_vector_allocation_domain(int cpu, struct cpumask *retmask,
+					  const struct cpumask *mask)
 {
 	cpumask_setall(retmask);
 }

commit b39f25a849d7677a7dbf183f2483fd41c201a5ce
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Jun 25 13:38:27 2012 -0700

    x86/apic: Optimize cpu traversal in __assign_irq_vector() using domain membership
    
    Currently __assign_irq_vector() goes through each cpu in the
    specified mask until it finds a free vector in all the cpu's
    that are part of the same interrupt domain. We visit all the
    interrupt domain sibling cpus to reserve the free vector. So,
    when we fail to find a free vector in an interrupt domain, it is
    safe to continue our search with a cpu belonging to a new
    interrupt domain. No need to go through each cpu, if the domain
    containing that cpu is already visited.
    
    Use the irq_cfg's old_domain to track the visited domains and
    optimize the cpu traversal while finding a free vector in the
    given cpumask.
    
    NOTE: We can also optimize the search by using for_each_cpu() and
    skip the current cpu, if it is not the first cpu in the mask
    returned by the vector_allocation_domain(). But re-using the
    cfg->old_domain to track the visited domains will be slightly
    faster.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Yinghai Lu <yinghai@kernel.org>
    Acked-by: Alexander Gordeev <agordeev@redhat.com>
    Acked-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Link: http://lkml.kernel.org/r/1340656709-11423-2-git-send-email-suresh.b.siddha@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index fa5adb7c228c..3f0285ac00fa 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -208,10 +208,9 @@ static int apicid_phys_pkg_id(int initial_apic_id, int index_msb)
  * In vSMP, all cpus should be capable of handling interrupts, regardless of
  * the APIC used.
  */
-static bool fill_vector_allocation_domain(int cpu, struct cpumask *retmask)
+static void fill_vector_allocation_domain(int cpu, struct cpumask *retmask)
 {
 	cpumask_setall(retmask);
-	return false;
 }
 
 static void vsmp_apic_post_init(void)

commit abf71f3066740f3b59c3f731b4b68ed335f7b24d
Author: Ido Yariv <ido@wizery.com>
Date:   Fri Jun 15 18:10:55 2012 +0300

    x86/vsmp: Fix vector_allocation_domain's return value
    
    Commit 8637e38a ("x86/apic: Avoid useless scanning thru a
    cpumask in assign_irq_vector()") modified
    vector_allocation_domain() to return a boolean indicating if
    cpumask is dynamic or static. Adjust vSMP's callback
    implementation accordingly.
    
    Signed-off-by: Ido Yariv <ido@wizery.com>
    Acked-by: Shai Fultheim <shai@scalemp.com>
    Cc: Alexander Gordeev <agordeev@redhat.com>
    Link: http://lkml.kernel.org/r/1339773055-27397-1-git-send-email-ido@wizery.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 3f0285ac00fa..fa5adb7c228c 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -208,9 +208,10 @@ static int apicid_phys_pkg_id(int initial_apic_id, int index_msb)
  * In vSMP, all cpus should be capable of handling interrupts, regardless of
  * the APIC used.
  */
-static void fill_vector_allocation_domain(int cpu, struct cpumask *retmask)
+static bool fill_vector_allocation_domain(int cpu, struct cpumask *retmask)
 {
 	cpumask_setall(retmask);
+	return false;
 }
 
 static void vsmp_apic_post_init(void)

commit d48daf37a3d2e2b28a61e615c0fc538301edb0dd
Author: Ido Yariv <ido@wizery.com>
Date:   Thu Jun 14 18:43:08 2012 +0300

    x86/vsmp: Fix linker error when CONFIG_PROC_FS is not set
    
    set_vsmp_pv_ops() references no_irq_affinity which is undeclared
    if CONFIG_PROC_FS isn't set. Fix this by adding an #ifdef around
    this variable's access.
    
    Reported-by: Fengguang Wu <wfg@linux.intel.com>
    Signed-off-by: Ido Yariv <ido@wizery.com>
    Acked-by: Shai Fultheim <shai@scalemp.com>
    Link: http://lkml.kernel.org/r/1339688588-12674-1-git-send-email-ido@wizery.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 6b96a7374f94..3f0285ac00fa 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -101,7 +101,10 @@ static void __init set_vsmp_pv_ops(void)
 #ifdef CONFIG_SMP
 	if (cap & ctl & BIT(8)) {
 		ctl &= ~BIT(8);
+#ifdef CONFIG_PROC_FS
+		/* Don't let users change irq affinity via procfs */
 		no_irq_affinity = 1;
+#endif
 	}
 #endif
 

commit 110c1e1f1bf61e5dca53ff5c9dc75243ce87c002
Author: Ravikiran Thirumalai <kiran.thirumalai@gmail.com>
Date:   Sun Jun 3 01:11:35 2012 +0300

    x86/vsmp: Ignore IOAPIC IRQ affinity if possible
    
    vSMP can route interrupts more optimally based on internal
    knowledge the OS does not have. In order to support this
    optimization, all CPUs must be able to handle all possible
    IOAPIC interrupts.
    
    Fix this by setting the vector allocation domain for all CPUs
    and by enabling this feature in vSMP.
    
    Signed-off-by: Ravikiran Thirumalai <kiran.thirumalai@gmail.com>
    Signed-off-by: Shai Fultheim <shai@scalemp.com>
    [ Rebased, simplified, and reworded the commit message. ]
    Signed-off-by: Ido Yariv <ido@wizery.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 59eea855f451..6b96a7374f94 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -16,6 +16,7 @@
 #include <linux/pci_ids.h>
 #include <linux/pci_regs.h>
 #include <linux/smp.h>
+#include <linux/irq.h>
 
 #include <asm/apic.h>
 #include <asm/pci-direct.h>
@@ -95,6 +96,15 @@ static void __init set_vsmp_pv_ops(void)
 	ctl = readl(address + 4);
 	printk(KERN_INFO "vSMP CTL: capabilities:0x%08x  control:0x%08x\n",
 	       cap, ctl);
+
+	/* If possible, let the vSMP foundation route the interrupt optimally */
+#ifdef CONFIG_SMP
+	if (cap & ctl & BIT(8)) {
+		ctl &= ~BIT(8);
+		no_irq_affinity = 1;
+	}
+#endif
+
 	if (cap & ctl & (1 << 4)) {
 		/* Setup irq ops and turn on vSMP  IRQ fastpath handling */
 		pv_irq_ops.irq_disable = PV_CALLEE_SAVE(vsmp_irq_disable);
@@ -102,12 +112,11 @@ static void __init set_vsmp_pv_ops(void)
 		pv_irq_ops.save_fl  = PV_CALLEE_SAVE(vsmp_save_fl);
 		pv_irq_ops.restore_fl  = PV_CALLEE_SAVE(vsmp_restore_fl);
 		pv_init_ops.patch = vsmp_patch;
-
 		ctl &= ~(1 << 4);
-		writel(ctl, address + 4);
-		ctl = readl(address + 4);
-		printk(KERN_INFO "vSMP CTL: control set to:0x%08x\n", ctl);
 	}
+	writel(ctl, address + 4);
+	ctl = readl(address + 4);
+	pr_info("vSMP CTL: control set to:0x%08x\n", ctl);
 
 	early_iounmap(address, 8);
 }
@@ -192,10 +201,20 @@ static int apicid_phys_pkg_id(int initial_apic_id, int index_msb)
 	return hard_smp_processor_id() >> index_msb;
 }
 
+/*
+ * In vSMP, all cpus should be capable of handling interrupts, regardless of
+ * the APIC used.
+ */
+static void fill_vector_allocation_domain(int cpu, struct cpumask *retmask)
+{
+	cpumask_setall(retmask);
+}
+
 static void vsmp_apic_post_init(void)
 {
 	/* need to update phys_pkg_id */
 	apic->phys_pkg_id = apicid_phys_pkg_id;
+	apic->vector_allocation_domain = fill_vector_allocation_domain;
 }
 
 void __init vsmp_init(void)

commit 7db971b235480849aa5b9209b67b62e987b3181b
Author: Ido Yariv <ido@wizery.com>
Date:   Sun Jun 3 01:11:34 2012 +0300

    x86/platform: Introduce APIC post-initialization callback
    
    Some subarchitectures (such as vSMP) need to slightly adjust the
    underlying APIC structure. Add an APIC post-initialization callback
    to 'struct x86_platform_ops' for this purpose and use it for
    adjusting the APIC structure on vSMP systems.
    
    Signed-off-by: Ido Yariv <ido@wizery.com>
    Acked-by: Shai Fultheim <shai@scalemp.com>
    Link: http://lkml.kernel.org/r/1338675095-27260-1-git-send-email-ido@wizery.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 8eeb55a551b4..59eea855f451 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -187,12 +187,25 @@ static void __init vsmp_cap_cpus(void)
 #endif
 }
 
+static int apicid_phys_pkg_id(int initial_apic_id, int index_msb)
+{
+	return hard_smp_processor_id() >> index_msb;
+}
+
+static void vsmp_apic_post_init(void)
+{
+	/* need to update phys_pkg_id */
+	apic->phys_pkg_id = apicid_phys_pkg_id;
+}
+
 void __init vsmp_init(void)
 {
 	detect_vsmp_box();
 	if (!is_vsmp_box())
 		return;
 
+	x86_platform.apic_post_init = vsmp_apic_post_init;
+
 	vsmp_cap_cpus();
 
 	set_vsmp_pv_ops();

commit ead91d4b8c3b1fb08a73aaa4a191230ecf717ee0
Author: Shai Fultheim <shai@scalemp.com>
Date:   Mon Apr 16 10:39:35 2012 +0300

    x86/vsmp: Fix number of CPUs when vsmp is disabled
    
    In case CONFIG_X86_VSMP is not set, limit the number of CPUs to
    the number of CPUs of the first board.
    
    Also make CONFIG_X86_VSMP depend on CONFIG_SMP, as there's
    little point in having a vsmp machine with a single CPU.
    
    Signed-off-by: Shai Fultheim <shai@scalemp.com>
    [ido@wizery.com: rebased, fixed minor coding-style issues]
    Signed-off-by: Ido Yariv <ido@wizery.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index a1d804bcd483..8eeb55a551b4 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -15,6 +15,7 @@
 #include <linux/init.h>
 #include <linux/pci_ids.h>
 #include <linux/pci_regs.h>
+#include <linux/smp.h>
 
 #include <asm/apic.h>
 #include <asm/pci-direct.h>
@@ -22,6 +23,8 @@
 #include <asm/paravirt.h>
 #include <asm/setup.h>
 
+#define TOPOLOGY_REGISTER_OFFSET 0x10
+
 #if defined CONFIG_PCI && defined CONFIG_PARAVIRT
 /*
  * Interrupt control on vSMPowered systems:
@@ -149,12 +152,49 @@ int is_vsmp_box(void)
 	return 0;
 }
 #endif
+
+static void __init vsmp_cap_cpus(void)
+{
+#if !defined(CONFIG_X86_VSMP) && defined(CONFIG_SMP)
+	void __iomem *address;
+	unsigned int cfg, topology, node_shift, maxcpus;
+
+	/*
+	 * CONFIG_X86_VSMP is not configured, so limit the number CPUs to the
+	 * ones present in the first board, unless explicitly overridden by
+	 * setup_max_cpus
+	 */
+	if (setup_max_cpus != NR_CPUS)
+		return;
+
+	/* Read the vSMP Foundation topology register */
+	cfg = read_pci_config(0, 0x1f, 0, PCI_BASE_ADDRESS_0);
+	address = early_ioremap(cfg + TOPOLOGY_REGISTER_OFFSET, 4);
+	if (WARN_ON(!address))
+		return;
+
+	topology = readl(address);
+	node_shift = (topology >> 16) & 0x7;
+	if (!node_shift)
+		/* The value 0 should be decoded as 8 */
+		node_shift = 8;
+	maxcpus = (topology & ((1 << node_shift) - 1)) + 1;
+
+	pr_info("vSMP CTL: Capping CPUs to %d (CONFIG_X86_VSMP is unset)\n",
+		maxcpus);
+	setup_max_cpus = maxcpus;
+	early_iounmap(address, 4);
+#endif
+}
+
 void __init vsmp_init(void)
 {
 	detect_vsmp_box();
 	if (!is_vsmp_box())
 		return;
 
+	vsmp_cap_cpus();
+
 	set_vsmp_pv_ops();
 	return;
 }

commit 70511134f61bd6e5eed19f767381f9fb3e762d49
Author: Ravikiran G Thirumalai <kiran@scalex86.org>
Date:   Mon Mar 23 23:14:29 2009 -0700

    Revert "x86: don't compile vsmp_64 for 32bit"
    
    Partial revert of commit 129d8bc828e011bda0b7110a097bf3a0167f966e
    titled 'x86: don't compile vsmp_64 for 32bit'
    
    Commit reverted to compile vsmp_64.c if CONFIG_X86_64 is defined,
    since is_vsmp_box() needs to indicate that TSCs are not synchronized, and
    hence, not a valid time source, even when CONFIG_X86_VSMP is not defined.
    
    Signed-off-by: Ravikiran Thirumalai <kiran@scalex86.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: shai@scalex86.org
    LKML-Reference: <20090324061429.GH7278@localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 74de562812cc..a1d804bcd483 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -22,7 +22,7 @@
 #include <asm/paravirt.h>
 #include <asm/setup.h>
 
-#ifdef CONFIG_PARAVIRT
+#if defined CONFIG_PCI && defined CONFIG_PARAVIRT
 /*
  * Interrupt control on vSMPowered systems:
  * ~AC is a shadow of IF.  If IF is 'on' AC should be 'off'
@@ -114,6 +114,7 @@ static void __init set_vsmp_pv_ops(void)
 }
 #endif
 
+#ifdef CONFIG_PCI
 static int is_vsmp = -1;
 
 static void __init detect_vsmp_box(void)
@@ -139,6 +140,15 @@ int is_vsmp_box(void)
 	}
 }
 
+#else
+static void __init detect_vsmp_box(void)
+{
+}
+int is_vsmp_box(void)
+{
+	return 0;
+}
+#endif
 void __init vsmp_init(void)
 {
 	detect_vsmp_box();

commit 129d8bc828e011bda0b7110a097bf3a0167f966e
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Wed Feb 25 21:20:50 2009 -0800

    x86: don't compile vsmp_64 for 32bit
    
    Impact: cleanup
    
    that is only needed when CONFIG_X86_VSMP is defined with 64bit
    also remove dead code about PCI, because CONFIG_X86_VSMP depends on PCI
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Ravikiran Thirumalai <kiran@scalex86.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index c609205df594..74de562812cc 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -22,7 +22,7 @@
 #include <asm/paravirt.h>
 #include <asm/setup.h>
 
-#if defined CONFIG_PCI && defined CONFIG_PARAVIRT
+#ifdef CONFIG_PARAVIRT
 /*
  * Interrupt control on vSMPowered systems:
  * ~AC is a shadow of IF.  If IF is 'on' AC should be 'off'
@@ -114,7 +114,6 @@ static void __init set_vsmp_pv_ops(void)
 }
 #endif
 
-#ifdef CONFIG_PCI
 static int is_vsmp = -1;
 
 static void __init detect_vsmp_box(void)
@@ -139,15 +138,6 @@ int is_vsmp_box(void)
 		return 0;
 	}
 }
-#else
-static void __init detect_vsmp_box(void)
-{
-}
-int is_vsmp_box(void)
-{
-	return 0;
-}
-#endif
 
 void __init vsmp_init(void)
 {

commit ecb93d1ccd0aac63f03be2db3cac3fa974716f4c
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jan 28 14:35:05 2009 -0800

    x86/paravirt: add register-saving thunks to reduce caller register pressure
    
    Impact: Optimization
    
    One of the problems with inserting a pile of C calls where previously
    there were none is that the register pressure is greatly increased.
    The C calling convention says that the caller must expect a certain
    set of registers may be trashed by the callee, and that the callee can
    use those registers without restriction.  This includes the function
    argument registers, and several others.
    
    This patch seeks to alleviate this pressure by introducing wrapper
    thunks that will do the register saving/restoring, so that the
    callsite doesn't need to worry about it, but the callee function can
    be conventional compiler-generated code.  In many cases (particularly
    performance-sensitive cases) the callee will be in assembler anyway,
    and need not use the compiler's calling convention.
    
    Standard calling convention is:
             arguments          return      scratch
    x86-32   eax edx ecx        eax         ?
    x86-64   rdi rsi rdx rcx    rax         r8 r9 r10 r11
    
    The thunk preserves all argument and scratch registers.  The return
    register is not preserved, and is available as a scratch register for
    unwrapped callee code (and of course the return value).
    
    Wrapped function pointers are themselves wrapped in a struct
    paravirt_callee_save structure, in order to get some warning from the
    compiler when functions with mismatched calling conventions are used.
    
    The most common paravirt ops, both statically and dynamically, are
    interrupt enable/disable/save/restore, so handle them first.  This is
    particularly easy since their calls are handled specially anyway.
    
    XXX Deal with VMI.  What's their calling convention?
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index a688f3bfaec2..c609205df594 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -37,6 +37,7 @@ static unsigned long vsmp_save_fl(void)
 		flags &= ~X86_EFLAGS_IF;
 	return flags;
 }
+PV_CALLEE_SAVE_REGS_THUNK(vsmp_save_fl);
 
 static void vsmp_restore_fl(unsigned long flags)
 {
@@ -46,6 +47,7 @@ static void vsmp_restore_fl(unsigned long flags)
 		flags |= X86_EFLAGS_AC;
 	native_restore_fl(flags);
 }
+PV_CALLEE_SAVE_REGS_THUNK(vsmp_restore_fl);
 
 static void vsmp_irq_disable(void)
 {
@@ -53,6 +55,7 @@ static void vsmp_irq_disable(void)
 
 	native_restore_fl((flags & ~X86_EFLAGS_IF) | X86_EFLAGS_AC);
 }
+PV_CALLEE_SAVE_REGS_THUNK(vsmp_irq_disable);
 
 static void vsmp_irq_enable(void)
 {
@@ -60,6 +63,7 @@ static void vsmp_irq_enable(void)
 
 	native_restore_fl((flags | X86_EFLAGS_IF) & (~X86_EFLAGS_AC));
 }
+PV_CALLEE_SAVE_REGS_THUNK(vsmp_irq_enable);
 
 static unsigned __init_or_module vsmp_patch(u8 type, u16 clobbers, void *ibuf,
 				  unsigned long addr, unsigned len)
@@ -90,10 +94,10 @@ static void __init set_vsmp_pv_ops(void)
 	       cap, ctl);
 	if (cap & ctl & (1 << 4)) {
 		/* Setup irq ops and turn on vSMP  IRQ fastpath handling */
-		pv_irq_ops.irq_disable = vsmp_irq_disable;
-		pv_irq_ops.irq_enable  = vsmp_irq_enable;
-		pv_irq_ops.save_fl  = vsmp_save_fl;
-		pv_irq_ops.restore_fl  = vsmp_restore_fl;
+		pv_irq_ops.irq_disable = PV_CALLEE_SAVE(vsmp_irq_disable);
+		pv_irq_ops.irq_enable  = PV_CALLEE_SAVE(vsmp_irq_enable);
+		pv_irq_ops.save_fl  = PV_CALLEE_SAVE(vsmp_save_fl);
+		pv_irq_ops.restore_fl  = PV_CALLEE_SAVE(vsmp_restore_fl);
 		pv_init_ops.patch = vsmp_patch;
 
 		ctl &= ~(1 << 4);

commit 9352f5698db2c6d7f2789f6cd37e3996d49ac4b5
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Tue Oct 28 23:05:22 2008 -0700

    x86: two trivial sparse annotations
    
    Impact: fewer sparse warnings, no functional changes
    
    arch/x86/kernel/vsmp_64.c:87:14: warning: incorrect type in argument 1 (different address spaces)
    arch/x86/kernel/vsmp_64.c:87:14:    expected void const volatile [noderef] <asn:2>*addr
    arch/x86/kernel/vsmp_64.c:87:14:    got void *[assigned] address
    arch/x86/kernel/vsmp_64.c:88:22: warning: incorrect type in argument 1 (different address spaces)
    arch/x86/kernel/vsmp_64.c:88:22:    expected void const volatile [noderef] <asn:2>*addr
    arch/x86/kernel/vsmp_64.c:88:22:    got void *
    arch/x86/kernel/vsmp_64.c:100:23: warning: incorrect type in argument 2 (different address spaces)
    arch/x86/kernel/vsmp_64.c:100:23:    expected void volatile [noderef] <asn:2>*addr
    arch/x86/kernel/vsmp_64.c:100:23:    got void *
    arch/x86/kernel/vsmp_64.c:101:23: warning: incorrect type in argument 1 (different address spaces)
    arch/x86/kernel/vsmp_64.c:101:23:    expected void const volatile [noderef] <asn:2>*addr
    arch/x86/kernel/vsmp_64.c:101:23:    got void *
    arch/x86/mm/gup.c:235:6: warning: incorrect type in argument 1 (different base types)
    arch/x86/mm/gup.c:235:6:    expected void const volatile [noderef] <asn:1>*<noident>
    arch/x86/mm/gup.c:235:6:    got unsigned long [unsigned] [assigned] start
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 7766d36983fc..a688f3bfaec2 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -78,7 +78,7 @@ static unsigned __init_or_module vsmp_patch(u8 type, u16 clobbers, void *ibuf,
 
 static void __init set_vsmp_pv_ops(void)
 {
-	void *address;
+	void __iomem *address;
 	unsigned int cap, ctl, cfg;
 
 	/* set vSMP magic bits to indicate vSMP capable kernel */

commit 05e12e1c4c09cd35ac9f4e6af1e42b0036375d72
Author: Ravikiran G Thirumalai <kiran@scalex86.org>
Date:   Mon Sep 22 22:58:47 2008 -0700

    x86: fix 27-rc crash on vsmp due to paravirt during module load
    
    27-rc fails to boot up if configured to use modules.
    
    Turns out vsmp_patch was marked __init, and vsmp_patch being the
    pvops 'patch' routine for vsmp, a call to vsmp_patch just turns out
    to execute a code page with series of 0xcc (POISON_FREE_INITMEM -- int3).
    
    vsmp_patch has been marked with __init ever since pvops, however,
    apply_paravirt can be called during module load causing calls to
    freed memory location.
    
    Since apply_paravirt can only be called during init/module load, make
    vsmp_patch with "__init_or_module"
    
    Signed-off-by: Ravikiran Thirumalai <kiran@scalex86.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 0c029e8959c7..7766d36983fc 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -61,7 +61,7 @@ static void vsmp_irq_enable(void)
 	native_restore_fl((flags | X86_EFLAGS_IF) & (~X86_EFLAGS_AC));
 }
 
-static unsigned __init vsmp_patch(u8 type, u16 clobbers, void *ibuf,
+static unsigned __init_or_module vsmp_patch(u8 type, u16 clobbers, void *ibuf,
 				  unsigned long addr, unsigned len)
 {
 	switch (type) {

commit eef8f871d84b5df1a24902a4e4700188be1aff2c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 12 15:43:34 2008 +0200

    x86: vsmp_64 add missing includes
    
    sparse mutters:
    arch/x86/kernel/vsmp_64.c:126:5: warning: symbol 'is_vsmp_box' was not declared. Should it be static?
    arch/x86/kernel/vsmp_64.c:145:13: warning: symbol 'vsmp_init' was not declared. Should it be static?
    
    Include the appropriate headers.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index ba8c0b75ab0a..0c029e8959c7 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -15,9 +15,12 @@
 #include <linux/init.h>
 #include <linux/pci_ids.h>
 #include <linux/pci_regs.h>
+
+#include <asm/apic.h>
 #include <asm/pci-direct.h>
 #include <asm/io.h>
 #include <asm/paravirt.h>
+#include <asm/setup.h>
 
 #if defined CONFIG_PCI && defined CONFIG_PARAVIRT
 /*

commit 8008abbd87644c84f93a7a86fec88f1e14031901
Author: Alexander van Heukelum <heukelum@mailshack.com>
Date:   Wed Apr 16 18:45:35 2008 +0200

    x86: fix warning in "x86: clean up vSMP detection"
    
    The function detect_vsmp_box is a void function in the PCI case.
    Change the !PCI stub to void too.
    
    Signed-off-by: Alexander van Heukelum <heukelum@fastmail.fm>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index caf2a26f5cfd..ba8c0b75ab0a 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -133,7 +133,7 @@ int is_vsmp_box(void)
 	}
 }
 #else
-static int __init detect_vsmp_box(void)
+static void __init detect_vsmp_box(void)
 {
 }
 int is_vsmp_box(void)

commit e5699a8231593d0e11e65ccf248549935304dab1
Author: Ravikiran G Thirumalai <kiran@scalex86.org>
Date:   Mon Mar 24 14:48:36 2008 -0700

    x86: clean up vSMP detection
    
    vSMP detection: access pci config space early in boot to detect if the
    system is a vSMPowered box, and cache the result in a flag, so that
    is_vsmp_box() retrieves the value of the flag always.
    
    Signed-off-by: Ravikiran Thirumalai <kiran@scalex86.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 1e9a791dbe39..caf2a26f5cfd 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -108,25 +108,34 @@ static void __init set_vsmp_pv_ops(void)
 #endif
 
 #ifdef CONFIG_PCI
-static int vsmp = -1;
+static int is_vsmp = -1;
 
-int is_vsmp_box(void)
+static void __init detect_vsmp_box(void)
 {
-	if (vsmp != -1)
-		return vsmp;
+	is_vsmp = 0;
 
-	vsmp = 0;
 	if (!early_pci_allowed())
-		return vsmp;
+		return;
 
-	/* Check if we are running on a ScaleMP vSMP box */
+	/* Check if we are running on a ScaleMP vSMPowered box */
 	if (read_pci_config(0, 0x1f, 0, PCI_VENDOR_ID) ==
 	     (PCI_VENDOR_ID_SCALEMP | (PCI_DEVICE_ID_SCALEMP_VSMP_CTL << 16)))
-		vsmp = 1;
+		is_vsmp = 1;
+}
 
-	return vsmp;
+int is_vsmp_box(void)
+{
+	if (is_vsmp != -1)
+		return is_vsmp;
+	else {
+		WARN_ON_ONCE(1);
+		return 0;
+	}
 }
 #else
+static int __init detect_vsmp_box(void)
+{
+}
 int is_vsmp_box(void)
 {
 	return 0;
@@ -135,6 +144,7 @@ int is_vsmp_box(void)
 
 void __init vsmp_init(void)
 {
+	detect_vsmp_box();
 	if (!is_vsmp_box())
 		return;
 

commit 6542fe80e6296cde50c1c3b8a9eede701ee51907
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 21 09:55:06 2008 +0100

    x86: vsmp fix x86 vsmp fix is vsmp box cleanup
    
    code got a bit smaller:
    
    arch/x86/kernel/vsmp_64.o:
    
       text    data     bss     dec     hex filename
        205       4       0     209      d1 vsmp_64.o.before
        181       4       0     185      b9 vsmp_64.o.after
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 13bd82453e4b..1e9a791dbe39 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -120,10 +120,8 @@ int is_vsmp_box(void)
 		return vsmp;
 
 	/* Check if we are running on a ScaleMP vSMP box */
-	if ((read_pci_config_16(0, 0x1f, 0, PCI_VENDOR_ID) ==
-	     PCI_VENDOR_ID_SCALEMP) &&
-	    (read_pci_config_16(0, 0x1f, 0, PCI_DEVICE_ID) ==
-	    PCI_DEVICE_ID_SCALEMP_VSMP_CTL))
+	if (read_pci_config(0, 0x1f, 0, PCI_VENDOR_ID) ==
+	     (PCI_VENDOR_ID_SCALEMP | (PCI_DEVICE_ID_SCALEMP_VSMP_CTL << 16)))
 		vsmp = 1;
 
 	return vsmp;

commit 9f6d8552a9cb49dc556777bbdf7ac8b3d7e18edb
Author: Ravikiran G Thirumalai <kiran@scalex86.org>
Date:   Thu Mar 20 00:43:16 2008 -0700

    x86: vSMP: use pvops only if platform has the capability to support it
    
    Re-arrange set_vsmp_pv_ops so that pv_ops are set only if
    the platform has capability to support paravirtualized irq ops
    
    Signed-off-by: Ravikiran Thirumalai <kiran@scalex86.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 4a790a5f61b7..13bd82453e4b 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -78,12 +78,6 @@ static void __init set_vsmp_pv_ops(void)
 	void *address;
 	unsigned int cap, ctl, cfg;
 
-	pv_irq_ops.irq_disable = vsmp_irq_disable;
-	pv_irq_ops.irq_enable  = vsmp_irq_enable;
-	pv_irq_ops.save_fl  = vsmp_save_fl;
-	pv_irq_ops.restore_fl  = vsmp_restore_fl;
-	pv_init_ops.patch = vsmp_patch;
-
 	/* set vSMP magic bits to indicate vSMP capable kernel */
 	cfg = read_pci_config(0, 0x1f, 0, PCI_BASE_ADDRESS_0);
 	address = early_ioremap(cfg, 8);
@@ -92,7 +86,13 @@ static void __init set_vsmp_pv_ops(void)
 	printk(KERN_INFO "vSMP CTL: capabilities:0x%08x  control:0x%08x\n",
 	       cap, ctl);
 	if (cap & ctl & (1 << 4)) {
-		/* Turn on vSMP IRQ fastpath handling (see system.h) */
+		/* Setup irq ops and turn on vSMP  IRQ fastpath handling */
+		pv_irq_ops.irq_disable = vsmp_irq_disable;
+		pv_irq_ops.irq_enable  = vsmp_irq_enable;
+		pv_irq_ops.save_fl  = vsmp_save_fl;
+		pv_irq_ops.restore_fl  = vsmp_restore_fl;
+		pv_init_ops.patch = vsmp_patch;
+
 		ctl &= ~(1 << 4);
 		writel(ctl, address + 4);
 		ctl = readl(address + 4);

commit aa7d8e25eca5deb33eb08013bc78a80514349b40
Author: Ravikiran G Thirumalai <kiran@scalex86.org>
Date:   Thu Mar 20 00:41:16 2008 -0700

    x86: fix build breakage when PCI is define and PARAVIRT is not
    
    - Fix the the build breakage when PARAVIRT is defined
      but PCI is not
      This fixes problem reported at:
            http://marc.info/?l=linux-kernel&m=120525966600698&w=2
    - Make is_vsmp_box() available even when PARAVIRT is not defined.
      This is needed to determine if tsc's are reliable as a time source
      even when PARAVIRT is not defined.
    - split vsmp_init to use is_vsmp_box() and set_vsmp_pv_ops()
      set_vsmp_pv_ops will do nothing if PCI is not enabled in the config.
    
    Signed-off-by: Ravikiran Thirumalai <kiran@scalex86.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index eb25584c54c3..4a790a5f61b7 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -19,6 +19,7 @@
 #include <asm/io.h>
 #include <asm/paravirt.h>
 
+#if defined CONFIG_PCI && defined CONFIG_PARAVIRT
 /*
  * Interrupt control on vSMPowered systems:
  * ~AC is a shadow of IF.  If IF is 'on' AC should be 'off'
@@ -72,39 +73,11 @@ static unsigned __init vsmp_patch(u8 type, u16 clobbers, void *ibuf,
 
 }
 
-static int vsmp = -1;
-
-int is_vsmp_box(void)
-{
-	if (vsmp != -1)
-		return vsmp;
-
-	vsmp = 0;
-	if (!early_pci_allowed())
-		return vsmp;
-
-	/* Check if we are running on a ScaleMP vSMP box */
-	if ((read_pci_config_16(0, 0x1f, 0, PCI_VENDOR_ID) ==
-	     PCI_VENDOR_ID_SCALEMP) &&
-	    (read_pci_config_16(0, 0x1f, 0, PCI_DEVICE_ID) ==
-	    PCI_DEVICE_ID_SCALEMP_VSMP_CTL))
-		vsmp = 1;
-
-	return vsmp;
-}
-
-void __init vsmp_init(void)
+static void __init set_vsmp_pv_ops(void)
 {
 	void *address;
 	unsigned int cap, ctl, cfg;
 
-	if (!is_vsmp_box())
-		return;
-
-	if (!early_pci_allowed())
-		return;
-
-	/* If we are, use the distinguished irq functions */
 	pv_irq_ops.irq_disable = vsmp_irq_disable;
 	pv_irq_ops.irq_enable  = vsmp_irq_enable;
 	pv_irq_ops.save_fl  = vsmp_save_fl;
@@ -127,5 +100,46 @@ void __init vsmp_init(void)
 	}
 
 	early_iounmap(address, 8);
+}
+#else
+static void __init set_vsmp_pv_ops(void)
+{
+}
+#endif
+
+#ifdef CONFIG_PCI
+static int vsmp = -1;
+
+int is_vsmp_box(void)
+{
+	if (vsmp != -1)
+		return vsmp;
+
+	vsmp = 0;
+	if (!early_pci_allowed())
+		return vsmp;
+
+	/* Check if we are running on a ScaleMP vSMP box */
+	if ((read_pci_config_16(0, 0x1f, 0, PCI_VENDOR_ID) ==
+	     PCI_VENDOR_ID_SCALEMP) &&
+	    (read_pci_config_16(0, 0x1f, 0, PCI_DEVICE_ID) ==
+	    PCI_DEVICE_ID_SCALEMP_VSMP_CTL))
+		vsmp = 1;
+
+	return vsmp;
+}
+#else
+int is_vsmp_box(void)
+{
+	return 0;
+}
+#endif
+
+void __init vsmp_init(void)
+{
+	if (!is_vsmp_box())
+		return;
+
+	set_vsmp_pv_ops();
 	return;
 }

commit 3250c91ada16a06de5afef55bce7b766c894d75c
Author: Ravikiran G Thirumalai <kiran@scalex86.org>
Date:   Thu Mar 20 00:39:02 2008 -0700

    x86: vSMP: Fix is_vsmp_box()
    
    is_vsmp_box() currently does not work on vSMPowered systems,  as pci cfg
    space is not read correctly -- This patch fixes it.
    
    Signed-off-by: Ravikiran Thirumalai <kiran@scalex86.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index a00961d42e75..eb25584c54c3 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -84,8 +84,10 @@ int is_vsmp_box(void)
 		return vsmp;
 
 	/* Check if we are running on a ScaleMP vSMP box */
-	if (read_pci_config(0, 0x1f, 0, PCI_VENDOR_ID) ==
-	     (PCI_VENDOR_ID_SCALEMP || (PCI_DEVICE_ID_SCALEMP_VSMP_CTL << 16)))
+	if ((read_pci_config_16(0, 0x1f, 0, PCI_VENDOR_ID) ==
+	     PCI_VENDOR_ID_SCALEMP) &&
+	    (read_pci_config_16(0, 0x1f, 0, PCI_DEVICE_ID) ==
+	    PCI_DEVICE_ID_SCALEMP_VSMP_CTL))
 		vsmp = 1;
 
 	return vsmp;

commit f8fffa458368ed3d57385698f775880db629bd1a
Author: Yinghai Lu <Yinghai.Lu@Sun.COM>
Date:   Sun Feb 24 21:36:28 2008 -0800

    x86: apic_is_clustered_box for vsmp
    
    quad core 8 socket system will have apic id lifting.the apic id range could
    be [4, 0x23]. and apic_is_clustered_box will think that need to three clusters
    and that is larger than 2. So it is treated as a clustered_box.
    
    and will get:
    
       Marking TSC unstable due to TSCs unsynchronized
    
    even if the CPUs have X86_FEATURE_CONSTANT_TSC set.
    
    this quick fix will check if the cpu is from AMD.
    
    but vsmp still needs that checking...
    
    this patch is fix to make sure that vsmp not to be passed.
    
    Signed-off-by: Yinghai Lu <yinghai.lu@sun.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 54202b1805da..a00961d42e75 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -72,19 +72,34 @@ static unsigned __init vsmp_patch(u8 type, u16 clobbers, void *ibuf,
 
 }
 
+static int vsmp = -1;
+
+int is_vsmp_box(void)
+{
+	if (vsmp != -1)
+		return vsmp;
+
+	vsmp = 0;
+	if (!early_pci_allowed())
+		return vsmp;
+
+	/* Check if we are running on a ScaleMP vSMP box */
+	if (read_pci_config(0, 0x1f, 0, PCI_VENDOR_ID) ==
+	     (PCI_VENDOR_ID_SCALEMP || (PCI_DEVICE_ID_SCALEMP_VSMP_CTL << 16)))
+		vsmp = 1;
+
+	return vsmp;
+}
+
 void __init vsmp_init(void)
 {
 	void *address;
 	unsigned int cap, ctl, cfg;
 
-	if (!early_pci_allowed())
+	if (!is_vsmp_box())
 		return;
 
-	/* Check if we are running on a ScaleMP vSMP box */
-	if ((read_pci_config_16(0, 0x1f, 0, PCI_VENDOR_ID) !=
-	     PCI_VENDOR_ID_SCALEMP) ||
-	    (read_pci_config_16(0, 0x1f, 0, PCI_DEVICE_ID) !=
-	     PCI_DEVICE_ID_SCALEMP_VSMP_CTL))
+	if (!early_pci_allowed())
 		return;
 
 	/* If we are, use the distinguished irq functions */

commit bc7c314d7048017caa0725b41cc577cccf4fc53b
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Feb 11 17:16:05 2008 -0200

    x86, vsmp: use the paravirt helpers
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ravikiran Thirumalai <kiran@scalemp.com>
    Acked-by: Shai Fultheim <shai@scalemp.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index b93ed66c754f..54202b1805da 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -87,6 +87,13 @@ void __init vsmp_init(void)
 	     PCI_DEVICE_ID_SCALEMP_VSMP_CTL))
 		return;
 
+	/* If we are, use the distinguished irq functions */
+	pv_irq_ops.irq_disable = vsmp_irq_disable;
+	pv_irq_ops.irq_enable  = vsmp_irq_enable;
+	pv_irq_ops.save_fl  = vsmp_save_fl;
+	pv_irq_ops.restore_fl  = vsmp_restore_fl;
+	pv_init_ops.patch = vsmp_patch;
+
 	/* set vSMP magic bits to indicate vSMP capable kernel */
 	cfg = read_pci_config(0, 0x1f, 0, PCI_BASE_ADDRESS_0);
 	address = early_ioremap(cfg, 8);

commit 96597fd2be7070631ad0776cd8bced21415fd5e3
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Feb 11 17:16:04 2008 -0200

    x86: introduce vsmp paravirt helpers
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ravikiran Thirumalai <kiran@scalemp.com>
    Acked-by: Shai Fultheim <shai@scalemp.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index fdf9fba6ba9c..b93ed66c754f 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -8,6 +8,8 @@
  *
  * Ravikiran Thirumalai <kiran@scalemp.com>,
  * Shai Fultheim <shai@scalemp.com>
+ * Paravirt ops integration: Glauber de Oliveira Costa <gcosta@redhat.com>,
+ *			     Ravikiran Thirumalai <kiran@scalemp.com>
  */
 
 #include <linux/init.h>
@@ -15,6 +17,60 @@
 #include <linux/pci_regs.h>
 #include <asm/pci-direct.h>
 #include <asm/io.h>
+#include <asm/paravirt.h>
+
+/*
+ * Interrupt control on vSMPowered systems:
+ * ~AC is a shadow of IF.  If IF is 'on' AC should be 'off'
+ * and vice versa.
+ */
+
+static unsigned long vsmp_save_fl(void)
+{
+	unsigned long flags = native_save_fl();
+
+	if (!(flags & X86_EFLAGS_IF) || (flags & X86_EFLAGS_AC))
+		flags &= ~X86_EFLAGS_IF;
+	return flags;
+}
+
+static void vsmp_restore_fl(unsigned long flags)
+{
+	if (flags & X86_EFLAGS_IF)
+		flags &= ~X86_EFLAGS_AC;
+	else
+		flags |= X86_EFLAGS_AC;
+	native_restore_fl(flags);
+}
+
+static void vsmp_irq_disable(void)
+{
+	unsigned long flags = native_save_fl();
+
+	native_restore_fl((flags & ~X86_EFLAGS_IF) | X86_EFLAGS_AC);
+}
+
+static void vsmp_irq_enable(void)
+{
+	unsigned long flags = native_save_fl();
+
+	native_restore_fl((flags | X86_EFLAGS_IF) & (~X86_EFLAGS_AC));
+}
+
+static unsigned __init vsmp_patch(u8 type, u16 clobbers, void *ibuf,
+				  unsigned long addr, unsigned len)
+{
+	switch (type) {
+	case PARAVIRT_PATCH(pv_irq_ops.irq_enable):
+	case PARAVIRT_PATCH(pv_irq_ops.irq_disable):
+	case PARAVIRT_PATCH(pv_irq_ops.save_fl):
+	case PARAVIRT_PATCH(pv_irq_ops.restore_fl):
+		return paravirt_patch_default(type, clobbers, ibuf, addr, len);
+	default:
+		return native_patch(type, clobbers, ibuf, addr, len);
+	}
+
+}
 
 void __init vsmp_init(void)
 {

commit 2785c8d052278228cc3806233c09295088f83d42
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Feb 11 17:16:03 2008 -0200

    x86: call vsmp_init explicitly
    
    It becomes to early for ioremap, so we use early_ioremap
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ravikiran Thirumalai <kiran@scalemp.com>
    Acked-by: Shai Fultheim <shai@scalemp.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 976691726de4..fdf9fba6ba9c 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -16,10 +16,10 @@
 #include <asm/pci-direct.h>
 #include <asm/io.h>
 
-static void __init vsmp_init(void)
+void __init vsmp_init(void)
 {
 	void *address;
-	unsigned int cap, ctl;
+	unsigned int cap, ctl, cfg;
 
 	if (!early_pci_allowed())
 		return;
@@ -32,7 +32,8 @@ static void __init vsmp_init(void)
 		return;
 
 	/* set vSMP magic bits to indicate vSMP capable kernel */
-	address = ioremap(read_pci_config(0, 0x1f, 0, PCI_BASE_ADDRESS_0), 8);
+	cfg = read_pci_config(0, 0x1f, 0, PCI_BASE_ADDRESS_0);
+	address = early_ioremap(cfg, 8);
 	cap = readl(address);
 	ctl = readl(address + 4);
 	printk(KERN_INFO "vSMP CTL: capabilities:0x%08x  control:0x%08x\n",
@@ -45,8 +46,6 @@ static void __init vsmp_init(void)
 		printk(KERN_INFO "vSMP CTL: control set to:0x%08x\n", ctl);
 	}
 
-	iounmap(address);
+	early_iounmap(address, 8);
 	return;
 }
-
-core_initcall(vsmp_init);

commit a2beab31b167bd8ba49bb84944e07ac096f2ab0a
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Feb 11 17:16:02 2008 -0200

    x86: make vsmp_init void, instead of static int
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ravikiran Thirumalai <kiran@scalemp.com>
    Acked-by: Shai Fultheim <shai@scalemp.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index d971210a6d36..976691726de4 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -16,20 +16,20 @@
 #include <asm/pci-direct.h>
 #include <asm/io.h>
 
-static int __init vsmp_init(void)
+static void __init vsmp_init(void)
 {
 	void *address;
 	unsigned int cap, ctl;
 
 	if (!early_pci_allowed())
-		return 0;
+		return;
 
 	/* Check if we are running on a ScaleMP vSMP box */
 	if ((read_pci_config_16(0, 0x1f, 0, PCI_VENDOR_ID) !=
 	     PCI_VENDOR_ID_SCALEMP) ||
 	    (read_pci_config_16(0, 0x1f, 0, PCI_DEVICE_ID) !=
 	     PCI_DEVICE_ID_SCALEMP_VSMP_CTL))
-		return 0;
+		return;
 
 	/* set vSMP magic bits to indicate vSMP capable kernel */
 	address = ioremap(read_pci_config(0, 0x1f, 0, PCI_BASE_ADDRESS_0), 8);
@@ -46,7 +46,7 @@ static int __init vsmp_init(void)
 	}
 
 	iounmap(address);
-	return 0;
+	return;
 }
 
 core_initcall(vsmp_init);

commit ed4aed98da8d042716d327a0b538dd8002c0a767
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 30 13:30:24 2008 +0100

    x86: clean up arch/x86/kernel/vsmp_64.c
    
    White space and coding style clenaup.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
index 414caf0c5f9a..d971210a6d36 100644
--- a/arch/x86/kernel/vsmp_64.c
+++ b/arch/x86/kernel/vsmp_64.c
@@ -25,21 +25,24 @@ static int __init vsmp_init(void)
 		return 0;
 
 	/* Check if we are running on a ScaleMP vSMP box */
-	if ((read_pci_config_16(0, 0x1f, 0, PCI_VENDOR_ID) != PCI_VENDOR_ID_SCALEMP) ||
-	    (read_pci_config_16(0, 0x1f, 0, PCI_DEVICE_ID) != PCI_DEVICE_ID_SCALEMP_VSMP_CTL))
+	if ((read_pci_config_16(0, 0x1f, 0, PCI_VENDOR_ID) !=
+	     PCI_VENDOR_ID_SCALEMP) ||
+	    (read_pci_config_16(0, 0x1f, 0, PCI_DEVICE_ID) !=
+	     PCI_DEVICE_ID_SCALEMP_VSMP_CTL))
 		return 0;
 
 	/* set vSMP magic bits to indicate vSMP capable kernel */
 	address = ioremap(read_pci_config(0, 0x1f, 0, PCI_BASE_ADDRESS_0), 8);
 	cap = readl(address);
 	ctl = readl(address + 4);
-	printk("vSMP CTL: capabilities:0x%08x  control:0x%08x\n", cap, ctl);
+	printk(KERN_INFO "vSMP CTL: capabilities:0x%08x  control:0x%08x\n",
+	       cap, ctl);
 	if (cap & ctl & (1 << 4)) {
 		/* Turn on vSMP IRQ fastpath handling (see system.h) */
 		ctl &= ~(1 << 4);
 		writel(ctl, address + 4);
 		ctl = readl(address + 4);
-		printk("vSMP CTL: control set to:0x%08x\n", ctl);
+		printk(KERN_INFO "vSMP CTL: control set to:0x%08x\n", ctl);
 	}
 
 	iounmap(address);

commit 250c22777fe1ccd7ac588579a6c16db4c0161cc5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:17:24 2007 +0200

    x86_64: move kernel
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/vsmp_64.c b/arch/x86/kernel/vsmp_64.c
new file mode 100644
index 000000000000..414caf0c5f9a
--- /dev/null
+++ b/arch/x86/kernel/vsmp_64.c
@@ -0,0 +1,49 @@
+/*
+ * vSMPowered(tm) systems specific initialization
+ * Copyright (C) 2005 ScaleMP Inc.
+ *
+ * Use of this code is subject to the terms and conditions of the
+ * GNU general public license version 2. See "COPYING" or
+ * http://www.gnu.org/licenses/gpl.html
+ *
+ * Ravikiran Thirumalai <kiran@scalemp.com>,
+ * Shai Fultheim <shai@scalemp.com>
+ */
+
+#include <linux/init.h>
+#include <linux/pci_ids.h>
+#include <linux/pci_regs.h>
+#include <asm/pci-direct.h>
+#include <asm/io.h>
+
+static int __init vsmp_init(void)
+{
+	void *address;
+	unsigned int cap, ctl;
+
+	if (!early_pci_allowed())
+		return 0;
+
+	/* Check if we are running on a ScaleMP vSMP box */
+	if ((read_pci_config_16(0, 0x1f, 0, PCI_VENDOR_ID) != PCI_VENDOR_ID_SCALEMP) ||
+	    (read_pci_config_16(0, 0x1f, 0, PCI_DEVICE_ID) != PCI_DEVICE_ID_SCALEMP_VSMP_CTL))
+		return 0;
+
+	/* set vSMP magic bits to indicate vSMP capable kernel */
+	address = ioremap(read_pci_config(0, 0x1f, 0, PCI_BASE_ADDRESS_0), 8);
+	cap = readl(address);
+	ctl = readl(address + 4);
+	printk("vSMP CTL: capabilities:0x%08x  control:0x%08x\n", cap, ctl);
+	if (cap & ctl & (1 << 4)) {
+		/* Turn on vSMP IRQ fastpath handling (see system.h) */
+		ctl &= ~(1 << 4);
+		writel(ctl, address + 4);
+		ctl = readl(address + 4);
+		printk("vSMP CTL: control set to:0x%08x\n", ctl);
+	}
+
+	iounmap(address);
+	return 0;
+}
+
+core_initcall(vsmp_init);
