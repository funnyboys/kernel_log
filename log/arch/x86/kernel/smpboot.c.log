commit 65fddcfca8ad14778f71a57672fd01e8112d30fa
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:42 2020 -0700

    mm: reorder includes after introduction of linux/pgtable.h
    
    The replacement of <asm/pgrable.h> with <linux/pgtable.h> made the include
    of the latter in the middle of asm includes.  Fix this up with the aid of
    the below script and manual adjustments here and there.
    
            import sys
            import re
    
            if len(sys.argv) is not 3:
                print "USAGE: %s <file> <header>" % (sys.argv[0])
                sys.exit(1)
    
            hdr_to_move="#include <linux/%s>" % sys.argv[2]
            moved = False
            in_hdrs = False
    
            with open(sys.argv[1], "r") as f:
                lines = f.readlines()
                for _line in lines:
                    line = _line.rstrip('
    ')
                    if line == hdr_to_move:
                        continue
                    if line.startswith("#include <linux/"):
                        in_hdrs = True
                    elif not moved and in_hdrs:
                        moved = True
                        print hdr_to_move
                    print line
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 48caa588fd53..ffbd9a3d78d8 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -55,6 +55,7 @@
 #include <linux/gfp.h>
 #include <linux/cpuidle.h>
 #include <linux/numa.h>
+#include <linux/pgtable.h>
 
 #include <asm/acpi.h>
 #include <asm/desc.h>
@@ -63,7 +64,6 @@
 #include <asm/realmode.h>
 #include <asm/cpu.h>
 #include <asm/numa.h>
-#include <linux/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mtrr.h>
 #include <asm/mwait.h>

commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2467f3dd35d3..48caa588fd53 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -63,7 +63,7 @@
 #include <asm/realmode.h>
 #include <asm/cpu.h>
 #include <asm/numa.h>
-#include <asm/pgtable.h>
+#include <linux/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mtrr.h>
 #include <asm/mwait.h>

commit 17e0a7cb6a254c6d086562e7adf8b7ac24d267f3
Merge: bb548bedf5c5 2ca41f555e85
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 13:47:10 2020 -0700

    Merge tag 'x86-cleanups-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc cleanups, with an emphasis on removing obsolete/dead code"
    
    * tag 'x86-cleanups-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/spinlock: Remove obsolete ticket spinlock macros and types
      x86/mm: Drop deprecated DISCONTIGMEM support for 32-bit
      x86/apb_timer: Drop unused declaration and macro
      x86/apb_timer: Drop unused TSC calibration
      x86/io_apic: Remove unused function mp_init_irq_at_boot()
      x86/mm: Stop printing BRK addresses
      x86/audit: Fix a -Wmissing-prototypes warning for ia32_classify_syscall()
      x86/nmi: Remove edac.h include leftover
      mm: Remove MPX leftovers
      x86/mm/mmap: Fix -Wmissing-prototypes warnings
      x86/early_printk: Remove unused includes
      crash_dump: Remove no longer used saved_max_pfn
      x86/smpboot: Remove the last ICPU() macro

commit d861f6e6829ce586bcf5823ab7d348f09be3c8fb
Merge: 58ff3b7604a4 fb7fb84a0c4e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 13:38:55 2020 -0700

    Merge tag 'smp-core-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull SMP updates from Ingo Molnar:
     "Misc cleanups in the SMP hotplug and cross-call code"
    
    * tag 'smp-core-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Remove __freeze_secondary_cpus()
      cpu/hotplug: Remove disable_nonboot_cpus()
      cpu/hotplug: Fix a typo in comment "broadacasted"->"broadcasted"
      smp: Use smp_call_func_t in on_each_cpu()

commit a9a3ed1eff3601b63aea4fb462d8b3b92c7c1e7e
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Apr 22 18:11:30 2020 +0200

    x86: Fix early boot crash on gcc-10, third try
    
    ... or the odyssey of trying to disable the stack protector for the
    function which generates the stack canary value.
    
    The whole story started with Sergei reporting a boot crash with a kernel
    built with gcc-10:
    
      Kernel panic — not syncing: stack-protector: Kernel stack is corrupted in: start_secondary
      CPU: 1 PID: 0 Comm: swapper/1 Not tainted 5.6.0-rc5—00235—gfffb08b37df9 #139
      Hardware name: Gigabyte Technology Co., Ltd. To be filled by O.E.M./H77M—D3H, BIOS F12 11/14/2013
      Call Trace:
        dump_stack
        panic
        ? start_secondary
        __stack_chk_fail
        start_secondary
        secondary_startup_64
      -—-[ end Kernel panic — not syncing: stack—protector: Kernel stack is corrupted in: start_secondary
    
    This happens because gcc-10 tail-call optimizes the last function call
    in start_secondary() - cpu_startup_entry() - and thus emits a stack
    canary check which fails because the canary value changes after the
    boot_init_stack_canary() call.
    
    To fix that, the initial attempt was to mark the one function which
    generates the stack canary with:
    
      __attribute__((optimize("-fno-stack-protector"))) ... start_secondary(void *unused)
    
    however, using the optimize attribute doesn't work cumulatively
    as the attribute does not add to but rather replaces previously
    supplied optimization options - roughly all -fxxx options.
    
    The key one among them being -fno-omit-frame-pointer and thus leading to
    not present frame pointer - frame pointer which the kernel needs.
    
    The next attempt to prevent compilers from tail-call optimizing
    the last function call cpu_startup_entry(), shy of carving out
    start_secondary() into a separate compilation unit and building it with
    -fno-stack-protector, was to add an empty asm("").
    
    This current solution was short and sweet, and reportedly, is supported
    by both compilers but we didn't get very far this time: future (LTO?)
    optimization passes could potentially eliminate this, which leads us
    to the third attempt: having an actual memory barrier there which the
    compiler cannot ignore or move around etc.
    
    That should hold for a long time, but hey we said that about the other
    two solutions too so...
    
    Reported-by: Sergei Trofimovich <slyfox@gentoo.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Tested-by: Kalle Valo <kvalo@codeaurora.org>
    Cc: <stable@vger.kernel.org>
    Link: https://lkml.kernel.org/r/20200314164451.346497-1-slyfox@gentoo.org

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8c89e4d9ad28..2f24c334a938 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -266,6 +266,14 @@ static void notrace start_secondary(void *unused)
 
 	wmb();
 	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+
+	/*
+	 * Prevent tail call to cpu_startup_entry() because the stack protector
+	 * guard has been changed a couple of function calls up, in
+	 * boot_init_stack_canary() and must not be checked before tail calling
+	 * another function.
+	 */
+	prevent_tail_call_optimization();
 }
 
 /**

commit 565558558985b1d7cd43b21f18c1ad6b232788d0
Author: Qais Yousef <qais.yousef@arm.com>
Date:   Thu Apr 30 12:40:03 2020 +0100

    cpu/hotplug: Remove disable_nonboot_cpus()
    
    The single user could have called freeze_secondary_cpus() directly.
    
    Since this function was a source of confusion, remove it as it's
    just a pointless wrapper.
    
    While at it, rename enable_nonboot_cpus() to thaw_secondary_cpus() to
    preserve the naming symmetry.
    
    Done automatically via:
    
            git grep -l enable_nonboot_cpus | xargs sed -i 's/enable_nonboot_cpus/thaw_secondary_cpus/g'
    
    Signed-off-by: Qais Yousef <qais.yousef@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Link: https://lkml.kernel.org/r/20200430114004.17477-1-qais.yousef@arm.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fe3ab9632f3b..997b66c18154 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1376,12 +1376,12 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	speculative_store_bypass_ht_init();
 }
 
-void arch_enable_nonboot_cpus_begin(void)
+void arch_thaw_secondary_cpus_begin(void)
 {
 	set_mtrr_aps_delayed_init();
 }
 
-void arch_enable_nonboot_cpus_end(void)
+void arch_thaw_secondary_cpus_end(void)
 {
 	mtrr_aps_init();
 }

commit db441bd9f630329c402d5cdd319f11bfcf509fb6
Author: Giovanni Gherdovich <ggherdovich@suse.cz>
Date:   Thu Apr 16 07:47:45 2020 +0200

    x86, sched: Move check for CPU type to caller function
    
    Improve readability of the function intel_set_max_freq_ratio() by moving
    the check for KNL CPUs there, together with checks for GLM and SKX.
    
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lkml.kernel.org/r/20200416054745.740-5-ggherdovich@suse.cz

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index dd8e15f648bc..8c89e4d9ad28 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1877,9 +1877,6 @@ static bool knl_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq,
 	int err, i;
 	u64 msr;
 
-	if (!x86_match_cpu(has_knl_turbo_ratio_limits))
-		return false;
-
 	err = rdmsrl_safe(MSR_PLATFORM_INFO, base_freq);
 	if (err)
 		return false;
@@ -1977,7 +1974,8 @@ static bool intel_set_max_freq_ratio(void)
 	    skx_set_max_freq_ratio(&base_freq, &turbo_freq, 1))
 		goto out;
 
-	if (knl_set_max_freq_ratio(&base_freq, &turbo_freq, 1))
+	if (x86_match_cpu(has_knl_turbo_ratio_limits) &&
+	    knl_set_max_freq_ratio(&base_freq, &turbo_freq, 1))
 		goto out;
 
 	if (x86_match_cpu(has_skx_turbo_ratio_limits) &&

commit b56e7d45e80796ca963ac10902245b244d823caf
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Thu Apr 16 07:47:44 2020 +0200

    x86, sched: Don't enable static key when starting secondary CPUs
    
    The static key arch_scale_freq_key only needs to be enabled once (at
    boot). This change fixes a bug by which the key was enabled every time cpu0
    is started, even as a secondary CPU during cpu hotplug. Secondary CPUs are
    started from the idle thread: setting a static key from there means
    acquiring a lock and may result in sleeping in the idle task, causing CPU
    lockup.
    
    Another consequence of this change is that init_counter_refs() is now
    called on each CPU correctly; previously the function on_each_cpu() was
    used, but it was called at boot when the only online cpu is cpu0.
    
    [ggherdovich@suse.cz: Tested and wrote changelog]
    Fixes: 1567c3e3467c ("x86, sched: Add support for frequency invariance")
    Reported-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lkml.kernel.org/r/20200416054745.740-4-ggherdovich@suse.cz

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 5d346b70844b..dd8e15f648bc 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -147,7 +147,7 @@ static inline void smpboot_restore_warm_reset_vector(void)
 	*((volatile u32 *)phys_to_virt(TRAMPOLINE_PHYS_LOW)) = 0;
 }
 
-static void init_freq_invariance(void);
+static void init_freq_invariance(bool secondary);
 
 /*
  * Report back to the Boot Processor during boot time or to the caller processor
@@ -185,7 +185,7 @@ static void smp_callin(void)
 	 */
 	set_cpu_sibling_map(raw_smp_processor_id());
 
-	init_freq_invariance();
+	init_freq_invariance(true);
 
 	/*
 	 * Get our bogomips.
@@ -1341,7 +1341,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	set_sched_topology(x86_topology);
 
 	set_cpu_sibling_map(0);
-	init_freq_invariance();
+	init_freq_invariance(false);
 	smp_sanity_check();
 
 	switch (apic_intr_mode) {
@@ -2005,7 +2005,7 @@ static bool intel_set_max_freq_ratio(void)
 	return true;
 }
 
-static void init_counter_refs(void *arg)
+static void init_counter_refs(void)
 {
 	u64 aperf, mperf;
 
@@ -2016,18 +2016,25 @@ static void init_counter_refs(void *arg)
 	this_cpu_write(arch_prev_mperf, mperf);
 }
 
-static void init_freq_invariance(void)
+static void init_freq_invariance(bool secondary)
 {
 	bool ret = false;
 
-	if (smp_processor_id() != 0 || !boot_cpu_has(X86_FEATURE_APERFMPERF))
+	if (!boot_cpu_has(X86_FEATURE_APERFMPERF))
 		return;
 
+	if (secondary) {
+		if (static_branch_likely(&arch_scale_freq_key)) {
+			init_counter_refs();
+		}
+		return;
+	}
+
 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)
 		ret = intel_set_max_freq_ratio();
 
 	if (ret) {
-		on_each_cpu(init_counter_refs, NULL, 1);
+		init_counter_refs();
 		static_branch_enable(&arch_scale_freq_key);
 	} else {
 		pr_debug("Couldn't determine max cpu frequency, necessary for scale-invariant accounting.\n");

commit 23ccee22e834eca236b9a20989caf6905bd6954a
Author: Giovanni Gherdovich <ggherdovich@suse.cz>
Date:   Thu Apr 16 07:47:43 2020 +0200

    x86, sched: Account for CPUs with less than 4 cores in freq. invariance
    
    If a CPU has less than 4 physical cores, MSR_TURBO_RATIO_LIMIT will
    rightfully report that the 4C turbo ratio is zero. In such cases, use the
    1C turbo ratio instead for frequency invariance calculations.
    
    Fixes: 1567c3e3467c ("x86, sched: Add support for frequency invariance")
    Reported-by: Like Xu <like.xu@linux.intel.com>
    Reported-by: Neil Rickert <nwr10cst-oslnx@yahoo.com>
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Tested-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Link: https://lkml.kernel.org/r/20200416054745.740-3-ggherdovich@suse.cz

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3a318ec9bc17..5d346b70844b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1945,18 +1945,23 @@ static bool skx_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq, int size)
 
 static bool core_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq)
 {
+	u64 msr;
 	int err;
 
 	err = rdmsrl_safe(MSR_PLATFORM_INFO, base_freq);
 	if (err)
 		return false;
 
-	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, turbo_freq);
+	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, &msr);
 	if (err)
 		return false;
 
-	*base_freq = (*base_freq >> 8) & 0xFF;      /* max P state */
-	*turbo_freq = (*turbo_freq >> 24) & 0xFF;   /* 4C turbo    */
+	*base_freq = (*base_freq >> 8) & 0xFF;    /* max P state */
+	*turbo_freq = (msr >> 24) & 0xFF;         /* 4C turbo    */
+
+	/* The CPU may have less than 4 cores */
+	if (!*turbo_freq)
+		*turbo_freq = msr & 0xFF;         /* 1C turbo    */
 
 	return true;
 }

commit 9a6c2c3c7a73ce315c57c1b002caad6fcc858d0f
Author: Giovanni Gherdovich <ggherdovich@suse.cz>
Date:   Thu Apr 16 07:47:42 2020 +0200

    x86, sched: Bail out of frequency invariance if base frequency is unknown
    
    Some hypervisors such as VMWare ESXi 5.5 advertise support for
    X86_FEATURE_APERFMPERF but then fill all MSR's with zeroes. In particular,
    MSR_PLATFORM_INFO set to zero tricks the code that wants to know the base
    clock frequency of the CPU (highest non-turbo frequency), producing a
    division by zero when computing the ratio turbo_freq/base_freq necessary
    for frequency invariant accounting.
    
    It is to be noted that even if MSR_PLATFORM_INFO contained the appropriate
    data, APERF and MPERF are constantly zero on ESXi 5.5, thus freq-invariance
    couldn't be done in principle (not that it would make a lot of sense in a
    VM anyway). The real problem is advertising X86_FEATURE_APERFMPERF. This
    appears to be fixed in more recent versions: ESXi 6.7 doesn't advertise
    that feature.
    
    Fixes: 1567c3e3467c ("x86, sched: Add support for frequency invariance")
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lkml.kernel.org/r/20200416054745.740-2-ggherdovich@suse.cz

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fe3ab9632f3b..3a318ec9bc17 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1985,6 +1985,15 @@ static bool intel_set_max_freq_ratio(void)
 	return false;
 
 out:
+	/*
+	 * Some hypervisors advertise X86_FEATURE_APERFMPERF
+	 * but then fill all MSR's with zeroes.
+	 */
+	if (!base_freq) {
+		pr_debug("Couldn't determine cpu base frequency, necessary for scale-invariant accounting.\n");
+		return false;
+	}
+
 	arch_turbo_freq_ratio = div_u64(turbo_freq * SCHED_CAPACITY_SCALE,
 					base_freq);
 	arch_set_max_freq_ratio(turbo_disabled());

commit 2fa9a3cf3055db07a4835eb7bd48c648cb17ac26
Author: Borislav Petkov <bp@alien8.de>
Date:   Tue Mar 24 19:58:36 2020 +0100

    x86/smpboot: Remove the last ICPU() macro
    
    Now all is using the shiny new macros.
    
    No code changed:
    
      # arch/x86/kernel/smpboot.o:
    
       text    data     bss     dec     hex filename
      16432    2649      40   19121    4ab1 smpboot.o.before
      16432    2649      40   19121    4ab1 smpboot.o.after
    
    md5:
       a58104003b72c1de533095bc5a4c30a9  smpboot.o.before.asm
       a58104003b72c1de533095bc5a4c30a9  smpboot.o.after.asm
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200324185836.GI22931@zn.tnic

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fe3ab9632f3b..3b9bf8c7e29d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1849,24 +1849,25 @@ static bool slv_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq)
 #include <asm/cpu_device_id.h>
 #include <asm/intel-family.h>
 
-#define ICPU(model) \
-	{X86_VENDOR_INTEL, 6, model, X86_FEATURE_APERFMPERF, 0}
+#define X86_MATCH(model)					\
+	X86_MATCH_VENDOR_FAM_MODEL_FEATURE(INTEL, 6,		\
+		INTEL_FAM6_##model, X86_FEATURE_APERFMPERF, NULL)
 
 static const struct x86_cpu_id has_knl_turbo_ratio_limits[] = {
-	ICPU(INTEL_FAM6_XEON_PHI_KNL),
-	ICPU(INTEL_FAM6_XEON_PHI_KNM),
+	X86_MATCH(XEON_PHI_KNL),
+	X86_MATCH(XEON_PHI_KNM),
 	{}
 };
 
 static const struct x86_cpu_id has_skx_turbo_ratio_limits[] = {
-	ICPU(INTEL_FAM6_SKYLAKE_X),
+	X86_MATCH(SKYLAKE_X),
 	{}
 };
 
 static const struct x86_cpu_id has_glm_turbo_ratio_limits[] = {
-	ICPU(INTEL_FAM6_ATOM_GOLDMONT),
-	ICPU(INTEL_FAM6_ATOM_GOLDMONT_D),
-	ICPU(INTEL_FAM6_ATOM_GOLDMONT_PLUS),
+	X86_MATCH(ATOM_GOLDMONT),
+	X86_MATCH(ATOM_GOLDMONT_D),
+	X86_MATCH(ATOM_GOLDMONT_PLUS),
 	{}
 };
 

commit fdf5563a720004199324371c08071b8ea27bd994
Merge: 97cddfc34549 a2150327250e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 31 11:04:05 2020 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "This topic tree contains more commits than usual:
    
       - most of it are uaccess cleanups/reorganization by Al
    
       - there's a bunch of prototype declaration (--Wmissing-prototypes)
         cleanups
    
       - misc other cleanups all around the map"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (36 commits)
      x86/mm/set_memory: Fix -Wmissing-prototypes warnings
      x86/efi: Add a prototype for efi_arch_mem_reserve()
      x86/mm: Mark setup_emu2phys_nid() static
      x86/jump_label: Move 'inline' keyword placement
      x86/platform/uv: Add a missing prototype for uv_bau_message_interrupt()
      kill uaccess_try()
      x86: unsafe_put-style macro for sigmask
      x86: x32_setup_rt_frame(): consolidate uaccess areas
      x86: __setup_rt_frame(): consolidate uaccess areas
      x86: __setup_frame(): consolidate uaccess areas
      x86: setup_sigcontext(): list user_access_{begin,end}() into callers
      x86: get rid of put_user_try in __setup_rt_frame() (both 32bit and 64bit)
      x86: ia32_setup_rt_frame(): consolidate uaccess areas
      x86: ia32_setup_frame(): consolidate uaccess areas
      x86: ia32_setup_sigcontext(): lift user_access_{begin,end}() into the callers
      x86/alternatives: Mark text_poke_loc_init() static
      x86/cpu: Fix a -Wmissing-prototypes warning for init_ia32_feat_ctl()
      x86/mm: Drop pud_mknotpresent()
      x86: Replace setup_irq() by request_irq()
      x86/configs: Slightly reduce defconfigs
      ...

commit 642e53ead6aea8740a219ede509a5d138fd4f780
Merge: 9b82f05f869a 313f16e2e35a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 17:01:51 2020 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle are:
    
       - Various NUMA scheduling updates: harmonize the load-balancer and
         NUMA placement logic to not work against each other. The intended
         result is better locality, better utilization and fewer migrations.
    
       - Introduce Thermal Pressure tracking and optimizations, to improve
         task placement on thermally overloaded systems.
    
       - Implement frequency invariant scheduler accounting on (some) x86
         CPUs. This is done by observing and sampling the 'recent' CPU
         frequency average at ~tick boundaries. The CPU provides this data
         via the APERF/MPERF MSRs. This hopefully makes our capacity
         estimates more precise and keeps tasks on the same CPU better even
         if it might seem overloaded at a lower momentary frequency. (As
         usual, turbo mode is a complication that we resolve by observing
         the maximum frequency and renormalizing to it.)
    
       - Add asymmetric CPU capacity wakeup scan to improve capacity
         utilization on asymmetric topologies. (big.LITTLE systems)
    
       - PSI fixes and optimizations.
    
       - RT scheduling capacity awareness fixes & improvements.
    
       - Optimize the CONFIG_RT_GROUP_SCHED constraints code.
    
       - Misc fixes, cleanups and optimizations - see the changelog for
         details"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (62 commits)
      threads: Update PID limit comment according to futex UAPI change
      sched/fair: Fix condition of avg_load calculation
      sched/rt: cpupri_find: Trigger a full search as fallback
      kthread: Do not preempt current task if it is going to call schedule()
      sched/fair: Improve spreading of utilization
      sched: Avoid scale real weight down to zero
      psi: Move PF_MEMSTALL out of task->flags
      MAINTAINERS: Add maintenance information for psi
      psi: Optimize switching tasks inside shared cgroups
      psi: Fix cpu.pressure for cpu.max and competing cgroups
      sched/core: Distribute tasks within affinity masks
      sched/fair: Fix enqueue_task_fair warning
      thermal/cpu-cooling, sched/core: Move the arch_set_thermal_pressure() API to generic scheduler code
      sched/rt: Remove unnecessary push for unfit tasks
      sched/rt: Allow pulling unfitting task
      sched/rt: Optimize cpupri_find() on non-heterogenous systems
      sched/rt: Re-instate old behavior in select_task_rq_rt()
      sched/rt: cpupri_find: Implement fallback mechanism for !fit case
      sched/fair: Fix reordering of enqueue/dequeue_task_fair()
      sched/fair: Fix runnable_avg for throttled cfs
      ...

commit adefe55e725821e8ae23207992ded5994f1650a9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 20 14:13:51 2020 +0100

    x86/kernel: Convert to new CPU match macros
    
    The new macro set has a consistent namespace and uses C99 initializers
    instead of the grufty C89 ones.
    
    Get rid the of the local macro wrappers for consistency.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Link: https://lkml.kernel.org/r/20200320131509.250559388@linutronix.de

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 69881b2d446c..3076ef0864dd 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -466,7 +466,7 @@ static bool match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
  */
 
 static const struct x86_cpu_id snc_cpu[] = {
-	{ X86_VENDOR_INTEL, 6, INTEL_FAM6_SKYLAKE_X },
+	X86_MATCH_INTEL_FAM6_MODEL(SKYLAKE_X, NULL),
 	{}
 };
 

commit 4d1d0977a2156a1dafe8f1cd890ab918c803485b
Author: Martin Molnar <martin.molnar.programming@gmail.com>
Date:   Sun Feb 16 16:17:39 2020 +0100

    x86: Fix a handful of typos
    
    Fix a couple of typos in code comments.
    
     [ bp: While at it: s/IRQ's/IRQs/. ]
    
    Signed-off-by: Martin Molnar <martin.molnar.programming@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Randy Dunlap <rdunlap@infradead.org>
    Link: https://lkml.kernel.org/r/0819a044-c360-44a4-f0b6-3f5bafe2d35c@gmail.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 69881b2d446c..3feaeee8a926 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1434,7 +1434,7 @@ early_param("possible_cpus", _setup_possible_cpus);
 /*
  * cpu_possible_mask should be static, it cannot change as cpu's
  * are onlined, or offlined. The reason is per-cpu data-structures
- * are allocated by some modules at init time, and dont expect to
+ * are allocated by some modules at init time, and don't expect to
  * do this dynamically on cpu arrival/departure.
  * cpu_present_mask on the other hand can change dynamically.
  * In case when cpu_hotplug is not compiled, then we resort to current

commit 918229cdd5abb50d8a2edfcd8dc6b6bc53afd765
Author: Giovanni Gherdovich <ggherdovich@suse.cz>
Date:   Wed Jan 22 16:16:17 2020 +0100

    x86/intel_pstate: Handle runtime turbo disablement/enablement in frequency invariance
    
    On some platforms such as the Dell XPS 13 laptop the firmware disables turbo
    when the machine is disconnected from AC, and viceversa it enables it again
    when it's reconnected. In these cases a _PPC ACPI notification is issued.
    
    The scheduler needs to know freq_max for frequency-invariant calculations.
    To account for turbo availability to come and go, record freq_max at boot as
    if turbo was available and store it in a helper variable. Use a setter
    function to swap between freq_base and freq_max every time turbo goes off or on.
    
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lkml.kernel.org/r/20200122151617.531-7-ggherdovich@suse.cz

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 5f04bf8419f9..467191e51196 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1807,8 +1807,15 @@ DEFINE_STATIC_KEY_FALSE(arch_scale_freq_key);
 
 static DEFINE_PER_CPU(u64, arch_prev_aperf);
 static DEFINE_PER_CPU(u64, arch_prev_mperf);
+static u64 arch_turbo_freq_ratio = SCHED_CAPACITY_SCALE;
 static u64 arch_max_freq_ratio = SCHED_CAPACITY_SCALE;
 
+void arch_set_max_freq_ratio(bool turbo_disabled)
+{
+	arch_max_freq_ratio = turbo_disabled ? SCHED_CAPACITY_SCALE :
+					arch_turbo_freq_ratio;
+}
+
 static bool turbo_disabled(void)
 {
 	u64 misc_en;
@@ -1956,10 +1963,7 @@ static bool core_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq)
 
 static bool intel_set_max_freq_ratio(void)
 {
-	u64 base_freq = 1, turbo_freq = 1;
-
-	if (turbo_disabled())
-		goto out;
+	u64 base_freq, turbo_freq;
 
 	if (slv_set_max_freq_ratio(&base_freq, &turbo_freq))
 		goto out;
@@ -1981,8 +1985,9 @@ static bool intel_set_max_freq_ratio(void)
 	return false;
 
 out:
-	arch_max_freq_ratio = div_u64(turbo_freq * SCHED_CAPACITY_SCALE,
+	arch_turbo_freq_ratio = div_u64(turbo_freq * SCHED_CAPACITY_SCALE,
 					base_freq);
+	arch_set_max_freq_ratio(turbo_disabled());
 	return true;
 }
 

commit 298c6f99bf30ef735e79f7f6d086bdfae505d380
Author: Giovanni Gherdovich <ggherdovich@suse.cz>
Date:   Wed Jan 22 16:16:16 2020 +0100

    x86, sched: Add support for frequency invariance on ATOM
    
    The scheduler needs the ratio freq_curr/freq_max for frequency-invariant
    accounting. On all ATOM CPUs prior to Goldmont, set freq_max to the 1-core
    turbo ratio.
    
    We intended to perform tests validating that this patch doesn't regress in
    terms of energy efficiency, given that this is the primary concern on Atom
    processors. Alas, we found out that turbostat doesn't support reading RAPL
    interfaces on our test machine (Airmont), and we don't have external equipment
    to measure power consumption; all we have is the performance results of the
    benchmarks we ran.
    
    Test machine:
    
    Platform    : Dell Wyse 3040 Thin Client[1]
    CPU Model   : Intel Atom x5-Z8350 (aka Cherry Trail, aka Airmont)
    Fam/Mod/Ste : 6:76:4
    Topology    : 1 socket, 4 cores / 4 threads
    Memory      : 2G
    Storage     : onboard flash, XFS filesystem
    
    [1] https://www.dell.com/en-us/work/shop/wyse-endpoints-and-software/wyse-3040-thin-client/spd/wyse-3040-thin-client
    
    Base frequency and available turbo levels (MHz):
    
        Min Operating Freq   266 |***
        Low Freq Mode        800 |********
        Base Freq           2400 |************************
        4 Cores             2800 |****************************
        3 Cores             2800 |****************************
        2 Cores             3200 |********************************
        1 Core              3200 |********************************
    
    Tested kernels:
    
    Baseline      : v5.4-rc1,              intel_pstate passive,  schedutil
    Comparison #1 : v5.4-rc1,              intel_pstate active ,  powersave
    Comparison #2 : v5.4-rc1, this patch,  intel_pstate passive,  schedutil
    
    tbench, hackbench and kernbench performed the same under all three kernels;
    dbench ran faster with intel_pstate/powersave and the git unit tests were a
    lot faster with intel_pstate/powersave and invariant schedutil wrt the
    baseline. Not that any of this is terrbily interesting anyway, one doesn't buy
    an Atom system to go fast. Power consumption regressions aren't expected but
    we lack the equipment to make that measurement. Turbostat seems to think that
    reading RAPL on this machine isn't a good idea and we're trusting that
    decision.
    
    comparison ratio of performance with baseline; 1.00 means neutral,
    lower is better:
    
                          I_PSTATE      FREQ-INV
        ----------------------------------------
        dbench                0.90             ~
        kernbench             0.98          0.97
        gitsource             0.63          0.43
    
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lkml.kernel.org/r/20200122151617.531-6-ggherdovich@suse.cz

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3e32d620f1fb..5f04bf8419f9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1821,6 +1821,24 @@ static bool turbo_disabled(void)
 	return (misc_en & MSR_IA32_MISC_ENABLE_TURBO_DISABLE);
 }
 
+static bool slv_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq)
+{
+	int err;
+
+	err = rdmsrl_safe(MSR_ATOM_CORE_RATIOS, base_freq);
+	if (err)
+		return false;
+
+	err = rdmsrl_safe(MSR_ATOM_CORE_TURBO_RATIOS, turbo_freq);
+	if (err)
+		return false;
+
+	*base_freq = (*base_freq >> 16) & 0x3F;     /* max P state */
+	*turbo_freq = *turbo_freq & 0x3F;           /* 1C turbo    */
+
+	return true;
+}
+
 #include <asm/cpu_device_id.h>
 #include <asm/intel-family.h>
 
@@ -1938,17 +1956,14 @@ static bool core_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq)
 
 static bool intel_set_max_freq_ratio(void)
 {
-	/*
-	 * TODO: add support for:
-	 *
-	 * - Atom Silvermont
-	 */
-
 	u64 base_freq = 1, turbo_freq = 1;
 
 	if (turbo_disabled())
 		goto out;
 
+	if (slv_set_max_freq_ratio(&base_freq, &turbo_freq))
+		goto out;
+
 	if (x86_match_cpu(has_glm_turbo_ratio_limits) &&
 	    skx_set_max_freq_ratio(&base_freq, &turbo_freq, 1))
 		goto out;

commit eacf0474aec8bdccdc7f19386319127c67be3588
Author: Giovanni Gherdovich <ggherdovich@suse.cz>
Date:   Wed Jan 22 16:16:15 2020 +0100

    x86, sched: Add support for frequency invariance on ATOM_GOLDMONT*
    
    The scheduler needs the ratio freq_curr/freq_max for frequency-invariant
    accounting. On GOLDMONT (aka Apollo Lake), GOLDMONT_D (aka Denverton) and
    GOLDMONT_PLUS CPUs (aka Gemini Lake) set freq_max to the highest frequency
    reported by the CPU.
    
    The encoding of turbo ratios for GOLDMONT* is identical to the one for
    SKYLAKE_X, but we treat the Atom case apart because we want to set freq_max to
    a higher value, thus the ratio freq_curr/freq_max to be lower, leading to more
    conservative frequency selections (favoring power efficiency).
    
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lkml.kernel.org/r/20200122151617.531-5-ggherdovich@suse.cz

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8cb3113377a9..3e32d620f1fb 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1795,6 +1795,10 @@ void native_play_dead(void)
  * which would ignore the entire turbo range (a conspicuous part, making
  * freq_curr/freq_max always maxed out).
  *
+ * An exception to the heuristic above is the Atom uarch, where we choose the
+ * highest turbo level for freq_max since Atom's are generally oriented towards
+ * power efficiency.
+ *
  * Setting freq_max to anything less than the 1C turbo ratio makes the ratio
  * freq_curr / freq_max to eventually grow >1, in which case we clip it to 1.
  */
@@ -1937,18 +1941,18 @@ static bool intel_set_max_freq_ratio(void)
 	/*
 	 * TODO: add support for:
 	 *
-	 * - Atom Goldmont
 	 * - Atom Silvermont
 	 */
 
 	u64 base_freq = 1, turbo_freq = 1;
 
-	if (x86_match_cpu(has_glm_turbo_ratio_limits))
-		return false;
-
 	if (turbo_disabled())
 		goto out;
 
+	if (x86_match_cpu(has_glm_turbo_ratio_limits) &&
+	    skx_set_max_freq_ratio(&base_freq, &turbo_freq, 1))
+		goto out;
+
 	if (knl_set_max_freq_ratio(&base_freq, &turbo_freq, 1))
 		goto out;
 

commit 8bea0dfb4a820ae063568a87cc2e7d8f587377af
Author: Giovanni Gherdovich <ggherdovich@suse.cz>
Date:   Wed Jan 22 16:16:14 2020 +0100

    x86, sched: Add support for frequency invariance on XEON_PHI_KNL/KNM
    
    The scheduler needs the ratio freq_curr/freq_max for frequency-invariant
    accounting. On Xeon Phi CPUs set freq_max to the second-highest frequency
    reported by the CPU.
    
    Xeon Phi CPUs such as Knights Landing and Knights Mill typically have either
    one or two turbo frequencies; in the former case that's 100 MHz above the base
    frequency, in the latter case the two levels are 100 MHz and 200 MHz above
    base frequency.
    
    We set freq_max to the second-highest frequency reported by the CPU. This
    could be the base frequency (if only one turbo level is available) or the first
    turbo level (if two levels are available). The rationale is to compromise
    between power efficiency or performance -- going straight to max turbo would
    favor efficiency and blindly using base freq would favor performance.
    
    For reference, this is how MSR_TURBO_RATIO_LIMIT must be parsed on a Xeon Phi
    to get the available frequencies (taken from a comment in turbostat's sources):
    
        [0] -- Reserved
        [7:1] -- Base value of number of active cores of bucket 1.
        [15:8] -- Base value of freq ratio of bucket 1.
        [20:16] -- +ve delta of number of active cores of bucket 2.
        i.e. active cores of bucket 2 =
        active cores of bucket 1 + delta
        [23:21] -- Negative delta of freq ratio of bucket 2.
        i.e. freq ratio of bucket 2 =
        freq ratio of bucket 1 - delta
        [28:24]-- +ve delta of number of active cores of bucket 3.
        [31:29]-- -ve delta of freq ratio of bucket 3.
        [36:32]-- +ve delta of number of active cores of bucket 4.
        [39:37]-- -ve delta of freq ratio of bucket 4.
        [44:40]-- +ve delta of number of active cores of bucket 5.
        [47:45]-- -ve delta of freq ratio of bucket 5.
        [52:48]-- +ve delta of number of active cores of bucket 6.
        [55:53]-- -ve delta of freq ratio of bucket 6.
        [60:56]-- +ve delta of number of active cores of bucket 7.
        [63:61]-- -ve delta of freq ratio of bucket 7.
    
    1. PERFORMANCE EVALUATION: TBENCH +5%
    2. NEUTRAL BENCHMARKS (ALL OTHERS)
    3. TEST SETUP
    
    1. PERFORMANCE EVALUATION: TBENCH +5%
    -------------------------------------
    
    A performance evaluation was conducted on a Knights Mill machine (see "Test
    Setup" below), were the frequency-invariance patch (on schedutil) is compared
    to both non-invariant schedutil and active intel_pstate with powersave: all
    three tested kernels behave the same performance-wise and with regard to power
    consumption (performance per watt). The only notable difference is tbench:
    
    comparison ratio of performance with baseline; 1.00 means neutral,
    higher is better:
    
                          I_PSTATE      FREQ-INV
        ----------------------------------------
        tbench                1.04          1.05
    
    performance-per-watt ratios with baseline; 1.00 means neutral, higher is better:
    
                          I_PSTATE      FREQ-INV
        ----------------------------------------
        tbench                1.03          1.04
    
    which essentially means that frequency-invariant schedutil is 5% better than
    baseline, the same as intel_pstate+powersave.
    
    As the results above are averaged over the varying parameter, here the detailed
    table.
    
    Varying parameter  : number of clients
    Unit               : MB/sec (higher is better)
    
                        5.2.0 vanilla (BASELINE)                 5.2.0 intel_pstate                     5.2.0 freq-inv
    - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Hmean   1         49.06  +- 2.12% (        )         51.66  +- 1.52% (   5.30%)         52.87  +- 0.88% (   7.76%)
    Hmean   2         93.82  +- 0.45% (        )        103.24  +- 0.70% (  10.05%)        105.90  +- 0.70% (  12.88%)
    Hmean   4        192.46  +- 1.15% (        )        215.95  +- 0.60% (  12.21%)        215.78  +- 1.43% (  12.12%)
    Hmean   8        406.74  +- 2.58% (        )        438.58  +- 0.36% (   7.83%)        437.61  +- 0.97% (   7.59%)
    Hmean   16       857.70  +- 1.22% (        )        890.26  +- 0.72% (   3.80%)        889.11  +- 0.73% (   3.66%)
    Hmean   32      1760.10  +- 0.92% (        )       1791.70  +- 0.44% (   1.79%)       1787.95  +- 0.44% (   1.58%)
    Hmean   64      3183.50  +- 0.34% (        )       3183.19  +- 0.36% (  -0.01%)       3187.53  +- 0.36% (   0.13%)
    Hmean   128     4830.96  +- 0.31% (        )       4846.53  +- 0.30% (   0.32%)       4855.86  +- 0.30% (   0.52%)
    Hmean   256     5467.98  +- 0.38% (        )       5793.80  +- 0.28% (   5.96%)       5821.94  +- 0.17% (   6.47%)
    Hmean   512     5398.10  +- 0.06% (        )       5745.56  +- 0.08% (   6.44%)       5503.68  +- 0.07% (   1.96%)
    Hmean   1024    5290.43  +- 0.63% (        )       5221.07  +- 0.47% (  -1.31%)       5277.22  +- 0.80% (  -0.25%)
    Hmean   1088    5139.71  +- 0.57% (        )       5236.02  +- 0.71% (   1.87%)       5190.57  +- 0.41% (   0.99%)
    
    2. NEUTRAL BENCHMARKS (ALL OTHERS)
    ----------------------------------
    
    * pgbench (both read/write and read-only)
    * NASA Parallel Benchmarks (NPB), MPI or OpenMP for message-passing
    * hackbench
    * netperf
    * dbench
    * kernbench
    * gitsource (git unit test suite)
    
    3. TEST SETUP
    -------------
    
    Test machine:
    
    CPU Model   : Intel Xeon Phi CPU 7255 @ 1.10GHz (a.k.a. Knights Mill)
    Fam/Mod/Ste : 6:133:0
    Topology    : 1 socket, 68 cores / 272 threads
    Memory      : 96G
    Storage     : rotary, XFS filesystem
    
    Max EFFICiency, BASE frequency and available turbo levels (MHz):
    
        EFFIC   1000 |**********
        BASE    1100 |***********
        68C     1100 |***********
        30C     1200 |************
    
    Tested kernels:
    
    Baseline      : v5.2,              intel_pstate passive,  schedutil
    Comparison #1 : v5.2,              intel_pstate active ,  powersave
    Comparison #2 : v5.2, this patch,  intel_pstate passive,  schedutil
    
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lkml.kernel.org/r/20200122151617.531-4-ggherdovich@suse.cz

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ba9d3bdc191c..8cb3113377a9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1841,6 +1841,48 @@ static const struct x86_cpu_id has_glm_turbo_ratio_limits[] = {
 	{}
 };
 
+static bool knl_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq,
+				int num_delta_fratio)
+{
+	int fratio, delta_fratio, found;
+	int err, i;
+	u64 msr;
+
+	if (!x86_match_cpu(has_knl_turbo_ratio_limits))
+		return false;
+
+	err = rdmsrl_safe(MSR_PLATFORM_INFO, base_freq);
+	if (err)
+		return false;
+
+	*base_freq = (*base_freq >> 8) & 0xFF;	    /* max P state */
+
+	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, &msr);
+	if (err)
+		return false;
+
+	fratio = (msr >> 8) & 0xFF;
+	i = 16;
+	found = 0;
+	do {
+		if (found >= num_delta_fratio) {
+			*turbo_freq = fratio;
+			return true;
+		}
+
+		delta_fratio = (msr >> (i + 5)) & 0x7;
+
+		if (delta_fratio) {
+			found += 1;
+			fratio -= delta_fratio;
+		}
+
+		i += 8;
+	} while (i < 64);
+
+	return true;
+}
+
 static bool skx_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq, int size)
 {
 	u64 ratios, counts;
@@ -1895,20 +1937,21 @@ static bool intel_set_max_freq_ratio(void)
 	/*
 	 * TODO: add support for:
 	 *
-	 * - Xeon Phi (KNM, KNL)
 	 * - Atom Goldmont
 	 * - Atom Silvermont
 	 */
 
 	u64 base_freq = 1, turbo_freq = 1;
 
-	if (x86_match_cpu(has_knl_turbo_ratio_limits) ||
-		x86_match_cpu(has_glm_turbo_ratio_limits))
+	if (x86_match_cpu(has_glm_turbo_ratio_limits))
 		return false;
 
 	if (turbo_disabled())
 		goto out;
 
+	if (knl_set_max_freq_ratio(&base_freq, &turbo_freq, 1))
+		goto out;
+
 	if (x86_match_cpu(has_skx_turbo_ratio_limits) &&
 	    skx_set_max_freq_ratio(&base_freq, &turbo_freq, 4))
 		goto out;

commit 2a0abc59699896f03bf6f16efb8a3a490511216f
Author: Giovanni Gherdovich <ggherdovich@suse.cz>
Date:   Wed Jan 22 16:16:13 2020 +0100

    x86, sched: Add support for frequency invariance on SKYLAKE_X
    
    The scheduler needs the ratio freq_curr/freq_max for frequency-invariant
    accounting. On SKYLAKE_X CPUs set freq_max to the highest frequency that can
    be sustained by a group of at least 4 cores.
    
    From the changelog of commit 31e07522be56 ("tools/power turbostat: fix
    decoding for GLM, DNV, SKX turbo-ratio limits"):
    
     >   Newer processors do not hard-code the the number of cpus in each bin
     >   to {1, 2, 3, 4, 5, 6, 7, 8}  Rather, they can specify any number
     >   of CPUS in each of the 8 bins:
     >
     >   eg.
     >
     >   ...
     >   37 * 100.0 = 3600.0 MHz max turbo 4 active cores
     >   38 * 100.0 = 3700.0 MHz max turbo 3 active cores
     >   39 * 100.0 = 3800.0 MHz max turbo 2 active cores
     >   39 * 100.0 = 3900.0 MHz max turbo 1 active cores
     >
     >   could now look something like this:
     >
     >   ...
     >   37 * 100.0 = 3600.0 MHz max turbo 16 active cores
     >   38 * 100.0 = 3700.0 MHz max turbo 8 active cores
     >   39 * 100.0 = 3800.0 MHz max turbo 4 active cores
     >   39 * 100.0 = 3900.0 MHz max turbo 2 active cores
    
    This encoding of turbo levels applies to both SKYLAKE_X and GOLDMONT/GOLDMONT_D,
    but we treat these two classes in separate commits because their freq_max
    values need to be different. For SKX we prefer a lower freq_max in the ratio
    freq_curr/freq_max, allowing load and utilization to overshoot and the
    schedutil governor to be more performance-oriented. Models from the Atom
    series (such as GOLDMONT*) are handled in a forthcoming commit as they have to
    favor power-efficiency over performance.
    
    Results from a performance evaluation follow.
    
    1. TEST SETUP
    2. NEUTRAL BENCHMARKS
    3. NON-NEUTRAL BENCHMARKS
    4. DETAILED TABLES
    
    1. TEST SETUP
    -------------
    
    Test machine:
    
    CPU Model   : Intel Xeon Platinum 8260L CPU @ 2.40GHz (a.k.a. Cascade Lake)
    Fam/Mod/Ste : 6:85:6
    Topology    : 2 sockets, 24 cores / 48 threads each socket
    Memory      : 192G
    Storage     : SSD, XFS filesystem
    
    Max EFFICiency, BASE frequency and available turbo levels (MHz):
    
        EFFIC   1000 |**********
        BASE    2400 |************************
        24C     3100 |*******************************
        20C     3300 |*********************************
        16C     3600 |************************************
        12C     3600 |************************************
        8C      3600 |************************************
        4C      3700 |*************************************
        2C      3900 |***************************************
    
    Tested kernels:
    
    Baseline      : v5.2,              intel_pstate passive,  schedutil
    Comparison #1 : v5.2,              intel_pstate active ,  powersave+HWP
    Comparison #2 : v5.2, this patch,  intel_pstate passive,  schedutil
    
    2. NEUTRAL BENCHMARKS
    ---------------------
    
    * pgbench read/write
    * NASA Parallel Benchmarks (NPB), MPI or OpenMP for message-passing
    * hackbench
    * netperf
    
    3. NON-NEUTRAL BENCHMARKS
    -------------------------
    
    comparison ratio with baseline; 1.00 means neutral, higher is better:
    
                          I_PSTATE      FREQ-INV
        ----------------------------------------
        pgbench read-only     1.10             ~
        tbench                1.82          1.14
    
    comparison ratio with baseline; 1.00 means neutral, lower is better:
    
                          I_PSTATE      FREQ-INV
        ----------------------------------------
        dbench                   ~          0.97
        kernbench             0.88          0.78
        gitsource[*]             ~          0.46
    
    [*] "gitsource" consists in running git's unit tests
    tilde (~) means 1.00, ie result identical to baseline
    
    Performance per watt:
    
    performance-per-watt ratios with baseline; 1.00 means neutral, higher is better:
    
                          I_PSTATE      FREQ-INV
        ----------------------------------------
        dbench                0.92          0.91
        tbench                1.26          1.04
        kernbench             0.95          0.96
        gitsource             1.03          1.30
    
    Similarly to earlier Xeons, measurable performance gains over non-invariant
    schedutil are observed on dbench, tbench, kernel compilation and running the
    git unit tests suite. Looking at the detailed tables show that the patch
    scores the largest difference when the machine is lightly loaded. Power
    efficiency suffers lightly on kernbench and a bit more on dbench, but largely
    improves on gitsource (which also runs considerably faster). For reference, we
    also report results using active intel_pstate with powersave and HWP; the
    largest gap between non-invariant schedutil and intel_pstate+powersave is
    still tbench, which runs 82% better and with 26% improved efficiency on the
    latter configuration -- this divide isn't closed yet by frequency-invariant
    schedutil.
    
    4. DETAILED TABLES
    ------------------
    
    Benchmark          : tbench4 (i.e. dbench4 over the network, actually loopback)
    Varying parameter  : number of clients
    Unit               : MB/sec (higher is better)
    
                         5.2.0 vanilla (BASELINE)            5.2.0 intel_pstate/HWP                    5.2.0 freq-inv
    - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Hmean   1         183.56  +- 0.21% (        )       516.12  +- 0.57% ( 181.18%)       185.59  +- 0.59% (   1.11%)
    Hmean   2         365.75  +- 0.25% (        )      1015.14  +- 0.33% ( 177.55%)       402.59  +- 4.48% (  10.07%)
    Hmean   4         720.99  +- 0.44% (        )      1951.75  +- 0.28% ( 170.70%)       738.39  +- 1.72% (   2.41%)
    Hmean   8        1449.93  +- 0.34% (        )      3830.56  +- 0.24% ( 164.19%)      1750.36  +- 4.65% (  20.72%)
    Hmean   16       2874.26  +- 0.57% (        )      7381.62  +- 0.53% ( 156.82%)      4348.35  +- 2.22% (  51.29%)
    Hmean   32       6116.17  +- 5.10% (        )     13013.05  +- 0.08% ( 112.76%)      8980.35  +- 0.66% (  46.83%)
    Hmean   64      14485.04  +- 3.46% (        )     17835.12  +- 0.35% (  23.13%)     16540.73  +- 0.51% (  14.19%)
    Hmean   128     30779.16  +- 3.20% (        )     32796.94  +- 2.13% (   6.56%)     31512.58  +- 0.20% (   2.38%)
    Hmean   256     34664.66  +- 0.81% (        )     34604.67  +- 0.46% (  -0.17%)     34943.70  +- 0.25% (   0.80%)
    Hmean   384     33957.51  +- 0.11% (        )     34091.50  +- 0.14% (   0.39%)     33921.41  +- 0.09% (  -0.11%)
    
    Benchmark          : kernbench (kernel compilation)
    Varying parameter  : number of jobs
    Unit               : seconds (lower is better)
    
                        5.2.0 vanilla (BASELINE)             5.2.0 intel_pstate/HWP                     5.2.0 freq-inv
    - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Amean   2        332.94  +- 0.40% (        )        260.16  +- 0.45% (  21.86%)        233.56  +- 0.21% (  29.85%)
    Amean   4        173.04  +- 0.43% (        )        138.76  +- 0.03% (  19.81%)        123.59  +- 0.11% (  28.58%)
    Amean   8         89.65  +- 0.20% (        )         73.54  +- 0.09% (  17.97%)         65.69  +- 0.10% (  26.72%)
    Amean   16        48.08  +- 1.41% (        )         41.64  +- 1.61% (  13.40%)         36.00  +- 1.80% (  25.11%)
    Amean   32        28.78  +- 0.72% (        )         26.61  +- 1.99% (   7.55%)         23.19  +- 1.68% (  19.43%)
    Amean   64        20.46  +- 1.85% (        )         19.76  +- 0.35% (   3.42%)         17.38  +- 0.92% (  15.06%)
    Amean   128       18.69  +- 1.70% (        )         17.59  +- 1.04% (   5.90%)         15.73  +- 1.40% (  15.85%)
    Amean   192       18.82  +- 1.01% (        )         17.76  +- 0.77% (   5.67%)         15.57  +- 1.80% (  17.28%)
    
    Benchmark          : gitsource (time to run the git unit test suite)
    Varying parameter  : none
    Unit               : seconds (lower is better)
    
                     5.2.0 vanilla (BASELINE)           5.2.0 intel_pstate/HWP                    5.2.0 freq-inv
    - - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Amean         792.49  +- 0.20% (        )      779.35  +- 0.24% (   1.66%)      427.14  +- 0.16% (   46.10%)
    
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lkml.kernel.org/r/20200122151617.531-3-ggherdovich@suse.cz

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 28696bccf912..ba9d3bdc191c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1841,24 +1841,52 @@ static const struct x86_cpu_id has_glm_turbo_ratio_limits[] = {
 	{}
 };
 
-static bool core_set_max_freq_ratio(void)
+static bool skx_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq, int size)
+{
+	u64 ratios, counts;
+	u32 group_size;
+	int err, i;
+
+	err = rdmsrl_safe(MSR_PLATFORM_INFO, base_freq);
+	if (err)
+		return false;
+
+	*base_freq = (*base_freq >> 8) & 0xFF;      /* max P state */
+
+	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, &ratios);
+	if (err)
+		return false;
+
+	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT1, &counts);
+	if (err)
+		return false;
+
+	for (i = 0; i < 64; i += 8) {
+		group_size = (counts >> i) & 0xFF;
+		if (group_size >= size) {
+			*turbo_freq = (ratios >> i) & 0xFF;
+			return true;
+		}
+	}
+
+	return false;
+}
+
+static bool core_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq)
 {
-	u64 base_freq, turbo_freq;
 	int err;
 
-	err = rdmsrl_safe(MSR_PLATFORM_INFO, &base_freq);
+	err = rdmsrl_safe(MSR_PLATFORM_INFO, base_freq);
 	if (err)
 		return false;
 
-	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, &turbo_freq);
+	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, turbo_freq);
 	if (err)
 		return false;
 
-	base_freq = (base_freq >> 8) & 0xFF;      /* max P state */
-	turbo_freq = (turbo_freq >> 24) & 0xFF;   /* 4C turbo    */
+	*base_freq = (*base_freq >> 8) & 0xFF;      /* max P state */
+	*turbo_freq = (*turbo_freq >> 24) & 0xFF;   /* 4C turbo    */
 
-	arch_max_freq_ratio = div_u64(turbo_freq * SCHED_CAPACITY_SCALE,
-					base_freq);
 	return true;
 }
 
@@ -1867,21 +1895,33 @@ static bool intel_set_max_freq_ratio(void)
 	/*
 	 * TODO: add support for:
 	 *
-	 * - Xeon Gold/Platinum
 	 * - Xeon Phi (KNM, KNL)
 	 * - Atom Goldmont
 	 * - Atom Silvermont
 	 */
 
-	if (x86_match_cpu(has_skx_turbo_ratio_limits) ||
-		x86_match_cpu(has_knl_turbo_ratio_limits) ||
+	u64 base_freq = 1, turbo_freq = 1;
+
+	if (x86_match_cpu(has_knl_turbo_ratio_limits) ||
 		x86_match_cpu(has_glm_turbo_ratio_limits))
 		return false;
 
-	if (turbo_disabled() || core_set_max_freq_ratio())
-		return true;
+	if (turbo_disabled())
+		goto out;
+
+	if (x86_match_cpu(has_skx_turbo_ratio_limits) &&
+	    skx_set_max_freq_ratio(&base_freq, &turbo_freq, 4))
+		goto out;
+
+	if (core_set_max_freq_ratio(&base_freq, &turbo_freq))
+		goto out;
 
 	return false;
+
+out:
+	arch_max_freq_ratio = div_u64(turbo_freq * SCHED_CAPACITY_SCALE,
+					base_freq);
+	return true;
 }
 
 static void init_counter_refs(void *arg)

commit 1567c3e3467cddeb019a7b53ec632f834b6a9239
Author: Giovanni Gherdovich <ggherdovich@suse.cz>
Date:   Wed Jan 22 16:16:12 2020 +0100

    x86, sched: Add support for frequency invariance
    
    Implement arch_scale_freq_capacity() for 'modern' x86. This function
    is used by the scheduler to correctly account usage in the face of
    DVFS.
    
    The present patch addresses Intel processors specifically and has positive
    performance and performance-per-watt implications for the schedutil cpufreq
    governor, bringing it closer to, if not on-par with, the powersave governor
    from the intel_pstate driver/framework.
    
    Large performance gains are obtained when the machine is lightly loaded and
    no regression are observed at saturation. The benchmarks with the largest
    gains are kernel compilation, tbench (the networking version of dbench) and
    shell-intensive workloads.
    
    1. FREQUENCY INVARIANCE: MOTIVATION
       * Without it, a task looks larger if the CPU runs slower
    
    2. PECULIARITIES OF X86
       * freq invariance accounting requires knowing the ratio freq_curr/freq_max
       2.1 CURRENT FREQUENCY
           * Use delta_APERF / delta_MPERF * freq_base (a.k.a "BusyMHz")
       2.2 MAX FREQUENCY
           * It varies with time (turbo). As an approximation, we set it to a
             constant, i.e. 4-cores turbo frequency.
    
    3. EFFECTS ON THE SCHEDUTIL FREQUENCY GOVERNOR
       * The invariant schedutil's formula has no feedback loop and reacts faster
         to utilization changes
    
    4. KNOWN LIMITATIONS
       * In some cases tasks can't reach max util despite how hard they try
    
    5. PERFORMANCE TESTING
       5.1 MACHINES
           * Skylake, Broadwell, Haswell
       5.2 SETUP
           * baseline Linux v5.2 w/ non-invariant schedutil. Tested freq_max = 1-2-3-4-8-12
             active cores turbo w/ invariant schedutil, and intel_pstate/powersave
       5.3 BENCHMARK RESULTS
           5.3.1 NEUTRAL BENCHMARKS
                 * NAS Parallel Benchmark (HPC), hackbench
           5.3.2 NON-NEUTRAL BENCHMARKS
                 * tbench (10-30% better), kernbench (10-15% better),
                   shell-intensive-scripts (30-50% better)
                 * no regressions
           5.3.3 SELECTION OF DETAILED RESULTS
           5.3.4 POWER CONSUMPTION, PERFORMANCE-PER-WATT
                 * dbench (5% worse on one machine), kernbench (3% worse),
                   tbench (5-10% better), shell-intensive-scripts (10-40% better)
    
    6. MICROARCH'ES ADDRESSED HERE
       * Xeon Core before Scalable Performance processors line (Xeon Gold/Platinum
         etc have different MSRs semantic for querying turbo levels)
    
    7. REFERENCES
       * MMTests performance testing framework, github.com/gormanm/mmtests
    
     +-------------------------------------------------------------------------+
     | 1. FREQUENCY INVARIANCE: MOTIVATION
     +-------------------------------------------------------------------------+
    
    For example; suppose a CPU has two frequencies: 500 and 1000 Mhz. When
    running a task that would consume 1/3rd of a CPU at 1000 MHz, it would
    appear to consume 2/3rd (or 66.6%) when running at 500 MHz, giving the
    false impression this CPU is almost at capacity, even though it can go
    faster [*]. In a nutshell, without frequency scale-invariance tasks look
    larger just because the CPU is running slower.
    
    [*] (footnote: this assumes a linear frequency/performance relation; which
    everybody knows to be false, but given realities its the best approximation
    we can make.)
    
     +-------------------------------------------------------------------------+
     | 2. PECULIARITIES OF X86
     +-------------------------------------------------------------------------+
    
    Accounting for frequency changes in PELT signals requires the computation of
    the ratio freq_curr / freq_max. On x86 neither of those terms is readily
    available.
    
    2.1 CURRENT FREQUENCY
    ====================
    
    Since modern x86 has hardware control over the actual frequency we run
    at (because amongst other things, Turbo-Mode), we cannot simply use
    the frequency as requested through cpufreq.
    
    Instead we use the APERF/MPERF MSRs to compute the effective frequency
    over the recent past. Also, because reading MSRs is expensive, don't
    do so every time we need the value, but amortize the cost by doing it
    every tick.
    
    2.2 MAX FREQUENCY
    =================
    
    Obtaining freq_max is also non-trivial because at any time the hardware can
    provide a frequency boost to a selected subset of cores if the package has
    enough power to spare (eg: Turbo Boost). This means that the maximum frequency
    available to a given core changes with time.
    
    The approach taken in this change is to arbitrarily set freq_max to a constant
    value at boot. The value chosen is the "4-cores (4C) turbo frequency" on most
    microarchitectures, after evaluating the following candidates:
    
        * 1-core (1C) turbo frequency (the fastest turbo state available)
        * around base frequency (a.k.a. max P-state)
        * something in between, such as 4C turbo
    
    To interpret these options, consider that this is the denominator in
    freq_curr/freq_max, and that ratio will be used to scale PELT signals such as
    util_avg and load_avg. A large denominator will undershoot (util_avg looks a
    bit smaller than it really is), viceversa with a smaller denominator PELT
    signals will tend to overshoot. Given that PELT drives frequency selection
    in the schedutil governor, we will have:
    
        freq_max set to     | effect on DVFS
        --------------------+------------------
        1C turbo            | power efficiency (lower freq choices)
        base freq           | performance (higher util_avg, higher freq requests)
        4C turbo            | a bit of both
    
    4C turbo proves to be a good compromise in a number of benchmarks (see below).
    
     +-------------------------------------------------------------------------+
     | 3. EFFECTS ON THE SCHEDUTIL FREQUENCY GOVERNOR
     +-------------------------------------------------------------------------+
    
    Once an architecture implements a frequency scale-invariant utilization (the
    PELT signal util_avg), schedutil switches its frequency selection formula from
    
        freq_next = 1.25 * freq_curr * util            [non-invariant util signal]
    
    to
    
        freq_next = 1.25 * freq_max * util             [invariant util signal]
    
    where, in the second formula, freq_max is set to the 1C turbo frequency (max
    turbo). The advantage of the second formula, whose usage we unlock with this
    patch, is that freq_next doesn't depend on the current frequency in an
    iterative fashion, but can jump to any frequency in a single update. This
    absence of feedback in the formula makes it quicker to react to utilization
    changes and more robust against pathological instabilities.
    
    Compare it to the update formula of intel_pstate/powersave:
    
        freq_next = 1.25 * freq_max * Busy%
    
    where again freq_max is 1C turbo and Busy% is the percentage of time not spent
    idling (calculated with delta_MPERF / delta_TSC); essentially the same as
    invariant schedutil, and largely responsible for intel_pstate/powersave good
    reputation. The non-invariant schedutil formula is derived from the invariant
    one by approximating util_inv with util_raw * freq_curr / freq_max, but this
    has limitations.
    
    Testing shows improved performances due to better frequency selections when
    the machine is lightly loaded, and essentially no change in behaviour at
    saturation / overutilization.
    
     +-------------------------------------------------------------------------+
     | 4. KNOWN LIMITATIONS
     +-------------------------------------------------------------------------+
    
    It's been shown that it is possible to create pathological scenarios where a
    CPU-bound task cannot reach max utilization, if the normalizing factor
    freq_max is fixed to a constant value (see [Lelli-2018]).
    
    If freq_max is set to 4C turbo as we do here, one needs to peg at least 5
    cores in a package doing some busywork, and observe that none of those task
    will ever reach max util (1024) because they're all running at less than the
    4C turbo frequency.
    
    While this concern still applies, we believe the performance benefit of
    frequency scale-invariant PELT signals outweights the cost of this limitation.
    
     [Lelli-2018]
     https://lore.kernel.org/lkml/20180517150418.GF22493@localhost.localdomain/
    
     +-------------------------------------------------------------------------+
     | 5. PERFORMANCE TESTING
     +-------------------------------------------------------------------------+
    
    5.1 MACHINES
    ============
    
    We tested the patch on three machines, with Skylake, Broadwell and Haswell
    CPUs. The details are below, together with the available turbo ratios as
    reported by the appropriate MSRs.
    
    * 8x-SKYLAKE-UMA:
      Single socket E3-1240 v5, Skylake 4 cores/8 threads
      Max EFFiciency, BASE frequency and available turbo levels (MHz):
    
        EFFIC    800 |********
        BASE    3500 |***********************************
        4C      3700 |*************************************
        3C      3800 |**************************************
        2C      3900 |***************************************
        1C      3900 |***************************************
    
    * 80x-BROADWELL-NUMA:
      Two sockets E5-2698 v4, 2x Broadwell 20 cores/40 threads
      Max EFFiciency, BASE frequency and available turbo levels (MHz):
    
        EFFIC   1200 |************
        BASE    2200 |**********************
        8C      2900 |*****************************
        7C      3000 |******************************
        6C      3100 |*******************************
        5C      3200 |********************************
        4C      3300 |*********************************
        3C      3400 |**********************************
        2C      3600 |************************************
        1C      3600 |************************************
    
    * 48x-HASWELL-NUMA
      Two sockets E5-2670 v3, 2x Haswell 12 cores/24 threads
      Max EFFiciency, BASE frequency and available turbo levels (MHz):
    
        EFFIC   1200 |************
        BASE    2300 |***********************
        12C     2600 |**************************
        11C     2600 |**************************
        10C     2600 |**************************
        9C      2600 |**************************
        8C      2600 |**************************
        7C      2600 |**************************
        6C      2600 |**************************
        5C      2700 |***************************
        4C      2800 |****************************
        3C      2900 |*****************************
        2C      3100 |*******************************
        1C      3100 |*******************************
    
    5.2 SETUP
    =========
    
    * The baseline is Linux v5.2 with schedutil (non-invariant) and the intel_pstate
      driver in passive mode.
    * The rationale for choosing the various freq_max values to test have been to
      try all the 1-2-3-4C turbo levels (note that 1C and 2C turbo are identical
      on all machines), plus one more value closer to base_freq but still in the
      turbo range (8C turbo for both 80x-BROADWELL-NUMA and 48x-HASWELL-NUMA).
    * In addition we've run all tests with intel_pstate/powersave for comparison.
    * The filesystem is always XFS, the userspace is openSUSE Leap 15.1.
    * 8x-SKYLAKE-UMA is capable of HWP (Hardware-Managed P-States), so the runs
      with active intel_pstate on this machine use that.
    
    This gives, in terms of combinations tested on each machine:
    
    * 8x-SKYLAKE-UMA
      * Baseline: Linux v5.2, non-invariant schedutil, intel_pstate passive
      * intel_pstate active + powersave + HWP
      * invariant schedutil, freq_max = 1C turbo
      * invariant schedutil, freq_max = 3C turbo
      * invariant schedutil, freq_max = 4C turbo
    
    * both 80x-BROADWELL-NUMA and 48x-HASWELL-NUMA
      * [same as 8x-SKYLAKE-UMA, but no HWP capable]
      * invariant schedutil, freq_max = 8C turbo
        (which on 48x-HASWELL-NUMA is the same as 12C turbo, or "all cores turbo")
    
    5.3 BENCHMARK RESULTS
    =====================
    
    5.3.1 NEUTRAL BENCHMARKS
    ------------------------
    
    Tests that didn't show any measurable difference in performance on any of the
    test machines between non-invariant schedutil and our patch are:
    
    * NAS Parallel Benchmarks (NPB) using either MPI or openMP for IPC, any
      computational kernel
    * flexible I/O (FIO)
    * hackbench (using threads or processes, and using pipes or sockets)
    
    5.3.2 NON-NEUTRAL BENCHMARKS
    ----------------------------
    
    What follow are summary tables where each benchmark result is given a score.
    
    * A tilde (~) means a neutral result, i.e. no difference from baseline.
    * Scores are computed with the ratio result_new / result_baseline, so a tilde
      means a score of 1.00.
    * The results in the score ratio are the geometric means of results running
      the benchmark with different parameters (eg: for kernbench: using 1, 2, 4,
      ... number of processes; for pgbench: varying the number of clients, and so
      on).
    * The first three tables show higher-is-better kind of tests (i.e. measured in
      operations/second), the subsequent three show lower-is-better kind of tests
      (i.e. the workload is fixed and we measure elapsed time, think kernbench).
    * "gitsource" is a name we made up for the test consisting in running the
      entire unit tests suite of the Git SCM and measuring how long it takes. We
      take it as a typical example of shell-intensive serialized workload.
    * In the "I_PSTATE" column we have the results for intel_pstate/powersave. Other
      columns show invariant schedutil for different values of freq_max. 4C turbo
      is circled as it's the value we've chosen for the final implementation.
    
    80x-BROADWELL-NUMA (comparison ratio; higher is better)
                                             +------+
                     I_PSTATE   1C     3C    | 4C   |  8C
    pgbench-ro           1.14   ~      ~     | 1.11 |  1.14
    pgbench-rw           ~      ~      ~     | ~    |  ~
    netperf-udp          1.06   ~      1.06  | 1.05 |  1.07
    netperf-tcp          ~      1.03   ~     | 1.01 |  1.02
    tbench4              1.57   1.18   1.22  | 1.30 |  1.56
                                             +------+
    
    8x-SKYLAKE-UMA (comparison ratio; higher is better)
                                             +------+
                 I_PSTATE/HWP   1C     3C    | 4C   |
    pgbench-ro           ~      ~      ~     | ~    |
    pgbench-rw           ~      ~      ~     | ~    |
    netperf-udp          ~      ~      ~     | ~    |
    netperf-tcp          ~      ~      ~     | ~    |
    tbench4              1.30   1.14   1.14  | 1.16 |
                                             +------+
    
    48x-HASWELL-NUMA (comparison ratio; higher is better)
                                             +------+
                     I_PSTATE   1C     3C    | 4C   |  12C
    pgbench-ro           1.15   ~      ~     | 1.06 |  1.16
    pgbench-rw           ~      ~      ~     | ~    |  ~
    netperf-udp          1.05   0.97   1.04  | 1.04 |  1.02
    netperf-tcp          0.96   1.01   1.01  | 1.01 |  1.01
    tbench4              1.50   1.05   1.13  | 1.13 |  1.25
                                             +------+
    
    In the table above we see that active intel_pstate is slightly better than our
    4C-turbo patch (both in reference to the baseline non-invariant schedutil) on
    read-only pgbench and much better on tbench. Both cases are notable in which
    it shows that lowering our freq_max (to 8C-turbo and 12C-turbo on
    80x-BROADWELL-NUMA and 48x-HASWELL-NUMA respectively) helps invariant
    schedutil to get closer.
    
    If we ignore active intel_pstate and focus on the comparison with baseline
    alone, there are several instances of double-digit performance improvement.
    
    80x-BROADWELL-NUMA (comparison ratio; lower is better)
                                             +------+
                     I_PSTATE   1C     3C    | 4C   |  8C
    dbench4              1.23   0.95   0.95  | 0.95 |  0.95
    kernbench            0.93   0.83   0.83  | 0.83 |  0.82
    gitsource            0.98   0.49   0.49  | 0.49 |  0.48
                                             +------+
    
    8x-SKYLAKE-UMA (comparison ratio; lower is better)
                                             +------+
                 I_PSTATE/HWP   1C     3C    | 4C   |
    dbench4              ~      ~      ~     | ~    |
    kernbench            ~      ~      ~     | ~    |
    gitsource            0.92   0.55   0.55  | 0.55 |
                                             +------+
    
    48x-HASWELL-NUMA (comparison ratio; lower is better)
                                             +------+
                     I_PSTATE   1C     3C    | 4C   |  8C
    dbench4              ~      ~      ~     | ~    |  ~
    kernbench            0.94   0.90   0.89  | 0.90 |  0.90
    gitsource            0.97   0.69   0.69  | 0.69 |  0.69
                                             +------+
    
    dbench is not very remarkable here, unless we notice how poorly active
    intel_pstate is performing on 80x-BROADWELL-NUMA: 23% regression versus
    non-invariant schedutil. We repeated that run getting consistent results. Out
    of scope for the patch at hand, but deserving future investigation. Other than
    that, we previously ran this campaign with Linux v5.0 and saw the patch doing
    better on dbench a the time. We haven't checked closely and can only speculate
    at this point.
    
    On the NUMA boxes kernbench gets 10-15% improvements on average; we'll see in
    the detailed tables that the gains concentrate on low process counts (lightly
    loaded machines).
    
    The test we call "gitsource" (running the git unit test suite, a long-running
    single-threaded shell script) appears rather spectacular in this table (gains
    of 30-50% depending on the machine). It is to be noted, however, that
    gitsource has no adjustable parameters (such as the number of jobs in
    kernbench, which we average over in order to get a single-number summary
    score) and is exactly the kind of low-parallelism workload that benefits the
    most from this patch. When looking at the detailed tables of kernbench or
    tbench4, at low process or client counts one can see similar numbers.
    
    5.3.3 SELECTION OF DETAILED RESULTS
    -----------------------------------
    
    Machine            : 48x-HASWELL-NUMA
    Benchmark          : tbench4 (i.e. dbench4 over the network, actually loopback)
    Varying parameter  : number of clients
    Unit               : MB/sec (higher is better)
    
                       5.2.0 vanilla (BASELINE)               5.2.0 intel_pstate                   5.2.0 1C-turbo
    - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Hmean  1        126.73  +- 0.31% (        )      315.91  +- 0.66% ( 149.28%)      125.03  +- 0.76% (  -1.34%)
    Hmean  2        258.04  +- 0.62% (        )      614.16  +- 0.51% ( 138.01%)      269.58  +- 1.45% (   4.47%)
    Hmean  4        514.30  +- 0.67% (        )     1146.58  +- 0.54% ( 122.94%)      533.84  +- 1.99% (   3.80%)
    Hmean  8       1111.38  +- 2.52% (        )     2159.78  +- 0.38% (  94.33%)     1359.92  +- 1.56% (  22.36%)
    Hmean  16      2286.47  +- 1.36% (        )     3338.29  +- 0.21% (  46.00%)     2720.20  +- 0.52% (  18.97%)
    Hmean  32      4704.84  +- 0.35% (        )     4759.03  +- 0.43% (   1.15%)     4774.48  +- 0.30% (   1.48%)
    Hmean  64      7578.04  +- 0.27% (        )     7533.70  +- 0.43% (  -0.59%)     7462.17  +- 0.65% (  -1.53%)
    Hmean  128     6998.52  +- 0.16% (        )     6987.59  +- 0.12% (  -0.16%)     6909.17  +- 0.14% (  -1.28%)
    Hmean  192     6901.35  +- 0.25% (        )     6913.16  +- 0.10% (   0.17%)     6855.47  +- 0.21% (  -0.66%)
    
                                 5.2.0 3C-turbo                   5.2.0 4C-turbo                  5.2.0 12C-turbo
    - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Hmean  1        128.43  +- 0.28% (   1.34%)      130.64  +- 3.81% (   3.09%)      153.71  +- 5.89% (  21.30%)
    Hmean  2        311.70  +- 6.15% (  20.79%)      281.66  +- 3.40% (   9.15%)      305.08  +- 5.70% (  18.23%)
    Hmean  4        641.98  +- 2.32% (  24.83%)      623.88  +- 5.28% (  21.31%)      906.84  +- 4.65% (  76.32%)
    Hmean  8       1633.31  +- 1.56% (  46.96%)     1714.16  +- 0.93% (  54.24%)     2095.74  +- 0.47% (  88.57%)
    Hmean  16      3047.24  +- 0.42% (  33.27%)     3155.02  +- 0.30% (  37.99%)     3634.58  +- 0.15% (  58.96%)
    Hmean  32      4734.31  +- 0.60% (   0.63%)     4804.38  +- 0.23% (   2.12%)     4674.62  +- 0.27% (  -0.64%)
    Hmean  64      7699.74  +- 0.35% (   1.61%)     7499.72  +- 0.34% (  -1.03%)     7659.03  +- 0.25% (   1.07%)
    Hmean  128     6935.18  +- 0.15% (  -0.91%)     6942.54  +- 0.10% (  -0.80%)     7004.85  +- 0.12% (   0.09%)
    Hmean  192     6901.62  +- 0.12% (   0.00%)     6856.93  +- 0.10% (  -0.64%)     6978.74  +- 0.10% (   1.12%)
    
    This is one of the cases where the patch still can't surpass active
    intel_pstate, not even when freq_max is as low as 12C-turbo. Otherwise, gains are
    visible up to 16 clients and the saturated scenario is the same as baseline.
    
    The scores in the summary table from the previous sections are ratios of
    geometric means of the results over different clients, as seen in this table.
    
    Machine            : 80x-BROADWELL-NUMA
    Benchmark          : kernbench (kernel compilation)
    Varying parameter  : number of jobs
    Unit               : seconds (lower is better)
    
                       5.2.0 vanilla (BASELINE)               5.2.0 intel_pstate                   5.2.0 1C-turbo
    - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Amean  2        379.68  +- 0.06% (        )      330.20  +- 0.43% (  13.03%)      285.93  +- 0.07% (  24.69%)
    Amean  4        200.15  +- 0.24% (        )      175.89  +- 0.22% (  12.12%)      153.78  +- 0.25% (  23.17%)
    Amean  8        106.20  +- 0.31% (        )       95.54  +- 0.23% (  10.03%)       86.74  +- 0.10% (  18.32%)
    Amean  16        56.96  +- 1.31% (        )       53.25  +- 1.22% (   6.50%)       48.34  +- 1.73% (  15.13%)
    Amean  32        34.80  +- 2.46% (        )       33.81  +- 0.77% (   2.83%)       30.28  +- 1.59% (  12.99%)
    Amean  64        26.11  +- 1.63% (        )       25.04  +- 1.07% (   4.10%)       22.41  +- 2.37% (  14.16%)
    Amean  128       24.80  +- 1.36% (        )       23.57  +- 1.23% (   4.93%)       21.44  +- 1.37% (  13.55%)
    Amean  160       24.85  +- 0.56% (        )       23.85  +- 1.17% (   4.06%)       21.25  +- 1.12% (  14.49%)
    
                                 5.2.0 3C-turbo                   5.2.0 4C-turbo                   5.2.0 8C-turbo
    - - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Amean  2        284.08  +- 0.13% (  25.18%)      283.96  +- 0.51% (  25.21%)      285.05  +- 0.21% (  24.92%)
    Amean  4        153.18  +- 0.22% (  23.47%)      154.70  +- 1.64% (  22.71%)      153.64  +- 0.30% (  23.24%)
    Amean  8         87.06  +- 0.28% (  18.02%)       86.77  +- 0.46% (  18.29%)       86.78  +- 0.22% (  18.28%)
    Amean  16        48.03  +- 0.93% (  15.68%)       47.75  +- 1.99% (  16.17%)       47.52  +- 1.61% (  16.57%)
    Amean  32        30.23  +- 1.20% (  13.14%)       30.08  +- 1.67% (  13.57%)       30.07  +- 1.67% (  13.60%)
    Amean  64        22.59  +- 2.02% (  13.50%)       22.63  +- 0.81% (  13.32%)       22.42  +- 0.76% (  14.12%)
    Amean  128       21.37  +- 0.67% (  13.82%)       21.31  +- 1.15% (  14.07%)       21.17  +- 1.93% (  14.63%)
    Amean  160       21.68  +- 0.57% (  12.76%)       21.18  +- 1.74% (  14.77%)       21.22  +- 1.00% (  14.61%)
    
    The patch outperform active intel_pstate (and baseline) by a considerable
    margin; the summary table from the previous section says 4C turbo and active
    intel_pstate are 0.83 and 0.93 against baseline respectively, so 4C turbo is
    0.83/0.93=0.89 against intel_pstate (~10% better on average). There is no
    noticeable difference with regard to the value of freq_max.
    
    Machine            : 8x-SKYLAKE-UMA
    Benchmark          : gitsource (time to run the git unit test suite)
    Varying parameter  : none
    Unit               : seconds (lower is better)
    
                                5.2.0 vanilla           5.2.0 intel_pstate/hwp         5.2.0 1C-turbo
    - - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Amean         858.85  +- 1.16% (        )      791.94  +- 0.21% (   7.79%)      474.95 (  44.70%)
    
                               5.2.0 3C-turbo                   5.2.0 4C-turbo
    - - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Amean         475.26  +- 0.20% (  44.66%)      474.34  +- 0.13% (  44.77%)
    
    In this test, which is of interest as representing shell-intensive
    (i.e. fork-intensive) serialized workloads, invariant schedutil outperforms
    intel_pstate/powersave by a whopping 40% margin.
    
    5.3.4 POWER CONSUMPTION, PERFORMANCE-PER-WATT
    ---------------------------------------------
    
    The following table shows average power consumption in watt for each
    benchmark. Data comes from turbostat (package average), which in turn is read
    from the RAPL interface on CPUs. We know the patch affects CPU frequencies so
    it's reasonable to ignore other power consumers (such as memory or I/O). Also,
    we don't have a power meter available in the lab so RAPL is the best we have.
    
    turbostat sampled average power every 10 seconds for the entire duration of
    each benchmark. We took all those values and averaged them (i.e. with don't
    have detail on a per-parameter granularity, only on whole benchmarks).
    
    80x-BROADWELL-NUMA (power consumption, watts)
                                                        +--------+
                   BASELINE I_PSTATE       1C       3C  |     4C |      8C
    pgbench-ro       130.01   142.77   131.11   132.45  | 134.65 |  136.84
    pgbench-rw        68.30    60.83    71.45    71.70  |  71.65 |   72.54
    dbench4           90.25    59.06   101.43    99.89  | 101.10 |  102.94
    netperf-udp       65.70    69.81    66.02    68.03  |  68.27 |   68.95
    netperf-tcp       88.08    87.96    88.97    88.89  |  88.85 |   88.20
    tbench4          142.32   176.73   153.02   163.91  | 165.58 |  176.07
    kernbench         92.94   101.95   114.91   115.47  | 115.52 |  115.10
    gitsource         40.92    41.87    75.14    75.20  |  75.40 |   75.70
                                                        +--------+
    8x-SKYLAKE-UMA (power consumption, watts)
                                                        +--------+
                  BASELINE I_PSTATE/HWP    1C       3C  |     4C |
    pgbench-ro        46.49    46.68    46.56    46.59  |  46.52 |
    pgbench-rw        29.34    31.38    30.98    31.00  |  31.00 |
    dbench4           27.28    27.37    27.49    27.41  |  27.38 |
    netperf-udp       22.33    22.41    22.36    22.35  |  22.36 |
    netperf-tcp       27.29    27.29    27.30    27.31  |  27.33 |
    tbench4           41.13    45.61    43.10    43.33  |  43.56 |
    kernbench         42.56    42.63    43.01    43.01  |  43.01 |
    gitsource         13.32    13.69    17.33    17.30  |  17.35 |
                                                        +--------+
    48x-HASWELL-NUMA (power consumption, watts)
                                                        +--------+
                   BASELINE I_PSTATE       1C       3C  |     4C |     12C
    pgbench-ro       128.84   136.04   129.87   132.43  | 132.30 |  134.86
    pgbench-rw        37.68    37.92    37.17    37.74  |  37.73 |   37.31
    dbench4           28.56    28.73    28.60    28.73  |  28.70 |   28.79
    netperf-udp       56.70    60.44    56.79    57.42  |  57.54 |   57.52
    netperf-tcp       75.49    75.27    75.87    76.02  |  76.01 |   75.95
    tbench4          115.44   139.51   119.53   123.07  | 123.97 |  130.22
    kernbench         83.23    91.55    95.58    95.69  |  95.72 |   96.04
    gitsource         36.79    36.99    39.99    40.34  |  40.35 |   40.23
                                                        +--------+
    
    A lower power consumption isn't necessarily better, it depends on what is done
    with that energy. Here are tables with the ratio of performance-per-watt on
    each machine and benchmark. Higher is always better; a tilde (~) means a
    neutral ratio (i.e. 1.00).
    
    80x-BROADWELL-NUMA (performance-per-watt ratios; higher is better)
                                         +------+
                 I_PSTATE     1C     3C  |   4C |    8C
    pgbench-ro       1.04   1.06   0.94  | 1.07 |  1.08
    pgbench-rw       1.10   0.97   0.96  | 0.96 |  0.97
    dbench4          1.24   0.94   0.95  | 0.94 |  0.92
    netperf-udp      ~      1.02   1.02  | ~    |  1.02
    netperf-tcp      ~      1.02   ~     | ~    |  1.02
    tbench4          1.26   1.10   1.06  | 1.12 |  1.26
    kernbench        0.98   0.97   0.97  | 0.97 |  0.98
    gitsource        ~      1.11   1.11  | 1.11 |  1.13
                                         +------+
    
    8x-SKYLAKE-UMA (performance-per-watt ratios; higher is better)
                                         +------+
             I_PSTATE/HWP     1C     3C  |   4C |
    pgbench-ro       ~      ~      ~     | ~    |
    pgbench-rw       0.95   0.97   0.96  | 0.96 |
    dbench4          ~      ~      ~     | ~    |
    netperf-udp      ~      ~      ~     | ~    |
    netperf-tcp      ~      ~      ~     | ~    |
    tbench4          1.17   1.09   1.08  | 1.10 |
    kernbench        ~      ~      ~     | ~    |
    gitsource        1.06   1.40   1.40  | 1.40 |
                                         +------+
    
    48x-HASWELL-NUMA  (performance-per-watt ratios; higher is better)
                                         +------+
                 I_PSTATE     1C     3C  |   4C |   12C
    pgbench-ro       1.09   ~      1.09  | 1.03 |  1.11
    pgbench-rw       ~      0.86   ~     | ~    |  0.86
    dbench4          ~      1.02   1.02  | 1.02 |  ~
    netperf-udp      ~      0.97   1.03  | 1.02 |  ~
    netperf-tcp      0.96   ~      ~     | ~    |  ~
    tbench4          1.24   ~      1.06  | 1.05 |  1.11
    kernbench        0.97   0.97   0.98  | 0.97 |  0.96
    gitsource        1.03   1.33   1.32  | 1.32 |  1.33
                                         +------+
    
    These results are overall pleasing: in plenty of cases we observe
    performance-per-watt improvements. The few regressions (read/write pgbench and
    dbench on the Broadwell machine) are of small magnitude. kernbench loses a few
    percentage points (it has a 10-15% performance improvement, but apparently the
    increase in power consumption is larger than that). tbench4 and gitsource, which
    benefit the most from the patch, keep a positive score in this table which is
    a welcome surprise; that suggests that in those particular workloads the
    non-invariant schedutil (and active intel_pstate, too) makes some rather
    suboptimal frequency selections.
    
    +-------------------------------------------------------------------------+
    | 6. MICROARCH'ES ADDRESSED HERE
    +-------------------------------------------------------------------------+
    
    The patch addresses Xeon Core processors that use MSR_PLATFORM_INFO and
    MSR_TURBO_RATIO_LIMIT to advertise their base frequency and turbo frequencies
    respectively. This excludes the recent Xeon Scalable Performance processors
    line (Xeon Gold, Platinum etc) whose MSRs have to be parsed differently.
    
    Subsequent patches will address:
    
    * Xeon Scalable Performance processors and Atom Goldmont/Goldmont Plus
    * Xeon Phi (Knights Landing, Knights Mill)
    * Atom Silvermont
    
    +-------------------------------------------------------------------------+
    | 7. REFERENCES
    +-------------------------------------------------------------------------+
    
    Tests have been run with the help of the MMTests performance testing
    framework, see github.com/gormanm/mmtests. The configuration file names for
    the benchmark used are:
    
        db-pgbench-timed-ro-small-xfs
        db-pgbench-timed-rw-small-xfs
        io-dbench4-async-xfs
        network-netperf-unbound
        network-tbench
        scheduler-unbound
        workload-kerndevel-xfs
        workload-shellscripts-xfs
        hpc-nas-c-class-mpi-full-xfs
        hpc-nas-c-class-omp-full
    
    All those benchmarks are generally available on the web:
    
    pgbench: https://www.postgresql.org/docs/10/pgbench.html
    netperf: https://hewlettpackard.github.io/netperf/
    dbench/tbench: https://dbench.samba.org/
    gitsource: git unit test suite, github.com/git/git
    NAS Parallel Benchmarks: https://www.nas.nasa.gov/publications/npb.html
    hackbench: https://people.redhat.com/mingo/cfs-scheduler/tools/hackbench.c
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Doug Smythies <dsmythies@telus.net>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lkml.kernel.org/r/20200122151617.531-2-ggherdovich@suse.cz

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 69881b2d446c..28696bccf912 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -147,6 +147,8 @@ static inline void smpboot_restore_warm_reset_vector(void)
 	*((volatile u32 *)phys_to_virt(TRAMPOLINE_PHYS_LOW)) = 0;
 }
 
+static void init_freq_invariance(void);
+
 /*
  * Report back to the Boot Processor during boot time or to the caller processor
  * during CPU online.
@@ -183,6 +185,8 @@ static void smp_callin(void)
 	 */
 	set_cpu_sibling_map(raw_smp_processor_id());
 
+	init_freq_invariance();
+
 	/*
 	 * Get our bogomips.
 	 * Update loops_per_jiffy in cpu_data. Previous call to
@@ -1337,7 +1341,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	set_sched_topology(x86_topology);
 
 	set_cpu_sibling_map(0);
-
+	init_freq_invariance();
 	smp_sanity_check();
 
 	switch (apic_intr_mode) {
@@ -1764,3 +1768,180 @@ void native_play_dead(void)
 }
 
 #endif
+
+/*
+ * APERF/MPERF frequency ratio computation.
+ *
+ * The scheduler wants to do frequency invariant accounting and needs a <1
+ * ratio to account for the 'current' frequency, corresponding to
+ * freq_curr / freq_max.
+ *
+ * Since the frequency freq_curr on x86 is controlled by micro-controller and
+ * our P-state setting is little more than a request/hint, we need to observe
+ * the effective frequency 'BusyMHz', i.e. the average frequency over a time
+ * interval after discarding idle time. This is given by:
+ *
+ *   BusyMHz = delta_APERF / delta_MPERF * freq_base
+ *
+ * where freq_base is the max non-turbo P-state.
+ *
+ * The freq_max term has to be set to a somewhat arbitrary value, because we
+ * can't know which turbo states will be available at a given point in time:
+ * it all depends on the thermal headroom of the entire package. We set it to
+ * the turbo level with 4 cores active.
+ *
+ * Benchmarks show that's a good compromise between the 1C turbo ratio
+ * (freq_curr/freq_max would rarely reach 1) and something close to freq_base,
+ * which would ignore the entire turbo range (a conspicuous part, making
+ * freq_curr/freq_max always maxed out).
+ *
+ * Setting freq_max to anything less than the 1C turbo ratio makes the ratio
+ * freq_curr / freq_max to eventually grow >1, in which case we clip it to 1.
+ */
+
+DEFINE_STATIC_KEY_FALSE(arch_scale_freq_key);
+
+static DEFINE_PER_CPU(u64, arch_prev_aperf);
+static DEFINE_PER_CPU(u64, arch_prev_mperf);
+static u64 arch_max_freq_ratio = SCHED_CAPACITY_SCALE;
+
+static bool turbo_disabled(void)
+{
+	u64 misc_en;
+	int err;
+
+	err = rdmsrl_safe(MSR_IA32_MISC_ENABLE, &misc_en);
+	if (err)
+		return false;
+
+	return (misc_en & MSR_IA32_MISC_ENABLE_TURBO_DISABLE);
+}
+
+#include <asm/cpu_device_id.h>
+#include <asm/intel-family.h>
+
+#define ICPU(model) \
+	{X86_VENDOR_INTEL, 6, model, X86_FEATURE_APERFMPERF, 0}
+
+static const struct x86_cpu_id has_knl_turbo_ratio_limits[] = {
+	ICPU(INTEL_FAM6_XEON_PHI_KNL),
+	ICPU(INTEL_FAM6_XEON_PHI_KNM),
+	{}
+};
+
+static const struct x86_cpu_id has_skx_turbo_ratio_limits[] = {
+	ICPU(INTEL_FAM6_SKYLAKE_X),
+	{}
+};
+
+static const struct x86_cpu_id has_glm_turbo_ratio_limits[] = {
+	ICPU(INTEL_FAM6_ATOM_GOLDMONT),
+	ICPU(INTEL_FAM6_ATOM_GOLDMONT_D),
+	ICPU(INTEL_FAM6_ATOM_GOLDMONT_PLUS),
+	{}
+};
+
+static bool core_set_max_freq_ratio(void)
+{
+	u64 base_freq, turbo_freq;
+	int err;
+
+	err = rdmsrl_safe(MSR_PLATFORM_INFO, &base_freq);
+	if (err)
+		return false;
+
+	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, &turbo_freq);
+	if (err)
+		return false;
+
+	base_freq = (base_freq >> 8) & 0xFF;      /* max P state */
+	turbo_freq = (turbo_freq >> 24) & 0xFF;   /* 4C turbo    */
+
+	arch_max_freq_ratio = div_u64(turbo_freq * SCHED_CAPACITY_SCALE,
+					base_freq);
+	return true;
+}
+
+static bool intel_set_max_freq_ratio(void)
+{
+	/*
+	 * TODO: add support for:
+	 *
+	 * - Xeon Gold/Platinum
+	 * - Xeon Phi (KNM, KNL)
+	 * - Atom Goldmont
+	 * - Atom Silvermont
+	 */
+
+	if (x86_match_cpu(has_skx_turbo_ratio_limits) ||
+		x86_match_cpu(has_knl_turbo_ratio_limits) ||
+		x86_match_cpu(has_glm_turbo_ratio_limits))
+		return false;
+
+	if (turbo_disabled() || core_set_max_freq_ratio())
+		return true;
+
+	return false;
+}
+
+static void init_counter_refs(void *arg)
+{
+	u64 aperf, mperf;
+
+	rdmsrl(MSR_IA32_APERF, aperf);
+	rdmsrl(MSR_IA32_MPERF, mperf);
+
+	this_cpu_write(arch_prev_aperf, aperf);
+	this_cpu_write(arch_prev_mperf, mperf);
+}
+
+static void init_freq_invariance(void)
+{
+	bool ret = false;
+
+	if (smp_processor_id() != 0 || !boot_cpu_has(X86_FEATURE_APERFMPERF))
+		return;
+
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)
+		ret = intel_set_max_freq_ratio();
+
+	if (ret) {
+		on_each_cpu(init_counter_refs, NULL, 1);
+		static_branch_enable(&arch_scale_freq_key);
+	} else {
+		pr_debug("Couldn't determine max cpu frequency, necessary for scale-invariant accounting.\n");
+	}
+}
+
+DEFINE_PER_CPU(unsigned long, arch_freq_scale) = SCHED_CAPACITY_SCALE;
+
+void arch_scale_freq_tick(void)
+{
+	u64 freq_scale;
+	u64 aperf, mperf;
+	u64 acnt, mcnt;
+
+	if (!arch_scale_freq_invariant())
+		return;
+
+	rdmsrl(MSR_IA32_APERF, aperf);
+	rdmsrl(MSR_IA32_MPERF, mperf);
+
+	acnt = aperf - this_cpu_read(arch_prev_aperf);
+	mcnt = mperf - this_cpu_read(arch_prev_mperf);
+	if (!mcnt)
+		return;
+
+	this_cpu_write(arch_prev_aperf, aperf);
+	this_cpu_write(arch_prev_mperf, mperf);
+
+	acnt <<= 2*SCHED_CAPACITY_SHIFT;
+	mcnt *= arch_max_freq_ratio;
+
+	freq_scale = div64_u64(acnt, mcnt);
+
+	if (freq_scale > SCHED_CAPACITY_SCALE)
+		freq_scale = SCHED_CAPACITY_SCALE;
+
+	this_cpu_write(arch_freq_scale, freq_scale);
+}

commit c5f12fdb8bd873aa3ffdb79512e6bdac92b257b0
Merge: a572ba63298d 743dac494d61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 17 12:04:39 2019 -0700

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 apic updates from Thomas Gleixner:
    
     - Cleanup the apic IPI implementation by removing duplicated code and
       consolidating the functions into the APIC core.
    
     - Implement a safe variant of the IPI broadcast mode. Contrary to
       earlier attempts this uses the core tracking of which CPUs have been
       brought online at least once so that a broadcast does not end up in
       some dead end in BIOS/SMM code when the CPU is still waiting for
       init. Once all CPUs have been brought up once, IPI broadcasting is
       enabled. Before that regular one by one IPIs are issued.
    
     - Drop the paravirt CR8 related functions as they have no user anymore
    
     - Initialize the APIC TPR to block interrupt 16-31 as they are reserved
       for CPU exceptions and should never be raised by any well behaving
       device.
    
     - Emit a warning when vector space exhaustion breaks the admin set
       affinity of an interrupt.
    
     - Make sure to use the NMI fallback when shutdown via reboot vector IPI
       fails. The original code had conditions which prevent the code path
       to be reached.
    
     - Annotate various APIC config variables as RO after init.
    
    [ The ipi broadcase change came in earlier through the cpu hotplug
      branch, but I left the explanation in the commit message since it was
      shared between the two different branches    - Linus ]
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (28 commits)
      x86/apic/vector: Warn when vector space exhaustion breaks affinity
      x86/apic: Annotate global config variables as "read-only after init"
      x86/apic/x2apic: Implement IPI shorthands support
      x86/apic/flat64: Remove the IPI shorthand decision logic
      x86/apic: Share common IPI helpers
      x86/apic: Remove the shorthand decision logic
      x86/smp: Enhance native_send_call_func_ipi()
      x86/smp: Move smp_function_call implementations into IPI code
      x86/apic: Provide and use helper for send_IPI_allbutself()
      x86/apic: Add static key to Control IPI shorthands
      x86/apic: Move no_ipi_broadcast() out of 32bit
      x86/apic: Add NMI_VECTOR wait to IPI shorthand
      x86/apic: Remove dest argument from __default_send_IPI_shortcut()
      x86/hotplug: Silence APIC and NMI when CPU is dead
      x86/cpu: Move arch_smt_update() to a neutral place
      x86/apic/uv: Make x2apic_extra_bits static
      x86/apic: Consolidate the apic local headers
      x86/apic: Move apic_flat_64 header into apic directory
      x86/apic: Move ipi header into apic directory
      x86/apic: Cleanup the include maze
      ...

commit 60dcaad5736faff5a6b1abba5a292499f57197fe
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 24 17:25:52 2019 +0200

    x86/hotplug: Silence APIC and NMI when CPU is dead
    
    In order to support IPI/NMI broadcasting via the shorthand mechanism side
    effects of shorthands need to be mitigated:
    
     Shorthand IPIs and NMIs hit all CPUs including unplugged CPUs
    
    Neither of those can be handled on unplugged CPUs for obvious reasons.
    
    It would be trivial to just fully disable the APIC via the enable bit in
    MSR_APICBASE. But that's not possible because clearing that bit on systems
    based on the 3 wire APIC bus would require a hardware reset to bring it
    back as the APIC would lose track of bus arbitration. On systems with FSB
    delivery APICBASE could be disabled, but it has to be guaranteed that no
    interrupt is sent to the APIC while in that state and it's not clear from
    the SDM whether it still responds to INIT/SIPI messages.
    
    Therefore stay on the safe side and switch the APIC into soft disabled mode
    so it won't deliver any regular vector to the CPU.
    
    NMIs are still propagated to the 'dead' CPUs. To mitigate that add a check
    for the CPU being offline on early nmi entry and if so bail.
    
    Note, this cannot use the stop/restart_nmi() magic which is used in the
    alternatives code. A dead CPU cannot invoke nmi_enter() or anything else
    due to RCU and other reasons.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1907241723290.1791@nanos.tec.linutronix.de

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fdbd47ceb84d..c19f8e21b748 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1596,7 +1596,12 @@ int native_cpu_disable(void)
 	if (ret)
 		return ret;
 
-	clear_local_APIC();
+	/*
+	 * Disable the local APIC. Otherwise IPI broadcasts will reach
+	 * it. It still responds normally to INIT, NMI, SMI, and SIPI
+	 * messages.
+	 */
+	apic_soft_disable();
 	cpu_disable_common();
 
 	return 0;

commit 69732102426b1c55a257386841fb80ec1f425d32
Author: Pingfan Liu <kernelfans@gmail.com>
Date:   Tue Jul 16 16:40:24 2019 +0800

    x86/realmode: Remove trampoline_status
    
    There is no reader of trampoline_status, it's only written.
    
    It turns out that after commit ce4b1b16502b ("x86/smpboot: Initialize
    secondary CPU only if master CPU will wait for it"), trampoline_status is
    not needed any more.
    
    Signed-off-by: Pingfan Liu <kernelfans@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/1563266424-3472-1-git-send-email-kernelfans@gmail.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fdbd47ceb84d..497e9b7077c1 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1023,8 +1023,6 @@ int common_cpu_up(unsigned int cpu, struct task_struct *idle)
 static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 		       int *cpu0_nmi_registered)
 {
-	volatile u32 *trampoline_status =
-		(volatile u32 *) __va(real_mode_header->trampoline_status);
 	/* start_ip had better be page-aligned! */
 	unsigned long start_ip = real_mode_header->trampoline_start;
 
@@ -1116,9 +1114,6 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 		}
 	}
 
-	/* mark "stuck" area as not stuck */
-	*trampoline_status = 0;
-
 	if (x86_platform.legacy.warm_reset) {
 		/*
 		 * Cleanup possible dangling ends...

commit 090d54bcbc54af75e94442e60f42d973341a5f53
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Wed Jun 26 16:57:09 2019 +0800

    Revert "x86/paravirt: Set up the virt_spin_lock_key after static keys get initialized"
    
    This reverts commit ca5d376e17072c1b60c3fee66f3be58ef018952d.
    
    Commit 8990cac6e5ea ("x86/jump_label: Initialize static branching
    early") adds jump_label_init() call in setup_arch() to make static
    keys initialized early, so we could use the original simpler code
    again.
    
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 259d1d2be076..fdbd47ceb84d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1368,8 +1368,6 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	pr_info("CPU0: ");
 	print_cpu_info(&cpu_data(0));
 
-	native_pv_lock_init();
-
 	uv_system_init();
 
 	set_mtrr_aps_delayed_init();
@@ -1399,6 +1397,7 @@ void __init native_smp_prepare_boot_cpu(void)
 	/* already set me in cpu_online_mask in boot_cpu_init() */
 	cpumask_set_cpu(me, cpu_callout_mask);
 	cpu_set_state_online(me);
+	native_pv_lock_init();
 }
 
 void __init calculate_max_logical_packages(void)

commit 7652ac92018536eb807b6c2130100c85f1ba7e3b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 10 21:42:46 2019 +0200

    x86/asm: Move native_write_cr0/4() out of line
    
    The pinning of sensitive CR0 and CR4 bits caused a boot crash when loading
    the kvm_intel module on a kernel compiled with CONFIG_PARAVIRT=n.
    
    The reason is that the static key which controls the pinning is marked RO
    after init. The kvm_intel module contains a CR4 write which requires to
    update the static key entry list. That obviously does not work when the key
    is in a RO section.
    
    With CONFIG_PARAVIRT enabled this does not happen because the CR4 write
    uses the paravirt indirection and the actual write function is built in.
    
    As the key is intended to be immutable after init, move
    native_write_cr0/4() out of line.
    
    While at it consolidate the update of the cr4 shadow variable and store the
    value right away when the pinning is initialized on a booting CPU. No point
    in reading it back 20 instructions later. This allows to confine the static
    key and the pinning variable to cpu/common and allows to mark them static.
    
    Fixes: 8dbec27a242c ("x86/asm: Pin sensitive CR0 bits")
    Fixes: 873d50d58f67 ("x86/asm: Pin sensitive CR4 bits")
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Xi Ruoyao <xry111@mengyan1223.wang>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Xi Ruoyao <xry111@mengyan1223.wang>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1907102140340.1758@nanos.tec.linutronix.de

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f78801114ee1..259d1d2be076 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -210,28 +210,16 @@ static int enable_start_cpu0;
  */
 static void notrace start_secondary(void *unused)
 {
-	unsigned long cr4 = __read_cr4();
-
 	/*
 	 * Don't put *anything* except direct CPU state initialization
 	 * before cpu_init(), SMP booting is too fragile that we want to
 	 * limit the things done here to the most necessary things.
 	 */
-	if (boot_cpu_has(X86_FEATURE_PCID))
-		cr4 |= X86_CR4_PCIDE;
-	if (static_branch_likely(&cr_pinning))
-		cr4 |= cr4_pinned_bits;
-
-	__write_cr4(cr4);
+	cr4_init();
 
 #ifdef CONFIG_X86_32
 	/* switch away from the initial page table */
 	load_cr3(swapper_pg_dir);
-	/*
-	 * Initialize the CR4 shadow before doing anything that could
-	 * try to read it.
-	 */
-	cr4_init_shadow();
 	__flush_tlb_all();
 #endif
 	load_current_idt();

commit 222a21d29521d144f3dd7a0bc4d4020e448f0126
Merge: 8faef7125d02 eb876fbc248e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 18:28:44 2019 -0700

    Merge branch 'x86-topology-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 topology updates from Ingo Molnar:
     "Implement multi-die topology support on Intel CPUs and expose the die
      topology to user-space tooling, by Len Brown, Kan Liang and Zhang Rui.
    
      These changes should have no effect on the kernel's existing
      understanding of topologies, i.e. there should be no behavioral impact
      on cache, NUMA, scheduler, perf and other topologies and overall
      system performance"
    
    * 'x86-topology-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf/x86/intel/rapl: Cosmetic rename internal variables in response to multi-die/pkg support
      perf/x86/intel/uncore: Cosmetic renames in response to multi-die/pkg support
      hwmon/coretemp: Cosmetic: Rename internal variables to zones from packages
      thermal/x86_pkg_temp_thermal: Cosmetic: Rename internal variables to zones from packages
      perf/x86/intel/cstate: Support multi-die/package
      perf/x86/intel/rapl: Support multi-die/package
      perf/x86/intel/uncore: Support multi-die/package
      topology: Create core_cpus and die_cpus sysfs attributes
      topology: Create package_cpus sysfs attribute
      hwmon/coretemp: Support multi-die/package
      powercap/intel_rapl: Update RAPL domain name and debug messages
      thermal/x86_pkg_temp_thermal: Support multi-die/package
      powercap/intel_rapl: Support multi-die/package
      powercap/intel_rapl: Simplify rapl_find_package()
      x86/topology: Define topology_logical_die_id()
      x86/topology: Define topology_die_id()
      cpu/topology: Export die_id
      x86/topology: Create topology_max_die_per_package()
      x86/topology: Add CPUID.1F multi-die/package support

commit 873d50d58f67ef15d2777b5e7f7a5268bb1fbae2
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Jun 17 21:55:02 2019 -0700

    x86/asm: Pin sensitive CR4 bits
    
    Several recent exploits have used direct calls to the native_write_cr4()
    function to disable SMEP and SMAP before then continuing their exploits
    using userspace memory access.
    
    Direct calls of this form can be mitigate by pinning bits of CR4 so that
    they cannot be changed through a common function. This is not intended to
    be a general ROP protection (which would require CFI to defend against
    properly), but rather a way to avoid trivial direct function calling (or
    CFI bypasses via a matching function prototype) as seen in:
    
    https://googleprojectzero.blogspot.com/2017/05/exploiting-linux-kernel-via-packet.html
    (https://github.com/xairy/kernel-exploits/tree/master/CVE-2017-7308)
    
    The goals of this change:
    
     - Pin specific bits (SMEP, SMAP, and UMIP) when writing CR4.
    
     - Avoid setting the bits too early (they must become pinned only after
       CPU feature detection and selection has finished).
    
     - Pinning mask needs to be read-only during normal runtime.
    
     - Pinning needs to be checked after write to validate the cr4 state
    
    Using __ro_after_init on the mask is done so it can't be first disabled
    with a malicious write.
    
    Since these bits are global state (once established by the boot CPU and
    kernel boot parameters), they are safe to write to secondary CPUs before
    those CPUs have finished feature detection. As such, the bits are set at
    the first cr4 write, so that cr4 write bugs can be detected (instead of
    silently papered over). This uses a few bytes less storage of a location we
    don't have: read-only per-CPU data.
    
    A check is performed after the register write because an attack could just
    skip directly to the register write. Such a direct jump is possible because
    of how this function may be built by the compiler (especially due to the
    removal of frame pointers) where it doesn't add a stack frame (function
    exit may only be a retq without pops) which is sufficient for trivial
    exploitation like in the timer overwrites mentioned above).
    
    The asm argument constraints gain the "+" modifier to convince the compiler
    that it shouldn't make ordering assumptions about the arguments or memory,
    and treat them as changed.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: kernel-hardening@lists.openwall.com
    Link: https://lkml.kernel.org/r/20190618045503.39105-3-keescook@chromium.org

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 362dd8953f48..1af7a2d89419 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -205,13 +205,19 @@ static int enable_start_cpu0;
  */
 static void notrace start_secondary(void *unused)
 {
+	unsigned long cr4 = __read_cr4();
+
 	/*
 	 * Don't put *anything* except direct CPU state initialization
 	 * before cpu_init(), SMP booting is too fragile that we want to
 	 * limit the things done here to the most necessary things.
 	 */
 	if (boot_cpu_has(X86_FEATURE_PCID))
-		__write_cr4(__read_cr4() | X86_CR4_PCIDE);
+		cr4 |= X86_CR4_PCIDE;
+	if (static_branch_likely(&cr_pinning))
+		cr4 |= cr4_pinned_bits;
+
+	__write_cr4(cr4);
 
 #ifdef CONFIG_X86_32
 	/* switch away from the initial page table */

commit 9ff554e9be1f78d47cc77ad563e57e6c0612dbb3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 22 09:51:28 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 82
    
    Based on 1 normalized pattern(s):
    
      this code is released under the gnu general public license version 2
      or later
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Reviewed-by: Armijn Hemel <armijn@tjaldur.nl>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190520075211.232210963@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 73e69aaaa117..362dd8953f48 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
  /*
  *	x86 SMP booting functions
  *
@@ -12,9 +13,6 @@
  *	Pentium Pro and Pentium-II/Xeon MP machines.
  *	Original development of Linux SMP code supported by Caldera.
  *
- *	This code is released under the GNU General Public License version 2 or
- *	later.
- *
  *	Fixes
  *		Felix Koop	:	NR_CPUS used properly
  *		Jose Renau	:	Handle single CPU case.

commit 2e4c54dac7b360c3820399bdf06cde9134a4495b
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 13 13:58:56 2019 -0400

    topology: Create core_cpus and die_cpus sysfs attributes
    
    Create CPU topology sysfs attributes: "core_cpus" and "core_cpus_list"
    
    These attributes represent all of the logical CPUs that share the
    same core.
    
    These attriutes is synonymous with the existing "thread_siblings" and
    "thread_siblings_list" attribute, which will be deprecated.
    
    Create CPU topology sysfs attributes: "die_cpus" and "die_cpus_list".
    These attributes represent all of the logical CPUs that share the
    same die.
    
    Suggested-by: Brice Goglin <Brice.Goglin@inria.fr>
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/071c23a298cd27ede6ed0b6460cae190d193364f.1557769318.git.len.brown@intel.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a6e01b6c2709..1a19a5171949 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -91,6 +91,10 @@ EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
 DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_core_map);
 EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 
+/* representing HT, core, and die siblings of each logical CPU */
+DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_die_map);
+EXPORT_PER_CPU_SYMBOL(cpu_die_map);
+
 DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_llc_shared_map);
 
 /* Per CPU bogomips and other parameters */
@@ -509,6 +513,15 @@ static bool match_pkg(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 	return false;
 }
 
+static bool match_die(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
+{
+	if ((c->phys_proc_id == o->phys_proc_id) &&
+		(c->cpu_die_id == o->cpu_die_id))
+		return true;
+	return false;
+}
+
+
 #if defined(CONFIG_SCHED_SMT) || defined(CONFIG_SCHED_MC)
 static inline int x86_sched_itmt_flags(void)
 {
@@ -571,6 +584,7 @@ void set_cpu_sibling_map(int cpu)
 		cpumask_set_cpu(cpu, topology_sibling_cpumask(cpu));
 		cpumask_set_cpu(cpu, cpu_llc_shared_mask(cpu));
 		cpumask_set_cpu(cpu, topology_core_cpumask(cpu));
+		cpumask_set_cpu(cpu, topology_die_cpumask(cpu));
 		c->booted_cores = 1;
 		return;
 	}
@@ -619,6 +633,9 @@ void set_cpu_sibling_map(int cpu)
 		}
 		if (match_pkg(c, o) && !topology_same_node(c, o))
 			x86_has_numa_in_package = true;
+
+		if ((i == cpu) || (has_mp && match_die(c, o)))
+			link_mask(topology_die_cpumask, cpu, i);
 	}
 
 	threads = cpumask_weight(topology_sibling_cpumask(cpu));
@@ -1223,6 +1240,7 @@ static __init void disable_smp(void)
 		physid_set_mask_of_physid(0, &phys_cpu_present_map);
 	cpumask_set_cpu(0, topology_sibling_cpumask(0));
 	cpumask_set_cpu(0, topology_core_cpumask(0));
+	cpumask_set_cpu(0, topology_die_cpumask(0));
 }
 
 /*
@@ -1318,6 +1336,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	for_each_possible_cpu(i) {
 		zalloc_cpumask_var(&per_cpu(cpu_sibling_map, i), GFP_KERNEL);
 		zalloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);
+		zalloc_cpumask_var(&per_cpu(cpu_die_map, i), GFP_KERNEL);
 		zalloc_cpumask_var(&per_cpu(cpu_llc_shared_map, i), GFP_KERNEL);
 	}
 
@@ -1538,6 +1557,8 @@ static void remove_siblinginfo(int cpu)
 			cpu_data(sibling).booted_cores--;
 	}
 
+	for_each_cpu(sibling, topology_die_cpumask(cpu))
+		cpumask_clear_cpu(cpu, topology_die_cpumask(sibling));
 	for_each_cpu(sibling, topology_sibling_cpumask(cpu))
 		cpumask_clear_cpu(cpu, topology_sibling_cpumask(sibling));
 	for_each_cpu(sibling, cpu_llc_shared_mask(cpu))
@@ -1545,6 +1566,7 @@ static void remove_siblinginfo(int cpu)
 	cpumask_clear(cpu_llc_shared_mask(cpu));
 	cpumask_clear(topology_sibling_cpumask(cpu));
 	cpumask_clear(topology_core_cpumask(cpu));
+	cpumask_clear(topology_die_cpumask(cpu));
 	c->cpu_core_id = 0;
 	c->booted_cores = 0;
 	cpumask_clear_cpu(cpu, cpu_sibling_setup_mask);

commit 212bf4fdb7f9eeeb99afd97ebad677d43e7b55ac
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 13 13:58:49 2019 -0400

    x86/topology: Define topology_logical_die_id()
    
    Define topology_logical_die_id() ala existing topology_logical_package_id()
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Zhang Rui <rui.zhang@intel.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/2f3526e25ae14fbeff26fb26e877d159df8946d9.1557769318.git.len.brown@intel.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 40ffe23249c0..a6e01b6c2709 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -101,6 +101,7 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 unsigned int __max_logical_packages __read_mostly;
 EXPORT_SYMBOL(__max_logical_packages);
 static unsigned int logical_packages __read_mostly;
+static unsigned int logical_die __read_mostly;
 
 /* Maximum number of SMT threads on any online core */
 int __read_mostly __max_smt_threads = 1;
@@ -302,6 +303,26 @@ int topology_phys_to_logical_pkg(unsigned int phys_pkg)
 	return -1;
 }
 EXPORT_SYMBOL(topology_phys_to_logical_pkg);
+/**
+ * topology_phys_to_logical_die - Map a physical die id to logical
+ *
+ * Returns logical die id or -1 if not found
+ */
+int topology_phys_to_logical_die(unsigned int die_id, unsigned int cur_cpu)
+{
+	int cpu;
+	int proc_id = cpu_data(cur_cpu).phys_proc_id;
+
+	for_each_possible_cpu(cpu) {
+		struct cpuinfo_x86 *c = &cpu_data(cpu);
+
+		if (c->initialized && c->cpu_die_id == die_id &&
+		    c->phys_proc_id == proc_id)
+			return c->logical_die_id;
+	}
+	return -1;
+}
+EXPORT_SYMBOL(topology_phys_to_logical_die);
 
 /**
  * topology_update_package_map - Update the physical to logical package map
@@ -326,6 +347,29 @@ int topology_update_package_map(unsigned int pkg, unsigned int cpu)
 	cpu_data(cpu).logical_proc_id = new;
 	return 0;
 }
+/**
+ * topology_update_die_map - Update the physical to logical die map
+ * @die:	The die id as retrieved via CPUID
+ * @cpu:	The cpu for which this is updated
+ */
+int topology_update_die_map(unsigned int die, unsigned int cpu)
+{
+	int new;
+
+	/* Already available somewhere? */
+	new = topology_phys_to_logical_die(die, cpu);
+	if (new >= 0)
+		goto found;
+
+	new = logical_die++;
+	if (new != die) {
+		pr_info("CPU %u Converting physical %u to logical die %u\n",
+			cpu, die, new);
+	}
+found:
+	cpu_data(cpu).logical_die_id = new;
+	return 0;
+}
 
 void __init smp_store_boot_cpu_info(void)
 {
@@ -335,6 +379,7 @@ void __init smp_store_boot_cpu_info(void)
 	*c = boot_cpu_data;
 	c->cpu_index = id;
 	topology_update_package_map(c->phys_proc_id, id);
+	topology_update_die_map(c->cpu_die_id, id);
 	c->initialized = true;
 }
 

commit 7745f03eb39587dd15a1fb26e6223678b8e906d2
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 13 13:58:45 2019 -0400

    x86/topology: Add CPUID.1F multi-die/package support
    
    Some new systems have multiple software-visible die within each package.
    
    Update Linux parsing of the Intel CPUID "Extended Topology Leaf" to handle
    either CPUID.B, or the new CPUID.1F.
    
    Add cpuinfo_x86.die_id and cpuinfo_x86.max_dies to store the result.
    
    die_id will be non-zero only for multi-die/package systems.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-doc@vger.kernel.org
    Link: https://lkml.kernel.org/r/7b23d2d26d717b8e14ba137c94b70943f1ae4b5c.1557769318.git.len.brown@intel.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 73e69aaaa117..40ffe23249c0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -389,6 +389,7 @@ static bool match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 		int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
 
 		if (c->phys_proc_id == o->phys_proc_id &&
+		    c->cpu_die_id == o->cpu_die_id &&
 		    per_cpu(cpu_llc_id, cpu1) == per_cpu(cpu_llc_id, cpu2)) {
 			if (c->cpu_core_id == o->cpu_core_id)
 				return topology_sane(c, o, "smt");
@@ -400,6 +401,7 @@ static bool match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 		}
 
 	} else if (c->phys_proc_id == o->phys_proc_id &&
+		   c->cpu_die_id == o->cpu_die_id &&
 		   c->cpu_core_id == o->cpu_core_id) {
 		return topology_sane(c, o, "smt");
 	}

commit 948a64995aca6820abefd17f1a4258f5835c5ad9
Merge: db10ad041b31 8fea0f59e97d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 16:33:06 2019 -0700

    Merge branch 'x86-topology-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 topology updates from Ingo Molnar:
     "Two main changes: preparatory changes for Intel multi-die topology
      support, plus a syslog message tweak"
    
    * 'x86-topology-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/topology: Make DEBUG_HOTPLUG_CPU0 pr_info() more descriptive
      x86/smpboot: Rename match_die() to match_pkg()
      topology: Simplify cputopology.txt formatting and wording
      x86/topology: Fix documentation typo

commit 169d0869962da362b5058e31f87911b2960418af
Author: Len Brown <len.brown@intel.com>
Date:   Tue Feb 26 01:20:01 2019 -0500

    x86/smpboot: Rename match_die() to match_pkg()
    
    Syntax only, no functional or semantic change.
    
    This routine matches packages, not die, so name it thus.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/7ca18c4ae7816a1f9eda37414725df676e63589d.1551160674.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ce1a67b70168..3f8bbfb26c18 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -455,7 +455,7 @@ static bool match_llc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
  * multicore group inside a NUMA node.  If this happens, we will
  * discard the MC level of the topology later.
  */
-static bool match_die(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
+static bool match_pkg(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 {
 	if (c->phys_proc_id == o->phys_proc_id)
 		return true;
@@ -546,7 +546,7 @@ void set_cpu_sibling_map(int cpu)
 	for_each_cpu(i, cpu_sibling_setup_mask) {
 		o = &cpu_data(i);
 
-		if ((i == cpu) || (has_mp && match_die(c, o))) {
+		if ((i == cpu) || (has_mp && match_pkg(c, o))) {
 			link_mask(topology_core_cpumask, cpu, i);
 
 			/*
@@ -570,7 +570,7 @@ void set_cpu_sibling_map(int cpu)
 			} else if (i != cpu && !c->booted_cores)
 				c->booted_cores = cpu_data(i).booted_cores;
 		}
-		if (match_die(c, o) && !topology_same_node(c, o))
+		if (match_pkg(c, o) && !topology_same_node(c, o))
 			x86_has_numa_in_package = true;
 	}
 

commit 66c7ceb47f628c8bd4f84a6d01c2725ded6a342d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 18:00:04 2019 +0200

    x86/irq/32: Handle irq stack allocation failure proper
    
    irq_ctx_init() crashes hard on page allocation failures. While that's ok
    during early boot, it's just wrong in the CPU hotplug bringup code.
    
    Check the page allocation failure and return -ENOMEM and handle it at the
    call sites. On early boot the only way out is to BUG(), but on CPU hotplug
    there is no reason to crash, so just abort the operation.
    
    Rename the function to something more sensible while at it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alison Schofield <alison.schofield@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Shaokun Zhang <zhangshaokun@hisilicon.com>
    Cc: Stefano Stabellini <sstabellini@kernel.org>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: x86-ml <x86@kernel.org>
    Cc: xen-devel@lists.xenproject.org
    Cc: Yazen Ghannam <yazen.ghannam@amd.com>
    Cc: Yi Wang <wang.yi59@zte.com.cn>
    Cc: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Link: https://lkml.kernel.org/r/20190414160146.089060584@linutronix.de

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ce1a67b70168..c92b21f9e9dc 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -935,20 +935,27 @@ wakeup_cpu_via_init_nmi(int cpu, unsigned long start_ip, int apicid,
 	return boot_error;
 }
 
-void common_cpu_up(unsigned int cpu, struct task_struct *idle)
+int common_cpu_up(unsigned int cpu, struct task_struct *idle)
 {
+	int ret;
+
 	/* Just in case we booted with a single CPU. */
 	alternatives_enable_smp();
 
 	per_cpu(current_task, cpu) = idle;
 
+	/* Initialize the interrupt stack(s) */
+	ret = irq_init_percpu_irqstack(cpu);
+	if (ret)
+		return ret;
+
 #ifdef CONFIG_X86_32
 	/* Stack for startup_32 can be just as for start_secondary onwards */
-	irq_ctx_init(cpu);
 	per_cpu(cpu_current_top_of_stack, cpu) = task_top_of_stack(idle);
 #else
 	initial_gs = per_cpu_offset(cpu);
 #endif
+	return 0;
 }
 
 /*
@@ -1106,7 +1113,9 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 	/* the FPU context is blank, nobody can own it */
 	per_cpu(fpu_fpregs_owner_ctx, cpu) = NULL;
 
-	common_cpu_up(cpu, tidle);
+	err = common_cpu_up(cpu, tidle);
+	if (err)
+		return err;
 
 	err = do_boot_cpu(apicid, cpu, tidle, &cpu0_nmi_registered);
 	if (err) {

commit bcd49c3dd172c38e14faf151adca63c8db4c9557
Merge: f14b5f05cde1 2e7614c0736d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 7 16:36:57 2019 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Various cleanups and simplifications, none of them really stands out,
      they are all over the place"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/uaccess: Remove unused __addr_ok() macro
      x86/smpboot: Remove unused phys_id variable
      x86/mm/dump_pagetables: Remove the unused prev_pud variable
      x86/fpu: Move init_xstate_size() to __init section
      x86/cpu_entry_area: Move percpu_setup_debug_store() to __init section
      x86/mtrr: Remove unused variable
      x86/boot/compressed/64: Explain paging_prepare()'s return value
      x86/resctrl: Remove duplicate MSR_MISC_FEATURE_CONTROL definition
      x86/asm/suspend: Drop ENTRY from local data
      x86/hw_breakpoints, kprobes: Remove kprobes ifdeffery
      x86/boot: Save several bytes in decompressor
      x86/trap: Remove useless declaration
      x86/mm/tlb: Remove unused cpu variable
      x86/events: Mark expected switch-case fall-throughs
      x86/asm-prototypes: Remove duplicate include <asm/page.h>
      x86/kernel: Mark expected switch-case fall-throughs
      x86/insn-eval: Mark expected switch-case fall-through
      x86/platform/UV: Replace kmalloc() and memset() with k[cz]alloc() calls
      x86/e820: Replace kmalloc() + memcpy() with kmemdup()

commit 98fa15f34cb379864757670b8e8743b21456a20e
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Mar 5 15:42:58 2019 -0800

    mm: replace all open encodings for NUMA_NO_NODE
    
    Patch series "Replace all open encodings for NUMA_NO_NODE", v3.
    
    All these places for replacement were found by running the following
    grep patterns on the entire kernel code.  Please let me know if this
    might have missed some instances.  This might also have replaced some
    false positives.  I will appreciate suggestions, inputs and review.
    
    1. git grep "nid == -1"
    2. git grep "node == -1"
    3. git grep "nid = -1"
    4. git grep "node = -1"
    
    This patch (of 2):
    
    At present there are multiple places where invalid node number is
    encoded as -1.  Even though implicitly understood it is always better to
    have macros in there.  Replace these open encodings for an invalid node
    number with the global macro NUMA_NO_NODE.  This helps remove NUMA
    related assumptions like 'invalid node' from various places redirecting
    them to a common definition.
    
    Link: http://lkml.kernel.org/r/1545127933-10711-2-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>    [ixgbe]
    Acked-by: Jens Axboe <axboe@kernel.dk>                  [mtip32xx]
    Acked-by: Vinod Koul <vkoul@kernel.org>                 [dmaengine.c]
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>         [powerpc]
    Acked-by: Doug Ledford <dledford@redhat.com>            [drivers/infiniband]
    Cc: Joseph Qi <jiangqi903@gmail.com>
    Cc: Hans Verkuil <hverkuil@xs4all.nl>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ccd1f2a8e557..c91ff9f9fe8a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -56,6 +56,7 @@
 #include <linux/stackprotector.h>
 #include <linux/gfp.h>
 #include <linux/cpuidle.h>
+#include <linux/numa.h>
 
 #include <asm/acpi.h>
 #include <asm/desc.h>
@@ -841,7 +842,7 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 /* reduce the number of lines printed when booting a large cpu count system */
 static void announce_cpu(int cpu, int apicid)
 {
-	static int current_node = -1;
+	static int current_node = NUMA_NO_NODE;
 	int node = early_cpu_to_node(cpu);
 	static int width, node_width;
 

commit f91fecc09e498529230b4d5053cb361619a0c42d
Author: Shaokun Zhang <zhangshaokun@hisilicon.com>
Date:   Mon Feb 18 21:05:01 2019 +0800

    x86/smpboot: Remove unused phys_id variable
    
    The 'phys_id' local variable became unused after commit
    
      ce4b1b16502b ("x86/smpboot: Initialize secondary CPU only if master CPU will wait for it").
    
    Remove it.
    
    Signed-off-by: Shaokun Zhang <zhangshaokun@hisilicon.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alison Schofield <alison.schofield@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yazen Ghannam <yazen.ghannam@amd.com>
    Cc: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Link: https://lkml.kernel.org/r/1550495101-41755-1-git-send-email-zhangshaokun@hisilicon.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ccd1f2a8e557..5d5421b48e55 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -149,7 +149,7 @@ static inline void smpboot_restore_warm_reset_vector(void)
  */
 static void smp_callin(void)
 {
-	int cpuid, phys_id;
+	int cpuid;
 
 	/*
 	 * If waken up by an INIT in an 82489DX configuration
@@ -159,11 +159,6 @@ static void smp_callin(void)
 	 */
 	cpuid = smp_processor_id();
 
-	/*
-	 * (This works even if the APIC is not enabled.)
-	 */
-	phys_id = read_apic_id();
-
 	/*
 	 * the boot CPU has finished the init stage and is spinning
 	 * on callin_map until we finish. We are free to set up this

commit aa02ef099cff042c2a9109782ec2bf1bffc955d4
Author: Hui Wang <john.wanghui@huawei.com>
Date:   Wed Nov 7 10:36:43 2018 +0800

    x86/topology: Use total_cpus for max logical packages calculation
    
    nr_cpu_ids can be limited on the command line via nr_cpus=. This can break the
    logical package management because it results in a smaller number of packages
    while in kdump kernel.
    
    Check below case:
    There is a two sockets system, each socket has 8 cores, which has 16 logical
    cpus while HT was turn on.
    
     0  1  2  3  4  5  6  7     |    16 17 18 19 20 21 22 23
     cores on socket 0               threads on socket 0
     8  9 10 11 12 13 14 15     |    24 25 26 27 28 29 30 31
     cores on socket 1               threads on socket 1
    
    While starting the kdump kernel with command line option nr_cpus=16 panic
    was triggered on one of the cpus 24-31 eg. 26, then online cpu will be
    1-15, 26(cpu 0 was disabled in kdump), ncpus will be 16 and
    __max_logical_packages will be 1, but actually two packages were booted on.
    
    This issue can reproduced by set kdump option nr_cpus=<real physical core
    numbers>, and then trigger panic on last socket's thread, for example:
    
    taskset -c 26 echo c > /proc/sysrq-trigger
    
    Use total_cpus which will not be limited by nr_cpus command line to calculate
    the value of __max_logical_packages.
    
    Signed-off-by: Hui Wang <john.wanghui@huawei.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: <guijianfeng@huawei.com>
    Cc: <wencongyang2@huawei.com>
    Cc: <douliyang1@huawei.com>
    Cc: <qiaonuohan@huawei.com>
    Link: https://lkml.kernel.org/r/20181107023643.22174-1-john.wanghui@huawei.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a9134d1910b9..ccd1f2a8e557 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1347,7 +1347,7 @@ void __init calculate_max_logical_packages(void)
 	 * extrapolate the boot cpu's data to all packages.
 	 */
 	ncpus = cpu_data(0).booted_cores * topology_max_smt_threads();
-	__max_logical_packages = DIV_ROUND_UP(nr_cpu_ids, ncpus);
+	__max_logical_packages = DIV_ROUND_UP(total_cpus, ncpus);
 	pr_info("Max logical packages: %u\n", __max_logical_packages);
 }
 

commit 57c8a661d95dff48dd9c2f2496139082bbaf241a
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:49 2018 -0700

    mm: remove include/linux/bootmem.h
    
    Move remaining definitions and declarations from include/linux/bootmem.h
    into include/linux/memblock.h and remove the redundant header.
    
    The includes were replaced with the semantic patch below and then
    semi-automated removal of duplicated '#include <linux/memblock.h>
    
    @@
    @@
    - #include <linux/bootmem.h>
    + #include <linux/memblock.h>
    
    [sfr@canb.auug.org.au: dma-direct: fix up for the removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181002185342.133d1680@canb.auug.org.au
    [sfr@canb.auug.org.au: powerpc: fix up for removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181005161406.73ef8727@canb.auug.org.au
    [sfr@canb.auug.org.au: x86/kaslr, ACPI/NUMA: fix for linux/bootmem.h removal]
      Link: http://lkml.kernel.org/r/20181008190341.5e396491@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1536927045-23536-30-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 5369d7fac797..a9134d1910b9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -49,7 +49,7 @@
 #include <linux/sched/hotplug.h>
 #include <linux/sched/task_stack.h>
 #include <linux/percpu.h>
-#include <linux/bootmem.h>
+#include <linux/memblock.h>
 #include <linux/err.h>
 #include <linux/nmi.h>
 #include <linux/tboot.h>

commit 0b13bec787dccca96f8c431da732657ae01baf9a
Author: Pu Wen <puwen@hygon.cn>
Date:   Sun Sep 23 17:34:32 2018 +0800

    x86/smpboot: Do not use BSP INIT delay and MWAIT to idle on Dhyana
    
    The Hygon Dhyana CPU uses no delay in smp_quirk_init_udelay(), and does
    HLT on idle just like AMD does.
    
    Signed-off-by: Pu Wen <puwen@hygon.cn>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: bp@alien8.de
    Cc: tglx@linutronix.de
    Cc: mingo@redhat.com
    Cc: hpa@zytor.com
    Cc: x86@kernel.org
    Cc: thomas.lendacky@amd.com
    Link: https://lkml.kernel.org/r/87000fa82e273f5967c908448414228faf61e077.1537533369.git.puwen@hygon.cn

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f02ecaf97904..5369d7fac797 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -676,6 +676,7 @@ static void __init smp_quirk_init_udelay(void)
 
 	/* if modern processor, use no delay */
 	if (((boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) && (boot_cpu_data.x86 == 6)) ||
+	    ((boot_cpu_data.x86_vendor == X86_VENDOR_HYGON) && (boot_cpu_data.x86 >= 0x18)) ||
 	    ((boot_cpu_data.x86_vendor == X86_VENDOR_AMD) && (boot_cpu_data.x86 >= 0xF))) {
 		init_udelay = 0;
 		return;
@@ -1592,7 +1593,8 @@ static inline void mwait_play_dead(void)
 	void *mwait_ptr;
 	int i;
 
-	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD ||
+	    boot_cpu_data.x86_vendor == X86_VENDOR_HYGON)
 		return;
 	if (!this_cpu_has(X86_FEATURE_MWAIT))
 		return;

commit f2701b77bbd992f3df4631de8493f21db0830452
Merge: 18b57ce2eb8c acb1872577b3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Aug 5 16:39:29 2018 +0200

    Merge 4.18-rc7 into master to pick up the KVM dependcy
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 447ae316670230d7d29430e2cbf1f5db4f49d14c
Author: Nicolai Stange <nstange@suse.de>
Date:   Sun Jul 29 12:15:33 2018 +0200

    x86: Don't include linux/irq.h from asm/hardirq.h
    
    The next patch in this series will have to make the definition of
    irq_cpustat_t available to entering_irq().
    
    Inclusion of asm/hardirq.h into asm/apic.h would cause circular header
    dependencies like
    
      asm/smp.h
        asm/apic.h
          asm/hardirq.h
            linux/irq.h
              linux/topology.h
                linux/smp.h
                  asm/smp.h
    
    or
    
      linux/gfp.h
        linux/mmzone.h
          asm/mmzone.h
            asm/mmzone_64.h
              asm/smp.h
                asm/apic.h
                  asm/hardirq.h
                    linux/irq.h
                      linux/irqdesc.h
                        linux/kobject.h
                          linux/sysfs.h
                            linux/kernfs.h
                              linux/idr.h
                                linux/gfp.h
    
    and others.
    
    This causes compilation errors because of the header guards becoming
    effective in the second inclusion: symbols/macros that had been defined
    before wouldn't be available to intermediate headers in the #include chain
    anymore.
    
    A possible workaround would be to move the definition of irq_cpustat_t
    into its own header and include that from both, asm/hardirq.h and
    asm/apic.h.
    
    However, this wouldn't solve the real problem, namely asm/harirq.h
    unnecessarily pulling in all the linux/irq.h cruft: nothing in
    asm/hardirq.h itself requires it. Also, note that there are some other
    archs, like e.g. arm64, which don't have that #include in their
    asm/hardirq.h.
    
    Remove the linux/irq.h #include from x86' asm/hardirq.h.
    
    Fix resulting compilation errors by adding appropriate #includes to *.c
    files as needed.
    
    Note that some of these *.c files could be cleaned up a bit wrt. to their
    set of #includes, but that should better be done from separate patches, if
    at all.
    
    Signed-off-by: Nicolai Stange <nstange@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7f7def989fb0..f5d30c68fd09 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -80,6 +80,7 @@
 #include <asm/intel-family.h>
 #include <asm/cpu_device_id.h>
 #include <asm/spec-ctrl.h>
+#include <asm/hw_irq.h>
 
 /* representing HT siblings of each logical CPU */
 DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_sibling_map);

commit 4fb5f58e8d191f7c81637ad81284e4848afb4244
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Mon Jul 2 23:49:54 2018 -0700

    x86/mm/32: Initialize the CR4 shadow before __flush_tlb_all()
    
    On 32-bit kernels, __flush_tlb_all() may have read the CR4 shadow before the
    initialization of CR4 shadow in cpu_init().
    
    Fix it by adding an explicit cr4_init_shadow() call into start_secondary()
    which is the first function called on non-boot SMP CPUs - ahead of the
    __flush_tlb_all() call.
    
    ( This is somewhat of a layering violation, but start_secondary() does
      CR4 bootstrap in the PCID case anyway. )
    
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/b07b6ae9-4b57-4b40-b9bc-50c2c67f1d91@default
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c2f7d1d2a5c3..db9656e13ea0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -221,6 +221,11 @@ static void notrace start_secondary(void *unused)
 #ifdef CONFIG_X86_32
 	/* switch away from the initial page table */
 	load_cr3(swapper_pg_dir);
+	/*
+	 * Initialize the CR4 shadow before doing anything that could
+	 * try to read it.
+	 */
+	cr4_init_shadow();
 	__flush_tlb_all();
 #endif
 	load_current_idt();

commit f048c399e0f7490ab7296bc2c255d37eb14a9675
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jun 21 10:37:20 2018 +0200

    x86/topology: Provide topology_smt_supported()
    
    Provide information whether SMT is supoorted by the CPUs. Preparatory patch
    for SMT control mechanism.
    
    Suggested-by: Dave Hansen <dave.hansen@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f9c731240142..7f7def989fb0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -274,6 +274,14 @@ bool topology_is_primary_thread(unsigned int cpu)
 	return apic_id_is_primary_thread(per_cpu(x86_cpu_to_apicid, cpu));
 }
 
+/**
+ * topology_smt_supported - Check whether SMT is supported by the CPUs
+ */
+bool topology_smt_supported(void)
+{
+	return smp_num_siblings > 1;
+}
+
 /**
  * topology_phys_to_logical_pkg - Map a physical package id to a logical
  *

commit 6a4d2657e048f096c7ffcad254010bd94891c8c0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 29 17:50:22 2018 +0200

    x86/smp: Provide topology_is_primary_thread()
    
    If the CPU is supporting SMT then the primary thread can be found by
    checking the lower APIC ID bits for zero. smp_num_siblings is used to build
    the mask for the APIC ID bits which need to be taken into account.
    
    This uses the MPTABLE or ACPI/MADT supplied APIC ID, which can be different
    than the initial APIC ID in CPUID. But according to AMD the lower bits have
    to be consistent. Intel gave a tentative confirmation as well.
    
    Preparatory patch to support disabling SMT at boot/runtime.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c2f7d1d2a5c3..f9c731240142 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -265,6 +265,15 @@ static void notrace start_secondary(void *unused)
 	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
 }
 
+/**
+ * topology_is_primary_thread - Check whether CPU is the primary SMT thread
+ * @cpu:	CPU to check
+ */
+bool topology_is_primary_thread(unsigned int cpu)
+{
+	return apic_id_is_primary_thread(per_cpu(x86_cpu_to_apicid, cpu));
+}
+
 /**
  * topology_phys_to_logical_pkg - Map a physical package id to a logical
  *

commit 5cef8c2a2289117b7f65de4313b7157578ec1a71
Merge: f7f4e7fc6c51 e4e961e36f06
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 18:19:18 2018 -0700

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Ingo Molnar:
    
     - Centaur CPU updates (David Wang)
    
     - AMD and other CPU topology enumeration improvements and fixes
       (Borislav Petkov, Thomas Gleixner, Suravee Suthikulpanit)
    
     - Continued 5-level paging work (Kirill A. Shutemov)
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Mark __pgtable_l5_enabled __initdata
      x86/mm: Mark p4d_offset() __always_inline
      x86/mm: Introduce the 'no5lvl' kernel parameter
      x86/mm: Stop pretending pgtable_l5_enabled is a variable
      x86/mm: Unify pgtable_l5_enabled usage in early boot code
      x86/boot/compressed/64: Fix trampoline page table address calculation
      x86/CPU: Move x86_cpuinfo::x86_max_cores assignment to detect_num_cpu_cores()
      x86/Centaur: Report correct CPU/cache topology
      x86/CPU: Move cpu_detect_cache_sizes() into init_intel_cacheinfo()
      x86/CPU: Make intel_num_cpu_cores() generic
      x86/CPU: Move cpu local function declarations to local header
      x86/CPU/AMD: Derive CPU topology from CPUID function 0xB when available
      x86/CPU: Modify detect_extended_topology() to return result
      x86/CPU/AMD: Calculate last level cache ID from number of sharing threads
      x86/CPU: Rename intel_cacheinfo.c to cacheinfo.c
      perf/events/amd/uncore: Fix amd_uncore_llc ID to use pre-defined cpu_llc_id
      x86/CPU/AMD: Have smp_num_siblings and cpu_llc_id always be present
      x86/Centaur: Initialize supported CPU features properly

commit 177bfd725bd1b67c7254248cf19f0465d493e631
Merge: 9305bd6ca7b4 fed71f7d9879 7dec80ccbe31
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat May 19 08:18:56 2018 +0200

    Merge branches 'x86/urgent' and 'core/urgent' into x86/boot, to pick up fixes and avoid conflicts
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1f50ddb4f4189243c05926b842dc1a0332195f31
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 9 21:53:09 2018 +0200

    x86/speculation: Handle HT correctly on AMD
    
    The AMD64_LS_CFG MSR is a per core MSR on Family 17H CPUs. That means when
    hyperthreading is enabled the SSBD bit toggle needs to take both cores into
    account. Otherwise the following situation can happen:
    
    CPU0            CPU1
    
    disable SSB
                    disable SSB
                    enable  SSB <- Enables it for the Core, i.e. for CPU0 as well
    
    So after the SSB enable on CPU1 the task on CPU0 runs with SSB enabled
    again.
    
    On Intel the SSBD control is per core as well, but the synchronization
    logic is implemented behind the per thread SPEC_CTRL MSR. It works like
    this:
    
      CORE_SPEC_CTRL = THREAD0_SPEC_CTRL | THREAD1_SPEC_CTRL
    
    i.e. if one of the threads enables a mitigation then this affects both and
    the mitigation is only disabled in the core when both threads disabled it.
    
    Add the necessary synchronization logic for AMD family 17H. Unfortunately
    that requires a spinlock to serialize the access to the MSR, but the locks
    are only shared between siblings.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0f1cbb042f49..9dd324ae4832 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -79,6 +79,7 @@
 #include <asm/qspinlock.h>
 #include <asm/intel-family.h>
 #include <asm/cpu_device_id.h>
+#include <asm/spec-ctrl.h>
 
 /* Number of siblings per CPU package */
 int smp_num_siblings = 1;
@@ -244,6 +245,8 @@ static void notrace start_secondary(void *unused)
 	 */
 	check_tsc_sync_target();
 
+	speculative_store_bypass_ht_init();
+
 	/*
 	 * Lock vector_lock, set CPU online and bring the vector
 	 * allocator online. Online must be set with vector_lock held
@@ -1292,6 +1295,8 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	set_mtrr_aps_delayed_init();
 
 	smp_quirk_init_udelay();
+
+	speculative_store_bypass_ht_init();
 }
 
 void arch_enable_nonboot_cpus_begin(void)

commit f8b64d08dde2714c62751d18ba77f4aeceb161d3
Author: Borislav Petkov <bpetkov@suse.de>
Date:   Fri Apr 27 16:34:34 2018 -0500

    x86/CPU/AMD: Have smp_num_siblings and cpu_llc_id always be present
    
    Move smp_num_siblings and cpu_llc_id to cpu/common.c so that they're
    always present as symbols and not only in the CONFIG_SMP case. Then,
    other code using them doesn't need ugly ifdeffery anymore. Get rid of
    some ifdeffery.
    
    Signed-off-by: Borislav Petkov <bpetkov@suse.de>
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1524864877-111962-2-git-send-email-suravee.suthikulpanit@amd.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ff99e2b6fc54..91d48f3eeda8 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -78,13 +78,6 @@
 #include <asm/misc.h>
 #include <asm/qspinlock.h>
 
-/* Number of siblings per CPU package */
-int smp_num_siblings = 1;
-EXPORT_SYMBOL(smp_num_siblings);
-
-/* Last level cache ID of each logical CPU */
-DEFINE_PER_CPU_READ_MOSTLY(u16, cpu_llc_id) = BAD_APICID;
-
 /* representing HT siblings of each logical CPU */
 DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_sibling_map);
 EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);

commit da6fa7ef67f07108a1b0cb9fd9e7fcaabd39c051
Author: Yazen Ghannam <yazen.ghannam@amd.com>
Date:   Tue Apr 3 09:02:28 2018 -0500

    x86/smpboot: Don't use mwait_play_dead() on AMD systems
    
    Recent AMD systems support using MWAIT for C1 state. However, MWAIT will
    not allow deeper cstates than C1 on current systems.
    
    play_dead() expects to use the deepest state available.  The deepest state
    available on AMD systems is reached through SystemIO or HALT. If MWAIT is
    available, it is preferred over the other methods, so the CPU never reaches
    the deepest possible state.
    
    Don't try to use MWAIT to play_dead() on AMD systems. Instead, use CPUIDLE
    to enter the deepest state advertised by firmware. If CPUIDLE is not
    available then fallback to HALT.
    
    Signed-off-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: stable@vger.kernel.org
    Cc: Yazen Ghannam <Yazen.Ghannam@amd.com>
    Link: https://lkml.kernel.org/r/20180403140228.58540-1-Yazen.Ghannam@amd.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 45175b81dd5b..0f1cbb042f49 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1571,6 +1571,8 @@ static inline void mwait_play_dead(void)
 	void *mwait_ptr;
 	int i;
 
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
+		return;
 	if (!this_cpu_has(X86_FEATURE_MWAIT))
 		return;
 	if (!this_cpu_has(X86_FEATURE_CLFLUSH))

commit 1340ccfa9a9afefdbab90d7935d4ed19817e37c2
Author: Alison Schofield <alison.schofield@intel.com>
Date:   Fri Apr 6 17:21:30 2018 -0700

    x86,sched: Allow topologies where NUMA nodes share an LLC
    
    Intel's Skylake Server CPUs have a different LLC topology than previous
    generations. When in Sub-NUMA-Clustering (SNC) mode, the package is divided
    into two "slices", each containing half the cores, half the LLC, and one
    memory controller and each slice is enumerated to Linux as a NUMA
    node. This is similar to how the cores and LLC were arranged for the
    Cluster-On-Die (CoD) feature.
    
    CoD allowed the same cache line to be present in each half of the LLC.
    But, with SNC, each line is only ever present in *one* slice. This means
    that the portion of the LLC *available* to a CPU depends on the data being
    accessed:
    
        Remote socket: entire package LLC is shared
        Local socket->local slice: data goes into local slice LLC
        Local socket->remote slice: data goes into remote-slice LLC. Slightly
                                    higher latency than local slice LLC.
    
    The biggest implication from this is that a process accessing all
    NUMA-local memory only sees half the LLC capacity.
    
    The CPU describes its cache hierarchy with the CPUID instruction. One of
    the CPUID leaves enumerates the "logical processors sharing this
    cache". This information is used for scheduling decisions so that tasks
    move more freely between CPUs sharing the cache.
    
    But, the CPUID for the SNC configuration discussed above enumerates the LLC
    as being shared by the entire package. This is not 100% precise because the
    entire cache is not usable by all accesses. But, it *is* the way the
    hardware enumerates itself, and this is not likely to change.
    
    The userspace visible impact of all the above is that the sysfs info
    reports the entire LLC as being available to the entire package. As noted
    above, this is not true for local socket accesses. This patch does not
    correct the sysfs info. It is the same, pre and post patch.
    
    The current code emits the following warning:
    
     sched: CPU #3's llc-sibling CPU #0 is not on the same node! [node: 1 != 0]. Ignoring dependency.
    
    The warning is coming from the topology_sane() check in smpboot.c because
    the topology is not matching the expectations of the model for obvious
    reasons.
    
    To fix this, add a vendor and model specific check to never call
    topology_sane() for these systems. Also, just like "Cluster-on-Die" disable
    the "coregroup" sched_domain_topology_level and use NUMA information from
    the SRAT alone.
    
    This is OK at least on the hardware we are immediately concerned about
    because the LLC sharing happens at both the slice and at the package level,
    which are also NUMA boundaries.
    
    Signed-off-by: Alison Schofield <alison.schofield@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: brice.goglin@gmail.com
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@linux.intel.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Link: https://lkml.kernel.org/r/20180407002130.GA18984@alison-desk.jf.intel.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ff99e2b6fc54..45175b81dd5b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -77,6 +77,8 @@
 #include <asm/i8259.h>
 #include <asm/misc.h>
 #include <asm/qspinlock.h>
+#include <asm/intel-family.h>
+#include <asm/cpu_device_id.h>
 
 /* Number of siblings per CPU package */
 int smp_num_siblings = 1;
@@ -390,15 +392,47 @@ static bool match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 	return false;
 }
 
+/*
+ * Define snc_cpu[] for SNC (Sub-NUMA Cluster) CPUs.
+ *
+ * These are Intel CPUs that enumerate an LLC that is shared by
+ * multiple NUMA nodes. The LLC on these systems is shared for
+ * off-package data access but private to the NUMA node (half
+ * of the package) for on-package access.
+ *
+ * CPUID (the source of the information about the LLC) can only
+ * enumerate the cache as being shared *or* unshared, but not
+ * this particular configuration. The CPU in this case enumerates
+ * the cache to be shared across the entire package (spanning both
+ * NUMA nodes).
+ */
+
+static const struct x86_cpu_id snc_cpu[] = {
+	{ X86_VENDOR_INTEL, 6, INTEL_FAM6_SKYLAKE_X },
+	{}
+};
+
 static bool match_llc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 {
 	int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
 
-	if (per_cpu(cpu_llc_id, cpu1) != BAD_APICID &&
-	    per_cpu(cpu_llc_id, cpu1) == per_cpu(cpu_llc_id, cpu2))
-		return topology_sane(c, o, "llc");
+	/* Do not match if we do not have a valid APICID for cpu: */
+	if (per_cpu(cpu_llc_id, cpu1) == BAD_APICID)
+		return false;
 
-	return false;
+	/* Do not match if LLC id does not match: */
+	if (per_cpu(cpu_llc_id, cpu1) != per_cpu(cpu_llc_id, cpu2))
+		return false;
+
+	/*
+	 * Allow the SNC topology without warning. Return of false
+	 * means 'c' does not share the LLC of 'o'. This will be
+	 * reflected to userspace.
+	 */
+	if (!topology_same_node(c, o) && x86_match_cpu(snc_cpu))
+		return false;
+
+	return topology_sane(c, o, "llc");
 }
 
 /*
@@ -456,7 +490,8 @@ static struct sched_domain_topology_level x86_topology[] = {
 
 /*
  * Set if a package/die has multiple NUMA nodes inside.
- * AMD Magny-Cours and Intel Cluster-on-Die have this.
+ * AMD Magny-Cours, Intel Cluster-on-Die, and Intel
+ * Sub-NUMA Clustering have this.
  */
 static bool x86_has_numa_in_package;
 

commit 4596749339e06dc7a424fc08a15eded850ed78b7
Author: Samuel Neves <sneves@dei.uc.pt>
Date:   Wed Feb 21 20:50:36 2018 +0000

    x86/topology: Update the 'cpu cores' field in /proc/cpuinfo correctly across CPU hotplug operations
    
    Without this fix, /proc/cpuinfo will display an incorrect amount
    of CPU cores, after bringing them offline and online again, as
    exemplified below:
    
      $ cat /proc/cpuinfo | grep cores
      cpu cores     : 4
      cpu cores     : 8
      cpu cores     : 8
      cpu cores     : 20
      cpu cores     : 4
      cpu cores     : 3
      cpu cores     : 2
      cpu cores     : 2
    
    This patch fixes this by always zeroing the booted_cores variable
    upon turning off a logical CPU.
    
    Tested-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Samuel Neves <sneves@dei.uc.pt>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: jgross@suse.com
    Cc: luto@kernel.org
    Cc: prarit@redhat.com
    Cc: vkuznets@redhat.com
    Link: http://lkml.kernel.org/r/20180221205036.5244-1-sneves@dei.uc.pt
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9eee25d07586..ff99e2b6fc54 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1437,6 +1437,7 @@ static void remove_siblinginfo(int cpu)
 	cpumask_clear(topology_sibling_cpumask(cpu));
 	cpumask_clear(topology_core_cpumask(cpu));
 	c->cpu_core_id = 0;
+	c->booted_cores = 0;
 	cpumask_clear_cpu(cpu, cpu_sibling_setup_mask);
 	recompute_smt_state();
 }

commit 63e708f826bb21470155d37b103a75d8a9e25b18
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Wed Feb 7 18:49:23 2018 -0500

    x86/xen: Calculate __max_logical_packages on PV domains
    
    The kernel panics on PV domains because native_smp_cpus_done() is
    only called for HVM domains.
    
    Calculate __max_logical_packages for PV domains.
    
    Fixes: b4c0a7326f5d ("x86/smpboot: Fix __max_logical_packages estimate")
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Tested-and-reported-by: Simon Gaiser <simon@invisiblethingslab.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: xen-devel@lists.xenproject.org
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index cfc61e1d45e2..9eee25d07586 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1281,11 +1281,10 @@ void __init native_smp_prepare_boot_cpu(void)
 	cpu_set_state_online(me);
 }
 
-void __init native_smp_cpus_done(unsigned int max_cpus)
+void __init calculate_max_logical_packages(void)
 {
 	int ncpus;
 
-	pr_debug("Boot done\n");
 	/*
 	 * Today neither Intel nor AMD support heterogenous systems so
 	 * extrapolate the boot cpu's data to all packages.
@@ -1293,6 +1292,13 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 	ncpus = cpu_data(0).booted_cores * topology_max_smt_threads();
 	__max_logical_packages = DIV_ROUND_UP(nr_cpu_ids, ncpus);
 	pr_info("Max logical packages: %u\n", __max_logical_packages);
+}
+
+void __init native_smp_cpus_done(unsigned int max_cpus)
+{
+	pr_debug("Boot done\n");
+
+	calculate_max_logical_packages();
 
 	if (x86_has_numa_in_package)
 		set_sched_topology(x86_numa_in_package_topology);

commit 295cc7eb314eb3321fb6d67ca6f7305f5c50d10f
Author: Masayoshi Mizuma <m.mizuma@jp.fujitsu.com>
Date:   Thu Feb 8 09:19:08 2018 -0500

    x86/smpboot: Fix uncore_pci_remove() indexing bug when hot-removing a physical CPU
    
    When a physical CPU is hot-removed, the following warning messages
    are shown while the uncore device is removed in uncore_pci_remove():
    
      WARNING: CPU: 120 PID: 5 at arch/x86/events/intel/uncore.c:988
      uncore_pci_remove+0xf1/0x110
      ...
      CPU: 120 PID: 5 Comm: kworker/u1024:0 Not tainted 4.15.0-rc8 #1
      Workqueue: kacpi_hotplug acpi_hotplug_work_fn
      ...
      Call Trace:
      pci_device_remove+0x36/0xb0
      device_release_driver_internal+0x145/0x210
      pci_stop_bus_device+0x76/0xa0
      pci_stop_root_bus+0x44/0x60
      acpi_pci_root_remove+0x1f/0x80
      acpi_bus_trim+0x54/0x90
      acpi_bus_trim+0x2e/0x90
      acpi_device_hotplug+0x2bc/0x4b0
      acpi_hotplug_work_fn+0x1a/0x30
      process_one_work+0x141/0x340
      worker_thread+0x47/0x3e0
      kthread+0xf5/0x130
    
    When uncore_pci_remove() runs, it tries to get the package ID to
    clear the value of uncore_extra_pci_dev[].dev[] by using
    topology_phys_to_logical_pkg(). The warning messesages are
    shown because topology_phys_to_logical_pkg() returns -1.
    
      arch/x86/events/intel/uncore.c:
      static void uncore_pci_remove(struct pci_dev *pdev)
      {
      ...
              phys_id = uncore_pcibus_to_physid(pdev->bus);
      ...
                      pkg = topology_phys_to_logical_pkg(phys_id); // returns -1
                      for (i = 0; i < UNCORE_EXTRA_PCI_DEV_MAX; i++) {
                              if (uncore_extra_pci_dev[pkg].dev[i] == pdev) {
                                      uncore_extra_pci_dev[pkg].dev[i] = NULL;
                                      break;
                              }
                      }
                      WARN_ON_ONCE(i >= UNCORE_EXTRA_PCI_DEV_MAX); // <=========== HERE!!
    
    topology_phys_to_logical_pkg() tries to find
    cpuinfo_x86->phys_proc_id that matches the phys_pkg argument.
    
      arch/x86/kernel/smpboot.c:
      int topology_phys_to_logical_pkg(unsigned int phys_pkg)
      {
              int cpu;
    
              for_each_possible_cpu(cpu) {
                      struct cpuinfo_x86 *c = &cpu_data(cpu);
    
                      if (c->initialized && c->phys_proc_id == phys_pkg)
                              return c->logical_proc_id;
              }
              return -1;
      }
    
    However, the phys_proc_id was already set to 0 by remove_siblinginfo()
    when the CPU was offlined.
    
    So, topology_phys_to_logical_pkg() cannot find the correct
    logical_proc_id and always returns -1.
    
    As the result, uncore_pci_remove() calls WARN_ON_ONCE() and the warning
    messages are shown.
    
    What is worse is that the bogus 'pkg' index results in two bugs:
    
     - We dereference uncore_extra_pci_dev[] with a negative index
     - We fail to clean up a stale pointer in uncore_extra_pci_dev[][]
    
    To fix these bugs, remove the clearing of ->phys_proc_id from remove_siblinginfo().
    
    This should not cause any problems, because ->phys_proc_id is not
    used after it is hot-removed and it is re-set while hot-adding.
    
    Signed-off-by: Masayoshi Mizuma <m.mizuma@jp.fujitsu.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: yasu.isimatu@gmail.com
    Cc: <stable@vger.kernel.org>
    Fixes: 30bb9811856f ("x86/topology: Avoid wasting 128k for package id array")
    Link: http://lkml.kernel.org/r/ed738d54-0f01-b38b-b794-c31dc118c207@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6f27facbaa9b..cfc61e1d45e2 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1430,7 +1430,6 @@ static void remove_siblinginfo(int cpu)
 	cpumask_clear(cpu_llc_shared_mask(cpu));
 	cpumask_clear(topology_sibling_cpumask(cpu));
 	cpumask_clear(topology_core_cpumask(cpu));
-	c->phys_proc_id = 0;
 	c->cpu_core_id = 0;
 	cpumask_clear_cpu(cpu, cpu_sibling_setup_mask);
 	recompute_smt_state();

commit 3ccabd6d9d9b0da5780e0386b4bf7c5f07669e37
Merge: 5289d3005a36 782bf20c2a17
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 13:01:09 2018 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc cleanups"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Remove unused IOMMU_STRESS Kconfig
      x86/extable: Mark exception handler functions visible
      x86/timer: Don't inline __const_udelay
      x86/headers: Remove duplicate #includes

commit e348caef8b4a161cc27bec8f7500b7e100370ef1
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Nov 27 09:11:44 2017 +0100

    x86/platform: Control warm reset setup via legacy feature flag
    
    Allow to turn off the setup of BIOS-managed warm reset via a new flag in
    x86_legacy_features. Besides the UV1, the upcoming jailhose guest support
    needs this switched off.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: jailhouse-dev@googlegroups.com
    Link: https://lkml.kernel.org/r/44376558129d70a2c1527959811371ef4b82e829.1511770314.git.jan.kiszka@siemens.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ed556d50d7ed..9adcae1b135c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -934,7 +934,7 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 	 * the targeted processor.
 	 */
 
-	if (get_uv_system_type() != UV_NON_UNIQUE_APIC) {
+	if (x86_platform.legacy.warm_reset) {
 
 		pr_debug("Setting warm reset code and vector.\n");
 
@@ -1006,7 +1006,7 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 	/* mark "stuck" area as not stuck */
 	*trampoline_status = 0;
 
-	if (get_uv_system_type() != UV_NON_UNIQUE_APIC) {
+	if (x86_platform.legacy.warm_reset) {
 		/*
 		 * Cleanup possible dangling ends...
 		 */

commit 52c90f2d32bfa7d6eccd66a56c44ace1f78fbadd
Merge: cea92e843e40 7f414195b0c3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 31 13:03:05 2017 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 page table isolation fixes from Thomas Gleixner:
     "Four patches addressing the PTI fallout as discussed and debugged
      yesterday:
    
       - Remove stale and pointless TLB flush invocations from the hotplug
         code
    
       - Remove stale preempt_disable/enable from __native_flush_tlb()
    
       - Plug the memory leak in the write_ldt() error path"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/ldt: Make LDT pgtable free conditional
      x86/ldt: Plug memory leak in error path
      x86/mm: Remove preempt_disable/enable() from __native_flush_tlb()
      x86/smpboot: Remove stale TLB flush invocations

commit 322f8b8b340c824aef891342b0f5795d15e11562
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Dec 30 22:13:53 2017 +0100

    x86/smpboot: Remove stale TLB flush invocations
    
    smpboot_setup_warm_reset_vector() and smpboot_restore_warm_reset_vector()
    invoke local_flush_tlb() for no obvious reason.
    
    Digging in history revealed that the original code in the 2.1 era added
    those because the code manipulated a swapper_pg_dir pagetable entry. The
    pagetable manipulation was removed long ago in the 2.3 timeframe, but the
    TLB flush invocations stayed around forever.
    
    Remove them along with the pointless pr_debug()s which come from the same 2.1
    change.
    
    Reported-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: <stable@vger.kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linuxfoundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20171230211829.586548655@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 33d6000265aa..c3402fc30865 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -128,25 +128,16 @@ static inline void smpboot_setup_warm_reset_vector(unsigned long start_eip)
 	spin_lock_irqsave(&rtc_lock, flags);
 	CMOS_WRITE(0xa, 0xf);
 	spin_unlock_irqrestore(&rtc_lock, flags);
-	local_flush_tlb();
-	pr_debug("1.\n");
 	*((volatile unsigned short *)phys_to_virt(TRAMPOLINE_PHYS_HIGH)) =
 							start_eip >> 4;
-	pr_debug("2.\n");
 	*((volatile unsigned short *)phys_to_virt(TRAMPOLINE_PHYS_LOW)) =
 							start_eip & 0xf;
-	pr_debug("3.\n");
 }
 
 static inline void smpboot_restore_warm_reset_vector(void)
 {
 	unsigned long flags;
 
-	/*
-	 * Install writable page 0 entry to set BIOS data area.
-	 */
-	local_flush_tlb();
-
 	/*
 	 * Paranoid:  Set warm reset code and vector here back
 	 * to default values.

commit caf9a82657b313106aae8f4a35936c116a152299
Merge: 9c294ec08408 f6c4fd506cb6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 23 11:53:04 2017 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 PTI preparatory patches from Thomas Gleixner:
     "Todays Advent calendar window contains twentyfour easy to digest
      patches. The original plan was to have twenty three matching the date,
      but a late fixup made that moot.
    
       - Move the cpu_entry_area mapping out of the fixmap into a separate
         address space. That's necessary because the fixmap becomes too big
         with NRCPUS=8192 and this caused already subtle and hard to
         diagnose failures.
    
         The top most patch is fresh from today and cures a brain slip of
         that tall grumpy german greybeard, who ignored the intricacies of
         32bit wraparounds.
    
       - Limit the number of CPUs on 32bit to 64. That's insane big already,
         but at least it's small enough to prevent address space issues with
         the cpu_entry_area map, which have been observed and debugged with
         the fixmap code
    
       - A few TLB flush fixes in various places plus documentation which of
         the TLB functions should be used for what.
    
       - Rename the SYSENTER stack to CPU_ENTRY_AREA stack as it is used for
         more than sysenter now and keeping the name makes backtraces
         confusing.
    
       - Prevent LDT inheritance on exec() by moving it to arch_dup_mmap(),
         which is only invoked on fork().
    
       - Make vysycall more robust.
    
       - A few fixes and cleanups of the debug_pagetables code. Check
         PAGE_PRESENT instead of checking the PTE for 0 and a cleanup of the
         C89 initialization of the address hint array which already was out
         of sync with the index enums.
    
       - Move the ESPFIX init to a different place to prepare for PTI.
    
       - Several code moves with no functional change to make PTI
         integration simpler and header files less convoluted.
    
       - Documentation fixes and clarifications"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      x86/cpu_entry_area: Prevent wraparound in setup_cpu_entry_area_ptes() on 32bit
      init: Invoke init_espfix_bsp() from mm_init()
      x86/cpu_entry_area: Move it out of the fixmap
      x86/cpu_entry_area: Move it to a separate unit
      x86/mm: Create asm/invpcid.h
      x86/mm: Put MMU to hardware ASID translation in one place
      x86/mm: Remove hard-coded ASID limit checks
      x86/mm: Move the CR3 construction functions to tlbflush.h
      x86/mm: Add comments to clarify which TLB-flush functions are supposed to flush what
      x86/mm: Remove superfluous barriers
      x86/mm: Use __flush_tlb_one() for kernel memory
      x86/microcode: Dont abuse the TLB-flush interface
      x86/uv: Use the right TLB-flush API
      x86/entry: Rename SYSENTER_stack to CPU_ENTRY_AREA_entry_stack
      x86/doc: Remove obvious weirdnesses from the x86 MM layout documentation
      x86/mm/64: Improve the memory map documentation
      x86/ldt: Prevent LDT inheritance on exec
      x86/ldt: Rework locking
      arch, mm: Allow arch_dup_mmap() to fail
      x86/vsyscall/64: Warn and fail vsyscall emulation in NATIVE mode
      ...

commit 613e396bc0d4c7604fba23256644e78454c68cf6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Dec 17 10:56:29 2017 +0100

    init: Invoke init_espfix_bsp() from mm_init()
    
    init_espfix_bsp() needs to be invoked before the page table isolation
    initialization. Move it into mm_init() which is the place where pti_init()
    will be added.
    
    While at it get rid of the #ifdeffery and provide proper stub functions.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d56c1d209283..33d6000265aa 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -990,12 +990,8 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 	initial_code = (unsigned long)start_secondary;
 	initial_stack  = idle->thread.sp;
 
-	/*
-	 * Enable the espfix hack for this CPU
-	*/
-#ifdef CONFIG_X86_ESPFIX64
+	/* Enable the espfix hack for this CPU */
 	init_espfix_ap(cpu);
-#endif
 
 	/* So we see what's up */
 	announce_cpu(cpu, apicid);

commit 0fd2e9c53d82704a3ba87ea1980ec515188c5316
Merge: 1784f9144b14 1e4c4f610f77
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Dec 1 10:32:48 2017 +0100

    Merge commit 'upstream-x86-entry' into WIP.x86/mm
    
    Pull in a minimal set of v4.15 entry code changes, for a base for the MM isolation patches.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 81bf665d00baf1aef01118c6c9e51520e57c0757
Author: Pravin Shedge <pravin.shedge4linux@gmail.com>
Date:   Tue Dec 12 02:12:31 2017 +0530

    x86/headers: Remove duplicate #includes
    
    These duplicate includes have been found with scripts/checkincludes.pl but
    they have been removed manually to avoid removing false positives.
    
    Signed-off-by: Pravin Shedge <pravin.shedge4linux@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: ard.biesheuvel@linaro.org
    Cc: boris.ostrovsky@oracle.com
    Cc: geert@linux-m68k.org
    Cc: jgross@suse.com
    Cc: linux-efi@vger.kernel.org
    Cc: luto@kernel.org
    Cc: matt@codeblueprint.co.uk
    Cc: thomas.lendacky@amd.com
    Cc: tim.c.chen@linux.intel.com
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1513024951-9221-1-git-send-email-pravin.shedge4linux@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 05a97d5fe298..d44b64d571b4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -75,7 +75,6 @@
 #include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>
 #include <asm/i8259.h>
-#include <asm/realmode.h>
 #include <asm/misc.h>
 #include <asm/qspinlock.h>
 

commit 947134d9b00f342415af7eddd42a5fce7262a1b9
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Mon Dec 4 11:45:21 2017 -0500

    x86/smpboot: Do not use smp_num_siblings in __max_logical_packages calculation
    
    Documentation/x86/topology.txt defines smp_num_siblings as "The number of
    threads in a core".  Since commit bbb65d2d365e ("x86: use cpuid vector 0xb
    when available for detecting cpu topology") smp_num_siblings is the
    maximum number of threads in a core.  If Simultaneous MultiThreading
    (SMT) is disabled on a system, smp_num_siblings is 2 and not 1 as
    expected.
    
    Use topology_max_smt_threads(), which contains the active numer of threads,
    in the __max_logical_packages calculation.
    
    On a single socket, single core, single thread system __max_smt_threads has
    not been updated when the __max_logical_packages calculation happens, so its
    zero which makes the package estimate fail. Initialize it to one, which is
    the minimum number of threads on a core.
    
    [ tglx: Folded the __max_smt_threads fix in ]
    
    Fixes: b4c0a7326f5d ("x86/smpboot: Fix __max_logical_packages estimate")
    Reported-by: Jakub Kicinski <kubakici@wp.pl>
    Signed-off-by: Prarit Bhargava <prarit@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Jakub Kicinski <kubakici@wp.pl>
    Cc: netdev@vger.kernel.org
    Cc: "netdev@vger.kernel.org"
    Cc: Clark Williams <williams@redhat.com>
    Link: https://lkml.kernel.org/r/20171204164521.17870-1-prarit@redhat.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 05a97d5fe298..35cb20994e32 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -106,7 +106,7 @@ EXPORT_SYMBOL(__max_logical_packages);
 static unsigned int logical_packages __read_mostly;
 
 /* Maximum number of SMT threads on any online core */
-int __max_smt_threads __read_mostly;
+int __read_mostly __max_smt_threads = 1;
 
 /* Flag to indicate if a complete sched domain rebuild is required */
 bool x86_topology_update;
@@ -1304,7 +1304,7 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 	 * Today neither Intel nor AMD support heterogenous systems so
 	 * extrapolate the boot cpu's data to all packages.
 	 */
-	ncpus = cpu_data(0).booted_cores * smp_num_siblings;
+	ncpus = cpu_data(0).booted_cores * topology_max_smt_threads();
 	__max_logical_packages = DIV_ROUND_UP(nr_cpu_ids, ncpus);
 	pr_info("Max logical packages: %u\n", __max_logical_packages);
 

commit 55d2d0ad2fb4325f615d1950486fbc5e6fba1769
Author: Chunyu Hu <chuhu@redhat.com>
Date:   Mon Nov 27 22:21:39 2017 +0800

    x86/idt: Load idt early in start_secondary
    
    On a secondary, idt is first loaded in cpu_init() with load_current_idt(),
    i.e. no exceptions can be handled before that point.
    
    The conversion of WARN() to use UD requires the IDT being loaded earlier as
    any warning between start_secondary() and load_curren_idt() in cpu_init()
    will result in an unhandled @UD exception and therefore fail the bringup of
    the CPU.
    
    Install the IDT handlers right in start_secondary() before calling cpu_init().
    
    [ tglx: Massaged changelog ]
    
    Fixes: 9a93848fe787 ("x86/debug: Implement __WARN() using UD0")
    Signed-off-by: Chunyu Hu <chuhu@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Cc: peterz@infradead.org
    Cc: bp@alien8.de
    Cc: rostedt@goodmis.org
    Cc: luto@kernel.org
    Link: https://lkml.kernel.org/r/1511792499-4073-1-git-send-email-chuhu@redhat.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3d01df7d7cf6..05a97d5fe298 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -237,7 +237,7 @@ static void notrace start_secondary(void *unused)
 	load_cr3(swapper_pg_dir);
 	__flush_tlb_all();
 #endif
-
+	load_current_idt();
 	cpu_init();
 	x86_cpuinit.early_percpu_clock_init();
 	preempt_disable();

commit b4c0a7326f5dc0ef7a64128b0ae7d081f4b2cbd1
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Tue Nov 14 07:42:57 2017 -0500

    x86/smpboot: Fix __max_logical_packages estimate
    
    A system booted with a small number of cores enabled per package
    panics because the estimate of __max_logical_packages is too low.
    
    This occurs when the total number of active cores across all packages is
    less than the maximum core count for a single package. e.g.:
    
      On a 4 package system with 20 cores/package where only 4 cores are
      enabled on each package, the value of __max_logical_packages is
      calculated as DIV_ROUND_UP(16 / 20) = 1 and not 4.
    
    Calculate __max_logical_packages after the cpu enumeration has completed.
    Use the boot cpu's data to extrapolate the number of packages.
    
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: He Chen <he.chen@linux.intel.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Piotr Luc <piotr.luc@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arvind Yadav <arvind.yadav.cs@gmail.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Mathias Krause <minipli@googlemail.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Link: https://lkml.kernel.org/r/20171114124257.22013-4-prarit@redhat.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index da5e162636fd..3d01df7d7cf6 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -310,12 +310,6 @@ int topology_update_package_map(unsigned int pkg, unsigned int cpu)
 	if (new >= 0)
 		goto found;
 
-	if (logical_packages >= __max_logical_packages) {
-		pr_warn("Package %u of CPU %u exceeds BIOS package data %u.\n",
-			logical_packages, cpu, __max_logical_packages);
-		return -ENOSPC;
-	}
-
 	new = logical_packages++;
 	if (new != pkg) {
 		pr_info("CPU %u Converting physical %u to logical package %u\n",
@@ -326,44 +320,6 @@ int topology_update_package_map(unsigned int pkg, unsigned int cpu)
 	return 0;
 }
 
-static void __init smp_init_package_map(struct cpuinfo_x86 *c, unsigned int cpu)
-{
-	unsigned int ncpus;
-
-	/*
-	 * Today neither Intel nor AMD support heterogenous systems. That
-	 * might change in the future....
-	 *
-	 * While ideally we'd want '* smp_num_siblings' in the below @ncpus
-	 * computation, this won't actually work since some Intel BIOSes
-	 * report inconsistent HT data when they disable HT.
-	 *
-	 * In particular, they reduce the APIC-IDs to only include the cores,
-	 * but leave the CPUID topology to say there are (2) siblings.
-	 * This means we don't know how many threads there will be until
-	 * after the APIC enumeration.
-	 *
-	 * By not including this we'll sometimes over-estimate the number of
-	 * logical packages by the amount of !present siblings, but this is
-	 * still better than MAX_LOCAL_APIC.
-	 *
-	 * We use total_cpus not nr_cpu_ids because nr_cpu_ids can be limited
-	 * on the command line leading to a similar issue as the HT disable
-	 * problem because the hyperthreads are usually enumerated after the
-	 * primary cores.
-	 */
-	ncpus = boot_cpu_data.x86_max_cores;
-	if (!ncpus) {
-		pr_warn("x86_max_cores == zero !?!?");
-		ncpus = 1;
-	}
-
-	__max_logical_packages = DIV_ROUND_UP(total_cpus, ncpus);
-	pr_info("Max logical packages: %u\n", __max_logical_packages);
-
-	topology_update_package_map(c->phys_proc_id, cpu);
-}
-
 void __init smp_store_boot_cpu_info(void)
 {
 	int id = 0; /* CPU 0 */
@@ -371,7 +327,7 @@ void __init smp_store_boot_cpu_info(void)
 
 	*c = boot_cpu_data;
 	c->cpu_index = id;
-	smp_init_package_map(c, id);
+	topology_update_package_map(c->phys_proc_id, id);
 	c->initialized = true;
 }
 
@@ -1341,7 +1297,16 @@ void __init native_smp_prepare_boot_cpu(void)
 
 void __init native_smp_cpus_done(unsigned int max_cpus)
 {
+	int ncpus;
+
 	pr_debug("Boot done\n");
+	/*
+	 * Today neither Intel nor AMD support heterogenous systems so
+	 * extrapolate the boot cpu's data to all packages.
+	 */
+	ncpus = cpu_data(0).booted_cores * smp_num_siblings;
+	__max_logical_packages = DIV_ROUND_UP(nr_cpu_ids, ncpus);
+	pr_info("Max logical packages: %u\n", __max_logical_packages);
 
 	if (x86_has_numa_in_package)
 		set_sched_topology(x86_numa_in_package_topology);

commit 30bb9811856f667042e746d8033883b1091a46ce
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Nov 14 07:42:56 2017 -0500

    x86/topology: Avoid wasting 128k for package id array
    
    Analyzing large early boot allocations unveiled the logical package id
    storage as a prominent memory waste. Since commit 1f12e32f4cd5
    ("x86/topology: Create logical package id") every 64-bit system allocates a
    128k array to convert logical package ids.
    
    This happens because the array is sized for MAX_LOCAL_APIC which is always
    32k on 64bit systems, and it needs 4 bytes for each entry.
    
    This is fairly wasteful, especially for the common case of having only one
    socket, which uses exactly 4 byte out of 128K.
    
    There is no user of the package id map which is performance critical, so
    the lookup is not required to be O(1). Store the logical processor id in
    cpu_data and use a loop based lookup.
    
    To keep the mapping stable accross cpu hotplug operations, add a flag to
    cpu_data which is set when the CPU is brought up the first time. When the
    flag is set, then cpu_data is not reinitialized by copying boot_cpu_data on
    subsequent bringups.
    
    [ tglx: Rename the flag to 'initialized', use proper pointers instead of
            repeated cpu_data(x) evaluation and massage changelog. ]
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: He Chen <he.chen@linux.intel.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Piotr Luc <piotr.luc@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arvind Yadav <arvind.yadav.cs@gmail.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Mathias Krause <minipli@googlemail.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Link: https://lkml.kernel.org/r/20171114124257.22013-3-prarit@redhat.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 5f59e6bee123..da5e162636fd 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -101,9 +101,6 @@ DEFINE_PER_CPU_READ_MOSTLY(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
 
 /* Logical package management. We might want to allocate that dynamically */
-static int *physical_to_logical_pkg __read_mostly;
-static unsigned long *physical_package_map __read_mostly;;
-static unsigned int max_physical_pkg_id __read_mostly;
 unsigned int __max_logical_packages __read_mostly;
 EXPORT_SYMBOL(__max_logical_packages);
 static unsigned int logical_packages __read_mostly;
@@ -280,6 +277,25 @@ static void notrace start_secondary(void *unused)
 	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
 }
 
+/**
+ * topology_phys_to_logical_pkg - Map a physical package id to a logical
+ *
+ * Returns logical package id or -1 if not found
+ */
+int topology_phys_to_logical_pkg(unsigned int phys_pkg)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		struct cpuinfo_x86 *c = &cpu_data(cpu);
+
+		if (c->initialized && c->phys_proc_id == phys_pkg)
+			return c->logical_proc_id;
+	}
+	return -1;
+}
+EXPORT_SYMBOL(topology_phys_to_logical_pkg);
+
 /**
  * topology_update_package_map - Update the physical to logical package map
  * @pkg:	The physical package id as retrieved via CPUID
@@ -287,17 +303,11 @@ static void notrace start_secondary(void *unused)
  */
 int topology_update_package_map(unsigned int pkg, unsigned int cpu)
 {
-	unsigned int new;
-
-	/* Called from early boot ? */
-	if (!physical_package_map)
-		return 0;
-
-	if (pkg >= max_physical_pkg_id)
-		return -EINVAL;
+	int new;
 
-	/* Set the logical package id */
-	if (test_and_set_bit(pkg, physical_package_map))
+	/* Already available somewhere? */
+	new = topology_phys_to_logical_pkg(pkg);
+	if (new >= 0)
 		goto found;
 
 	if (logical_packages >= __max_logical_packages) {
@@ -311,30 +321,14 @@ int topology_update_package_map(unsigned int pkg, unsigned int cpu)
 		pr_info("CPU %u Converting physical %u to logical package %u\n",
 			cpu, pkg, new);
 	}
-	physical_to_logical_pkg[pkg] = new;
-
 found:
-	cpu_data(cpu).logical_proc_id = physical_to_logical_pkg[pkg];
+	cpu_data(cpu).logical_proc_id = new;
 	return 0;
 }
 
-/**
- * topology_phys_to_logical_pkg - Map a physical package id to a logical
- *
- * Returns logical package id or -1 if not found
- */
-int topology_phys_to_logical_pkg(unsigned int phys_pkg)
-{
-	if (phys_pkg >= max_physical_pkg_id)
-		return -1;
-	return physical_to_logical_pkg[phys_pkg];
-}
-EXPORT_SYMBOL(topology_phys_to_logical_pkg);
-
 static void __init smp_init_package_map(struct cpuinfo_x86 *c, unsigned int cpu)
 {
 	unsigned int ncpus;
-	size_t size;
 
 	/*
 	 * Today neither Intel nor AMD support heterogenous systems. That
@@ -365,19 +359,6 @@ static void __init smp_init_package_map(struct cpuinfo_x86 *c, unsigned int cpu)
 	}
 
 	__max_logical_packages = DIV_ROUND_UP(total_cpus, ncpus);
-	logical_packages = 0;
-
-	/*
-	 * Possibly larger than what we need as the number of apic ids per
-	 * package can be smaller than the actual used apic ids.
-	 */
-	max_physical_pkg_id = DIV_ROUND_UP(MAX_LOCAL_APIC, ncpus);
-	size = max_physical_pkg_id * sizeof(unsigned int);
-	physical_to_logical_pkg = kmalloc(size, GFP_KERNEL);
-	memset(physical_to_logical_pkg, 0xff, size);
-	size = BITS_TO_LONGS(max_physical_pkg_id) * sizeof(unsigned long);
-	physical_package_map = kzalloc(size, GFP_KERNEL);
-
 	pr_info("Max logical packages: %u\n", __max_logical_packages);
 
 	topology_update_package_map(c->phys_proc_id, cpu);
@@ -391,6 +372,7 @@ void __init smp_store_boot_cpu_info(void)
 	*c = boot_cpu_data;
 	c->cpu_index = id;
 	smp_init_package_map(c, id);
+	c->initialized = true;
 }
 
 /*
@@ -401,13 +383,16 @@ void smp_store_cpu_info(int id)
 {
 	struct cpuinfo_x86 *c = &cpu_data(id);
 
-	*c = boot_cpu_data;
+	/* Copy boot_cpu_data only on the first bringup */
+	if (!c->initialized)
+		*c = boot_cpu_data;
 	c->cpu_index = id;
 	/*
 	 * During boot time, CPU0 has this setup already. Save the info when
 	 * bringing up AP or offlined CPU0.
 	 */
 	identify_secondary_cpu(c);
+	c->initialized = true;
 }
 
 static bool

commit b18d62891aaff49d0ee8367d4b6bb9452469f807
Merge: 7d58e1c9059e 141d3b1daacd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 18:29:23 2017 -0800

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 APIC updates from Thomas Gleixner:
     "This update provides a major overhaul of the APIC initialization and
      vector allocation code:
    
       - Unification of the APIC and interrupt mode setup which was
         scattered all over the place and was hard to follow. This also
         distangles the timer setup from the APIC initialization which
         brings a clear separation of functionality.
    
         Great detective work from Dou Lyiang!
    
       - Refactoring of the x86 vector allocation mechanism. The existing
         code was based on nested loops and rather convoluted APIC callbacks
         which had a horrible worst case behaviour and tried to serve all
         different use cases in one go. This led to quite odd hacks when
         supporting the new managed interupt facility for multiqueue devices
         and made it more or less impossible to deal with the vector space
         exhaustion which was a major roadblock for server hibernation.
    
         Aside of that the code dealing with cpu hotplug and the system
         vectors was disconnected from the actual vector management and
         allocation code, which made it hard to follow and maintain.
    
         Utilizing the new bitmap matrix allocator core mechanism, the new
         allocator and management code consolidates the handling of system
         vectors, legacy vectors, cpu hotplug mechanisms and the actual
         allocation which needs to be aware of system and legacy vectors and
         hotplug constraints into a single consistent entity.
    
         This has one visible change: The support for multi CPU targets of
         interrupts, which is only available on a certain subset of
         CPUs/APIC variants has been removed in favour of single interrupt
         targets. A proper analysis of the multi CPU target feature revealed
         that there is no real advantage as the vast majority of interrupts
         end up on the CPU with the lowest APIC id in the set of target CPUs
         anyway. That change was agreed on by the relevant folks and allowed
         to simplify the implementation significantly and to replace rather
         fragile constructs like the vector cleanup IPI with straight
         forward and solid code.
    
         Furthermore this allowed to cleanly separate the allocation details
         for legacy, normal and managed interrupts:
    
          * Legacy interrupts are not longer wasting 16 vectors
            unconditionally
    
          * Managed interrupts have now a guaranteed vector reservation, but
            the actual vector assignment happens when the interrupt is
            requested. It's guaranteed not to fail.
    
          * Normal interrupts no longer allocate vectors unconditionally
            when the interrupt is set up (IO/APIC init or MSI(X) enable).
            The mechanism has been switched to a best effort reservation
            mode. The actual allocation happens when the interrupt is
            requested. Contrary to managed interrupts the request can fail
            due to vector space exhaustion, but drivers must handle a fail
            of request_irq() anyway. When the interrupt is freed, the vector
            is handed back as well.
    
            This solves a long standing problem with large unconditional
            vector allocations for a certain class of enterprise devices
            which prevented server hibernation due to vector space
            exhaustion when the unused allocated vectors had to be migrated
            to CPU0 while unplugging all non boot CPUs.
    
         The code has been equipped with trace points and detailed debugfs
         information to aid analysis of the vector space"
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (60 commits)
      x86/vector/msi: Select CONFIG_GENERIC_IRQ_RESERVATION_MODE
      PCI/MSI: Set MSI_FLAG_MUST_REACTIVATE in core code
      genirq: Add config option for reservation mode
      x86/vector: Use correct per cpu variable in free_moved_vector()
      x86/apic/vector: Ignore set_affinity call for inactive interrupts
      x86/apic: Fix spelling mistake: "symmectic" -> "symmetric"
      x86/apic: Use dead_cpu instead of current CPU when cleaning up
      ACPI/init: Invoke early ACPI initialization earlier
      x86/vector: Respect affinity mask in irq descriptor
      x86/irq: Simplify hotplug vector accounting
      x86/vector: Switch IOAPIC to global reservation mode
      x86/vector/msi: Switch to global reservation mode
      x86/vector: Handle managed interrupts proper
      x86/io_apic: Reevaluate vector configuration on activate()
      iommu/amd: Reevaluate vector configuration on activate()
      iommu/vt-d: Reevaluate vector configuration on activate()
      x86/apic/msi: Force reactivation of interrupts at startup time
      x86/vector: Untangle internal state from irq_cfg
      x86/vector: Compile SMP only code conditionally
      x86/apic: Remove unused callbacks
      ...

commit 6a9f70b0a5b3ca5db1dd5c7743ca555bfca2ae08
Merge: d6ec9d9a4def 6c3b56b19730
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 16:32:30 2017 -0800

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Ingo Molnar:
     "Three smaller changes:
    
       - clang fix
    
       - boot message beautification
    
       - unnecessary header inclusion removal"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/boot: Disable Clang warnings about GNU extensions
      x86/boot: Remove unnecessary #include <generated/utsrelease.h>
      x86/boot: Spell out "boot CPU" for BP

commit d6ec9d9a4def52a5094237564eaf6f6979fd7a27
Merge: 3e2014637c50 91a6a6cfee8a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 14:13:48 2017 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 core updates from Ingo Molnar:
     "Note that in this cycle most of the x86 topics interacted at a level
      that caused them to be merged into tip:x86/asm - but this should be a
      temporary phenomenon, hopefully we'll back to the usual patterns in
      the next merge window.
    
      The main changes in this cycle were:
    
      Hardware enablement:
    
       - Add support for the Intel UMIP (User Mode Instruction Prevention)
         CPU feature. This is a security feature that disables certain
         instructions such as SGDT, SLDT, SIDT, SMSW and STR. (Ricardo Neri)
    
         [ Note that this is disabled by default for now, there are some
           smaller enhancements in the pipeline that I'll follow up with in
           the next 1-2 days, which allows this to be enabled by default.]
    
       - Add support for the AMD SEV (Secure Encrypted Virtualization) CPU
         feature, on top of SME (Secure Memory Encryption) support that was
         added in v4.14. (Tom Lendacky, Brijesh Singh)
    
       - Enable new SSE/AVX/AVX512 CPU features: AVX512_VBMI2, GFNI, VAES,
         VPCLMULQDQ, AVX512_VNNI, AVX512_BITALG. (Gayatri Kammela)
    
      Other changes:
    
       - A big series of entry code simplifications and enhancements (Andy
         Lutomirski)
    
       - Make the ORC unwinder default on x86 and various objtool
         enhancements. (Josh Poimboeuf)
    
       - 5-level paging enhancements (Kirill A. Shutemov)
    
       - Micro-optimize the entry code a bit (Borislav Petkov)
    
       - Improve the handling of interdependent CPU features in the early
         FPU init code (Andi Kleen)
    
       - Build system enhancements (Changbin Du, Masahiro Yamada)
    
       - ... plus misc enhancements, fixes and cleanups"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (118 commits)
      x86/build: Make the boot image generation less verbose
      selftests/x86: Add tests for the STR and SLDT instructions
      selftests/x86: Add tests for User-Mode Instruction Prevention
      x86/traps: Fix up general protection faults caused by UMIP
      x86/umip: Enable User-Mode Instruction Prevention at runtime
      x86/umip: Force a page fault when unable to copy emulated result to user
      x86/umip: Add emulation code for UMIP instructions
      x86/cpufeature: Add User-Mode Instruction Prevention definitions
      x86/insn-eval: Add support to resolve 16-bit address encodings
      x86/insn-eval: Handle 32-bit address encodings in virtual-8086 mode
      x86/insn-eval: Add wrapper function for 32 and 64-bit addresses
      x86/insn-eval: Add support to resolve 32-bit address encodings
      x86/insn-eval: Compute linear address in several utility functions
      resource: Fix resource_size.cocci warnings
      X86/KVM: Clear encryption attribute when SEV is active
      X86/KVM: Decrypt shared per-cpu variables when SEV is active
      percpu: Introduce DEFINE_PER_CPU_DECRYPTED
      x86: Add support for changing memory encryption attribute in early boot
      x86/io: Unroll string I/O when SEV is active
      x86/boot: Add early boot support when running with SEV active
      ...

commit 8e9a2dba8686187d8c8179e5b86640e653963889
Merge: 6098850e7e69 450cbdd0125c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 12:38:26 2017 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core locking updates from Ingo Molnar:
     "The main changes in this cycle are:
    
       - Another attempt at enabling cross-release lockdep dependency
         tracking (automatically part of CONFIG_PROVE_LOCKING=y), this time
         with better performance and fewer false positives. (Byungchul Park)
    
       - Introduce lockdep_assert_irqs_enabled()/disabled() and convert
         open-coded equivalents to lockdep variants. (Frederic Weisbecker)
    
       - Add down_read_killable() and use it in the VFS's iterate_dir()
         method. (Kirill Tkhai)
    
       - Convert remaining uses of ACCESS_ONCE() to
         READ_ONCE()/WRITE_ONCE(). Most of the conversion was Coccinelle
         driven. (Mark Rutland, Paul E. McKenney)
    
       - Get rid of lockless_dereference(), by strengthening Alpha atomics,
         strengthening READ_ONCE() with smp_read_barrier_depends() and thus
         being able to convert users of lockless_dereference() to
         READ_ONCE(). (Will Deacon)
    
       - Various micro-optimizations:
    
            - better PV qspinlocks (Waiman Long),
            - better x86 barriers (Michael S. Tsirkin)
            - better x86 refcounts (Kees Cook)
    
       - ... plus other fixes and enhancements. (Borislav Petkov, Juergen
         Gross, Miguel Bernal Marin)"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      locking/x86: Use LOCK ADD for smp_mb() instead of MFENCE
      rcu: Use lockdep to assert IRQs are disabled/enabled
      netpoll: Use lockdep to assert IRQs are disabled/enabled
      timers/posix-cpu-timers: Use lockdep to assert IRQs are disabled/enabled
      sched/clock, sched/cputime: Use lockdep to assert IRQs are disabled/enabled
      irq_work: Use lockdep to assert IRQs are disabled/enabled
      irq/timings: Use lockdep to assert IRQs are disabled/enabled
      perf/core: Use lockdep to assert IRQs are disabled/enabled
      x86: Use lockdep to assert IRQs are disabled/enabled
      smp/core: Use lockdep to assert IRQs are disabled/enabled
      timers/hrtimer: Use lockdep to assert IRQs are disabled/enabled
      timers/nohz: Use lockdep to assert IRQs are disabled/enabled
      workqueue: Use lockdep to assert IRQs are disabled/enabled
      irq/softirqs: Use lockdep to assert IRQs are disabled/enabled
      locking/lockdep: Add IRQs disabled/enabled assertion APIs: lockdep_assert_irqs_enabled()/disabled()
      locking/pvqspinlock: Implement hybrid PV queued/unfair locks
      locking/rwlocks: Fix comments
      x86/paravirt: Set up the virt_spin_lock_key after static keys get initialized
      block, locking/lockdep: Assign a lock_class per gendisk used for wait_for_completion()
      workqueue: Remove now redundant lock acquisitions wrt. workqueue flushes
      ...

commit 7a10e2a9190628a4024ea394ce7bd641ae40ffd1
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Mon Nov 6 16:01:23 2017 +0100

    x86: Use lockdep to assert IRQs are disabled/enabled
    
    Use lockdep to check that IRQs are enabled or disabled as expected. This
    way the sanity check only shows overhead when concurrency correctness
    debug code is enabled.
    
    It also makes no more sense to fix the IRQ flags when a bug is detected
    as the assertion is now pure config-dependent debugging. And to quote
    Peter Zijlstra:
    
            The whole if !disabled, disable logic is uber paranoid programming,
            but I don't think we've ever seen that WARN trigger, and if it does
            (and then burns the kernel) we at least know what happend.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: David S . Miller <davem@davemloft.net>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/1509980490-4285-8-git-send-email-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 198416ddff01..4008b6b9ad72 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1095,7 +1095,7 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 	unsigned long flags;
 	int err, ret = 0;
 
-	WARN_ON(irqs_disabled());
+	lockdep_assert_irqs_enabled();
 
 	pr_debug("++++++++++++++++++++=_---CPU UP  %u\n", cpu);
 

commit 76ce7cfe35ef58f34e6ba85327afb5fbf6c3ff9b
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Fri Oct 27 20:11:00 2017 -0400

    x86/smpboot: Make optimization of delay calibration work correctly
    
    If the TSC has constant frequency then the delay calibration can be skipped
    when it has been calibrated for a package already. This is checked in
    calibrate_delay_is_known(), but that function is buggy in two aspects:
    
    It returns 'false' if
    
      (!tsc_disabled && !cpu_has(&cpu_data(cpu), X86_FEATURE_CONSTANT_TSC)
    
    which is obviously the reverse of the intended check and the check for the
    sibling mask cannot work either because the topology links have not been
    set up yet.
    
    Correct the condition and move the call to set_cpu_sibling_map() before
    invoking calibrate_delay() so the sibling check works correctly.
    
    [ tglx: Rewrote changelong ]
    
    Fixes: c25323c07345 ("x86/tsc: Use topology functions")
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: peterz@infradead.org
    Cc: bob.picco@oracle.com
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20171028001100.26603-1-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ad59edd84de7..65a0ccdc3050 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -193,6 +193,12 @@ static void smp_callin(void)
 	 */
 	smp_store_cpu_info(cpuid);
 
+	/*
+	 * The topology information must be up to date before
+	 * calibrate_delay() and notify_cpu_starting().
+	 */
+	set_cpu_sibling_map(raw_smp_processor_id());
+
 	/*
 	 * Get our bogomips.
 	 * Update loops_per_jiffy in cpu_data. Previous call to
@@ -203,11 +209,6 @@ static void smp_callin(void)
 	cpu_data(cpuid).loops_per_jiffy = loops_per_jiffy;
 	pr_debug("Stack at about %p\n", &cpuid);
 
-	/*
-	 * This must be done before setting cpu_online_mask
-	 * or calling notify_cpu_starting.
-	 */
-	set_cpu_sibling_map(raw_smp_processor_id());
 	wmb();
 
 	notify_cpu_starting(cpuid);

commit cd493a6deb8b78eca280d05f7fa73fd69403ae29
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:15 2017 -0700

    x86/entry/32: Fix cpu_current_top_of_stack initialization at boot
    
    cpu_current_top_of_stack's initialization forgot about
    TOP_OF_KERNEL_STACK_PADDING.  This bug didn't matter because the
    idle threads never enter user mode.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/e5e370a7e6e4fddd1c4e4cf619765d96bb874b21.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ad59edd84de7..06c18fe1c09e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -961,8 +961,7 @@ void common_cpu_up(unsigned int cpu, struct task_struct *idle)
 #ifdef CONFIG_X86_32
 	/* Stack for startup_32 can be just as for start_secondary onwards */
 	irq_ctx_init(cpu);
-	per_cpu(cpu_current_top_of_stack, cpu) =
-		(unsigned long)task_stack_page(idle) + THREAD_SIZE;
+	per_cpu(cpu_current_top_of_stack, cpu) = task_top_of_stack(idle);
 #else
 	initial_gs = per_cpu_offset(cpu);
 #endif

commit ca5d376e17072c1b60c3fee66f3be58ef018952d
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Sat Oct 28 14:06:44 2017 +0800

    x86/paravirt: Set up the virt_spin_lock_key after static keys get initialized
    
    Commit:
    
      9043442b43b1 ("locking/paravirt: Use new static key for controlling call of virt_spin_lock()")
    
    sets the static virt_spin_lock_key to a value before jump_label_init()
    has been called, which will result in a WARN().
    
    Reorder the initialization sequence:
    
     - Move the native_pv_lock_init() into native_smp_prepare_cpus()
     - set the value in xen_init_lock_cpu()
    
    to avoid calling into the not yet initialized static keys subsystem.
    
    Suggested-by: Juergen Gross <jgross@suse.com>
    Reported-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: boris.ostrovsky@oracle.com
    Cc: bp@suse.de
    Cc: luto@kernel.org
    Cc: vkuznets@redhat.com
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1509170804-3813-1-git-send-email-douly.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 361f91674ce5..198416ddff01 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1358,6 +1358,8 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	pr_info("CPU0: ");
 	print_cpu_info(&cpu_data(0));
 
+	native_pv_lock_init();
+
 	uv_system_init();
 
 	set_mtrr_aps_delayed_init();
@@ -1385,7 +1387,6 @@ void __init native_smp_prepare_boot_cpu(void)
 	/* already set me in cpu_online_mask in boot_cpu_init() */
 	cpumask_set_cpu(me, cpu_callout_mask);
 	cpu_set_state_online(me);
-	native_pv_lock_init();
 }
 
 void __init native_smp_cpus_done(unsigned int max_cpus)

commit 9043442b43b1fddf202591b84702863286700c1a
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Sep 6 19:36:24 2017 +0200

    locking/paravirt: Use new static key for controlling call of virt_spin_lock()
    
    There are cases where a guest tries to switch spinlocks to bare metal
    behavior (e.g. by setting "xen_nopvspin" boot parameter). Today this
    has the downside of falling back to unfair test and set scheme for
    qspinlocks due to virt_spin_lock() detecting the virtualized
    environment.
    
    Add a static key controlling whether virt_spin_lock() should be
    called or not. When running on bare metal set the new key to false.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akataria@vmware.com
    Cc: boris.ostrovsky@oracle.com
    Cc: chrisw@sous-sol.org
    Cc: hpa@zytor.com
    Cc: jeremy@goop.org
    Cc: rusty@rustcorp.com.au
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/20170906173625.18158-2-jgross@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ad59edd84de7..361f91674ce5 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -77,6 +77,7 @@
 #include <asm/i8259.h>
 #include <asm/realmode.h>
 #include <asm/misc.h>
+#include <asm/qspinlock.h>
 
 /* Number of siblings per CPU package */
 int smp_num_siblings = 1;
@@ -1384,6 +1385,7 @@ void __init native_smp_prepare_boot_cpu(void)
 	/* already set me in cpu_online_mask in boot_cpu_init() */
 	cpumask_set_cpu(me, cpu_callout_mask);
 	cpu_set_state_online(me);
+	native_pv_lock_init();
 }
 
 void __init native_smp_cpus_done(unsigned int max_cpus)

commit a1652bb8a01c1a830cacbe958aec17f880cc1e47
Author: Jean Delvare <jdelvare@suse.de>
Date:   Tue Oct 3 11:47:27 2017 +0200

    x86/boot: Spell out "boot CPU" for BP
    
    It's not obvious to everybody that BP stands for boot processor. At
    least it was not for me. And BP is also a CPU register on x86, so it
    is ambiguous. Spell out "boot CPU" everywhere instead.
    
    Signed-off-by: Jean Delvare <jdelvare@suse.de>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ad59edd84de7..a98253e183be 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -249,7 +249,7 @@ static void notrace start_secondary(void *unused)
 	/* otherwise gcc will move up smp_processor_id before the cpu_init */
 	barrier();
 	/*
-	 * Check TSC synchronization with the BP:
+	 * Check TSC synchronization with the boot CPU:
 	 */
 	check_tsc_sync_target();
 

commit 2cffad7bad83157f89332872015f4305d2ac09ac
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 13 23:29:53 2017 +0200

    x86/irq: Simplify hotplug vector accounting
    
    Before a CPU is taken offline the number of active interrupt vectors on the
    outgoing CPU and the number of vectors which are available on the other
    online CPUs are counted and compared. If the active vectors are more than
    the available vectors on the other CPUs then the CPU hot-unplug operation
    is aborted. This again uses loop based search and is inaccurate.
    
    The bitmap matrix allocator has accurate accounting information and can
    tell exactly whether the vector space is sufficient or not.
    
    Emit a message when the number of globaly reserved (unallocated) vectors is
    larger than the number of available vectors after offlining a CPU because
    after that point request_irq() might fail.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Juergen Gross <jgross@suse.com>
    Tested-by: Yu Chen <yu.c.chen@intel.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Link: https://lkml.kernel.org/r/20170913213156.351193962@linutronix.de

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 86739f04701b..92aadfa30d61 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1525,7 +1525,7 @@ int native_cpu_disable(void)
 {
 	int ret;
 
-	ret = check_irq_vectors_for_cpu_disable();
+	ret = lapic_can_unplug_cpu();
 	if (ret)
 		return ret;
 

commit 8ed4f3e66665cd186bc6b1d35f25a481e35c62ab
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 13 23:29:40 2017 +0200

    x86/smpboot: Set online before setting up vectors
    
    There is no reason to set the CPU online after establishing the vectors on
    the upcoming CPU. The vector space is protected by the vector lock so no
    changes can happen.
    
    Marking the CPU online before setting up the vector space makes tracing
    work in the early vector management cpu online code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Juergen Gross <jgross@suse.com>
    Tested-by: Yu Chen <yu.c.chen@intel.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Link: https://lkml.kernel.org/r/20170913213155.264311994@linutronix.de

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 91c0d1cd651e..86739f04701b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -254,14 +254,14 @@ static void notrace start_secondary(void *unused)
 	check_tsc_sync_target();
 
 	/*
-	 * Lock vector_lock and initialize the vectors on this cpu
-	 * before setting the cpu online. We must set it online with
-	 * vector_lock held to prevent a concurrent setup/teardown
-	 * from seeing a half valid vector space.
+	 * Lock vector_lock, set CPU online and bring the vector
+	 * allocator online. Online must be set with vector_lock held
+	 * to prevent a concurrent irq setup/teardown from seeing a
+	 * half valid vector space.
 	 */
 	lock_vector_lock();
-	lapic_online();
 	set_cpu_online(smp_processor_id(), true);
+	lapic_online();
 	unlock_vector_lock();
 	cpu_set_state_online(smp_processor_id());
 	x86_platform.nmi_init();

commit 0fa115da408f645cca419a60a5af8f4426ad4188
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 13 23:29:38 2017 +0200

    x86/irq/vector: Initialize matrix allocator
    
    Initialize the matrix allocator and add the proper accounting points to the
    code.
    
    No functional change, just preparation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Juergen Gross <jgross@suse.com>
    Tested-by: Yu Chen <yu.c.chen@intel.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Link: https://lkml.kernel.org/r/20170913213155.108410660@linutronix.de

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d8cef3222887..91c0d1cd651e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -260,7 +260,7 @@ static void notrace start_secondary(void *unused)
 	 * from seeing a half valid vector space.
 	 */
 	lock_vector_lock();
-	setup_vector_irq(smp_processor_id());
+	lapic_online();
 	set_cpu_online(smp_processor_id(), true);
 	unlock_vector_lock();
 	cpu_set_state_online(smp_processor_id());
@@ -1518,6 +1518,7 @@ void cpu_disable_common(void)
 	remove_cpu_from_maps(cpu);
 	unlock_vector_lock();
 	fixup_irqs();
+	lapic_offline();
 }
 
 int native_cpu_disable(void)

commit ef9e56d894eab99a33a06b96ba8057afa67d3702
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 13 23:29:28 2017 +0200

    x86/ioapic: Remove obsolete post hotplug update
    
    With single CPU affinities the post SMP boot vector update is pointless as
    it will just leave the affinities on the same vectors and the same CPUs.
    
    Remove it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Juergen Gross <jgross@suse.com>
    Tested-by: Yu Chen <yu.c.chen@intel.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Link: https://lkml.kernel.org/r/20170913213154.308697243@linutronix.de

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 81652e3b8c17..d8cef3222887 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1360,7 +1360,6 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 
 	nmi_selftest();
 	impress_friends();
-	setup_ioapic_dest();
 	mtrr_aps_init();
 }
 

commit 935356cecda851d94381e1c6fea9dec443f908fe
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Wed Sep 13 17:12:54 2017 +0800

    x86/apic: Initialize interrupt mode after timer init
    
    A cold or warm boot through BIOS sets the APIC in default interrupt
    delivery mode. A dump-capture kernel will not go through a BIOS reset and
    leave the interrupt delivery mode in the state which was active on the
    crashed kernel, but the dump kernel startup code assumes default delivery
    mode which can result in interrupt delivery/handling to fail.
    
    To solve this problem, it's required to set up the final interrupt delivery
    mode as soon as possible. As IOAPIC setup needs the timer initialized for
    verifying the timer interrupt delivery mode, the earliest point is right
    after timer setup in late_time_init().
    
    That results in the following init order:
    
      1) Set up the legacy timer, if applicable on the platform
    
      2) Set up APIC/IOAPIC which includes the verification of the legacy timer
         interrupt delivery.
    
      3) TSC calibration
    
      4) Local APIC timer setup
    
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: yinghai@kernel.org
    Cc: bhe@redhat.com
    Link: https://lkml.kernel.org/r/1505293975-26005-12-git-send-email-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3d045e82352d..81652e3b8c17 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1263,8 +1263,9 @@ static void __init smp_get_logical_apicid(void)
 }
 
 /*
- * Prepare for SMP bootup.  The MP table or ACPI has been read
- * earlier.  Just do some sanity checking here and enable APIC mode.
+ * Prepare for SMP bootup.
+ * @max_cpus: configured maximum number of CPUs, It is a legacy parameter
+ *            for common interface support.
  */
 void __init native_smp_prepare_cpus(unsigned int max_cpus)
 {
@@ -1296,8 +1297,6 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 	set_cpu_sibling_map(0);
 
-	x86_init.irqs.intr_mode_init();
-
 	smp_sanity_check();
 
 	switch (apic_intr_mode) {

commit 34fba3e6b1e5d42c81fc00ede715e0cdd2ebfada
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Wed Sep 13 17:12:52 2017 +0800

    x86/init: Add intr_mode_init to x86_init_ops
    
    X86 and XEN initialize interrupt delivery mode in different way.
    
    To avoid conditionals, add a new x86_init_ops function which defaults to
    the standard function and can be overridden by the early XEN platform code.
    
    [ tglx: Folded the XEN part which was a separate patch to preserve
            bisectability ]
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: yinghai@kernel.org
    Cc: bhe@redhat.com
    Link: https://lkml.kernel.org/r/1505293975-26005-10-git-send-email-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 161935c49166..3d045e82352d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1296,7 +1296,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 	set_cpu_sibling_map(0);
 
-	apic_intr_mode_init();
+	x86_init.irqs.intr_mode_init();
 
 	smp_sanity_check();
 

commit 4f45ed9f848f0721967e2f79e5409b6538894a43
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Wed Sep 13 17:12:49 2017 +0800

    x86/apic: Mark the apic_intr_mode extern for sanity check cleanup
    
    Calling native_smp_prepare_cpus() to prepare for SMP bootup, does some
    sanity checking, enables APIC mode and disables SMP feature.
    
    Now, APIC mode setup has been unified to apic_intr_mode_init(), some sanity
    checks are redundant and need to be cleanup.
    
    Mark the apic_intr_mode extern to refine the switch and remove the
    redundant sanity check.
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: yinghai@kernel.org
    Cc: bhe@redhat.com
    Link: https://lkml.kernel.org/r/1505293975-26005-7-git-send-email-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d0a1d28c23e8..161935c49166 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1190,17 +1190,10 @@ static __init void disable_smp(void)
 	cpumask_set_cpu(0, topology_core_cpumask(0));
 }
 
-enum {
-	SMP_OK,
-	SMP_NO_CONFIG,
-	SMP_NO_APIC,
-	SMP_FORCE_UP,
-};
-
 /*
  * Various sanity checks.
  */
-static int __init smp_sanity_check(unsigned max_cpus)
+static void __init smp_sanity_check(void)
 {
 	preempt_disable();
 
@@ -1237,16 +1230,6 @@ static int __init smp_sanity_check(unsigned max_cpus)
 		physid_set(hard_smp_processor_id(), phys_cpu_present_map);
 	}
 
-	/*
-	 * If we couldn't find an SMP configuration at boot time,
-	 * get out of here now!
-	 */
-	if (!smp_found_config && !acpi_lapic) {
-		preempt_enable();
-		pr_notice("SMP motherboard not detected\n");
-		return SMP_NO_CONFIG;
-	}
-
 	/*
 	 * Should not be necessary because the MP table should list the boot
 	 * CPU too, but we do it for the sake of robustness anyway.
@@ -1257,29 +1240,6 @@ static int __init smp_sanity_check(unsigned max_cpus)
 		physid_set(hard_smp_processor_id(), phys_cpu_present_map);
 	}
 	preempt_enable();
-
-	/*
-	 * If we couldn't find a local APIC, then get out of here now!
-	 */
-	if (APIC_INTEGRATED(boot_cpu_apic_version) &&
-	    !boot_cpu_has(X86_FEATURE_APIC)) {
-		if (!disable_apic) {
-			pr_err("BIOS bug, local APIC #%d not detected!...\n",
-				boot_cpu_physical_apicid);
-			pr_err("... forcing use of dummy APIC emulation (tell your hw vendor)\n");
-		}
-		return SMP_NO_APIC;
-	}
-
-	/*
-	 * If SMP should be disabled, then really disable it!
-	 */
-	if (!max_cpus) {
-		pr_info("SMP mode deactivated\n");
-		return SMP_FORCE_UP;
-	}
-
-	return SMP_OK;
 }
 
 static void __init smp_cpu_index_default(void)
@@ -1338,19 +1298,20 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 	apic_intr_mode_init();
 
-	switch (smp_sanity_check(max_cpus)) {
-	case SMP_NO_CONFIG:
-		disable_smp();
-		return;
-	case SMP_NO_APIC:
+	smp_sanity_check();
+
+	switch (apic_intr_mode) {
+	case APIC_PIC:
+	case APIC_VIRTUAL_WIRE_NO_CONFIG:
 		disable_smp();
 		return;
-	case SMP_FORCE_UP:
+	case APIC_SYMMETRIC_IO_NO_ROUTING:
 		disable_smp();
 		/* Setup local timer */
 		x86_init.timers.setup_percpu_clockev();
 		return;
-	case SMP_OK:
+	case APIC_VIRTUAL_WIRE:
+	case APIC_SYMMETRIC_IO:
 		break;
 	}
 

commit 3e730dad3b6da42d21c05007445ca1bfd219d7ce
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Wed Sep 13 17:12:48 2017 +0800

    x86/apic: Unify interrupt mode setup for SMP-capable system
    
    On a SMP-capable system, the kernel enables and sets up the APIC interrupt
    delivery mode in native_smp_prepare_cpus(). The decision how to setup the
    APIC is intermingled with the decision of setting up SMP or not.
    
    Split the initialization of the APIC interrupt mode independent from other
    decisions and have a separate apic_intr_mode_init() function for it.
    
    The invocation time stays the same for now.
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: yinghai@kernel.org
    Cc: bhe@redhat.com
    Link: https://lkml.kernel.org/r/1505293975-26005-6-git-send-email-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d367ddbec5d0..d0a1d28c23e8 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1336,18 +1336,17 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 	set_cpu_sibling_map(0);
 
+	apic_intr_mode_init();
+
 	switch (smp_sanity_check(max_cpus)) {
 	case SMP_NO_CONFIG:
 		disable_smp();
-		if (APIC_init_uniprocessor())
-			pr_notice("Local APIC not detected. Using dummy APIC emulation.\n");
 		return;
 	case SMP_NO_APIC:
 		disable_smp();
 		return;
 	case SMP_FORCE_UP:
 		disable_smp();
-		apic_bsp_setup(false);
 		/* Setup local timer */
 		x86_init.timers.setup_percpu_clockev();
 		return;
@@ -1355,15 +1354,6 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		break;
 	}
 
-	if (read_apic_id() != boot_cpu_physical_apicid) {
-		panic("Boot APIC ID in local APIC unexpected (%d vs %d)",
-		     read_apic_id(), boot_cpu_physical_apicid);
-		/* Or can we switch back to PIC here? */
-	}
-
-	default_setup_apic_routing();
-	apic_bsp_setup(false);
-
 	/* Setup local timer */
 	x86_init.timers.setup_percpu_clockev();
 

commit 4b1244b45c16cef63fa3282e5bb1cc4fa1aef06a
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Wed Sep 13 17:12:47 2017 +0800

    x86/apic: Move logical APIC ID away from apic_bsp_setup()
    
    apic_bsp_setup() sets and returns logical APIC ID for initializing
    cpu0_logical_apicid in a SMP-capable system.
    
    The id has nothing to do with the initialization of local APIC and I/O
    APIC. And apic_bsp_setup() should be called for interrupt mode setup only.
    
    Move the id setup into a separate helper function for cleanup and mark
    apic_bsp_setup() void.
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: yinghai@kernel.org
    Cc: bhe@redhat.com
    Link: https://lkml.kernel.org/r/1505293975-26005-5-git-send-email-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index dad0a099e433..d367ddbec5d0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1294,6 +1294,14 @@ static void __init smp_cpu_index_default(void)
 	}
 }
 
+static void __init smp_get_logical_apicid(void)
+{
+	if (x2apic_mode)
+		cpu0_logical_apicid = apic_read(APIC_LDR);
+	else
+		cpu0_logical_apicid = GET_APIC_LOGICAL_ID(apic_read(APIC_LDR));
+}
+
 /*
  * Prepare for SMP bootup.  The MP table or ACPI has been read
  * earlier.  Just do some sanity checking here and enable APIC mode.
@@ -1354,11 +1362,13 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	}
 
 	default_setup_apic_routing();
-	cpu0_logical_apicid = apic_bsp_setup(false);
+	apic_bsp_setup(false);
 
 	/* Setup local timer */
 	x86_init.timers.setup_percpu_clockev();
 
+	smp_get_logical_apicid();
+
 	pr_info("CPU0: ");
 	print_cpu_info(&cpu_data(0));
 

commit a2510d156eae9cf85c928d428471e44edd82c5ca
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Wed Sep 13 17:12:46 2017 +0800

    x86/apic: Split local APIC timer setup from the APIC setup
    
    apic_bsp_setup() sets up the local APIC, I/O APIC and APIC timer.
    
    The local APIC and I/O APIC setup belongs to interrupt delivery mode
    setup. Setting up the local APIC timer for booting CPU is another job
    and has nothing to do with interrupt delivery mode setup.
    
    Split local APIC timer setup from the APIC setup, keep it in the original
    position for SMP and UP kernel for now.
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: yinghai@kernel.org
    Cc: bhe@redhat.com
    Link: https://lkml.kernel.org/r/1505293975-26005-4-git-send-email-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ad59edd84de7..dad0a099e433 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1340,6 +1340,8 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	case SMP_FORCE_UP:
 		disable_smp();
 		apic_bsp_setup(false);
+		/* Setup local timer */
+		x86_init.timers.setup_percpu_clockev();
 		return;
 	case SMP_OK:
 		break;
@@ -1354,6 +1356,9 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	default_setup_apic_routing();
 	cpu0_logical_apicid = apic_bsp_setup(false);
 
+	/* Setup local timer */
+	x86_init.timers.setup_percpu_clockev();
+
 	pr_info("CPU0: ");
 	print_cpu_info(&cpu_data(0));
 

commit 4ba55e65f471d011d3ba2ac2022180ea0877d68e
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Sep 17 09:03:51 2017 -0700

    x86/mm/32: Load a sane CR3 before cpu_init() on secondary CPUs
    
    For unknown historical reasons (i.e. Borislav doesn't recall),
    32-bit kernels invoke cpu_init() on secondary CPUs with
    initial_page_table loaded into CR3.  Then they set
    current->active_mm to &init_mm and call enter_lazy_tlb() before
    fixing CR3.  This means that the x86 TLB code gets invoked while CR3
    is inconsistent, and, with the improved PCID sanity checks I added,
    we warn.
    
    Fix it by loading swapper_pg_dir (i.e. init_mm.pgd) earlier.
    
    Reported-by: Paul Menzel <pmenzel@molgen.mpg.de>
    Reported-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 72c0098d92ce ("x86/mm: Reinitialize TLB state on hotplug and resume")
    Link: http://lkml.kernel.org/r/30cdfea504682ba3b9012e77717800a91c22097f.1505663533.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0854ff169274..ad59edd84de7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -232,12 +232,6 @@ static void notrace start_secondary(void *unused)
 	 */
 	if (boot_cpu_has(X86_FEATURE_PCID))
 		__write_cr4(__read_cr4() | X86_CR4_PCIDE);
-	cpu_init();
-	x86_cpuinit.early_percpu_clock_init();
-	preempt_disable();
-	smp_callin();
-
-	enable_start_cpu0 = 0;
 
 #ifdef CONFIG_X86_32
 	/* switch away from the initial page table */
@@ -245,6 +239,13 @@ static void notrace start_secondary(void *unused)
 	__flush_tlb_all();
 #endif
 
+	cpu_init();
+	x86_cpuinit.early_percpu_clock_init();
+	preempt_disable();
+	smp_callin();
+
+	enable_start_cpu0 = 0;
+
 	/* otherwise gcc will move up smp_processor_id before the cpu_init */
 	barrier();
 	/*

commit c7ad5ad297e644601747d6dbee978bf85e14f7bc
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Sep 10 17:48:27 2017 -0700

    x86/mm/64: Initialize CR4.PCIDE early
    
    cpu_init() is weird: it's called rather late (after early
    identification and after most MMU state is initialized) on the boot
    CPU but is called extremely early (before identification) on secondary
    CPUs.  It's called just late enough on the boot CPU that its CR4 value
    isn't propagated to mmu_cr4_features.
    
    Even if we put CR4.PCIDE into mmu_cr4_features, we'd hit two
    problems.  First, we'd crash in the trampoline code.  That's
    fixable, and I tried that.  It turns out that mmu_cr4_features is
    totally ignored by secondary_start_64(), though, so even with the
    trampoline code fixed, it wouldn't help.
    
    This means that we don't currently have CR4.PCIDE reliably initialized
    before we start playing with cpu_tlbstate.  This is very fragile and
    tends to cause boot failures if I make even small changes to the TLB
    handling code.
    
    Make it more robust: initialize CR4.PCIDE earlier on the boot CPU
    and propagate it to secondary CPUs in start_secondary().
    
    ( Yes, this is ugly.  I think we should have improved mmu_cr4_features
      to actually control CR4 during secondary bootup, but that would be
      fairly intrusive at this stage. )
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reported-by: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Tested-by: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Fixes: 660da7c9228f ("x86/mm: Enable CR4.PCIDE on supported systems")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index cd6622c3204e..0854ff169274 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -226,10 +226,12 @@ static int enable_start_cpu0;
 static void notrace start_secondary(void *unused)
 {
 	/*
-	 * Don't put *anything* before cpu_init(), SMP booting is too
-	 * fragile that we want to limit the things done here to the
-	 * most necessary things.
+	 * Don't put *anything* except direct CPU state initialization
+	 * before cpu_init(), SMP booting is too fragile that we want to
+	 * limit the things done here to the most necessary things.
 	 */
+	if (boot_cpu_has(X86_FEATURE_PCID))
+		__write_cr4(__read_cr4() | X86_CR4_PCIDE);
 	cpu_init();
 	x86_cpuinit.early_percpu_clock_init();
 	preempt_disable();

commit 9b130ad5bb8255ee8534d92d67e12b2a4887eacb
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Sep 8 16:14:18 2017 -0700

    treewide: make "nr_cpu_ids" unsigned
    
    First, number of CPUs can't be negative number.
    
    Second, different signnnedness leads to suboptimal code in the following
    cases:
    
    1)
            kmalloc(nr_cpu_ids * sizeof(X));
    
    "int" has to be sign extended to size_t.
    
    2)
            while (loff_t *pos < nr_cpu_ids)
    
    MOVSXD is 1 byte longed than the same MOV.
    
    Other cases exist as well. Basically compiler is told that nr_cpu_ids
    can't be negative which can't be deduced if it is "int".
    
    Code savings on allyesconfig kernel: -3KB
    
            add/remove: 0/0 grow/shrink: 25/264 up/down: 261/-3631 (-3370)
            function                                     old     new   delta
            coretemp_cpu_online                          450     512     +62
            rcu_init_one                                1234    1272     +38
            pci_device_probe                             374     399     +25
    
                                    ...
    
            pgdat_reclaimable_pages                      628     556     -72
            select_fallback_rq                           446     369     -77
            task_numa_find_cpu                          1923    1807    -116
    
    Link: http://lkml.kernel.org/r/20170819114959.GA30580@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 54b9e89d4d6b..cd6622c3204e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1461,7 +1461,7 @@ __init void prefill_possible_map(void)
 
 	/* nr_cpu_ids could be reduced via nr_cpus= */
 	if (possible > nr_cpu_ids) {
-		pr_warn("%d Processors exceeds NR_CPUS limit of %d\n",
+		pr_warn("%d Processors exceeds NR_CPUS limit of %u\n",
 			possible, nr_cpu_ids);
 		possible = nr_cpu_ids;
 	}

commit 10e66760fa8ee11f254a69433fc132d04758a5fc
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Aug 3 12:58:18 2017 +0200

    x86/smpboot: Unbreak CPU0 hotplug
    
    A hang on CPU0 onlining after a preceding offlining is observed. Trace
    shows that CPU0 is stuck in check_tsc_sync_target() waiting for source
    CPU to run check_tsc_sync_source() but this never happens. Source CPU,
    in its turn, is stuck on synchronize_sched() which is called from
    native_cpu_up() -> do_boot_cpu() -> unregister_nmi_handler().
    
    So it's a classic ABBA deadlock, due to the use of synchronize_sched() in
    unregister_nmi_handler().
    
    Fix the bug by moving unregister_nmi_handler() from do_boot_cpu() to
    native_cpu_up() after cpu onlining is done.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170803105818.9934-1-vkuznets@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b474c8de7fba..54b9e89d4d6b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -971,7 +971,8 @@ void common_cpu_up(unsigned int cpu, struct task_struct *idle)
  * Returns zero if CPU booted OK, else error code from
  * ->wakeup_secondary_cpu.
  */
-static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
+static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
+		       int *cpu0_nmi_registered)
 {
 	volatile u32 *trampoline_status =
 		(volatile u32 *) __va(real_mode_header->trampoline_status);
@@ -979,7 +980,6 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	unsigned long start_ip = real_mode_header->trampoline_start;
 
 	unsigned long boot_error = 0;
-	int cpu0_nmi_registered = 0;
 	unsigned long timeout;
 
 	idle->thread.sp = (unsigned long)task_pt_regs(idle);
@@ -1035,7 +1035,7 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 		boot_error = apic->wakeup_secondary_cpu(apicid, start_ip);
 	else
 		boot_error = wakeup_cpu_via_init_nmi(cpu, start_ip, apicid,
-						     &cpu0_nmi_registered);
+						     cpu0_nmi_registered);
 
 	if (!boot_error) {
 		/*
@@ -1080,12 +1080,6 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 		 */
 		smpboot_restore_warm_reset_vector();
 	}
-	/*
-	 * Clean up the nmi handler. Do this after the callin and callout sync
-	 * to avoid impact of possible long unregister time.
-	 */
-	if (cpu0_nmi_registered)
-		unregister_nmi_handler(NMI_LOCAL, "wake_cpu0");
 
 	return boot_error;
 }
@@ -1093,8 +1087,9 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
 	int apicid = apic->cpu_present_to_apicid(cpu);
+	int cpu0_nmi_registered = 0;
 	unsigned long flags;
-	int err;
+	int err, ret = 0;
 
 	WARN_ON(irqs_disabled());
 
@@ -1131,10 +1126,11 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 
 	common_cpu_up(cpu, tidle);
 
-	err = do_boot_cpu(apicid, cpu, tidle);
+	err = do_boot_cpu(apicid, cpu, tidle, &cpu0_nmi_registered);
 	if (err) {
 		pr_err("do_boot_cpu failed(%d) to wakeup CPU#%u\n", err, cpu);
-		return -EIO;
+		ret = -EIO;
+		goto unreg_nmi;
 	}
 
 	/*
@@ -1150,7 +1146,15 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 		touch_nmi_watchdog();
 	}
 
-	return 0;
+unreg_nmi:
+	/*
+	 * Clean up the nmi handler. Do this after the callin and callout sync
+	 * to avoid impact of possible long unregister time.
+	 */
+	if (cpu0_nmi_registered)
+		unregister_nmi_handler(NMI_LOCAL, "wake_cpu0");
+
+	return ret;
 }
 
 /**

commit 7a69f9c60b49699579f5bfb71f928cceba0afe1a
Merge: 9bc088ab66be 8781fb7e9749
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 3 14:45:09 2017 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Continued work to add support for 5-level paging provided by future
         Intel CPUs. In particular we switch the x86 GUP code to the generic
         implementation. (Kirill A. Shutemov)
    
       - Continued work to add PCID CPU support to native kernels as well.
         In this round most of the focus is on reworking/refreshing the TLB
         flush infrastructure for the upcoming PCID changes. (Andy
         Lutomirski)"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (34 commits)
      x86/mm: Delete a big outdated comment about TLB flushing
      x86/mm: Don't reenter flush_tlb_func_common()
      x86/KASLR: Fix detection 32/64 bit bootloaders for 5-level paging
      x86/ftrace: Exclude functions in head64.c from function-tracing
      x86/mmap, ASLR: Do not treat unlimited-stack tasks as legacy mmap
      x86/mm: Remove reset_lazy_tlbstate()
      x86/ldt: Simplify the LDT switching logic
      x86/boot/64: Put __startup_64() into .head.text
      x86/mm: Add support for 5-level paging for KASLR
      x86/mm: Make kernel_physical_mapping_init() support 5-level paging
      x86/mm: Add sync_global_pgds() for configuration with 5-level paging
      x86/boot/64: Add support of additional page table level during early boot
      x86/boot/64: Rename init_level4_pgt and early_level4_pgt
      x86/boot/64: Rewrite startup_64() in C
      x86/boot/compressed: Enable 5-level paging during decompression stage
      x86/boot/efi: Define __KERNEL32_CS GDT on 64-bit configurations
      x86/boot/efi: Fix __KERNEL_CS definition of GDT entry on 64-bit configurations
      x86/boot/efi: Cleanup initialization of GDT entries
      x86/asm: Fix comment in return_from_SYSCALL_64()
      x86/mm/gup: Switch GUP to the generic get_user_page_fast() implementation
      ...

commit d54368127a11c6da0776c109a4c65a7b6a815f32
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Jun 20 22:22:09 2017 -0700

    x86/mm: Remove reset_lazy_tlbstate()
    
    The only call site also calls idle_task_exit(), and idle_task_exit()
    puts us into a clean state by explicitly switching to init_mm.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/3acc7ad02a2ec060d2321a1e0f6de1cb90069517.1498022414.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f04479a8f74f..6169a56aab49 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1589,7 +1589,6 @@ void native_cpu_die(unsigned int cpu)
 void play_dead_common(void)
 {
 	idle_task_exit();
-	reset_lazy_tlbstate();
 
 	/* Ack it */
 	(void)cpu_report_death();

commit 719b3680d1f789c1e3054e3fcb26bfff07c3c623
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 16 20:42:35 2017 +0200

    x86/smp: Adjust system_state check
    
    To enable smp_processor_id() and might_sleep() debug checks earlier, it's
    required to add system states between SYSTEM_BOOTING and SYSTEM_RUNNING.
    
    Adjust the system_state check in announce_cpu() to handle the extra states.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20170516184735.191715856@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f04479a8f74f..045e4f993bd2 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -863,7 +863,7 @@ static void announce_cpu(int cpu, int apicid)
 	if (cpu == 1)
 		printk(KERN_INFO "x86: Booting SMP configuration:\n");
 
-	if (system_state == SYSTEM_BOOTING) {
+	if (system_state < SYSTEM_RUNNING) {
 		if (node != current_node) {
 			if (current_node > (-1))
 				pr_cont("\n");

commit 69218e47994da614e7af600bf06887750ab6657a
Author: Thomas Garnier <thgarnie@google.com>
Date:   Tue Mar 14 10:05:07 2017 -0700

    x86: Remap GDT tables in the fixmap section
    
    Each processor holds a GDT in its per-cpu structure. The sgdt
    instruction gives the base address of the current GDT. This address can
    be used to bypass KASLR memory randomization. With another bug, an
    attacker could target other per-cpu structures or deduce the base of
    the main memory section (PAGE_OFFSET).
    
    This patch relocates the GDT table for each processor inside the
    fixmap section. The space is reserved based on number of supported
    processors.
    
    For consistency, the remapping is done by default on 32 and 64-bit.
    
    Each processor switches to its remapped GDT at the end of
    initialization. For hibernation, the main processor returns with the
    original GDT and switches back to the remapping at completion.
    
    This patch was tested on both architectures. Hibernation and KVM were
    both tested specially for their usage of the GDT.
    
    Thanks to Boris Ostrovsky <boris.ostrovsky@oracle.com> for testing and
    recommending changes for Xen support.
    
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Lorenzo Stoakes <lstoakes@gmail.com>
    Cc: Luis R . Rodriguez <mcgrof@kernel.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kernel-hardening@lists.openwall.com
    Cc: kvm@vger.kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-pm@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    Cc: zijun_hu <zijun_hu@htc.com>
    Link: http://lkml.kernel.org/r/20170314170508.100882-2-thgarnie@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index bd1f1ad35284..f04479a8f74f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -983,7 +983,7 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	unsigned long timeout;
 
 	idle->thread.sp = (unsigned long)task_pt_regs(idle);
-	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
+	early_gdt_descr.address = (unsigned long)get_cpu_gdt_rw(cpu);
 	initial_code = (unsigned long)start_secondary;
 	initial_stack  = idle->thread.sp;
 

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f3eb266d75ff..bd1f1ad35284 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -47,6 +47,7 @@
 #include <linux/sched.h>
 #include <linux/sched/topology.h>
 #include <linux/sched/hotplug.h>
+#include <linux/sched/task_stack.h>
 #include <linux/percpu.h>
 #include <linux/bootmem.h>
 #include <linux/err.h>

commit ef8bd77f332bb0a4e467d7171bbfc6c57aa08a88
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/hotplug.h>
    
    We are going to split <linux/sched/hotplug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/hotplug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fe7a66e6b5a0..f3eb266d75ff 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -46,6 +46,7 @@
 #include <linux/export.h>
 #include <linux/sched.h>
 #include <linux/sched/topology.h>
+#include <linux/sched/hotplug.h>
 #include <linux/percpu.h>
 #include <linux/bootmem.h>
 #include <linux/err.h>

commit 105ab3d8ce7269887d24d224054677125e18037c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/topology.h>
    
    We are going to split <linux/sched/topology.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/topology.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a0d38685f7df..fe7a66e6b5a0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -45,6 +45,7 @@
 #include <linux/smp.h>
 #include <linux/export.h>
 #include <linux/sched.h>
+#include <linux/sched/topology.h>
 #include <linux/percpu.h>
 #include <linux/bootmem.h>
 #include <linux/err.h>

commit c945d0227d86ddc3485290fa5da1a7d2c9b759de
Merge: 8b5abde16bdc d48085f0716f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 16:26:57 2017 -0800

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 platform updates from Ingo Molnar:
     "Misc platform updates: SGI UV4 support additions, intel-mid Merrifield
      enhancements and purge of old code"
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (21 commits)
      x86/platform/UV/NMI: Fix uneccessary kABI breakage
      x86/platform/UV: Clean up the NMI code to match current coding style
      x86/platform/UV: Ensure uv_system_init is called when necessary
      x86/platform/UV: Initialize PCH GPP_D_0 NMI Pin to be NMI source
      x86/platform/UV: Verify NMI action is valid, default is standard
      x86/platform/UV: Add basic CPU NMI health check
      x86/platform/UV: Add Support for UV4 Hubless NMIs
      x86/platform/UV: Add Support for UV4 Hubless systems
      x86/platform/UV: Clean up the UV APIC code
      x86/platform/intel-mid: Move watchdog registration to arch_initcall()
      x86/platform/intel-mid: Don't shadow error code of mp_map_gsi_to_irq()
      x86/platform/intel-mid: Allocate RTC interrupt for Merrifield
      x86/ioapic: Return suitable error code in mp_map_gsi_to_irq()
      x86/platform/UV: Fix 2 socket config problem
      x86/platform/UV: Fix panic with missing UVsystab support
      x86/platform/intel-mid: Enable RTC on Intel Merrifield
      x86/platform/intel: Remove PMIC GPIO block support
      x86/platform/intel-mid: Make intel_scu_device_register() static
      x86/platform/intel-mid: Enable GPIO keys on Merrifield
      x86/platform/intel-mid: Get rid of duplication of IPC handler
      ...

commit 79a8b9aa388b0620cc1d525d7c0f0d9a8a85e08e
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Feb 5 11:50:21 2017 +0100

    x86/CPU/AMD: Bring back Compute Unit ID
    
    Commit:
    
      a33d331761bc ("x86/CPU/AMD: Fix Bulldozer topology")
    
    restored the initial approach we had with the Fam15h topology of
    enumerating CU (Compute Unit) threads as cores. And this is still
    correct - they're beefier than HT threads but still have some
    shared functionality.
    
    Our current approach has a problem with the Mad Max Steam game, for
    example. Yves Dionne reported a certain "choppiness" while playing on
    v4.9.5.
    
    That problem stems most likely from the fact that the CU threads share
    resources within one CU and when we schedule to a thread of a different
    compute unit, this incurs latency due to migrating the working set to a
    different CU through the caches.
    
    When the thread siblings mask mirrors that aspect of the CUs and
    threads, the scheduler pays attention to it and tries to schedule within
    one CU first. Which takes care of the latency, of course.
    
    Reported-by: Yves Dionne <yves.dionne@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org> # 4.9
    Cc: Brice Goglin <Brice.Goglin@inria.fr>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yazen Ghannam <yazen.ghannam@amd.com>
    Link: http://lkml.kernel.org/r/20170205105022.8705-1-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 46732dc3b73c..99b920d0e516 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -433,9 +433,15 @@ static bool match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 		int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
 
 		if (c->phys_proc_id == o->phys_proc_id &&
-		    per_cpu(cpu_llc_id, cpu1) == per_cpu(cpu_llc_id, cpu2) &&
-		    c->cpu_core_id == o->cpu_core_id)
-			return topology_sane(c, o, "smt");
+		    per_cpu(cpu_llc_id, cpu1) == per_cpu(cpu_llc_id, cpu2)) {
+			if (c->cpu_core_id == o->cpu_core_id)
+				return topology_sane(c, o, "smt");
+
+			if ((c->cu_id != 0xff) &&
+			    (o->cu_id != 0xff) &&
+			    (c->cu_id == o->cu_id))
+				return topology_sane(c, o, "smt");
+		}
 
 	} else if (c->phys_proc_id == o->phys_proc_id &&
 		   c->cpu_core_id == o->cpu_core_id) {

commit 9ec808a0225aabab59fb2932b70784b087ac0f58
Author: travis@sgi.com <travis@sgi.com>
Date:   Wed Jan 25 10:35:23 2017 -0600

    x86/platform/UV: Ensure uv_system_init is called when necessary
    
    Move the check to whether this is a UV system that needs initialization
    from is_uv_system() to the internal uv_system_init() function.  This is
    because on a UV system without a HUB the is_uv_system() returns false.
    But we still need some specific UV system initialization.  See the
    uv_system_init() for change to a quick check if UV is applicable. This
    change should not increase overhead since is_uv_system() also called
    into this same area.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Reviewed-by: Russ Anderson <rja@hpe.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Dimitri Sivanich <sivanich@hpe.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20170125163518.256403963@asylum.americas.sgi.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 46732dc3b73c..386c7f713c2a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1341,8 +1341,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	pr_info("CPU0: ");
 	print_cpu_info(&cpu_data(0));
 
-	if (is_uv_system())
-		uv_system_init();
+	uv_system_init();
 
 	set_mtrr_aps_delayed_init();
 

commit 427d77a32365d5f942d335248305a5c237baf63a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 13 19:32:28 2016 +0100

    x86/smpboot: Prevent false positive out of bounds cpumask access warning
    
    prefill_possible_map() reinitializes the cpu_possible_map by setting the
    possible cpu bits and clearing all other bits up to NR_CPUS.
    
    This is technically always correct because cpu_possible_map is statically
    allocated and sized NR_CPUS. With CPUMASK_OFFSTACK and DEBUG_PER_CPU_MAPS
    enabled the bounds check of cpu masks happens on nr_cpu_ids. nr_cpu_ids is
    initialized to NR_CPUS and only limited after the set/clear bit loops have
    been executed.
    
    But if the system was booted with "nr_cpus=N" on the command line, where N
    is < NR_CPUS then nr_cpu_ids is limited in the parameter parsing function
    before prefill_possible_map() is invoked. As a consequence the cpumask
    bounds check triggers when clearing the bits past nr_cpu_ids.
    
    Add a helper which allows to reset cpu_possible_map w/o the bounds check
    and then set only the possible bits which are well inside bounds.
    
    Reported-by: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: 0x7f454c46@gmail.com
    Cc: Jan Beulich <JBeulich@novell.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1612131836050.3415@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e09aa58a7603..46732dc3b73c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1463,15 +1463,15 @@ __init void prefill_possible_map(void)
 		possible = i;
 	}
 
+	nr_cpu_ids = possible;
+
 	pr_info("Allowing %d CPUs, %d hotplug CPUs\n",
 		possible, max_t(int, possible - num_processors, 0));
 
+	reset_cpu_possible_mask();
+
 	for (i = 0; i < possible; i++)
 		set_cpu_possible(i, true);
-	for (; i < NR_CPUS; i++)
-		set_cpu_possible(i, false);
-
-	nr_cpu_ids = possible;
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit 9d85eb9119f4eeeb48e87adfcd71f752655700e9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Dec 12 11:04:53 2016 +0100

    x86/smpboot: Make logical package management more robust
    
    The logical package management has several issues:
    
     - The APIC ids provided by ACPI are not required to be the same as the
       initial APIC id which can be retrieved by CPUID. The APIC ids provided
       by ACPI are those which are written by the BIOS into the APIC. The
       initial id is set by hardware and can not be changed. The hardware
       provided ids contain the real hardware package information.
    
       Especially AMD sets the effective APIC id different from the hardware id
       as they need to reserve space for the IOAPIC ids starting at id 0.
    
       As a consequence those machines trigger the currently active firmware
       bug printouts in dmesg, These are obviously wrong.
    
     - Virtual machines have their own interesting of enumerating APICs and
       packages which are not reliably covered by the current implementation.
    
    The sizing of the mapping array has been tweaked to be generously large to
    handle systems which provide a wrong core count when HT is disabled so the
    whole magic which checks for space in the physical hotplug case is not
    needed anymore.
    
    Simplify the whole machinery and do the mapping when the CPU starts and the
    CPUID derived physical package information is available. This solves the
    observed problems on AMD machines and works for the virtualization issues
    as well.
    
    Remove the extra call from XEN cpu bringup code as it is not longer
    required.
    
    Fixes: d49597fd3bc7 ("x86/cpu: Deal with broken firmware (VMWare/XEN)")
    Reported-and-tested-by: Borislav Petkov <bp@suse.de>
    Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: M. Vefa Bicakci <m.v.b@runbox.com>
    Cc: xen-devel <xen-devel@lists.xen.org>
    Cc: Charles (Chas) Williams <ciwillia@brocade.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1612121102260.3429@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0c37d4fd01b2..e09aa58a7603 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -103,7 +103,6 @@ static unsigned int max_physical_pkg_id __read_mostly;
 unsigned int __max_logical_packages __read_mostly;
 EXPORT_SYMBOL(__max_logical_packages);
 static unsigned int logical_packages __read_mostly;
-static bool logical_packages_frozen __read_mostly;
 
 /* Maximum number of SMT threads on any online core */
 int __max_smt_threads __read_mostly;
@@ -273,9 +272,14 @@ static void notrace start_secondary(void *unused)
 	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
 }
 
-int topology_update_package_map(unsigned int apicid, unsigned int cpu)
+/**
+ * topology_update_package_map - Update the physical to logical package map
+ * @pkg:	The physical package id as retrieved via CPUID
+ * @cpu:	The cpu for which this is updated
+ */
+int topology_update_package_map(unsigned int pkg, unsigned int cpu)
 {
-	unsigned int new, pkg = apicid >> boot_cpu_data.x86_coreid_bits;
+	unsigned int new;
 
 	/* Called from early boot ? */
 	if (!physical_package_map)
@@ -288,16 +292,17 @@ int topology_update_package_map(unsigned int apicid, unsigned int cpu)
 	if (test_and_set_bit(pkg, physical_package_map))
 		goto found;
 
-	if (logical_packages_frozen) {
-		physical_to_logical_pkg[pkg] = -1;
-		pr_warn("APIC(%x) Package %u exceeds logical package max\n",
-			apicid, pkg);
+	if (logical_packages >= __max_logical_packages) {
+		pr_warn("Package %u of CPU %u exceeds BIOS package data %u.\n",
+			logical_packages, cpu, __max_logical_packages);
 		return -ENOSPC;
 	}
 
 	new = logical_packages++;
-	pr_info("APIC(%x) Converting physical %u to logical package %u\n",
-		apicid, pkg, new);
+	if (new != pkg) {
+		pr_info("CPU %u Converting physical %u to logical package %u\n",
+			cpu, pkg, new);
+	}
 	physical_to_logical_pkg[pkg] = new;
 
 found:
@@ -318,9 +323,9 @@ int topology_phys_to_logical_pkg(unsigned int phys_pkg)
 }
 EXPORT_SYMBOL(topology_phys_to_logical_pkg);
 
-static void __init smp_init_package_map(void)
+static void __init smp_init_package_map(struct cpuinfo_x86 *c, unsigned int cpu)
 {
-	unsigned int ncpus, cpu;
+	unsigned int ncpus;
 	size_t size;
 
 	/*
@@ -365,27 +370,9 @@ static void __init smp_init_package_map(void)
 	size = BITS_TO_LONGS(max_physical_pkg_id) * sizeof(unsigned long);
 	physical_package_map = kzalloc(size, GFP_KERNEL);
 
-	for_each_present_cpu(cpu) {
-		unsigned int apicid = apic->cpu_present_to_apicid(cpu);
-
-		if (apicid == BAD_APICID || !apic->apic_id_valid(apicid))
-			continue;
-		if (!topology_update_package_map(apicid, cpu))
-			continue;
-		pr_warn("CPU %u APICId %x disabled\n", cpu, apicid);
-		per_cpu(x86_bios_cpu_apicid, cpu) = BAD_APICID;
-		set_cpu_possible(cpu, false);
-		set_cpu_present(cpu, false);
-	}
-
-	if (logical_packages > __max_logical_packages) {
-		pr_warn("Detected more packages (%u), then computed by BIOS data (%u).\n",
-			logical_packages, __max_logical_packages);
-		logical_packages_frozen = true;
-		__max_logical_packages  = logical_packages;
-	}
-
 	pr_info("Max logical packages: %u\n", __max_logical_packages);
+
+	topology_update_package_map(c->phys_proc_id, cpu);
 }
 
 void __init smp_store_boot_cpu_info(void)
@@ -395,7 +382,7 @@ void __init smp_store_boot_cpu_info(void)
 
 	*c = boot_cpu_data;
 	c->cpu_index = id;
-	smp_init_package_map();
+	smp_init_package_map(c, id);
 }
 
 /*

commit 212f30008a284a9312d95dad6cc237ff81173d73
Merge: 6f3be0f04354 34bc3560c657
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 14:55:04 2016 -0800

    Merge branch 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 idle updates from Ingo Molnar:
     "There were two bigger changes in this development cycle:
    
       - remove idle notifiers:
    
           32 files changed, 74 insertions(+), 803 deletions(-)
    
         These notifiers were of questionable value and the main usecase,
         the i7300 driver, was essentially unmaintained and can be removed,
         plus modern power management concepts don't need the callback - so
         use this golden opportunity and get rid of this opaque and fragile
         callback from a latency sensitive code path.
    
         (Len Brown, Thomas Gleixner)
    
       - improve the AMD Erratum 400 workaround that used high overhead MSR
         polling in the idle loop (Borisla Petkov, Thomas Gleixner)"
    
    * 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Remove empty idle.h header
      x86/amd: Simplify AMD E400 aware idle routine
      x86/amd: Check for the C1E bug post ACPI subsystem init
      x86/bugs: Separate AMD E400 erratum and C1E bug
      x86/cpufeature: Provide helper to set bugs bits
      x86/idle: Remove enter_idle(), exit_idle()
      x86: Remove x86_test_and_clear_bit_percpu()
      x86/idle: Remove is_idle flag
      x86/idle: Remove idle_notifier
      i7300_idle: Remove this driver

commit 518bacf5a569d111e256d58b9fbc8d7b80ec42ea
Merge: 535b2f73f6f6 064e6a8ba61a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 14:27:49 2016 -0800

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - do a large round of simplifications after all CPUs do 'eager' FPU
         context switching in v4.9: remove CR0 twiddling, remove leftover
         eager/lazy bts, etc (Andy Lutomirski)
    
       - more FPU code simplifications: remove struct fpu::counter, clarify
         nomenclature, remove unnecessary arguments/functions and better
         structure the code (Rik van Riel)"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fpu: Remove clts()
      x86/fpu: Remove stts()
      x86/fpu: Handle #NM without FPU emulation as an error
      x86/fpu, lguest: Remove CR0.TS support
      x86/fpu, kvm: Remove host CR0.TS manipulation
      x86/fpu: Remove irq_ts_save() and irq_ts_restore()
      x86/fpu: Stop saving and restoring CR0.TS in fpu__init_check_bugs()
      x86/fpu: Get rid of two redundant clts() calls
      x86/fpu: Finish excising 'eagerfpu'
      x86/fpu: Split old_fpu & new_fpu handling into separate functions
      x86/fpu: Remove 'cpu' argument from __cpu_invalidate_fpregs_state()
      x86/fpu: Split old & new FPU code paths
      x86/fpu: Remove __fpregs_(de)activate()
      x86/fpu: Rename lazy restore functions to "register state valid"
      x86/fpu, kvm: Remove KVM vcpu->fpu_counter
      x86/fpu: Remove struct fpu::counter
      x86/fpu: Remove use_eager_fpu()
      x86/fpu: Remove the XFEATURE_MASK_EAGER/LAZY distinction
      x86/fpu: Hard-disable lazy FPU mode
      x86/crypto, x86/fpu: Remove X86_FEATURE_EAGER_FPU #ifdef from the crc32c code

commit 535b2f73f6f60fb227b700136c134c5d7c8f8ad3
Merge: ef486c599a1f b6a50cddbcbd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 14:25:21 2016 -0800

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 CPU updates from Ingo Molnar:
     "The changes in this development cycle were:
    
       - AMD CPU topology enhancements that are cleanups on current CPUs but
         which enable future Fam17 hardware. (Yazen Ghannam)
    
       - unify bugs.c and bugs_64.c (Borislav Petkov)
    
       - remove the show_msr= boot option (Borislav Petkov)
    
       - simplify a boot message (Borislav Petkov)"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/cpu/AMD: Clean up cpu_llc_id assignment per topology feature
      x86/cpu: Get rid of the show_msr= boot option
      x86/cpu: Merge bugs.c and bugs_64.c
      x86/cpu: Remove the printk format specifier in "CPU0: "

commit 5645688f9d0d5a32f030f9c5429e1a58bedca23b
Merge: 4ade5b2268b9 53938ee427bf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 13:49:57 2016 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this development cycle were:
    
       - a large number of call stack dumping/printing improvements: higher
         robustness, better cross-context dumping, improved output, etc.
         (Josh Poimboeuf)
    
       - vDSO getcpu() performance improvement for future Intel CPUs with
         the RDPID instruction (Andy Lutomirski)
    
       - add two new Intel AVX512 features and the CPUID support
         infrastructure for it: AVX512IFMA and AVX512VBMI. (Gayatri Kammela,
         He Chen)
    
       - more copy-user unification (Borislav Petkov)
    
       - entry code assembly macro simplifications (Alexander Kuleshov)
    
       - vDSO C/R support improvements (Dmitry Safonov)
    
       - misc fixes and cleanups (Borislav Petkov, Paul Bolle)"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (40 commits)
      scripts/decode_stacktrace.sh: Fix address line detection on x86
      x86/boot/64: Use defines for page size
      x86/dumpstack: Make stack name tags more comprehensible
      selftests/x86: Add test_vdso to test getcpu()
      x86/vdso: Use RDPID in preference to LSL when available
      x86/dumpstack: Handle NULL stack pointer in show_trace_log_lvl()
      x86/cpufeatures: Enable new AVX512 cpu features
      x86/cpuid: Provide get_scattered_cpuid_leaf()
      x86/cpuid: Cleanup cpuid_regs definitions
      x86/copy_user: Unify the code by removing the 64-bit asm _copy_*_user() variants
      x86/unwind: Ensure stack grows down
      x86/vdso: Set vDSO pointer only after success
      x86/prctl/uapi: Remove #ifdef for CHECKPOINT_RESTORE
      x86/unwind: Detect bad stack return address
      x86/dumpstack: Warn on stack recursion
      x86/unwind: Warn on bad frame pointer
      x86/decoder: Use stderr if insn sanity test fails
      x86/decoder: Use stdout if insn decoder test is successful
      mm/page_alloc: Remove kernel address exposure in free_reserved_area()
      x86/dumpstack: Remove raw stack dump
      ...

commit 92c020d08d83673ecd15a9069d4457378668da31
Merge: bca13ce4554a 6b94780e45c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 12:15:10 2016 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main scheduler changes in this cycle were:
    
       - support Intel Turbo Boost Max Technology 3.0 (TBM3) by introducig a
         notion of 'better cores', which the scheduler will prefer to
         schedule single threaded workloads on. (Tim Chen, Srinivas
         Pandruvada)
    
       - enhance the handling of asymmetric capacity CPUs further (Morten
         Rasmussen)
    
       - improve/fix load handling when moving tasks between task groups
         (Vincent Guittot)
    
       - simplify and clean up the cputime code (Stanislaw Gruszka)
    
       - improve mass fork()ed task spread a.k.a. hackbench speedup (Vincent
         Guittot)
    
       - make struct kthread kmalloc()ed and related fixes (Oleg Nesterov)
    
       - add uaccess atomicity debugging (when using access_ok() in the
         wrong context), under CONFIG_DEBUG_ATOMIC_SLEEP=y (Peter Zijlstra)
    
       - implement various fixes, cleanups and other enhancements (Daniel
         Bristot de Oliveira, Martin Schwidefsky, Rafael J. Wysocki)"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (41 commits)
      sched/core: Use load_avg for selecting idlest group
      sched/core: Fix find_idlest_group() for fork
      kthread: Don't abuse kthread_create_on_cpu() in __kthread_create_worker()
      kthread: Don't use to_live_kthread() in kthread_[un]park()
      kthread: Don't use to_live_kthread() in kthread_stop()
      Revert "kthread: Pin the stack via try_get_task_stack()/put_task_stack() in to_live_kthread() function"
      kthread: Make struct kthread kmalloc'ed
      x86/uaccess, sched/preempt: Verify access_ok() context
      sched/x86: Make CONFIG_SCHED_MC_PRIO=y easier to enable
      sched/x86: Change CONFIG_SCHED_ITMT to CONFIG_SCHED_MC_PRIO
      x86/sched: Use #include <linux/mutex.h> instead of #include <asm/mutex.h>
      cpufreq/intel_pstate: Use CPPC to get max performance
      acpi/bus: Set _OSC for diverse core support
      acpi/bus: Enable HWP CPPC objects
      x86/sched: Add SD_ASYM_PACKING flags to x86 ITMT CPU
      x86/sysctl: Add sysctl for ITMT scheduling feature
      x86: Enable Intel Turbo Boost Max Technology 3.0
      x86/topology: Define x86's arch_update_cpu_topology
      sched: Extend scheduler's asym packing
      sched/fair: Clean up the tunable parameter definitions
      ...

commit 34bc3560c657d3d4fb17367ed9bfda803166dce0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Dec 9 19:29:12 2016 +0100

    x86: Remove empty idle.h header
    
    One include less is always a good thing(tm). Good riddance.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/20161209182912.2726-6-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0229ccbfcd66..352ddd560d19 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -58,7 +58,6 @@
 #include <asm/desc.h>
 #include <asm/nmi.h>
 #include <asm/irq.h>
-#include <asm/idle.h>
 #include <asm/realmode.h>
 #include <asm/cpu.h>
 #include <asm/numa.h>

commit 07c94a38125376d70d156bd8bff98ddfe4c8ea95
Author: Borislav Petkov <bp@alien8.de>
Date:   Fri Dec 9 19:29:11 2016 +0100

    x86/amd: Simplify AMD E400 aware idle routine
    
    Reorganize the E400 detection now that we have everything in place:
    switch the CPUs to broadcast mode after the LAPIC has been initialized
    and remove the facilities that were used previously on the idle path.
    
    Unfortunately static_cpu_has_bug() cannpt be used in the E400 idle routine
    because alternatives have been applied when the actual detection happens,
    so the static switching does not take effect and the test will stay
    false. Use boot_cpu_has_bug() instead which is definitely an improvement
    over the RDMSR and the cpumask handling.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/20161209182912.2726-5-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 42f5eb7b4f6c..0229ccbfcd66 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1575,7 +1575,6 @@ void play_dead_common(void)
 {
 	idle_task_exit();
 	reset_lazy_tlbstate();
-	amd_e400_remove_cpu(raw_smp_processor_id());
 
 	/* Ack it */
 	(void)cpu_report_death();

commit d3d37d850d1d77bd66bceb8326e6353d3314b270
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Tue Nov 22 12:23:57 2016 -0800

    x86/sched: Add SD_ASYM_PACKING flags to x86 ITMT CPU
    
    Some Intel cores in a package can be boosted to a higher turbo frequency
    with ITMT 3.0 technology. The scheduler can use the asymmetric packing
    feature to move tasks to the more capable cores.
    
    If ITMT is enabled, add SD_ASYM_PACKING flag to the thread and core
    sched domains to enable asymmetric packing.
    
    Co-developed-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: linux-pm@vger.kernel.org
    Cc: peterz@infradead.org
    Cc: jolsa@redhat.com
    Cc: rjw@rjwysocki.net
    Cc: linux-acpi@vger.kernel.org
    Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Cc: bp@suse.de
    Link: http://lkml.kernel.org/r/9bbb885bedbef4eb50e197305eb16b160cff0831.1479844244.git.tim.c.chen@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ac61ee71d50e..4f130624c3f4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -482,22 +482,42 @@ static bool match_die(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 	return false;
 }
 
+#if defined(CONFIG_SCHED_SMT) || defined(CONFIG_SCHED_MC)
+static inline int x86_sched_itmt_flags(void)
+{
+	return sysctl_sched_itmt_enabled ? SD_ASYM_PACKING : 0;
+}
+
+#ifdef CONFIG_SCHED_MC
+static int x86_core_flags(void)
+{
+	return cpu_core_flags() | x86_sched_itmt_flags();
+}
+#endif
+#ifdef CONFIG_SCHED_SMT
+static int x86_smt_flags(void)
+{
+	return cpu_smt_flags() | x86_sched_itmt_flags();
+}
+#endif
+#endif
+
 static struct sched_domain_topology_level x86_numa_in_package_topology[] = {
 #ifdef CONFIG_SCHED_SMT
-	{ cpu_smt_mask, cpu_smt_flags, SD_INIT_NAME(SMT) },
+	{ cpu_smt_mask, x86_smt_flags, SD_INIT_NAME(SMT) },
 #endif
 #ifdef CONFIG_SCHED_MC
-	{ cpu_coregroup_mask, cpu_core_flags, SD_INIT_NAME(MC) },
+	{ cpu_coregroup_mask, x86_core_flags, SD_INIT_NAME(MC) },
 #endif
 	{ NULL, },
 };
 
 static struct sched_domain_topology_level x86_topology[] = {
 #ifdef CONFIG_SCHED_SMT
-	{ cpu_smt_mask, cpu_smt_flags, SD_INIT_NAME(SMT) },
+	{ cpu_smt_mask, x86_smt_flags, SD_INIT_NAME(SMT) },
 #endif
 #ifdef CONFIG_SCHED_MC
-	{ cpu_coregroup_mask, cpu_core_flags, SD_INIT_NAME(MC) },
+	{ cpu_coregroup_mask, x86_core_flags, SD_INIT_NAME(MC) },
 #endif
 	{ cpu_cpu_mask, SD_INIT_NAME(DIE) },
 	{ NULL, },

commit 7d25127cef44924f1013d119ba385095ca4b4a83
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Tue Nov 22 12:23:54 2016 -0800

    x86/topology: Define x86's arch_update_cpu_topology
    
    The scheduler calls arch_update_cpu_topology() to check whether the
    scheduler domains have to be rebuilt.
    
    So far x86 has no requirement for this, but the upcoming ITMT support
    makes this necessary.
    
    Request the rebuild when the x86 internal update flag is set.
    
    Suggested-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: linux-pm@vger.kernel.org
    Cc: peterz@infradead.org
    Cc: jolsa@redhat.com
    Cc: rjw@rjwysocki.net
    Cc: linux-acpi@vger.kernel.org
    Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Cc: bp@suse.de
    Link: http://lkml.kernel.org/r/bfbf5591276ec60b2af2da798adc1060df1e2a5f.1479844244.git.tim.c.chen@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 42f5eb7b4f6c..ac61ee71d50e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -109,6 +109,17 @@ static bool logical_packages_frozen __read_mostly;
 /* Maximum number of SMT threads on any online core */
 int __max_smt_threads __read_mostly;
 
+/* Flag to indicate if a complete sched domain rebuild is required */
+bool x86_topology_update;
+
+int arch_update_cpu_topology(void)
+{
+	int retval = x86_topology_update;
+
+	x86_topology_update = false;
+	return retval;
+}
+
 static inline void smpboot_setup_warm_reset_vector(unsigned long start_eip)
 {
 	unsigned long flags;

commit c29c716662d0cefc0cda4903aea5ed6794174679
Merge: e63650840e8b 405c0759712f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 1 07:47:40 2016 +0100

    Merge branch 'core/urgent' into x86/fpu, to merge fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 05b93c19d50af2bd0d30fc000d817418ae8d33f1
Merge: 24d86f59093b 0c183d92b20b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 1 07:41:06 2016 +0100

    Merge branch 'linus' into x86/asm, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 92b23278298304f72bbc786a737f2646f4b9aa9d
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Oct 26 16:37:54 2016 +1100

    kernel/smp: Make the SMP boot message common on all arches
    
    Currently after bringing up secondary CPUs all arches print "Brought up
    %d CPUs". On x86 they also print the number of nodes that were brought
    online.
    
    It would be nice to also print the number of nodes on other arches.
    Although we could override smp_announce() on the other ~10 NUMA aware
    arches, it seems simpler to just always print the number of nodes. On
    non-NUMA arches there is just always 1 node.
    
    Having done that, smp_announce() is no longer weak, and seems small
    enough to just pull directly into smp_init().
    
    Also update the printing of "%d CPUs" to be smart when an SMP kernel is
    booted on a single CPU system, or when only one CPU is available, eg:
    
       smp: Brought up 2 nodes, 1 CPU
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: akpm@osdl.org
    Cc: jgross@suse.com
    Cc: ak@linux.intel.com
    Cc: tim.c.chen@linux.intel.com
    Cc: len.brown@intel.com
    Cc: peterz@infradead.org
    Cc: richard@nod.at
    Cc: jolsa@redhat.com
    Cc: boris.ostrovsky@oracle.com
    Cc: mgorman@techsingularity.net
    Link: http://lkml.kernel.org/r/1477460275-8266-2-git-send-email-mpe@ellerman.id.au
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 42f5eb7b4f6c..b9f02383f372 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -821,14 +821,6 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	return (send_status | accept_status);
 }
 
-void smp_announce(void)
-{
-	int num_nodes = num_online_nodes();
-
-	printk(KERN_INFO "x86: Booted up %d node%s, %d CPUs\n",
-	       num_nodes, (num_nodes > 1 ? "s" : ""), num_online_cpus());
-}
-
 /* reduce the number of lines printed when booting a large cpu count system */
 static void announce_cpu(int cpu, int apicid)
 {

commit d54ff31dd86c6df734d6b2d6382206f2b0816642
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Oct 24 19:38:42 2016 +0200

    x86/cpu: Remove the printk format specifier in "CPU0: "
    
    We're using a literal, move it into the string.
    
    No functionality change.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20161024173844.23038-2-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 42f5eb7b4f6c..e2e0e35e7fc4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1331,7 +1331,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	default_setup_apic_routing();
 	cpu0_logical_apicid = apic_bsp_setup(false);
 
-	pr_info("CPU%d: ", 0);
+	pr_info("CPU0: ");
 	print_cpu_info(&cpu_data(0));
 
 	if (is_uv_system())

commit ff8560512b8d4b7ca3ef4fd69166634ac30b2525
Author: Ville Syrjälä <ville.syrjala@linux.intel.com>
Date:   Sat Oct 22 05:18:04 2016 +0300

    x86/boot/smp: Don't try to poke disabled/non-existent APIC
    
    Apparently trying to poke a disabled or non-existent APIC
    leads to a box that doesn't even boot. Let's not do that.
    
    No real clue if this is the right fix, but at least my
    P3 machine boots again.
    
    Signed-off-by: Ville Syrjälä <ville.syrjala@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: dyoung@redhat.com
    Cc: kexec@lists.infradead.org
    Cc: stable@vger.kernel.org
    Fixes: 2a51fe083eba ("arch/x86: Handle non enumerated CPU after physical hotplug")
    Link: http://lkml.kernel.org/r/1477102684-5092-1-git-send-email-ville.syrjala@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 951f093a96fe..42f5eb7b4f6c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1409,15 +1409,17 @@ __init void prefill_possible_map(void)
 
 	/* No boot processor was found in mptable or ACPI MADT */
 	if (!num_processors) {
-		int apicid = boot_cpu_physical_apicid;
-		int cpu = hard_smp_processor_id();
+		if (boot_cpu_has(X86_FEATURE_APIC)) {
+			int apicid = boot_cpu_physical_apicid;
+			int cpu = hard_smp_processor_id();
 
-		pr_warn("Boot CPU (id %d) not listed by BIOS\n", cpu);
+			pr_warn("Boot CPU (id %d) not listed by BIOS\n", cpu);
 
-		/* Make sure boot cpu is enumerated */
-		if (apic->cpu_present_to_apicid(0) == BAD_APICID &&
-		    apic->apic_id_valid(apicid))
-			generic_processor_info(apicid, boot_cpu_apic_version);
+			/* Make sure boot cpu is enumerated */
+			if (apic->cpu_present_to_apicid(0) == BAD_APICID &&
+			    apic->apic_id_valid(apicid))
+				generic_processor_info(apicid, boot_cpu_apic_version);
+		}
 
 		if (!num_processors)
 			num_processors = 1;

commit b9b1a9c363ff7b17b2a35e20e28e86a449cfde1f
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Wed Sep 21 16:04:03 2016 -0500

    x86/boot/smp/32: Fix initial idle stack location on 32-bit kernels
    
    On 32-bit kernels, the initial idle stack calculation doesn't take into
    account the TOP_OF_KERNEL_STACK_PADDING, making the stack end address
    inconsistent with other tasks on 32-bit.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nilay Vaish <nilayvaish@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/6cf569410bfa84cf923902fc4d628444cace94be.1474480779.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 951f093a96fe..dcbd45ad8db6 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -964,9 +964,7 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	int cpu0_nmi_registered = 0;
 	unsigned long timeout;
 
-	idle->thread.sp = (unsigned long) (((struct pt_regs *)
-			  (THREAD_SIZE +  task_stack_page(idle))) - 1);
-
+	idle->thread.sp = (unsigned long)task_pt_regs(idle);
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	initial_code = (unsigned long)start_secondary;
 	initial_stack  = idle->thread.sp;

commit 4d69f155d58d0f75c5404ea502178b1943a04755
Merge: c474e50711aa 1001354ca341
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Oct 16 13:04:34 2016 +0200

    Merge tag 'v4.9-rc1' into x86/fpu, to resolve conflict
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 317b622cb2fda1812d8646e211cdb23dce2564d0
Author: Rik van Riel <riel@redhat.com>
Date:   Fri Oct 14 08:15:30 2016 -0400

    x86/fpu: Remove 'cpu' argument from __cpu_invalidate_fpregs_state()
    
    The __{fpu,cpu}_invalidate_fpregs_state() functions can only be used
    to invalidate a resource they control.  Document that, and change
    the API a little bit to reflect that.
    
    Go back to open coding the fpu_fpregs_owner_ctx write in the CPU
    hotplug code, which should be the exception, and move __kernel_fpu_begin()
    to this API.
    
    This patch has no functional changes to the current code.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Dave Hansen <dave.hansen@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1476447331-21566-2-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ca4c4ca2f6af..5cb801acc2e5 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1111,7 +1111,7 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 		return err;
 
 	/* the FPU context is blank, nobody can own it */
-	__cpu_invalidate_fpregs_state(cpu);
+	per_cpu(fpu_fpregs_owner_ctx, cpu) = NULL;
 
 	common_cpu_up(cpu, tidle);
 

commit 2a51fe083eba7f99cbda72f5ef90cdf2f4df882c
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Mon Oct 3 13:07:12 2016 -0400

    arch/x86: Handle non enumerated CPU after physical hotplug
    
    When a CPU is physically added to a system then the MADT table is not
    updated.
    
    If subsequently a kdump kernel is started on that physically added CPU then
    the ACPI enumeration fails to provide the information for this CPU which is
    now the boot CPU of the kdump kernel.
    
    As a consequence, generic_processor_info() is not invoked for that CPU so
    the number of enumerated processors is 0 and none of the initializations,
    including the logical package id management, are performed.
    
    We have code which relies on the correctness of the logical package map and
    other information which is initialized via generic_processor_info().
    Executing such code will result in undefined behaviour or kernel crashes.
    
    This problem applies only to the kdump kernel because a normal kexec will
    switch to the original boot CPU, which is enumerated in MADT, before
    jumping into the kexec kernel.
    
    The boot code already has a check for num_processors equal 0 in
    prefill_possible_map(). We can use that check as an indicator that the
    enumeration of the boot CPU did not happen and invoke generic_processor_info()
    for it. That initializes the relevant data for the boot CPU and therefore
    prevents subsequent failure.
    
    [ tglx: Refined the code and rewrote the changelog ]
    
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Fixes: 1f12e32f4cd5 ("x86/topology: Create logical package id")
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: dyoung@redhat.com
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: kexec@lists.infradead.org
    Link: http://lkml.kernel.org/r/1475514432-27682-1-git-send-email-prarit@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 42a93621f5b0..951f093a96fe 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1407,9 +1407,21 @@ __init void prefill_possible_map(void)
 {
 	int i, possible;
 
-	/* no processor from mptable or madt */
-	if (!num_processors)
-		num_processors = 1;
+	/* No boot processor was found in mptable or ACPI MADT */
+	if (!num_processors) {
+		int apicid = boot_cpu_physical_apicid;
+		int cpu = hard_smp_processor_id();
+
+		pr_warn("Boot CPU (id %d) not listed by BIOS\n", cpu);
+
+		/* Make sure boot cpu is enumerated */
+		if (apic->cpu_present_to_apicid(0) == BAD_APICID &&
+		    apic->apic_id_valid(apicid))
+			generic_processor_info(apicid, boot_cpu_apic_version);
+
+		if (!num_processors)
+			num_processors = 1;
+	}
 
 	i = setup_max_cpus ?: 1;
 	if (setup_possible_cpus == -1) {

commit 25d83b531c1aa4fca5b4e24ed10f493268f162bc
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Oct 4 20:34:36 2016 -0400

    x86/fpu: Rename lazy restore functions to "register state valid"
    
    Name the functions after the state they track, rather than the function
    they currently enable. This should make it more obvious when we use the
    fpu_register_state_valid() function for something else in the future.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: pbonzini@redhat.com
    Link: http://lkml.kernel.org/r/1475627678-20788-8-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 42a93621f5b0..ca4c4ca2f6af 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1111,7 +1111,7 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 		return err;
 
 	/* the FPU context is blank, nobody can own it */
-	__cpu_disable_lazy_restore(cpu);
+	__cpu_invalidate_fpregs_state(cpu);
 
 	common_cpu_up(cpu, tidle);
 

commit 597f03f9d133e9837d00965016170271d4f87dcf
Merge: 999dcbe2414e 0bf71e4d02ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 19:43:08 2016 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug updates from Thomas Gleixner:
     "Yet another batch of cpu hotplug core updates and conversions:
    
       - Provide core infrastructure for multi instance drivers so the
         drivers do not have to keep custom lists.
    
       - Convert custom lists to the new infrastructure. The block-mq custom
         list conversion comes through the block tree and makes the diffstat
         tip over to more lines removed than added.
    
       - Handle unbalanced hotplug enable/disable calls more gracefully.
    
       - Remove the obsolete CPU_STARTING/DYING notifier support.
    
       - Convert another batch of notifier users.
    
       The relayfs changes which conflicted with the conversion have been
       shipped to me by Andrew.
    
       The remaining lot is targeted for 4.10 so that we finally can remove
       the rest of the notifiers"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (46 commits)
      cpufreq: Fix up conversion to hotplug state machine
      blk/mq: Reserve hotplug states for block multiqueue
      x86/apic/uv: Convert to hotplug state machine
      s390/mm/pfault: Convert to hotplug state machine
      mips/loongson/smp: Convert to hotplug state machine
      mips/octeon/smp: Convert to hotplug state machine
      fault-injection/cpu: Convert to hotplug state machine
      padata: Convert to hotplug state machine
      cpufreq: Convert to hotplug state machine
      ACPI/processor: Convert to hotplug state machine
      virtio scsi: Convert to hotplug state machine
      oprofile/timer: Convert to hotplug state machine
      block/softirq: Convert to hotplug state machine
      lib/irq_poll: Convert to hotplug state machine
      x86/microcode: Convert to hotplug state machine
      sh/SH-X3 SMP: Convert to hotplug state machine
      ia64/mca: Convert to hotplug state machine
      ARM/OMAP/wakeupgen: Convert to hotplug state machine
      ARM/shmobile: Convert to hotplug state machine
      arm64/FP/SIMD: Convert to hotplug state machine
      ...

commit 1a4a2bc460721bc8f91e4c1294d39b38e5af132f
Merge: 110a9e42b687 1ef55be16ed6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 16:13:28 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull low-level x86 updates from Ingo Molnar:
     "In this cycle this topic tree has become one of those 'super topics'
      that accumulated a lot of changes:
    
       - Add CONFIG_VMAP_STACK=y support to the core kernel and enable it on
         x86 - preceded by an array of changes. v4.8 saw preparatory changes
         in this area already - this is the rest of the work. Includes the
         thread stack caching performance optimization. (Andy Lutomirski)
    
       - switch_to() cleanups and all around enhancements. (Brian Gerst)
    
       - A large number of dumpstack infrastructure enhancements and an
         unwinder abstraction. The secret long term plan is safe(r) live
         patching plus maybe another attempt at debuginfo based unwinding -
         but all these current bits are standalone enhancements in a frame
         pointer based debug environment as well. (Josh Poimboeuf)
    
       - More __ro_after_init and const annotations. (Kees Cook)
    
       - Enable KASLR for the vmemmap memory region. (Thomas Garnier)"
    
    [ The virtually mapped stack changes are pretty fundamental, and not
      x86-specific per se, even if they are only used on x86 right now. ]
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      x86/asm: Get rid of __read_cr4_safe()
      thread_info: Use unsigned long for flags
      x86/alternatives: Add stack frame dependency to alternative_call_2()
      x86/dumpstack: Fix show_stack() task pointer regression
      x86/dumpstack: Remove dump_trace() and related callbacks
      x86/dumpstack: Convert show_trace_log_lvl() to use the new unwinder
      oprofile/x86: Convert x86_backtrace() to use the new unwinder
      x86/stacktrace: Convert save_stack_trace_*() to use the new unwinder
      perf/x86: Convert perf_callchain_kernel() to use the new unwinder
      x86/unwind: Add new unwind interface and implementations
      x86/dumpstack: Remove NULL task pointer convention
      fork: Optimize task creation by caching two thread stacks per CPU if CONFIG_VMAP_STACK=y
      sched/core: Free the stack early if CONFIG_THREAD_INFO_IN_TASK
      lib/syscall: Pin the task stack in collect_syscall()
      x86/process: Pin the target stack in get_wchan()
      x86/dumpstack: Pin the target stack when dumping it
      kthread: Pin the stack via try_get_task_stack()/put_task_stack() in to_live_kthread() function
      sched/core: Add try_get_task_stack() and put_task_stack()
      x86/entry/64: Fix a minor comment rebase error
      iommu/amd: Don't put completion-wait semaphore on stack
      ...

commit 110a9e42b68719f584879c5c5c727bbae90d15f9
Merge: af79ad2b1f33 eb6296dec19f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 15:36:06 2016 -0700

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 apic updates from Ingo Molnar:
     "The main changes are:
    
       - Persistent CPU/node numbering across CPU hotplug/unplug events.
         This is a pretty involved series of changes that first fetches all
         the information during bootup and then uses it for the various
         hotplug/unplug methods. (Gu Zheng, Dou Liyang)
    
       - IO-APIC hot-add/remove fixes and enhancements. (Rui Wang)
    
       - ... various fixes, cleanups and enhancements"
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (22 commits)
      x86/apic: Fix silent & fatal merge conflict in __generic_processor_info()
      acpi: Fix broken error check in map_processor()
      acpi: Validate processor id when mapping the processor
      acpi: Provide mechanism to validate processors in the ACPI tables
      x86/acpi: Set persistent cpuid <-> nodeid mapping when booting
      x86/acpi: Enable MADT APIs to return disabled apicids
      x86/acpi: Introduce persistent storage for cpuid <-> apicid mapping
      x86/acpi: Enable acpi to register all possible cpus at boot time
      x86/numa: Online memory-less nodes at boot time
      x86/apic: Get rid of apic_version[] array
      x86/apic: Order irq_enter/exit() calls correctly vs. ack_APIC_irq()
      x86/ioapic: Ignore root bridges without a companion ACPI device
      x86/apic: Update comment about disabling processor focus
      x86/smpboot: Check APIC ID before setting up default routing
      x86/ioapic: Fix IOAPIC failing to request resource
      x86/ioapic: Fix lost IOAPIC resource after hot-removal and hotadd
      x86/ioapic: Fix setup_res() failing to get resource
      x86/ioapic: Support hot-removal of IOAPICs present during boot
      x86/ioapic: Change prototype of acpi_ioapic_add()
      x86/apic, ACPI: Fix incorrect assignment when handling apic/x2apic entries
      ...

commit 8f37961cf22304fb286c7604d3a7f6104dcc1283
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Wed Sep 21 12:19:03 2016 -0700

    sched/core, x86/topology: Fix NUMA in package topology bug
    
    Current code can call set_cpu_sibling_map() and invoke sched_set_topology()
    more than once (e.g. on CPU hot plug).  When this happens after
    sched_init_smp() has been called, we lose the NUMA topology extension to
    sched_domain_topology in sched_init_numa().  This results in incorrect
    topology when the sched domain is rebuilt.
    
    This patch fixes the bug and issues warning if we call sched_set_topology()
    after sched_init_smp().
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@suse.de
    Cc: jolsa@redhat.com
    Cc: rjw@rjwysocki.net
    Link: http://lkml.kernel.org/r/1474485552-141429-2-git-send-email-srinivas.pandruvada@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 4296beb8fdd3..7137ec4eea9a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -471,7 +471,7 @@ static bool match_die(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 	return false;
 }
 
-static struct sched_domain_topology_level numa_inside_package_topology[] = {
+static struct sched_domain_topology_level x86_numa_in_package_topology[] = {
 #ifdef CONFIG_SCHED_SMT
 	{ cpu_smt_mask, cpu_smt_flags, SD_INIT_NAME(SMT) },
 #endif
@@ -480,22 +480,23 @@ static struct sched_domain_topology_level numa_inside_package_topology[] = {
 #endif
 	{ NULL, },
 };
+
+static struct sched_domain_topology_level x86_topology[] = {
+#ifdef CONFIG_SCHED_SMT
+	{ cpu_smt_mask, cpu_smt_flags, SD_INIT_NAME(SMT) },
+#endif
+#ifdef CONFIG_SCHED_MC
+	{ cpu_coregroup_mask, cpu_core_flags, SD_INIT_NAME(MC) },
+#endif
+	{ cpu_cpu_mask, SD_INIT_NAME(DIE) },
+	{ NULL, },
+};
+
 /*
- * set_sched_topology() sets the topology internal to a CPU.  The
- * NUMA topologies are layered on top of it to build the full
- * system topology.
- *
- * If NUMA nodes are observed to occur within a CPU package, this
- * function should be called.  It forces the sched domain code to
- * only use the SMT level for the CPU portion of the topology.
- * This essentially falls back to relying on NUMA information
- * from the SRAT table to describe the entire system topology
- * (except for hyperthreads).
+ * Set if a package/die has multiple NUMA nodes inside.
+ * AMD Magny-Cours and Intel Cluster-on-Die have this.
  */
-static void primarily_use_numa_for_topology(void)
-{
-	set_sched_topology(numa_inside_package_topology);
-}
+static bool x86_has_numa_in_package;
 
 void set_cpu_sibling_map(int cpu)
 {
@@ -558,7 +559,7 @@ void set_cpu_sibling_map(int cpu)
 				c->booted_cores = cpu_data(i).booted_cores;
 		}
 		if (match_die(c, o) && !topology_same_node(c, o))
-			primarily_use_numa_for_topology();
+			x86_has_numa_in_package = true;
 	}
 
 	threads = cpumask_weight(topology_sibling_cpumask(cpu));
@@ -1304,6 +1305,16 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		zalloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);
 		zalloc_cpumask_var(&per_cpu(cpu_llc_shared_map, i), GFP_KERNEL);
 	}
+
+	/*
+	 * Set 'default' x86 topology, this matches default_topology() in that
+	 * it has NUMA nodes as a topology level. See also
+	 * native_smp_cpus_done().
+	 *
+	 * Must be done before set_cpus_sibling_map() is ran.
+	 */
+	set_sched_topology(x86_topology);
+
 	set_cpu_sibling_map(0);
 
 	switch (smp_sanity_check(max_cpus)) {
@@ -1370,6 +1381,9 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 {
 	pr_debug("Boot done\n");
 
+	if (x86_has_numa_in_package)
+		set_sched_topology(x86_numa_in_package_topology);
+
 	nmi_selftest();
 	impress_friends();
 	setup_ioapic_dest();

commit 1e1b37273cf719545da50b76f214f983a710aaf4
Merge: c183a603e8d8 c291b0151585
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 26 15:47:03 2016 -0400

    Merge branch 'x86/urgent' into x86/apic
    
    Bring in the upstream modifications so we can fixup the silent merge
    conflict which is introduced by this merge.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit cff9ab2b291e64259d97add48fe073c081afe4e2
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Tue Sep 13 20:12:32 2016 +0200

    x86/apic: Get rid of apic_version[] array
    
    The array has a size of MAX_LOCAL_APIC, which can be as large as 32k, so it
    can consume up to 128k.
    
    The array has been there forever and was never used for anything useful
    other than a version mismatch check which was introduced in 2009.
    
    There is no reason to store the version in an array. The kernel is not
    prepared to handle different APIC versions anyway, so the real important
    part is to detect a version mismatch and warn about it, which can be done
    with a single variable as well.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    CC: Andy Lutomirski <luto@amacapital.net>
    CC: Borislav Petkov <bp@alien8.de>
    CC: Brian Gerst <brgerst@gmail.com>
    CC: Mike Travis <travis@sgi.com>
    Link: http://lkml.kernel.org/r/20160913181232.30815-1-dvlasenk@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8216b997c1c9..f2b8e4574d69 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -683,7 +683,7 @@ wakeup_secondary_cpu_via_nmi(int apicid, unsigned long start_eip)
 	 * Give the other CPU some time to accept the IPI.
 	 */
 	udelay(200);
-	if (APIC_INTEGRATED(apic_version[boot_cpu_physical_apicid])) {
+	if (APIC_INTEGRATED(boot_cpu_apic_version)) {
 		maxlvt = lapic_get_maxlvt();
 		if (maxlvt > 3)			/* Due to the Pentium erratum 3AP.  */
 			apic_write(APIC_ESR, 0);
@@ -710,7 +710,7 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	/*
 	 * Be paranoid about clearing APIC errors.
 	 */
-	if (APIC_INTEGRATED(apic_version[phys_apicid])) {
+	if (APIC_INTEGRATED(boot_cpu_apic_version)) {
 		if (maxlvt > 3)		/* Due to the Pentium erratum 3AP.  */
 			apic_write(APIC_ESR, 0);
 		apic_read(APIC_ESR);
@@ -749,7 +749,7 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	 * Determine this based on the APIC version.
 	 * If we don't have an integrated APIC, don't send the STARTUP IPIs.
 	 */
-	if (APIC_INTEGRATED(apic_version[phys_apicid]))
+	if (APIC_INTEGRATED(boot_cpu_apic_version))
 		num_starts = 2;
 	else
 		num_starts = 0;
@@ -987,7 +987,7 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 		/*
 		 * Be paranoid about clearing APIC errors.
 		*/
-		if (APIC_INTEGRATED(apic_version[boot_cpu_physical_apicid])) {
+		if (APIC_INTEGRATED(boot_cpu_apic_version)) {
 			apic_write(APIC_ESR, 0);
 			apic_read(APIC_ESR);
 		}
@@ -1242,7 +1242,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	/*
 	 * If we couldn't find a local APIC, then get out of here now!
 	 */
-	if (APIC_INTEGRATED(apic_version[boot_cpu_physical_apicid]) &&
+	if (APIC_INTEGRATED(boot_cpu_apic_version) &&
 	    !boot_cpu_has(X86_FEATURE_APIC)) {
 		if (!disable_apic) {
 			pr_err("BIOS bug, local APIC #%d not detected!...\n",

commit 0cb7bf61b1e9f05027de58c80f9b46a714d24e35
Merge: aa877175e7a9 3eab887a5542
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 1 18:33:46 2016 +0200

    Merge branch 'linus' into smp/hotplug
    
    Apply upstream changes to avoid conflicts with pending patches.

commit 0100301bfdf56a2a370c7157b5ab0fbf9313e1cd
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sat Aug 13 12:38:19 2016 -0400

    sched/x86: Rewrite the switch_to() code
    
    Move the low-level context switch code to an out-of-line asm stub instead of
    using complex inline asm.  This allows constructing a new stack frame for the
    child process to make it seamlessly flow to ret_from_fork without an extra
    test and branch in __switch_to().  It also improves code generation for
    __schedule() by using the C calling convention instead of clobbering all
    registers.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1471106302-10159-5-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c85d2c636092..7e52f83d3a4b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -942,7 +942,6 @@ void common_cpu_up(unsigned int cpu, struct task_struct *idle)
 	per_cpu(cpu_current_top_of_stack, cpu) =
 		(unsigned long)task_stack_page(idle) + THREAD_SIZE;
 #else
-	clear_tsk_thread_flag(idle, TIF_FORK);
 	initial_gs = per_cpu_offset(cpu);
 #endif
 }

commit 384d9fe3741657c8ed8cd9bf30bc1d4611864d56
Author: Wei Jiangang <weijg.fnst@cn.fujitsu.com>
Date:   Fri Aug 19 11:22:36 2016 +0800

    x86/smpboot: Check APIC ID before setting up default routing
    
    This is not a bugfix, but code optimization.
    
    If the BSP's APIC ID in local APIC is unexpected,
    a kernel panic will occur and the system will halt.
    That means no need to enable APIC mode, and no reason
    to set up the default routing for APIC.
    
    The combination of default_setup_apic_routing() and
    apic_bsp_setup() are used to enable APIC mode.
    They two should be kept together, rather than being
    separated by the codes of checking APIC ID.
    Just like their usage in APIC_init_uniprocessor().
    
    Signed-off-by: Wei Jiangang <weijg.fnst@cn.fujitsu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@suse.de
    Link: http://lkml.kernel.org/r/1471576957-12961-1-git-send-email-weijg.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2a6e84a30a54..8216b997c1c9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1316,14 +1316,13 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		break;
 	}
 
-	default_setup_apic_routing();
-
 	if (read_apic_id() != boot_cpu_physical_apicid) {
 		panic("Boot APIC ID in local APIC unexpected (%d vs %d)",
 		     read_apic_id(), boot_cpu_physical_apicid);
 		/* Or can we switch back to PIC here? */
 	}
 
+	default_setup_apic_routing();
 	cpu0_logical_apicid = apic_bsp_setup(false);
 
 	pr_info("CPU%d: ", 0);

commit b32f96c75d0dcbb9bf9cc7994e8022c8ce20a668
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Thu Aug 18 10:59:03 2016 -0500

    x86/asm/head: Rename 'stack_start' -> 'initial_stack'
    
    The 'stack_start' variable is similar in usage to 'initial_code' and
    'initial_gs': they're all stored in head_64.S and they're all updated by
    SMP and ACPI suspend before starting a CPU.
    
    Rename it to 'initial_stack' to be consistent with the others.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nilay Vaish <nilayvaish@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/87063d773a3212051b77e17b0ee427f6582a5050.1471535549.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 4296beb8fdd3..c85d2c636092 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -969,7 +969,7 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	initial_code = (unsigned long)start_secondary;
-	stack_start  = idle->thread.sp;
+	initial_stack  = idle->thread.sp;
 
 	/*
 	 * Enable the espfix hack for this CPU

commit 7b0501b1e7cddd32b265178e32d332bdfbb532d4
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Mon Aug 15 12:17:00 2016 +0200

    x86/smp: Fix __max_logical_packages value setup
    
    Frank reported kernel panic when he disabled several cores in BIOS
    via following option:
    
      Core Disable Bitmap(Hex)   [0]
    
    with number 0xFFE, which leaves 16 CPUs in system (out of 48).
    
    The kernel panic below goes along with following messages:
    
     smpboot: Max logical packages: 2^M
     smpboot: APIC(0) Converting physical 0 to logical package 0^M
     smpboot: APIC(20) Converting physical 1 to logical package 1^M
     smpboot: APIC(40) Package 2 exceeds logical package map^M
     smpboot: CPU 8 APICId 40 disabled^M
     smpboot: APIC(60) Package 3 exceeds logical package map^M
     smpboot: CPU 12 APICId 60 disabled^M
     ...
     general protection fault: 0000 [#1] SMP^M
     Modules linked in:^M
     CPU: 15 PID: 1 Comm: swapper/0 Not tainted 4.7.0-rc5+ #1^M
     Hardware name: SGI UV300/UV300, BIOS SGI UV 300 series BIOS 05/25/2016^M
     task: ffff8801673e0000 ti: ffff8801673ac000 task.ti: ffff8801673ac000^M
     RIP: 0010:[<ffffffff81014d54>]  [<ffffffff81014d54>] uncore_change_context+0xd4/0x180^M
     ...
      [<ffffffff810158ac>] uncore_event_init_cpu+0x6c/0x70^M
      [<ffffffff81d8c91c>] intel_uncore_init+0x1c2/0x2dd^M
      [<ffffffff81d8c75a>] ? uncore_cpu_setup+0x17/0x17^M
      [<ffffffff81002190>] do_one_initcall+0x50/0x190^M
      [<ffffffff810ab193>] ? parse_args+0x293/0x480^M
      [<ffffffff81d87365>] kernel_init_freeable+0x1a5/0x249^M
      [<ffffffff81d86a35>] ? set_debug_rodata+0x12/0x12^M
      [<ffffffff816dc19e>] kernel_init+0xe/0x110^M
      [<ffffffff816e93bf>] ret_from_fork+0x1f/0x40^M
      [<ffffffff816dc190>] ? rest_init+0x80/0x80^M
    
    The reason for the panic is wrong value of __max_logical_packages,
    which lets logical_package_map uninitialized and the uncore code
    relying on this map being properly initialized (maybe we should
    add some safety checks there as well).
    
    The __max_logical_packages is computed as:
    
      DIV_ROUND_UP(total_cpus, ncpus);
      - ncpus being number of cores
    
    With above BIOS setup we get total_cpus == 16 which set
    __max_logical_packages to 2 (ncpus is 12).
    
    Once topology_update_package_map processes CPU with logical
    pkg over 2 we display above messages and fail to initialize
    the physical_to_logical_pkg map, which makes the uncore code
    crash.
    
    The fix is to remove logical_package_map bitmap completely
    and keep and update the logical_packages number instead.
    
    After we enumerate all the present CPUs, we check if the
    enumerated logical packages count is within its computed
    maximum from BIOS data.
    
    If it's not the case, we set this maximum to the new enumerated
    value and freeze any new addition of logical packages.
    
    The freeze is because lot of init code like uncore/rapl/cqm
    depends on having maximum logical package value set to allocate
    their data, so we can't change it later on.
    
    Prarit Bhargava tested the patch and confirms that it solves
    the problem:
    
      From dmidecode:
              Core Count: 24
              Core Enabled: 24
              Thread Count: 48
    
    Orig kernel boot log:
    
     [    0.464981] smpboot: Max logical packages: 19
     [    0.469861] smpboot: APIC(0) Converting physical 0 to logical package 0
     [    0.477261] smpboot: APIC(40) Converting physical 1 to logical package 1
     [    0.484760] smpboot: APIC(80) Converting physical 2 to logical package 2
     [    0.492258] smpboot: APIC(c0) Converting physical 3 to logical package 3
    
    1.  nr_cpus=8, should stop enumerating in package 0:
    
     [    0.533664] smpboot: APIC(0) Converting physical 0 to logical package 0
     [    0.539596] smpboot: Max logical packages: 19
    
    2.  max_cpus=8, should still enumerate all packages:
    
     [    0.526494] smpboot: APIC(0) Converting physical 0 to logical package 0
     [    0.532428] smpboot: APIC(40) Converting physical 1 to logical package 1
     [    0.538456] smpboot: APIC(80) Converting physical 2 to logical package 2
     [    0.544486] smpboot: APIC(c0) Converting physical 3 to logical package 3
     [    0.550524] smpboot: Max logical packages: 19
    
    3.  nr_cpus=49 ( 2 socket + 1 core on 3rd socket), should stop enumerating in
        package 2:
    
     [    0.521378] smpboot: APIC(0) Converting physical 0 to logical package 0
     [    0.527314] smpboot: APIC(40) Converting physical 1 to logical package 1
     [    0.533345] smpboot: APIC(80) Converting physical 2 to logical package 2
     [    0.539368] smpboot: Max logical packages: 19
    
    4.  maxcpus=49, should still enumerate all packages:
    
     [    0.525591] smpboot: APIC(0) Converting physical 0 to logical package 0
     [    0.531525] smpboot: APIC(40) Converting physical 1 to logical package 1
     [    0.537547] smpboot: APIC(80) Converting physical 2 to logical package 2
     [    0.543579] smpboot: APIC(c0) Converting physical 3 to logical package 3
     [    0.549624] smpboot: Max logical packages: 19
    
    5.  kdump (nr_cpus=1) works as well.
    
    Reported-by: Frank Ramsay <framsay@redhat.com>
    Tested-by: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Reviewed-by: Prarit Bhargava <prarit@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160815101700.GA30090@krava
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2a6e84a30a54..4296beb8fdd3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -100,10 +100,11 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 /* Logical package management. We might want to allocate that dynamically */
 static int *physical_to_logical_pkg __read_mostly;
 static unsigned long *physical_package_map __read_mostly;;
-static unsigned long *logical_package_map  __read_mostly;
 static unsigned int max_physical_pkg_id __read_mostly;
 unsigned int __max_logical_packages __read_mostly;
 EXPORT_SYMBOL(__max_logical_packages);
+static unsigned int logical_packages __read_mostly;
+static bool logical_packages_frozen __read_mostly;
 
 /* Maximum number of SMT threads on any online core */
 int __max_smt_threads __read_mostly;
@@ -277,14 +278,14 @@ int topology_update_package_map(unsigned int apicid, unsigned int cpu)
 	if (test_and_set_bit(pkg, physical_package_map))
 		goto found;
 
-	new = find_first_zero_bit(logical_package_map, __max_logical_packages);
-	if (new >= __max_logical_packages) {
+	if (logical_packages_frozen) {
 		physical_to_logical_pkg[pkg] = -1;
-		pr_warn("APIC(%x) Package %u exceeds logical package map\n",
+		pr_warn("APIC(%x) Package %u exceeds logical package max\n",
 			apicid, pkg);
 		return -ENOSPC;
 	}
-	set_bit(new, logical_package_map);
+
+	new = logical_packages++;
 	pr_info("APIC(%x) Converting physical %u to logical package %u\n",
 		apicid, pkg, new);
 	physical_to_logical_pkg[pkg] = new;
@@ -341,6 +342,7 @@ static void __init smp_init_package_map(void)
 	}
 
 	__max_logical_packages = DIV_ROUND_UP(total_cpus, ncpus);
+	logical_packages = 0;
 
 	/*
 	 * Possibly larger than what we need as the number of apic ids per
@@ -352,10 +354,6 @@ static void __init smp_init_package_map(void)
 	memset(physical_to_logical_pkg, 0xff, size);
 	size = BITS_TO_LONGS(max_physical_pkg_id) * sizeof(unsigned long);
 	physical_package_map = kzalloc(size, GFP_KERNEL);
-	size = BITS_TO_LONGS(__max_logical_packages) * sizeof(unsigned long);
-	logical_package_map = kzalloc(size, GFP_KERNEL);
-
-	pr_info("Max logical packages: %u\n", __max_logical_packages);
 
 	for_each_present_cpu(cpu) {
 		unsigned int apicid = apic->cpu_present_to_apicid(cpu);
@@ -369,6 +367,15 @@ static void __init smp_init_package_map(void)
 		set_cpu_possible(cpu, false);
 		set_cpu_present(cpu, false);
 	}
+
+	if (logical_packages > __max_logical_packages) {
+		pr_warn("Detected more packages (%u), then computed by BIOS data (%u).\n",
+			logical_packages, __max_logical_packages);
+		logical_packages_frozen = true;
+		__max_logical_packages  = logical_packages;
+	}
+
+	pr_info("Max logical packages: %u\n", __max_logical_packages);
 }
 
 void __init smp_store_boot_cpu_info(void)

commit aa877175e7a9982233ed8f10cb4bfddd78d82741
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Wed Aug 3 13:22:28 2016 -0400

    cpu/hotplug: Prevent alloc/free of irq descriptors during CPU up/down (again)
    
    Now that Xen no longer allocates irqs in _cpu_up() we can restore
    commit:
    
      a89941816726 ("hotplug: Prevent alloc/free of irq descriptors during cpu up/down")
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: david.vrabel@citrix.com
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1470244948-17674-3-git-send-email-boris.ostrovsky@oracle.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2a6e84a30a54..067de612d3fa 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1108,17 +1108,8 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 
 	common_cpu_up(cpu, tidle);
 
-	/*
-	 * We have to walk the irq descriptors to setup the vector
-	 * space for the cpu which comes online.  Prevent irq
-	 * alloc/free across the bringup.
-	 */
-	irq_lock_sparse();
-
 	err = do_boot_cpu(apicid, cpu, tidle);
-
 	if (err) {
-		irq_unlock_sparse();
 		pr_err("do_boot_cpu failed(%d) to wakeup CPU#%u\n", err, cpu);
 		return -EIO;
 	}
@@ -1136,8 +1127,6 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 		touch_nmi_watchdog();
 	}
 
-	irq_unlock_sparse();
-
 	return 0;
 }
 

commit aeb35d6b74174ed08daab84e232b456bbd89d1d9
Merge: 7a66ecfd319a a47177d360a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 1 14:23:42 2016 -0400

    Merge branch 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 header cleanups from Ingo Molnar:
     "This tree is a cleanup of the x86 tree reducing spurious uses of
      module.h - which should improve build performance a bit"
    
    * 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, crypto: Restore MODULE_LICENSE() to glue_helper.c so it loads
      x86/apic: Remove duplicated include from probe_64.c
      x86/ce4100: Remove duplicated include from ce4100.c
      x86/headers: Include spinlock_types.h in x8664_ksyms_64.c for missing spinlock_t
      x86/platform: Delete extraneous MODULE_* tags fromm ts5500
      x86: Audit and remove any remaining unnecessary uses of module.h
      x86/kvm: Audit and remove any unnecessary uses of module.h
      x86/xen: Audit and remove any unnecessary uses of module.h
      x86/platform: Audit and remove any unnecessary uses of module.h
      x86/lib: Audit and remove any unnecessary uses of module.h
      x86/kernel: Audit and remove any unnecessary uses of module.h
      x86/mm: Audit and remove any unnecessary uses of module.h
      x86: Don't use module.h just for AUTHOR / LICENSE tags

commit 6453dbdda30428a3c56568c96fe70ea3612f07e2
Merge: 27b79027bc11 bc841e260c95
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 26 17:29:07 2016 -0700

    Merge tag 'pm-4.8-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael  Wysocki:
     "Again, the majority of changes go into the cpufreq subsystem, but
      there are no big features this time.  The cpufreq changes that stand
      out somewhat are the governor interface rework and improvements
      related to the handling of frequency tables.  Apart from those, there
      are fixes and new device/CPU IDs in drivers, cleanups and an
      improvement of the new schedutil governor.
    
      Next, there are some changes in the hibernation core, including a fix
      for a nasty problem related to the MONITOR/MWAIT usage by CPU offline
      during resume from hibernation, a few core improvements related to
      memory management during resume, a couple of additional debug features
      and cleanups.
    
      Finally, we have some fixes and cleanups in the devfreq subsystem,
      generic power domains framework improvements related to system
      suspend/resume, support for some new chips in intel_idle and in the
      power capping RAPL driver, a new version of the AnalyzeSuspend utility
      and some assorted fixes and cleanups.
    
      Specifics:
    
       - Rework the cpufreq governor interface to make it more
         straightforward and modify the conservative governor to avoid using
         transition notifications (Rafael Wysocki).
    
       - Rework the handling of frequency tables by the cpufreq core to make
         it more efficient (Viresh Kumar).
    
       - Modify the schedutil governor to reduce the number of wakeups it
         causes to occur in cases when the CPU frequency doesn't need to be
         changed (Steve Muckle, Viresh Kumar).
    
       - Fix some minor issues and clean up code in the cpufreq core and
         governors (Rafael Wysocki, Viresh Kumar).
    
       - Add Intel Broxton support to the intel_pstate driver (Srinivas
         Pandruvada).
    
       - Fix problems related to the config TDP feature and to the validity
         of the MSR_HWP_INTERRUPT register in intel_pstate (Jan Kiszka,
         Srinivas Pandruvada).
    
       - Make intel_pstate update the cpu_frequency tracepoint even if the
         frequency doesn't change to avoid confusing powertop (Rafael
         Wysocki).
    
       - Clean up the usage of __init/__initdata in intel_pstate, mark some
         of its internal variables as __read_mostly and drop an unused
         structure element from it (Jisheng Zhang, Carsten Emde).
    
       - Clean up the usage of some duplicate MSR symbols in intel_pstate
         and turbostat (Srinivas Pandruvada).
    
       - Update/fix the powernv, s3c24xx and mvebu cpufreq drivers (Akshay
         Adiga, Viresh Kumar, Ben Dooks).
    
       - Fix a regression (introduced during the 4.5 cycle) in the
         pcc-cpufreq driver by reverting the problematic commit (Andreas
         Herrmann).
    
       - Add support for Intel Denverton to intel_idle, clean up Broxton
         support in it and make it explicitly non-modular (Jacob Pan, Jan
         Beulich, Paul Gortmaker).
    
       - Add support for Denverton and Ivy Bridge server to the Intel RAPL
         power capping driver and make it more careful about the handing of
         MSRs that may not be present (Jacob Pan, Xiaolong Wang).
    
       - Fix resume from hibernation on x86-64 by making the CPU offline
         during resume avoid using MONITOR/MWAIT in the "play dead" loop
         which may lead to an inadvertent "revival" of a "dead" CPU and a
         page fault leading to a kernel crash from it (Rafael Wysocki).
    
       - Make memory management during resume from hibernation more
         straightforward (Rafael Wysocki).
    
       - Add debug features that should help to detect problems related to
         hibernation and resume from it (Rafael Wysocki, Chen Yu).
    
       - Clean up hibernation core somewhat (Rafael Wysocki).
    
       - Prevent KASAN from instrumenting the hibernation core which leads
         to large numbers of false-positives from it (James Morse).
    
       - Prevent PM (hibernate and suspend) notifiers from being called
         during the cleanup phase if they have not been called during the
         corresponding preparation phase which is possible if one of the
         other notifiers returns an error at that time (Lianwei Wang).
    
       - Improve suspend-related debug printout in the tasks freezer and
         clean up suspend-related console handling (Roger Lu, Borislav
         Petkov).
    
       - Update the AnalyzeSuspend script in the kernel sources to version
         4.2 (Todd Brandt).
    
       - Modify the generic power domains framework to make it handle system
         suspend/resume better (Ulf Hansson).
    
       - Make the runtime PM framework avoid resuming devices synchronously
         when user space changes the runtime PM settings for them and
         improve its error reporting (Rafael Wysocki, Linus Walleij).
    
       - Fix error paths in devfreq drivers (exynos, exynos-ppmu,
         exynos-bus) and in the core, make some devfreq code explicitly
         non-modular and change some of it into tristate (Bartlomiej
         Zolnierkiewicz, Peter Chen, Paul Gortmaker).
    
       - Add DT support to the generic PM clocks management code and make it
         export some more symbols (Jon Hunter, Paul Gortmaker).
    
       - Make the PCI PM core code slightly more robust against possible
         driver errors (Andy Shevchenko).
    
       - Make it possible to change DESTDIR and PREFIX in turbostat (Andy
         Shevchenko)"
    
    * tag 'pm-4.8-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (89 commits)
      Revert "cpufreq: pcc-cpufreq: update default value of cpuinfo_transition_latency"
      PM / hibernate: Introduce test_resume mode for hibernation
      cpufreq: export cpufreq_driver_resolve_freq()
      cpufreq: Disallow ->resolve_freq() for drivers providing ->target_index()
      PCI / PM: check all fields in pci_set_platform_pm()
      cpufreq: acpi-cpufreq: use cached frequency mapping when possible
      cpufreq: schedutil: map raw required frequency to driver frequency
      cpufreq: add cpufreq_driver_resolve_freq()
      cpufreq: intel_pstate: Check cpuid for MSR_HWP_INTERRUPT
      intel_pstate: Update cpu_frequency tracepoint every time
      cpufreq: intel_pstate: clean remnant struct element
      PM / tools: scripts: AnalyzeSuspend v4.2
      x86 / hibernate: Use hlt_play_dead() when resuming from hibernation
      cpufreq: powernv: Replacing pstate_id with frequency table index
      intel_pstate: Fix MSR_CONFIG_TDP_x addressing in core_get_max_pstate()
      PM / hibernate: Image data protection during restoration
      PM / hibernate: Add missing braces in __register_nosave_region()
      PM / hibernate: Clean up comments in snapshot.c
      PM / hibernate: Clean up function headers in snapshot.c
      PM / hibernate: Add missing braces in hibernate_setup()
      ...

commit 0f657262d5f99ad86b9a63fb5dcd29036c2ed916
Merge: 425dbc6db34d 55920d31f1e3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 15:34:18 2016 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Ingo Molnar:
     "Various x86 low level modifications:
    
       - preparatory work to support virtually mapped kernel stacks (Andy
         Lutomirski)
    
       - support for 64-bit __get_user() on 32-bit kernels (Benjamin
         LaHaise)
    
       - (involved) workaround for Knights Landing CPU erratum (Dave Hansen)
    
       - MPX enhancements (Dave Hansen)
    
       - mremap() extension to allow remapping of the special VDSO vma, for
         purposes of user level context save/restore (Dmitry Safonov)
    
       - hweight and entry code cleanups (Borislav Petkov)
    
       - bitops code generation optimizations and cleanups with modern GCC
         (H. Peter Anvin)
    
       - syscall entry code optimizations (Paolo Bonzini)"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (43 commits)
      x86/mm/cpa: Add missing comment in populate_pdg()
      x86/mm/cpa: Fix populate_pgd(): Stop trying to deallocate failed PUDs
      x86/syscalls: Add compat_sys_preadv64v2/compat_sys_pwritev64v2
      x86/smp: Remove unnecessary initialization of thread_info::cpu
      x86/smp: Remove stack_smp_processor_id()
      x86/uaccess: Move thread_info::addr_limit to thread_struct
      x86/dumpstack: Rename thread_struct::sig_on_uaccess_error to sig_on_uaccess_err
      x86/uaccess: Move thread_info::uaccess_err and thread_info::sig_on_uaccess_err to thread_struct
      x86/dumpstack: When OOPSing, rewind the stack before do_exit()
      x86/mm/64: In vmalloc_fault(), use CR3 instead of current->active_mm
      x86/dumpstack/64: Handle faults when printing the "Stack: " part of an OOPS
      x86/dumpstack: Try harder to get a call trace on stack overflow
      x86/mm: Remove kernel_unmap_pages_in_pgd() and efi_cleanup_page_tables()
      x86/mm/cpa: In populate_pgd(), don't set the PGD entry until it's populated
      x86/mm/hotplug: Don't remove PGD entries in remove_pagetable()
      x86/mm: Use pte_none() to test for empty PTE
      x86/mm: Disallow running with 32-bit PTEs to work around erratum
      x86/mm: Ignore A/D bits in pte/pmd/pud_none()
      x86/mm: Move swap offset/type up in PTE to work around erratum
      x86/entry: Inline enter_from_user_mode()
      ...

commit 406f992e4a372dafbe3c2cff7efbb2002a5c8ebd
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Jul 14 03:55:23 2016 +0200

    x86 / hibernate: Use hlt_play_dead() when resuming from hibernation
    
    On Intel hardware, native_play_dead() uses mwait_play_dead() by
    default and only falls back to the other methods if that fails.
    That also happens during resume from hibernation, when the restore
    (boot) kernel runs disable_nonboot_cpus() to take all of the CPUs
    except for the boot one offline.
    
    However, that is problematic, because the address passed to
    __monitor() in mwait_play_dead() is likely to be written to in the
    last phase of hibernate image restoration and that causes the "dead"
    CPU to start executing instructions again.  Unfortunately, the page
    containing the address in that CPU's instruction pointer may not be
    valid any more at that point.
    
    First, that page may have been overwritten with image kernel memory
    contents already, so the instructions the CPU attempts to execute may
    simply be invalid.  Second, the page tables previously used by that
    CPU may have been overwritten by image kernel memory contents, so the
    address in its instruction pointer is impossible to resolve then.
    
    A report from Varun Koyyalagunta and investigation carried out by
    Chen Yu show that the latter sometimes happens in practice.
    
    To prevent it from happening, temporarily change the smp_ops.play_dead
    pointer during resume from hibernation so that it points to a special
    "play dead" routine which uses hlt_play_dead() and avoids the
    inadvertent "revivals" of "dead" CPUs this way.
    
    A slightly unpleasant consequence of this change is that if the
    system is hibernated with one or more CPUs offline, it will generally
    draw more power after resume than it did before hibernation, because
    the physical state entered by CPUs via hlt_play_dead() is higher-power
    than the mwait_play_dead() one in the majority of cases.  It is
    possible to work around this, but it is unclear how much of a problem
    that's going to be in practice, so the workaround will be implemented
    later if it turns out to be necessary.
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=106371
    Reported-by: Varun Koyyalagunta <cpudebug@centtech.com>
    Original-by: Chen Yu <yu.c.chen@intel.com>
    Tested-by: Chen Yu <yu.c.chen@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fafe8b923cac..8264dfad9cf8 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1622,7 +1622,7 @@ static inline void mwait_play_dead(void)
 	}
 }
 
-static inline void hlt_play_dead(void)
+void hlt_play_dead(void)
 {
 	if (__this_cpu_read(cpu_info.x86) >= 4)
 		wbinvd();

commit eb43e8f85fffc1ba535e0362a872101dfe48abe3
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 14 13:22:59 2016 -0700

    x86/smp: Remove unnecessary initialization of thread_info::cpu
    
    It's statically initialized to zero -- no need to dynamically
    initialize it to zero as well.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/6cf6314dce3051371a913ee19d1b88e29c68c560.1468527351.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fafe8b923cac..0e91dbeca2fd 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1285,7 +1285,6 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	cpumask_copy(cpu_callin_mask, cpumask_of(0));
 	mb();
 
-	current_thread_info()->cpu = 0;  /* needed? */
 	for_each_possible_cpu(i) {
 		zalloc_cpumask_var(&per_cpu(cpu_sibling_map, i), GFP_KERNEL);
 		zalloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);

commit 186f43608a5c827f8284fe4559225b4dccaa49ef
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:56 2016 -0400

    x86/kernel: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed some implicit header usage that was fixed up accordingly.
    
    Note that some bool/obj-y instances remain since module.h is
    the header for some exception table entry stuff, and for things
    like __init_or_module (code that is tossed when MODULES=n).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-4-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fafe8b923cac..472cc6a69e4a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -43,7 +43,7 @@
 
 #include <linux/init.h>
 #include <linux/smp.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/sched.h>
 #include <linux/percpu.h>
 #include <linux/bootmem.h>

commit 70b8301f6b8f7bc053377a9cbd0c4e42e29d9807
Author: Andi Kleen <ak@linux.intel.com>
Date:   Thu May 19 17:09:55 2016 -0700

    x86/topology: Add topology_max_smt_threads()
    
    For SMT specific workarounds it is useful to know if SMT is active
    on any online CPU in the system. This currently requires a loop
    over all online CPUs.
    
    Add a global variable that is updated with the maximum number
    of smt threads on any CPU on online/offline, and use it for
    topology_max_smt_threads()
    
    The single call is easier to use than a loop.
    
    Not exported to user space because user space already can use
    the existing sibling interfaces to find this out.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: acme@kernel.org
    Cc: jolsa@kernel.org
    Link: http://lkml.kernel.org/r/1463703002-19686-2-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fafe8b923cac..2ed0ec1353f8 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -105,6 +105,9 @@ static unsigned int max_physical_pkg_id __read_mostly;
 unsigned int __max_logical_packages __read_mostly;
 EXPORT_SYMBOL(__max_logical_packages);
 
+/* Maximum number of SMT threads on any online core */
+int __max_smt_threads __read_mostly;
+
 static inline void smpboot_setup_warm_reset_vector(unsigned long start_eip)
 {
 	unsigned long flags;
@@ -493,7 +496,7 @@ void set_cpu_sibling_map(int cpu)
 	bool has_mp = has_smt || boot_cpu_data.x86_max_cores > 1;
 	struct cpuinfo_x86 *c = &cpu_data(cpu);
 	struct cpuinfo_x86 *o;
-	int i;
+	int i, threads;
 
 	cpumask_set_cpu(cpu, cpu_sibling_setup_mask);
 
@@ -550,6 +553,10 @@ void set_cpu_sibling_map(int cpu)
 		if (match_die(c, o) && !topology_same_node(c, o))
 			primarily_use_numa_for_topology();
 	}
+
+	threads = cpumask_weight(topology_sibling_cpumask(cpu));
+	if (threads > __max_smt_threads)
+		__max_smt_threads = threads;
 }
 
 /* maps the cpu to the sched domain representing multi-core */
@@ -1441,6 +1448,21 @@ __init void prefill_possible_map(void)
 
 #ifdef CONFIG_HOTPLUG_CPU
 
+/* Recompute SMT state for all CPUs on offline */
+static void recompute_smt_state(void)
+{
+	int max_threads, cpu;
+
+	max_threads = 0;
+	for_each_online_cpu (cpu) {
+		int threads = cpumask_weight(topology_sibling_cpumask(cpu));
+
+		if (threads > max_threads)
+			max_threads = threads;
+	}
+	__max_smt_threads = max_threads;
+}
+
 static void remove_siblinginfo(int cpu)
 {
 	int sibling;
@@ -1465,6 +1487,7 @@ static void remove_siblinginfo(int cpu)
 	c->phys_proc_id = 0;
 	c->cpu_core_id = 0;
 	cpumask_clear_cpu(cpu, cpu_sibling_setup_mask);
+	recompute_smt_state();
 }
 
 static void remove_cpu_from_maps(int cpu)

commit 168f1a7163b37294a0ef33829e1ed54d41e33c42
Merge: 825a3b2605c3 4afd0565552c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 15:15:17 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - MSR access API fixes and enhancements (Andy Lutomirski)
    
       - early exception handling improvements (Andy Lutomirski)
    
       - user-space FS/GS prctl usage fixes and improvements (Andy
         Lutomirski)
    
       - Remove the cpu_has_*() APIs and replace them with equivalents
         (Borislav Petkov)
    
       - task switch micro-optimization (Brian Gerst)
    
       - 32-bit entry code simplification (Denys Vlasenko)
    
       - enhance PAT handling in enumated CPUs (Toshi Kani)
    
      ... and lots of other cleanups/fixlets"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      x86/arch_prctl/64: Restore accidentally removed put_cpu() in ARCH_SET_GS
      x86/entry/32: Remove asmlinkage_protect()
      x86/entry/32: Remove GET_THREAD_INFO() from entry code
      x86/entry, sched/x86: Don't save/restore EFLAGS on task switch
      x86/asm/entry/32: Simplify pushes of zeroed pt_regs->REGs
      selftests/x86/ldt_gdt: Test set_thread_area() deletion of an active segment
      x86/tls: Synchronize segment registers in set_thread_area()
      x86/asm/64: Rename thread_struct's fs and gs to fsbase and gsbase
      x86/arch_prctl/64: Remove FSBASE/GSBASE < 4G optimization
      x86/segments/64: When load_gs_index fails, clear the base
      x86/segments/64: When loadsegment(fs, ...) fails, clear the base
      x86/asm: Make asm/alternative.h safe from assembly
      x86/asm: Stop depending on ptrace.h in alternative.h
      x86/entry: Rename is_{ia32,x32}_task() to in_{ia32,x32}_syscall()
      x86/asm: Make sure verify_cpu() has a good stack
      x86/extable: Add a comment about early exception handlers
      x86/msr: Set the return value to zero when native_rdmsr_safe() fails
      x86/paravirt: Make "unsafe" MSR accesses unsafe even if PARAVIRT=y
      x86/paravirt: Add paravirt_{read,write}_msr()
      x86/msr: Carry on after a non-"safe" MSR access fails
      ...

commit 56402d63eefe22179f7311a51ff2094731420406
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 6 20:48:16 2016 +0200

    x86/topology: Handle CPUID bogosity gracefully
    
    Joseph reported that a XEN guest dies with a division by 0 in the package
    topology setup code. This happens if cpu_info.x86_max_cores is zero.
    
    Handle that case and emit a warning. This does not fix the underlying XEN bug,
    but makes the code more robust.
    
    Reported-and-tested-by: Joseph Salisbury <joseph.salisbury@canonical.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1605062046270.3540@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a2065d3b3b39..0e4329ed91ef 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -332,6 +332,11 @@ static void __init smp_init_package_map(void)
 	 * primary cores.
 	 */
 	ncpus = boot_cpu_data.x86_max_cores;
+	if (!ncpus) {
+		pr_warn("x86_max_cores == zero !?!?");
+		ncpus = 1;
+	}
+
 	__max_logical_packages = DIV_ROUND_UP(total_cpus, ncpus);
 
 	/*

commit 93984fbd4e33cc861d5b49caed02a02cbfb01340
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Apr 4 22:25:00 2016 +0200

    x86/cpufeature: Replace cpu_has_apic with boot_cpu_has() usage
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: linux-pm@vger.kernel.org
    Cc: oprofile-list@lists.sf.net
    Link: http://lkml.kernel.org/r/1459801503-15600-8-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a2065d3b3b39..1fe4130b14d9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1231,7 +1231,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	 * If we couldn't find a local APIC, then get out of here now!
 	 */
 	if (APIC_INTEGRATED(apic_version[boot_cpu_physical_apicid]) &&
-	    !cpu_has_apic) {
+	    !boot_cpu_has(X86_FEATURE_APIC)) {
 		if (!disable_apic) {
 			pr_err("BIOS bug, local APIC #%d not detected!...\n",
 				boot_cpu_physical_apicid);

commit 8196dab4fc159943df6baaac04973bb1accb7100
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Mar 25 15:52:36 2016 +0100

    x86/cpu: Get rid of compute_unit_id
    
    It is cpu_core_id anyway.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1458917557-8757-3-git-send-email-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b2c99f811c3f..a2065d3b3b39 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -422,7 +422,7 @@ static bool match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 
 		if (c->phys_proc_id == o->phys_proc_id &&
 		    per_cpu(cpu_llc_id, cpu1) == per_cpu(cpu_llc_id, cpu2) &&
-		    c->compute_unit_id == o->compute_unit_id)
+		    c->cpu_core_id == o->cpu_core_id)
 			return topology_sane(c, o, "smt");
 
 	} else if (c->phys_proc_id == o->phys_proc_id &&

commit 3e8db2246b434c6b18a6a9f09904038bddcf76c7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 18 17:20:30 2016 +0100

    x86/topology: Use total_cpus not nr_cpu_ids for logical packages
    
    nr_cpu_ids can be limited on the command line via nr_cpus=. That can break the
    logical package management because it results in a smaller number of packages,
    but the cpus to online are occupying the full package space as the hyper
    threads are enumerated after the physical cores typically.
    
    total_cpus is the real possible cpu space not limited by nr_cpus command line
    and gives us the proper number of packages.
    
    Reported-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Fixes: 1f12e32f4cd5 ("x86/topology: Create logical package id")
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Xiong Zhou <jencce.kernel@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Andreas Herrmann <aherrmann@suse.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1603181254330.3978@nanos

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 81e6a432f23c..b2c99f811c3f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -325,9 +325,14 @@ static void __init smp_init_package_map(void)
 	 * By not including this we'll sometimes over-estimate the number of
 	 * logical packages by the amount of !present siblings, but this is
 	 * still better than MAX_LOCAL_APIC.
+	 *
+	 * We use total_cpus not nr_cpu_ids because nr_cpu_ids can be limited
+	 * on the command line leading to a similar issue as the HT disable
+	 * problem because the hyperthreads are usually enumerated after the
+	 * primary cores.
 	 */
 	ncpus = boot_cpu_data.x86_max_cores;
-	__max_logical_packages = DIV_ROUND_UP(nr_cpu_ids, ncpus);
+	__max_logical_packages = DIV_ROUND_UP(total_cpus, ncpus);
 
 	/*
 	 * Possibly larger than what we need as the number of apic ids per

commit 63d1e995be455ae9196270eb4b789de21afd42ed
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Mar 18 16:03:48 2016 +0100

    x86/topology: Fix Intel HT disable
    
    As per the comment in the code; due to BIOS it is sometimes impossible to know
    if there actually are smp siblings until the machine is fully enumerated. So
    we rather overestimate the number of possible packages.
    
    Fixes: 1f12e32f4cd5 ("x86/topology: Create logical package id")
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: aherrmann@suse.com
    Cc: jencce.kernel@gmail.com
    Cc: bp@alien8.de
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Link: http://lkml.kernel.org/r/20160318150538.611014173@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 64b669dcbf23..81e6a432f23c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -312,8 +312,21 @@ static void __init smp_init_package_map(void)
 	/*
 	 * Today neither Intel nor AMD support heterogenous systems. That
 	 * might change in the future....
+	 *
+	 * While ideally we'd want '* smp_num_siblings' in the below @ncpus
+	 * computation, this won't actually work since some Intel BIOSes
+	 * report inconsistent HT data when they disable HT.
+	 *
+	 * In particular, they reduce the APIC-IDs to only include the cores,
+	 * but leave the CPUID topology to say there are (2) siblings.
+	 * This means we don't know how many threads there will be until
+	 * after the APIC enumeration.
+	 *
+	 * By not including this we'll sometimes over-estimate the number of
+	 * logical packages by the amount of !present siblings, but this is
+	 * still better than MAX_LOCAL_APIC.
 	 */
-	ncpus = boot_cpu_data.x86_max_cores * smp_num_siblings;
+	ncpus = boot_cpu_data.x86_max_cores;
 	__max_logical_packages = DIV_ROUND_UP(nr_cpu_ids, ncpus);
 
 	/*

commit b5d5f27d938fb6fc8d3202704e699d2694a02da6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Mar 18 16:03:46 2016 +0100

    x86/topology: Fix logical package mapping
    
    That first branch testing pkg against __max_logical_packages is wrong,
    because if the first pkg id is larger, then the find_first_zero will
    find us logical package id 0. However, if the second pkg id is indeed
    0, we'll again claim it without testing if it was already taken.
    
    Also, it fails to print the mapping.
    
    Fixes: 1f12e32f4cd5 ("x86/topology: Create logical package id")
    Reported-by: Xiong Zhou <jencce.kernel@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: aherrmann@suse.com
    Cc: bp@alien8.de
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Link: http://lkml.kernel.org/r/20160317095220.GO6344@twins.programming.kicks-ass.net
    Link: http://lkml.kernel.org/r/20160318150538.482393396@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 643dbdccf4bc..64b669dcbf23 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -274,11 +274,6 @@ int topology_update_package_map(unsigned int apicid, unsigned int cpu)
 	if (test_and_set_bit(pkg, physical_package_map))
 		goto found;
 
-	if (pkg < __max_logical_packages) {
-		set_bit(pkg, logical_package_map);
-		physical_to_logical_pkg[pkg] = pkg;
-		goto found;
-	}
 	new = find_first_zero_bit(logical_package_map, __max_logical_packages);
 	if (new >= __max_logical_packages) {
 		physical_to_logical_pkg[pkg] = -1;

commit 710d60cbf1b312a8075a2158cbfbbd9c66132dcc
Merge: df2e37c814d5 d10ef6f9380b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 13:50:29 2016 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull cpu hotplug updates from Thomas Gleixner:
     "This is the first part of the ongoing cpu hotplug rework:
    
       - Initial implementation of the state machine
    
       - Runs all online and prepare down callbacks on the plugged cpu and
         not on some random processor
    
       - Replaces busy loop waiting with completions
    
       - Adds tracepoints so the states can be followed"
    
    More detailed commentary on this work from an earlier email:
     "What's wrong with the current cpu hotplug infrastructure?
    
       - Asymmetry
    
         The hotplug notifier mechanism is asymmetric versus the bringup and
         teardown.  This is mostly caused by the notifier mechanism.
    
       - Largely undocumented dependencies
    
         While some notifiers use explicitely defined notifier priorities,
         we have quite some notifiers which use numerical priorities to
         express dependencies without any documentation why.
    
       - Control processor driven
    
         Most of the bringup/teardown of a cpu is driven by a control
         processor.  While it is understandable, that preperatory steps,
         like idle thread creation, memory allocation for and initialization
         of essential facilities needs to be done before a cpu can boot,
         there is no reason why everything else must run on a control
         processor.  Before this patch series, bringup looks like this:
    
           Control CPU                     Booting CPU
    
           do preparatory steps
           kick cpu into life
    
                                           do low level init
    
           sync with booting cpu           sync with control cpu
    
           bring the rest up
    
       - All or nothing approach
    
         There is no way to do partial bringups.  That's something which is
         really desired because we waste e.g.  at boot substantial amount of
         time just busy waiting that the cpu comes to life.  That's stupid
         as we could very well do preparatory steps and the initial IPI for
         other cpus and then go back and do the necessary low level
         synchronization with the freshly booted cpu.
    
       - Minimal debuggability
    
         Due to the notifier based design, it's impossible to switch between
         two stages of the bringup/teardown back and forth in order to test
         the correctness.  So in many hotplug notifiers the cancel
         mechanisms are either not existant or completely untested.
    
       - Notifier [un]registering is tedious
    
         To [un]register notifiers we need to protect against hotplug at
         every callsite.  There is no mechanism that bringup/teardown
         callbacks are issued on the online cpus, so every caller needs to
         do it itself.  That also includes error rollback.
    
      What's the new design?
    
         The base of the new design is a symmetric state machine, where both
         the control processor and the booting/dying cpu execute a well
         defined set of states.  Each state is symmetric in the end, except
         for some well defined exceptions, and the bringup/teardown can be
         stopped and reversed at almost all states.
    
         So the bringup of a cpu will look like this in the future:
    
           Control CPU                     Booting CPU
    
           do preparatory steps
           kick cpu into life
    
                                           do low level init
    
           sync with booting cpu           sync with control cpu
    
                                           bring itself up
    
         The synchronization step does not require the control cpu to wait.
         That mechanism can be done asynchronously via a worker or some
         other mechanism.
    
         The teardown can be made very similar, so that the dying cpu cleans
         up and brings itself down.  Cleanups which need to be done after
         the cpu is gone, can be scheduled asynchronously as well.
    
      There is a long way to this, as we need to refactor the notion when a
      cpu is available.  Today we set the cpu online right after it comes
      out of the low level bringup, which is not really correct.
    
      The proper mechanism is to set it to available, i.e. cpu local
      threads, like softirqd, hotplug thread etc. can be scheduled on that
      cpu, and once it finished all booting steps, it's set to online, so
      general workloads can be scheduled on it.  The reverse happens on
      teardown.  First thing to do is to forbid scheduling of general
      workloads, then teardown all the per cpu resources and finally shut it
      off completely.
    
      This patch series implements the basic infrastructure for this at the
      core level.  This includes the following:
    
       - Basic state machine implementation with well defined states, so
         ordering and prioritization can be expressed.
    
       - Interfaces to [un]register state callbacks
    
         This invokes the bringup/teardown callback on all online cpus with
         the proper protection in place and [un]installs the callbacks in
         the state machine array.
    
         For callbacks which have no particular ordering requirement we have
         a dynamic state space, so that drivers don't have to register an
         explicit hotplug state.
    
         If a callback fails, the code automatically does a rollback to the
         previous state.
    
       - Sysfs interface to drive the state machine to a particular step.
    
         This is only partially functional today.  Full functionality and
         therefor testability will be achieved once we converted all
         existing hotplug notifiers over to the new scheme.
    
       - Run all CPU_ONLINE/DOWN_PREPARE notifiers on the booting/dying
         processor:
    
           Control CPU                     Booting CPU
    
           do preparatory steps
           kick cpu into life
    
                                           do low level init
    
           sync with booting cpu           sync with control cpu
           wait for boot
                                           bring itself up
    
                                           Signal completion to control cpu
    
         In a previous step of this work we've done a full tree mechanical
         conversion of all hotplug notifiers to the new scheme.  The balance
         is a net removal of about 4000 lines of code.
    
         This is not included in this series, as we decided to take a
         different approach.  Instead of mechanically converting everything
         over, we will do a proper overhaul of the usage sites one by one so
         they nicely fit into the symmetric callback scheme.
    
         I decided to do that after I looked at the ugliness of some of the
         converted sites and figured out that their hotplug mechanism is
         completely buggered anyway.  So there is no point to do a
         mechanical conversion first as we need to go through the usage
         sites one by one again in order to achieve a full symmetric and
         testable behaviour"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      cpu/hotplug: Document states better
      cpu/hotplug: Fix smpboot thread ordering
      cpu/hotplug: Remove redundant state check
      cpu/hotplug: Plug death reporting race
      rcu: Make CPU_DYING_IDLE an explicit call
      cpu/hotplug: Make wait for dead cpu completion based
      cpu/hotplug: Let upcoming cpu bring itself fully up
      arch/hotplug: Call into idle with a proper state
      cpu/hotplug: Move online calls to hotplugged cpu
      cpu/hotplug: Create hotplug threads
      cpu/hotplug: Split out the state walk into functions
      cpu/hotplug: Unpark smpboot threads from the state machine
      cpu/hotplug: Move scheduler cpu_online notifier to hotplug core
      cpu/hotplug: Implement setup/removal interface
      cpu/hotplug: Make target state writeable
      cpu/hotplug: Add sysfs state interface
      cpu/hotplug: Hand in target state to _cpu_up/down
      cpu/hotplug: Convert the hotplugged cpu work to a state machine
      cpu/hotplug: Convert to a state machine for the control processor
      cpu/hotplug: Add tracepoints
      ...

commit fc6d73d67436e7784758a831227bd019547a3f73
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 26 18:43:40 2016 +0000

    arch/hotplug: Call into idle with a proper state
    
    Let the non boot cpus call into idle with the corresponding hotplug state, so
    the hotplug core can handle the further bringup. That's a first step to
    convert the boot side of the hotplugged cpus to do all the synchronization
    with the other side through the state machine. For now it'll only start the
    hotplug thread and kick the full bringup of the cpu.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: "Srivatsa S. Bhat" <srivatsa@mit.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/20160226182341.614102639@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 24d57f77b3c1..293b22a7ab02 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -248,7 +248,7 @@ static void notrace start_secondary(void *unused)
 	x86_cpuinit.setup_percpu_clockev();
 
 	wmb();
-	cpu_startup_entry(CPUHP_ONLINE);
+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
 }
 
 void __init smp_store_boot_cpu_info(void)

commit 1f12e32f4cd5243ae46d8b933181be0d022c6793
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 22 22:19:15 2016 +0000

    x86/topology: Create logical package id
    
    For per package oriented services we must be able to rely on the number of CPU
    packages to be within bounds. Create a tracking facility, which
    
    - calculates the number of possible packages depending on nr_cpu_ids after boot
    
    - makes sure that the package id is within the number of possible packages. If
      the apic id is outside we map it to a logical package id if there is enough
      space available.
    
    Provide interfaces for drivers to query the mapping and do translations from
    physcial to logical ids.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Harish Chegondi <harish.chegondi@intel.com>
    Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20160222221011.541071755@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 24d57f77b3c1..3bf1e0b5f827 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -97,6 +97,14 @@ DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_llc_shared_map);
 DEFINE_PER_CPU_READ_MOSTLY(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
 
+/* Logical package management. We might want to allocate that dynamically */
+static int *physical_to_logical_pkg __read_mostly;
+static unsigned long *physical_package_map __read_mostly;;
+static unsigned long *logical_package_map  __read_mostly;
+static unsigned int max_physical_pkg_id __read_mostly;
+unsigned int __max_logical_packages __read_mostly;
+EXPORT_SYMBOL(__max_logical_packages);
+
 static inline void smpboot_setup_warm_reset_vector(unsigned long start_eip)
 {
 	unsigned long flags;
@@ -251,6 +259,97 @@ static void notrace start_secondary(void *unused)
 	cpu_startup_entry(CPUHP_ONLINE);
 }
 
+int topology_update_package_map(unsigned int apicid, unsigned int cpu)
+{
+	unsigned int new, pkg = apicid >> boot_cpu_data.x86_coreid_bits;
+
+	/* Called from early boot ? */
+	if (!physical_package_map)
+		return 0;
+
+	if (pkg >= max_physical_pkg_id)
+		return -EINVAL;
+
+	/* Set the logical package id */
+	if (test_and_set_bit(pkg, physical_package_map))
+		goto found;
+
+	if (pkg < __max_logical_packages) {
+		set_bit(pkg, logical_package_map);
+		physical_to_logical_pkg[pkg] = pkg;
+		goto found;
+	}
+	new = find_first_zero_bit(logical_package_map, __max_logical_packages);
+	if (new >= __max_logical_packages) {
+		physical_to_logical_pkg[pkg] = -1;
+		pr_warn("APIC(%x) Package %u exceeds logical package map\n",
+			apicid, pkg);
+		return -ENOSPC;
+	}
+	set_bit(new, logical_package_map);
+	pr_info("APIC(%x) Converting physical %u to logical package %u\n",
+		apicid, pkg, new);
+	physical_to_logical_pkg[pkg] = new;
+
+found:
+	cpu_data(cpu).logical_proc_id = physical_to_logical_pkg[pkg];
+	return 0;
+}
+
+/**
+ * topology_phys_to_logical_pkg - Map a physical package id to a logical
+ *
+ * Returns logical package id or -1 if not found
+ */
+int topology_phys_to_logical_pkg(unsigned int phys_pkg)
+{
+	if (phys_pkg >= max_physical_pkg_id)
+		return -1;
+	return physical_to_logical_pkg[phys_pkg];
+}
+EXPORT_SYMBOL(topology_phys_to_logical_pkg);
+
+static void __init smp_init_package_map(void)
+{
+	unsigned int ncpus, cpu;
+	size_t size;
+
+	/*
+	 * Today neither Intel nor AMD support heterogenous systems. That
+	 * might change in the future....
+	 */
+	ncpus = boot_cpu_data.x86_max_cores * smp_num_siblings;
+	__max_logical_packages = DIV_ROUND_UP(nr_cpu_ids, ncpus);
+
+	/*
+	 * Possibly larger than what we need as the number of apic ids per
+	 * package can be smaller than the actual used apic ids.
+	 */
+	max_physical_pkg_id = DIV_ROUND_UP(MAX_LOCAL_APIC, ncpus);
+	size = max_physical_pkg_id * sizeof(unsigned int);
+	physical_to_logical_pkg = kmalloc(size, GFP_KERNEL);
+	memset(physical_to_logical_pkg, 0xff, size);
+	size = BITS_TO_LONGS(max_physical_pkg_id) * sizeof(unsigned long);
+	physical_package_map = kzalloc(size, GFP_KERNEL);
+	size = BITS_TO_LONGS(__max_logical_packages) * sizeof(unsigned long);
+	logical_package_map = kzalloc(size, GFP_KERNEL);
+
+	pr_info("Max logical packages: %u\n", __max_logical_packages);
+
+	for_each_present_cpu(cpu) {
+		unsigned int apicid = apic->cpu_present_to_apicid(cpu);
+
+		if (apicid == BAD_APICID || !apic->apic_id_valid(apicid))
+			continue;
+		if (!topology_update_package_map(apicid, cpu))
+			continue;
+		pr_warn("CPU %u APICId %x disabled\n", cpu, apicid);
+		per_cpu(x86_bios_cpu_apicid, cpu) = BAD_APICID;
+		set_cpu_possible(cpu, false);
+		set_cpu_present(cpu, false);
+	}
+}
+
 void __init smp_store_boot_cpu_info(void)
 {
 	int id = 0; /* CPU 0 */
@@ -258,6 +357,7 @@ void __init smp_store_boot_cpu_info(void)
 
 	*c = boot_cpu_data;
 	c->cpu_index = id;
+	smp_init_package_map();
 }
 
 /*

commit 362f924b64ba0f4be2ee0cb697690c33d40be721
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Dec 7 10:39:41 2015 +0100

    x86/cpufeature: Remove unused and seldomly used cpu_has_xx macros
    
    Those are stupid and code should use static_cpu_has_safe() or
    boot_cpu_has() instead. Kill the least used and unused ones.
    
    The remaining ones need more careful inspection before a conversion can
    happen. On the TODO.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1449481182-27541-4-git-send-email-bp@alien8.de
    Cc: David Sterba <dsterba@suse.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f2281e9cfdbe..24d57f77b3c1 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -304,7 +304,7 @@ do {									\
 
 static bool match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 {
-	if (cpu_has_topoext) {
+	if (boot_cpu_has(X86_FEATURE_TOPOEXT)) {
 		int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
 
 		if (c->phys_proc_id == o->phys_proc_id &&

commit 0fa85119cd480c1ded7a81ed64f723fe16a15355
Merge: d6ccc3ec9525 1eab0e42450c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Dec 19 11:49:13 2015 +0100

    Merge branch 'linus' into x86/cleanups
    
    Pull in upstream changes so we can apply depending patches.

commit 656279a1f3b210cf48ccc572fd7c6b8e2250be77
Author: Len Brown <len.brown@intel.com>
Date:   Sun Nov 22 18:16:15 2015 -0500

    x86 smpboot: Re-enable init_udelay=0 by default on modern CPUs
    
    commit f1ccd249319e allowed the cmdline "cpu_init_udelay=" to work
    with all values, including the default of 10000.
    
    But in setting the default of 10000, it over-rode the code that sets
    the delay 0 on modern processors.
    
    Also, tidy up use of INT/UINT.
    
    Fixes: f1ccd249319e "x86/smpboot: Fix cpu_init_udelay=10000 corner case boot parameter misbehavior"
    Reported-by: Shane <shrybman@teksavvy.com>
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: dparsons@brightdsl.net
    Cc: stable@kernel.org
    Link: http://lkml.kernel.org/r/9082eb809ef40dad02db714759c7aaf618c518d4.1448232494.git.len.brown@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 892ee2e5ecbc..fbabe4fcc7fb 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -509,7 +509,7 @@ void __inquire_remote_apic(int apicid)
  */
 #define UDELAY_10MS_DEFAULT 10000
 
-static unsigned int init_udelay = INT_MAX;
+static unsigned int init_udelay = UINT_MAX;
 
 static int __init cpu_init_udelay(char *str)
 {
@@ -522,14 +522,15 @@ early_param("cpu_init_udelay", cpu_init_udelay);
 static void __init smp_quirk_init_udelay(void)
 {
 	/* if cmdline changed it from default, leave it alone */
-	if (init_udelay != INT_MAX)
+	if (init_udelay != UINT_MAX)
 		return;
 
 	/* if modern processor, use no delay */
 	if (((boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) && (boot_cpu_data.x86 == 6)) ||
-	    ((boot_cpu_data.x86_vendor == X86_VENDOR_AMD) && (boot_cpu_data.x86 >= 0xF)))
+	    ((boot_cpu_data.x86_vendor == X86_VENDOR_AMD) && (boot_cpu_data.x86 >= 0xF))) {
 		init_udelay = 0;
-
+		return;
+	}
 	/* else, use legacy delay */
 	init_udelay = UDELAY_10MS_DEFAULT;
 }

commit 460958659270b7d750d4ccfe052171cb6f655cbb
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Nov 17 14:44:32 2015 +0100

    x86/paravirt: Remove unused pv_apic_ops structure
    
    The only member of that structure is startup_ipi_hook which is always
    set to paravirt_nop.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Cc: jeremy@goop.org
    Cc: chrisw@sous-sol.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xen.org
    Cc: konrad.wilk@oracle.com
    Cc: boris.ostrovsky@oracle.com
    Link: http://lkml.kernel.org/r/1447767872-16730-1-git-send-email-jgross@suse.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 892ee2e5ecbc..4df777710ab7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -628,13 +628,6 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	else
 		num_starts = 0;
 
-	/*
-	 * Paravirt / VMI wants a startup IPI hook here to set up the
-	 * target processor state.
-	 */
-	startup_ipi_hook(phys_apicid, (unsigned long) start_secondary,
-			 stack_start);
-
 	/*
 	 * Run STARTUP IPI loop.
 	 */

commit fcafddec4e78a7776db4b6685db6b2902d4300fc
Author: Len Brown <len.brown@intel.com>
Date:   Fri Oct 16 00:14:29 2015 -0400

    x86/smpboot: Fix CPU #1 boot timeout
    
    The following commit:
    
      a9bcaa02a5104ac ("x86/smpboot: Remove SIPI delays from cpu_up()")
    
    Caused some Intel Core2 processors to time-out when bringing up CPU #1,
    resulting in the missing of that CPU after bootup.
    
    That patch reduced the SIPI delays from udelay() 300, 200 to udelay() 0,
    0 on modern processors.
    
    Several Intel(R) Core(TM)2 systems failed to bring up CPU #1 10/10 times
    after that change.
    
    Increasing either of the SIPI delays to udelay(1) results in
    success. So here we increase both to udelay(10).  While this may
    be 20x slower than the absolute minimum, it is still 20x to 30x
    faster than the original code.
    
    Tested-by: Donald Parsons <dparsons@brightdsl.net>
    Tested-by: Shane <shrybman@teksavvy.com>
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dparsons@brightdsl.net
    Cc: shrybman@teksavvy.com
    Link: http://lkml.kernel.org/r/6dd554ee8945984d85aafb2ad35793174d068af0.1444968087.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 32267ccac3d7..892ee2e5ecbc 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -660,7 +660,9 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 		/*
 		 * Give the other CPU some time to accept the IPI.
 		 */
-		if (init_udelay)
+		if (init_udelay == 0)
+			udelay(10);
+		else
 			udelay(300);
 
 		pr_debug("Startup point 1\n");
@@ -671,7 +673,9 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 		/*
 		 * Give the other CPU some time to accept the IPI.
 		 */
-		if (init_udelay)
+		if (init_udelay == 0)
+			udelay(10);
+		else
 			udelay(200);
 
 		if (maxlvt > 3)		/* Due to the Pentium erratum 3AP.  */

commit f1ccd249319efca4ee4faf1d904f5a362cac7c81
Author: Len Brown <len.brown@intel.com>
Date:   Fri Oct 16 00:14:28 2015 -0400

    x86/smpboot: Fix cpu_init_udelay=10000 corner case boot parameter misbehavior
    
    For legacy machines cpu_init_udelay defaults to 10,000.
    For modern machines it is set to 0.
    
    The user should be able to set cpu_init_udelay to
    any value on the cmdline, including 10,000.
    
    Before this patch, that was seen as "unchanged from default"
    and thus on a modern machine, the user request was ignored
    and the delay was set to 0.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dparsons@brightdsl.net
    Cc: shrybman@teksavvy.com
    Link: http://lkml.kernel.org/r/de363cdbbcfcca1d22569683f7eb9873e0177251.1444968087.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e0c198e5f920..32267ccac3d7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -509,7 +509,7 @@ void __inquire_remote_apic(int apicid)
  */
 #define UDELAY_10MS_DEFAULT 10000
 
-static unsigned int init_udelay = UDELAY_10MS_DEFAULT;
+static unsigned int init_udelay = INT_MAX;
 
 static int __init cpu_init_udelay(char *str)
 {
@@ -522,13 +522,16 @@ early_param("cpu_init_udelay", cpu_init_udelay);
 static void __init smp_quirk_init_udelay(void)
 {
 	/* if cmdline changed it from default, leave it alone */
-	if (init_udelay != UDELAY_10MS_DEFAULT)
+	if (init_udelay != INT_MAX)
 		return;
 
 	/* if modern processor, use no delay */
 	if (((boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) && (boot_cpu_data.x86 == 6)) ||
 	    ((boot_cpu_data.x86_vendor == X86_VENDOR_AMD) && (boot_cpu_data.x86 >= 0xF)))
 		init_udelay = 0;
+
+	/* else, use legacy delay */
+	init_udelay = UDELAY_10MS_DEFAULT;
 }
 
 /*

commit 0c0fee018d14b585461b146bdeda8bab9a61c211
Merge: a0c0d985ded5 4daa832d9987
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 1 09:33:26 2015 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 init code fixlet from Ingo Molnar:
     "A single change: fix obsolete init code annotations"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Drop bogus __ref / __refdata annotations

commit 656bba306827a44ed73b3f93f75bb3147de17fae
Author: Len Brown <len.brown@intel.com>
Date:   Sun Aug 16 11:45:48 2015 -0400

    x86/smpboot: Remove APIC.wait_for_init_deassert and atomic init_deasserted
    
    Both the per-APIC flag ".wait_for_init_deassert",
    and the global atomic_t "init_deasserted"
    are dead code -- remove them.
    
    For all APIC types, "wait_for_master()"
    prevents an AP from proceeding until the BSP has set
    cpu_callout_mask, making "init_deasserted" {unnecessary}:
    
            BSP: <de-assert INIT>
            ...
            BSP: {set init_deasserted}
            AP: wait_for_master()
                    set cpu_initialized_mask
                    wait for cpu_callout_mask
            BSP: test cpu_initialized_mask
            BSP: set cpu_callout_mask
            AP: test cpu_callout_mask
            AP: {wait for init_deasserted}
            ...
            AP: <touch APIC>
    
    Deleting the {dead code} above is necessary to enable
    some parallelism in a future patch.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Zhu Guihua <zhugh.fnst@cn.fujitsu.com>
    Link: http://lkml.kernel.org/r/de4b3a9bab894735e285870b5296da25ee6a8a5a.1439739165.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 674026455fee..c15d0073c829 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -97,8 +97,6 @@ DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_llc_shared_map);
 DEFINE_PER_CPU_READ_MOSTLY(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
 
-atomic_t init_deasserted;
-
 static inline void smpboot_setup_warm_reset_vector(unsigned long start_eip)
 {
 	unsigned long flags;
@@ -146,16 +144,11 @@ static void smp_callin(void)
 
 	/*
 	 * If waken up by an INIT in an 82489DX configuration
-	 * we may get here before an INIT-deassert IPI reaches
-	 * our local APIC.  We have to wait for the IPI or we'll
-	 * lock up on an APIC access.
-	 *
-	 * Since CPU0 is not wakened up by INIT, it doesn't wait for the IPI.
+	 * cpu_callout_mask guarantees we don't get here before
+	 * an INIT_deassert IPI reaches our local APIC, so it is
+	 * now safe to touch our local APIC.
 	 */
 	cpuid = smp_processor_id();
-	if (apic->wait_for_init_deassert && cpuid)
-		while (!atomic_read(&init_deasserted))
-			cpu_relax();
 
 	/*
 	 * (This works even if the APIC is not enabled.)
@@ -620,7 +613,6 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	send_status = safe_apic_wait_icr_idle();
 
 	mb();
-	atomic_set(&init_deasserted, 1);
 
 	/*
 	 * Should we send STARTUP IPIs ?
@@ -861,8 +853,6 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	 * the targeted processor.
 	 */
 
-	atomic_set(&init_deasserted, 0);
-
 	if (get_uv_system_type() != UV_NON_UNIQUE_APIC) {
 
 		pr_debug("Setting warm reset code and vector.\n");

commit a9bcaa02a5104ace6a9d9e4a9cd9192a9e7744d6
Author: Len Brown <len.brown@intel.com>
Date:   Sun Aug 16 11:45:47 2015 -0400

    x86/smpboot: Remove SIPI delays from cpu_up()
    
    MPS 1.4 example code shows the following required delays during processor
    on-lining:
    
            INIT
             udelay(10,000)
            SIPI
             udelay(200)
            SIPI
             udelay(200) /* Linux actually implements this as udelay(300) */
    
    Linux skips the udelay(10,000) on modern processors.
    This patch removes the udelay(200) after each SIPI
    on those same processors.
    
    All three legacy delays can be restored by the cmdline
    "cpu_init_udelay=10000".
    
    As measured by analyze_suspend.py, this patch speeds
    processor resume time on my desktop from 2.4ms to 1.8ms, per AP.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Zhu Guihua <zhugh.fnst@cn.fujitsu.com>
    Link: http://lkml.kernel.org/r/a5dfdbc8fbfdd813784da204aad5677fe459ac37.1439739165.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 310b6f0bf6f7..674026455fee 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -665,7 +665,8 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 		/*
 		 * Give the other CPU some time to accept the IPI.
 		 */
-		udelay(300);
+		if (init_udelay)
+			udelay(300);
 
 		pr_debug("Startup point 1\n");
 
@@ -675,7 +676,8 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 		/*
 		 * Give the other CPU some time to accept the IPI.
 		 */
-		udelay(200);
+		if (init_udelay)
+			udelay(200);
 
 		if (maxlvt > 3)		/* Due to the Pentium erratum 3AP.  */
 			apic_write(APIC_ESR, 0);

commit 2d99af8e8fd6c2dea11ab539f7aba69c37b845b4
Author: Len Brown <len.brown@intel.com>
Date:   Sun Aug 16 11:45:46 2015 -0400

    x86/smpboot: Remove udelay(100) when polling cpu_callin_map
    
    After the BSP sends INIT/SIPI/SIP to the AP and sees the AP
    in the cpu_initialized_map, it sets the AP loose via the
    cpu_callout_map, and waits for it via the cpu_callin_map.
    
    The BSP polls the cpu_callin_map with a udelay(100)
    and a schedule() in each iteration.
    
    The udelay(100) adds no value.
    
    For example, on my 4-CPU dekstop, the AP finishes
    cpu_callin() in under 70 usec and sets the cpu_callin_mask.
    The BSP, however, doesn't see that setting until over 30 usec
    later, because it was still running its udelay(100)
    when the AP finished.
    
    Deleting the udelay(100) in the cpu_callin_mask polling loop,
    saves from 0 to 100 usec per Application Processor.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Zhu Guihua <zhugh.fnst@cn.fujitsu.com>
    Link: http://lkml.kernel.org/r/0aade12eabeb89a688c929fe80856eaea0544bb7.1439739165.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9ad88fb0a303..310b6f0bf6f7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -926,7 +926,6 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 			 * for the MTRR work(triggered by the AP coming online)
 			 * to be completed in the stop machine context.
 			 */
-			udelay(100);
 			schedule();
 		}
 	}

commit 6e38f1e79d16f4fa9e5cf06792500e11c96a6f84
Author: Len Brown <len.brown@intel.com>
Date:   Sun Aug 16 11:45:45 2015 -0400

    x86/smpboot: Remove udelay(100) when polling cpu_initialized_map
    
    After the BSP sends the APIC INIT/SIPI/SIPI to the AP,
    it waits for the AP to come up and indicate that it is alive
    by setting its own bit in the cpu_initialized_mask.
    
    Linux polls for up to 10 seconds for this to happen.
    Each polling loop has a udelay(100) and a call to schedule().
    
    The udelay(100) adds no value.
    
    For example, on my desktop, the BSP waits for the
    other 3 CPUs to come on line at boot for 305, 404, 405 usec.
    For resume from S3, it waits 317, 404, 405 usec.
    
    But when the udelay(100) is removed, the BSP waits
    305, 310, 306 for boot, and 305, 307, 306 for resume.
    
    So for both boot and resume, removing the udelay(100)
    speeds online by about 100us in 2 of 3 cases.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Zhu Guihua <zhugh.fnst@cn.fujitsu.com>
    Link: http://lkml.kernel.org/r/33ef746c67d2489cad0a9b1958cf71167232ff2b.1439739165.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b1f3ed9c7a9e..9ad88fb0a303 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -898,7 +898,7 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 
 	if (!boot_error) {
 		/*
-		 * Wait 10s total for a response from AP
+		 * Wait 10s total for first sign of life from AP
 		 */
 		boot_error = -1;
 		timeout = jiffies + 10*HZ;
@@ -911,7 +911,6 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 				boot_error = 0;
 				break;
 			}
-			udelay(100);
 			schedule();
 		}
 	}

commit 4daa832d99871356f5fdc52372c975e40f73a15e
Author: Mathias Krause <minipli@googlemail.com>
Date:   Mon Jul 20 18:32:53 2015 +0200

    x86: Drop bogus __ref / __refdata annotations
    
    The __ref / __refdata annotations used to be needed because of
    referencing functions / variables annotated __cpuinit /
    __cpuinitdata.
    
    But those annotations vanished during the development of v3.11.
    
    Therefore most of the __ref / __refdata annotations are not needed
    anymore. As they may hide legitimate sections mismatches, we
    better get rid of them.
    
    Signed-off-by: Mathias Krause <minipli@googlemail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437409973-8927-1-git-send-email-minipli@googlemail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b1f3ed9c7a9e..1d06cf8c0093 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1358,7 +1358,7 @@ static void remove_siblinginfo(int cpu)
 	cpumask_clear_cpu(cpu, cpu_sibling_setup_mask);
 }
 
-static void __ref remove_cpu_from_maps(int cpu)
+static void remove_cpu_from_maps(int cpu)
 {
 	set_cpu_online(cpu, false);
 	cpumask_clear_cpu(cpu, cpu_callout_mask);

commit ce0d3c0a6fb1422101498ef378c0851dabbbf67f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 14 22:03:57 2015 +0200

    genirq: Revert sparse irq locking around __cpu_up() and move it to x86 for now
    
    Boris reported that the sparse_irq protection around __cpu_up() in the
    generic code causes a regression on Xen. Xen allocates interrupts and
    some more in the xen_cpu_up() function, so it deadlocks on the
    sparse_irq_lock.
    
    There is no simple fix for this and we really should have the
    protection for all architectures, but for now the only solution is to
    move it to x86 where actual wreckage due to the lack of protection has
    been observed.
    
    Reported-and-tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Fixes: a89941816726 'hotplug: Prevent alloc/free of irq descriptors during cpu up/down'
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: xiao jin <jin.xiao@intel.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Yanmin Zhang <yanmin_zhang@linux.intel.com>
    Cc: xen-devel <xen-devel@lists.xenproject.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d3010aa79daf..b1f3ed9c7a9e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -992,8 +992,17 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 
 	common_cpu_up(cpu, tidle);
 
+	/*
+	 * We have to walk the irq descriptors to setup the vector
+	 * space for the cpu which comes online.  Prevent irq
+	 * alloc/free across the bringup.
+	 */
+	irq_lock_sparse();
+
 	err = do_boot_cpu(apicid, cpu, tidle);
+
 	if (err) {
+		irq_unlock_sparse();
 		pr_err("do_boot_cpu failed(%d) to wakeup CPU#%u\n", err, cpu);
 		return -EIO;
 	}
@@ -1011,6 +1020,8 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 		touch_nmi_watchdog();
 	}
 
+	irq_unlock_sparse();
+
 	return 0;
 }
 

commit 5a3f75e3f02836518ce49536e9c460ca8e1fa290
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 5 17:12:32 2015 +0000

    x86/irq: Plug irq vector hotplug race
    
    Jin debugged a nasty cpu hotplug race which results in leaking a irq
    vector on the newly hotplugged cpu.
    
    cpu N                           cpu M
    native_cpu_up                   device_shutdown
      do_boot_cpu                     free_msi_irqs
      start_secondary                   arch_teardown_msi_irqs
        smp_callin                        default_teardown_msi_irqs
           setup_vector_irq                  arch_teardown_msi_irq
            __setup_vector_irq                 native_teardown_msi_irq
              lock(vector_lock)                  destroy_irq
              install vectors
              unlock(vector_lock)
                                                   lock(vector_lock)
    --->                                           __clear_irq_vector
                                                   unlock(vector_lock)
        lock(vector_lock)
        set_cpu_online
        unlock(vector_lock)
    
    This leaves the irq vector(s) which are torn down on CPU M stale in
    the vector array of CPU N, because CPU M does not see CPU N online
    yet. There is a similar issue with concurrent newly setup interrupts.
    
    The alloc/free protection of irq descriptors does not prevent the
    above race, because it merily prevents interrupt descriptors from
    going away or changing concurrently.
    
    Prevent this by moving the call to setup_vector_irq() into the
    vector_lock held region which protects set_cpu_online():
    
    cpu N                           cpu M
    native_cpu_up                   device_shutdown
      do_boot_cpu                     free_msi_irqs
      start_secondary                   arch_teardown_msi_irqs
        smp_callin                        default_teardown_msi_irqs
           lock(vector_lock)                arch_teardown_msi_irq
           setup_vector_irq()
            __setup_vector_irq                 native_teardown_msi_irq
              install vectors                    destroy_irq
           set_cpu_online
           unlock(vector_lock)
                                                   lock(vector_lock)
                                                   __clear_irq_vector
                                                   unlock(vector_lock)
    
    So cpu M either sees the cpu N online before clearing the vector or
    cpu N installs the vectors after cpu M has cleared it.
    
    Reported-by: xiao jin <jin.xiao@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Yanmin Zhang <yanmin_zhang@linux.intel.com>
    Link: http://lkml.kernel.org/r/20150705171102.141898931@linutronix.de

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0bd8c1d507b3..d3010aa79daf 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -170,11 +170,6 @@ static void smp_callin(void)
 	 */
 	apic_ap_setup();
 
-	/*
-	 * Need to setup vector mappings before we enable interrupts.
-	 */
-	setup_vector_irq(smp_processor_id());
-
 	/*
 	 * Save our processor parameters. Note: this information
 	 * is needed for clock calibration.
@@ -239,11 +234,13 @@ static void notrace start_secondary(void *unused)
 	check_tsc_sync_target();
 
 	/*
-	 * We need to hold vector_lock so there the set of online cpus
-	 * does not change while we are assigning vectors to cpus.  Holding
-	 * this lock ensures we don't half assign or remove an irq from a cpu.
+	 * Lock vector_lock and initialize the vectors on this cpu
+	 * before setting the cpu online. We must set it online with
+	 * vector_lock held to prevent a concurrent setup/teardown
+	 * from seeing a half valid vector space.
 	 */
 	lock_vector_lock();
+	setup_vector_irq(smp_processor_id());
 	set_cpu_online(smp_processor_id(), true);
 	unlock_vector_lock();
 	cpu_set_state_online(smp_processor_id());

commit 20d5e4a9cd453991e2590a4c25230a99b42dee0c
Author: Zhu Guihua <zhugh.fnst@cn.fujitsu.com>
Date:   Fri Jul 3 17:37:19 2015 +0800

    x86/espfix: Init espfix on the boot CPU side
    
    As we alloc pages with GFP_KERNEL in init_espfix_ap() which is
    called before we enable local irqs, so the lockdep sub-system
    would (correctly) trigger a warning about the potentially
    blocking API.
    
    So we allocate them on the boot CPU side when the secondary CPU is
    brought up by the boot CPU, and hand them over to the secondary
    CPU.
    
    And we use alloc_pages_node() with the secondary CPU's node, to
    make sure the espfix stack is NUMA-local to the CPU that is
    going to use it.
    
    Signed-off-by: Zhu Guihua <zhugh.fnst@cn.fujitsu.com>
    Cc: <bp@alien8.de>
    Cc: <luto@amacapital.net>
    Cc: <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/c97add2670e9abebb90095369f0cfc172373ac94.1435824469.git.zhugh.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6f5abac6c28b..0bd8c1d507b3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -238,13 +238,6 @@ static void notrace start_secondary(void *unused)
 	 */
 	check_tsc_sync_target();
 
-	/*
-	 * Enable the espfix hack for this CPU
-	 */
-#ifdef CONFIG_X86_ESPFIX64
-	init_espfix_ap(smp_processor_id());
-#endif
-
 	/*
 	 * We need to hold vector_lock so there the set of online cpus
 	 * does not change while we are assigning vectors to cpus.  Holding
@@ -854,6 +847,13 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	initial_code = (unsigned long)start_secondary;
 	stack_start  = idle->thread.sp;
 
+	/*
+	 * Enable the espfix hack for this CPU
+	*/
+#ifdef CONFIG_X86_ESPFIX64
+	init_espfix_ap(cpu);
+#endif
+
 	/* So we see what's up */
 	announce_cpu(cpu, apicid);
 

commit 1db875631f8d5bbf497f67e47f254eece888d51d
Author: Zhu Guihua <zhugh.fnst@cn.fujitsu.com>
Date:   Fri Jul 3 17:37:18 2015 +0800

    x86/espfix: Add 'cpu' parameter to init_espfix_ap()
    
    Add a CPU index parameter to init_espfix_ap(), so that the
    parameter could be propagated to the function for espfix
    page allocation.
    
    Signed-off-by: Zhu Guihua <zhugh.fnst@cn.fujitsu.com>
    Cc: <bp@alien8.de>
    Cc: <luto@amacapital.net>
    Cc: <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/cde3fcf1b3211f3f03feb1a995bce3fee850f0fc.1435824469.git.zhugh.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8add66b22f33..6f5abac6c28b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -242,7 +242,7 @@ static void notrace start_secondary(void *unused)
 	 * Enable the espfix hack for this CPU
 	 */
 #ifdef CONFIG_X86_ESPFIX64
-	init_espfix_ap();
+	init_espfix_ap(smp_processor_id());
 #endif
 
 	/*

commit d70b3ef54ceaf1c7c92209f5a662a670d04cbed9
Merge: 650ec5a6bd5d 7ef3d7d58d9d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 17:59:09 2015 -0700

    Merge branch 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 core updates from Ingo Molnar:
     "There were so many changes in the x86/asm, x86/apic and x86/mm topics
      in this cycle that the topical separation of -tip broke down somewhat -
      so the result is a more traditional architecture pull request,
      collected into the 'x86/core' topic.
    
      The topics were still maintained separately as far as possible, so
      bisectability and conceptual separation should still be pretty good -
      but there were a handful of merge points to avoid excessive
      dependencies (and conflicts) that would have been poorly tested in the
      end.
    
      The next cycle will hopefully be much more quiet (or at least will
      have fewer dependencies).
    
      The main changes in this cycle were:
    
       * x86/apic changes, with related IRQ core changes: (Jiang Liu, Thomas
         Gleixner)
    
         - This is the second and most intrusive part of changes to the x86
           interrupt handling - full conversion to hierarchical interrupt
           domains:
    
              [IOAPIC domain]   -----
                                     |
              [MSI domain]      --------[Remapping domain] ----- [ Vector domain ]
                                     |   (optional)          |
              [HPET MSI domain] -----                        |
                                                             |
              [DMAR domain]     -----------------------------
                                                             |
              [Legacy domain]   -----------------------------
    
           This now reflects the actual hardware and allowed us to distangle
           the domain specific code from the underlying parent domain, which
           can be optional in the case of interrupt remapping.  It's a clear
           separation of functionality and removes quite some duct tape
           constructs which plugged the remap code between ioapic/msi/hpet
           and the vector management.
    
         - Intel IOMMU IRQ remapping enhancements, to allow direct interrupt
           injection into guests (Feng Wu)
    
       * x86/asm changes:
    
         - Tons of cleanups and small speedups, micro-optimizations.  This
           is in preparation to move a good chunk of the low level entry
           code from assembly to C code (Denys Vlasenko, Andy Lutomirski,
           Brian Gerst)
    
         - Moved all system entry related code to a new home under
           arch/x86/entry/ (Ingo Molnar)
    
         - Removal of the fragile and ugly CFI dwarf debuginfo annotations.
           Conversion to C will reintroduce many of them - but meanwhile
           they are only getting in the way, and the upstream kernel does
           not rely on them (Ingo Molnar)
    
         - NOP handling refinements. (Borislav Petkov)
    
       * x86/mm changes:
    
         - Big PAT and MTRR rework: making the code more robust and
           preparing to phase out exposing direct MTRR interfaces to drivers -
           in favor of using PAT driven interfaces (Toshi Kani, Luis R
           Rodriguez, Borislav Petkov)
    
         - New ioremap_wt()/set_memory_wt() interfaces to support
           Write-Through cached memory mappings.  This is especially
           important for good performance on NVDIMM hardware (Toshi Kani)
    
       * x86/ras changes:
    
         - Add support for deferred errors on AMD (Aravind Gopalakrishnan)
    
           This is an important RAS feature which adds hardware support for
           poisoned data.  That means roughly that the hardware marks data
           which it has detected as corrupted but wasn't able to correct, as
           poisoned data and raises an APIC interrupt to signal that in the
           form of a deferred error.  It is the OS's responsibility then to
           take proper recovery action and thus prolonge system lifetime as
           far as possible.
    
         - Add support for Intel "Local MCE"s: upcoming CPUs will support
           CPU-local MCE interrupts, as opposed to the traditional system-
           wide broadcasted MCE interrupts (Ashok Raj)
    
         - Misc cleanups (Borislav Petkov)
    
       * x86/platform changes:
    
         - Intel Atom SoC updates
    
      ... and lots of other cleanups, fixlets and other changes - see the
      shortlog and the Git log for details"
    
    * 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (222 commits)
      x86/hpet: Use proper hpet device number for MSI allocation
      x86/hpet: Check for irq==0 when allocating hpet MSI interrupts
      x86/mm/pat, drivers/infiniband/ipath: Use arch_phys_wc_add() and require PAT disabled
      x86/mm/pat, drivers/media/ivtv: Use arch_phys_wc_add() and require PAT disabled
      x86/platform/intel/baytrail: Add comments about why we disabled HPET on Baytrail
      genirq: Prevent crash in irq_move_irq()
      genirq: Enhance irq_data_to_desc() to support hierarchy irqdomain
      iommu, x86: Properly handle posted interrupts for IOMMU hotplug
      iommu, x86: Provide irq_remapping_cap() interface
      iommu, x86: Setup Posted-Interrupts capability for Intel iommu
      iommu, x86: Add cap_pi_support() to detect VT-d PI capability
      iommu, x86: Avoid migrating VT-d posted interrupts
      iommu, x86: Save the mode (posted or remapped) of an IRTE
      iommu, x86: Implement irq_set_vcpu_affinity for intel_ir_chip
      iommu: dmar: Provide helper to copy shared irte fields
      iommu: dmar: Extend struct irte for VT-d Posted-Interrupts
      iommu: Add new member capability to struct irq_remap_ops
      x86/asm/entry/64: Disentangle error_entry/exit gsbase/ebx/usermode code
      x86/asm/entry/32: Shorten __audit_syscall_entry() args preparation
      x86/asm/entry/32: Explain reloading of registers after __audit_syscall_entry()
      ...

commit e75c73ad64478c12b3a44b86a3e7f62a4f65b93e
Merge: cfe3eceb7a2e a8424003679e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 17:16:11 2015 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU updates from Ingo Molnar:
     "This tree contains two main changes:
    
       - The big FPU code rewrite: wide reaching cleanups and reorganization
         that pulls all the FPU code together into a clean base in
         arch/x86/fpu/.
    
         The resulting code is leaner and faster, and much easier to
         understand.  This enables future work to further simplify the FPU
         code (such as removing lazy FPU restores).
    
         By its nature these changes have a substantial regression risk: FPU
         code related bugs are long lived, because races are often subtle
         and bugs mask as user-space failures that are difficult to track
         back to kernel side backs.  I'm aware of no unfixed (or even
         suspected) FPU related regression so far.
    
       - MPX support rework/fixes.  As this is still not a released CPU
         feature, there were some buglets in the code - should be much more
         robust now (Dave Hansen)"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (250 commits)
      x86/fpu: Fix double-increment in setup_xstate_features()
      x86/mpx: Allow 32-bit binaries on 64-bit kernels again
      x86/mpx: Do not count MPX VMAs as neighbors when unmapping
      x86/mpx: Rewrite the unmap code
      x86/mpx: Support 32-bit binaries on 64-bit kernels
      x86/mpx: Use 32-bit-only cmpxchg() for 32-bit apps
      x86/mpx: Introduce new 'directory entry' to 'addr' helper function
      x86/mpx: Add temporary variable to reduce masking
      x86: Make is_64bit_mm() widely available
      x86/mpx: Trace allocation of new bounds tables
      x86/mpx: Trace the attempts to find bounds tables
      x86/mpx: Trace entry to bounds exception paths
      x86/mpx: Trace #BR exceptions
      x86/mpx: Introduce a boot-time disable flag
      x86/mpx: Restrict the mmap() size check to bounds tables
      x86/mpx: Remove redundant MPX_BNDCFG_ADDR_MASK
      x86/mpx: Clean up the code by not passing a task pointer around when unnecessary
      x86/mpx: Use the new get_xsave_field_ptr()API
      x86/fpu/xstate: Wrap get_xsave_addr() to make it safer
      x86/fpu/xstate: Fix up bad get_xsave_addr() assumptions
      ...

commit 7d79a7bd7554d420313451fb805ebc37a8da97fe
Author: Bartosz Golaszewski <bgolaszewski@baylibre.com>
Date:   Tue May 26 15:11:35 2015 +0200

    x86: Replace cpu_**_mask() with topology_**_cpumask()
    
    The former duplicate the functionalities of the latter but are
    neither documented nor arch-independent.
    
    Signed-off-by: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Benoit Cousson <bcousson@baylibre.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: Jean Delvare <jdelvare@suse.de>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Drokin <oleg.drokin@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: http://lkml.kernel.org/r/1432645896-12588-9-git-send-email-bgolaszewski@baylibre.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 50e547eac8cd..0e8209619455 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -314,10 +314,10 @@ topology_sane(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o, const char *name)
 		cpu1, name, cpu2, cpu_to_node(cpu1), cpu_to_node(cpu2));
 }
 
-#define link_mask(_m, c1, c2)						\
+#define link_mask(mfunc, c1, c2)					\
 do {									\
-	cpumask_set_cpu((c1), cpu_##_m##_mask(c2));			\
-	cpumask_set_cpu((c2), cpu_##_m##_mask(c1));			\
+	cpumask_set_cpu((c1), mfunc(c2));				\
+	cpumask_set_cpu((c2), mfunc(c1));				\
 } while (0)
 
 static bool match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
@@ -398,9 +398,9 @@ void set_cpu_sibling_map(int cpu)
 	cpumask_set_cpu(cpu, cpu_sibling_setup_mask);
 
 	if (!has_mp) {
-		cpumask_set_cpu(cpu, cpu_sibling_mask(cpu));
+		cpumask_set_cpu(cpu, topology_sibling_cpumask(cpu));
 		cpumask_set_cpu(cpu, cpu_llc_shared_mask(cpu));
-		cpumask_set_cpu(cpu, cpu_core_mask(cpu));
+		cpumask_set_cpu(cpu, topology_core_cpumask(cpu));
 		c->booted_cores = 1;
 		return;
 	}
@@ -409,32 +409,34 @@ void set_cpu_sibling_map(int cpu)
 		o = &cpu_data(i);
 
 		if ((i == cpu) || (has_smt && match_smt(c, o)))
-			link_mask(sibling, cpu, i);
+			link_mask(topology_sibling_cpumask, cpu, i);
 
 		if ((i == cpu) || (has_mp && match_llc(c, o)))
-			link_mask(llc_shared, cpu, i);
+			link_mask(cpu_llc_shared_mask, cpu, i);
 
 	}
 
 	/*
 	 * This needs a separate iteration over the cpus because we rely on all
-	 * cpu_sibling_mask links to be set-up.
+	 * topology_sibling_cpumask links to be set-up.
 	 */
 	for_each_cpu(i, cpu_sibling_setup_mask) {
 		o = &cpu_data(i);
 
 		if ((i == cpu) || (has_mp && match_die(c, o))) {
-			link_mask(core, cpu, i);
+			link_mask(topology_core_cpumask, cpu, i);
 
 			/*
 			 *  Does this new cpu bringup a new core?
 			 */
-			if (cpumask_weight(cpu_sibling_mask(cpu)) == 1) {
+			if (cpumask_weight(
+			    topology_sibling_cpumask(cpu)) == 1) {
 				/*
 				 * for each core in package, increment
 				 * the booted_cores for this new cpu
 				 */
-				if (cpumask_first(cpu_sibling_mask(i)) == i)
+				if (cpumask_first(
+				    topology_sibling_cpumask(i)) == i)
 					c->booted_cores++;
 				/*
 				 * increment the core count for all
@@ -1009,8 +1011,8 @@ static __init void disable_smp(void)
 		physid_set_mask_of_physid(boot_cpu_physical_apicid, &phys_cpu_present_map);
 	else
 		physid_set_mask_of_physid(0, &phys_cpu_present_map);
-	cpumask_set_cpu(0, cpu_sibling_mask(0));
-	cpumask_set_cpu(0, cpu_core_mask(0));
+	cpumask_set_cpu(0, topology_sibling_cpumask(0));
+	cpumask_set_cpu(0, topology_core_cpumask(0));
 }
 
 enum {
@@ -1293,22 +1295,22 @@ static void remove_siblinginfo(int cpu)
 	int sibling;
 	struct cpuinfo_x86 *c = &cpu_data(cpu);
 
-	for_each_cpu(sibling, cpu_core_mask(cpu)) {
-		cpumask_clear_cpu(cpu, cpu_core_mask(sibling));
+	for_each_cpu(sibling, topology_core_cpumask(cpu)) {
+		cpumask_clear_cpu(cpu, topology_core_cpumask(sibling));
 		/*/
 		 * last thread sibling in this cpu core going down
 		 */
-		if (cpumask_weight(cpu_sibling_mask(cpu)) == 1)
+		if (cpumask_weight(topology_sibling_cpumask(cpu)) == 1)
 			cpu_data(sibling).booted_cores--;
 	}
 
-	for_each_cpu(sibling, cpu_sibling_mask(cpu))
-		cpumask_clear_cpu(cpu, cpu_sibling_mask(sibling));
+	for_each_cpu(sibling, topology_sibling_cpumask(cpu))
+		cpumask_clear_cpu(cpu, topology_sibling_cpumask(sibling));
 	for_each_cpu(sibling, cpu_llc_shared_mask(cpu))
 		cpumask_clear_cpu(cpu, cpu_llc_shared_mask(sibling));
 	cpumask_clear(cpu_llc_shared_mask(cpu));
-	cpumask_clear(cpu_sibling_mask(cpu));
-	cpumask_clear(cpu_core_mask(cpu));
+	cpumask_clear(topology_sibling_cpumask(cpu));
+	cpumask_clear(topology_core_cpumask(cpu));
 	c->phys_proc_id = 0;
 	c->cpu_core_id = 0;
 	cpumask_clear_cpu(cpu, cpu_sibling_setup_mask);

commit 78f7f1e54bac032b98956862a5bcf8c28ed22d07
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 02:54:44 2015 +0200

    x86/fpu: Rename fpu-internal.h to fpu/internal.h
    
    This unifies all the FPU related header files under a unified, hiearchical
    naming scheme:
    
     - asm/fpu/types.h:      FPU related data types, needed for 'struct task_struct',
                             widely included in almost all kernel code, and hence kept
                             as small as possible.
    
     - asm/fpu/api.h:        FPU related 'public' methods exported to other subsystems.
    
     - asm/fpu/internal.h:   FPU subsystem internal methods
    
     - asm/fpu/xsave.h:      XSAVE support internal methods
    
    (Also standardize the header guard in asm/fpu/internal.h.)
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 60e331ceb844..29f105f0d9fb 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -68,7 +68,7 @@
 #include <asm/mwait.h>
 #include <asm/apic.h>
 #include <asm/io_apic.h>
-#include <asm/fpu-internal.h>
+#include <asm/fpu/internal.h>
 #include <asm/setup.h>
 #include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>

commit f89e32e0a3df2f29d61fdc120ac62654ef267111
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 10:58:10 2015 +0200

    x86/fpu: Fix header file dependencies of fpu-internal.h
    
    Fix a minor header file dependency bug in asm/fpu-internal.h: it
    relies on i387.h but does not include it. All users of fpu-internal.h
    included it explicitly.
    
    Also remove unnecessary includes, to reduce compilation time.
    
    This also makes it easier to use it as a standalone header file
    for FPU internals, such as an upcoming C module in arch/x86/kernel/fpu/.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 50e547eac8cd..60e331ceb844 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -68,7 +68,6 @@
 #include <asm/mwait.h>
 #include <asm/apic.h>
 #include <asm/io_apic.h>
-#include <asm/i387.h>
 #include <asm/fpu-internal.h>
 #include <asm/setup.h>
 #include <asm/uv/uv.h>

commit 7cb685982157567dcc55eb92d1c38d237465203b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 18 12:05:13 2015 +0200

    x86/smp/boot: Fix legacy SMP bootup slow-boot bug
    
    So while testing kernels using tools/kvm/ (kvmtool) I noticed that it
    booted super slow:
    
    [    0.142991] Performance Events: no PMU driver, software events only.
    [    0.149265] x86: Booting SMP configuration:
    [    0.149765] .... node  #0, CPUs:          #1
    [    0.148304] kvm-clock: cpu 1, msr 2:1bfe9041, secondary cpu clock
    [   10.158813] KVM setup async PF for cpu 1
    [   10.159000]    #2
    [   10.159000] kvm-stealtime: cpu 1, msr 211a4d400
    [   10.158829] kvm-clock: cpu 2, msr 2:1bfe9081, secondary cpu clock
    [   20.167805] KVM setup async PF for cpu 2
    [   20.168000]    #3
    [   20.168000] kvm-stealtime: cpu 2, msr 211a8d400
    [   20.167818] kvm-clock: cpu 3, msr 2:1bfe90c1, secondary cpu clock
    [   30.176902] KVM setup async PF for cpu 3
    [   30.177000]    #4
    [   30.177000] kvm-stealtime: cpu 3, msr 211acd400
    
    One CPU booted up per 10 seconds. With 120 CPUs that takes a while.
    
    Bisection pinpointed this commit:
    
      853b160aaafb ("Revert f5d6a52f5111 ("x86/smpboot: Skip delays during SMP initialization similar to Xen")")
    
    But that commit just restores previous behavior, so it cannot cause the
    problem. After some head scratching it turns out that these two commits:
    
      1a744cb356c5 ("x86/smp/boot: Remove 10ms delay from cpu_up() on modern processors")
      d68921f9bd14 ("x86/smp/boot: Add cmdline "cpu_init_udelay=N" to specify cpu_up() delay")
    
    added the following code to smpboot.c:
    
    -               mdelay(10);
    +               mdelay(init_udelay);
    
    Note the mismatch in the units: the delay is called 'udelay' and is set
    to microseconds - while the function used here is actually 'mdelay',
    which counts in milliseconds ...
    
    So the delay for legacy systems is off by a factor of 1,000, so instead
    of 10 msecs we waited for 10 seconds ...
    
    The reason bisection pointed to 853b160aaafb was that 853b160aaafb removed
    a (broken) boot-time speedup patch, which masked the factor 1,000 bug.
    
    Fix it by using udelay(). This fixes my bootup problems.
    
    Cc: Len Brown <len.brown@intel.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b9aaa3930b2f..fd6291c921b6 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -617,7 +617,7 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	pr_debug("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();
 
-	mdelay(init_udelay);
+	udelay(init_udelay);
 
 	pr_debug("Deasserting INIT\n");
 

commit 853b160aaafbe27d6304c8832bb7340d57c6b04e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 13 08:40:49 2015 +0200

    Revert f5d6a52f5111 ("x86/smpboot: Skip delays during SMP initialization similar to Xen")
    
    Huang Ying reported x86 boot hangs due to this commit.
    
    Turns out that the change, despite its changelog, does more
    than just change timeouts: it also changes the way we
    assert/deassert INIT via the APIC_DM_INIT IPI, in the x2apic
    case it skips the deassert step.
    
    This is historically fragile code and the patch did not
    improve it, so revert these changes.
    
    This commit:
    
      1a744cb356c5 ("x86/smp/boot: Remove 10ms delay from cpu_up() on modern processors")
    
    independently removes the worst of the delays (the 10 msec delay).
    
    The remaining delays can be addressed one by one, combined
    with careful testing.
    
    Reported-by: Huang Ying <ying.huang@intel.com>
    Cc: Anthony Liguori <aliguori@amazon.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Gang Wei <gang.wei@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Deegan <tim@xen.org>
    Link: http://lkml.kernel.org/r/1430732554-7294-1-git-send-email-jschoenh@amazon.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 85bd6aad8c74..b9aaa3930b2f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -614,34 +614,22 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	apic_icr_write(APIC_INT_LEVELTRIG | APIC_INT_ASSERT | APIC_DM_INIT,
 		       phys_apicid);
 
-	if (!cpu_has_x2apic) {
-		pr_debug("Waiting for send to finish...\n");
-		send_status = safe_apic_wait_icr_idle();
+	pr_debug("Waiting for send to finish...\n");
+	send_status = safe_apic_wait_icr_idle();
 
-		mdelay(init_udelay);
+	mdelay(init_udelay);
 
-		pr_debug("Deasserting INIT\n");
+	pr_debug("Deasserting INIT\n");
 
-		/* Target chip */
-		/* Send IPI */
-		apic_icr_write(APIC_INT_LEVELTRIG | APIC_DM_INIT, phys_apicid);
+	/* Target chip */
+	/* Send IPI */
+	apic_icr_write(APIC_INT_LEVELTRIG | APIC_DM_INIT, phys_apicid);
 
-		pr_debug("Waiting for send to finish...\n");
-		send_status = safe_apic_wait_icr_idle();
+	pr_debug("Waiting for send to finish...\n");
+	send_status = safe_apic_wait_icr_idle();
 
-		mb();
-		atomic_set(&init_deasserted, 1);
-	} else if (tboot_enabled()) {
-		/*
-		 * With tboot AP is actually spinning in a mini-guest before
-		 * receiving INIT. Upon receiving INIT ipi, AP need time to
-		 * VMExit, update VMCS to tracking SIPIs and VMResume.
-		 *
-		 * While AP is in root mode handling the INIT the CPU will drop
-		 * any SIPIs
-		 */
-		udelay(10);
-	}
+	mb();
+	atomic_set(&init_deasserted, 1);
 
 	/*
 	 * Should we send STARTUP IPIs ?
@@ -683,22 +671,20 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 		apic_icr_write(APIC_DM_STARTUP | (start_eip >> 12),
 			       phys_apicid);
 
-		if (!cpu_has_x2apic) {
-			/*
-			 * Give the other CPU some time to accept the IPI.
-			 */
-			udelay(300);
+		/*
+		 * Give the other CPU some time to accept the IPI.
+		 */
+		udelay(300);
 
-			pr_debug("Startup point 1\n");
+		pr_debug("Startup point 1\n");
 
-			pr_debug("Waiting for send to finish...\n");
-			send_status = safe_apic_wait_icr_idle();
+		pr_debug("Waiting for send to finish...\n");
+		send_status = safe_apic_wait_icr_idle();
 
-			/*
-			 * Give the other CPU some time to accept the IPI.
-			 */
-			udelay(200);
-		}
+		/*
+		 * Give the other CPU some time to accept the IPI.
+		 */
+		udelay(200);
 
 		if (maxlvt > 3)		/* Due to the Pentium erratum 3AP.  */
 			apic_write(APIC_ESR, 0);

commit 1a744cb356c57303fc97eb15a298032170f841fa
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 11 17:27:10 2015 -0400

    x86/smp/boot: Remove 10ms delay from cpu_up() on modern processors
    
    Modern processor familes do not require the 10ms delay
    in cpu_up() to de-assert INIT.  This speeds up boot
    and resume by 10ms per (application) processor.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/021ce30c88f216ad39686646421194dc25671e55.1431379433.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0629a8e513af..85bd6aad8c74 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -521,6 +521,7 @@ void __inquire_remote_apic(int apicid)
  * many cores and don't require that delay.
  *
  * Cmdline "init_cpu_udelay=" is available to over-ride this delay.
+ * Modern processor families are quirked to remove the delay entirely.
  */
 #define UDELAY_10MS_DEFAULT 10000
 
@@ -534,6 +535,18 @@ static int __init cpu_init_udelay(char *str)
 }
 early_param("cpu_init_udelay", cpu_init_udelay);
 
+static void __init smp_quirk_init_udelay(void)
+{
+	/* if cmdline changed it from default, leave it alone */
+	if (init_udelay != UDELAY_10MS_DEFAULT)
+		return;
+
+	/* if modern processor, use no delay */
+	if (((boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) && (boot_cpu_data.x86 == 6)) ||
+	    ((boot_cpu_data.x86_vendor == X86_VENDOR_AMD) && (boot_cpu_data.x86 >= 0xF)))
+		init_udelay = 0;
+}
+
 /*
  * Poke the other CPU in the eye via NMI to wake it up. Remember that the normal
  * INIT, INIT, STARTUP sequence will reset the chip hard for us, and this
@@ -1210,6 +1223,8 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		uv_system_init();
 
 	set_mtrr_aps_delayed_init();
+
+	smp_quirk_init_udelay();
 }
 
 void arch_enable_nonboot_cpus_begin(void)

commit d68921f9bd148359e6d01c84aaa2e32bfbd82970
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 11 17:27:09 2015 -0400

    x86/smp/boot: Add cmdline "cpu_init_udelay=N" to specify cpu_up() delay
    
    No change to default behavior.
    
    Replace the hard-coded mdelay(10) in cpu_up() with a variable
    udelay, that is set to a defined default -- rather than a magic
    number.
    
    Add a boot-time override, "cpu_init_udelay=N"
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/2fe8e6c798e8def271122f62df9bbf58dc283e2a.1431379433.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 51203f60587f..0629a8e513af 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -513,6 +513,27 @@ void __inquire_remote_apic(int apicid)
 	}
 }
 
+/*
+ * The Multiprocessor Specification 1.4 (1997) example code suggests
+ * that there should be a 10ms delay between the BSP asserting INIT
+ * and de-asserting INIT, when starting a remote processor.
+ * But that slows boot and resume on modern processors, which include
+ * many cores and don't require that delay.
+ *
+ * Cmdline "init_cpu_udelay=" is available to over-ride this delay.
+ */
+#define UDELAY_10MS_DEFAULT 10000
+
+static unsigned int init_udelay = UDELAY_10MS_DEFAULT;
+
+static int __init cpu_init_udelay(char *str)
+{
+	get_option(&str, &init_udelay);
+
+	return 0;
+}
+early_param("cpu_init_udelay", cpu_init_udelay);
+
 /*
  * Poke the other CPU in the eye via NMI to wake it up. Remember that the normal
  * INIT, INIT, STARTUP sequence will reset the chip hard for us, and this
@@ -584,7 +605,7 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 		pr_debug("Waiting for send to finish...\n");
 		send_status = safe_apic_wait_icr_idle();
 
-		mdelay(10);
+		mdelay(init_udelay);
 
 		pr_debug("Deasserting INIT\n");
 

commit 191a66353b22fad8ac89404ab4c929cbe7b0afb2
Merge: f5d6a52f5111 f21262b8e092
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 11 16:05:09 2015 +0200

    Merge branch 'x86/asm' into x86/apic, to resolve a conflict
    
    Conflicts:
            arch/x86/kernel/apic/io_apic.c
            arch/x86/kernel/apic/vector.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit fed7c3f0f750f225317828d691e9eb76eec887b3
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Fri Apr 24 17:31:34 2015 +0200

    x86/entry: Remove unused 'kernel_stack' per-cpu variable
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1429889495-27850-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 50e547eac8cd..023cccf5a4ae 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -792,8 +792,6 @@ void common_cpu_up(unsigned int cpu, struct task_struct *idle)
 	clear_tsk_thread_flag(idle, TIF_FORK);
 	initial_gs = per_cpu_offset(cpu);
 #endif
-	per_cpu(kernel_stack, cpu) =
-		(unsigned long)task_stack_page(idle) + THREAD_SIZE;
 }
 
 /*

commit f5d6a52f511157c7476590532a23b5664b1ed877
Author: Jan H. Schönherr <jschoenh@amazon.de>
Date:   Mon May 4 11:42:34 2015 +0200

    x86/smpboot: Skip delays during SMP initialization similar to Xen
    
    Remove the per-CPU delays during SMP initialization, which seems
    to be possible on newer architectures with an x2APIC.
    
    Xen does this since 2011. In fact, this commit is basically a
    combination of the following Xen commits. The first removes the
    delays, the second fixes an issue with the removal:
    
      commit 68fce206f6dba9981e8322269db49692c95ce250
      Author: Tim Deegan <Tim.Deegan@citrix.com>
      Date:   Tue Jul 19 14:13:01 2011 +0100
    
        x86: Remove timeouts from INIT-SIPI-SIPI sequence when using x2apic.
    
        Some of the timeouts are pointless since they're waiting for the ICR
        to ack the IPI delivery and that doesn't happen on x2apic.
        The others should be benign (and are suggested in the SDM) but
        removing them makes AP bringup much more reliable on some test boxes.
    
        Signed-off-by: Tim Deegan <Tim.Deegan@citrix.com>
    
      commit f12ee533150761df5a7099c83f2a5fa6c07d1187
      Author: Gang Wei <gang.wei@intel.com>
      Date:   Thu Dec 29 10:07:54 2011 +0000
    
        X86: Add a delay between INIT & SIPIs for tboot AP bring-up in X2APIC case
    
        Without this delay, Xen could not bring APs up while working with
        TXT/tboot, because tboot needs some time in APs to handle INIT before
        becoming ready for receiving SIPIs (this delay was removed as part of
        c/s 23724 by Tim Deegan).
    
        Signed-off-by: Gang Wei <gang.wei@intel.com>
        Acked-by: Keir Fraser <keir@xen.org>
        Acked-by: Tim Deegan <tim@xen.org>
        Committed-by: Tim Deegan <tim@xen.org>
    
    Signed-off-by: Jan H. Schönherr <jschoenh@amazon.de>
    Cc: Anthony Liguori <aliguori@amazon.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Gang Wei <gang.wei@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Deegan <tim@xen.org>
    Link: http://lkml.kernel.org/r/1430732554-7294-1-git-send-email-jschoenh@amazon.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 50e547eac8cd..63b46414c80c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -555,7 +555,7 @@ wakeup_secondary_cpu_via_nmi(int apicid, unsigned long start_eip)
 static int
 wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 {
-	unsigned long send_status, accept_status = 0;
+	unsigned long send_status = 0, accept_status = 0;
 	int maxlvt, num_starts, j;
 
 	maxlvt = lapic_get_maxlvt();
@@ -580,22 +580,34 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	apic_icr_write(APIC_INT_LEVELTRIG | APIC_INT_ASSERT | APIC_DM_INIT,
 		       phys_apicid);
 
-	pr_debug("Waiting for send to finish...\n");
-	send_status = safe_apic_wait_icr_idle();
+	if (!cpu_has_x2apic) {
+		pr_debug("Waiting for send to finish...\n");
+		send_status = safe_apic_wait_icr_idle();
 
-	mdelay(10);
+		mdelay(10);
 
-	pr_debug("Deasserting INIT\n");
+		pr_debug("Deasserting INIT\n");
 
-	/* Target chip */
-	/* Send IPI */
-	apic_icr_write(APIC_INT_LEVELTRIG | APIC_DM_INIT, phys_apicid);
+		/* Target chip */
+		/* Send IPI */
+		apic_icr_write(APIC_INT_LEVELTRIG | APIC_DM_INIT, phys_apicid);
 
-	pr_debug("Waiting for send to finish...\n");
-	send_status = safe_apic_wait_icr_idle();
+		pr_debug("Waiting for send to finish...\n");
+		send_status = safe_apic_wait_icr_idle();
 
-	mb();
-	atomic_set(&init_deasserted, 1);
+		mb();
+		atomic_set(&init_deasserted, 1);
+	} else if (tboot_enabled()) {
+		/*
+		 * With tboot AP is actually spinning in a mini-guest before
+		 * receiving INIT. Upon receiving INIT ipi, AP need time to
+		 * VMExit, update VMCS to tracking SIPIs and VMResume.
+		 *
+		 * While AP is in root mode handling the INIT the CPU will drop
+		 * any SIPIs
+		 */
+		udelay(10);
+	}
 
 	/*
 	 * Should we send STARTUP IPIs ?
@@ -637,20 +649,23 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 		apic_icr_write(APIC_DM_STARTUP | (start_eip >> 12),
 			       phys_apicid);
 
-		/*
-		 * Give the other CPU some time to accept the IPI.
-		 */
-		udelay(300);
+		if (!cpu_has_x2apic) {
+			/*
+			 * Give the other CPU some time to accept the IPI.
+			 */
+			udelay(300);
 
-		pr_debug("Startup point 1\n");
+			pr_debug("Startup point 1\n");
 
-		pr_debug("Waiting for send to finish...\n");
-		send_status = safe_apic_wait_icr_idle();
+			pr_debug("Waiting for send to finish...\n");
+			send_status = safe_apic_wait_icr_idle();
+
+			/*
+			 * Give the other CPU some time to accept the IPI.
+			 */
+			udelay(200);
+		}
 
-		/*
-		 * Give the other CPU some time to accept the IPI.
-		 */
-		udelay(200);
 		if (maxlvt > 3)		/* Due to the Pentium erratum 3AP.  */
 			apic_write(APIC_ESR, 0);
 		accept_status = (apic_read(APIC_ESR) & 0xEF);

commit 078838d56574694d0a4815d9c1b7f28e8844638b
Merge: eeee78cf77df 590ee7dbd569
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 13:36:04 2015 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - changes permitting use of call_rcu() and friends very early in
         boot, for example, before rcu_init() is invoked.
    
       - add in-kernel API to enable and disable expediting of normal RCU
         grace periods.
    
       - improve RCU's handling of (hotplug-) outgoing CPUs.
    
       - NO_HZ_FULL_SYSIDLE fixes.
    
       - tiny-RCU updates to make it more tiny.
    
       - documentation updates.
    
       - miscellaneous fixes"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (58 commits)
      cpu: Provide smpboot_thread_init() on !CONFIG_SMP kernels as well
      cpu: Defer smpboot kthread unparking until CPU known to scheduler
      rcu: Associate quiescent-state reports with grace period
      rcu: Yet another fix for preemption and CPU hotplug
      rcu: Add diagnostics to grace-period cleanup
      rcutorture: Default to grace-period-initialization delays
      rcu: Handle outgoing CPUs on exit from idle loop
      cpu: Make CPU-offline idle-loop transition point more precise
      rcu: Eliminate ->onoff_mutex from rcu_node structure
      rcu: Process offlining and onlining only at grace-period start
      rcu: Move rcu_report_unblock_qs_rnp() to common code
      rcu: Rework preemptible expedited bitmask handling
      rcu: Remove event tracing from rcu_cpu_notify(), used by offline CPUs
      rcutorture: Enable slow grace-period initializations
      rcu: Provide diagnostic option to slow down grace-period initialization
      rcu: Detect stalls caused by failure to propagate up rcu_node tree
      rcu: Eliminate empty HOTPLUG_CPU ifdef
      rcu: Simplify sync_rcu_preempt_exp_init()
      rcu: Put all orphan-callback-related code under same comment
      rcu: Consolidate offline-CPU callback initialization
      ...

commit 60f898eeaaa1c5d0162a4240bacf33a6c87ecef6
Merge: 977e1ba50893 3b75232d5568
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 13:16:36 2015 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm changes from Ingo Molnar:
     "There were lots of changes in this development cycle:
    
       - over 100 separate cleanups, restructuring changes, speedups and
         fixes in the x86 system call, irq, trap and other entry code, part
         of a heroic effort to deobfuscate a decade old spaghetti asm code
         and its C code dependencies (Denys Vlasenko, Andy Lutomirski)
    
       - alternatives code fixes and enhancements (Borislav Petkov)
    
       - simplifications and cleanups to the compat code (Brian Gerst)
    
       - signal handling fixes and new x86 testcases (Andy Lutomirski)
    
       - various other fixes and cleanups
    
      By their nature many of these changes are risky - we tried to test
      them well on many different x86 systems (there are no known
      regressions), and they are split up finely to help bisection - but
      there's still a fair bit of residual risk left so caveat emptor"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (148 commits)
      perf/x86/64: Report regs_user->ax too in get_regs_user()
      perf/x86/64: Simplify regs_user->abi setting code in get_regs_user()
      perf/x86/64: Do report user_regs->cx while we are in syscall, in get_regs_user()
      perf/x86/64: Do not guess user_regs->cs, ss, sp in get_regs_user()
      x86/asm/entry/32: Tidy up JNZ instructions after TESTs
      x86/asm/entry/64: Reduce padding in execve stubs
      x86/asm/entry/64: Remove GET_THREAD_INFO() in ret_from_fork
      x86/asm/entry/64: Simplify jumps in ret_from_fork
      x86/asm/entry/64: Remove a redundant jump
      x86/asm/entry/64: Optimize [v]fork/clone stubs
      x86/asm/entry: Zero EXTRA_REGS for stub32_execve() too
      x86/asm/entry/64: Move stub_x32_execvecloser() to stub_execveat()
      x86/asm/entry/64: Use common code for rt_sigreturn() epilogue
      x86/asm/entry/64: Add forgotten CFI annotation
      x86/asm/entry/irq: Simplify interrupt dispatch table (IDT) layout
      x86/asm/entry/64: Move opportunistic sysret code to syscall code path
      x86, selftests: Add sigreturn selftest
      x86/alternatives: Guard NOPs optimization
      x86/asm/entry: Clear EXTRA_REGS for all executable formats
      x86/signal: Remove pax argument from restore_sigcontext
      ...

commit 3f85483bd80ef1de8cbbf0361be59f6a069b59d4
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Wed Apr 1 10:12:14 2015 -0400

    x86/cpu: Factor out common CPU initialization code, fix 32-bit Xen PV guests
    
    Some of x86 bare-metal and Xen CPU initialization code is common
    between the two and therefore can be factored out to avoid code
    duplication.
    
    As a side effect, doing so will also extend the fix provided by
    commit a7fcf28d431e ("x86/asm/entry: Replace this_cpu_sp0() with
    current_top_of_stack() to x86_32") to 32-bit Xen PV guests.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: konrad.wilk@oracle.com
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1427897534-5086-1-git-send-email-boris.ostrovsky@oracle.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7b20ffd2fffc..5b298a95d567 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -779,6 +779,26 @@ wakeup_cpu_via_init_nmi(int cpu, unsigned long start_ip, int apicid,
 	return boot_error;
 }
 
+void common_cpu_up(unsigned int cpu, struct task_struct *idle)
+{
+	/* Just in case we booted with a single CPU. */
+	alternatives_enable_smp();
+
+	per_cpu(current_task, cpu) = idle;
+
+#ifdef CONFIG_X86_32
+	/* Stack for startup_32 can be just as for start_secondary onwards */
+	irq_ctx_init(cpu);
+	per_cpu(cpu_current_top_of_stack, cpu) =
+		(unsigned long)task_stack_page(idle) + THREAD_SIZE;
+#else
+	clear_tsk_thread_flag(idle, TIF_FORK);
+	initial_gs = per_cpu_offset(cpu);
+#endif
+	per_cpu(kernel_stack, cpu) =
+		(unsigned long)task_stack_page(idle) + THREAD_SIZE;
+}
+
 /*
  * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad
  * (ie clustered apic addressing mode), this is a LOGICAL apic ID.
@@ -796,24 +816,9 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	int cpu0_nmi_registered = 0;
 	unsigned long timeout;
 
-	/* Just in case we booted with a single CPU. */
-	alternatives_enable_smp();
-
 	idle->thread.sp = (unsigned long) (((struct pt_regs *)
 			  (THREAD_SIZE +  task_stack_page(idle))) - 1);
-	per_cpu(current_task, cpu) = idle;
 
-#ifdef CONFIG_X86_32
-	/* Stack for startup_32 can be just as for start_secondary onwards */
-	irq_ctx_init(cpu);
-	per_cpu(cpu_current_top_of_stack, cpu) =
-		(unsigned long)task_stack_page(idle) + THREAD_SIZE;
-#else
-	clear_tsk_thread_flag(idle, TIF_FORK);
-	initial_gs = per_cpu_offset(cpu);
-#endif
-	per_cpu(kernel_stack, cpu) =
-		(unsigned long)task_stack_page(idle) + THREAD_SIZE;
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	initial_code = (unsigned long)start_secondary;
 	stack_start  = idle->thread.sp;
@@ -954,6 +959,8 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 	/* the FPU context is blank, nobody can own it */
 	__cpu_disable_lazy_restore(cpu);
 
+	common_cpu_up(cpu, tidle);
+
 	err = do_boot_cpu(apicid, cpu, tidle);
 	if (err) {
 		pr_err("do_boot_cpu failed(%d) to wakeup CPU#%u\n", err, cpu);

commit 4399c03c6780ed75fa26e09a7b3a175b3aac5760
Author: Bandan Das <bsd@redhat.com>
Date:   Tue Mar 31 16:43:17 2015 -0400

    x86/apic: Remove verify_local_APIC()
    
    __verify_local_APIC() is detritus from the early APIC days.
    Its return value isn't used anywhere and the information it
    prints when debug is enabled is already part of APIC
    initialization messages printed to syslog. Off with it!
    
    Signed-off-by: Bandan Das <bsd@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/jpgy4mcsxsq.fsf@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index febc6aabc72e..ddd2c0674cda 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1086,8 +1086,6 @@ static int __init smp_sanity_check(unsigned max_cpus)
 		return SMP_NO_APIC;
 	}
 
-	verify_local_APIC();
-
 	/*
 	 * If SMP should be disabled, then really disable it!
 	 */

commit ef593260f0cae2699874f098fb5b19fb46502cb3
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Thu Mar 19 18:17:46 2015 +0100

    x86/asm/entry: Get rid of KERNEL_STACK_OFFSET
    
    PER_CPU_VAR(kernel_stack) was set up in a way where it points
    five stack slots below the top of stack.
    
    Presumably, it was done to avoid one "sub $5*8,%rsp"
    in syscall/sysenter code paths, where iret frame needs to be
    created by hand.
    
    Ironically, none of them benefits from this optimization,
    since all of them need to allocate additional data on stack
    (struct pt_regs), so they still have to perform subtraction.
    
    This patch eliminates KERNEL_STACK_OFFSET.
    
    PER_CPU_VAR(kernel_stack) now points directly to top of stack.
    pt_regs allocations are adjusted to allocate iret frame as well.
    Hopefully we can merge it later with 32-bit specific
    PER_CPU_VAR(cpu_current_top_of_stack) variable...
    
    Net result in generated code is that constants in several insns
    are changed.
    
    This change is necessary for changing struct pt_regs creation
    in SYSCALL64 code path from MOV to PUSH instructions.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1426785469-15125-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 759388c538cf..7b20ffd2fffc 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -813,8 +813,7 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	initial_gs = per_cpu_offset(cpu);
 #endif
 	per_cpu(kernel_stack, cpu) =
-		(unsigned long)task_stack_page(idle) -
-		KERNEL_STACK_OFFSET + THREAD_SIZE;
+		(unsigned long)task_stack_page(idle) + THREAD_SIZE;
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	initial_code = (unsigned long)start_secondary;
 	stack_start  = idle->thread.sp;

commit 2a442c9c6453d3d043dfd89f2e03a1deff8a6f06
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Feb 25 11:42:15 2015 -0800

    x86: Use common outgoing-CPU-notification code
    
    This commit removes the open-coded CPU-offline notification with new
    common code.  Among other things, this change avoids calling scheduler
    code using RCU from an offline CPU that RCU is ignoring.  It also allows
    Xen to notice at online time that the CPU did not go offline correctly.
    Note that Xen has the surviving CPU carry out some cleanup operations,
    so if the surviving CPU times out, these cleanup operations might have
    been carried out while the outgoing CPU was still running.  It might
    therefore be unwise to bring this CPU back online, and this commit
    avoids doing so.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: <x86@kernel.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: <xen-devel@lists.xenproject.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index febc6aabc72e..c8fa34963ead 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -77,9 +77,6 @@
 #include <asm/realmode.h>
 #include <asm/misc.h>
 
-/* State of each CPU */
-DEFINE_PER_CPU(int, cpu_state) = { 0 };
-
 /* Number of siblings per CPU package */
 int smp_num_siblings = 1;
 EXPORT_SYMBOL(smp_num_siblings);
@@ -257,7 +254,7 @@ static void notrace start_secondary(void *unused)
 	lock_vector_lock();
 	set_cpu_online(smp_processor_id(), true);
 	unlock_vector_lock();
-	per_cpu(cpu_state, smp_processor_id()) = CPU_ONLINE;
+	cpu_set_state_online(smp_processor_id());
 	x86_platform.nmi_init();
 
 	/* enable local interrupts */
@@ -948,7 +945,10 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 	 */
 	mtrr_save_state();
 
-	per_cpu(cpu_state, cpu) = CPU_UP_PREPARE;
+	/* x86 CPUs take themselves offline, so delayed offline is OK. */
+	err = cpu_check_up_prepare(cpu);
+	if (err && err != -EBUSY)
+		return err;
 
 	/* the FPU context is blank, nobody can own it */
 	__cpu_disable_lazy_restore(cpu);
@@ -1191,7 +1191,7 @@ void __init native_smp_prepare_boot_cpu(void)
 	switch_to_new_gdt(me);
 	/* already set me in cpu_online_mask in boot_cpu_init() */
 	cpumask_set_cpu(me, cpu_callout_mask);
-	per_cpu(cpu_state, me) = CPU_ONLINE;
+	cpu_set_state_online(me);
 }
 
 void __init native_smp_cpus_done(unsigned int max_cpus)
@@ -1318,14 +1318,10 @@ static void __ref remove_cpu_from_maps(int cpu)
 	numa_remove_cpu(cpu);
 }
 
-static DEFINE_PER_CPU(struct completion, die_complete);
-
 void cpu_disable_common(void)
 {
 	int cpu = smp_processor_id();
 
-	init_completion(&per_cpu(die_complete, smp_processor_id()));
-
 	remove_siblinginfo(cpu);
 
 	/* It's now safe to remove this processor from the online map */
@@ -1349,24 +1345,27 @@ int native_cpu_disable(void)
 	return 0;
 }
 
-void cpu_die_common(unsigned int cpu)
+int common_cpu_die(unsigned int cpu)
 {
-	wait_for_completion_timeout(&per_cpu(die_complete, cpu), HZ);
-}
+	int ret = 0;
 
-void native_cpu_die(unsigned int cpu)
-{
 	/* We don't do anything here: idle task is faking death itself. */
 
-	cpu_die_common(cpu);
-
 	/* They ack this in play_dead() by setting CPU_DEAD */
-	if (per_cpu(cpu_state, cpu) == CPU_DEAD) {
+	if (cpu_wait_death(cpu, 5)) {
 		if (system_state == SYSTEM_RUNNING)
 			pr_info("CPU %u is now offline\n", cpu);
 	} else {
 		pr_err("CPU %u didn't die...\n", cpu);
+		ret = -1;
 	}
+
+	return ret;
+}
+
+void native_cpu_die(unsigned int cpu)
+{
+	common_cpu_die(cpu);
 }
 
 void play_dead_common(void)
@@ -1375,10 +1374,8 @@ void play_dead_common(void)
 	reset_lazy_tlbstate();
 	amd_e400_remove_cpu(raw_smp_processor_id());
 
-	mb();
 	/* Ack it */
-	__this_cpu_write(cpu_state, CPU_DEAD);
-	complete(&per_cpu(die_complete, smp_processor_id()));
+	(void)cpu_report_death();
 
 	/*
 	 * With physical CPU hotplug, we should halt the cpu

commit a7fcf28d431ef70afaa91496e64e16dc51dccec4
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Mar 6 17:50:19 2015 -0800

    x86/asm/entry: Replace this_cpu_sp0() with current_top_of_stack() and fix it on x86_32
    
    I broke 32-bit kernels.  The implementation of sp0 was correct
    as far as I can tell, but sp0 was much weirder on x86_32 than I
    realized.  It has the following issues:
    
     - Init's sp0 is inconsistent with everything else's: non-init tasks
       are offset by 8 bytes.  (I have no idea why, and the comment is unhelpful.)
    
     - vm86 does crazy things to sp0.
    
    Fix it up by replacing this_cpu_sp0() with
    current_top_of_stack() and using a new percpu variable to track
    the top of the stack on x86_32.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 75182b1632a8 ("x86/asm/entry: Switch all C consumers of kernel_stack to this_cpu_sp0()")
    Link: http://lkml.kernel.org/r/d09dbe270883433776e0cbee3c7079433349e96d.1425692936.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index febc6aabc72e..759388c538cf 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -806,6 +806,8 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 #ifdef CONFIG_X86_32
 	/* Stack for startup_32 can be just as for start_secondary onwards */
 	irq_ctx_init(cpu);
+	per_cpu(cpu_current_top_of_stack, cpu) =
+		(unsigned long)task_stack_page(idle) + THREAD_SIZE;
 #else
 	clear_tsk_thread_flag(idle, TIF_FORK);
 	initial_gs = per_cpu_offset(cpu);

commit 9c4d9c73dd380ecfe1893600174f96d0eb068997
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 15 21:22:45 2015 +0000

    x86: Consolidate boot cpu timer setup
    
    Now that the APIC bringup is consolidated we can move the setup call
    for the percpu clock event device to apic_bsp_setup().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20150115211704.162567839@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0a46e5e4fa1f..febc6aabc72e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1163,12 +1163,8 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 	cpu0_logical_apicid = apic_bsp_setup(false);
 
-	/*
-	 * Set up local APIC timer on boot CPU.
-	 */
 	pr_info("CPU%d: ", 0);
 	print_cpu_info(&cpu_data(0));
-	x86_init.timers.setup_percpu_clockev();
 
 	if (is_uv_system())
 		uv_system_init();

commit 374aab339f10f0510cec0e79d752d31d84b08aa2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 15 21:22:44 2015 +0000

    x86/apic: Reuse apic_bsp_setup() for UP APIC setup
    
    Extend apic_bsp_setup() so the same code flow can be used for
    APIC_init_uniprocessor().
    
    Folded Jiangs fix to provide proper ordering of the UP setup.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20150115211704.084765674@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fe8783d500c2..0a46e5e4fa1f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1147,7 +1147,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		return;
 	case SMP_FORCE_UP:
 		disable_smp();
-		apic_bsp_setup();
+		apic_bsp_setup(false);
 		return;
 	case SMP_OK:
 		break;
@@ -1161,7 +1161,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		/* Or can we switch back to PIC here? */
 	}
 
-	cpu0_logical_apicid = apic_bsp_setup();
+	cpu0_logical_apicid = apic_bsp_setup(false);
 
 	/*
 	 * Set up local APIC timer on boot CPU.

commit 613c25efbdc763ee8b9d732368106d2456279356
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 15 21:22:42 2015 +0000

    x86/smpboot: Sanitize uniprocessor init
    
    The UP related setups for local apic are mangled into smp_sanity_check().
    
    That results in duplicate calls to disable_smp() and makes the code
    hard to follow. Let smp_sanity_check() return dedicated values for the
    various exit reasons and handle them at the call site.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Link: http://lkml.kernel.org/r/20150115211703.987833932@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d53870928824..fe8783d500c2 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -990,6 +990,8 @@ void arch_disable_smp_support(void)
  */
 static __init void disable_smp(void)
 {
+	pr_info("SMP disabled\n");
+
 	disable_ioapic_support();
 
 	init_cpu_present(cpumask_of(0));
@@ -1003,6 +1005,13 @@ static __init void disable_smp(void)
 	cpumask_set_cpu(0, cpu_core_mask(0));
 }
 
+enum {
+	SMP_OK,
+	SMP_NO_CONFIG,
+	SMP_NO_APIC,
+	SMP_FORCE_UP,
+};
+
 /*
  * Various sanity checks.
  */
@@ -1050,10 +1059,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	if (!smp_found_config && !acpi_lapic) {
 		preempt_enable();
 		pr_notice("SMP motherboard not detected\n");
-		disable_smp();
-		if (APIC_init_uniprocessor())
-			pr_notice("Local APIC not detected. Using dummy APIC emulation.\n");
-		return -1;
+		return SMP_NO_CONFIG;
 	}
 
 	/*
@@ -1077,7 +1083,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 				boot_cpu_physical_apicid);
 			pr_err("... forcing use of dummy APIC emulation (tell your hw vendor)\n");
 		}
-		return -1;
+		return SMP_NO_APIC;
 	}
 
 	verify_local_APIC();
@@ -1087,12 +1093,10 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	 */
 	if (!max_cpus) {
 		pr_info("SMP mode deactivated\n");
-		disable_ioapic_support();
-		apic_bsp_setup();
-		return -1;
+		return SMP_FORCE_UP;
 	}
 
-	return 0;
+	return SMP_OK;
 }
 
 static void __init smp_cpu_index_default(void)
@@ -1132,10 +1136,21 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	}
 	set_cpu_sibling_map(0);
 
-	if (smp_sanity_check(max_cpus) < 0) {
-		pr_info("SMP disabled\n");
+	switch (smp_sanity_check(max_cpus)) {
+	case SMP_NO_CONFIG:
 		disable_smp();
+		if (APIC_init_uniprocessor())
+			pr_notice("Local APIC not detected. Using dummy APIC emulation.\n");
+		return;
+	case SMP_NO_APIC:
+		disable_smp();
+		return;
+	case SMP_FORCE_UP:
+		disable_smp();
+		apic_bsp_setup();
 		return;
+	case SMP_OK:
+		break;
 	}
 
 	default_setup_apic_routing();

commit 05f7e46d2aac359b6bcfc06b302bdd03ca0bcada
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 15 21:22:40 2015 +0000

    x86/smpboot: Move apic init code to apic.c
    
    We better provide proper functions which implement the required code
    flow in the apic code rather than letting the smpboot code open code
    it. That allows to make more functions static and confines the APIC
    functionality to apic.c where it belongs.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Link: http://lkml.kernel.org/r/20150115211703.907616730@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ca7f7b696f07..d53870928824 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -172,8 +172,7 @@ static void smp_callin(void)
 	 * CPU, first the APIC. (this is probably redundant on most
 	 * boards)
 	 */
-	setup_local_APIC();
-	end_local_APIC_setup();
+	apic_ap_setup();
 
 	/*
 	 * Need to setup vector mappings before we enable interrupts.
@@ -1078,7 +1077,6 @@ static int __init smp_sanity_check(unsigned max_cpus)
 				boot_cpu_physical_apicid);
 			pr_err("... forcing use of dummy APIC emulation (tell your hw vendor)\n");
 		}
-		disable_ioapic_support();
 		return -1;
 	}
 
@@ -1090,10 +1088,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	if (!max_cpus) {
 		pr_info("SMP mode deactivated\n");
 		disable_ioapic_support();
-
-		connect_bsp_APIC();
-		setup_local_APIC();
-		bsp_end_local_APIC_setup();
+		apic_bsp_setup();
 		return -1;
 	}
 
@@ -1151,23 +1146,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		/* Or can we switch back to PIC here? */
 	}
 
-	connect_bsp_APIC();
-
-	/*
-	 * Switch from PIC to APIC mode.
-	 */
-	setup_local_APIC();
-
-	if (x2apic_mode)
-		cpu0_logical_apicid = apic_read(APIC_LDR);
-	else
-		cpu0_logical_apicid = GET_APIC_LOGICAL_ID(apic_read(APIC_LDR));
-
-	/* Enable IO APIC before setting up error vector */
-	enable_IO_APIC();
-
-	bsp_end_local_APIC_setup();
-	setup_IO_APIC();
+	cpu0_logical_apicid = apic_bsp_setup();
 
 	/*
 	 * Set up local APIC timer on boot CPU.

commit ef4c59a4b64c62f977187cae444aee25bebb02fe
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 15 21:22:35 2015 +0000

    x86/smpboot: Cleanup ioapic handling
    
    smpboot is very creative with the ways to disable ioapic.
    
    smpboot_clear_io_apic() smpboot_clear_io_apic_irqs() and
    disable_ioapic_support() serve a similar purpose.
    
    smpboot_clear_io_apic_irqs() is the most useless of all
    functions as it clears a variable which has not been setup yet.
    
    Aside of that it has the same ifdef mess and conditionals around the
    ioapic related code, which can now be removed.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Link: http://lkml.kernel.org/r/20150115211703.650280684@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 110ed1145e27..ca7f7b696f07 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -103,13 +103,6 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 
 atomic_t init_deasserted;
 
-static inline void smpboot_clear_io_apic_irqs(void)
-{
-#ifdef CONFIG_X86_IO_APIC
-	io_apic_irqs = 0;
-#endif
-}
-
 static inline void smpboot_setup_warm_reset_vector(unsigned long start_eip)
 {
 	unsigned long flags;
@@ -147,27 +140,6 @@ static inline void smpboot_restore_warm_reset_vector(void)
 	*((volatile u32 *)phys_to_virt(TRAMPOLINE_PHYS_LOW)) = 0;
 }
 
-static inline void __init smpboot_setup_io_apic(void)
-{
-#ifdef CONFIG_X86_IO_APIC
-	/*
-	 * Here we can be sure that there is an IO-APIC in the system. Let's
-	 * go and set it up:
-	 */
-	if (!skip_ioapic_setup && nr_ioapics)
-		setup_IO_APIC();
-	else
-		nr_ioapics = 0;
-#endif
-}
-
-static inline void smpboot_clear_io_apic(void)
-{
-#ifdef CONFIG_X86_IO_APIC
-	nr_ioapics = 0;
-#endif
-}
-
 /*
  * Report back to the Boot Processor during boot time or to the caller processor
  * during CPU online.
@@ -1019,9 +991,10 @@ void arch_disable_smp_support(void)
  */
 static __init void disable_smp(void)
 {
+	disable_ioapic_support();
+
 	init_cpu_present(cpumask_of(0));
 	init_cpu_possible(cpumask_of(0));
-	smpboot_clear_io_apic_irqs();
 
 	if (smp_found_config)
 		physid_set_mask_of_physid(boot_cpu_physical_apicid, &phys_cpu_present_map);
@@ -1105,7 +1078,6 @@ static int __init smp_sanity_check(unsigned max_cpus)
 				boot_cpu_physical_apicid);
 			pr_err("... forcing use of dummy APIC emulation (tell your hw vendor)\n");
 		}
-		smpboot_clear_io_apic();
 		disable_ioapic_support();
 		return -1;
 	}
@@ -1117,7 +1089,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	 */
 	if (!max_cpus) {
 		pr_info("SMP mode deactivated\n");
-		smpboot_clear_io_apic();
+		disable_ioapic_support();
 
 		connect_bsp_APIC();
 		setup_local_APIC();
@@ -1191,18 +1163,15 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	else
 		cpu0_logical_apicid = GET_APIC_LOGICAL_ID(apic_read(APIC_LDR));
 
-	/*
-	 * Enable IO APIC before setting up error vector
-	 */
-	if (!skip_ioapic_setup && nr_ioapics)
-		enable_IO_APIC();
+	/* Enable IO APIC before setting up error vector */
+	enable_IO_APIC();
 
 	bsp_end_local_APIC_setup();
-	smpboot_setup_io_apic();
+	setup_IO_APIC();
+
 	/*
 	 * Set up local APIC timer on boot CPU.
 	 */
-
 	pr_info("CPU%d: ", 0);
 	print_cpu_info(&cpu_data(0));
 	x86_init.timers.setup_percpu_clockev();
@@ -1241,9 +1210,7 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 
 	nmi_selftest();
 	impress_friends();
-#ifdef CONFIG_X86_IO_APIC
 	setup_ioapic_dest();
-#endif
 	mtrr_aps_init();
 }
 

commit f77aa308e5a6144a47311ad6905a1a72bc0014f9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 15 21:22:29 2015 +0000

    x86/smpboot: Move smpboot inlines to code
    
    No point for a separate header file.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20150115211703.304126687@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6d7022c683e3..110ed1145e27 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -73,7 +73,6 @@
 #include <asm/setup.h>
 #include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>
-#include <asm/smpboot_hooks.h>
 #include <asm/i8259.h>
 #include <asm/realmode.h>
 #include <asm/misc.h>
@@ -104,6 +103,71 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 
 atomic_t init_deasserted;
 
+static inline void smpboot_clear_io_apic_irqs(void)
+{
+#ifdef CONFIG_X86_IO_APIC
+	io_apic_irqs = 0;
+#endif
+}
+
+static inline void smpboot_setup_warm_reset_vector(unsigned long start_eip)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&rtc_lock, flags);
+	CMOS_WRITE(0xa, 0xf);
+	spin_unlock_irqrestore(&rtc_lock, flags);
+	local_flush_tlb();
+	pr_debug("1.\n");
+	*((volatile unsigned short *)phys_to_virt(TRAMPOLINE_PHYS_HIGH)) =
+							start_eip >> 4;
+	pr_debug("2.\n");
+	*((volatile unsigned short *)phys_to_virt(TRAMPOLINE_PHYS_LOW)) =
+							start_eip & 0xf;
+	pr_debug("3.\n");
+}
+
+static inline void smpboot_restore_warm_reset_vector(void)
+{
+	unsigned long flags;
+
+	/*
+	 * Install writable page 0 entry to set BIOS data area.
+	 */
+	local_flush_tlb();
+
+	/*
+	 * Paranoid:  Set warm reset code and vector here back
+	 * to default values.
+	 */
+	spin_lock_irqsave(&rtc_lock, flags);
+	CMOS_WRITE(0, 0xf);
+	spin_unlock_irqrestore(&rtc_lock, flags);
+
+	*((volatile u32 *)phys_to_virt(TRAMPOLINE_PHYS_LOW)) = 0;
+}
+
+static inline void __init smpboot_setup_io_apic(void)
+{
+#ifdef CONFIG_X86_IO_APIC
+	/*
+	 * Here we can be sure that there is an IO-APIC in the system. Let's
+	 * go and set it up:
+	 */
+	if (!skip_ioapic_setup && nr_ioapics)
+		setup_IO_APIC();
+	else
+		nr_ioapics = 0;
+#endif
+}
+
+static inline void smpboot_clear_io_apic(void)
+{
+#ifdef CONFIG_X86_IO_APIC
+	nr_ioapics = 0;
+#endif
+}
+
 /*
  * Report back to the Boot Processor during boot time or to the caller processor
  * during CPU online.

commit 250a1ac685f147d4f4b2f132cfaffcce1a6792c1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Dec 5 08:48:29 2014 +0000

    x86, smpboot: Remove pointless preempt_disable() in native_smp_prepare_cpus()
    
    There is no reason to keep preemption disabled in this function.
    
    We only have two other threads live: kthreadd and idle. Neither of
    them is going to preempt. But that preempt_disable forces all the code
    inside to do GFP_ATOMIC allocations which is just insane.
    
    Remove it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Borislav Petkov <bp@alien8.de>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20141205084147.153643952@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7a8f5845e8eb..6d7022c683e3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1084,7 +1084,6 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 {
 	unsigned int i;
 
-	preempt_disable();
 	smp_cpu_index_default();
 
 	/*
@@ -1102,22 +1101,19 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	}
 	set_cpu_sibling_map(0);
 
-
 	if (smp_sanity_check(max_cpus) < 0) {
 		pr_info("SMP disabled\n");
 		disable_smp();
-		goto out;
+		return;
 	}
 
 	default_setup_apic_routing();
 
-	preempt_disable();
 	if (read_apic_id() != boot_cpu_physical_apicid) {
 		panic("Boot APIC ID in local APIC unexpected (%d vs %d)",
 		     read_apic_id(), boot_cpu_physical_apicid);
 		/* Or can we switch back to PIC here? */
 	}
-	preempt_enable();
 
 	connect_bsp_APIC();
 
@@ -1151,8 +1147,6 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		uv_system_init();
 
 	set_mtrr_aps_delayed_init();
-out:
-	preempt_enable();
 }
 
 void arch_enable_nonboot_cpus_begin(void)

commit b6444bd0a18eb47343e16749ce80a6ebd521f124
Merge: 9d0cf6f56454 97b67ae55994
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 10 12:10:24 2014 -0800

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot and percpu updates from Ingo Molnar:
     "This tree contains a bootable images documentation update plus three
      slightly misplaced x86/asm percpu changes/optimizations"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86-64: Use RIP-relative addressing for most per-CPU accesses
      x86-64: Handle PC-relative relocations on per-CPU data
      x86: Convert a few more per-CPU items to read-mostly ones
      x86, boot: Document intermediates more clearly

commit 54279552bd260532d90e7a59fbc931924bbb0f7b
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Fri Oct 31 11:49:32 2014 -0400

    x86/core, x86/xen/smp: Use 'die_complete' completion when taking CPU down
    
    Commit 2ed53c0d6cc9 ("x86/smpboot: Speed up suspend/resume by
    avoiding 100ms sleep for CPU offline during S3") introduced
    completions to CPU offlining process. These completions are not
    initialized on Xen kernels causing a panic in
    play_dead_common().
    
    Move handling of die_complete into common routines to make them
    available to Xen guests.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Cc: tianyu.lan@intel.com
    Cc: konrad.wilk@oracle.com
    Cc: xen-devel@lists.xenproject.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1414770572-7950-1-git-send-email-boris.ostrovsky@oracle.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 4d2128ac70bd..668d8f2a8781 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1303,10 +1303,14 @@ static void __ref remove_cpu_from_maps(int cpu)
 	numa_remove_cpu(cpu);
 }
 
+static DEFINE_PER_CPU(struct completion, die_complete);
+
 void cpu_disable_common(void)
 {
 	int cpu = smp_processor_id();
 
+	init_completion(&per_cpu(die_complete, smp_processor_id()));
+
 	remove_siblinginfo(cpu);
 
 	/* It's now safe to remove this processor from the online map */
@@ -1316,8 +1320,6 @@ void cpu_disable_common(void)
 	fixup_irqs();
 }
 
-static DEFINE_PER_CPU(struct completion, die_complete);
-
 int native_cpu_disable(void)
 {
 	int ret;
@@ -1327,16 +1329,21 @@ int native_cpu_disable(void)
 		return ret;
 
 	clear_local_APIC();
-	init_completion(&per_cpu(die_complete, smp_processor_id()));
 	cpu_disable_common();
 
 	return 0;
 }
 
+void cpu_die_common(unsigned int cpu)
+{
+	wait_for_completion_timeout(&per_cpu(die_complete, cpu), HZ);
+}
+
 void native_cpu_die(unsigned int cpu)
 {
 	/* We don't do anything here: idle task is faking death itself. */
-	wait_for_completion_timeout(&per_cpu(die_complete, cpu), HZ);
+
+	cpu_die_common(cpu);
 
 	/* They ack this in play_dead() by setting CPU_DEAD */
 	if (per_cpu(cpu_state, cpu) == CPU_DEAD) {

commit 2c773dd31fbacbbb6425f8a9d3f97e0010272368
Author: Jan Beulich <JBeulich@suse.com>
Date:   Tue Nov 4 08:26:42 2014 +0000

    x86: Convert a few more per-CPU items to read-mostly ones
    
    Both this_cpu_off and cpu_info aren't getting modified post boot, yet
    are being accessed on enough code paths that grouping them with other
    frequently read items seems desirable. For cpu_info this at the same
    time implies removing the cache line alignment (which afaict became
    pointless when it got converted to per-CPU data years ago).
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Link: http://lkml.kernel.org/r/54589BD20200007800044A84@mail.emea.novell.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 4d2128ac70bd..a03ec604ceed 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -99,7 +99,7 @@ EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_llc_shared_map);
 
 /* Per CPU bogomips and other parameters */
-DEFINE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
+DEFINE_PER_CPU_READ_MOSTLY(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
 
 atomic_t init_deasserted;

commit db6a00b4bed3abbb038077ba4fdc5be481fe5559
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Oct 19 11:41:52 2014 +0200

    x86/smpboot: Move data structure to its primary usage scope
    
    Makes the code more readable by moving variable and usage closer
    to each other, which also avoids this build warning in the
    !CONFIG_HOTPLUG_CPU case:
    
      arch/x86/kernel/smpboot.c:105:42: warning: ‘die_complete’ defined but not used [-Wunused-variable]
    
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Lan Tianyu <tianyu.lan@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: srostedt@redhat.com
    Cc: toshi.kani@hp.com
    Cc: imammedo@redhat.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1409039025-32310-1-git-send-email-tianyu.lan@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2d5200e56357..4d2128ac70bd 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -102,8 +102,6 @@ DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_llc_shared_map);
 DEFINE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
 
-static DEFINE_PER_CPU(struct completion, die_complete);
-
 atomic_t init_deasserted;
 
 /*
@@ -1318,6 +1316,8 @@ void cpu_disable_common(void)
 	fixup_irqs();
 }
 
+static DEFINE_PER_CPU(struct completion, die_complete);
+
 int native_cpu_disable(void)
 {
 	int ret;

commit f1d0d14120a8a6224a8aead925cf4310f48947d5
Merge: bf10fa857f06 2ed53c0d6cc9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 13 18:20:39 2014 +0200

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu offlining patch from Ingo Molnar:
     "This tree includes a single commit that speeds up x86 suspend/resume
      by replacing a naive 100msec sleep based polling loop with proper
      completion notification.
    
      This gives some real suspend/resume benefit on servers with larger
      core counts"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/smpboot: Speed up suspend/resume by avoiding 100ms sleep for CPU offline during S3

commit 19e00d593e9a273ecbfbe131676ed11c140670ac
Merge: 197fe6b0e684 eeeda4cd06e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 13 18:16:32 2014 +0200

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 bootup updates from Ingo Molnar:
     "The changes in this cycle were:
    
       - Fix rare SMP-boot hang (mostly in virtual environments)
    
       - Fix build warning with certain (rare) toolchains"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/relocs: Make per_cpu_load_addr static
      x86/smpboot: Initialize secondary CPU only if master CPU will wait for it

commit faafcba3b5e15999cf75d5c5a513ac8e47e2545f
Merge: 13ead805c5a1 f10e00f4bf36
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 13 16:23:15 2014 +0200

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Optimized support for Intel "Cluster-on-Die" (CoD) topologies (Dave
         Hansen)
    
       - Various sched/idle refinements for better idle handling (Nicolas
         Pitre, Daniel Lezcano, Chuansheng Liu, Vincent Guittot)
    
       - sched/numa updates and optimizations (Rik van Riel)
    
       - sysbench speedup (Vincent Guittot)
    
       - capacity calculation cleanups/refactoring (Vincent Guittot)
    
       - Various cleanups to thread group iteration (Oleg Nesterov)
    
       - Double-rq-lock removal optimization and various refactorings
         (Kirill Tkhai)
    
       - various sched/deadline fixes
    
      ... and lots of other changes"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (72 commits)
      sched/dl: Use dl_bw_of() under rcu_read_lock_sched()
      sched/fair: Delete resched_cpu() from idle_balance()
      sched, time: Fix build error with 64 bit cputime_t on 32 bit systems
      sched: Improve sysbench performance by fixing spurious active migration
      sched/x86: Fix up typo in topology detection
      x86, sched: Add new topology for multi-NUMA-node CPUs
      sched/rt: Use resched_curr() in task_tick_rt()
      sched: Use rq->rd in sched_setaffinity() under RCU read lock
      sched: cleanup: Rename 'out_unlock' to 'out_free_new_mask'
      sched: Use dl_bw_of() under RCU read lock
      sched/fair: Remove duplicate code from can_migrate_task()
      sched, mips, ia64: Remove __ARCH_WANT_UNLOCKED_CTXSW
      sched: print_rq(): Don't use tasklist_lock
      sched: normalize_rt_tasks(): Don't use _irqsave for tasklist_lock, use task_rq_lock()
      sched: Fix the task-group check in tg_has_rt_tasks()
      sched/fair: Leverage the idle state info when choosing the "idlest" cpu
      sched: Let the scheduler see CPU idle states
      sched/deadline: Fix inter- exclusive cpusets migrations
      sched/deadline: Clear dl_entity params when setscheduling to different class
      sched/numa: Kill the wrong/dead TASK_DEAD check in task_numa_fault()
      ...

commit 728e5653e6fdb2a0892e94a600aef8c9a036c7eb
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Tue Sep 30 14:45:46 2014 -0700

    sched/x86: Fix up typo in topology detection
    
    Commit:
    
      cebf15eb09a2 ("x86, sched: Add new topology for multi-NUMA-node CPUs")
    
    some code to try to detect the situation where we have a NUMA node
    inside of the "DIE" sched domain.
    
    It detected this by looking for cpus which match_die() but do not match
    NUMA nodes via topology_same_node().
    
    I wrote it up as:
    
            if (match_die(c, o) == !topology_same_node(c, o))
    
    which actually seemed to work some of the time, albiet
    accidentally.
    
    It should have been doing an &&, not an ==.
    
    This code essentially chopped off the "DIE" domain on one of
    Andrew Morton's systems.  He reported that this patch fixed his
    issue.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jan Kiszka <jan.kiszka@siemens.com>
    Cc: Lan Tianyu <tianyu.lan@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Link: http://lkml.kernel.org/r/20140930214546.FD481CFF@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8de8eb756d1f..bae9e09291ec 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -445,7 +445,7 @@ void set_cpu_sibling_map(int cpu)
 			} else if (i != cpu && !c->booted_cores)
 				c->booted_cores = cpu_data(i).booted_cores;
 		}
-		if (match_die(c, o) == !topology_same_node(c, o))
+		if (match_die(c, o) && !topology_same_node(c, o))
 			primarily_use_numa_for_topology();
 	}
 }

commit 03bd4e1f7265548832a76e7919a81f3137c44fd1
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Wed Sep 24 16:38:05 2014 +0800

    sched: Fix unreleased llc_shared_mask bit during CPU hotplug
    
    The following bug can be triggered by hot adding and removing a large number of
    xen domain0's vcpus repeatedly:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000004 IP: [..] find_busiest_group
            PGD 5a9d5067 PUD 13067 PMD 0
            Oops: 0000 [#3] SMP
            [...]
            Call Trace:
            load_balance
            ? _raw_spin_unlock_irqrestore
            idle_balance
            __schedule
            schedule
            schedule_timeout
            ? lock_timer_base
            schedule_timeout_uninterruptible
            msleep
            lock_device_hotplug_sysfs
            online_store
            dev_attr_store
            sysfs_write_file
            vfs_write
            SyS_write
            system_call_fastpath
    
    Last level cache shared mask is built during CPU up and the
    build_sched_domain() routine takes advantage of it to setup
    the sched domain CPU topology.
    
    However, llc_shared_mask is not released during CPU disable,
    which leads to an invalid sched domainCPU topology.
    
    This patch fix it by releasing the llc_shared_mask correctly
    during CPU disable.
    
    Yasuaki also reported that this can happen on real hardware:
    
      https://lkml.org/lkml/2014/7/22/1018
    
    His case is here:
    
            ==
            Here is an example on my system.
            My system has 4 sockets and each socket has 15 cores and HT is
            enabled. In this case, each core of sockes is numbered as
            follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-44, 90-104
            Socket#3 | 45-59, 105-119
    
            Then llc_shared_mask of CPU#30 has 0x3fff80000001fffc0000000.
    
            It means that last level cache of Socket#2 is shared with
            CPU#30-44 and 90-104.
    
            When hot-removing socket#2 and #3, each core of sockets is
            numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
    
            But llc_shared_mask is not cleared. So llc_shared_mask of CPU#30
            remains having 0x3fff80000001fffc0000000.
    
            After that, when hot-adding socket#2 and #3, each core of
            sockets is numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-59
            Socket#3 | 90-119
    
            Then llc_shared_mask of CPU#30 becomes
            0x3fff8000fffffffc0000000. It means that last level cache of
            Socket#2 is shared with CPU#30-59 and 90-104. So the mask has
            the wrong value.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Tested-by: Linn Crosetto <linn@hp.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: <stable@vger.kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1411547885-48165-1-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2d872e08fab9..42a2dca984b3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1284,6 +1284,9 @@ static void remove_siblinginfo(int cpu)
 
 	for_each_cpu(sibling, cpu_sibling_mask(cpu))
 		cpumask_clear_cpu(cpu, cpu_sibling_mask(sibling));
+	for_each_cpu(sibling, cpu_llc_shared_mask(cpu))
+		cpumask_clear_cpu(cpu, cpu_llc_shared_mask(sibling));
+	cpumask_clear(cpu_llc_shared_mask(cpu));
 	cpumask_clear(cpu_sibling_mask(cpu));
 	cpumask_clear(cpu_core_mask(cpu));
 	c->phys_proc_id = 0;

commit 2ed53c0d6cc99fc712f7c037e41d9ec4eb8d6b08
Author: Lan Tianyu <tianyu.lan@intel.com>
Date:   Tue Aug 26 15:43:45 2014 +0800

    x86/smpboot: Speed up suspend/resume by avoiding 100ms sleep for CPU offline during S3
    
    With certain kernel configurations, CPU offline consumes more than
    100ms during S3.
    
    It's a timing related issue: native_cpu_die() would occasionally fall
    into a 100ms sleep when the CPU idle loop thread marked the CPU state
    to DEAD too slowly.
    
    What native_cpu_die() does is that it polls the CPU state and waits
    for 100ms if CPU state hasn't been marked to DEAD. The 100ms sleep
    doesn't make sense and is purely historic.
    
    To avoid such long sleeping, this patch adds a 'struct completion'
    to each CPU, waits for the completion in native_cpu_die() and wakes
    up the completion when the CPU state is marked to DEAD.
    
    Tested on an Intel Xeon server with 48 cores, Ivybridge and on
    Haswell laptops. The CPU offlining cost on these machines is
    reduced from more than 100ms to less than 5ms. The system
    suspend time is reduced by 2.3s on the servers.
    
    Borislav and Prarit also helped to test the patch on an AMD
    machine and a few systems of various sizes and configurations
    (multi-socket, single-socket, no hyper threading, etc.). No
    issues were seen.
    
    Tested-by: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: Lan Tianyu <tianyu.lan@intel.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: srostedt@redhat.com
    Cc: toshi.kani@hp.com
    Cc: imammedo@redhat.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1409039025-32310-1-git-send-email-tianyu.lan@intel.com
    [ Improved a few minor details in the code, cleaned up the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2d872e08fab9..fdbc5fce8b97 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -102,6 +102,8 @@ DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_llc_shared_map);
 DEFINE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
 
+static DEFINE_PER_CPU(struct completion, die_complete);
+
 atomic_t init_deasserted;
 
 /*
@@ -1323,26 +1325,24 @@ int native_cpu_disable(void)
 		return ret;
 
 	clear_local_APIC();
-
+	init_completion(&per_cpu(die_complete, smp_processor_id()));
 	cpu_disable_common();
+
 	return 0;
 }
 
 void native_cpu_die(unsigned int cpu)
 {
 	/* We don't do anything here: idle task is faking death itself. */
-	unsigned int i;
+	wait_for_completion_timeout(&per_cpu(die_complete, cpu), HZ);
 
-	for (i = 0; i < 10; i++) {
-		/* They ack this in play_dead by setting CPU_DEAD */
-		if (per_cpu(cpu_state, cpu) == CPU_DEAD) {
-			if (system_state == SYSTEM_RUNNING)
-				pr_info("CPU %u is now offline\n", cpu);
-			return;
-		}
-		msleep(100);
+	/* They ack this in play_dead() by setting CPU_DEAD */
+	if (per_cpu(cpu_state, cpu) == CPU_DEAD) {
+		if (system_state == SYSTEM_RUNNING)
+			pr_info("CPU %u is now offline\n", cpu);
+	} else {
+		pr_err("CPU %u didn't die...\n", cpu);
 	}
-	pr_err("CPU %u didn't die...\n", cpu);
 }
 
 void play_dead_common(void)
@@ -1354,6 +1354,7 @@ void play_dead_common(void)
 	mb();
 	/* Ack it */
 	__this_cpu_write(cpu_state, CPU_DEAD);
+	complete(&per_cpu(die_complete, smp_processor_id()));
 
 	/*
 	 * With physical CPU hotplug, we should halt the cpu

commit cebf15eb09a2fd2fa73ee4faa9c4d2f813cf0f09
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Sep 18 12:33:34 2014 -0700

    x86, sched: Add new topology for multi-NUMA-node CPUs
    
    I'm getting the spew below when booting with Haswell (Xeon
    E5-2699 v3) CPUs and the "Cluster-on-Die" (CoD) feature enabled
    in the BIOS.  It seems similar to the issue that some folks from
    AMD ran in to on their systems and addressed in this commit:
    
      161270fc1f9d ("x86/smp: Fix topology checks on AMD MCM CPUs")
    
    Both these Intel and AMD systems break an assumption which is
    being enforced by topology_sane(): a socket may not contain more
    than one NUMA node.
    
    AMD special-cased their system by looking for a cpuid flag.  The
    Intel mode is dependent on BIOS options and I do not know of a
    way which it is enumerated other than the tables being parsed
    during the CPU bringup process.  In other words, we have to trust
    the ACPI tables <shudder>.
    
    This detects the situation where a NUMA node occurs at a place in
    the middle of the "CPU" sched domains.  It replaces the default
    topology with one that relies on the NUMA information from the
    firmware (SRAT table) for all levels of sched domains above the
    hyperthreads.
    
    This also fixes a sysfs bug.  We used to freak out when we saw
    the "mc" group cross a node boundary, so we stopped building the
    MC group.  MC gets exported as the 'core_siblings_list' in
    /sys/devices/system/cpu/cpu*/topology/ and this caused CPUs with
    the same 'physical_package_id' to not be listed together in
    'core_siblings_list'.  This violates a statement from
    Documentation/ABI/testing/sysfs-devices-system-cpu:
    
            core_siblings: internal kernel map of cpu#'s hardware threads
            within the same physical_package_id.
    
            core_siblings_list: human-readable list of the logical CPU
            numbers within the same physical_package_id as cpu#.
    
    The sysfs effects here cause an issue with the hwloc tool where
    it gets confused and thinks there are more sockets than are
    physically present.
    
    Before this patch, there are two packages:
    
    # cd /sys/devices/system/cpu/
    # cat cpu*/topology/physical_package_id | sort | uniq -c
         18 0
         18 1
    
    But 4 _sets_ of core siblings:
    
    # cat cpu*/topology/core_siblings_list | sort | uniq -c
          9 0-8
          9 18-26
          9 27-35
          9 9-17
    
    After this set, there are only 2 sets of core siblings, which
    is what we expect for a 2-socket system.
    
    # cat cpu*/topology/physical_package_id | sort | uniq -c
         18 0
         18 1
    # cat cpu*/topology/core_siblings_list | sort | uniq -c
         18 0-17
         18 18-35
    
    Example spew:
    ...
            NMI watchdog: enabled on all CPUs, permanently consumes one hw-PMU counter.
             #2  #3  #4  #5  #6  #7  #8
            .... node  #1, CPUs:    #9
            ------------[ cut here ]------------
            WARNING: CPU: 9 PID: 0 at /home/ak/hle/linux-hle-2.6/arch/x86/kernel/smpboot.c:306 topology_sane.isra.2+0x74/0x90()
            sched: CPU #9's mc-sibling CPU #0 is not on the same node! [node: 1 != 0]. Ignoring dependency.
            Modules linked in:
            CPU: 9 PID: 0 Comm: swapper/9 Not tainted 3.17.0-rc1-00293-g8e01c4d-dirty #631
            Hardware name: Intel Corporation S2600WTT/S2600WTT, BIOS GRNDSDP1.86B.0036.R05.1407140519 07/14/2014
            0000000000000009 ffff88046ddabe00 ffffffff8172e485 ffff88046ddabe48
            ffff88046ddabe38 ffffffff8109691d 000000000000b001 0000000000000009
            ffff88086fc12580 000000000000b020 0000000000000009 ffff88046ddabe98
            Call Trace:
            [<ffffffff8172e485>] dump_stack+0x45/0x56
            [<ffffffff8109691d>] warn_slowpath_common+0x7d/0xa0
            [<ffffffff8109698c>] warn_slowpath_fmt+0x4c/0x50
            [<ffffffff81074f94>] topology_sane.isra.2+0x74/0x90
            [<ffffffff8107530e>] set_cpu_sibling_map+0x31e/0x4f0
            [<ffffffff8107568d>] start_secondary+0x1ad/0x240
            ---[ end trace 3fe5f587a9fcde61 ]---
            #10 #11 #12 #13 #14 #15 #16 #17
            .... node  #2, CPUs:   #18 #19 #20 #21 #22 #23 #24 #25 #26
            .... node  #3, CPUs:   #27 #28 #29 #30 #31 #32 #33 #34 #35
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    [ Added LLC domain and s/match_mc/match_die/ ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: brice.goglin@gmail.com
    Cc: "H. Peter Anvin" <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/r/20140918193334.C065EBCE@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2d872e08fab9..8de8eb756d1f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -295,12 +295,20 @@ void smp_store_cpu_info(int id)
 	identify_secondary_cpu(c);
 }
 
+static bool
+topology_same_node(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
+{
+	int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
+
+	return (cpu_to_node(cpu1) == cpu_to_node(cpu2));
+}
+
 static bool
 topology_sane(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o, const char *name)
 {
 	int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
 
-	return !WARN_ONCE(cpu_to_node(cpu1) != cpu_to_node(cpu2),
+	return !WARN_ONCE(!topology_same_node(c, o),
 		"sched: CPU #%d's %s-sibling CPU #%d is not on the same node! "
 		"[node: %d != %d]. Ignoring dependency.\n",
 		cpu1, name, cpu2, cpu_to_node(cpu1), cpu_to_node(cpu2));
@@ -341,17 +349,44 @@ static bool match_llc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 	return false;
 }
 
-static bool match_mc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
+/*
+ * Unlike the other levels, we do not enforce keeping a
+ * multicore group inside a NUMA node.  If this happens, we will
+ * discard the MC level of the topology later.
+ */
+static bool match_die(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 {
-	if (c->phys_proc_id == o->phys_proc_id) {
-		if (cpu_has(c, X86_FEATURE_AMD_DCM))
-			return true;
-
-		return topology_sane(c, o, "mc");
-	}
+	if (c->phys_proc_id == o->phys_proc_id)
+		return true;
 	return false;
 }
 
+static struct sched_domain_topology_level numa_inside_package_topology[] = {
+#ifdef CONFIG_SCHED_SMT
+	{ cpu_smt_mask, cpu_smt_flags, SD_INIT_NAME(SMT) },
+#endif
+#ifdef CONFIG_SCHED_MC
+	{ cpu_coregroup_mask, cpu_core_flags, SD_INIT_NAME(MC) },
+#endif
+	{ NULL, },
+};
+/*
+ * set_sched_topology() sets the topology internal to a CPU.  The
+ * NUMA topologies are layered on top of it to build the full
+ * system topology.
+ *
+ * If NUMA nodes are observed to occur within a CPU package, this
+ * function should be called.  It forces the sched domain code to
+ * only use the SMT level for the CPU portion of the topology.
+ * This essentially falls back to relying on NUMA information
+ * from the SRAT table to describe the entire system topology
+ * (except for hyperthreads).
+ */
+static void primarily_use_numa_for_topology(void)
+{
+	set_sched_topology(numa_inside_package_topology);
+}
+
 void set_cpu_sibling_map(int cpu)
 {
 	bool has_smt = smp_num_siblings > 1;
@@ -388,7 +423,7 @@ void set_cpu_sibling_map(int cpu)
 	for_each_cpu(i, cpu_sibling_setup_mask) {
 		o = &cpu_data(i);
 
-		if ((i == cpu) || (has_mp && match_mc(c, o))) {
+		if ((i == cpu) || (has_mp && match_die(c, o))) {
 			link_mask(core, cpu, i);
 
 			/*
@@ -410,6 +445,8 @@ void set_cpu_sibling_map(int cpu)
 			} else if (i != cpu && !c->booted_cores)
 				c->booted_cores = cpu_data(i).booted_cores;
 		}
+		if (match_die(c, o) == !topology_same_node(c, o))
+			primarily_use_numa_for_topology();
 	}
 }
 

commit ce4b1b16502b182368cda20a61de2995762c8bcc
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Fri Jun 20 14:23:11 2014 +0200

    x86/smpboot: Initialize secondary CPU only if master CPU will wait for it
    
    Hang is observed on virtual machines during CPU hotplug,
    especially in big guests with many CPUs. (It reproducible
    more often if host is over-committed).
    
    It happens because master CPU gives up waiting on
    secondary CPU and allows it to run wild. As result
    AP causes locking or crashing system. For example
    as described here:
    
      https://lkml.org/lkml/2014/3/6/257
    
    If master CPU have sent STARTUP IPI successfully,
    and AP signalled to master CPU that it's ready
    to start initialization, make master CPU wait
    indefinitely till AP is onlined.
    
    To ensure that AP won't ever run wild, make it
    wait at early startup till master CPU confirms its
    intention to wait for AP. If AP doesn't respond in 10
    seconds, the master CPU will timeout and cancel
    AP onlining.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1403266991-12233-1-git-send-email-imammedo@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2d872e08fab9..735c420eba2d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -111,7 +111,6 @@ atomic_t init_deasserted;
 static void smp_callin(void)
 {
 	int cpuid, phys_id;
-	unsigned long timeout;
 
 	/*
 	 * If waken up by an INIT in an 82489DX configuration
@@ -130,37 +129,6 @@ static void smp_callin(void)
 	 * (This works even if the APIC is not enabled.)
 	 */
 	phys_id = read_apic_id();
-	if (cpumask_test_cpu(cpuid, cpu_callin_mask)) {
-		panic("%s: phys CPU#%d, CPU#%d already present??\n", __func__,
-					phys_id, cpuid);
-	}
-	pr_debug("CPU#%d (phys ID: %d) waiting for CALLOUT\n", cpuid, phys_id);
-
-	/*
-	 * STARTUP IPIs are fragile beasts as they might sometimes
-	 * trigger some glue motherboard logic. Complete APIC bus
-	 * silence for 1 second, this overestimates the time the
-	 * boot CPU is spending to send the up to 2 STARTUP IPIs
-	 * by a factor of two. This should be enough.
-	 */
-
-	/*
-	 * Waiting 2s total for startup (udelay is not yet working)
-	 */
-	timeout = jiffies + 2*HZ;
-	while (time_before(jiffies, timeout)) {
-		/*
-		 * Has the boot CPU finished it's STARTUP sequence?
-		 */
-		if (cpumask_test_cpu(cpuid, cpu_callout_mask))
-			break;
-		cpu_relax();
-	}
-
-	if (!time_before(jiffies, timeout)) {
-		panic("%s: CPU%d started up but did not get a callout!\n",
-		      __func__, cpuid);
-	}
 
 	/*
 	 * the boot CPU has finished the init stage and is spinning
@@ -753,8 +721,8 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	unsigned long start_ip = real_mode_header->trampoline_start;
 
 	unsigned long boot_error = 0;
-	int timeout;
 	int cpu0_nmi_registered = 0;
+	unsigned long timeout;
 
 	/* Just in case we booted with a single CPU. */
 	alternatives_enable_smp();
@@ -801,6 +769,15 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 		}
 	}
 
+	/*
+	 * AP might wait on cpu_callout_mask in cpu_init() with
+	 * cpu_initialized_mask set if previous attempt to online
+	 * it timed-out. Clear cpu_initialized_mask so that after
+	 * INIT/SIPI it could start with a clean state.
+	 */
+	cpumask_clear_cpu(cpu, cpu_initialized_mask);
+	smp_mb();
+
 	/*
 	 * Wake up a CPU in difference cases:
 	 * - Use the method in the APIC driver if it's defined
@@ -815,53 +792,38 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 
 	if (!boot_error) {
 		/*
-		 * allow APs to start initializing.
+		 * Wait 10s total for a response from AP
 		 */
-		pr_debug("Before Callout %d\n", cpu);
-		cpumask_set_cpu(cpu, cpu_callout_mask);
-		pr_debug("After Callout %d\n", cpu);
+		boot_error = -1;
+		timeout = jiffies + 10*HZ;
+		while (time_before(jiffies, timeout)) {
+			if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+				/*
+				 * Tell AP to proceed with initialization
+				 */
+				cpumask_set_cpu(cpu, cpu_callout_mask);
+				boot_error = 0;
+				break;
+			}
+			udelay(100);
+			schedule();
+		}
+	}
 
+	if (!boot_error) {
 		/*
-		 * Wait 5s total for a response
+		 * Wait till AP completes initial initialization
 		 */
-		for (timeout = 0; timeout < 50000; timeout++) {
-			if (cpumask_test_cpu(cpu, cpu_callin_mask))
-				break;	/* It has booted */
-			udelay(100);
+		while (!cpumask_test_cpu(cpu, cpu_callin_mask)) {
 			/*
 			 * Allow other tasks to run while we wait for the
 			 * AP to come online. This also gives a chance
 			 * for the MTRR work(triggered by the AP coming online)
 			 * to be completed in the stop machine context.
 			 */
+			udelay(100);
 			schedule();
 		}
-
-		if (cpumask_test_cpu(cpu, cpu_callin_mask)) {
-			print_cpu_msr(&cpu_data(cpu));
-			pr_debug("CPU%d: has booted.\n", cpu);
-		} else {
-			boot_error = 1;
-			if (*trampoline_status == 0xA5A5A5A5)
-				/* trampoline started but...? */
-				pr_err("CPU%d: Stuck ??\n", cpu);
-			else
-				/* trampoline code not run */
-				pr_err("CPU%d: Not responding\n", cpu);
-			if (apic->inquire_remote_apic)
-				apic->inquire_remote_apic(apicid);
-		}
-	}
-
-	if (boot_error) {
-		/* Try to put things back the way they were before ... */
-		numa_remove_cpu(cpu); /* was set by numa_add_cpu */
-
-		/* was set by do_boot_cpu() */
-		cpumask_clear_cpu(cpu, cpu_callout_mask);
-
-		/* was set by cpu_init() */
-		cpumask_clear_cpu(cpu, cpu_initialized_mask);
 	}
 
 	/* mark "stuck" area as not stuck */

commit 11a8318ef5a69cdb9be61f726d6e078d70af6129
Author: David Rientjes <rientjes@google.com>
Date:   Wed Jul 30 23:53:47 2014 -0700

    x86, apic: Remove setup_portio_remap callback
    
    Since commit b5660ba76b41 ("x86, platforms: Remove NUMAQ") removed NUMAQ,
    the setup_portio_remap() apic callback has been obsolete.  Remove it.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.02.1407302351480.17503@chino.kir.corp.google.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f0f3b194e9f9..2d872e08fab9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1139,10 +1139,6 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		enable_IO_APIC();
 
 	bsp_end_local_APIC_setup();
-
-	if (apic->setup_portio_remap)
-		apic->setup_portio_remap();
-
 	smpboot_setup_io_apic();
 	/*
 	 * Set up local APIC timer on boot CPU.

commit 300eddf967920d35affa75db77c50c0fa493446a
Author: David Rientjes <rientjes@google.com>
Date:   Wed Jul 30 23:53:30 2014 -0700

    x86, apic: Remove smp_callin_clear_local_apic callback
    
    Since commit b5660ba76b41 ("x86, platforms: Remove NUMAQ") removed NUMAQ,
    the smp_callin_clear_local_apic() apic callback has been obsolete.
    Remove it.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.02.1407302349040.17503@chino.kir.corp.google.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 5492798930ef..f0f3b194e9f9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -168,10 +168,6 @@ static void smp_callin(void)
 	 * CPU, first the APIC. (this is probably redundant on most
 	 * boards)
 	 */
-
-	pr_debug("CALLIN, before setup_local_APIC()\n");
-	if (apic->smp_callin_clear_local_apic)
-		apic->smp_callin_clear_local_apic();
 	setup_local_APIC();
 	end_local_APIC_setup();
 

commit 3f17ea6dea8ba5668873afa54628a91aaa3fb1c0
Merge: 1860e379875d 1a5700bc2d10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jun 8 11:31:16 2014 -0700

    Merge branch 'next' (accumulated 3.16 merge window patches) into master
    
    Now that 3.15 is released, this merges the 'next' branch into 'master',
    bringing us to the normal situation where my 'master' branch is the
    merge window.
    
    * accumulated work in next: (6809 commits)
      ufs: sb mutex merge + mutex_destroy
      powerpc: update comments for generic idle conversion
      cris: update comments for generic idle conversion
      idle: remove cpu_idle() forward declarations
      nbd: zero from and len fields in NBD_CMD_DISCONNECT.
      mm: convert some level-less printks to pr_*
      MAINTAINERS: adi-buildroot-devel is moderated
      MAINTAINERS: add linux-api for review of API/ABI changes
      mm/kmemleak-test.c: use pr_fmt for logging
      fs/dlm/debug_fs.c: replace seq_printf by seq_puts
      fs/dlm/lockspace.c: convert simple_str to kstr
      fs/dlm/config.c: convert simple_str to kstr
      mm: mark remap_file_pages() syscall as deprecated
      mm: memcontrol: remove unnecessary memcg argument from soft limit functions
      mm: memcontrol: clean up memcg zoneinfo lookup
      mm/memblock.c: call kmemleak directly from memblock_(alloc|free)
      mm/mempool.c: update the kmemleak stack trace for mempool allocations
      lib/radix-tree.c: update the kmemleak stack trace for radix tree allocations
      mm: introduce kmemleak_update_trace()
      mm/kmemleak.c: use %u to print ->checksum
      ...

commit bb077d600689dbf9305758efed1e16775db1c84c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jun 8 10:09:49 2014 -0700

    Revert "x86/smpboot: Initialize secondary CPU only if master CPU will wait for it"
    
    This reverts commit 3e1a878b7ccdb31da6d9d2b855c72ad87afeba3f.
    
    It came in very late, and already has one reported failure: Sitsofe
    reports that the current tree fails to boot on his EeePC, and bisected
    it down to this.  Rather than waste time trying to figure out what's
    wrong, just revert it.
    
    Reported-by: Sitsofe Wheeler <sitsofe@gmail.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index bc52fac39dd3..ae2fd975b782 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -111,6 +111,7 @@ atomic_t init_deasserted;
 static void smp_callin(void)
 {
 	int cpuid, phys_id;
+	unsigned long timeout;
 
 	/*
 	 * If waken up by an INIT in an 82489DX configuration
@@ -129,6 +130,37 @@ static void smp_callin(void)
 	 * (This works even if the APIC is not enabled.)
 	 */
 	phys_id = read_apic_id();
+	if (cpumask_test_cpu(cpuid, cpu_callin_mask)) {
+		panic("%s: phys CPU#%d, CPU#%d already present??\n", __func__,
+					phys_id, cpuid);
+	}
+	pr_debug("CPU#%d (phys ID: %d) waiting for CALLOUT\n", cpuid, phys_id);
+
+	/*
+	 * STARTUP IPIs are fragile beasts as they might sometimes
+	 * trigger some glue motherboard logic. Complete APIC bus
+	 * silence for 1 second, this overestimates the time the
+	 * boot CPU is spending to send the up to 2 STARTUP IPIs
+	 * by a factor of two. This should be enough.
+	 */
+
+	/*
+	 * Waiting 2s total for startup (udelay is not yet working)
+	 */
+	timeout = jiffies + 2*HZ;
+	while (time_before(jiffies, timeout)) {
+		/*
+		 * Has the boot CPU finished it's STARTUP sequence?
+		 */
+		if (cpumask_test_cpu(cpuid, cpu_callout_mask))
+			break;
+		cpu_relax();
+	}
+
+	if (!time_before(jiffies, timeout)) {
+		panic("%s: CPU%d started up but did not get a callout!\n",
+		      __func__, cpuid);
+	}
 
 	/*
 	 * the boot CPU has finished the init stage and is spinning
@@ -718,8 +750,8 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	unsigned long start_ip = real_mode_header->trampoline_start;
 
 	unsigned long boot_error = 0;
+	int timeout;
 	int cpu0_nmi_registered = 0;
-	unsigned long timeout;
 
 	/* Just in case we booted with a single CPU. */
 	alternatives_enable_smp();
@@ -766,15 +798,6 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 		}
 	}
 
-	/*
-	 * AP might wait on cpu_callout_mask in cpu_init() with
-	 * cpu_initialized_mask set if previous attempt to online
-	 * it timed-out. Clear cpu_initialized_mask so that after
-	 * INIT/SIPI it could start with a clean state.
-	 */
-	cpumask_clear_cpu(cpu, cpu_initialized_mask);
-	smp_mb();
-
 	/*
 	 * Wake up a CPU in difference cases:
 	 * - Use the method in the APIC driver if it's defined
@@ -787,41 +810,55 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 		boot_error = wakeup_cpu_via_init_nmi(cpu, start_ip, apicid,
 						     &cpu0_nmi_registered);
 
-
 	if (!boot_error) {
 		/*
-		 * Wait 10s total for a response from AP
+		 * allow APs to start initializing.
 		 */
-		boot_error = -1;
-		timeout = jiffies + 10*HZ;
-		while (time_before(jiffies, timeout)) {
-			if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
-				/*
-				 * Tell AP to proceed with initialization
-				 */
-				cpumask_set_cpu(cpu, cpu_callout_mask);
-				boot_error = 0;
-				break;
-			}
-			udelay(100);
-			schedule();
-		}
-	}
+		pr_debug("Before Callout %d\n", cpu);
+		cpumask_set_cpu(cpu, cpu_callout_mask);
+		pr_debug("After Callout %d\n", cpu);
 
-	if (!boot_error) {
 		/*
-		 * Wait till AP completes initial initialization
+		 * Wait 5s total for a response
 		 */
-		while (!cpumask_test_cpu(cpu, cpu_callin_mask)) {
+		for (timeout = 0; timeout < 50000; timeout++) {
+			if (cpumask_test_cpu(cpu, cpu_callin_mask))
+				break;	/* It has booted */
+			udelay(100);
 			/*
 			 * Allow other tasks to run while we wait for the
 			 * AP to come online. This also gives a chance
 			 * for the MTRR work(triggered by the AP coming online)
 			 * to be completed in the stop machine context.
 			 */
-			udelay(100);
 			schedule();
 		}
+
+		if (cpumask_test_cpu(cpu, cpu_callin_mask)) {
+			print_cpu_msr(&cpu_data(cpu));
+			pr_debug("CPU%d: has booted.\n", cpu);
+		} else {
+			boot_error = 1;
+			if (*trampoline_status == 0xA5A5A5A5)
+				/* trampoline started but...? */
+				pr_err("CPU%d: Stuck ??\n", cpu);
+			else
+				/* trampoline code not run */
+				pr_err("CPU%d: Not responding\n", cpu);
+			if (apic->inquire_remote_apic)
+				apic->inquire_remote_apic(apicid);
+		}
+	}
+
+	if (boot_error) {
+		/* Try to put things back the way they were before ... */
+		numa_remove_cpu(cpu); /* was set by numa_add_cpu */
+
+		/* was set by do_boot_cpu() */
+		cpumask_clear_cpu(cpu, cpu_callout_mask);
+
+		/* was set by cpu_init() */
+		cpumask_clear_cpu(cpu, cpu_initialized_mask);
 	}
 
 	/* mark "stuck" area as not stuck */

commit 3e1a878b7ccdb31da6d9d2b855c72ad87afeba3f
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Thu Jun 5 15:42:45 2014 +0200

    x86/smpboot: Initialize secondary CPU only if master CPU will wait for it
    
    Hang is observed on virtual machines during CPU hotplug,
    especially in big guests with many CPUs. (It reproducible
    more often if host is over-committed).
    
    It happens because master CPU gives up waiting on
    secondary CPU and allows it to run wild. As result
    AP causes locking or crashing system. For example
    as described here:
    
       https://lkml.org/lkml/2014/3/6/257
    
    If master CPU have sent STARTUP IPI successfully,
    and AP signalled to master CPU that it's ready
    to start initialization, make master CPU wait
    indefinitely till AP is onlined.
    To ensure that AP won't ever run wild, make it
    wait at early startup till master CPU confirms its
    intention to wait for AP. If AP doesn't respond in 10
    seconds, the master CPU will timeout and cancel
    AP onlining.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1401975765-22328-4-git-send-email-imammedo@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ae2fd975b782..bc52fac39dd3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -111,7 +111,6 @@ atomic_t init_deasserted;
 static void smp_callin(void)
 {
 	int cpuid, phys_id;
-	unsigned long timeout;
 
 	/*
 	 * If waken up by an INIT in an 82489DX configuration
@@ -130,37 +129,6 @@ static void smp_callin(void)
 	 * (This works even if the APIC is not enabled.)
 	 */
 	phys_id = read_apic_id();
-	if (cpumask_test_cpu(cpuid, cpu_callin_mask)) {
-		panic("%s: phys CPU#%d, CPU#%d already present??\n", __func__,
-					phys_id, cpuid);
-	}
-	pr_debug("CPU#%d (phys ID: %d) waiting for CALLOUT\n", cpuid, phys_id);
-
-	/*
-	 * STARTUP IPIs are fragile beasts as they might sometimes
-	 * trigger some glue motherboard logic. Complete APIC bus
-	 * silence for 1 second, this overestimates the time the
-	 * boot CPU is spending to send the up to 2 STARTUP IPIs
-	 * by a factor of two. This should be enough.
-	 */
-
-	/*
-	 * Waiting 2s total for startup (udelay is not yet working)
-	 */
-	timeout = jiffies + 2*HZ;
-	while (time_before(jiffies, timeout)) {
-		/*
-		 * Has the boot CPU finished it's STARTUP sequence?
-		 */
-		if (cpumask_test_cpu(cpuid, cpu_callout_mask))
-			break;
-		cpu_relax();
-	}
-
-	if (!time_before(jiffies, timeout)) {
-		panic("%s: CPU%d started up but did not get a callout!\n",
-		      __func__, cpuid);
-	}
 
 	/*
 	 * the boot CPU has finished the init stage and is spinning
@@ -750,8 +718,8 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	unsigned long start_ip = real_mode_header->trampoline_start;
 
 	unsigned long boot_error = 0;
-	int timeout;
 	int cpu0_nmi_registered = 0;
+	unsigned long timeout;
 
 	/* Just in case we booted with a single CPU. */
 	alternatives_enable_smp();
@@ -798,6 +766,15 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 		}
 	}
 
+	/*
+	 * AP might wait on cpu_callout_mask in cpu_init() with
+	 * cpu_initialized_mask set if previous attempt to online
+	 * it timed-out. Clear cpu_initialized_mask so that after
+	 * INIT/SIPI it could start with a clean state.
+	 */
+	cpumask_clear_cpu(cpu, cpu_initialized_mask);
+	smp_mb();
+
 	/*
 	 * Wake up a CPU in difference cases:
 	 * - Use the method in the APIC driver if it's defined
@@ -810,55 +787,41 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 		boot_error = wakeup_cpu_via_init_nmi(cpu, start_ip, apicid,
 						     &cpu0_nmi_registered);
 
+
 	if (!boot_error) {
 		/*
-		 * allow APs to start initializing.
+		 * Wait 10s total for a response from AP
 		 */
-		pr_debug("Before Callout %d\n", cpu);
-		cpumask_set_cpu(cpu, cpu_callout_mask);
-		pr_debug("After Callout %d\n", cpu);
+		boot_error = -1;
+		timeout = jiffies + 10*HZ;
+		while (time_before(jiffies, timeout)) {
+			if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+				/*
+				 * Tell AP to proceed with initialization
+				 */
+				cpumask_set_cpu(cpu, cpu_callout_mask);
+				boot_error = 0;
+				break;
+			}
+			udelay(100);
+			schedule();
+		}
+	}
 
+	if (!boot_error) {
 		/*
-		 * Wait 5s total for a response
+		 * Wait till AP completes initial initialization
 		 */
-		for (timeout = 0; timeout < 50000; timeout++) {
-			if (cpumask_test_cpu(cpu, cpu_callin_mask))
-				break;	/* It has booted */
-			udelay(100);
+		while (!cpumask_test_cpu(cpu, cpu_callin_mask)) {
 			/*
 			 * Allow other tasks to run while we wait for the
 			 * AP to come online. This also gives a chance
 			 * for the MTRR work(triggered by the AP coming online)
 			 * to be completed in the stop machine context.
 			 */
+			udelay(100);
 			schedule();
 		}
-
-		if (cpumask_test_cpu(cpu, cpu_callin_mask)) {
-			print_cpu_msr(&cpu_data(cpu));
-			pr_debug("CPU%d: has booted.\n", cpu);
-		} else {
-			boot_error = 1;
-			if (*trampoline_status == 0xA5A5A5A5)
-				/* trampoline started but...? */
-				pr_err("CPU%d: Stuck ??\n", cpu);
-			else
-				/* trampoline code not run */
-				pr_err("CPU%d: Not responding\n", cpu);
-			if (apic->inquire_remote_apic)
-				apic->inquire_remote_apic(apicid);
-		}
-	}
-
-	if (boot_error) {
-		/* Try to put things back the way they were before ... */
-		numa_remove_cpu(cpu); /* was set by numa_add_cpu */
-
-		/* was set by do_boot_cpu() */
-		cpumask_clear_cpu(cpu, cpu_callout_mask);
-
-		/* was set by cpu_init() */
-		cpumask_clear_cpu(cpu, cpu_initialized_mask);
 	}
 
 	/* mark "stuck" area as not stuck */

commit feef1e8ecbadf24f8e6829c935df8683cabae41b
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Thu Jun 5 15:42:44 2014 +0200

    x86/smpboot: Log error on secondary CPU wakeup failure at ERR level
    
    If system is running without debug level logging,
    it will not log error if do_boot_cpu() failed to
    wakeup AP. It may lead to silent AP bringup
    failures at boot time.
    Change message level to KERN_ERR to make error
    visible to user as it's done on other architectures.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1401975765-22328-3-git-send-email-imammedo@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2988f69f673e..ae2fd975b782 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -918,7 +918,7 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 
 	err = do_boot_cpu(apicid, cpu, tidle);
 	if (err) {
-		pr_debug("do_boot_cpu failed %d\n", err);
+		pr_err("do_boot_cpu failed(%d) to wakeup CPU#%u\n", err, cpu);
 		return -EIO;
 	}
 

commit 89f898c1e195fa6235c869bb457e500b7b3ac49d
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Thu Jun 5 15:42:43 2014 +0200

    x86: Fix list/memory corruption on CPU hotplug
    
    currently if AP wake up is failed, master CPU marks AP as not
    present in do_boot_cpu() by calling set_cpu_present(cpu, false).
    That leads to following list corruption on the next physical CPU
    hotplug:
    
    [  418.107336] WARNING: CPU: 1 PID: 45 at lib/list_debug.c:33 __list_add+0xbe/0xd0()
    [  418.115268] list_add corruption. prev->next should be next (ffff88003dc57600), but was ffff88003e20c3a0. (prev=ffff88003e20c3a0).
    [  418.123693] Modules linked in: nf_conntrack_netbios_ns nf_conntrack_broadcast ipt_MASQUERADE ip6t_REJECT ipt_REJECT cfg80211 xt_conntrack rfkill ee
    [  418.138979] CPU: 1 PID: 45 Comm: kworker/u10:1 Not tainted 3.14.0-rc6+ #387
    [  418.149989] Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2007
    [  418.165750] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
    [  418.166433]  0000000000000021 ffff880038ca7988 ffffffff8159b22d 0000000000000021
    [  418.176460]  ffff880038ca79d8 ffff880038ca79c8 ffffffff8106942c ffff880038ca79e8
    [  418.177453]  ffff88003e20c3a0 ffff88003dc57600 ffff88003e20c3a0 00000000ffffffea
    [  418.178445] Call Trace:
    [  418.185811]  [<ffffffff8159b22d>] dump_stack+0x49/0x5c
    [  418.186440]  [<ffffffff8106942c>] warn_slowpath_common+0x8c/0xc0
    [  418.187192]  [<ffffffff81069516>] warn_slowpath_fmt+0x46/0x50
    [  418.191231]  [<ffffffff8136ef51>] ? acpi_ns_get_node+0xb7/0xc7
    [  418.193889]  [<ffffffff812f796e>] __list_add+0xbe/0xd0
    [  418.196649]  [<ffffffff812e2aa9>] kobject_add_internal+0x79/0x200
    [  418.208610]  [<ffffffff812e2e18>] kobject_add_varg+0x38/0x60
    [  418.213831]  [<ffffffff812e2ef4>] kobject_add+0x44/0x70
    [  418.229961]  [<ffffffff813e2c60>] device_add+0xd0/0x550
    [  418.234991]  [<ffffffff813f0e95>] ? pm_runtime_init+0xe5/0xf0
    [  418.250226]  [<ffffffff813e32be>] device_register+0x1e/0x30
    [  418.255296]  [<ffffffff813e82a3>] register_cpu+0xe3/0x130
    [  418.266539]  [<ffffffff81592be5>] arch_register_cpu+0x65/0x150
    [  418.285845]  [<ffffffff81355c0d>] acpi_processor_hotadd_init+0x5a/0x9b
    ...
    Which is caused by the fact that generic_processor_info() allocates
    logical CPU id by calling:
    
     cpu = cpumask_next_zero(-1, cpu_present_mask);
    
    which returns id of previously failed to wake up CPU, since its
    bit is cleared by do_boot_cpu() and as result register_cpu()
    tries to register another CPU with the same id as already
    present but failed to be onlined CPU.
    
    Taking in account that AP will not do anything if master CPU
    failed to wake it up, there is no reason to mark that AP as not
    present and break next cpu hotplug attempts. As a side effect of
    not marking AP as not present, user would be allowed to online
    it again later.
    
    Also fix memory corruption in acpi_unmap_lsapic()
    
    if during CPU hotplug master CPU failed to wake up AP
    it set percpu x86_cpu_to_apicid to BAD_APICID=0xFFFF for AP.
    
    However following attempt to unplug that CPU will lead to
    out of bound write access to __apicid_to_node[] which is
    32768 items long on x86_64 kernel.
    
    So with above fix of cpu_present_mask make sure that a present
    CPU has a valid APIC ID by not setting x86_cpu_to_apicid
    to BAD_APICID in do_boot_cpu() on failure and allow
    acpi_processor_remove()->acpi_unmap_lsapic() cleanly remove CPU.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1401975765-22328-2-git-send-email-imammedo@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 34826934d4a7..2988f69f673e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -859,9 +859,6 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 
 		/* was set by cpu_init() */
 		cpumask_clear_cpu(cpu, cpu_initialized_mask);
-
-		set_cpu_present(cpu, false);
-		per_cpu(x86_cpu_to_apicid, cpu) = BAD_APICID;
 	}
 
 	/* mark "stuck" area as not stuck */

commit 197725de65477bc8509b41388157c1a2283542bb
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Sun May 4 10:00:49 2014 -0700

    x86, espfix: Make espfix64 a Kconfig option, fix UML
    
    Make espfix64 a hidden Kconfig option.  This fixes the x86-64 UML
    build which had broken due to the non-existence of init_espfix_bsp()
    in UML: since UML uses its own Kconfig, this option does not appear in
    the UML build.
    
    This also makes it possible to make support for 16-bit segments a
    configuration option, for the people who want to minimize the size of
    the kernel.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Richard Weinberger <richard@nod.at>
    Link: http://lkml.kernel.org/r/1398816946-3351-1-git-send-email-hpa@linux.intel.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 61a5350850fb..5d93ac1b72db 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -246,7 +246,7 @@ static void notrace start_secondary(void *unused)
 	/*
 	 * Enable the espfix hack for this CPU
 	 */
-#ifdef CONFIG_X86_64
+#ifdef CONFIG_X86_ESPFIX64
 	init_espfix_ap();
 #endif
 

commit 3891a04aafd668686239349ea58f3314ea2af86b
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Tue Apr 29 16:46:09 2014 -0700

    x86-64, espfix: Don't leak bits 31:16 of %esp returning to 16-bit stack
    
    The IRET instruction, when returning to a 16-bit segment, only
    restores the bottom 16 bits of the user space stack pointer.  This
    causes some 16-bit software to break, but it also leaks kernel state
    to user space.  We have a software workaround for that ("espfix") for
    the 32-bit kernel, but it relies on a nonzero stack segment base which
    is not available in 64-bit mode.
    
    In checkin:
    
        b3b42ac2cbae x86-64, modify_ldt: Ban 16-bit segments on 64-bit kernels
    
    we "solved" this by forbidding 16-bit segments on 64-bit kernels, with
    the logic that 16-bit support is crippled on 64-bit kernels anyway (no
    V86 support), but it turns out that people are doing stuff like
    running old Win16 binaries under Wine and expect it to work.
    
    This works around this by creating percpu "ministacks", each of which
    is mapped 2^16 times 64K apart.  When we detect that the return SS is
    on the LDT, we copy the IRET frame to the ministack and use the
    relevant alias to return to userspace.  The ministacks are mapped
    readonly, so if IRET faults we promote #GP to #DF which is an IST
    vector and thus has its own stack; we then do the fixup in the #DF
    handler.
    
    (Making #GP an IST exception would make the msr_safe functions unsafe
    in NMI/MC context, and quite possibly have other effects.)
    
    Special thanks to:
    
    - Andy Lutomirski, for the suggestion of using very small stack slots
      and copy (as opposed to map) the IRET frame there, and for the
      suggestion to mark them readonly and let the fault promote to #DF.
    - Konrad Wilk for paravirt fixup and testing.
    - Borislav Petkov for testing help and useful comments.
    
    Reported-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1398816946-3351-1-git-send-email-hpa@linux.intel.com
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Andrew Lutomriski <amluto@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Dirk Hohndel <dirk@hohndel.org>
    Cc: Arjan van de Ven <arjan.van.de.ven@intel.com>
    Cc: comex <comexk@gmail.com>
    Cc: Alexander van Heukelum <heukelum@fastmail.fm>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: <stable@vger.kernel.org> # consider after upstream merge

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 34826934d4a7..61a5350850fb 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -243,6 +243,13 @@ static void notrace start_secondary(void *unused)
 	 */
 	check_tsc_sync_target();
 
+	/*
+	 * Enable the espfix hack for this CPU
+	 */
+#ifdef CONFIG_X86_64
+	init_espfix_ap();
+#endif
+
 	/*
 	 * We need to hold vector_lock so there the set of online cpus
 	 * does not change while we are assigning vectors to cpus.  Holding

commit 99f7b025bfadd7fac5216dcfb2a08312804674c0
Merge: a21e40877ad1 6cce16f99d7b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 1 10:17:18 2014 -0700

    Merge branch 'x86-threadinfo-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 threadinfo changes from Ingo Molnar:
     "The main change here is the consolidation/unification of 32 and 64 bit
      thread_info handling methods, from Steve Rostedt"
    
    * 'x86-threadinfo-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, threadinfo: Redo "x86: Use inline assembler to get sp"
      x86: Clean up dumpstack_64.c code
      x86: Keep thread_info on thread stack in x86_32
      x86: Prepare removal of previous_esp from i386 thread_info structure
      x86: Nuke GET_THREAD_INFO_WITH_ESP() macro for i386
      x86: Nuke the supervisor_stack field in i386 thread_info

commit b9b16a792241e304834f43e2a5f02e6e43576f09
Merge: 4b2ce8f15f8f da4aaa7d860c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 1 10:11:21 2014 -0700

    Merge branch 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpufeature update from Ingo Molnar:
     "Two refinements to clflushopt support"
    
    * 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, cpufeature: If we disable CLFLUSH, we should disable CLFLUSHOPT
      x86, cpufeature: Rename X86_FEATURE_CLFLSH to X86_FEATURE_CLFLUSH

commit ea7bdc65bca8cf837a63e0ff7b75daed83222511
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Jan 27 20:14:06 2014 +0100

    x86/apic: Plug racy xAPIC access of CPU hotplug code
    
    apic_icr_write() and its users in smpboot.c were apparently
    written under the assumption that this code would only run
    during early boot. But nowadays we also execute it when onlining
    a CPU later on while the system is fully running. That will make
    wakeup_cpu_via_init_nmi and, thus, also native_apic_icr_write
    run in plain process context. If we migrate the caller to a
    different CPU at the wrong time or interrupt it and write to
    ICR/ICR2 to send unrelated IPIs, we can end up sending INIT,
    SIPI or NMIs to wrong CPUs.
    
    Fix this by disabling interrupts during the write to the ICR
    halves and disable preemption around waiting for ICR
    availability and using it.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Tested-By: Igor Mammedov <imammedo@redhat.com>
    Link: http://lkml.kernel.org/r/52E6AFFE.3030004@siemens.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c77acc69ecf6..60179ec39d4c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -702,11 +702,15 @@ wakeup_cpu_via_init_nmi(int cpu, unsigned long start_ip, int apicid,
 	int id;
 	int boot_error;
 
+	preempt_disable();
+
 	/*
 	 * Wake up AP by INIT, INIT, STARTUP sequence.
 	 */
-	if (cpu)
-		return wakeup_secondary_cpu_via_init(apicid, start_ip);
+	if (cpu) {
+		boot_error = wakeup_secondary_cpu_via_init(apicid, start_ip);
+		goto out;
+	}
 
 	/*
 	 * Wake up BSP by nmi.
@@ -726,6 +730,9 @@ wakeup_cpu_via_init_nmi(int cpu, unsigned long start_ip, int apicid,
 		boot_error = wakeup_secondary_cpu_via_nmi(id, start_ip);
 	}
 
+out:
+	preempt_enable();
+
 	return boot_error;
 }
 

commit 198d208df4371734ac4728f69cb585c284d20a15
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 6 09:41:31 2014 -0500

    x86: Keep thread_info on thread stack in x86_32
    
    x86_64 uses a per_cpu variable kernel_stack to always point to
    the thread stack of current. This is where the thread_info is stored
    and is accessed from this location even when the irq or exception stack
    is in use. This removes the complexity of having to maintain the
    thread info on the stack when interrupts are running and having to
    copy the preempt_count and other fields to the interrupt stack.
    
    x86_32 uses the old method of copying the thread_info from the thread
    stack to the exception stack just before executing the exception.
    
    Having the two different requires #ifdefs and also the x86_32 way
    is a bit of a pain to maintain. By converting x86_32 to the same
    method of x86_64, we can remove #ifdefs, clean up the x86_32 code
    a little, and remove the overhead of the copy.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20110806012354.263834829@goodmis.org
    Link: http://lkml.kernel.org/r/20140206144321.852942014@goodmis.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a32da804252e..867d53ea88a3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -758,10 +758,10 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 #else
 	clear_tsk_thread_flag(idle, TIF_FORK);
 	initial_gs = per_cpu_offset(cpu);
+#endif
 	per_cpu(kernel_stack, cpu) =
 		(unsigned long)task_stack_page(idle) -
 		KERNEL_STACK_OFFSET + THREAD_SIZE;
-#endif
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	initial_code = (unsigned long)start_secondary;
 	stack_start  = idle->thread.sp;

commit 840d2830e6e56b8fdacc7ff12915dd91bf91566b
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Feb 27 08:31:30 2014 -0800

    x86, cpufeature: Rename X86_FEATURE_CLFLSH to X86_FEATURE_CLFLUSH
    
    We call this "clflush" in /proc/cpuinfo, and have
    cpu_has_clflush()... let's be consistent and just call it that.
    
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Link: http://lkml.kernel.org/n/tip-mlytfzjkvuf739okyn40p8a5@git.kernel.org

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a32da804252e..ffc78c3e9be1 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1379,7 +1379,7 @@ static inline void mwait_play_dead(void)
 
 	if (!this_cpu_has(X86_FEATURE_MWAIT))
 		return;
-	if (!this_cpu_has(X86_FEATURE_CLFLSH))
+	if (!this_cpu_has(X86_FEATURE_CLFLUSH))
 		return;
 	if (__this_cpu_read(cpu_info.cpuid_level) < CPUID_MWAIT_LEAF)
 		return;

commit 465822cfc8cb850ba76046965cc7b6fd1f8c3d73
Author: David Rientjes <rientjes@google.com>
Date:   Tue Feb 4 23:55:01 2014 -0800

    x86/apic: Switch wait_for_init_deassert() to a bool flag
    
    Now that there is only a single wait_for_init_deassert()
    function, just convert the member of struct apic to a bool to
    determine whether we need to wait for init_deassert to become
    non-zero.
    
    There are no more callers of default_wait_for_init_deassert(),
    so fold it into the caller.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.02.1402042354010.7839@chino.kir.corp.google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a32da804252e..c77acc69ecf6 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -122,8 +122,9 @@ static void smp_callin(void)
 	 * Since CPU0 is not wakened up by INIT, it doesn't wait for the IPI.
 	 */
 	cpuid = smp_processor_id();
-	if (apic->wait_for_init_deassert && cpuid != 0)
-		apic->wait_for_init_deassert(&init_deasserted);
+	if (apic->wait_for_init_deassert && cpuid)
+		while (!atomic_read(&init_deasserted))
+			cpu_relax();
 
 	/*
 	 * (This works even if the APIC is not enabled.)

commit 7fe67a1180db49d41a3f764c379a08f8e31580ec
Merge: fab5669d5562 da6139e49c7c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 12:11:41 2014 -0800

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull leftover x86 fixes from Ingo Molnar:
     "Two leftover fixes that did not make it into v3.13"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Add check for number of available vectors before CPU down
      x86, cpu, amd: Add workaround for family 16h, erratum 793

commit da6139e49c7cb0f4251265cb5243b8d220adb48d
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Mon Jan 13 06:51:01 2014 -0500

    x86: Add check for number of available vectors before CPU down
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=64791
    
    When a cpu is downed on a system, the irqs on the cpu are assigned to
    other cpus.  It is possible, however, that when a cpu is downed there
    aren't enough free vectors on the remaining cpus to account for the
    vectors from the cpu that is being downed.
    
    This results in an interesting "overflow" condition where irqs are
    "assigned" to a CPU but are not handled.
    
    For example, when downing cpus on a 1-64 logical processor system:
    
    <snip>
    [  232.021745] smpboot: CPU 61 is now offline
    [  238.480275] smpboot: CPU 62 is now offline
    [  245.991080] ------------[ cut here ]------------
    [  245.996270] WARNING: CPU: 0 PID: 0 at net/sched/sch_generic.c:264 dev_watchdog+0x246/0x250()
    [  246.005688] NETDEV WATCHDOG: p786p1 (ixgbe): transmit queue 0 timed out
    [  246.013070] Modules linked in: lockd sunrpc iTCO_wdt iTCO_vendor_support sb_edac ixgbe microcode e1000e pcspkr joydev edac_core lpc_ich ioatdma ptp mdio mfd_core i2c_i801 dca pps_core i2c_core wmi acpi_cpufreq isci libsas scsi_transport_sas
    [  246.037633] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 3.12.0+ #14
    [  246.044451] Hardware name: Intel Corporation S4600LH ........../SVRBD-ROW_T, BIOS SE5C600.86B.01.08.0003.022620131521 02/26/2013
    [  246.057371]  0000000000000009 ffff88081fa03d40 ffffffff8164fbf6 ffff88081fa0ee48
    [  246.065728]  ffff88081fa03d90 ffff88081fa03d80 ffffffff81054ecc ffff88081fa13040
    [  246.074073]  0000000000000000 ffff88200cce0000 0000000000000040 0000000000000000
    [  246.082430] Call Trace:
    [  246.085174]  <IRQ>  [<ffffffff8164fbf6>] dump_stack+0x46/0x58
    [  246.091633]  [<ffffffff81054ecc>] warn_slowpath_common+0x8c/0xc0
    [  246.098352]  [<ffffffff81054fb6>] warn_slowpath_fmt+0x46/0x50
    [  246.104786]  [<ffffffff815710d6>] dev_watchdog+0x246/0x250
    [  246.110923]  [<ffffffff81570e90>] ? dev_deactivate_queue.constprop.31+0x80/0x80
    [  246.119097]  [<ffffffff8106092a>] call_timer_fn+0x3a/0x110
    [  246.125224]  [<ffffffff8106280f>] ? update_process_times+0x6f/0x80
    [  246.132137]  [<ffffffff81570e90>] ? dev_deactivate_queue.constprop.31+0x80/0x80
    [  246.140308]  [<ffffffff81061db0>] run_timer_softirq+0x1f0/0x2a0
    [  246.146933]  [<ffffffff81059a80>] __do_softirq+0xe0/0x220
    [  246.152976]  [<ffffffff8165fedc>] call_softirq+0x1c/0x30
    [  246.158920]  [<ffffffff810045f5>] do_softirq+0x55/0x90
    [  246.164670]  [<ffffffff81059d35>] irq_exit+0xa5/0xb0
    [  246.170227]  [<ffffffff8166062a>] smp_apic_timer_interrupt+0x4a/0x60
    [  246.177324]  [<ffffffff8165f40a>] apic_timer_interrupt+0x6a/0x70
    [  246.184041]  <EOI>  [<ffffffff81505a1b>] ? cpuidle_enter_state+0x5b/0xe0
    [  246.191559]  [<ffffffff81505a17>] ? cpuidle_enter_state+0x57/0xe0
    [  246.198374]  [<ffffffff81505b5d>] cpuidle_idle_call+0xbd/0x200
    [  246.204900]  [<ffffffff8100b7ae>] arch_cpu_idle+0xe/0x30
    [  246.210846]  [<ffffffff810a47b0>] cpu_startup_entry+0xd0/0x250
    [  246.217371]  [<ffffffff81646b47>] rest_init+0x77/0x80
    [  246.223028]  [<ffffffff81d09e8e>] start_kernel+0x3ee/0x3fb
    [  246.229165]  [<ffffffff81d0989f>] ? repair_env_string+0x5e/0x5e
    [  246.235787]  [<ffffffff81d095a5>] x86_64_start_reservations+0x2a/0x2c
    [  246.242990]  [<ffffffff81d0969f>] x86_64_start_kernel+0xf8/0xfc
    [  246.249610] ---[ end trace fb74fdef54d79039 ]---
    [  246.254807] ixgbe 0000:c2:00.0 p786p1: initiating reset due to tx timeout
    [  246.262489] ixgbe 0000:c2:00.0 p786p1: Reset adapter
    Last login: Mon Nov 11 08:35:14 from 10.18.17.119
    [root@(none) ~]# [  246.792676] ixgbe 0000:c2:00.0 p786p1: detected SFP+: 5
    [  249.231598] ixgbe 0000:c2:00.0 p786p1: NIC Link is Up 10 Gbps, Flow Control: RX/TX
    [  246.792676] ixgbe 0000:c2:00.0 p786p1: detected SFP+: 5
    [  249.231598] ixgbe 0000:c2:00.0 p786p1: NIC Link is Up 10 Gbps, Flow Control: RX/TX
    
    (last lines keep repeating.  ixgbe driver is dead until module reload.)
    
    If the downed cpu has more vectors than are free on the remaining cpus on the
    system, it is possible that some vectors are "orphaned" even though they are
    assigned to a cpu.  In this case, since the ixgbe driver had a watchdog, the
    watchdog fired and notified that something was wrong.
    
    This patch adds a function, check_vectors(), to compare the number of vectors
    on the CPU going down and compares it to the number of vectors available on
    the system.  If there aren't enough vectors for the CPU to go down, an
    error is returned and propogated back to userspace.
    
    v2: Do not need to look at percpu irqs
    v3: Need to check affinity to prevent counting of MSIs in IOAPIC Lowest
        Priority Mode
    v4: Additional changes suggested by Gong Chen.
    v5/v6/v7/v8: Updated comment text
    
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1389613861-3853-1-git-send-email-prarit@redhat.com
    Reviewed-by: Gong Chen <gong.chen@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Cc: Yang Zhang <yang.z.zhang@Intel.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Janet Morgan <janet.morgan@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Ruiv Wang <ruiv.wang@gmail.com>
    Cc: Gong Chen <gong.chen@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: <stable@vger.kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 85dc05a3aa02..391ea529dc26 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1312,6 +1312,12 @@ void cpu_disable_common(void)
 
 int native_cpu_disable(void)
 {
+	int ret;
+
+	ret = check_irq_vectors_for_cpu_disable();
+	if (ret)
+		return ret;
+
 	clear_local_APIC();
 
 	cpu_disable_common();

commit 7d590cca7cd2cce4ed7c47d221d6f90566653ba8
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Dec 19 12:30:03 2013 -0800

    x86, idle: Add memory barriers around clflush in mwait_play_dead()
    
    For consistency with mwait_idle_with_hints().  Not sure they help, but
    they really won't hurt...
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Len Brown <len.brown@intel.com>
    Link: http://lkml.kernel.org/r/CA%2B55aFzGxcML7j8CEvQPYzh0W81uVoAAVmGctMOUZ7CZ1yYd2A@mail.gmail.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 85dc05a3aa02..f5252c4eec8c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1417,7 +1417,9 @@ static inline void mwait_play_dead(void)
 		 * The WBINVD is insufficient due to the spurious-wakeup
 		 * case where we return around the loop.
 		 */
+		mb();
 		clflush(mwait_ptr);
+		mb();
 		__monitor(mwait_ptr, 0, 0);
 		mb();
 		__mwait(eax, 0);

commit f9300eaaac1ca300083ad41937923a90cc3a2394
Merge: 7f2dc5c4bcbf faddf2f5d278
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 14 13:41:48 2013 +0900

    Merge tag 'pm+acpi-3.13-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull ACPI and power management updates from Rafael J Wysocki:
    
     - New power capping framework and the the Intel Running Average Power
       Limit (RAPL) driver using it from Srinivas Pandruvada and Jacob Pan.
    
     - Addition of the in-kernel switching feature to the arm_big_little
       cpufreq driver from Viresh Kumar and Nicolas Pitre.
    
     - cpufreq support for iMac G5 from Aaro Koskinen.
    
     - Baytrail processors support for intel_pstate from Dirk Brandewie.
    
     - cpufreq support for Midway/ECX-2000 from Mark Langsdorf.
    
     - ARM vexpress/TC2 cpufreq support from Sudeep KarkadaNagesha.
    
     - ACPI power management support for the I2C and SPI bus types from Mika
       Westerberg and Lv Zheng.
    
     - cpufreq core fixes and cleanups from Viresh Kumar, Srivatsa S Bhat,
       Stratos Karafotis, Xiaoguang Chen, Lan Tianyu.
    
     - cpufreq drivers updates (mostly fixes and cleanups) from Viresh
       Kumar, Aaro Koskinen, Jungseok Lee, Sudeep KarkadaNagesha, Lukasz
       Majewski, Manish Badarkhe, Hans-Christian Egtvedt, Evgeny Kapaev.
    
     - intel_pstate updates from Dirk Brandewie and Adrian Huang.
    
     - ACPICA update to version 20130927 includig fixes and cleanups and
       some reduction of divergences between the ACPICA code in the kernel
       and ACPICA upstream in order to improve the automatic ACPICA patch
       generation process.  From Bob Moore, Lv Zheng, Tomasz Nowicki, Naresh
       Bhat, Bjorn Helgaas, David E Box.
    
     - ACPI IPMI driver fixes and cleanups from Lv Zheng.
    
     - ACPI hotplug fixes and cleanups from Bjorn Helgaas, Toshi Kani, Zhang
       Yanfei, Rafael J Wysocki.
    
     - Conversion of the ACPI AC driver to the platform bus type and
       multiple driver fixes and cleanups related to ACPI from Zhang Rui.
    
     - ACPI processor driver fixes and cleanups from Hanjun Guo, Jiang Liu,
       Bartlomiej Zolnierkiewicz, Mathieu Rhéaume, Rafael J Wysocki.
    
     - Fixes and cleanups and new blacklist entries related to the ACPI
       video support from Aaron Lu, Felipe Contreras, Lennart Poettering,
       Kirill Tkhai.
    
     - cpuidle core cleanups from Viresh Kumar and Lorenzo Pieralisi.
    
     - cpuidle drivers fixes and cleanups from Daniel Lezcano, Jingoo Han,
       Bartlomiej Zolnierkiewicz, Prarit Bhargava.
    
     - devfreq updates from Sachin Kamat, Dan Carpenter, Manish Badarkhe.
    
     - Operation Performance Points (OPP) core updates from Nishanth Menon.
    
     - Runtime power management core fix from Rafael J Wysocki and update
       from Ulf Hansson.
    
     - Hibernation fixes from Aaron Lu and Rafael J Wysocki.
    
     - Device suspend/resume lockup detection mechanism from Benoit Goby.
    
     - Removal of unused proc directories created for various ACPI drivers
       from Lan Tianyu.
    
     - ACPI LPSS driver fix and new device IDs for the ACPI platform scan
       handler from Heikki Krogerus and Jarkko Nikula.
    
     - New ACPI _OSI blacklist entry for Toshiba NB100 from Levente Kurusa.
    
     - Assorted fixes and cleanups related to ACPI from Andy Shevchenko, Al
       Stone, Bartlomiej Zolnierkiewicz, Colin Ian King, Dan Carpenter,
       Felipe Contreras, Jianguo Wu, Lan Tianyu, Yinghai Lu, Mathias Krause,
       Liu Chuansheng.
    
     - Assorted PM fixes and cleanups from Andy Shevchenko, Thierry Reding,
       Jean-Christophe Plagniol-Villard.
    
    * tag 'pm+acpi-3.13-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (386 commits)
      cpufreq: conservative: fix requested_freq reduction issue
      ACPI / hotplug: Consolidate deferred execution of ACPI hotplug routines
      PM / runtime: Use pm_runtime_put_sync() in __device_release_driver()
      ACPI / event: remove unneeded NULL pointer check
      Revert "ACPI / video: Ignore BIOS initial backlight value for HP 250 G1"
      ACPI / video: Quirk initial backlight level 0
      ACPI / video: Fix initial level validity test
      intel_pstate: skip the driver if ACPI has power mgmt option
      PM / hibernate: Avoid overflow in hibernate_preallocate_memory()
      ACPI / hotplug: Do not execute "insert in progress" _OST
      ACPI / hotplug: Carry out PCI root eject directly
      ACPI / hotplug: Merge device hot-removal routines
      ACPI / hotplug: Make acpi_bus_hot_remove_device() internal
      ACPI / hotplug: Simplify device ejection routines
      ACPI / hotplug: Fix handle_root_bridge_removal()
      ACPI / hotplug: Refuse to hot-remove all objects with disabled hotplug
      ACPI / scan: Start matching drivers after trying scan handlers
      ACPI: Remove acpi_pci_slot_init() headers from internal.h
      ACPI / blacklist: fix name of ThinkPad Edge E530
      PowerCap: Fix build error with option -Werror=format-security
      ...
    
    Conflicts:
            arch/arm/mach-omap2/opp.c
            drivers/Kconfig
            drivers/spi/spi.c

commit a17bce4d1dce8f3cf714bc2e5d8e4bac009dc077
Author: Borislav Petkov <bp@alien8.de>
Date:   Mon Sep 30 11:56:24 2013 +0200

    x86/boot: Further compress CPUs bootup message
    
    Turn it into (for example):
    
    [    0.073380] x86: Booting SMP configuration:
    [    0.074005] .... node   #0, CPUs:          #1   #2   #3   #4   #5   #6   #7
    [    0.603005] .... node   #1, CPUs:     #8   #9  #10  #11  #12  #13  #14  #15
    [    1.200005] .... node   #2, CPUs:    #16  #17  #18  #19  #20  #21  #22  #23
    [    1.796005] .... node   #3, CPUs:    #24  #25  #26  #27  #28  #29  #30  #31
    [    2.393005] .... node   #4, CPUs:    #32  #33  #34  #35  #36  #37  #38  #39
    [    2.996005] .... node   #5, CPUs:    #40  #41  #42  #43  #44  #45  #46  #47
    [    3.600005] .... node   #6, CPUs:    #48  #49  #50  #51  #52  #53  #54  #55
    [    4.202005] .... node   #7, CPUs:    #56  #57  #58  #59  #60  #61  #62  #63
    [    4.811005] .... node   #8, CPUs:    #64  #65  #66  #67  #68  #69  #70  #71
    [    5.421006] .... node   #9, CPUs:    #72  #73  #74  #75  #76  #77  #78  #79
    [    6.032005] .... node  #10, CPUs:    #80  #81  #82  #83  #84  #85  #86  #87
    [    6.648006] .... node  #11, CPUs:    #88  #89  #90  #91  #92  #93  #94  #95
    [    7.262005] .... node  #12, CPUs:    #96  #97  #98  #99 #100 #101 #102 #103
    [    7.865005] .... node  #13, CPUs:   #104 #105 #106 #107 #108 #109 #110 #111
    [    8.466005] .... node  #14, CPUs:   #112 #113 #114 #115 #116 #117 #118 #119
    [    9.073006] .... node  #15, CPUs:   #120 #121 #122 #123 #124 #125 #126 #127
    [    9.679901] x86: Booted up 16 nodes, 128 CPUs
    
    and drop useless elements.
    
    Change num_digits() to hpa's division-avoiding, cell-phone-typed
    version which he went at great lengths and pains to submit on a
    Saturday evening.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: huawei.libin@huawei.com
    Cc: wangyijing@huawei.com
    Cc: fenghua.yu@intel.com
    Cc: guohanjun@huawei.com
    Cc: paul.gortmaker@windriver.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20130930095624.GB16383@pd.tnic
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d41f3ba26ced..2a165580fa16 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -647,22 +647,38 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	return (send_status | accept_status);
 }
 
+void smp_announce(void)
+{
+	int num_nodes = num_online_nodes();
+
+	printk(KERN_INFO "x86: Booted up %d node%s, %d CPUs\n",
+	       num_nodes, (num_nodes > 1 ? "s" : ""), num_online_cpus());
+}
+
 /* reduce the number of lines printed when booting a large cpu count system */
 static void announce_cpu(int cpu, int apicid)
 {
 	static int current_node = -1;
 	int node = early_cpu_to_node(cpu);
-	static int width;
+	static int width, node_width;
 
 	if (!width)
 		width = num_digits(num_possible_cpus()) + 1; /* + '#' sign */
 
+	if (!node_width)
+		node_width = num_digits(num_possible_nodes()) + 1; /* + '#' */
+
+	if (cpu == 1)
+		printk(KERN_INFO "x86: Booting SMP configuration:\n");
+
 	if (system_state == SYSTEM_BOOTING) {
 		if (node != current_node) {
 			if (current_node > (-1))
-				pr_cont(" OK\n");
+				pr_cont("\n");
 			current_node = node;
-			pr_info("Booting Node %3d, Processors:", node);
+
+			printk(KERN_INFO ".... node %*s#%d, CPUs:  ",
+			       node_width - num_digits(node), " ", node);
 		}
 
 		/* Add padding for the BSP */
@@ -671,8 +687,6 @@ static void announce_cpu(int cpu, int apicid)
 
 		pr_cont("%*s#%d", width - num_digits(cpu), " ", cpu);
 
-		if (cpu == num_present_cpus() - 1)
-			pr_cont(" OK\n");
 	} else
 		pr_info("Booting Node %d Processor %d APIC 0x%x\n",
 			node, cpu, apicid);

commit 646e29a1789a3a936871008c15199c50367bf291
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Sep 27 16:35:54 2013 +0200

    x86: Improve the printout of the SMP bootup CPU table
    
    As the new x86 CPU bootup printout format code maintainer, I am
    taking immediate action to improve and clean (and thus indulge
    my OCD) the reporting of the cores when coming up online.
    
    Fix padding to a right-hand alignment, cleanup code and bind
    reporting width to the max number of supported CPUs on the
    system, like this:
    
     [    0.074509] smpboot: Booting Node   0, Processors:      #1  #2  #3  #4  #5  #6  #7 OK
     [    0.644008] smpboot: Booting Node   1, Processors:  #8  #9 #10 #11 #12 #13 #14 #15 OK
     [    1.245006] smpboot: Booting Node   2, Processors: #16 #17 #18 #19 #20 #21 #22 #23 OK
     [    1.864005] smpboot: Booting Node   3, Processors: #24 #25 #26 #27 #28 #29 #30 #31 OK
     [    2.489005] smpboot: Booting Node   4, Processors: #32 #33 #34 #35 #36 #37 #38 #39 OK
     [    3.093005] smpboot: Booting Node   5, Processors: #40 #41 #42 #43 #44 #45 #46 #47 OK
     [    3.698005] smpboot: Booting Node   6, Processors: #48 #49 #50 #51 #52 #53 #54 #55 OK
     [    4.304005] smpboot: Booting Node   7, Processors: #56 #57 #58 #59 #60 #61 #62 #63 OK
     [    4.961413] Brought up 64 CPUs
    
    and this:
    
     [    0.072367] smpboot: Booting Node   0, Processors:    #1 #2 #3 #4 #5 #6 #7 OK
     [    0.686329] Brought up 8 CPUs
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Libin <huawei.libin@huawei.com>
    Cc: wangyijing@huawei.com
    Cc: fenghua.yu@intel.com
    Cc: guohanjun@huawei.com
    Cc: paul.gortmaker@windriver.com
    Link: http://lkml.kernel.org/r/20130927143554.GF4422@pd.tnic
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6cacab671f9b..d41f3ba26ced 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -73,11 +73,10 @@
 #include <asm/setup.h>
 #include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>
-
 #include <asm/smpboot_hooks.h>
 #include <asm/i8259.h>
-
 #include <asm/realmode.h>
+#include <asm/misc.h>
 
 /* State of each CPU */
 DEFINE_PER_CPU(int, cpu_state) = { 0 };
@@ -653,17 +652,27 @@ static void announce_cpu(int cpu, int apicid)
 {
 	static int current_node = -1;
 	int node = early_cpu_to_node(cpu);
-	int max_cpu_present = find_last_bit(cpumask_bits(cpu_present_mask), NR_CPUS);
+	static int width;
+
+	if (!width)
+		width = num_digits(num_possible_cpus()) + 1; /* + '#' sign */
 
 	if (system_state == SYSTEM_BOOTING) {
 		if (node != current_node) {
 			if (current_node > (-1))
 				pr_cont(" OK\n");
 			current_node = node;
-			pr_info("Booting Node %3d, Processors ", node);
+			pr_info("Booting Node %3d, Processors:", node);
 		}
-		pr_cont(" #%4d%s", cpu, cpu == max_cpu_present ? " OK\n" : "");
-		return;
+
+		/* Add padding for the BSP */
+		if (cpu == 1)
+			pr_cont("%*s", width + 1, " ");
+
+		pr_cont("%*s#%d", width - num_digits(cpu), " ", cpu);
+
+		if (cpu == num_present_cpus() - 1)
+			pr_cont(" OK\n");
 	} else
 		pr_info("Booting Node %d Processor %d APIC 0x%x\n",
 			node, cpu, apicid);

commit 1cad5e9a3978d182aa9b0e909fb0379da5ba45af
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Thu Aug 29 18:22:08 2013 -0600

    hotplug / x86: Disable ARCH_CPU_PROBE_RELEASE on x86
    
    Commit d7c53c9e enabled ARCH_CPU_PROBE_RELEASE on x86 in order to
    serialize CPU online/offline operations.  Although it is the config
    option to enable CPU hotplug test interfaces, probe & release, it is
    also the option to enable cpu_hotplug_driver_lock() as well.  Therefore,
    this option had to be enabled on x86 with dummy arch_cpu_probe() and
    arch_cpu_release().
    
    Since then, lock_device_hotplug() was introduced to serialize CPU
    online/offline & hotplug operations.  Therefore, this config option
    is no longer required for the serialization.  This patch disables
    this config option on x86 and revert the changes made by commit
    d7c53c9e.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Acked-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6cacab671f9b..e73b3f53310c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -82,27 +82,6 @@
 /* State of each CPU */
 DEFINE_PER_CPU(int, cpu_state) = { 0 };
 
-#ifdef CONFIG_HOTPLUG_CPU
-/*
- * We need this for trampoline_base protection from concurrent accesses when
- * off- and onlining cores wildly.
- */
-static DEFINE_MUTEX(x86_cpu_hotplug_driver_mutex);
-
-void cpu_hotplug_driver_lock(void)
-{
-	mutex_lock(&x86_cpu_hotplug_driver_mutex);
-}
-
-void cpu_hotplug_driver_unlock(void)
-{
-	mutex_unlock(&x86_cpu_hotplug_driver_mutex);
-}
-
-ssize_t arch_cpu_probe(const char *buf, size_t count) { return -1; }
-ssize_t arch_cpu_release(const char *buf, size_t count) { return -1; }
-#endif
-
 /* Number of siblings per CPU package */
 int smp_num_siblings = 1;
 EXPORT_SYMBOL(smp_num_siblings);

commit 52239484bf8aec031afa84ae08aa88224d819b93
Author: Libin <huawei.libin@huawei.com>
Date:   Thu Sep 5 18:57:56 2013 +0800

    x86/smpboot: Fix announce_cpu() to printk() the last "OK" properly
    
    When booting secondary CPUs, announce_cpu() is called to show which cpu has
    been brought up. For example:
    
    [    0.402751] smpboot: Booting Node   0, Processors  #1 #2 #3 #4 #5 OK
    [    0.525667] smpboot: Booting Node   1, Processors  #6 #7 #8 #9 #10 #11 OK
    [    0.755592] smpboot: Booting Node   0, Processors  #12 #13 #14 #15 #16 #17 OK
    [    0.890495] smpboot: Booting Node   1, Processors  #18 #19 #20 #21 #22 #23
    
    But the last "OK" is lost, because 'nr_cpu_ids-1' represents the maximum
    possible cpu id. It should use the maximum present cpu id in case not all
    CPUs booted up.
    
    Signed-off-by: Libin <huawei.libin@huawei.com>
    Cc: <guohanjun@huawei.com>
    Cc: <wangyijing@huawei.com>
    Cc: <fenghua.yu@intel.com>
    Cc: <paul.gortmaker@windriver.com>
    Link: http://lkml.kernel.org/r/1378378676-18276-1-git-send-email-huawei.libin@huawei.com
    [ tweaked the changelog, removed unnecessary line break, tweaked the format to align the fields vertically. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index aecc98a93d1b..6cacab671f9b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -653,6 +653,7 @@ static void announce_cpu(int cpu, int apicid)
 {
 	static int current_node = -1;
 	int node = early_cpu_to_node(cpu);
+	int max_cpu_present = find_last_bit(cpumask_bits(cpu_present_mask), NR_CPUS);
 
 	if (system_state == SYSTEM_BOOTING) {
 		if (node != current_node) {
@@ -661,7 +662,7 @@ static void announce_cpu(int cpu, int apicid)
 			current_node = node;
 			pr_info("Booting Node %3d, Processors ", node);
 		}
-		pr_cont(" #%d%s", cpu, cpu == (nr_cpu_ids - 1) ? " OK\n" : "");
+		pr_cont(" #%4d%s", cpu, cpu == max_cpu_present ? " OK\n" : "");
 		return;
 	} else
 		pr_info("Booting Node %d Processor %d APIC 0x%x\n",

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index bfd348e99369..aecc98a93d1b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -130,7 +130,7 @@ atomic_t init_deasserted;
  * Report back to the Boot Processor during boot time or to the caller processor
  * during CPU online.
  */
-static void __cpuinit smp_callin(void)
+static void smp_callin(void)
 {
 	int cpuid, phys_id;
 	unsigned long timeout;
@@ -237,7 +237,7 @@ static int enable_start_cpu0;
 /*
  * Activate a secondary processor.
  */
-notrace static void __cpuinit start_secondary(void *unused)
+static void notrace start_secondary(void *unused)
 {
 	/*
 	 * Don't put *anything* before cpu_init(), SMP booting is too
@@ -300,7 +300,7 @@ void __init smp_store_boot_cpu_info(void)
  * The bootstrap kernel entry code has set these up. Save them for
  * a given CPU
  */
-void __cpuinit smp_store_cpu_info(int id)
+void smp_store_cpu_info(int id)
 {
 	struct cpuinfo_x86 *c = &cpu_data(id);
 
@@ -313,7 +313,7 @@ void __cpuinit smp_store_cpu_info(int id)
 	identify_secondary_cpu(c);
 }
 
-static bool __cpuinit
+static bool
 topology_sane(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o, const char *name)
 {
 	int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
@@ -330,7 +330,7 @@ do {									\
 	cpumask_set_cpu((c2), cpu_##_m##_mask(c1));			\
 } while (0)
 
-static bool __cpuinit match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
+static bool match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 {
 	if (cpu_has_topoext) {
 		int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
@@ -348,7 +348,7 @@ static bool __cpuinit match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 	return false;
 }
 
-static bool __cpuinit match_llc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
+static bool match_llc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 {
 	int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
 
@@ -359,7 +359,7 @@ static bool __cpuinit match_llc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 	return false;
 }
 
-static bool __cpuinit match_mc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
+static bool match_mc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 {
 	if (c->phys_proc_id == o->phys_proc_id) {
 		if (cpu_has(c, X86_FEATURE_AMD_DCM))
@@ -370,7 +370,7 @@ static bool __cpuinit match_mc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 	return false;
 }
 
-void __cpuinit set_cpu_sibling_map(int cpu)
+void set_cpu_sibling_map(int cpu)
 {
 	bool has_smt = smp_num_siblings > 1;
 	bool has_mp = has_smt || boot_cpu_data.x86_max_cores > 1;
@@ -499,7 +499,7 @@ void __inquire_remote_apic(int apicid)
  * INIT, INIT, STARTUP sequence will reset the chip hard for us, and this
  * won't ... remember to clear down the APIC, etc later.
  */
-int __cpuinit
+int
 wakeup_secondary_cpu_via_nmi(int apicid, unsigned long start_eip)
 {
 	unsigned long send_status, accept_status = 0;
@@ -533,7 +533,7 @@ wakeup_secondary_cpu_via_nmi(int apicid, unsigned long start_eip)
 	return (send_status | accept_status);
 }
 
-static int __cpuinit
+static int
 wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 {
 	unsigned long send_status, accept_status = 0;
@@ -649,7 +649,7 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 }
 
 /* reduce the number of lines printed when booting a large cpu count system */
-static void __cpuinit announce_cpu(int cpu, int apicid)
+static void announce_cpu(int cpu, int apicid)
 {
 	static int current_node = -1;
 	int node = early_cpu_to_node(cpu);
@@ -691,7 +691,7 @@ static int wakeup_cpu0_nmi(unsigned int cmd, struct pt_regs *regs)
  * We'll change this code in the future to wake up hard offlined CPU0 if
  * real platform and request are available.
  */
-static int __cpuinit
+static int
 wakeup_cpu_via_init_nmi(int cpu, unsigned long start_ip, int apicid,
 	       int *cpu0_nmi_registered)
 {
@@ -731,7 +731,7 @@ wakeup_cpu_via_init_nmi(int cpu, unsigned long start_ip, int apicid,
  * Returns zero if CPU booted OK, else error code from
  * ->wakeup_secondary_cpu.
  */
-static int __cpuinit do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
+static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 {
 	volatile u32 *trampoline_status =
 		(volatile u32 *) __va(real_mode_header->trampoline_status);
@@ -872,7 +872,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	return boot_error;
 }
 
-int __cpuinit native_cpu_up(unsigned int cpu, struct task_struct *tidle)
+int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
 	int apicid = apic->cpu_present_to_apicid(cpu);
 	unsigned long flags;

commit b0bc225d0e5de887340d4d92a8c594ef0f60d412
Author: Andrew Jones <drjones@redhat.com>
Date:   Wed May 29 14:48:15 2013 +0200

    sched/x86: Construct all sibling maps if smt
    
    Commit 316ad248307fb ("sched/x86: Rewrite
    set_cpu_sibling_map()") broke the construction of sibling maps,
    which also broke the booted_cores accounting.
    
    Before the rewrite, if smt was present, then each map was
    updated for each smt sibling. After the rewrite only
    cpu_sibling_mask gets updated, as the llc and core maps depend
    on 'has_mc = x86_max_cores > 1' instead. This leads to problems
    with topologies like the following
    
    (qemu -smp sockets=2,cores=1,threads=2)
    
      processor       : 0
      physical id     : 0
      siblings        : 1    <= should be 2
      core id         : 0
      cpu cores       : 1
    
      processor       : 1
      physical id     : 0
      siblings        : 1    <= should be 2
      core id         : 0
      cpu cores       : 0    <= should be 1
    
      processor       : 2
      physical id     : 1
      siblings        : 1    <= should be 2
      core id         : 0
      cpu cores       : 1
    
      processor       : 3
      physical id     : 1
      siblings        : 1    <= should be 2
      core id         : 0
      cpu cores       : 0    <= should be 1
    
    This patch restores the former construction by defining has_mc
    as (has_smt || x86_max_cores > 1). This should be fine as there
    were no (has_smt && !has_mc) conditions in the context.
    
    Aso rename has_mc to has_mp now that it's not just for cores.
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: a.p.zijlstra@chello.nl
    Cc: fenghua.yu@intel.com
    Link: http://lkml.kernel.org/r/1369831695-11970-1-git-send-email-drjones@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9c73b51817e4..bfd348e99369 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -372,15 +372,15 @@ static bool __cpuinit match_mc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 
 void __cpuinit set_cpu_sibling_map(int cpu)
 {
-	bool has_mc = boot_cpu_data.x86_max_cores > 1;
 	bool has_smt = smp_num_siblings > 1;
+	bool has_mp = has_smt || boot_cpu_data.x86_max_cores > 1;
 	struct cpuinfo_x86 *c = &cpu_data(cpu);
 	struct cpuinfo_x86 *o;
 	int i;
 
 	cpumask_set_cpu(cpu, cpu_sibling_setup_mask);
 
-	if (!has_smt && !has_mc) {
+	if (!has_mp) {
 		cpumask_set_cpu(cpu, cpu_sibling_mask(cpu));
 		cpumask_set_cpu(cpu, cpu_llc_shared_mask(cpu));
 		cpumask_set_cpu(cpu, cpu_core_mask(cpu));
@@ -394,7 +394,7 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 		if ((i == cpu) || (has_smt && match_smt(c, o)))
 			link_mask(sibling, cpu, i);
 
-		if ((i == cpu) || (has_mc && match_llc(c, o)))
+		if ((i == cpu) || (has_mp && match_llc(c, o)))
 			link_mask(llc_shared, cpu, i);
 
 	}
@@ -406,7 +406,7 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 	for_each_cpu(i, cpu_sibling_setup_mask) {
 		o = &cpu_data(i);
 
-		if ((i == cpu) || (has_mc && match_mc(c, o))) {
+		if ((i == cpu) || (has_mp && match_mc(c, o))) {
 			link_mask(core, cpu, i);
 
 			/*

commit 7d1a941731fabf27e5fb6edbebb79fe856edb4e5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 22:50:03 2013 +0100

    x86: Use generic idle loop
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130321215235.486594473@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9f190a2a00e9..9c73b51817e4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -284,7 +284,7 @@ notrace static void __cpuinit start_secondary(void *unused)
 	x86_cpuinit.setup_percpu_clockev();
 
 	wmb();
-	cpu_idle();
+	cpu_startup_entry(CPUHP_ONLINE);
 }
 
 void __init smp_store_boot_cpu_info(void)

commit 576cfb404c9cab728e9462ea713f3422679d5cf7
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Mar 4 21:16:16 2013 +0100

    x86, smpboot: Remove unused variable
    
    The cpuinfo_x86 ptr is unused now. Drop it. Got obsolete by 69fb3676df33
    ("x86 idle: remove mwait_idle() and "idle=mwait" cmdline param")
    removing its only user.
    
    [ hpa: fixes gcc warning ]
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1362428180-8865-2-git-send-email-bp@alien8.de
    Cc: Len Brown <len.brown@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a6ceaedc396a..9f190a2a00e9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1365,9 +1365,8 @@ static inline void mwait_play_dead(void)
 	unsigned int eax, ebx, ecx, edx;
 	unsigned int highest_cstate = 0;
 	unsigned int highest_subcstate = 0;
-	int i;
 	void *mwait_ptr;
-	struct cpuinfo_x86 *c = __this_cpu_ptr(&cpu_info);
+	int i;
 
 	if (!this_cpu_has(X86_FEATURE_MWAIT))
 		return;

commit 69fb3676df3329a7142803bb3502fa59dc0db2e3
Author: Len Brown <len.brown@intel.com>
Date:   Sun Feb 10 01:38:39 2013 -0500

    x86 idle: remove mwait_idle() and "idle=mwait" cmdline param
    
    mwait_idle() is a C1-only idle loop intended to be more efficient
    than HLT, starting on Pentium-4 HT-enabled processors.
    
    But mwait_idle() has been replaced by the more general
    mwait_idle_with_hints(), which handles both C1 and deeper C-states.
    ACPI processor_idle and intel_idle use only mwait_idle_with_hints(),
    and no longer use mwait_idle().
    
    Here we simplify the x86 native idle code by removing mwait_idle(),
    and the "idle=mwait" bootparam used to invoke it.
    
    Since Linux 3.0 there has been a boot-time warning when "idle=mwait"
    was invoked saying it would be removed in 2012.  This removal
    was also noted in the (now removed:-) feature-removal-schedule.txt.
    
    After this change, kernels configured with
    (CONFIG_ACPI=n && CONFIG_INTEL_IDLE=n) when run on hardware
    that supports MWAIT will simply use HLT.  If MWAIT is desired
    on those systems, cpuidle and the cpuidle drivers above
    can be enabled.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ed0fe385289d..a6ceaedc396a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1369,7 +1369,7 @@ static inline void mwait_play_dead(void)
 	void *mwait_ptr;
 	struct cpuinfo_x86 *c = __this_cpu_ptr(&cpu_info);
 
-	if (!(this_cpu_has(X86_FEATURE_MWAIT) && mwait_usable(c)))
+	if (!this_cpu_has(X86_FEATURE_MWAIT))
 		return;
 	if (!this_cpu_has(X86_FEATURE_CLFLSH))
 		return;

commit a05a4e24dcd73c2de4ef3f8d520b8bbb44570c60
Merge: e9a5a9197196 27d3a8a26ada
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 11 19:58:29 2012 -0800

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 topology discovery improvements from Ingo Molnar:
     "These changes improve topology discovery on AMD CPUs.
    
      Right now this feeds information displayed in
      /sys/devices/system/cpu/cpuX/cache/indexY/* - but in the future we
      could use this to set up a better scheduling topology."
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, cacheinfo: Base cache sharing info on CPUID 0x8000001d on AMD
      x86, cacheinfo: Make use of CPUID 0x8000001d for cache information on AMD
      x86, cacheinfo: Determine number of cache leafs using CPUID 0x8000001d on AMD
      x86: Add cpu_has_topoext

commit 74b84233458e9db7c160cec67638efdbec748ca9
Merge: 507447473756 a71c8bc5dfef
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 11 19:56:33 2012 -0800

    Merge branch 'x86-bsp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 BSP hotplug changes from Ingo Molnar:
     "This tree enables CPU#0 (the boot processor) to be onlined/offlined on
      x86, just like any other CPU.  Enabled on Intel CPUs for now.
    
      Allowing this required the identification and fixing of latent CPU#0
      assumptions (such as CPU#0 initializations, etc.) in the x86
      architecture code, plus the identification of barriers to
      BSP-offlining, such as active PIC interrupts which can only be
      serviced on the BSP.
    
      It's behind a default-off option, and there's a debug option that
      allows the automatic testing of this feature.
    
      The motivation of this feature is to allow and prepare for true
      CPU-hotplug hardware support: recent changes to MCE support enable us
      to detect a deteriorating but not yet hard-failing L1/L2 cache on a
      CPU that could be soft-unplugged - or a failing L3 cache on a
      multi-socket system.
    
      Note that true hardware hot-plug is not yet fully enabled by this,
      because that requires a special platform wakeup sequence to be sent to
      the freshly powered up CPU#0.  Future patches for this are planned,
      once such a platform exists.  Chicken and egg"
    
    * 'x86-bsp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, topology: Debug CPU0 hotplug
      x86/i387.c: Initialize thread xstate only on CPU0 only once
      x86, hotplug: Handle retrigger irq by the first available CPU
      x86, hotplug: The first online processor saves the MTRR state
      x86, hotplug: During CPU0 online, enable x2apic, set_numa_node.
      x86, hotplug: Wake up CPU0 via NMI instead of INIT, SIPI, SIPI
      x86-32, hotplug: Add start_cpu0() entry point to head_32.S
      x86-64, hotplug: Add start_cpu0() entry point to head_64.S
      kernel/cpu.c: Add comment for priority in cpu_hotplug_pm_callback
      x86, hotplug, suspend: Online CPU0 for suspend or hibernate
      x86, hotplug: Support functions for CPU0 online/offline
      x86, topology: Don't offline CPU0 if any PIC irq can not be migrated out of it
      x86, Kconfig: Add config switch for CPU0 hotplug
      doc: Add x86 CPU0 online/offline feature

commit 644c154186386bb1fa6446bc5e037b9ed098db46
Author: Vincent Palatin <vpalatin@chromium.org>
Date:   Fri Nov 30 12:15:32 2012 -0800

    x86, fpu: Avoid FPU lazy restore after suspend
    
    When a cpu enters S3 state, the FPU state is lost.
    After resuming for S3, if we try to lazy restore the FPU for a process running
    on the same CPU, this will result in a corrupted FPU context.
    
    Ensure that "fpu_owner_task" is properly invalided when (re-)initializing a CPU,
    so nobody will try to lazy restore a state which doesn't exist in the hardware.
    
    Tested with a 64-bit kernel on a 4-core Ivybridge CPU with eagerfpu=off,
    by doing thousands of suspend/resume cycles with 4 processes doing FPU
    operations running. Without the patch, a process is killed after a
    few hundreds cycles by a SIGFPE.
    
    Cc: Duncan Laurie <dlaurie@chromium.org>
    Cc: Olof Johansson <olofj@chromium.org>
    Cc: <stable@kernel.org> v3.4+ # for 3.4 need to replace this_cpu_write by percpu_write
    Signed-off-by: Vincent Palatin <vpalatin@chromium.org>
    Link: http://lkml.kernel.org/r/1354306532-1014-1-git-send-email-vpalatin@chromium.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c80a33bc528b..f3e2ec878b8c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -68,6 +68,8 @@
 #include <asm/mwait.h>
 #include <asm/apic.h>
 #include <asm/io_apic.h>
+#include <asm/i387.h>
+#include <asm/fpu-internal.h>
 #include <asm/setup.h>
 #include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>
@@ -818,6 +820,9 @@ int __cpuinit native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 
 	per_cpu(cpu_state, cpu) = CPU_UP_PREPARE;
 
+	/* the FPU context is blank, nobody can own it */
+	__cpu_disable_lazy_restore(cpu);
+
 	err = do_boot_cpu(apicid, cpu, tidle);
 	if (err) {
 		pr_debug("do_boot_cpu failed %d\n", err);

commit e1c467e69040c3be68959332959c07fb3d818e87
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Wed Nov 14 04:36:53 2012 -0800

    x86, hotplug: Wake up CPU0 via NMI instead of INIT, SIPI, SIPI
    
    Instead of waiting for STARTUP after INITs, BSP will execute the BIOS boot-strap
    code which is not a desired behavior for waking up BSP. To avoid the boot-strap
    code, wake up CPU0 by NMI instead.
    
    This works to wake up soft offlined CPU0 only. If CPU0 is hard offlined (i.e.
    physically hot removed and then hot added), NMI won't wake it up. We'll change
    this code in the future to wake up hard offlined CPU0 if real platform and
    request are available.
    
    AP is still waken up as before by INIT, SIPI, SIPI sequence.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1352896613-25957-1-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c297907f3c75..ef53e667e051 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -138,15 +138,17 @@ static void __cpuinit smp_callin(void)
 	 * we may get here before an INIT-deassert IPI reaches
 	 * our local APIC.  We have to wait for the IPI or we'll
 	 * lock up on an APIC access.
+	 *
+	 * Since CPU0 is not wakened up by INIT, it doesn't wait for the IPI.
 	 */
-	if (apic->wait_for_init_deassert)
+	cpuid = smp_processor_id();
+	if (apic->wait_for_init_deassert && cpuid != 0)
 		apic->wait_for_init_deassert(&init_deasserted);
 
 	/*
 	 * (This works even if the APIC is not enabled.)
 	 */
 	phys_id = read_apic_id();
-	cpuid = smp_processor_id();
 	if (cpumask_test_cpu(cpuid, cpu_callin_mask)) {
 		panic("%s: phys CPU#%d, CPU#%d already present??\n", __func__,
 					phys_id, cpuid);
@@ -228,6 +230,8 @@ static void __cpuinit smp_callin(void)
 	cpumask_set_cpu(cpuid, cpu_callin_mask);
 }
 
+static int cpu0_logical_apicid;
+static int enable_start_cpu0;
 /*
  * Activate a secondary processor.
  */
@@ -243,6 +247,8 @@ notrace static void __cpuinit start_secondary(void *unused)
 	preempt_disable();
 	smp_callin();
 
+	enable_start_cpu0 = 0;
+
 #ifdef CONFIG_X86_32
 	/* switch away from the initial page table */
 	load_cr3(swapper_pg_dir);
@@ -492,7 +498,7 @@ void __inquire_remote_apic(int apicid)
  * won't ... remember to clear down the APIC, etc later.
  */
 int __cpuinit
-wakeup_secondary_cpu_via_nmi(int logical_apicid, unsigned long start_eip)
+wakeup_secondary_cpu_via_nmi(int apicid, unsigned long start_eip)
 {
 	unsigned long send_status, accept_status = 0;
 	int maxlvt;
@@ -500,7 +506,7 @@ wakeup_secondary_cpu_via_nmi(int logical_apicid, unsigned long start_eip)
 	/* Target chip */
 	/* Boot on the stack */
 	/* Kick the second */
-	apic_icr_write(APIC_DM_NMI | apic->dest_logical, logical_apicid);
+	apic_icr_write(APIC_DM_NMI | apic->dest_logical, apicid);
 
 	pr_debug("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();
@@ -660,6 +666,63 @@ static void __cpuinit announce_cpu(int cpu, int apicid)
 			node, cpu, apicid);
 }
 
+static int wakeup_cpu0_nmi(unsigned int cmd, struct pt_regs *regs)
+{
+	int cpu;
+
+	cpu = smp_processor_id();
+	if (cpu == 0 && !cpu_online(cpu) && enable_start_cpu0)
+		return NMI_HANDLED;
+
+	return NMI_DONE;
+}
+
+/*
+ * Wake up AP by INIT, INIT, STARTUP sequence.
+ *
+ * Instead of waiting for STARTUP after INITs, BSP will execute the BIOS
+ * boot-strap code which is not a desired behavior for waking up BSP. To
+ * void the boot-strap code, wake up CPU0 by NMI instead.
+ *
+ * This works to wake up soft offlined CPU0 only. If CPU0 is hard offlined
+ * (i.e. physically hot removed and then hot added), NMI won't wake it up.
+ * We'll change this code in the future to wake up hard offlined CPU0 if
+ * real platform and request are available.
+ */
+static int __cpuinit
+wakeup_cpu_via_init_nmi(int cpu, unsigned long start_ip, int apicid,
+	       int *cpu0_nmi_registered)
+{
+	int id;
+	int boot_error;
+
+	/*
+	 * Wake up AP by INIT, INIT, STARTUP sequence.
+	 */
+	if (cpu)
+		return wakeup_secondary_cpu_via_init(apicid, start_ip);
+
+	/*
+	 * Wake up BSP by nmi.
+	 *
+	 * Register a NMI handler to help wake up CPU0.
+	 */
+	boot_error = register_nmi_handler(NMI_LOCAL,
+					  wakeup_cpu0_nmi, 0, "wake_cpu0");
+
+	if (!boot_error) {
+		enable_start_cpu0 = 1;
+		*cpu0_nmi_registered = 1;
+		if (apic->dest_logical == APIC_DEST_LOGICAL)
+			id = cpu0_logical_apicid;
+		else
+			id = apicid;
+		boot_error = wakeup_secondary_cpu_via_nmi(id, start_ip);
+	}
+
+	return boot_error;
+}
+
 /*
  * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad
  * (ie clustered apic addressing mode), this is a LOGICAL apic ID.
@@ -675,6 +738,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 
 	unsigned long boot_error = 0;
 	int timeout;
+	int cpu0_nmi_registered = 0;
 
 	/* Just in case we booted with a single CPU. */
 	alternatives_enable_smp();
@@ -722,13 +786,16 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	}
 
 	/*
-	 * Kick the secondary CPU. Use the method in the APIC driver
-	 * if it's defined - or use an INIT boot APIC message otherwise:
+	 * Wake up a CPU in difference cases:
+	 * - Use the method in the APIC driver if it's defined
+	 * Otherwise,
+	 * - Use an INIT boot APIC message for APs or NMI for BSP.
 	 */
 	if (apic->wakeup_secondary_cpu)
 		boot_error = apic->wakeup_secondary_cpu(apicid, start_ip);
 	else
-		boot_error = wakeup_secondary_cpu_via_init(apicid, start_ip);
+		boot_error = wakeup_cpu_via_init_nmi(cpu, start_ip, apicid,
+						     &cpu0_nmi_registered);
 
 	if (!boot_error) {
 		/*
@@ -793,6 +860,13 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 		 */
 		smpboot_restore_warm_reset_vector();
 	}
+	/*
+	 * Clean up the nmi handler. Do this after the callin and callout sync
+	 * to avoid impact of possible long unregister time.
+	 */
+	if (cpu0_nmi_registered)
+		unregister_nmi_handler(NMI_LOCAL, "wake_cpu0");
+
 	return boot_error;
 }
 
@@ -1037,6 +1111,11 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	 */
 	setup_local_APIC();
 
+	if (x2apic_mode)
+		cpu0_logical_apicid = apic_read(APIC_LDR);
+	else
+		cpu0_logical_apicid = GET_APIC_LOGICAL_ID(apic_read(APIC_LDR));
+
 	/*
 	 * Enable IO APIC before setting up error vector
 	 */
@@ -1264,6 +1343,14 @@ void play_dead_common(void)
 	local_irq_disable();
 }
 
+static bool wakeup_cpu0(void)
+{
+	if (smp_processor_id() == 0 && enable_start_cpu0)
+		return true;
+
+	return false;
+}
+
 /*
  * We need to flush the caches before going to sleep, lest we have
  * dirty data in our caches when we come back up.
@@ -1327,6 +1414,11 @@ static inline void mwait_play_dead(void)
 		__monitor(mwait_ptr, 0, 0);
 		mb();
 		__mwait(eax, 0);
+		/*
+		 * If NMI wants to wake up CPU0, start CPU0.
+		 */
+		if (wakeup_cpu0())
+			start_cpu0();
 	}
 }
 
@@ -1337,6 +1429,11 @@ static inline void hlt_play_dead(void)
 
 	while (1) {
 		native_halt();
+		/*
+		 * If NMI wants to wake up CPU0, start CPU0.
+		 */
+		if (wakeup_cpu0())
+			start_cpu0();
 	}
 }
 

commit 30106c174311b8cfaaa3186c7f6f9c36c62d17da
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Tue Nov 13 11:32:41 2012 -0800

    x86, hotplug: Support functions for CPU0 online/offline
    
    Add smp_store_boot_cpu_info() to store cpu info for BSP during boot time.
    
    Now smp_store_cpu_info() stores cpu info for bringing up BSP or AP after
    it's offline.
    
    Continue to online CPU0 in native_cpu_up().
    
    Continue to offline CPU0 in native_cpu_disable().
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1352835171-3958-5-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c80a33bc528b..c297907f3c75 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -125,8 +125,8 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 atomic_t init_deasserted;
 
 /*
- * Report back to the Boot Processor.
- * Running on AP.
+ * Report back to the Boot Processor during boot time or to the caller processor
+ * during CPU online.
  */
 static void __cpuinit smp_callin(void)
 {
@@ -279,19 +279,30 @@ notrace static void __cpuinit start_secondary(void *unused)
 	cpu_idle();
 }
 
+void __init smp_store_boot_cpu_info(void)
+{
+	int id = 0; /* CPU 0 */
+	struct cpuinfo_x86 *c = &cpu_data(id);
+
+	*c = boot_cpu_data;
+	c->cpu_index = id;
+}
+
 /*
  * The bootstrap kernel entry code has set these up. Save them for
  * a given CPU
  */
-
 void __cpuinit smp_store_cpu_info(int id)
 {
 	struct cpuinfo_x86 *c = &cpu_data(id);
 
 	*c = boot_cpu_data;
 	c->cpu_index = id;
-	if (id != 0)
-		identify_secondary_cpu(c);
+	/*
+	 * During boot time, CPU0 has this setup already. Save the info when
+	 * bringing up AP or offlined CPU0.
+	 */
+	identify_secondary_cpu(c);
 }
 
 static bool __cpuinit
@@ -795,7 +806,7 @@ int __cpuinit native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 
 	pr_debug("++++++++++++++++++++=_---CPU UP  %u\n", cpu);
 
-	if (apicid == BAD_APICID || apicid == boot_cpu_physical_apicid ||
+	if (apicid == BAD_APICID ||
 	    !physid_isset(apicid, phys_cpu_present_map) ||
 	    !apic->apic_id_valid(apicid)) {
 		pr_err("%s: bad cpu %d\n", __func__, cpu);
@@ -990,7 +1001,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	/*
 	 * Setup boot CPU information
 	 */
-	smp_store_cpu_info(0); /* Final full version of the data */
+	smp_store_boot_cpu_info(); /* Final full version of the data */
 	cpumask_copy(cpu_callin_mask, cpumask_of(0));
 	mb();
 
@@ -1214,19 +1225,6 @@ void cpu_disable_common(void)
 
 int native_cpu_disable(void)
 {
-	int cpu = smp_processor_id();
-
-	/*
-	 * Perhaps use cpufreq to drop frequency, but that could go
-	 * into generic code.
-	 *
-	 * We won't take down the boot processor on i386 due to some
-	 * interrupts only being able to be serviced by the BSP.
-	 * Especially so if we're not using an IOAPIC	-zwane
-	 */
-	if (cpu == 0)
-		return -EBUSY;
-
 	clear_local_APIC();
 
 	cpu_disable_common();

commit 193f3fcb3ab769bab4a2b9fa181eef3e5699a352
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Fri Oct 19 10:58:13 2012 +0200

    x86: Add cpu_has_topoext
    
    Introduce cpu_has_topoext to check for AMD's CPUID topology extensions
    support. It indicates support for
    CPUID Fn8000_001D_EAX_x[N:0]-CPUID Fn8000_001E_EDX
    
    See AMD's CPUID Specification, Publication # 25481
    (as of Rev. 2.34 September 2010)
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Link: http://lkml.kernel.org/r/20121019085813.GD26718@alberich
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c80a33bc528b..732bf5cff645 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -313,7 +313,7 @@ do {									\
 
 static bool __cpuinit match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 {
-	if (cpu_has(c, X86_FEATURE_TOPOEXT)) {
+	if (cpu_has_topoext) {
 		int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
 
 		if (c->phys_proc_id == o->phys_proc_id &&

commit 816afe4ff98ee10b1d30fd66361be132a0a5cee6
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Mon Aug 6 17:29:49 2012 +0930

    x86/smp: Don't ever patch back to UP if we unplug cpus
    
    We still patch SMP instructions to UP variants if we boot with a
    single CPU, but not at any other time.  In particular, not if we
    unplug CPUs to return to a single cpu.
    
    Paul McKenney points out:
    
     mean offline overhead is 6251/48=130.2 milliseconds.
    
     If I remove the alternatives_smp_switch() from the offline
     path [...] the mean offline overhead is 550/42=13.1 milliseconds
    
    Basically, we're never going to get those 120ms back, and the
    code is pretty messy.
    
    We get rid of:
    
     1) The "smp-alt-once" boot option. It's actually "smp-alt-boot", the
        documentation is wrong. It's now the default.
    
     2) The skip_smp_alternatives flag used by suspend.
    
     3) arch_disable_nonboot_cpus_begin() and arch_disable_nonboot_cpus_end()
        which were only used to set this one flag.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paul.mckenney@us.ibm.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/87vcgwwive.fsf@rustcorp.com.au
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7c5a8c314c02..c80a33bc528b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -665,7 +665,8 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 	unsigned long boot_error = 0;
 	int timeout;
 
-	alternatives_smp_switch(1);
+	/* Just in case we booted with a single CPU. */
+	alternatives_enable_smp();
 
 	idle->thread.sp = (unsigned long) (((struct pt_regs *)
 			  (THREAD_SIZE +  task_stack_page(idle))) - 1);
@@ -1053,20 +1054,6 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	preempt_enable();
 }
 
-void arch_disable_nonboot_cpus_begin(void)
-{
-	/*
-	 * Avoid the smp alternatives switch during the disable_nonboot_cpus().
-	 * In the suspend path, we will be back in the SMP mode shortly anyways.
-	 */
-	skip_smp_alternatives = true;
-}
-
-void arch_disable_nonboot_cpus_end(void)
-{
-	skip_smp_alternatives = false;
-}
-
 void arch_enable_nonboot_cpus_begin(void)
 {
 	set_mtrr_aps_delayed_init();
@@ -1256,9 +1243,6 @@ void native_cpu_die(unsigned int cpu)
 		if (per_cpu(cpu_state, cpu) == CPU_DEAD) {
 			if (system_state == SYSTEM_RUNNING)
 				pr_info("CPU %u is now offline\n", cpu);
-
-			if (1 == num_online_cpus())
-				alternatives_smp_switch(0);
 			return;
 		}
 		msleep(100);

commit 4cb38750d49010ae72e718d46605ac9ba5a851b4
Merge: 0a2fe19ccc4b 7efa1c87963d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 26 13:17:17 2012 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/mm changes from Peter Anvin:
     "The big change here is the patchset by Alex Shi to use INVLPG to flush
      only the affected pages when we only need to flush a small page range.
    
      It also removes the special INVALIDATE_TLB_VECTOR interrupts (32
      vectors!) and replace it with an ordinary IPI function call."
    
    Fix up trivial conflicts in arch/x86/include/asm/apic.h (added code next
    to changed line)
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/tlb: Fix build warning and crash when building for !SMP
      x86/tlb: do flush_tlb_kernel_range by 'invlpg'
      x86/tlb: replace INVALIDATE_TLB_VECTOR by CALL_FUNCTION_VECTOR
      x86/tlb: enable tlb flush range support for x86
      mm/mmu_gather: enable tlb flush range in generic mmu_gather
      x86/tlb: add tlb_flushall_shift knob into debugfs
      x86/tlb: add tlb_flushall_shift for specific CPU
      x86/tlb: fall back to flush all when meet a THP large page
      x86/flush_tlb: try flush_tlb_single one by one in flush_tlb_range
      x86/tlb_info: get last level TLB entry number of CPU
      x86: Add read_mostly declaration/definition to variables from smp.h
      x86: Define early read-mostly per-cpu macros

commit 3fad0953a12f92289f1e35f091c4fa09d8e1884e
Merge: a065de0d2577 0fa0e2f02e8e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 22 12:04:44 2012 -0700

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull debug-for-linus git tree from Ingo Molnar.
    
    Fix up trivial conflict in arch/x86/kernel/cpu/perf_event_intel.c due to
    a printk() having changed to a pr_info() differently in the two branches.
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Move call to print_modules() out of show_regs()
      x86/mm: Mark free_initrd_mem() as __init
      x86/microcode: Mark microcode_id[] as __initconst
      x86/nmi: Clean up register_nmi_handler() usage
      x86: Save cr2 in NMI in case NMIs take a page fault (for i386)
      x86: Remove cmpxchg from i386 NMI nesting code
      x86: Save cr2 in NMI in case NMIs take a page fault
      x86/debug: Add KERN_<LEVEL> to bare printks, convert printks to pr_<level>

commit 55acdddbac1725b80df0c41970505e8a41c84956
Merge: 2eafeb6a4158 b871a42b6091
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 22 11:22:15 2012 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull smp/hotplug changes from Ingo Molnar:
     "Various cleanups to the SMP hotplug code - a continuing effort of
      Thomas et al"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      smpboot: Remove leftover declaration
      smp: Remove num_booting_cpus()
      smp: Remove ipi_call_lock[_irq]()/ipi_call_unlock[_irq]()
      POWERPC: Smp: remove call to ipi_call_lock()/ipi_call_unlock()
      SPARC: SMP: Remove call to ipi_call_lock_irq()/ipi_call_unlock_irq()
      ia64: SMP: Remove call to ipi_call_lock_irq()/ipi_call_unlock_irq()
      x86-smp-remove-call-to-ipi_call_lock-ipi_call_unlock
      tile: SMP: Remove call to ipi_call_lock()/ipi_call_unlock()
      S390: Smp: remove call to ipi_call_lock()/ipi_call_unlock()
      parisc: Smp: remove call to ipi_call_lock()/ipi_call_unlock()
      mn10300: SMP: Remove call to ipi_call_lock()/ipi_call_unlock()
      hexagon: SMP: Remove call to ipi_call_lock()/ipi_call_unlock()

commit 6a991acceedce3ca93caef8ba7af2468c9451614
Merge: 70fb74a5420f 485802a6c524
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jun 20 14:22:32 2012 +0200

    Merge commit 'v3.5-rc3' into x86/debug
    
    Merge it in to pick up a fix that we are going to clean up in this
    branch.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c83119a980166539bc14033d89e83cd2e4f2da52
Merge: ed21a66c1862 161270fc1f9d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 15 16:59:19 2012 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar.
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/smp: Fix topology checks on AMD MCM CPUs
      x86/mm: Fix some kernel-doc warnings
      x86, um: Correct syscall table type attributes breaking gcc 4.8

commit 0816b0f0365539c8f6280634d2c1778d0108d8f5
Author: Vlad Zolotarov <vlad@scalemp.com>
Date:   Mon Jun 11 12:56:52 2012 +0300

    x86: Add read_mostly declaration/definition to variables from smp.h
    
    Add "read-mostly" qualifier to the following variables in
    smp.h:
    
     - cpu_sibling_map
     - cpu_core_map
     - cpu_llc_shared_map
     - cpu_llc_id
     - cpu_number
     - x86_cpu_to_apicid
     - x86_bios_cpu_apicid
     - x86_cpu_to_logical_apicid
    
    As long as all the variables above are only written during the
    initialization, this change is meant to prevent the false
    sharing. More specifically, on vSMP Foundation platform
    x86_cpu_to_apicid shared the same internode_cache_line with
    frequently written lapic_events.
    
    From the analysis of the first 33 per_cpu variables out of 219
    (memories they describe, to be more specific) the 8 have read_mostly
    nature (tlb_vector_offset, cpu_loops_per_jiffy, xen_debug_irq, etc.)
    and 25 are frequently written (irq_stack_union, gdt_page,
    exception_stacks, idt_desc, etc.).
    
    Assuming that the spread of the rest of the per_cpu variables is
    similar, identifying the read mostly memories will make more sense
    in terms of long-term code maintenance comparing to identifying
    frequently written memories.
    
    Signed-off-by: Vlad Zolotarov <vlad@scalemp.com>
    Acked-by: Shai Fultheim <shai@scalemp.com>
    Cc: Shai Fultheim (Shai@ScaleMP.com) <Shai@scalemp.com>
    Cc: ido@wizery.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1719258.EYKzE4Zbq5@vlad
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3fab55bea29b..e61110e29a8c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -104,17 +104,17 @@ int smp_num_siblings = 1;
 EXPORT_SYMBOL(smp_num_siblings);
 
 /* Last level cache ID of each logical CPU */
-DEFINE_PER_CPU(u16, cpu_llc_id) = BAD_APICID;
+DEFINE_PER_CPU_READ_MOSTLY(u16, cpu_llc_id) = BAD_APICID;
 
 /* representing HT siblings of each logical CPU */
-DEFINE_PER_CPU(cpumask_var_t, cpu_sibling_map);
+DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_sibling_map);
 EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
 
 /* representing HT and core siblings of each logical CPU */
-DEFINE_PER_CPU(cpumask_var_t, cpu_core_map);
+DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_core_map);
 EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 
-DEFINE_PER_CPU(cpumask_var_t, cpu_llc_shared_map);
+DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_llc_shared_map);
 
 /* Per CPU bogomips and other parameters */
 DEFINE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);

commit 161270fc1f9ddfc17154e0d49291472a9cdef7db
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Wed Jun 6 17:31:26 2012 +0200

    x86/smp: Fix topology checks on AMD MCM CPUs
    
    The warning below triggers on AMD MCM packages because physical package
    IDs on the cores of a _physical_ socket are the same. I.e., this field
    says which CPUs belong to the same physical package.
    
    However, the same two CPUs belong to two different internal, i.e.
    "logical" nodes in the same physical socket which is reflected in the
    CPU-to-node map on x86 with NUMA.
    
    Which makes this check wrong on the above topologies so circumvent it.
    
    [    0.444413] Booting Node   0, Processors  #1 #2 #3 #4 #5 Ok.
    [    0.461388] ------------[ cut here ]------------
    [    0.465997] WARNING: at arch/x86/kernel/smpboot.c:310 topology_sane.clone.1+0x6e/0x81()
    [    0.473960] Hardware name: Dinar
    [    0.477170] sched: CPU #6's mc-sibling CPU #0 is not on the same node! [node: 1 != 0]. Ignoring dependency.
    [    0.486860] Booting Node   1, Processors  #6
    [    0.491104] Modules linked in:
    [    0.494141] Pid: 0, comm: swapper/6 Not tainted 3.4.0+ #1
    [    0.499510] Call Trace:
    [    0.501946]  [<ffffffff8144bf92>] ? topology_sane.clone.1+0x6e/0x81
    [    0.508185]  [<ffffffff8102f1fc>] warn_slowpath_common+0x85/0x9d
    [    0.514163]  [<ffffffff8102f2b7>] warn_slowpath_fmt+0x46/0x48
    [    0.519881]  [<ffffffff8144bf92>] topology_sane.clone.1+0x6e/0x81
    [    0.525943]  [<ffffffff8144c234>] set_cpu_sibling_map+0x251/0x371
    [    0.532004]  [<ffffffff8144c4ee>] start_secondary+0x19a/0x218
    [    0.537729] ---[ end trace 4eaa2a86a8e2da22 ]---
    [    0.628197]  #7 #8 #9 #10 #11 Ok.
    [    0.807108] Booting Node   3, Processors  #12 #13 #14 #15 #16 #17 Ok.
    [    0.897587] Booting Node   2, Processors  #18 #19 #20 #21 #22 #23 Ok.
    [    0.917443] Brought up 24 CPUs
    
    We ran a topology sanity check test we have here on it and
    it all looks ok... hopefully :).
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120529135442.GE29157@aftab.osrc.amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fd019d78b1f4..c3a6bacc8a3e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -349,9 +349,12 @@ static bool __cpuinit match_llc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 
 static bool __cpuinit match_mc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 {
-	if (c->phys_proc_id == o->phys_proc_id)
-		return topology_sane(c, o, "mc");
+	if (c->phys_proc_id == o->phys_proc_id) {
+		if (cpu_has(c, X86_FEATURE_AMD_DCM))
+			return true;
 
+		return topology_sane(c, o, "mc");
+	}
 	return false;
 }
 

commit ceb1cbac8eda66cf0f889def226b4e82f8ff857b
Author: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
Date:   Thu May 31 13:07:38 2012 +0530

    sched/x86: Calculate booted cores after construction of sibling_mask
    
    Commit 316ad248307fb ("sched/x86: Rewrite set_cpu_sibling_map()")
    broke the booted_cores accounting.
    
    The problem is that the booted_cores accounting needs all the
    sibling links set up. So restore the second loop and add a comment as
    to why its needed.
    
    On qemu booted with -smp sockets=1,cores=2,threads=2;
    Before:
     $ grep cores /proc/cpuinfo
     cpu cores       : 2
     cpu cores       : 1
     cpu cores       : 4
     cpu cores       : 3
    
    With the patch:
     $ grep cores /proc/cpuinfo
     cpu cores       : 2
     cpu cores       : 2
     cpu cores       : 2
     cpu cores       : 2
    
    Reported-by: Prarit Bhargava <prarit@redhat.com>
    Reported-by: Borislav Petkov <bp@amd64.org>
    Signed-off-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120531073738.GH7511@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fd019d78b1f4..3fab55bea29b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -382,6 +382,15 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 		if ((i == cpu) || (has_mc && match_llc(c, o)))
 			link_mask(llc_shared, cpu, i);
 
+	}
+
+	/*
+	 * This needs a separate iteration over the cpus because we rely on all
+	 * cpu_sibling_mask links to be set-up.
+	 */
+	for_each_cpu(i, cpu_sibling_setup_mask) {
+		o = &cpu_data(i);
+
 		if ((i == cpu) || (has_mc && match_mc(c, o))) {
 			link_mask(core, cpu, i);
 

commit c767a54ba0657e52e6edaa97cbe0b0a8bf1c1655
Author: Joe Perches <joe@perches.com>
Date:   Mon May 21 19:50:07 2012 -0700

    x86/debug: Add KERN_<LEVEL> to bare printks, convert printks to pr_<level>
    
    Use a more current logging style:
    
     - Bare printks should have a KERN_<LEVEL> for consistency's sake
     - Add pr_fmt where appropriate
     - Neaten some macro definitions
     - Convert some Ok output to OK
     - Use "%s: ", __func__ in pr_fmt for summit
     - Convert some printks to pr_<level>
    
    Message output is not identical in all cases.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: levinsasha928@gmail.com
    Link: http://lkml.kernel.org/r/1337655007.24226.10.camel@joe2Laptop
    [ merged two similar patches, tidied up the changelog ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fd019d78b1f4..456d64806c8f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1,4 +1,4 @@
-/*
+ /*
  *	x86 SMP booting functions
  *
  *	(c) 1995 Alan Cox, Building #3 <alan@lxorguk.ukuu.org.uk>
@@ -39,6 +39,8 @@
  *	Glauber Costa		:	i386 and x86_64 integration
  */
 
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
 #include <linux/init.h>
 #include <linux/smp.h>
 #include <linux/module.h>
@@ -184,7 +186,7 @@ static void __cpuinit smp_callin(void)
 	 * boards)
 	 */
 
-	pr_debug("CALLIN, before setup_local_APIC().\n");
+	pr_debug("CALLIN, before setup_local_APIC()\n");
 	if (apic->smp_callin_clear_local_apic)
 		apic->smp_callin_clear_local_apic();
 	setup_local_APIC();
@@ -420,17 +422,16 @@ static void impress_friends(void)
 	/*
 	 * Allow the user to impress friends.
 	 */
-	pr_debug("Before bogomips.\n");
+	pr_debug("Before bogomips\n");
 	for_each_possible_cpu(cpu)
 		if (cpumask_test_cpu(cpu, cpu_callout_mask))
 			bogosum += cpu_data(cpu).loops_per_jiffy;
-	printk(KERN_INFO
-		"Total of %d processors activated (%lu.%02lu BogoMIPS).\n",
+	pr_info("Total of %d processors activated (%lu.%02lu BogoMIPS)\n",
 		num_online_cpus(),
 		bogosum/(500000/HZ),
 		(bogosum/(5000/HZ))%100);
 
-	pr_debug("Before bogocount - setting activated=1.\n");
+	pr_debug("Before bogocount - setting activated=1\n");
 }
 
 void __inquire_remote_apic(int apicid)
@@ -440,18 +441,17 @@ void __inquire_remote_apic(int apicid)
 	int timeout;
 	u32 status;
 
-	printk(KERN_INFO "Inquiring remote APIC 0x%x...\n", apicid);
+	pr_info("Inquiring remote APIC 0x%x...\n", apicid);
 
 	for (i = 0; i < ARRAY_SIZE(regs); i++) {
-		printk(KERN_INFO "... APIC 0x%x %s: ", apicid, names[i]);
+		pr_info("... APIC 0x%x %s: ", apicid, names[i]);
 
 		/*
 		 * Wait for idle.
 		 */
 		status = safe_apic_wait_icr_idle();
 		if (status)
-			printk(KERN_CONT
-			       "a previous APIC delivery may have failed\n");
+			pr_cont("a previous APIC delivery may have failed\n");
 
 		apic_icr_write(APIC_DM_REMRD | regs[i], apicid);
 
@@ -464,10 +464,10 @@ void __inquire_remote_apic(int apicid)
 		switch (status) {
 		case APIC_ICR_RR_VALID:
 			status = apic_read(APIC_RRR);
-			printk(KERN_CONT "%08x\n", status);
+			pr_cont("%08x\n", status);
 			break;
 		default:
-			printk(KERN_CONT "failed\n");
+			pr_cont("failed\n");
 		}
 	}
 }
@@ -501,12 +501,12 @@ wakeup_secondary_cpu_via_nmi(int logical_apicid, unsigned long start_eip)
 			apic_write(APIC_ESR, 0);
 		accept_status = (apic_read(APIC_ESR) & 0xEF);
 	}
-	pr_debug("NMI sent.\n");
+	pr_debug("NMI sent\n");
 
 	if (send_status)
-		printk(KERN_ERR "APIC never delivered???\n");
+		pr_err("APIC never delivered???\n");
 	if (accept_status)
-		printk(KERN_ERR "APIC delivery error (%lx).\n", accept_status);
+		pr_err("APIC delivery error (%lx)\n", accept_status);
 
 	return (send_status | accept_status);
 }
@@ -528,7 +528,7 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 		apic_read(APIC_ESR);
 	}
 
-	pr_debug("Asserting INIT.\n");
+	pr_debug("Asserting INIT\n");
 
 	/*
 	 * Turn INIT on target chip
@@ -544,7 +544,7 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 
 	mdelay(10);
 
-	pr_debug("Deasserting INIT.\n");
+	pr_debug("Deasserting INIT\n");
 
 	/* Target chip */
 	/* Send IPI */
@@ -577,14 +577,14 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	/*
 	 * Run STARTUP IPI loop.
 	 */
-	pr_debug("#startup loops: %d.\n", num_starts);
+	pr_debug("#startup loops: %d\n", num_starts);
 
 	for (j = 1; j <= num_starts; j++) {
-		pr_debug("Sending STARTUP #%d.\n", j);
+		pr_debug("Sending STARTUP #%d\n", j);
 		if (maxlvt > 3)		/* Due to the Pentium erratum 3AP.  */
 			apic_write(APIC_ESR, 0);
 		apic_read(APIC_ESR);
-		pr_debug("After apic_write.\n");
+		pr_debug("After apic_write\n");
 
 		/*
 		 * STARTUP IPI
@@ -601,7 +601,7 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 		 */
 		udelay(300);
 
-		pr_debug("Startup point 1.\n");
+		pr_debug("Startup point 1\n");
 
 		pr_debug("Waiting for send to finish...\n");
 		send_status = safe_apic_wait_icr_idle();
@@ -616,12 +616,12 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 		if (send_status || accept_status)
 			break;
 	}
-	pr_debug("After Startup.\n");
+	pr_debug("After Startup\n");
 
 	if (send_status)
-		printk(KERN_ERR "APIC never delivered???\n");
+		pr_err("APIC never delivered???\n");
 	if (accept_status)
-		printk(KERN_ERR "APIC delivery error (%lx).\n", accept_status);
+		pr_err("APIC delivery error (%lx)\n", accept_status);
 
 	return (send_status | accept_status);
 }
@@ -635,11 +635,11 @@ static void __cpuinit announce_cpu(int cpu, int apicid)
 	if (system_state == SYSTEM_BOOTING) {
 		if (node != current_node) {
 			if (current_node > (-1))
-				pr_cont(" Ok.\n");
+				pr_cont(" OK\n");
 			current_node = node;
 			pr_info("Booting Node %3d, Processors ", node);
 		}
-		pr_cont(" #%d%s", cpu, cpu == (nr_cpu_ids - 1) ? " Ok.\n" : "");
+		pr_cont(" #%d%s", cpu, cpu == (nr_cpu_ids - 1) ? " OK\n" : "");
 		return;
 	} else
 		pr_info("Booting Node %d Processor %d APIC 0x%x\n",
@@ -719,9 +719,9 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 		/*
 		 * allow APs to start initializing.
 		 */
-		pr_debug("Before Callout %d.\n", cpu);
+		pr_debug("Before Callout %d\n", cpu);
 		cpumask_set_cpu(cpu, cpu_callout_mask);
-		pr_debug("After Callout %d.\n", cpu);
+		pr_debug("After Callout %d\n", cpu);
 
 		/*
 		 * Wait 5s total for a response
@@ -749,7 +749,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 				pr_err("CPU%d: Stuck ??\n", cpu);
 			else
 				/* trampoline code not run */
-				pr_err("CPU%d: Not responding.\n", cpu);
+				pr_err("CPU%d: Not responding\n", cpu);
 			if (apic->inquire_remote_apic)
 				apic->inquire_remote_apic(apicid);
 		}
@@ -794,7 +794,7 @@ int __cpuinit native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 	if (apicid == BAD_APICID || apicid == boot_cpu_physical_apicid ||
 	    !physid_isset(apicid, phys_cpu_present_map) ||
 	    !apic->apic_id_valid(apicid)) {
-		printk(KERN_ERR "%s: bad cpu %d\n", __func__, cpu);
+		pr_err("%s: bad cpu %d\n", __func__, cpu);
 		return -EINVAL;
 	}
 
@@ -875,9 +875,8 @@ static int __init smp_sanity_check(unsigned max_cpus)
 		unsigned int cpu;
 		unsigned nr;
 
-		printk(KERN_WARNING
-		       "More than 8 CPUs detected - skipping them.\n"
-		       "Use CONFIG_X86_BIGSMP.\n");
+		pr_warn("More than 8 CPUs detected - skipping them\n"
+			"Use CONFIG_X86_BIGSMP\n");
 
 		nr = 0;
 		for_each_present_cpu(cpu) {
@@ -898,8 +897,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 #endif
 
 	if (!physid_isset(hard_smp_processor_id(), phys_cpu_present_map)) {
-		printk(KERN_WARNING
-			"weird, boot CPU (#%d) not listed by the BIOS.\n",
+		pr_warn("weird, boot CPU (#%d) not listed by the BIOS\n",
 			hard_smp_processor_id());
 
 		physid_set(hard_smp_processor_id(), phys_cpu_present_map);
@@ -911,11 +909,10 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	 */
 	if (!smp_found_config && !acpi_lapic) {
 		preempt_enable();
-		printk(KERN_NOTICE "SMP motherboard not detected.\n");
+		pr_notice("SMP motherboard not detected\n");
 		disable_smp();
 		if (APIC_init_uniprocessor())
-			printk(KERN_NOTICE "Local APIC not detected."
-					   " Using dummy APIC emulation.\n");
+			pr_notice("Local APIC not detected. Using dummy APIC emulation.\n");
 		return -1;
 	}
 
@@ -924,9 +921,8 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	 * CPU too, but we do it for the sake of robustness anyway.
 	 */
 	if (!apic->check_phys_apicid_present(boot_cpu_physical_apicid)) {
-		printk(KERN_NOTICE
-			"weird, boot CPU (#%d) not listed by the BIOS.\n",
-			boot_cpu_physical_apicid);
+		pr_notice("weird, boot CPU (#%d) not listed by the BIOS\n",
+			  boot_cpu_physical_apicid);
 		physid_set(hard_smp_processor_id(), phys_cpu_present_map);
 	}
 	preempt_enable();
@@ -939,8 +935,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 		if (!disable_apic) {
 			pr_err("BIOS bug, local APIC #%d not detected!...\n",
 				boot_cpu_physical_apicid);
-			pr_err("... forcing use of dummy APIC emulation."
-				"(tell your hw vendor)\n");
+			pr_err("... forcing use of dummy APIC emulation (tell your hw vendor)\n");
 		}
 		smpboot_clear_io_apic();
 		disable_ioapic_support();
@@ -953,7 +948,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	 * If SMP should be disabled, then really disable it!
 	 */
 	if (!max_cpus) {
-		printk(KERN_INFO "SMP mode deactivated.\n");
+		pr_info("SMP mode deactivated\n");
 		smpboot_clear_io_apic();
 
 		connect_bsp_APIC();
@@ -1005,7 +1000,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 
 	if (smp_sanity_check(max_cpus) < 0) {
-		printk(KERN_INFO "SMP disabled\n");
+		pr_info("SMP disabled\n");
 		disable_smp();
 		goto out;
 	}
@@ -1043,7 +1038,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	 * Set up local APIC timer on boot CPU.
 	 */
 
-	printk(KERN_INFO "CPU%d: ", 0);
+	pr_info("CPU%d: ", 0);
 	print_cpu_info(&cpu_data(0));
 	x86_init.timers.setup_percpu_clockev();
 
@@ -1093,7 +1088,7 @@ void __init native_smp_prepare_boot_cpu(void)
 
 void __init native_smp_cpus_done(unsigned int max_cpus)
 {
-	pr_debug("Boot done.\n");
+	pr_debug("Boot done\n");
 
 	nmi_selftest();
 	impress_friends();
@@ -1154,8 +1149,7 @@ __init void prefill_possible_map(void)
 
 	/* nr_cpu_ids could be reduced via nr_cpus= */
 	if (possible > nr_cpu_ids) {
-		printk(KERN_WARNING
-			"%d Processors exceeds NR_CPUS limit of %d\n",
+		pr_warn("%d Processors exceeds NR_CPUS limit of %d\n",
 			possible, nr_cpu_ids);
 		possible = nr_cpu_ids;
 	}
@@ -1164,13 +1158,12 @@ __init void prefill_possible_map(void)
 	if (!setup_max_cpus)
 #endif
 	if (possible > i) {
-		printk(KERN_WARNING
-			"%d Processors exceeds max_cpus limit of %u\n",
+		pr_warn("%d Processors exceeds max_cpus limit of %u\n",
 			possible, setup_max_cpus);
 		possible = i;
 	}
 
-	printk(KERN_INFO "SMP: Allowing %d CPUs, %d hotplug CPUs\n",
+	pr_info("Allowing %d CPUs, %d hotplug CPUs\n",
 		possible, max_t(int, possible - num_processors, 0));
 
 	for (i = 0; i < possible; i++)

commit 3b6f70fd7dd4e19fc674ec99e389bf0da5589525
Author: Yong Zhang <yong.zhang0@gmail.com>
Date:   Tue May 29 15:16:01 2012 +0800

    x86-smp-remove-call-to-ipi_call_lock-ipi_call_unlock
    
    ipi_call_lock/unlock() lock resp. unlock call_function.lock. This lock
    protects only the call_function data structure itself, but it's
    completely unrelated to cpu_online_mask. The mask to which the IPIs
    are sent is calculated before call_function.lock is taken in
    smp_call_function_many(), so the locking around set_cpu_online() is
    pointless and can be removed.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Cc: ralf@linux-mips.org
    Cc: sshtylyov@mvista.com
    Cc: david.daney@cavium.com
    Cc: nikunj@linux.vnet.ibm.com
    Cc: paulmck@linux.vnet.ibm.com
    Cc: axboe@kernel.dk
    Cc: peterz@infradead.org
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1338275765-3217-7-git-send-email-yong.zhang0@gmail.com
    Acked-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f56f96da77f5..b2fd28ff84b5 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -255,22 +255,13 @@ notrace static void __cpuinit start_secondary(void *unused)
 	check_tsc_sync_target();
 
 	/*
-	 * We need to hold call_lock, so there is no inconsistency
-	 * between the time smp_call_function() determines number of
-	 * IPI recipients, and the time when the determination is made
-	 * for which cpus receive the IPI. Holding this
-	 * lock helps us to not include this cpu in a currently in progress
-	 * smp_call_function().
-	 *
 	 * We need to hold vector_lock so there the set of online cpus
 	 * does not change while we are assigning vectors to cpus.  Holding
 	 * this lock ensures we don't half assign or remove an irq from a cpu.
 	 */
-	ipi_call_lock();
 	lock_vector_lock();
 	set_cpu_online(smp_processor_id(), true);
 	unlock_vector_lock();
-	ipi_call_unlock();
 	per_cpu(cpu_state, smp_processor_id()) = CPU_ONLINE;
 	x86_platform.nmi_init();
 

commit 9f646389aa7727a2fd8f9ae6337b92af9cfbc264
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 29 16:39:09 2012 +0200

    sched/x86: Use cpu_llc_shared_mask(cpu) for coregroup_mask
    
    Commit commit 8e7fbcbc2 ("sched: Remove stale power aware scheduling
    remnants and dysfunctional knobs") made a boo-boo with removing the
    power aware scheduling muck from the x86 topology bits.
    
    We should unconditionally use the llc_shared mask for multi-core.
    
    Reported-and-tested-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Borislav Petkov <bp@amd64.org>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Link: http://lkml.kernel.org/n/tip-lsksc2kfyeveb13avh327p0d@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f56f96da77f5..fd019d78b1f4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -410,15 +410,7 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 /* maps the cpu to the sched domain representing multi-core */
 const struct cpumask *cpu_coregroup_mask(int cpu)
 {
-	struct cpuinfo_x86 *c = &cpu_data(cpu);
-	/*
-	 * For perf, we return last level cache shared map.
-	 * And for power savings, we return cpu_core_map
-	 */
-	if (!(cpu_has(c, X86_FEATURE_AMD_DCM)))
-		return cpu_core_mask(cpu);
-	else
-		return cpu_llc_shared_mask(cpu);
+	return cpu_llc_shared_mask(cpu);
 }
 
 static void impress_friends(void)

commit 731a7378b81c2f5fa88ca1ae20b83d548d5613dc
Merge: 87a5af24e548 61f544616904
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 29 20:14:53 2012 -0700

    Merge branch 'x86-trampoline-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 trampoline rework from H. Peter Anvin:
     "This code reworks all the "trampoline"/"realmode" code (various bits
      that need to live in the first megabyte of memory, most but not all of
      which runs in real mode at some point) in the kernel into a single
      object.  The main reason for doing this is that it eliminates the last
      place in the kernel where we needed pages to be mapped RWX.  This code
      separates all that code into proper R/RW/RX pages."
    
    Fix up conflicts in arch/x86/kernel/Makefile (mca removed next to reboot
    code), and arch/x86/kernel/reboot.c (reboot code moved around in one
    branch, modified in this one), and arch/x86/tools/relocs.c (mostly same
    code came in earlier due to working around the ld bugs just before the
    3.4 release).
    
    Also remove stale x86-relocs entry from scripts/.gitignore as per Peter
    Anvin.
    
    * commit '61f5446169046c217a5479517edac3a890c3bee7': (36 commits)
      x86, realmode: Move end signature into header.S
      x86, relocs: When printing an error, say relative or absolute
      x86, relocs: More relocations which may end up as absolute
      x86, relocs: Workaround for binutils 2.22.52.0.1 section bug
      xen-acpi-processor: Add missing #include <xen/xen.h>
      acpi, bgrd: Add missing <linux/io.h> to drivers/acpi/bgrt.c
      x86, realmode: Change EFER to a single u64 field
      x86, realmode: Move kernel/realmode.c to realmode/init.c
      x86, realmode: Move not-common bits out of trampoline_common.S
      x86, realmode: Mask out EFER.LMA when saving trampoline EFER
      x86, realmode: Fix no cache bits test in reboot_32.S
      x86, realmode: Make sure all generated files are listed in targets
      x86, realmode: build fix: remove duplicate build
      x86, realmode: read cr4 and EFER from kernel for 64-bit trampoline
      x86, realmode: fixes compilation issue in tboot.c
      x86, realmode: move relocs from scripts/ to arch/x86/tools
      x86, realmode: header for trampoline code
      x86, realmode: flattened rm hierachy
      x86, realmode: don't copy real_mode_header
      x86, realmode: fix 64-bit wakeup sequence
      ...

commit d79ee93de909dfb252279b9a95978bbda9a814a9
Merge: 2ff2b289a695 1c2927f18576
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 22 18:27:32 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The biggest change is the cleanup/simplification of the load-balancer:
      instead of the current practice of architectures twiddling scheduler
      internal data structures and providing the scheduler domains in
      colorfully inconsistent ways, we now have generic scheduler code in
      kernel/sched/core.c:sched_init_numa() that looks at the architecture's
      node_distance() parameters and (while not fully trusting it) deducts a
      NUMA topology from it.
    
      This inevitably changes balancing behavior - hopefully for the better.
    
      There are various smaller optimizations, cleanups and fixlets as well"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Taint kernel with TAINT_WARN after sleep-in-atomic bug
      sched: Remove stale power aware scheduling remnants and dysfunctional knobs
      sched/debug: Fix printing large integers on 32-bit platforms
      sched/fair: Improve the ->group_imb logic
      sched/nohz: Fix rq->cpu_load[] calculations
      sched/numa: Don't scale the imbalance
      sched/fair: Revert sched-domain iteration breakage
      sched/x86: Rewrite set_cpu_sibling_map()
      sched/numa: Fix the new NUMA topology bits
      sched/numa: Rewrite the CONFIG_NUMA sched domain support
      sched/fair: Propagate 'struct lb_env' usage into find_busiest_group
      sched/fair: Add some serialization to the sched_domain load-balance walk
      sched/fair: Let minimally loaded cpu balance the group
      sched: Change rq->nr_running to unsigned int
      x86/numa: Check for nonsensical topologies on real hw as well
      x86/numa: Hard partition cpu topology masks on node boundaries
      x86/numa: Allow specifying node_distance() for numa=fake
      x86/sched: Make mwait_usable() heed to "idle=" kernel parameters properly
      sched: Update documentation and comments
      sched_rt: Avoid unnecessary dequeue and enqueue of pushable tasks in set_cpus_allowed_rt()

commit 8e7fbcbc22c12414bcc9dfdd683637f58fb32759
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 9 11:28:35 2012 +0100

    sched: Remove stale power aware scheduling remnants and dysfunctional knobs
    
    It's been broken forever (i.e. it's not scheduling in a power
    aware fashion), as reported by Suresh and others sending
    patches, and nobody cares enough to fix it properly ...
    so remove it to make space free for something better.
    
    There's various problems with the code as it stands today, first
    and foremost the user interface which is bound to topology
    levels and has multiple values per level. This results in a
    state explosion which the administrator or distro needs to
    master and almost nobody does.
    
    Furthermore large configuration state spaces aren't good, it
    means the thing doesn't just work right because it's either
    under so many impossibe to meet constraints, or even if
    there's an achievable state workloads have to be aware of
    it precisely and can never meet it for dynamic workloads.
    
    So pushing this kind of decision to user-space was a bad idea
    even with a single knob - it's exponentially worse with knobs
    on every node of the topology.
    
    There is a proposal to replace the user interface with a single
    3 state knob:
    
     sched_balance_policy := { performance, power, auto }
    
    where 'auto' would be the preferred default which looks at things
    like Battery/AC mode and possible cpufreq state or whatever the hw
    exposes to show us power use expectations - but there's been no
    progress on it in the past many months.
    
    Aside from that, the actual implementation of the various knobs
    is known to be broken. There have been sporadic attempts at
    fixing things but these always stop short of reaching a mergable
    state.
    
    Therefore this wholesale removal with the hopes of spurring
    people who care to come forward once again and work on a
    coherent replacement.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1326104915.2442.53.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e84c1bbea339..256c20cc5e96 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -429,8 +429,7 @@ const struct cpumask *cpu_coregroup_mask(int cpu)
 	 * For perf, we return last level cache shared map.
 	 * And for power savings, we return cpu_core_map
 	 */
-	if ((sched_mc_power_savings || sched_smt_power_savings) &&
-	    !(cpu_has(c, X86_FEATURE_AMD_DCM)))
+	if (!(cpu_has(c, X86_FEATURE_AMD_DCM)))
 		return cpu_core_mask(cpu);
 	else
 		return cpu_llc_shared_mask(cpu);

commit 316ad248307fba13be40f01e92a22b89457c32bc
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 11 13:05:59 2012 +0200

    sched/x86: Rewrite set_cpu_sibling_map()
    
    Commit ad7687dde ("x86/numa: Check for nonsensical topologies on real
    hw as well") is broken in that the condition can trigger for valid
    setups but only changes the end result for invalid setups with no real
    means of discerning between those.
    
    Rewrite set_cpu_sibling_map() to make the code clearer and make sure
    to only warn when the check changes the end result.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-klcwahu3gx467uhfiqjyhdcs@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7c53d96d44ab..e84c1bbea339 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -315,70 +315,90 @@ void __cpuinit smp_store_cpu_info(int id)
 		identify_secondary_cpu(c);
 }
 
-static void __cpuinit link_thread_siblings(int cpu1, int cpu2)
+static bool __cpuinit
+topology_sane(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o, const char *name)
 {
-	cpumask_set_cpu(cpu1, cpu_sibling_mask(cpu2));
-	cpumask_set_cpu(cpu2, cpu_sibling_mask(cpu1));
-	cpumask_set_cpu(cpu1, cpu_core_mask(cpu2));
-	cpumask_set_cpu(cpu2, cpu_core_mask(cpu1));
-	cpumask_set_cpu(cpu1, cpu_llc_shared_mask(cpu2));
-	cpumask_set_cpu(cpu2, cpu_llc_shared_mask(cpu1));
+	int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
+
+	return !WARN_ONCE(cpu_to_node(cpu1) != cpu_to_node(cpu2),
+		"sched: CPU #%d's %s-sibling CPU #%d is not on the same node! "
+		"[node: %d != %d]. Ignoring dependency.\n",
+		cpu1, name, cpu2, cpu_to_node(cpu1), cpu_to_node(cpu2));
 }
 
+#define link_mask(_m, c1, c2)						\
+do {									\
+	cpumask_set_cpu((c1), cpu_##_m##_mask(c2));			\
+	cpumask_set_cpu((c2), cpu_##_m##_mask(c1));			\
+} while (0)
 
-void __cpuinit set_cpu_sibling_map(int cpu)
+static bool __cpuinit match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 {
-	int i;
-	struct cpuinfo_x86 *c = &cpu_data(cpu);
+	if (cpu_has(c, X86_FEATURE_TOPOEXT)) {
+		int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
 
-	cpumask_set_cpu(cpu, cpu_sibling_setup_mask);
+		if (c->phys_proc_id == o->phys_proc_id &&
+		    per_cpu(cpu_llc_id, cpu1) == per_cpu(cpu_llc_id, cpu2) &&
+		    c->compute_unit_id == o->compute_unit_id)
+			return topology_sane(c, o, "smt");
 
-	if (smp_num_siblings > 1) {
-		for_each_cpu(i, cpu_sibling_setup_mask) {
-			struct cpuinfo_x86 *o = &cpu_data(i);
+	} else if (c->phys_proc_id == o->phys_proc_id &&
+		   c->cpu_core_id == o->cpu_core_id) {
+		return topology_sane(c, o, "smt");
+	}
 
-			if (cpu_to_node(cpu) != cpu_to_node(i)) {
-				WARN_ONCE(1, "sched: CPU #%d's thread-sibling CPU #%d not on the same node! [node %d != %d]. Ignoring sibling dependency.\n", cpu, i, cpu_to_node(cpu), cpu_to_node(i));
-				continue;
-			}
+	return false;
+}
 
-			if (cpu_has(c, X86_FEATURE_TOPOEXT)) {
-				if (c->phys_proc_id == o->phys_proc_id &&
-				    per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, i) &&
-				    c->compute_unit_id == o->compute_unit_id)
-					link_thread_siblings(cpu, i);
-			} else if (c->phys_proc_id == o->phys_proc_id &&
-				   c->cpu_core_id == o->cpu_core_id) {
-				link_thread_siblings(cpu, i);
-			}
-		}
-	} else {
-		cpumask_set_cpu(cpu, cpu_sibling_mask(cpu));
-	}
+static bool __cpuinit match_llc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
+{
+	int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
+
+	if (per_cpu(cpu_llc_id, cpu1) != BAD_APICID &&
+	    per_cpu(cpu_llc_id, cpu1) == per_cpu(cpu_llc_id, cpu2))
+		return topology_sane(c, o, "llc");
+
+	return false;
+}
+
+static bool __cpuinit match_mc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
+{
+	if (c->phys_proc_id == o->phys_proc_id)
+		return topology_sane(c, o, "mc");
+
+	return false;
+}
+
+void __cpuinit set_cpu_sibling_map(int cpu)
+{
+	bool has_mc = boot_cpu_data.x86_max_cores > 1;
+	bool has_smt = smp_num_siblings > 1;
+	struct cpuinfo_x86 *c = &cpu_data(cpu);
+	struct cpuinfo_x86 *o;
+	int i;
 
-	cpumask_set_cpu(cpu, cpu_llc_shared_mask(cpu));
+	cpumask_set_cpu(cpu, cpu_sibling_setup_mask);
 
-	if (__this_cpu_read(cpu_info.x86_max_cores) == 1) {
-		cpumask_copy(cpu_core_mask(cpu), cpu_sibling_mask(cpu));
+	if (!has_smt && !has_mc) {
+		cpumask_set_cpu(cpu, cpu_sibling_mask(cpu));
+		cpumask_set_cpu(cpu, cpu_llc_shared_mask(cpu));
+		cpumask_set_cpu(cpu, cpu_core_mask(cpu));
 		c->booted_cores = 1;
 		return;
 	}
 
 	for_each_cpu(i, cpu_sibling_setup_mask) {
-		if (cpu_to_node(cpu) != cpu_to_node(i)) {
-			WARN_ONCE(1, "sched: CPU #%d's core-sibling CPU #%d not on the same node! [node %d != %d]. Ignoring sibling dependency.\n", cpu, i, cpu_to_node(cpu), cpu_to_node(i));
-			continue;
-		}
+		o = &cpu_data(i);
 
-		if (per_cpu(cpu_llc_id, cpu) != BAD_APICID &&
-		    per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, i)) {
-			cpumask_set_cpu(i, cpu_llc_shared_mask(cpu));
-			cpumask_set_cpu(cpu, cpu_llc_shared_mask(i));
-		}
+		if ((i == cpu) || (has_smt && match_smt(c, o)))
+			link_mask(sibling, cpu, i);
+
+		if ((i == cpu) || (has_mc && match_llc(c, o)))
+			link_mask(llc_shared, cpu, i);
+
+		if ((i == cpu) || (has_mc && match_mc(c, o))) {
+			link_mask(core, cpu, i);
 
-		if (c->phys_proc_id == cpu_data(i).phys_proc_id) {
-			cpumask_set_cpu(i, cpu_core_mask(cpu));
-			cpumask_set_cpu(cpu, cpu_core_mask(i));
 			/*
 			 *  Does this new cpu bringup a new core?
 			 */

commit ad7687dde8780a0d618a3e3b5a62bb383696fc22
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 9 13:31:47 2012 +0200

    x86/numa: Check for nonsensical topologies on real hw as well
    
    Instead of only checking nonsensical topologies on numa-emu, do it
    on real hardware as well, and print a warning.
    
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/n/tip-re15l0jqjtpz709oxozt2zoh@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index edfd03a9e390..7c53d96d44ab 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -337,10 +337,10 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 		for_each_cpu(i, cpu_sibling_setup_mask) {
 			struct cpuinfo_x86 *o = &cpu_data(i);
 
-#ifdef CONFIG_NUMA_EMU
-			if (cpu_to_node(cpu) != cpu_to_node(i))
+			if (cpu_to_node(cpu) != cpu_to_node(i)) {
+				WARN_ONCE(1, "sched: CPU #%d's thread-sibling CPU #%d not on the same node! [node %d != %d]. Ignoring sibling dependency.\n", cpu, i, cpu_to_node(cpu), cpu_to_node(i));
 				continue;
-#endif
+			}
 
 			if (cpu_has(c, X86_FEATURE_TOPOEXT)) {
 				if (c->phys_proc_id == o->phys_proc_id &&
@@ -365,10 +365,10 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 	}
 
 	for_each_cpu(i, cpu_sibling_setup_mask) {
-#ifdef CONFIG_NUMA_EMU
-		if (cpu_to_node(cpu) != cpu_to_node(i))
+		if (cpu_to_node(cpu) != cpu_to_node(i)) {
+			WARN_ONCE(1, "sched: CPU #%d's core-sibling CPU #%d not on the same node! [node %d != %d]. Ignoring sibling dependency.\n", cpu, i, cpu_to_node(cpu), cpu_to_node(i));
 			continue;
-#endif
+		}
 
 		if (per_cpu(cpu_llc_id, cpu) != BAD_APICID &&
 		    per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, i)) {

commit 0acbb440f06302058e1515861dd534594521e892
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Apr 18 19:04:09 2012 +0200

    x86/numa: Hard partition cpu topology masks on node boundaries
    
    When using numa=fake= you can get weird topologies where LLCs can span
    nodes and other such nonsense. Cure this by hard partitioning these
    masks on node boundaries.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/n/tip-di5vwjm96q5vrb76opwuflwx@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6e1e406038c2..edfd03a9e390 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -337,6 +337,11 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 		for_each_cpu(i, cpu_sibling_setup_mask) {
 			struct cpuinfo_x86 *o = &cpu_data(i);
 
+#ifdef CONFIG_NUMA_EMU
+			if (cpu_to_node(cpu) != cpu_to_node(i))
+				continue;
+#endif
+
 			if (cpu_has(c, X86_FEATURE_TOPOEXT)) {
 				if (c->phys_proc_id == o->phys_proc_id &&
 				    per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, i) &&
@@ -360,11 +365,17 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 	}
 
 	for_each_cpu(i, cpu_sibling_setup_mask) {
+#ifdef CONFIG_NUMA_EMU
+		if (cpu_to_node(cpu) != cpu_to_node(i))
+			continue;
+#endif
+
 		if (per_cpu(cpu_llc_id, cpu) != BAD_APICID &&
 		    per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, i)) {
 			cpumask_set_cpu(i, cpu_llc_shared_mask(cpu));
 			cpumask_set_cpu(cpu, cpu_llc_shared_mask(i));
 		}
+
 		if (c->phys_proc_id == cpu_data(i).phys_proc_id) {
 			cpumask_set_cpu(i, cpu_core_mask(cpu));
 			cpumask_set_cpu(cpu, cpu_core_mask(i));

commit f37240f16bec91f15ce564515f70a6ca9715ce96
Author: Jarkko Sakkinen <jarkko.sakkinen@intel.com>
Date:   Tue May 8 21:22:43 2012 +0300

    x86, realmode: header for trampoline code
    
    Added header for trampoline code that can be used to supply
    input data to it. This makes interface between real mode code
    and kernel cleaner and simpler. Replaced two confusing pointers
    to level4 pgt in trampoline_64.S with a single pointer to the
    beginning of the page table.
    
    Signed-off-by: Jarkko Sakkinen <jarkko.sakkinen@intel.com>
    Link: http://lkml.kernel.org/r/1336501366-28617-21-git-send-email-jarkko.sakkinen@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b8c0661e2341..757c4b1d0a02 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -667,7 +667,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	volatile u32 *trampoline_status =
 		(volatile u32 *) __va(real_mode_header->trampoline_status);
 	/* start_ip had better be page-aligned! */
-	unsigned long start_ip = real_mode_header->trampoline_data;
+	unsigned long start_ip = real_mode_header->trampoline_start;
 
 	unsigned long boot_error = 0;
 	int timeout;

commit b429dbf6e866bd6dadb56fae66f61f611cde57ff
Author: Jarkko Sakkinen <jarkko.sakkinen@intel.com>
Date:   Tue May 8 21:22:41 2012 +0300

    x86, realmode: don't copy real_mode_header
    
    Replaced copying of real_mode_header with a pointer
    to beginning of RM memory.
    
    Signed-off-by: Jarkko Sakkinen <jarkko.sakkinen@intel.com>
    Link: http://lkml.kernel.org/r/1336501366-28617-19-git-send-email-jarkko.sakkinen@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c7971ea74bd0..b8c0661e2341 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -665,9 +665,9 @@ static void __cpuinit announce_cpu(int cpu, int apicid)
 static int __cpuinit do_boot_cpu(int apicid, int cpu)
 {
 	volatile u32 *trampoline_status =
-		(volatile u32 *) __va(real_mode_header.trampoline_status);
+		(volatile u32 *) __va(real_mode_header->trampoline_status);
 	/* start_ip had better be page-aligned! */
-	unsigned long start_ip = real_mode_header.trampoline_data;
+	unsigned long start_ip = real_mode_header->trampoline_data;
 
 	unsigned long boot_error = 0;
 	int timeout;

commit 48927bbb97c7d4cf343c05827ab9ac30c60678cb
Author: Jarkko Sakkinen <jarkko.sakkinen@intel.com>
Date:   Tue May 8 21:22:28 2012 +0300

    x86, realmode: Move SMP trampoline to unified realmode code
    
    Migrated SMP trampoline code to the real mode blob.
    SMP trampoline code is not yet removed from
    .x86_trampoline because it is needed by the wakeup
    code.
    
    [ hpa: always enable compiling startup_32_smp in head_32.S... it is
      only a few instructions which go into .init on UP builds, and it makes
      the rest of the code less #ifdef ugly. ]
    
    Signed-off-by: Jarkko Sakkinen <jarkko.sakkinen@intel.com>
    Link: http://lkml.kernel.org/r/1336501366-28617-6-git-send-email-jarkko.sakkinen@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6e1e406038c2..c7971ea74bd0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -57,7 +57,7 @@
 #include <asm/nmi.h>
 #include <asm/irq.h>
 #include <asm/idle.h>
-#include <asm/trampoline.h>
+#include <asm/realmode.h>
 #include <asm/cpu.h>
 #include <asm/numa.h>
 #include <asm/pgtable.h>
@@ -73,6 +73,8 @@
 #include <asm/smpboot_hooks.h>
 #include <asm/i8259.h>
 
+#include <asm/realmode.h>
+
 /* State of each CPU */
 DEFINE_PER_CPU(int, cpu_state) = { 0 };
 
@@ -662,8 +664,12 @@ static void __cpuinit announce_cpu(int cpu, int apicid)
  */
 static int __cpuinit do_boot_cpu(int apicid, int cpu)
 {
+	volatile u32 *trampoline_status =
+		(volatile u32 *) __va(real_mode_header.trampoline_status);
+	/* start_ip had better be page-aligned! */
+	unsigned long start_ip = real_mode_header.trampoline_data;
+
 	unsigned long boot_error = 0;
-	unsigned long start_ip;
 	int timeout;
 	struct create_idle c_idle = {
 		.cpu	= cpu,
@@ -713,9 +719,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	initial_code = (unsigned long)start_secondary;
 	stack_start  = c_idle.idle->thread.sp;
 
-	/* start_ip had better be page-aligned! */
-	start_ip = trampoline_address();
-
 	/* So we see what's up */
 	announce_cpu(cpu, apicid);
 
@@ -778,8 +781,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 			pr_debug("CPU%d: has booted.\n", cpu);
 		} else {
 			boot_error = 1;
-			if (*(volatile u32 *)TRAMPOLINE_SYM(trampoline_status)
-			    == 0xA5A5A5A5)
+			if (*trampoline_status == 0xA5A5A5A5)
 				/* trampoline started but...? */
 				pr_err("CPU%d: Stuck ??\n", cpu);
 			else
@@ -805,7 +807,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	}
 
 	/* mark "stuck" area as not stuck */
-	*(volatile u32 *)TRAMPOLINE_SYM(trampoline_status) = 0;
+	*trampoline_status = 0;
 
 	if (get_uv_system_type() != UV_NON_UNIQUE_APIC) {
 		/*

commit 7eb43a6d232bfa46464b501cd1987ec2d705d8cf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:48 2012 +0000

    x86: Use generic idle thread allocation
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20120420124557.246929343@linutronix.de

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index def235bf7594..3acaf51dfddb 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -76,19 +76,7 @@
 /* State of each CPU */
 DEFINE_PER_CPU(int, cpu_state) = { 0 };
 
-/* Store all idle threads, this can be reused instead of creating
-* a new thread. Also avoids complicated thread destroy functionality
-* for idle threads.
-*/
 #ifdef CONFIG_HOTPLUG_CPU
-/*
- * Needed only for CONFIG_HOTPLUG_CPU because __cpuinitdata is
- * removed after init for !CONFIG_HOTPLUG_CPU.
- */
-static DEFINE_PER_CPU(struct task_struct *, idle_thread_array);
-#define get_idle_for_cpu(x)      (per_cpu(idle_thread_array, x))
-#define set_idle_for_cpu(x, p)   (per_cpu(idle_thread_array, x) = (p))
-
 /*
  * We need this for trampoline_base protection from concurrent accesses when
  * off- and onlining cores wildly.
@@ -97,20 +85,16 @@ static DEFINE_MUTEX(x86_cpu_hotplug_driver_mutex);
 
 void cpu_hotplug_driver_lock(void)
 {
-        mutex_lock(&x86_cpu_hotplug_driver_mutex);
+	mutex_lock(&x86_cpu_hotplug_driver_mutex);
 }
 
 void cpu_hotplug_driver_unlock(void)
 {
-        mutex_unlock(&x86_cpu_hotplug_driver_mutex);
+	mutex_unlock(&x86_cpu_hotplug_driver_mutex);
 }
 
 ssize_t arch_cpu_probe(const char *buf, size_t count) { return -1; }
 ssize_t arch_cpu_release(const char *buf, size_t count) { return -1; }
-#else
-static struct task_struct *idle_thread_array[NR_CPUS] __cpuinitdata ;
-#define get_idle_for_cpu(x)      (idle_thread_array[(x)])
-#define set_idle_for_cpu(x, p)   (idle_thread_array[(x)] = (p))
 #endif
 
 /* Number of siblings per CPU package */
@@ -618,22 +602,6 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	return (send_status | accept_status);
 }
 
-struct create_idle {
-	struct work_struct work;
-	struct task_struct *idle;
-	struct completion done;
-	int cpu;
-};
-
-static void __cpuinit do_fork_idle(struct work_struct *work)
-{
-	struct create_idle *c_idle =
-		container_of(work, struct create_idle, work);
-
-	c_idle->idle = fork_idle(c_idle->cpu);
-	complete(&c_idle->done);
-}
-
 /* reduce the number of lines printed when booting a large cpu count system */
 static void __cpuinit announce_cpu(int cpu, int apicid)
 {
@@ -660,58 +628,31 @@ static void __cpuinit announce_cpu(int cpu, int apicid)
  * Returns zero if CPU booted OK, else error code from
  * ->wakeup_secondary_cpu.
  */
-static int __cpuinit do_boot_cpu(int apicid, int cpu)
+static int __cpuinit do_boot_cpu(int apicid, int cpu, struct task_struct *idle)
 {
 	unsigned long boot_error = 0;
 	unsigned long start_ip;
 	int timeout;
-	struct create_idle c_idle = {
-		.cpu	= cpu,
-		.done	= COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
-	};
-
-	INIT_WORK_ONSTACK(&c_idle.work, do_fork_idle);
 
 	alternatives_smp_switch(1);
 
-	c_idle.idle = get_idle_for_cpu(cpu);
-
-	/*
-	 * We can't use kernel_thread since we must avoid to
-	 * reschedule the child.
-	 */
-	if (c_idle.idle) {
-		c_idle.idle->thread.sp = (unsigned long) (((struct pt_regs *)
-			(THREAD_SIZE +  task_stack_page(c_idle.idle))) - 1);
-		init_idle(c_idle.idle, cpu);
-		goto do_rest;
-	}
+	idle->thread.sp = (unsigned long) (((struct pt_regs *)
+			  (THREAD_SIZE +  task_stack_page(idle))) - 1);
+	per_cpu(current_task, cpu) = idle;
 
-	schedule_work(&c_idle.work);
-	wait_for_completion(&c_idle.done);
-
-	if (IS_ERR(c_idle.idle)) {
-		printk("failed fork for CPU %d\n", cpu);
-		destroy_work_on_stack(&c_idle.work);
-		return PTR_ERR(c_idle.idle);
-	}
-
-	set_idle_for_cpu(cpu, c_idle.idle);
-do_rest:
-	per_cpu(current_task, cpu) = c_idle.idle;
 #ifdef CONFIG_X86_32
 	/* Stack for startup_32 can be just as for start_secondary onwards */
 	irq_ctx_init(cpu);
 #else
-	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
+	clear_tsk_thread_flag(idle, TIF_FORK);
 	initial_gs = per_cpu_offset(cpu);
 	per_cpu(kernel_stack, cpu) =
-		(unsigned long)task_stack_page(c_idle.idle) -
+		(unsigned long)task_stack_page(idle) -
 		KERNEL_STACK_OFFSET + THREAD_SIZE;
 #endif
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	initial_code = (unsigned long)start_secondary;
-	stack_start  = c_idle.idle->thread.sp;
+	stack_start  = idle->thread.sp;
 
 	/* start_ip had better be page-aligned! */
 	start_ip = trampoline_address();
@@ -813,8 +754,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		 */
 		smpboot_restore_warm_reset_vector();
 	}
-
-	destroy_work_on_stack(&c_idle.work);
 	return boot_error;
 }
 
@@ -851,7 +790,7 @@ int __cpuinit native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 
 	per_cpu(cpu_state, cpu) = CPU_UP_PREPARE;
 
-	err = do_boot_cpu(apicid, cpu);
+	err = do_boot_cpu(apicid, cpu, tidle);
 	if (err) {
 		pr_debug("do_boot_cpu failed %d\n", err);
 		return -EIO;

commit 5cdaf1834f43b0edc4a3aa683aa4ec98f6bfe8a7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:47 2012 +0000

    x86: Add task_struct argument to smp_ops.cpu_up
    
    Preparatory patch to use the generic idle thread allocation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20120420124557.176604405@linutronix.de

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6e1e406038c2..def235bf7594 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -818,7 +818,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	return boot_error;
 }
 
-int __cpuinit native_cpu_up(unsigned int cpu)
+int __cpuinit native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
 	int apicid = apic->cpu_present_to_apicid(cpu);
 	unsigned long flags;

commit a335750b9a039a9d4cd727cdccacfb90fd63c4e8
Merge: 10f3cb41d48a d326f44e5f22
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 30 16:45:38 2012 -0700

    Merge branch 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux
    
    Pull ACPI & Power Management changes from Len Brown:
     - ACPI 5.0 after-ripples, ACPICA/Linux divergence cleanup
     - cpuidle evolving, more ARM use
     - thermal sub-system evolving, ditto
     - assorted other PM bits
    
    Fix up conflicts in various cpuidle implementations due to ARM cpuidle
    cleanups (ARM at91 self-refresh and cpu idle code rewritten into
    "standby" in asm conflicting with the consolidation of cpuidle time
    keeping), trivial SH include file context conflict and RCU tracing fixes
    in generic code.
    
    * 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux: (77 commits)
      ACPI throttling: fix endian bug in acpi_read_throttling_status()
      Disable MCP limit exceeded messages from Intel IPS driver
      ACPI video: Don't start video device until its associated input device has been allocated
      ACPI video: Harden video bus adding.
      ACPI: Add support for exposing BGRT data
      ACPI: export acpi_kobj
      ACPI: Fix logic for removing mappings in 'acpi_unmap'
      CPER failed to handle generic error records with multiple sections
      ACPI: Clean redundant codes in scan.c
      ACPI: Fix unprotected smp_processor_id() in acpi_processor_cst_has_changed()
      ACPI: consistently use should_use_kmap()
      PNPACPI: Fix device ref leaking in acpi_pnp_match
      ACPI: Fix use-after-free in acpi_map_lsapic
      ACPI: processor_driver: add missing kfree
      ACPI, APEI: Fix incorrect APEI register bit width check and usage
      Update documentation for parameter *notrigger* in einj.txt
      ACPI, APEI, EINJ, new parameter to control trigger action
      ACPI, APEI, EINJ, limit the range of einj_param
      ACPI, APEI, Fix ERST header length check
      cpuidle: power_usage should be declared signed integer
      ...

commit 1a022e3f1be11730bd8747b1af96a0274bf6356e
Author: Boris Ostrovsky <boris.ostrovsky@amd.com>
Date:   Tue Mar 13 19:55:09 2012 +0100

    idle, x86: Allow off-lined CPU to enter deeper C states
    
    Currently when a CPU is off-lined it enters either MWAIT-based idle or,
    if MWAIT is not desired or supported, HLT-based idle (which places the
    processor in C1 state). This patch allows processors without MWAIT
    support to stay in states deeper than C1.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@amd.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 66d250c00d11..93a2a0932b51 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -50,6 +50,7 @@
 #include <linux/tboot.h>
 #include <linux/stackprotector.h>
 #include <linux/gfp.h>
+#include <linux/cpuidle.h>
 
 #include <asm/acpi.h>
 #include <asm/desc.h>
@@ -1422,7 +1423,8 @@ void native_play_dead(void)
 	tboot_shutdown(TB_SHUTDOWN_WFS);
 
 	mwait_play_dead();	/* Only returns on failure */
-	hlt_play_dead();
+	if (cpuidle_play_dead())
+		hlt_play_dead();
 }
 
 #else /* ... !CONFIG_HOTPLUG_CPU */

commit 7fda0412c5f7afdd1a5ff518f98dee5157266d8a
Merge: 6b8212a313da 160594e99dbb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 14:46:05 2012 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar.
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpusets: Remove an unused variable
      sched/rt: Improve pick_next_highest_task_rt()
      sched: Fix select_fallback_rq() vs cpu_active/cpu_online
      sched/x86/smp: Do not enable IRQs over calibrate_delay()
      sched: Fix compiler warning about declared inline after use
      MAINTAINERS: Update email address for SCHEDULER and PERF EVENTS

commit 2e7580b0e75d771d93e24e681031a165b1d31071
Merge: d25413efa953 cf9eeac46350
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 14:35:31 2012 -0700

    Merge branch 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Avi Kivity:
     "Changes include timekeeping improvements, support for assigning host
      PCI devices that share interrupt lines, s390 user-controlled guests, a
      large ppc update, and random fixes."
    
    This is with the sign-off's fixed, hopefully next merge window we won't
    have rebased commits.
    
    * 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (130 commits)
      KVM: Convert intx_mask_lock to spin lock
      KVM: x86: fix kvm_write_tsc() TSC matching thinko
      x86: kvmclock: abstract save/restore sched_clock_state
      KVM: nVMX: Fix erroneous exception bitmap check
      KVM: Ignore the writes to MSR_K7_HWCR(3)
      KVM: MMU: make use of ->root_level in reset_rsvds_bits_mask
      KVM: PMU: add proper support for fixed counter 2
      KVM: PMU: Fix raw event check
      KVM: PMU: warn when pin control is set in eventsel msr
      KVM: VMX: Fix delayed load of shared MSRs
      KVM: use correct tlbs dirty type in cmpxchg
      KVM: Allow host IRQ sharing for assigned PCI 2.3 devices
      KVM: Ensure all vcpus are consistent with in-kernel irqchip settings
      KVM: x86 emulator: Allow PM/VM86 switch during task switch
      KVM: SVM: Fix CPL updates
      KVM: x86 emulator: VM86 segments must have DPL 3
      KVM: x86 emulator: Fix task switch privilege checks
      arch/powerpc/kvm/book3s_hv.c: included linux/sched.h twice
      KVM: x86 emulator: correctly mask pmc index bits in RDPMC instruction emulation
      KVM: mmu_notifier: Flush TLBs before releasing mmu_lock
      ...

commit bc758133ed73d4b06952bec21da23e28e62bf3ba
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Mar 26 13:16:15 2012 +0200

    sched/x86/smp: Do not enable IRQs over calibrate_delay()
    
    We should not ever enable IRQs until we're fully set up. This opens up
    a window where interrupts can hit the cpu and interrupts can do
    wakeups, wakeups need state that isn't set-up yet, in particular this
    cpu isn't elegible to run tasks, so if any cpu-affine task that got
    created in CPU_UP_PREPARE manages to get a wakeup, its affinity mask
    will get broken and we'll run into lots of 'interesting' problems.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-yaezmlbriluh166tfkgni22m@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 58f78165d308..89571a0c4a49 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -219,14 +219,9 @@ static void __cpuinit smp_callin(void)
 	 * Update loops_per_jiffy in cpu_data. Previous call to
 	 * smp_store_cpu_info() stored a value that is close but not as
 	 * accurate as the value just calculated.
-	 *
-	 * Need to enable IRQs because it can take longer and then
-	 * the NMI watchdog might kill us.
 	 */
-	local_irq_enable();
 	calibrate_delay();
 	cpu_data(cpuid).loops_per_jiffy = loops_per_jiffy;
-	local_irq_disable();
 	pr_debug("Stack at about %p\n", &cpuid);
 
 	/*

commit 23904815461ba223a8baf7490051597fa054299b
Merge: 754b98007794 fa63030e9c79
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 09:42:36 2012 -0700

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 platform changes from Ingo Molnar.
    
    Removes the Moorestown platform that nobody ever used.
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/platform: Move APIC ID validity check into platform APIC code
      x86/olpc/xo15/sci: Enable lid close wakeup control
      x86/geode/net5501: Add platform driver for Soekris Engineering net5501
      x86/geode/alix2: Supplement driver to include GPIO button support
      x86/mid/powerbtn: Use MSIC read/write instead of ipc_scu
      x86/mid/thermal: Turn off thermistor
      x86/mid/thermal: Add msic_thermal alias
      x86/mid/thermal: Convert to use Intel MSIC API
      x86/mid/scu_ipc: Remove Moorestown support
      x86/mid: Kill off Moorestown
      x86/mrst: Add msic_thermal platform support
      x86/config: Select MSIC MFD driver on Intel Medfield platform
      x86/mid: Remove Intel Moorestown
      x86/mrst: Set ISA bus type for fake MP IRQs
      x86/ioapic: Use legacy_pic to set correct gsi-irq mapping

commit 4c64616bb51b399886ded8f4f69bad4da2da1817
Merge: c5c7fb8fbd7c 943bc7e110f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 09:30:39 2012 -0700

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/debug changes from Ingo Molnar.
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Fix section warnings
      x86-64: Fix CFI data for common_interrupt()
      x86: Properly _init-annotate NMI selftest code
      x86/debug: Fix/improve the show_msr=<cpus> debug print out

commit c5c7fb8fbd7cd228132b6e2a17a10f246ffc06ee
Merge: 1b674bf106f5 140f190bc3a3 35f1790e6c6a 513c4ec6e475 42dfc43ee599 b0deca2e0270
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 09:28:15 2012 -0700

    Merge branches 'x86-cpu-for-linus', 'x86-boot-for-linus', 'x86-cpufeature-for-linus', 'x86-process-for-linus' and 'x86-uv-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull trivial x86 branches from Ingo Molnar: small one-liners to fix up
    details.
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Remove some noise from boot log when starting cpus
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, boot: Fix port argument to inl() function
    
    * 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, cpufeature: Add CPU features from Intel document 319433-012A
    
    * 'x86-process-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86_64: Record stack pointer before task execution begins
    
    * 'x86-uv-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/UV: Lower UV rtc clocksource rating

commit fa63030e9c79e37b4d4e63b39ffb09cfb7aa0fe4
Author: Daniel J Blueman <daniel@numascale-asia.com>
Date:   Wed Mar 14 15:17:34 2012 +0800

    x86/platform: Move APIC ID validity check into platform APIC code
    
    Move APIC ID validity check into platform APIC code, so it can
    be overridden when needed. For NumaChip systems, always trust
    MADT, as it's constructed with high APIC IDs.
    
    Behaviour verifies on standard x86 systems and on NumaChip
    systems with this, and compile-tested with allyesconfig.
    
    Signed-off-by: Daniel J Blueman <daniel@numascale-asia.com>
    Reviewed-by: Steffen Persvold <sp@numascale.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1331709454-27966-1-git-send-email-daniel@numascale-asia.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 66d250c00d11..d279e6e1d1b7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -847,7 +847,7 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 
 	if (apicid == BAD_APICID || apicid == boot_cpu_physical_apicid ||
 	    !physid_isset(apicid, phys_cpu_present_map) ||
-	    (!x2apic_mode && apicid >= 255)) {
+	    !apic->apic_id_valid(apicid)) {
 		printk(KERN_ERR "%s: bad cpu %d\n", __func__, cpu);
 		return -EINVAL;
 	}

commit 5fbd036b552f633abb394a319f7c62a5c86a9cd7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Dec 15 17:09:22 2011 +0100

    sched: Cleanup cpu_active madness
    
    Stepan found:
    
    CPU0            CPUn
    
    _cpu_up()
      __cpu_up()
    
                    boostrap()
                      notify_cpu_starting()
                      set_cpu_online()
                      while (!cpu_active())
                        cpu_relax()
    
    <PREEMPT-out>
    
    smp_call_function(.wait=1)
      /* we find cpu_online() is true */
      arch_send_call_function_ipi_mask()
    
      /* wait-forever-more */
    
    <PREEMPT-in>
                      local_irq_enable()
    
      cpu_notify(CPU_ONLINE)
        sched_cpu_active()
          set_cpu_active()
    
    Now the purpose of cpu_active is mostly with bringing down a cpu, where
    we mark it !active to avoid the load-balancer from moving tasks to it
    while we tear down the cpu. This is required because we only update the
    sched_domain tree after we brought the cpu-down. And this is needed so
    that some tasks can still run while we bring it down, we just don't want
    new tasks to appear.
    
    On cpu-up however the sched_domain tree doesn't yet include the new cpu,
    so its invisible to the load-balancer, regardless of the active state.
    So instead of setting the active state after we boot the new cpu (and
    consequently having to wait for it before enabling interrupts) set the
    cpu active before we set it online and avoid the whole mess.
    
    Reported-by: Stepan Moskovchenko <stepanm@codeaurora.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1323965362.18942.71.camel@twins
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 66d250c00d11..58f78165d308 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -291,19 +291,6 @@ notrace static void __cpuinit start_secondary(void *unused)
 	per_cpu(cpu_state, smp_processor_id()) = CPU_ONLINE;
 	x86_platform.nmi_init();
 
-	/*
-	 * Wait until the cpu which brought this one up marked it
-	 * online before enabling interrupts. If we don't do that then
-	 * we can end up waking up the softirq thread before this cpu
-	 * reached the active state, which makes the scheduler unhappy
-	 * and schedule the softirq thread on the wrong cpu. This is
-	 * only observable with forced threaded interrupts, but in
-	 * theory it could also happen w/o them. It's just way harder
-	 * to achieve.
-	 */
-	while (!cpumask_test_cpu(smp_processor_id(), cpu_active_mask))
-		cpu_relax();
-
 	/* enable local interrupts */
 	local_irq_enable();
 

commit df156f90a0f90649dd38b7667901ef85478f3d2b
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Tue Feb 7 15:52:44 2012 +0100

    x86: Introduce x86_cpuinit.early_percpu_clock_init hook
    
    When kvm guest uses kvmclock, it may hang on vcpu hot-plug.
    This is caused by an overflow in pvclock_get_nsec_offset,
    
        u64 delta = tsc - shadow->tsc_timestamp;
    
    which in turn is caused by an undefined values from percpu
    hv_clock that hasn't been initialized yet.
    Uninitialized clock on being booted cpu is accessed from
       start_secondary
        -> smp_callin
          ->  smp_store_cpu_info
            -> identify_secondary_cpu
              -> mtrr_ap_init
                -> mtrr_restore
                  -> stop_machine_from_inactive_cpu
                    -> queue_stop_cpus_work
                      ...
                        -> sched_clock
                          -> kvm_clock_read
    which is well before x86_cpuinit.setup_percpu_clockev call in
    start_secondary, where percpu clock is initialized.
    
    This patch introduces a hook that allows to setup/initialize
    per_cpu clock early and avoid overflow due to reading
      - undefined values
      - old values if cpu was offlined and then onlined again
    
    Another possible early user of this clock source is ftrace that
    accesses it to get timestamps for ring buffer entries. So if
    mtrr_ap_init is moved from identify_secondary_cpu to past
    x86_cpuinit.setup_percpu_clockev in start_secondary, ftrace
    may cause the same overflow/hang on cpu hot-plug anyway.
    
    More complete description of the problem:
      https://lkml.org/lkml/2012/2/2/101
    
    Credits to Marcelo Tosatti <mtosatti@redhat.com> for hook idea.
    
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 66d250c00d11..a05d6fd5e06d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -255,6 +255,7 @@ notrace static void __cpuinit start_secondary(void *unused)
 	 * most necessary things.
 	 */
 	cpu_init();
+	x86_cpuinit.early_percpu_clock_init();
 	preempt_disable();
 	smp_callin();
 

commit 140f190bc3a3b6f200548d204befd998eadd63fd
Author: Luck, Tony <tony.luck@intel.com>
Date:   Wed Feb 22 10:06:44 2012 -0800

    x86: Remove some noise from boot log when starting cpus
    
    Printing the "start_ip" for every secondary cpu is very noisy on a large
    system - and doesn't add any value. Drop this message.
    
    Console log before:
    Booting Node   0, Processors  #1
    smpboot cpu 1: start_ip = 96000
     #2
    smpboot cpu 2: start_ip = 96000
     #3
    smpboot cpu 3: start_ip = 96000
     #4
    smpboot cpu 4: start_ip = 96000
           ...
     #31
    smpboot cpu 31: start_ip = 96000
    Brought up 32 CPUs
    
    Console log after:
    Booting Node   0, Processors  #1 #2 #3 #4 #5 #6 #7 Ok.
    Booting Node   1, Processors  #8 #9 #10 #11 #12 #13 #14 #15 Ok.
    Booting Node   0, Processors  #16 #17 #18 #19 #20 #21 #22 #23 Ok.
    Booting Node   1, Processors  #24 #25 #26 #27 #28 #29 #30 #31
    Brought up 32 CPUs
    
    Acked-by: Borislav Petkov <bp@amd64.org>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Link: http://lkml.kernel.org/r/4f452eb42507460426@agluck-desktop.sc.intel.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 66d250c00d11..683575250a65 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -740,8 +740,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	 * the targeted processor.
 	 */
 
-	printk(KERN_DEBUG "smpboot cpu %d: start_ip = %lx\n", cpu, start_ip);
-
 	atomic_set(&init_deasserted, 0);
 
 	if (get_uv_system_type() != UV_NON_UNIQUE_APIC) {

commit 21c3fcf3e39353d4f21d50e257cc74f3204b1988
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sun Feb 12 09:53:57 2012 -0800

    x86/debug: Fix/improve the show_msr=<cpus> debug print out
    
    Found out that show_msr=<cpus> is broken, when I asked a
    user to use it to capture debug info about broken MTRR's
    whose MTRR settings are probably different between CPUs.
    
    Only the first CPUs MSRs are printed, but that is not
    enough to track down the suspected bug.
    
    For years we called print_cpu_msr from print_cpu_info(),
    but this commit:
    
    | commit 2eaad1fddd7450a48ad464229775f97fbfe8af36
    | Author: Mike Travis <travis@sgi.com>
    | Date:   Thu Dec 10 17:19:36 2009 -0800
    |
    |    x86: Limit the number of processor bootup messages
    
    removed the print_cpu_info() call from all APs.
    
    Put it back - it will only print MSRs when the user
    specifically requests them via show_msr=<cpus>.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Mike Travis <travis@sgi.com>
    Link: http://lkml.kernel.org/r/1329069237-11483-1-git-send-email-yinghai@kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 66d250c00d11..257049d7c657 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -791,9 +791,10 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 			schedule();
 		}
 
-		if (cpumask_test_cpu(cpu, cpu_callin_mask))
+		if (cpumask_test_cpu(cpu, cpu_callin_mask)) {
+			print_cpu_msr(&cpu_data(cpu));
 			pr_debug("CPU%d: has booted.\n", cpu);
-		else {
+		} else {
 			boot_error = 1;
 			if (*(volatile u32 *)TRAMPOLINE_SYM(trampoline_status)
 			    == 0xA5A5A5A5)

commit 9fc5c3e3237e02a94f41cd1d2b4291593d29791d
Merge: 541048a1d313 7c9c3a1e5fc8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 11 19:13:40 2012 -0800

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/intel config: Fix the APB_TIMER selection
      x86/mrst: Add additional debug prints for pb_keys
      x86/intel config: Revamp configuration to allow for Moorestown and Medfield
      x86/intel/scu/ipc: Match the changes in the x86 configuration
      x86/apb: Fix configuration constraints
      x86: Fix INTEL_MID silly
      x86/Kconfig: Cyclone-timer depends on x86-summit
      x86: Reduce clock calibration time during slave cpu startup
      x86/config: Revamp configuration for MID devices
      x86/sfi: Kill the IRQ as id hack

commit 541048a1d31399ccdda27346a37eae4a2ad55186
Merge: bcede2f64a3b e58d42920910
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 11 19:13:04 2012 -0800

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, reboot: Fix typo in nmi reboot path
      x86, NMI: Add to_cpumask() to silence compile warning
      x86, NMI: NMI selftest depends on the local apic
      x86: Add stack top margin for stack overflow checking
      x86, NMI: NMI-selftest should handle the UP case properly
      x86: Fix the 32-bit stackoverflow-debug build
      x86, NMI: Add knob to disable using NMI IPIs to stop cpus
      x86, NMI: Add NMI IPI selftest
      x86, reboot: Use NMI instead of REBOOT_VECTOR to stop cpus
      x86: Clean up the range of stack overflow checking
      x86: Panic on detection of stack overflow
      x86: Check stack overflow in detail

commit c284b42abadbb22083bfde24d308899c08d44ffa
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed Dec 21 17:45:19 2011 -0800

    x86: Skip cpus with apic-ids >= 255 in !x2apic_mode
    
    If the x2apic mode is disabled for reasons like interrupt-remapping
    not available etc, then we need to skip the logical cpu bringup of
    apic-id's >= 255. Otherwise as the platform is in xapic mode, init/startup
    IPI's will consider only the low 8-bits and there is a possibility of
    re-sending init/startup IPI's to the logical cpu that is already online.
    
    This will avoid potential reboots/unpredictable behavior etc.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/20111222014632.702932458@sbsiddha-desk.sc.intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9f548cb4a958..e38e21754eea 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -840,7 +840,8 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 	pr_debug("++++++++++++++++++++=_---CPU UP  %u\n", cpu);
 
 	if (apicid == BAD_APICID || apicid == boot_cpu_physical_apicid ||
-	    !physid_isset(apicid, phys_cpu_present_map)) {
+	    !physid_isset(apicid, phys_cpu_present_map) ||
+	    (!x2apic_mode && apicid >= 255)) {
 		printk(KERN_ERR "%s: bad cpu %d\n", __func__, cpu);
 		return -EINVAL;
 	}

commit b565201cf75210614903ef2ae5917b4379681647
Author: Jack Steiner <steiner@sgi.com>
Date:   Tue Nov 15 15:33:56 2011 -0800

    x86: Reduce clock calibration time during slave cpu startup
    
    Reduce the startup time for slave cpus.
    
    Adds hooks for an arch-specific function for clock calibration.
    These hooks are used on x86.  If a newly started cpu has the
    same phys_proc_id as a core already active, uses the TSC for the
    delay loop and has a CONSTANT_TSC, use the already-calculated
    value of loops_per_jiffy.
    
    This patch reduces the time required to start slave cpus on a
    4096 cpu system from: 465 sec OLD 62 sec NEW
    
    This reduces boot time on a 4096p system by almost 7 minutes.
    Nice...
    
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: John Stultz <john.stultz@linaro.org>
    [fix CONFIG_SMP=n build]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9f548cb4a958..00eef55c8327 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -207,22 +207,28 @@ static void __cpuinit smp_callin(void)
 	 * Need to setup vector mappings before we enable interrupts.
 	 */
 	setup_vector_irq(smp_processor_id());
+
+	/*
+	 * Save our processor parameters. Note: this information
+	 * is needed for clock calibration.
+	 */
+	smp_store_cpu_info(cpuid);
+
 	/*
 	 * Get our bogomips.
+	 * Update loops_per_jiffy in cpu_data. Previous call to
+	 * smp_store_cpu_info() stored a value that is close but not as
+	 * accurate as the value just calculated.
 	 *
 	 * Need to enable IRQs because it can take longer and then
 	 * the NMI watchdog might kill us.
 	 */
 	local_irq_enable();
 	calibrate_delay();
+	cpu_data(cpuid).loops_per_jiffy = loops_per_jiffy;
 	local_irq_disable();
 	pr_debug("Stack at about %p\n", &cpuid);
 
-	/*
-	 * Save our processor parameters
-	 */
-	smp_store_cpu_info(cpuid);
-
 	/*
 	 * This must be done before setting cpu_online_mask
 	 * or calling notify_cpu_starting.

commit 99e8b9ca90d688c3ac7d3a141b701c9694a93925
Author: Don Zickus <dzickus@redhat.com>
Date:   Thu Oct 13 15:14:26 2011 -0400

    x86, NMI: Add NMI IPI selftest
    
    The previous patch modified the stop cpus path to use NMI
    instead of IRQ as the way to communicate to the other cpus to
    shutdown.  There were some concerns that various machines may
    have problems with using an NMI IPI.
    
    This patch creates a selftest to check if NMI is working at
    boot. The idea is to help catch any issues before the machine
    panics and we learn the hard way.
    
    Loosely based on the locking-selftest.c file, this separate file
    runs a couple of simple tests and reports the results.  The
    output looks like:
    
    ...
    Brought up 4 CPUs
    ----------------
    | NMI testsuite:
    --------------------
      remote IPI:  ok  |
       local IPI:  ok  |
    --------------------
    Good, all   2 testcases passed! |
    ---------------------------------
    Total of 4 processors activated (21330.61 BogoMIPS).
    ...
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: seiji.aguchi@hds.com
    Cc: vgoyal@redhat.com
    Cc: mjg@redhat.com
    Cc: tony.luck@intel.com
    Cc: gong.chen@intel.com
    Cc: satoru.moriya@hds.com
    Cc: avi@redhat.com
    Cc: Andi Kleen <andi@firstfloor.org>
    Link: http://lkml.kernel.org/r/1318533267-18880-3-git-send-email-dzickus@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9f548cb4a958..19277817effa 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1142,6 +1142,7 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 {
 	pr_debug("Boot done.\n");
 
+	nmi_selftest();
 	impress_friends();
 #ifdef CONFIG_X86_IO_APIC
 	setup_ioapic_dest();

commit 0a613b647bac0cfab7b7d81f11271883209a70ef
Merge: eb47418dc56b a6c23905ff0d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 22 17:02:38 2011 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, smpboot: Mark the names[] array in __inquire_remote_apic() as const
      x86: Convert vmalloc()+memset() to vzalloc()

commit a6c23905ff0d6bbddf590ef0838489ee0f6c74ac
Author: Greg Dietsche <Gregory.Dietsche@cuw.edu>
Date:   Thu Jun 30 20:10:53 2011 -0500

    x86, smpboot: Mark the names[] array in __inquire_remote_apic() as const
    
    This array is read-only. Make it explicit by marking as const.
    
    Signed-off-by: Greg Dietsche <Gregory.Dietsche@cuw.edu>
    Link: http://lkml.kernel.org/r/1309482653-23648-1-git-send-email-Gregory.Dietsche@cuw.edu
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a3c430bdfb60..e02653a2e4f7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -425,7 +425,7 @@ static void impress_friends(void)
 void __inquire_remote_apic(int apicid)
 {
 	unsigned i, regs[] = { APIC_ID >> 4, APIC_LVR >> 4, APIC_SPIV >> 4 };
-	char *names[] = { "ID", "VERSION", "SPIV" };
+	const char * const names[] = { "ID", "VERSION", "SPIV" };
 	int timeout;
 	u32 status;
 

commit fd8a7de177b6f56a0fc59ad211c197a7df06b1ad
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 20 14:34:50 2010 +0200

    x86: cpu-hotplug: Prevent softirq wakeup on wrong CPU
    
    After a newly plugged CPU sets the cpu_online bit it enables
    interrupts and goes idle. The cpu which brought up the new cpu waits
    for the cpu_online bit and when it observes it, it sets the cpu_active
    bit for this cpu. The cpu_active bit is the relevant one for the
    scheduler to consider the cpu as a viable target.
    
    With forced threaded interrupt handlers which imply forced threaded
    softirqs we observed the following race:
    
    cpu 0                         cpu 1
    
    bringup(cpu1);
                                  set_cpu_online(smp_processor_id(), true);
                                  local_irq_enable();
    while (!cpu_online(cpu1));
                                  timer_interrupt()
                                    -> wake_up(softirq_thread_cpu1);
                                         -> enqueue_on(softirq_thread_cpu1, cpu0);
    
                                                                            ^^^^
    
    cpu_notify(CPU_ONLINE, cpu1);
      -> sched_cpu_active(cpu1)
         -> set_cpu_active((cpu1, true);
    
    When an interrupt happens before the cpu_active bit is set by the cpu
    which brought up the newly onlined cpu, then the scheduler refuses to
    enqueue the woken thread which is bound to that newly onlined cpu on
    that newly onlined cpu due to the not yet set cpu_active bit and
    selects a fallback runqueue. Not really an expected and desirable
    behaviour.
    
    So far this has only been observed with forced hard/softirq threading,
    but in theory this could happen without forced threaded hard/softirqs
    as well. It's probably unobservable as it would take a massive
    interrupt storm on the newly onlined cpu which causes the softirq loop
    to wake up the softirq thread and an even longer delay of the cpu
    which waits for the cpu_online bit.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Peter Zijlstra <peterz@infradead.org>
    Cc: stable@kernel.org # 2.6.39

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 33a0c11797de..9fd3137230d4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -285,6 +285,19 @@ notrace static void __cpuinit start_secondary(void *unused)
 	per_cpu(cpu_state, smp_processor_id()) = CPU_ONLINE;
 	x86_platform.nmi_init();
 
+	/*
+	 * Wait until the cpu which brought this one up marked it
+	 * online before enabling interrupts. If we don't do that then
+	 * we can end up waking up the softirq thread before this cpu
+	 * reached the active state, which makes the scheduler unhappy
+	 * and schedule the softirq thread on the wrong cpu. This is
+	 * only observable with forced threaded interrupts, but in
+	 * theory it could also happen w/o them. It's just way harder
+	 * to achieve.
+	 */
+	while (!cpumask_test_cpu(smp_processor_id(), cpu_active_mask))
+		cpu_relax();
+
 	/* enable local interrupts */
 	local_irq_enable();
 

commit 4f3c125c7420c85eaff627145557e392a871922d
Author: Avi Kivity <avi@redhat.com>
Date:   Mon May 30 08:23:57 2011 -0400

    x86: Fix mwait_play_dead() faulting on mwait-incapable cpus
    
    A logic error in mwait_play_dead() causes the kernel to use
    mwait even on cpus which don't support it, such as KVM virtual
    cpus.
    
    Introduced by:
    
      349c004e3d31: x86: A fast way to check capabilities of the current cpu
    
    Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=36222
    Reported-by: Török Edwin <edwintorok@gmail.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/1306758237-9327-1-git-send-email-avi@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index eefd96765e79..33a0c11797de 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1332,7 +1332,7 @@ static inline void mwait_play_dead(void)
 	void *mwait_ptr;
 	struct cpuinfo_x86 *c = __this_cpu_ptr(&cpu_info);
 
-	if (!this_cpu_has(X86_FEATURE_MWAIT) && mwait_usable(c))
+	if (!(this_cpu_has(X86_FEATURE_MWAIT) && mwait_usable(c)))
 		return;
 	if (!this_cpu_has(X86_FEATURE_CLFLSH))
 		return;

commit f310642123e0d32d919c60ca3fab5acd130c4ba3
Merge: ef1d57599dc9 5d4c47e0195b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 29 11:18:09 2011 -0700

    Merge branch 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6
    
    * 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6:
      x86 idle: deprecate mwait_idle() and "idle=mwait" cmdline param
      x86 idle: deprecate "no-hlt" cmdline param
      x86 idle APM: deprecate CONFIG_APM_CPU_IDLE
      x86 idle floppy: deprecate disable_hlt()
      x86 idle: EXPORT_SYMBOL(default_idle, pm_idle) only when APM demands it
      x86 idle: clarify AMD erratum 400 workaround
      idle governor: Avoid lock acquisition to read pm_qos before entering idle
      cpuidle: menu: fixed wrapping timers at 4.294 seconds

commit 02c68a02018669d1817c43c42de800975cbec467
Author: Len Brown <len.brown@intel.com>
Date:   Fri Apr 1 16:59:53 2011 -0400

    x86 idle: clarify AMD erratum 400 workaround
    
    The workaround for AMD erratum 400 uses the term "c1e" falsely suggesting:
    1. Intel C1E is somehow involved
    2. All AMD processors with C1E are involved
    
    Use the string "amd_c1e" instead of simply "c1e" to clarify that
    this workaround is specific to AMD's version of C1E.
    Use the string "e400" to clarify that the workaround is specific
    to AMD processors with Erratum 400.
    
    This patch is text-substitution only, with no functional change.
    
    cc: x86@kernel.org
    Acked-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 08776a953487..2c33633595cc 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1379,7 +1379,7 @@ void play_dead_common(void)
 {
 	idle_task_exit();
 	reset_lazy_tlbstate();
-	c1e_remove_cpu(raw_smp_processor_id());
+	amd_e400_remove_cpu(raw_smp_processor_id());
 
 	mb();
 	/* Ack it */

commit 349c004e3d31fda23ad225b61861be38047fff16
Author: Christoph Lameter <cl@linux.com>
Date:   Sat Mar 12 12:50:10 2011 +0100

    x86: A fast way to check capabilities of the current cpu
    
    Add this_cpu_has() which determines if the current cpu has a certain
    ability using a segment prefix and a bit test operation.
    
    For that we need to add bit operations to x86s percpu.h.
    
    Many uses of cpu_has use a pointer passed to a function to determine
    the current flags. That is no longer necessary after this patch.
    
    However, this patch only converts the straightforward cases where
    cpu_has is used with this_cpu_ptr. The rest is work for later.
    
    -tj: Rolled up patch to add x86_ prefix and use percpu_read() instead
         of percpu_read_stable().
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c2871d3c71b6..a3c430bdfb60 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1332,9 +1332,9 @@ static inline void mwait_play_dead(void)
 	void *mwait_ptr;
 	struct cpuinfo_x86 *c = __this_cpu_ptr(&cpu_info);
 
-	if (!(cpu_has(c, X86_FEATURE_MWAIT) && mwait_usable(c)))
+	if (!this_cpu_has(X86_FEATURE_MWAIT) && mwait_usable(c))
 		return;
-	if (!cpu_has(__this_cpu_ptr(&cpu_info), X86_FEATURE_CLFLSH))
+	if (!this_cpu_has(X86_FEATURE_CLFLSH))
 		return;
 	if (__this_cpu_read(cpu_info.cpuid_level) < CPUID_MWAIT_LEAF)
 		return;

commit e7fd3b4669f5b835c8afce28425d9f698a558115
Merge: fc82e1d59a24 2ae9d293b14d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 16 10:10:02 2011 -0700

    Merge branch 'x86-trampoline-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-trampoline-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Fix binutils-2.21 symbol related build failures
      x86-64, trampoline: Remove unused variable
      x86, reboot: Fix the use of passed arguments in 32-bit BIOS reboot
      x86, reboot: Move the real-mode reboot code to an assembly file
      x86: Make the GDT_ENTRY() macro in <asm/segment.h> safe for assembly
      x86, trampoline: Use the unified trampoline setup for ACPI wakeup
      x86, trampoline: Common infrastructure for low memory trampolines
    
    Fix up trivial conflicts in arch/x86/kernel/Makefile

commit 181f977d134a9f8e3f8839f42af655b045fc059e
Merge: d5d42399bd7b 25542c646afb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 19:49:10 2011 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (93 commits)
      x86, tlb, UV: Do small micro-optimization for native_flush_tlb_others()
      x86-64, NUMA: Don't call numa_set_distanc() for all possible node combinations during emulation
      x86-64, NUMA: Don't assume phys node 0 is always online in numa_emulation()
      x86-64, NUMA: Clean up initmem_init()
      x86-64, NUMA: Fix numa_emulation code with node0 without RAM
      x86-64, NUMA: Revert NUMA affine page table allocation
      x86: Work around old gas bug
      x86-64, NUMA: Better explain numa_distance handling
      x86-64, NUMA: Fix distance table handling
      mm: Move early_node_map[] reverse scan helpers under HAVE_MEMBLOCK
      x86-64, NUMA: Fix size of numa_distance array
      x86: Rename e820_table_* to pgt_buf_*
      bootmem: Move __alloc_memory_core_early() to nobootmem.c
      bootmem: Move contig_page_data definition to bootmem.c/nobootmem.c
      bootmem: Separate out CONFIG_NO_BOOTMEM code into nobootmem.c
      x86-64, NUMA: Seperate out numa_alloc_distance() from numa_set_distance()
      x86-64, NUMA: Add proper function comments to global functions
      x86-64, NUMA: Move NUMA emulation into numa_emulation.c
      x86-64, NUMA: Prepare numa_emulation() for moving NUMA emulation into a separate file
      x86-64, NUMA: Do not scan two times for setup_node_bootmem()
      ...
    
    Fix up conflicts in arch/x86/kernel/smpboot.c

commit 5f6fb45466b2273ffb91c9cf209f164f666c33b1
Merge: 3904afb41d43 c0185808eb85
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 19:23:40 2011 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (116 commits)
      x86: Enable forced interrupt threading support
      x86: Mark low level interrupts IRQF_NO_THREAD
      x86: Use generic show_interrupts
      x86: ioapic: Avoid redundant lookup of irq_cfg
      x86: ioapic: Use new move_irq functions
      x86: Use the proper accessors in fixup_irqs()
      x86: ioapic: Use irq_data->state
      x86: ioapic: Simplify irq chip and handler setup
      x86: Cleanup the genirq name space
      genirq: Add chip flag to force mask on suspend
      genirq: Add desc->irq_data accessor
      genirq: Add comments to Kconfig switches
      genirq: Fixup fasteoi handler for oneshot mode
      genirq: Provide forced interrupt threading
      sched: Switch wait_task_inactive to schedule_hrtimeout()
      genirq: Add IRQF_NO_THREAD
      genirq: Allow shared oneshot interrupts
      genirq: Prepare the handling of shared oneshot interrupts
      genirq: Make warning in handle_percpu_event useful
      x86: ioapic: Move trigger defines to io_apic.h
      ...
    
    Fix up trivial(?) conflicts in arch/x86/pci/xen.c due to genirq name
    space changes clashing with the Xen cleanups.  The set_irq_msi() had
    moved to xen_bind_pirq_msi_to_irq().

commit 502f4d4f74219749a9758b9bbc27fb665b2e83ab
Merge: da849abeb86d e5fea868e6c0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 19:00:53 2011 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Fix and clean up generic_processor_info()
      x86: Don't copy per_cpu cpuinfo for BSP two times
      x86: Move llc_shared_map out of cpu_info

commit 8460b3e5bc64955aeefdd8357b3bf7b5ff79b3f2
Merge: 56396e6823fe 521cb40b0c44
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 15 08:29:44 2011 +0100

    Merge commit 'v2.6.38' into x86/mm
    
    Conflicts:
            arch/x86/mm/numa_64.c
    
    Merge reason: Resolve the conflict, update the branch to .38.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 7167d08e780a722fa79ea414fc4e72bc00751392
Author: Henrik Kretzschmar <henne@nachtwindheim.de>
Date:   Tue Feb 22 15:38:05 2011 +0100

    x86: Rework arch_disable_smp_support() for x86
    
    Currently arch_disable_smp_support() on x86 disables only the
    support for the IOAPIC and is also compiled in if SMP-support is
    not.
    
    Therefore this function is renamed to disable_ioapic_support(),
    which meets its purpose and is only compiled in the kernel
    when IOAPIC support is also.
    
    A new arch_disable_smp_support() is created in smpboot.c,
    which calls disable_ioapic_support() and gets only compiled
    in the kernel when SMP support is also.
    
    Signed-off-by: Henrik Kretzschmar <henne@nachtwindheim.de>
    LKML-Reference: <1298385487-4708-3-git-send-email-henne@nachtwindheim.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 08776a953487..09d0172a0059 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -64,6 +64,7 @@
 #include <asm/mtrr.h>
 #include <asm/mwait.h>
 #include <asm/apic.h>
+#include <asm/io_apic.h>
 #include <asm/setup.h>
 #include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>
@@ -945,6 +946,14 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 	return 0;
 }
 
+/**
+ * arch_disable_smp_support() - disables SMP support for x86 at runtime
+ */
+void arch_disable_smp_support(void)
+{
+	disable_ioapic_support();
+}
+
 /*
  * Fall back to non SMP mode after errors.
  *
@@ -1045,7 +1054,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 				"(tell your hw vendor)\n");
 		}
 		smpboot_clear_io_apic();
-		arch_disable_smp_support();
+		disable_ioapic_support();
 		return -1;
 	}
 

commit 4822b7fc6d4870685a9feadfc348d48f5e47460a
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Mon Feb 14 15:34:57 2011 -0800

    x86, trampoline: Common infrastructure for low memory trampolines
    
    Common infrastructure for low memory trampolines.  This code installs
    the trampolines permanently in low memory very early.  It also permits
    multiple pieces of code to be used for this purpose.
    
    This code also introduces a standard infrastructure for computing
    symbol addresses in the trampoline code.
    
    The only change to the actual SMP trampolines themselves is that the
    64-bit trampoline has been made reusable -- the previous version would
    overwrite the code with a status variable; this moves the status
    variable to a separate location.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    LKML-Reference: <4D5DFBE4.7090104@intel.com>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Matthieu Castet <castet.matthieu@free.fr>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 08776a953487..545273369efa 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -788,7 +788,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	stack_start  = c_idle.idle->thread.sp;
 
 	/* start_ip had better be page-aligned! */
-	start_ip = setup_trampoline();
+	start_ip = trampoline_address();
 
 	/* So we see what's up */
 	announce_cpu(cpu, apicid);
@@ -798,6 +798,8 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	 * the targeted processor.
 	 */
 
+	printk(KERN_DEBUG "smpboot cpu %d: start_ip = %lx\n", cpu, start_ip);
+
 	atomic_set(&init_deasserted, 0);
 
 	if (get_uv_system_type() != UV_NON_UNIQUE_APIC) {
@@ -851,8 +853,8 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 			pr_debug("CPU%d: has booted.\n", cpu);
 		else {
 			boot_error = 1;
-			if (*((volatile unsigned char *)trampoline_base)
-					== 0xA5)
+			if (*(volatile u32 *)TRAMPOLINE_SYM(trampoline_status)
+			    == 0xA5A5A5A5)
 				/* trampoline started but...? */
 				pr_err("CPU%d: Stuck ??\n", cpu);
 			else
@@ -878,7 +880,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	}
 
 	/* mark "stuck" area as not stuck */
-	*((volatile unsigned long *)trampoline_base) = 0;
+	*(volatile u32 *)TRAMPOLINE_SYM(trampoline_status) = 0;
 
 	if (get_uv_system_type() != UV_NON_UNIQUE_APIC) {
 		/*

commit 275a88d3cf0e2f08a98dc5ce9494af0cb6ed2092
Merge: 52b8b8d7251f 9e81509efc4f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Feb 16 09:45:33 2011 +0100

    Merge branch 'x86/amd-nb' into x86/mm
    
    Merge reason: consolidate it into the more generic x86/mm tree to prevent conflicts
                  with ongoing NUMA work.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit b366801c95bdbeda811ac9668a3943051a18c188
Merge: eff9073790e1 100b33c8bd8a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 14 13:28:29 2011 +0100

    Merge commit 'v2.6.38-rc4' into x86/numa
    
    Merge reason: Merge latest fixes before applying new patch.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 91e04ec05838a5b2c790decf2a91af98cb1666e8
Merge: 792363d2bece 100b33c8bd8a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 14 13:18:51 2011 +0100

    Merge commit 'v2.6.38-rc4' into x86/cpu
    
    Merge reason: pick up the latest fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 2fb270f3212a1e6a73f86f76c85caee93aae4386
Author: Jan Beulich <JBeulich@novell.com>
Date:   Wed Feb 9 08:21:02 2011 +0000

    x86: Fix section mismatch in LAPIC initialization
    
    Additionally doing things conditionally upon smp_processor_id()
    being zero is generally a bad idea, as this means CPU 0 cannot
    be offlined and brought back online later again.
    
    While there may be other places where this is done, I think adding
    more of those should be avoided so that some day SMP can really
    become "symmetrical".
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    LKML-Reference: <4D525C7E0200007800030EE1@vpn.id2.novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 03273b6c272c..08776a953487 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1060,7 +1060,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 
 		connect_bsp_APIC();
 		setup_local_APIC();
-		end_local_APIC_setup();
+		bsp_end_local_APIC_setup();
 		return -1;
 	}
 
@@ -1137,7 +1137,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	if (!skip_ioapic_setup && nr_ioapics)
 		enable_IO_APIC();
 
-	end_local_APIC_setup();
+	bsp_end_local_APIC_setup();
 
 	map_cpu_to_logical_apicid();
 

commit 11d4c3f9b671720e80353dd7e433ff2bf65e9500
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Feb 4 16:14:11 2011 -0800

    x86-32: Make sure the stack is set up before we use it
    
    Since checkin ebba638ae723d8a8fc2f7abce5ec18b688b791d7 we call
    verify_cpu even in 32-bit mode.  Unfortunately, calling a function
    means using the stack, and the stack pointer was not initialized in
    the 32-bit setup code!  This code initializes the stack pointer, and
    simplifies the interface slightly since it is easier to rely on just a
    pointer value rather than a descriptor; we need to have different
    values for the segment register anyway.
    
    This retains start_stack as a virtual address, even though a physical
    address would be more convenient for 32 bits; the 64-bit code wants
    the other way around...
    
    Reported-by: Matthieu Castet <castet.matthieu@free.fr>
    LKML-Reference: <4D41E86D.8060205@free.fr>
    Tested-by: Kees Cook <kees.cook@canonical.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0cbe8c0b35ed..03273b6c272c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -638,7 +638,7 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	 * target processor state.
 	 */
 	startup_ipi_hook(phys_apicid, (unsigned long) start_secondary,
-			 (unsigned long)stack_start.sp);
+			 stack_start);
 
 	/*
 	 * Run STARTUP IPI loop.
@@ -785,7 +785,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 #endif
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	initial_code = (unsigned long)start_secondary;
-	stack_start.sp = (void *) c_idle.idle->thread.sp;
+	stack_start  = c_idle.idle->thread.sp;
 
 	/* start_ip had better be page-aligned! */
 	start_ip = setup_trampoline();

commit de2d9445f1627830ed2ebd00ee9d851986c940b5
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:41 2011 +0100

    x86: Unify node_to_cpumask_map handling between 32 and 64bit
    
    x86_32 has been managing node_to_cpumask_map explicitly from
    map_cpu_to_node() and friends in a rather ugly way.  With
    previous changes, it's now possible to share the code with
    64bit.
    
    * When CONFIG_NUMA_EMU is disabled, numa_add/remove_cpu() are
      implemented in numa.c and shared by 32 and 64bit.  CONFIG_NUMA_EMU
      versions still live in numa_64.c.
    
      NUMA_EMU's dependency on 64bit is planned to be removed and the
      above should go away together.
    
    * identify_cpu() now calls numa_add_cpu() for 32bit too.  This
      makes the explicit mask management from map_cpu_to_node() unnecessary.
    
    * The whole x86_32 specific map_cpu_to_node() chunk is no longer
      necessary.  Dropped.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-16-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Shaohui Zheng <shaohui.zheng@intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2c203822424f..522b2173888a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -132,49 +132,6 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 
 atomic_t init_deasserted;
 
-#if defined(CONFIG_NUMA) && defined(CONFIG_X86_32)
-/* set up a mapping between cpu and node. */
-static void map_cpu_to_node(int cpu, int node)
-{
-	printk(KERN_INFO "Mapping cpu %d to node %d\n", cpu, node);
-	cpumask_set_cpu(cpu, node_to_cpumask_map[node]);
-}
-
-/* undo a mapping between cpu and node. */
-static void unmap_cpu_to_node(int cpu)
-{
-	int node;
-
-	printk(KERN_INFO "Unmapping cpu %d from all nodes\n", cpu);
-	for (node = 0; node < MAX_NUMNODES; node++)
-		cpumask_clear_cpu(cpu, node_to_cpumask_map[node]);
-}
-#else /* !(CONFIG_NUMA && CONFIG_X86_32) */
-#define map_cpu_to_node(cpu, node)	({})
-#define unmap_cpu_to_node(cpu)	({})
-#endif
-
-#ifdef CONFIG_X86_32
-static void map_cpu_to_logical_apicid(void)
-{
-	int cpu = smp_processor_id();
-	int node;
-
-	node = numa_cpu_node(cpu);
-	if (!node_online(node))
-		node = first_online_node;
-
-	map_cpu_to_node(cpu, node);
-}
-
-void numa_remove_cpu(int cpu)
-{
-	unmap_cpu_to_node(cpu);
-}
-#else
-#define map_cpu_to_logical_apicid()  do {} while (0)
-#endif
-
 /*
  * Report back to the Boot Processor.
  * Running on AP.
@@ -242,7 +199,6 @@ static void __cpuinit smp_callin(void)
 		apic->smp_callin_clear_local_apic();
 	setup_local_APIC();
 	end_local_APIC_setup();
-	map_cpu_to_logical_apicid();
 
 	/*
 	 * Need to setup vector mappings before we enable interrupts.
@@ -943,7 +899,6 @@ static __init void disable_smp(void)
 		physid_set_mask_of_physid(boot_cpu_physical_apicid, &phys_cpu_present_map);
 	else
 		physid_set_mask_of_physid(0, &phys_cpu_present_map);
-	map_cpu_to_logical_apicid();
 	cpumask_set_cpu(0, cpu_sibling_mask(0));
 	cpumask_set_cpu(0, cpu_core_mask(0));
 }
@@ -1120,8 +1075,6 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 	end_local_APIC_setup();
 
-	map_cpu_to_logical_apicid();
-
 	if (apic->setup_portio_remap)
 		apic->setup_portio_remap();
 

commit 645a79195f66eb68ef3ab2b21d9829ac3aa085a9
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:40 2011 +0100

    x86: Unify CPU -> NUMA node mapping between 32 and 64bit
    
    Unlike 64bit, 32bit has been using its own cpu_to_node_map[] for
    CPU -> NUMA node mapping.  Replace it with early_percpu variable
    x86_cpu_to_node_map and share the mapping code with 64bit.
    
    * USE_PERCPU_NUMA_NODE_ID is now enabled for 32bit too.
    
    * x86_cpu_to_node_map and numa_set/clear_node() are moved from
      numa_64 to numa.  For now, on 32bit, x86_cpu_to_node_map is initialized
      with 0 instead of NUMA_NO_NODE.  This is to avoid introducing unexpected
      behavior change and will be updated once init path is unified.
    
    * srat_detect_node() is now enabled for x86_32 too.  It calls
      numa_set_node() and initializes the mapping making explicit
      cpu_to_node_map[] updates from map/unmap_cpu_to_node() unnecessary.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: penberg@kernel.org
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-15-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Rientjes <rientjes@google.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b7cfce535cb0..2c203822424f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -133,16 +133,11 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 atomic_t init_deasserted;
 
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_32)
-/* which node each logical CPU is on */
-int cpu_to_node_map[NR_CPUS] __read_mostly = { [0 ... NR_CPUS-1] = 0 };
-EXPORT_SYMBOL(cpu_to_node_map);
-
 /* set up a mapping between cpu and node. */
 static void map_cpu_to_node(int cpu, int node)
 {
 	printk(KERN_INFO "Mapping cpu %d to node %d\n", cpu, node);
 	cpumask_set_cpu(cpu, node_to_cpumask_map[node]);
-	cpu_to_node_map[cpu] = node;
 }
 
 /* undo a mapping between cpu and node. */
@@ -153,7 +148,6 @@ static void unmap_cpu_to_node(int cpu)
 	printk(KERN_INFO "Unmapping cpu %d from all nodes\n", cpu);
 	for (node = 0; node < MAX_NUMNODES; node++)
 		cpumask_clear_cpu(cpu, node_to_cpumask_map[node]);
-	cpu_to_node_map[cpu] = 0;
 }
 #else /* !(CONFIG_NUMA && CONFIG_X86_32) */
 #define map_cpu_to_node(cpu, node)	({})

commit bbc9e2f452d9c4b166d1f9a78d941d80173312fe
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:39 2011 +0100

    x86: Unify cpu/apicid <-> NUMA node mapping between 32 and 64bit
    
    The mapping between cpu/apicid and node is done via
    apicid_to_node[] on 64bit and apicid_2_node[] +
    apic->x86_32_numa_cpu_node() on 32bit. This difference makes it
    difficult to further unify 32 and 64bit NUMA handling.
    
    This patch unifies it by replacing both apicid_to_node[] and
    apicid_2_node[] with __apicid_to_node[] array, which is accessed
    by two accessors - set_apicid_to_node() and numa_cpu_node().  On
    64bit, numa_cpu_node() always consults __apicid_to_node[]
    directly while 32bit goes through apic->numa_cpu_node() method
    to allow apic implementations to override it.
    
    srat_detect_node() for amd cpus contains workaround for broken
    NUMA configuration which assumes relationship between APIC ID,
    HT node ID and NUMA topology.  Leave it to access
    __apicid_to_node[] directly as mapping through CPU might result
    in undesirable behavior change.  The comment is reformatted and
    updated to note the ugliness.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-14-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Rientjes <rientjes@google.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 5319cdd53765..b7cfce535cb0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -71,10 +71,6 @@
 #include <asm/smpboot_hooks.h>
 #include <asm/i8259.h>
 
-#ifdef CONFIG_X86_32
-u8 apicid_2_node[MAX_LOCAL_APIC];
-#endif
-
 /* State of each CPU */
 DEFINE_PER_CPU(int, cpu_state) = { 0 };
 
@@ -170,7 +166,7 @@ static void map_cpu_to_logical_apicid(void)
 	int cpu = smp_processor_id();
 	int node;
 
-	node = apic->x86_32_numa_cpu_node(cpu);
+	node = numa_cpu_node(cpu);
 	if (!node_online(node))
 		node = first_online_node;
 

commit 89e5dc218e084e13a3996db6693b01478912f4ee
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:38 2011 +0100

    x86: Replace apic->apicid_to_node() with ->x86_32_numa_cpu_node()
    
    apic->apicid_to_node() is 32bit specific apic operation which
    determines NUMA node for a CPU.  Depending on the APIC
    implementation, it can be easier to determine NUMA node from
    either physical or logical apicid.  Currently,
    ->apicid_to_node() takes @logical_apicid and calls
    hard_smp_processor_id() if the physical apicid is needed.
    
    This prevents NUMA mapping from being queried from a different
    CPU, which in turn makes it impossible to initialize NUMA
    mapping before SMP bringup.
    
    This patch replaces apic->apicid_to_node() with
    ->x86_32_numa_cpu_node() which takes @cpu, from which both
    logical and physical apicids can easily be determined.  While at
    it, drop duplicate implementations from bigsmp_32 and summit_32,
    and use the default one.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-13-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ca20f6bee3be..5319cdd53765 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -168,10 +168,9 @@ static void unmap_cpu_to_node(int cpu)
 static void map_cpu_to_logical_apicid(void)
 {
 	int cpu = smp_processor_id();
-	int logical_apicid = early_per_cpu(x86_cpu_to_logical_apicid, cpu);
 	int node;
 
-	node = apic->apicid_to_node(logical_apicid);
+	node = apic->x86_32_numa_cpu_node(cpu);
 	if (!node_online(node))
 		node = first_online_node;
 

commit 6f802c4bfa2acf1bffa8341fe9084da0205d581d
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:31 2011 +0100

    x86: Always use x86_cpu_to_logical_apicid for cpu -> logical apic id
    
    Currently, cpu -> logical apic id translation is done by
    apic->cpu_to_logical_apicid() callback which may or may not use
    x86_cpu_to_logical_apicid.  This is unnecessary as it should
    always equal logical_smp_processor_id() which is known early
    during CPU bring up.
    
    Initialize x86_cpu_to_logical_apicid after apic->init_apic_ldr()
    in setup_local_APIC() and always use x86_cpu_to_logical_apicid
    for cpu -> logical apic id mapping.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: penberg@kernel.org
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-6-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index df934e46bf53..ca20f6bee3be 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -168,19 +168,18 @@ static void unmap_cpu_to_node(int cpu)
 static void map_cpu_to_logical_apicid(void)
 {
 	int cpu = smp_processor_id();
-	int apicid = logical_smp_processor_id();
-	int node = apic->apicid_to_node(apicid);
+	int logical_apicid = early_per_cpu(x86_cpu_to_logical_apicid, cpu);
+	int node;
 
+	node = apic->apicid_to_node(logical_apicid);
 	if (!node_online(node))
 		node = first_online_node;
 
-	early_per_cpu(x86_cpu_to_logical_apicid, cpu) = apicid;
 	map_cpu_to_node(cpu, node);
 }
 
 void numa_remove_cpu(int cpu)
 {
-	early_per_cpu(x86_cpu_to_logical_apicid, cpu) = BAD_APICID;
 	unmap_cpu_to_node(cpu);
 }
 #else

commit 4c321ff8a01a95badf5d5403d80ca4e0ab07fce7
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:30 2011 +0100

    x86: Replace cpu_2_logical_apicid[] with early percpu variable
    
    Unlike x86_64, on x86_32, the mapping from cpu to logical apicid
    may vary depending on apic in use.  cpu_2_logical_apicid[] array
    is used for this mapping.  Replace it with early percpu variable
    x86_cpu_to_logical_apicid to make it better aligned with other
    mappings.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: penberg@kernel.org
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-5-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 53a85baaecca..df934e46bf53 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -165,9 +165,6 @@ static void unmap_cpu_to_node(int cpu)
 #endif
 
 #ifdef CONFIG_X86_32
-u8 cpu_2_logical_apicid[NR_CPUS] __read_mostly =
-					{ [0 ... NR_CPUS-1] = BAD_APICID };
-
 static void map_cpu_to_logical_apicid(void)
 {
 	int cpu = smp_processor_id();
@@ -177,13 +174,13 @@ static void map_cpu_to_logical_apicid(void)
 	if (!node_online(node))
 		node = first_online_node;
 
-	cpu_2_logical_apicid[cpu] = apicid;
+	early_per_cpu(x86_cpu_to_logical_apicid, cpu) = apicid;
 	map_cpu_to_node(cpu, node);
 }
 
 void numa_remove_cpu(int cpu)
 {
-	cpu_2_logical_apicid[cpu] = BAD_APICID;
+	early_per_cpu(x86_cpu_to_logical_apicid, cpu) = BAD_APICID;
 	unmap_cpu_to_node(cpu);
 }
 #else

commit b78aa66b1fe4179d28e3f6502dc179773519a1bb
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:28 2011 +0100

    x86: Drop x86_32 MAX_APICID
    
    Commit 56d91f13 (x86, acpi: Add MAX_LOCAL_APIC for 32bit) added
    MAX_LOCAL_APIC for x86_32 but didn't replace MAX_APICID users
    with it. Convert MAX_APICID users to MAX_LOCAL_APIC and drop
    MAX_APICID.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Acked-by: Yinghai Lu <yinghai@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-3-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ee9203a17d67..53a85baaecca 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -72,7 +72,7 @@
 #include <asm/i8259.h>
 
 #ifdef CONFIG_X86_32
-u8 apicid_2_node[MAX_APICID];
+u8 apicid_2_node[MAX_LOCAL_APIC];
 #endif
 
 /* State of each CPU */

commit bd22a2f1982fa3e90ce7d5d011c37d88aa67e73c
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:27 2011 +0100

    x86: Kill unused static boot_cpu_logical_apicid in smpboot.c
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Acked-by: Yinghai Lu <yinghai@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-2-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0cbe8c0b35ed..ee9203a17d67 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -165,8 +165,6 @@ static void unmap_cpu_to_node(int cpu)
 #endif
 
 #ifdef CONFIG_X86_32
-static int boot_cpu_logical_apicid;
-
 u8 cpu_2_logical_apicid[NR_CPUS] __read_mostly =
 					{ [0 ... NR_CPUS-1] = BAD_APICID };
 
@@ -1096,9 +1094,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	 * Setup boot CPU information
 	 */
 	smp_store_cpu_info(0); /* Final full version of the data */
-#ifdef CONFIG_X86_32
-	boot_cpu_logical_apicid = logical_smp_processor_id();
-#endif
+
 	current_thread_info()->cpu = 0;  /* needed? */
 	for_each_possible_cpu(i) {
 		zalloc_cpumask_var(&per_cpu(cpu_sibling_map, i), GFP_KERNEL);

commit 792363d2beceb1c7d865e517fa9939c8b8c1442a
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Jan 21 15:29:54 2011 -0800

    x86: Don't copy per_cpu cpuinfo for BSP two times
    
    smp_store_cpu_info(0) will do that.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    LKML-Reference: <4D3A16F2.5090902@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d396155f436c..a7a3d1acc632 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1071,13 +1071,13 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 	preempt_disable();
 	smp_cpu_index_default();
-	memcpy(__this_cpu_ptr(&cpu_info), &boot_cpu_data, sizeof(cpu_info));
-	cpumask_copy(cpu_callin_mask, cpumask_of(0));
-	mb();
+
 	/*
 	 * Setup boot CPU information
 	 */
 	smp_store_cpu_info(0); /* Final full version of the data */
+	cpumask_copy(cpu_callin_mask, cpumask_of(0));
+	mb();
 #ifdef CONFIG_X86_32
 	boot_cpu_logical_apicid = logical_smp_processor_id();
 #endif

commit b3d7336db553d318e7ec042eb50a70d307013339
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Jan 21 15:29:44 2011 -0800

    x86: Move llc_shared_map out of cpu_info
    
    cpu_info is already with per_cpu, We can take llc_shared_map out
    of cpu_info, and declare it as per_cpu variable directly.
    
    So later referencing could be simple and directly instead of
    diving to find cpu_info at first.
    
    Also could make smp_store_cpu_info() much simple to avoid to do
    save and restore trick.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Hans Rosenfeld <hans.rosenfeld@amd.com>
    Cc: Alok N Kataria <akataria@vmware.com>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Hans J. Koch <hjk@linutronix.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <4D3A16E8.5020608@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0cbe8c0b35ed..d396155f436c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -130,6 +130,8 @@ EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
 DEFINE_PER_CPU(cpumask_var_t, cpu_core_map);
 EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 
+DEFINE_PER_CPU(cpumask_var_t, cpu_llc_shared_map);
+
 /* Per CPU bogomips and other parameters */
 DEFINE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
@@ -355,23 +357,6 @@ notrace static void __cpuinit start_secondary(void *unused)
 	cpu_idle();
 }
 
-#ifdef CONFIG_CPUMASK_OFFSTACK
-/* In this case, llc_shared_map is a pointer to a cpumask. */
-static inline void copy_cpuinfo_x86(struct cpuinfo_x86 *dst,
-				    const struct cpuinfo_x86 *src)
-{
-	struct cpumask *llc = dst->llc_shared_map;
-	*dst = *src;
-	dst->llc_shared_map = llc;
-}
-#else
-static inline void copy_cpuinfo_x86(struct cpuinfo_x86 *dst,
-				    const struct cpuinfo_x86 *src)
-{
-	*dst = *src;
-}
-#endif /* CONFIG_CPUMASK_OFFSTACK */
-
 /*
  * The bootstrap kernel entry code has set these up. Save them for
  * a given CPU
@@ -381,7 +366,7 @@ void __cpuinit smp_store_cpu_info(int id)
 {
 	struct cpuinfo_x86 *c = &cpu_data(id);
 
-	copy_cpuinfo_x86(c, &boot_cpu_data);
+	*c = boot_cpu_data;
 	c->cpu_index = id;
 	if (id != 0)
 		identify_secondary_cpu(c);
@@ -389,15 +374,12 @@ void __cpuinit smp_store_cpu_info(int id)
 
 static void __cpuinit link_thread_siblings(int cpu1, int cpu2)
 {
-	struct cpuinfo_x86 *c1 = &cpu_data(cpu1);
-	struct cpuinfo_x86 *c2 = &cpu_data(cpu2);
-
 	cpumask_set_cpu(cpu1, cpu_sibling_mask(cpu2));
 	cpumask_set_cpu(cpu2, cpu_sibling_mask(cpu1));
 	cpumask_set_cpu(cpu1, cpu_core_mask(cpu2));
 	cpumask_set_cpu(cpu2, cpu_core_mask(cpu1));
-	cpumask_set_cpu(cpu1, c2->llc_shared_map);
-	cpumask_set_cpu(cpu2, c1->llc_shared_map);
+	cpumask_set_cpu(cpu1, cpu_llc_shared_mask(cpu2));
+	cpumask_set_cpu(cpu2, cpu_llc_shared_mask(cpu1));
 }
 
 
@@ -425,7 +407,7 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 		cpumask_set_cpu(cpu, cpu_sibling_mask(cpu));
 	}
 
-	cpumask_set_cpu(cpu, c->llc_shared_map);
+	cpumask_set_cpu(cpu, cpu_llc_shared_mask(cpu));
 
 	if (__this_cpu_read(cpu_info.x86_max_cores) == 1) {
 		cpumask_copy(cpu_core_mask(cpu), cpu_sibling_mask(cpu));
@@ -436,8 +418,8 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 	for_each_cpu(i, cpu_sibling_setup_mask) {
 		if (per_cpu(cpu_llc_id, cpu) != BAD_APICID &&
 		    per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, i)) {
-			cpumask_set_cpu(i, c->llc_shared_map);
-			cpumask_set_cpu(cpu, cpu_data(i).llc_shared_map);
+			cpumask_set_cpu(i, cpu_llc_shared_mask(cpu));
+			cpumask_set_cpu(cpu, cpu_llc_shared_mask(i));
 		}
 		if (c->phys_proc_id == cpu_data(i).phys_proc_id) {
 			cpumask_set_cpu(i, cpu_core_mask(cpu));
@@ -476,7 +458,7 @@ const struct cpumask *cpu_coregroup_mask(int cpu)
 	    !(cpu_has(c, X86_FEATURE_AMD_DCM)))
 		return cpu_core_mask(cpu);
 	else
-		return c->llc_shared_map;
+		return cpu_llc_shared_mask(cpu);
 }
 
 static void impress_friends(void)
@@ -1103,7 +1085,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	for_each_possible_cpu(i) {
 		zalloc_cpumask_var(&per_cpu(cpu_sibling_map, i), GFP_KERNEL);
 		zalloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);
-		zalloc_cpumask_var(&cpu_data(i).llc_shared_map, GFP_KERNEL);
+		zalloc_cpumask_var(&per_cpu(cpu_llc_shared_map, i), GFP_KERNEL);
 	}
 	set_cpu_sibling_map(0);
 

commit d518573de63fb119e5e9a3137386544671387681
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Mon Jan 24 16:05:40 2011 +0100

    x86, amd: Normalize compute unit IDs on multi-node processors
    
    On multi-node CPUs we don't need the socket wide compute unit ID
    but the node-wide compute unit ID. Thus we need to normalize the
    value. This is similar to what we do with cpu_core_id.
    
    A compute unit is then identified by physical_package_id,
    node_id, and compute_unit_id.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    LKML-Reference: <1295881543-572552-2-git-send-email-hans.rosenfeld@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0cbe8c0b35ed..fbaa2229af5b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -414,6 +414,7 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 
 			if (cpu_has(c, X86_FEATURE_TOPOEXT)) {
 				if (c->phys_proc_id == o->phys_proc_id &&
+				    per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, i) &&
 				    c->compute_unit_id == o->compute_unit_id)
 					link_thread_siblings(cpu, i);
 			} else if (c->phys_proc_id == o->phys_proc_id &&

commit 93789b32dbf355e70f18b17a82e8661677a7f7fb
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Thu Jan 20 15:42:52 2011 +0100

    x86, hotplug: Fix powersavings with offlined cores on AMD
    
    ea53069231f9317062910d6e772cca4ce93de8c8 made a CPU use monitor/mwait
    when offline. This is not the optimal choice for AMD wrt to powersavings
    and we'd prefer our cores to halt (i.e. enter C1) instead. For this, the
    same selection whether to use monitor/mwait has to be used as when we
    select the idle routine for the machine.
    
    With this patch, offlining cores 1-5 on a X6 machine allows core0 to
    boost again.
    
    [ hpa: putting this in urgent since it is a (power) regression fix ]
    
    Reported-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: stable@kernel.org # 37.x
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.hl>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <1295534572-10730-1-git-send-email-bp@amd64.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 763df77343dd..0cbe8c0b35ed 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1402,8 +1402,9 @@ static inline void mwait_play_dead(void)
 	unsigned int highest_subcstate = 0;
 	int i;
 	void *mwait_ptr;
+	struct cpuinfo_x86 *c = __this_cpu_ptr(&cpu_info);
 
-	if (!cpu_has(__this_cpu_ptr(&cpu_info), X86_FEATURE_MWAIT))
+	if (!(cpu_has(c, X86_FEATURE_MWAIT) && mwait_usable(c)))
 		return;
 	if (!cpu_has(__this_cpu_ptr(&cpu_info), X86_FEATURE_CLFLSH))
 		return;

commit 16ee8db6a93ffbc021132599f33288613f042c3d
Merge: 5943a268002f fa36e956c502
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 11 11:11:46 2011 -0800

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Fix Moorestown VRTC fixmap placement
      x86/gpio: Implement x86 gpio_to_irq convert function
      x86, UV: Fix APICID shift for Westmere processors
      x86: Use PCI method for enabling AMD extended config space before MSR method
      x86: tsc: Prevent delayed init if initial tsc calibration failed
      x86, lapic-timer: Increase the max_delta to 31 bits
      x86: Fix sparse non-ANSI function warnings in smpboot.c
      x86, numa: Fix CONFIG_DEBUG_PER_CPU_MAPS without NUMA emulation
      x86, AMD, PCI: Add AMD northbridge PCI device id for CPU families 12h and 14h
      x86, numa: Fix cpu to node mapping for sparse node ids
      x86, numa: Fake node-to-cpumask for NUMA emulation
      x86, numa: Fake apicid and pxm mappings for NUMA emulation
      x86, numa: Avoid compiling NUMA emulation functions without CONFIG_NUMA_EMU
      x86, numa: Reduce minimum fake node size to 32M
    
    Fix up trivial conflict in arch/x86/kernel/apic/x2apic_uv_x.c

commit 91d88ce22bca3dcf269661b54d4ea75576dc4e29
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Sat Jan 8 19:59:14 2011 -0800

    x86: Fix sparse non-ANSI function warnings in smpboot.c
    
    Fix sparse warning for non-ANSI function declaration:
    
      arch/x86/kernel/smpboot.c:100:30: warning: non-ANSI function declaration of function 'cpu_hotplug_driver_lock'
      arch/x86/kernel/smpboot.c:105:32: warning: non-ANSI function declaration of function 'cpu_hotplug_driver_unlock'
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    LKML-Reference: <20110108195914.95d366ea.randy.dunlap@oracle.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ee886fe10ef4..5fdc0950da1d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -97,12 +97,12 @@ static DEFINE_PER_CPU(struct task_struct *, idle_thread_array);
  */
 static DEFINE_MUTEX(x86_cpu_hotplug_driver_mutex);
 
-void cpu_hotplug_driver_lock()
+void cpu_hotplug_driver_lock(void)
 {
         mutex_lock(&x86_cpu_hotplug_driver_mutex);
 }
 
-void cpu_hotplug_driver_unlock()
+void cpu_hotplug_driver_unlock(void)
 {
         mutex_unlock(&x86_cpu_hotplug_driver_mutex);
 }

commit 72eb6a791459c87a0340318840bb3bd9252b627b
Merge: 23d69b09b78c 55ee4ef30241
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 7 17:02:58 2011 -0800

    Merge branch 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (30 commits)
      gameport: use this_cpu_read instead of lookup
      x86: udelay: Use this_cpu_read to avoid address calculation
      x86: Use this_cpu_inc_return for nmi counter
      x86: Replace uses of current_cpu_data with this_cpu ops
      x86: Use this_cpu_ops to optimize code
      vmstat: User per cpu atomics to avoid interrupt disable / enable
      irq_work: Use per cpu atomics instead of regular atomics
      cpuops: Use cmpxchg for xchg to avoid lock semantics
      x86: this_cpu_cmpxchg and this_cpu_xchg operations
      percpu: Generic this_cpu_cmpxchg() and this_cpu_xchg support
      percpu,x86: relocate this_cpu_add_return() and friends
      connector: Use this_cpu operations
      xen: Use this_cpu_inc_return
      taskstats: Use this_cpu_ops
      random: Use this_cpu_inc_return
      fs: Use this_cpu_inc_return in buffer.c
      highmem: Use this_cpu_xx_return() operations
      vmstat: Use this_cpu_inc_return for vm statistics
      x86: Support for this_cpu_add, sub, dec, inc_return
      percpu: Generic support for this_cpu_add, sub, dec, inc_return
      ...
    
    Fixed up conflicts: in arch/x86/kernel/{apic/nmi.c, apic/x2apic_uv_x.c, process.c}
    as per Tejun.

commit 47935a731b7b850a4c6c0e55ed0741e3dd25d889
Merge: 77a0dd54ba3c 3fb82d56ad00 fd35fbcdd1b2 9e76a97efd31 c8217b8305e5 3cf9b85b474e f6cd24777513
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 6 11:11:50 2011 -0800

    Merge branches 'x86-alternatives-for-linus', 'x86-fpu-for-linus', 'x86-hwmon-for-linus', 'x86-paravirt-for-linus', 'core-locking-for-linus' and 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-alternatives-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, suspend: Avoid unnecessary smp alternatives switch during suspend/resume
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86-64, asm: Use fxsaveq/fxrestorq in more places
    
    * 'x86-hwmon-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, hwmon: Add core threshold notification to therm_throt.c
    
    * 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, paravirt: Use native_halt on a halt, not native_safe_halt
    
    * 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      locking, lockdep: Convert sprintf_symbol to %pS
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      irq: Better struct irqaction layout

commit 7b543a5334ff4ea2e3ad3b777fc23cdb8072a988
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Dec 18 16:30:05 2010 +0100

    x86: Replace uses of current_cpu_data with this_cpu ops
    
    Replace all uses of current_cpu_data with this_cpu operations on the
    per cpu structure cpu_info.  The scala accesses are replaced with the
    matching this_cpu ops which results in smaller and more efficient
    code.
    
    In the long run, it might be a good idea to remove cpu_data() macro
    too and use per_cpu macro directly.
    
    tj: updated description
    
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ff4e5a113a5b..0720071086d1 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -430,7 +430,7 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 
 	cpumask_set_cpu(cpu, c->llc_shared_map);
 
-	if (current_cpu_data.x86_max_cores == 1) {
+	if (__this_cpu_read(cpu_info.x86_max_cores) == 1) {
 		cpumask_copy(cpu_core_mask(cpu), cpu_sibling_mask(cpu));
 		c->booted_cores = 1;
 		return;
@@ -1094,7 +1094,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 	preempt_disable();
 	smp_cpu_index_default();
-	current_cpu_data = boot_cpu_data;
+	memcpy(__this_cpu_ptr(&cpu_info), &boot_cpu_data, sizeof(cpu_info));
 	cpumask_copy(cpu_callin_mask, cpumask_of(0));
 	mb();
 	/*
@@ -1397,11 +1397,11 @@ static inline void mwait_play_dead(void)
 	int i;
 	void *mwait_ptr;
 
-	if (!cpu_has(&current_cpu_data, X86_FEATURE_MWAIT))
+	if (!cpu_has(__this_cpu_ptr(&cpu_info), X86_FEATURE_MWAIT))
 		return;
-	if (!cpu_has(&current_cpu_data, X86_FEATURE_CLFLSH))
+	if (!cpu_has(__this_cpu_ptr(&cpu_info), X86_FEATURE_CLFLSH))
 		return;
-	if (current_cpu_data.cpuid_level < CPUID_MWAIT_LEAF)
+	if (__this_cpu_read(cpu_info.cpuid_level) < CPUID_MWAIT_LEAF)
 		return;
 
 	eax = CPUID_MWAIT_LEAF;
@@ -1452,7 +1452,7 @@ static inline void mwait_play_dead(void)
 
 static inline void hlt_play_dead(void)
 {
-	if (current_cpu_data.x86 >= 4)
+	if (__this_cpu_read(cpu_info.x86) >= 4)
 		wbinvd();
 
 	while (1) {

commit 0a3aee0da4402aa19b66e458038533c896fb80c6
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Dec 18 16:28:55 2010 +0100

    x86: Use this_cpu_ops to optimize code
    
    Go through x86 code and replace __get_cpu_var and get_cpu_var
    instances that refer to a scalar and are not used for address
    determinations.
    
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 083e99d1b7df..ff4e5a113a5b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1377,7 +1377,7 @@ void play_dead_common(void)
 
 	mb();
 	/* Ack it */
-	__get_cpu_var(cpu_state) = CPU_DEAD;
+	__this_cpu_write(cpu_state, CPU_DEAD);
 
 	/*
 	 * With physical CPU hotplug, we should halt the cpu

commit 3fb82d56ad003e804923185316236f26b30dfdd5
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Tue Nov 23 16:11:40 2010 -0800

    x86, suspend: Avoid unnecessary smp alternatives switch during suspend/resume
    
    During suspend, we disable all the non boot cpus. And during resume we bring
    them all back again. So no need to do alternatives_smp_switch() in between.
    
    On my core 2 based laptop, this speeds up the suspend path by 15msec and the
    resume path by 5 msec (suspend/resume speed up differences can be attributed
    to the different P-states that the cpu is in during suspend/resume).
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1290557500.4946.8.camel@sbsiddha-MOBL3.sc.intel.com>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 083e99d1b7df..837c81e99edf 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1166,6 +1166,20 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	preempt_enable();
 }
 
+void arch_disable_nonboot_cpus_begin(void)
+{
+	/*
+	 * Avoid the smp alternatives switch during the disable_nonboot_cpus().
+	 * In the suspend path, we will be back in the SMP mode shortly anyways.
+	 */
+	skip_smp_alternatives = true;
+}
+
+void arch_disable_nonboot_cpus_end(void)
+{
+	skip_smp_alternatives = false;
+}
+
 void arch_enable_nonboot_cpus_begin(void)
 {
 	set_mtrr_aps_delayed_init();

commit 5ef428c4b5950dddce7311e84321abb3aff7ebb0
Author: Andi Kleen <ak@linux.intel.com>
Date:   Thu Nov 18 11:47:31 2010 +0100

    x86: Set cpu masks before calling CPU_STARTING notifiers
    
    When booting up a CPU set the various topology masks before
    calling the CPU_STARTING notifier. This way the notifier
    can actually use the masks.
    
    This is needed for a perf change.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1290077254-12165-2-git-send-email-andi@firstfloor.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f0a0624eea55..68f61ac632e1 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -281,6 +281,13 @@ static void __cpuinit smp_callin(void)
 	 */
 	smp_store_cpu_info(cpuid);
 
+	/*
+	 * This must be done before setting cpu_online_mask
+	 * or calling notify_cpu_starting.
+	 */
+	set_cpu_sibling_map(raw_smp_processor_id());
+	wmb();
+
 	notify_cpu_starting(cpuid);
 
 	/*
@@ -316,10 +323,6 @@ notrace static void __cpuinit start_secondary(void *unused)
 	 */
 	check_tsc_sync_target();
 
-	/* This must be done before setting cpu_online_mask */
-	set_cpu_sibling_map(raw_smp_processor_id());
-	wmb();
-
 	/*
 	 * We need to hold call_lock, so there is no inconsistency
 	 * between the time smp_call_function() determines number of

commit 072b198a4ad48bd722ec6d203d65422a4698eae7
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri Nov 12 11:22:24 2010 -0500

    x86, nmi_watchdog: Remove all stub function calls from old nmi_watchdog
    
    Now that the bulk of the old nmi_watchdog is gone, remove all
    the stub variables and hooks associated with it.
    
    This touches lots of files mainly because of how the io_apic
    nmi_watchdog was implemented.  Now that the io_apic nmi_watchdog
    is forever gone, remove all its fingers.
    
    Most of this code was not being exercised by virtue of
    nmi_watchdog != NMI_IO_APIC, so there shouldn't be anything to
    risky here.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: fweisbec@gmail.com
    Cc: gorcunov@openvz.org
    LKML-Reference: <1289578944-28564-3-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 083e99d1b7df..f0a0624eea55 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -316,12 +316,6 @@ notrace static void __cpuinit start_secondary(void *unused)
 	 */
 	check_tsc_sync_target();
 
-	if (nmi_watchdog == NMI_IO_APIC) {
-		legacy_pic->mask(0);
-		enable_NMI_through_LVT0();
-		legacy_pic->unmask(0);
-	}
-
 	/* This must be done before setting cpu_online_mask */
 	set_cpu_sibling_map(raw_smp_processor_id());
 	wmb();
@@ -1061,8 +1055,6 @@ static int __init smp_sanity_check(unsigned max_cpus)
 		printk(KERN_INFO "SMP mode deactivated.\n");
 		smpboot_clear_io_apic();
 
-		localise_nmi_watchdog();
-
 		connect_bsp_APIC();
 		setup_local_APIC();
 		end_local_APIC_setup();
@@ -1196,7 +1188,6 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 #ifdef CONFIG_X86_IO_APIC
 	setup_ioapic_dest();
 #endif
-	check_nmi_watchdog();
 	mtrr_aps_init();
 }
 
@@ -1341,8 +1332,6 @@ int native_cpu_disable(void)
 	if (cpu == 0)
 		return -EBUSY;
 
-	if (nmi_watchdog == NMI_LOCAL_APIC)
-		stop_apic_nmi_watchdog(NULL);
 	clear_local_APIC();
 
 	cpu_disable_common();

commit 0671b7674f42ab3a200401ea0e48d6f47d34acae
Merge: 0b2d8d9e562d 47f19a0814e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 27 18:38:55 2010 -0700

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      percpu: Remove the multi-page alignment facility
      x86-32: Allocate irq stacks seperate from percpu area
      x86-32, mm: Remove duplicated #include
      x86, printk: Get rid of <0> from stack output
      x86, kexec: Make sure to stop all CPUs before exiting the kernel
      x86/vsmp: Eliminate kconfig dependency warning

commit 22d4cd4c4dce6d7b7d9a7e396aa4f87fe7a649b1
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Oct 27 01:43:02 2010 -0400

    x86-32: Allocate irq stacks seperate from percpu area
    
    The percpu allocator cannot handle alignments larger than one
    page. Allocate the irq stacks seperately, and only keep the
    pointers as percpu data.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: tj@kernel.org
    LKML-Reference: <1288158182-1753-1-git-send-email-brgerst@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6af118511b4a..90baf567bbf7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1373,7 +1373,6 @@ void play_dead_common(void)
 {
 	idle_task_exit();
 	reset_lazy_tlbstate();
-	irq_ctx_exit(raw_smp_processor_id());
 	c1e_remove_cpu(raw_smp_processor_id());
 
 	mb();

commit ca1cab37d91cbe8a8333732540d43cabb54cfa85
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Oct 26 14:22:34 2010 -0700

    workqueues: s/ON_STACK/ONSTACK/
    
    Silly though it is, completions and wait_queue_heads use foo_ONSTACK
    (COMPLETION_INITIALIZER_ONSTACK, DECLARE_COMPLETION_ONSTACK,
    __WAIT_QUEUE_HEAD_INIT_ONSTACK and DECLARE_WAIT_QUEUE_HEAD_ONSTACK) so I
    guess workqueues should do the same thing.
    
    s/INIT_WORK_ON_STACK/INIT_WORK_ONSTACK/
    s/INIT_DELAYED_WORK_ON_STACK/INIT_DELAYED_WORK_ONSTACK/
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6af118511b4a..6c7faecd9e4a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -747,7 +747,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		.done	= COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
 	};
 
-	INIT_WORK_ON_STACK(&c_idle.work, do_fork_idle);
+	INIT_WORK_ONSTACK(&c_idle.work, do_fork_idle);
 
 	alternatives_smp_switch(1);
 

commit 10f2a2b0f68abf39c06cf519cbc1740fa50f900b
Merge: 8814011679d1 b40827fa7268
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 22 20:37:50 2010 -0700

    Merge branch 'x86-trampoline-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-trampoline-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86-32, mm: Add an initial page table for core bootstrapping

commit 4a60cfa9457749f7987fd4f3c956dbba5a281129
Merge: 62bea97f54d8 27afdf2008da
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 14:11:46 2010 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (96 commits)
      apic, x86: Use BIOS settings for IBS and MCE threshold interrupt LVT offsets
      apic, x86: Check if EILVT APIC registers are available (AMD only)
      x86: ioapic: Call free_irte only if interrupt remapping enabled
      arm: Use ARCH_IRQ_INIT_FLAGS
      genirq, ARM: Fix boot on ARM platforms
      genirq: Fix CONFIG_GENIRQ_NO_DEPRECATED=y build
      x86: Switch sparse_irq allocations to GFP_KERNEL
      genirq: Switch sparse_irq allocator to GFP_KERNEL
      genirq: Make sparse_lock a mutex
      x86: lguest: Use new irq allocator
      genirq: Remove the now unused sparse irq leftovers
      genirq: Sanitize dynamic irq handling
      genirq: Remove arch_init_chip_data()
      x86: xen: Sanitise sparse_irq handling
      x86: Use sane enumeration
      x86: uv: Clean up the direct access to irq_desc
      x86: Make io_apic.c local functions static
      genirq: Remove irq_2_iommu
      x86: Speed up the irq_remapped check in hot pathes
      intr_remap: Simplify the code further
      ...
    
    Fix up trivial conflicts in arch/x86/Kconfig

commit 5fe8321b8886d814e65952d74b207fe59e1096ea
Merge: 709d9f54cc18 fa47f7e52874
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:54:05 2010 -0700

    Merge branch 'x86-x2apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-x2apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, x2apic: Simplify apic init in SMP and UP builds
      x86, intr-remap: Remove IRTE setup duplicate code
      x86, intr-remap: Set redirection hint in the IRTE

commit 709d9f54cc1847a2d24224ffedec7fd4d0f3c714
Merge: cca8209ed962 b0f4c062fb6d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:53:24 2010 -0700

    Merge branch 'x86-vmware-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-vmware-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, paravirt: Remove alloc_pmd_clone hook, only used by VMI
      x86, vmware: Remove deprecated VMI kernel support
    
    Fix up trivial #include conflict in arch/x86/kernel/smpboot.c

commit 2a8b67fb72c4c4bc15fe8095e3ed613789c8b82f
Merge: b6f7e38dbb31 ce5f68246bf2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:45:38 2010 -0700

    Merge branch 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, hotplug: In the MWAIT case of play_dead, CLFLUSH the cache line
      x86, hotplug: Move WBINVD back outside the play_dead loop
      x86, hotplug: Use mwait to offline a processor, fix the legacy case
      x86, mwait: Move mwait constants to a common header file

commit b40827fa7268fda8a62490728a61c2856f33830b
Author: Borislav Petkov <bp@alien8.de>
Date:   Sat Aug 28 15:58:33 2010 +0200

    x86-32, mm: Add an initial page table for core bootstrapping
    
    This patch adds an initial page table with low mappings used exclusively
    for booting APs/resuming after ACPI suspend/machine restart. After this,
    there's no need to add low mappings to swapper_pg_dir and zap them later
    or create own swsusp PGD page solely for ACPI sleep needs - we have
    initial_page_table for that.
    
    Signed-off-by: Borislav Petkov <bp@alien8.de>
    LKML-Reference: <20101020070526.GA9588@liondog.tnic>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 63a1a5596ac0..e63bb5185855 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -298,22 +298,16 @@ notrace static void __cpuinit start_secondary(void *unused)
 	 * fragile that we want to limit the things done here to the
 	 * most necessary things.
 	 */
+	cpu_init();
+	preempt_disable();
+	smp_callin();
 
 #ifdef CONFIG_X86_32
-	/*
-	 * Switch away from the trampoline page-table
-	 *
-	 * Do this before cpu_init() because it needs to access per-cpu
-	 * data which may not be mapped in the trampoline page-table.
-	 */
+	/* switch away from the initial page table */
 	load_cr3(swapper_pg_dir);
 	__flush_tlb_all();
 #endif
 
-	cpu_init();
-	preempt_disable();
-	smp_callin();
-
 	/* otherwise gcc will move up smp_processor_id before the cpu_init */
 	barrier();
 	/*
@@ -772,7 +766,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 #ifdef CONFIG_X86_32
 	/* Stack for startup_32 can be just as for start_secondary onwards */
 	irq_ctx_init(cpu);
-	initial_page_table = __pa(&trampoline_pg_dir);
 #else
 	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
 	initial_gs = per_cpu_offset(cpu);
@@ -921,7 +914,6 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 	per_cpu(cpu_state, cpu) = CPU_UP_PREPARE;
 
 	err = do_boot_cpu(apicid, cpu);
-
 	if (err) {
 		pr_debug("do_boot_cpu failed %d\n", err);
 		return -EIO;

commit 4305df947ca1fd52867c8d56837a4e6b1e33167c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 28 15:01:33 2010 +0200

    x86: i8259: Convert to new irq_chip functions
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 87a8c6b00f8d..864b386f6c0e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -324,9 +324,9 @@ notrace static void __cpuinit start_secondary(void *unused)
 	check_tsc_sync_target();
 
 	if (nmi_watchdog == NMI_IO_APIC) {
-		legacy_pic->chip->mask(0);
+		legacy_pic->mask(0);
 		enable_NMI_through_LVT0();
-		legacy_pic->chip->unmask(0);
+		legacy_pic->unmask(0);
 	}
 
 	/* This must be done before setting cpu_online_mask */

commit d4fbe4f03557e1fd4d9bbb3a1957aad560f39e96
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Thu Sep 30 14:41:56 2010 +0200

    x86, amd: Use compute unit information to determine thread siblings
    
    This information is vital for different load balancing policies.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    LKML-Reference: <20100930124156.GF20545@loge.amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8b3bfc4dd708..bc2cc444844a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -397,6 +397,19 @@ void __cpuinit smp_store_cpu_info(int id)
 		identify_secondary_cpu(c);
 }
 
+static void __cpuinit link_thread_siblings(int cpu1, int cpu2)
+{
+	struct cpuinfo_x86 *c1 = &cpu_data(cpu1);
+	struct cpuinfo_x86 *c2 = &cpu_data(cpu2);
+
+	cpumask_set_cpu(cpu1, cpu_sibling_mask(cpu2));
+	cpumask_set_cpu(cpu2, cpu_sibling_mask(cpu1));
+	cpumask_set_cpu(cpu1, cpu_core_mask(cpu2));
+	cpumask_set_cpu(cpu2, cpu_core_mask(cpu1));
+	cpumask_set_cpu(cpu1, c2->llc_shared_map);
+	cpumask_set_cpu(cpu2, c1->llc_shared_map);
+}
+
 
 void __cpuinit set_cpu_sibling_map(int cpu)
 {
@@ -409,14 +422,13 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 		for_each_cpu(i, cpu_sibling_setup_mask) {
 			struct cpuinfo_x86 *o = &cpu_data(i);
 
-			if (c->phys_proc_id == o->phys_proc_id &&
-			    c->cpu_core_id == o->cpu_core_id) {
-				cpumask_set_cpu(i, cpu_sibling_mask(cpu));
-				cpumask_set_cpu(cpu, cpu_sibling_mask(i));
-				cpumask_set_cpu(i, cpu_core_mask(cpu));
-				cpumask_set_cpu(cpu, cpu_core_mask(i));
-				cpumask_set_cpu(i, c->llc_shared_map);
-				cpumask_set_cpu(cpu, o->llc_shared_map);
+			if (cpu_has(c, X86_FEATURE_TOPOEXT)) {
+				if (c->phys_proc_id == o->phys_proc_id &&
+				    c->compute_unit_id == o->compute_unit_id)
+					link_thread_siblings(cpu, i);
+			} else if (c->phys_proc_id == o->phys_proc_id &&
+				   c->cpu_core_id == o->cpu_core_id) {
+				link_thread_siblings(cpu, i);
 			}
 		}
 	} else {

commit ce5f68246bf2385d6174856708d0b746dc378f20
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Mon Sep 20 13:04:45 2010 -0700

    x86, hotplug: In the MWAIT case of play_dead, CLFLUSH the cache line
    
    When we're using MWAIT for play_dead, explicitly CLFLUSH the cache
    line before executing MONITOR.  This is a potential workaround for the
    Xeon 7400 erratum AAI65 after having a spurious wakeup and returning
    around the loop.  "Potential" here because it is not certain that that
    erratum could actually trigger; however, the CLFLUSH should be
    harmless.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Acked-by: Venkatesh Pallipadi <venki@google.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.kernel.org>
    Cc: Len Brown <lenb@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 55c80ffb8719..fdccfe9dc63d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1394,9 +1394,12 @@ static inline void mwait_play_dead(void)
 	unsigned int highest_cstate = 0;
 	unsigned int highest_subcstate = 0;
 	int i;
+	void *mwait_ptr;
 
 	if (!cpu_has(&current_cpu_data, X86_FEATURE_MWAIT))
 		return;
+	if (!cpu_has(&current_cpu_data, X86_FEATURE_CLFLSH))
+		return;
 	if (current_cpu_data.cpuid_level < CPUID_MWAIT_LEAF)
 		return;
 
@@ -1422,10 +1425,25 @@ static inline void mwait_play_dead(void)
 			(highest_subcstate - 1);
 	}
 
+	/*
+	 * This should be a memory location in a cache line which is
+	 * unlikely to be touched by other processors.  The actual
+	 * content is immaterial as it is not actually modified in any way.
+	 */
+	mwait_ptr = &current_thread_info()->flags;
+
 	wbinvd();
 
 	while (1) {
-		__monitor(&current_thread_info()->flags, 0, 0);
+		/*
+		 * The CLFLUSH is a workaround for erratum AAI65 for
+		 * the Xeon 7400 series.  It's not clear it is actually
+		 * needed, but it should be harmless in either case.
+		 * The WBINVD is insufficient due to the spurious-wakeup
+		 * case where we return around the loop.
+		 */
+		clflush(mwait_ptr);
+		__monitor(mwait_ptr, 0, 0);
 		mb();
 		__mwait(eax, 0);
 	}

commit a68e5c94f7d3dd64fef34dd5d97e365cae4bb42a
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Sep 17 17:06:46 2010 -0700

    x86, hotplug: Move WBINVD back outside the play_dead loop
    
    On processors with hyperthreading, when only one thread is offlined
    the other thread can cause a spurious wakeup on the idled thread.  We
    do not want to re-WBINVD when that happens.
    
    Ideally, we should simply skip WBINVD unless we're the last thread on
    a particular core to shut down, but there might be similar issues
    elsewhere in the system.
    
    Thus, revert to previous behavior of only WBINVD outside the loop.
    Partly as a result, remove the mb()'s around it: they are not
    necessary since wbinvd() is a serializing instruction, but they were
    intended to make sure the compiler didn't do any funny loop
    optimizations.
    
    Reported-by: Asit Mallick <asit.k.mallick@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Arjan van de Ven <arjan@linux.kernel.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.hl>
    LKML-Reference: <tip-ea53069231f9317062910d6e772cca4ce93de8c8@git.kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 07bf4233441d..55c80ffb8719 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1422,9 +1422,9 @@ static inline void mwait_play_dead(void)
 			(highest_subcstate - 1);
 	}
 
+	wbinvd();
+
 	while (1) {
-		mb();
-		wbinvd();
 		__monitor(&current_thread_info()->flags, 0, 0);
 		mb();
 		__mwait(eax, 0);
@@ -1433,11 +1433,10 @@ static inline void mwait_play_dead(void)
 
 static inline void hlt_play_dead(void)
 {
+	if (current_cpu_data.x86 >= 4)
+		wbinvd();
+
 	while (1) {
-		mb();
-		if (current_cpu_data.x86 >= 4)
-			wbinvd();
-		mb();
 		native_halt();
 	}
 }

commit ea53069231f9317062910d6e772cca4ce93de8c8
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Sep 17 15:39:11 2010 -0700

    x86, hotplug: Use mwait to offline a processor, fix the legacy case
    
    The code in native_play_dead() has a number of problems:
    
    1. We should use MWAIT when available, to put ourselves into a deeper
       sleep state.
    2. We use the existence of CLFLUSH to determine if WBINVD is safe, but
       that is totally bogus -- WBINVD is 486+, whereas CLFLUSH is a much
       later addition.
    3. We should do WBINVD inside the loop, just in case of something like
       setting an A bit on page tables.  Pointed out by Arjan van de Ven.
    
    This code is based in part of a previous patch by Venki Pallipadi, but
    unlike that patch this one keeps all the detection code local instead
    of pre-caching a bunch of information.  We're shutting down the CPU;
    there is absolutely no hurry.
    
    This patch moves all the code to C and deletes the global
    wbinvd_halt() which is broken anyway.
    
    Originally-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.hl>
    LKML-Reference: <20090522232230.162239000@intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8b3bfc4dd708..07bf4233441d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -62,6 +62,7 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mtrr.h>
+#include <asm/mwait.h>
 #include <asm/vmi.h>
 #include <asm/apic.h>
 #include <asm/setup.h>
@@ -1383,11 +1384,71 @@ void play_dead_common(void)
 	local_irq_disable();
 }
 
+/*
+ * We need to flush the caches before going to sleep, lest we have
+ * dirty data in our caches when we come back up.
+ */
+static inline void mwait_play_dead(void)
+{
+	unsigned int eax, ebx, ecx, edx;
+	unsigned int highest_cstate = 0;
+	unsigned int highest_subcstate = 0;
+	int i;
+
+	if (!cpu_has(&current_cpu_data, X86_FEATURE_MWAIT))
+		return;
+	if (current_cpu_data.cpuid_level < CPUID_MWAIT_LEAF)
+		return;
+
+	eax = CPUID_MWAIT_LEAF;
+	ecx = 0;
+	native_cpuid(&eax, &ebx, &ecx, &edx);
+
+	/*
+	 * eax will be 0 if EDX enumeration is not valid.
+	 * Initialized below to cstate, sub_cstate value when EDX is valid.
+	 */
+	if (!(ecx & CPUID5_ECX_EXTENSIONS_SUPPORTED)) {
+		eax = 0;
+	} else {
+		edx >>= MWAIT_SUBSTATE_SIZE;
+		for (i = 0; i < 7 && edx; i++, edx >>= MWAIT_SUBSTATE_SIZE) {
+			if (edx & MWAIT_SUBSTATE_MASK) {
+				highest_cstate = i;
+				highest_subcstate = edx & MWAIT_SUBSTATE_MASK;
+			}
+		}
+		eax = (highest_cstate << MWAIT_SUBSTATE_SIZE) |
+			(highest_subcstate - 1);
+	}
+
+	while (1) {
+		mb();
+		wbinvd();
+		__monitor(&current_thread_info()->flags, 0, 0);
+		mb();
+		__mwait(eax, 0);
+	}
+}
+
+static inline void hlt_play_dead(void)
+{
+	while (1) {
+		mb();
+		if (current_cpu_data.x86 >= 4)
+			wbinvd();
+		mb();
+		native_halt();
+	}
+}
+
 void native_play_dead(void)
 {
 	play_dead_common();
 	tboot_shutdown(TB_SHUTDOWN_WFS);
-	wbinvd_halt();
+
+	mwait_play_dead();	/* Only returns on failure */
+	hlt_play_dead();
 }
 
 #else /* ... !CONFIG_HOTPLUG_CPU */

commit fa47f7e52874683a9659df2f1f143105f676dc0f
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Aug 27 11:09:50 2010 -0700

    x86, x2apic: Simplify apic init in SMP and UP builds
    
    Move enable_IR_x2apic() inside the default_setup_apic_routing(),
    and for SMP platforms, move the default_setup_apic_routing() after
    smp_sanity_check(). This cleans up the code that tries to avoid multiple
    calls to default_setup_apic_routing() when smp_sanity_check() fails (which
    goes through the APIC_init_uniprocessor() path).
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <20100827181049.173087246@sbsiddha-MOBL3.sc.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8b3bfc4dd708..87a8c6b00f8d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1109,8 +1109,6 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	}
 	set_cpu_sibling_map(0);
 
-	enable_IR_x2apic();
-	default_setup_apic_routing();
 
 	if (smp_sanity_check(max_cpus) < 0) {
 		printk(KERN_INFO "SMP disabled\n");
@@ -1118,6 +1116,8 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		goto out;
 	}
 
+	default_setup_apic_routing();
+
 	preempt_disable();
 	if (read_apic_id() != boot_cpu_physical_apicid) {
 		panic("Boot APIC ID in local APIC unexpected (%d vs %d)",

commit 9863c90f682fba34cdc26c3437e8c00da6c83fa4
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Aug 23 14:49:11 2010 -0700

    x86, vmware: Remove deprecated VMI kernel support
    
    With the recent innovations in CPU hardware acceleration technologies
    from Intel and AMD, VMware ran a few experiments to compare these
    techniques to guest paravirtualization technique on VMware's platform.
    These hardware assisted virtualization techniques have outperformed the
    performance benefits provided by VMI in most of the workloads. VMware
    expects that these hardware features will be ubiquitous in a couple of
    years, as a result, VMware has started a phased retirement of this
    feature from the hypervisor.
    
    Please note that VMI has always been an optimization and non-VMI kernels
    still work fine on VMware's platform.
    Latest versions of VMware's product which support VMI are,
    Workstation 7.0 and VSphere 4.0 on ESX side, future maintainence
    releases for these products will continue supporting VMI.
    
    For more details about VMI retirement take a look at this,
    http://blogs.vmware.com/guestosguide/2009/09/vmi-retirement.html
    
    This feature removal was scheduled for 2.6.37 back in September 2009.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    LKML-Reference: <1282600151.19396.22.camel@ank32.eng.vmware.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8b3bfc4dd708..63a1a5596ac0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -62,7 +62,6 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mtrr.h>
-#include <asm/vmi.h>
 #include <asm/apic.h>
 #include <asm/setup.h>
 #include <asm/uv/uv.h>
@@ -311,7 +310,6 @@ notrace static void __cpuinit start_secondary(void *unused)
 	__flush_tlb_all();
 #endif
 
-	vmi_bringup();
 	cpu_init();
 	preempt_disable();
 	smp_callin();

commit d7c53c9e822a4fefa13a0cae76f3190bfd0d5c11
Author: Borislav Petkov <bp@amd64.org>
Date:   Thu Aug 19 20:10:29 2010 +0200

    x86, hotplug: Serialize CPU hotplug to avoid bringup concurrency issues
    
    When testing cpu hotplug code on 32-bit we kept hitting the "CPU%d:
    Stuck ??" message due to multiple cores concurrently accessing the
    cpu_callin_mask, among others.
    
    Since these codepaths are not protected from concurrent access due to
    the fact that there's no sane reason for making an already complex
    code unnecessarily more complex - we hit the issue only when insanely
    switching cores off- and online - serialize hotplugging cores on the
    sysfs level and be done with it.
    
    [ v2.1: fix !HOTPLUG_CPU build ]
    
    Cc: <stable@kernel.org>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <20100819181029.GC17171@aftab>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index abf4a86ffc54..8b3bfc4dd708 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -90,6 +90,25 @@ DEFINE_PER_CPU(int, cpu_state) = { 0 };
 static DEFINE_PER_CPU(struct task_struct *, idle_thread_array);
 #define get_idle_for_cpu(x)      (per_cpu(idle_thread_array, x))
 #define set_idle_for_cpu(x, p)   (per_cpu(idle_thread_array, x) = (p))
+
+/*
+ * We need this for trampoline_base protection from concurrent accesses when
+ * off- and onlining cores wildly.
+ */
+static DEFINE_MUTEX(x86_cpu_hotplug_driver_mutex);
+
+void cpu_hotplug_driver_lock()
+{
+        mutex_lock(&x86_cpu_hotplug_driver_mutex);
+}
+
+void cpu_hotplug_driver_unlock()
+{
+        mutex_unlock(&x86_cpu_hotplug_driver_mutex);
+}
+
+ssize_t arch_cpu_probe(const char *buf, size_t count) { return -1; }
+ssize_t arch_cpu_release(const char *buf, size_t count) { return -1; }
 #else
 static struct task_struct *idle_thread_array[NR_CPUS] __cpuinitdata ;
 #define get_idle_for_cpu(x)      (idle_thread_array[(x)])

commit fd89a137924e0710078c3ae855e7cec1c43cb845
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Aug 16 14:38:33 2010 +0200

    x86-32: Separate 1:1 pagetables from swapper_pg_dir
    
    This patch fixes machine crashes which occur when heavily exercising the
    CPU hotplug codepaths on a 32-bit kernel. These crashes are caused by
    AMD Erratum 383 and result in a fatal machine check exception. Here's
    the scenario:
    
    1. On 32-bit, the swapper_pg_dir page table is used as the initial page
    table for booting a secondary CPU.
    
    2. To make this work, swapper_pg_dir needs a direct mapping of physical
    memory in it (the low mappings). By adding those low, large page (2M)
    mappings (PAE kernel), we create the necessary conditions for Erratum
    383 to occur.
    
    3. Other CPUs which do not participate in the off- and onlining game may
    use swapper_pg_dir while the low mappings are present (when leave_mm is
    called). For all steps below, the CPU referred to is a CPU that is using
    swapper_pg_dir, and not the CPU which is being onlined.
    
    4. The presence of the low mappings in swapper_pg_dir can result
    in TLB entries for addresses below __PAGE_OFFSET to be established
    speculatively. These TLB entries are marked global and large.
    
    5. When the CPU with such TLB entry switches to another page table, this
    TLB entry remains because it is global.
    
    6. The process then generates an access to an address covered by the
    above TLB entry but there is a permission mismatch - the TLB entry
    covers a large global page not accessible to userspace.
    
    7. Due to this permission mismatch a new 4kb, user TLB entry gets
    established. Further, Erratum 383 provides for a small window of time
    where both TLB entries are present. This results in an uncorrectable
    machine check exception signalling a TLB multimatch which panics the
    machine.
    
    There are two ways to fix this issue:
    
            1. Always do a global TLB flush when a new cr3 is loaded and the
            old page table was swapper_pg_dir. I consider this a hack hard
            to understand and with performance implications
    
            2. Do not use swapper_pg_dir to boot secondary CPUs like 64-bit
            does.
    
    This patch implements solution 2. It introduces a trampoline_pg_dir
    which has the same layout as swapper_pg_dir with low_mappings. This page
    table is used as the initial page table of the booting CPU. Later in the
    bringup process, it switches to swapper_pg_dir and does a global TLB
    flush. This fixes the crashes in our test cases.
    
    -v2: switch to swapper_pg_dir right after entering start_secondary() so
    that we are able to access percpu data which might not be mapped in the
    trampoline page table.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    LKML-Reference: <20100816123833.GB28147@aftab>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a5e928b0cb5f..abf4a86ffc54 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -73,7 +73,6 @@
 
 #ifdef CONFIG_X86_32
 u8 apicid_2_node[MAX_APICID];
-static int low_mappings;
 #endif
 
 /* State of each CPU */
@@ -281,6 +280,18 @@ notrace static void __cpuinit start_secondary(void *unused)
 	 * fragile that we want to limit the things done here to the
 	 * most necessary things.
 	 */
+
+#ifdef CONFIG_X86_32
+	/*
+	 * Switch away from the trampoline page-table
+	 *
+	 * Do this before cpu_init() because it needs to access per-cpu
+	 * data which may not be mapped in the trampoline page-table.
+	 */
+	load_cr3(swapper_pg_dir);
+	__flush_tlb_all();
+#endif
+
 	vmi_bringup();
 	cpu_init();
 	preempt_disable();
@@ -299,12 +310,6 @@ notrace static void __cpuinit start_secondary(void *unused)
 		legacy_pic->chip->unmask(0);
 	}
 
-#ifdef CONFIG_X86_32
-	while (low_mappings)
-		cpu_relax();
-	__flush_tlb_all();
-#endif
-
 	/* This must be done before setting cpu_online_mask */
 	set_cpu_sibling_map(raw_smp_processor_id());
 	wmb();
@@ -750,6 +755,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 #ifdef CONFIG_X86_32
 	/* Stack for startup_32 can be just as for start_secondary onwards */
 	irq_ctx_init(cpu);
+	initial_page_table = __pa(&trampoline_pg_dir);
 #else
 	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
 	initial_gs = per_cpu_offset(cpu);
@@ -897,20 +903,8 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 
 	per_cpu(cpu_state, cpu) = CPU_UP_PREPARE;
 
-#ifdef CONFIG_X86_32
-	/* init low mem mapping */
-	clone_pgd_range(swapper_pg_dir, swapper_pg_dir + KERNEL_PGD_BOUNDARY,
-		min_t(unsigned long, KERNEL_PGD_PTRS, KERNEL_PGD_BOUNDARY));
-	flush_tlb_all();
-	low_mappings = 1;
-
 	err = do_boot_cpu(apicid, cpu);
 
-	zap_low_mappings(false);
-	low_mappings = 0;
-#else
-	err = do_boot_cpu(apicid, cpu);
-#endif
 	if (err) {
 		pr_debug("do_boot_cpu failed %d\n", err);
 		return -EIO;

commit d7a7c573936a86474c4a5090a45a4bc6e680c117
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Aug 9 17:20:33 2010 -0700

    x86, ia64, smp: use workqueues unconditionally during do_boot_cpu()
    
    Workqueues are now initialized as part of the early_initcall().  So they
    are available for use during cold boot process aswell.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 51620953b18a..a5e928b0cb5f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -735,12 +735,8 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		goto do_rest;
 	}
 
-	if (!keventd_up())
-		c_idle.work.func(&c_idle.work);
-	else {
-		schedule_work(&c_idle.work);
-		wait_for_completion(&c_idle.done);
-	}
+	schedule_work(&c_idle.work);
+	wait_for_completion(&c_idle.done);
 
 	if (IS_ERR(c_idle.idle)) {
 		printk("failed fork for CPU %d\n", cpu);

commit 3b7433b8a8a83c87972065b1852b7dcae691e464
Merge: 4a386c3e177c 6ee0578b4daa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Aug 7 12:42:58 2010 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq: (55 commits)
      workqueue: mark init_workqueues() as early_initcall()
      workqueue: explain for_each_*cwq_cpu() iterators
      fscache: fix build on !CONFIG_SYSCTL
      slow-work: kill it
      gfs2: use workqueue instead of slow-work
      drm: use workqueue instead of slow-work
      cifs: use workqueue instead of slow-work
      fscache: drop references to slow-work
      fscache: convert operation to use workqueue instead of slow-work
      fscache: convert object to use workqueue instead of slow-work
      workqueue: fix how cpu number is stored in work->data
      workqueue: fix mayday_mask handling on UP
      workqueue: fix build problem on !CONFIG_SMP
      workqueue: fix locking in retry path of maybe_create_worker()
      async: use workqueue for worker pool
      workqueue: remove WQ_SINGLE_CPU and use WQ_UNBOUND instead
      workqueue: implement unbound workqueue
      workqueue: prepare for WQ_UNBOUND implementation
      libata: take advantage of cmwq and remove concurrency limitations
      workqueue: fix worker management invocation without pending works
      ...
    
    Fixed up conflicts in fs/cifs/* as per Tejun. Other trivial conflicts in
    include/linux/workqueue.h, kernel/trace/Kconfig and kernel/workqueue.c

commit 68f202e4e87cfab4439568bf397fcc5c7cf8d729
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Jul 30 11:46:42 2010 -0700

    x86, mtrr: Use stop machine context to rendezvous all the cpu's
    
    Use the stop machine context rather than IPI's to rendezvous all the cpus for
    MTRR initialization that happens during cpu bringup or for MTRR modifications
    during runtime.
    
    This avoids deadlock scenario (reported by Prarit) like:
    
    cpu A holds a read_lock (tasklist_lock for example) with irqs enabled
    cpu B waits for the same lock with irqs disabled using write_lock_irq
    cpu C doing set_mtrr() (during AP bringup for example), which will try to
    rendezvous all the cpus using IPI's
    
    This will result in C and A come to the rendezvous point and waiting
    for B. B is stuck forever waiting for the lock and thus not
    reaching the rendezvous point.
    
    Using stop cpu (run in the process context of per cpu based keventd) to do
    this rendezvous, avoids this deadlock scenario.
    
    Also make sure all the cpu's are in the rendezvous handler before we proceed
    with the local_irq_save() on each cpu. This lock step disabling irqs on all
    the cpus will avoid other deadlock scenarios (for example involving
    with the blocking smp_call_function's etc).
    
       [ This problem is very old. Marking -stable only for 2.6.35 as the
         stop_one_cpu_nowait() API is present only in 2.6.35. Any older
         kernel interested in this fix need to do some more work in backporting
         this patch. ]
    
    Reported-by: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1280515602.2682.10.camel@sbsiddha-MOBL3.sc.intel.com>
    Acked-by: Prarit Bhargava <prarit@redhat.com>
    Cc: stable@kernel.org   [2.6.35]
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c4f33b2e77d6..11015fd1abbc 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -816,6 +816,13 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 			if (cpumask_test_cpu(cpu, cpu_callin_mask))
 				break;	/* It has booted */
 			udelay(100);
+			/*
+			 * Allow other tasks to run while we wait for the
+			 * AP to come online. This also gives a chance
+			 * for the MTRR work(triggered by the AP coming online)
+			 * to be completed in the stop machine context.
+			 */
+			schedule();
 		}
 
 		if (cpumask_test_cpu(cpu, cpu_callin_mask))

commit b71ab8c2025caef8db719aa41af0ed735dc543cd
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:14 2010 +0200

    workqueue: increase max_active of keventd and kill current_is_keventd()
    
    Define WQ_MAX_ACTIVE and create keventd with max_active set to half of
    it which means that keventd now can process upto WQ_MAX_ACTIVE / 2 - 1
    works concurrently.  Unless some combination can result in dependency
    loop longer than max_active, deadlock won't happen and thus it's
    unnecessary to check whether current_is_keventd() before trying to
    schedule a work.  Kill current_is_keventd().
    
    (Lockdep annotations are broken.  We need lock_map_acquire_read_norecurse())
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c4f33b2e77d6..4d90f376e985 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -735,7 +735,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		goto do_rest;
 	}
 
-	if (!keventd_up() || current_is_keventd())
+	if (!keventd_up())
 		c_idle.work.func(&c_idle.work);
 	else {
 		schedule_work(&c_idle.work);

commit 4adc8b71cc142f9a7b44b13b99aab38ba897c56f
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Tue Jun 1 21:04:55 2010 +0200

    x86, smpboot: Fix cores per node printing on boot
    
    Percpu initialization happens now after booting the cores on the
    machine and this causes them all to be displayed as belonging to
    node 0:
    
    Jun  8 05:57:21 kepek kernel: [    0.106999] Booting Node   0,
    Processors  #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 Ok.
    
    Use early_cpu_to_node() to get the correct node of each core
    instead.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Mike Travis <travis@sgi.com>
    LKML-Reference: <20100601190455.GA14237@aftab>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 37462f1ddba5..c4f33b2e77d6 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -686,7 +686,7 @@ static void __cpuinit do_fork_idle(struct work_struct *work)
 static void __cpuinit announce_cpu(int cpu, int apicid)
 {
 	static int current_node = -1;
-	int node = cpu_to_node(cpu);
+	int node = early_cpu_to_node(cpu);
 
 	if (system_state == SYSTEM_BOOTING) {
 		if (node != current_node) {

commit 5f2eb55026c91f8400ab4469aff88b2e201b5616
Author: Jan Beulich <JBeulich@novell.com>
Date:   Mon May 24 12:13:17 2010 -0700

    x86: "nosmp" command line option should force the system into UP mode
    
    Bits set in cpu_possible_mask prior to the execution of
    prefill_possible_map() (i.e.  when parsing ACPI or MPS tables) would
    prevent the SMP alternatives logic from switching to UP mode, plus
    unnecessary setup of per-CPU data for CPUs that can never come online.
    
    Additionally, without CONFIG_HOTPLUG_CPU disabled CPUs can never come
    online, and hence setting cpu_possible_mask bits for them is again a
    simple waste of resources.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    LKML-Reference: <201005241913.o4OJDH3Z010874@imap1.linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 763d815e27a0..37462f1ddba5 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1215,9 +1215,17 @@ __init void prefill_possible_map(void)
 	if (!num_processors)
 		num_processors = 1;
 
-	if (setup_possible_cpus == -1)
-		possible = num_processors + disabled_cpus;
-	else
+	i = setup_max_cpus ?: 1;
+	if (setup_possible_cpus == -1) {
+		possible = num_processors;
+#ifdef CONFIG_HOTPLUG_CPU
+		if (setup_max_cpus)
+			possible += disabled_cpus;
+#else
+		if (possible > i)
+			possible = i;
+#endif
+	} else
 		possible = setup_possible_cpus;
 
 	total_cpus = max_t(int, possible, num_processors + disabled_cpus);
@@ -1230,11 +1238,23 @@ __init void prefill_possible_map(void)
 		possible = nr_cpu_ids;
 	}
 
+#ifdef CONFIG_HOTPLUG_CPU
+	if (!setup_max_cpus)
+#endif
+	if (possible > i) {
+		printk(KERN_WARNING
+			"%d Processors exceeds max_cpus limit of %u\n",
+			possible, setup_max_cpus);
+		possible = i;
+	}
+
 	printk(KERN_INFO "SMP: Allowing %d CPUs, %d hotplug CPUs\n",
 		possible, max_t(int, possible - num_processors, 0));
 
 	for (i = 0; i < possible; i++)
 		set_cpu_possible(i, true);
+	for (; i < NR_CPUS; i++)
+		set_cpu_possible(i, false);
 
 	nr_cpu_ids = possible;
 }

commit 336f5899d287f06d8329e208fc14ce50f7ec9698
Merge: a4ab2773205e db217dece300
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Apr 5 11:37:28 2010 +0900

    Merge branch 'master' into export-slabh

commit 85257024096a96fc5c00ce59d685f62bbed3ad95
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Mar 23 19:30:52 2010 +0100

    x86: Move notify_cpu_starting() callback to a later stage
    
    Because we need to have cpu identification things done by the time we run
    CPU_STARTING notifiers.
    
    ( This init ordering will be relied on by the next fix. )
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1269353485.5109.48.camel@twins>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 06d98ae5a802..6808b934d6c0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -242,8 +242,6 @@ static void __cpuinit smp_callin(void)
 	end_local_APIC_setup();
 	map_cpu_to_logical_apicid();
 
-	notify_cpu_starting(cpuid);
-
 	/*
 	 * Need to setup vector mappings before we enable interrupts.
 	 */
@@ -264,6 +262,8 @@ static void __cpuinit smp_callin(void)
 	 */
 	smp_store_cpu_info(cpuid);
 
+	notify_cpu_starting(cpuid);
+
 	/*
 	 * Allow the master to continue.
 	 */

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 06d98ae5a802..be40f82b09af 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -49,6 +49,7 @@
 #include <linux/nmi.h>
 #include <linux/tboot.h>
 #include <linux/stackprotector.h>
+#include <linux/gfp.h>
 
 #include <asm/acpi.h>
 #include <asm/desc.h>

commit 36e9e1eab777e077f7484d309ff676d0568e27d1
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Mar 15 14:33:06 2010 -0800

    x86: Handle legacy PIC interrupts on all the cpu's
    
    Ingo Molnar reported that with the recent changes of not
    statically blocking IRQ0_VECTOR..IRQ15_VECTOR's on all the
    cpu's, broke an AMD platform (with Nvidia chipset) boot when
    "noapic" boot option is used.
    
    On this platform, legacy PIC interrupts are getting delivered to
    all the cpu's instead of just the boot cpu. Thus not
    initializing the vector to irq mapping for the legacy irq's
    resulted in not handling certain interrupts causing boot hang.
    
    Fix this by initializing the vector to irq mapping on all the
    logical cpu's, if the legacy IRQ is handled by the legacy PIC.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    [ -v2: io-apic-enabled improvement ]
    Acked-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    LKML-Reference: <1268692386.3296.43.camel@sbs-t61.sc.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a02e80c3c54b..06d98ae5a802 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -247,7 +247,7 @@ static void __cpuinit smp_callin(void)
 	/*
 	 * Need to setup vector mappings before we enable interrupts.
 	 */
-	__setup_vector_irq(smp_processor_id());
+	setup_vector_irq(smp_processor_id());
 	/*
 	 * Get our bogomips.
 	 *

commit 322aafa6645a48c3b7837ca7385f126ab78127fd
Merge: dd04265b028c c7bbf52aa4fa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 7 15:59:39 2010 -0800

    Merge branch 'x86-mrst-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-mrst-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (30 commits)
      x86, mrst: Fix whitespace breakage in apb_timer.c
      x86, mrst: Fix APB timer per cpu clockevent
      x86, mrst: Remove X86_MRST dependency on PCI_IOAPIC
      x86, olpc: Use pci subarch init for OLPC
      x86, pci: Add arch_init to x86_init abstraction
      x86, mrst: Add Kconfig dependencies for Moorestown
      x86, pci: Exclude Moorestown PCI code if CONFIG_X86_MRST=n
      x86, numaq: Make CONFIG_X86_NUMAQ depend on CONFIG_PCI
      x86, pci: Add sanity check for PCI fixed bar probing
      x86, legacy_irq: Remove duplicate vector assigment
      x86, legacy_irq: Remove left over nr_legacy_irqs
      x86, mrst: Platform clock setup code
      x86, apbt: Moorestown APB system timer driver
      x86, mrst: Add vrtc platform data setup code
      x86, mrst: Add platform timer info parsing code
      x86, mrst: Fill in PCI functions in x86_init layer
      x86, mrst: Add dummy legacy pic to platform setup
      x86/PCI: Moorestown PCI support
      x86, ioapic: Add dummy ioapic functions
      x86, ioapic: Early enable ioapic for timer irq
      ...
    
    Fixed up semantic conflict of new clocksources due to commit
    17622339af25 ("clocksource: add argument to resume callback").

commit fb7b096d949fa852442ed9d8f982bce526ccfe7e
Merge: a626b46e17d0 fad539956c9e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 3 08:15:37 2010 -0800

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (25 commits)
      x86: Fix out of order of gsi
      x86: apic: Fix mismerge, add arch_probe_nr_irqs() again
      x86, irq: Keep chip_data in create_irq_nr and destroy_irq
      xen: Remove unnecessary arch specific xen irq functions.
      smp: Use nr_cpus= to set nr_cpu_ids early
      x86, irq: Remove arch_probe_nr_irqs
      sparseirq: Use radix_tree instead of ptrs array
      sparseirq: Change irq_desc_ptrs to static
      init: Move radix_tree_init() early
      irq: Remove unnecessary bootmem code
      x86: Add iMac9,1 to pci_reboot_dmi_table
      x86: Convert i8259_lock to raw_spinlock
      x86: Convert nmi_lock to raw_spinlock
      x86: Convert ioapic_lock and vector_lock to raw_spinlock
      x86: Avoid race condition in pci_enable_msix()
      x86: Fix SCI on IOAPIC != 0
      x86, ia32_aout: do not kill argument mapping
      x86, irq: Move __setup_vector_irq() before the first irq enable in cpu online path
      x86, irq: Update the vector domain for legacy irqs handled by io-apic
      x86, irq: Don't block IRQ0_VECTOR..IRQ15_VECTOR's on all cpu's
      ...

commit c7e15899d07c9813c1aa96b21699d2d9c8314c4b
Merge: f6a0b5cd34d6 78c06176466c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 28 10:59:18 2010 -0800

    Merge branch 'x86-pci-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-pci-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Enable NMI on all cpus on UV
      vgaarb: Add user selectability of the number of GPUS in a system
      vgaarb: Fix VGA arbiter to accept PCI domains other than 0
      x86, uv: Update UV arch to target Legacy VGA I/O correctly.
      pci: Update pci_set_vga_state() to call arch functions

commit 78c06176466cbd1b3f0f67709d3023c40dbebcbd
Author: Russ Anderson <rja@sgi.com>
Date:   Fri Feb 26 10:49:12 2010 -0600

    x86: Enable NMI on all cpus on UV
    
    Enable NMI on all cpus in UV system and add an NMI handler
    to dump_stack on each cpu.
    
    By default on x86 all the cpus except the boot cpu have NMI
    masked off.  This patch enables NMI on all cpus in UV system
    and adds an NMI handler to dump_stack on each cpu.  This
    way if a system hangs we can NMI the machine and get a
    backtrace from all the cpus.
    
    Version 2: Use x86_platform driver mechanism for nmi init, per
               Ingo's suggestion.
    
    Version 3: Clean up Ingo's nits.
    
    Signed-off-by: Russ Anderson <rja@sgi.com>
    LKML-Reference: <20100226164912.GA24439@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 678d0b8c26f3..838a118876c0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -320,6 +320,7 @@ notrace static void __cpuinit start_secondary(void *unused)
 	unlock_vector_lock();
 	ipi_call_unlock();
 	per_cpu(cpu_state, smp_processor_id()) = CPU_ONLINE;
+	x86_platform.nmi_init();
 
 	/* enable local interrupts */
 	local_irq_enable();

commit 54b56170e4517e9606b245c3f805fc96baa059f0
Merge: 1f91233c26fd d02e30c31c57
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Feb 22 16:25:18 2010 -0800

    Merge remote branch 'origin/x86/apic' into x86/mrst
    
    Conflicts:
            arch/x86/kernel/apic/io_apic.c

commit d02e30c31c57683a66ed68a1bcff900ca78f6d56
Merge: 0fdc7a8022c3 aef55d4922e6
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Feb 22 16:20:34 2010 -0800

    Merge branch 'x86/irq' into x86/apic
    
    Merge reason:
            Conflicts in arch/x86/kernel/apic/io_apic.c
    
    Resolved Conflicts:
            arch/x86/kernel/apic/io_apic.c
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

commit aef55d4922e62a0d887e60d87319f3718aec6ced
Merge: ca4dbc668412 eb5b37940628
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Sat Feb 20 21:51:42 2010 -0800

    Merge branch 'x86/urgent' into x86/irq
    
    Merge reason: conflict in arch/x86/kernel/apic/io_apic.c
    
    Resolved Conflicts:
            arch/x86/kernel/apic/io_apic.c
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

commit b81bb373a7e832a43921356aa1291044d7f52fb1
Author: Jacob Pan <jacob.jun.pan@intel.com>
Date:   Mon Nov 9 11:27:04 2009 -0800

    x86, pic: Make use of legacy_pic abstraction
    
    This patch replaces legacy PIC-related global variable and functions
    with the new legacy_pic abstraction.
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@intel.com>
    LKML-Reference: <43F901BD926A4E43B106BF17856F07559FB80D04@orsmsx508.amr.corp.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3e6150d421e4..f7a52f4a21a5 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -68,6 +68,7 @@
 #include <linux/mc146818rtc.h>
 
 #include <asm/smpboot_hooks.h>
+#include <asm/i8259.h>
 
 #ifdef CONFIG_X86_32
 u8 apicid_2_node[MAX_APICID];
@@ -287,9 +288,9 @@ notrace static void __cpuinit start_secondary(void *unused)
 	check_tsc_sync_target();
 
 	if (nmi_watchdog == NMI_IO_APIC) {
-		disable_8259A_irq(0);
+		legacy_pic->chip->mask(0);
 		enable_NMI_through_LVT0();
-		enable_8259A_irq(0);
+		legacy_pic->chip->unmask(0);
 	}
 
 #ifdef CONFIG_X86_32

commit 35f720c5930f689647d51ad77e2a8d6f0abf66c8
Author: Jacob Pan <jacob.jun.pan@intel.com>
Date:   Thu Sep 17 07:36:43 2009 -0700

    x86: Initialize stack canary in secondary start
    
    Some secondary clockevent setup code needs to call request_irq, which
    will cause fake stack check failure in schedule() if voluntary
    preemption model is chosen.  It is safe to have stack canary
    initialized here early, since start_secondary() does not return.
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@intel.com>
    LKML-Reference: <43F901BD926A4E43B106BF17856F07559FB80D02@orsmsx508.amr.corp.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b4e870cbdc60..3e6150d421e4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -48,6 +48,7 @@
 #include <linux/err.h>
 #include <linux/nmi.h>
 #include <linux/tboot.h>
+#include <linux/stackprotector.h>
 
 #include <asm/acpi.h>
 #include <asm/desc.h>
@@ -324,6 +325,9 @@ notrace static void __cpuinit start_secondary(void *unused)
 	/* enable local interrupts */
 	local_irq_enable();
 
+	/* to prevent fake stack check failure in clock setup */
+	boot_init_stack_canary();
+
 	x86_cpuinit.setup_percpu_clockev();
 
 	wmb();

commit 2b633e3fac5efada088b57d31e65401f22bcc18f
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Wed Feb 10 01:20:37 2010 -0800

    smp: Use nr_cpus= to set nr_cpu_ids early
    
    On x86, before prefill_possible_map(), nr_cpu_ids will be NR_CPUS aka
    CONFIG_NR_CPUS.
    
    Add nr_cpus= to set nr_cpu_ids. so we can simulate cpus <=8 are installed on
    normal config.
    
    -v2: accordging to Christoph, acpi_numa_init should use nr_cpu_ids in stead of
         NR_CPUS.
    -v3: add doc in kernel-parameters.txt according to Andrew.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <1265793639-15071-34-git-send-email-yinghai@kernel.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Tony Luck <tony.luck@intel.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 678d0b8c26f3..eff2fe175422 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1213,11 +1213,12 @@ __init void prefill_possible_map(void)
 
 	total_cpus = max_t(int, possible, num_processors + disabled_cpus);
 
-	if (possible > CONFIG_NR_CPUS) {
+	/* nr_cpu_ids could be reduced via nr_cpus= */
+	if (possible > nr_cpu_ids) {
 		printk(KERN_WARNING
 			"%d Processors exceeds NR_CPUS limit of %d\n",
-			possible, CONFIG_NR_CPUS);
-		possible = CONFIG_NR_CPUS;
+			possible, nr_cpu_ids);
+		possible = nr_cpu_ids;
 	}
 
 	printk(KERN_INFO "SMP: Allowing %d CPUs, %d hotplug CPUs\n",

commit 681ee44d40d7c93b42118320e4620d07d8704fd6
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Tue Feb 9 18:01:44 2010 -0800

    x86, apic: Don't use logical-flat mode when CPU hotplug may exceed 8 CPUs
    
    We need to fall back from logical-flat APIC mode to physical-flat mode
    when we have more than 8 CPUs.  However, in the presence of CPU
    hotplug(with bios listing not enabled but possible cpus as disabled cpus in
    MADT), we have to consider the number of possible CPUs rather than
    the number of current CPUs; otherwise we may cross the 8-CPU boundary
    when CPUs are added later.
    
    32bit apic code can use more cleanups (like the removal of vendor checks in
    32bit default_setup_apic_routing()) and more unifications with 64bit code.
    Yinghai has some patches in works already. This patch addresses the boot issue
    that is reported in the virtualization guest context.
    
    [ hpa: incorporated function annotation feedback from Yinghai Lu ]
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1265767304.2833.19.camel@sbs-t61.sc.intel.com>
    Acked-by: Shaohui Zheng <shaohui.zheng@intel.com>
    Reviewed-by: Yinghai Lu <yinghai@kernel.org>
    Cc: <stable@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 678d0b8c26f3..b4e870cbdc60 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1083,9 +1083,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	set_cpu_sibling_map(0);
 
 	enable_IR_x2apic();
-#ifdef CONFIG_X86_64
 	default_setup_apic_routing();
-#endif
 
 	if (smp_sanity_check(max_cpus) < 0) {
 		printk(KERN_INFO "SMP disabled\n");

commit 9d133e5db993d577bd868b54083869fe5479fcff
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Jan 29 11:42:21 2010 -0800

    x86, irq: Move __setup_vector_irq() before the first irq enable in cpu online path
    
    Lowest priority delivery of logical flat mode is broken on some systems,
    such that even when IO-APIC RTE says deliver the interrupt to a particular CPU,
    interrupt subsystem delivers the interrupt to totally different CPU.
    
    For example, this behavior was observed on a P4 based system with SiS chipset
    which was reported by Li Zefan. We have been handling this kind of behavior by
    making sure that in logical flat mode, we assign the same vector to irq
    mappings on all the 8 possible logical cpu's.
    
    But we have been doing this initial assignment (__setup_vector_irq()) a little
    late (before which interrupts were already enabled for a short duration).
    
    Move the __setup_vector_irq() before the first irq enable point in the
    cpu online path to avoid the issue of not handling some interrupts that
    wrongly hit the cpu which is still coming online.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <20100129194330.283696385@sbs-t61.sc.intel.com>
    Tested-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 678d0b8c26f3..b2ebcba729d9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -241,6 +241,11 @@ static void __cpuinit smp_callin(void)
 	map_cpu_to_logical_apicid();
 
 	notify_cpu_starting(cpuid);
+
+	/*
+	 * Need to setup vector mappings before we enable interrupts.
+	 */
+	__setup_vector_irq(smp_processor_id());
 	/*
 	 * Get our bogomips.
 	 *
@@ -315,7 +320,6 @@ notrace static void __cpuinit start_secondary(void *unused)
 	 */
 	ipi_call_lock();
 	lock_vector_lock();
-	__setup_vector_irq(smp_processor_id());
 	set_cpu_online(smp_processor_id(), true);
 	unlock_vector_lock();
 	ipi_call_unlock();

commit 2eaad1fddd7450a48ad464229775f97fbfe8af36
Author: Mike Travis <travis@sgi.com>
Date:   Thu Dec 10 17:19:36 2009 -0800

    x86: Limit the number of processor bootup messages
    
    When there are a large number of processors in a system, there
    is an excessive amount of messages sent to the system console.
    It's estimated that with 4096 processors in a system, and the
    console baudrate set to 56K, the startup messages will take
    about 84 minutes to clear the serial port.
    
    This set of patches limits the number of repetitious messages
    which contain no additional information.  Much of this information
    is obtainable from the /proc and /sysfs.   Some of the messages
    are also sent to the kernel log buffer as KERN_DEBUG messages so
    dmesg can be used to examine more closely any details specific to
    a problem.
    
    The new cpu bootup sequence for system_state == SYSTEM_BOOTING:
    
    Booting Node   0, Processors  #1 #2 #3 #4 #5 #6 #7 Ok.
    Booting Node   1, Processors  #8 #9 #10 #11 #12 #13 #14 #15 Ok.
    ...
    Booting Node   3, Processors  #56 #57 #58 #59 #60 #61 #62 #63 Ok.
    Brought up 64 CPUs
    
    After the system is running, a single line boot message is displayed
    when CPU's are hotplugged on:
    
        Booting Node %d Processor %d APIC 0x%x
    
    Status of the following lines:
    
        CPU: Physical Processor ID:         printed once (for boot cpu)
        CPU: Processor Core ID:             printed once (for boot cpu)
        CPU: Hyper-Threading is disabled    printed once (for boot cpu)
        CPU: Thermal monitoring enabled     printed once (for boot cpu)
        CPU %d/0x%x -> Node %d:             removed
        CPU %d is now offline:              only if system_state == RUNNING
        Initializing CPU#%d:                KERN_DEBUG
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    LKML-Reference: <4B219E28.8080601@sgi.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 29e6744f51e3..678d0b8c26f3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -671,6 +671,26 @@ static void __cpuinit do_fork_idle(struct work_struct *work)
 	complete(&c_idle->done);
 }
 
+/* reduce the number of lines printed when booting a large cpu count system */
+static void __cpuinit announce_cpu(int cpu, int apicid)
+{
+	static int current_node = -1;
+	int node = cpu_to_node(cpu);
+
+	if (system_state == SYSTEM_BOOTING) {
+		if (node != current_node) {
+			if (current_node > (-1))
+				pr_cont(" Ok.\n");
+			current_node = node;
+			pr_info("Booting Node %3d, Processors ", node);
+		}
+		pr_cont(" #%d%s", cpu, cpu == (nr_cpu_ids - 1) ? " Ok.\n" : "");
+		return;
+	} else
+		pr_info("Booting Node %d Processor %d APIC 0x%x\n",
+			node, cpu, apicid);
+}
+
 /*
  * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad
  * (ie clustered apic addressing mode), this is a LOGICAL apic ID.
@@ -737,9 +757,8 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	/* start_ip had better be page-aligned! */
 	start_ip = setup_trampoline();
 
-	/* So we see what's up   */
-	printk(KERN_INFO "Booting processor %d APIC 0x%x ip 0x%lx\n",
-			  cpu, apicid, start_ip);
+	/* So we see what's up */
+	announce_cpu(cpu, apicid);
 
 	/*
 	 * This grunge runs the startup process for
@@ -788,21 +807,17 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 			udelay(100);
 		}
 
-		if (cpumask_test_cpu(cpu, cpu_callin_mask)) {
-			/* number CPUs logically, starting from 1 (BSP is 0) */
-			pr_debug("OK.\n");
-			printk(KERN_INFO "CPU%d: ", cpu);
-			print_cpu_info(&cpu_data(cpu));
-			pr_debug("CPU has booted.\n");
-		} else {
+		if (cpumask_test_cpu(cpu, cpu_callin_mask))
+			pr_debug("CPU%d: has booted.\n", cpu);
+		else {
 			boot_error = 1;
 			if (*((volatile unsigned char *)trampoline_base)
 					== 0xA5)
 				/* trampoline started but...? */
-				printk(KERN_ERR "Stuck ??\n");
+				pr_err("CPU%d: Stuck ??\n", cpu);
 			else
 				/* trampoline code not run */
-				printk(KERN_ERR "Not responding.\n");
+				pr_err("CPU%d: Not responding.\n", cpu);
 			if (apic->inquire_remote_apic)
 				apic->inquire_remote_apic(apicid);
 		}
@@ -1293,14 +1308,16 @@ void native_cpu_die(unsigned int cpu)
 	for (i = 0; i < 10; i++) {
 		/* They ack this in play_dead by setting CPU_DEAD */
 		if (per_cpu(cpu_state, cpu) == CPU_DEAD) {
-			printk(KERN_INFO "CPU %d is now offline\n", cpu);
+			if (system_state == SYSTEM_RUNNING)
+				pr_info("CPU %u is now offline\n", cpu);
+
 			if (1 == num_online_cpus())
 				alternatives_smp_switch(0);
 			return;
 		}
 		msleep(100);
 	}
-	printk(KERN_ERR "CPU %u didn't die...\n", cpu);
+	pr_err("CPU %u didn't die...\n", cpu);
 }
 
 void play_dead_common(void)

commit d71cb81af3817193bc605de061da0499934263a6
Merge: ab1831b0b878 dc186ad741c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 10 09:35:44 2009 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq:
      workqueue: Add debugobjects support

commit 6d20792e85187b27ae3d1b76678a2dd7025e8bc2
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Tue Dec 1 15:31:18 2009 -0800

    x86: Remove unnecessary mdelay() from cpu_disable_common()
    
    fixup_irqs() already has a mdelay(). Remove the extra and
    unnecessary mdelay() from cpu_disable_common().
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Maciej W. Rozycki <macro@linux-mips.org>
    Cc: ebiederm@xmission.com
    Cc: garyhade@us.ibm.com
    LKML-Reference: <20091201233335.232177348@sbs-t61.sc.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 565ebc65920e..324f2a44c221 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1250,16 +1250,7 @@ static void __ref remove_cpu_from_maps(int cpu)
 void cpu_disable_common(void)
 {
 	int cpu = smp_processor_id();
-	/*
-	 * HACK:
-	 * Allow any queued timer interrupts to get serviced
-	 * This is only a temporary solution until we cleanup
-	 * fixup_irqs as we do for IA64.
-	 */
-	local_irq_enable();
-	mdelay(1);
 
-	local_irq_disable();
 	remove_siblinginfo(cpu);
 
 	/* It's now safe to remove this processor from the online map */

commit dc186ad741c12ae9ecac8b89e317ef706fdaf8f6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 16 01:09:48 2009 +0900

    workqueue: Add debugobjects support
    
    Add debugobject support to track the life time of work_structs.
    
    While at it, remove duplicate definition of
    INIT_DELAYED_WORK_ON_STACK().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 565ebc65920e..ba43dfed353d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -687,7 +687,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		.done	= COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
 	};
 
-	INIT_WORK(&c_idle.work, do_fork_idle);
+	INIT_WORK_ON_STACK(&c_idle.work, do_fork_idle);
 
 	alternatives_smp_switch(1);
 
@@ -713,6 +713,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 
 	if (IS_ERR(c_idle.idle)) {
 		printk("failed fork for CPU %d\n", cpu);
+		destroy_work_on_stack(&c_idle.work);
 		return PTR_ERR(c_idle.idle);
 	}
 
@@ -831,6 +832,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		smpboot_restore_warm_reset_vector();
 	}
 
+	destroy_work_on_stack(&c_idle.work);
 	return boot_error;
 }
 

commit 79f5599772ac2f138d7a75b8f3f06a93f09c75f7
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon Jun 15 14:58:26 2009 +0800

    cpumask: use zalloc_cpumask_var() where possible
    
    Remove open-coded zalloc_cpumask_var() and zalloc_cpumask_var_node().
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 09c5e077dff7..565ebc65920e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1059,12 +1059,9 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 #endif
 	current_thread_info()->cpu = 0;  /* needed? */
 	for_each_possible_cpu(i) {
-		alloc_cpumask_var(&per_cpu(cpu_sibling_map, i), GFP_KERNEL);
-		alloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);
-		alloc_cpumask_var(&cpu_data(i).llc_shared_map, GFP_KERNEL);
-		cpumask_clear(per_cpu(cpu_core_map, i));
-		cpumask_clear(per_cpu(cpu_sibling_map, i));
-		cpumask_clear(cpu_data(i).llc_shared_map);
+		zalloc_cpumask_var(&per_cpu(cpu_sibling_map, i), GFP_KERNEL);
+		zalloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);
+		zalloc_cpumask_var(&cpu_data(i).llc_shared_map, GFP_KERNEL);
 	}
 	set_cpu_sibling_map(0);
 

commit 78f28b7c555359c67c2a0d23f7436e915329421e
Merge: 3240a77b515f 7bd867dfb4e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 18 14:05:47 2009 -0700

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (38 commits)
      x86: Move get/set_wallclock to x86_platform_ops
      x86: platform: Fix section annotations
      x86: apic namespace cleanup
      x86: Distangle ioapic and i8259
      x86: Add Moorestown early detection
      x86: Add hardware_subarch ID for Moorestown
      x86: Add early platform detection
      x86: Move tsc_init to late_time_init
      x86: Move tsc_calibration to x86_init_ops
      x86: Replace the now identical time_32/64.c by time.c
      x86: time_32/64.c unify profile_pc
      x86: Move calibrate_cpu to tsc.c
      x86: Make timer setup and global variables the same in time_32/64.c
      x86: Remove mca bus ifdef from timer interrupt
      x86: Simplify timer_ack magic in time_32.c
      x86: Prepare unification of time_32/64.c
      x86: Remove do_timer hook
      x86: Add timer_init to x86_init_ops
      x86: Move percpu clockevents setup to x86_init_ops
      x86: Move xen_post_allocator_init into xen_pagetable_setup_done
      ...
    
    Fix up conflicts in arch/x86/include/asm/io_apic.h

commit 227423904c709a8e60245c97081bbeb4fb500655
Merge: 1aaf2e59135f fa526d0d641b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 15 09:19:38 2009 -0700

    Merge branch 'x86-pat-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-pat-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, pat: Fix cacheflush address in change_page_attr_set_clr()
      mm: remove !NUMA condition from PAGEFLAGS_EXTENDED condition set
      x86: Fix earlyprintk=dbgp for machines without NX
      x86, pat: Sanity check remap_pfn_range for RAM region
      x86, pat: Lookup the protection from memtype list on vm_insert_pfn()
      x86, pat: Add lookup_memtype to get the current memtype of a paddr
      x86, pat: Use page flags to track memtypes of RAM pages
      x86, pat: Generalize the use of page flag PG_uncached
      x86, pat: Add rbtree to do quick lookup in memtype tracking
      x86, pat: Add PAT reserve free to io_mapping* APIs
      x86, pat: New i/f for driver to request memtype for IO regions
      x86, pat: ioremap to follow same PAT restrictions as other PAT users
      x86, pat: Keep identity maps consistent with mmaps even when pat_disabled
      x86, mtrr: make mtrr_aps_delayed_init static bool
      x86, pat/mtrr: Rendezvous all the cpus for MTRR/PAT init
      generic-ipi: Allow cpus not yet online to call smp_call_function with irqs disabled
      x86: Fix an incorrect argument of reserve_bootmem()
      x86: Fix system crash when loading with "reservetop" parameter

commit 1aaf2e59135fd67321f47c11c64a54aac27014e9
Merge: 66a4fe0cb80a 936e894a976d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 15 09:19:20 2009 -0700

    Merge branch 'x86-txt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-txt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, intel_txt: clean up the impact on generic code, unbreak non-x86
      x86, intel_txt: Handle ACPI_SLEEP without X86_TRAMPOLINE
      x86, intel_txt: Fix typos in Kconfig help
      x86, intel_txt: Factor out the code for S3 setup
      x86, intel_txt: tboot.c needs <asm/fixmap.h>
      intel_txt: Force IOMMU on for Intel TXT launch
      x86, intel_txt: Intel TXT Sx shutdown support
      x86, intel_txt: Intel TXT reboot/halt shutdown support
      x86, intel_txt: Intel TXT boot support

commit 5a925b4282d7f805deafde62001a83dbaf8be275
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Thu Sep 3 09:44:28 2009 +0200

    x86, sched: Workaround broken sched domain creation for AMD Magny-Cours
    
    Current sched domain creation code can't handle multi-node processors.
    When switching to power_savings scheduling errors show up and
    system might hang later on (due to broken sched domain hierarchy):
    
      # echo 0  >> /sys/devices/system/cpu/sched_mc_power_savings
      CPU0 attaching sched-domain:
       domain 0: span 0-5 level MC
        groups: 0 1 2 3 4 5
        domain 1: span 0-23 level NODE
         groups: 0-5 6-11 18-23 12-17
      ...
      # echo 1  >> /sys/devices/system/cpu/sched_mc_power_savings
      CPU0 attaching sched-domain:
       domain 0: span 0-11 level MC
        groups: 0 1 2 3 4 5 6 7 8 9 10 11
      ERROR: parent span is not a superset of domain->span
        domain 1: span 0-5 level CPU
      ERROR: domain->groups does not contain CPU0
         groups: 6-11 (__cpu_power = 12288)
      ERROR: groups don't span domain->span
         domain 2: span 0-23 level NODE
          groups:
      ERROR: domain->cpu_power not set
    
      ERROR: groups don't span domain->span
      ...
    
    Fixing all aspects of power-savings scheduling for Magny-Cours needs
    some larger changes in the sched domain creation code.
    
    As a short-term and temporary workaround avoid the problems by
    extending "the worst possible hack" ;-(
    and always use llc_shared_map on AMD Magny-Cours when MC domain span
    is calculated.
    
    With this I get:
    
      # echo 1  >> /sys/devices/system/cpu/sched_mc_power_savings
      CPU0 attaching sched-domain:
       domain 0: span 0-5 level MC
        groups: 0 1 2 3 4 5
        domain 1: span 0-5 level CPU
         groups: 0-5 (__cpu_power = 6144)
         domain 2: span 0-23 level NODE
          groups: 0-5 (__cpu_power = 6144) 6-11 (__cpu_power = 6144) 18-23 (__cpu_power = 6144) 12-17 (__cpu_power = 6144)
      ...
    
    I.e. no errors during sched domain creation, no system hangs, and also
    mc_power_savings scheduling works to a certain extend.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2fecda69ee64..c36cc1452cdc 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -434,7 +434,8 @@ const struct cpumask *cpu_coregroup_mask(int cpu)
 	 * For perf, we return last level cache shared map.
 	 * And for power savings, we return cpu_core_map
 	 */
-	if (sched_mc_power_savings || sched_smt_power_savings)
+	if ((sched_mc_power_savings || sched_smt_power_savings) &&
+	    !(cpu_has(c, X86_FEATURE_AMD_DCM)))
 		return cpu_core_mask(cpu);
 	else
 		return c->llc_shared_map;

commit 69575d388603365f2afbf4166df93152df59b165
Author: Shane Wang <shane.wang@intel.com>
Date:   Tue Sep 1 18:25:07 2009 -0700

    x86, intel_txt: clean up the impact on generic code, unbreak non-x86
    
    Move tboot.h from asm to linux to fix the build errors of intel_txt
    patch on non-X86 platforms. Remove the tboot code from generic code
    init/main.c and kernel/cpu.c.
    
    Signed-off-by: Shane Wang <shane.wang@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 61cc40887c48..7d9d8eea20a0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -47,6 +47,7 @@
 #include <linux/bootmem.h>
 #include <linux/err.h>
 #include <linux/nmi.h>
+#include <linux/tboot.h>
 
 #include <asm/acpi.h>
 #include <asm/desc.h>
@@ -62,7 +63,6 @@
 #include <asm/vmi.h>
 #include <asm/apic.h>
 #include <asm/setup.h>
-#include <asm/tboot.h>
 #include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>
 

commit 736decac643e8982655e22ac7f0e5e61c5b7f9bd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 19 12:35:53 2009 +0200

    x86: Move percpu clockevents setup to x86_init_ops
    
    paravirt overrides the setup of the default apic timers as per cpu
    timers. Moorestown needs to override that as well.
    
    Move it to x86_init_ops setup and create a separate x86_cpuinit struct
    which holds the function for the secondary evtl. hotplugabble CPUs.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2fecda69ee64..6eb81a87b4b7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -323,7 +323,7 @@ notrace static void __cpuinit start_secondary(void *unused)
 	/* enable local interrupts */
 	local_irq_enable();
 
-	setup_secondary_clock();
+	x86_cpuinit.setup_percpu_clockev();
 
 	wmb();
 	cpu_idle();
@@ -1112,7 +1112,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 	printk(KERN_INFO "CPU%d: ", 0);
 	print_cpu_info(&cpu_data(0));
-	setup_boot_clock();
+	x86_init.timers.setup_percpu_clockev();
 
 	if (is_uv_system())
 		uv_system_init();

commit d0af9eed5aa91b6b7b5049cae69e5ea956fd85c3
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed Aug 19 18:05:36 2009 -0700

    x86, pat/mtrr: Rendezvous all the cpus for MTRR/PAT init
    
    SDM Vol 3a section titled "MTRR considerations in MP systems" specifies
    the need for synchronizing the logical cpu's while initializing/updating
    MTRR.
    
    Currently Linux kernel does the synchronization of all cpu's only when
    a single MTRR register is programmed/updated. During an AP online
    (during boot/cpu-online/resume)  where we initialize all the MTRR/PAT registers,
    we don't follow this synchronization algorithm.
    
    This can lead to scenarios where during a dynamic cpu online, that logical cpu
    is initializing MTRR/PAT with cache disabled (cr0.cd=1) etc while other logical
    HT sibling continue to run (also with cache disabled because of cr0.cd=1
    on its sibling).
    
    Starting from Westmere, VMX transitions with cr0.cd=1 don't work properly
    (because of some VMX performance optimizations) and the above scenario
    (with one logical cpu doing VMX activity and another logical cpu coming online)
    can result in system crash.
    
    Fix the MTRR initialization by doing rendezvous of all the cpus. During
    boot and resume, we delay the MTRR/PAT init for APs till all the
    logical cpu's come online and the rendezvous process at the end of AP's bringup,
    will initialize the MTRR/PAT for all AP's.
    
    For dynamic single cpu online, we synchronize all the logical cpus and
    do the MTRR/PAT init on the AP that is coming online.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2fecda69ee64..d720b7e0cf3d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1116,9 +1116,22 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 	if (is_uv_system())
 		uv_system_init();
+
+	set_mtrr_aps_delayed_init();
 out:
 	preempt_enable();
 }
+
+void arch_enable_nonboot_cpus_begin(void)
+{
+	set_mtrr_aps_delayed_init();
+}
+
+void arch_enable_nonboot_cpus_end(void)
+{
+	mtrr_aps_init();
+}
+
 /*
  * Early setup to make printk work.
  */
@@ -1140,6 +1153,7 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 	setup_ioapic_dest();
 #endif
 	check_nmi_watchdog();
+	mtrr_aps_init();
 }
 
 static int __initdata setup_possible_cpus = -1;

commit 86886e55b273f565935491816c7c96b82469d4f8
Author: Joseph Cihula <joseph.cihula@intel.com>
Date:   Tue Jun 30 19:31:07 2009 -0700

    x86, intel_txt: Intel TXT Sx shutdown support
    
    Support for graceful handling of sleep states (S3/S4/S5) after an Intel(R) TXT launch.
    
    Without this patch, attempting to place the system in one of the ACPI sleep
    states (S3/S4/S5) will cause the TXT hardware to treat this as an attack and
    will cause a system reset, with memory locked.  Not only may the subsequent
    memory scrub take some time, but the platform will be unable to enter the
    requested power state.
    
    This patch calls back into the tboot so that it may properly and securely clean
    up system state and clear the secrets-in-memory flag, after which it will place
    the system into the requested sleep state using ACPI information passed by the kernel.
    
     arch/x86/kernel/smpboot.c     |    2 ++
     drivers/acpi/acpica/hwsleep.c |    3 +++
     kernel/cpu.c                  |    7 ++++++-
     3 files changed, 11 insertions(+), 1 deletion(-)
    
    Signed-off-by: Joseph Cihula <joseph.cihula@intel.com>
    Signed-off-by: Shane Wang <shane.wang@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2fecda69ee64..61cc40887c48 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -62,6 +62,7 @@
 #include <asm/vmi.h>
 #include <asm/apic.h>
 #include <asm/setup.h>
+#include <asm/tboot.h>
 #include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>
 
@@ -1317,6 +1318,7 @@ void play_dead_common(void)
 void native_play_dead(void)
 {
 	play_dead_common();
+	tboot_shutdown(TB_SHUTDOWN_WFS);
 	wbinvd_halt();
 }
 

commit 55cd63676e0c5710fbe1ea86dfd9f8ea9aaa90f2
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Jun 12 11:36:52 2009 +0300

    x86: make zap_low_mapping could be used early
    
    Only one cpu is there, just call __flush_tlb for it. Fixes the following boot
    warning on x86:
    
      [    0.000000] Memory: 885032k/915540k available (5993k kernel code, 29844k reserved, 3842k data, 428k init, 0k highmem)
      [    0.000000] virtual kernel memory layout:
      [    0.000000]     fixmap  : 0xffe17000 - 0xfffff000   (1952 kB)
      [    0.000000]     vmalloc : 0xf8615000 - 0xffe15000   ( 120 MB)
      [    0.000000]     lowmem  : 0xc0000000 - 0xf7e15000   ( 894 MB)
      [    0.000000]       .init : 0xc19a5000 - 0xc1a10000   ( 428 kB)
      [    0.000000]       .data : 0xc15da4bb - 0xc199af6c   (3842 kB)
      [    0.000000]       .text : 0xc1000000 - 0xc15da4bb   (5993 kB)
      [    0.000000] Checking if this processor honours the WP bit even in supervisor mode...Ok.
      [    0.000000] ------------[ cut here ]------------
      [    0.000000] WARNING: at kernel/smp.c:369 smp_call_function_many+0x50/0x1b0()
      [    0.000000] Hardware name: System Product Name
      [    0.000000] Modules linked in:
      [    0.000000] Pid: 0, comm: swapper Not tainted 2.6.30-tip #52504
      [    0.000000] Call Trace:
      [    0.000000]  [<c104aa16>] warn_slowpath_common+0x65/0x95
      [    0.000000]  [<c104aa58>] warn_slowpath_null+0x12/0x15
      [    0.000000]  [<c1073bbe>] smp_call_function_many+0x50/0x1b0
      [    0.000000]  [<c1037615>] ? do_flush_tlb_all+0x0/0x41
      [    0.000000]  [<c1037615>] ? do_flush_tlb_all+0x0/0x41
      [    0.000000]  [<c1073d4f>] smp_call_function+0x31/0x58
      [    0.000000]  [<c1037615>] ? do_flush_tlb_all+0x0/0x41
      [    0.000000]  [<c104f635>] on_each_cpu+0x26/0x65
      [    0.000000]  [<c10374b5>] flush_tlb_all+0x19/0x1b
      [    0.000000]  [<c1032ab3>] zap_low_mappings+0x4d/0x56
      [    0.000000]  [<c15d64b5>] ? printk+0x14/0x17
      [    0.000000]  [<c19b42a8>] mem_init+0x23d/0x245
      [    0.000000]  [<c19a56a1>] start_kernel+0x17a/0x2d5
      [    0.000000]  [<c19a5347>] ? unknown_bootoption+0x0/0x19a
      [    0.000000]  [<c19a5039>] __init_begin+0x39/0x41
      [    0.000000] ---[ end trace 4eaa2a86a8e2da22 ]---
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7c80007ea5f7..2fecda69ee64 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -873,7 +873,7 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 
 	err = do_boot_cpu(apicid, cpu);
 
-	zap_low_mappings();
+	zap_low_mappings(false);
 	low_mappings = 0;
 #else
 	err = do_boot_cpu(apicid, cpu);

commit 103428e57be323c3c5545db8ad12667099bc6005
Author: Cyrill Gorcunov <gorcunov@openvz.org>
Date:   Sun Jun 7 16:48:40 2009 +0400

    x86, apic: Fix dummy apic read operation together with broken MP handling
    
    Ingo Molnar reported that read_apic is buggy novadays:
    
    [    0.000000] Using APIC driver default
    [    0.000000] SMP: Allowing 1 CPUs, 0 hotplug CPUs
    [    0.000000] Local APIC disabled by BIOS -- you can enable it with "lapic"
    [    0.000000] APIC: disable apic facility
    [    0.000000] ------------[ cut here ]------------
    [    0.000000] WARNING: at arch/x86/kernel/apic/apic.c:254 native_apic_read_dummy+0x2d/0x3b()
    [    0.000000] Hardware name: HP OmniBook PC
    
    Indeed we still rely on apic->read operation for SMP compiled
    kernel. And instead of disfigure the SMP code with #ifdef we
    allow to call apic->read. To capture any unexpected results
    we check for apic->read being called for sane reason via
    WARN_ON_ONCE but(!) instead of OR we should use AND logical
    operation (thanks Yinghai for spotting the root of the problem).
    
    Along with that we could be have bad MP table and we are
    to fix it that way no SMP started and no complains about
    BIOS bug if apic was just disabled via command line.
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <20090607124840.GD4547@lenovo>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d2e8de958156..7c80007ea5f7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -992,10 +992,12 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	 */
 	if (APIC_INTEGRATED(apic_version[boot_cpu_physical_apicid]) &&
 	    !cpu_has_apic) {
-		printk(KERN_ERR "BIOS bug, local APIC #%d not detected!...\n",
-			boot_cpu_physical_apicid);
-		printk(KERN_ERR "... forcing use of dummy APIC emulation."
+		if (!disable_apic) {
+			pr_err("BIOS bug, local APIC #%d not detected!...\n",
+				boot_cpu_physical_apicid);
+			pr_err("... forcing use of dummy APIC emulation."
 				"(tell your hw vendor)\n");
+		}
 		smpboot_clear_io_apic();
 		arch_disable_smp_support();
 		return -1;

commit cece3155d869a50ba534ed161b5a05e8a29dcad0
Author: Cyrill Gorcunov <gorcunov@gmail.com>
Date:   Sat Apr 18 23:45:28 2009 +0400

    x86: smpboot - wakeup_secondary should be done via __cpuinit section
    
    A caller (do_boot_cpu) already has __cpuinit attribute.
    
    Since HOTPLUG_CPU depends on SMP && HOTPLUG it doesn't
    lead to panic at moment.
    
    [ Impact: cleanup ]
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
    LKML-Reference: <20090418194528.GD25510@lenovo>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index bf8ad6344b18..d2e8de958156 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -504,7 +504,7 @@ void __inquire_remote_apic(int apicid)
  * INIT, INIT, STARTUP sequence will reset the chip hard for us, and this
  * won't ... remember to clear down the APIC, etc later.
  */
-int __devinit
+int __cpuinit
 wakeup_secondary_cpu_via_nmi(int logical_apicid, unsigned long start_eip)
 {
 	unsigned long send_status, accept_status = 0;
@@ -538,7 +538,7 @@ wakeup_secondary_cpu_via_nmi(int logical_apicid, unsigned long start_eip)
 	return (send_status | accept_status);
 }
 
-static int __devinit
+static int __cpuinit
 wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 {
 	unsigned long send_status, accept_status = 0;

commit 02421f98ec55c3ff118f358740ff640f096c7ad6
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Apr 3 17:15:53 2009 -0700

    x86: consistent about warm_reset_vector for UN_NON_UNIQUE_APIC
    
    Impact: cleanup
    
    didn't set it for UV_NON_UNIQUE_APIC, so don't restore it
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <49D6A6B9.6060501@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index bddf2ccaf325..bf8ad6344b18 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -822,10 +822,12 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	/* mark "stuck" area as not stuck */
 	*((volatile unsigned long *)trampoline_base) = 0;
 
-	/*
-	 * Cleanup possible dangling ends...
-	 */
-	smpboot_restore_warm_reset_vector();
+	if (get_uv_system_type() != UV_NON_UNIQUE_APIC) {
+		/*
+		 * Cleanup possible dangling ends...
+		 */
+		smpboot_restore_warm_reset_vector();
+	}
 
 	return boot_error;
 }

commit cdc1cb0d4445f39561a65204d26f89365f917550
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Apr 3 17:15:14 2009 -0700

    x86: make wakeup_secondary_cpu_via_init static
    
    Impact: cleanup
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <49D6A692.6040400@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 58d24ef917d8..bddf2ccaf325 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -538,7 +538,7 @@ wakeup_secondary_cpu_via_nmi(int logical_apicid, unsigned long start_eip)
 	return (send_status | accept_status);
 }
 
-int __devinit
+static int __devinit
 wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 {
 	unsigned long send_status, accept_status = 0;

commit 4f0628963c86d2f97b8cb9acc024a7fe288a6a57
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:54 2009 +1030

    cpumask: use new cpumask functions throughout x86
    
    Impact: cleanup
    
    1) &cpu_online_map -> cpu_online_mask
    2) first_cpu/next_cpu_nr -> cpumask_first/cpumask_next
    3) cpu_*_map manipulation -> init_cpu_* / set_cpu_*
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d6427aa56966..58d24ef917d8 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -296,7 +296,7 @@ notrace static void __cpuinit start_secondary(void *unused)
 	__flush_tlb_all();
 #endif
 
-	/* This must be done before setting cpu_online_map */
+	/* This must be done before setting cpu_online_mask */
 	set_cpu_sibling_map(raw_smp_processor_id());
 	wmb();
 
@@ -904,9 +904,8 @@ int __cpuinit native_cpu_up(unsigned int cpu)
  */
 static __init void disable_smp(void)
 {
-	/* use the read/write pointers to the present and possible maps */
-	cpumask_copy(&cpu_present_map, cpumask_of(0));
-	cpumask_copy(&cpu_possible_map, cpumask_of(0));
+	init_cpu_present(cpumask_of(0));
+	init_cpu_possible(cpumask_of(0));
 	smpboot_clear_io_apic_irqs();
 
 	if (smp_found_config)
@@ -1149,11 +1148,11 @@ early_param("possible_cpus", _setup_possible_cpus);
 
 
 /*
- * cpu_possible_map should be static, it cannot change as cpu's
+ * cpu_possible_mask should be static, it cannot change as cpu's
  * are onlined, or offlined. The reason is per-cpu data-structures
  * are allocated by some modules at init time, and dont expect to
  * do this dynamically on cpu arrival/departure.
- * cpu_present_map on the other hand can change dynamically.
+ * cpu_present_mask on the other hand can change dynamically.
  * In case when cpu_hotplug is not compiled, then we resort to current
  * behaviour, which is cpu_possible == cpu_present.
  * - Ashok Raj

commit 155dd720d06a219ddf5a56b473cb3325441fc879
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:53 2009 +1030

    cpumask: convert struct cpuinfo_x86's llc_shared_map to cpumask_var_t
    
    Impact: reduce kernel memory usage when CONFIG_CPUMASK_OFFSTACK=y
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 5a58a45ac1e3..d6427aa56966 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -329,6 +329,23 @@ notrace static void __cpuinit start_secondary(void *unused)
 	cpu_idle();
 }
 
+#ifdef CONFIG_CPUMASK_OFFSTACK
+/* In this case, llc_shared_map is a pointer to a cpumask. */
+static inline void copy_cpuinfo_x86(struct cpuinfo_x86 *dst,
+				    const struct cpuinfo_x86 *src)
+{
+	struct cpumask *llc = dst->llc_shared_map;
+	*dst = *src;
+	dst->llc_shared_map = llc;
+}
+#else
+static inline void copy_cpuinfo_x86(struct cpuinfo_x86 *dst,
+				    const struct cpuinfo_x86 *src)
+{
+	*dst = *src;
+}
+#endif /* CONFIG_CPUMASK_OFFSTACK */
+
 /*
  * The bootstrap kernel entry code has set these up. Save them for
  * a given CPU
@@ -338,7 +355,7 @@ void __cpuinit smp_store_cpu_info(int id)
 {
 	struct cpuinfo_x86 *c = &cpu_data(id);
 
-	*c = boot_cpu_data;
+	copy_cpuinfo_x86(c, &boot_cpu_data);
 	c->cpu_index = id;
 	if (id != 0)
 		identify_secondary_cpu(c);
@@ -362,15 +379,15 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 				cpumask_set_cpu(cpu, cpu_sibling_mask(i));
 				cpumask_set_cpu(i, cpu_core_mask(cpu));
 				cpumask_set_cpu(cpu, cpu_core_mask(i));
-				cpumask_set_cpu(i, &c->llc_shared_map);
-				cpumask_set_cpu(cpu, &o->llc_shared_map);
+				cpumask_set_cpu(i, c->llc_shared_map);
+				cpumask_set_cpu(cpu, o->llc_shared_map);
 			}
 		}
 	} else {
 		cpumask_set_cpu(cpu, cpu_sibling_mask(cpu));
 	}
 
-	cpumask_set_cpu(cpu, &c->llc_shared_map);
+	cpumask_set_cpu(cpu, c->llc_shared_map);
 
 	if (current_cpu_data.x86_max_cores == 1) {
 		cpumask_copy(cpu_core_mask(cpu), cpu_sibling_mask(cpu));
@@ -381,8 +398,8 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 	for_each_cpu(i, cpu_sibling_setup_mask) {
 		if (per_cpu(cpu_llc_id, cpu) != BAD_APICID &&
 		    per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, i)) {
-			cpumask_set_cpu(i, &c->llc_shared_map);
-			cpumask_set_cpu(cpu, &cpu_data(i).llc_shared_map);
+			cpumask_set_cpu(i, c->llc_shared_map);
+			cpumask_set_cpu(cpu, cpu_data(i).llc_shared_map);
 		}
 		if (c->phys_proc_id == cpu_data(i).phys_proc_id) {
 			cpumask_set_cpu(i, cpu_core_mask(cpu));
@@ -420,7 +437,7 @@ const struct cpumask *cpu_coregroup_mask(int cpu)
 	if (sched_mc_power_savings || sched_smt_power_savings)
 		return cpu_core_mask(cpu);
 	else
-		return &c->llc_shared_map;
+		return c->llc_shared_map;
 }
 
 static void impress_friends(void)
@@ -1039,8 +1056,10 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	for_each_possible_cpu(i) {
 		alloc_cpumask_var(&per_cpu(cpu_sibling_map, i), GFP_KERNEL);
 		alloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);
+		alloc_cpumask_var(&cpu_data(i).llc_shared_map, GFP_KERNEL);
 		cpumask_clear(per_cpu(cpu_core_map, i));
 		cpumask_clear(per_cpu(cpu_sibling_map, i));
+		cpumask_clear(cpu_data(i).llc_shared_map);
 	}
 	set_cpu_sibling_map(0);
 

commit c032ef60d1aa9af33730b7a35bbea751b131adc1
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:53 2009 +1030

    cpumask: convert node_to_cpumask_map[] to cpumask_var_t
    
    Impact: reduce kernel memory usage when CONFIG_CPUMASK_OFFSTACK=y
    
    Straightforward conversion: done for 32 and 64 bit kernels.
    node_to_cpumask_map is now a cpumask_var_t array.
    
    64-bit used to be a dynamic cpumask_t array, and 32-bit used to be a
    static cpumask_t array.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c55639b1ab8a..5a58a45ac1e3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -123,7 +123,7 @@ EXPORT_SYMBOL(cpu_to_node_map);
 static void map_cpu_to_node(int cpu, int node)
 {
 	printk(KERN_INFO "Mapping cpu %d to node %d\n", cpu, node);
-	cpumask_set_cpu(cpu, &node_to_cpumask_map[node]);
+	cpumask_set_cpu(cpu, node_to_cpumask_map[node]);
 	cpu_to_node_map[cpu] = node;
 }
 
@@ -134,7 +134,7 @@ static void unmap_cpu_to_node(int cpu)
 
 	printk(KERN_INFO "Unmapping cpu %d from all nodes\n", cpu);
 	for (node = 0; node < MAX_NUMNODES; node++)
-		cpumask_clear_cpu(cpu, &node_to_cpumask_map[node]);
+		cpumask_clear_cpu(cpu, node_to_cpumask_map[node]);
 	cpu_to_node_map[cpu] = 0;
 }
 #else /* !(CONFIG_NUMA && CONFIG_X86_32) */

commit 71ee73e72228775a076a502b3c92028fa59e2889
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:52 2009 +1030

    x86: unify 32 and 64-bit node_to_cpumask_map
    
    Impact: cleanup
    
    We take the 64-bit code and use it on 32-bit as well.  The new file
    is called mm/numa.c.
    
    In a minor cleanup, we use cpu_none_mask instead of declaring a local
    cpu_mask_none.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7f051c170add..c55639b1ab8a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -115,11 +115,6 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 atomic_t init_deasserted;
 
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_32)
-
-/* which logical CPUs are on which nodes */
-cpumask_t node_to_cpumask_map[MAX_NUMNODES] __read_mostly =
-				{ [0 ... MAX_NUMNODES-1] = CPU_MASK_NONE };
-EXPORT_SYMBOL(node_to_cpumask_map);
 /* which node each logical CPU is on */
 int cpu_to_node_map[NR_CPUS] __read_mostly = { [0 ... NR_CPUS-1] = 0 };
 EXPORT_SYMBOL(cpu_to_node_map);

commit 7ad728f98162cb1af06a85b2a5fc422dddd4fb78
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:50 2009 +1030

    cpumask: x86: convert cpu_sibling_map/cpu_core_map to cpumask_var_t
    
    Impact: reduce per-cpu size for CONFIG_CPUMASK_OFFSTACK=y
    
    In most places it's cleaner to use the accessors cpu_sibling_mask()
    and cpu_core_mask() wrappers which already exist.
    
    I couldn't avoid cleaning up the access in oprofile, either.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f534257d4b46..7f051c170add 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -101,11 +101,11 @@ EXPORT_SYMBOL(smp_num_siblings);
 DEFINE_PER_CPU(u16, cpu_llc_id) = BAD_APICID;
 
 /* representing HT siblings of each logical CPU */
-DEFINE_PER_CPU(cpumask_t, cpu_sibling_map);
+DEFINE_PER_CPU(cpumask_var_t, cpu_sibling_map);
 EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
 
 /* representing HT and core siblings of each logical CPU */
-DEFINE_PER_CPU(cpumask_t, cpu_core_map);
+DEFINE_PER_CPU(cpumask_var_t, cpu_core_map);
 EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 
 /* Per CPU bogomips and other parameters */
@@ -1026,6 +1026,8 @@ static void __init smp_cpu_index_default(void)
  */
 void __init native_smp_prepare_cpus(unsigned int max_cpus)
 {
+	unsigned int i;
+
 	preempt_disable();
 	smp_cpu_index_default();
 	current_cpu_data = boot_cpu_data;
@@ -1039,6 +1041,12 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	boot_cpu_logical_apicid = logical_smp_processor_id();
 #endif
 	current_thread_info()->cpu = 0;  /* needed? */
+	for_each_possible_cpu(i) {
+		alloc_cpumask_var(&per_cpu(cpu_sibling_map, i), GFP_KERNEL);
+		alloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);
+		cpumask_clear(per_cpu(cpu_core_map, i));
+		cpumask_clear(per_cpu(cpu_sibling_map, i));
+	}
 	set_cpu_sibling_map(0);
 
 	enable_IR_x2apic();

commit 23c5c9c66263311de1295b42382e5bc1e7c36c47
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:48 2009 +1030

    cpumask: remove cpu_coregroup_map: x86
    
    Impact: cleanup
    
    cpu_coregroup_mask is the New Hotness.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ef7d10170c30..f534257d4b46 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -428,11 +428,6 @@ const struct cpumask *cpu_coregroup_mask(int cpu)
 		return &c->llc_shared_map;
 }
 
-cpumask_t cpu_coregroup_map(int cpu)
-{
-	return *cpu_coregroup_mask(cpu);
-}
-
 static void impress_friends(void)
 {
 	int cpu;

commit 1f442d70c84aa798e243e721eba728a98434cd86
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sat Mar 7 23:46:26 2009 -0800

    x86: remove smp_apply_quirks()/smp_checks()
    
    Impact: cleanup and code size reduction on 64-bit
    
    This code is only applied to Intel Pentium and AMD K7 32-bit cpus.
    
    Move those checks to intel_init()/amd_init() for 32-bit
    so 64-bit will not build this code.
    
    Also change to use cpu_index check to see if we need to emit warning.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <49B377D2.8030108@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 249334f5080a..ef7d10170c30 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -114,10 +114,6 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 
 atomic_t init_deasserted;
 
-
-/* Set if we find a B stepping CPU */
-static int __cpuinitdata smp_b_stepping;
-
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_32)
 
 /* which logical CPUs are on which nodes */
@@ -271,8 +267,6 @@ static void __cpuinit smp_callin(void)
 	cpumask_set_cpu(cpuid, cpu_callin_mask);
 }
 
-static int __cpuinitdata unsafe_smp;
-
 /*
  * Activate a secondary processor.
  */
@@ -340,76 +334,6 @@ notrace static void __cpuinit start_secondary(void *unused)
 	cpu_idle();
 }
 
-static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
-{
-	/*
-	 * Mask B, Pentium, but not Pentium MMX
-	 */
-	if (c->x86_vendor == X86_VENDOR_INTEL &&
-	    c->x86 == 5 &&
-	    c->x86_mask >= 1 && c->x86_mask <= 4 &&
-	    c->x86_model <= 3)
-		/*
-		 * Remember we have B step Pentia with bugs
-		 */
-		smp_b_stepping = 1;
-
-	/*
-	 * Certain Athlons might work (for various values of 'work') in SMP
-	 * but they are not certified as MP capable.
-	 */
-	if ((c->x86_vendor == X86_VENDOR_AMD) && (c->x86 == 6)) {
-
-		if (num_possible_cpus() == 1)
-			goto valid_k7;
-
-		/* Athlon 660/661 is valid. */
-		if ((c->x86_model == 6) && ((c->x86_mask == 0) ||
-		    (c->x86_mask == 1)))
-			goto valid_k7;
-
-		/* Duron 670 is valid */
-		if ((c->x86_model == 7) && (c->x86_mask == 0))
-			goto valid_k7;
-
-		/*
-		 * Athlon 662, Duron 671, and Athlon >model 7 have capability
-		 * bit. It's worth noting that the A5 stepping (662) of some
-		 * Athlon XP's have the MP bit set.
-		 * See http://www.heise.de/newsticker/data/jow-18.10.01-000 for
-		 * more.
-		 */
-		if (((c->x86_model == 6) && (c->x86_mask >= 2)) ||
-		    ((c->x86_model == 7) && (c->x86_mask >= 1)) ||
-		     (c->x86_model > 7))
-			if (cpu_has_mp)
-				goto valid_k7;
-
-		/* If we get here, not a certified SMP capable AMD system. */
-		unsafe_smp = 1;
-	}
-
-valid_k7:
-	;
-}
-
-static void __cpuinit smp_checks(void)
-{
-	if (smp_b_stepping)
-		printk(KERN_WARNING "WARNING: SMP operation may be unreliable"
-				    "with B stepping processors.\n");
-
-	/*
-	 * Don't taint if we are running SMP kernel on a single non-MP
-	 * approved Athlon
-	 */
-	if (unsafe_smp && num_online_cpus() > 1) {
-		printk(KERN_INFO "WARNING: This combination of AMD"
-			"processors is not suitable for SMP.\n");
-		add_taint(TAINT_UNSAFE_SMP);
-	}
-}
-
 /*
  * The bootstrap kernel entry code has set these up. Save them for
  * a given CPU
@@ -423,7 +347,6 @@ void __cpuinit smp_store_cpu_info(int id)
 	c->cpu_index = id;
 	if (id != 0)
 		identify_secondary_cpu(c);
-	smp_apply_quirks(c);
 }
 
 
@@ -1193,7 +1116,6 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 	pr_debug("Boot done.\n");
 
 	impress_friends();
-	smp_checks();
 #ifdef CONFIG_X86_IO_APIC
 	setup_ioapic_dest();
 #endif

commit 1f5bcabf1b997d6b76a09114b5a79423495a1263
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 26 13:51:40 2009 +0100

    x86: apic: simplify secondary CPU wakeup methods
    
    Impact: cleanup
    
    - rename apic->wakeup_cpu  to apic->wakeup_secondary_cpu, to
      make it apparent that this is an SMP-only method
    
    - handle NULL ->wakeup_secondary_cpus to mean the default INIT
      wakeup sequence - this allows simplification of the APIC
      driver templates.
    
    Cc: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9b338aa03b40..249334f5080a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -742,7 +742,8 @@ static void __cpuinit do_fork_idle(struct work_struct *work)
 /*
  * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad
  * (ie clustered apic addressing mode), this is a LOGICAL apic ID.
- * Returns zero if CPU booted OK, else error code from ->wakeup_cpu.
+ * Returns zero if CPU booted OK, else error code from
+ * ->wakeup_secondary_cpu.
  */
 static int __cpuinit do_boot_cpu(int apicid, int cpu)
 {
@@ -829,9 +830,13 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	}
 
 	/*
-	 * Starting actual IPI sequence...
+	 * Kick the secondary CPU. Use the method in the APIC driver
+	 * if it's defined - or use an INIT boot APIC message otherwise:
 	 */
-	boot_error = apic->wakeup_cpu(apicid, start_ip);
+	if (apic->wakeup_secondary_cpu)
+		boot_error = apic->wakeup_secondary_cpu(apicid, start_ip);
+	else
+		boot_error = wakeup_secondary_cpu_via_init(apicid, start_ip);
 
 	if (!boot_error) {
 		/*

commit 2b6163bf5772644068694583816fa41e8474239f
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Wed Feb 25 20:50:49 2009 -0800

    x86: remove update_apic from x86_quirks
    
    Impact: cleanup
    
    x86_quirks->update_apic() calling looks crazy. so try to remove it:
    
     1. every apic take wakeup_cpu member directly
     2. separate es7000_apic to es7000_apic_cluster
     3. use uv_wakeup_cpu directly
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9ce666387f37..9b338aa03b40 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -112,7 +112,7 @@ EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 DEFINE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
 
-static atomic_t init_deasserted;
+atomic_t init_deasserted;
 
 
 /* Set if we find a B stepping CPU */
@@ -614,12 +614,6 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	unsigned long send_status, accept_status = 0;
 	int maxlvt, num_starts, j;
 
-	if (get_uv_system_type() == UV_NON_UNIQUE_APIC) {
-		send_status = uv_wakeup_secondary(phys_apicid, start_eip);
-		atomic_set(&init_deasserted, 1);
-		return send_status;
-	}
-
 	maxlvt = lapic_get_maxlvt();
 
 	/*

commit ab6fb7c0b03e2c3286f316c840347be8b9ee3d9f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 16:22:09 2009 +0100

    x86, apic: remove ->store_NMI_vector()
    
    Impact: cleanup
    
    It's not used by anything anymore.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 09e73876a44f..9ce666387f37 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -745,21 +745,21 @@ static void __cpuinit do_fork_idle(struct work_struct *work)
 	complete(&c_idle->done);
 }
 
-static int __cpuinit do_boot_cpu(int apicid, int cpu)
 /*
  * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad
  * (ie clustered apic addressing mode), this is a LOGICAL apic ID.
  * Returns zero if CPU booted OK, else error code from ->wakeup_cpu.
  */
+static int __cpuinit do_boot_cpu(int apicid, int cpu)
 {
 	unsigned long boot_error = 0;
-	int timeout;
 	unsigned long start_ip;
-	unsigned short nmi_high = 0, nmi_low = 0;
+	int timeout;
 	struct create_idle c_idle = {
-		.cpu = cpu,
-		.done = COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
+		.cpu	= cpu,
+		.done	= COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
 	};
+
 	INIT_WORK(&c_idle.work, do_fork_idle);
 
 	alternatives_smp_switch(1);
@@ -824,9 +824,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 
 		pr_debug("Setting warm reset code and vector.\n");
 
-		if (apic->store_NMI_vector)
-			apic->store_NMI_vector(&nmi_high, &nmi_low);
-
 		smpboot_setup_warm_reset_vector(start_ip);
 		/*
 		 * Be paranoid about clearing APIC errors.

commit e641f5f525acb163ba71d92de79c9c7366deae03
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 14:02:01 2009 +0100

    x86, apic: remove duplicate asm/apic.h inclusions
    
    Impact: cleanup
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 562a9fc3bc34..09e73876a44f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -65,7 +65,6 @@
 #include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>
 
-#include <asm/apic.h>
 #include <asm/smpboot_hooks.h>
 
 #ifdef CONFIG_X86_32

commit 7b6aa335ca1a845c2262ec7a595b4521bca0f79d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 13:58:15 2009 +0100

    x86, apic: remove genapic.h
    
    Impact: cleanup
    
    Remove genapic.h and remove all references to it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b5f2b698973f..562a9fc3bc34 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -60,12 +60,12 @@
 #include <asm/tlbflush.h>
 #include <asm/mtrr.h>
 #include <asm/vmi.h>
-#include <asm/genapic.h>
+#include <asm/apic.h>
 #include <asm/setup.h>
 #include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>
 
-#include <asm/genapic.h>
+#include <asm/apic.h>
 #include <asm/smpboot_hooks.h>
 
 #ifdef CONFIG_X86_32

commit 06cd9a7dc8a58186060a91b6ddc031057435fd34
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Feb 16 17:29:58 2009 -0800

    x86: add x2apic config
    
    Impact: cleanup
    
    so could deselect x2apic
    and INTR_REMAP will select x2apic
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 10834954e301..b5f2b698973f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1128,8 +1128,8 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	current_thread_info()->cpu = 0;  /* needed? */
 	set_cpu_sibling_map(0);
 
-#ifdef CONFIG_X86_64
 	enable_IR_x2apic();
+#ifdef CONFIG_X86_64
 	default_setup_apic_routing();
 #endif
 

commit 88d0f550d71493cd975a11a03c166211b2f3bd32
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sat Feb 14 23:57:28 2009 -0800

    x86: make 32bit to call enable_IO_APIC early like 64bit
    
    Impact: cleanup
    
    So we remove some #ifdefs.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index af57f88186e7..10834954e301 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1154,13 +1154,12 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	 */
 	setup_local_APIC();
 
-#ifdef CONFIG_X86_64
 	/*
 	 * Enable IO APIC before setting up error vector
 	 */
 	if (!skip_ioapic_setup && nr_ioapics)
 		enable_IO_APIC();
-#endif
+
 	end_local_APIC_setup();
 
 	map_cpu_to_logical_apicid();

commit eca217b36e5d7d4377493d5cedd89105e66a5a72
Merge: 54a353a0f845 e4d0407185cd
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 9 12:16:59 2009 +0100

    Merge branch 'x86/paravirt' into x86/apic
    
    Conflicts:
            arch/x86/mach-voyager/voyager_smp.c

commit 65a4e574d2382d83f71b30ea92f86d2e40a6ef8d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jan 31 03:36:17 2009 +0100

    smp, generic: introduce arch_disable_smp_support() instead of disable_ioapic_setup()
    
    Impact: cleanup
    
    disable_ioapic_setup() in init/main.c is ugly as the function is
    x86-specific. The #ifdef inline prototype there is ugly too.
    
    Replace it with a generic arch_disable_smp_support() function - which
    has a weak alias for non-x86 architectures and for non-ioapic x86 builds.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f40f86fec2fe..96f7d304f5c9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1071,7 +1071,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 		printk(KERN_ERR "... forcing use of dummy APIC emulation."
 				"(tell your hw vendor)\n");
 		smpboot_clear_io_apic();
-		disable_ioapic_setup();
+		arch_disable_smp_support();
 		return -1;
 	}
 

commit 552be871e67ff577ed36beb2f53d078b42304739
Author: Brian Gerst <brgerst@gmail.com>
Date:   Fri Jan 30 17:47:53 2009 +0900

    x86: pass in cpu number to switch_to_new_gdt()
    
    Impact: cleanup, prepare for xen boot fix.
    
    Xen needs to call this function very early to setup the GDT and
    per-cpu segments.  Remove the call to smp_processor_id() and just
    pass in the cpu number.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f9dbcff43546..612d3c74f6a3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1185,7 +1185,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 void __init native_smp_prepare_boot_cpu(void)
 {
 	int me = smp_processor_id();
-	switch_to_new_gdt();
+	switch_to_new_gdt(me);
 	/* already set me in cpu_online_mask in boot_cpu_init() */
 	cpumask_set_cpu(me, cpu_callout_mask);
 	per_cpu(cpu_state, me) = CPU_ONLINE;

commit 8f47e16348e8e25eedf639092a8a2f10a66aba34
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jan 31 02:03:42 2009 +0100

    x86: update copyrights
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 1268a862abb7..f40f86fec2fe 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -2,7 +2,7 @@
  *	x86 SMP booting functions
  *
  *	(c) 1995 Alan Cox, Building #3 <alan@lxorguk.ukuu.org.uk>
- *	(c) 1998, 1999, 2000 Ingo Molnar <mingo@redhat.com>
+ *	(c) 1998, 1999, 2000, 2009 Ingo Molnar <mingo@redhat.com>
  *	Copyright 2001 Andi Kleen, SuSE Labs.
  *
  *	Much of the core SMP work is based on previous work by Thomas Radke, to

commit 26f7ef14a76b0e590a3797fd7b2f3cee868d9664
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Thu Jan 29 14:19:22 2009 -0800

    x86: don't treat bigsmp as non-standard
    
    just like 64 bit switch from flat logical APIC messages to
    flat physical mode automatically.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 4c3cff574947..1268a862abb7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1007,7 +1007,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 
 		printk(KERN_WARNING
 		       "More than 8 CPUs detected - skipping them.\n"
-		       "Use CONFIG_X86_32_NON_STANDARD and CONFIG_X86_BIGSMP.\n");
+		       "Use CONFIG_X86_BIGSMP.\n");
 
 		nr = 0;
 		for_each_present_cpu(cpu) {

commit 1ff2f20de354a621ef4b56b9cfe6f9139a7e493b
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Thu Jan 29 19:30:04 2009 -0800

    x86: fix compiling with 64bit with def_to_bigsmp
    
    only need to do cut off with 32bit
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2912fa3a8ef2..4c3cff574947 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1000,7 +1000,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 {
 	preempt_disable();
 
-#ifndef CONFIG_X86_BIGSMP
+#if !defined(CONFIG_X86_BIGSMP) && defined(CONFIG_X86_32)
 	if (def_to_bigsmp && nr_cpu_ids > 8) {
 		unsigned int cpu;
 		unsigned nr;

commit 4272ebfbefd0db40073f3ee5990bceaf2894f08b
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Thu Jan 29 15:14:46 2009 -0800

    x86: allow more than 8 cpus to be used on 32-bit
    
    X86_PC is the only remaining 'sub' architecture, so we dont need
    it anymore.
    
    This also cleans up a few spurious references to X86_PC in the
    driver space - those certainly should be X86.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fc80bc18943e..2912fa3a8ef2 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1000,7 +1000,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 {
 	preempt_disable();
 
-#if defined(CONFIG_X86_PC) && defined(CONFIG_X86_32)
+#ifndef CONFIG_X86_BIGSMP
 	if (def_to_bigsmp && nr_cpu_ids > 8) {
 		unsigned int cpu;
 		unsigned nr;

commit e0c7ae376a13fd79a4dad8becab51040d13dfa90
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 27 18:43:09 2009 +0100

    x86: rename X86_GENERICARCH to X86_32_NON_STANDARD
    
    X86_GENERICARCH is a misnomer - it contains non-PC 32-bit architectures
    that are not included in the default build.
    
    Rename it to X86_32_NON_STANDARD.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index bc7e220ba0b4..fc80bc18943e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1007,7 +1007,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 
 		printk(KERN_WARNING
 		       "More than 8 CPUs detected - skipping them.\n"
-		       "Use CONFIG_X86_GENERICARCH and CONFIG_X86_BIGSMP.\n");
+		       "Use CONFIG_X86_32_NON_STANDARD and CONFIG_X86_BIGSMP.\n");
 
 		nr = 0;
 		for_each_present_cpu(cpu) {

commit c0b5842a457d44c8788b3fd0c64969be7ef673cd
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 27 17:13:05 2009 +0100

    x86: generalize boot_cpu_id
    
    x86/Voyager can boot on non-zero processors. While that can probably
    be fixed by properly remapping the physical CPU IDs, keep boot_cpu_id
    for now for easier transition - and expand it to all of x86.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e90b3e50b54f..bc7e220ba0b4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -905,18 +905,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	return boot_error;
 }
 
-#ifdef CONFIG_X86_64
-int default_cpu_present_to_apicid(int mps_cpu)
-{
-	return __default_cpu_present_to_apicid(mps_cpu);
-}
-
-int default_check_phys_apicid_present(int boot_cpu_physical_apicid)
-{
-	return __default_check_phys_apicid_present(boot_cpu_physical_apicid);
-}
-#endif
-
 int __cpuinit native_cpu_up(unsigned int cpu)
 {
 	int apicid = apic->cpu_present_to_apicid(cpu);

commit 1164dd0099c0d79146a55319670f57ab7ad1d352
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 19:34:09 2009 +0100

    x86: move mach-default/*.h files to asm/
    
    We are getting rid of subarchitecture support - move the hook files
    to asm/. (These are now stale and should be replaced with more explicit
    runtime mechanisms - but the transition is simpler this way.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 489fde9d9476..e90b3e50b54f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -66,7 +66,7 @@
 #include <linux/mc146818rtc.h>
 
 #include <asm/genapic.h>
-#include <smpboot_hooks.h>
+#include <asm/smpboot_hooks.h>
 
 #ifdef CONFIG_X86_32
 u8 apicid_2_node[MAX_APICID];

commit 1dcdd3d15ecea0c22a09d4d001a39d425fceff2c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 17:55:37 2009 +0100

    x86: remove mach_apic.h
    
    Spread mach_apic.h definitions into genapic.h. (with some knock-on effects
    on smp.h and apic.h.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3fed177f3457..489fde9d9476 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -65,7 +65,7 @@
 #include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>
 
-#include <mach_apic.h>
+#include <asm/genapic.h>
 #include <smpboot_hooks.h>
 
 #ifdef CONFIG_X86_32

commit 328386d7ab600aa0993a1226f5817ac30a735724
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 17:50:18 2009 +0100

    x86, smp: refactor ->wake_cpu
    
    - remove macro wrappers
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 1fdc1a7e7b56..3fed177f3457 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -750,7 +750,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 /*
  * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad
  * (ie clustered apic addressing mode), this is a LOGICAL apic ID.
- * Returns zero if CPU booted OK, else error code from wakeup_secondary_cpu.
+ * Returns zero if CPU booted OK, else error code from ->wakeup_cpu.
  */
 {
 	unsigned long boot_error = 0;
@@ -841,7 +841,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	/*
 	 * Starting actual IPI sequence...
 	 */
-	boot_error = wakeup_secondary_cpu(apicid, start_ip);
+	boot_error = apic->wakeup_cpu(apicid, start_ip);
 
 	if (!boot_error) {
 		/*

commit 0939e4fd351c58d08d25650797749f18904461af
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 17:16:25 2009 +0100

    x86, smp: eliminate asm/mach-default/mach_wakecpu.h
    
    Spread mach_wakecpu.h's definitions into apic.h and genapic.h
    and remove mach_wakecpu.h.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 170adc5b6cb3..1fdc1a7e7b56 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -66,7 +66,6 @@
 #include <linux/mc146818rtc.h>
 
 #include <mach_apic.h>
-#include <mach_wakecpu.h>
 #include <smpboot_hooks.h>
 
 #ifdef CONFIG_X86_32

commit 25dc004903a38f0b6f6626dbbab058c8709c5398
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 16:31:52 2009 +0100

    x86, smp: refactor ->inquire_remote_apic() methods
    
    Nothing exciting - a few subarches dont want APIC remote reads to
    be performed - the others are content with the default method.
    
     - extend the generic code to handle NULL methods
    
     - clear out dummy methods and replace them with NULL
    
     - clean up: remove wrapper macros, etc.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 1492024592ff..170adc5b6cb3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -876,8 +876,8 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 			else
 				/* trampoline code not run */
 				printk(KERN_ERR "Not responding.\n");
-			if (get_uv_system_type() != UV_NON_UNIQUE_APIC)
-				inquire_remote_apic(apicid);
+			if (apic->inquire_remote_apic)
+				apic->inquire_remote_apic(apicid);
 		}
 	}
 

commit 7bd06ec63a1204ca44b9f1dc487b8632016162d1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 16:31:52 2009 +0100

    x86, smp: refactor ->store/restore_NMI_vector() methods
    
    Only NUMAQ does something substantial here, because it initializes
    via NMIs (not via INIT as standard SMP startup) - so it needs to
    store and restore the NMI vector.
    
     - extend the generic code to handle NULL methods
    
     - clear out dummy methods and replace them with NULL
    
     - clean up: remove wrapper macros, etc.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 10873a46b299..1492024592ff 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -826,7 +826,8 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 
 		pr_debug("Setting warm reset code and vector.\n");
 
-		store_NMI_vector(&nmi_high, &nmi_low);
+		if (apic->store_NMI_vector)
+			apic->store_NMI_vector(&nmi_high, &nmi_low);
 
 		smpboot_setup_warm_reset_vector(start_ip);
 		/*

commit 333344d94300500e401cffb4eea10a5ab6e5a41d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 16:31:52 2009 +0100

    x86, smp: refactor ->smp_callin_clear_local_apic() methods
    
    Only NUMAQ does something substantial here, because it initializes
    via NMIs (not via INIT as standard SMP startup) - so it needs to
    reset the APIC.
    
     - extend the generic code to handle NULL methods
    
     - clear out dummy methods and replace them with NULL
    
     - clean up: remove wrapper macros, etc.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 558af378a61d..10873a46b299 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -244,7 +244,8 @@ static void __cpuinit smp_callin(void)
 	 */
 
 	pr_debug("CALLIN, before setup_local_APIC().\n");
-	smp_callin_clear_local_apic();
+	if (apic->smp_callin_clear_local_apic)
+		apic->smp_callin_clear_local_apic();
 	setup_local_APIC();
 	end_local_APIC_setup();
 	map_cpu_to_logical_apicid();

commit a965936643e28af8152d9e960b966baa1a5588a2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 16:21:32 2009 +0100

    x86, smp: refactor ->wait_for_init_deassert()
    
    - spread out the namespace on a per APIC driver basis
    
     - handle a NULL ->wait_for_init_deassert() as a 'dont wait' default method
    
     - remove NUMAQ and Summit handlers
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ab83be2f8e0f..558af378a61d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -196,7 +196,8 @@ static void __cpuinit smp_callin(void)
 	 * our local APIC.  We have to wait for the IPI or we'll
 	 * lock up on an APIC access.
 	 */
-	wait_for_init_deassert(&init_deasserted);
+	if (apic->wait_for_init_deassert)
+		apic->wait_for_init_deassert(&init_deasserted);
 
 	/*
 	 * (This works even if the APIC is not enabled.)

commit a27a621001f4c3e57caf47feff4b014577fd01c6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 12:43:18 2009 +0100

    x86: refactor ->check_phys_apicid_present() subarch methods
    
    - spread out the namespace to per driver methods
    
     - extend it to 64-bit as well so that we can use
       apic->check_phys_apicid_present() unconditionally
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0e7d26c01f9f..ab83be2f8e0f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -908,6 +908,11 @@ int default_cpu_present_to_apicid(int mps_cpu)
 {
 	return __default_cpu_present_to_apicid(mps_cpu);
 }
+
+int default_check_phys_apicid_present(int boot_cpu_physical_apicid)
+{
+	return __default_check_phys_apicid_present(boot_cpu_physical_apicid);
+}
 #endif
 
 int __cpuinit native_cpu_up(unsigned int cpu)
@@ -1058,7 +1063,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	 * Should not be necessary because the MP table should list the boot
 	 * CPU too, but we do it for the sake of robustness anyway.
 	 */
-	if (!check_phys_apicid_present(boot_cpu_physical_apicid)) {
+	if (!apic->check_phys_apicid_present(boot_cpu_physical_apicid)) {
 		printk(KERN_NOTICE
 			"weird, boot CPU (#%d) not listed by the BIOS.\n",
 			boot_cpu_physical_apicid);

commit d83093b50416f4ca59d3a84b2ddc217748214d64
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 12:43:18 2009 +0100

    x86: refactor ->setup_portio_remap() subarch methods
    
    Only NUMAQ has a real ->setup_portio_remap() method, the other
    subarchitectures define it but keep it empty.
    
    So mark the vector as NULL, extend the generic code to handle
    NULL -setup_portio_remap() entries and remove all the empty
    handlers.
    
    Also move the NUMAQ method from the header file into the
     apic driver .c file.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 812bf39de355..0e7d26c01f9f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1170,7 +1170,8 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 	map_cpu_to_logical_apicid();
 
-	setup_portio_remap();
+	if (apic->setup_portio_remap)
+		apic->setup_portio_remap();
 
 	smpboot_setup_io_apic();
 	/*

commit a21769a4461801454930a06bc18bd8249cd9e993
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 06:50:47 2009 +0100

    x86, apic: clean up ->cpu_present_to_apicid()
    
    - separate the namespace
    
     - remove macros
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 1dd4cecd4bc0..812bf39de355 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -903,9 +903,16 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	return boot_error;
 }
 
+#ifdef CONFIG_X86_64
+int default_cpu_present_to_apicid(int mps_cpu)
+{
+	return __default_cpu_present_to_apicid(mps_cpu);
+}
+#endif
+
 int __cpuinit native_cpu_up(unsigned int cpu)
 {
-	int apicid = cpu_present_to_apicid(cpu);
+	int apicid = apic->cpu_present_to_apicid(cpu);
 	unsigned long flags;
 	int err;
 

commit 3f57a318c36e1f24070a18df8c4971ca08d33142
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 06:50:47 2009 +0100

    x86, apic: clean up ->apicid_to_node()
    
    - separate the namespace
    
     - remove macros
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3791b4ae567f..1dd4cecd4bc0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -163,7 +163,7 @@ static void map_cpu_to_logical_apicid(void)
 {
 	int cpu = smp_processor_id();
 	int apicid = logical_smp_processor_id();
-	int node = apicid_to_node(apicid);
+	int node = apic->apicid_to_node(apicid);
 
 	if (!node_online(node))
 		node = first_online_node;

commit 72ce016583916fb7ffcbaa6a3e1f8f731b79a865
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 06:50:47 2009 +0100

    x86, apic: clean up ->setup_apic_routing()
    
    - separate the namespace
    
     - remove macros
    
     - remove namespace clash on 64-bit
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 45c096f605fe..3791b4ae567f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1128,7 +1128,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 
 #ifdef CONFIG_X86_64
 	enable_IR_x2apic();
-	setup_apic_routing();
+	default_setup_apic_routing();
 #endif
 
 	if (smp_sanity_check(max_cpus) < 0) {

commit bdb1a9b62fc182d4da3143e346f7a0925d243352
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 05:29:25 2009 +0100

    x86, apic: rename genapic::apic_destination_logical to genapic::dest_logical
    
    This field name was unreasonably long - shorten it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f0a173718d9f..45c096f605fe 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -583,7 +583,7 @@ wakeup_secondary_cpu_via_nmi(int logical_apicid, unsigned long start_eip)
 	/* Target chip */
 	/* Boot on the stack */
 	/* Kick the second */
-	apic_icr_write(APIC_DM_NMI |  apic->apic_destination_logical, logical_apicid);
+	apic_icr_write(APIC_DM_NMI | apic->dest_logical, logical_apicid);
 
 	pr_debug("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();

commit 0b06e734bff7554c31eac4aad2fc9be4adb7c1c1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 05:13:04 2009 +0100

    x86: clean up the APIC_DEST_LOGICAL logic
    
    Impact: cleanup
    
    The bigsmp and es7000 subarchitectures un-defined APIC_DEST_LOGICAL in
    a rather nasty way by re-defining it to zero. That is infinitely
    fragile and makes it very hard to see what to code really does in
    a given context. The very same constant has different meanings and
    values - depending on which subarch is enabled.
    
    Untangle this mess by never undefining the constant, but instead
    propagating the right values into the genapic driver templates.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f9dbcff43546..f0a173718d9f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -583,7 +583,7 @@ wakeup_secondary_cpu_via_nmi(int logical_apicid, unsigned long start_eip)
 	/* Target chip */
 	/* Boot on the stack */
 	/* Kick the second */
-	apic_icr_write(APIC_DM_NMI | APIC_DEST_LOGICAL, logical_apicid);
+	apic_icr_write(APIC_DM_NMI |  apic->apic_destination_logical, logical_apicid);
 
 	pr_debug("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();

commit b2d2f4312b117a6cc647c8521e2643a88771f757
Author: Brian Gerst <brgerst@gmail.com>
Date:   Tue Jan 27 12:56:48 2009 +0900

    x86: initialize per-cpu GDT segment in per-cpu setup
    
    Impact: cleanup
    
    Rename init_gdt() to setup_percpu_segment(), and move it to
    setup_percpu.c.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index def770b57b5a..f9dbcff43546 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -793,7 +793,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 do_rest:
 	per_cpu(current_task, cpu) = c_idle.idle;
 #ifdef CONFIG_X86_32
-	init_gdt(cpu);
 	/* Stack for startup_32 can be just as for start_secondary onwards */
 	irq_ctx_init(cpu);
 #else
@@ -1186,9 +1185,6 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 void __init native_smp_prepare_boot_cpu(void)
 {
 	int me = smp_processor_id();
-#ifdef CONFIG_X86_32
-	init_gdt(me);
-#endif
 	switch_to_new_gdt();
 	/* already set me in cpu_online_mask in boot_cpu_init() */
 	cpumask_set_cpu(me, cpu_callout_mask);

commit bdbcdd48883940bbd8d17eb01172d58a261a413a
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Jan 21 17:26:06 2009 +0900

    x86: uv cleanup
    
    Impact: cleanup
    
    Make the following uv related cleanups.
    
    * collect visible uv related definitions and interfaces into uv/uv.h
      and use it.  this cleans up the messy situation where on 64bit, uv
      is defined properly, on 32bit generic it's dummy and on the rest
      undefined.  after this clean up, uv is defined on 64 and dummy on
      32.
    
    * update uv_flush_tlb_others() such that it takes cpumask of
      to-be-flushed cpus as argument, instead of that minus self, and
      returns yet-to-be-flushed cpumask, instead of modifying the passed
      in parameter.  this interface change will ease dummy implementation
      of uv_flush_tlb_others() and makes uv tlb flush related stuff
      defined in tlb_uv proper.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 869b98840fd0..def770b57b5a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -62,6 +62,7 @@
 #include <asm/vmi.h>
 #include <asm/genapic.h>
 #include <asm/setup.h>
+#include <asm/uv/uv.h>
 #include <linux/mc146818rtc.h>
 
 #include <mach_apic.h>

commit 9af45651f1f7c89942e016a1a00a7ebddfa727f8
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Move kernelstack from PDA to per-cpu.
    
    Also clean up PER_CPU_VAR usage in xen-asm_64.S
    
    tj: * remove now unused stack_thread_info()
        * s/kernelstack/kernel_stack/
        * added FIXME comment in xen-asm_64.S
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 5854be0fb804..869b98840fd0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -798,6 +798,9 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 #else
 	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
 	initial_gs = per_cpu_offset(cpu);
+	per_cpu(kernel_stack, cpu) =
+		(unsigned long)task_stack_page(c_idle.idle) -
+		KERNEL_STACK_OFFSET + THREAD_SIZE;
 #endif
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	initial_code = (unsigned long)start_secondary;

commit c6f5e0acd5d12ee23f701f15889872e67b47caa6
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Move current task from PDA to per-cpu and consolidate with 32-bit.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2f0e0f1090f6..5854be0fb804 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -790,13 +790,12 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 
 	set_idle_for_cpu(cpu, c_idle.idle);
 do_rest:
-#ifdef CONFIG_X86_32
 	per_cpu(current_task, cpu) = c_idle.idle;
+#ifdef CONFIG_X86_32
 	init_gdt(cpu);
 	/* Stack for startup_32 can be just as for start_secondary onwards */
 	irq_ctx_init(cpu);
 #else
-	cpu_pda(cpu)->pcurrent = c_idle.idle;
 	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
 	initial_gs = per_cpu_offset(cpu);
 #endif

commit 004aa322f855a765741d9437a98dd8fe2e4f32a6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 13 20:41:35 2009 +0900

    x86: misc clean up after the percpu update
    
    Do the following cleanups:
    
    * kill x86_64_init_pda() which now is equivalent to pda_init()
    
    * use per_cpu_offset() instead of cpu_pda() when initializing
      initial_gs
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f2f77ca494d4..2f0e0f1090f6 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -798,7 +798,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 #else
 	cpu_pda(cpu)->pcurrent = c_idle.idle;
 	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
-	initial_gs = (unsigned long)cpu_pda(cpu);
+	initial_gs = per_cpu_offset(cpu);
 #endif
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	initial_code = (unsigned long)start_secondary;

commit 1a51e3a0aed18767cf2762e95456ecfeb0bca5e6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 13 20:41:35 2009 +0900

    x86: fold pda into percpu area on SMP
    
    [ Based on original patch from Christoph Lameter and Mike Travis. ]
    
    Currently pdas and percpu areas are allocated separately.  %gs points
    to local pda and percpu area can be reached using pda->data_offset.
    This patch folds pda into percpu area.
    
    Due to strange gcc requirement, pda needs to be at the beginning of
    the percpu area so that pda->stack_canary is at %gs:40.  To achieve
    this, a new percpu output section macro - PERCPU_VADDR_PREALLOC() - is
    added and used to reserve pda sized chunk at the start of the percpu
    area.
    
    After this change, for boot cpu, %gs first points to pda in the
    data.init area and later during setup_per_cpu_areas() gets updated to
    point to the actual pda.  This means that setup_per_cpu_areas() need
    to reload %gs for CPU0 while clearing pda area for other cpus as cpu0
    already has modified it when control reaches setup_per_cpu_areas().
    
    This patch also removes now unnecessary get_local_pda() and its call
    sites.
    
    A lot of this patch is taken from Mike Travis' "x86_64: Fold pda into
    per cpu area" patch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 70d846628bbf..f2f77ca494d4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -744,52 +744,6 @@ static void __cpuinit do_fork_idle(struct work_struct *work)
 	complete(&c_idle->done);
 }
 
-#ifdef CONFIG_X86_64
-
-/* __ref because it's safe to call free_bootmem when after_bootmem == 0. */
-static void __ref free_bootmem_pda(struct x8664_pda *oldpda)
-{
-	if (!after_bootmem)
-		free_bootmem((unsigned long)oldpda, sizeof(*oldpda));
-}
-
-/*
- * Allocate node local memory for the AP pda.
- *
- * Must be called after the _cpu_pda pointer table is initialized.
- */
-int __cpuinit get_local_pda(int cpu)
-{
-	struct x8664_pda *oldpda, *newpda;
-	unsigned long size = sizeof(struct x8664_pda);
-	int node = cpu_to_node(cpu);
-
-	if (cpu_pda(cpu) && !cpu_pda(cpu)->in_bootmem)
-		return 0;
-
-	oldpda = cpu_pda(cpu);
-	newpda = kmalloc_node(size, GFP_ATOMIC, node);
-	if (!newpda) {
-		printk(KERN_ERR "Could not allocate node local PDA "
-			"for CPU %d on node %d\n", cpu, node);
-
-		if (oldpda)
-			return 0;	/* have a usable pda */
-		else
-			return -1;
-	}
-
-	if (oldpda) {
-		memcpy(newpda, oldpda, size);
-		free_bootmem_pda(oldpda);
-	}
-
-	newpda->in_bootmem = 0;
-	cpu_pda(cpu) = newpda;
-	return 0;
-}
-#endif /* CONFIG_X86_64 */
-
 static int __cpuinit do_boot_cpu(int apicid, int cpu)
 /*
  * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad
@@ -807,16 +761,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	};
 	INIT_WORK(&c_idle.work, do_fork_idle);
 
-#ifdef CONFIG_X86_64
-	/* Allocate node local memory for AP pdas */
-	if (cpu > 0) {
-		boot_error = get_local_pda(cpu);
-		if (boot_error)
-			goto restore_state;
-			/* if can't get pda memory, can't start cpu */
-	}
-#endif
-
 	alternatives_smp_switch(1);
 
 	c_idle.idle = get_idle_for_cpu(cpu);
@@ -931,9 +875,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 				inquire_remote_apic(apicid);
 		}
 	}
-#ifdef CONFIG_X86_64
-restore_state:
-#endif
+
 	if (boot_error) {
 		/* Try to put things back the way they were before ... */
 		numa_remove_cpu(cpu); /* was set by numa_add_cpu */

commit f32ff5388d86518c0375ccdb330d3b459b9c405e
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 13 20:41:35 2009 +0900

    x86: load pointer to pda into %gs while brining up a CPU
    
    [ Based on original patch from Christoph Lameter and Mike Travis. ]
    
    CPU startup code in head_64.S loaded address of a zero page into %gs
    for temporary use till pda is loaded but address to the actual pda is
    available at the point.  Load the real address directly instead.
    
    This will help unifying percpu and pda handling later on.
    
    This patch is mostly taken from Mike Travis' "x86_64: Fold pda into
    per cpu area" patch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 1a712da1dfa0..70d846628bbf 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -854,6 +854,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 #else
 	cpu_pda(cpu)->pcurrent = c_idle.idle;
 	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
+	initial_gs = (unsigned long)cpu_pda(cpu);
 #endif
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	initial_code = (unsigned long)start_secondary;

commit 7f268f4352cd7d3d18a20268887600aaebd9d974
Merge: a6525042bfdf 54da5b3d4423 b665967979d0 a08c4743ed5b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jan 15 13:18:57 2009 +0100

    Merge branches 'cpus4096', 'x86/cleanups' and 'x86/urgent' into x86/percpu

commit f11826385b63566d98c02d35f592232ee77cd791
Author: Jan Beulich <jbeulich@novell.com>
Date:   Wed Jan 14 12:27:35 2009 +0000

    x86: fully honor "nolapic"
    
    Impact: widen the effect of the 'nolapic' boot parameter
    
    "nolapic" should not only suppress SMP and use of the LAPIC, but it
    also ought to have the effect of disabling all IO-APIC related activity
    as well as PCI MSI and HT-IRQs.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index bb1a3b1fc87f..31f99ec2e0fd 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1125,6 +1125,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 		printk(KERN_ERR "... forcing use of dummy APIC emulation."
 				"(tell your hw vendor)\n");
 		smpboot_clear_io_apic();
+		disable_ioapic_setup();
 		return -1;
 	}
 

commit fb8fd077fbf0de6662acfd240e8e6b25cf3202ca
Author: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
Date:   Sat Jan 10 12:20:24 2009 +0530

    x86: smp.h move cpu_callout_mask and cpu_callout_map declartion to cpumask.h
    
    Impact: cleanup
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 84ac1cf46d87..6c2b8444b830 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -55,7 +55,6 @@
 #include <asm/idle.h>
 #include <asm/trampoline.h>
 #include <asm/cpu.h>
-#include <asm/cpumask.h>
 #include <asm/numa.h>
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>

commit 068790334cececc3d2d945617ccc585477da2e38
Author: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
Date:   Sat Jan 10 12:17:37 2009 +0530

    x86: smp.h move cpu_callin_mask and cpu_callin_map declartion to cpumask.h
    
    Impact: cleanup
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6c2b8444b830..84ac1cf46d87 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -55,6 +55,7 @@
 #include <asm/idle.h>
 #include <asm/trampoline.h>
 #include <asm/cpu.h>
+#include <asm/cpumask.h>
 #include <asm/numa.h>
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>

commit 1de8cd3cb9f61e854e743c7210df43db517d4832
Merge: 1eb1b3b65dc3 3d14bdad4031
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jan 10 23:56:42 2009 +0100

    Merge branch 'linus' into x86/cleanups

commit 3d14bdad40315b54470cb7812293d14c8af2bf7d
Merge: 4e9b1c184cad 51d7a1398d18
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 10 06:13:09 2009 -0800

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (36 commits)
      x86: fix section mismatch warnings in mcheck/mce_amd_64.c
      x86: offer frame pointers in all build modes
      x86: remove duplicated #include's
      x86: k8 numa register active regions later
      x86: update Alan Cox's email addresses
      x86: rename all fields of mpc_table mpc_X to X
      x86: rename all fields of mpc_oemtable oem_X to X
      x86: rename all fields of mpc_bus mpc_X to X
      x86: rename all fields of mpc_cpu mpc_X to X
      x86: rename all fields of mpc_intsrc mpc_X to X
      x86: rename all fields of mpc_lintsrc mpc_X to X
      x86: rename all fields of mpc_iopic mpc_X to X
      x86: irqinit_64.c init_ISA_irqs should be static
      Documentation/x86/boot.txt: payload length was changed to payload_length
      x86: setup_percpu.c fix style problems
      x86: irqinit_64.c fix style problems
      x86: irqinit_32.c fix style problems
      x86: i8259.c fix style problems
      x86: irq_32.c fix style problems
      x86: ioport.c fix style problems
      ...

commit 6e5385d44b2df05e50a8d07ba0e14d3e32685237
Author: Jaswinder Singh Rajput <jaswinder@infradead.org>
Date:   Wed Jan 7 18:11:35 2009 +0530

    x86: smp.h move prefill_possible_map declartion to cpu.h
    
    Impact: cleanup, moving NON-SMP stuff from smp.h
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 07576bee03ef..f8c885bed18c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -53,7 +53,6 @@
 #include <asm/nmi.h>
 #include <asm/irq.h>
 #include <asm/idle.h>
-#include <asm/smp.h>
 #include <asm/trampoline.h>
 #include <asm/cpu.h>
 #include <asm/numa.h>

commit 0936912274af78a21fd8d54c3f94a50a285cf7f9
Merge: 87c6fe26186d 9e9197370daf 40bcc69b399d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 6 17:39:52 2009 +0100

    Merge branches 'x86/cleanups', 'x86/mpparse', 'x86/numa' and 'x86/uv' into x86/urgent

commit 87c6fe26186d734e932426cc8ab9fd8cf9aeed94
Author: Alan Cox <alan@lxorguk.ukuu.org.uk>
Date:   Mon Jan 5 14:08:04 2009 +0000

    x86: update Alan Cox's email addresses
    
    Signed-off-by: Alan Cox <alan@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 31869bf5fabd..c628f9178cd8 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1,7 +1,7 @@
 /*
  *	x86 SMP booting functions
  *
- *	(c) 1995 Alan Cox, Building #3 <alan@redhat.com>
+ *	(c) 1995 Alan Cox, Building #3 <alan@lxorguk.ukuu.org.uk>
  *	(c) 1998, 1999, 2000 Ingo Molnar <mingo@redhat.com>
  *	Copyright 2001 Andi Kleen, SuSE Labs.
  *

commit c2d1cec1c77f7714672c1efeae075424c929e0d5
Author: Mike Travis <travis@sgi.com>
Date:   Sun Jan 4 05:18:03 2009 -0800

    x86: cleanup remaining cpumask_t ops in smpboot code
    
    Impact: use new cpumask API to reduce memory and stack usage
    
    Allocate the following local cpumasks based on the number of cpus that
    are present.  References will use new cpumask API.  (Currently only
    modified for x86_64, x86_32 continues to use the *_map variants.)
    
        cpu_callin_mask
        cpu_callout_mask
        cpu_initialized_mask
        cpu_sibling_setup_mask
    
    Provide the following accessor functions:
    
        struct cpumask *cpu_sibling_mask(int cpu)
        struct cpumask *cpu_core_mask(int cpu)
    
    Other changes are when setting or clearing the cpu online, possible
    or present maps, use the accessor functions.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6bd4d9b73870..00e17e589482 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -102,9 +102,6 @@ EXPORT_SYMBOL(smp_num_siblings);
 /* Last level cache ID of each logical CPU */
 DEFINE_PER_CPU(u16, cpu_llc_id) = BAD_APICID;
 
-cpumask_t cpu_callin_map;
-cpumask_t cpu_callout_map;
-
 /* representing HT siblings of each logical CPU */
 DEFINE_PER_CPU(cpumask_t, cpu_sibling_map);
 EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
@@ -120,9 +117,6 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 static atomic_t init_deasserted;
 
 
-/* representing cpus for which sibling maps can be computed */
-static cpumask_t cpu_sibling_setup_map;
-
 /* Set if we find a B stepping CPU */
 static int __cpuinitdata smp_b_stepping;
 
@@ -140,7 +134,7 @@ EXPORT_SYMBOL(cpu_to_node_map);
 static void map_cpu_to_node(int cpu, int node)
 {
 	printk(KERN_INFO "Mapping cpu %d to node %d\n", cpu, node);
-	cpu_set(cpu, node_to_cpumask_map[node]);
+	cpumask_set_cpu(cpu, &node_to_cpumask_map[node]);
 	cpu_to_node_map[cpu] = node;
 }
 
@@ -151,7 +145,7 @@ static void unmap_cpu_to_node(int cpu)
 
 	printk(KERN_INFO "Unmapping cpu %d from all nodes\n", cpu);
 	for (node = 0; node < MAX_NUMNODES; node++)
-		cpu_clear(cpu, node_to_cpumask_map[node]);
+		cpumask_clear_cpu(cpu, &node_to_cpumask_map[node]);
 	cpu_to_node_map[cpu] = 0;
 }
 #else /* !(CONFIG_NUMA && CONFIG_X86_32) */
@@ -209,7 +203,7 @@ static void __cpuinit smp_callin(void)
 	 */
 	phys_id = read_apic_id();
 	cpuid = smp_processor_id();
-	if (cpu_isset(cpuid, cpu_callin_map)) {
+	if (cpumask_test_cpu(cpuid, cpu_callin_mask)) {
 		panic("%s: phys CPU#%d, CPU#%d already present??\n", __func__,
 					phys_id, cpuid);
 	}
@@ -231,7 +225,7 @@ static void __cpuinit smp_callin(void)
 		/*
 		 * Has the boot CPU finished it's STARTUP sequence?
 		 */
-		if (cpu_isset(cpuid, cpu_callout_map))
+		if (cpumask_test_cpu(cpuid, cpu_callout_mask))
 			break;
 		cpu_relax();
 	}
@@ -274,7 +268,7 @@ static void __cpuinit smp_callin(void)
 	/*
 	 * Allow the master to continue.
 	 */
-	cpu_set(cpuid, cpu_callin_map);
+	cpumask_set_cpu(cpuid, cpu_callin_mask);
 }
 
 static int __cpuinitdata unsafe_smp;
@@ -332,7 +326,7 @@ notrace static void __cpuinit start_secondary(void *unused)
 	ipi_call_lock();
 	lock_vector_lock();
 	__setup_vector_irq(smp_processor_id());
-	cpu_set(smp_processor_id(), cpu_online_map);
+	set_cpu_online(smp_processor_id(), true);
 	unlock_vector_lock();
 	ipi_call_unlock();
 	per_cpu(cpu_state, smp_processor_id()) = CPU_ONLINE;
@@ -438,50 +432,52 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 	int i;
 	struct cpuinfo_x86 *c = &cpu_data(cpu);
 
-	cpu_set(cpu, cpu_sibling_setup_map);
+	cpumask_set_cpu(cpu, cpu_sibling_setup_mask);
 
 	if (smp_num_siblings > 1) {
-		for_each_cpu_mask_nr(i, cpu_sibling_setup_map) {
-			if (c->phys_proc_id == cpu_data(i).phys_proc_id &&
-			    c->cpu_core_id == cpu_data(i).cpu_core_id) {
-				cpu_set(i, per_cpu(cpu_sibling_map, cpu));
-				cpu_set(cpu, per_cpu(cpu_sibling_map, i));
-				cpu_set(i, per_cpu(cpu_core_map, cpu));
-				cpu_set(cpu, per_cpu(cpu_core_map, i));
-				cpu_set(i, c->llc_shared_map);
-				cpu_set(cpu, cpu_data(i).llc_shared_map);
+		for_each_cpu(i, cpu_sibling_setup_mask) {
+			struct cpuinfo_x86 *o = &cpu_data(i);
+
+			if (c->phys_proc_id == o->phys_proc_id &&
+			    c->cpu_core_id == o->cpu_core_id) {
+				cpumask_set_cpu(i, cpu_sibling_mask(cpu));
+				cpumask_set_cpu(cpu, cpu_sibling_mask(i));
+				cpumask_set_cpu(i, cpu_core_mask(cpu));
+				cpumask_set_cpu(cpu, cpu_core_mask(i));
+				cpumask_set_cpu(i, &c->llc_shared_map);
+				cpumask_set_cpu(cpu, &o->llc_shared_map);
 			}
 		}
 	} else {
-		cpu_set(cpu, per_cpu(cpu_sibling_map, cpu));
+		cpumask_set_cpu(cpu, cpu_sibling_mask(cpu));
 	}
 
-	cpu_set(cpu, c->llc_shared_map);
+	cpumask_set_cpu(cpu, &c->llc_shared_map);
 
 	if (current_cpu_data.x86_max_cores == 1) {
-		per_cpu(cpu_core_map, cpu) = per_cpu(cpu_sibling_map, cpu);
+		cpumask_copy(cpu_core_mask(cpu), cpu_sibling_mask(cpu));
 		c->booted_cores = 1;
 		return;
 	}
 
-	for_each_cpu_mask_nr(i, cpu_sibling_setup_map) {
+	for_each_cpu(i, cpu_sibling_setup_mask) {
 		if (per_cpu(cpu_llc_id, cpu) != BAD_APICID &&
 		    per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, i)) {
-			cpu_set(i, c->llc_shared_map);
-			cpu_set(cpu, cpu_data(i).llc_shared_map);
+			cpumask_set_cpu(i, &c->llc_shared_map);
+			cpumask_set_cpu(cpu, &cpu_data(i).llc_shared_map);
 		}
 		if (c->phys_proc_id == cpu_data(i).phys_proc_id) {
-			cpu_set(i, per_cpu(cpu_core_map, cpu));
-			cpu_set(cpu, per_cpu(cpu_core_map, i));
+			cpumask_set_cpu(i, cpu_core_mask(cpu));
+			cpumask_set_cpu(cpu, cpu_core_mask(i));
 			/*
 			 *  Does this new cpu bringup a new core?
 			 */
-			if (cpus_weight(per_cpu(cpu_sibling_map, cpu)) == 1) {
+			if (cpumask_weight(cpu_sibling_mask(cpu)) == 1) {
 				/*
 				 * for each core in package, increment
 				 * the booted_cores for this new cpu
 				 */
-				if (first_cpu(per_cpu(cpu_sibling_map, i)) == i)
+				if (cpumask_first(cpu_sibling_mask(i)) == i)
 					c->booted_cores++;
 				/*
 				 * increment the core count for all
@@ -504,7 +500,7 @@ const struct cpumask *cpu_coregroup_mask(int cpu)
 	 * And for power savings, we return cpu_core_map
 	 */
 	if (sched_mc_power_savings || sched_smt_power_savings)
-		return &per_cpu(cpu_core_map, cpu);
+		return cpu_core_mask(cpu);
 	else
 		return &c->llc_shared_map;
 }
@@ -523,7 +519,7 @@ static void impress_friends(void)
 	 */
 	pr_debug("Before bogomips.\n");
 	for_each_possible_cpu(cpu)
-		if (cpu_isset(cpu, cpu_callout_map))
+		if (cpumask_test_cpu(cpu, cpu_callout_mask))
 			bogosum += cpu_data(cpu).loops_per_jiffy;
 	printk(KERN_INFO
 		"Total of %d processors activated (%lu.%02lu BogoMIPS).\n",
@@ -904,19 +900,19 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		 * allow APs to start initializing.
 		 */
 		pr_debug("Before Callout %d.\n", cpu);
-		cpu_set(cpu, cpu_callout_map);
+		cpumask_set_cpu(cpu, cpu_callout_mask);
 		pr_debug("After Callout %d.\n", cpu);
 
 		/*
 		 * Wait 5s total for a response
 		 */
 		for (timeout = 0; timeout < 50000; timeout++) {
-			if (cpu_isset(cpu, cpu_callin_map))
+			if (cpumask_test_cpu(cpu, cpu_callin_mask))
 				break;	/* It has booted */
 			udelay(100);
 		}
 
-		if (cpu_isset(cpu, cpu_callin_map)) {
+		if (cpumask_test_cpu(cpu, cpu_callin_mask)) {
 			/* number CPUs logically, starting from 1 (BSP is 0) */
 			pr_debug("OK.\n");
 			printk(KERN_INFO "CPU%d: ", cpu);
@@ -941,9 +937,14 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	if (boot_error) {
 		/* Try to put things back the way they were before ... */
 		numa_remove_cpu(cpu); /* was set by numa_add_cpu */
-		cpu_clear(cpu, cpu_callout_map); /* was set by do_boot_cpu() */
-		cpu_clear(cpu, cpu_initialized); /* was set by cpu_init() */
-		cpu_clear(cpu, cpu_present_map);
+
+		/* was set by do_boot_cpu() */
+		cpumask_clear_cpu(cpu, cpu_callout_mask);
+
+		/* was set by cpu_init() */
+		cpumask_clear_cpu(cpu, cpu_initialized_mask);
+
+		set_cpu_present(cpu, false);
 		per_cpu(x86_cpu_to_apicid, cpu) = BAD_APICID;
 	}
 
@@ -977,7 +978,7 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 	/*
 	 * Already booted CPU?
 	 */
-	if (cpu_isset(cpu, cpu_callin_map)) {
+	if (cpumask_test_cpu(cpu, cpu_callin_mask)) {
 		pr_debug("do_boot_cpu %d Already started\n", cpu);
 		return -ENOSYS;
 	}
@@ -1032,8 +1033,9 @@ int __cpuinit native_cpu_up(unsigned int cpu)
  */
 static __init void disable_smp(void)
 {
-	cpu_present_map = cpumask_of_cpu(0);
-	cpu_possible_map = cpumask_of_cpu(0);
+	/* use the read/write pointers to the present and possible maps */
+	cpumask_copy(&cpu_present_map, cpumask_of(0));
+	cpumask_copy(&cpu_possible_map, cpumask_of(0));
 	smpboot_clear_io_apic_irqs();
 
 	if (smp_found_config)
@@ -1041,8 +1043,8 @@ static __init void disable_smp(void)
 	else
 		physid_set_mask_of_physid(0, &phys_cpu_present_map);
 	map_cpu_to_logical_apicid();
-	cpu_set(0, per_cpu(cpu_sibling_map, 0));
-	cpu_set(0, per_cpu(cpu_core_map, 0));
+	cpumask_set_cpu(0, cpu_sibling_mask(0));
+	cpumask_set_cpu(0, cpu_core_mask(0));
 }
 
 /*
@@ -1064,14 +1066,14 @@ static int __init smp_sanity_check(unsigned max_cpus)
 		nr = 0;
 		for_each_present_cpu(cpu) {
 			if (nr >= 8)
-				cpu_clear(cpu, cpu_present_map);
+				set_cpu_present(cpu, false);
 			nr++;
 		}
 
 		nr = 0;
 		for_each_possible_cpu(cpu) {
 			if (nr >= 8)
-				cpu_clear(cpu, cpu_possible_map);
+				set_cpu_possible(cpu, false);
 			nr++;
 		}
 
@@ -1167,7 +1169,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	preempt_disable();
 	smp_cpu_index_default();
 	current_cpu_data = boot_cpu_data;
-	cpu_callin_map = cpumask_of_cpu(0);
+	cpumask_copy(cpu_callin_mask, cpumask_of(0));
 	mb();
 	/*
 	 * Setup boot CPU information
@@ -1242,8 +1244,8 @@ void __init native_smp_prepare_boot_cpu(void)
 	init_gdt(me);
 #endif
 	switch_to_new_gdt();
-	/* already set me in cpu_online_map in boot_cpu_init() */
-	cpu_set(me, cpu_callout_map);
+	/* already set me in cpu_online_mask in boot_cpu_init() */
+	cpumask_set_cpu(me, cpu_callout_mask);
 	per_cpu(cpu_state, me) = CPU_ONLINE;
 }
 
@@ -1311,7 +1313,7 @@ __init void prefill_possible_map(void)
 		possible, max_t(int, possible - num_processors, 0));
 
 	for (i = 0; i < possible; i++)
-		cpu_set(i, cpu_possible_map);
+		set_cpu_possible(i, true);
 
 	nr_cpu_ids = possible;
 }
@@ -1323,31 +1325,31 @@ static void remove_siblinginfo(int cpu)
 	int sibling;
 	struct cpuinfo_x86 *c = &cpu_data(cpu);
 
-	for_each_cpu_mask_nr(sibling, per_cpu(cpu_core_map, cpu)) {
-		cpu_clear(cpu, per_cpu(cpu_core_map, sibling));
+	for_each_cpu(sibling, cpu_core_mask(cpu)) {
+		cpumask_clear_cpu(cpu, cpu_core_mask(sibling));
 		/*/
 		 * last thread sibling in this cpu core going down
 		 */
-		if (cpus_weight(per_cpu(cpu_sibling_map, cpu)) == 1)
+		if (cpumask_weight(cpu_sibling_mask(cpu)) == 1)
 			cpu_data(sibling).booted_cores--;
 	}
 
-	for_each_cpu_mask_nr(sibling, per_cpu(cpu_sibling_map, cpu))
-		cpu_clear(cpu, per_cpu(cpu_sibling_map, sibling));
-	cpus_clear(per_cpu(cpu_sibling_map, cpu));
-	cpus_clear(per_cpu(cpu_core_map, cpu));
+	for_each_cpu(sibling, cpu_sibling_mask(cpu))
+		cpumask_clear_cpu(cpu, cpu_sibling_mask(sibling));
+	cpumask_clear(cpu_sibling_mask(cpu));
+	cpumask_clear(cpu_core_mask(cpu));
 	c->phys_proc_id = 0;
 	c->cpu_core_id = 0;
-	cpu_clear(cpu, cpu_sibling_setup_map);
+	cpumask_clear_cpu(cpu, cpu_sibling_setup_mask);
 }
 
 static void __ref remove_cpu_from_maps(int cpu)
 {
-	cpu_clear(cpu, cpu_online_map);
-	cpu_clear(cpu, cpu_callout_map);
-	cpu_clear(cpu, cpu_callin_map);
+	set_cpu_online(cpu, false);
+	cpumask_clear_cpu(cpu, cpu_callout_mask);
+	cpumask_clear_cpu(cpu, cpu_callin_mask);
 	/* was set by cpu_init() */
-	cpu_clear(cpu, cpu_initialized);
+	cpumask_clear_cpu(cpu, cpu_initialized_mask);
 	numa_remove_cpu(cpu);
 }
 

commit 9628937d5b37169151c5f6bbd40919c6ac958a46
Author: Mike Travis <travis@sgi.com>
Date:   Wed Dec 31 18:08:46 2008 -0800

    x86: cleanup some remaining usages of NR_CPUS where s/b nr_cpu_ids
    
    Impact: Reduce future system panics due to cpumask operations using NR_CPUS
    
    Insure that code does not look at bits >= nr_cpu_ids as when cpumasks are
    allocated based on nr_cpu_ids, these extra bits will not be defined.
    
    Also some other minor updates:
    
       * change in to use cpu accessor function set_cpu_present() instead of
         directly accessing cpu_present_map w/cpu_clear() [arch/x86/kernel/reboot.c]
    
       * use cpumask_of() instead of &cpumask_of_cpu() [arch/x86/kernel/reboot.c]
    
       * optimize some cpu_mask_to_apicid_and functions.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f49c26bd7e2d..6bd4d9b73870 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1154,7 +1154,7 @@ static void __init smp_cpu_index_default(void)
 	for_each_possible_cpu(i) {
 		c = &cpu_data(i);
 		/* mark all to hotplug */
-		c->cpu_index = NR_CPUS;
+		c->cpu_index = nr_cpu_ids;
 	}
 }
 

commit 730cf27246225d56ca1603b2f3c4fdbf882d4e51
Author: Mike Travis <travis@sgi.com>
Date:   Wed Dec 31 18:08:45 2008 -0800

    x86: enable cpus display of kernel_max and offlined cpus
    
    Impact: enables /sys/devices/system/cpu/{kernel_max,offline} user interface
    
    By setting total_cpus, the drivers/base/cpu.c will display the
    values of kernel_max (NR_CPUS-1) and the offlined cpu map.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9e177a4077ee..f49c26bd7e2d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1298,6 +1298,8 @@ __init void prefill_possible_map(void)
 	else
 		possible = setup_possible_cpus;
 
+	total_cpus = max_t(int, possible, num_processors + disabled_cpus);
+
 	if (possible > CONFIG_NR_CPUS) {
 		printk(KERN_WARNING
 			"%d Processors exceeds NR_CPUS limit of %d\n",

commit 7eb19553369c46cc1fa64caf120cbcab1b597f7c
Merge: 6092848a2a23 8c384cdee3e0
Author: Mike Travis <travis@sgi.com>
Date:   Wed Dec 31 17:34:16 2008 -0800

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux-2.6-cpumask into merge-rr-cpumask
    
    Conflicts:
            arch/x86/kernel/io_apic.c
            kernel/rcuclassic.c
            kernel/sched.c
            kernel/time/tick-sched.c
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    [ mingo@elte.hu: backmerged typo fix for io_apic.c ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit b840d79631c882786925303c2b0f4fefc31845ed
Merge: 597b0d21626d c3d80000e3a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 2 11:44:09 2009 -0800

    Merge branch 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (66 commits)
      x86: export vector_used_by_percpu_irq
      x86: use logical apicid in x2apic_cluster's x2apic_cpu_mask_to_apicid_and()
      sched: nominate preferred wakeup cpu, fix
      x86: fix lguest used_vectors breakage, -v2
      x86: fix warning in arch/x86/kernel/io_apic.c
      sched: fix warning in kernel/sched.c
      sched: move test_sd_parent() to an SMP section of sched.h
      sched: add SD_BALANCE_NEWIDLE at MC and CPU level for sched_mc>0
      sched: activate active load balancing in new idle cpus
      sched: bias task wakeups to preferred semi-idle packages
      sched: nominate preferred wakeup cpu
      sched: favour lower logical cpu number for sched_mc balance
      sched: framework for sched_mc/smt_power_savings=N
      sched: convert BALANCE_FOR_xx_POWER to inline functions
      x86: use possible_cpus=NUM to extend the possible cpus allowed
      x86: fix cpu_mask_to_apicid_and to include cpu_online_mask
      x86: update io_apic.c to the new cpumask code
      x86: Introduce topology_core_cpumask()/topology_thread_cpumask()
      x86: xen: use smp_call_function_many()
      x86: use work_on_cpu in x86/kernel/cpu/mcheck/mce_amd_64.c
      ...
    
    Fixed up trivial conflict in kernel/time/tick-sched.c manually

commit 33edcf133ba93ecba2e4b6472e97b689895d805c
Merge: be4d638c1597 3c92ec8ae91e
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Dec 30 08:02:35 2008 +1030

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6

commit b0f4b285d7ed174804658539129a834270f4829a
Merge: be9c5ae4eeec 5250d329e38c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 28 12:21:10 2008 -0800

    Merge branch 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (241 commits)
      sched, trace: update trace_sched_wakeup()
      tracing/ftrace: don't trace on early stage of a secondary cpu boot, v3
      Revert "x86: disable X86_PTRACE_BTS"
      ring-buffer: prevent false positive warning
      ring-buffer: fix dangling commit race
      ftrace: enable format arguments checking
      x86, bts: memory accounting
      x86, bts: add fork and exit handling
      ftrace: introduce tracing_reset_online_cpus() helper
      tracing: fix warnings in kernel/trace/trace_sched_switch.c
      tracing: fix warning in kernel/trace/trace.c
      tracing/ring-buffer: remove unused ring_buffer size
      trace: fix task state printout
      ftrace: add not to regex on filtering functions
      trace: better use of stack_trace_enabled for boot up code
      trace: add a way to enable or disable the stack tracer
      x86: entry_64 - introduce FTRACE_ frame macro v2
      tracing/ftrace: add the printk-msg-only option
      tracing/ftrace: use preempt_enable_no_resched_notrace in ring_buffer_time_stamp()
      x86, bts: correctly report invalid bts records
      ...
    
    Fixed up trivial conflict in scripts/recordmcount.pl due to SH bits
    being already partly merged by the SH merge.

commit 030bb203e01db12e3f2866799f4f03a114d06349
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Dec 26 22:23:41 2008 +1030

    cpumask: cpu_coregroup_mask(): x86
    
    Impact: New API
    
    Like cpu_coregroup_map, but returns a (const) pointer.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Cc: Ingo Molnar <mingo@redhat.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 468c2f9d47ae..d5274b6b088e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -497,7 +497,7 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 }
 
 /* maps the cpu to the sched domain representing multi-core */
-cpumask_t cpu_coregroup_map(int cpu)
+const struct cpumask *cpu_coregroup_mask(int cpu)
 {
 	struct cpuinfo_x86 *c = &cpu_data(cpu);
 	/*
@@ -505,9 +505,14 @@ cpumask_t cpu_coregroup_map(int cpu)
 	 * And for power savings, we return cpu_core_map
 	 */
 	if (sched_mc_power_savings || sched_smt_power_savings)
-		return per_cpu(cpu_core_map, cpu);
+		return &per_cpu(cpu_core_map, cpu);
 	else
-		return c->llc_shared_map;
+		return &c->llc_shared_map;
+}
+
+cpumask_t cpu_coregroup_map(int cpu)
+{
+	return *cpu_coregroup_mask(cpu);
 }
 
 static void impress_friends(void)

commit 0ca59dd948a51c95d5a366d35f897bc5ef9df55d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 24 23:30:02 2008 +0100

    tracing/ftrace: don't trace on early stage of a secondary cpu boot, v3
    
    Impact: fix a crash/hard-reboot on certain configs while enabling cpu runtime
    
    On some archs, the boot of a secondary cpu can have an early fragile state.
    On x86-64, the pda is not initialized on the first stage of a cpu boot but
    it is needed to get the cpu number and the current task pointer. This data
    is needed during tracing. As they were dereferenced at this stage, we got a
    crash while tracing a cpu being enabled at runtime.
    
    Some other archs like ia64 can have such kind of issue too.
    
    Changes on v2:
    
    We dropped the previous solution of a per-arch called function to guess the
    current state of a cpu. That could slow down the tracing.
    
    This patch removes the -pg flag on arch/x86/kernel/cpu/common.c where
    the low level cpu boot functions exist, on start_secondary() and a helper
    function used at this stage.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f71f96fc9e62..f6174d229024 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -287,7 +287,7 @@ static int __cpuinitdata unsafe_smp;
 /*
  * Activate a secondary processor.
  */
-static void __cpuinit start_secondary(void *unused)
+notrace static void __cpuinit start_secondary(void *unused)
 {
 	/*
 	 * Don't put *anything* before cpu_init(), SMP booting is too

commit fa623d1b0222adbe8f822e53c08003b9679a410c
Merge: 3d44cc3e01ee 1ccedb7cdba6 34945ede3107 d43779740621 c415b3dce30d beeb4195cbc8 f269b07e862c 4e42ebd57b2e e1286f2c686f 878719e831d9 fd28a5b58ddd adf77bac052b 8f2466f45f75 93093d099e5d bb5574608a83 f34a10bd9f8c b6fd6f26733e 30604bb410b5 5b9a0e14eb4b 67bac792cd0c 7a9787e1eba9 f4166c54bfe0 69b88afa8d11 8daa19051e1c 3e1e9002aa8b 8403295e0fa4 4db646b1af8f 205516c12dbb c8182f0016fb ecbf29cdb399
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 23 16:27:23 2008 +0100

    Merge branches 'x86/apic', 'x86/cleanups', 'x86/cpufeature', 'x86/crashdump', 'x86/debug', 'x86/defconfig', 'x86/detect-hyper', 'x86/doc', 'x86/dumpstack', 'x86/early-printk', 'x86/fpu', 'x86/idle', 'x86/io', 'x86/memory-corruption-check', 'x86/microcode', 'x86/mm', 'x86/mtrr', 'x86/nmi-watchdog', 'x86/pat2', 'x86/pci-ioapic-boot-irq-quirks', 'x86/ptrace', 'x86/quirks', 'x86/reboot', 'x86/setup-memory', 'x86/signal', 'x86/sparse-fixes', 'x86/time', 'x86/uv' and 'x86/xen' into x86/core

commit 3b11ce7f542e415c90267b4482d4611410b468e6
Author: Mike Travis <travis@sgi.com>
Date:   Wed Dec 17 15:21:39 2008 -0800

    x86: use possible_cpus=NUM to extend the possible cpus allowed
    
    Impact: add new boot parameter
    
    Use possible_cpus=NUM kernel parameter to extend the number of possible
    cpus.
    
    The ability to HOTPLUG ON cpus that are "possible" but not "present" is
    dealt with in a later patch.
    
    Signed-off-by: Mike Travis <travis@sgi.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index be9466788043..1a9941b11150 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1252,6 +1252,15 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 	check_nmi_watchdog();
 }
 
+static int __initdata setup_possible_cpus = -1;
+static int __init _setup_possible_cpus(char *str)
+{
+	get_option(&str, &setup_possible_cpus);
+	return 0;
+}
+early_param("possible_cpus", _setup_possible_cpus);
+
+
 /*
  * cpu_possible_map should be static, it cannot change as cpu's
  * are onlined, or offlined. The reason is per-cpu data-structures
@@ -1264,7 +1273,7 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
  *
  * Three ways to find out the number of additional hotplug CPUs:
  * - If the BIOS specified disabled CPUs in ACPI/mptables use that.
- * - The user can overwrite it with additional_cpus=NUM
+ * - The user can overwrite it with possible_cpus=NUM
  * - Otherwise don't reserve additional CPUs.
  * We do this because additional CPUs waste a lot of memory.
  * -AK
@@ -1277,9 +1286,17 @@ __init void prefill_possible_map(void)
 	if (!num_processors)
 		num_processors = 1;
 
-	possible = num_processors + disabled_cpus;
-	if (possible > NR_CPUS)
-		possible = NR_CPUS;
+	if (setup_possible_cpus == -1)
+		possible = num_processors + disabled_cpus;
+	else
+		possible = setup_possible_cpus;
+
+	if (possible > CONFIG_NR_CPUS) {
+		printk(KERN_WARNING
+			"%d Processors exceeds NR_CPUS limit of %d\n",
+			possible, CONFIG_NR_CPUS);
+		possible = CONFIG_NR_CPUS;
+	}
 
 	printk(KERN_INFO "SMP: Allowing %d CPUs, %d hotplug CPUs\n",
 		possible, max_t(int, possible - num_processors, 0));

commit 9466d6036f73481104039fbe99338baed11c8fea
Merge: 1f3f424a6bee 83b19597f793
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 17 13:08:34 2008 +0100

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/travis/linux-2.6-cpus4096-for-ingo into cpus4096

commit 1f3f424a6bee9de4d839be9951f4296333fac014
Merge: c8cae544bba6 1bda71282ded
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 17 13:07:48 2008 +0100

    Merge branch 'linus' into cpus4096

commit d7b381bb7b1ad69ff008ea063d26e988b686c8de
Author: Mike Travis <travis@sgi.com>
Date:   Tue Dec 16 17:33:58 2008 -0800

    x86: fixup_irqs() doesnt need an argument.
    
    Impact: cleanup, remove on-stack cpumask.
    
    The "map" arg is always cpu_online_mask.  Importantly, set_affinity
    always ands the argument with cpu_online_mask anyway, so we don't need
    to do it in fixup_irqs(), avoiding a temporary.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9d58134e0231..8b6f675b363b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1346,7 +1346,7 @@ void cpu_disable_common(void)
 	lock_vector_lock();
 	remove_cpu_from_maps(cpu);
 	unlock_vector_lock();
-	fixup_irqs(cpu_online_map);
+	fixup_irqs();
 }
 
 int native_cpu_disable(void)

commit ae8d04e2ecbb233926860e9ce145eac19c7835dc
Author: Zachary Amsden <zach@vmware.com>
Date:   Sat Dec 13 12:36:58 2008 -0800

    x86 Fix VMI crash on boot in 2.6.28-rc8
    
    VMI initialiation can relocate the fixmap, causing early_ioremap to
    malfunction if it is initialized before the relocation.  To fix this,
    VMI activation is split into two phases; the detection, which must
    happen before setting up ioremap, and the activation, which must happen
    after parsing early boot parameters.
    
    This fixes a crash on boot when VMI is enabled under VMware.
    
    Signed-off-by: Zachary Amsden <zach@vmware.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7b1093397319..f71f96fc9e62 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -294,9 +294,7 @@ static void __cpuinit start_secondary(void *unused)
 	 * fragile that we want to limit the things done here to the
 	 * most necessary things.
 	 */
-#ifdef CONFIG_VMI
 	vmi_bringup();
-#endif
 	cpu_init();
 	preempt_disable();
 	smp_callin();

commit 968ea6d80e395cf11a51143cfa1b9a14ada676df
Merge: 7be7585393d3 8299608f140a
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sat Dec 13 21:55:51 2008 +1030

    Merge ../linux-2.6-x86
    
    Conflicts:
    
            arch/x86/kernel/io_apic.c
            kernel/sched.c
            kernel/sched_stats.h

commit 98a79d6a50181ca1ecf7400eda01d5dc1bc0dbf0
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sat Dec 13 21:19:41 2008 +1030

    cpumask: centralize cpu_online_map and cpu_possible_map
    
    Impact: cleanup
    
    Each SMP arch defines these themselves.  Move them to a central
    location.
    
    Twists:
    1) Some archs (m32, parisc, s390) set possible_map to all 1, so we add a
       CONFIG_INIT_ALL_POSSIBLE for this rather than break them.
    
    2) mips and sparc32 '#define cpu_possible_map phys_cpu_present_map'.
       Those archs simply have phys_cpu_present_map replaced everywhere.
    
    3) Alpha defined cpu_possible_map to cpu_present_map; this is tricky
       so I just manipulate them both in sync.
    
    4) IA64, cris and m32r have gratuitous 'extern cpumask_t cpu_possible_map'
       declarations.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Grant Grundler <grundler@parisc-linux.org>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Mike Travis <travis@sgi.com>
    Cc: ink@jurassic.park.msu.ru
    Cc: rmk@arm.linux.org.uk
    Cc: starvik@axis.com
    Cc: tony.luck@intel.com
    Cc: takata@linux-m32r.org
    Cc: ralf@linux-mips.org
    Cc: grundler@parisc-linux.org
    Cc: paulus@samba.org
    Cc: schwidefsky@de.ibm.com
    Cc: lethal@linux-sh.org
    Cc: wli@holomorphy.com
    Cc: davem@davemloft.net
    Cc: jdike@addtoit.com
    Cc: mingo@redhat.com

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7b1093397319..468c2f9d47ae 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -101,14 +101,8 @@ EXPORT_SYMBOL(smp_num_siblings);
 /* Last level cache ID of each logical CPU */
 DEFINE_PER_CPU(u16, cpu_llc_id) = BAD_APICID;
 
-/* bitmap of online cpus */
-cpumask_t cpu_online_map __read_mostly;
-EXPORT_SYMBOL(cpu_online_map);
-
 cpumask_t cpu_callin_map;
 cpumask_t cpu_callout_map;
-cpumask_t cpu_possible_map;
-EXPORT_SYMBOL(cpu_possible_map);
 
 /* representing HT siblings of each logical CPU */
 DEFINE_PER_CPU(cpumask_t, cpu_sibling_map);

commit 55c395b47042e12d5c25aa07f271f56ffe44f793
Author: Michael Tokarev <mjt@tls.msk.ru>
Date:   Fri Dec 5 14:42:20 2008 +0300

    x86: fix missing space in printk
    
    Just come across this when booting on an old hw..
    Looks somewhat ugly, that single missing space ;)
    
    Signed-off-by: Michael Tokarev <mjt@tls.msk.ru>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7b1093397319..1a3c3253f0ed 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1086,8 +1086,10 @@ static int __init smp_sanity_check(unsigned max_cpus)
 #endif
 
 	if (!physid_isset(hard_smp_processor_id(), phys_cpu_present_map)) {
-		printk(KERN_WARNING "weird, boot CPU (#%d) not listed"
-				    "by the BIOS.\n", hard_smp_processor_id());
+		printk(KERN_WARNING
+			"weird, boot CPU (#%d) not listed by the BIOS.\n",
+			hard_smp_processor_id());
+
 		physid_set(hard_smp_processor_id(), phys_cpu_present_map);
 	}
 

commit 54ac14a8e982ae6c7ac71ee2b0d0173b974509e2
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Nov 17 15:19:53 2008 -0800

    x86: fix wakeup_cpu with numaq/es7000, v2, fix
    
    Impact: fix wakeup_secondary_cpu with hotplug
    
    We can not put that into x86_quirks, because that is __initdata.
    So try to move that to genapic, and add update_genapic in x86_quirks.
    
    later we even could use that stub to:
    
     1. autodetect CONFIG_ES7000_CLUSTERED_APIC
     2. more correct inquire_remote_apic with apic_verbosity setting.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 498c1ef37fe0..0e9f446269f4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -615,7 +615,7 @@ wakeup_secondary_cpu_via_nmi(int logical_apicid, unsigned long start_eip)
 	return (send_status | accept_status);
 }
 
-static int __devinit
+int __devinit
 wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 {
 	unsigned long send_status, accept_status = 0;
@@ -736,15 +736,6 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 	return (send_status | accept_status);
 }
 
-static int __devinit
-wakeup_secondary_cpu(int apicid, unsigned long start_eip)
-{
-	if (x86_quirks->wakeup_secondary_cpu)
-		return x86_quirks->wakeup_secondary_cpu(apicid, start_eip);
-
-	return wakeup_secondary_cpu_via_init(apicid, start_eip);
-}
-
 struct create_idle {
 	struct work_struct work;
 	struct task_struct *idle;

commit 569712b2b0970fa5b19673544d62ae661d04a220
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sun Nov 16 03:12:49 2008 -0800

    x86: fix wakeup_cpu with numaq/es7000, v2
    
    Impact: fix secondary-CPU wakeup/init path with numaq and es7000
    
    While looking at wakeup_secondary_cpu for WAKE_SECONDARY_VIA_NMI:
    
    |#ifdef WAKE_SECONDARY_VIA_NMI
    |/*
    | * Poke the other CPU in the eye via NMI to wake it up. Remember that the normal
    | * INIT, INIT, STARTUP sequence will reset the chip hard for us, and this
    | * won't ... remember to clear down the APIC, etc later.
    | */
    |static int __devinit
    |wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
    |{
    |        unsigned long send_status, accept_status = 0;
    |        int maxlvt;
    |...
    |        if (APIC_INTEGRATED(apic_version[phys_apicid])) {
    |                maxlvt = lapic_get_maxlvt();
    
    I noticed that there is no warning about undefined phys_apicid...
    
    because WAKE_SECONDARY_VIA_NMI and WAKE_SECONDARY_VIA_INIT can not be
    defined at the same time. So NUMAQ is using wrong wakeup_secondary_cpu.
    
    WAKE_SECONDARY_VIA_NMI, WAKE_SECONDARY_VIA_INIT and
    WAKE_SECONDARY_VIA_MIP are variants of a weird and fragile
    preprocessor-driven "HAL" mechanisms to specify the kind of secondary-CPU
    wakeup strategy a given x86 kernel will use.
    
    The vast majority of systems want to use INIT for secondary wakeup - NUMAQ
    uses an NMI, (old-style-) ES7000 uses 'MIP' (a firmware driven in-memory
    flag to let secondaries continue).
    
    So convert these mechanisms to x86_quirks and add a
    ->wakeup_secondary_cpu() method to specify the rare exception
    to the sane default.
    
    Extend genapic accordingly as well, for 32-bit.
    
    While looking further, I noticed that functions in wakecup.h for numaq
    and es7000 are different to the default in mach_wakecpu.h - but smpboot.c
    will only use default mach_wakecpu.h with smphook.h.
    
    So we need to add mach_wakecpu.h for mach_generic, to properly support
    numaq and es7000, and vectorize the following SMP init methods:
    
            int trampoline_phys_low;
            int trampoline_phys_high;
            void (*wait_for_init_deassert)(atomic_t *deassert);
            void (*smp_callin_clear_local_apic)(void);
            void (*store_NMI_vector)(unsigned short *high, unsigned short *low);
            void (*restore_NMI_vector)(unsigned short *high, unsigned short *low);
            void (*inquire_remote_apic)(int apicid);
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7b1093397319..498c1ef37fe0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -62,6 +62,7 @@
 #include <asm/mtrr.h>
 #include <asm/vmi.h>
 #include <asm/genapic.h>
+#include <asm/setup.h>
 #include <linux/mc146818rtc.h>
 
 #include <mach_apic.h>
@@ -536,7 +537,7 @@ static void impress_friends(void)
 	pr_debug("Before bogocount - setting activated=1.\n");
 }
 
-static inline void __inquire_remote_apic(int apicid)
+void __inquire_remote_apic(int apicid)
 {
 	unsigned i, regs[] = { APIC_ID >> 4, APIC_LVR >> 4, APIC_SPIV >> 4 };
 	char *names[] = { "ID", "VERSION", "SPIV" };
@@ -575,14 +576,13 @@ static inline void __inquire_remote_apic(int apicid)
 	}
 }
 
-#ifdef WAKE_SECONDARY_VIA_NMI
 /*
  * Poke the other CPU in the eye via NMI to wake it up. Remember that the normal
  * INIT, INIT, STARTUP sequence will reset the chip hard for us, and this
  * won't ... remember to clear down the APIC, etc later.
  */
-static int __devinit
-wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
+int __devinit
+wakeup_secondary_cpu_via_nmi(int logical_apicid, unsigned long start_eip)
 {
 	unsigned long send_status, accept_status = 0;
 	int maxlvt;
@@ -599,7 +599,7 @@ wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
 	 * Give the other CPU some time to accept the IPI.
 	 */
 	udelay(200);
-	if (APIC_INTEGRATED(apic_version[phys_apicid])) {
+	if (APIC_INTEGRATED(apic_version[boot_cpu_physical_apicid])) {
 		maxlvt = lapic_get_maxlvt();
 		if (maxlvt > 3)			/* Due to the Pentium erratum 3AP.  */
 			apic_write(APIC_ESR, 0);
@@ -614,11 +614,9 @@ wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
 
 	return (send_status | accept_status);
 }
-#endif	/* WAKE_SECONDARY_VIA_NMI */
 
-#ifdef WAKE_SECONDARY_VIA_INIT
 static int __devinit
-wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
+wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 {
 	unsigned long send_status, accept_status = 0;
 	int maxlvt, num_starts, j;
@@ -737,7 +735,15 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 
 	return (send_status | accept_status);
 }
-#endif	/* WAKE_SECONDARY_VIA_INIT */
+
+static int __devinit
+wakeup_secondary_cpu(int apicid, unsigned long start_eip)
+{
+	if (x86_quirks->wakeup_secondary_cpu)
+		return x86_quirks->wakeup_secondary_cpu(apicid, start_eip);
+
+	return wakeup_secondary_cpu_via_init(apicid, start_eip);
+}
 
 struct create_idle {
 	struct work_struct work;

commit db96b0a0e4158806fcf03945a870f9320e6bac9b
Author: Cyrill Gorcunov <gorcunov@gmail.com>
Date:   Wed Oct 22 18:00:09 2008 +0400

    x86: do_boot_cpu - check if we have ESR register
    
    Impact: fix APIC IRQ irregularities on certain older boxes
    
    We should touch the APIC ESR register if only we have it.
    
    The patch fixes the problem mentioned by Max Kellermann:
    
            http://lkml.org/lkml/2008/10/17/147
    
    Bisected-by: Max Kellermann <mk@cm4all.com>
    Signed-off-by: Cyrill Gorcunov <gorcunov@gmail.com>
    [ mingo@elte.hu: build fix ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7ece815ea637..7b1093397319 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -893,9 +893,11 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		smpboot_setup_warm_reset_vector(start_ip);
 		/*
 		 * Be paranoid about clearing APIC errors.
-	 	*/
-		apic_write(APIC_ESR, 0);
-		apic_read(APIC_ESR);
+		*/
+		if (APIC_INTEGRATED(apic_version[boot_cpu_physical_apicid])) {
+			apic_write(APIC_ESR, 0);
+			apic_read(APIC_ESR);
+		}
 	}
 
 	/*

commit 9301975ec251bab1ad7cfcb84a688b26187e4e4a
Merge: 7110879cf2af dd3a1db900f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 20 13:22:50 2008 -0700

    Merge branch 'genirq-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    This merges branches irq/genirq, irq/sparseirq-v4, timers/hpet-percpu
    and x86/uv.
    
    The sparseirq branch is just preliminary groundwork: no sparse IRQs are
    actually implemented by this tree anymore - just the new APIs are added
    while keeping the old way intact as well (the new APIs map 1:1 to
    irq_desc[]).  The 'real' sparse IRQ support will then be a relatively
    small patch ontop of this - with a v2.6.29 merge target.
    
    * 'genirq-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (178 commits)
      genirq: improve include files
      intr_remapping: fix typo
      io_apic: make irq_mis_count available on 64-bit too
      genirq: fix name space collisions of nr_irqs in arch/*
      genirq: fix name space collision of nr_irqs in autoprobe.c
      genirq: use iterators for irq_desc loops
      proc: fixup irq iterator
      genirq: add reverse iterator for irq_desc
      x86: move ack_bad_irq() to irq.c
      x86: unify show_interrupts() and proc helpers
      x86: cleanup show_interrupts
      genirq: cleanup the sparseirq modifications
      genirq: remove artifacts from sparseirq removal
      genirq: revert dynarray
      genirq: remove irq_to_desc_alloc
      genirq: remove sparse irq code
      genirq: use inline function for irq_to_desc
      genirq: consolidate nr_irqs and for_each_irq_desc()
      x86: remove sparse irq from Kconfig
      genirq: define nr_irqs for architectures with GENERIC_HARDIRQS=n
      ...

commit 25ddbb18aae33ad255eb9f35aacebe3af01e1e9c
Author: Andi Kleen <andi@firstfloor.org>
Date:   Wed Oct 15 22:01:41 2008 -0700

    Make the taint flags reliable
    
    It's somewhat unlikely that it happens, but right now a race window
    between interrupts or machine checks or oopses could corrupt the tainted
    bitmap because it is modified in a non atomic fashion.
    
    Convert the taint variable to an unsigned long and use only atomic bit
    operations on it.
    
    Unfortunately this means the intvec sysctl functions cannot be used on it
    anymore.
    
    It turned out the taint sysctl handler could actually be simplified a bit
    (since it only increases capabilities) so this patch actually removes
    code.
    
    [akpm@linux-foundation.org: remove unneeded include]
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8c3aca7cb343..7ed9e070a6e9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -282,6 +282,8 @@ static void __cpuinit smp_callin(void)
 	cpu_set(cpuid, cpu_callin_map);
 }
 
+static int __cpuinitdata unsafe_smp;
+
 /*
  * Activate a secondary processor.
  */
@@ -397,7 +399,7 @@ static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
 				goto valid_k7;
 
 		/* If we get here, not a certified SMP capable AMD system. */
-		add_taint(TAINT_UNSAFE_SMP);
+		unsafe_smp = 1;
 	}
 
 valid_k7:
@@ -414,12 +416,10 @@ static void __cpuinit smp_checks(void)
 	 * Don't taint if we are running SMP kernel on a single non-MP
 	 * approved Athlon
 	 */
-	if (tainted & TAINT_UNSAFE_SMP) {
-		if (num_online_cpus())
-			printk(KERN_INFO "WARNING: This combination of AMD"
-				"processors is not suitable for SMP.\n");
-		else
-			tainted &= ~TAINT_UNSAFE_SMP;
+	if (unsafe_smp && num_online_cpus() > 1) {
+		printk(KERN_INFO "WARNING: This combination of AMD"
+			"processors is not suitable for SMP.\n");
+		add_taint(TAINT_UNSAFE_SMP);
 	}
 }
 

commit 823b259b80158a5fb694f6784e18b5bae669c599
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Wed Sep 10 21:56:46 2008 -0700

    x86: print out apic id in hex format
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8c3aca7cb343..f84de2ff933c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -543,10 +543,10 @@ static inline void __inquire_remote_apic(int apicid)
 	int timeout;
 	u32 status;
 
-	printk(KERN_INFO "Inquiring remote APIC #%d...\n", apicid);
+	printk(KERN_INFO "Inquiring remote APIC 0x%x...\n", apicid);
 
 	for (i = 0; i < ARRAY_SIZE(regs); i++) {
-		printk(KERN_INFO "... APIC #%d %s: ", apicid, names[i]);
+		printk(KERN_INFO "... APIC 0x%x %s: ", apicid, names[i]);
 
 		/*
 		 * Wait for idle.
@@ -874,7 +874,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	start_ip = setup_trampoline();
 
 	/* So we see what's up   */
-	printk(KERN_INFO "Booting processor %d/%d ip %lx\n",
+	printk(KERN_INFO "Booting processor %d APIC 0x%x ip 0x%lx\n",
 			  cpu, apicid, start_ip);
 
 	/*

commit cb48bb59995d2d14a0c732835c80bbcfb354de31
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Oct 5 17:51:52 2008 +0200

    x86: remove additional_cpus
    
    remove remainder of additional_cpus logic. We now just listen to the
    disabled_cpus value like we did for years. disabled_cpus is always >=
    0 so no need for an extra check.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8dd201c31329..8c3aca7cb343 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1261,8 +1261,6 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 	check_nmi_watchdog();
 }
 
-static int additional_cpus = -1;
-
 /*
  * cpu_possible_map should be static, it cannot change as cpu's
  * are onlined, or offlined. The reason is per-cpu data-structures
@@ -1282,21 +1280,13 @@ static int additional_cpus = -1;
  */
 __init void prefill_possible_map(void)
 {
-	int i;
-	int possible;
+	int i, possible;
 
 	/* no processor from mptable or madt */
 	if (!num_processors)
 		num_processors = 1;
 
-	if (additional_cpus == -1) {
-		if (disabled_cpus > 0)
-			additional_cpus = disabled_cpus;
-		else
-			additional_cpus = 0;
-	}
-
-	possible = num_processors + additional_cpus;
+	possible = num_processors + disabled_cpus;
 	if (possible > NR_CPUS)
 		possible = NR_CPUS;
 

commit b807305059c28fb8197496c944bfaa6b372a40ad
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Oct 5 17:12:36 2008 +0200

    x86: remove additional_cpus configurability
    
    additional_cpus=<x> parameter is dangerous and broken: for example
    if we boot additional_cpus=-2 on a stock dual-core system it will
    crash the box on bootup.
    
    So reduce the maze of code a bit by removingthe user-configurability
    angle.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 857a88bb9195..8dd201c31329 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1261,7 +1261,7 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 	check_nmi_watchdog();
 }
 
-static int additional_cpus __initdata = CONFIG_HOTPLUG_ADDITIONAL_CPUS;
+static int additional_cpus = -1;
 
 /*
  * cpu_possible_map should be static, it cannot change as cpu's
@@ -1334,12 +1334,6 @@ static void remove_siblinginfo(int cpu)
 	cpu_clear(cpu, cpu_sibling_setup_map);
 }
 
-static __init int setup_additional_cpus(char *s)
-{
-	return s && get_option(&s, &additional_cpus) ? 0 : -EINVAL;
-}
-early_param("additional_cpus", setup_additional_cpus);
-
 static void __ref remove_cpu_from_maps(int cpu)
 {
 	cpu_clear(cpu, cpu_online_map);

commit 7f2f49a58283110083a7358d2d98025a11653373
Author: Chuck Ebbert <cebbert@redhat.com>
Date:   Thu Oct 2 15:30:07 2008 -0400

    x86: allow number of additional hotplug CPUs to be set at compile time, V2
    
    x86: allow number of additional hotplug CPUs to be set at compile time, V2
    
    The default number of additional CPU IDs for hotplugging is determined
    by asking ACPI or mptables how many "disabled" CPUs there are in the
    system, but many systems get this wrong so that e.g. a uniprocessor
    machine gets an extra CPU allocated and never switches to single CPU
    mode.
    
    And sometimes CPU hotplugging is enabled only for suspend/hibernate
    anyway, so the additional CPU IDs are not wanted. Allow the number
    to be set to zero at compile time.
    
    Also, force the number of extra CPUs to zero if hotplugging is disabled
    which allows removing some conditional code.
    
    Tested on uniprocessor x86_64 that ACPI claims has a disabled processor,
    with CPU hotplugging configured.
    
    ("After" has the number of additional CPUs set to 0)
    Before: NR_CPUS: 512, nr_cpu_ids: 2, nr_node_ids 1
    After: NR_CPUS: 512, nr_cpu_ids: 1, nr_node_ids 1
    
    [Changed the name of the option and the prompt according to Ingo's
     suggestion.]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 23913785c262..857a88bb9195 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1261,7 +1261,7 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 	check_nmi_watchdog();
 }
 
-static int additional_cpus __initdata = -1;
+static int additional_cpus __initdata = CONFIG_HOTPLUG_ADDITIONAL_CPUS;
 
 /*
  * cpu_possible_map should be static, it cannot change as cpu's

commit 14adf855baefad5ac3b545be23a64e6b61d6b74a
Author: Chuck Ebbert <cebbert@redhat.com>
Date:   Mon Sep 29 18:29:42 2008 -0400

    x86: move prefill_possible_map calling early, fix, V2
    
    Commit 4a701737 ("x86: move prefill_possible_map calling early, fix")
    is the wrong fix: prefill_possible_map() needs to be available
    even when CONFIG_HOTPLUG_CPU is not set. A followon patch will do that.
    
    Fix this correctly by making prefill_possible_map() available even when
    CONFIG_HOTPLUG_CPU is not set. The function is needed so that
    the number of possible CPUs can be determined.
    
    Tested on uniprocessor machine with CPU hotplug disabled.
    
    From boot log:
      Before: NR_CPUS: 512, nr_cpu_ids: 512, nr_node_ids 1
      After: NR_CPUS: 512, nr_cpu_ids: 1, nr_node_ids 1
    
    Signed-off-by: Chuck Ebbert <cebbert@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a778e221ccfb..23913785c262 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1261,39 +1261,8 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 	check_nmi_watchdog();
 }
 
-#ifdef CONFIG_HOTPLUG_CPU
-
-static void remove_siblinginfo(int cpu)
-{
-	int sibling;
-	struct cpuinfo_x86 *c = &cpu_data(cpu);
-
-	for_each_cpu_mask_nr(sibling, per_cpu(cpu_core_map, cpu)) {
-		cpu_clear(cpu, per_cpu(cpu_core_map, sibling));
-		/*/
-		 * last thread sibling in this cpu core going down
-		 */
-		if (cpus_weight(per_cpu(cpu_sibling_map, cpu)) == 1)
-			cpu_data(sibling).booted_cores--;
-	}
-
-	for_each_cpu_mask_nr(sibling, per_cpu(cpu_sibling_map, cpu))
-		cpu_clear(cpu, per_cpu(cpu_sibling_map, sibling));
-	cpus_clear(per_cpu(cpu_sibling_map, cpu));
-	cpus_clear(per_cpu(cpu_core_map, cpu));
-	c->phys_proc_id = 0;
-	c->cpu_core_id = 0;
-	cpu_clear(cpu, cpu_sibling_setup_map);
-}
-
 static int additional_cpus __initdata = -1;
 
-static __init int setup_additional_cpus(char *s)
-{
-	return s && get_option(&s, &additional_cpus) ? 0 : -EINVAL;
-}
-early_param("additional_cpus", setup_additional_cpus);
-
 /*
  * cpu_possible_map should be static, it cannot change as cpu's
  * are onlined, or offlined. The reason is per-cpu data-structures
@@ -1340,6 +1309,37 @@ __init void prefill_possible_map(void)
 	nr_cpu_ids = possible;
 }
 
+#ifdef CONFIG_HOTPLUG_CPU
+
+static void remove_siblinginfo(int cpu)
+{
+	int sibling;
+	struct cpuinfo_x86 *c = &cpu_data(cpu);
+
+	for_each_cpu_mask_nr(sibling, per_cpu(cpu_core_map, cpu)) {
+		cpu_clear(cpu, per_cpu(cpu_core_map, sibling));
+		/*/
+		 * last thread sibling in this cpu core going down
+		 */
+		if (cpus_weight(per_cpu(cpu_sibling_map, cpu)) == 1)
+			cpu_data(sibling).booted_cores--;
+	}
+
+	for_each_cpu_mask_nr(sibling, per_cpu(cpu_sibling_map, cpu))
+		cpu_clear(cpu, per_cpu(cpu_sibling_map, sibling));
+	cpus_clear(per_cpu(cpu_sibling_map, cpu));
+	cpus_clear(per_cpu(cpu_core_map, cpu));
+	c->phys_proc_id = 0;
+	c->cpu_core_id = 0;
+	cpu_clear(cpu, cpu_sibling_setup_map);
+}
+
+static __init int setup_additional_cpus(char *s)
+{
+	return s && get_option(&s, &additional_cpus) ? 0 : -EINVAL;
+}
+early_param("additional_cpus", setup_additional_cpus);
+
 static void __ref remove_cpu_from_maps(int cpu)
 {
 	cpu_clear(cpu, cpu_online_map);

commit 59ef48a58e59cc27255d526ae3fa60ddcd977208
Author: Cyrill Gorcunov <gorcunov@gmail.com>
Date:   Sun Sep 14 21:58:49 2008 +0400

    x86: smpboot - check if we have ESR register in wakeup_secondary_cpu
    
    We should check if we have ESR register before reading from it.
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: "Maciej W. Rozycki" <macro@linux-mips.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b700c9a10644..a778e221ccfb 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -599,10 +599,12 @@ wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
 	 * Give the other CPU some time to accept the IPI.
 	 */
 	udelay(200);
-	maxlvt = lapic_get_maxlvt();
-	if (maxlvt > 3)			/* Due to the Pentium erratum 3AP.  */
-		apic_write(APIC_ESR, 0);
-	accept_status = (apic_read(APIC_ESR) & 0xEF);
+	if (APIC_INTEGRATED(apic_version[phys_apicid])) {
+		maxlvt = lapic_get_maxlvt();
+		if (maxlvt > 3)			/* Due to the Pentium erratum 3AP.  */
+			apic_write(APIC_ESR, 0);
+		accept_status = (apic_read(APIC_ESR) & 0xEF);
+	}
 	pr_debug("NMI sent.\n");
 
 	if (send_status)

commit 0cefa5b9b0a61b62442c5d0ca00a304c5896b6e9
Author: Manfred Spraul <manfred@colorfullife.com>
Date:   Sun Sep 7 11:29:58 2008 +0200

    arch/x86/kernel/smpboot.c: Clarify when irq processing begins.
    
    Secondary cpus start with local interrupts disabled.
    start_secondary() first initializes the new cpu, then it enables the
    local interrupts. (although interrupts are enabled within smp_callin()
    as well).
    
    Right now, the local interrupts are enabled as a side effect of calling
    ipi_call_lock_irq().
    
    The attached patch clarifies when local interrupts are enabled.
    
    Signed-off-by: Manfred Spraul <manfred@colorfullife.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 76b6f50978f7..b700c9a10644 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -334,14 +334,17 @@ static void __cpuinit start_secondary(void *unused)
 	 * does not change while we are assigning vectors to cpus.  Holding
 	 * this lock ensures we don't half assign or remove an irq from a cpu.
 	 */
-	ipi_call_lock_irq();
+	ipi_call_lock();
 	lock_vector_lock();
 	__setup_vector_irq(smp_processor_id());
 	cpu_set(smp_processor_id(), cpu_online_map);
 	unlock_vector_lock();
-	ipi_call_unlock_irq();
+	ipi_call_unlock();
 	per_cpu(cpu_state, smp_processor_id()) = CPU_ONLINE;
 
+	/* enable local interrupts */
+	local_irq_enable();
+
 	setup_secondary_clock();
 
 	wmb();

commit 365d46dc9be9b3c833990a06f3994b1987eda578
Merge: 5dc64a3442b9 fd0480883066
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Oct 12 12:35:23 2008 +0200

    Merge branch 'linus' into x86/xen
    
    Conflicts:
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/process_64.c
            arch/x86/xen/enlighten.c

commit ead9d23d803ea3a73766c3cb27bf7563ac8d7266
Merge: bf6f51e3a46f 0afe2db21394
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 11 11:47:30 2008 -0700

    Merge phase #4 (X2APIC, APIC unification, CPU identification unification) of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-v28-for-linus-phase4-D' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (186 commits)
      x86, debug: print more information about unknown CPUs
      x86 setup: handle more than 8 CPU flag words
      x86: cpuid, fix typo
      x86: move transmeta cap read to early_init_transmeta()
      x86: identify_cpu_without_cpuid v2
      x86: extended "flags" to show virtualization HW feature in /proc/cpuinfo
      x86: move VMX MSRs to msr-index.h
      x86: centaur_64.c remove duplicated setting of CONSTANT_TSC
      x86: intel.c put workaround for old cpus together
      x86: let intel 64-bit use intel.c
      x86: make intel_64.c the same as intel.c
      x86: make intel.c have 64-bit support code
      x86: little clean up of intel.c/intel_64.c
      x86: make 64 bit to use amd.c
      x86: make amd_64 have 32 bit code
      x86: make amd.c have 64bit support code
      x86: merge header in amd_64.c
      x86: add srat_detect_node for amd64
      x86: remove duplicated force_mwait
      x86: cpu make amd.c more like amd_64.c v2
      ...

commit d84705969f898f294bc3fc32eca33580f14105bd
Merge: 725c25819e4a 11494547b175
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Oct 10 19:50:00 2008 +0200

    Merge branch 'x86/apic' into x86-v28-for-linus-phase4-B
    
    Conflicts:
            arch/x86/kernel/apic_32.c
            arch/x86/kernel/apic_64.c
            arch/x86/kernel/setup.c
            drivers/pci/intel-iommu.c
            include/asm-x86/cpufeature.h
            include/asm-x86/dma-mapping.h

commit b11ce8a26d26ed9019a8803aa90d580b52f23e79
Merge: f6bccf695431 a5d8c3483a6e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 10 12:42:31 2008 -0700

    Merge branch 'sched-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (38 commits)
      sched debug: add name to sched_domain sysctl entries
      sched: sync wakeups vs avg_overlap
      sched: remove redundant code in cpu_cgroup_create()
      sched_rt.c: resch needed in rt_rq_enqueue() for the root rt_rq
      cpusets: scan_for_empty_cpusets(), cpuset doesn't seem to be so const
      sched: minor optimizations in wake_affine and select_task_rq_fair
      sched: maintain only task entities in cfs_rq->tasks list
      sched: fixup buddy selection
      sched: more sanity checks on the bandwidth settings
      sched: add some comments to the bandwidth code
      sched: fixlet for group load balance
      sched: rework wakeup preemption
      CFS scheduler: documentation about scheduling policies
      sched: clarify ifdef tangle
      sched: fix list traversal to use _rcu variant
      sched: turn off WAKEUP_OVERLAP
      sched: wakeup preempt when small overlap
      kernel/cpu.c: create a CPU_STARTING cpu_chain notifier
      kernel/cpu.c: Move the CPU_DYING notifiers
      sched: fix __load_balance_iterator() for cfq with only one task
      ...

commit e496e3d645c93206faf61ff6005995ebd08cc39c
Merge: b159d7a989e5 5bbd4c372400 175e438f7a2d 516cbf3730c4 af2d237bf574 9b1568458a3e 5b7e41ff3726 1befdefcf476 a03352d2c1dc 7b22ff5344fd 2c7e9fd4c6cb 91030ca1e739 dd5523552c28 b3e15bdef689 20211e4d3447 efd327a2d412 c7ffa6c26277 e51a1ac2dfca 5df455155124 d99e90164e6c e621bd18958e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 6 18:17:07 2008 +0200

    Merge branches 'x86/alternatives', 'x86/cleanups', 'x86/commandline', 'x86/crashdump', 'x86/debug', 'x86/defconfig', 'x86/doc', 'x86/exports', 'x86/fpu', 'x86/gart', 'x86/idle', 'x86/mm', 'x86/mtrr', 'x86/nmi-watchdog', 'x86/oprofile', 'x86/paravirt', 'x86/reboot', 'x86/sparse-fixes', 'x86/tsc', 'x86/urgent' and 'x86/vmalloc' into x86-v28-for-linus-phase1

commit 0962f402af1bb0b53ccee626785d202a10c12fff
Merge: 19268ed7449c 8d7ccaa54549
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 6 16:18:26 2008 +0200

    Merge branch 'x86/prototypes' into x86-v28-for-linus-phase1
    
    Conflicts:
            arch/x86/kernel/process_32.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 07bbc16a8676b06950a21f35b59f69b2fe763bbd
Merge: 6a9e91846bf5 f8e256c687eb
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Sep 23 23:26:42 2008 +0200

    Merge branch 'timers/urgent' into x86/xen
    
    Conflicts:
            arch/x86/kernel/process_32.c
            arch/x86/kernel/process_64.c
    
    Manual merge:
    
            arch/x86/kernel/smpboot.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e545a6140b698b2494daf0b32107bdcc5e901390
Author: Manfred Spraul <manfred@colorfullife.com>
Date:   Sun Sep 7 16:57:22 2008 +0200

    kernel/cpu.c: create a CPU_STARTING cpu_chain notifier
    
    Right now, there is no notifier that is called on a new cpu, before the new
    cpu begins processing interrupts/softirqs.
    Various kernel function would need that notification, e.g. kvm works around
    by calling smp_call_function_single(), rcu polls cpu_online_map.
    
    The patch adds a CPU_STARTING notification. It also adds a helper function
    that sends the message to all cpu_chain handlers.
    
    Tested on x86-64.
    All other archs are untested. Especially on sparc, I'm not sure if I got
    it right.
    
    Signed-off-by: Manfred Spraul <manfred@colorfullife.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7985c5b3f916..0b8261c3cac2 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -257,6 +257,7 @@ static void __cpuinit smp_callin(void)
 	end_local_APIC_setup();
 	map_cpu_to_logical_apicid();
 
+	notify_cpu_starting(cpuid);
 	/*
 	 * Get our bogomips.
 	 *

commit ea1c9de45ecb162841c9b4e0fa303a245d59b1c8
Merge: 4e1d112cac08 a2bd7274b471
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Aug 25 11:10:42 2008 +0200

    Merge branch 'x86/urgent' into x86/cleanups

commit 8227dce7dc2cfdcc28ee0eadfb482a7ee77fba03
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Fri Aug 22 11:52:14 2008 +0100

    x86: separate generic cpu disabling code from APIC writes in cpu_disable
    
    It allows paravirt implementations of cpu_disable to share the
    cpu_disable_common code, without having to take on board APIC
    writes, which may not be appropriate.
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 28b4287296aa..66b04e598817 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1346,25 +1346,9 @@ static void __ref remove_cpu_from_maps(int cpu)
 	numa_remove_cpu(cpu);
 }
 
-int native_cpu_disable(void)
+void cpu_disable_common(void)
 {
 	int cpu = smp_processor_id();
-
-	/*
-	 * Perhaps use cpufreq to drop frequency, but that could go
-	 * into generic code.
-	 *
-	 * We won't take down the boot processor on i386 due to some
-	 * interrupts only being able to be serviced by the BSP.
-	 * Especially so if we're not using an IOAPIC	-zwane
-	 */
-	if (cpu == 0)
-		return -EBUSY;
-
-	if (nmi_watchdog == NMI_LOCAL_APIC)
-		stop_apic_nmi_watchdog(NULL);
-	clear_local_APIC();
-
 	/*
 	 * HACK:
 	 * Allow any queued timer interrupts to get serviced
@@ -1382,6 +1366,28 @@ int native_cpu_disable(void)
 	remove_cpu_from_maps(cpu);
 	unlock_vector_lock();
 	fixup_irqs(cpu_online_map);
+}
+
+int native_cpu_disable(void)
+{
+	int cpu = smp_processor_id();
+
+	/*
+	 * Perhaps use cpufreq to drop frequency, but that could go
+	 * into generic code.
+	 *
+	 * We won't take down the boot processor on i386 due to some
+	 * interrupts only being able to be serviced by the BSP.
+	 * Especially so if we're not using an IOAPIC	-zwane
+	 */
+	if (cpu == 0)
+		return -EBUSY;
+
+	if (nmi_watchdog == NMI_LOCAL_APIC)
+		stop_apic_nmi_watchdog(NULL);
+	clear_local_APIC();
+
+	cpu_disable_common();
 	return 0;
 }
 

commit a21f5d88c17a40941f6239d1959d89e8493e8e01
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Fri Aug 22 11:52:13 2008 +0100

    x86: unify x86_32 and x86_64 play_dead into one function
    
    Add the new play_dead into smpboot.c, as it fits more cleanly in there
    alongside other CONFIG_HOTPLUG functions.
    
    Separate out the common code into its own function.
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c414cee296ba..28b4287296aa 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1402,6 +1402,29 @@ void native_cpu_die(unsigned int cpu)
 	}
 	printk(KERN_ERR "CPU %u didn't die...\n", cpu);
 }
+
+void play_dead_common(void)
+{
+	idle_task_exit();
+	reset_lazy_tlbstate();
+	irq_ctx_exit(raw_smp_processor_id());
+
+	mb();
+	/* Ack it */
+	__get_cpu_var(cpu_state) = CPU_DEAD;
+
+	/*
+	 * With physical CPU hotplug, we should halt the cpu
+	 */
+	local_irq_disable();
+}
+
+void native_play_dead(void)
+{
+	play_dead_common();
+	wbinvd_halt();
+}
+
 #else /* ... !CONFIG_HOTPLUG_CPU */
 int native_cpu_disable(void)
 {
@@ -1413,4 +1436,10 @@ void native_cpu_die(unsigned int cpu)
 	/* We said "no" in __cpu_disable */
 	BUG();
 }
+
+void native_play_dead(void)
+{
+	BUG();
+}
+
 #endif

commit 93be71b672f167b1e8c23725114f86305354f0ac
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Fri Aug 22 11:52:11 2008 +0100

    x86: add cpu hotplug hooks into smp_ops
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7985c5b3f916..c414cee296ba 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1346,7 +1346,7 @@ static void __ref remove_cpu_from_maps(int cpu)
 	numa_remove_cpu(cpu);
 }
 
-int __cpu_disable(void)
+int native_cpu_disable(void)
 {
 	int cpu = smp_processor_id();
 
@@ -1385,7 +1385,7 @@ int __cpu_disable(void)
 	return 0;
 }
 
-void __cpu_die(unsigned int cpu)
+void native_cpu_die(unsigned int cpu)
 {
 	/* We don't do anything here: idle task is faking death itself. */
 	unsigned int i;
@@ -1403,12 +1403,12 @@ void __cpu_die(unsigned int cpu)
 	printk(KERN_ERR "CPU %u didn't die...\n", cpu);
 }
 #else /* ... !CONFIG_HOTPLUG_CPU */
-int __cpu_disable(void)
+int native_cpu_disable(void)
 {
 	return -ENOSYS;
 }
 
-void __cpu_die(unsigned int cpu)
+void native_cpu_die(unsigned int cpu)
 {
 	/* We said "no" in __cpu_disable */
 	BUG();

commit c4bd1fdab0deec0f69aeabab22075cb22ac8ad44
Author: Marcin Slusarz <marcin.slusarz@gmail.com>
Date:   Thu Aug 21 20:49:05 2008 +0200

    x86: fix section mismatch warning - uv_cpu_init
    
    WARNING: vmlinux.o(.cpuinit.text+0x3cc4): Section mismatch in reference from the function uv_cpu_init() to the function .init.text:uv_system_init()
    The function __cpuinit uv_cpu_init() references
    a function __init uv_system_init().
    If uv_system_init is only used by uv_cpu_init then
    annotate uv_system_init with a matching annotation.
    
    uv_system_init was ment to be called only once, so do it from codepath
    (native_smp_prepare_cpus) which is called once, right before activation
    of other cpus (smp_init).
    
    Note: old code relied on uv_node_to_blade being initialized to 0,
    but it'a not initialized from anywhere.
    
    Signed-off-by: Marcin Slusarz <marcin.slusarz@gmail.com>
    Acked-by: Jack Steiner <steiner@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e139e617f422..7985c5b3f916 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1221,6 +1221,9 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	printk(KERN_INFO "CPU%d: ", 0);
 	print_cpu_info(&cpu_data(0));
 	setup_boot_clock();
+
+	if (is_uv_system())
+		uv_system_init();
 out:
 	preempt_enable();
 }

commit 7393423dd9b5790a3115873be355e9fc862bce8f
Merge: 8df9676d6402 1fca25427482
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Aug 20 11:52:15 2008 +0200

    Merge branch 'linus' into x86/cleanups

commit d19fbfdfe6a7034c8b6a7062365780485ab5aeaa
Author: Marcin Slusarz <marcin.slusarz@gmail.com>
Date:   Sun Aug 17 17:50:51 2008 +0200

    x86: silence section mismatch warning - get_local_pda
    
    Take out part of get_local_pda referencing __init function (free_bootmem)
    to new (static) function marked as __ref. It's safe to do because free_bootmem
    is called before __init sections are dropped.
    
    WARNING: vmlinux.o(.cpuinit.text+0x3cd7): Section mismatch in reference from the function get_local_pda() to the function .init.text:free_bootmem()
    The function __cpuinit get_local_pda() references
    a function __init free_bootmem().
    If free_bootmem is only used by get_local_pda then
    annotate free_bootmem with a matching annotation.
    
    Signed-off-by: Marcin Slusarz <marcin.slusarz@gmail.com>
    Cc: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a8fb8a980fae..e139e617f422 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -756,6 +756,14 @@ static void __cpuinit do_fork_idle(struct work_struct *work)
 }
 
 #ifdef CONFIG_X86_64
+
+/* __ref because it's safe to call free_bootmem when after_bootmem == 0. */
+static void __ref free_bootmem_pda(struct x8664_pda *oldpda)
+{
+	if (!after_bootmem)
+		free_bootmem((unsigned long)oldpda, sizeof(*oldpda));
+}
+
 /*
  * Allocate node local memory for the AP pda.
  *
@@ -784,8 +792,7 @@ int __cpuinit get_local_pda(int cpu)
 
 	if (oldpda) {
 		memcpy(newpda, oldpda, size);
-		if (!after_bootmem)
-			free_bootmem((unsigned long)oldpda, size);
+		free_bootmem_pda(oldpda);
 	}
 
 	newpda->in_bootmem = 0;

commit 2bd455dbfebfd632a8dcf1d3d1612737986fde0a
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon Aug 4 11:26:38 2008 +0800

    x86: remove nesting CONFIG_HOTPLUG_CPU
    
    prefill_possible_map() is defined inside CONFIG_HOTPLUG_CPU,
    so the nesting CONFIG_HOTPLUG_CPU is just redundant.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 332512767f4f..d5e19c362046 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1271,16 +1271,13 @@ __init void prefill_possible_map(void)
 	if (!num_processors)
 		num_processors = 1;
 
-#ifdef CONFIG_HOTPLUG_CPU
 	if (additional_cpus == -1) {
 		if (disabled_cpus > 0)
 			additional_cpus = disabled_cpus;
 		else
 			additional_cpus = 0;
 	}
-#else
-	additional_cpus = 0;
-#endif
+
 	possible = num_processors + additional_cpus;
 	if (possible > NR_CPUS)
 		possible = NR_CPUS;

commit c83d12806b6185131ece682de8696d8cfb78df69
Merge: 51ca3c679194 8d7ccaa54549 8067794bec1c a677f58a8c8c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 14 14:58:22 2008 +0200

    Merge branches 'x86/prototypes', 'x86/x2apic' and 'x86/debug' into x86/core

commit 51ca3c679194e7435c25b8e77b0a73c597e41ae9
Merge: b55793f7528c b635acec48bc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 14 14:58:01 2008 +0200

    Merge branch 'linus' into x86/core
    
    Conflicts:
            arch/x86/kernel/genapic_64.c
            include/asm-x86/kvm_host.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 8d7ccaa545490cdffdfaff0842436a8dd85cf47b
Merge: b2139aa0eec3 30a2f3c60a84
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 14 12:19:59 2008 +0200

    Merge commit 'v2.6.27-rc3' into x86/prototypes
    
    Conflicts:
    
            include/asm-x86/dma-mapping.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a58f03b07539f6575adaa011712fa139c9343742
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Aug 14 02:16:30 2008 -0700

    x86: check bigsmp in smp_sanity_check instead of cpu_up
    
    clear bits for cpu nr > 8.
    
    This allows us to boot the full range of possible CPUs that the
    supported APIC model will allow. Previously we'd hang or boot up
    with less than 8 CPUs.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Tested-by: Jeff Chua <jeff.chua.linux@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e25287e4a85f..a8fb8a980fae 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -994,17 +994,7 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 	flush_tlb_all();
 	low_mappings = 1;
 
-#ifdef CONFIG_X86_PC
-	if (def_to_bigsmp && apicid > 8) {
-		printk(KERN_WARNING
-			"More than 8 CPUs detected - skipping them.\n"
-			"Use CONFIG_X86_GENERICARCH and CONFIG_X86_BIGSMP.\n");
-		err = -1;
-	} else
-		err = do_boot_cpu(apicid, cpu);
-#else
 	err = do_boot_cpu(apicid, cpu);
-#endif
 
 	zap_low_mappings();
 	low_mappings = 0;
@@ -1058,6 +1048,34 @@ static __init void disable_smp(void)
 static int __init smp_sanity_check(unsigned max_cpus)
 {
 	preempt_disable();
+
+#if defined(CONFIG_X86_PC) && defined(CONFIG_X86_32)
+	if (def_to_bigsmp && nr_cpu_ids > 8) {
+		unsigned int cpu;
+		unsigned nr;
+
+		printk(KERN_WARNING
+		       "More than 8 CPUs detected - skipping them.\n"
+		       "Use CONFIG_X86_GENERICARCH and CONFIG_X86_BIGSMP.\n");
+
+		nr = 0;
+		for_each_present_cpu(cpu) {
+			if (nr >= 8)
+				cpu_clear(cpu, cpu_present_map);
+			nr++;
+		}
+
+		nr = 0;
+		for_each_possible_cpu(cpu) {
+			if (nr >= 8)
+				cpu_clear(cpu, cpu_possible_map);
+			nr++;
+		}
+
+		nr_cpu_ids = 8;
+	}
+#endif
+
 	if (!physid_isset(hard_smp_processor_id(), phys_cpu_present_map)) {
 		printk(KERN_WARNING "weird, boot CPU (#%d) not listed"
 				    "by the BIOS.\n", hard_smp_processor_id());

commit 23b49c19f6946cc33392a1fc75dd788dd4a90fb7
Author: Max Krasnyansky <maxk@qualcomm.com>
Date:   Mon Aug 11 14:55:31 2008 -0700

    x86: resurrect proper handling of maxcpus= kernel option (v2)
    
    For some reason we had two parsers registered for maxcpus=. One in init/main.c
    and another in arch/x86/smpboot.c. So I nuked the one in arch/x86.
    
    Also 64-bit kernels used to handle maxcpus= as documented in
    Documentation/cpu-hotplug.txt. CPUs with 'id > maxcpus' are initialized
    but not booted. 32-bit version for some reason ignored them even though
    all the infrastructure for booting them later is there.
    
    In the current mainline both 64 and 32 bit versions are broken.
    This patch restores the correct behaviour. I've tested x86_64 version on
    4- and 8- way Core2 and 2-way Opteron based machines. Various config
    combinations SMP, !SMP, CPU_HOTPLUG, !CPU_HOTPLUG.
    Booted with maxcpus=1 and maxcpus=4, etc. Everything is working as expected.
    
    So far we've received two reports from different people confirming that 32-bit
    version also works fine, both on dual core laptops and 16way server machines.
    
    [v2: This version fixes visws breakage pointed out by Ingo.]
    
    Signed-off-by: Max Krasnyansky <maxk@qualcomm.com>
    Cc: lizf@cn.fujitsu.com
    Cc: jeff.chua.linux@gmail.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 91055d7fc1b0..e25287e4a85f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1386,17 +1386,3 @@ void __cpu_die(unsigned int cpu)
 	BUG();
 }
 #endif
-
-/*
- * If the BIOS enumerates physical processors before logical,
- * maxcpus=N at enumeration-time can be used to disable HT.
- */
-static int __init parse_maxcpus(char *arg)
-{
-	extern unsigned int maxcpus;
-
-	if (arg)
-		maxcpus = simple_strtoul(arg, NULL, 0);
-	return 0;
-}
-early_param("maxcpus", parse_maxcpus);

commit b74548e76a0eab1f29546e7c5a589429c069a680
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Mon Aug 11 13:36:04 2008 -0700

    x86: fix 2.6.27rc1 cannot boot more than 8CPUs
    
    Jeff Chua reported that booting a !bigsmp kernel on a 16-way box
    hangs silently.
    
    this is a long-standing issue, smp start AP cpu could check the
    apic id >=8 etc before trying to start it.
    
    achieve this by moving the def_to_bigsmp check later and skip the
    apicid id > 8
    
    [ mingo@elte.hu: clean up the message that is printed. ]
    
    Reported-by: "Jeff Chua" <jeff.chua.linux@gmail.com>
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    
     arch/x86/kernel/setup.c   |    6 ------
     arch/x86/kernel/smpboot.c |   10 ++++++++++
     2 files changed, 10 insertions(+), 6 deletions(-)

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index da10f07fc59c..91055d7fc1b0 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -994,7 +994,17 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 	flush_tlb_all();
 	low_mappings = 1;
 
+#ifdef CONFIG_X86_PC
+	if (def_to_bigsmp && apicid > 8) {
+		printk(KERN_WARNING
+			"More than 8 CPUs detected - skipping them.\n"
+			"Use CONFIG_X86_GENERICARCH and CONFIG_X86_BIGSMP.\n");
+		err = -1;
+	} else
+		err = do_boot_cpu(apicid, cpu);
+#else
 	err = do_boot_cpu(apicid, cpu);
+#endif
 
 	zap_low_mappings();
 	low_mappings = 0;

commit d388e5fdc461344d04307a3fa83862b9ed429647
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Aug 9 15:09:02 2008 -0700

    x86: Restore proper vector locking during cpu hotplug
    
    Having cpu_online_map change during assign_irq_vector can result
    in some really nasty and weird things happening.  The one that
    bit me last time was accessing non existent per cpu memory for non
    existent cpus.
    
    This locking was removed in a sloppy x86_64 and x86_32 merge patch.
    
    Guys can we please try and avoid subtly breaking x86 when we are
    merging files together?
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 332512767f4f..da10f07fc59c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -326,12 +326,16 @@ static void __cpuinit start_secondary(void *unused)
 	 * for which cpus receive the IPI. Holding this
 	 * lock helps us to not include this cpu in a currently in progress
 	 * smp_call_function().
+	 *
+	 * We need to hold vector_lock so there the set of online cpus
+	 * does not change while we are assigning vectors to cpus.  Holding
+	 * this lock ensures we don't half assign or remove an irq from a cpu.
 	 */
 	ipi_call_lock_irq();
-#ifdef CONFIG_X86_IO_APIC
-	setup_vector_irq(smp_processor_id());
-#endif
+	lock_vector_lock();
+	__setup_vector_irq(smp_processor_id());
 	cpu_set(smp_processor_id(), cpu_online_map);
+	unlock_vector_lock();
 	ipi_call_unlock_irq();
 	per_cpu(cpu_state, smp_processor_id()) = CPU_ONLINE;
 
@@ -1336,7 +1340,9 @@ int __cpu_disable(void)
 	remove_siblinginfo(cpu);
 
 	/* It's now safe to remove this processor from the online map */
+	lock_vector_lock();
 	remove_cpu_from_maps(cpu);
+	unlock_vector_lock();
 	fixup_irqs(cpu_online_map);
 	return 0;
 }

commit 10a010f6953b5a14ba2f0be40a4fce1bea220875
Merge: 510b37258dfd fb2e405fc1fc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 25 13:08:16 2008 +0200

    Merge branch 'linus' into x86/x2apic
    
    Conflicts:
    
            drivers/pci/dmar.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f86c99853b22576ee8dc4fa27ff6f3c0c7ce0ef8
Author: Jaswinder Singh <jaswinder@infradead.org>
Date:   Fri Jul 25 10:52:53 2008 +0530

    X86_SMP: smpboot.c declare idle_thread_array and smp_b_stepping as static
    
    Signed-off-by: Jaswinder Singh <jaswinder@infradead.org>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 4b53a647bc0a..a9331a42f76f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -88,7 +88,7 @@ static DEFINE_PER_CPU(struct task_struct *, idle_thread_array);
 #define get_idle_for_cpu(x)      (per_cpu(idle_thread_array, x))
 #define set_idle_for_cpu(x, p)   (per_cpu(idle_thread_array, x) = (p))
 #else
-struct task_struct *idle_thread_array[NR_CPUS] __cpuinitdata ;
+static struct task_struct *idle_thread_array[NR_CPUS] __cpuinitdata ;
 #define get_idle_for_cpu(x)      (idle_thread_array[(x)])
 #define set_idle_for_cpu(x, p)   (idle_thread_array[(x)] = (p))
 #endif
@@ -129,7 +129,7 @@ static int boot_cpu_logical_apicid;
 static cpumask_t cpu_sibling_setup_map;
 
 /* Set if we find a B stepping CPU */
-int __cpuinitdata smp_b_stepping;
+static int __cpuinitdata smp_b_stepping;
 
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_32)
 

commit 26dcce0fabbef75ae426461edf21b5030bad60f3
Merge: d7b6de14a0ef eb6a12c2428d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 23 18:37:44 2008 -0700

    Merge branch 'cpus4096-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'cpus4096-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (31 commits)
      NR_CPUS: Replace NR_CPUS in speedstep-centrino.c
      cpumask: Provide a generic set of CPUMASK_ALLOC macros, FIXUP
      NR_CPUS: Replace NR_CPUS in cpufreq userspace routines
      NR_CPUS: Replace per_cpu(..., smp_processor_id()) with __get_cpu_var
      NR_CPUS: Replace NR_CPUS in arch/x86/kernel/genapic_flat_64.c
      NR_CPUS: Replace NR_CPUS in arch/x86/kernel/genx2apic_uv_x.c
      NR_CPUS: Replace NR_CPUS in arch/x86/kernel/cpu/proc.c
      NR_CPUS: Replace NR_CPUS in arch/x86/kernel/cpu/mcheck/mce_64.c
      cpumask: Optimize cpumask_of_cpu in lib/smp_processor_id.c, fix
      cpumask: Use optimized CPUMASK_ALLOC macros in the centrino_target
      cpumask: Provide a generic set of CPUMASK_ALLOC macros
      cpumask: Optimize cpumask_of_cpu in lib/smp_processor_id.c
      cpumask: Optimize cpumask_of_cpu in kernel/time/tick-common.c
      cpumask: Optimize cpumask_of_cpu in drivers/misc/sgi-xp/xpc_main.c
      cpumask: Optimize cpumask_of_cpu in arch/x86/kernel/ldt.c
      cpumask: Optimize cpumask_of_cpu in arch/x86/kernel/io_apic_64.c
      cpumask: Replace cpumask_of_cpu with cpumask_of_cpu_ptr
      Revert "cpumask: introduce new APIs"
      cpumask: make for_each_cpu_mask a bit smaller
      net: Pass reference to cpumask variable in net/sunrpc/svc.c
      ...
    
    Fix up trivial conflicts in drivers/cpufreq/cpufreq.c manually

commit 76c3bb15d6786a0b8da0ad0090e0c9c3672fc08b
Merge: 7be42004065c 93ded9b8fd42
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 22 09:06:21 2008 +0200

    Merge branch 'linus' into x86/x2apic

commit cfc1b9a6a683c835a20d5b565ade55baf639f72f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 21 21:35:38 2008 +0200

    x86: convert Dprintk to pr_debug
    
    There are a couple of places where (P)Dprintk is used which is an old
    compile time enabled printk wrapper. Convert it to the generic
    pr_debug().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 27640196eb7c..4b53a647bc0a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -216,7 +216,7 @@ static void __cpuinit smp_callin(void)
 		panic("%s: phys CPU#%d, CPU#%d already present??\n", __func__,
 					phys_id, cpuid);
 	}
-	Dprintk("CPU#%d (phys ID: %d) waiting for CALLOUT\n", cpuid, phys_id);
+	pr_debug("CPU#%d (phys ID: %d) waiting for CALLOUT\n", cpuid, phys_id);
 
 	/*
 	 * STARTUP IPIs are fragile beasts as they might sometimes
@@ -251,7 +251,7 @@ static void __cpuinit smp_callin(void)
 	 * boards)
 	 */
 
-	Dprintk("CALLIN, before setup_local_APIC().\n");
+	pr_debug("CALLIN, before setup_local_APIC().\n");
 	smp_callin_clear_local_apic();
 	setup_local_APIC();
 	end_local_APIC_setup();
@@ -266,7 +266,7 @@ static void __cpuinit smp_callin(void)
 	local_irq_enable();
 	calibrate_delay();
 	local_irq_disable();
-	Dprintk("Stack at about %p\n", &cpuid);
+	pr_debug("Stack at about %p\n", &cpuid);
 
 	/*
 	 * Save our processor parameters
@@ -513,7 +513,7 @@ static void impress_friends(void)
 	/*
 	 * Allow the user to impress friends.
 	 */
-	Dprintk("Before bogomips.\n");
+	pr_debug("Before bogomips.\n");
 	for_each_possible_cpu(cpu)
 		if (cpu_isset(cpu, cpu_callout_map))
 			bogosum += cpu_data(cpu).loops_per_jiffy;
@@ -523,7 +523,7 @@ static void impress_friends(void)
 		bogosum/(500000/HZ),
 		(bogosum/(5000/HZ))%100);
 
-	Dprintk("Before bogocount - setting activated=1.\n");
+	pr_debug("Before bogocount - setting activated=1.\n");
 }
 
 static inline void __inquire_remote_apic(int apicid)
@@ -585,7 +585,7 @@ wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
 	/* Kick the second */
 	apic_write(APIC_ICR, APIC_DM_NMI | APIC_DEST_LOGICAL);
 
-	Dprintk("Waiting for send to finish...\n");
+	pr_debug("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();
 
 	/*
@@ -596,7 +596,7 @@ wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
 	if (maxlvt > 3)			/* Due to the Pentium erratum 3AP.  */
 		apic_write(APIC_ESR, 0);
 	accept_status = (apic_read(APIC_ESR) & 0xEF);
-	Dprintk("NMI sent.\n");
+	pr_debug("NMI sent.\n");
 
 	if (send_status)
 		printk(KERN_ERR "APIC never delivered???\n");
@@ -631,7 +631,7 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 		apic_read(APIC_ESR);
 	}
 
-	Dprintk("Asserting INIT.\n");
+	pr_debug("Asserting INIT.\n");
 
 	/*
 	 * Turn INIT on target chip
@@ -644,12 +644,12 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 	apic_write(APIC_ICR,
 		   APIC_INT_LEVELTRIG | APIC_INT_ASSERT | APIC_DM_INIT);
 
-	Dprintk("Waiting for send to finish...\n");
+	pr_debug("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();
 
 	mdelay(10);
 
-	Dprintk("Deasserting INIT.\n");
+	pr_debug("Deasserting INIT.\n");
 
 	/* Target chip */
 	apic_write(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
@@ -657,7 +657,7 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 	/* Send IPI */
 	apic_write(APIC_ICR, APIC_INT_LEVELTRIG | APIC_DM_INIT);
 
-	Dprintk("Waiting for send to finish...\n");
+	pr_debug("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();
 
 	mb();
@@ -684,14 +684,14 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 	/*
 	 * Run STARTUP IPI loop.
 	 */
-	Dprintk("#startup loops: %d.\n", num_starts);
+	pr_debug("#startup loops: %d.\n", num_starts);
 
 	for (j = 1; j <= num_starts; j++) {
-		Dprintk("Sending STARTUP #%d.\n", j);
+		pr_debug("Sending STARTUP #%d.\n", j);
 		if (maxlvt > 3)		/* Due to the Pentium erratum 3AP.  */
 			apic_write(APIC_ESR, 0);
 		apic_read(APIC_ESR);
-		Dprintk("After apic_write.\n");
+		pr_debug("After apic_write.\n");
 
 		/*
 		 * STARTUP IPI
@@ -709,9 +709,9 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 		 */
 		udelay(300);
 
-		Dprintk("Startup point 1.\n");
+		pr_debug("Startup point 1.\n");
 
-		Dprintk("Waiting for send to finish...\n");
+		pr_debug("Waiting for send to finish...\n");
 		send_status = safe_apic_wait_icr_idle();
 
 		/*
@@ -724,7 +724,7 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 		if (send_status || accept_status)
 			break;
 	}
-	Dprintk("After Startup.\n");
+	pr_debug("After Startup.\n");
 
 	if (send_status)
 		printk(KERN_ERR "APIC never delivered???\n");
@@ -875,7 +875,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 
 	if (get_uv_system_type() != UV_NON_UNIQUE_APIC) {
 
-		Dprintk("Setting warm reset code and vector.\n");
+		pr_debug("Setting warm reset code and vector.\n");
 
 		store_NMI_vector(&nmi_high, &nmi_low);
 
@@ -896,9 +896,9 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		/*
 		 * allow APs to start initializing.
 		 */
-		Dprintk("Before Callout %d.\n", cpu);
+		pr_debug("Before Callout %d.\n", cpu);
 		cpu_set(cpu, cpu_callout_map);
-		Dprintk("After Callout %d.\n", cpu);
+		pr_debug("After Callout %d.\n", cpu);
 
 		/*
 		 * Wait 5s total for a response
@@ -911,10 +911,10 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 
 		if (cpu_isset(cpu, cpu_callin_map)) {
 			/* number CPUs logically, starting from 1 (BSP is 0) */
-			Dprintk("OK.\n");
+			pr_debug("OK.\n");
 			printk(KERN_INFO "CPU%d: ", cpu);
 			print_cpu_info(&cpu_data(cpu));
-			Dprintk("CPU has booted.\n");
+			pr_debug("CPU has booted.\n");
 		} else {
 			boot_error = 1;
 			if (*((volatile unsigned char *)trampoline_base)
@@ -959,7 +959,7 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 
 	WARN_ON(irqs_disabled());
 
-	Dprintk("++++++++++++++++++++=_---CPU UP  %u\n", cpu);
+	pr_debug("++++++++++++++++++++=_---CPU UP  %u\n", cpu);
 
 	if (apicid == BAD_APICID || apicid == boot_cpu_physical_apicid ||
 	    !physid_isset(apicid, phys_cpu_present_map)) {
@@ -971,7 +971,7 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 	 * Already booted CPU?
 	 */
 	if (cpu_isset(cpu, cpu_callin_map)) {
-		Dprintk("do_boot_cpu %d Already started\n", cpu);
+		pr_debug("do_boot_cpu %d Already started\n", cpu);
 		return -ENOSYS;
 	}
 
@@ -998,7 +998,7 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 	err = do_boot_cpu(apicid, cpu);
 #endif
 	if (err) {
-		Dprintk("do_boot_cpu failed %d\n", err);
+		pr_debug("do_boot_cpu failed %d\n", err);
 		return -EIO;
 	}
 
@@ -1202,7 +1202,7 @@ void __init native_smp_prepare_boot_cpu(void)
 
 void __init native_smp_cpus_done(unsigned int max_cpus)
 {
-	Dprintk("Boot done.\n");
+	pr_debug("Boot done.\n");
 
 	impress_friends();
 	smp_checks();

commit acee709cab689ec7703770e8b8cb5cc3a4abcb31
Merge: 33a37eb411d1 5ff4789d045c 35b680557f95 c4dc59ae7af8 7edf8891ad7a 9781f39fd209 48fe4a76e27d be54f9d1c8df 77e442461c74 caadbdce240c 5e5a29bf2624 e3a61b0a8c0e fec0962e0bed fab3b58d3b24 f2ba93929fdb 48ae74443403 3cabf37f6167 7019cc2dd6fa 2ddf9b7b3e66 e66d90fb4abd
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 21 16:37:17 2008 +0200

    Merge branches 'x86/urgent', 'x86/amd-iommu', 'x86/apic', 'x86/cleanups', 'x86/core', 'x86/cpu', 'x86/fixmap', 'x86/gart', 'x86/kprobes', 'x86/memtest', 'x86/modules', 'x86/nmi', 'x86/pat', 'x86/reboot', 'x86/setup', 'x86/step', 'x86/unify-pci', 'x86/uv', 'x86/xen' and 'xen-64bit' into x86/for-linus

commit 453c1404c5273a30d715e5a83372a78cff70b6d9
Merge: a208f37a465e 35b680557f95
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 18 23:00:05 2008 +0200

    Merge branch 'x86/apic' into x86/x2apic
    
    Conflicts:
    
            arch/x86/kernel/paravirt.c
            arch/x86/kernel/smpboot.c
            arch/x86/kernel/vmi_32.c
            arch/x86/lguest/boot.c
            arch/x86/xen/enlighten.c
            include/asm-x86/apic.h
            include/asm-x86/paravirt.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a208f37a465e222218974ab20a31b42b7b4893b2
Merge: 511d9d341836 5b664cb235e9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 18 22:50:34 2008 +0200

    Merge branch 'linus' into x86/x2apic

commit cdbfc557c43ea1f1f9b7062300ecb1254969814b
Merge: 4d8cc874d7ed 5b664cb235e9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 18 13:53:16 2008 +0200

    Merge branch 'linus' into x86/cleanups

commit 593f4a788e5d09e9f00182561437461b0b564de4
Author: Maciej W. Rozycki <macro@linux-mips.org>
Date:   Wed Jul 16 19:15:30 2008 +0100

    x86: APIC: remove apic_write_around(); use alternatives
    
    Use alternatives to select the workaround for the 11AP Pentium erratum
    for the affected steppings on the fly rather than build time.  Remove the
    X86_GOOD_APIC configuration option and replace all the calls to
    apic_write_around() with plain apic_write(), protecting accesses to the
    ESR as appropriate due to the 3AP Pentium erratum.  Remove
    apic_read_around() and all its invocations altogether as not needed.
    Remove apic_write_atomic() and all its implementing backends.  The use of
    ASM_OUTPUT2() is not strictly needed for input constraints, but I have
    used it for readability's sake.
    
    I had the feeling no one else was brave enough to do it, so I went ahead
    and here it is.  Verified by checking the generated assembly and tested
    with both a 32-bit and a 64-bit configuration, also with the 11AP
    "feature" forced on and verified with gdb on /proc/kcore to work as
    expected (as an 11AP machines are quite hard to get hands on these days).
    Some script complained about the use of "volatile", but apic_write() needs
    it for the same reason and is effectively a replacement for writel(), so I
    have disregarded it.
    
    I am not sure what the policy wrt defconfig files is, they are generated
    and there is risk of a conflict resulting from an unrelated change, so I
    have left changes to them out.  The option will get removed from them at
    the next run.
    
    Some testing with machines other than mine will be needed to avoid some
    stupid mistake, but despite its volume, the change is not really that
    intrusive, so I am fairly confident that because it works for me, it will
    everywhere.
    
    Signed-off-by: Maciej W. Rozycki <macro@linux-mips.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 687376ab07e8..f251f5c38823 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -546,8 +546,8 @@ static inline void __inquire_remote_apic(int apicid)
 			printk(KERN_CONT
 			       "a previous APIC delivery may have failed\n");
 
-		apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(apicid));
-		apic_write_around(APIC_ICR, APIC_DM_REMRD | regs[i]);
+		apic_write(APIC_ICR2, SET_APIC_DEST_FIELD(apicid));
+		apic_write(APIC_ICR, APIC_DM_REMRD | regs[i]);
 
 		timeout = 0;
 		do {
@@ -579,11 +579,11 @@ wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
 	int maxlvt;
 
 	/* Target chip */
-	apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(logical_apicid));
+	apic_write(APIC_ICR2, SET_APIC_DEST_FIELD(logical_apicid));
 
 	/* Boot on the stack */
 	/* Kick the second */
-	apic_write_around(APIC_ICR, APIC_DM_NMI | APIC_DEST_LOGICAL);
+	apic_write(APIC_ICR, APIC_DM_NMI | APIC_DEST_LOGICAL);
 
 	Dprintk("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();
@@ -592,14 +592,9 @@ wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
 	 * Give the other CPU some time to accept the IPI.
 	 */
 	udelay(200);
-	/*
-	 * Due to the Pentium erratum 3AP.
-	 */
 	maxlvt = lapic_get_maxlvt();
-	if (maxlvt > 3) {
-		apic_read_around(APIC_SPIV);
+	if (maxlvt > 3)			/* Due to the Pentium erratum 3AP.  */
 		apic_write(APIC_ESR, 0);
-	}
 	accept_status = (apic_read(APIC_ESR) & 0xEF);
 	Dprintk("NMI sent.\n");
 
@@ -625,12 +620,14 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 		return send_status;
 	}
 
+	maxlvt = lapic_get_maxlvt();
+
 	/*
 	 * Be paranoid about clearing APIC errors.
 	 */
 	if (APIC_INTEGRATED(apic_version[phys_apicid])) {
-		apic_read_around(APIC_SPIV);
-		apic_write(APIC_ESR, 0);
+		if (maxlvt > 3)		/* Due to the Pentium erratum 3AP.  */
+			apic_write(APIC_ESR, 0);
 		apic_read(APIC_ESR);
 	}
 
@@ -639,13 +636,13 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 	/*
 	 * Turn INIT on target chip
 	 */
-	apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
+	apic_write(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
 
 	/*
 	 * Send IPI
 	 */
-	apic_write_around(APIC_ICR, APIC_INT_LEVELTRIG | APIC_INT_ASSERT
-				| APIC_DM_INIT);
+	apic_write(APIC_ICR,
+		   APIC_INT_LEVELTRIG | APIC_INT_ASSERT | APIC_DM_INIT);
 
 	Dprintk("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();
@@ -655,10 +652,10 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 	Dprintk("Deasserting INIT.\n");
 
 	/* Target chip */
-	apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
+	apic_write(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
 
 	/* Send IPI */
-	apic_write_around(APIC_ICR, APIC_INT_LEVELTRIG | APIC_DM_INIT);
+	apic_write(APIC_ICR, APIC_INT_LEVELTRIG | APIC_DM_INIT);
 
 	Dprintk("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();
@@ -689,12 +686,10 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 	 */
 	Dprintk("#startup loops: %d.\n", num_starts);
 
-	maxlvt = lapic_get_maxlvt();
-
 	for (j = 1; j <= num_starts; j++) {
 		Dprintk("Sending STARTUP #%d.\n", j);
-		apic_read_around(APIC_SPIV);
-		apic_write(APIC_ESR, 0);
+		if (maxlvt > 3)		/* Due to the Pentium erratum 3AP.  */
+			apic_write(APIC_ESR, 0);
 		apic_read(APIC_ESR);
 		Dprintk("After apic_write.\n");
 
@@ -703,12 +698,11 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 		 */
 
 		/* Target chip */
-		apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
+		apic_write(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
 
 		/* Boot on the stack */
 		/* Kick the second */
-		apic_write_around(APIC_ICR, APIC_DM_STARTUP
-					| (start_eip >> 12));
+		apic_write(APIC_ICR, APIC_DM_STARTUP | (start_eip >> 12));
 
 		/*
 		 * Give the other CPU some time to accept the IPI.
@@ -724,13 +718,8 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 		 * Give the other CPU some time to accept the IPI.
 		 */
 		udelay(200);
-		/*
-		 * Due to the Pentium erratum 3AP.
-		 */
-		if (maxlvt > 3) {
-			apic_read_around(APIC_SPIV);
+		if (maxlvt > 3)		/* Due to the Pentium erratum 3AP.  */
 			apic_write(APIC_ESR, 0);
-		}
 		accept_status = (apic_read(APIC_ESR) & 0xEF);
 		if (send_status || accept_status)
 			break;

commit 29cbeb0e17d9d2ca824f62f71cfa7360b3157112
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Jul 17 21:50:23 2008 -0700

    x86: use cpu_clear in remove_cpu_from_maps
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 687376ab07e8..27456574f070 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1311,7 +1311,7 @@ static void __ref remove_cpu_from_maps(int cpu)
 	cpu_clear(cpu, cpu_callout_map);
 	cpu_clear(cpu, cpu_callin_map);
 	/* was set by cpu_init() */
-	clear_bit(cpu, (unsigned long *)&cpu_initialized);
+	cpu_clear(cpu, cpu_initialized);
 	numa_remove_cpu(cpu);
 }
 

commit 7c33b1e6ee26d67551109aca04d46544d0ce55b1
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:29 2008 -0700

    x86_64: unstatic get_local_pda
    
    This allows Xen's xen_cpu_up() to allocate a pda for the new CPU.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 687376ab07e8..1deb3b624a79 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -768,7 +768,7 @@ static void __cpuinit do_fork_idle(struct work_struct *work)
  *
  * Must be called after the _cpu_pda pointer table is initialized.
  */
-static int __cpuinit get_local_pda(int cpu)
+int __cpuinit get_local_pda(int cpu)
 {
 	struct x8664_pda *oldpda, *newpda;
 	unsigned long size = sizeof(struct x8664_pda);

commit 82638844d9a8581bbf33201cc209a14876eca167
Merge: 9982fbface82 63cf13b77ab7
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jul 16 00:29:07 2008 +0200

    Merge branch 'linus' into cpus4096
    
    Conflicts:
    
            arch/x86/xen/smp.c
            kernel/sched_rt.c
            net/iucv/iucv.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 1a781a777b2f6ac46523fe92396215762ced624d
Merge: b9d2252c1e44 42a2f217a5e3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 15 21:55:59 2008 +0200

    Merge branch 'generic-ipi' into generic-ipi-for-linus
    
    Conflicts:
    
            arch/powerpc/Kconfig
            arch/s390/kernel/time.c
            arch/x86/kernel/apic_32.c
            arch/x86/kernel/cpu/perfctr-watchdog.c
            arch/x86/kernel/i8259_64.c
            arch/x86/kernel/ldt.c
            arch/x86/kernel/nmi_64.c
            arch/x86/kernel/smpboot.c
            arch/x86/xen/smp.c
            include/asm-x86/hw_irq_32.h
            include/asm-x86/hw_irq_64.h
            include/asm-x86/mach-default/irq_vectors.h
            include/asm-x86/mach-voyager/irq_vectors.h
            include/asm-x86/smp.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 4c9961d56ec20c27ec5d02e49fd7427748312741
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Fri Jul 11 18:44:16 2008 -0700

    x86: make read_apic_id return final apicid
    
    also remove GET_APIC_ID when read_apic_id is used.
    
    need to apply after
            [PATCH] x86: mach_apicdef.h need to include before smp.h
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 0c43e1f2e7d3..6cd002f3e20e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -211,7 +211,7 @@ static void __cpuinit smp_callin(void)
 	/*
 	 * (This works even if the APIC is not enabled.)
 	 */
-	phys_id = GET_APIC_ID(read_apic_id());
+	phys_id = read_apic_id();
 	cpuid = smp_processor_id();
 	if (cpu_isset(cpuid, cpu_callin_map)) {
 		panic("%s: phys CPU#%d, CPU#%d already present??\n", __func__,
@@ -1157,9 +1157,9 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	}
 
 	preempt_disable();
-	if (GET_APIC_ID(read_apic_id()) != boot_cpu_physical_apicid) {
+	if (read_apic_id() != boot_cpu_physical_apicid) {
 		panic("Boot APIC ID in local APIC unexpected (%d vs %d)",
-		     GET_APIC_ID(read_apic_id()), boot_cpu_physical_apicid);
+		     read_apic_id(), boot_cpu_physical_apicid);
 		/* Or can we switch back to PIC here? */
 	}
 	preempt_enable();

commit 6e1cb38a2aef7680975e71f23de187859ee8b158
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Jul 10 11:16:58 2008 -0700

    x64, x2apic/intr-remap: add x2apic support, including enabling interrupt-remapping
    
    x2apic support.  Interrupt-remapping must be enabled before enabling x2apic,
    this is needed to ensure that IO interrupts continue to work properly after the
    cpu mode is changed to x2apic(which uses 32bit extended physical/cluster
    apic id).
    
    On systems where apicid's are > 255, BIOS can handover the control to OS in
    x2apic mode. Or if the OS handover was in legacy xapic mode, check
    if it is capable of x2apic mode. And if we succeed in enabling
    Interrupt-remapping, then we can enable x2apic mode in the CPU.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: akpm@linux-foundation.org
    Cc: arjan@linux.intel.com
    Cc: andi@firstfloor.org
    Cc: ebiederm@xmission.com
    Cc: jbarnes@virtuousgeek.org
    Cc: steiner@sgi.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c55263b3df02..0c43e1f2e7d3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1145,6 +1145,11 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	current_thread_info()->cpu = 0;  /* needed? */
 	set_cpu_sibling_map(0);
 
+#ifdef CONFIG_X86_64
+	enable_IR_x2apic();
+	setup_apic_routing();
+#endif
+
 	if (smp_sanity_check(max_cpus) < 0) {
 		printk(KERN_INFO "SMP disabled\n");
 		disable_smp();

commit 1b374e4d6f8b3eb2fcd034fcc24ea8ba1dfde7aa
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Jul 10 11:16:49 2008 -0700

    x64, x2apic/intr-remap: basic apic ops support
    
    Introduce basic apic operations which handle the apic programming. This
    will be used later to introduce another specific operations for x2apic.
    
    For the perfomance critial accesses like IPI's, EOI etc, we use the
    native operations as they are already referenced by different
    indirections like genapic, irq_chip etc.
    
    64bit Paravirt ops can also define their apic operations accordingly.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: akpm@linux-foundation.org
    Cc: arjan@linux.intel.com
    Cc: andi@firstfloor.org
    Cc: ebiederm@xmission.com
    Cc: jbarnes@virtuousgeek.org
    Cc: steiner@sgi.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f35c2d8016ac..c55263b3df02 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -123,7 +123,6 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 
 static atomic_t init_deasserted;
 
-static int boot_cpu_logical_apicid;
 
 /* representing cpus for which sibling maps can be computed */
 static cpumask_t cpu_sibling_setup_map;
@@ -165,6 +164,8 @@ static void unmap_cpu_to_node(int cpu)
 #endif
 
 #ifdef CONFIG_X86_32
+static int boot_cpu_logical_apicid;
+
 u8 cpu_2_logical_apicid[NR_CPUS] __read_mostly =
 					{ [0 ... NR_CPUS-1] = BAD_APICID };
 
@@ -546,8 +547,7 @@ static inline void __inquire_remote_apic(int apicid)
 			printk(KERN_CONT
 			       "a previous APIC delivery may have failed\n");
 
-		apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(apicid));
-		apic_write_around(APIC_ICR, APIC_DM_REMRD | regs[i]);
+		apic_icr_write(APIC_DM_REMRD | regs[i], apicid);
 
 		timeout = 0;
 		do {
@@ -579,11 +579,9 @@ wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
 	int maxlvt;
 
 	/* Target chip */
-	apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(logical_apicid));
-
 	/* Boot on the stack */
 	/* Kick the second */
-	apic_write_around(APIC_ICR, APIC_DM_NMI | APIC_DEST_LOGICAL);
+	apic_icr_write(APIC_DM_NMI | APIC_DEST_LOGICAL, logical_apicid);
 
 	Dprintk("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();
@@ -639,13 +637,11 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 	/*
 	 * Turn INIT on target chip
 	 */
-	apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
-
 	/*
 	 * Send IPI
 	 */
-	apic_write_around(APIC_ICR, APIC_INT_LEVELTRIG | APIC_INT_ASSERT
-				| APIC_DM_INIT);
+	apic_icr_write(APIC_INT_LEVELTRIG | APIC_INT_ASSERT | APIC_DM_INIT,
+		       phys_apicid);
 
 	Dprintk("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();
@@ -655,10 +651,8 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 	Dprintk("Deasserting INIT.\n");
 
 	/* Target chip */
-	apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
-
 	/* Send IPI */
-	apic_write_around(APIC_ICR, APIC_INT_LEVELTRIG | APIC_DM_INIT);
+	apic_icr_write(APIC_INT_LEVELTRIG | APIC_DM_INIT, phys_apicid);
 
 	Dprintk("Waiting for send to finish...\n");
 	send_status = safe_apic_wait_icr_idle();
@@ -703,12 +697,10 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 		 */
 
 		/* Target chip */
-		apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
-
 		/* Boot on the stack */
 		/* Kick the second */
-		apic_write_around(APIC_ICR, APIC_DM_STARTUP
-					| (start_eip >> 12));
+		apic_icr_write(APIC_DM_STARTUP | (start_eip >> 12),
+			       phys_apicid);
 
 		/*
 		 * Give the other CPU some time to accept the IPI.
@@ -1147,7 +1139,9 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	 * Setup boot CPU information
 	 */
 	smp_store_cpu_info(0); /* Final full version of the data */
+#ifdef CONFIG_X86_32
 	boot_cpu_logical_apicid = logical_smp_processor_id();
+#endif
 	current_thread_info()->cpu = 0;  /* needed? */
 	set_cpu_sibling_map(0);
 

commit 4d8cc874d7ed43eda72765e9c0e141e170fee4f3
Author: Cyrill Gorcunov <gorcunov@gmail.com>
Date:   Sat Jul 5 15:53:38 2008 +0400

    x86: smpboot maxcpus - add checking for NULL early param
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: akpm@linux-foundation.org
    Cc: andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fd933b5465b6..e47bfac70c38 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1452,7 +1452,8 @@ static int __init parse_maxcpus(char *arg)
 {
 	extern unsigned int maxcpus;
 
-	maxcpus = simple_strtoul(arg, NULL, 0);
+	if (arg)
+		maxcpus = simple_strtoul(arg, NULL, 0);
 	return 0;
 }
 early_param("maxcpus", parse_maxcpus);

commit 6f585e01614f38195599c1bc5379d3c9dae6be47
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Fri Jul 4 12:36:21 2008 +0200

    arch/x86/kernel/smpboot.c: fix warning
    
    arch/x86/kernel/smpboot.c: In function 'do_boot_cpu':
    arch/x86/kernel/smpboot.c:943: warning: label 'restore_state' defined but not used
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e1200b202ed7..f35c2d8016ac 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -939,9 +939,9 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 				inquire_remote_apic(apicid);
 		}
 	}
-
+#ifdef CONFIG_X86_64
 restore_state:
-
+#endif
 	if (boot_error) {
 		/* Try to put things back the way they were before ... */
 		numa_remove_cpu(cpu); /* was set by numa_add_cpu */

commit 329513a35d1a2b6b28d54f5c2c0dde4face8200b
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Wed Jul 2 18:54:40 2008 -0700

    x86: move prefill_possible_map calling early
    
    call it right after we are done with MADT/mptable handling, instead of
    doing that in setup_per_cpu_areas() later on...
    
    this way for_possible_cpu() can be used early.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3b19441d78b8..e1200b202ed7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1278,12 +1278,20 @@ __init void prefill_possible_map(void)
 	int i;
 	int possible;
 
+	/* no processor from mptable or madt */
+	if (!num_processors)
+		num_processors = 1;
+
+#ifdef CONFIG_HOTPLUG_CPU
 	if (additional_cpus == -1) {
 		if (disabled_cpus > 0)
 			additional_cpus = disabled_cpus;
 		else
 			additional_cpus = 0;
 	}
+#else
+	additional_cpus = 0;
+#endif
 	possible = num_processors + additional_cpus;
 	if (possible > NR_CPUS)
 		possible = NR_CPUS;

commit c376d45432d935e6f1e0ff2d6be3734bcd3ba455
Author: Cyrill Gorcunov <gorcunov@gmail.com>
Date:   Tue Jun 24 22:52:05 2008 +0200

    x86: nmi_watchdog - use NMI_NONE by default
    
    There is no need to keep NMI_DISABLED definition and use it
    for nmi_watchdog by default. Here is the point why:
    
    - IO-APIC and APIC chips are programmed for nmi_watchdog support at very
      early stage of kernel booting and not having nmi_watchdog specified as
      boot option lead only to nmi_watchdog becomes to NMI_NONE anyway
    - enable nmi_watchdog thru /proc/sys/kernel/nmi if it was not specified at
      boot is not possible too (even having this sysfs entry)
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: macro@linux-mips.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3b48d1f4c7c3..3b19441d78b8 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1139,7 +1139,6 @@ static void __init smp_cpu_index_default(void)
 void __init native_smp_prepare_cpus(unsigned int max_cpus)
 {
 	preempt_disable();
-	nmi_watchdog_default();
 	smp_cpu_index_default();
 	current_cpu_data = boot_cpu_data;
 	cpu_callin_map = cpumask_of_cpu(0);

commit 7f6cbc905ee22c457e0dcd0bba9d4affbc290a6f
Author: Glauber Costa <gcosta@redhat.com>
Date:   Wed Jun 4 23:05:39 2008 -0700

    x86: take load_sp0 out of smpboot.c
    
    there's no particular reason to do load_sp0 in different
    places for i386 and x86_64. They should all be in cpu_init.
    Right now, cpu_init itself is not integrated, but with this patch,
    the code becomes closer to each other, making in easier to integrate
    when the time comes.
    
    Furthermore, although doing it in do_boot_cpu for x86_64 is fine, since it's
    only a copy, load_sp0 should be executed in the cpu it refers to anyway.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 67af727f733a..3b48d1f4c7c3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -864,7 +864,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	irq_ctx_init(cpu);
 #else
 	cpu_pda(cpu)->pcurrent = c_idle.idle;
-	load_sp0(&per_cpu(init_tss, cpu), &c_idle.idle->thread);
 	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
 #endif
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);

commit 1481a3dd42c21ac4a8b9497cb9f5df816d6b064f
Author: Glauber Costa <gcosta@redhat.com>
Date:   Wed Jun 4 15:35:03 2008 -0300

    x86: move cpu_exit_clear to process_32.c
    
    Take it out of smpboot.c, and move it to process_32.c, closer
    to its only user.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 820c23dbe761..67af727f733a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -181,7 +181,7 @@ static void map_cpu_to_logical_apicid(void)
 	map_cpu_to_node(cpu, node);
 }
 
-static void numa_remove_cpu(int cpu)
+void numa_remove_cpu(int cpu)
 {
 	cpu_2_logical_apicid[cpu] = BAD_APICID;
 	unmap_cpu_to_node(cpu);
@@ -1227,23 +1227,6 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 
 #ifdef CONFIG_HOTPLUG_CPU
 
-#  ifdef CONFIG_X86_32
-void cpu_exit_clear(void)
-{
-	int cpu = raw_smp_processor_id();
-
-	idle_task_exit();
-
-	cpu_uninit();
-	irq_ctx_exit(cpu);
-
-	cpu_clear(cpu, cpu_callout_map);
-	cpu_clear(cpu, cpu_callin_map);
-
-	numa_remove_cpu(cpu);
-}
-#  endif /* CONFIG_X86_32 */
-
 static void remove_siblinginfo(int cpu)
 {
 	int sibling;

commit b553a1e0ff48bd66fd18f705370e47c0b4ecea61
Author: Glauber Costa <gcosta@redhat.com>
Date:   Wed Jun 4 02:05:03 2008 -0300

    x86: remove cpu from maps
    
    during cpu disable, take cpus out of all maps in i386, instead
    of just the online map.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b99c386af77d..820c23dbe761 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1319,13 +1319,11 @@ __init void prefill_possible_map(void)
 static void __ref remove_cpu_from_maps(int cpu)
 {
 	cpu_clear(cpu, cpu_online_map);
-#ifdef CONFIG_X86_64
 	cpu_clear(cpu, cpu_callout_map);
 	cpu_clear(cpu, cpu_callin_map);
 	/* was set by cpu_init() */
 	clear_bit(cpu, (unsigned long *)&cpu_initialized);
 	numa_remove_cpu(cpu);
-#endif
 }
 
 int __cpu_disable(void)

commit 78e622705c69da9649ba87071d8de85054b62df8
Author: Glauber Costa <gcosta@redhat.com>
Date:   Wed Jun 4 02:03:07 2008 -0300

    x86: change naming to match x86_64
    
    Change unmap_cpu_to_logical_apicid to numa_remove_cpu.
    Besides being shorter, it is the same name x86_64 uses. We
    can save an ifdef in the code this way.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a569f06d789e..b99c386af77d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -181,13 +181,12 @@ static void map_cpu_to_logical_apicid(void)
 	map_cpu_to_node(cpu, node);
 }
 
-static void unmap_cpu_to_logical_apicid(int cpu)
+static void numa_remove_cpu(int cpu)
 {
 	cpu_2_logical_apicid[cpu] = BAD_APICID;
 	unmap_cpu_to_node(cpu);
 }
 #else
-#define unmap_cpu_to_logical_apicid(cpu) do {} while (0)
 #define map_cpu_to_logical_apicid()  do {} while (0)
 #endif
 
@@ -946,10 +945,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 
 	if (boot_error) {
 		/* Try to put things back the way they were before ... */
-		unmap_cpu_to_logical_apicid(cpu);
-#ifdef CONFIG_X86_64
 		numa_remove_cpu(cpu); /* was set by numa_add_cpu */
-#endif
 		cpu_clear(cpu, cpu_callout_map); /* was set by do_boot_cpu() */
 		cpu_clear(cpu, cpu_initialized); /* was set by cpu_init() */
 		cpu_clear(cpu, cpu_present_map);
@@ -1244,7 +1240,7 @@ void cpu_exit_clear(void)
 	cpu_clear(cpu, cpu_callout_map);
 	cpu_clear(cpu, cpu_callin_map);
 
-	unmap_cpu_to_logical_apicid(cpu);
+	numa_remove_cpu(cpu);
 }
 #  endif /* CONFIG_X86_32 */
 

commit b5841765a2e735a38612c4e4a82170c33d701b3c
Author: Glauber Costa <gcosta@redhat.com>
Date:   Wed May 28 13:38:28 2008 -0300

    x86: provide connect_bsp_APIC for x86_64
    
    Although it is not really needed, we provide it to get
    closer to i386. ifdefs around it are removed in smpboot.c
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e09f3124738a..a569f06d789e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1116,9 +1116,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 
 		localise_nmi_watchdog();
 
-#ifdef CONFIG_X86_32
 		connect_bsp_APIC();
-#endif
 		setup_local_APIC();
 		end_local_APIC_setup();
 		return -1;
@@ -1173,9 +1171,8 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	}
 	preempt_enable();
 
-#ifdef CONFIG_X86_32
 	connect_bsp_APIC();
-#endif
+
 	/*
 	 * Switch from PIC to APIC mode.
 	 */

commit 3fde690011a84e19f98f77bfaa349b2119ddd2d2
Author: Glauber Costa <gcosta@redhat.com>
Date:   Wed May 28 20:34:19 2008 -0700

    x86: change __setup_vector_irq with setup_vector_irq
    
    We create a version of it for i386, and then take the CONFIG_X86_64
    ifdef out of the game. We could create a __setup_vector_irq for i386,
    but it would incur in an unnecessary lock taking. Moreover, it is better
    practice to only export setup_vector_irq anyway.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6f9a31a18811..e09f3124738a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -329,15 +329,8 @@ static void __cpuinit start_secondary(void *unused)
 	 * smp_call_function().
 	 */
 	lock_ipi_call_lock();
-#ifdef CONFIG_X86_64
-	spin_lock(&vector_lock);
-
-	/* Setup the per cpu irq handling data structures */
-	__setup_vector_irq(smp_processor_id());
-	/*
-	 * Allow the master to continue.
-	 */
-	spin_unlock(&vector_lock);
+#ifdef CONFIG_X86_IO_APIC
+	setup_vector_irq(smp_processor_id());
 #endif
 	cpu_set(smp_processor_id(), cpu_online_map);
 	unlock_ipi_call_lock();

commit 86e430edf462e872ecfab28d6b8619be5ab9c300
Author: Glauber Costa <gcosta@redhat.com>
Date:   Wed May 28 20:05:46 2008 -0700

    x86: remove ifdef from stepping
    
    The stepping won't affect x86_64, since there are not x86_64 k7's
    or pentiums. So, although it adds to the binary size, remove the ifdef
    for smoother integration
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a74a261c5626..6f9a31a18811 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -351,7 +351,6 @@ static void __cpuinit start_secondary(void *unused)
 
 static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
 {
-#ifdef CONFIG_X86_32
 	/*
 	 * Mask B, Pentium, but not Pentium MMX
 	 */
@@ -401,7 +400,6 @@ static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
 
 valid_k7:
 	;
-#endif
 }
 
 static void __cpuinit smp_checks(void)

commit 0f385d1ddd0952e01a968bfa5512378ad23de6df
Author: Glauber Costa <gcosta@redhat.com>
Date:   Wed May 28 17:09:53 2008 -0700

    x86: clearing io_apic harmless for x86_64
    
    so remove ifdef.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2a0d39f3f2f1..a74a261c5626 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1052,9 +1052,8 @@ static __init void disable_smp(void)
 {
 	cpu_present_map = cpumask_of_cpu(0);
 	cpu_possible_map = cpumask_of_cpu(0);
-#ifdef CONFIG_X86_32
 	smpboot_clear_io_apic_irqs();
-#endif
+
 	if (smp_found_config)
 		physid_set_mask_of_physid(boot_cpu_physical_apicid, &phys_cpu_present_map);
 	else

commit 3e9704739daf46a8ba6593d749c67b5f7cd633d2
Author: Glauber Costa <gcosta@redhat.com>
Date:   Wed May 28 13:01:54 2008 -0300

    x86: boot secondary cpus through initial_code
    
    remove "initialize_secondary". Boot both architectures via
    initial_code.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fe2bd515d6cc..2a0d39f3f2f1 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -349,28 +349,6 @@ static void __cpuinit start_secondary(void *unused)
 	cpu_idle();
 }
 
-#ifdef CONFIG_X86_32
-/*
- * Everything has been set up for the secondary
- * CPUs - they just need to reload everything
- * from the task structure
- * This function must not return.
- */
-void __devinit initialize_secondary(void)
-{
-	/*
-	 * We don't actually need to load the full TSS,
-	 * basically just the stack pointer and the ip.
-	 */
-
-	asm volatile(
-		"movl %0,%%esp\n\t"
-		"jmp *%1"
-		:
-		:"m" (current->thread.sp), "m" (current->thread.ip));
-}
-#endif
-
 static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_32
@@ -892,16 +870,15 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 #ifdef CONFIG_X86_32
 	per_cpu(current_task, cpu) = c_idle.idle;
 	init_gdt(cpu);
-	c_idle.idle->thread.ip = (unsigned long) start_secondary;
 	/* Stack for startup_32 can be just as for start_secondary onwards */
 	irq_ctx_init(cpu);
 #else
 	cpu_pda(cpu)->pcurrent = c_idle.idle;
 	load_sp0(&per_cpu(init_tss, cpu), &c_idle.idle->thread);
-	initial_code = (unsigned long)start_secondary;
 	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
 #endif
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
+	initial_code = (unsigned long)start_secondary;
 	stack_start.sp = (void *) c_idle.idle->thread.sp;
 
 	/* start_ip had better be page-aligned! */

commit a939098afcfa5f81d3474782ec15c6d114e57763
Author: Glauber Costa <gcosta@redhat.com>
Date:   Wed May 28 16:19:53 2008 -0700

    x86: move x86_64 gdt closer to i386
    
    i386 and x86_64 used two different schemes for maintaining the gdt.
    With this patch, x86_64 initial gdt table is defined in a .c file,
    same way as i386 is now. Also, we call it "gdt_page", and the descriptor,
    "early_gdt_descr". This way we achieve common naming, which can allow for
    more code integration.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a71e3cad5470..fe2bd515d6cc 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -849,14 +849,8 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		.done = COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
 	};
 	INIT_WORK(&c_idle.work, do_fork_idle);
-#ifdef CONFIG_X86_64
-	/* allocate memory for gdts of secondary cpus. Hotplug is considered */
-	if (!cpu_gdt_descr[cpu].address &&
-		!(cpu_gdt_descr[cpu].address = get_zeroed_page(GFP_KERNEL))) {
-		printk(KERN_ERR "Failed to allocate GDT for CPU %d\n", cpu);
-		return -1;
-	}
 
+#ifdef CONFIG_X86_64
 	/* Allocate node local memory for AP pdas */
 	if (cpu > 0) {
 		boot_error = get_local_pda(cpu);
@@ -898,7 +892,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 #ifdef CONFIG_X86_32
 	per_cpu(current_task, cpu) = c_idle.idle;
 	init_gdt(cpu);
-	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	c_idle.idle->thread.ip = (unsigned long) start_secondary;
 	/* Stack for startup_32 can be just as for start_secondary onwards */
 	irq_ctx_init(cpu);
@@ -908,6 +901,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	initial_code = (unsigned long)start_secondary;
 	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
 #endif
+	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	stack_start.sp = (void *) c_idle.idle->thread.sp;
 
 	/* start_ip had better be page-aligned! */
@@ -1252,8 +1246,8 @@ void __init native_smp_prepare_boot_cpu(void)
 	int me = smp_processor_id();
 #ifdef CONFIG_X86_32
 	init_gdt(me);
-	switch_to_new_gdt();
 #endif
+	switch_to_new_gdt();
 	/* already set me in cpu_online_map in boot_cpu_init() */
 	cpu_set(me, cpu_callout_map);
 	per_cpu(cpu_state, me) = CPU_ONLINE;

commit 9cf4f298e29abba25c16679fe7be70898223167e
Author: Glauber Costa <gcosta@redhat.com>
Date:   Tue May 27 18:22:54 2008 -0700

    x86: use stack_start in x86_64
    
    call x86_64's init_rsp stack_start, just as i386 does.
    Put a zeroed stack segment for consistency. With this,
    we can eliminate one ugly ifdef in smpboot.c.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d3ad4e09455b..a71e3cad5470 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -714,11 +714,7 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 	 * target processor state.
 	 */
 	startup_ipi_hook(phys_apicid, (unsigned long) start_secondary,
-#ifdef CONFIG_X86_64
-			 (unsigned long)init_rsp);
-#else
 			 (unsigned long)stack_start.sp);
-#endif
 
 	/*
 	 * Run STARTUP IPI loop.
@@ -905,15 +901,14 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	c_idle.idle->thread.ip = (unsigned long) start_secondary;
 	/* Stack for startup_32 can be just as for start_secondary onwards */
-	stack_start.sp = (void *) c_idle.idle->thread.sp;
 	irq_ctx_init(cpu);
 #else
 	cpu_pda(cpu)->pcurrent = c_idle.idle;
-	init_rsp = c_idle.idle->thread.sp;
 	load_sp0(&per_cpu(init_tss, cpu), &c_idle.idle->thread);
 	initial_code = (unsigned long)start_secondary;
 	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
 #endif
+	stack_start.sp = (void *) c_idle.idle->thread.sp;
 
 	/* start_ip had better be page-aligned! */
 	start_ip = setup_trampoline();

commit e3ae0acf59244ecf5b023ec99cef4b6b29d649bc
Merge: 4b62ac9a2b85 e7eb8726d0e1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 8 12:24:13 2008 +0200

    Merge branch 'x86/uv' into x86/devel

commit b6df1b8bc1250191cfee15627697111c1cbda53f
Author: Jack Steiner <steiner@sgi.com>
Date:   Thu Jun 19 21:51:05 2008 -0500

    x86: fix stack overflow for large values of MAX_APICS
    
    physid_mask_of_physid() causes a huge stack (12k) to be created if the
    number of APICS is large. Replace physid_mask_of_physid() with a
    new function that does not create large stacks. This is a problem only
    on large x86_64 systems.
    
    this paves the way to increase MAX_APICS.
    
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Cc: linux-mm@kvack.org
    Cc: mingo@elte.hu
    Cc: tglx@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3e1cecedde42..664a5db36d40 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1091,10 +1091,9 @@ static __init void disable_smp(void)
 	smpboot_clear_io_apic_irqs();
 #endif
 	if (smp_found_config)
-		phys_cpu_present_map =
-				physid_mask_of_physid(boot_cpu_physical_apicid);
+		physid_set_mask_of_physid(boot_cpu_physical_apicid, &phys_cpu_present_map);
 	else
-		phys_cpu_present_map = physid_mask_of_physid(0);
+		physid_set_mask_of_physid(0, &phys_cpu_present_map);
 	map_cpu_to_logical_apicid();
 	cpu_set(0, per_cpu(cpu_sibling_map, 0));
 	cpu_set(0, per_cpu(cpu_core_map, 0));

commit 2b4fa851b2f06fdb04cac808b57324f5e51e1578
Merge: 3de352bbd86f 46f68e1c6b04
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 8 11:59:23 2008 +0200

    Merge branch 'x86/numa' into x86/devel
    
    Conflicts:
    
            arch/x86/Kconfig
            arch/x86/kernel/e820.c
            arch/x86/kernel/efi_64.c
            arch/x86/kernel/mpparse.c
            arch/x86/kernel/setup.c
            arch/x86/kernel/setup_32.c
            arch/x86/mm/init_64.c
            include/asm-x86/proto.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f307d25e638d3408659a2ec98fb3fd1737f7cb31
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed May 21 11:21:13 2008 +0100

    x86: compile error fix for smpboot.c
    
    Without this patch, my link fails with:
    
    arch/x86/kernel/built-in.o(.cpuinit.text+0x3c6e): In function `get_local_pda':
    : undefined reference to `_cpu_pda'
    arch/x86/kernel/built-in.o(.cpuinit.text+0x3cd1): In function `get_local_pda':
    : undefined reference to `after_bootmem'
    arch/x86/kernel/built-in.o(.cpuinit.text+0x3cec): In function `get_local_pda':
    : undefined reference to `_cpu_pda'
    make[2]: *** [.tmp_vmlinux1] Error 1
    
    Caused by commit 766da892634694f795b18b9538407816896fc470
        x86: remove static boot_cpu_pda array v2
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index bf0833487455..bc1e1257e515 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -816,6 +816,7 @@ static void __cpuinit do_fork_idle(struct work_struct *work)
 	complete(&c_idle->done);
 }
 
+#ifdef CONFIG_X86_64
 /*
  * Allocate node local memory for the AP pda.
  *
@@ -852,6 +853,7 @@ static int __cpuinit get_local_pda(int cpu)
 	cpu_pda(cpu) = newpda;
 	return 0;
 }
+#endif /* CONFIG_X86_64 */
 
 static int __cpuinit do_boot_cpu(int apicid, int cpu)
 /*

commit 3461b0af025251bbc6b3d56c821c6ac2de6f7209
Author: Mike Travis <travis@sgi.com>
Date:   Mon May 12 21:21:13 2008 +0200

    x86: remove static boot_cpu_pda array v2
    
      * Remove the boot_cpu_pda array and pointer table from the data section.
        Allocate the pointer table and array during init.  do_boot_cpu()
        will reallocate the pda in node local memory and if the cpu is being
        brought up before the bootmem array is released (after_bootmem = 0),
        then it will free the initial pda.  This will happen for all cpus
        present at system startup.
    
        This removes 512k + 32k bytes from the data section.
    
    For inclusion into sched-devel/latest tree.
    
    Based on:
            git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6.git
        +   sched-devel/latest  .../mingo/linux-2.6-sched-devel.git
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 036604d3daed..bf0833487455 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -816,6 +816,43 @@ static void __cpuinit do_fork_idle(struct work_struct *work)
 	complete(&c_idle->done);
 }
 
+/*
+ * Allocate node local memory for the AP pda.
+ *
+ * Must be called after the _cpu_pda pointer table is initialized.
+ */
+static int __cpuinit get_local_pda(int cpu)
+{
+	struct x8664_pda *oldpda, *newpda;
+	unsigned long size = sizeof(struct x8664_pda);
+	int node = cpu_to_node(cpu);
+
+	if (cpu_pda(cpu) && !cpu_pda(cpu)->in_bootmem)
+		return 0;
+
+	oldpda = cpu_pda(cpu);
+	newpda = kmalloc_node(size, GFP_ATOMIC, node);
+	if (!newpda) {
+		printk(KERN_ERR "Could not allocate node local PDA "
+			"for CPU %d on node %d\n", cpu, node);
+
+		if (oldpda)
+			return 0;	/* have a usable pda */
+		else
+			return -1;
+	}
+
+	if (oldpda) {
+		memcpy(newpda, oldpda, size);
+		if (!after_bootmem)
+			free_bootmem((unsigned long)oldpda, size);
+	}
+
+	newpda->in_bootmem = 0;
+	cpu_pda(cpu) = newpda;
+	return 0;
+}
+
 static int __cpuinit do_boot_cpu(int apicid, int cpu)
 /*
  * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad
@@ -841,19 +878,11 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	}
 
 	/* Allocate node local memory for AP pdas */
-	if (cpu_pda(cpu) == &boot_cpu_pda[cpu]) {
-		struct x8664_pda *newpda, *pda;
-		int node = cpu_to_node(cpu);
-		pda = cpu_pda(cpu);
-		newpda = kmalloc_node(sizeof(struct x8664_pda), GFP_ATOMIC,
-				      node);
-		if (newpda) {
-			memcpy(newpda, pda, sizeof(struct x8664_pda));
-			cpu_pda(cpu) = newpda;
-		} else
-			printk(KERN_ERR
-		"Could not allocate node local PDA for CPU %d on node %d\n",
-				cpu, node);
+	if (cpu > 0) {
+		boot_error = get_local_pda(cpu);
+		if (boot_error)
+			goto restore_state;
+			/* if can't get pda memory, can't start cpu */
 	}
 #endif
 
@@ -972,6 +1001,8 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		}
 	}
 
+restore_state:
+
 	if (boot_error) {
 		/* Try to put things back the way they were before ... */
 		unmap_cpu_to_logical_apicid(cpu);
@@ -1347,6 +1378,8 @@ __init void prefill_possible_map(void)
 
 	for (i = 0; i < possible; i++)
 		cpu_set(i, cpu_possible_map);
+
+	nr_cpu_ids = possible;
 }
 
 static void __ref remove_cpu_from_maps(int cpu)

commit 23ca4bba3e20c6c3cb11c1bb0ab4770b724d39ac
Author: Mike Travis <travis@sgi.com>
Date:   Mon May 12 21:21:12 2008 +0200

    x86: cleanup early per cpu variables/accesses v4
    
      * Introduce a new PER_CPU macro called "EARLY_PER_CPU".  This is
        used by some per_cpu variables that are initialized and accessed
        before there are per_cpu areas allocated.
    
        ["Early" in respect to per_cpu variables is "earlier than the per_cpu
        areas have been setup".]
    
        This patchset adds these new macros:
    
            DEFINE_EARLY_PER_CPU(_type, _name, _initvalue)
            EXPORT_EARLY_PER_CPU_SYMBOL(_name)
            DECLARE_EARLY_PER_CPU(_type, _name)
    
            early_per_cpu_ptr(_name)
            early_per_cpu_map(_name, _idx)
            early_per_cpu(_name, _cpu)
    
        The DEFINE macro defines the per_cpu variable as well as the early
        map and pointer.  It also initializes the per_cpu variable and map
        elements to "_initvalue".  The early_* macros provide access to
        the initial map (usually setup during system init) and the early
        pointer.  This pointer is initialized to point to the early map
        but is then NULL'ed when the actual per_cpu areas are setup.  After
        that the per_cpu variable is the correct access to the variable.
    
        The early_per_cpu() macro is not very efficient but does show how to
        access the variable if you have a function that can be called both
        "early" and "late".  It tests the early ptr to be NULL, and if not
        then it's still valid.  Otherwise, the per_cpu variable is used
        instead:
    
            #define early_per_cpu(_name, _cpu)                      \
                    (early_per_cpu_ptr(_name) ?                     \
                            early_per_cpu_ptr(_name)[_cpu] :        \
                            per_cpu(_name, _cpu))
    
        A better method is to actually check the pointer manually.  In the
        case below, numa_set_node can be called both "early" and "late":
    
            void __cpuinit numa_set_node(int cpu, int node)
            {
                int *cpu_to_node_map = early_per_cpu_ptr(x86_cpu_to_node_map);
    
                if (cpu_to_node_map)
                        cpu_to_node_map[cpu] = node;
                else
                        per_cpu(x86_cpu_to_node_map, cpu) = node;
            }
    
      * Add a flag "arch_provides_topology_pointers" that indicates pointers
        to topology cpumask_t maps are available.  Otherwise, use the function
        returning the cpumask_t value.  This is useful if cpumask_t set size
        is very large to avoid copying data on to/off of the stack.
    
      * The coverage of CONFIG_DEBUG_PER_CPU_MAPS has been increased while
        the non-debug case has been optimized a bit.
    
      * Remove an unreferenced compiler warning in drivers/base/topology.c
    
      * Clean up #ifdef in setup.c
    
    For inclusion into sched-devel/latest tree.
    
    Based on:
            git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6.git
        +   sched-devel/latest  .../mingo/linux-2.6-sched-devel.git
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3e1cecedde42..036604d3daed 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -68,22 +68,6 @@
 #include <mach_wakecpu.h>
 #include <smpboot_hooks.h>
 
-/*
- * FIXME: For x86_64, those are defined in other files. But moving them here,
- * would make the setup areas dependent on smp, which is a loss. When we
- * integrate apic between arches, we can probably do a better job, but
- * right now, they'll stay here -- glommer
- */
-
-/* which logical CPU number maps to which CPU (physical APIC ID) */
-u16 x86_cpu_to_apicid_init[NR_CPUS] __initdata =
-			{ [0 ... NR_CPUS-1] = BAD_APICID };
-void *x86_cpu_to_apicid_early_ptr;
-
-u16 x86_bios_cpu_apicid_init[NR_CPUS] __initdata
-				= { [0 ... NR_CPUS-1] = BAD_APICID };
-void *x86_bios_cpu_apicid_early_ptr;
-
 #ifdef CONFIG_X86_32
 u8 apicid_2_node[MAX_APICID];
 static int low_mappings;
@@ -992,7 +976,7 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 		/* Try to put things back the way they were before ... */
 		unmap_cpu_to_logical_apicid(cpu);
 #ifdef CONFIG_X86_64
-		clear_node_cpumask(cpu); /* was set by numa_add_cpu */
+		numa_remove_cpu(cpu); /* was set by numa_add_cpu */
 #endif
 		cpu_clear(cpu, cpu_callout_map); /* was set by do_boot_cpu() */
 		cpu_clear(cpu, cpu_initialized); /* was set by cpu_init() */
@@ -1373,7 +1357,7 @@ static void __ref remove_cpu_from_maps(int cpu)
 	cpu_clear(cpu, cpu_callin_map);
 	/* was set by cpu_init() */
 	clear_bit(cpu, (unsigned long *)&cpu_initialized);
-	clear_node_cpumask(cpu);
+	numa_remove_cpu(cpu);
 #endif
 }
 

commit 3de352bbd86f890dd0c5e1c09a6a1b0b29e0f8ce
Merge: 1b8ba39a3fad 9340e1ccdf7b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 8 11:14:58 2008 +0200

    Merge branch 'x86/mpparse' into x86/devel
    
    Conflicts:
    
            arch/x86/Kconfig
            arch/x86/kernel/io_apic_32.c
            arch/x86/kernel/setup_64.c
            arch/x86/mm/init_32.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 896395c290f902576270d84291c1f7f8bfbe339d
Merge: af1cf204ba2f 1b40a895df6c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 8 10:32:56 2008 +0200

    Merge branch 'linus' into tmp.x86.mpparse.new

commit 6924d1ab8b7bbe5ab416713f5701b3316b2df85b
Merge: 4e78c91abe1a 25556c1699ad b764a15f6799 437a0a54eea7 41b3eae669fb 84e65b0a84a2 684eb0163a98 93022136fff9 5cb04df8d3f0 44974c8fc1d7 48cf937f48f6 205f93288093 c54f9da1c8ce 0ed368c71aa6 b478458aeebf 2d144e63098b 607baf1f4ef9 33af9039cbf6 3557b18fcbe0 63687a528c39 009b9fc98ddd f6477cc76c73 e6b0edef3453 400d34944c4a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 8 09:16:56 2008 +0200

    Merge branches 'x86/numa-fixes', 'x86/apic', 'x86/apm', 'x86/bitops', 'x86/build', 'x86/cleanups', 'x86/cpa', 'x86/cpu', 'x86/defconfig', 'x86/gart', 'x86/i8259', 'x86/intel', 'x86/irqstats', 'x86/kconfig', 'x86/ldt', 'x86/mce', 'x86/memtest', 'x86/pat', 'x86/ptemask', 'x86/resumetrace', 'x86/threadinfo', 'x86/timers', 'x86/vdso' and 'x86/xen' into x86/devel

commit d54db1ac9ecde9bcb8a561595b02c1d970d3a4d6
Author: Maciej W. Rozycki <macro@linux-mips.org>
Date:   Fri Jun 6 03:28:02 2008 +0100

    x86: APIC/SMP: Downgrade the NMI watchdog for "nosmp"
    
     If configured to use the I/O APIC, the NMI watchdog is deemed to fail if
    the chip has been deactivated as a result of "nosmp".  Downgrade to the
    local APIC watchdog similarly to what is done for the UP case.
    
    Signed-off-by: Maciej W. Rozycki <macro@linux-mips.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8c53d86e3e86..f3760349c84f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1159,6 +1159,9 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	if (!max_cpus) {
 		printk(KERN_INFO "SMP mode deactivated.\n");
 		smpboot_clear_io_apic();
+
+		localise_nmi_watchdog();
+
 #ifdef CONFIG_X86_32
 		connect_bsp_APIC();
 #endif

commit 6fe9fe875691e15eda61b992e03257e68aa5ba4f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jun 8 10:14:40 2008 +0200

    Revert "x86: APIC/SMP: downgrade the NMI watchdog for "nosmp""
    
    This reverts commit 791b93d3dfaf16c23e978bec0cc0a3dd9d855d63.
    
    A better fix from Maciej will be merged.

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index df60bc7c9cb7..8c53d86e3e86 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1159,10 +1159,6 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	if (!max_cpus) {
 		printk(KERN_INFO "SMP mode deactivated.\n");
 		smpboot_clear_io_apic();
-
-		if (nmi_watchdog != NMI_NONE && nmi_watchdog != NMI_DISABLED)
-			nmi_watchdog = NMI_LOCAL_APIC;
-
 #ifdef CONFIG_X86_32
 		connect_bsp_APIC();
 #endif

commit a1133d8e4ffc2db751eb987a2f3cf8ead67927c3
Author: Maciej W. Rozycki <macro@linux-mips.org>
Date:   Wed May 21 22:10:16 2008 +0100

    x86: APIC/SMP: downgrade the NMI watchdog for "nosmp"
    
    If configured to use the I/O APIC, the NMI watchdog is deemed to fail if
    the chip has been deactivated as a result of "nosmp".  Downgrade to the
    local APIC watchdog similarly to what is done for the UP case.
    
    Signed-off-by: Maciej W. Rozycki <macro@linux-mips.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8c53d86e3e86..df60bc7c9cb7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1159,6 +1159,10 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	if (!max_cpus) {
 		printk(KERN_INFO "SMP mode deactivated.\n");
 		smpboot_clear_io_apic();
+
+		if (nmi_watchdog != NMI_NONE && nmi_watchdog != NMI_DISABLED)
+			nmi_watchdog = NMI_LOCAL_APIC;
+
 #ifdef CONFIG_X86_32
 		connect_bsp_APIC();
 #endif

commit 73d08e636026bbcb413d4864ca5e917502f8a0f9
Author: Maciej W. Rozycki <macro@linux-mips.org>
Date:   Wed May 21 22:09:43 2008 +0100

    x86: APIC/SMP: correct the message for "nosmp"
    
    The local APIC is no longer forced off when "nosmp" has been specified.
    Correct the message printed.
    
    Signed-off-by: Maciej W. Rozycki <macro@linux-mips.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3e1cecedde42..8c53d86e3e86 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1157,8 +1157,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	 * If SMP should be disabled, then really disable it!
 	 */
 	if (!max_cpus) {
-		printk(KERN_INFO "SMP mode deactivated,"
-				 "forcing use of dummy APIC emulation.\n");
+		printk(KERN_INFO "SMP mode deactivated.\n");
 		smpboot_clear_io_apic();
 #ifdef CONFIG_X86_32
 		connect_bsp_APIC();

commit 68083e05d72d94f347293d8cc0067050ba904bfa
Merge: 7baac8b91f98 b7279469d66b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jul 6 14:23:39 2008 +0200

    Merge commit 'v2.6.26-rc9' into cpus4096

commit fcb43042ef55d2f46b0efa5d7746967cef38f056
Author: Zhang, Yanmin <yanmin_zhang@linux.intel.com>
Date:   Tue Jun 24 16:06:23 2008 +0800

    x86: fix cpu hotplug crash
    
    Vegard Nossum reported crashes during cpu hotplug tests:
    
      http://marc.info/?l=linux-kernel&m=121413950227884&w=4
    
    In function _cpu_up, the panic happens when calling
    __raw_notifier_call_chain at the second time. Kernel doesn't panic when
    calling it at the first time. If just say because of nr_cpu_ids, that's
    not right.
    
    By checking the source code, I found that function do_boot_cpu is the culprit.
    Consider below call chain:
     _cpu_up=>__cpu_up=>smp_ops.cpu_up=>native_cpu_up=>do_boot_cpu.
    
    So do_boot_cpu is called in the end. In do_boot_cpu, if
    boot_error==true, cpu_clear(cpu, cpu_possible_map) is executed. So later
    on, when _cpu_up calls __raw_notifier_call_chain at the second time to
    report CPU_UP_CANCELED, because this cpu is already cleared from
    cpu_possible_map, get_cpu_sysdev returns NULL.
    
    Many resources are related to cpu_possible_map, so it's better not to
    change it.
    
    Below patch against 2.6.26-rc7 fixes it by removing the bit clearing in
    cpu_possible_map.
    
    Signed-off-by: Zhang Yanmin <yanmin_zhang@linux.intel.com>
    Tested-by: Vegard Nossum <vegard.nossum@gmail.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 56078d61c793..3e1cecedde42 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -996,7 +996,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 #endif
 		cpu_clear(cpu, cpu_callout_map); /* was set by do_boot_cpu() */
 		cpu_clear(cpu, cpu_initialized); /* was set by cpu_init() */
-		cpu_clear(cpu, cpu_possible_map);
 		cpu_clear(cpu, cpu_present_map);
 		per_cpu(x86_cpu_to_apicid, cpu) = BAD_APICID;
 	}

commit 3b16cf874861436725c43ba0b68bdd799297be7c
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jun 26 11:21:54 2008 +0200

    x86: convert to generic helpers for IPI function calls
    
    This converts x86, x86-64, and xen to use the new helpers for
    smp_call_function() and friends, and adds support for
    smp_call_function_single().
    
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 56078d61c793..89647898f546 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -345,7 +345,7 @@ static void __cpuinit start_secondary(void *unused)
 	 * lock helps us to not include this cpu in a currently in progress
 	 * smp_call_function().
 	 */
-	lock_ipi_call_lock();
+	ipi_call_lock_irq();
 #ifdef CONFIG_X86_64
 	spin_lock(&vector_lock);
 
@@ -357,7 +357,7 @@ static void __cpuinit start_secondary(void *unused)
 	spin_unlock(&vector_lock);
 #endif
 	cpu_set(smp_processor_id(), cpu_online_map);
-	unlock_ipi_call_lock();
+	ipi_call_unlock_irq();
 	per_cpu(cpu_state, smp_processor_id()) = CPU_ONLINE;
 
 	setup_secondary_clock();

commit 1791a78c0b10fe548bf08a2ed7f84a4ea1385430
Merge: bf07dc864902 066519068ad2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jun 16 11:17:50 2008 +0200

    Merge branch 'linus' into x86/cleanups

commit deef325086c3897393b8f7d6bccd03405244fe18
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 15:44:38 2008 +0200

    x86: disable preemption in native_smp_prepare_cpus
    
    Priit Laes reported the following warning:
    
    Call Trace:
     [<ffffffff8022f1e1>] warn_on_slowpath+0x51/0x63
     [<ffffffff80282e48>] sys_ioctl+0x2d/0x5d
     [<ffffffff805185ff>] _spin_lock+0xe/0x24
     [<ffffffff80227459>] task_rq_lock+0x3d/0x73
     [<ffffffff805133c3>] set_cpu_sibling_map+0x336/0x350
     [<ffffffff8021c1b8>] read_apic_id+0x30/0x62
     [<ffffffff806d921d>] verify_local_APIC+0x90/0x138
     [<ffffffff806d84b5>] native_smp_prepare_cpus+0x1f9/0x305
     [<ffffffff806ce7b1>] kernel_init+0x59/0x2d9
     [<ffffffff80518a26>] _spin_unlock_irq+0x11/0x2b
     [<ffffffff8020bf48>] child_rip+0xa/0x12
     [<ffffffff806ce758>] kernel_init+0x0/0x2d9
     [<ffffffff8020bf3e>] child_rip+0x0/0x12
    
    fix this by generally disabling preemption in native_smp_prepare_cpus().
    
    Reported-and-bisected-by: Priit Laes <plaes@plaes.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 38988491c622..56078d61c793 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1190,6 +1190,7 @@ static void __init smp_cpu_index_default(void)
  */
 void __init native_smp_prepare_cpus(unsigned int max_cpus)
 {
+	preempt_disable();
 	nmi_watchdog_default();
 	smp_cpu_index_default();
 	current_cpu_data = boot_cpu_data;
@@ -1206,7 +1207,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	if (smp_sanity_check(max_cpus) < 0) {
 		printk(KERN_INFO "SMP disabled\n");
 		disable_smp();
-		return;
+		goto out;
 	}
 
 	preempt_disable();
@@ -1246,6 +1247,8 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	printk(KERN_INFO "CPU%d: ", 0);
 	print_cpu_info(&cpu_data(0));
 	setup_boot_clock();
+out:
+	preempt_enable();
 }
 /*
  * Early setup to make printk work.

commit a4c81cf684350797939416c99effb9d3ae46bca6
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun May 18 01:18:57 2008 -0700

    x86: extend e820 ealy_res support 32bit
    
    move early_res related from e820_64.c to e820.c
    make edba detection to be done in head32.c
    remove smp_alloc_memory, because we have fixed trampoline address now.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    
     arch/x86/kernel/e820.c              |  214 ++++++++++++++++++++++++++++++++++++
     arch/x86/kernel/e820_64.c           |  196 --------------------------------
     arch/x86/kernel/head32.c            |   76 ++++++++++++
     arch/x86/kernel/setup_32.c          |  109 +++---------------
     arch/x86/kernel/smpboot.c           |   17 --
     arch/x86/kernel/trampoline.c        |    2
     arch/x86/mach-voyager/voyager_smp.c |    9 -
     include/asm-x86/e820.h              |    6 +
     include/asm-x86/e820_64.h           |    9 -
     include/asm-x86/smp.h               |    1
     arch/x86/kernel/e820.c              |  214 ++++++++++++++++++++++++++++++++++++
     arch/x86/kernel/e820_64.c           |  196 --------------------------------
     arch/x86/kernel/head32.c            |   76 ++++++++++++
     arch/x86/kernel/setup_32.c          |  109 +++---------------
     arch/x86/kernel/smpboot.c           |   17 --
     arch/x86/kernel/trampoline.c        |    2
     arch/x86/mach-voyager/voyager_smp.c |    9 -
     include/asm-x86/e820.h              |    6 +
     include/asm-x86/e820_64.h           |    9 -
     include/asm-x86/smp.h               |    1
     arch/x86/kernel/e820.c              |  214 ++++++++++++++++++++++++++++++++++++
     arch/x86/kernel/e820_64.c           |  196 --------------------------------
     arch/x86/kernel/head32.c            |   76 ++++++++++++
     arch/x86/kernel/setup_32.c          |  109 +++---------------
     arch/x86/kernel/smpboot.c           |   17 --
     arch/x86/kernel/trampoline.c        |    2
     arch/x86/mach-voyager/voyager_smp.c |    9 -
     include/asm-x86/e820.h              |    6 +
     include/asm-x86/e820_64.h           |    9 -
     include/asm-x86/smp.h               |    1
     10 files changed, 320 insertions(+), 319 deletions(-)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 38988491c622..843722e2b79e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -555,23 +555,6 @@ cpumask_t cpu_coregroup_map(int cpu)
 		return c->llc_shared_map;
 }
 
-#ifdef CONFIG_X86_32
-/*
- * We are called very early to get the low memory for the
- * SMP bootup trampoline page.
- */
-void __init smp_alloc_memory(void)
-{
-	trampoline_base = alloc_bootmem_low_pages(PAGE_SIZE);
-	/*
-	 * Has to be in very low memory so we can execute
-	 * real-mode AP code.
-	 */
-	if (__pa(trampoline_base) >= 0x9F000)
-		BUG();
-}
-#endif
-
 static void impress_friends(void)
 {
 	int cpu;

commit 883b7af932b4435eb4798cfa4fec0848639c2a87
Author: Huang Weiyi <weiyi.huang@gmail.com>
Date:   Sun May 11 19:36:57 2008 +0800

    x86: smpboot.c: removed duplicated include
    
    Removed duplicated include <asm/nmi.h> in
    arch/x86/kernel/smpboot.c.
    
    Signed-off-by: Huang Weiyi <weiyi.huang@gmail.com>
    Cc: mingo@redhat.com
    Cc: hpa@zytor.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 38988491c622..0974fc0997b9 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -59,7 +59,6 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mtrr.h>
-#include <asm/nmi.h>
 #include <asm/vmi.h>
 #include <asm/genapic.h>
 #include <linux/mc146818rtc.h>

commit 334ef7a7ab8f80b689a2be95d5e62d2167900865
Author: Mike Travis <travis@sgi.com>
Date:   Mon May 12 21:21:13 2008 +0200

    x86: use performance variant for_each_cpu_mask_nr
    
    Change references from for_each_cpu_mask to for_each_cpu_mask_nr
    where appropriate
    
    Reviewed-by: Paul Jackson <pj@sgi.com>
    Reviewed-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    commit 2d474871e2fb092eb46a0930aba5442e10eb96cc
    Author: Mike Travis <travis@sgi.com>
    Date:   Mon May 12 21:21:13 2008 +0200

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 38988491c622..fff8ebaa554f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -487,7 +487,7 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 	cpu_set(cpu, cpu_sibling_setup_map);
 
 	if (smp_num_siblings > 1) {
-		for_each_cpu_mask(i, cpu_sibling_setup_map) {
+		for_each_cpu_mask_nr(i, cpu_sibling_setup_map) {
 			if (c->phys_proc_id == cpu_data(i).phys_proc_id &&
 			    c->cpu_core_id == cpu_data(i).cpu_core_id) {
 				cpu_set(i, per_cpu(cpu_sibling_map, cpu));
@@ -510,7 +510,7 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 		return;
 	}
 
-	for_each_cpu_mask(i, cpu_sibling_setup_map) {
+	for_each_cpu_mask_nr(i, cpu_sibling_setup_map) {
 		if (per_cpu(cpu_llc_id, cpu) != BAD_APICID &&
 		    per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, i)) {
 			cpu_set(i, c->llc_shared_map);
@@ -1298,7 +1298,7 @@ static void remove_siblinginfo(int cpu)
 	int sibling;
 	struct cpuinfo_x86 *c = &cpu_data(cpu);
 
-	for_each_cpu_mask(sibling, per_cpu(cpu_core_map, cpu)) {
+	for_each_cpu_mask_nr(sibling, per_cpu(cpu_core_map, cpu)) {
 		cpu_clear(cpu, per_cpu(cpu_core_map, sibling));
 		/*/
 		 * last thread sibling in this cpu core going down
@@ -1307,7 +1307,7 @@ static void remove_siblinginfo(int cpu)
 			cpu_data(sibling).booted_cores--;
 	}
 
-	for_each_cpu_mask(sibling, per_cpu(cpu_sibling_map, cpu))
+	for_each_cpu_mask_nr(sibling, per_cpu(cpu_sibling_map, cpu))
 		cpu_clear(cpu, per_cpu(cpu_sibling_map, sibling));
 	cpus_clear(per_cpu(cpu_sibling_map, cpu));
 	cpus_clear(per_cpu(cpu_core_map, cpu));

commit 61165d7a035f6571c7576e7f51e7230157724c8d
Author: Hugh Dickins <hugh@veritas.com>
Date:   Tue May 13 14:26:57 2008 +0100

    x86: fix app crashes after SMP resume
    
    After resume on a 2cpu laptop, kernel builds collapse with a sed hang,
    sh or make segfault (often on 20295564), real-time signal to cc1 etc.
    
    Several hurdles to jump, but a manually-assisted bisect led to -rc1's
    d2bcbad5f3ad38a1c09861bca7e252dde7bb8259 x86: do not zap_low_mappings
    in __smp_prepare_cpus.  Though the low mappings were removed at bootup,
    they were left behind (with Global flags helping to keep them in TLB)
    after resume or cpu online, causing the crashes seen.
    
    Reinstate zap_low_mappings (with local __flush_tlb_all) for each cpu_up
    on x86_32.  This used to be serialized by smp_commenced_mask: that's now
    gone, but a low_mappings flag will do.  No need for native_smp_cpus_done
    to repeat the zap: let mem_init zap BSP's low mappings just like on UP.
    
    (In passing, fix error code from native_cpu_up: do_boot_cpu returns a
    variety of diagnostic values, Dprintk what it says but convert to -EIO.
    And save_pg_dir separately before zap_low_mappings: doesn't matter now,
    but zapping twice in succession wiped out resume's swsusp_pg_dir.)
    
    That worked well on the duo and one quad, but wouldn't boot 3rd or 4th
    cpu on P4 Xeon, oopsing just after unlock_ipi_call_lock.  The TLB flush
    IPI now being sent reveals a long-standing bug: the booting cpu has its
    APIC readied in smp_callin at the top of start_secondary, but isn't put
    into the cpu_online_map until just before that unlock_ipi_call_lock.
    
    So native_smp_call_function_mask to online cpus would send_IPI_allbutself,
    including the cpu just coming up, though it has been excluded from the
    count to wait for: by the time it handles the IPI, the call data on
    native_smp_call_function_mask's stack may well have been overwritten.
    
    So fall back to send_IPI_mask while cpu_online_map does not match
    cpu_callout_map: perhaps there's a better APICological fix to be
    made at the start_secondary end, but I wouldn't know that.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6b087ab6cd8f..38988491c622 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -86,6 +86,7 @@ void *x86_bios_cpu_apicid_early_ptr;
 
 #ifdef CONFIG_X86_32
 u8 apicid_2_node[MAX_APICID];
+static int low_mappings;
 #endif
 
 /* State of each CPU */
@@ -326,6 +327,12 @@ static void __cpuinit start_secondary(void *unused)
 		enable_8259A_irq(0);
 	}
 
+#ifdef CONFIG_X86_32
+	while (low_mappings)
+		cpu_relax();
+	__flush_tlb_all();
+#endif
+
 	/* This must be done before setting cpu_online_map */
 	set_cpu_sibling_map(raw_smp_processor_id());
 	wmb();
@@ -1040,14 +1047,20 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 #ifdef CONFIG_X86_32
 	/* init low mem mapping */
 	clone_pgd_range(swapper_pg_dir, swapper_pg_dir + KERNEL_PGD_BOUNDARY,
-			min_t(unsigned long, KERNEL_PGD_PTRS, KERNEL_PGD_BOUNDARY));
+		min_t(unsigned long, KERNEL_PGD_PTRS, KERNEL_PGD_BOUNDARY));
 	flush_tlb_all();
-#endif
+	low_mappings = 1;
 
 	err = do_boot_cpu(apicid, cpu);
-	if (err < 0) {
+
+	zap_low_mappings();
+	low_mappings = 0;
+#else
+	err = do_boot_cpu(apicid, cpu);
+#endif
+	if (err) {
 		Dprintk("do_boot_cpu failed %d\n", err);
-		return err;
+		return -EIO;
 	}
 
 	/*
@@ -1259,9 +1272,6 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 	setup_ioapic_dest();
 #endif
 	check_nmi_watchdog();
-#ifdef CONFIG_X86_32
-	zap_low_mappings();
-#endif
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit c5562faeaacf19e81a78ee37cc6b96ab1f3e68e4
Author: Adrian Bunk <bunk@kernel.org>
Date:   Tue Apr 22 00:31:37 2008 +0300

    x86: make additional_cpus static
    
    This patch makes the needlessly global additional_cpus static.
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b78e4d47a42b..6b087ab6cd8f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1306,7 +1306,7 @@ static void remove_siblinginfo(int cpu)
 	cpu_clear(cpu, cpu_sibling_setup_map);
 }
 
-int additional_cpus __initdata = -1;
+static int additional_cpus __initdata = -1;
 
 static __init int setup_additional_cpus(char *s)
 {

commit dbe55f4797712f86691a0ee0b5f508693c7310fe
Author: Adrian Bunk <bunk@kernel.org>
Date:   Tue Apr 22 01:50:26 2008 +0300

    x86: make start_secondary() static
    
    start_secondary() needlessly became global.
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 84241a256dc8..b78e4d47a42b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -299,7 +299,7 @@ static void __cpuinit smp_callin(void)
 /*
  * Activate a secondary processor.
  */
-void __cpuinit start_secondary(void *unused)
+static void __cpuinit start_secondary(void *unused)
 {
 	/*
 	 * Don't put *anything* before cpu_init(), SMP booting is too

commit e90955c26d8af318658c45caadb1d330ac6a506c
Author: Jesse Barnes <jbarnes@virtuousgeek.org>
Date:   Mon Apr 21 14:14:44 2008 -0700

    x86: fix PCI MSI breaks when booting with nosmp
    
    set up sane APIC state even in the nosmp case.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 04c662ba18f1..84241a256dc8 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1149,14 +1149,10 @@ static int __init smp_sanity_check(unsigned max_cpus)
 				 "forcing use of dummy APIC emulation.\n");
 		smpboot_clear_io_apic();
 #ifdef CONFIG_X86_32
-		if (nmi_watchdog == NMI_LOCAL_APIC) {
-			printk(KERN_INFO "activating minimal APIC for"
-					 "NMI watchdog use.\n");
-			connect_bsp_APIC();
-			setup_local_APIC();
-			end_local_APIC_setup();
-		}
+		connect_bsp_APIC();
 #endif
+		setup_local_APIC();
+		end_local_APIC_setup();
 		return -1;
 	}
 

commit 7c04e64a1b43b4c8fea281ce1f82df30ed9bab4e
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sat Apr 19 23:55:17 2008 +0900

    x86: use cpumask function for present, possible, and online cpus
    
    cpu_online(), cpu_present(), for_each_possible_cpu(), num_possible_cpus()
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index eef79e84145f..04c662ba18f1 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1058,7 +1058,7 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 	check_tsc_sync_source(cpu);
 	local_irq_restore(flags);
 
-	while (!cpu_isset(cpu, cpu_online_map)) {
+	while (!cpu_online(cpu)) {
 		cpu_relax();
 		touch_nmi_watchdog();
 	}
@@ -1168,7 +1168,7 @@ static void __init smp_cpu_index_default(void)
 	int i;
 	struct cpuinfo_x86 *c;
 
-	for_each_cpu_mask(i, cpu_possible_map) {
+	for_each_possible_cpu(i) {
 		c = &cpu_data(i);
 		/* mark all to hotplug */
 		c->cpu_index = NR_CPUS;

commit 4b7227ca321ccf447cdc04538687c895db8b77f5
Merge: 5dae61b80564 1775826ceec5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 25 12:32:10 2008 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/x86/linux-2.6-xen-next
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/x86/linux-2.6-xen-next: (52 commits)
      xen: add balloon driver
      xen: allow compilation with non-flat memory
      xen: fold xen_sysexit into xen_iret
      xen: allow set_pte_at on init_mm to be lockless
      xen: disable preemption during tlb flush
      xen pvfb: Para-virtual framebuffer, keyboard and pointer driver
      xen: Add compatibility aliases for frontend drivers
      xen: Module autoprobing support for frontend drivers
      xen blkfront: Delay wait for block devices until after the disk is added
      xen/blkfront: use bdget_disk
      xen: Make xen-blkfront write its protocol ABI to xenstore
      xen: import arch generic part of xencomm
      xen: make grant table arch portable
      xen: replace callers of alloc_vm_area()/free_vm_area() with xen_ prefixed one
      xen: make include/xen/page.h portable moving those definitions under asm dir
      xen: add resend_irq_on_evtchn() definition into events.c
      Xen: make events.c portable for ia64/xen support
      xen: move events.c to drivers/xen for IA64/Xen support
      xen: move features.c from arch/x86/xen/features.c to drivers/xen
      xen: add missing definitions in include/xen/interface/vcpu.h which ia64/xen needs
      ...

commit 68db065c845bd9d0eb96946ab104b4c82d0ae9da
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Mar 17 16:37:13 2008 -0700

    x86: unify KERNEL_PGD_PTRS
    
    Make KERNEL_PGD_PTRS common, as previously it was only being defined
    for 32-bit.
    
    There are a couple of follow-on changes from this:
     - KERNEL_PGD_PTRS was being defined in terms of USER_PGD_PTRS.  The
       definition of USER_PGD_PTRS doesn't really make much sense on x86-64,
       since it can have two different user address-space configurations.
       I renamed USER_PGD_PTRS to KERNEL_PGD_BOUNDARY, which is meaningful
       for all of 32/32, 32/64 and 64/64 process configurations.
    
     - USER_PTRS_PER_PGD was also defined and was being used for similar
       purposes.  Converting its users to KERNEL_PGD_BOUNDARY left it
       completely unused, and so I removed it.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Zach Amsden <zach@vmware.com>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6a925394bc7e..2de2f7a2ed5d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1039,8 +1039,8 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 
 #ifdef CONFIG_X86_32
 	/* init low mem mapping */
-	clone_pgd_range(swapper_pg_dir, swapper_pg_dir + USER_PGD_PTRS,
-			min_t(unsigned long, KERNEL_PGD_PTRS, USER_PGD_PTRS));
+	clone_pgd_range(swapper_pg_dir, swapper_pg_dir + KERNEL_PGD_BOUNDARY,
+			min_t(unsigned long, KERNEL_PGD_PTRS, KERNEL_PGD_BOUNDARY));
 	flush_tlb_all();
 #endif
 

commit a4928cffe6435caf427ae673131a633c1329dbf3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 23 13:20:56 2008 +0200

    "make namespacecheck" fixes
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6a925394bc7e..ade371f9663a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -184,7 +184,7 @@ static void unmap_cpu_to_node(int cpu)
 u8 cpu_2_logical_apicid[NR_CPUS] __read_mostly =
 					{ [0 ... NR_CPUS-1] = BAD_APICID };
 
-void map_cpu_to_logical_apicid(void)
+static void map_cpu_to_logical_apicid(void)
 {
 	int cpu = smp_processor_id();
 	int apicid = logical_smp_processor_id();
@@ -197,7 +197,7 @@ void map_cpu_to_logical_apicid(void)
 	map_cpu_to_node(cpu, node);
 }
 
-void unmap_cpu_to_logical_apicid(int cpu)
+static void unmap_cpu_to_logical_apicid(int cpu)
 {
 	cpu_2_logical_apicid[cpu] = BAD_APICID;
 	unmap_cpu_to_node(cpu);
@@ -211,7 +211,7 @@ void unmap_cpu_to_logical_apicid(int cpu)
  * Report back to the Boot Processor.
  * Running on AP.
  */
-void __cpuinit smp_callin(void)
+static void __cpuinit smp_callin(void)
 {
 	int cpuid, phys_id;
 	unsigned long timeout;
@@ -436,7 +436,7 @@ static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
 #endif
 }
 
-void __cpuinit smp_checks(void)
+static void __cpuinit smp_checks(void)
 {
 	if (smp_b_stepping)
 		printk(KERN_WARNING "WARNING: SMP operation may be unreliable"
@@ -565,7 +565,7 @@ void __init smp_alloc_memory(void)
 }
 #endif
 
-void impress_friends(void)
+static void impress_friends(void)
 {
 	int cpu;
 	unsigned long bogosum = 0;
@@ -1287,7 +1287,7 @@ void cpu_exit_clear(void)
 }
 #  endif /* CONFIG_X86_32 */
 
-void remove_siblinginfo(int cpu)
+static void remove_siblinginfo(int cpu)
 {
 	int sibling;
 	struct cpuinfo_x86 *c = &cpu_data(cpu);

commit 34d0559178393547505ec9492321255405f4e441
Author: Jack Steiner <steiner@sgi.com>
Date:   Wed Apr 16 11:45:15 2008 -0500

    x86: UV startup of slave cpus
    
    This patch changes smpboot.c so that it can start slave cpus running
    in UV non-unique apicid mode. The SIPI must be sent using a UV-specific
    mechanism.
    
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e6abe8a49b1f..6a925394bc7e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -61,6 +61,7 @@
 #include <asm/mtrr.h>
 #include <asm/nmi.h>
 #include <asm/vmi.h>
+#include <asm/genapic.h>
 #include <linux/mc146818rtc.h>
 
 #include <mach_apic.h>
@@ -677,6 +678,12 @@ wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
 	unsigned long send_status, accept_status = 0;
 	int maxlvt, num_starts, j;
 
+	if (get_uv_system_type() == UV_NON_UNIQUE_APIC) {
+		send_status = uv_wakeup_secondary(phys_apicid, start_eip);
+		atomic_set(&init_deasserted, 1);
+		return send_status;
+	}
+
 	/*
 	 * Be paranoid about clearing APIC errors.
 	 */
@@ -918,16 +925,19 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 
 	atomic_set(&init_deasserted, 0);
 
-	Dprintk("Setting warm reset code and vector.\n");
+	if (get_uv_system_type() != UV_NON_UNIQUE_APIC) {
 
-	store_NMI_vector(&nmi_high, &nmi_low);
+		Dprintk("Setting warm reset code and vector.\n");
 
-	smpboot_setup_warm_reset_vector(start_ip);
-	/*
-	 * Be paranoid about clearing APIC errors.
-	 */
-	apic_write(APIC_ESR, 0);
-	apic_read(APIC_ESR);
+		store_NMI_vector(&nmi_high, &nmi_low);
+
+		smpboot_setup_warm_reset_vector(start_ip);
+		/*
+		 * Be paranoid about clearing APIC errors.
+	 	*/
+		apic_write(APIC_ESR, 0);
+		apic_read(APIC_ESR);
+	}
 
 	/*
 	 * Starting actual IPI sequence...
@@ -966,7 +976,8 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 			else
 				/* trampoline code not run */
 				printk(KERN_ERR "Not responding.\n");
-			inquire_remote_apic(apicid);
+			if (get_uv_system_type() != UV_NON_UNIQUE_APIC)
+				inquire_remote_apic(apicid);
 		}
 	}
 

commit 77ad386e596c6b0930cc2e09e3cce485e3ee7f72
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 21 15:23:19 2008 +0100

    x86: standalone trampoline code
    
    move the trampoline setup code out of smpboot.c - UP kernels can have
    suspend support too.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 424600e671bd..e6abe8a49b1f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -140,9 +140,6 @@ static atomic_t init_deasserted;
 
 static int boot_cpu_logical_apicid;
 
-/* ready for x86_64, no harm for x86, since it will overwrite after alloc */
-unsigned char *trampoline_base = __va(TRAMPOLINE_BASE);
-
 /* representing cpus for which sibling maps can be computed */
 static cpumask_t cpu_sibling_setup_map;
 
@@ -550,18 +547,6 @@ cpumask_t cpu_coregroup_map(int cpu)
 		return c->llc_shared_map;
 }
 
-/*
- * Currently trivial. Write the real->protected mode
- * bootstrap into the page concerned. The caller
- * has made sure it's suitably aligned.
- */
-unsigned long setup_trampoline(void)
-{
-	memcpy(trampoline_base, trampoline_data,
-	       trampoline_end - trampoline_data);
-	return virt_to_phys(trampoline_base);
-}
-
 #ifdef CONFIG_X86_32
 /*
  * We are called very early to get the low memory for the

commit e44b7b7525ad9d43163ab5e60c784325419e0ea6
Author: Pavel Machek <pavel@suse.cz>
Date:   Thu Apr 10 23:28:10 2008 +0200

    x86: move suspend wakeup code to C
    
    Move wakeup code to .c, so that video mode setting code can be shared
    between boot and wakeup. Remove nasty assembly code in 64-bit case by
    re-using trampoline code. Stack setup was fixed to clear high 16bits
    of %esp, maybe that fixes some machines.
    
    .c code sharing and morse code was done H. Peter Anvin, Sam Ravnborg
    reviewed kbuild related stuff, and it seems okay to him. Rafael did
    some cleanups.
    
    [rjw:
    * Made the patch stop breaking compilation on x86-32
    * Added arch/x86/kernel/acpi/sleep.h
    * Got rid of compiler warnings in arch/x86/kernel/acpi/sleep.c
    * Fixed 32-bit compilation on x86-64 systems
    * Added include/asm-x86/trampoline.h and fixed the non-SMP
      compilation on 64-bit x86
    * Removed arch/x86/kernel/acpi/sleep_32.c which was not used
    * Fixed some breakage caused by the integration of smpboot.c done
      under us in the meantime]
    
    Signed-off-by: Pavel Machek <pavel@suse.cz>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Reviewed-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ca3929b16049..424600e671bd 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -53,6 +53,7 @@
 #include <asm/nmi.h>
 #include <asm/irq.h>
 #include <asm/smp.h>
+#include <asm/trampoline.h>
 #include <asm/cpu.h>
 #include <asm/numa.h>
 #include <asm/pgtable.h>
@@ -140,7 +141,7 @@ static atomic_t init_deasserted;
 static int boot_cpu_logical_apicid;
 
 /* ready for x86_64, no harm for x86, since it will overwrite after alloc */
-unsigned char *trampoline_base = __va(SMP_TRAMPOLINE_BASE);
+unsigned char *trampoline_base = __va(TRAMPOLINE_BASE);
 
 /* representing cpus for which sibling maps can be computed */
 static cpumask_t cpu_sibling_setup_map;
@@ -554,8 +555,7 @@ cpumask_t cpu_coregroup_map(int cpu)
  * bootstrap into the page concerned. The caller
  * has made sure it's suitably aligned.
  */
-
-unsigned long __cpuinit setup_trampoline(void)
+unsigned long setup_trampoline(void)
 {
 	memcpy(trampoline_base, trampoline_data,
 	       trampoline_end - trampoline_data);

commit df96323dfaebdf7e17cdf0656096e6ab2158ec76
Author: Jacek Luczak <difrost.kernel@gmail.com>
Date:   Fri Apr 11 13:28:37 2008 +0200

    x86: section mismatch fixes, #1
    
    This patch fixes mismatch warnings in smp_checks() (in arch/x86/kernel/smpboot.c):
    
    WARNING: arch/x86/kernel/built-in.o(.text+0x11922): Section mismatch in reference from the function smp_checks()
    to the variable .cpuinit.data:smp_b_stepping
    The function smp_checks() references
    the variable __cpuinitdata smp_b_stepping.
    This is often because smp_checks lacks a __cpuinitdata
    annotation or the annotation of smp_b_stepping is wrong.
    
    Signed-off-by: Jacek Luczak <luczak.jacek@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 4517d1c01eb5..ca3929b16049 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -437,7 +437,7 @@ static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
 #endif
 }
 
-void smp_checks(void)
+void __cpuinit smp_checks(void)
 {
 	if (smp_b_stepping)
 		printk(KERN_WARNING "WARNING: SMP operation may be unreliable"

commit 63d38198a0f57dca87e6cb79931c7bedbb7ab069
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Apr 7 11:38:33 2008 -0700

    x86: fix paranoia about using BIOS quickboot mechanism.
    
    > > Make sure that we clear the "shutdown status flag" in the CMOS
    > > register after each CPU is brought up.  This fixes a problem where the
    > > "shutdown status flag" may remain set when a CPU is brought up after
    > > booting.
    >
    > btw., what problem does this result in, exactly?
    
    The shutdown status flag set to "0xA", corresponds to "JMP double word
    request without INT init".
    
    This JMP at reboot time is at an unintended location. And results in
    Triple faults in our case.
    Though this error at reboot can be safely ignored in a VM environment,
    am not sure what the effect would be on a physical system. May be it
    will result in a triple fault and an eventual hardware reset thus
    masking this BUG in the kernel.
    
    This fix just makes sure that we reset that status flag after
    initialization is done.
    
    Fix paranoia about using BIOS quickboot mechanism.
    
    Make sure that we clear the "shutdown status flag" in the CMOS register
    after each CPU is brought up.  This fixes a problem where the "shutdown
    status flag" may remain set when a CPU is brought up after booting.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Arai <arai@vmware.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 21ad3f396a05..4517d1c01eb5 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1001,6 +1001,11 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	/* mark "stuck" area as not stuck */
 	*((volatile unsigned long *)trampoline_base) = 0;
 
+	/*
+	 * Cleanup possible dangling ends...
+	 */
+	smpboot_restore_warm_reset_vector();
+
 	return boot_error;
 }
 
@@ -1254,11 +1259,6 @@ void __init native_smp_prepare_boot_cpu(void)
 
 void __init native_smp_cpus_done(unsigned int max_cpus)
 {
-	/*
-	 * Cleanup possible dangling ends...
-	 */
-	smpboot_restore_warm_reset_vector();
-
 	Dprintk("Boot done.\n");
 
 	impress_friends();

commit 2fe60147570231cde0d1f14711d2e34ccdf54b65
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Fri Apr 4 23:41:44 2008 +0400

    x86: move up & smp variables to setup.c
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index abf63767cd46..21ad3f396a05 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -86,14 +86,9 @@ void *x86_bios_cpu_apicid_early_ptr;
 u8 apicid_2_node[MAX_APICID];
 #endif
 
-/* Internal processor count */
-unsigned int num_processors;
-
 /* State of each CPU */
 DEFINE_PER_CPU(int, cpu_state) = { 0 };
 
-unsigned disabled_cpus __cpuinitdata;
-
 /* Store all idle threads, this can be reused instead of creating
 * a new thread. Also avoids complicated thread destroy functionality
 * for idle threads.

commit 16ecf7a47cf4f1c97189a551b001195aed550cc2
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Fri Apr 4 23:41:00 2008 +0400

    x86: move x86_bios_cpu_apicid_init to smpboot.c
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e3ea074ba6a4..abf63767cd46 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -78,11 +78,11 @@ u16 x86_cpu_to_apicid_init[NR_CPUS] __initdata =
 			{ [0 ... NR_CPUS-1] = BAD_APICID };
 void *x86_cpu_to_apicid_early_ptr;
 
-#ifdef CONFIG_X86_32
 u16 x86_bios_cpu_apicid_init[NR_CPUS] __initdata
 				= { [0 ... NR_CPUS-1] = BAD_APICID };
 void *x86_bios_cpu_apicid_early_ptr;
 
+#ifdef CONFIG_X86_32
 u8 apicid_2_node[MAX_APICID];
 #endif
 

commit 708650afe98a50d0b280bea9dcf5f160b94ee9fb
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Fri Apr 4 23:40:54 2008 +0400

    x86: move x86_cpu_to_apicid_init to smpboot.c
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7e6aa1c790a2..e3ea074ba6a4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -72,12 +72,13 @@
  * integrate apic between arches, we can probably do a better job, but
  * right now, they'll stay here -- glommer
  */
-#ifdef CONFIG_X86_32
+
 /* which logical CPU number maps to which CPU (physical APIC ID) */
 u16 x86_cpu_to_apicid_init[NR_CPUS] __initdata =
 			{ [0 ... NR_CPUS-1] = BAD_APICID };
 void *x86_cpu_to_apicid_early_ptr;
 
+#ifdef CONFIG_X86_32
 u16 x86_bios_cpu_apicid_init[NR_CPUS] __initdata
 				= { [0 ... NR_CPUS-1] = BAD_APICID };
 void *x86_bios_cpu_apicid_early_ptr;

commit 0fc0906e59df1427d194b78376d15ca48079f6bf
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Fri Apr 4 23:40:48 2008 +0400

    x86: move phys_cpu_present_map to setup.c
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 412061a0bf2b..7e6aa1c790a2 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -88,9 +88,6 @@ u8 apicid_2_node[MAX_APICID];
 /* Internal processor count */
 unsigned int num_processors;
 
-/* Bitmask of physically existing CPUs */
-physid_mask_t phys_cpu_present_map;
-
 /* State of each CPU */
 DEFINE_PER_CPU(int, cpu_state) = { 0 };
 

commit 76eb41319d6ab98d17c81a8001a6d7ed9f8359ee
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Fri Apr 4 23:40:41 2008 +0400

    x86: move x86_cpu_to_apicid to setup.c
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 22bf6c29454f..412061a0bf2b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -77,8 +77,6 @@
 u16 x86_cpu_to_apicid_init[NR_CPUS] __initdata =
 			{ [0 ... NR_CPUS-1] = BAD_APICID };
 void *x86_cpu_to_apicid_early_ptr;
-DEFINE_PER_CPU(u16, x86_cpu_to_apicid) = BAD_APICID;
-EXPORT_PER_CPU_SYMBOL(x86_cpu_to_apicid);
 
 u16 x86_bios_cpu_apicid_init[NR_CPUS] __initdata
 				= { [0 ... NR_CPUS-1] = BAD_APICID };

commit ac23d4ee3f84de33c16ed7e68f9adee2386e74fb
Author: Jack Steiner <steiner@sgi.com>
Date:   Fri Mar 28 14:12:16 2008 -0500

    x86: support for new UV apic
    
    UV supports really big systems. So big, in fact, that the APICID register
    does not contain enough bits to contain an APICID that is unique across all
    cpus.
    
    The UV BIOS supports 3 APICID modes:
    
            - legacy mode. This mode uses the old APIC mode where
              APICID is in bits [31:24] of the APICID register.
    
            - x2apic mode. This mode is whitebox-compatible. APICIDs
              are unique across all cpus. Standard x2apic APIC operations
              (Intel-defined) can be used for IPIs. The node identifier
              fits within the Intel-defined portion of the APICID register.
    
            - x2apic-uv mode. In this mode, the APICIDs on each node have
              unique IDs, but IDs on different node are not unique. For example,
              if each mode has 32 cpus, the APICIDs on each node might be
              0 - 31. Every node has the same set of IDs.
              The UV hub is used to route IPIs/interrupts to the correct node.
              Traditional APIC operations WILL NOT WORK.
    
    In x2apic-uv mode, the ACPI tables all contain a full unique ID (note:
    exact bit layout still changing but the following is close):
    
            nnnnnnnnnnlc0cch
                    n = unique node number
                    l = socket number on board
                    c = core
                    h = hyperthread
    
    Only the "lc0cch" bits are written to the APICID register. The remaining bits are
    supplied by having the get_apic_id() function "OR" the extra bits into the value
    read from the APICID register. (Hmmm.. why not keep the ENTIRE APICID register
    in per-cpu data....)
    
    The x2apic-uv mode is recognized by the MADT table containing:
              oem_id = "SGI"
              oem_table_id = "UV-X"
    
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 5da35d2cdbd8..22bf6c29454f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1101,6 +1101,7 @@ static __init void disable_smp(void)
  */
 static int __init smp_sanity_check(unsigned max_cpus)
 {
+	preempt_disable();
 	if (!physid_isset(hard_smp_processor_id(), phys_cpu_present_map)) {
 		printk(KERN_WARNING "weird, boot CPU (#%d) not listed"
 				    "by the BIOS.\n", hard_smp_processor_id());
@@ -1112,6 +1113,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	 * get out of here now!
 	 */
 	if (!smp_found_config && !acpi_lapic) {
+		preempt_enable();
 		printk(KERN_NOTICE "SMP motherboard not detected.\n");
 		disable_smp();
 		if (APIC_init_uniprocessor())
@@ -1130,6 +1132,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 			boot_cpu_physical_apicid);
 		physid_set(hard_smp_processor_id(), phys_cpu_present_map);
 	}
+	preempt_enable();
 
 	/*
 	 * If we couldn't find a local APIC, then get out of here now!
@@ -1205,11 +1208,13 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		return;
 	}
 
+	preempt_disable();
 	if (GET_APIC_ID(read_apic_id()) != boot_cpu_physical_apicid) {
 		panic("Boot APIC ID in local APIC unexpected (%d vs %d)",
 		     GET_APIC_ID(read_apic_id()), boot_cpu_physical_apicid);
 		/* Or can we switch back to PIC here? */
 	}
+	preempt_enable();
 
 #ifdef CONFIG_X86_32
 	connect_bsp_APIC();

commit 05f2d12c3563dea8c81b301f9f3cf7919af23b13
Author: Jack Steiner <steiner@sgi.com>
Date:   Fri Mar 28 14:12:02 2008 -0500

    x86: change GET_APIC_ID() from an inline function to an out-of-line function
    
    Introduce a function to read the local APIC_ID.
    
    This change is in preparation for additional changes to
    the APICID functions that will come in a later patch.
    
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f45d740b1b6a..5da35d2cdbd8 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -237,7 +237,7 @@ void __cpuinit smp_callin(void)
 	/*
 	 * (This works even if the APIC is not enabled.)
 	 */
-	phys_id = GET_APIC_ID(apic_read(APIC_ID));
+	phys_id = GET_APIC_ID(read_apic_id());
 	cpuid = smp_processor_id();
 	if (cpu_isset(cpuid, cpu_callin_map)) {
 		panic("%s: phys CPU#%d, CPU#%d already present??\n", __func__,
@@ -1205,9 +1205,9 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		return;
 	}
 
-	if (GET_APIC_ID(apic_read(APIC_ID)) != boot_cpu_physical_apicid) {
+	if (GET_APIC_ID(read_apic_id()) != boot_cpu_physical_apicid) {
 		panic("Boot APIC ID in local APIC unexpected (%d vs %d)",
-		     GET_APIC_ID(apic_read(APIC_ID)), boot_cpu_physical_apicid);
+		     GET_APIC_ID(read_apic_id()), boot_cpu_physical_apicid);
 		/* Or can we switch back to PIC here? */
 	}
 

commit 3103623eed1a3ea4a36ee26725842a8038760648
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Thu Mar 27 23:56:06 2008 +0300

    x86: move disabled_cpus to smpboot.c (64bit)
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index fd0bdd36f4ea..f45d740b1b6a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -84,8 +84,6 @@ u16 x86_bios_cpu_apicid_init[NR_CPUS] __initdata
 				= { [0 ... NR_CPUS-1] = BAD_APICID };
 void *x86_bios_cpu_apicid_early_ptr;
 
-unsigned disabled_cpus __cpuinitdata;
-
 u8 apicid_2_node[MAX_APICID];
 #endif
 
@@ -98,6 +96,8 @@ physid_mask_t phys_cpu_present_map;
 /* State of each CPU */
 DEFINE_PER_CPU(int, cpu_state) = { 0 };
 
+unsigned disabled_cpus __cpuinitdata;
+
 /* Store all idle threads, this can be reused instead of creating
 * a new thread. Also avoids complicated thread destroy functionality
 * for idle threads.

commit 7b8cbd2c2f1bf9e3090d3c3fc09330ed1ca28d25
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Thu Mar 27 23:55:59 2008 +0300

    x86: move num_processors to smpboot.c (64 bit)
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index eee7768de2ae..fd0bdd36f4ea 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -84,13 +84,14 @@ u16 x86_bios_cpu_apicid_init[NR_CPUS] __initdata
 				= { [0 ... NR_CPUS-1] = BAD_APICID };
 void *x86_bios_cpu_apicid_early_ptr;
 
-/* Internal processor count */
-unsigned int num_processors;
 unsigned disabled_cpus __cpuinitdata;
 
 u8 apicid_2_node[MAX_APICID];
 #endif
 
+/* Internal processor count */
+unsigned int num_processors;
+
 /* Bitmask of physically existing CPUs */
 physid_mask_t phys_cpu_present_map;
 

commit 1d8554326533568c7e9d5285600c3d0c027b45cc
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Thu Mar 27 23:55:53 2008 +0300

    x86: move phys_cpu_present_map to smpboot.c (64bit)
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7bcee1584b50..eee7768de2ae 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -88,12 +88,12 @@ void *x86_bios_cpu_apicid_early_ptr;
 unsigned int num_processors;
 unsigned disabled_cpus __cpuinitdata;
 
-/* Bitmask of physically existing CPUs */
-physid_mask_t phys_cpu_present_map;
-
 u8 apicid_2_node[MAX_APICID];
 #endif
 
+/* Bitmask of physically existing CPUs */
+physid_mask_t phys_cpu_present_map;
+
 /* State of each CPU */
 DEFINE_PER_CPU(int, cpu_state) = { 0 };
 

commit acff5a768935f7f39e4e3be03940d70c005ffe96
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Thu Mar 27 23:55:16 2008 +0300

    x86: move x86_bios_cpu_apicid to apic_32.c
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d3402e2c57eb..7bcee1584b50 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -83,8 +83,6 @@ EXPORT_PER_CPU_SYMBOL(x86_cpu_to_apicid);
 u16 x86_bios_cpu_apicid_init[NR_CPUS] __initdata
 				= { [0 ... NR_CPUS-1] = BAD_APICID };
 void *x86_bios_cpu_apicid_early_ptr;
-DEFINE_PER_CPU(u16, x86_bios_cpu_apicid) = BAD_APICID;
-EXPORT_PER_CPU_SYMBOL(x86_bios_cpu_apicid);
 
 /* Internal processor count */
 unsigned int num_processors;

commit 53c4c793b30bbf6e1a25cab61790b18f205dd365
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Thu Mar 27 23:54:57 2008 +0300

    x86: move disabled_cpus to smpboot.c
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e1288c2626f8..d3402e2c57eb 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -88,6 +88,7 @@ EXPORT_PER_CPU_SYMBOL(x86_bios_cpu_apicid);
 
 /* Internal processor count */
 unsigned int num_processors;
+unsigned disabled_cpus __cpuinitdata;
 
 /* Bitmask of physically existing CPUs */
 physid_mask_t phys_cpu_present_map;

commit 2bb9e9d7c1b03454665cd99f7d73e67139cdf2e6
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Thu Mar 27 23:54:50 2008 +0300

    x86: move num_processors to smpboot.c
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 8b6eefd9e906..e1288c2626f8 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -86,6 +86,9 @@ void *x86_bios_cpu_apicid_early_ptr;
 DEFINE_PER_CPU(u16, x86_bios_cpu_apicid) = BAD_APICID;
 EXPORT_PER_CPU_SYMBOL(x86_bios_cpu_apicid);
 
+/* Internal processor count */
+unsigned int num_processors;
+
 /* Bitmask of physically existing CPUs */
 physid_mask_t phys_cpu_present_map;
 

commit 40014bace17ba393409fd8a4915a87e43687aac8
Author: Alexey Starikovskiy <astarikovskiy@suse.de>
Date:   Thu Mar 27 23:54:44 2008 +0300

    x86: move phys_cpu_present_map to smpboot.c
    
    Signed-off-by: Alexey Starikovskiy <astarikovskiy@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 61b9a5b6fc07..8b6eefd9e906 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -85,6 +85,10 @@ u16 x86_bios_cpu_apicid_init[NR_CPUS] __initdata
 void *x86_bios_cpu_apicid_early_ptr;
 DEFINE_PER_CPU(u16, x86_bios_cpu_apicid) = BAD_APICID;
 EXPORT_PER_CPU_SYMBOL(x86_bios_cpu_apicid);
+
+/* Bitmask of physically existing CPUs */
+physid_mask_t phys_cpu_present_map;
+
 u8 apicid_2_node[MAX_APICID];
 #endif
 

commit 4cedb3343f0b087275b9a8e23fc90737881ac91c
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:26:14 2008 -0300

    x86: remove smpboot_32.c and smpboot_64.c
    
    Remove the last leftovers from the files. Move the ones
    that are still used to the files they belong, the others
    that grep can't reach, simply throw away.
    
    Merge comments ontop of file and that's it: smpboot integrated
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 75637fb760e7..61b9a5b6fc07 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1,3 +1,44 @@
+/*
+ *	x86 SMP booting functions
+ *
+ *	(c) 1995 Alan Cox, Building #3 <alan@redhat.com>
+ *	(c) 1998, 1999, 2000 Ingo Molnar <mingo@redhat.com>
+ *	Copyright 2001 Andi Kleen, SuSE Labs.
+ *
+ *	Much of the core SMP work is based on previous work by Thomas Radke, to
+ *	whom a great many thanks are extended.
+ *
+ *	Thanks to Intel for making available several different Pentium,
+ *	Pentium Pro and Pentium-II/Xeon MP machines.
+ *	Original development of Linux SMP code supported by Caldera.
+ *
+ *	This code is released under the GNU General Public License version 2 or
+ *	later.
+ *
+ *	Fixes
+ *		Felix Koop	:	NR_CPUS used properly
+ *		Jose Renau	:	Handle single CPU case.
+ *		Alan Cox	:	By repeated request 8) - Total BogoMIPS report.
+ *		Greg Wright	:	Fix for kernel stacks panic.
+ *		Erich Boleyn	:	MP v1.4 and additional changes.
+ *	Matthias Sattler	:	Changes for 2.1 kernel map.
+ *	Michel Lespinasse	:	Changes for 2.1 kernel map.
+ *	Michael Chastain	:	Change trampoline.S to gnu as.
+ *		Alan Cox	:	Dumb bug: 'B' step PPro's are fine
+ *		Ingo Molnar	:	Added APIC timers, based on code
+ *					from Jose Renau
+ *		Ingo Molnar	:	various cleanups and rewrites
+ *		Tigran Aivazian	:	fixed "0.00 in /proc/uptime on SMP" bug.
+ *	Maciej W. Rozycki	:	Bits for genuine 82489DX APICs
+ *	Andi Kleen		:	Changed for SMP boot into long mode.
+ *		Martin J. Bligh	: 	Added support for multi-quad systems
+ *		Dave Jones	:	Report invalid combinations of Athlon CPUs.
+ *		Rusty Russell	:	Hacked into shape for new "hotplug" boot process.
+ *      Andi Kleen              :       Converted to new state machine.
+ *	Ashok Raj		: 	CPU hotplug support
+ *	Glauber Costa		:	i386 and x86_64 integration
+ */
+
 #include <linux/init.h>
 #include <linux/smp.h>
 #include <linux/module.h>
@@ -44,6 +85,7 @@ u16 x86_bios_cpu_apicid_init[NR_CPUS] __initdata
 void *x86_bios_cpu_apicid_early_ptr;
 DEFINE_PER_CPU(u16, x86_bios_cpu_apicid) = BAD_APICID;
 EXPORT_PER_CPU_SYMBOL(x86_bios_cpu_apicid);
+u8 apicid_2_node[MAX_APICID];
 #endif
 
 /* State of each CPU */

commit acbb67341805d3b9ef263d8cbd103a6054164491
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:26:13 2008 -0300

    x86: move apicid mappings to smpboot.c
    
    They are i386 specific (the x86_64 definitions live
    elsewhere, and should remain there), so are enclosed around
    an ifdef
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6a7fb1300073..75637fb760e7 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -25,6 +25,27 @@
 #include <mach_wakecpu.h>
 #include <smpboot_hooks.h>
 
+/*
+ * FIXME: For x86_64, those are defined in other files. But moving them here,
+ * would make the setup areas dependent on smp, which is a loss. When we
+ * integrate apic between arches, we can probably do a better job, but
+ * right now, they'll stay here -- glommer
+ */
+#ifdef CONFIG_X86_32
+/* which logical CPU number maps to which CPU (physical APIC ID) */
+u16 x86_cpu_to_apicid_init[NR_CPUS] __initdata =
+			{ [0 ... NR_CPUS-1] = BAD_APICID };
+void *x86_cpu_to_apicid_early_ptr;
+DEFINE_PER_CPU(u16, x86_cpu_to_apicid) = BAD_APICID;
+EXPORT_PER_CPU_SYMBOL(x86_cpu_to_apicid);
+
+u16 x86_bios_cpu_apicid_init[NR_CPUS] __initdata
+				= { [0 ... NR_CPUS-1] = BAD_APICID };
+void *x86_bios_cpu_apicid_early_ptr;
+DEFINE_PER_CPU(u16, x86_bios_cpu_apicid) = BAD_APICID;
+EXPORT_PER_CPU_SYMBOL(x86_bios_cpu_apicid);
+#endif
+
 /* State of each CPU */
 DEFINE_PER_CPU(int, cpu_state) = { 0 };
 

commit 2cd9fb71eedffb3a208a84daff705b9da5c915e8
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:26:12 2008 -0300

    x86: merge cpu_exit_clear
    
    this is the last remaining function in smpboot_32.c
    Since it is i386 specific, move it around an ifdef to
    smpboot.c
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 45119d39f31e..6a7fb1300073 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1207,6 +1207,24 @@ void __init native_smp_cpus_done(unsigned int max_cpus)
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
+
+#  ifdef CONFIG_X86_32
+void cpu_exit_clear(void)
+{
+	int cpu = raw_smp_processor_id();
+
+	idle_task_exit();
+
+	cpu_uninit();
+	irq_ctx_exit(cpu);
+
+	cpu_clear(cpu, cpu_callout_map);
+	cpu_clear(cpu, cpu_callin_map);
+
+	unmap_cpu_to_logical_apicid(cpu);
+}
+#  endif /* CONFIG_X86_32 */
+
 void remove_siblinginfo(int cpu)
 {
 	int sibling;

commit 8aef135c73436fa46fdb4dc8aba49d5539dee72d
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:26:11 2008 -0300

    x86: merge native_smp_prepare_cpus
    
    With the previous changes, code for native_smp_prepare_cpus()
    in i386 and x86_64 now look very similar. merge them into
    smpboot.c. Minor differences are inside ifdef
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 26118b4a1c38..45119d39f31e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -7,6 +7,7 @@
 #include <linux/err.h>
 #include <linux/nmi.h>
 
+#include <asm/acpi.h>
 #include <asm/desc.h>
 #include <asm/nmi.h>
 #include <asm/irq.h>
@@ -75,6 +76,8 @@ EXPORT_PER_CPU_SYMBOL(cpu_info);
 
 static atomic_t init_deasserted;
 
+static int boot_cpu_logical_apicid;
+
 /* ready for x86_64, no harm for x86, since it will overwrite after alloc */
 unsigned char *trampoline_base = __va(SMP_TRAMPOLINE_BASE);
 
@@ -1001,6 +1004,173 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * Fall back to non SMP mode after errors.
+ *
+ * RED-PEN audit/test this more. I bet there is more state messed up here.
+ */
+static __init void disable_smp(void)
+{
+	cpu_present_map = cpumask_of_cpu(0);
+	cpu_possible_map = cpumask_of_cpu(0);
+#ifdef CONFIG_X86_32
+	smpboot_clear_io_apic_irqs();
+#endif
+	if (smp_found_config)
+		phys_cpu_present_map =
+				physid_mask_of_physid(boot_cpu_physical_apicid);
+	else
+		phys_cpu_present_map = physid_mask_of_physid(0);
+	map_cpu_to_logical_apicid();
+	cpu_set(0, per_cpu(cpu_sibling_map, 0));
+	cpu_set(0, per_cpu(cpu_core_map, 0));
+}
+
+/*
+ * Various sanity checks.
+ */
+static int __init smp_sanity_check(unsigned max_cpus)
+{
+	if (!physid_isset(hard_smp_processor_id(), phys_cpu_present_map)) {
+		printk(KERN_WARNING "weird, boot CPU (#%d) not listed"
+				    "by the BIOS.\n", hard_smp_processor_id());
+		physid_set(hard_smp_processor_id(), phys_cpu_present_map);
+	}
+
+	/*
+	 * If we couldn't find an SMP configuration at boot time,
+	 * get out of here now!
+	 */
+	if (!smp_found_config && !acpi_lapic) {
+		printk(KERN_NOTICE "SMP motherboard not detected.\n");
+		disable_smp();
+		if (APIC_init_uniprocessor())
+			printk(KERN_NOTICE "Local APIC not detected."
+					   " Using dummy APIC emulation.\n");
+		return -1;
+	}
+
+	/*
+	 * Should not be necessary because the MP table should list the boot
+	 * CPU too, but we do it for the sake of robustness anyway.
+	 */
+	if (!check_phys_apicid_present(boot_cpu_physical_apicid)) {
+		printk(KERN_NOTICE
+			"weird, boot CPU (#%d) not listed by the BIOS.\n",
+			boot_cpu_physical_apicid);
+		physid_set(hard_smp_processor_id(), phys_cpu_present_map);
+	}
+
+	/*
+	 * If we couldn't find a local APIC, then get out of here now!
+	 */
+	if (APIC_INTEGRATED(apic_version[boot_cpu_physical_apicid]) &&
+	    !cpu_has_apic) {
+		printk(KERN_ERR "BIOS bug, local APIC #%d not detected!...\n",
+			boot_cpu_physical_apicid);
+		printk(KERN_ERR "... forcing use of dummy APIC emulation."
+				"(tell your hw vendor)\n");
+		smpboot_clear_io_apic();
+		return -1;
+	}
+
+	verify_local_APIC();
+
+	/*
+	 * If SMP should be disabled, then really disable it!
+	 */
+	if (!max_cpus) {
+		printk(KERN_INFO "SMP mode deactivated,"
+				 "forcing use of dummy APIC emulation.\n");
+		smpboot_clear_io_apic();
+#ifdef CONFIG_X86_32
+		if (nmi_watchdog == NMI_LOCAL_APIC) {
+			printk(KERN_INFO "activating minimal APIC for"
+					 "NMI watchdog use.\n");
+			connect_bsp_APIC();
+			setup_local_APIC();
+			end_local_APIC_setup();
+		}
+#endif
+		return -1;
+	}
+
+	return 0;
+}
+
+static void __init smp_cpu_index_default(void)
+{
+	int i;
+	struct cpuinfo_x86 *c;
+
+	for_each_cpu_mask(i, cpu_possible_map) {
+		c = &cpu_data(i);
+		/* mark all to hotplug */
+		c->cpu_index = NR_CPUS;
+	}
+}
+
+/*
+ * Prepare for SMP bootup.  The MP table or ACPI has been read
+ * earlier.  Just do some sanity checking here and enable APIC mode.
+ */
+void __init native_smp_prepare_cpus(unsigned int max_cpus)
+{
+	nmi_watchdog_default();
+	smp_cpu_index_default();
+	current_cpu_data = boot_cpu_data;
+	cpu_callin_map = cpumask_of_cpu(0);
+	mb();
+	/*
+	 * Setup boot CPU information
+	 */
+	smp_store_cpu_info(0); /* Final full version of the data */
+	boot_cpu_logical_apicid = logical_smp_processor_id();
+	current_thread_info()->cpu = 0;  /* needed? */
+	set_cpu_sibling_map(0);
+
+	if (smp_sanity_check(max_cpus) < 0) {
+		printk(KERN_INFO "SMP disabled\n");
+		disable_smp();
+		return;
+	}
+
+	if (GET_APIC_ID(apic_read(APIC_ID)) != boot_cpu_physical_apicid) {
+		panic("Boot APIC ID in local APIC unexpected (%d vs %d)",
+		     GET_APIC_ID(apic_read(APIC_ID)), boot_cpu_physical_apicid);
+		/* Or can we switch back to PIC here? */
+	}
+
+#ifdef CONFIG_X86_32
+	connect_bsp_APIC();
+#endif
+	/*
+	 * Switch from PIC to APIC mode.
+	 */
+	setup_local_APIC();
+
+#ifdef CONFIG_X86_64
+	/*
+	 * Enable IO APIC before setting up error vector
+	 */
+	if (!skip_ioapic_setup && nr_ioapics)
+		enable_IO_APIC();
+#endif
+	end_local_APIC_setup();
+
+	map_cpu_to_logical_apicid();
+
+	setup_portio_remap();
+
+	smpboot_setup_io_apic();
+	/*
+	 * Set up local APIC timer on boot CPU.
+	 */
+
+	printk(KERN_INFO "CPU%d: ", 0);
+	print_cpu_info(&cpu_data(0));
+	setup_boot_clock();
+}
 /*
  * Early setup to make printk work.
  */

commit 83f7eb9c674c1bcaad6ca258fdd7dd3b96465a62
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:26:02 2008 -0300

    x86: merge native_smp_cpus_done
    
    They look similar enough, and are merged. Only difference
    (zap_low_mapping for i386) is inside ifdef
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b214d8dcc07a..26118b4a1c38 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -880,7 +880,6 @@ static int __cpuinit do_boot_cpu(int apicid, int cpu)
 	apic_write(APIC_ESR, 0);
 	apic_read(APIC_ESR);
 
-
 	/*
 	 * Starting actual IPI sequence...
 	 */
@@ -1017,6 +1016,26 @@ void __init native_smp_prepare_boot_cpu(void)
 	per_cpu(cpu_state, me) = CPU_ONLINE;
 }
 
+void __init native_smp_cpus_done(unsigned int max_cpus)
+{
+	/*
+	 * Cleanup possible dangling ends...
+	 */
+	smpboot_restore_warm_reset_vector();
+
+	Dprintk("Boot done.\n");
+
+	impress_friends();
+	smp_checks();
+#ifdef CONFIG_X86_IO_APIC
+	setup_ioapic_dest();
+#endif
+	check_nmi_watchdog();
+#ifdef CONFIG_X86_32
+	zap_low_mappings();
+#endif
+}
+
 #ifdef CONFIG_HOTPLUG_CPU
 void remove_siblinginfo(int cpu)
 {

commit a8db8453ff52609b14716361651ad10d2ab66682
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:26:01 2008 -0300

    x86: merge smp_prepare_boot_cpu
    
    it is practically the same between arches now, so it is
    moved to smpboot.c. Minor differences (gdt initialization)
    live inside an ifdef
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a36ae2785c48..b214d8dcc07a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -24,6 +24,9 @@
 #include <mach_wakecpu.h>
 #include <smpboot_hooks.h>
 
+/* State of each CPU */
+DEFINE_PER_CPU(int, cpu_state) = { 0 };
+
 /* Store all idle threads, this can be reused instead of creating
 * a new thread. Also avoids complicated thread destroy functionality
 * for idle threads.
@@ -999,6 +1002,21 @@ int __cpuinit native_cpu_up(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * Early setup to make printk work.
+ */
+void __init native_smp_prepare_boot_cpu(void)
+{
+	int me = smp_processor_id();
+#ifdef CONFIG_X86_32
+	init_gdt(me);
+	switch_to_new_gdt();
+#endif
+	/* already set me in cpu_online_map in boot_cpu_init() */
+	cpu_set(me, cpu_callout_map);
+	per_cpu(cpu_state, me) = CPU_ONLINE;
+}
+
 #ifdef CONFIG_HOTPLUG_CPU
 void remove_siblinginfo(int cpu)
 {

commit bbc2ff6a91a4eef8030018cd389bb12352d11b34
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:26:00 2008 -0300

    x86: integrate start_secondary
    
    It now looks the same between architectures, so we
    merge it in smpboot.c. Minor differences goes inside
    an ifdef
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 69c17965f48d..a36ae2785c48 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -17,6 +17,7 @@
 #include <asm/tlbflush.h>
 #include <asm/mtrr.h>
 #include <asm/nmi.h>
+#include <asm/vmi.h>
 #include <linux/mc146818rtc.h>
 
 #include <mach_apic.h>
@@ -229,6 +230,90 @@ void __cpuinit smp_callin(void)
 	cpu_set(cpuid, cpu_callin_map);
 }
 
+/*
+ * Activate a secondary processor.
+ */
+void __cpuinit start_secondary(void *unused)
+{
+	/*
+	 * Don't put *anything* before cpu_init(), SMP booting is too
+	 * fragile that we want to limit the things done here to the
+	 * most necessary things.
+	 */
+#ifdef CONFIG_VMI
+	vmi_bringup();
+#endif
+	cpu_init();
+	preempt_disable();
+	smp_callin();
+
+	/* otherwise gcc will move up smp_processor_id before the cpu_init */
+	barrier();
+	/*
+	 * Check TSC synchronization with the BP:
+	 */
+	check_tsc_sync_target();
+
+	if (nmi_watchdog == NMI_IO_APIC) {
+		disable_8259A_irq(0);
+		enable_NMI_through_LVT0();
+		enable_8259A_irq(0);
+	}
+
+	/* This must be done before setting cpu_online_map */
+	set_cpu_sibling_map(raw_smp_processor_id());
+	wmb();
+
+	/*
+	 * We need to hold call_lock, so there is no inconsistency
+	 * between the time smp_call_function() determines number of
+	 * IPI recipients, and the time when the determination is made
+	 * for which cpus receive the IPI. Holding this
+	 * lock helps us to not include this cpu in a currently in progress
+	 * smp_call_function().
+	 */
+	lock_ipi_call_lock();
+#ifdef CONFIG_X86_64
+	spin_lock(&vector_lock);
+
+	/* Setup the per cpu irq handling data structures */
+	__setup_vector_irq(smp_processor_id());
+	/*
+	 * Allow the master to continue.
+	 */
+	spin_unlock(&vector_lock);
+#endif
+	cpu_set(smp_processor_id(), cpu_online_map);
+	unlock_ipi_call_lock();
+	per_cpu(cpu_state, smp_processor_id()) = CPU_ONLINE;
+
+	setup_secondary_clock();
+
+	wmb();
+	cpu_idle();
+}
+
+#ifdef CONFIG_X86_32
+/*
+ * Everything has been set up for the secondary
+ * CPUs - they just need to reload everything
+ * from the task structure
+ * This function must not return.
+ */
+void __devinit initialize_secondary(void)
+{
+	/*
+	 * We don't actually need to load the full TSS,
+	 * basically just the stack pointer and the ip.
+	 */
+
+	asm volatile(
+		"movl %0,%%esp\n\t"
+		"jmp *%1"
+		:
+		:"m" (current->thread.sp), "m" (current->thread.ip));
+}
+#endif
 
 static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
 {
@@ -533,7 +618,6 @@ wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
 }
 #endif	/* WAKE_SECONDARY_VIA_NMI */
 
-extern void start_secondary(void *unused);
 #ifdef WAKE_SECONDARY_VIA_INIT
 static int __devinit
 wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)

commit cb3c8b9003f15efa4a750a32d2d602d40cc45d5a
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:25:59 2008 -0300

    x86: integrate do_boot_cpu
    
    This is a very large patch, because it depends on a lot
    of auxiliary static functions. But they all have been modified
    to the point that they're sufficiently close now. So they're just
    merged in smpboot.c
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 5bff87e99898..69c17965f48d 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -4,14 +4,42 @@
 #include <linux/sched.h>
 #include <linux/percpu.h>
 #include <linux/bootmem.h>
+#include <linux/err.h>
+#include <linux/nmi.h>
 
+#include <asm/desc.h>
 #include <asm/nmi.h>
 #include <asm/irq.h>
 #include <asm/smp.h>
 #include <asm/cpu.h>
 #include <asm/numa.h>
+#include <asm/pgtable.h>
+#include <asm/tlbflush.h>
+#include <asm/mtrr.h>
+#include <asm/nmi.h>
+#include <linux/mc146818rtc.h>
 
 #include <mach_apic.h>
+#include <mach_wakecpu.h>
+#include <smpboot_hooks.h>
+
+/* Store all idle threads, this can be reused instead of creating
+* a new thread. Also avoids complicated thread destroy functionality
+* for idle threads.
+*/
+#ifdef CONFIG_HOTPLUG_CPU
+/*
+ * Needed only for CONFIG_HOTPLUG_CPU because __cpuinitdata is
+ * removed after init for !CONFIG_HOTPLUG_CPU.
+ */
+static DEFINE_PER_CPU(struct task_struct *, idle_thread_array);
+#define get_idle_for_cpu(x)      (per_cpu(idle_thread_array, x))
+#define set_idle_for_cpu(x, p)   (per_cpu(idle_thread_array, x) = (p))
+#else
+struct task_struct *idle_thread_array[NR_CPUS] __cpuinitdata ;
+#define get_idle_for_cpu(x)      (idle_thread_array[(x)])
+#define set_idle_for_cpu(x, p)   (idle_thread_array[(x)] = (p))
+#endif
 
 /* Number of siblings per CPU package */
 int smp_num_siblings = 1;
@@ -41,6 +69,8 @@ EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 DEFINE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
 
+static atomic_t init_deasserted;
+
 /* ready for x86_64, no harm for x86, since it will overwrite after alloc */
 unsigned char *trampoline_base = __va(SMP_TRAMPOLINE_BASE);
 
@@ -110,6 +140,96 @@ void unmap_cpu_to_logical_apicid(int cpu)
 #define map_cpu_to_logical_apicid()  do {} while (0)
 #endif
 
+/*
+ * Report back to the Boot Processor.
+ * Running on AP.
+ */
+void __cpuinit smp_callin(void)
+{
+	int cpuid, phys_id;
+	unsigned long timeout;
+
+	/*
+	 * If waken up by an INIT in an 82489DX configuration
+	 * we may get here before an INIT-deassert IPI reaches
+	 * our local APIC.  We have to wait for the IPI or we'll
+	 * lock up on an APIC access.
+	 */
+	wait_for_init_deassert(&init_deasserted);
+
+	/*
+	 * (This works even if the APIC is not enabled.)
+	 */
+	phys_id = GET_APIC_ID(apic_read(APIC_ID));
+	cpuid = smp_processor_id();
+	if (cpu_isset(cpuid, cpu_callin_map)) {
+		panic("%s: phys CPU#%d, CPU#%d already present??\n", __func__,
+					phys_id, cpuid);
+	}
+	Dprintk("CPU#%d (phys ID: %d) waiting for CALLOUT\n", cpuid, phys_id);
+
+	/*
+	 * STARTUP IPIs are fragile beasts as they might sometimes
+	 * trigger some glue motherboard logic. Complete APIC bus
+	 * silence for 1 second, this overestimates the time the
+	 * boot CPU is spending to send the up to 2 STARTUP IPIs
+	 * by a factor of two. This should be enough.
+	 */
+
+	/*
+	 * Waiting 2s total for startup (udelay is not yet working)
+	 */
+	timeout = jiffies + 2*HZ;
+	while (time_before(jiffies, timeout)) {
+		/*
+		 * Has the boot CPU finished it's STARTUP sequence?
+		 */
+		if (cpu_isset(cpuid, cpu_callout_map))
+			break;
+		cpu_relax();
+	}
+
+	if (!time_before(jiffies, timeout)) {
+		panic("%s: CPU%d started up but did not get a callout!\n",
+		      __func__, cpuid);
+	}
+
+	/*
+	 * the boot CPU has finished the init stage and is spinning
+	 * on callin_map until we finish. We are free to set up this
+	 * CPU, first the APIC. (this is probably redundant on most
+	 * boards)
+	 */
+
+	Dprintk("CALLIN, before setup_local_APIC().\n");
+	smp_callin_clear_local_apic();
+	setup_local_APIC();
+	end_local_APIC_setup();
+	map_cpu_to_logical_apicid();
+
+	/*
+	 * Get our bogomips.
+	 *
+	 * Need to enable IRQs because it can take longer and then
+	 * the NMI watchdog might kill us.
+	 */
+	local_irq_enable();
+	calibrate_delay();
+	local_irq_disable();
+	Dprintk("Stack at about %p\n", &cpuid);
+
+	/*
+	 * Save our processor parameters
+	 */
+	smp_store_cpu_info(cpuid);
+
+	/*
+	 * Allow the master to continue.
+	 */
+	cpu_set(cpuid, cpu_callin_map);
+}
+
+
 static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_32
@@ -327,6 +447,474 @@ void impress_friends(void)
 	Dprintk("Before bogocount - setting activated=1.\n");
 }
 
+static inline void __inquire_remote_apic(int apicid)
+{
+	unsigned i, regs[] = { APIC_ID >> 4, APIC_LVR >> 4, APIC_SPIV >> 4 };
+	char *names[] = { "ID", "VERSION", "SPIV" };
+	int timeout;
+	u32 status;
+
+	printk(KERN_INFO "Inquiring remote APIC #%d...\n", apicid);
+
+	for (i = 0; i < ARRAY_SIZE(regs); i++) {
+		printk(KERN_INFO "... APIC #%d %s: ", apicid, names[i]);
+
+		/*
+		 * Wait for idle.
+		 */
+		status = safe_apic_wait_icr_idle();
+		if (status)
+			printk(KERN_CONT
+			       "a previous APIC delivery may have failed\n");
+
+		apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(apicid));
+		apic_write_around(APIC_ICR, APIC_DM_REMRD | regs[i]);
+
+		timeout = 0;
+		do {
+			udelay(100);
+			status = apic_read(APIC_ICR) & APIC_ICR_RR_MASK;
+		} while (status == APIC_ICR_RR_INPROG && timeout++ < 1000);
+
+		switch (status) {
+		case APIC_ICR_RR_VALID:
+			status = apic_read(APIC_RRR);
+			printk(KERN_CONT "%08x\n", status);
+			break;
+		default:
+			printk(KERN_CONT "failed\n");
+		}
+	}
+}
+
+#ifdef WAKE_SECONDARY_VIA_NMI
+/*
+ * Poke the other CPU in the eye via NMI to wake it up. Remember that the normal
+ * INIT, INIT, STARTUP sequence will reset the chip hard for us, and this
+ * won't ... remember to clear down the APIC, etc later.
+ */
+static int __devinit
+wakeup_secondary_cpu(int logical_apicid, unsigned long start_eip)
+{
+	unsigned long send_status, accept_status = 0;
+	int maxlvt;
+
+	/* Target chip */
+	apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(logical_apicid));
+
+	/* Boot on the stack */
+	/* Kick the second */
+	apic_write_around(APIC_ICR, APIC_DM_NMI | APIC_DEST_LOGICAL);
+
+	Dprintk("Waiting for send to finish...\n");
+	send_status = safe_apic_wait_icr_idle();
+
+	/*
+	 * Give the other CPU some time to accept the IPI.
+	 */
+	udelay(200);
+	/*
+	 * Due to the Pentium erratum 3AP.
+	 */
+	maxlvt = lapic_get_maxlvt();
+	if (maxlvt > 3) {
+		apic_read_around(APIC_SPIV);
+		apic_write(APIC_ESR, 0);
+	}
+	accept_status = (apic_read(APIC_ESR) & 0xEF);
+	Dprintk("NMI sent.\n");
+
+	if (send_status)
+		printk(KERN_ERR "APIC never delivered???\n");
+	if (accept_status)
+		printk(KERN_ERR "APIC delivery error (%lx).\n", accept_status);
+
+	return (send_status | accept_status);
+}
+#endif	/* WAKE_SECONDARY_VIA_NMI */
+
+extern void start_secondary(void *unused);
+#ifdef WAKE_SECONDARY_VIA_INIT
+static int __devinit
+wakeup_secondary_cpu(int phys_apicid, unsigned long start_eip)
+{
+	unsigned long send_status, accept_status = 0;
+	int maxlvt, num_starts, j;
+
+	/*
+	 * Be paranoid about clearing APIC errors.
+	 */
+	if (APIC_INTEGRATED(apic_version[phys_apicid])) {
+		apic_read_around(APIC_SPIV);
+		apic_write(APIC_ESR, 0);
+		apic_read(APIC_ESR);
+	}
+
+	Dprintk("Asserting INIT.\n");
+
+	/*
+	 * Turn INIT on target chip
+	 */
+	apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
+
+	/*
+	 * Send IPI
+	 */
+	apic_write_around(APIC_ICR, APIC_INT_LEVELTRIG | APIC_INT_ASSERT
+				| APIC_DM_INIT);
+
+	Dprintk("Waiting for send to finish...\n");
+	send_status = safe_apic_wait_icr_idle();
+
+	mdelay(10);
+
+	Dprintk("Deasserting INIT.\n");
+
+	/* Target chip */
+	apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
+
+	/* Send IPI */
+	apic_write_around(APIC_ICR, APIC_INT_LEVELTRIG | APIC_DM_INIT);
+
+	Dprintk("Waiting for send to finish...\n");
+	send_status = safe_apic_wait_icr_idle();
+
+	mb();
+	atomic_set(&init_deasserted, 1);
+
+	/*
+	 * Should we send STARTUP IPIs ?
+	 *
+	 * Determine this based on the APIC version.
+	 * If we don't have an integrated APIC, don't send the STARTUP IPIs.
+	 */
+	if (APIC_INTEGRATED(apic_version[phys_apicid]))
+		num_starts = 2;
+	else
+		num_starts = 0;
+
+	/*
+	 * Paravirt / VMI wants a startup IPI hook here to set up the
+	 * target processor state.
+	 */
+	startup_ipi_hook(phys_apicid, (unsigned long) start_secondary,
+#ifdef CONFIG_X86_64
+			 (unsigned long)init_rsp);
+#else
+			 (unsigned long)stack_start.sp);
+#endif
+
+	/*
+	 * Run STARTUP IPI loop.
+	 */
+	Dprintk("#startup loops: %d.\n", num_starts);
+
+	maxlvt = lapic_get_maxlvt();
+
+	for (j = 1; j <= num_starts; j++) {
+		Dprintk("Sending STARTUP #%d.\n", j);
+		apic_read_around(APIC_SPIV);
+		apic_write(APIC_ESR, 0);
+		apic_read(APIC_ESR);
+		Dprintk("After apic_write.\n");
+
+		/*
+		 * STARTUP IPI
+		 */
+
+		/* Target chip */
+		apic_write_around(APIC_ICR2, SET_APIC_DEST_FIELD(phys_apicid));
+
+		/* Boot on the stack */
+		/* Kick the second */
+		apic_write_around(APIC_ICR, APIC_DM_STARTUP
+					| (start_eip >> 12));
+
+		/*
+		 * Give the other CPU some time to accept the IPI.
+		 */
+		udelay(300);
+
+		Dprintk("Startup point 1.\n");
+
+		Dprintk("Waiting for send to finish...\n");
+		send_status = safe_apic_wait_icr_idle();
+
+		/*
+		 * Give the other CPU some time to accept the IPI.
+		 */
+		udelay(200);
+		/*
+		 * Due to the Pentium erratum 3AP.
+		 */
+		if (maxlvt > 3) {
+			apic_read_around(APIC_SPIV);
+			apic_write(APIC_ESR, 0);
+		}
+		accept_status = (apic_read(APIC_ESR) & 0xEF);
+		if (send_status || accept_status)
+			break;
+	}
+	Dprintk("After Startup.\n");
+
+	if (send_status)
+		printk(KERN_ERR "APIC never delivered???\n");
+	if (accept_status)
+		printk(KERN_ERR "APIC delivery error (%lx).\n", accept_status);
+
+	return (send_status | accept_status);
+}
+#endif	/* WAKE_SECONDARY_VIA_INIT */
+
+struct create_idle {
+	struct work_struct work;
+	struct task_struct *idle;
+	struct completion done;
+	int cpu;
+};
+
+static void __cpuinit do_fork_idle(struct work_struct *work)
+{
+	struct create_idle *c_idle =
+		container_of(work, struct create_idle, work);
+
+	c_idle->idle = fork_idle(c_idle->cpu);
+	complete(&c_idle->done);
+}
+
+static int __cpuinit do_boot_cpu(int apicid, int cpu)
+/*
+ * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad
+ * (ie clustered apic addressing mode), this is a LOGICAL apic ID.
+ * Returns zero if CPU booted OK, else error code from wakeup_secondary_cpu.
+ */
+{
+	unsigned long boot_error = 0;
+	int timeout;
+	unsigned long start_ip;
+	unsigned short nmi_high = 0, nmi_low = 0;
+	struct create_idle c_idle = {
+		.cpu = cpu,
+		.done = COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
+	};
+	INIT_WORK(&c_idle.work, do_fork_idle);
+#ifdef CONFIG_X86_64
+	/* allocate memory for gdts of secondary cpus. Hotplug is considered */
+	if (!cpu_gdt_descr[cpu].address &&
+		!(cpu_gdt_descr[cpu].address = get_zeroed_page(GFP_KERNEL))) {
+		printk(KERN_ERR "Failed to allocate GDT for CPU %d\n", cpu);
+		return -1;
+	}
+
+	/* Allocate node local memory for AP pdas */
+	if (cpu_pda(cpu) == &boot_cpu_pda[cpu]) {
+		struct x8664_pda *newpda, *pda;
+		int node = cpu_to_node(cpu);
+		pda = cpu_pda(cpu);
+		newpda = kmalloc_node(sizeof(struct x8664_pda), GFP_ATOMIC,
+				      node);
+		if (newpda) {
+			memcpy(newpda, pda, sizeof(struct x8664_pda));
+			cpu_pda(cpu) = newpda;
+		} else
+			printk(KERN_ERR
+		"Could not allocate node local PDA for CPU %d on node %d\n",
+				cpu, node);
+	}
+#endif
+
+	alternatives_smp_switch(1);
+
+	c_idle.idle = get_idle_for_cpu(cpu);
+
+	/*
+	 * We can't use kernel_thread since we must avoid to
+	 * reschedule the child.
+	 */
+	if (c_idle.idle) {
+		c_idle.idle->thread.sp = (unsigned long) (((struct pt_regs *)
+			(THREAD_SIZE +  task_stack_page(c_idle.idle))) - 1);
+		init_idle(c_idle.idle, cpu);
+		goto do_rest;
+	}
+
+	if (!keventd_up() || current_is_keventd())
+		c_idle.work.func(&c_idle.work);
+	else {
+		schedule_work(&c_idle.work);
+		wait_for_completion(&c_idle.done);
+	}
+
+	if (IS_ERR(c_idle.idle)) {
+		printk("failed fork for CPU %d\n", cpu);
+		return PTR_ERR(c_idle.idle);
+	}
+
+	set_idle_for_cpu(cpu, c_idle.idle);
+do_rest:
+#ifdef CONFIG_X86_32
+	per_cpu(current_task, cpu) = c_idle.idle;
+	init_gdt(cpu);
+	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
+	c_idle.idle->thread.ip = (unsigned long) start_secondary;
+	/* Stack for startup_32 can be just as for start_secondary onwards */
+	stack_start.sp = (void *) c_idle.idle->thread.sp;
+	irq_ctx_init(cpu);
+#else
+	cpu_pda(cpu)->pcurrent = c_idle.idle;
+	init_rsp = c_idle.idle->thread.sp;
+	load_sp0(&per_cpu(init_tss, cpu), &c_idle.idle->thread);
+	initial_code = (unsigned long)start_secondary;
+	clear_tsk_thread_flag(c_idle.idle, TIF_FORK);
+#endif
+
+	/* start_ip had better be page-aligned! */
+	start_ip = setup_trampoline();
+
+	/* So we see what's up   */
+	printk(KERN_INFO "Booting processor %d/%d ip %lx\n",
+			  cpu, apicid, start_ip);
+
+	/*
+	 * This grunge runs the startup process for
+	 * the targeted processor.
+	 */
+
+	atomic_set(&init_deasserted, 0);
+
+	Dprintk("Setting warm reset code and vector.\n");
+
+	store_NMI_vector(&nmi_high, &nmi_low);
+
+	smpboot_setup_warm_reset_vector(start_ip);
+	/*
+	 * Be paranoid about clearing APIC errors.
+	 */
+	apic_write(APIC_ESR, 0);
+	apic_read(APIC_ESR);
+
+
+	/*
+	 * Starting actual IPI sequence...
+	 */
+	boot_error = wakeup_secondary_cpu(apicid, start_ip);
+
+	if (!boot_error) {
+		/*
+		 * allow APs to start initializing.
+		 */
+		Dprintk("Before Callout %d.\n", cpu);
+		cpu_set(cpu, cpu_callout_map);
+		Dprintk("After Callout %d.\n", cpu);
+
+		/*
+		 * Wait 5s total for a response
+		 */
+		for (timeout = 0; timeout < 50000; timeout++) {
+			if (cpu_isset(cpu, cpu_callin_map))
+				break;	/* It has booted */
+			udelay(100);
+		}
+
+		if (cpu_isset(cpu, cpu_callin_map)) {
+			/* number CPUs logically, starting from 1 (BSP is 0) */
+			Dprintk("OK.\n");
+			printk(KERN_INFO "CPU%d: ", cpu);
+			print_cpu_info(&cpu_data(cpu));
+			Dprintk("CPU has booted.\n");
+		} else {
+			boot_error = 1;
+			if (*((volatile unsigned char *)trampoline_base)
+					== 0xA5)
+				/* trampoline started but...? */
+				printk(KERN_ERR "Stuck ??\n");
+			else
+				/* trampoline code not run */
+				printk(KERN_ERR "Not responding.\n");
+			inquire_remote_apic(apicid);
+		}
+	}
+
+	if (boot_error) {
+		/* Try to put things back the way they were before ... */
+		unmap_cpu_to_logical_apicid(cpu);
+#ifdef CONFIG_X86_64
+		clear_node_cpumask(cpu); /* was set by numa_add_cpu */
+#endif
+		cpu_clear(cpu, cpu_callout_map); /* was set by do_boot_cpu() */
+		cpu_clear(cpu, cpu_initialized); /* was set by cpu_init() */
+		cpu_clear(cpu, cpu_possible_map);
+		cpu_clear(cpu, cpu_present_map);
+		per_cpu(x86_cpu_to_apicid, cpu) = BAD_APICID;
+	}
+
+	/* mark "stuck" area as not stuck */
+	*((volatile unsigned long *)trampoline_base) = 0;
+
+	return boot_error;
+}
+
+int __cpuinit native_cpu_up(unsigned int cpu)
+{
+	int apicid = cpu_present_to_apicid(cpu);
+	unsigned long flags;
+	int err;
+
+	WARN_ON(irqs_disabled());
+
+	Dprintk("++++++++++++++++++++=_---CPU UP  %u\n", cpu);
+
+	if (apicid == BAD_APICID || apicid == boot_cpu_physical_apicid ||
+	    !physid_isset(apicid, phys_cpu_present_map)) {
+		printk(KERN_ERR "%s: bad cpu %d\n", __func__, cpu);
+		return -EINVAL;
+	}
+
+	/*
+	 * Already booted CPU?
+	 */
+	if (cpu_isset(cpu, cpu_callin_map)) {
+		Dprintk("do_boot_cpu %d Already started\n", cpu);
+		return -ENOSYS;
+	}
+
+	/*
+	 * Save current MTRR state in case it was changed since early boot
+	 * (e.g. by the ACPI SMI) to initialize new CPUs with MTRRs in sync:
+	 */
+	mtrr_save_state();
+
+	per_cpu(cpu_state, cpu) = CPU_UP_PREPARE;
+
+#ifdef CONFIG_X86_32
+	/* init low mem mapping */
+	clone_pgd_range(swapper_pg_dir, swapper_pg_dir + USER_PGD_PTRS,
+			min_t(unsigned long, KERNEL_PGD_PTRS, USER_PGD_PTRS));
+	flush_tlb_all();
+#endif
+
+	err = do_boot_cpu(apicid, cpu);
+	if (err < 0) {
+		Dprintk("do_boot_cpu failed %d\n", err);
+		return err;
+	}
+
+	/*
+	 * Check TSC synchronization with the AP (keep irqs disabled
+	 * while doing so):
+	 */
+	local_irq_save(flags);
+	check_tsc_sync_source(cpu);
+	local_irq_restore(flags);
+
+	while (!cpu_isset(cpu, cpu_online_map)) {
+		cpu_relax();
+		touch_nmi_watchdog();
+	}
+
+	return 0;
+}
+
 #ifdef CONFIG_HOTPLUG_CPU
 void remove_siblinginfo(int cpu)
 {

commit 7cc3959ecd830796231f50bf5e42dc018b3694f2
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:25:56 2008 -0300

    x86: move {un}map_cpu_to_logical_apicid to smpboot.c
    
    Move map_cpu_to_logical_apicid() and unmap_cpu_to_logical_apicid()
    to smpboot.c. They take together all the bunch of static functions
    they rely upon
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 253be86a88e4..5bff87e99898 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -50,6 +50,66 @@ static cpumask_t cpu_sibling_setup_map;
 /* Set if we find a B stepping CPU */
 int __cpuinitdata smp_b_stepping;
 
+#if defined(CONFIG_NUMA) && defined(CONFIG_X86_32)
+
+/* which logical CPUs are on which nodes */
+cpumask_t node_to_cpumask_map[MAX_NUMNODES] __read_mostly =
+				{ [0 ... MAX_NUMNODES-1] = CPU_MASK_NONE };
+EXPORT_SYMBOL(node_to_cpumask_map);
+/* which node each logical CPU is on */
+int cpu_to_node_map[NR_CPUS] __read_mostly = { [0 ... NR_CPUS-1] = 0 };
+EXPORT_SYMBOL(cpu_to_node_map);
+
+/* set up a mapping between cpu and node. */
+static void map_cpu_to_node(int cpu, int node)
+{
+	printk(KERN_INFO "Mapping cpu %d to node %d\n", cpu, node);
+	cpu_set(cpu, node_to_cpumask_map[node]);
+	cpu_to_node_map[cpu] = node;
+}
+
+/* undo a mapping between cpu and node. */
+static void unmap_cpu_to_node(int cpu)
+{
+	int node;
+
+	printk(KERN_INFO "Unmapping cpu %d from all nodes\n", cpu);
+	for (node = 0; node < MAX_NUMNODES; node++)
+		cpu_clear(cpu, node_to_cpumask_map[node]);
+	cpu_to_node_map[cpu] = 0;
+}
+#else /* !(CONFIG_NUMA && CONFIG_X86_32) */
+#define map_cpu_to_node(cpu, node)	({})
+#define unmap_cpu_to_node(cpu)	({})
+#endif
+
+#ifdef CONFIG_X86_32
+u8 cpu_2_logical_apicid[NR_CPUS] __read_mostly =
+					{ [0 ... NR_CPUS-1] = BAD_APICID };
+
+void map_cpu_to_logical_apicid(void)
+{
+	int cpu = smp_processor_id();
+	int apicid = logical_smp_processor_id();
+	int node = apicid_to_node(apicid);
+
+	if (!node_online(node))
+		node = first_online_node;
+
+	cpu_2_logical_apicid[cpu] = apicid;
+	map_cpu_to_node(cpu, node);
+}
+
+void unmap_cpu_to_logical_apicid(int cpu)
+{
+	cpu_2_logical_apicid[cpu] = BAD_APICID;
+	unmap_cpu_to_node(cpu);
+}
+#else
+#define unmap_cpu_to_logical_apicid(cpu) do {} while (0)
+#define map_cpu_to_logical_apicid()  do {} while (0)
+#endif
+
 static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_32

commit f6bc40290964b5fcb48c226ccafa4b7536d62663
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:25:53 2008 -0300

    x86: include mach_apic.h in smpboot_64.c and smpboot.c
    
    After the inclusion, a lot of files needs fixing for conflicts,
    some of them in the headers themselves, to accomodate for both
    i386 and x86_64 versions.
    
    [ mingo@elte.hu: build fix ]
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 6978f1bf6533..253be86a88e4 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -11,6 +11,8 @@
 #include <asm/cpu.h>
 #include <asm/numa.h>
 
+#include <mach_apic.h>
+
 /* Number of siblings per CPU package */
 int smp_num_siblings = 1;
 EXPORT_SYMBOL(smp_num_siblings);

commit f68e00a32b4f5a2881c3a39d71cc2c22e92f1d99
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:25:29 2008 -0300

    x86: move impress_friends and smp_check to cpus_done
    
    the cpu count is changed accordingly: now, what matters is
    online cpus.
    Also, we add those functions for x86_64
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index ddb94ef37789..6978f1bf6533 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -114,7 +114,7 @@ void smp_checks(void)
 	 * approved Athlon
 	 */
 	if (tainted & TAINT_UNSAFE_SMP) {
-		if (cpus_weight(cpu_present_map))
+		if (num_online_cpus())
 			printk(KERN_INFO "WARNING: This combination of AMD"
 				"processors is not suitable for SMP.\n");
 		else
@@ -258,7 +258,7 @@ void impress_friends(void)
 			bogosum += cpu_data(cpu).loops_per_jiffy;
 	printk(KERN_INFO
 		"Total of %d processors activated (%lu.%02lu BogoMIPS).\n",
-		cpus_weight(cpu_present_map),
+		num_online_cpus(),
 		bogosum/(500000/HZ),
 		(bogosum/(5000/HZ))%100);
 

commit 693d4b8a6429af7f2029df20a59e22f4d752e141
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:25:28 2008 -0300

    x86: do smp tainting checks in a separate function
    
    It will ease integration for x86_64
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 02427d1003d3..ddb94ef37789 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -45,10 +45,8 @@ unsigned char *trampoline_base = __va(SMP_TRAMPOLINE_BASE);
 /* representing cpus for which sibling maps can be computed */
 static cpumask_t cpu_sibling_setup_map;
 
-#ifdef CONFIG_X86_32
 /* Set if we find a B stepping CPU */
 int __cpuinitdata smp_b_stepping;
-#endif
 
 static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
 {
@@ -105,6 +103,25 @@ static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
 #endif
 }
 
+void smp_checks(void)
+{
+	if (smp_b_stepping)
+		printk(KERN_WARNING "WARNING: SMP operation may be unreliable"
+				    "with B stepping processors.\n");
+
+	/*
+	 * Don't taint if we are running SMP kernel on a single non-MP
+	 * approved Athlon
+	 */
+	if (tainted & TAINT_UNSAFE_SMP) {
+		if (cpus_weight(cpu_present_map))
+			printk(KERN_INFO "WARNING: This combination of AMD"
+				"processors is not suitable for SMP.\n");
+		else
+			tainted &= ~TAINT_UNSAFE_SMP;
+	}
+}
+
 /*
  * The bootstrap kernel entry code has set these up. Save them for
  * a given CPU

commit 904541e2f76bc3efe4cc9978b7adb3323ea8607e
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:25:27 2008 -0300

    x86: allow user to impress friends.
    
    Impressing friends is a very important thing.
    Do it in a separate function to make it even more
    explicit, and ease integration.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index a157a5245923..02427d1003d3 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -228,6 +228,26 @@ void __init smp_alloc_memory(void)
 }
 #endif
 
+void impress_friends(void)
+{
+	int cpu;
+	unsigned long bogosum = 0;
+	/*
+	 * Allow the user to impress friends.
+	 */
+	Dprintk("Before bogomips.\n");
+	for_each_possible_cpu(cpu)
+		if (cpu_isset(cpu, cpu_callout_map))
+			bogosum += cpu_data(cpu).loops_per_jiffy;
+	printk(KERN_INFO
+		"Total of %d processors activated (%lu.%02lu BogoMIPS).\n",
+		cpus_weight(cpu_present_map),
+		bogosum/(500000/HZ),
+		(bogosum/(5000/HZ))%100);
+
+	Dprintk("Before bogocount - setting activated=1.\n");
+}
+
 #ifdef CONFIG_HOTPLUG_CPU
 void remove_siblinginfo(int cpu)
 {

commit 1d89a7f072d4f76f0538edfb474d527066ee7838
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Mar 19 14:25:05 2008 -0300

    x86: merge smp_store_cpu_info
    
    now that it is the same between arches, put it into smpboot.c
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b13b9d55f9ce..a157a5245923 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -45,6 +45,83 @@ unsigned char *trampoline_base = __va(SMP_TRAMPOLINE_BASE);
 /* representing cpus for which sibling maps can be computed */
 static cpumask_t cpu_sibling_setup_map;
 
+#ifdef CONFIG_X86_32
+/* Set if we find a B stepping CPU */
+int __cpuinitdata smp_b_stepping;
+#endif
+
+static void __cpuinit smp_apply_quirks(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_X86_32
+	/*
+	 * Mask B, Pentium, but not Pentium MMX
+	 */
+	if (c->x86_vendor == X86_VENDOR_INTEL &&
+	    c->x86 == 5 &&
+	    c->x86_mask >= 1 && c->x86_mask <= 4 &&
+	    c->x86_model <= 3)
+		/*
+		 * Remember we have B step Pentia with bugs
+		 */
+		smp_b_stepping = 1;
+
+	/*
+	 * Certain Athlons might work (for various values of 'work') in SMP
+	 * but they are not certified as MP capable.
+	 */
+	if ((c->x86_vendor == X86_VENDOR_AMD) && (c->x86 == 6)) {
+
+		if (num_possible_cpus() == 1)
+			goto valid_k7;
+
+		/* Athlon 660/661 is valid. */
+		if ((c->x86_model == 6) && ((c->x86_mask == 0) ||
+		    (c->x86_mask == 1)))
+			goto valid_k7;
+
+		/* Duron 670 is valid */
+		if ((c->x86_model == 7) && (c->x86_mask == 0))
+			goto valid_k7;
+
+		/*
+		 * Athlon 662, Duron 671, and Athlon >model 7 have capability
+		 * bit. It's worth noting that the A5 stepping (662) of some
+		 * Athlon XP's have the MP bit set.
+		 * See http://www.heise.de/newsticker/data/jow-18.10.01-000 for
+		 * more.
+		 */
+		if (((c->x86_model == 6) && (c->x86_mask >= 2)) ||
+		    ((c->x86_model == 7) && (c->x86_mask >= 1)) ||
+		     (c->x86_model > 7))
+			if (cpu_has_mp)
+				goto valid_k7;
+
+		/* If we get here, not a certified SMP capable AMD system. */
+		add_taint(TAINT_UNSAFE_SMP);
+	}
+
+valid_k7:
+	;
+#endif
+}
+
+/*
+ * The bootstrap kernel entry code has set these up. Save them for
+ * a given CPU
+ */
+
+void __cpuinit smp_store_cpu_info(int id)
+{
+	struct cpuinfo_x86 *c = &cpu_data(id);
+
+	*c = boot_cpu_data;
+	c->cpu_index = id;
+	if (id != 0)
+		identify_secondary_cpu(c);
+	smp_apply_quirks(c);
+}
+
+
 void __cpuinit set_cpu_sibling_map(int cpu)
 {
 	int i;

commit 91718e8d13c23bfe0aa6fa6b730c5c33ee9771bf
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Mar 3 14:13:12 2008 -0300

    x86: unify setup_trampoline
    
    setup_trampoline() looks very similar between architectures, and this
    patch unifies them. The i386 version allocates bootmem memory, while
    the x86_64 version uses a fixed address.
    
    In this patch, we initialize the global trampoline_base to the x86_64 version,
    and i386 allocation can later override it.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 34c31178041b..b13b9d55f9ce 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -3,6 +3,7 @@
 #include <linux/module.h>
 #include <linux/sched.h>
 #include <linux/percpu.h>
+#include <linux/bootmem.h>
 
 #include <asm/nmi.h>
 #include <asm/irq.h>
@@ -38,6 +39,9 @@ EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 DEFINE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
 
+/* ready for x86_64, no harm for x86, since it will overwrite after alloc */
+unsigned char *trampoline_base = __va(SMP_TRAMPOLINE_BASE);
+
 /* representing cpus for which sibling maps can be computed */
 static cpumask_t cpu_sibling_setup_map;
 
@@ -117,6 +121,35 @@ cpumask_t cpu_coregroup_map(int cpu)
 		return c->llc_shared_map;
 }
 
+/*
+ * Currently trivial. Write the real->protected mode
+ * bootstrap into the page concerned. The caller
+ * has made sure it's suitably aligned.
+ */
+
+unsigned long __cpuinit setup_trampoline(void)
+{
+	memcpy(trampoline_base, trampoline_data,
+	       trampoline_end - trampoline_data);
+	return virt_to_phys(trampoline_base);
+}
+
+#ifdef CONFIG_X86_32
+/*
+ * We are called very early to get the low memory for the
+ * SMP bootup trampoline page.
+ */
+void __init smp_alloc_memory(void)
+{
+	trampoline_base = alloc_bootmem_low_pages(PAGE_SIZE);
+	/*
+	 * Has to be in very low memory so we can execute
+	 * real-mode AP code.
+	 */
+	if (__pa(trampoline_base) >= 0x9F000)
+		BUG();
+}
+#endif
 
 #ifdef CONFIG_HOTPLUG_CPU
 void remove_siblinginfo(int cpu)

commit 89b08200ad8bc8fb860da218c4f3bcc292bf286c
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Mar 3 14:13:08 2008 -0300

    x86: make x86_64 accept the max_cpus parameter
    
    The parameter passing parsing is done in the common smpboot.c
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c35cd319d1ed..34c31178041b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -268,3 +268,15 @@ void __cpu_die(unsigned int cpu)
 }
 #endif
 
+/*
+ * If the BIOS enumerates physical processors before logical,
+ * maxcpus=N at enumeration-time can be used to disable HT.
+ */
+static int __init parse_maxcpus(char *arg)
+{
+	extern unsigned int maxcpus;
+
+	maxcpus = simple_strtoul(arg, NULL, 0);
+	return 0;
+}
+early_param("maxcpus", parse_maxcpus);

commit 69c18c15d39c4622c6e2f97e5db4d8c9c43adaaa
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Mar 3 14:13:07 2008 -0300

    x86: merge __cpu_disable and cpu_die
    
    They are now equal, and are moved to a common file
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 644e60969f90..c35cd319d1ed 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -2,6 +2,13 @@
 #include <linux/smp.h>
 #include <linux/module.h>
 #include <linux/sched.h>
+#include <linux/percpu.h>
+
+#include <asm/nmi.h>
+#include <asm/irq.h>
+#include <asm/smp.h>
+#include <asm/cpu.h>
+#include <asm/numa.h>
 
 /* Number of siblings per CPU package */
 int smp_num_siblings = 1;
@@ -181,5 +188,83 @@ __init void prefill_possible_map(void)
 	for (i = 0; i < possible; i++)
 		cpu_set(i, cpu_possible_map);
 }
+
+static void __ref remove_cpu_from_maps(int cpu)
+{
+	cpu_clear(cpu, cpu_online_map);
+#ifdef CONFIG_X86_64
+	cpu_clear(cpu, cpu_callout_map);
+	cpu_clear(cpu, cpu_callin_map);
+	/* was set by cpu_init() */
+	clear_bit(cpu, (unsigned long *)&cpu_initialized);
+	clear_node_cpumask(cpu);
+#endif
+}
+
+int __cpu_disable(void)
+{
+	int cpu = smp_processor_id();
+
+	/*
+	 * Perhaps use cpufreq to drop frequency, but that could go
+	 * into generic code.
+	 *
+	 * We won't take down the boot processor on i386 due to some
+	 * interrupts only being able to be serviced by the BSP.
+	 * Especially so if we're not using an IOAPIC	-zwane
+	 */
+	if (cpu == 0)
+		return -EBUSY;
+
+	if (nmi_watchdog == NMI_LOCAL_APIC)
+		stop_apic_nmi_watchdog(NULL);
+	clear_local_APIC();
+
+	/*
+	 * HACK:
+	 * Allow any queued timer interrupts to get serviced
+	 * This is only a temporary solution until we cleanup
+	 * fixup_irqs as we do for IA64.
+	 */
+	local_irq_enable();
+	mdelay(1);
+
+	local_irq_disable();
+	remove_siblinginfo(cpu);
+
+	/* It's now safe to remove this processor from the online map */
+	remove_cpu_from_maps(cpu);
+	fixup_irqs(cpu_online_map);
+	return 0;
+}
+
+void __cpu_die(unsigned int cpu)
+{
+	/* We don't do anything here: idle task is faking death itself. */
+	unsigned int i;
+
+	for (i = 0; i < 10; i++) {
+		/* They ack this in play_dead by setting CPU_DEAD */
+		if (per_cpu(cpu_state, cpu) == CPU_DEAD) {
+			printk(KERN_INFO "CPU %d is now offline\n", cpu);
+			if (1 == num_online_cpus())
+				alternatives_smp_switch(0);
+			return;
+		}
+		msleep(100);
+	}
+	printk(KERN_ERR "CPU %u didn't die...\n", cpu);
+}
+#else /* ... !CONFIG_HOTPLUG_CPU */
+int __cpu_disable(void)
+{
+	return -ENOSYS;
+}
+
+void __cpu_die(unsigned int cpu)
+{
+	/* We said "no" in __cpu_disable */
+	BUG();
+}
 #endif
 

commit 70708a18e834fd709a4f497bb419ec84d1eb3511
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Mar 3 14:13:03 2008 -0300

    x86: move cpu_coregroup_map to common file
    
    it is equal between architectures
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index d774520a6b48..644e60969f90 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1,6 +1,7 @@
 #include <linux/init.h>
 #include <linux/smp.h>
 #include <linux/module.h>
+#include <linux/sched.h>
 
 /* Number of siblings per CPU package */
 int smp_num_siblings = 1;
@@ -95,6 +96,21 @@ void __cpuinit set_cpu_sibling_map(int cpu)
 	}
 }
 
+/* maps the cpu to the sched domain representing multi-core */
+cpumask_t cpu_coregroup_map(int cpu)
+{
+	struct cpuinfo_x86 *c = &cpu_data(cpu);
+	/*
+	 * For perf, we return last level cache shared map.
+	 * And for power savings, we return cpu_core_map
+	 */
+	if (sched_mc_power_savings || sched_smt_power_savings)
+		return per_cpu(cpu_core_map, cpu);
+	else
+		return c->llc_shared_map;
+}
+
+
 #ifdef CONFIG_HOTPLUG_CPU
 void remove_siblinginfo(int cpu)
 {

commit 768d95051bdaf60b4eb89b42c133b14627f478f2
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Mar 3 14:13:02 2008 -0300

    x86: move sibling functions to common file
    
    set_cpu_sibling_map() and remove_sibling_info() are
    equal between architectures, and are now moved to common
    file
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 40a3b56952ef..d774520a6b48 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -29,7 +29,95 @@ EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 /* Per CPU bogomips and other parameters */
 DEFINE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
+
+/* representing cpus for which sibling maps can be computed */
+static cpumask_t cpu_sibling_setup_map;
+
+void __cpuinit set_cpu_sibling_map(int cpu)
+{
+	int i;
+	struct cpuinfo_x86 *c = &cpu_data(cpu);
+
+	cpu_set(cpu, cpu_sibling_setup_map);
+
+	if (smp_num_siblings > 1) {
+		for_each_cpu_mask(i, cpu_sibling_setup_map) {
+			if (c->phys_proc_id == cpu_data(i).phys_proc_id &&
+			    c->cpu_core_id == cpu_data(i).cpu_core_id) {
+				cpu_set(i, per_cpu(cpu_sibling_map, cpu));
+				cpu_set(cpu, per_cpu(cpu_sibling_map, i));
+				cpu_set(i, per_cpu(cpu_core_map, cpu));
+				cpu_set(cpu, per_cpu(cpu_core_map, i));
+				cpu_set(i, c->llc_shared_map);
+				cpu_set(cpu, cpu_data(i).llc_shared_map);
+			}
+		}
+	} else {
+		cpu_set(cpu, per_cpu(cpu_sibling_map, cpu));
+	}
+
+	cpu_set(cpu, c->llc_shared_map);
+
+	if (current_cpu_data.x86_max_cores == 1) {
+		per_cpu(cpu_core_map, cpu) = per_cpu(cpu_sibling_map, cpu);
+		c->booted_cores = 1;
+		return;
+	}
+
+	for_each_cpu_mask(i, cpu_sibling_setup_map) {
+		if (per_cpu(cpu_llc_id, cpu) != BAD_APICID &&
+		    per_cpu(cpu_llc_id, cpu) == per_cpu(cpu_llc_id, i)) {
+			cpu_set(i, c->llc_shared_map);
+			cpu_set(cpu, cpu_data(i).llc_shared_map);
+		}
+		if (c->phys_proc_id == cpu_data(i).phys_proc_id) {
+			cpu_set(i, per_cpu(cpu_core_map, cpu));
+			cpu_set(cpu, per_cpu(cpu_core_map, i));
+			/*
+			 *  Does this new cpu bringup a new core?
+			 */
+			if (cpus_weight(per_cpu(cpu_sibling_map, cpu)) == 1) {
+				/*
+				 * for each core in package, increment
+				 * the booted_cores for this new cpu
+				 */
+				if (first_cpu(per_cpu(cpu_sibling_map, i)) == i)
+					c->booted_cores++;
+				/*
+				 * increment the core count for all
+				 * the other cpus in this package
+				 */
+				if (i != cpu)
+					cpu_data(i).booted_cores++;
+			} else if (i != cpu && !c->booted_cores)
+				c->booted_cores = cpu_data(i).booted_cores;
+		}
+	}
+}
+
 #ifdef CONFIG_HOTPLUG_CPU
+void remove_siblinginfo(int cpu)
+{
+	int sibling;
+	struct cpuinfo_x86 *c = &cpu_data(cpu);
+
+	for_each_cpu_mask(sibling, per_cpu(cpu_core_map, cpu)) {
+		cpu_clear(cpu, per_cpu(cpu_core_map, sibling));
+		/*/
+		 * last thread sibling in this cpu core going down
+		 */
+		if (cpus_weight(per_cpu(cpu_sibling_map, cpu)) == 1)
+			cpu_data(sibling).booted_cores--;
+	}
+
+	for_each_cpu_mask(sibling, per_cpu(cpu_sibling_map, cpu))
+		cpu_clear(cpu, per_cpu(cpu_sibling_map, sibling));
+	cpus_clear(per_cpu(cpu_sibling_map, cpu));
+	cpus_clear(per_cpu(cpu_core_map, cpu));
+	c->phys_proc_id = 0;
+	c->cpu_core_id = 0;
+	cpu_clear(cpu, cpu_sibling_setup_map);
+}
 
 int additional_cpus __initdata = -1;
 

commit a355352b97901d987f54ea7c7d7161eb51a3799c
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Mar 3 14:12:58 2008 -0300

    x86: move equal types to common file
    
    move definitions that are now equal in type from
    smpboot_{32,64}.c to smpboot.c
    
    cpu_callin_map is put temporarily in smp_64.h (already
    exists in smp_32.h), and will soon be merged.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index bffe10861390..40a3b56952ef 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1,6 +1,34 @@
 #include <linux/init.h>
 #include <linux/smp.h>
+#include <linux/module.h>
 
+/* Number of siblings per CPU package */
+int smp_num_siblings = 1;
+EXPORT_SYMBOL(smp_num_siblings);
+
+/* Last level cache ID of each logical CPU */
+DEFINE_PER_CPU(u16, cpu_llc_id) = BAD_APICID;
+
+/* bitmap of online cpus */
+cpumask_t cpu_online_map __read_mostly;
+EXPORT_SYMBOL(cpu_online_map);
+
+cpumask_t cpu_callin_map;
+cpumask_t cpu_callout_map;
+cpumask_t cpu_possible_map;
+EXPORT_SYMBOL(cpu_possible_map);
+
+/* representing HT siblings of each logical CPU */
+DEFINE_PER_CPU(cpumask_t, cpu_sibling_map);
+EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
+
+/* representing HT and core siblings of each logical CPU */
+DEFINE_PER_CPU(cpumask_t, cpu_core_map);
+EXPORT_PER_CPU_SYMBOL(cpu_core_map);
+
+/* Per CPU bogomips and other parameters */
+DEFINE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
+EXPORT_PER_CPU_SYMBOL(cpu_info);
 #ifdef CONFIG_HOTPLUG_CPU
 
 int additional_cpus __initdata = -1;

commit 68a1c3f8cd893f5c3c1396fec5be7d8acac4fc93
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Mar 3 14:12:42 2008 -0300

    x86: move prefill_possible_map to common file
    
    this patches moves prefill_possible_map() to smpboot.c
    Right now it is x86_64-specific, but nothing intrinsically
    prevents it to be used by i386
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
new file mode 100644
index 000000000000..bffe10861390
--- /dev/null
+++ b/arch/x86/kernel/smpboot.c
@@ -0,0 +1,53 @@
+#include <linux/init.h>
+#include <linux/smp.h>
+
+#ifdef CONFIG_HOTPLUG_CPU
+
+int additional_cpus __initdata = -1;
+
+static __init int setup_additional_cpus(char *s)
+{
+	return s && get_option(&s, &additional_cpus) ? 0 : -EINVAL;
+}
+early_param("additional_cpus", setup_additional_cpus);
+
+/*
+ * cpu_possible_map should be static, it cannot change as cpu's
+ * are onlined, or offlined. The reason is per-cpu data-structures
+ * are allocated by some modules at init time, and dont expect to
+ * do this dynamically on cpu arrival/departure.
+ * cpu_present_map on the other hand can change dynamically.
+ * In case when cpu_hotplug is not compiled, then we resort to current
+ * behaviour, which is cpu_possible == cpu_present.
+ * - Ashok Raj
+ *
+ * Three ways to find out the number of additional hotplug CPUs:
+ * - If the BIOS specified disabled CPUs in ACPI/mptables use that.
+ * - The user can overwrite it with additional_cpus=NUM
+ * - Otherwise don't reserve additional CPUs.
+ * We do this because additional CPUs waste a lot of memory.
+ * -AK
+ */
+__init void prefill_possible_map(void)
+{
+	int i;
+	int possible;
+
+	if (additional_cpus == -1) {
+		if (disabled_cpus > 0)
+			additional_cpus = disabled_cpus;
+		else
+			additional_cpus = 0;
+	}
+	possible = num_processors + additional_cpus;
+	if (possible > NR_CPUS)
+		possible = NR_CPUS;
+
+	printk(KERN_INFO "SMP: Allowing %d CPUs, %d hotplug CPUs\n",
+		possible, max_t(int, possible - num_processors, 0));
+
+	for (i = 0; i < possible; i++)
+		cpu_set(i, cpu_possible_map);
+}
+#endif
+
