commit 696ac2e3bf267f5a2b2ed7d34e64131f2287d0ad
Author: Qian Cai <cai@lca.pw>
Date:   Fri Apr 3 10:03:45 2020 -0400

    x86: ACPI: fix CPU hotplug deadlock
    
    Similar to commit 0266d81e9bf5 ("acpi/processor: Prevent cpu hotplug
    deadlock") except this is for acpi_processor_ffh_cstate_probe():
    
    "The problem is that the work is scheduled on the current CPU from the
    hotplug thread associated with that CPU.
    
    It's not required to invoke these functions via the workqueue because
    the hotplug thread runs on the target CPU already.
    
    Check whether current is a per cpu thread pinned on the target CPU and
    invoke the function directly to avoid the workqueue."
    
     WARNING: possible circular locking dependency detected
     ------------------------------------------------------
     cpuhp/1/15 is trying to acquire lock:
     ffffc90003447a28 ((work_completion)(&wfc.work)){+.+.}-{0:0}, at: __flush_work+0x4c6/0x630
    
     but task is already holding lock:
     ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #1 (cpu_hotplug_lock){++++}-{0:0}:
     cpus_read_lock+0x3e/0xc0
     irq_calc_affinity_vectors+0x5f/0x91
     __pci_enable_msix_range+0x10f/0x9a0
     pci_alloc_irq_vectors_affinity+0x13e/0x1f0
     pci_alloc_irq_vectors_affinity at drivers/pci/msi.c:1208
     pqi_ctrl_init+0x72f/0x1618 [smartpqi]
     pqi_pci_probe.cold.63+0x882/0x892 [smartpqi]
     local_pci_probe+0x7a/0xc0
     work_for_cpu_fn+0x2e/0x50
     process_one_work+0x57e/0xb90
     worker_thread+0x363/0x5b0
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     -> #0 ((work_completion)(&wfc.work)){+.+.}-{0:0}:
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     other info that might help us debug this:
    
     Chain exists of:
     (work_completion)(&wfc.work) --> cpuhp_state-up --> cpuidle_lock
    
     Possible unsafe locking scenario:
    
     CPU0                    CPU1
     ----                    ----
     lock(cpuidle_lock);
                             lock(cpuhp_state-up);
                             lock(cpuidle_lock);
     lock((work_completion)(&wfc.work));
    
     *** DEADLOCK ***
    
     3 locks held by cpuhp/1/15:
     #0: ffffffffaf51ab10 (cpu_hotplug_lock){++++}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #1: ffffffffaf51ad40 (cpuhp_state-up){+.+.}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #2: ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     Call Trace:
     dump_stack+0xa0/0xea
     print_circular_bug.cold.52+0x147/0x14c
     check_noncircular+0x295/0x2d0
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Tested-by: Borislav Petkov <bp@suse.de>
    [ rjw: Subject ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index caf2edccbad2..49ae4e1ac9cd 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -161,7 +161,8 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 
 	/* Make sure we are running on right CPU */
 
-	retval = work_on_cpu(cpu, acpi_processor_ffh_cstate_probe_cpu, cx);
+	retval = call_on_cpu(cpu, acpi_processor_ffh_cstate_probe_cpu, cx,
+			     false);
 	if (retval == 0) {
 		/* Use the hint in CST */
 		percpu_entry->states[cx->index].eax = cx->address;

commit f8c0e061cb83bd528ff0843e717bcebc846d4838
Author: Tony W Wang-oc <TonyWWang-oc@zhaoxin.com>
Date:   Tue Jun 18 08:37:29 2019 +0000

    x86/acpi/cstate: Add Zhaoxin processors support for cache flush policy in C3
    
    Same as Intel, Zhaoxin MP CPUs support C3 share cache and on all
    recent Zhaoxin platforms ARB_DISABLE is a nop. So set related
    flags correctly in the same way as Intel does.
    
    Signed-off-by: Tony W Wang-oc <TonyWWang-oc@zhaoxin.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "hpa@zytor.com" <hpa@zytor.com>
    Cc: "gregkh@linuxfoundation.org" <gregkh@linuxfoundation.org>
    Cc: "rjw@rjwysocki.net" <rjw@rjwysocki.net>
    Cc: "lenb@kernel.org" <lenb@kernel.org>
    Cc: David Wang <DavidWang@zhaoxin.com>
    Cc: "Cooper Yan(BJ-RD)" <CooperYan@zhaoxin.com>
    Cc: "Qiyuan Wang(BJ-RD)" <QiyuanWang@zhaoxin.com>
    Cc: "Herry Yang(BJ-RD)" <HerryYang@zhaoxin.com>
    Link: https://lkml.kernel.org/r/a370503660994669991a7f7cda7c5e98@zhaoxin.com

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index a5e5484988fd..caf2edccbad2 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -64,6 +64,21 @@ void acpi_processor_power_init_bm_check(struct acpi_processor_flags *flags,
 		    c->x86_stepping >= 0x0e))
 			flags->bm_check = 1;
 	}
+
+	if (c->x86_vendor == X86_VENDOR_ZHAOXIN) {
+		/*
+		 * All Zhaoxin CPUs that support C3 share cache.
+		 * And caches should not be flushed by software while
+		 * entering C3 type state.
+		 */
+		flags->bm_check = 1;
+		/*
+		 * On all recent Zhaoxin platforms, ARB_DISABLE is a nop.
+		 * So, set bm_control to zero to indicate that ARB_DISABLE
+		 * is not required while entering C3 type state.
+		 */
+		flags->bm_control = 0;
+	}
 }
 EXPORT_SYMBOL(acpi_processor_power_init_bm_check);
 

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index cb6e076a6d39..a5e5484988fd 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Copyright (C) 2005 Intel Corporation
  * 	Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>

commit 987ddbe4870b53623d76ac64044c55a13e368113
Author: David Wang <davidwang@zhaoxin.com>
Date:   Thu Dec 27 16:41:50 2018 +0800

    x86/power: Optimize C3 entry on Centaur CPUs
    
    For new Centaur CPUs the ucode will take care of the preservation of cache coherence
    between CPU cores in C-states regardless of how deep the C-states are. So, it is not
    necessary to flush the caches in software befor entering C3. This useless operation
    will cause performance drop for the cores which share some caches with the idling core.
    
    Signed-off-by: David Wang <davidwang@zhaoxin.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: brucechang@via-alliance.com
    Cc: cooperyan@zhaoxin.com
    Cc: len.brown@intel.com
    Cc: linux-pm@kernel.org
    Cc: qiyuanwang@zhaoxin.com
    Cc: rjw@rjwysocki.net
    Cc: timguo@zhaoxin.com
    Link: http://lkml.kernel.org/r/1545900110-2757-1-git-send-email-davidwang@zhaoxin.com
    [ Tidy up the comment. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 158ad1483c43..cb6e076a6d39 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -51,6 +51,18 @@ void acpi_processor_power_init_bm_check(struct acpi_processor_flags *flags,
 	if (c->x86_vendor == X86_VENDOR_INTEL &&
 	    (c->x86 > 0xf || (c->x86 == 6 && c->x86_model >= 0x0f)))
 			flags->bm_control = 0;
+	/*
+	 * For all recent Centaur CPUs, the ucode will make sure that each
+	 * core can keep cache coherence with each other while entering C3
+	 * type state. So, set bm_check to 1 to indicate that the kernel
+	 * doesn't need to execute a cache flush operation (WBINVD) when
+	 * entering C3 type state.
+	 */
+	if (c->x86_vendor == X86_VENDOR_CENTAUR) {
+		if (c->x86 > 6 || (c->x86 == 6 && c->x86_model == 0x0f &&
+		    c->x86_stepping >= 0x0e))
+			flags->bm_check = 1;
+	}
 }
 EXPORT_SYMBOL(acpi_processor_power_init_bm_check);
 

commit 3d95b89e573bb8460886ff2ca769b067bfcc50aa
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Mon Aug 13 08:43:09 2018 -0400

    x86/ACPI/cstate: Make APCI C1 FFH MWAIT C-state description vendor-neutral
    
    Commit 5209654a46ee (x86/ACPI/cstate: Allow ACPI C1 FFH MWAIT use on
    AMD systems) forgot to update the ACPI C1 idle state description and
    tools like turbostat display "ACPI FFH INTEL MWAIT 0x0" which is
    quite confusing on an AMD system.
    
    Drop the "INTEL" part from the ACPI C1 FFH MWAIT C-state description
    to avoid confusion.
    
    Fixes: 5209654a46ee (x86/ACPI/cstate: Allow ACPI C1 FFH MWAIT use on AMD systems)
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    [ rjw: Subject & changelog ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index dde437f5d14f..158ad1483c43 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -108,7 +108,7 @@ static long acpi_processor_ffh_cstate_probe_cpu(void *_cx)
 			cx->type);
 	}
 	snprintf(cx->desc,
-			ACPI_CX_DESC_LEN, "ACPI FFH INTEL MWAIT 0x%x",
+			ACPI_CX_DESC_LEN, "ACPI FFH MWAIT 0x%x",
 			cx->address);
 out:
 	return retval;

commit 5209654a46ee71137ad9b06da99d4ef2794475af
Author: Yazen Ghannam <yazen.ghannam@amd.com>
Date:   Wed Jun 7 10:19:46 2017 -0500

    x86/ACPI/cstate: Allow ACPI C1 FFH MWAIT use on AMD systems
    
    AMD systems support the Monitor/Mwait instructions and these can be used
    for ACPI C1 in the same way as on Intel systems.
    
    Three things are needed:
     1) This patch.
     2) BIOS that declares a C1 state in _CST to use FFH, with correct values.
     3) CPUID_Fn00000005_EDX is non-zero on the system.
    
    The BIOS on AMD systems have historically not defined a C1 state in _CST,
    so the acpi_idle driver uses HALT for ACPI C1.
    
    Currently released systems have CPUID_Fn00000005_EDX as reserved/RAZ. If a
    BIOS is released for these systems that requests a C1 state with FFH, the
    FFH implementation in Linux will fail since CPUID_Fn00000005_EDX is 0. The
    acpi_idle driver will then fallback to using HALT for ACPI C1.
    
    Future systems are expected to have non-zero CPUID_Fn00000005_EDX and BIOS
    support for using FFH for ACPI C1.
    
    Allow ffh_cstate_init() to succeed on AMD systems.
    
    Tested on Fam15h and Fam17h systems.
    
    Signed-off-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 8233a630280f..dde437f5d14f 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -167,7 +167,8 @@ static int __init ffh_cstate_init(void)
 {
 	struct cpuinfo_x86 *c = &boot_cpu_data;
 
-	if (c->x86_vendor != X86_VENDOR_INTEL)
+	if (c->x86_vendor != X86_VENDOR_INTEL &&
+	    c->x86_vendor != X86_VENDOR_AMD)
 		return -1;
 
 	cpu_cstate_entry = alloc_percpu(struct cstate_entry);

commit 2dc8ffad8c53e65f85d1a9ece2721463d729054a
Author: Nick Desaulniers <nick.desaulniers@gmail.com>
Date:   Mon Dec 12 15:28:05 2016 -0800

    ACPI / idle: small formatting fixes
    
    A quick cleanup with scripts/checkpatch.pl -f <file>.
    
    Signed-off-by: Nick Desaulniers <nick.desaulniers@gmail.com>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index af15f4444330..8233a630280f 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -12,7 +12,6 @@
 #include <linux/sched.h>
 
 #include <acpi/processor.h>
-#include <asm/acpi.h>
 #include <asm/mwait.h>
 #include <asm/special_insns.h>
 
@@ -89,7 +88,8 @@ static long acpi_processor_ffh_cstate_probe_cpu(void *_cx)
 	retval = 0;
 	/* If the HW does not support any sub-states in this C-state */
 	if (num_cstate_subtype == 0) {
-		pr_warn(FW_BUG "ACPI MWAIT C-state 0x%x not supported by HW (0x%x)\n", cx->address, edx_part);
+		pr_warn(FW_BUG "ACPI MWAIT C-state 0x%x not supported by HW (0x%x)\n",
+				cx->address, edx_part);
 		retval = -1;
 		goto out;
 	}
@@ -104,8 +104,8 @@ static long acpi_processor_ffh_cstate_probe_cpu(void *_cx)
 	if (!mwait_supported[cstate_type]) {
 		mwait_supported[cstate_type] = 1;
 		printk(KERN_DEBUG
-			"Monitor-Mwait will be used to enter C-%d "
-			"state\n", cx->type);
+			"Monitor-Mwait will be used to enter C-%d state\n",
+			cx->type);
 	}
 	snprintf(cx->desc,
 			ACPI_CX_DESC_LEN, "ACPI FFH INTEL MWAIT 0x%x",
@@ -166,6 +166,7 @@ EXPORT_SYMBOL_GPL(acpi_processor_ffh_cstate_enter);
 static int __init ffh_cstate_init(void)
 {
 	struct cpuinfo_x86 *c = &boot_cpu_data;
+
 	if (c->x86_vendor != X86_VENDOR_INTEL)
 		return -1;
 

commit 6727ad9e206cc08b80d8000a4d67f8417e53539d
Author: Chris Metcalf <cmetcalf@mellanox.com>
Date:   Fri Oct 7 17:02:55 2016 -0700

    nmi_backtrace: generate one-line reports for idle cpus
    
    When doing an nmi backtrace of many cores, most of which are idle, the
    output is a little overwhelming and very uninformative.  Suppress
    messages for cpus that are idling when they are interrupted and just
    emit one line, "NMI backtrace for N skipped: idling at pc 0xNNN".
    
    We do this by grouping all the cpuidle code together into a new
    .cpuidle.text section, and then checking the address of the interrupted
    PC to see if it lies within that section.
    
    This commit suitably tags x86 and tile idle routines, and only adds in
    the minimal framework for other architectures.
    
    Link: http://lkml.kernel.org/r/1472487169-14923-5-git-send-email-cmetcalf@mellanox.com
    Signed-off-by: Chris Metcalf <cmetcalf@mellanox.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Daniel Thompson <daniel.thompson@linaro.org> [arm]
    Tested-by: Petr Mladek <pmladek@suse.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index bdfad642123f..af15f4444330 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -152,7 +152,7 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 }
 EXPORT_SYMBOL_GPL(acpi_processor_ffh_cstate_probe);
 
-void acpi_processor_ffh_cstate_enter(struct acpi_processor_cx *cx)
+void __cpuidle acpi_processor_ffh_cstate_enter(struct acpi_processor_cx *cx)
 {
 	unsigned int cpu = smp_processor_id();
 	struct cstate_entry *percpu_entry;

commit 186f43608a5c827f8284fe4559225b4dccaa49ef
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:56 2016 -0400

    x86/kernel: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed some implicit header usage that was fixed up accordingly.
    
    Note that some bool/obj-y instances remain since module.h is
    the header for some exception table entry stuff, and for things
    like __init_or_module (code that is tossed when MODULES=n).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-4-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 4b28159e0421..bdfad642123f 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -5,7 +5,7 @@
  */
 
 #include <linux/kernel.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/init.h>
 #include <linux/acpi.h>
 #include <linux/cpu.h>

commit 2194324d8bbbad1b179c08b6095649b06abd62d5
Author: Len Brown <len.brown@intel.com>
Date:   Fri Feb 14 01:14:13 2014 -0500

    ACPI idle: permit sparse C-state sub-state numbers
    
    Linux uses CPUID.MWAIT.EDX to validate the C-states
    reported by ACPI, silently discarding states which
    are not supported by the HW.
    
    This test is too restrictive, as some HW now uses
    sparse sub-state numbering, so the sub-state number
    may be higher than the number of sub-states...
    
    Also, rather than silently ignoring an invalid state,
    we should complain about a firmware bug.
    
    In practice...
    
    Bay Trail systems originally supported C6-no-shrink as
    MWAIT sub-state 0x58, and in CPUID.MWAIT.EDX 0x03000000
    indicated that there were 3 MWAIT-C6 sub-states.
    So acpi_idle would discard that C-state because 8 >= 3.
    
    Upon discovering this issue, the ucode was updated so that
    C6-no-shrink was also exported as 0x51, and the BIOS was
    updated to match.  However, systems shipped with 0x58,
    will never get a BIOS update, and this patch allows
    Linux to see C6-no-shrink on early Bay Trail.
    
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index e69182fd01cf..4b28159e0421 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -87,7 +87,9 @@ static long acpi_processor_ffh_cstate_probe_cpu(void *_cx)
 	num_cstate_subtype = edx_part & MWAIT_SUBSTATE_MASK;
 
 	retval = 0;
-	if (num_cstate_subtype < (cx->address & MWAIT_SUBSTATE_MASK)) {
+	/* If the HW does not support any sub-states in this C-state */
+	if (num_cstate_subtype == 0) {
+		pr_warn(FW_BUG "ACPI MWAIT C-state 0x%x not supported by HW (0x%x)\n", cx->address, edx_part);
 		retval = -1;
 		goto out;
 	}

commit 16824255394f55adf31b9a96a9965d8c15bdac4c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Dec 12 15:08:36 2013 +0100

    x86, acpi, idle: Restructure the mwait idle routines
    
    People seem to delight in writing wrong and broken mwait idle routines;
    collapse the lot.
    
    This leaves mwait_play_dead() the sole remaining user of __mwait() and
    new __mwait() users are probably doing it wrong.
    
    Also remove __sti_mwait() as its unused.
    
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jacob Jun Pan <jacob.jun.pan@linux.intel.com>
    Cc: Mike Galbraith <bitbucket@online.de>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Acked-by: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20131212141654.616820819@infradead.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index d2b7f27781bc..e69182fd01cf 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -150,29 +150,6 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 }
 EXPORT_SYMBOL_GPL(acpi_processor_ffh_cstate_probe);
 
-/*
- * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
- * which can obviate IPI to trigger checking of need_resched.
- * We execute MONITOR against need_resched and enter optimized wait state
- * through MWAIT. Whenever someone changes need_resched, we would be woken
- * up from MWAIT (without an IPI).
- *
- * New with Core Duo processors, MWAIT can take some hints based on CPU
- * capability.
- */
-void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
-{
-	if (!need_resched()) {
-		if (this_cpu_has(X86_FEATURE_CLFLUSH_MONITOR))
-			clflush((void *)&current_thread_info()->flags);
-
-		__monitor((void *)&current_thread_info()->flags, 0, 0);
-		smp_mb();
-		if (!need_resched())
-			__mwait(ax, cx);
-	}
-}
-
 void acpi_processor_ffh_cstate_enter(struct acpi_processor_cx *cx)
 {
 	unsigned int cpu = smp_processor_id();

commit f05e798ad4c09255f590f5b2c00a7ca6c172f983
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:11:12 2012 +0100

    Disintegrate asm/system.h for X86
    
    Disintegrate asm/system.h for X86.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    cc: x86@kernel.org

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index f50e7fb2a201..d2b7f27781bc 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -14,6 +14,7 @@
 #include <acpi/processor.h>
 #include <asm/acpi.h>
 #include <asm/mwait.h>
+#include <asm/special_insns.h>
 
 /*
  * Initialize bm_flags based on the CPU cache properties

commit 4bfc8288bc4a64529c5547d17349a2a1f4675507
Author: Len Brown <len.brown@intel.com>
Date:   Wed Mar 30 23:52:29 2011 -0400

    x86 idle: move mwait_idle_with_hints() to where it is used
    
    ...and make it static
    
    no functional change
    
    cc: x86@kernel.org
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 5812404a0d4c..f50e7fb2a201 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -149,6 +149,29 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 }
 EXPORT_SYMBOL_GPL(acpi_processor_ffh_cstate_probe);
 
+/*
+ * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
+ * which can obviate IPI to trigger checking of need_resched.
+ * We execute MONITOR against need_resched and enter optimized wait state
+ * through MWAIT. Whenever someone changes need_resched, we would be woken
+ * up from MWAIT (without an IPI).
+ *
+ * New with Core Duo processors, MWAIT can take some hints based on CPU
+ * capability.
+ */
+void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
+{
+	if (!need_resched()) {
+		if (this_cpu_has(X86_FEATURE_CLFLUSH_MONITOR))
+			clflush((void *)&current_thread_info()->flags);
+
+		__monitor((void *)&current_thread_info()->flags, 0, 0);
+		smp_mb();
+		if (!need_resched())
+			__mwait(ax, cx);
+	}
+}
+
 void acpi_processor_ffh_cstate_enter(struct acpi_processor_cx *cx)
 {
 	unsigned int cpu = smp_processor_id();

commit 2a8b67fb72c4c4bc15fe8095e3ed613789c8b82f
Merge: b6f7e38dbb31 ce5f68246bf2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:45:38 2010 -0700

    Merge branch 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, hotplug: In the MWAIT case of play_dead, CLFLUSH the cache line
      x86, hotplug: Move WBINVD back outside the play_dead loop
      x86, hotplug: Use mwait to offline a processor, fix the legacy case
      x86, mwait: Move mwait constants to a common header file

commit bd126b23a2f30c3c7d268db2b96866923eb732a5
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Sun Aug 8 02:17:29 2010 +0900

    ACPI: add missing __percpu markup in arch/x86/kernel/acpi/cstate.c
    
    cpu_cstate_entry is a percpu pointer
    but was missing __percpu markup.
    
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index fb7a5f052e2b..fb16f17e59be 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -61,7 +61,7 @@ struct cstate_entry {
 		unsigned int ecx;
 	} states[ACPI_PROCESSOR_MAX_POWER];
 };
-static struct cstate_entry *cpu_cstate_entry;	/* per CPU ptr */
+static struct cstate_entry __percpu *cpu_cstate_entry;	/* per CPU ptr */
 
 static short mwait_supported[ACPI_PROCESSOR_MAX_POWER];
 

commit bc83cccc761953f878088cdfa682de0970b5561f
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Sep 17 15:36:40 2010 -0700

    x86, mwait: Move mwait constants to a common header file
    
    We have MWAIT constants spread across three different .c files, for no
    good reason.  Move them all into a common header file.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Len Brown <lenb@kernel.org>
    LKML-Reference: <tip-*@git.kernel.org>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index fb7a5f052e2b..bcc4adda7b9b 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -13,6 +13,7 @@
 
 #include <acpi/processor.h>
 #include <asm/acpi.h>
+#include <asm/mwait.h>
 
 /*
  * Initialize bm_flags based on the CPU cache properties
@@ -65,16 +66,6 @@ static struct cstate_entry *cpu_cstate_entry;	/* per CPU ptr */
 
 static short mwait_supported[ACPI_PROCESSOR_MAX_POWER];
 
-#define MWAIT_SUBSTATE_MASK	(0xf)
-#define MWAIT_CSTATE_MASK	(0xf)
-#define MWAIT_SUBSTATE_SIZE	(4)
-
-#define CPUID_MWAIT_LEAF (5)
-#define CPUID5_ECX_EXTENSIONS_SUPPORTED (0x1)
-#define CPUID5_ECX_INTERRUPT_BREAK	(0x2)
-
-#define MWAIT_ECX_INTERRUPT_BREAK	(0x1)
-
 #define NATIVE_CSTATE_BEYOND_HALT	(2)
 
 static long acpi_processor_ffh_cstate_probe_cpu(void *_cx)

commit 718be4aaf3613cf7c2d097f925abc3d3553c0605
Author: Len Brown <len.brown@intel.com>
Date:   Thu Jul 22 16:54:27 2010 -0400

    ACPI: skip checking BM_STS if the BIOS doesn't ask for it
    
    It turns out that there is a bit in the _CST for Intel FFH C3
    that tells the OS if we should be checking BM_STS or not.
    
    Linux has been unconditionally checking BM_STS.
    If the chip-set is configured to enable BM_STS,
    it can retard or completely prevent entry into
    deep C-states -- as illustrated by turbostat:
    
    http://userweb.kernel.org/~lenb/acpi/utils/pmtools/turbostat/
    
    ref: Intel Processor Vendor-Specific ACPI Interface Specification
    table 4 "_CST FFH GAS Field Encoding"
    Bit 1: Set to 1 if OSPM should use Bus Master avoidance for this C-state
    
    https://bugzilla.kernel.org/show_bug.cgi?id=15886
    
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 2e837f5080fe..fb7a5f052e2b 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -145,6 +145,15 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 		percpu_entry->states[cx->index].eax = cx->address;
 		percpu_entry->states[cx->index].ecx = MWAIT_ECX_INTERRUPT_BREAK;
 	}
+
+	/*
+	 * For _CST FFH on Intel, if GAS.access_size bit 1 is cleared,
+	 * then we should skip checking BM_STS for this C-state.
+	 * ref: "Intel Processor Vendor-Specific ACPI Interface Specification"
+	 */
+	if ((c->x86_vendor == X86_VENDOR_INTEL) && !(reg->access_size & 0x2))
+		cx->bm_sts_skip = 1;
+
 	return retval;
 }
 EXPORT_SYMBOL_GPL(acpi_processor_ffh_cstate_probe);

commit 03a05ed1152944000151d57b71000de287a1eb02
Author: Zhao Yakui <yakui.zhao@intel.com>
Date:   Fri Dec 11 15:17:20 2009 +0800

    ACPI: Use the ARB_DISABLE for the CPU which model id is less than 0x0f.
    
    Currently, ARB_DISABLE is a NOP on all of the recent Intel platforms.
    For such platforms, reduce contention on c3_lock by skipping the fake
    ARB_DISABLE.
    
    The cpu model id on one laptop is 14. If we disable ARB_DISABLE on this box,
    the box can't be booted correctly. But if we still enable ARB_DISABLE on this
    box, the box can be booted correctly.
    
    So we still use the ARB_DISABLE for the cpu which mode id is less than 0x0f.
    
    http://bugzilla.kernel.org/show_bug.cgi?id=14700
    
    Signed-off-by: Zhao Yakui <yakui.zhao@intel.com>
    Acked-by: Pallipadi, Venkatesh <venkatesh.pallipadi@intel.com>
    cc: stable@kernel.org
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 59cdfa4686b2..2e837f5080fe 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -48,7 +48,7 @@ void acpi_processor_power_init_bm_check(struct acpi_processor_flags *flags,
 	 * P4, Core and beyond CPUs
 	 */
 	if (c->x86_vendor == X86_VENDOR_INTEL &&
-	    (c->x86 > 0xf || (c->x86 == 6 && c->x86_model >= 14)))
+	    (c->x86 > 0xf || (c->x86 == 6 && c->x86_model >= 0x0f)))
 			flags->bm_control = 0;
 }
 EXPORT_SYMBOL(acpi_processor_power_init_bm_check);

commit 3e2ada5867b7e9fa0b296d30fa8f3726ebd0a8b7
Author: Zhao Yakui <yakui.zhao@intel.com>
Date:   Sun Sep 27 03:30:51 2009 -0400

    ACPI: fix Compaq Evo N800c (Pentium 4m) boot hang regression
    
    Don't disable ARB_DISABLE when the familary ID is 0x0F.
    
    http://bugzilla.kernel.org/show_bug.cgi?id=14211
    
    This was a 2.6.31 regression, and so this patch
    needs to be applied to 2.6.31.stable
    
    Signed-off-by: Zhao Yakui <yakui.zhao@intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 8c44c232efcb..59cdfa4686b2 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -48,7 +48,7 @@ void acpi_processor_power_init_bm_check(struct acpi_processor_flags *flags,
 	 * P4, Core and beyond CPUs
 	 */
 	if (c->x86_vendor == X86_VENDOR_INTEL &&
-	    (c->x86 > 0x6 || (c->x86 == 6 && c->x86_model >= 14)))
+	    (c->x86 > 0xf || (c->x86 == 6 && c->x86_model >= 14)))
 			flags->bm_control = 0;
 }
 EXPORT_SYMBOL(acpi_processor_power_init_bm_check);

commit ee1ca48fae7e575d5e399d4fdcfe0afc1212a64c
Author: Pallipadi, Venkatesh <venkatesh.pallipadi@intel.com>
Date:   Thu May 21 17:09:10 2009 -0700

    ACPI: Disable ARB_DISABLE on platforms where it is not needed
    
    ARB_DISABLE is a NOP on all of the recent Intel platforms.
    
    For such platforms, reduce contention on c3_lock
    by skipping the fake ARB_DISABLE.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index bbbe4bbb6f34..8c44c232efcb 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -34,12 +34,22 @@ void acpi_processor_power_init_bm_check(struct acpi_processor_flags *flags,
 		flags->bm_check = 1;
 	else if (c->x86_vendor == X86_VENDOR_INTEL) {
 		/*
-		 * Today all CPUs that support C3 share cache.
-		 * TBD: This needs to look at cache shared map, once
-		 * multi-core detection patch makes to the base.
+		 * Today all MP CPUs that support C3 share cache.
+		 * And caches should not be flushed by software while
+		 * entering C3 type state.
 		 */
 		flags->bm_check = 1;
 	}
+
+	/*
+	 * On all recent Intel platforms, ARB_DISABLE is a nop.
+	 * So, set bm_control to zero to indicate that ARB_DISABLE
+	 * is not required while entering C3 type state on
+	 * P4, Core and beyond CPUs
+	 */
+	if (c->x86_vendor == X86_VENDOR_INTEL &&
+	    (c->x86 > 0x6 || (c->x86 == 6 && c->x86_model >= 14)))
+			flags->bm_control = 0;
 }
 EXPORT_SYMBOL(acpi_processor_power_init_bm_check);
 

commit 4e9b1c184cadbece3694603de5f880b6e35bd7a7
Merge: 0176260fc308 36c401a44abc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 10 06:12:18 2009 -0800

    Merge branch 'cpus4096-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'cpus4096-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      [IA64] fix typo in cpumask_of_pcibus()
      x86: fix x86_32 builds for summit and es7000 arch's
      cpumask: use work_on_cpu in acpi-cpufreq.c for read_measured_perf_ctrs
      cpumask: use work_on_cpu in acpi-cpufreq.c for drv_read and drv_write
      cpumask: use cpumask_var_t in acpi-cpufreq.c
      cpumask: use work_on_cpu in acpi/cstate.c
      cpumask: convert struct cpufreq_policy to cpumask_var_t
      cpumask: replace CPUMASK_ALLOC etc with cpumask_var_t
      x86: cleanup remaining cpumask_t ops in smpboot code
      cpumask: update pci_bus_show_cpuaffinity to use new cpumask API
      cpumask: update local_cpus_show to use new cpumask API
      ia64: cpumask fix for is_affinity_mask_valid()

commit 13b40a1a065824d2d4e55c8b48ea9f3f9d162929
Author: Zhao Yakui <yakui.zhao@intel.com>
Date:   Sun Jan 4 12:04:21 2009 +0800

    ACPI: Avoid array address overflow when _CST MWAIT hint bits are set
    
    The Cx Register address obtained from the _CST object is used as the MWAIT
    hints if the register type is FFixedHW. And it is used to check whether
    the Cx type is supported or not.
    
    On some boxes the following Cx state package is obtained from _CST object:
        >{
                    ResourceTemplate ()
                    {
                        Register (FFixedHW,
                            0x01,               // Bit Width
                            0x02,               // Bit Offset
                            0x0000000000889759, // Address
                            0x03,               // Access Size
                            )
                    },
    
                    0x03,
                    0xF5,
                    0x015E }
    
       In such case we should use the bit[7:4] of Cx address to check whether
    the Cx type is supported or not.
    
    mask the MWAIT hint to avoid array address overflow
    
    Signed-off-by: Zhao Yakui <yakui.zhao@intel.com>
    Acked-by:Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index c2502eb9aa83..a4805b3b4095 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -56,6 +56,7 @@ static struct cstate_entry *cpu_cstate_entry;	/* per CPU ptr */
 static short mwait_supported[ACPI_PROCESSOR_MAX_POWER];
 
 #define MWAIT_SUBSTATE_MASK	(0xf)
+#define MWAIT_CSTATE_MASK	(0xf)
 #define MWAIT_SUBSTATE_SIZE	(4)
 
 #define CPUID_MWAIT_LEAF (5)
@@ -98,7 +99,8 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 	cpuid(CPUID_MWAIT_LEAF, &eax, &ebx, &ecx, &edx);
 
 	/* Check whether this particular cx_type (in CST) is supported or not */
-	cstate_type = (cx->address >> MWAIT_SUBSTATE_SIZE) + 1;
+	cstate_type = ((cx->address >> MWAIT_SUBSTATE_SIZE) &
+			MWAIT_CSTATE_MASK) + 1;
 	edx_part = edx >> (cstate_type * MWAIT_SUBSTATE_SIZE);
 	num_cstate_subtype = edx_part & MWAIT_SUBSTATE_MASK;
 

commit c74f31c035f46a095a0c72f80246a65b314205a5
Author: Mike Travis <travis@sgi.com>
Date:   Sun Jan 4 05:18:07 2009 -0800

    cpumask: use work_on_cpu in acpi/cstate.c
    
    Impact: use new cpumask API to reduce stack usage
    
    Replace the saving of current->cpus_allowed and set_cpus_allowed_ptr() with
    a work_on_cpu function for the acpi_processor_ffh_cstate_probe() function.
    
    Basically splits acpi_processor_ffh_cstate_probe() into two functions, the
    other being acpi_processor_ffh_cstate_probe_cpu which is the work function
    run on the designated cpu.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index c2502eb9aa83..cf5ec586f220 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -66,35 +66,15 @@ static short mwait_supported[ACPI_PROCESSOR_MAX_POWER];
 
 #define NATIVE_CSTATE_BEYOND_HALT	(2)
 
-int acpi_processor_ffh_cstate_probe(unsigned int cpu,
-		struct acpi_processor_cx *cx, struct acpi_power_register *reg)
+static long acpi_processor_ffh_cstate_probe_cpu(void *_cx)
 {
-	struct cstate_entry *percpu_entry;
-	struct cpuinfo_x86 *c = &cpu_data(cpu);
-
-	cpumask_t saved_mask;
-	int retval;
+	struct acpi_processor_cx *cx = _cx;
+	long retval;
 	unsigned int eax, ebx, ecx, edx;
 	unsigned int edx_part;
 	unsigned int cstate_type; /* C-state type and not ACPI C-state type */
 	unsigned int num_cstate_subtype;
 
-	if (!cpu_cstate_entry || c->cpuid_level < CPUID_MWAIT_LEAF )
-		return -1;
-
-	if (reg->bit_offset != NATIVE_CSTATE_BEYOND_HALT)
-		return -1;
-
-	percpu_entry = per_cpu_ptr(cpu_cstate_entry, cpu);
-	percpu_entry->states[cx->index].eax = 0;
-	percpu_entry->states[cx->index].ecx = 0;
-
-	/* Make sure we are running on right CPU */
-	saved_mask = current->cpus_allowed;
-	retval = set_cpus_allowed_ptr(current, &cpumask_of_cpu(cpu));
-	if (retval)
-		return -1;
-
 	cpuid(CPUID_MWAIT_LEAF, &eax, &ebx, &ecx, &edx);
 
 	/* Check whether this particular cx_type (in CST) is supported or not */
@@ -114,21 +94,45 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 		retval = -1;
 		goto out;
 	}
-	percpu_entry->states[cx->index].ecx = MWAIT_ECX_INTERRUPT_BREAK;
-
-	/* Use the hint in CST */
-	percpu_entry->states[cx->index].eax = cx->address;
 
 	if (!mwait_supported[cstate_type]) {
 		mwait_supported[cstate_type] = 1;
-		printk(KERN_DEBUG "Monitor-Mwait will be used to enter C-%d "
-		       "state\n", cx->type);
+		printk(KERN_DEBUG
+			"Monitor-Mwait will be used to enter C-%d "
+			"state\n", cx->type);
 	}
-	snprintf(cx->desc, ACPI_CX_DESC_LEN, "ACPI FFH INTEL MWAIT 0x%x",
-		 cx->address);
-
+	snprintf(cx->desc,
+			ACPI_CX_DESC_LEN, "ACPI FFH INTEL MWAIT 0x%x",
+			cx->address);
 out:
-	set_cpus_allowed_ptr(current, &saved_mask);
+	return retval;
+}
+
+int acpi_processor_ffh_cstate_probe(unsigned int cpu,
+		struct acpi_processor_cx *cx, struct acpi_power_register *reg)
+{
+	struct cstate_entry *percpu_entry;
+	struct cpuinfo_x86 *c = &cpu_data(cpu);
+	long retval;
+
+	if (!cpu_cstate_entry || c->cpuid_level < CPUID_MWAIT_LEAF)
+		return -1;
+
+	if (reg->bit_offset != NATIVE_CSTATE_BEYOND_HALT)
+		return -1;
+
+	percpu_entry = per_cpu_ptr(cpu_cstate_entry, cpu);
+	percpu_entry->states[cx->index].eax = 0;
+	percpu_entry->states[cx->index].ecx = 0;
+
+	/* Make sure we are running on right CPU */
+
+	retval = work_on_cpu(cpu, acpi_processor_ffh_cstate_probe_cpu, cx);
+	if (retval == 0) {
+		/* Use the hint in CST */
+		percpu_entry->states[cx->index].eax = cx->address;
+		percpu_entry->states[cx->index].ecx = MWAIT_ECX_INTERRUPT_BREAK;
+	}
 	return retval;
 }
 EXPORT_SYMBOL_GPL(acpi_processor_ffh_cstate_probe);

commit 0bc3cc03fa6e1c20aecb5a33356bcaae410640b9
Author: Mike Travis <travis@sgi.com>
Date:   Thu Jul 24 18:21:31 2008 -0700

    cpumask: change cpumask_of_cpu_ptr to use new cpumask_of_cpu
    
      * Replace previous instances of the cpumask_of_cpu_ptr* macros
        with a the new (lvalue capable) generic cpumask_of_cpu().
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jack Steiner <steiner@sgi.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 9220cf46aa10..c2502eb9aa83 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -73,7 +73,6 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 	struct cpuinfo_x86 *c = &cpu_data(cpu);
 
 	cpumask_t saved_mask;
-	cpumask_of_cpu_ptr(new_mask, cpu);
 	int retval;
 	unsigned int eax, ebx, ecx, edx;
 	unsigned int edx_part;
@@ -92,7 +91,7 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 
 	/* Make sure we are running on right CPU */
 	saved_mask = current->cpus_allowed;
-	retval = set_cpus_allowed_ptr(current, new_mask);
+	retval = set_cpus_allowed_ptr(current, &cpumask_of_cpu(cpu));
 	if (retval)
 		return -1;
 

commit 65c011845316d3c1381f478ca0d8265c43b3b039
Author: Mike Travis <travis@sgi.com>
Date:   Tue Jul 15 14:14:30 2008 -0700

    cpumask: Replace cpumask_of_cpu with cpumask_of_cpu_ptr
    
      * This patch replaces the dangerous lvalue version of cpumask_of_cpu
        with new cpumask_of_cpu_ptr macros.  These are patterned after the
        node_to_cpumask_ptr macros.
    
        In general terms, if there is a cpumask_of_cpu_map[] then a pointer to
        the cpumask_of_cpu_map[cpu] entry is used.  The cpumask_of_cpu_map
        is provided when there is a large NR_CPUS count, reducing
        greatly the amount of code generated and stack space used for
        cpumask_of_cpu().  The pointer to the cpumask_t value is needed for
        calling set_cpus_allowed_ptr() to reduce the amount of stack space
        needed to pass the cpumask_t value.
    
        If there isn't a cpumask_of_cpu_map[], then a temporary variable is
        declared and filled in with value from cpumask_of_cpu(cpu) as well as
        a pointer variable pointing to this temporary variable.  Afterwards,
        the pointer is used to reference the cpumask value.  The compiler
        will optimize out the extra dereference through the pointer as well
        as the stack space used for the pointer, resulting in identical code.
    
        A good example of the orthogonal usages is in net/sunrpc/svc.c:
    
            case SVC_POOL_PERCPU:
            {
                    unsigned int cpu = m->pool_to[pidx];
                    cpumask_of_cpu_ptr(cpumask, cpu);
    
                    *oldmask = current->cpus_allowed;
                    set_cpus_allowed_ptr(current, cpumask);
                    return 1;
            }
            case SVC_POOL_PERNODE:
            {
                    unsigned int node = m->pool_to[pidx];
                    node_to_cpumask_ptr(nodecpumask, node);
    
                    *oldmask = current->cpus_allowed;
                    set_cpus_allowed_ptr(current, nodecpumask);
                    return 1;
            }
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index c2502eb9aa83..9220cf46aa10 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -73,6 +73,7 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 	struct cpuinfo_x86 *c = &cpu_data(cpu);
 
 	cpumask_t saved_mask;
+	cpumask_of_cpu_ptr(new_mask, cpu);
 	int retval;
 	unsigned int eax, ebx, ecx, edx;
 	unsigned int edx_part;
@@ -91,7 +92,7 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 
 	/* Make sure we are running on right CPU */
 	saved_mask = current->cpus_allowed;
-	retval = set_cpus_allowed_ptr(current, &cpumask_of_cpu(cpu));
+	retval = set_cpus_allowed_ptr(current, new_mask);
 	if (retval)
 		return -1;
 

commit ec965350bb98bd291eb34f6ecddfdcfc36da1e6e
Merge: 5f033bb9bc5c 486fdae21458
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 21 15:40:24 2008 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mingo/linux-2.6-sched-devel
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mingo/linux-2.6-sched-devel: (62 commits)
      sched: build fix
      sched: better rt-group documentation
      sched: features fix
      sched: /debug/sched_features
      sched: add SCHED_FEAT_DEADLINE
      sched: debug: show a weight tree
      sched: fair: weight calculations
      sched: fair-group: de-couple load-balancing from the rb-trees
      sched: fair-group scheduling vs latency
      sched: rt-group: optimize dequeue_rt_stack
      sched: debug: add some debug code to handle the full hierarchy
      sched: fair-group: SMP-nice for group scheduling
      sched, cpuset: customize sched domains, core
      sched, cpuset: customize sched domains, docs
      sched: prepatory code movement
      sched: rt: multi level group constraints
      sched: task_group hierarchy
      sched: fix the task_group hierarchy for UID grouping
      sched: allow the group scheduler to have multiple levels
      sched: mix tasks and groups
      ...

commit fc0e474840d1fd96f28fbd76d4f36b80e7ad1cc3
Author: Mike Travis <travis@sgi.com>
Date:   Fri Apr 4 18:11:05 2008 -0700

    x86: use new set_cpus_allowed_ptr function
    
      * Use new set_cpus_allowed_ptr() function added by previous patch,
        which instead of passing the "newly allowed cpus" cpumask_t arg
        by value,  pass it by pointer:
    
        -int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
        +int set_cpus_allowed_ptr(struct task_struct *p, const cpumask_t *new_mask)
    
      * Cleanup uses of CPU_MASK_ALL.
    
      * Collapse other NR_CPUS changes to arch/x86/kernel/cpu/cpufreq/acpi-cpufreq.c
        Use pointers to cpumask_t arguments whenever possible.
    
    Depends on:
            [sched-devel]: sched: add new set_cpus_allowed_ptr function
    
    Cc: Len Brown <len.brown@intel.com>
    Cc: Dave Jones <davej@codemonkey.org.uk>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 8ca3557a6d59..c6dc05af8827 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -93,7 +93,7 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 
 	/* Make sure we are running on right CPU */
 	saved_mask = current->cpus_allowed;
-	retval = set_cpus_allowed(current, cpumask_of_cpu(cpu));
+	retval = set_cpus_allowed_ptr(current, &cpumask_of_cpu(cpu));
 	if (retval)
 		return -1;
 
@@ -130,7 +130,7 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 		 cx->address);
 
 out:
-	set_cpus_allowed(current, saved_mask);
+	set_cpus_allowed_ptr(current, &saved_mask);
 	return retval;
 }
 EXPORT_SYMBOL_GPL(acpi_processor_ffh_cstate_probe);

commit cf9b111c170733dde39139e8989b676ec8b81573
Author: WANG Cong <xiyou.wangcong@gmail.com>
Date:   Sat Mar 8 18:15:06 2008 +0800

    x86: remove pointless comments
    
    Remove old comments that include the old arch/i386 directory.
    
    Signed-off-by: WANG Cong <xiyou.wangcong@gmail.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 8ca3557a6d59..9366fb68d8d8 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -1,6 +1,4 @@
 /*
- * arch/i386/kernel/acpi/cstate.c
- *
  * Copyright (C) 2005 Intel Corporation
  * 	Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
  * 	- Added _PDC for SMP C-states on Intel CPUs

commit 4fcb2fcd4d0678b8ae103d257dcb28074cbfc7fa
Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Mon Feb 11 17:46:31 2008 -0800

    ACPI, cpuidle: Clarify C-state description in sysfs
    
    Add a new sysfs entry under cpuidle states. desc - can be used by driver to
    communicate to userspace any specific information about the state.
    This helps in identifying the exact hardware C-states behind the ACPI C-state
    definition.
    
    Idea is to export this through powertop, which will help to map the C-state
    reported by powertop to actual hardware C-state.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 10b67170b133..8ca3557a6d59 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -126,6 +126,8 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 		printk(KERN_DEBUG "Monitor-Mwait will be used to enter C-%d "
 		       "state\n", cx->type);
 	}
+	snprintf(cx->desc, ACPI_CX_DESC_LEN, "ACPI FFH INTEL MWAIT 0x%x",
+		 cx->address);
 
 out:
 	set_cpus_allowed(current, saved_mask);

commit 92cb7612aee39642d109b8d935ad265e602c0563
Author: Mike Travis <travis@sgi.com>
Date:   Fri Oct 19 20:35:04 2007 +0200

    x86: convert cpuinfo_x86 array to a per_cpu array
    
    cpu_data is currently an array defined using NR_CPUS.  This means that
    we overallocate since we will rarely really use maximum configured cpus.
    When NR_CPU count is raised to 4096 the size of cpu_data becomes
    3,145,728 bytes.
    
    These changes were adopted from the sparc64 (and ia64) code.  An
    additional field was added to cpuinfo_x86 to be a non-ambiguous cpu
    index.  This corresponds to the index into a cpumask_t as well as the
    per_cpu index.  It's used in various places like show_cpuinfo().
    
    cpu_data is defined to be the boot_cpu_data structure for the NON-SMP
    case.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: James Bottomley <James.Bottomley@steeleye.com>
    Cc: Dmitry Torokhov <dtor@mail.ru>
    Cc: "Antonino A. Daplas" <adaplas@pol.net>
    Cc: Mark M. Hoffman <mhoffman@lightlink.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
index 2d39f55d29a8..10b67170b133 100644
--- a/arch/x86/kernel/acpi/cstate.c
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -29,7 +29,7 @@
 void acpi_processor_power_init_bm_check(struct acpi_processor_flags *flags,
 					unsigned int cpu)
 {
-	struct cpuinfo_x86 *c = cpu_data + cpu;
+	struct cpuinfo_x86 *c = &cpu_data(cpu);
 
 	flags->bm_check = 0;
 	if (num_online_cpus() == 1)
@@ -72,7 +72,7 @@ int acpi_processor_ffh_cstate_probe(unsigned int cpu,
 		struct acpi_processor_cx *cx, struct acpi_power_register *reg)
 {
 	struct cstate_entry *percpu_entry;
-	struct cpuinfo_x86 *c = cpu_data + cpu;
+	struct cpuinfo_x86 *c = &cpu_data(cpu);
 
 	cpumask_t saved_mask;
 	int retval;

commit 23d6f82bd1f07886b3a974c5193baa715475dd37
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:16:23 2007 +0200

    i386: move kernel/acpi
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
new file mode 100644
index 000000000000..2d39f55d29a8
--- /dev/null
+++ b/arch/x86/kernel/acpi/cstate.c
@@ -0,0 +1,164 @@
+/*
+ * arch/i386/kernel/acpi/cstate.c
+ *
+ * Copyright (C) 2005 Intel Corporation
+ * 	Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
+ * 	- Added _PDC for SMP C-states on Intel CPUs
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/acpi.h>
+#include <linux/cpu.h>
+#include <linux/sched.h>
+
+#include <acpi/processor.h>
+#include <asm/acpi.h>
+
+/*
+ * Initialize bm_flags based on the CPU cache properties
+ * On SMP it depends on cache configuration
+ * - When cache is not shared among all CPUs, we flush cache
+ *   before entering C3.
+ * - When cache is shared among all CPUs, we use bm_check
+ *   mechanism as in UP case
+ *
+ * This routine is called only after all the CPUs are online
+ */
+void acpi_processor_power_init_bm_check(struct acpi_processor_flags *flags,
+					unsigned int cpu)
+{
+	struct cpuinfo_x86 *c = cpu_data + cpu;
+
+	flags->bm_check = 0;
+	if (num_online_cpus() == 1)
+		flags->bm_check = 1;
+	else if (c->x86_vendor == X86_VENDOR_INTEL) {
+		/*
+		 * Today all CPUs that support C3 share cache.
+		 * TBD: This needs to look at cache shared map, once
+		 * multi-core detection patch makes to the base.
+		 */
+		flags->bm_check = 1;
+	}
+}
+EXPORT_SYMBOL(acpi_processor_power_init_bm_check);
+
+/* The code below handles cstate entry with monitor-mwait pair on Intel*/
+
+struct cstate_entry {
+	struct {
+		unsigned int eax;
+		unsigned int ecx;
+	} states[ACPI_PROCESSOR_MAX_POWER];
+};
+static struct cstate_entry *cpu_cstate_entry;	/* per CPU ptr */
+
+static short mwait_supported[ACPI_PROCESSOR_MAX_POWER];
+
+#define MWAIT_SUBSTATE_MASK	(0xf)
+#define MWAIT_SUBSTATE_SIZE	(4)
+
+#define CPUID_MWAIT_LEAF (5)
+#define CPUID5_ECX_EXTENSIONS_SUPPORTED (0x1)
+#define CPUID5_ECX_INTERRUPT_BREAK	(0x2)
+
+#define MWAIT_ECX_INTERRUPT_BREAK	(0x1)
+
+#define NATIVE_CSTATE_BEYOND_HALT	(2)
+
+int acpi_processor_ffh_cstate_probe(unsigned int cpu,
+		struct acpi_processor_cx *cx, struct acpi_power_register *reg)
+{
+	struct cstate_entry *percpu_entry;
+	struct cpuinfo_x86 *c = cpu_data + cpu;
+
+	cpumask_t saved_mask;
+	int retval;
+	unsigned int eax, ebx, ecx, edx;
+	unsigned int edx_part;
+	unsigned int cstate_type; /* C-state type and not ACPI C-state type */
+	unsigned int num_cstate_subtype;
+
+	if (!cpu_cstate_entry || c->cpuid_level < CPUID_MWAIT_LEAF )
+		return -1;
+
+	if (reg->bit_offset != NATIVE_CSTATE_BEYOND_HALT)
+		return -1;
+
+	percpu_entry = per_cpu_ptr(cpu_cstate_entry, cpu);
+	percpu_entry->states[cx->index].eax = 0;
+	percpu_entry->states[cx->index].ecx = 0;
+
+	/* Make sure we are running on right CPU */
+	saved_mask = current->cpus_allowed;
+	retval = set_cpus_allowed(current, cpumask_of_cpu(cpu));
+	if (retval)
+		return -1;
+
+	cpuid(CPUID_MWAIT_LEAF, &eax, &ebx, &ecx, &edx);
+
+	/* Check whether this particular cx_type (in CST) is supported or not */
+	cstate_type = (cx->address >> MWAIT_SUBSTATE_SIZE) + 1;
+	edx_part = edx >> (cstate_type * MWAIT_SUBSTATE_SIZE);
+	num_cstate_subtype = edx_part & MWAIT_SUBSTATE_MASK;
+
+	retval = 0;
+	if (num_cstate_subtype < (cx->address & MWAIT_SUBSTATE_MASK)) {
+		retval = -1;
+		goto out;
+	}
+
+	/* mwait ecx extensions INTERRUPT_BREAK should be supported for C2/C3 */
+	if (!(ecx & CPUID5_ECX_EXTENSIONS_SUPPORTED) ||
+	    !(ecx & CPUID5_ECX_INTERRUPT_BREAK)) {
+		retval = -1;
+		goto out;
+	}
+	percpu_entry->states[cx->index].ecx = MWAIT_ECX_INTERRUPT_BREAK;
+
+	/* Use the hint in CST */
+	percpu_entry->states[cx->index].eax = cx->address;
+
+	if (!mwait_supported[cstate_type]) {
+		mwait_supported[cstate_type] = 1;
+		printk(KERN_DEBUG "Monitor-Mwait will be used to enter C-%d "
+		       "state\n", cx->type);
+	}
+
+out:
+	set_cpus_allowed(current, saved_mask);
+	return retval;
+}
+EXPORT_SYMBOL_GPL(acpi_processor_ffh_cstate_probe);
+
+void acpi_processor_ffh_cstate_enter(struct acpi_processor_cx *cx)
+{
+	unsigned int cpu = smp_processor_id();
+	struct cstate_entry *percpu_entry;
+
+	percpu_entry = per_cpu_ptr(cpu_cstate_entry, cpu);
+	mwait_idle_with_hints(percpu_entry->states[cx->index].eax,
+	                      percpu_entry->states[cx->index].ecx);
+}
+EXPORT_SYMBOL_GPL(acpi_processor_ffh_cstate_enter);
+
+static int __init ffh_cstate_init(void)
+{
+	struct cpuinfo_x86 *c = &boot_cpu_data;
+	if (c->x86_vendor != X86_VENDOR_INTEL)
+		return -1;
+
+	cpu_cstate_entry = alloc_percpu(struct cstate_entry);
+	return 0;
+}
+
+static void __exit ffh_cstate_exit(void)
+{
+	free_percpu(cpu_cstate_entry);
+	cpu_cstate_entry = NULL;
+}
+
+arch_initcall(ffh_cstate_init);
+__exitcall(ffh_cstate_exit);
