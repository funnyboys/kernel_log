commit cadfad870154e14f745ec845708bc17d166065f2
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Jul 17 16:53:55 2020 -0700

    x86/ioperm: Fix io bitmap invalidation on Xen PV
    
    tss_invalidate_io_bitmap() wasn't wired up properly through the pvop
    machinery, so the TSS and Xen's io bitmap would get out of sync
    whenever disabling a valid io bitmap.
    
    Add a new pvop for tss_invalidate_io_bitmap() to fix it.
    
    This is XSA-329.
    
    Fixes: 22fe5b0439dd ("x86/ioperm: Move TSS bitmap update to exit to user work")
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/d53075590e1f91c19f8af705059d3ff99424c020.1595030016.git.luto@kernel.org

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index f362ce0d5ac0..fe67dbd76e51 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -322,20 +322,6 @@ void arch_setup_new_exec(void)
 }
 
 #ifdef CONFIG_X86_IOPL_IOPERM
-static inline void tss_invalidate_io_bitmap(struct tss_struct *tss)
-{
-	/*
-	 * Invalidate the I/O bitmap by moving io_bitmap_base outside the
-	 * TSS limit so any subsequent I/O access from user space will
-	 * trigger a #GP.
-	 *
-	 * This is correct even when VMEXIT rewrites the TSS limit
-	 * to 0x67 as the only requirement is that the base points
-	 * outside the limit.
-	 */
-	tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
-}
-
 static inline void switch_to_bitmap(unsigned long tifp)
 {
 	/*
@@ -346,7 +332,7 @@ static inline void switch_to_bitmap(unsigned long tifp)
 	 * user mode.
 	 */
 	if (tifp & _TIF_IO_BITMAP)
-		tss_invalidate_io_bitmap(this_cpu_ptr(&cpu_tss_rw));
+		tss_invalidate_io_bitmap();
 }
 
 static void tss_copy_io_bitmap(struct tss_struct *tss, struct io_bitmap *iobm)
@@ -380,7 +366,7 @@ void native_tss_update_io_bitmap(void)
 	u16 *base = &tss->x86_tss.io_bitmap_base;
 
 	if (!test_thread_flag(TIF_IO_BITMAP)) {
-		tss_invalidate_io_bitmap(tss);
+		native_tss_invalidate_io_bitmap();
 		return;
 	}
 

commit 6a45a65888393eda692fce0851c40d9f5ce4ef66
Merge: 92ac971219a2 7778d8417b74
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 11 15:54:31 2020 -0700

    Merge tag 'x86-urgent-2020-06-11' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull more x86 updates from Thomas Gleixner:
     "A set of fixes and updates for x86:
    
       - Unbreak paravirt VDSO clocks.
    
         While the VDSO code was moved into lib for sharing a subtle check
         for the validity of paravirt clocks got replaced. While the
         replacement works perfectly fine for bare metal as the update of
         the VDSO clock mode is synchronous, it fails for paravirt clocks
         because the hypervisor can invalidate them asynchronously.
    
         Bring it back as an optional function so it does not inflict this
         on architectures which are free of PV damage.
    
       - Fix the jiffies to jiffies64 mapping on 64bit so it does not
         trigger an ODR violation on newer compilers
    
       - Three fixes for the SSBD and *IB* speculation mitigation maze to
         ensure consistency, not disabling of some *IB* variants wrongly and
         to prevent a rogue cross process shutdown of SSBD. All marked for
         stable.
    
       - Add yet more CPU models to the splitlock detection capable list
         !@#%$!
    
       - Bring the pr_info() back which tells that TSC deadline timer is
         enabled.
    
       - Reboot quirk for MacBook6,1"
    
    * tag 'x86-urgent-2020-06-11' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/vdso: Unbreak paravirt VDSO clocks
      lib/vdso: Provide sanity check for cycles (again)
      clocksource: Remove obsolete ifdef
      x86_64: Fix jiffies ODR violation
      x86/speculation: PR_SPEC_FORCE_DISABLE enforcement for indirect branches.
      x86/speculation: Prevent rogue cross-process SSBD shutdown
      x86/speculation: Avoid force-disabling IBPB based on STIBP and enhanced IBRS.
      x86/cpu: Add Sapphire Rapids CPU model number
      x86/split_lock: Add Icelake microserver and Tigerlake CPU models
      x86/apic: Make TSC deadline timer detection message visible
      x86/reboot/quirks: Add MacBook6,1 reboot quirk

commit dbbe2ad02e9df26e372f38cc3e70dab9222c832e
Author: Anthony Steinhauser <asteinhauser@google.com>
Date:   Sun Jan 5 12:19:43 2020 -0800

    x86/speculation: Prevent rogue cross-process SSBD shutdown
    
    On context switch the change of TIF_SSBD and TIF_SPEC_IB are evaluated
    to adjust the mitigations accordingly. This is optimized to avoid the
    expensive MSR write if not needed.
    
    This optimization is buggy and allows an attacker to shutdown the SSBD
    protection of a victim process.
    
    The update logic reads the cached base value for the speculation control
    MSR which has neither the SSBD nor the STIBP bit set. It then OR's the
    SSBD bit only when TIF_SSBD is different and requests the MSR update.
    
    That means if TIF_SSBD of the previous and next task are the same, then
    the base value is not updated, even if TIF_SSBD is set. The MSR write is
    not requested.
    
    Subsequently if the TIF_STIBP bit differs then the STIBP bit is updated
    in the base value and the MSR is written with a wrong SSBD value.
    
    This was introduced when the per task/process conditional STIPB
    switching was added on top of the existing SSBD switching.
    
    It is exploitable if the attacker creates a process which enforces SSBD
    and has the contrary value of STIBP than the victim process (i.e. if the
    victim process enforces STIBP, the attacker process must not enforce it;
    if the victim process does not enforce STIBP, the attacker process must
    enforce it) and schedule it on the same core as the victim process. If
    the victim runs after the attacker the victim becomes vulnerable to
    Spectre V4.
    
    To fix this, update the MSR value independent of the TIF_SSBD difference
    and dependent on the SSBD mitigation method available. This ensures that
    a subsequent STIPB initiated MSR write has the correct state of SSBD.
    
    [ tglx: Handle X86_FEATURE_VIRT_SSBD & X86_FEATURE_VIRT_SSBD correctly
            and massaged changelog ]
    
    Fixes: 5bfbe3ad5840 ("x86/speculation: Prepare for per task indirect branch speculation control")
    Signed-off-by: Anthony Steinhauser <asteinhauser@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 35638f1c5791..8f4533c1a4ec 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -545,28 +545,20 @@ static __always_inline void __speculation_ctrl_update(unsigned long tifp,
 
 	lockdep_assert_irqs_disabled();
 
-	/*
-	 * If TIF_SSBD is different, select the proper mitigation
-	 * method. Note that if SSBD mitigation is disabled or permanentely
-	 * enabled this branch can't be taken because nothing can set
-	 * TIF_SSBD.
-	 */
-	if (tif_diff & _TIF_SSBD) {
-		if (static_cpu_has(X86_FEATURE_VIRT_SSBD)) {
+	/* Handle change of TIF_SSBD depending on the mitigation method. */
+	if (static_cpu_has(X86_FEATURE_VIRT_SSBD)) {
+		if (tif_diff & _TIF_SSBD)
 			amd_set_ssb_virt_state(tifn);
-		} else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD)) {
+	} else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD)) {
+		if (tif_diff & _TIF_SSBD)
 			amd_set_core_ssb_state(tifn);
-		} else if (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) ||
-			   static_cpu_has(X86_FEATURE_AMD_SSBD)) {
-			msr |= ssbd_tif_to_spec_ctrl(tifn);
-			updmsr  = true;
-		}
+	} else if (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) ||
+		   static_cpu_has(X86_FEATURE_AMD_SSBD)) {
+		updmsr |= !!(tif_diff & _TIF_SSBD);
+		msr |= ssbd_tif_to_spec_ctrl(tifn);
 	}
 
-	/*
-	 * Only evaluate TIF_SPEC_IB if conditional STIBP is enabled,
-	 * otherwise avoid the MSR write.
-	 */
+	/* Only evaluate TIF_SPEC_IB if conditional STIBP is enabled. */
 	if (IS_ENABLED(CONFIG_SMP) &&
 	    static_branch_unlikely(&switch_to_cond_stibp)) {
 		updmsr |= !!(tif_diff & _TIF_SPEC_IB);

commit f4dd60a3d4c7656dcaa0ba2afb503528c86f913f
Merge: 435faf5c218a bd1de2a7aace
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 5 11:18:53 2020 -0700

    Merge tag 'x86-mm-2020-06-05' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Ingo Molnar:
     "Misc changes:
    
       - Unexport various PAT primitives
    
       - Unexport per-CPU tlbstate and uninline TLB helpers"
    
    * tag 'x86-mm-2020-06-05' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      x86/tlb/uv: Add a forward declaration for struct flush_tlb_info
      x86/cpu: Export native_write_cr4() only when CONFIG_LKTDM=m
      x86/tlb: Restrict access to tlbstate
      xen/privcmd: Remove unneeded asm/tlb.h include
      x86/tlb: Move PCID helpers where they are used
      x86/tlb: Uninline nmi_uaccess_okay()
      x86/tlb: Move cr4_set_bits_and_update_boot() to the usage site
      x86/tlb: Move paravirt_tlb_remove_table() to the usage site
      x86/tlb: Move __flush_tlb_all() out of line
      x86/tlb: Move flush_tlb_others() out of line
      x86/tlb: Move __flush_tlb_one_kernel() out of line
      x86/tlb: Move __flush_tlb_one_user() out of line
      x86/tlb: Move __flush_tlb_global() out of line
      x86/tlb: Move __flush_tlb() out of line
      x86/alternatives: Move temporary_mm helpers into C
      x86/cr4: Sanitize CR4.PCE update
      x86/cpu: Uninline CR4 accessors
      x86/tlb: Uninline __get_current_cr3_fast()
      x86/mm: Use pgprotval_t in protval_4k_2_large() and protval_large_2_4k()
      x86/mm: Unexport __cachemode2pte_tbl
      ...

commit 0a319ef75d931de0b21882ec17d8d70ece0aa871
Merge: eff5ddadab04 55e00fb66fd5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 14:09:26 2020 -0700

    Merge tag 'x86-fpu-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU updates from Ingo Molnar:
     "Most of the changes here related to 'XSAVES supervisor state' support,
      which is a feature that allows kernel-only data to be automatically
      saved/restored by the FPU context switching code.
    
      CPU features that can be supported this way are Intel PT, 'PASID' and
      CET features"
    
    * tag 'x86-fpu-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fpu/xstate: Restore supervisor states for signal return
      x86/fpu/xstate: Preserve supervisor states for the slow path in __fpu__restore_sig()
      x86/fpu: Introduce copy_supervisor_to_kernel()
      x86/fpu/xstate: Update copy_kernel_to_xregs_err() for supervisor states
      x86/fpu/xstate: Update sanitize_restored_xstate() for supervisor xstates
      x86/fpu/xstate: Define new functions for clearing fpregs and xstates
      x86/fpu/xstate: Introduce XSAVES supervisor states
      x86/fpu/xstate: Separate user and supervisor xfeatures mask
      x86/fpu/xstate: Define new macros for supervisor and user xstates
      x86/fpu/xstate: Rename validate_xstate_header() to validate_user_xstate_header()

commit 4bfe6cce133cad82cea04490c308795275857782
Author: Jay Lang <jaytlang@mit.edu>
Date:   Sun May 24 12:27:39 2020 -0400

    x86/ioperm: Prevent a memory leak when fork fails
    
    In the copy_process() routine called by _do_fork(), failure to allocate
    a PID (or further along in the function) will trigger an invocation to
    exit_thread(). This is done to clean up from an earlier call to
    copy_thread_tls(). Naturally, the child task is passed into exit_thread(),
    however during the process, io_bitmap_exit() nullifies the parent's
    io_bitmap rather than the child's.
    
    As copy_thread_tls() has been called ahead of the failure, the reference
    count on the calling thread's io_bitmap is incremented as we would expect.
    However, io_bitmap_exit() doesn't accept any arguments, and thus assumes
    it should trash the current thread's io_bitmap reference rather than the
    child's. This is pretty sneaky in practice, because in all instances but
    this one, exit_thread() is called with respect to the current task and
    everything works out.
    
    A determined attacker can issue an appropriate ioctl (i.e. KDENABIO) to
    get a bitmap allocated, and force a clone3() syscall to fail by passing
    in a zeroed clone_args structure. The kernel handles the erroneous struct
    and the buggy code path is followed, and even though the parent's reference
    to the io_bitmap is trashed, the child still holds a reference and thus
    the structure will never be freed.
    
    Fix this by tweaking io_bitmap_exit() and its subroutines to accept a
    task_struct argument which to operate on.
    
    Fixes: ea5f1cd7ab49 ("x86/ioperm: Remove bitmap if all permissions dropped")
    Signed-off-by: Jay Lang <jaytlang@mit.edu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable#@vger.kernel.org
    Link: https://lkml.kernel.org/r/20200524162742.253727-1-jaytlang@mit.edu

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 9da70b279dad..35638f1c5791 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -96,7 +96,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 }
 
 /*
- * Free current thread data structures etc..
+ * Free thread data structures etc..
  */
 void exit_thread(struct task_struct *tsk)
 {
@@ -104,7 +104,7 @@ void exit_thread(struct task_struct *tsk)
 	struct fpu *fpu = &t->fpu;
 
 	if (test_thread_flag(TIF_IO_BITMAP))
-		io_bitmap_exit();
+		io_bitmap_exit(tsk);
 
 	free_vm86(t);
 

commit b860eb8dce5906b14e3a7f3c771e0b3d6ef61b94
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Tue May 12 07:54:39 2020 -0700

    x86/fpu/xstate: Define new functions for clearing fpregs and xstates
    
    Currently, fpu__clear() clears all fpregs and xstates.  Once XSAVES
    supervisor states are introduced, supervisor settings (e.g. CET xstates)
    must remain active for signals; It is necessary to have separate functions:
    
    - Create fpu__clear_user_states(): clear only user settings for signals;
    - Create fpu__clear_all(): clear both user and supervisor settings in
       flush_thread().
    
    Also modify copy_init_fpstate_to_fpregs() to take a mask from above two
    functions.
    
    Remove obvious side-comment in fpu__clear(), while at it.
    
     [ bp: Make the second argument of fpu__clear() bool after requesting it
       a bunch of times during review.
      - Add a comment about copy_init_fpstate_to_fpregs() locking needs. ]
    
    Co-developed-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Link: https://lkml.kernel.org/r/20200512145444.15483-6-yu-cheng.yu@intel.com

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 9da70b279dad..de182b84723a 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -191,7 +191,7 @@ void flush_thread(void)
 	flush_ptrace_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
 
-	fpu__clear(&tsk->thread.fpu);
+	fpu__clear_all(&tsk->thread.fpu);
 }
 
 void disable_TSC(void)

commit d8f0b35331c4423e033f81f10eb5e0c7e4e1dcec
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 21 11:20:29 2020 +0200

    x86/cpu: Uninline CR4 accessors
    
    cpu_tlbstate is exported because various TLB-related functions need
    access to it, but cpu_tlbstate is sensitive information which should
    only be accessed by well-contained kernel functions and not be directly
    exposed to modules.
    
    The various CR4 accessors require cpu_tlbstate as the CR4 shadow cache
    is located there.
    
    In preparation for unexporting cpu_tlbstate, create a builtin function
    for manipulating CR4 and rework the various helpers to use it.
    
    No functional change.
    
     [ bp: push the export of native_write_cr4() only when CONFIG_LKTDM=m to
       the last patch in the series. ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200421092558.939985695@linutronix.de

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 9da70b279dad..f2eab49d044e 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -612,6 +612,17 @@ void speculation_ctrl_update_current(void)
 	preempt_enable();
 }
 
+static inline void cr4_toggle_bits_irqsoff(unsigned long mask)
+{
+	unsigned long newval, cr4 = this_cpu_read(cpu_tlbstate.cr4);
+
+	newval = cr4 ^ mask;
+	if (newval != cr4) {
+		this_cpu_write(cpu_tlbstate.cr4, newval);
+		__write_cr4(newval);
+	}
+}
+
 void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)
 {
 	unsigned long tifp, tifn;

commit 2853d5fafb1e21707e89aa2e227c90bb2c1ea4a9
Merge: d5f744f9a2ac a6a60741035b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 19:35:52 2020 -0700

    Merge tag 'x86-splitlock-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 splitlock updates from Thomas Gleixner:
     "Support for 'split lock' detection:
    
      Atomic operations (lock prefixed instructions) which span two cache
      lines have to acquire the global bus lock. This is at least 1k cycles
      slower than an atomic operation within a cache line and disrupts
      performance on other cores. Aside of performance disruption this is a
      unpriviledged form of DoS.
    
      Some newer CPUs have the capability to raise an #AC trap when such an
      operation is attempted. The detection is by default enabled in warning
      mode which will warn once when a user space application is caught. A
      command line option allows to disable the detection or to select fatal
      mode which will terminate offending applications with SIGBUS"
    
    * tag 'x86-splitlock-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/split_lock: Avoid runtime reads of the TEST_CTRL MSR
      x86/split_lock: Rework the initialization flow of split lock detection
      x86/split_lock: Enable split lock detection by kernel

commit d5f744f9a2ac9ca6d5baf72e97ce6dc4c2f19fe4
Merge: dbb381b619aa 290a4474d019
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 19:14:28 2020 -0700

    Merge tag 'x86-entry-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 entry code updates from Thomas Gleixner:
    
     - Convert the 32bit syscalls to be pt_regs based which removes the
       requirement to push all 6 potential arguments onto the stack and
       consolidates the interface with the 64bit variant
    
     - The first small portion of the exception and syscall related entry
       code consolidation which aims to address the recently discovered
       issues vs. RCU, int3, NMI and some other exceptions which can
       interrupt any context. The bulk of the changes is still work in
       progress and aimed for 5.8.
    
     - A few lockdep namespace cleanups which have been applied into this
       branch to keep the prerequisites for the ongoing work confined.
    
    * tag 'x86-entry-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (35 commits)
      x86/entry: Fix build error x86 with !CONFIG_POSIX_TIMERS
      lockdep: Rename trace_{hard,soft}{irq_context,irqs_enabled}()
      lockdep: Rename trace_softirqs_{on,off}()
      lockdep: Rename trace_hardirq_{enter,exit}()
      x86/entry: Rename ___preempt_schedule
      x86: Remove unneeded includes
      x86/entry: Drop asmlinkage from syscalls
      x86/entry/32: Enable pt_regs based syscalls
      x86/entry/32: Use IA32-specific wrappers for syscalls taking 64-bit arguments
      x86/entry/32: Rename 32-bit specific syscalls
      x86/entry/32: Clean up syscall_32.tbl
      x86/entry: Remove ABI prefixes from functions in syscall tables
      x86/entry/64: Add __SYSCALL_COMMON()
      x86/entry: Remove syscall qualifier support
      x86/entry/64: Remove ptregs qualifier from syscall table
      x86/entry: Move max syscall number calculation to syscallhdr.sh
      x86/entry/64: Split X32 syscall table into its own file
      x86/entry/64: Move sys_ni_syscall stub to common.c
      x86/entry/64: Use syscall wrappers for x32_rt_sigreturn
      x86/entry: Refactor SYS_NI macros
      ...

commit ffd75b373f3656cbb593c5221cc36ce232b7bbc1
Author: Brian Gerst <brgerst@gmail.com>
Date:   Fri Mar 13 15:51:44 2020 -0400

    x86: Remove unneeded includes
    
    Clean up includes of and in <asm/syscalls.h>
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200313195144.164260-19-brgerst@gmail.com

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 839b5244e3b7..d78e2282df0f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -28,7 +28,6 @@
 #include <linux/hw_breakpoint.h>
 #include <asm/cpu.h>
 #include <asm/apic.h>
-#include <asm/syscalls.h>
 #include <linux/uaccess.h>
 #include <asm/mwait.h>
 #include <asm/fpu/internal.h>

commit 99bcd4a6e5b8ba201fdd252f1054689884899fee
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Feb 18 16:47:12 2020 +0100

    x86/ioperm: Add new paravirt function update_io_bitmap()
    
    Commit 111e7b15cf10f6 ("x86/ioperm: Extend IOPL config to control ioperm()
    as well") reworked the iopl syscall to use I/O bitmaps.
    
    Unfortunately this broke Xen PV domains using that syscall as there is
    currently no I/O bitmap support in PV domains.
    
    Add I/O bitmap support via a new paravirt function update_io_bitmap which
    Xen PV domains can use to update their I/O bitmaps via a hypercall.
    
    Fixes: 111e7b15cf10f6 ("x86/ioperm: Extend IOPL config to control ioperm() as well")
    Reported-by: Jan Beulich <jbeulich@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Jan Beulich <jbeulich@suse.com>
    Reviewed-by: Jan Beulich <jbeulich@suse.com>
    Cc: <stable@vger.kernel.org> # 5.5
    Link: https://lkml.kernel.org/r/20200218154712.25490-1-jgross@suse.com

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 839b5244e3b7..3053c85e0e42 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -374,7 +374,7 @@ static void tss_copy_io_bitmap(struct tss_struct *tss, struct io_bitmap *iobm)
 /**
  * tss_update_io_bitmap - Update I/O bitmap before exiting to usermode
  */
-void tss_update_io_bitmap(void)
+void native_tss_update_io_bitmap(void)
 {
 	struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);
 	struct thread_struct *t = &current->thread;

commit 6650cdd9a8ccf00555dbbe743d58541ad8feb6a7
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Sun Jan 26 12:05:35 2020 -0800

    x86/split_lock: Enable split lock detection by kernel
    
    A split-lock occurs when an atomic instruction operates on data that spans
    two cache lines. In order to maintain atomicity the core takes a global bus
    lock.
    
    This is typically >1000 cycles slower than an atomic operation within a
    cache line. It also disrupts performance on other cores (which must wait
    for the bus lock to be released before their memory operations can
    complete). For real-time systems this may mean missing deadlines. For other
    systems it may just be very annoying.
    
    Some CPUs have the capability to raise an #AC trap when a split lock is
    attempted.
    
    Provide a command line option to give the user choices on how to handle
    this:
    
    split_lock_detect=
            off     - not enabled (no traps for split locks)
            warn    - warn once when an application does a
                      split lock, but allow it to continue
                      running.
            fatal   - Send SIGBUS to applications that cause split lock
    
    On systems that support split lock detection the default is "warn". Note
    that if the kernel hits a split lock in any mode other than "off" it will
    OOPs.
    
    One implementation wrinkle is that the MSR to control the split lock
    detection is per-core, not per thread. This might result in some short
    lived races on HT systems in "warn" mode if Linux tries to enable on one
    thread while disabling on the other. Race analysis by Sean Christopherson:
    
      - Toggling of split-lock is only done in "warn" mode.  Worst case
        scenario of a race is that a misbehaving task will generate multiple
        #AC exceptions on the same instruction.  And this race will only occur
        if both siblings are running tasks that generate split-lock #ACs, e.g.
        a race where sibling threads are writing different values will only
        occur if CPUx is disabling split-lock after an #AC and CPUy is
        re-enabling split-lock after *its* previous task generated an #AC.
      - Transitioning between off/warn/fatal modes at runtime isn't supported
        and disabling is tracked per task, so hardware will always reach a steady
        state that matches the configured mode.  I.e. split-lock is guaranteed to
        be enabled in hardware once all _TIF_SLD threads have been scheduled out.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Co-developed-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/20200126200535.GB30377@agluck-desk2.amr.corp.intel.com

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 839b5244e3b7..a43c32868c3c 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -650,6 +650,9 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)
 		/* Enforce MSR update to ensure consistent state */
 		__speculation_ctrl_update(~tifn, tifn);
 	}
+
+	if ((tifp ^ tifn) & _TIF_SLD)
+		switch_to_sld(tifn);
 }
 
 /*

commit 27353d5785bca61bb49cfd7c78e14f1d21e66ec5
Author: yu kuai <yukuai3@huawei.com>
Date:   Fri Dec 13 20:12:53 2019 +0800

    x86/process: Remove set but not used variables prev and next
    
    Remove two unused variables:
    
      arch/x86/kernel/process.c: In function ‘__switch_to_xtra’:
      arch/x86/kernel/process.c:618:31: warning: variable ‘next’ set but not used [-Wunused-but-set-variable]
        618 |  struct thread_struct *prev, *next;
            |                               ^~~~
      arch/x86/kernel/process.c:618:24: warning: variable ‘prev’ set but not used [-Wunused-but-set-variable]
        618 |  struct thread_struct *prev, *next;
            |
    
    They are never used and so can be removed.
    
    Signed-off-by: yu kuai <yukuai3@huawei.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86-ml <x86@kernel.org>
    Cc: yi.zhang@huawei.com
    Cc: zhengbin13@huawei.com
    Link: https://lkml.kernel.org/r/20191213121253.10072-1-yukuai3@huawei.com

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 61e93a318983..839b5244e3b7 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -615,12 +615,8 @@ void speculation_ctrl_update_current(void)
 
 void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)
 {
-	struct thread_struct *prev, *next;
 	unsigned long tifp, tifn;
 
-	prev = &prev_p->thread;
-	next = &next_p->thread;
-
 	tifn = READ_ONCE(task_thread_info(next_p)->flags);
 	tifp = READ_ONCE(task_thread_info(prev_p)->flags);
 

commit 7b0b8cfd261c569177d64d6e9b1800fbe412fd65
Author: Borislav Petkov <bp@suse.de>
Date:   Sat Nov 30 16:00:53 2019 +0100

    x86/ioperm: Save an indentation level in tss_update_io_bitmap()
    
    ... for better readability.
    
    No functional changes.
    
    [ Minor edit. ]
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index bd2a11ca5dd6..61e93a318983 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -377,37 +377,37 @@ static void tss_copy_io_bitmap(struct tss_struct *tss, struct io_bitmap *iobm)
 void tss_update_io_bitmap(void)
 {
 	struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);
+	struct thread_struct *t = &current->thread;
 	u16 *base = &tss->x86_tss.io_bitmap_base;
 
-	if (test_thread_flag(TIF_IO_BITMAP)) {
-		struct thread_struct *t = &current->thread;
-
-		if (IS_ENABLED(CONFIG_X86_IOPL_IOPERM) && t->iopl_emul == 3) {
-			*base = IO_BITMAP_OFFSET_VALID_ALL;
-		} else {
-			struct io_bitmap *iobm = t->io_bitmap;
-			/*
-			 * Only copy bitmap data when the sequence number
-			 * differs. The update time is accounted to the
-			 * incoming task.
-			 */
-			if (tss->io_bitmap.prev_sequence != iobm->sequence)
-				tss_copy_io_bitmap(tss, iobm);
-
-			/* Enable the bitmap */
-			*base = IO_BITMAP_OFFSET_VALID_MAP;
-		}
+	if (!test_thread_flag(TIF_IO_BITMAP)) {
+		tss_invalidate_io_bitmap(tss);
+		return;
+	}
+
+	if (IS_ENABLED(CONFIG_X86_IOPL_IOPERM) && t->iopl_emul == 3) {
+		*base = IO_BITMAP_OFFSET_VALID_ALL;
+	} else {
+		struct io_bitmap *iobm = t->io_bitmap;
+
 		/*
-		 * Make sure that the TSS limit is covering the io bitmap.
-		 * It might have been cut down by a VMEXIT to 0x67 which
-		 * would cause a subsequent I/O access from user space to
-		 * trigger a #GP because tbe bitmap is outside the TSS
-		 * limit.
+		 * Only copy bitmap data when the sequence number differs. The
+		 * update time is accounted to the incoming task.
 		 */
-		refresh_tss_limit();
-	} else {
-		tss_invalidate_io_bitmap(tss);
+		if (tss->io_bitmap.prev_sequence != iobm->sequence)
+			tss_copy_io_bitmap(tss, iobm);
+
+		/* Enable the bitmap */
+		*base = IO_BITMAP_OFFSET_VALID_MAP;
 	}
+
+	/*
+	 * Make sure that the TSS limit is covering the IO bitmap. It might have
+	 * been cut down by a VMEXIT to 0x67 which would cause a subsequent I/O
+	 * access from user space to trigger a #GP because tbe bitmap is outside
+	 * the TSS limit.
+	 */
+	refresh_tss_limit();
 }
 #else /* CONFIG_X86_IOPL_IOPERM */
 static inline void switch_to_bitmap(unsigned long tifp) { }

commit e3cb0c7102f04c83bf1a7cb1d052e92749310b46
Author: Alexander Duyck <alexander.h.duyck@linux.intel.com>
Date:   Wed Nov 20 14:25:53 2019 -0800

    x86/ioperm: Fix use of deprecated config option
    
    The commit
    
      111e7b15cf10 ("x86/ioperm: Extend IOPL config to control ioperm() as well")
    
    replaced X86_IOPL_EMULATION with X86_IOPL_IOPERM. However it appears
    that there was at least one spot missed as tss_update_io_bitmap() still
    had a reference to it contained in the code.
    
    The result of this is that it exposed a NULL pointer dereference as seen
    below with a linux-next next-20191120 kernel:
    
      BUG: kernel NULL pointer dereference, address: 0000000000000000
      #PF: supervisor read access in kernel mode
      #PF: error_code(0x0000) - not-present page
      PGD 0 P4D 0
      Oops: 0000 [#1] SMP PTI
      CPU: 5 PID: 1542 Comm: ovs-vswitchd Tainted: G        W 5.4.0-rc8-next-20191120 #125
      RIP: 0010:tss_update_io_bitmap+0x4e/0x180
      Code: 10 31 c0 65 48 03 1d 69 54 5d 6d 65 48 8b 04 25 40 8c 01 00 48 8b 10 \
              f7 c2 00 00 40 00 0f 84 8c 00 00 00 4c 8b a0 c0 22 00 00 <49> 8b 04 \
              24 48 39 43 68 74 2e 8b 53 70 41 39 54 24 0c 48 8d 7b 78
      RSP: 0018:ffffb8888a0ebf08 EFLAGS: 00010006
      RAX: ffff8a429811a680 RBX: ffff8a4c3f946000 RCX: 0000000000000011
      RDX: 0000000000400080 RSI: 0000000000400080 RDI: 0000000000000000
      RBP: ffffb8888a0ebf30 R08: 00007ffffb5d7ce0 R09: 0000000000000000
      R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000
      R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
      FS:  00007f68a9635c40(0000) GS:ffff8a4c3f940000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      CR2: 0000000000000000 CR3: 000000103572a001 CR4: 00000000001606e0
      Call Trace:
       ? syscall_slow_exit_work+0x39/0xdb
       do_syscall_64+0x1a5/0x200
       entry_SYSCALL_64_after_hwframe+0x44/0xa9
      RIP: 0033:0x7f68a7aff797
    
    Fixes: 111e7b15cf10 ("x86/ioperm: Extend IOPL config to control ioperm() as well")
    Signed-off-by: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20191120222426.3060.18462.stgit@localhost.localdomain

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7964d7db9366..bd2a11ca5dd6 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -382,8 +382,7 @@ void tss_update_io_bitmap(void)
 	if (test_thread_flag(TIF_IO_BITMAP)) {
 		struct thread_struct *t = &current->thread;
 
-		if (IS_ENABLED(CONFIG_X86_IOPL_EMULATION) &&
-		    t->iopl_emul == 3) {
+		if (IS_ENABLED(CONFIG_X86_IOPL_IOPERM) && t->iopl_emul == 3) {
 			*base = IO_BITMAP_OFFSET_VALID_ALL;
 		} else {
 			struct io_bitmap *iobm = t->io_bitmap;

commit 111e7b15cf10f6e973ccf537c70c66a5de539060
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 12 21:40:33 2019 +0100

    x86/ioperm: Extend IOPL config to control ioperm() as well
    
    If iopl() is disabled, then providing ioperm() does not make much sense.
    
    Rename the config option and disable/enable both syscalls with it. Guard
    the code with #ifdefs where appropriate.
    
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 8a844a5d5ae8..7964d7db9366 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -322,6 +322,7 @@ void arch_setup_new_exec(void)
 	}
 }
 
+#ifdef CONFIG_X86_IOPL_IOPERM
 static inline void tss_invalidate_io_bitmap(struct tss_struct *tss)
 {
 	/*
@@ -409,6 +410,9 @@ void tss_update_io_bitmap(void)
 		tss_invalidate_io_bitmap(tss);
 	}
 }
+#else /* CONFIG_X86_IOPL_IOPERM */
+static inline void switch_to_bitmap(unsigned long tifp) { }
+#endif
 
 #ifdef CONFIG_SMP
 

commit c8137ace56383688af911fea5934c71ad158135e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:28 2019 +0100

    x86/iopl: Restrict iopl() permission scope
    
    The access to the full I/O port range can be also provided by the TSS I/O
    bitmap, but that would require to copy 8k of data on scheduling in the
    task. As shown with the sched out optimization TSS.io_bitmap_base can be
    used to switch the incoming task to a preallocated I/O bitmap which has all
    bits zero, i.e. allows access to all I/O ports.
    
    Implementing this allows to provide an iopl() emulation mode which restricts
    the IOPL level 3 permissions to I/O port access but removes the STI/CLI
    permission which is coming with the hardware IOPL mechansim.
    
    Provide a config option to switch IOPL to emulation mode, make it the
    default and while at it also provide an option to disable IOPL completely.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 0b19c137c442..8a844a5d5ae8 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -376,21 +376,27 @@ static void tss_copy_io_bitmap(struct tss_struct *tss, struct io_bitmap *iobm)
 void tss_update_io_bitmap(void)
 {
 	struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);
+	u16 *base = &tss->x86_tss.io_bitmap_base;
 
 	if (test_thread_flag(TIF_IO_BITMAP)) {
-		struct io_bitmap *iobm = current->thread.io_bitmap;
-
-		/*
-		 * Only copy bitmap data when the sequence number
-		 * differs. The update time is accounted to the incoming
-		 * task.
-		 */
-		if (tss->io_bitmap.prev_sequence != iobm->sequence)
-			tss_copy_io_bitmap(tss, iobm);
-
-		/* Enable the bitmap */
-		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_VALID;
-
+		struct thread_struct *t = &current->thread;
+
+		if (IS_ENABLED(CONFIG_X86_IOPL_EMULATION) &&
+		    t->iopl_emul == 3) {
+			*base = IO_BITMAP_OFFSET_VALID_ALL;
+		} else {
+			struct io_bitmap *iobm = t->io_bitmap;
+			/*
+			 * Only copy bitmap data when the sequence number
+			 * differs. The update time is accounted to the
+			 * incoming task.
+			 */
+			if (tss->io_bitmap.prev_sequence != iobm->sequence)
+				tss_copy_io_bitmap(tss, iobm);
+
+			/* Enable the bitmap */
+			*base = IO_BITMAP_OFFSET_VALID_MAP;
+		}
 		/*
 		 * Make sure that the TSS limit is covering the io bitmap.
 		 * It might have been cut down by a VMEXIT to 0x67 which

commit 4804e382c117ce213cd5c43512cf4b1d71bb2650
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:25 2019 +0100

    x86/ioperm: Share I/O bitmap if identical
    
    The I/O bitmap is duplicated on fork. That's wasting memory and slows down
    fork. There is no point to do so. As long as the bitmap is not modified it
    can be shared between threads and processes.
    
    Add a refcount and just share it on fork. If a task modifies the bitmap
    then it has to do the duplication if and only if it is shared.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7ba4d54aec17..0b19c137c442 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -122,37 +122,13 @@ static int set_new_tls(struct task_struct *p, unsigned long tls)
 		return do_set_thread_area_64(p, ARCH_SET_FS, tls);
 }
 
-static inline int copy_io_bitmap(struct task_struct *tsk)
-{
-	struct io_bitmap *iobm = current->thread.io_bitmap;
-
-	if (likely(!test_tsk_thread_flag(current, TIF_IO_BITMAP)))
-		return 0;
-
-	tsk->thread.io_bitmap = kmemdup(iobm, sizeof(*iobm), GFP_KERNEL);
-
-	if (!tsk->thread.io_bitmap)
-		return -ENOMEM;
-
-	set_tsk_thread_flag(tsk, TIF_IO_BITMAP);
-	return 0;
-}
-
-static inline void free_io_bitmap(struct task_struct *tsk)
-{
-	if (tsk->thread.io_bitmap) {
-		kfree(tsk->thread.io_bitmap);
-		tsk->thread.io_bitmap = NULL;
-	}
-}
-
 int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 		    unsigned long arg, struct task_struct *p, unsigned long tls)
 {
 	struct inactive_task_frame *frame;
 	struct fork_frame *fork_frame;
 	struct pt_regs *childregs;
-	int ret;
+	int ret = 0;
 
 	childregs = task_pt_regs(p);
 	fork_frame = container_of(childregs, struct fork_frame, regs);
@@ -199,16 +175,13 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 	task_user_gs(p) = get_user_gs(current_pt_regs());
 #endif
 
-	ret = copy_io_bitmap(p);
-	if (ret)
-		return ret;
-
 	/* Set a new TLS for the child thread? */
-	if (clone_flags & CLONE_SETTLS) {
+	if (clone_flags & CLONE_SETTLS)
 		ret = set_new_tls(p, tls);
-		if (ret)
-			free_io_bitmap(p);
-	}
+
+	if (!ret && unlikely(test_tsk_thread_flag(current, TIF_IO_BITMAP)))
+		io_bitmap_share(p);
+
 	return ret;
 }
 

commit ea5f1cd7ab494f65f50f338299eabb40ad6a1767
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:24 2019 +0100

    x86/ioperm: Remove bitmap if all permissions dropped
    
    If ioperm() results in a bitmap with all bits set (no permissions to any
    I/O port), then handling that bitmap on context switch and exit to user
    mode is pointless. Drop it.
    
    Move the bitmap exit handling to the ioport code and reuse it for both the
    thread exit path and dropping it. This allows to reuse this code for the
    upcoming iopl() emulation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 108af913ab3c..7ba4d54aec17 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -102,21 +102,10 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 void exit_thread(struct task_struct *tsk)
 {
 	struct thread_struct *t = &tsk->thread;
-	struct io_bitmap *iobm = t->io_bitmap;
 	struct fpu *fpu = &t->fpu;
-	struct tss_struct *tss;
-
-	if (iobm) {
-		preempt_disable();
-		tss = this_cpu_ptr(&cpu_tss_rw);
-
-		t->io_bitmap = NULL;
-		clear_thread_flag(TIF_IO_BITMAP);
-		/* Invalidate the io bitmap base in the TSS */
-		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
-		preempt_enable();
-		kfree(iobm);
-	}
+
+	if (test_thread_flag(TIF_IO_BITMAP))
+		io_bitmap_exit();
 
 	free_vm86(t);
 

commit 22fe5b0439dd53643fd6f4c582c46c6dba0fde53
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:23 2019 +0100

    x86/ioperm: Move TSS bitmap update to exit to user work
    
    There is no point to update the TSS bitmap for tasks which use I/O bitmaps
    on every context switch. It's enough to update it right before exiting to
    user space.
    
    That reduces the context switch bitmap handling to invalidating the io
    bitmap base offset in the TSS when the outgoing task has TIF_IO_BITMAP
    set. The invaldiation is done on purpose when a task with an IO bitmap
    switches out to prevent any possible leakage of an activated IO bitmap.
    
    It also removes the requirement to update the tasks bitmap atomically in
    ioperm().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7c49be90468b..108af913ab3c 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -360,8 +360,34 @@ void arch_setup_new_exec(void)
 	}
 }
 
-static void switch_to_update_io_bitmap(struct tss_struct *tss,
-				       struct io_bitmap *iobm)
+static inline void tss_invalidate_io_bitmap(struct tss_struct *tss)
+{
+	/*
+	 * Invalidate the I/O bitmap by moving io_bitmap_base outside the
+	 * TSS limit so any subsequent I/O access from user space will
+	 * trigger a #GP.
+	 *
+	 * This is correct even when VMEXIT rewrites the TSS limit
+	 * to 0x67 as the only requirement is that the base points
+	 * outside the limit.
+	 */
+	tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
+}
+
+static inline void switch_to_bitmap(unsigned long tifp)
+{
+	/*
+	 * Invalidate I/O bitmap if the previous task used it. This prevents
+	 * any possible leakage of an active I/O bitmap.
+	 *
+	 * If the next task has an I/O bitmap it will handle it on exit to
+	 * user mode.
+	 */
+	if (tifp & _TIF_IO_BITMAP)
+		tss_invalidate_io_bitmap(this_cpu_ptr(&cpu_tss_rw));
+}
+
+static void tss_copy_io_bitmap(struct tss_struct *tss, struct io_bitmap *iobm)
 {
 	/*
 	 * Copy at least the byte range of the incoming tasks bitmap which
@@ -382,13 +408,15 @@ static void switch_to_update_io_bitmap(struct tss_struct *tss,
 	tss->io_bitmap.prev_sequence = iobm->sequence;
 }
 
-static inline void switch_to_bitmap(struct thread_struct *next,
-				    unsigned long tifp, unsigned long tifn)
+/**
+ * tss_update_io_bitmap - Update I/O bitmap before exiting to usermode
+ */
+void tss_update_io_bitmap(void)
 {
 	struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);
 
-	if (tifn & _TIF_IO_BITMAP) {
-		struct io_bitmap *iobm = next->io_bitmap;
+	if (test_thread_flag(TIF_IO_BITMAP)) {
+		struct io_bitmap *iobm = current->thread.io_bitmap;
 
 		/*
 		 * Only copy bitmap data when the sequence number
@@ -396,7 +424,7 @@ static inline void switch_to_bitmap(struct thread_struct *next,
 		 * task.
 		 */
 		if (tss->io_bitmap.prev_sequence != iobm->sequence)
-			switch_to_update_io_bitmap(tss, iobm);
+			tss_copy_io_bitmap(tss, iobm);
 
 		/* Enable the bitmap */
 		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_VALID;
@@ -409,18 +437,8 @@ static inline void switch_to_bitmap(struct thread_struct *next,
 		 * limit.
 		 */
 		refresh_tss_limit();
-	} else if (tifp & _TIF_IO_BITMAP) {
-		/*
-		 * Do not touch the bitmap. Let the next bitmap using task
-		 * deal with the mess. Just make the io_bitmap_base invalid
-		 * by moving it outside the TSS limit so any subsequent I/O
-		 * access from user space will trigger a #GP.
-		 *
-		 * This is correct even when VMEXIT rewrites the TSS limit
-		 * to 0x67 as the only requirement is that the base points
-		 * outside the limit.
-		 */
-		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
+	} else {
+		tss_invalidate_io_bitmap(tss);
 	}
 }
 
@@ -634,7 +652,8 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)
 
 	tifn = READ_ONCE(task_thread_info(next_p)->flags);
 	tifp = READ_ONCE(task_thread_info(prev_p)->flags);
-	switch_to_bitmap(next, tifp, tifn);
+
+	switch_to_bitmap(tifp);
 
 	propagate_user_return_notify(prev_p, next_p);
 

commit 060aa16fdb7c5078a4159a76e5dc87d6a493af9b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:22 2019 +0100

    x86/ioperm: Add bitmap sequence number
    
    Add a globally unique sequence number which is incremented when ioperm() is
    changing the I/O bitmap of a task. Store the new sequence number in the
    io_bitmap structure and compare it with the sequence number of the I/O
    bitmap which was last loaded on a CPU. Only update the bitmap if the
    sequence is different.
    
    That should further reduce the overhead of I/O bitmap scheduling when there
    are only a few I/O bitmap users on the system.
    
    The 64bit sequence counter is sufficient. A wraparound of the sequence
    counter assuming an ioperm() call every nanosecond would require about 584
    years of uptime.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 1504fd2b9bc4..7c49be90468b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -360,6 +360,28 @@ void arch_setup_new_exec(void)
 	}
 }
 
+static void switch_to_update_io_bitmap(struct tss_struct *tss,
+				       struct io_bitmap *iobm)
+{
+	/*
+	 * Copy at least the byte range of the incoming tasks bitmap which
+	 * covers the permitted I/O ports.
+	 *
+	 * If the previous task which used an I/O bitmap had more bits
+	 * permitted, then the copy needs to cover those as well so they
+	 * get turned off.
+	 */
+	memcpy(tss->io_bitmap.bitmap, iobm->bitmap,
+	       max(tss->io_bitmap.prev_max, iobm->max));
+
+	/*
+	 * Store the new max and the sequence number of this bitmap
+	 * and a pointer to the bitmap itself.
+	 */
+	tss->io_bitmap.prev_max = iobm->max;
+	tss->io_bitmap.prev_sequence = iobm->sequence;
+}
+
 static inline void switch_to_bitmap(struct thread_struct *next,
 				    unsigned long tifp, unsigned long tifn)
 {
@@ -369,18 +391,14 @@ static inline void switch_to_bitmap(struct thread_struct *next,
 		struct io_bitmap *iobm = next->io_bitmap;
 
 		/*
-		 * Copy at least the size of the incoming tasks bitmap
-		 * which covers the last permitted I/O port.
-		 *
-		 * If the previous task which used an io bitmap had more
-		 * bits permitted, then the copy needs to cover those as
-		 * well so they get turned off.
+		 * Only copy bitmap data when the sequence number
+		 * differs. The update time is accounted to the incoming
+		 * task.
 		 */
-		memcpy(tss->io_bitmap.bitmap, next->io_bitmap->bitmap,
-		       max(tss->io_bitmap.prev_max, next->io_bitmap->max));
+		if (tss->io_bitmap.prev_sequence != iobm->sequence)
+			switch_to_update_io_bitmap(tss, iobm);
 
-		/* Store the new max and set io_bitmap_base valid */
-		tss->io_bitmap.prev_max = next->io_bitmap->max;
+		/* Enable the bitmap */
 		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_VALID;
 
 		/*

commit 577d5cd7e5851d3832066cd0422475fa7db2ee17
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:21 2019 +0100

    x86/ioperm: Move iobitmap data into a struct
    
    No point in having all the data in thread_struct, especially as upcoming
    changes add more.
    
    Make the bitmap in the new struct accessible as array of longs and as array
    of characters via a union, so both the bitmap functions and the update
    logic can avoid type casts.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 35f1c80df7a8..1504fd2b9bc4 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -41,6 +41,7 @@
 #include <asm/desc.h>
 #include <asm/prctl.h>
 #include <asm/spec-ctrl.h>
+#include <asm/io_bitmap.h>
 #include <asm/proto.h>
 
 #include "process.h"
@@ -101,21 +102,20 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 void exit_thread(struct task_struct *tsk)
 {
 	struct thread_struct *t = &tsk->thread;
-	unsigned long *bp = t->io_bitmap_ptr;
+	struct io_bitmap *iobm = t->io_bitmap;
 	struct fpu *fpu = &t->fpu;
 	struct tss_struct *tss;
 
-	if (bp) {
+	if (iobm) {
 		preempt_disable();
 		tss = this_cpu_ptr(&cpu_tss_rw);
 
-		t->io_bitmap_ptr = NULL;
-		t->io_bitmap_max = 0;
+		t->io_bitmap = NULL;
 		clear_thread_flag(TIF_IO_BITMAP);
 		/* Invalidate the io bitmap base in the TSS */
 		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
 		preempt_enable();
-		kfree(bp);
+		kfree(iobm);
 	}
 
 	free_vm86(t);
@@ -135,25 +135,25 @@ static int set_new_tls(struct task_struct *p, unsigned long tls)
 
 static inline int copy_io_bitmap(struct task_struct *tsk)
 {
+	struct io_bitmap *iobm = current->thread.io_bitmap;
+
 	if (likely(!test_tsk_thread_flag(current, TIF_IO_BITMAP)))
 		return 0;
 
-	tsk->thread.io_bitmap_ptr = kmemdup(current->thread.io_bitmap_ptr,
-					    IO_BITMAP_BYTES, GFP_KERNEL);
-	if (!tsk->thread.io_bitmap_ptr) {
-		tsk->thread.io_bitmap_max = 0;
+	tsk->thread.io_bitmap = kmemdup(iobm, sizeof(*iobm), GFP_KERNEL);
+
+	if (!tsk->thread.io_bitmap)
 		return -ENOMEM;
-	}
+
 	set_tsk_thread_flag(tsk, TIF_IO_BITMAP);
 	return 0;
 }
 
 static inline void free_io_bitmap(struct task_struct *tsk)
 {
-	if (tsk->thread.io_bitmap_ptr) {
-		kfree(tsk->thread.io_bitmap_ptr);
-		tsk->thread.io_bitmap_ptr = NULL;
-		tsk->thread.io_bitmap_max = 0;
+	if (tsk->thread.io_bitmap) {
+		kfree(tsk->thread.io_bitmap);
+		tsk->thread.io_bitmap = NULL;
 	}
 }
 
@@ -172,7 +172,7 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 	frame->bp = 0;
 	frame->ret_addr = (unsigned long) ret_from_fork;
 	p->thread.sp = (unsigned long) fork_frame;
-	p->thread.io_bitmap_ptr = NULL;
+	p->thread.io_bitmap = NULL;
 	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));
 
 #ifdef CONFIG_X86_64
@@ -366,6 +366,8 @@ static inline void switch_to_bitmap(struct thread_struct *next,
 	struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);
 
 	if (tifn & _TIF_IO_BITMAP) {
+		struct io_bitmap *iobm = next->io_bitmap;
+
 		/*
 		 * Copy at least the size of the incoming tasks bitmap
 		 * which covers the last permitted I/O port.
@@ -374,11 +376,11 @@ static inline void switch_to_bitmap(struct thread_struct *next,
 		 * bits permitted, then the copy needs to cover those as
 		 * well so they get turned off.
 		 */
-		memcpy(tss->io_bitmap.bitmap, next->io_bitmap_ptr,
-		       max(tss->io_bitmap.prev_max, next->io_bitmap_max));
+		memcpy(tss->io_bitmap.bitmap, next->io_bitmap->bitmap,
+		       max(tss->io_bitmap.prev_max, next->io_bitmap->max));
 
 		/* Store the new max and set io_bitmap_base valid */
-		tss->io_bitmap.prev_max = next->io_bitmap_max;
+		tss->io_bitmap.prev_max = next->io_bitmap->max;
 		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_VALID;
 
 		/*

commit f5848e5fd2f813c3a8009a642dfbcf635287c199
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 12 18:45:29 2019 +0100

    x86/tss: Move I/O bitmap data into a seperate struct
    
    Move the non hardware portion of I/O bitmap data into a seperate struct for
    readability sake.
    
    Originally-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 2444fe296de5..35f1c80df7a8 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -374,11 +374,11 @@ static inline void switch_to_bitmap(struct thread_struct *next,
 		 * bits permitted, then the copy needs to cover those as
 		 * well so they get turned off.
 		 */
-		memcpy(tss->io_bitmap, next->io_bitmap_ptr,
-		       max(tss->io_bitmap_prev_max, next->io_bitmap_max));
+		memcpy(tss->io_bitmap.bitmap, next->io_bitmap_ptr,
+		       max(tss->io_bitmap.prev_max, next->io_bitmap_max));
 
 		/* Store the new max and set io_bitmap_base valid */
-		tss->io_bitmap_prev_max = next->io_bitmap_max;
+		tss->io_bitmap.prev_max = next->io_bitmap_max;
 		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_VALID;
 
 		/*

commit ecc7e37d4dadd16f6be125ca496feccd05454da4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:20 2019 +0100

    x86/io: Speedup schedule out of I/O bitmap user
    
    There is no requirement to update the TSS I/O bitmap when a thread using it is
    scheduled out and the incoming thread does not use it.
    
    For the permission check based on the TSS I/O bitmap the CPU calculates the memory
    location of the I/O bitmap by the address of the TSS and the io_bitmap_base member
    of the tss_struct. The easiest way to invalidate the I/O bitmap is to switch the
    offset to an address outside of the TSS limit.
    
    If an I/O instruction is issued from user space the TSS limit causes #GP to be
    raised in the same was as valid I/O bitmap with all bits set to 1 would do.
    
    This removes the extra work when an I/O bitmap using task is scheduled out
    and puts the burden on the rare I/O bitmap users when they are scheduled
    in.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7e07f0bb0def..2444fe296de5 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -72,18 +72,9 @@ __visible DEFINE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw) = {
 #ifdef CONFIG_X86_32
 		.ss0 = __KERNEL_DS,
 		.ss1 = __KERNEL_CS,
-		.io_bitmap_base	= INVALID_IO_BITMAP_OFFSET,
 #endif
+		.io_bitmap_base	= IO_BITMAP_OFFSET_INVALID,
 	 },
-#ifdef CONFIG_X86_32
-	 /*
-	  * Note that the .io_bitmap member must be extra-big. This is because
-	  * the CPU will access an additional byte beyond the end of the IO
-	  * permission bitmap. The extra byte must be all 1 bits, and must
-	  * be within the limit.
-	  */
-	.io_bitmap		= { [0 ... IO_BITMAP_LONGS] = ~0 },
-#endif
 };
 EXPORT_PER_CPU_SYMBOL(cpu_tss_rw);
 
@@ -112,18 +103,18 @@ void exit_thread(struct task_struct *tsk)
 	struct thread_struct *t = &tsk->thread;
 	unsigned long *bp = t->io_bitmap_ptr;
 	struct fpu *fpu = &t->fpu;
+	struct tss_struct *tss;
 
 	if (bp) {
-		struct tss_struct *tss = &per_cpu(cpu_tss_rw, get_cpu());
+		preempt_disable();
+		tss = this_cpu_ptr(&cpu_tss_rw);
 
 		t->io_bitmap_ptr = NULL;
-		clear_thread_flag(TIF_IO_BITMAP);
-		/*
-		 * Careful, clear this in the TSS too:
-		 */
-		memset(tss->io_bitmap, 0xff, t->io_bitmap_max);
 		t->io_bitmap_max = 0;
-		put_cpu();
+		clear_thread_flag(TIF_IO_BITMAP);
+		/* Invalidate the io bitmap base in the TSS */
+		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
+		preempt_enable();
 		kfree(bp);
 	}
 
@@ -369,29 +360,47 @@ void arch_setup_new_exec(void)
 	}
 }
 
-static inline void switch_to_bitmap(struct thread_struct *prev,
-				    struct thread_struct *next,
+static inline void switch_to_bitmap(struct thread_struct *next,
 				    unsigned long tifp, unsigned long tifn)
 {
 	struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);
 
 	if (tifn & _TIF_IO_BITMAP) {
 		/*
-		 * Copy the relevant range of the IO bitmap.
-		 * Normally this is 128 bytes or less:
+		 * Copy at least the size of the incoming tasks bitmap
+		 * which covers the last permitted I/O port.
+		 *
+		 * If the previous task which used an io bitmap had more
+		 * bits permitted, then the copy needs to cover those as
+		 * well so they get turned off.
 		 */
 		memcpy(tss->io_bitmap, next->io_bitmap_ptr,
-		       max(prev->io_bitmap_max, next->io_bitmap_max));
+		       max(tss->io_bitmap_prev_max, next->io_bitmap_max));
+
+		/* Store the new max and set io_bitmap_base valid */
+		tss->io_bitmap_prev_max = next->io_bitmap_max;
+		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_VALID;
+
 		/*
-		 * Make sure that the TSS limit is correct for the CPU
-		 * to notice the IO bitmap.
+		 * Make sure that the TSS limit is covering the io bitmap.
+		 * It might have been cut down by a VMEXIT to 0x67 which
+		 * would cause a subsequent I/O access from user space to
+		 * trigger a #GP because tbe bitmap is outside the TSS
+		 * limit.
 		 */
 		refresh_tss_limit();
 	} else if (tifp & _TIF_IO_BITMAP) {
 		/*
-		 * Clear any possible leftover bits:
+		 * Do not touch the bitmap. Let the next bitmap using task
+		 * deal with the mess. Just make the io_bitmap_base invalid
+		 * by moving it outside the TSS limit so any subsequent I/O
+		 * access from user space will trigger a #GP.
+		 *
+		 * This is correct even when VMEXIT rewrites the TSS limit
+		 * to 0x67 as the only requirement is that the base points
+		 * outside the limit.
 		 */
-		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
+		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
 	}
 }
 
@@ -605,7 +614,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)
 
 	tifn = READ_ONCE(task_thread_info(next_p)->flags);
 	tifp = READ_ONCE(task_thread_info(prev_p)->flags);
-	switch_to_bitmap(prev, next, tifp, tifn);
+	switch_to_bitmap(next, tifp, tifn);
 
 	propagate_user_return_notify(prev_p, next_p);
 

commit 2fff071d28b54f050f62654dad4ec111b8416d8e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:16 2019 +0100

    x86/process: Unify copy_thread_tls()
    
    While looking at the TSS io bitmap it turned out that any change in that
    area would require identical changes to copy_thread_tls(). The 32 and 64
    bit variants share sufficient code to consolidate them into a common
    function to avoid duplication of upcoming modifications.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 5e94c4354d4e..7e07f0bb0def 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -132,6 +132,106 @@ void exit_thread(struct task_struct *tsk)
 	fpu__drop(fpu);
 }
 
+static int set_new_tls(struct task_struct *p, unsigned long tls)
+{
+	struct user_desc __user *utls = (struct user_desc __user *)tls;
+
+	if (in_ia32_syscall())
+		return do_set_thread_area(p, -1, utls, 0);
+	else
+		return do_set_thread_area_64(p, ARCH_SET_FS, tls);
+}
+
+static inline int copy_io_bitmap(struct task_struct *tsk)
+{
+	if (likely(!test_tsk_thread_flag(current, TIF_IO_BITMAP)))
+		return 0;
+
+	tsk->thread.io_bitmap_ptr = kmemdup(current->thread.io_bitmap_ptr,
+					    IO_BITMAP_BYTES, GFP_KERNEL);
+	if (!tsk->thread.io_bitmap_ptr) {
+		tsk->thread.io_bitmap_max = 0;
+		return -ENOMEM;
+	}
+	set_tsk_thread_flag(tsk, TIF_IO_BITMAP);
+	return 0;
+}
+
+static inline void free_io_bitmap(struct task_struct *tsk)
+{
+	if (tsk->thread.io_bitmap_ptr) {
+		kfree(tsk->thread.io_bitmap_ptr);
+		tsk->thread.io_bitmap_ptr = NULL;
+		tsk->thread.io_bitmap_max = 0;
+	}
+}
+
+int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
+		    unsigned long arg, struct task_struct *p, unsigned long tls)
+{
+	struct inactive_task_frame *frame;
+	struct fork_frame *fork_frame;
+	struct pt_regs *childregs;
+	int ret;
+
+	childregs = task_pt_regs(p);
+	fork_frame = container_of(childregs, struct fork_frame, regs);
+	frame = &fork_frame->frame;
+
+	frame->bp = 0;
+	frame->ret_addr = (unsigned long) ret_from_fork;
+	p->thread.sp = (unsigned long) fork_frame;
+	p->thread.io_bitmap_ptr = NULL;
+	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));
+
+#ifdef CONFIG_X86_64
+	savesegment(gs, p->thread.gsindex);
+	p->thread.gsbase = p->thread.gsindex ? 0 : current->thread.gsbase;
+	savesegment(fs, p->thread.fsindex);
+	p->thread.fsbase = p->thread.fsindex ? 0 : current->thread.fsbase;
+	savesegment(es, p->thread.es);
+	savesegment(ds, p->thread.ds);
+#else
+	p->thread.sp0 = (unsigned long) (childregs + 1);
+	/*
+	 * Clear all status flags including IF and set fixed bit. 64bit
+	 * does not have this initialization as the frame does not contain
+	 * flags. The flags consistency (especially vs. AC) is there
+	 * ensured via objtool, which lacks 32bit support.
+	 */
+	frame->flags = X86_EFLAGS_FIXED;
+#endif
+
+	/* Kernel thread ? */
+	if (unlikely(p->flags & PF_KTHREAD)) {
+		memset(childregs, 0, sizeof(struct pt_regs));
+		kthread_frame_init(frame, sp, arg);
+		return 0;
+	}
+
+	frame->bx = 0;
+	*childregs = *current_pt_regs();
+	childregs->ax = 0;
+	if (sp)
+		childregs->sp = sp;
+
+#ifdef CONFIG_X86_32
+	task_user_gs(p) = get_user_gs(current_pt_regs());
+#endif
+
+	ret = copy_io_bitmap(p);
+	if (ret)
+		return ret;
+
+	/* Set a new TLS for the child thread? */
+	if (clone_flags & CLONE_SETTLS) {
+		ret = set_new_tls(p, tls);
+		if (ret)
+			free_io_bitmap(p);
+	}
+	return ret;
+}
+
 void flush_thread(void)
 {
 	struct task_struct *tsk = current;

commit fa86ee90eb1111267de67cb4272b5ce711f18cbb
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Jul 3 20:51:25 2019 -0300

    add cpuidle-haltpoll driver
    
    Add a cpuidle driver that calls the architecture default_idle routine.
    
    To be used in conjunction with the haltpoll governor.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 75fea0d48c0e..5e94c4354d4e 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -580,7 +580,7 @@ void __cpuidle default_idle(void)
 	safe_halt();
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
-#ifdef CONFIG_APM_MODULE
+#if defined(CONFIG_APM_MODULE) || defined(CONFIG_HALTPOLL_CPUIDLE_MODULE)
 EXPORT_SYMBOL(default_idle);
 #endif
 

commit 8ff468c29e9a9c3afe9152c10c7b141343270bf3
Merge: 68253e718c27 d9c9ce34ed5c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 7 10:24:10 2019 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU state handling updates from Borislav Petkov:
     "This contains work started by Rik van Riel and brought to fruition by
      Sebastian Andrzej Siewior with the main goal to optimize when to load
      FPU registers: only when returning to userspace and not on every
      context switch (while the task remains in the kernel).
    
      In addition, this optimization makes kernel_fpu_begin() cheaper by
      requiring registers saving only on the first invocation and skipping
      that in following ones.
    
      What is more, this series cleans up and streamlines many aspects of
      the already complex FPU code, hopefully making it more palatable for
      future improvements and simplifications.
    
      Finally, there's a __user annotations fix from Jann Horn"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (29 commits)
      x86/fpu: Fault-in user stack if copy_fpstate_to_sigframe() fails
      x86/pkeys: Add PKRU value to init_fpstate
      x86/fpu: Restore regs in copy_fpstate_to_sigframe() in order to use the fastpath
      x86/fpu: Add a fastpath to copy_fpstate_to_sigframe()
      x86/fpu: Add a fastpath to __fpu__restore_sig()
      x86/fpu: Defer FPU state load until return to userspace
      x86/fpu: Merge the two code paths in __fpu__restore_sig()
      x86/fpu: Restore from kernel memory on the 64-bit path too
      x86/fpu: Inline copy_user_to_fpregs_zeroing()
      x86/fpu: Update xstate's PKRU value on write_pkru()
      x86/fpu: Prepare copy_fpstate_to_sigframe() for TIF_NEED_FPU_LOAD
      x86/fpu: Always store the registers in copy_fpstate_to_sigframe()
      x86/entry: Add TIF_NEED_FPU_LOAD
      x86/fpu: Eager switch PKRU state
      x86/pkeys: Don't check if PKRU is zero before writing it
      x86/fpu: Only write PKRU if it is different from current
      x86/pkeys: Provide *pkru() helpers
      x86/fpu: Use a feature number instead of mask in two more helpers
      x86/fpu: Make __raw_xsave_addr() use a feature number instead of mask
      x86/fpu: Add an __fpregs_load_activate() internal helper
      ...

commit f725492dd16f516c2b67d7cee90b8619d09fd534
Merge: 80e77644efcc 3855f11d54a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 15:32:35 2019 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "This includes the following changes:
    
       - cpu_has() cleanups
    
       - sync_bitops.h modernization to the rmwcc.h facility, similarly to
         bitops.h
    
       - continued LTO annotations/fixes
    
       - misc cleanups and smaller cleanups"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/um/vdso: Drop unnecessary cc-ldoption
      x86/vdso: Rename variable to fix -Wshadow warning
      x86/cpu/amd: Exclude 32bit only assembler from 64bit build
      x86/asm: Mark all top level asm statements as .text
      x86/build/vdso: Add FORCE to the build rule of %.so
      x86/asm: Modernize sync_bitops.h
      x86/mm: Convert some slow-path static_cpu_has() callers to boot_cpu_has()
      x86: Convert some slow-path static_cpu_has() callers to boot_cpu_has()
      x86/asm: Clarify static_cpu_has()'s intended use
      x86/uaccess: Fix implicit cast of __user pointer
      x86/cpufeature: Remove __pure attribute to _static_cpu_has()

commit 2f5fb19341883bb6e37da351bc3700489d8506a7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 19:51:06 2019 +0200

    x86/speculation: Prevent deadlock on ssb_state::lock
    
    Mikhail reported a lockdep splat related to the AMD specific ssb_state
    lock:
    
      CPU0                       CPU1
      lock(&st->lock);
                                 local_irq_disable();
                                 lock(&(&sighand->siglock)->rlock);
                                 lock(&st->lock);
      <Interrupt>
         lock(&(&sighand->siglock)->rlock);
    
      *** DEADLOCK ***
    
    The connection between sighand->siglock and st->lock comes through seccomp,
    which takes st->lock while holding sighand->siglock.
    
    Make sure interrupts are disabled when __speculation_ctrl_update() is
    invoked via prctl() -> speculation_ctrl_update(). Add a lockdep assert to
    catch future offenders.
    
    Fixes: 1f50ddb4f418 ("x86/speculation: Handle HT correctly on AMD")
    Reported-by: Mikhail Gavrilov <mikhail.v.gavrilov@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Mikhail Gavrilov <mikhail.v.gavrilov@gmail.com>
    Cc: Thomas Lendacky <thomas.lendacky@amd.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1904141948200.4917@nanos.tec.linutronix.de

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 58ac7be52c7a..957eae13b370 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -426,6 +426,8 @@ static __always_inline void __speculation_ctrl_update(unsigned long tifp,
 	u64 msr = x86_spec_ctrl_base;
 	bool updmsr = false;
 
+	lockdep_assert_irqs_disabled();
+
 	/*
 	 * If TIF_SSBD is different, select the proper mitigation
 	 * method. Note that if SSBD mitigation is disabled or permanentely
@@ -477,10 +479,12 @@ static unsigned long speculation_ctrl_update_tif(struct task_struct *tsk)
 
 void speculation_ctrl_update(unsigned long tif)
 {
+	unsigned long flags;
+
 	/* Forced update. Make sure all relevant TIF flags are different */
-	preempt_disable();
+	local_irq_save(flags);
 	__speculation_ctrl_update(~tif, tif);
-	preempt_enable();
+	local_irq_restore(flags);
 }
 
 /* Called from seccomp/prctl update */

commit 5f409e20b794565e2d60ad333e79334630a6c798
Author: Rik van Riel <riel@surriel.com>
Date:   Wed Apr 3 18:41:52 2019 +0200

    x86/fpu: Defer FPU state load until return to userspace
    
    Defer loading of FPU state until return to userspace. This gives
    the kernel the potential to skip loading FPU state for tasks that
    stay in kernel mode, or for tasks that end up with repeated
    invocations of kernel_fpu_begin() & kernel_fpu_end().
    
    The fpregs_lock/unlock() section ensures that the registers remain
    unchanged. Otherwise a context switch or a bottom half could save the
    registers to its FPU context and the processor's FPU registers would
    became random if modified at the same time.
    
    KVM swaps the host/guest registers on entry/exit path. This flow has
    been kept as is. First it ensures that the registers are loaded and then
    saves the current (host) state before it loads the guest's registers. The
    swap is done at the very end with disabled interrupts so it should not
    change anymore before theg guest is entered. The read/save version seems
    to be cheaper compared to memcpy() in a micro benchmark.
    
    Each thread gets TIF_NEED_FPU_LOAD set as part of fork() / fpu__copy().
    For kernel threads, this flag gets never cleared which avoids saving /
    restoring the FPU state for kernel threads and during in-kernel usage of
    the FPU registers.
    
     [
       bp: Correct and update commit message and fix checkpatch warnings.
       s/register/registers/ where it is used in plural.
       minor comment corrections.
       remove unused trace_x86_fpu_activate_state() TP.
     ]
    
    Signed-off-by: Rik van Riel <riel@surriel.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Babu Moger <Babu.Moger@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Waiman Long <longman@redhat.com>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yi Wang <wang.yi59@zte.com.cn>
    Link: https://lkml.kernel.org/r/20190403164156.19645-24-bigeasy@linutronix.de

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 58ac7be52c7a..b9b8e6eb74f2 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -101,7 +101,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	dst->thread.vm86 = NULL;
 #endif
 
-	return fpu__copy(&dst->thread.fpu, &src->thread.fpu);
+	return fpu__copy(dst, src);
 }
 
 /*

commit 67e87d43b794a8886b5d075b3e0fdd0c615a595f
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Mar 29 19:52:59 2019 +0100

    x86: Convert some slow-path static_cpu_has() callers to boot_cpu_has()
    
    Using static_cpu_has() is pointless on those paths, convert them to the
    boot_cpu_has() variant.
    
    No functional changes.
    
    Reported-by: Nadav Amit <nadav.amit@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Reviewed-by: Juergen Gross <jgross@suse.com> # for paravirt
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Thomas Lendacky <Thomas.Lendacky@amd.com>
    Cc: linux-edac@vger.kernel.org
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: virtualization@lists.linux-foundation.org
    Cc: x86@kernel.org
    Link: https://lkml.kernel.org/r/20190330112022.28888-3-bp@alien8.de

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 58ac7be52c7a..16a7113e91c5 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -236,7 +236,7 @@ static int get_cpuid_mode(void)
 
 static int set_cpuid_mode(struct task_struct *task, unsigned long cpuid_enabled)
 {
-	if (!static_cpu_has(X86_FEATURE_CPUID_FAULT))
+	if (!boot_cpu_has(X86_FEATURE_CPUID_FAULT))
 		return -ENODEV;
 
 	if (cpuid_enabled)
@@ -666,7 +666,7 @@ static int prefer_mwait_c1_over_halt(const struct cpuinfo_x86 *c)
 	if (c->x86_vendor != X86_VENDOR_INTEL)
 		return 0;
 
-	if (!cpu_has(c, X86_FEATURE_MWAIT) || static_cpu_has_bug(X86_BUG_MONITOR))
+	if (!cpu_has(c, X86_FEATURE_MWAIT) || boot_cpu_has_bug(X86_BUG_MONITOR))
 		return 0;
 
 	return 1;

commit 71368af9027f18fe5d1c6f372cfdff7e4bde8b48
Author: Waiman Long <longman@redhat.com>
Date:   Wed Jan 16 17:01:36 2019 -0500

    x86/speculation: Add PR_SPEC_DISABLE_NOEXEC
    
    With the default SPEC_STORE_BYPASS_SECCOMP/SPEC_STORE_BYPASS_PRCTL mode,
    the TIF_SSBD bit will be inherited when a new task is fork'ed or cloned.
    It will also remain when a new program is execve'ed.
    
    Only certain class of applications (like Java) that can run on behalf of
    multiple users on a single thread will require disabling speculative store
    bypass for security purposes. Those applications will call prctl(2) at
    startup time to disable SSB. They won't rely on the fact the SSB might have
    been disabled. Other applications that don't need SSBD will just move on
    without checking if SSBD has been turned on or not.
    
    The fact that the TIF_SSBD is inherited across execve(2) boundary will
    cause performance of applications that don't need SSBD but their
    predecessors have SSBD on to be unwittingly impacted especially if they
    write to memory a lot.
    
    To remedy this problem, a new PR_SPEC_DISABLE_NOEXEC argument for the
    PR_SET_SPECULATION_CTRL option of prctl(2) is added to allow applications
    to specify that the SSBD feature bit on the task structure should be
    cleared whenever a new program is being execve'ed.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: linux-doc@vger.kernel.org
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: KarimAllah Ahmed <karahmed@amazon.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Link: https://lkml.kernel.org/r/1547676096-3281-1-git-send-email-longman@redhat.com

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 90ae0ca51083..58ac7be52c7a 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -255,6 +255,18 @@ void arch_setup_new_exec(void)
 	/* If cpuid was previously disabled for this task, re-enable it. */
 	if (test_thread_flag(TIF_NOCPUID))
 		enable_cpuid();
+
+	/*
+	 * Don't inherit TIF_SSBD across exec boundary when
+	 * PR_SPEC_DISABLE_NOEXEC is used.
+	 */
+	if (test_thread_flag(TIF_SSBD) &&
+	    task_spec_ssb_noexec(current)) {
+		clear_thread_flag(TIF_SSBD);
+		task_clear_spec_ssb_disable(current);
+		task_clear_spec_ssb_noexec(current);
+		speculation_ctrl_update(task_thread_info(current)->flags);
+	}
 }
 
 static inline void switch_to_bitmap(struct thread_struct *prev,

commit df60673198ae678f68af54873b8904ba93fe13a0
Merge: 89f579ce99f7 2595646791c3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Dec 3 10:47:53 2018 +0100

    Merge tag 'v4.20-rc5' into x86/cleanups, to sync up the tree
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9137bb27e60e554dab694eafa4cca241fa3a694f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Nov 25 19:33:53 2018 +0100

    x86/speculation: Add prctl() control for indirect branch speculation
    
    Add the PR_SPEC_INDIRECT_BRANCH option for the PR_GET_SPECULATION_CTRL and
    PR_SET_SPECULATION_CTRL prctls to allow fine grained per task control of
    indirect branch speculation via STIBP and IBPB.
    
    Invocations:
     Check indirect branch speculation status with
     - prctl(PR_GET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, 0, 0, 0);
    
     Enable indirect branch speculation with
     - prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, PR_SPEC_ENABLE, 0, 0);
    
     Disable indirect branch speculation with
     - prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, PR_SPEC_DISABLE, 0, 0);
    
     Force disable indirect branch speculation with
     - prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, PR_SPEC_FORCE_DISABLE, 0, 0);
    
    See Documentation/userspace-api/spec_ctrl.rst.
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181125185005.866780996@linutronix.de

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index afbe2eb4a1c6..7d31192296a8 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -450,6 +450,11 @@ static unsigned long speculation_ctrl_update_tif(struct task_struct *tsk)
 			set_tsk_thread_flag(tsk, TIF_SSBD);
 		else
 			clear_tsk_thread_flag(tsk, TIF_SSBD);
+
+		if (task_spec_ib_disable(tsk))
+			set_tsk_thread_flag(tsk, TIF_SPEC_IB);
+		else
+			clear_tsk_thread_flag(tsk, TIF_SPEC_IB);
 	}
 	/* Return the updated threadinfo flags*/
 	return task_thread_info(tsk)->flags;

commit 6d991ba509ebcfcc908e009d1db51972a4f7a064
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 28 10:56:57 2018 +0100

    x86/speculation: Prevent stale SPEC_CTRL msr content
    
    The seccomp speculation control operates on all tasks of a process, but
    only the current task of a process can update the MSR immediately. For the
    other threads the update is deferred to the next context switch.
    
    This creates the following situation with Process A and B:
    
    Process A task 2 and Process B task 1 are pinned on CPU1. Process A task 2
    does not have the speculation control TIF bit set. Process B task 1 has the
    speculation control TIF bit set.
    
    CPU0                                    CPU1
                                            MSR bit is set
                                            ProcB.T1 schedules out
                                            ProcA.T2 schedules in
                                            MSR bit is cleared
    ProcA.T1
      seccomp_update()
      set TIF bit on ProcA.T2
                                            ProcB.T1 schedules in
                                            MSR is not updated  <-- FAIL
    
    This happens because the context switch code tries to avoid the MSR update
    if the speculation control TIF bits of the incoming and the outgoing task
    are the same. In the worst case ProcB.T1 and ProcA.T2 are the only tasks
    scheduling back and forth on CPU1, which keeps the MSR stale forever.
    
    In theory this could be remedied by IPIs, but chasing the remote task which
    could be migrated is complex and full of races.
    
    The straight forward solution is to avoid the asychronous update of the TIF
    bit and defer it to the next context switch. The speculation control state
    is stored in task_struct::atomic_flags by the prctl and seccomp updates
    already.
    
    Add a new TIF_SPEC_FORCE_UPDATE bit and set this after updating the
    atomic_flags. Check the bit on context switch and force a synchronous
    update of the speculation control if set. Use the same mechanism for
    updating the current task.
    
    Reported-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1811272247140.1875@nanos.tec.linutronix.de

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index cdf8e6694f71..afbe2eb4a1c6 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -443,6 +443,18 @@ static __always_inline void __speculation_ctrl_update(unsigned long tifp,
 		wrmsrl(MSR_IA32_SPEC_CTRL, msr);
 }
 
+static unsigned long speculation_ctrl_update_tif(struct task_struct *tsk)
+{
+	if (test_and_clear_tsk_thread_flag(tsk, TIF_SPEC_FORCE_UPDATE)) {
+		if (task_spec_ssb_disable(tsk))
+			set_tsk_thread_flag(tsk, TIF_SSBD);
+		else
+			clear_tsk_thread_flag(tsk, TIF_SSBD);
+	}
+	/* Return the updated threadinfo flags*/
+	return task_thread_info(tsk)->flags;
+}
+
 void speculation_ctrl_update(unsigned long tif)
 {
 	/* Forced update. Make sure all relevant TIF flags are different */
@@ -451,6 +463,14 @@ void speculation_ctrl_update(unsigned long tif)
 	preempt_enable();
 }
 
+/* Called from seccomp/prctl update */
+void speculation_ctrl_update_current(void)
+{
+	preempt_disable();
+	speculation_ctrl_update(speculation_ctrl_update_tif(current));
+	preempt_enable();
+}
+
 void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)
 {
 	struct thread_struct *prev, *next;
@@ -482,7 +502,15 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)
 	if ((tifp ^ tifn) & _TIF_NOCPUID)
 		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));
 
-	__speculation_ctrl_update(tifp, tifn);
+	if (likely(!((tifp | tifn) & _TIF_SPEC_FORCE_UPDATE))) {
+		__speculation_ctrl_update(tifp, tifn);
+	} else {
+		speculation_ctrl_update_tif(prev_p);
+		tifn = speculation_ctrl_update_tif(next_p);
+
+		/* Enforce MSR update to ensure consistent state */
+		__speculation_ctrl_update(~tifn, tifn);
+	}
 }
 
 /*

commit ff16701a29cba3aafa0bd1656d766813b2d0a811
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Nov 25 19:33:47 2018 +0100

    x86/process: Consolidate and simplify switch_to_xtra() code
    
    Move the conditional invocation of __switch_to_xtra() into an inline
    function so the logic can be shared between 32 and 64 bit.
    
    Remove the handthrough of the TSS pointer and retrieve the pointer directly
    in the bitmap handling function. Use this_cpu_ptr() instead of the
    per_cpu() indirection.
    
    This is a preparatory change so integration of conditional indirect branch
    speculation optimization happens only in one place.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181125185005.280855518@linutronix.de

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 574b144d2b53..cdf8e6694f71 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -40,6 +40,8 @@
 #include <asm/prctl.h>
 #include <asm/spec-ctrl.h>
 
+#include "process.h"
+
 /*
  * per-CPU TSS segments. Threads are completely 'soft' on Linux,
  * no more per-task TSS's. The TSS size is kept cacheline-aligned
@@ -252,11 +254,12 @@ void arch_setup_new_exec(void)
 		enable_cpuid();
 }
 
-static inline void switch_to_bitmap(struct tss_struct *tss,
-				    struct thread_struct *prev,
+static inline void switch_to_bitmap(struct thread_struct *prev,
 				    struct thread_struct *next,
 				    unsigned long tifp, unsigned long tifn)
 {
+	struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);
+
 	if (tifn & _TIF_IO_BITMAP) {
 		/*
 		 * Copy the relevant range of the IO bitmap.
@@ -448,8 +451,7 @@ void speculation_ctrl_update(unsigned long tif)
 	preempt_enable();
 }
 
-void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
-		      struct tss_struct *tss)
+void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)
 {
 	struct thread_struct *prev, *next;
 	unsigned long tifp, tifn;
@@ -459,7 +461,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 
 	tifn = READ_ONCE(task_thread_info(next_p)->flags);
 	tifp = READ_ONCE(task_thread_info(prev_p)->flags);
-	switch_to_bitmap(tss, prev, next, tifp, tifn);
+	switch_to_bitmap(prev, next, tifp, tifn);
 
 	propagate_user_return_notify(prev_p, next_p);
 

commit 5bfbe3ad5840d941b89bcac54b821ba14f50a0ba
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Sun Nov 25 19:33:46 2018 +0100

    x86/speculation: Prepare for per task indirect branch speculation control
    
    To avoid the overhead of STIBP always on, it's necessary to allow per task
    control of STIBP.
    
    Add a new task flag TIF_SPEC_IB and evaluate it during context switch if
    SMT is active and flag evaluation is enabled by the speculation control
    code. Add the conditional evaluation to x86_virt_spec_ctrl() as well so the
    guest/host switch works properly.
    
    This has no effect because TIF_SPEC_IB cannot be set yet and the static key
    which controls evaluation is off. Preparatory patch for adding the control
    code.
    
    [ tglx: Simplify the context switch logic and make the TIF evaluation
            depend on SMP=y and on the static key controlling the conditional
            update. Rename it to TIF_SPEC_IB because it controls both STIBP and
            IBPB ]
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181125185005.176917199@linutronix.de

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 70e9832379e1..574b144d2b53 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -404,11 +404,17 @@ static __always_inline void amd_set_ssb_virt_state(unsigned long tifn)
 static __always_inline void __speculation_ctrl_update(unsigned long tifp,
 						      unsigned long tifn)
 {
+	unsigned long tif_diff = tifp ^ tifn;
 	u64 msr = x86_spec_ctrl_base;
 	bool updmsr = false;
 
-	/* If TIF_SSBD is different, select the proper mitigation method */
-	if ((tifp ^ tifn) & _TIF_SSBD) {
+	/*
+	 * If TIF_SSBD is different, select the proper mitigation
+	 * method. Note that if SSBD mitigation is disabled or permanentely
+	 * enabled this branch can't be taken because nothing can set
+	 * TIF_SSBD.
+	 */
+	if (tif_diff & _TIF_SSBD) {
 		if (static_cpu_has(X86_FEATURE_VIRT_SSBD)) {
 			amd_set_ssb_virt_state(tifn);
 		} else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD)) {
@@ -420,6 +426,16 @@ static __always_inline void __speculation_ctrl_update(unsigned long tifp,
 		}
 	}
 
+	/*
+	 * Only evaluate TIF_SPEC_IB if conditional STIBP is enabled,
+	 * otherwise avoid the MSR write.
+	 */
+	if (IS_ENABLED(CONFIG_SMP) &&
+	    static_branch_unlikely(&switch_to_cond_stibp)) {
+		updmsr |= !!(tif_diff & _TIF_SPEC_IB);
+		msr |= stibp_tif_to_spec_ctrl(tifn);
+	}
+
 	if (updmsr)
 		wrmsrl(MSR_IA32_SPEC_CTRL, msr);
 }

commit 01daf56875ee0cd50ed496a09b20eb369b45dfa5
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Sun Nov 25 19:33:35 2018 +0100

    x86/speculation: Reorganize speculation control MSRs update
    
    The logic to detect whether there's a change in the previous and next
    task's flag relevant to update speculation control MSRs is spread out
    across multiple functions.
    
    Consolidate all checks needed for updating speculation control MSRs into
    the new __speculation_ctrl_update() helper function.
    
    This makes it easy to pick the right speculation control MSR and the bits
    in MSR_IA32_SPEC_CTRL that need updating based on TIF flags changes.
    
    Originally-by: Thomas Lendacky <Thomas.Lendacky@amd.com>
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181125185004.151077005@linutronix.de

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 8aa49604f9ae..70e9832379e1 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -395,27 +395,40 @@ static __always_inline void amd_set_ssb_virt_state(unsigned long tifn)
 	wrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, ssbd_tif_to_spec_ctrl(tifn));
 }
 
-static __always_inline void spec_ctrl_update_msr(unsigned long tifn)
-{
-	u64 msr = x86_spec_ctrl_base | ssbd_tif_to_spec_ctrl(tifn);
-
-	wrmsrl(MSR_IA32_SPEC_CTRL, msr);
-}
+/*
+ * Update the MSRs managing speculation control, during context switch.
+ *
+ * tifp: Previous task's thread flags
+ * tifn: Next task's thread flags
+ */
+static __always_inline void __speculation_ctrl_update(unsigned long tifp,
+						      unsigned long tifn)
+{
+	u64 msr = x86_spec_ctrl_base;
+	bool updmsr = false;
+
+	/* If TIF_SSBD is different, select the proper mitigation method */
+	if ((tifp ^ tifn) & _TIF_SSBD) {
+		if (static_cpu_has(X86_FEATURE_VIRT_SSBD)) {
+			amd_set_ssb_virt_state(tifn);
+		} else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD)) {
+			amd_set_core_ssb_state(tifn);
+		} else if (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) ||
+			   static_cpu_has(X86_FEATURE_AMD_SSBD)) {
+			msr |= ssbd_tif_to_spec_ctrl(tifn);
+			updmsr  = true;
+		}
+	}
 
-static __always_inline void __speculation_ctrl_update(unsigned long tifn)
-{
-	if (static_cpu_has(X86_FEATURE_VIRT_SSBD))
-		amd_set_ssb_virt_state(tifn);
-	else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD))
-		amd_set_core_ssb_state(tifn);
-	else
-		spec_ctrl_update_msr(tifn);
+	if (updmsr)
+		wrmsrl(MSR_IA32_SPEC_CTRL, msr);
 }
 
 void speculation_ctrl_update(unsigned long tif)
 {
+	/* Forced update. Make sure all relevant TIF flags are different */
 	preempt_disable();
-	__speculation_ctrl_update(tif);
+	__speculation_ctrl_update(~tif, tif);
 	preempt_enable();
 }
 
@@ -451,8 +464,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 	if ((tifp ^ tifn) & _TIF_NOCPUID)
 		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));
 
-	if ((tifp ^ tifn) & _TIF_SSBD)
-		__speculation_ctrl_update(tifn);
+	__speculation_ctrl_update(tifp, tifn);
 }
 
 /*

commit 26c4d75b234040c11728a8acb796b3a85ba7507c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Nov 25 19:33:34 2018 +0100

    x86/speculation: Rename SSBD update functions
    
    During context switch, the SSBD bit in SPEC_CTRL MSR is updated according
    to changes of the TIF_SSBD flag in the current and next running task.
    
    Currently, only the bit controlling speculative store bypass disable in
    SPEC_CTRL MSR is updated and the related update functions all have
    "speculative_store" or "ssb" in their names.
    
    For enhanced mitigation control other bits in SPEC_CTRL MSR need to be
    updated as well, which makes the SSB names inadequate.
    
    Rename the "speculative_store*" functions to a more generic name. No
    functional change.
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181125185004.058866968@linutronix.de

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c93fcfdf1673..8aa49604f9ae 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -395,27 +395,27 @@ static __always_inline void amd_set_ssb_virt_state(unsigned long tifn)
 	wrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, ssbd_tif_to_spec_ctrl(tifn));
 }
 
-static __always_inline void intel_set_ssb_state(unsigned long tifn)
+static __always_inline void spec_ctrl_update_msr(unsigned long tifn)
 {
 	u64 msr = x86_spec_ctrl_base | ssbd_tif_to_spec_ctrl(tifn);
 
 	wrmsrl(MSR_IA32_SPEC_CTRL, msr);
 }
 
-static __always_inline void __speculative_store_bypass_update(unsigned long tifn)
+static __always_inline void __speculation_ctrl_update(unsigned long tifn)
 {
 	if (static_cpu_has(X86_FEATURE_VIRT_SSBD))
 		amd_set_ssb_virt_state(tifn);
 	else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD))
 		amd_set_core_ssb_state(tifn);
 	else
-		intel_set_ssb_state(tifn);
+		spec_ctrl_update_msr(tifn);
 }
 
-void speculative_store_bypass_update(unsigned long tif)
+void speculation_ctrl_update(unsigned long tif)
 {
 	preempt_disable();
-	__speculative_store_bypass_update(tif);
+	__speculation_ctrl_update(tif);
 	preempt_enable();
 }
 
@@ -452,7 +452,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));
 
 	if ((tifp ^ tifn) & _TIF_SSBD)
-		__speculative_store_bypass_update(tifn);
+		__speculation_ctrl_update(tifn);
 }
 
 /*

commit 89f579ce99f7e028e81885d3965f973c0f787611
Author: Yi Wang <wang.yi59@zte.com.cn>
Date:   Thu Nov 22 10:04:09 2018 +0800

    x86/headers: Fix -Wmissing-prototypes warning
    
    When building the kernel with W=1 we get a lot of -Wmissing-prototypes
    warnings, which are trivial in nature and easy to fix - and which may
    mask some real future bugs if the prototypes get out of sync with
    the function definition.
    
    This patch fixes most of -Wmissing-prototypes warnings which
    are in the root directory of arch/x86/kernel, not including
    the subdirectories.
    
    These are the warnings fixed in this patch:
    
      arch/x86/kernel/signal.c:865:17: warning: no previous prototype for ‘sys32_x32_rt_sigreturn’ [-Wmissing-prototypes]
      arch/x86/kernel/signal_compat.c:164:6: warning: no previous prototype for ‘sigaction_compat_abi’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:625:46: warning: no previous prototype for ‘sync_regs’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:640:24: warning: no previous prototype for ‘fixup_bad_iret’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:929:13: warning: no previous prototype for ‘trap_init’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:270:28: warning: no previous prototype for ‘smp_x86_platform_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:301:16: warning: no previous prototype for ‘smp_kvm_posted_intr_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:314:16: warning: no previous prototype for ‘smp_kvm_posted_intr_wakeup_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:328:16: warning: no previous prototype for ‘smp_kvm_posted_intr_nested_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq_work.c:16:28: warning: no previous prototype for ‘smp_irq_work_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/irqinit.c:79:13: warning: no previous prototype for ‘init_IRQ’ [-Wmissing-prototypes]
      arch/x86/kernel/quirks.c:672:13: warning: no previous prototype for ‘early_platform_quirks’ [-Wmissing-prototypes]
      arch/x86/kernel/tsc.c:1499:15: warning: no previous prototype for ‘calibrate_delay_is_known’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:653:13: warning: no previous prototype for ‘arch_post_acpi_subsys_init’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:717:15: warning: no previous prototype for ‘arch_randomize_brk’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:784:6: warning: no previous prototype for ‘do_arch_prctl_common’ [-Wmissing-prototypes]
      arch/x86/kernel/reboot.c:869:6: warning: no previous prototype for ‘nmi_panic_self_stop’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:176:27: warning: no previous prototype for ‘smp_reboot_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:260:28: warning: no previous prototype for ‘smp_reschedule_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:281:28: warning: no previous prototype for ‘smp_call_function_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:291:28: warning: no previous prototype for ‘smp_call_function_single_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:840:6: warning: no previous prototype for ‘arch_ftrace_update_trampoline’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:934:7: warning: no previous prototype for ‘arch_ftrace_trampoline_func’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:946:6: warning: no previous prototype for ‘arch_ftrace_trampoline_free’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:114:6: warning: no previous prototype for ‘crash_smp_send_stop’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:351:5: warning: no previous prototype for ‘crash_setup_memmap_entries’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:424:5: warning: no previous prototype for ‘crash_load_segments’ [-Wmissing-prototypes]
      arch/x86/kernel/machine_kexec_64.c:372:7: warning: no previous prototype for ‘arch_kexec_kernel_image_load’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:12:16: warning: no previous prototype for ‘__native_queued_spin_unlock’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:18:6: warning: no previous prototype for ‘pv_is_native_spin_unlock’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:24:16: warning: no previous prototype for ‘__native_vcpu_is_preempted’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:30:6: warning: no previous prototype for ‘pv_is_native_vcpu_is_preempted’ [-Wmissing-prototypes]
      arch/x86/kernel/kvm.c:258:1: warning: no previous prototype for ‘do_async_page_fault’ [-Wmissing-prototypes]
      arch/x86/kernel/jailhouse.c:200:6: warning: no previous prototype for ‘jailhouse_paravirt’ [-Wmissing-prototypes]
      arch/x86/kernel/check.c:91:13: warning: no previous prototype for ‘setup_bios_corruption_check’ [-Wmissing-prototypes]
      arch/x86/kernel/check.c:139:6: warning: no previous prototype for ‘check_for_bios_corruption’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:32:13: warning: no previous prototype for ‘early_init_dt_scan_chosen_arch’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:42:13: warning: no previous prototype for ‘add_dtb’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:108:6: warning: no previous prototype for ‘x86_of_pci_init’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:314:13: warning: no previous prototype for ‘x86_dtb_init’ [-Wmissing-prototypes]
      arch/x86/kernel/tracepoint.c:16:5: warning: no previous prototype for ‘trace_pagefault_reg’ [-Wmissing-prototypes]
      arch/x86/kernel/tracepoint.c:22:6: warning: no previous prototype for ‘trace_pagefault_unreg’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:113:22: warning: no previous prototype for ‘__startup_64’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:262:15: warning: no previous prototype for ‘__startup_secondary_64’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:350:12: warning: no previous prototype for ‘early_make_pgtable’ [-Wmissing-prototypes]
    
    [ mingo: rewrote the changelog, fixed build errors. ]
    
    Signed-off-by: Yi Wang <wang.yi59@zte.com.cn>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akataria@vmware.com
    Cc: akpm@linux-foundation.org
    Cc: andy.shevchenko@gmail.com
    Cc: anton@enomsg.org
    Cc: ard.biesheuvel@linaro.org
    Cc: bhe@redhat.com
    Cc: bhelgaas@google.com
    Cc: bp@alien8.de
    Cc: ccross@android.com
    Cc: devicetree@vger.kernel.org
    Cc: douly.fnst@cn.fujitsu.com
    Cc: dwmw@amazon.co.uk
    Cc: dyoung@redhat.com
    Cc: ebiederm@xmission.com
    Cc: frank.rowand@sony.com
    Cc: frowand.list@gmail.com
    Cc: ivan.gorinov@intel.com
    Cc: jailhouse-dev@googlegroups.com
    Cc: jan.kiszka@siemens.com
    Cc: jgross@suse.com
    Cc: jroedel@suse.de
    Cc: keescook@chromium.org
    Cc: kexec@lists.infradead.org
    Cc: konrad.wilk@oracle.com
    Cc: kvm@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-pci@vger.kernel.org
    Cc: luto@kernel.org
    Cc: m.mizuma@jp.fujitsu.com
    Cc: namit@vmware.com
    Cc: oleg@redhat.com
    Cc: pasha.tatashin@oracle.com
    Cc: pbonzini@redhat.com
    Cc: prarit@redhat.com
    Cc: pravin.shedge4linux@gmail.com
    Cc: rajvi.jingar@intel.com
    Cc: rkrcmar@redhat.com
    Cc: robh+dt@kernel.org
    Cc: robh@kernel.org
    Cc: rostedt@goodmis.org
    Cc: takahiro.akashi@linaro.org
    Cc: thomas.lendacky@amd.com
    Cc: tony.luck@intel.com
    Cc: up2wing@gmail.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: zhe.he@windriver.com
    Cc: zhong.weidong@zte.com.cn
    Link: http://lkml.kernel.org/r/1542852249-19820-1-git-send-email-wang.yi59@zte.com.cn
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 3c3ee8982577..b7cb5348f37f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -22,6 +22,8 @@
 #include <linux/utsname.h>
 #include <linux/stackprotector.h>
 #include <linux/cpuidle.h>
+#include <linux/acpi.h>
+#include <linux/elf-randomize.h>
 #include <trace/events/power.h>
 #include <linux/hw_breakpoint.h>
 #include <asm/cpu.h>
@@ -39,6 +41,7 @@
 #include <asm/desc.h>
 #include <asm/prctl.h>
 #include <asm/spec-ctrl.h>
+#include <asm/proto.h>
 
 /*
  * per-CPU TSS segments. Threads are completely 'soft' on Linux,

commit 6e662ae7bce6db602f79e57791f5fb887fb7d371
Author: Yafang Shao <laoar.shao@gmail.com>
Date:   Wed Nov 21 19:12:14 2018 +0800

    x86/process: Avoid unnecessary NULL check in get_wchan()
    
    Task 'p' is always guaranteed to be non-NULL, because the only call
    sites are in fs/proc/ which all guarantee a non-NULL task pointer.
    
    [ mingo: Improved the changelog. ]
    Signed-off-by: Yafang Shao <laoar.shao@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1542798734-12532-1-git-send-email-laoar.shao@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c93fcfdf1673..3c3ee8982577 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -730,7 +730,7 @@ unsigned long get_wchan(struct task_struct *p)
 	unsigned long start, bottom, top, sp, fp, ip, ret = 0;
 	int count = 0;
 
-	if (!p || p == current || p->state == TASK_RUNNING)
+	if (p == current || p->state == TASK_RUNNING)
 		return 0;
 
 	if (!try_get_task_stack(p))

commit 45d7b255747c21fc4b1f5043bee0754d39c3bdbf
Author: Joerg Roedel <jroedel@suse.de>
Date:   Wed Jul 18 11:40:44 2018 +0200

    x86/entry/32: Enter the kernel via trampoline stack
    
    Use the entry-stack as a trampoline to enter the kernel. The entry-stack is
    already in the cpu_entry_area and will be mapped to userspace when PTI is
    enabled.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Pavel Machek <pavel@ucw.cz>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: linux-mm@kvack.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Waiman Long <llong@redhat.com>
    Cc: "David H . Gutteridge" <dhgutteridge@sympatico.ca>
    Cc: joro@8bytes.org
    Link: https://lkml.kernel.org/r/1531906876-13451-8-git-send-email-joro@8bytes.org

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 30ca2d1a9231..c93fcfdf1673 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -57,14 +57,12 @@ __visible DEFINE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw) = {
 		 */
 		.sp0 = (1UL << (BITS_PER_LONG-1)) + 1,
 
-#ifdef CONFIG_X86_64
 		/*
 		 * .sp1 is cpu_current_top_of_stack.  The init task never
 		 * runs user code, but cpu_current_top_of_stack should still
 		 * be well defined before the first context switch.
 		 */
 		.sp1 = TOP_OF_INIT_STACK,
-#endif
 
 #ifdef CONFIG_X86_32
 		.ss0 = __KERNEL_DS,

commit 0270be3e34efb05a88bc4c422572ece038ef3608
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 10 20:31:44 2018 +0200

    x86/speculation: Rework speculative_store_bypass_update()
    
    The upcoming support for the virtual SPEC_CTRL MSR on AMD needs to reuse
    speculative_store_bypass_update() to avoid code duplication. Add an
    argument for supplying a thread info (TIF) value and create a wrapper
    speculative_store_bypass_update_current() which is used at the existing
    call site.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 91c3398286d8..30ca2d1a9231 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -414,10 +414,10 @@ static __always_inline void __speculative_store_bypass_update(unsigned long tifn
 		intel_set_ssb_state(tifn);
 }
 
-void speculative_store_bypass_update(void)
+void speculative_store_bypass_update(unsigned long tif)
 {
 	preempt_disable();
-	__speculative_store_bypass_update(current_thread_info()->flags);
+	__speculative_store_bypass_update(tif);
 	preempt_enable();
 }
 

commit 11fb0683493b2da112cd64c9dada221b52463bf7
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Thu May 17 17:09:18 2018 +0200

    x86/speculation: Add virtualized speculative store bypass disable support
    
    Some AMD processors only support a non-architectural means of enabling
    speculative store bypass disable (SSBD).  To allow a simplified view of
    this to a guest, an architectural definition has been created through a new
    CPUID bit, 0x80000008_EBX[25], and a new MSR, 0xc001011f.  With this, a
    hypervisor can virtualize the existence of this definition and provide an
    architectural method for using SSBD to a guest.
    
    Add the new CPUID feature, the new MSR and update the existing SSBD
    support to use this MSR when present.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index d6fe8648d3f6..91c3398286d8 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -388,6 +388,15 @@ static __always_inline void amd_set_core_ssb_state(unsigned long tifn)
 }
 #endif
 
+static __always_inline void amd_set_ssb_virt_state(unsigned long tifn)
+{
+	/*
+	 * SSBD has the same definition in SPEC_CTRL and VIRT_SPEC_CTRL,
+	 * so ssbd_tif_to_spec_ctrl() just works.
+	 */
+	wrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, ssbd_tif_to_spec_ctrl(tifn));
+}
+
 static __always_inline void intel_set_ssb_state(unsigned long tifn)
 {
 	u64 msr = x86_spec_ctrl_base | ssbd_tif_to_spec_ctrl(tifn);
@@ -397,7 +406,9 @@ static __always_inline void intel_set_ssb_state(unsigned long tifn)
 
 static __always_inline void __speculative_store_bypass_update(unsigned long tifn)
 {
-	if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD))
+	if (static_cpu_has(X86_FEATURE_VIRT_SSBD))
+		amd_set_ssb_virt_state(tifn);
+	else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD))
 		amd_set_core_ssb_state(tifn);
 	else
 		intel_set_ssb_state(tifn);

commit 1f50ddb4f4189243c05926b842dc1a0332195f31
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 9 21:53:09 2018 +0200

    x86/speculation: Handle HT correctly on AMD
    
    The AMD64_LS_CFG MSR is a per core MSR on Family 17H CPUs. That means when
    hyperthreading is enabled the SSBD bit toggle needs to take both cores into
    account. Otherwise the following situation can happen:
    
    CPU0            CPU1
    
    disable SSB
                    disable SSB
                    enable  SSB <- Enables it for the Core, i.e. for CPU0 as well
    
    So after the SSB enable on CPU1 the task on CPU0 runs with SSB enabled
    again.
    
    On Intel the SSBD control is per core as well, but the synchronization
    logic is implemented behind the per thread SPEC_CTRL MSR. It works like
    this:
    
      CORE_SPEC_CTRL = THREAD0_SPEC_CTRL | THREAD1_SPEC_CTRL
    
    i.e. if one of the threads enables a mitigation then this affects both and
    the mitigation is only disabled in the core when both threads disabled it.
    
    Add the necessary synchronization logic for AMD family 17H. Unfortunately
    that requires a spinlock to serialize the access to the MSR, but the locks
    are only shared between siblings.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index d71ef7eaa7ef..d6fe8648d3f6 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -279,22 +279,135 @@ static inline void switch_to_bitmap(struct tss_struct *tss,
 	}
 }
 
-static __always_inline void __speculative_store_bypass_update(unsigned long tifn)
+#ifdef CONFIG_SMP
+
+struct ssb_state {
+	struct ssb_state	*shared_state;
+	raw_spinlock_t		lock;
+	unsigned int		disable_state;
+	unsigned long		local_state;
+};
+
+#define LSTATE_SSB	0
+
+static DEFINE_PER_CPU(struct ssb_state, ssb_state);
+
+void speculative_store_bypass_ht_init(void)
 {
-	u64 msr;
+	struct ssb_state *st = this_cpu_ptr(&ssb_state);
+	unsigned int this_cpu = smp_processor_id();
+	unsigned int cpu;
+
+	st->local_state = 0;
+
+	/*
+	 * Shared state setup happens once on the first bringup
+	 * of the CPU. It's not destroyed on CPU hotunplug.
+	 */
+	if (st->shared_state)
+		return;
+
+	raw_spin_lock_init(&st->lock);
 
-	if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD)) {
-		msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);
+	/*
+	 * Go over HT siblings and check whether one of them has set up the
+	 * shared state pointer already.
+	 */
+	for_each_cpu(cpu, topology_sibling_cpumask(this_cpu)) {
+		if (cpu == this_cpu)
+			continue;
+
+		if (!per_cpu(ssb_state, cpu).shared_state)
+			continue;
+
+		/* Link it to the state of the sibling: */
+		st->shared_state = per_cpu(ssb_state, cpu).shared_state;
+		return;
+	}
+
+	/*
+	 * First HT sibling to come up on the core.  Link shared state of
+	 * the first HT sibling to itself. The siblings on the same core
+	 * which come up later will see the shared state pointer and link
+	 * themself to the state of this CPU.
+	 */
+	st->shared_state = st;
+}
+
+/*
+ * Logic is: First HT sibling enables SSBD for both siblings in the core
+ * and last sibling to disable it, disables it for the whole core. This how
+ * MSR_SPEC_CTRL works in "hardware":
+ *
+ *  CORE_SPEC_CTRL = THREAD0_SPEC_CTRL | THREAD1_SPEC_CTRL
+ */
+static __always_inline void amd_set_core_ssb_state(unsigned long tifn)
+{
+	struct ssb_state *st = this_cpu_ptr(&ssb_state);
+	u64 msr = x86_amd_ls_cfg_base;
+
+	if (!static_cpu_has(X86_FEATURE_ZEN)) {
+		msr |= ssbd_tif_to_amd_ls_cfg(tifn);
 		wrmsrl(MSR_AMD64_LS_CFG, msr);
+		return;
+	}
+
+	if (tifn & _TIF_SSBD) {
+		/*
+		 * Since this can race with prctl(), block reentry on the
+		 * same CPU.
+		 */
+		if (__test_and_set_bit(LSTATE_SSB, &st->local_state))
+			return;
+
+		msr |= x86_amd_ls_cfg_ssbd_mask;
+
+		raw_spin_lock(&st->shared_state->lock);
+		/* First sibling enables SSBD: */
+		if (!st->shared_state->disable_state)
+			wrmsrl(MSR_AMD64_LS_CFG, msr);
+		st->shared_state->disable_state++;
+		raw_spin_unlock(&st->shared_state->lock);
 	} else {
-		msr = x86_spec_ctrl_base | ssbd_tif_to_spec_ctrl(tifn);
-		wrmsrl(MSR_IA32_SPEC_CTRL, msr);
+		if (!__test_and_clear_bit(LSTATE_SSB, &st->local_state))
+			return;
+
+		raw_spin_lock(&st->shared_state->lock);
+		st->shared_state->disable_state--;
+		if (!st->shared_state->disable_state)
+			wrmsrl(MSR_AMD64_LS_CFG, msr);
+		raw_spin_unlock(&st->shared_state->lock);
 	}
 }
+#else
+static __always_inline void amd_set_core_ssb_state(unsigned long tifn)
+{
+	u64 msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);
+
+	wrmsrl(MSR_AMD64_LS_CFG, msr);
+}
+#endif
+
+static __always_inline void intel_set_ssb_state(unsigned long tifn)
+{
+	u64 msr = x86_spec_ctrl_base | ssbd_tif_to_spec_ctrl(tifn);
+
+	wrmsrl(MSR_IA32_SPEC_CTRL, msr);
+}
+
+static __always_inline void __speculative_store_bypass_update(unsigned long tifn)
+{
+	if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD))
+		amd_set_core_ssb_state(tifn);
+	else
+		intel_set_ssb_state(tifn);
+}
 
 void speculative_store_bypass_update(void)
 {
+	preempt_disable();
 	__speculative_store_bypass_update(current_thread_info()->flags);
+	preempt_enable();
 }
 
 void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,

commit 52817587e706686fcdb27f14c1b000c92f266c96
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 10 20:21:36 2018 +0200

    x86/cpufeatures: Disentangle SSBD enumeration
    
    The SSBD enumeration is similarly to the other bits magically shared
    between Intel and AMD though the mechanisms are different.
    
    Make X86_FEATURE_SSBD synthetic and set it depending on the vendor specific
    features or family dependent setup.
    
    Change the Intel bit to X86_FEATURE_SPEC_CTRL_SSBD to denote that SSBD is
    controlled via MSR_SPEC_CTRL and fix up the usage sites.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index b77a091bf3b8..d71ef7eaa7ef 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -283,7 +283,7 @@ static __always_inline void __speculative_store_bypass_update(unsigned long tifn
 {
 	u64 msr;
 
-	if (static_cpu_has(X86_FEATURE_AMD_SSBD)) {
+	if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD)) {
 		msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);
 		wrmsrl(MSR_AMD64_LS_CFG, msr);
 	} else {

commit 9f65fb29374ee37856dbad847b4e121aab72b510
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 9 21:41:38 2018 +0200

    x86/bugs: Rename _RDS to _SSBD
    
    Intel collateral will reference the SSB mitigation bit in IA32_SPEC_CTL[2]
    as SSBD (Speculative Store Bypass Disable).
    
    Hence changing it.
    
    It is unclear yet what the MSR_IA32_ARCH_CAPABILITIES (0x10a) Bit(4) name
    is going to be. Following the rename it would be SSBD_NO but that rolls out
    to Speculative Store Bypass Disable No.
    
    Also fixed the missing space in X86_FEATURE_AMD_SSBD.
    
    [ tglx: Fixup x86_amd_rds_enable() and rds_tif_to_amd_ls_cfg() as well ]
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 397342725046..b77a091bf3b8 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -283,11 +283,11 @@ static __always_inline void __speculative_store_bypass_update(unsigned long tifn
 {
 	u64 msr;
 
-	if (static_cpu_has(X86_FEATURE_AMD_RDS)) {
-		msr = x86_amd_ls_cfg_base | rds_tif_to_amd_ls_cfg(tifn);
+	if (static_cpu_has(X86_FEATURE_AMD_SSBD)) {
+		msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);
 		wrmsrl(MSR_AMD64_LS_CFG, msr);
 	} else {
-		msr = x86_spec_ctrl_base | rds_tif_to_spec_ctrl(tifn);
+		msr = x86_spec_ctrl_base | ssbd_tif_to_spec_ctrl(tifn);
 		wrmsrl(MSR_IA32_SPEC_CTRL, msr);
 	}
 }
@@ -329,7 +329,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 	if ((tifp ^ tifn) & _TIF_NOCPUID)
 		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));
 
-	if ((tifp ^ tifn) & _TIF_RDS)
+	if ((tifp ^ tifn) & _TIF_SSBD)
 		__speculative_store_bypass_update(tifn);
 }
 

commit 885f82bfbc6fefb6664ea27965c3ab9ac4194b8c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 29 15:21:42 2018 +0200

    x86/process: Allow runtime control of Speculative Store Bypass
    
    The Speculative Store Bypass vulnerability can be mitigated with the
    Reduced Data Speculation (RDS) feature. To allow finer grained control of
    this eventually expensive mitigation a per task mitigation control is
    required.
    
    Add a new TIF_RDS flag and put it into the group of TIF flags which are
    evaluated for mismatch in switch_to(). If these bits differ in the previous
    and the next task, then the slow path function __switch_to_xtra() is
    invoked. Implement the TIF_RDS dependent mitigation control in the slow
    path.
    
    If the prctl for controlling Speculative Store Bypass is disabled or no
    task uses the prctl then there is no overhead in the switch_to() fast
    path.
    
    Update the KVM related speculation control functions to take TID_RDS into
    account as well.
    
    Based on a patch from Tim Chen. Completely rewritten.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 03408b942adb..397342725046 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -38,6 +38,7 @@
 #include <asm/switch_to.h>
 #include <asm/desc.h>
 #include <asm/prctl.h>
+#include <asm/spec-ctrl.h>
 
 /*
  * per-CPU TSS segments. Threads are completely 'soft' on Linux,
@@ -278,6 +279,24 @@ static inline void switch_to_bitmap(struct tss_struct *tss,
 	}
 }
 
+static __always_inline void __speculative_store_bypass_update(unsigned long tifn)
+{
+	u64 msr;
+
+	if (static_cpu_has(X86_FEATURE_AMD_RDS)) {
+		msr = x86_amd_ls_cfg_base | rds_tif_to_amd_ls_cfg(tifn);
+		wrmsrl(MSR_AMD64_LS_CFG, msr);
+	} else {
+		msr = x86_spec_ctrl_base | rds_tif_to_spec_ctrl(tifn);
+		wrmsrl(MSR_IA32_SPEC_CTRL, msr);
+	}
+}
+
+void speculative_store_bypass_update(void)
+{
+	__speculative_store_bypass_update(current_thread_info()->flags);
+}
+
 void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 		      struct tss_struct *tss)
 {
@@ -309,6 +328,9 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 
 	if ((tifp ^ tifn) & _TIF_NOCPUID)
 		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));
+
+	if ((tifp ^ tifn) & _TIF_RDS)
+		__speculative_store_bypass_update(tifn);
 }
 
 /*

commit 3ccabd6d9d9b0da5780e0386b4bf7c5f07669e37
Merge: 5289d3005a36 782bf20c2a17
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 13:01:09 2018 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc cleanups"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Remove unused IOMMU_STRESS Kconfig
      x86/extable: Mark exception handler functions visible
      x86/timer: Don't inline __const_udelay
      x86/headers: Remove duplicate #includes

commit f23d74f6c66c3697e032550eeef3f640391a3a7d
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Wed Jan 17 17:41:41 2018 -0600

    x86/mm: Rework wbinvd, hlt operation in stop_this_cpu()
    
    Some issues have been reported with the for loop in stop_this_cpu() that
    issues the 'wbinvd; hlt' sequence.  Reverting this sequence to halt()
    has been shown to resolve the issue.
    
    However, the wbinvd is needed when running with SME.  The reason for the
    wbinvd is to prevent cache flush races between encrypted and non-encrypted
    entries that have the same physical address.  This can occur when
    kexec'ing from memory encryption active to inactive or vice-versa.  The
    important thing is to not have outside of kernel text memory references
    (such as stack usage), so the usage of the native_*() functions is needed
    since these expand as inline asm sequences.  So instead of reverting the
    change, rework the sequence.
    
    Move the wbinvd instruction outside of the for loop as native_wbinvd()
    and make its execution conditional on X86_FEATURE_SME.  In the for loop,
    change the asm 'wbinvd; hlt' sequence back to a halt sequence but use
    the native_halt() call.
    
    Fixes: bba4ed011a52 ("x86/mm, kexec: Allow kexec to be used with SME")
    Reported-by: Dave Young <dyoung@redhat.com>
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Dave Young <dyoung@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Yu Chen <yu.c.chen@intel.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: kexec@lists.infradead.org
    Cc: ebiederm@redhat.com
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180117234141.21184.44067.stgit@tlendack-t1.amdoffice.net

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 832a6acd730f..cb368c2a22ab 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -380,19 +380,24 @@ void stop_this_cpu(void *dummy)
 	disable_local_APIC();
 	mcheck_cpu_clear(this_cpu_ptr(&cpu_info));
 
+	/*
+	 * Use wbinvd on processors that support SME. This provides support
+	 * for performing a successful kexec when going from SME inactive
+	 * to SME active (or vice-versa). The cache must be cleared so that
+	 * if there are entries with the same physical address, both with and
+	 * without the encryption bit, they don't race each other when flushed
+	 * and potentially end up with the wrong entry being committed to
+	 * memory.
+	 */
+	if (boot_cpu_has(X86_FEATURE_SME))
+		native_wbinvd();
 	for (;;) {
 		/*
-		 * Use wbinvd followed by hlt to stop the processor. This
-		 * provides support for kexec on a processor that supports
-		 * SME. With kexec, going from SME inactive to SME active
-		 * requires clearing cache entries so that addresses without
-		 * the encryption bit set don't corrupt the same physical
-		 * address that has the encryption bit set when caches are
-		 * flushed. To achieve this a wbinvd is performed followed by
-		 * a hlt. Even if the processor is not in the kexec/SME
-		 * scenario this only adds a wbinvd to a halting processor.
+		 * Use native_halt() so that memory contents don't change
+		 * (stack usage and variables) after possibly issuing the
+		 * native_wbinvd() above.
 		 */
-		asm volatile("wbinvd; hlt" : : : "memory");
+		native_halt();
 	}
 }
 

commit 00a5ae218d57741088068799b810416ac249a9ce
Merge: d6bbd51587ec 2fd9c41aea47
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 3 16:41:07 2018 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 page table isolation fixes from Thomas Gleixner:
     "A couple of urgent fixes for PTI:
    
       - Fix a PTE mismatch between user and kernel visible mapping of the
         cpu entry area (differs vs. the GLB bit) and causes a TLB mismatch
         MCE on older AMD K8 machines
    
       - Fix the misplaced CR3 switch in the SYSCALL compat entry code which
         causes access to unmapped kernel memory resulting in double faults.
    
       - Fix the section mismatch of the cpu_tss_rw percpu storage caused by
         using a different mechanism for declaration and definition.
    
       - Two fixes for dumpstack which help to decode entry stack issues
         better
    
       - Enable PTI by default in Kconfig. We should have done that earlier,
         but it slipped through the cracks.
    
       - Exclude AMD from the PTI enforcement. Not necessarily a fix, but if
         AMD is so confident that they are not affected, then we should not
         burden users with the overhead"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/process: Define cpu_tss_rw in same section as declaration
      x86/pti: Switch to kernel CR3 at early in entry_SYSCALL_compat()
      x86/dumpstack: Print registers for first stack frame
      x86/dumpstack: Fix partial register dumps
      x86/pti: Make sure the user/kernel PTEs match
      x86/cpu, x86/pti: Do not enable PTI on AMD processors
      x86/pti: Enable PTI by default

commit 2fd9c41aea47f4ad071accf94b94f94f2c4d31eb
Author: Nick Desaulniers <ndesaulniers@google.com>
Date:   Wed Jan 3 12:39:52 2018 -0800

    x86/process: Define cpu_tss_rw in same section as declaration
    
    cpu_tss_rw is declared with DECLARE_PER_CPU_PAGE_ALIGNED
    but then defined with DEFINE_PER_CPU_SHARED_ALIGNED
    leading to section mismatch warnings.
    
    Use DEFINE_PER_CPU_PAGE_ALIGNED consistently. This is necessary because
    it's mapped to the cpu entry area and must be page aligned.
    
    [ tglx: Massaged changelog a bit ]
    
    Fixes: 1a935bc3d4ea ("x86/entry: Move SYSENTER_stack to the beginning of struct tss_struct")
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: thomas.lendacky@amd.com
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: tklauser@distanz.ch
    Cc: minipli@googlemail.com
    Cc: me@kylehuey.com
    Cc: namit@vmware.com
    Cc: luto@kernel.org
    Cc: jpoimboe@redhat.com
    Cc: tj@kernel.org
    Cc: cl@linux.com
    Cc: bp@suse.de
    Cc: thgarnie@google.com
    Cc: kirill.shutemov@linux.intel.com
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180103203954.183360-1-ndesaulniers@google.com

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 517415978409..3cb2486c47e4 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -47,7 +47,7 @@
  * section. Since TSS's are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
-__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss_rw) = {
+__visible DEFINE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw) = {
 	.x86_tss = {
 		/*
 		 * .sp0 is only used when entering ring 0 from a lower

commit 64a48099b3b31568ac45716b7fafcb74a0c2fcfe
Merge: 1291a0d5049d 6cbd2171e89b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 18 08:59:15 2017 -0800

    Merge branch 'WIP.x86-pti.entry-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 syscall entry code changes for PTI from Ingo Molnar:
     "The main changes here are Andy Lutomirski's changes to switch the
      x86-64 entry code to use the 'per CPU entry trampoline stack'. This,
      besides helping fix KASLR leaks (the pending Page Table Isolation
      (PTI) work), also robustifies the x86 entry code"
    
    * 'WIP.x86-pti.entry-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (26 commits)
      x86/cpufeatures: Make CPU bugs sticky
      x86/paravirt: Provide a way to check for hypervisors
      x86/paravirt: Dont patch flush_tlb_single
      x86/entry/64: Make cpu_entry_area.tss read-only
      x86/entry: Clean up the SYSENTER_stack code
      x86/entry/64: Remove the SYSENTER stack canary
      x86/entry/64: Move the IST stacks into struct cpu_entry_area
      x86/entry/64: Create a per-CPU SYSCALL entry trampoline
      x86/entry/64: Return to userspace from the trampoline stack
      x86/entry/64: Use a per-CPU trampoline stack for IDT entries
      x86/espfix/64: Stop assuming that pt_regs is on the entry stack
      x86/entry/64: Separate cpu_current_top_of_stack from TSS.sp0
      x86/entry: Remap the TSS into the CPU entry area
      x86/entry: Move SYSENTER_stack to the beginning of struct tss_struct
      x86/dumpstack: Handle stack overflow on all stacks
      x86/entry: Fix assumptions that the HW TSS is at the beginning of cpu_tss
      x86/kasan/64: Teach KASAN about the cpu_entry_area
      x86/mm/fixmap: Generalize the GDT fixmap mechanism, introduce struct cpu_entry_area
      x86/entry/gdt: Put per-CPU GDT remaps in ascending order
      x86/dumpstack: Add get_stack_info() support for the SYSENTER stack
      ...

commit c482feefe1aeb150156248ba0fd3e029bc886605
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:29 2017 +0100

    x86/entry/64: Make cpu_entry_area.tss read-only
    
    The TSS is a fairly juicy target for exploits, and, now that the TSS
    is in the cpu_entry_area, it's no longer protected by kASLR.  Make it
    read-only on x86_64.
    
    On x86_32, it can't be RO because it's written by the CPU during task
    switches, and we use a task gate for double faults.  I'd also be
    nervous about errata if we tried to make it RO even on configurations
    without double fault handling.
    
    [ tglx: AMD confirmed that there is no problem on 64-bit with TSS RO.  So
            it's probably safe to assume that it's a non issue, though Intel
            might have been creative in that area. Still waiting for
            confirmation. ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.733700132@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6a04287f222b..517415978409 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -47,7 +47,7 @@
  * section. Since TSS's are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
-__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
+__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss_rw) = {
 	.x86_tss = {
 		/*
 		 * .sp0 is only used when entering ring 0 from a lower
@@ -82,7 +82,7 @@ __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
 	.io_bitmap		= { [0 ... IO_BITMAP_LONGS] = ~0 },
 #endif
 };
-EXPORT_PER_CPU_SYMBOL(cpu_tss);
+EXPORT_PER_CPU_SYMBOL(cpu_tss_rw);
 
 DEFINE_PER_CPU(bool, __tss_limit_invalid);
 EXPORT_PER_CPU_SYMBOL_GPL(__tss_limit_invalid);
@@ -111,7 +111,7 @@ void exit_thread(struct task_struct *tsk)
 	struct fpu *fpu = &t->fpu;
 
 	if (bp) {
-		struct tss_struct *tss = &per_cpu(cpu_tss, get_cpu());
+		struct tss_struct *tss = &per_cpu(cpu_tss_rw, get_cpu());
 
 		t->io_bitmap_ptr = NULL;
 		clear_thread_flag(TIF_IO_BITMAP);

commit 7fbbd5cbebf118a9e09f5453f686656a167c3d1c
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:27 2017 +0100

    x86/entry/64: Remove the SYSENTER stack canary
    
    Now that the SYSENTER stack has a guard page, there's no need for a canary
    to detect overflow after the fact.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.572577316@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 86e83762e3b3..6a04287f222b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -81,7 +81,6 @@ __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
 	  */
 	.io_bitmap		= { [0 ... IO_BITMAP_LONGS] = ~0 },
 #endif
-	.SYSENTER_stack_canary	= STACK_END_MAGIC,
 };
 EXPORT_PER_CPU_SYMBOL(cpu_tss);
 

commit 9aaefe7b59ae00605256a7d6bd1c1456432495fc
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:21 2017 +0100

    x86/entry/64: Separate cpu_current_top_of_stack from TSS.sp0
    
    On 64-bit kernels, we used to assume that TSS.sp0 was the current
    top of stack.  With the addition of an entry trampoline, this will
    no longer be the case.  Store the current top of stack in TSS.sp1,
    which is otherwise unused but shares the same cacheline.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.050864668@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 35d674157fda..86e83762e3b3 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -56,6 +56,16 @@ __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
 		 * Poison it.
 		 */
 		.sp0 = (1UL << (BITS_PER_LONG-1)) + 1,
+
+#ifdef CONFIG_X86_64
+		/*
+		 * .sp1 is cpu_current_top_of_stack.  The init task never
+		 * runs user code, but cpu_current_top_of_stack should still
+		 * be well defined before the first context switch.
+		 */
+		.sp1 = TOP_OF_INIT_STACK,
+#endif
+
 #ifdef CONFIG_X86_32
 		.ss0 = __KERNEL_DS,
 		.ss1 = __KERNEL_CS,

commit 1a79797b58cddfa948420a7553241c79c013e3ca
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:12 2017 +0100

    x86/entry/64: Allocate and enable the SYSENTER stack
    
    This will simplify future changes that want scratch variables early in
    the SYSENTER handler -- they'll be able to spill registers to the
    stack.  It also lets us get rid of a SWAPGS_UNSAFE_STACK user.
    
    This does not depend on CONFIG_IA32_EMULATION=y because we'll want the
    stack space even without IA32 emulation.
    
    As far as I can tell, the reason that this wasn't done from day 1 is
    that we use IST for #DB and #BP, which is IMO rather nasty and causes
    a lot more problems than it solves.  But, since #DB uses IST, we don't
    actually need a real stack for SYSENTER (because SYSENTER with TF set
    will invoke #DB on the IST stack rather than the SYSENTER stack).
    
    I want to remove IST usage from these vectors some day, and this patch
    is a prerequisite for that as well.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.312726423@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 97fb3e5737f5..35d674157fda 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -71,9 +71,7 @@ __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
 	  */
 	.io_bitmap		= { [0 ... IO_BITMAP_LONGS] = ~0 },
 #endif
-#ifdef CONFIG_X86_32
 	.SYSENTER_stack_canary	= STACK_END_MAGIC,
-#endif
 };
 EXPORT_PER_CPU_SYMBOL(cpu_tss);
 

commit 0fd2e9c53d82704a3ba87ea1980ec515188c5316
Merge: 1784f9144b14 1e4c4f610f77
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Dec 1 10:32:48 2017 +0100

    Merge commit 'upstream-x86-entry' into WIP.x86/mm
    
    Pull in a minimal set of v4.15 entry code changes, for a base for the MM isolation patches.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 81bf665d00baf1aef01118c6c9e51520e57c0757
Author: Pravin Shedge <pravin.shedge4linux@gmail.com>
Date:   Tue Dec 12 02:12:31 2017 +0530

    x86/headers: Remove duplicate #includes
    
    These duplicate includes have been found with scripts/checkincludes.pl but
    they have been removed manually to avoid removing false positives.
    
    Signed-off-by: Pravin Shedge <pravin.shedge4linux@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: ard.biesheuvel@linaro.org
    Cc: boris.ostrovsky@oracle.com
    Cc: geert@linux-m68k.org
    Cc: jgross@suse.com
    Cc: linux-efi@vger.kernel.org
    Cc: luto@kernel.org
    Cc: matt@codeblueprint.co.uk
    Cc: thomas.lendacky@amd.com
    Cc: tim.c.chen@linux.intel.com
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1513024951-9221-1-git-send-email-pravin.shedge4linux@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index bb988a24db92..d6321855f9da 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -21,7 +21,6 @@
 #include <linux/dmi.h>
 #include <linux/utsname.h>
 #include <linux/stackprotector.h>
-#include <linux/tick.h>
 #include <linux/cpuidle.h>
 #include <trace/events/power.h>
 #include <linux/hw_breakpoint.h>

commit 9d0b62328d34c7044114d4f4281981d4c537c4ba
Author: Nadav Amit <namit@vmware.com>
Date:   Fri Nov 24 19:29:07 2017 -0800

    x86/tlb: Disable interrupts when changing CR4
    
    CR4 modifications are implemented as RMW operations which update a shadow
    variable and write the result to CR4. The RMW operation is protected by
    preemption disable, but there is no enforcement or debugging mechanism.
    
    CR4 modifications happen also in interrupt context via
    __native_flush_tlb_global(). This implementation does not affect a
    interrupted thread context CR4 operation, because the CR4 toggle restores
    the original content and does not modify the shadow variable.
    
    So the current situation seems to be safe, but a recent patch tried to add
    an actual RMW operation in interrupt context, which will cause subtle
    corruptions.
    
    To prevent that and make the CR4 handling future proof:
    
     - Add a lockdep assertion to __cr4_set() which will catch interrupt
       enabled invocations
    
     - Disable interrupts in the cr4 manipulator inlines
    
     - Rename cr4_toggle_bits() to cr4_toggle_bits_irqsoff(). This is called
       from __switch_to_xtra() where interrupts are already disabled and
       performance matters.
    
    All other call sites are not performance critical, so the extra overhead of
    an additional local_irq_save/restore() pair is not a problem. If new call
    sites care about performance then the necessary _irqsoff() variants can be
    added.
    
    [ tglx: Condensed the patch by moving the irq protection inside the
            manipulator functions. Updated changelog ]
    
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Luck <tony.luck@intel.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: nadav.amit@gmail.com
    Cc: linux-edac@vger.kernel.org
    Link: https://lkml.kernel.org/r/20171125032907.2241-3-namit@vmware.com

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 97fb3e5737f5..bb988a24db92 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -299,7 +299,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 	}
 
 	if ((tifp ^ tifn) & _TIF_NOTSC)
-		cr4_toggle_bits(X86_CR4_TSD);
+		cr4_toggle_bits_irqsoff(X86_CR4_TSD);
 
 	if ((tifp ^ tifn) & _TIF_NOCPUID)
 		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));

commit b3d9a136815ca9284ade2a897a3b7d2b0084c33c
Merge: c7da092a1f24 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:53:06 2017 +0100

    Merge branch 'linus' into x86/asm, to pick up fixes and resolve conflicts
    
    Conflicts:
            arch/x86/kernel/cpu/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index bd6b85fac666..c67685337c5a 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
 #include <linux/errno.h>

commit 20bb83443ea79087b5e5f8dab4e9d80bb9bf7acb
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:13 2017 -0700

    x86/entry/64: Stop initializing TSS.sp0 at boot
    
    In my quest to get rid of thread_struct::sp0, I want to clean up or
    remove all of its readers.  Two of them are in cpu_init() (32-bit and
    64-bit), and they aren't needed.  This is because we never enter
    userspace at all on the threads that CPUs are initialized in.
    
    Poison the initial TSS.sp0 and stop initializing it on CPU init.
    
    The comment text mostly comes from Dave Hansen.  Thanks!
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/ee4a00540ad28c6cff475fbcc7769a4460acc861.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index bd6b85fac666..ff8a9acbcf8b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -48,7 +48,13 @@
  */
 __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
 	.x86_tss = {
-		.sp0 = TOP_OF_INIT_STACK,
+		/*
+		 * .sp0 is only used when entering ring 0 from a lower
+		 * privilege level.  Since the init task never runs anything
+		 * but ring 0 code, there is no need for a valid value here.
+		 * Poison it.
+		 */
+		.sp0 = (1UL << (BITS_PER_LONG-1)) + 1,
 #ifdef CONFIG_X86_32
 		.ss0 = __KERNEL_DS,
 		.ss1 = __KERNEL_CS,

commit bba4ed011a52d494aa7ef5e08cf226709bbf3f60
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:28 2017 -0500

    x86/mm, kexec: Allow kexec to be used with SME
    
    Provide support so that kexec can be used to boot a kernel when SME is
    enabled.
    
    Support is needed to allocate pages for kexec without encryption.  This
    is needed in order to be able to reboot in the kernel in the same manner
    as originally booted.
    
    Additionally, when shutting down all of the CPUs we need to be sure to
    flush the caches and then halt. This is needed when booting from a state
    where SME was not active into a state where SME is active (or vice-versa).
    Without these steps, it is possible for cache lines to exist for the same
    physical location but tagged both with and without the encryption bit. This
    can cause random memory corruption when caches are flushed depending on
    which cacheline is written last.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: <kexec@lists.infradead.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/b95ff075db3e7cd545313f2fb609a49619a09625.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 3ca198080ea9..bd6b85fac666 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -355,6 +355,7 @@ bool xen_set_default_idle(void)
 	return ret;
 }
 #endif
+
 void stop_this_cpu(void *dummy)
 {
 	local_irq_disable();
@@ -365,8 +366,20 @@ void stop_this_cpu(void *dummy)
 	disable_local_APIC();
 	mcheck_cpu_clear(this_cpu_ptr(&cpu_info));
 
-	for (;;)
-		halt();
+	for (;;) {
+		/*
+		 * Use wbinvd followed by hlt to stop the processor. This
+		 * provides support for kexec on a processor that supports
+		 * SME. With kexec, going from SME inactive to SME active
+		 * requires clearing cache entries so that addresses without
+		 * the encryption bit set don't corrupt the same physical
+		 * address that has the encryption bit set when caches are
+		 * flushed. To achieve this a wbinvd is performed followed by
+		 * a hlt. Even if the processor is not in the kexec/SME
+		 * scenario this only adds a wbinvd to a halting processor.
+		 */
+		asm volatile("wbinvd; hlt" : : : "memory");
+	}
 }
 
 /*

commit 6474924e2b5ddb0030c355558966adcbe3b49022
Author: Tobias Klauser <tklauser@distanz.ch>
Date:   Wed Jun 28 15:30:02 2017 +0200

    arch: remove unused macro/function thread_saved_pc()
    
    The only user of thread_saved_pc() in non-arch-specific code was removed
    in commit 8243d5597793 ("sched/core: Remove pointless printout in
    sched_show_task()").  Remove the implementations as well.
    
    Some architectures use thread_saved_pc() in their arch-specific code.
    Leave their thread_saved_pc() intact.
    
    Signed-off-by: Tobias Klauser <tklauser@distanz.ch>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 0bb88428cbf2..3ca198080ea9 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -544,17 +544,6 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 	return randomize_page(mm->brk, 0x02000000);
 }
 
-/*
- * Return saved PC of a blocked thread.
- * What is this good for? it will be always the scheduler or ret_from_fork.
- */
-unsigned long thread_saved_pc(struct task_struct *tsk)
-{
-	struct inactive_task_frame *frame =
-		(struct inactive_task_frame *) READ_ONCE(tsk->thread.sp);
-	return READ_ONCE_NOCHECK(frame->ret_addr);
-}
-
 /*
  * Called from fs/proc with a reference on @p to find the function
  * which called into schedule(). This needs to be done carefully

commit e9ea1e7f53b852147cbd568b0568c7ad97ec21a3
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:26 2017 -0700

    x86/arch_prctl: Add ARCH_[GET|SET]_CPUID
    
    Intel supports faulting on the CPUID instruction beginning with Ivy Bridge.
    When enabled, the processor will fault on attempts to execute the CPUID
    instruction with CPL>0. Exposing this feature to userspace will allow a
    ptracer to trap and emulate the CPUID instruction.
    
    When supported, this feature is controlled by toggling bit 0 of
    MSR_MISC_FEATURES_ENABLES. It is documented in detail in Section 2.3.2 of
    https://bugzilla.kernel.org/attachment.cgi?id=243991
    
    Implement a new pair of arch_prctls, available on both x86-32 and x86-64.
    
    ARCH_GET_CPUID: Returns the current CPUID state, either 0 if CPUID faulting
        is enabled (and thus the CPUID instruction is not available) or 1 if
        CPUID faulting is not enabled.
    
    ARCH_SET_CPUID: Set the CPUID state to the second argument. If
        cpuid_enabled is 0 CPUID faulting will be activated, otherwise it will
        be deactivated. Returns ENODEV if CPUID faulting is not supported on
        this system.
    
    The state of the CPUID faulting flag is propagated across forks, but reset
    upon exec.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-9-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index b12e95eceb83..0bb88428cbf2 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -37,6 +37,7 @@
 #include <asm/vm86.h>
 #include <asm/switch_to.h>
 #include <asm/desc.h>
+#include <asm/prctl.h>
 
 /*
  * per-CPU TSS segments. Threads are completely 'soft' on Linux,
@@ -172,6 +173,73 @@ int set_tsc_mode(unsigned int val)
 	return 0;
 }
 
+DEFINE_PER_CPU(u64, msr_misc_features_shadow);
+
+static void set_cpuid_faulting(bool on)
+{
+	u64 msrval;
+
+	msrval = this_cpu_read(msr_misc_features_shadow);
+	msrval &= ~MSR_MISC_FEATURES_ENABLES_CPUID_FAULT;
+	msrval |= (on << MSR_MISC_FEATURES_ENABLES_CPUID_FAULT_BIT);
+	this_cpu_write(msr_misc_features_shadow, msrval);
+	wrmsrl(MSR_MISC_FEATURES_ENABLES, msrval);
+}
+
+static void disable_cpuid(void)
+{
+	preempt_disable();
+	if (!test_and_set_thread_flag(TIF_NOCPUID)) {
+		/*
+		 * Must flip the CPU state synchronously with
+		 * TIF_NOCPUID in the current running context.
+		 */
+		set_cpuid_faulting(true);
+	}
+	preempt_enable();
+}
+
+static void enable_cpuid(void)
+{
+	preempt_disable();
+	if (test_and_clear_thread_flag(TIF_NOCPUID)) {
+		/*
+		 * Must flip the CPU state synchronously with
+		 * TIF_NOCPUID in the current running context.
+		 */
+		set_cpuid_faulting(false);
+	}
+	preempt_enable();
+}
+
+static int get_cpuid_mode(void)
+{
+	return !test_thread_flag(TIF_NOCPUID);
+}
+
+static int set_cpuid_mode(struct task_struct *task, unsigned long cpuid_enabled)
+{
+	if (!static_cpu_has(X86_FEATURE_CPUID_FAULT))
+		return -ENODEV;
+
+	if (cpuid_enabled)
+		enable_cpuid();
+	else
+		disable_cpuid();
+
+	return 0;
+}
+
+/*
+ * Called immediately after a successful exec.
+ */
+void arch_setup_new_exec(void)
+{
+	/* If cpuid was previously disabled for this task, re-enable it. */
+	if (test_thread_flag(TIF_NOCPUID))
+		enable_cpuid();
+}
+
 static inline void switch_to_bitmap(struct tss_struct *tss,
 				    struct thread_struct *prev,
 				    struct thread_struct *next,
@@ -225,6 +293,9 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 
 	if ((tifp ^ tifn) & _TIF_NOTSC)
 		cr4_toggle_bits(X86_CR4_TSD);
+
+	if ((tifp ^ tifn) & _TIF_NOCPUID)
+		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));
 }
 
 /*
@@ -549,5 +620,12 @@ unsigned long get_wchan(struct task_struct *p)
 long do_arch_prctl_common(struct task_struct *task, int option,
 			  unsigned long cpuid_enabled)
 {
+	switch (option) {
+	case ARCH_GET_CPUID:
+		return get_cpuid_mode();
+	case ARCH_SET_CPUID:
+		return set_cpuid_mode(task, cpuid_enabled);
+	}
+
 	return -EINVAL;
 }

commit b0b9b014016d16ca7a192da986aa8ebae21bb995
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:23 2017 -0700

    x86/arch_prctl: Add do_arch_prctl_common()
    
    Add do_arch_prctl_common() to handle arch_prctls that are not specific to 64
    bit mode. Call it from the syscall entry point, but not any of the other
    callsites in the kernel, which all want one of the existing 64 bit only
    arch_prctls.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-6-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 366db7782fc6..b12e95eceb83 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -545,3 +545,9 @@ unsigned long get_wchan(struct task_struct *p)
 	put_task_stack(p);
 	return ret;
 }
+
+long do_arch_prctl_common(struct task_struct *task, int option,
+			  unsigned long cpuid_enabled)
+{
+	return -EINVAL;
+}

commit 5a920155e388ec22a22e0532fb695b9215c9b34d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 14 00:11:04 2017 -0800

    x86/process: Optimize TIF_NOTSC switch
    
    Provide and use a toggle helper instead of doing it with a branch.
    
    x86_64: arch/x86/kernel/process.o
    text       data     bss     dec     hex
    3008       8577      16   11601    2d51 Before
    2976       8577      16   11569    2d31 After
    
    i386: arch/x86/kernel/process.o
    text       data     bss     dec     hex
    2925       8673       8   11606    2d56 Before
    2893       8673       8   11574    2d36 After
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Link: http://lkml.kernel.org/r/20170214081104.9244-4-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 83fa3cb4f8f0..366db7782fc6 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -124,11 +124,6 @@ void flush_thread(void)
 	fpu__clear(&tsk->thread.fpu);
 }
 
-static void hard_disable_TSC(void)
-{
-	cr4_set_bits(X86_CR4_TSD);
-}
-
 void disable_TSC(void)
 {
 	preempt_disable();
@@ -137,15 +132,10 @@ void disable_TSC(void)
 		 * Must flip the CPU state synchronously with
 		 * TIF_NOTSC in the current running context.
 		 */
-		hard_disable_TSC();
+		cr4_set_bits(X86_CR4_TSD);
 	preempt_enable();
 }
 
-static void hard_enable_TSC(void)
-{
-	cr4_clear_bits(X86_CR4_TSD);
-}
-
 static void enable_TSC(void)
 {
 	preempt_disable();
@@ -154,7 +144,7 @@ static void enable_TSC(void)
 		 * Must flip the CPU state synchronously with
 		 * TIF_NOTSC in the current running context.
 		 */
-		hard_enable_TSC();
+		cr4_clear_bits(X86_CR4_TSD);
 	preempt_enable();
 }
 
@@ -233,12 +223,8 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 		wrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);
 	}
 
-	if ((tifp ^ tifn) & _TIF_NOTSC) {
-		if (tifn & _TIF_NOTSC)
-			hard_disable_TSC();
-		else
-			hard_enable_TSC();
-	}
+	if ((tifp ^ tifn) & _TIF_NOTSC)
+		cr4_toggle_bits(X86_CR4_TSD);
 }
 
 /*

commit b9894a2f5bd18b1691cb6872c9afe32b148d0132
Author: Kyle Huey <me@kylehuey.com>
Date:   Tue Feb 14 00:11:03 2017 -0800

    x86/process: Correct and optimize TIF_BLOCKSTEP switch
    
    The debug control MSR is "highly magical" as the blockstep bit can be
    cleared by hardware under not well documented circumstances.
    
    So a task switch relying on the bit set by the previous task (according to
    the previous tasks thread flags) can trip over this and not update the flag
    for the next task.
    
    To fix this its required to handle DEBUGCTLMSR_BTF when either the previous
    or the next or both tasks have the TIF_BLOCKSTEP flag set.
    
    While at it avoid branching within the TIF_BLOCKSTEP case and evaluating
    boot_cpu_data twice in kernels without CONFIG_X86_DEBUGCTLMSR.
    
    x86_64: arch/x86/kernel/process.o
    text    data    bss     dec      hex
    3024    8577    16      11617    2d61   Before
    3008    8577    16      11601    2d51   After
    
    i386: No change
    
    [ tglx: Made the shift value explicit, use a local variable to make the
    code readable and massaged changelog]
    
    Originally-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Link: http://lkml.kernel.org/r/20170214081104.9244-3-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ea9ea2582dab..83fa3cb4f8f0 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -222,13 +222,15 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 
 	propagate_user_return_notify(prev_p, next_p);
 
-	if ((tifp ^ tifn) & _TIF_BLOCKSTEP) {
-		unsigned long debugctl = get_debugctlmsr();
+	if ((tifp & _TIF_BLOCKSTEP || tifn & _TIF_BLOCKSTEP) &&
+	    arch_has_block_step()) {
+		unsigned long debugctl, msk;
 
+		rdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);
 		debugctl &= ~DEBUGCTLMSR_BTF;
-		if (tifn & _TIF_BLOCKSTEP)
-			debugctl |= DEBUGCTLMSR_BTF;
-		update_debugctlmsr(debugctl);
+		msk = tifn & _TIF_BLOCKSTEP;
+		debugctl |= (msk >> TIF_BLOCKSTEP) << DEBUGCTLMSR_BTF_SHIFT;
+		wrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);
 	}
 
 	if ((tifp ^ tifn) & _TIF_NOTSC) {

commit af8b3cd3934ec60f4c2a420d19a9d416554f140b
Author: Kyle Huey <me@kylehuey.com>
Date:   Tue Feb 14 00:11:02 2017 -0800

    x86/process: Optimize TIF checks in __switch_to_xtra()
    
    Help the compiler to avoid reevaluating the thread flags for each checked
    bit by reordering the bit checks and providing an explicit xor for
    evaluation.
    
    With default defconfigs for each arch,
    
    x86_64: arch/x86/kernel/process.o
    text       data     bss     dec     hex
    3056       8577      16   11649    2d81 Before
    3024       8577      16   11617    2d61 After
    
    i386: arch/x86/kernel/process.o
    text       data     bss     dec     hex
    2957       8673       8   11638    2d76 Before
    2925       8673       8   11606    2d56 After
    
    Originally-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Link: http://lkml.kernel.org/r/20170214081104.9244-2-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index f67591561711..ea9ea2582dab 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -182,54 +182,61 @@ int set_tsc_mode(unsigned int val)
 	return 0;
 }
 
+static inline void switch_to_bitmap(struct tss_struct *tss,
+				    struct thread_struct *prev,
+				    struct thread_struct *next,
+				    unsigned long tifp, unsigned long tifn)
+{
+	if (tifn & _TIF_IO_BITMAP) {
+		/*
+		 * Copy the relevant range of the IO bitmap.
+		 * Normally this is 128 bytes or less:
+		 */
+		memcpy(tss->io_bitmap, next->io_bitmap_ptr,
+		       max(prev->io_bitmap_max, next->io_bitmap_max));
+		/*
+		 * Make sure that the TSS limit is correct for the CPU
+		 * to notice the IO bitmap.
+		 */
+		refresh_tss_limit();
+	} else if (tifp & _TIF_IO_BITMAP) {
+		/*
+		 * Clear any possible leftover bits:
+		 */
+		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
+	}
+}
+
 void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 		      struct tss_struct *tss)
 {
 	struct thread_struct *prev, *next;
+	unsigned long tifp, tifn;
 
 	prev = &prev_p->thread;
 	next = &next_p->thread;
 
-	if (test_tsk_thread_flag(prev_p, TIF_BLOCKSTEP) ^
-	    test_tsk_thread_flag(next_p, TIF_BLOCKSTEP)) {
+	tifn = READ_ONCE(task_thread_info(next_p)->flags);
+	tifp = READ_ONCE(task_thread_info(prev_p)->flags);
+	switch_to_bitmap(tss, prev, next, tifp, tifn);
+
+	propagate_user_return_notify(prev_p, next_p);
+
+	if ((tifp ^ tifn) & _TIF_BLOCKSTEP) {
 		unsigned long debugctl = get_debugctlmsr();
 
 		debugctl &= ~DEBUGCTLMSR_BTF;
-		if (test_tsk_thread_flag(next_p, TIF_BLOCKSTEP))
+		if (tifn & _TIF_BLOCKSTEP)
 			debugctl |= DEBUGCTLMSR_BTF;
-
 		update_debugctlmsr(debugctl);
 	}
 
-	if (test_tsk_thread_flag(prev_p, TIF_NOTSC) ^
-	    test_tsk_thread_flag(next_p, TIF_NOTSC)) {
-		/* prev and next are different */
-		if (test_tsk_thread_flag(next_p, TIF_NOTSC))
+	if ((tifp ^ tifn) & _TIF_NOTSC) {
+		if (tifn & _TIF_NOTSC)
 			hard_disable_TSC();
 		else
 			hard_enable_TSC();
 	}
-
-	if (test_tsk_thread_flag(next_p, TIF_IO_BITMAP)) {
-		/*
-		 * Copy the relevant range of the IO bitmap.
-		 * Normally this is 128 bytes or less:
-		 */
-		memcpy(tss->io_bitmap, next->io_bitmap_ptr,
-		       max(prev->io_bitmap_max, next->io_bitmap_max));
-
-		/*
-		 * Make sure that the TSS limit is correct for the CPU
-		 * to notice the IO bitmap.
-		 */
-		refresh_tss_limit();
-	} else if (test_tsk_thread_flag(prev_p, TIF_IO_BITMAP)) {
-		/*
-		 * Clear any possible leftover bits:
-		 */
-		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
-	}
-	propagate_user_return_notify(prev_p, next_p);
 }
 
 /*

commit 2d62e0768d3c28536d4cfe4c40ba1e5e8e442a93
Merge: be834aafdf5f 16ce771b93ab
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 4 11:36:19 2017 -0800

    Merge tag 'kvm-4.11-2' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull more KVM updates from Radim Krčmář:
     "Second batch of KVM changes for the 4.11 merge window:
    
      PPC:
       - correct assumption about ASDR on POWER9
       - fix MMIO emulation on POWER9
    
      x86:
       - add a simple test for ioperm
       - cleanup TSS (going through KVM tree as the whole undertaking was
         caused by VMX's use of TSS)
       - fix nVMX interrupt delivery
       - fix some performance counters in the guest
    
      ... and two cleanup patches"
    
    * tag 'kvm-4.11-2' of git://git.kernel.org/pub/scm/virt/kvm/kvm:
      KVM: nVMX: Fix pending events injection
      x86/kvm/vmx: remove unused variable in segment_base()
      selftests/x86: Add a basic selftest for ioperm
      x86/asm: Tidy up TSS limit code
      kvm: convert kvm.users_count from atomic_t to refcount_t
      KVM: x86: never specify a sample period for virtualized in_tx_cp counters
      KVM: PPC: Book3S HV: Don't use ASDR for real-mode HPT faults on POWER9
      KVM: PPC: Book3S HV: Fix software walk of guest process page tables

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 441f00e7be8f..56b059486c3b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -10,6 +10,7 @@
 #include <linux/sched/idle.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/task.h>
+#include <linux/sched/task_stack.h>
 #include <linux/init.h>
 #include <linux/export.h>
 #include <linux/pm.h>

commit 299300258d1bc4e997b7db340a2e06636757fe2e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task.h>
    
    We are going to split <linux/sched/task.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 505be5589e52..441f00e7be8f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -9,6 +9,7 @@
 #include <linux/sched.h>
 #include <linux/sched/idle.h>
 #include <linux/sched/debug.h>
+#include <linux/sched/task.h>
 #include <linux/init.h>
 #include <linux/export.h>
 #include <linux/pm.h>

commit b17b01533b719e9949e437abf66436a875739b40
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/debug.h>
    
    We are going to split <linux/sched/debug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/debug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6395e0cd7dd6..505be5589e52 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -8,6 +8,7 @@
 #include <linux/slab.h>
 #include <linux/sched.h>
 #include <linux/sched/idle.h>
+#include <linux/sched/debug.h>
 #include <linux/init.h>
 #include <linux/export.h>
 #include <linux/pm.h>

commit 4c822698cba8bdd93724117eded12bf34eb80252
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/idle.h>
    
    We are going to split  <linux/sched/idle.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/idle.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7780efa635b9..6395e0cd7dd6 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -7,6 +7,7 @@
 #include <linux/prctl.h>
 #include <linux/slab.h>
 #include <linux/sched.h>
+#include <linux/sched/idle.h>
 #include <linux/init.h>
 #include <linux/export.h>
 #include <linux/pm.h>

commit b7ceaec112aa35aa287325754d8c52b8304892cd
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Feb 22 07:36:16 2017 -0800

    x86/asm: Tidy up TSS limit code
    
    In an earlier version of the patch ("x86/kvm/vmx: Defer TR reload
    after VM exit") that introduced TSS limit validity tracking, I
    confused which helper was which.  On reflection, the names I chose
    sucked.  Rename the helpers to make it more obvious what's going on
    and add some comments.
    
    While I'm at it, clear __tss_limit_invalid when force-reloading as
    well as when contitionally reloading, since any TR reload fixes the
    limit.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7780efa635b9..0b302591b51f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -65,8 +65,8 @@ __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
 };
 EXPORT_PER_CPU_SYMBOL(cpu_tss);
 
-DEFINE_PER_CPU(bool, need_tr_refresh);
-EXPORT_PER_CPU_SYMBOL_GPL(need_tr_refresh);
+DEFINE_PER_CPU(bool, __tss_limit_invalid);
+EXPORT_PER_CPU_SYMBOL_GPL(__tss_limit_invalid);
 
 /*
  * this gets called so that we can store lazy state into memory and copy the
@@ -218,7 +218,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 		 * Make sure that the TSS limit is correct for the CPU
 		 * to notice the IO bitmap.
 		 */
-		refresh_TR();
+		refresh_tss_limit();
 	} else if (test_tsk_thread_flag(prev_p, TIF_IO_BITMAP)) {
 		/*
 		 * Clear any possible leftover bits:

commit b7ffc44d5b2ea163899d09289ca7743d5c32e926
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Feb 20 08:56:14 2017 -0800

    x86/kvm/vmx: Defer TR reload after VM exit
    
    Intel's VMX is daft and resets the hidden TSS limit register to 0x67
    on VMX reload, and the 0x67 is not configurable.  KVM currently
    reloads TR using the LTR instruction on every exit, but this is quite
    slow because LTR is serializing.
    
    The 0x67 limit is entirely harmless unless ioperm() is in use, so
    defer the reload until a task using ioperm() is actually running.
    
    Here's some poorly done benchmarking using kvm-unit-tests:
    
    Before:
    
    cpuid 1313
    vmcall 1195
    mov_from_cr8 11
    mov_to_cr8 17
    inl_from_pmtimer 6770
    inl_from_qemu 6856
    inl_from_kernel 2435
    outl_to_kernel 1402
    
    After:
    
    cpuid 1291
    vmcall 1181
    mov_from_cr8 11
    mov_to_cr8 16
    inl_from_pmtimer 6457
    inl_from_qemu 6209
    inl_from_kernel 2339
    outl_to_kernel 1391
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    [Force-reload TR in invalidate_tss_limit. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index b615a1113f58..7780efa635b9 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -32,6 +32,7 @@
 #include <asm/mce.h>
 #include <asm/vm86.h>
 #include <asm/switch_to.h>
+#include <asm/desc.h>
 
 /*
  * per-CPU TSS segments. Threads are completely 'soft' on Linux,
@@ -64,6 +65,9 @@ __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
 };
 EXPORT_PER_CPU_SYMBOL(cpu_tss);
 
+DEFINE_PER_CPU(bool, need_tr_refresh);
+EXPORT_PER_CPU_SYMBOL_GPL(need_tr_refresh);
+
 /*
  * this gets called so that we can store lazy state into memory and copy the
  * current task into the new thread.
@@ -209,6 +213,12 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 		 */
 		memcpy(tss->io_bitmap, next->io_bitmap_ptr,
 		       max(prev->io_bitmap_max, next->io_bitmap_max));
+
+		/*
+		 * Make sure that the TSS limit is correct for the CPU
+		 * to notice the IO bitmap.
+		 */
+		refresh_TR();
 	} else if (test_tsk_thread_flag(prev_p, TIF_IO_BITMAP)) {
 		/*
 		 * Clear any possible leftover bits:

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 37363e46b1f0..b615a1113f58 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -23,7 +23,7 @@
 #include <asm/cpu.h>
 #include <asm/apic.h>
 #include <asm/syscalls.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/mwait.h>
 #include <asm/fpu/internal.h>
 #include <asm/debugreg.h>

commit f7dd3b1734ea335fea01f103d48b3de26ea0d335
Merge: 1bbb05f52055 8c9b9d87b855
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 18 13:59:10 2016 -0800

    Merge branch 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "This is the last functional update from the tip tree for 4.10. It got
      delayed due to a newly reported and anlyzed variant of BIOS bug and
      the resulting wreckage:
    
       - Seperation of TSC being marked realiable and the fact that the
         platform provides the TSC frequency via CPUID/MSRs and making use
         for it for GOLDMONT.
    
       - TSC adjust MSR validation and sanitizing:
    
         The TSC adjust MSR contains the offset to the hardware counter. The
         sum of the adjust MSR and the counter is the TSC value which is
         read via RDTSC.
    
         On at least two machines from different vendors the BIOS sets the
         TSC adjust MSR to negative values. This happens on cold and warm
         boot. While on cold boot the offset is a few milliseconds, on warm
         boot it basically compensates the power on time of the system. The
         BIOSes are not even using the adjust MSR to set all CPUs in the
         package to the same offset. The offsets are different which renders
         the TSC unusable,
    
         What's worse is that the TSC deadline timer has a HW feature^Wbug.
         It malfunctions when the TSC adjust value is negative or greater
         equal 0x80000000 resulting in silent boot failures, hard lockups or
         non firing timers. This looks like some hardware internal 32/64bit
         issue with a sign extension problem. Intel has been silent so far
         on the issue.
    
         The update contains sanity checks and keeps the adjust register
         within working limits and in sync on the package.
    
         As it looks like this disease is spreading via BIOS crapware, we
         need to address this urgently as the boot failures are hard to
         debug for users"
    
    * 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/tsc: Limit the adjust value further
      x86/tsc: Annotate printouts as firmware bug
      x86/tsc: Force TSC_ADJUST register to value >= zero
      x86/tsc: Validate TSC_ADJUST after resume
      x86/tsc: Validate cpumask pointer before accessing it
      x86/tsc: Fix broken CONFIG_X86_TSC=n build
      x86/tsc: Try to adjust TSC if sync test fails
      x86/tsc: Prepare warp test for TSC adjustment
      x86/tsc: Move sync cleanup to a safe place
      x86/tsc: Sync test only for the first cpu in a package
      x86/tsc: Verify TSC_ADJUST from idle
      x86/tsc: Store and check TSC ADJUST MSR
      x86/tsc: Detect random warps
      x86/tsc: Use X86_FEATURE_TSC_ADJUST in detect_art()
      x86/tsc: Finalize the split of the TSC_RELIABLE flag
      x86/tsc: Set TSC_KNOWN_FREQ and TSC_RELIABLE flags on Intel Atom SoCs
      x86/tsc: Mark Intel ATOM_GOLDMONT TSC reliable
      x86/tsc: Mark TSC frequency determined by CPUID as known
      x86/tsc: Add X86_FEATURE_TSC_KNOWN_FREQ flag

commit 6a369583178d0b89c2c3919c4456ee22fee0f249
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 13 13:14:17 2016 +0000

    x86/tsc: Validate TSC_ADJUST after resume
    
    Some 'feature' BIOSes fiddle with the TSC_ADJUST register during
    suspend/resume which renders the TSC unusable.
    
    Add sanity checks into the resume path and restore the
    original value if it was adjusted.
    
    Reported-and-tested-by: Roland Scheidegger <rscheidegger_lists@hispeed.ch>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bruce Schlobohm <bruce.schlobohm@intel.com>
    Cc: Kevin Stanton <kevin.b.stanton@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Allen Hung <allen_hung@dell.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161213131211.317654500@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 4fe5dc84da4f..a67e0f0cdaab 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -277,7 +277,7 @@ void exit_idle(void)
 
 void arch_cpu_idle_enter(void)
 {
-	tsc_verify_tsc_adjust();
+	tsc_verify_tsc_adjust(false);
 	local_touch_nmi();
 	enter_idle();
 }

commit 34bc3560c657d3d4fb17367ed9bfda803166dce0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Dec 9 19:29:12 2016 +0100

    x86: Remove empty idle.h header
    
    One include less is always a good thing(tm). Good riddance.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/20161209182912.2726-6-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 2c2b55ab41e7..43c36d8a6ae2 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -23,7 +23,6 @@
 #include <asm/cpu.h>
 #include <asm/apic.h>
 #include <asm/syscalls.h>
-#include <asm/idle.h>
 #include <asm/uaccess.h>
 #include <asm/mwait.h>
 #include <asm/fpu/internal.h>

commit 07c94a38125376d70d156bd8bff98ddfe4c8ea95
Author: Borislav Petkov <bp@alien8.de>
Date:   Fri Dec 9 19:29:11 2016 +0100

    x86/amd: Simplify AMD E400 aware idle routine
    
    Reorganize the E400 detection now that we have everything in place:
    switch the CPUs to broadcast mode after the LAPIC has been initialized
    and remove the facilities that were used previously on the idle path.
    
    Unfortunately static_cpu_has_bug() cannpt be used in the E400 idle routine
    because alternatives have been applied when the actual detection happens,
    so the static switching does not take effect and the test will stay
    false. Use boot_cpu_has_bug() instead which is definitely an improvement
    over the RDMSR and the cpumask handling.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/20161209182912.2726-5-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ced76f13d20d..2c2b55ab41e7 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -289,59 +289,33 @@ void stop_this_cpu(void *dummy)
 		halt();
 }
 
-bool amd_e400_c1e_detected;
-EXPORT_SYMBOL(amd_e400_c1e_detected);
-
-static cpumask_var_t amd_e400_c1e_mask;
-
-void amd_e400_remove_cpu(int cpu)
-{
-	if (amd_e400_c1e_mask != NULL)
-		cpumask_clear_cpu(cpu, amd_e400_c1e_mask);
-}
-
 /*
- * AMD Erratum 400 aware idle routine. We check for C1E active in the interrupt
- * pending message MSR. If we detect C1E, then we handle it the same
- * way as C3 power states (local apic timer and TSC stop)
+ * AMD Erratum 400 aware idle routine. We handle it the same way as C3 power
+ * states (local apic timer and TSC stop).
  */
 static void amd_e400_idle(void)
 {
-	if (!amd_e400_c1e_detected) {
-		u32 lo, hi;
-
-		rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
-
-		if (lo & K8_INTP_C1E_ACTIVE_MASK) {
-			amd_e400_c1e_detected = true;
-			if (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))
-				mark_tsc_unstable("TSC halt in AMD C1E");
-			pr_info("System has AMD C1E enabled\n");
-		}
+	/*
+	 * We cannot use static_cpu_has_bug() here because X86_BUG_AMD_APIC_C1E
+	 * gets set after static_cpu_has() places have been converted via
+	 * alternatives.
+	 */
+	if (!boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E)) {
+		default_idle();
+		return;
 	}
 
-	if (amd_e400_c1e_detected) {
-		int cpu = smp_processor_id();
+	tick_broadcast_enter();
 
-		if (!cpumask_test_cpu(cpu, amd_e400_c1e_mask)) {
-			cpumask_set_cpu(cpu, amd_e400_c1e_mask);
-			/* Force broadcast so ACPI can not interfere. */
-			tick_broadcast_force();
-			pr_info("Switch to broadcast mode on CPU%d\n", cpu);
-		}
-		tick_broadcast_enter();
+	default_idle();
 
-		default_idle();
-
-		/*
-		 * The switch back from broadcast mode needs to be
-		 * called with interrupts disabled.
-		 */
-		local_irq_disable();
-		tick_broadcast_exit();
-		local_irq_enable();
-	} else
-		default_idle();
+	/*
+	 * The switch back from broadcast mode needs to be called with
+	 * interrupts disabled.
+	 */
+	local_irq_disable();
+	tick_broadcast_exit();
+	local_irq_enable();
 }
 
 /*
@@ -411,11 +385,14 @@ void select_idle_routine(const struct cpuinfo_x86 *c)
 		x86_idle = default_idle;
 }
 
-void __init init_amd_e400_c1e_mask(void)
+void amd_e400_c1e_apic_setup(void)
 {
-	/* If we're using amd_e400_idle, we need to allocate amd_e400_c1e_mask. */
-	if (x86_idle == amd_e400_idle)
-		zalloc_cpumask_var(&amd_e400_c1e_mask, GFP_KERNEL);
+	if (boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E)) {
+		pr_info("Switch to broadcast mode on CPU%d\n", smp_processor_id());
+		local_irq_disable();
+		tick_broadcast_force();
+		local_irq_enable();
+	}
 }
 
 void __init arch_post_acpi_subsys_init(void)

commit e7ff3a47630d9512d0bcbdfa73660021087ba445
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Dec 9 19:29:10 2016 +0100

    x86/amd: Check for the C1E bug post ACPI subsystem init
    
    AMD CPUs affected by the E400 erratum suffer from the issue that the
    local APIC timer stops when the CPU goes into C1E. Unfortunately there
    is no way to detect the affected CPUs on early boot. It's only possible
    to determine the range of possibly affected CPUs from the family/model
    range.
    
    The actual decision whether to enter C1E and thus cause the bug is done
    by the firmware and we need to detect that case late, after ACPI has
    been initialized.
    
    The current solution is to check in the idle routine whether the CPU is
    affected by reading the MSR_K8_INT_PENDING_MSG MSR and checking for the
    K8_INTP_C1E_ACTIVE_MASK bits. If one of the bits is set then the CPU is
    affected and the system is switched into forced broadcast mode.
    
    This is ineffective and on non-affected CPUs every entry to idle does
    the extra RDMSR.
    
    After doing some research it turns out that the bits are visible on the
    boot CPU right after the ACPI subsystem is initialized in the early
    boot process. So instead of polling for the bits in the idle loop, add
    a detection function after acpi_subsystem_init() and check for the MSR
    bits. If set, then the X86_BUG_AMD_APIC_C1E is set on the boot CPU and
    the TSC is marked unstable when X86_FEATURE_NONSTOP_TSC is not set as it
    will stop in C1E state as well.
    
    The switch to broadcast mode cannot be done at this point because the
    boot CPU still uses HPET as a clockevent device and the local APIC timer
    is not yet calibrated and installed. The switch to broadcast mode on the
    affected CPUs needs to be done when the local APIC timer is actually set
    up.
    
    This allows to cleanup the amd_e400_idle() function in the next step.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/20161209182912.2726-4-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 29bbbce4f2fd..ced76f13d20d 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -418,6 +418,29 @@ void __init init_amd_e400_c1e_mask(void)
 		zalloc_cpumask_var(&amd_e400_c1e_mask, GFP_KERNEL);
 }
 
+void __init arch_post_acpi_subsys_init(void)
+{
+	u32 lo, hi;
+
+	if (!boot_cpu_has_bug(X86_BUG_AMD_E400))
+		return;
+
+	/*
+	 * AMD E400 detection needs to happen after ACPI has been enabled. If
+	 * the machine is affected K8_INTP_C1E_ACTIVE_MASK bits are set in
+	 * MSR_K8_INT_PENDING_MSG.
+	 */
+	rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
+	if (!(lo & K8_INTP_C1E_ACTIVE_MASK))
+		return;
+
+	boot_cpu_set_bug(X86_BUG_AMD_APIC_C1E);
+
+	if (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))
+		mark_tsc_unstable("TSC halt in AMD C1E");
+	pr_info("System has AMD C1E enabled\n");
+}
+
 static int __init idle_setup(char *str)
 {
 	if (!str)

commit 3344ed30791af66dbbad5f375008f3d1863b6c99
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Dec 9 19:29:09 2016 +0100

    x86/bugs: Separate AMD E400 erratum and C1E bug
    
    The workaround for the AMD Erratum E400 (Local APIC timer stops in C1E
    state) is a two step process:
    
     - Selection of the E400 aware idle routine
    
     - Detection whether the platform is affected
    
    The idle routine selection happens for possibly affected CPUs depending on
    family/model/stepping information. These range of CPUs is not necessarily
    affected as the decision whether to enable the C1E feature is made by the
    firmware. Unfortunately there is no way to query this at early boot.
    
    The current implementation polls a MSR in the E400 aware idle routine to
    detect whether the CPU is affected. This is inefficient on non affected
    CPUs because every idle entry has to do the MSR read.
    
    There is a better way to detect this before going idle for the first time
    which requires to seperate the bug flags:
    
      X86_BUG_AMD_E400      - Selects the E400 aware idle routine and
                              enables the detection
    
      X86_BUG_AMD_APIC_C1E  - Set when the platform is affected by E400
    
    Replace the current X86_BUG_AMD_APIC_C1E usage by the new X86_BUG_AMD_E400
    bug bit to select the idle routine which currently does an unconditional
    detection poll. X86_BUG_AMD_APIC_C1E is going to be used in later patches
    to remove the MSR polling and simplify the handling of this misfeature.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/20161209182912.2726-3-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ee023919e476..29bbbce4f2fd 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -401,8 +401,7 @@ void select_idle_routine(const struct cpuinfo_x86 *c)
 	if (x86_idle || boot_option_idle_override == IDLE_POLL)
 		return;
 
-	if (cpu_has_bug(c, X86_BUG_AMD_APIC_C1E)) {
-		/* E400: APIC timer interrupt does not wake up CPU from C1e */
+	if (boot_cpu_has_bug(X86_BUG_AMD_E400)) {
 		pr_info("using AMD E400 aware idle routine\n");
 		x86_idle = amd_e400_idle;
 	} else if (prefer_mwait_c1_over_halt(c)) {

commit 1d0095feea591bbd94f35d8a98aed746319783e1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Nov 19 13:47:37 2016 +0000

    x86/tsc: Verify TSC_ADJUST from idle
    
    When entering idle, it's a good oportunity to verify that the TSC_ADJUST
    MSR has not been tampered with (BIOS hiding SMM cycles). If tampering is
    detected, emit a warning and restore it to the previous value.
    
    This is especially important for machines, which mark the TSC reliable
    because there is no watchdog clocksource available (SoCs).
    
    This is not sufficient for HPC (NOHZ_FULL) situations where a CPU never
    goes idle, but adding a timer to do the check periodically is not an option
    either. On a machine, which has this issue, the check triggeres right
    during boot, so there is a decent chance that the sysadmin will notice.
    
    Rate limit the check to once per second and warn only once per cpu.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161119134017.732180441@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 0888a879120f..4fe5dc84da4f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -277,6 +277,7 @@ void exit_idle(void)
 
 void arch_cpu_idle_enter(void)
 {
+	tsc_verify_tsc_adjust();
 	local_touch_nmi();
 	enter_idle();
 }

commit 7a3e686e1bb57c34f73d3f19d620fe29035a6c99
Author: Len Brown <len.brown@intel.com>
Date:   Fri Nov 18 01:23:21 2016 -0500

    x86/idle: Remove enter_idle(), exit_idle()
    
    Upon removal of the is_idle flag, these routines became NOPs.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/822f2c22cc5890f7b8ea0eeec60277eb44505b4e.1479449716.git.len.brown@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index d8e9d794e114..ee023919e476 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -234,34 +234,9 @@ static inline void play_dead(void)
 }
 #endif
 
-#ifdef CONFIG_X86_64
-void enter_idle(void)
-{
-}
-
-static void __exit_idle(void)
-{
-}
-
-/* Called from interrupts to signify idle end */
-void exit_idle(void)
-{
-	/* idle loop has pid 0 */
-	if (current->pid)
-		return;
-	__exit_idle();
-}
-#endif
-
 void arch_cpu_idle_enter(void)
 {
 	local_touch_nmi();
-	enter_idle();
-}
-
-void arch_cpu_idle_exit(void)
-{
-	__exit_idle();
 }
 
 void arch_cpu_idle_dead(void)

commit f08b5fe2d4eeb0a8a6e0e7e71928cf2c7b1b791d
Author: Len Brown <len.brown@intel.com>
Date:   Fri Nov 18 01:23:19 2016 -0500

    x86/idle: Remove is_idle flag
    
    Upon removal of the idle_notifier, all accesses to the "is_idle" flag serve
    no purpose.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/e4a24197cf9c227fcd1ca2df09999eaec9052f49.1479449716.git.len.brown@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index f51950715145..d8e9d794e114 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -65,10 +65,6 @@ __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
 };
 EXPORT_PER_CPU_SYMBOL(cpu_tss);
 
-#ifdef CONFIG_X86_64
-static DEFINE_PER_CPU(unsigned char, is_idle);
-#endif
-
 /*
  * this gets called so that we can store lazy state into memory and copy the
  * current task into the new thread.
@@ -241,13 +237,10 @@ static inline void play_dead(void)
 #ifdef CONFIG_X86_64
 void enter_idle(void)
 {
-	this_cpu_write(is_idle, 1);
 }
 
 static void __exit_idle(void)
 {
-	if (x86_test_and_clear_bit_percpu(0, is_idle) == 0)
-		return;
 }
 
 /* Called from interrupts to signify idle end */

commit 8e7a7ee9ddd5d4275f1194ef2cc243ea86b0fb6f
Author: Len Brown <len.brown@intel.com>
Date:   Fri Nov 18 01:23:18 2016 -0500

    x86/idle: Remove idle_notifier
    
    Upon removal of the i7300_idle driver, the idle_notifer is unused.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/f15385a82ec4bf51f4f06777193d83f03b28cfdd.1479449716.git.len.brown@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 0888a879120f..f51950715145 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -67,19 +67,6 @@ EXPORT_PER_CPU_SYMBOL(cpu_tss);
 
 #ifdef CONFIG_X86_64
 static DEFINE_PER_CPU(unsigned char, is_idle);
-static ATOMIC_NOTIFIER_HEAD(idle_notifier);
-
-void idle_notifier_register(struct notifier_block *n)
-{
-	atomic_notifier_chain_register(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_register);
-
-void idle_notifier_unregister(struct notifier_block *n)
-{
-	atomic_notifier_chain_unregister(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_unregister);
 #endif
 
 /*
@@ -255,14 +242,12 @@ static inline void play_dead(void)
 void enter_idle(void)
 {
 	this_cpu_write(is_idle, 1);
-	atomic_notifier_call_chain(&idle_notifier, IDLE_START, NULL);
 }
 
 static void __exit_idle(void)
 {
 	if (x86_test_and_clear_bit_percpu(0, is_idle) == 0)
 		return;
-	atomic_notifier_call_chain(&idle_notifier, IDLE_END, NULL);
 }
 
 /* Called from interrupts to signify idle end */

commit 9c6f0902a55226b08b9703408f04b258b9afa0bc
Author: Jason Cooper <jason@lakedaemon.net>
Date:   Tue Oct 11 13:53:56 2016 -0700

    x86: use simpler API for random address requests
    
    Currently, all callers to randomize_range() set the length to 0 and
    calculate end by adding a constant to the start address.  We can simplify
    the API to remove a bunch of needless checks and variables.
    
    Use the new randomize_addr(start, range) call to set the requested
    address.
    
    Link: http://lkml.kernel.org/r/20160803233913.32511-3-jason@lakedaemon.net
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 28cea7802ecb..0888a879120f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -509,8 +509,7 @@ unsigned long arch_align_stack(unsigned long sp)
 
 unsigned long arch_randomize_brk(struct mm_struct *mm)
 {
-	unsigned long range_end = mm->brk + 0x02000000;
-	return randomize_range(mm->brk, range_end, 0) ? : mm->brk;
+	return randomize_page(mm->brk, 0x02000000);
 }
 
 /*

commit 6727ad9e206cc08b80d8000a4d67f8417e53539d
Author: Chris Metcalf <cmetcalf@mellanox.com>
Date:   Fri Oct 7 17:02:55 2016 -0700

    nmi_backtrace: generate one-line reports for idle cpus
    
    When doing an nmi backtrace of many cores, most of which are idle, the
    output is a little overwhelming and very uninformative.  Suppress
    messages for cpus that are idling when they are interrupted and just
    emit one line, "NMI backtrace for N skipped: idling at pc 0xNNN".
    
    We do this by grouping all the cpuidle code together into a new
    .cpuidle.text section, and then checking the address of the interrupted
    PC to see if it lies within that section.
    
    This commit suitably tags x86 and tile idle routines, and only adds in
    the minimal framework for other architectures.
    
    Link: http://lkml.kernel.org/r/1472487169-14923-5-git-send-email-cmetcalf@mellanox.com
    Signed-off-by: Chris Metcalf <cmetcalf@mellanox.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Daniel Thompson <daniel.thompson@linaro.org> [arm]
    Tested-by: Petr Mladek <pmladek@suse.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 4002b475171c..28cea7802ecb 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -302,7 +302,7 @@ void arch_cpu_idle(void)
 /*
  * We use this if we don't have any better idle routine..
  */
-void default_idle(void)
+void __cpuidle default_idle(void)
 {
 	trace_cpu_idle_rcuidle(1, smp_processor_id());
 	safe_halt();
@@ -417,7 +417,7 @@ static int prefer_mwait_c1_over_halt(const struct cpuinfo_x86 *c)
  * with interrupts enabled and no flags, which is backwards compatible with the
  * original MWAIT implementation.
  */
-static void mwait_idle(void)
+static __cpuidle void mwait_idle(void)
 {
 	if (!current_set_polling_and_test()) {
 		trace_cpu_idle_rcuidle(1, smp_processor_id());

commit 74327a3e884a0ff895ba7b51d3488e6a177407b2
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Sep 15 22:45:46 2016 -0700

    x86/process: Pin the target stack in get_wchan()
    
    This will prevent a crash if get_wchan() runs after the task stack
    is freed.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/337aeca8614024aa4d8d9c81053bbf8fcffbe4ad.1474003868.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 0b9ed8ec5226..4002b475171c 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -532,15 +532,18 @@ unsigned long thread_saved_pc(struct task_struct *tsk)
  */
 unsigned long get_wchan(struct task_struct *p)
 {
-	unsigned long start, bottom, top, sp, fp, ip;
+	unsigned long start, bottom, top, sp, fp, ip, ret = 0;
 	int count = 0;
 
 	if (!p || p == current || p->state == TASK_RUNNING)
 		return 0;
 
+	if (!try_get_task_stack(p))
+		return 0;
+
 	start = (unsigned long)task_stack_page(p);
 	if (!start)
-		return 0;
+		goto out;
 
 	/*
 	 * Layout of the stack page:
@@ -564,16 +567,21 @@ unsigned long get_wchan(struct task_struct *p)
 
 	sp = READ_ONCE(p->thread.sp);
 	if (sp < bottom || sp > top)
-		return 0;
+		goto out;
 
 	fp = READ_ONCE_NOCHECK(((struct inactive_task_frame *)sp)->bp);
 	do {
 		if (fp < bottom || fp > top)
-			return 0;
+			goto out;
 		ip = READ_ONCE_NOCHECK(*(unsigned long *)(fp + sizeof(unsigned long)));
-		if (!in_sched_functions(ip))
-			return ip;
+		if (!in_sched_functions(ip)) {
+			ret = ip;
+			goto out;
+		}
 		fp = READ_ONCE_NOCHECK(*(unsigned long *)fp);
 	} while (count++ < 16 && p->state != TASK_RUNNING);
-	return 0;
+
+out:
+	put_task_stack(p);
+	return ret;
 }

commit 15f4eae70d365bba26854c90b6002aaabb18c8aa
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Sep 13 14:29:25 2016 -0700

    x86: Move thread_info into task_struct
    
    Now that most of the thread_info users have been cleaned up,
    this is straightforward.
    
    Most of this code was written by Linus.
    
    Originally-from: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/a50eab40abeaec9cb9a9e3cbdeafd32190206654.1473801993.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c1fa790c81cd..0b9ed8ec5226 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -549,9 +549,7 @@ unsigned long get_wchan(struct task_struct *p)
 	 * PADDING
 	 * ----------- top = topmax - TOP_OF_KERNEL_STACK_PADDING
 	 * stack
-	 * ----------- bottom = start + sizeof(thread_info)
-	 * thread_info
-	 * ----------- start
+	 * ----------- bottom = start
 	 *
 	 * The tasks stack pointer points at the location where the
 	 * framepointer is stored. The data on the stack is:
@@ -562,7 +560,7 @@ unsigned long get_wchan(struct task_struct *p)
 	 */
 	top = start + THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;
 	top -= 2 * sizeof(unsigned long);
-	bottom = start + sizeof(struct thread_info);
+	bottom = start;
 
 	sp = READ_ONCE(p->thread.sp);
 	if (sp < bottom || sp > top)

commit ffcb043ba524d3fbd979a9dac2c9ce8ad352000d
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sat Aug 13 12:38:21 2016 -0400

    sched/x86: Fix thread_saved_pc()
    
    thread_saved_pc() was using a completely bogus method to get the return
    address.  Since switch_to() was previously inlined, there was no sane way
    to know where on the stack the return address was stored.  Now with the
    frame of a sleeping thread well defined, this can be implemented correctly.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1471106302-10159-7-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 0115a4a4db96..c1fa790c81cd 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -513,6 +513,17 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 	return randomize_range(mm->brk, range_end, 0) ? : mm->brk;
 }
 
+/*
+ * Return saved PC of a blocked thread.
+ * What is this good for? it will be always the scheduler or ret_from_fork.
+ */
+unsigned long thread_saved_pc(struct task_struct *tsk)
+{
+	struct inactive_task_frame *frame =
+		(struct inactive_task_frame *) READ_ONCE(tsk->thread.sp);
+	return READ_ONCE_NOCHECK(frame->ret_addr);
+}
+
 /*
  * Called from fs/proc with a reference on @p to find the function
  * which called into schedule(). This needs to be done carefully

commit 7b32aeadbc95d4a41402c1c0da6aa3ab51af4c10
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sat Aug 13 12:38:18 2016 -0400

    sched/x86: Add 'struct inactive_task_frame' to better document the sleeping task stack frame
    
    Add 'struct inactive_task_frame', which defines the layout of the stack for
    a sleeping process.  For now, the only defined field is the BP register
    (frame pointer).
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1471106302-10159-4-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 62c0b0ea2ce4..0115a4a4db96 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -32,6 +32,7 @@
 #include <asm/tlbflush.h>
 #include <asm/mce.h>
 #include <asm/vm86.h>
+#include <asm/switch_to.h>
 
 /*
  * per-CPU TSS segments. Threads are completely 'soft' on Linux,
@@ -556,7 +557,7 @@ unsigned long get_wchan(struct task_struct *p)
 	if (sp < bottom || sp > top)
 		return 0;
 
-	fp = READ_ONCE_NOCHECK(*(unsigned long *)sp);
+	fp = READ_ONCE_NOCHECK(((struct inactive_task_frame *)sp)->bp);
 	do {
 		if (fp < bottom || fp > top)
 			return 0;

commit aeb35d6b74174ed08daab84e232b456bbd89d1d9
Merge: 7a66ecfd319a a47177d360a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 1 14:23:42 2016 -0400

    Merge branch 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 header cleanups from Ingo Molnar:
     "This tree is a cleanup of the x86 tree reducing spurious uses of
      module.h - which should improve build performance a bit"
    
    * 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, crypto: Restore MODULE_LICENSE() to glue_helper.c so it loads
      x86/apic: Remove duplicated include from probe_64.c
      x86/ce4100: Remove duplicated include from ce4100.c
      x86/headers: Include spinlock_types.h in x8664_ksyms_64.c for missing spinlock_t
      x86/platform: Delete extraneous MODULE_* tags fromm ts5500
      x86: Audit and remove any remaining unnecessary uses of module.h
      x86/kvm: Audit and remove any unnecessary uses of module.h
      x86/xen: Audit and remove any unnecessary uses of module.h
      x86/platform: Audit and remove any unnecessary uses of module.h
      x86/lib: Audit and remove any unnecessary uses of module.h
      x86/kernel: Audit and remove any unnecessary uses of module.h
      x86/mm: Audit and remove any unnecessary uses of module.h
      x86: Don't use module.h just for AUTHOR / LICENSE tags

commit 08e237fa56a1d95c1372033bc29c4a2517b3c0fa
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jul 18 11:41:10 2016 -0700

    x86/cpu: Add workaround for MONITOR instruction erratum on Goldmont based CPUs
    
    Monitored cached line may not wake up from mwait on certain
    Goldmont based CPUs. This patch will avoid calling
    current_set_polling_and_test() and thereby not set the TIF_ flag.
    The result is that we'll always send IPIs for wakeups.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1468867270-18493-1-git-send-email-jacob.jun.pan@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 96becbbb52e0..59f68f1d734b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -404,7 +404,7 @@ static int prefer_mwait_c1_over_halt(const struct cpuinfo_x86 *c)
 	if (c->x86_vendor != X86_VENDOR_INTEL)
 		return 0;
 
-	if (!cpu_has(c, X86_FEATURE_MWAIT))
+	if (!cpu_has(c, X86_FEATURE_MWAIT) || static_cpu_has_bug(X86_BUG_MONITOR))
 		return 0;
 
 	return 1;

commit 186f43608a5c827f8284fe4559225b4dccaa49ef
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:56 2016 -0400

    x86/kernel: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed some implicit header usage that was fixed up accordingly.
    
    Note that some bool/obj-y instances remain since module.h is
    the header for some exception table entry stuff, and for things
    like __init_or_module (code that is tossed when MODULES=n).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-4-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 96becbbb52e0..61b703c179d3 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -7,7 +7,8 @@
 #include <linux/prctl.h>
 #include <linux/slab.h>
 #include <linux/sched.h>
-#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/export.h>
 #include <linux/pm.h>
 #include <linux/tick.h>
 #include <linux/random.h>

commit e64646946ed32902fd597fa6e514b1da84642de3
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 20 17:00:20 2016 -0700

    exit_thread: accept a task parameter to be exited
    
    We need to call exit_thread from copy_process in a fail path.  So make it
    accept task_struct as a parameter.
    
    [v2]
    * s390: exit_thread_runtime_instr doesn't make sense to be called for
      non-current tasks.
    * arm: fix the comment in vfp_thread_copy
    * change 'me' to 'tsk' for task_struct
    * now we can change only archs that actually have exit_thread
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 2915d54e9dd5..96becbbb52e0 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -97,10 +97,9 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 /*
  * Free current thread data structures etc..
  */
-void exit_thread(void)
+void exit_thread(struct task_struct *tsk)
 {
-	struct task_struct *me = current;
-	struct thread_struct *t = &me->thread;
+	struct thread_struct *t = &tsk->thread;
 	unsigned long *bp = t->io_bitmap_ptr;
 	struct fpu *fpu = &t->fpu;
 

commit ba33ea811e1ff6726abb7f8f96df38c2d7b50304
Merge: e23604edac2a d05004944206
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 09:32:27 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "This is another big update. Main changes are:
    
       - lots of x86 system call (and other traps/exceptions) entry code
         enhancements.  In particular the complex parts of the 64-bit entry
         code have been migrated to C code as well, and a number of dusty
         corners have been refreshed.  (Andy Lutomirski)
    
       - vDSO special mapping robustification and general cleanups (Andy
         Lutomirski)
    
       - cpufeature refactoring, cleanups and speedups (Borislav Petkov)
    
       - lots of other changes ..."
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (64 commits)
      x86/cpufeature: Enable new AVX-512 features
      x86/entry/traps: Show unhandled signal for i386 in do_trap()
      x86/entry: Call enter_from_user_mode() with IRQs off
      x86/entry/32: Change INT80 to be an interrupt gate
      x86/entry: Improve system call entry comments
      x86/entry: Remove TIF_SINGLESTEP entry work
      x86/entry/32: Add and check a stack canary for the SYSENTER stack
      x86/entry/32: Simplify and fix up the SYSENTER stack #DB/NMI fixup
      x86/entry: Only allocate space for tss_struct::SYSENTER_stack if needed
      x86/entry: Vastly simplify SYSENTER TF (single-step) handling
      x86/entry/traps: Clear DR6 early in do_debug() and improve the comment
      x86/entry/traps: Clear TIF_BLOCKSTEP on all debug exceptions
      x86/entry/32: Restore FLAGS on SYSEXIT
      x86/entry/32: Filter NT and speed up AC filtering in SYSENTER
      x86/entry/compat: In SYSENTER, sink AC clearing below the existing FLAGS test
      selftests/x86: In syscall_nt, test NT|TF as well
      x86/asm-offsets: Remove PARAVIRT_enabled
      x86/entry/32: Introduce and use X86_BUG_ESPFIX instead of paravirt_enabled
      uprobes: __create_xol_area() must nullify xol_mapping.fault
      x86/cpufeature: Create a new synthetic cpu capability for machine check recovery
      ...

commit 2a41aa4feb25af3ead60b740c43df80c576efea2
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 9 19:00:33 2016 -0800

    x86/entry/32: Add and check a stack canary for the SYSENTER stack
    
    The first instruction of the SYSENTER entry runs on its own tiny
    stack.  That stack can be used if a #DB or NMI is delivered before
    the SYSENTER prologue switches to a real stack.
    
    We have code in place to prevent us from overflowing the tiny stack.
    For added paranoia, add a canary to the stack and check it in
    do_debug() -- that way, if something goes wrong with the #DB logic,
    we'll eventually notice.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/6ff9a806f39098b166dc2c41c1db744df5272f29.1457578375.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 9f7c21c22477..ee9a9792caeb 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -57,6 +57,9 @@ __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
 	  */
 	.io_bitmap		= { [0 ... IO_BITMAP_LONGS] = ~0 },
 #endif
+#ifdef CONFIG_X86_32
+	.SYSENTER_stack_canary	= STACK_END_MAGIC,
+#endif
 };
 EXPORT_PER_CPU_SYMBOL(cpu_tss);
 

commit ca59809ff6d572ae58fc6bedf7500f5a60fdbd64
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Thu Jan 28 19:02:51 2016 +0200

    locking/x86: Use mb() around clflush()
    
    The following commit:
    
      f8e617f4582995f ("sched/idle/x86: Optimize unnecessary mwait_idle() resched IPIs")
    
    adds memory barriers around clflush(), but this seems wrong for UP since
    barrier() has no effect on clflush().  We really want MFENCE, so switch
    to mb() instead.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <bitbucket@online.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization <virtualization@lists.linux-foundation.org>
    Link: http://lkml.kernel.org/r/1453921746-16178-5-git-send-email-mst@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 9f7c21c22477..9decee2bfdbe 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -418,9 +418,9 @@ static void mwait_idle(void)
 	if (!current_set_polling_and_test()) {
 		trace_cpu_idle_rcuidle(1, smp_processor_id());
 		if (this_cpu_has(X86_BUG_CLFLUSH_MONITOR)) {
-			smp_mb(); /* quirk */
+			mb(); /* quirk */
 			clflush((void *)&current_thread_info()->flags);
-			smp_mb(); /* quirk */
+			mb(); /* quirk */
 		}
 
 		__monitor((void *)&current_thread_info()->flags, 0, 0);

commit 2459ee8651dc5ab72790c2ffa99af288c7641b64
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Oct 30 22:42:46 2015 -0700

    x86/vm86: Set thread.vm86 to NULL on fork/clone
    
    thread.vm86 points to per-task information -- the pointer should not
    be copied on clone.
    
    Fixes: d4ce0f26c790 ("x86/vm86: Move fields from 'struct kernel_vm86_struct' to 'struct vm86'")
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Stas Sergeev <stsp@list.ru>
    Link: http://lkml.kernel.org/r/71c5d6985d70ec8197c8d72f003823c81b7dcf99.1446270067.git.luto@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e28db181e4fc..9f7c21c22477 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -84,6 +84,9 @@ EXPORT_SYMBOL_GPL(idle_notifier_unregister);
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	memcpy(dst, src, arch_task_struct_size);
+#ifdef CONFIG_VM86
+	dst->thread.vm86 = NULL;
+#endif
 
 	return fpu__copy(&dst->thread.fpu, &src->thread.fpu);
 }

commit f7d27c35ddff7c100d7a98db499ac0040149ac05
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Mon Oct 19 11:37:18 2015 +0300

    x86/mm, kasan: Silence KASAN warnings in get_wchan()
    
    get_wchan() is racy by design, it may access volatile stack
    of running task, thus it may access redzone in a stack frame
    and cause KASAN to warn about this.
    
    Use READ_ONCE_NOCHECK() to silence these warnings.
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wolfram Gloger <wmglo@dent.med.uni-muenchen.de>
    Cc: kasan-dev <kasan-dev@googlegroups.com>
    Link: http://lkml.kernel.org/r/1445243838-17763-3-git-send-email-aryabinin@virtuozzo.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 39e585a554b7..e28db181e4fc 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -550,14 +550,14 @@ unsigned long get_wchan(struct task_struct *p)
 	if (sp < bottom || sp > top)
 		return 0;
 
-	fp = READ_ONCE(*(unsigned long *)sp);
+	fp = READ_ONCE_NOCHECK(*(unsigned long *)sp);
 	do {
 		if (fp < bottom || fp > top)
 			return 0;
-		ip = READ_ONCE(*(unsigned long *)(fp + sizeof(unsigned long)));
+		ip = READ_ONCE_NOCHECK(*(unsigned long *)(fp + sizeof(unsigned long)));
 		if (!in_sched_functions(ip))
 			return ip;
-		fp = READ_ONCE(*(unsigned long *)fp);
+		fp = READ_ONCE_NOCHECK(*(unsigned long *)fp);
 	} while (count++ < 16 && p->state != TASK_RUNNING);
 	return 0;
 }

commit 7ba78053aacb89998a052843e3c56983c31d57f0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 30 08:38:23 2015 +0000

    x86/process: Unify 32bit and 64bit implementations of get_wchan()
    
    The stack layout and the functionality is identical. Use the 64bit
    version for all of x86.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: kasan-dev <kasan-dev@googlegroups.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Wolfram Gloger <wmglo@dent.med.uni-muenchen.de>
    Link: http://lkml.kernel.org/r/20150930083302.779694618@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6d0e62ae8516..39e585a554b7 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -506,3 +506,58 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 	return randomize_range(mm->brk, range_end, 0) ? : mm->brk;
 }
 
+/*
+ * Called from fs/proc with a reference on @p to find the function
+ * which called into schedule(). This needs to be done carefully
+ * because the task might wake up and we might look at a stack
+ * changing under us.
+ */
+unsigned long get_wchan(struct task_struct *p)
+{
+	unsigned long start, bottom, top, sp, fp, ip;
+	int count = 0;
+
+	if (!p || p == current || p->state == TASK_RUNNING)
+		return 0;
+
+	start = (unsigned long)task_stack_page(p);
+	if (!start)
+		return 0;
+
+	/*
+	 * Layout of the stack page:
+	 *
+	 * ----------- topmax = start + THREAD_SIZE - sizeof(unsigned long)
+	 * PADDING
+	 * ----------- top = topmax - TOP_OF_KERNEL_STACK_PADDING
+	 * stack
+	 * ----------- bottom = start + sizeof(thread_info)
+	 * thread_info
+	 * ----------- start
+	 *
+	 * The tasks stack pointer points at the location where the
+	 * framepointer is stored. The data on the stack is:
+	 * ... IP FP ... IP FP
+	 *
+	 * We need to read FP and IP, so we need to adjust the upper
+	 * bound by another unsigned long.
+	 */
+	top = start + THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;
+	top -= 2 * sizeof(unsigned long);
+	bottom = start + sizeof(struct thread_info);
+
+	sp = READ_ONCE(p->thread.sp);
+	if (sp < bottom || sp > top)
+		return 0;
+
+	fp = READ_ONCE(*(unsigned long *)sp);
+	do {
+		if (fp < bottom || fp > top)
+			return 0;
+		ip = READ_ONCE(*(unsigned long *)(fp + sizeof(unsigned long)));
+		if (!in_sched_functions(ip))
+			return ip;
+		fp = READ_ONCE(*(unsigned long *)fp);
+	} while (count++ < 16 && p->state != TASK_RUNNING);
+	return 0;
+}

commit 5778077d03cb25aac9b6a428e18970642fc019e3
Merge: 65a99597f044 7e01ebffffed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 1 08:40:25 2015 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm changes from Ingo Molnar:
     "The biggest changes in this cycle were:
    
       - Revamp, simplify (and in some cases fix) Time Stamp Counter (TSC)
         primitives.  (Andy Lutomirski)
    
       - Add new, comprehensible entry and exit handlers written in C.
         (Andy Lutomirski)
    
       - vm86 mode cleanups and fixes.  (Brian Gerst)
    
       - 32-bit compat code cleanups.  (Brian Gerst)
    
      The amount of simplification in low level assembly code is already
      palpable:
    
         arch/x86/entry/entry_32.S                          | 130 +----
         arch/x86/entry/entry_64.S                          | 197 ++-----
    
      but more simplifications are planned.
    
      There's also the usual laudry mix of low level changes - see the
      changelog for details"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (83 commits)
      x86/asm: Drop repeated macro of X86_EFLAGS_AC definition
      x86/asm/msr: Make wrmsrl() a function
      x86/asm/delay: Introduce an MWAITX-based delay with a configurable timer
      x86/asm: Add MONITORX/MWAITX instruction support
      x86/traps: Weaken context tracking entry assertions
      x86/asm/tsc: Add rdtscll() merge helper
      selftests/x86: Add syscall_nt selftest
      selftests/x86: Disable sigreturn_64
      x86/vdso: Emit a GNU hash
      x86/entry: Remove do_notify_resume(), syscall_trace_leave(), and their TIF masks
      x86/entry/32: Migrate to C exit path
      x86/entry/32: Remove 32-bit syscall audit optimizations
      x86/vm86: Rename vm86->v86flags and v86mask
      x86/vm86: Rename vm86->vm86_info to user_vm86
      x86/vm86: Clean up vm86.h includes
      x86/vm86: Move the vm86 IRQ definitions to vm86.h
      x86/vm86: Use the normal pt_regs area for vm86
      x86/vm86: Eliminate 'struct kernel_vm86_struct'
      x86/vm86: Move fields from 'struct kernel_vm86_struct' to 'struct vm86'
      x86/vm86: Move vm86 fields out of 'thread_struct'
      ...

commit 3959df1dfb9538498ec3372a2d390bc7fbdbfac2
Merge: 41d859a83c56 6c36dfe94918
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 31 20:20:30 2015 -0700

    Merge branch 'ras-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RAS updates from Ingo Molnar:
     "MCE handling updates, but also some generic drivers/edac/ changes to
      better organize the Kconfig space"
    
    * 'ras-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/ras: Move AMD MCE injector to arch/x86/ras/
      x86/mce: Add a wrapper around mce_log() for injection
      x86/mce: Rename rcu_dereference_check_mce() to mce_log_get_idx_check()
      RAS: Add a menuconfig option with descriptive text
      x86/mce: Reenable CMCI banks when swiching back to interrupt mode
      x86/mce: Clear Local MCE opt-in before kexec
      x86/mce: Remove unused function declarations
      x86/mce: Kill drain_mcelog_buffer()
      x86/mce: Avoid potential deadlock due to printk() in MCE context
      x86/mce: Remove the MCE ring for Action Optional errors
      x86/mce: Don't use percpu workqueues
      x86/mce: Provide a lockless memory pool to save error records
      x86/mce: Reuse one of the u16 padding fields in 'struct mce'

commit e43d0189ac02415fe4487f79fc35e8f147e9ea0d
Author: Jisheng Zhang <jszhang@marvell.com>
Date:   Thu Aug 20 12:54:39 2015 +0800

    x86/idle: Restore trace_cpu_idle to mwait_idle() calls
    
    Commit b253149b843f ("sched/idle/x86: Restore mwait_idle() to fix boot
    hangs, to improve power savings and to improve performance") restores
    mwait_idle(), but the trace_cpu_idle related calls are missing. This
    causes powertop on my old desktop powered by Intel Core2 E6550 to
    report zero wakeups and zero events.
    
    Add them back to restore the proper behaviour.
    
    Fixes: b253149b843f ("sched/idle/x86: Restore mwait_idle() to ...")
    Signed-off-by: Jisheng Zhang <jszhang@marvell.com>
    Cc: <len.brown@intel.com>
    Cc: stable@vger.kernel.org # 4.1
    Link: http://lkml.kernel.org/r/1440046479-4262-1-git-send-email-jszhang@marvell.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 397688beed4b..c27cad726765 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -408,6 +408,7 @@ static int prefer_mwait_c1_over_halt(const struct cpuinfo_x86 *c)
 static void mwait_idle(void)
 {
 	if (!current_set_polling_and_test()) {
+		trace_cpu_idle_rcuidle(1, smp_processor_id());
 		if (this_cpu_has(X86_BUG_CLFLUSH_MONITOR)) {
 			smp_mb(); /* quirk */
 			clflush((void *)&current_thread_info()->flags);
@@ -419,6 +420,7 @@ static void mwait_idle(void)
 			__sti_mwait(0, 0);
 		else
 			local_irq_enable();
+		trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 	} else {
 		local_irq_enable();
 	}

commit 8838eb6c0bf3b6a6494a163947ab3d1700ab45d2
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Wed Aug 12 18:29:40 2015 +0200

    x86/mce: Clear Local MCE opt-in before kexec
    
    kexec could boot a kernel that could be legacy with no knowledge
    of LMCE. Hence we should make sure we clear LMCE optin before
    kexec reboot.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux-edac <linux-edac@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1439396985-12812-9-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 397688beed4b..b20ef187ff41 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -29,6 +29,7 @@
 #include <asm/debugreg.h>
 #include <asm/nmi.h>
 #include <asm/tlbflush.h>
+#include <asm/mce.h>
 
 /*
  * per-CPU TSS segments. Threads are completely 'soft' on Linux,
@@ -319,6 +320,7 @@ void stop_this_cpu(void *dummy)
 	 */
 	set_cpu_online(smp_processor_id(), false);
 	disable_local_APIC();
+	mcheck_cpu_clear(this_cpu_ptr(&cpu_info));
 
 	for (;;)
 		halt();

commit 9fda6a0681e070b496235b132bc70ceb80300211
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Jul 29 01:41:16 2015 -0400

    x86/vm86: Move vm86 fields out of 'thread_struct'
    
    Allocate a separate structure for the vm86 fields.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1438148483-11932-2-git-send-email-brgerst@gmail.com
    [ Build fixes. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 397688beed4b..2199d9b774c8 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -29,6 +29,7 @@
 #include <asm/debugreg.h>
 #include <asm/nmi.h>
 #include <asm/tlbflush.h>
+#include <asm/vm86.h>
 
 /*
  * per-CPU TSS segments. Threads are completely 'soft' on Linux,
@@ -110,6 +111,8 @@ void exit_thread(void)
 		kfree(bp);
 	}
 
+	free_vm86(t);
+
 	fpu__drop(fpu);
 }
 

commit 5aaeb5c01c5b6c0be7b7aadbf3ace9f3a4458c3d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jul 17 12:28:12 2015 +0200

    x86/fpu, sched: Introduce CONFIG_ARCH_WANTS_DYNAMIC_TASK_STRUCT and use it on x86
    
    Don't burden architectures without dynamic task_struct sizing
    with the overhead of dynamic sizing.
    
    Also optimize the x86 code a bit by caching task_struct_size.
    
    Acked-and-Tested-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437128892-9831-3-git-send-email-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 975420eac105..397688beed4b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -81,7 +81,7 @@ EXPORT_SYMBOL_GPL(idle_notifier_unregister);
  */
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
-	memcpy(dst, src, arch_task_struct_size());
+	memcpy(dst, src, arch_task_struct_size);
 
 	return fpu__copy(&dst->thread.fpu, &src->thread.fpu);
 }

commit 0c8c0f03e3a292e031596484275c14cf39c0ab7a
Author: Dave Hansen <dave@sr71.net>
Date:   Fri Jul 17 12:28:11 2015 +0200

    x86/fpu, sched: Dynamically allocate 'struct fpu'
    
    The FPU rewrite removed the dynamic allocations of 'struct fpu'.
    But, this potentially wastes massive amounts of memory (2k per
    task on systems that do not have AVX-512 for instance).
    
    Instead of having a separate slab, this patch just appends the
    space that we need to the 'task_struct' which we dynamically
    allocate already.  This saves from doing an extra slab
    allocation at fork().
    
    The only real downside here is that we have to stick everything
    and the end of the task_struct.  But, I think the
    BUILD_BUG_ON()s I stuck in there should keep that from being too
    fragile.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437128892-9831-2-git-send-email-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 9cad694ed7c4..975420eac105 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -81,7 +81,7 @@ EXPORT_SYMBOL_GPL(idle_notifier_unregister);
  */
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
-	*dst = *src;
+	memcpy(dst, src, arch_task_struct_size());
 
 	return fpu__copy(&dst->thread.fpu, &src->thread.fpu);
 }

commit e75c73ad64478c12b3a44b86a3e7f62a4f65b93e
Merge: cfe3eceb7a2e a8424003679e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 17:16:11 2015 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU updates from Ingo Molnar:
     "This tree contains two main changes:
    
       - The big FPU code rewrite: wide reaching cleanups and reorganization
         that pulls all the FPU code together into a clean base in
         arch/x86/fpu/.
    
         The resulting code is leaner and faster, and much easier to
         understand.  This enables future work to further simplify the FPU
         code (such as removing lazy FPU restores).
    
         By its nature these changes have a substantial regression risk: FPU
         code related bugs are long lived, because races are often subtle
         and bugs mask as user-space failures that are difficult to track
         back to kernel side backs.  I'm aware of no unfixed (or even
         suspected) FPU related regression so far.
    
       - MPX support rework/fixes.  As this is still not a released CPU
         feature, there were some buglets in the code - should be much more
         robust now (Dave Hansen)"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (250 commits)
      x86/fpu: Fix double-increment in setup_xstate_features()
      x86/mpx: Allow 32-bit binaries on 64-bit kernels again
      x86/mpx: Do not count MPX VMAs as neighbors when unmapping
      x86/mpx: Rewrite the unmap code
      x86/mpx: Support 32-bit binaries on 64-bit kernels
      x86/mpx: Use 32-bit-only cmpxchg() for 32-bit apps
      x86/mpx: Introduce new 'directory entry' to 'addr' helper function
      x86/mpx: Add temporary variable to reduce masking
      x86: Make is_64bit_mm() widely available
      x86/mpx: Trace allocation of new bounds tables
      x86/mpx: Trace the attempts to find bounds tables
      x86/mpx: Trace entry to bounds exception paths
      x86/mpx: Trace #BR exceptions
      x86/mpx: Introduce a boot-time disable flag
      x86/mpx: Restrict the mmap() size check to bounds tables
      x86/mpx: Remove redundant MPX_BNDCFG_ADDR_MASK
      x86/mpx: Clean up the code by not passing a task pointer around when unnecessary
      x86/mpx: Use the new get_xsave_field_ptr()API
      x86/fpu/xstate: Wrap get_xsave_addr() to make it safer
      x86/fpu/xstate: Fix up bad get_xsave_addr() assumptions
      ...

commit 0fb0328d3458ff2d6ffbb280b75053c99a8a4b1f
Author: Huang Rui <ray.huang@amd.com>
Date:   Tue May 26 10:28:09 2015 +0200

    sched/x86: Drop repeated word from mwait_idle() comment
    
    A single "default" is fine.
    
    Signed-off-by: Huang Rui <ray.huang@amd.com>
    [ Fix another typo and reflow comment. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1432022472-2224-5-git-send-email-ray.huang@amd.com
    Link: http://lkml.kernel.org/r/1432628901-18044-7-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6e338e3b1dc0..c648139d68d7 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -445,11 +445,10 @@ static int prefer_mwait_c1_over_halt(const struct cpuinfo_x86 *c)
 }
 
 /*
- * MONITOR/MWAIT with no hints, used for default default C1 state.
- * This invokes MWAIT with interrutps enabled and no flags,
- * which is backwards compatible with the original MWAIT implementation.
+ * MONITOR/MWAIT with no hints, used for default C1 state. This invokes MWAIT
+ * with interrupts enabled and no flags, which is backwards compatible with the
+ * original MWAIT implementation.
  */
-
 static void mwait_idle(void)
 {
 	if (!current_set_polling_and_test()) {

commit 04c8e01d50ae1f2b33a46459e8b1e776b747e97d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 29 20:35:33 2015 +0200

    x86/fpu: Move fpu__clear() to 'struct fpu *' parameter passing
    
    Do it like all other high level FPU state handling functions: they
    only know about struct fpu, not about the task.
    
    (Also remove a dead prototype while at it.)
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index dde263fb2031..b53f6c939bc2 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -120,7 +120,7 @@ void flush_thread(void)
 	flush_ptrace_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
 
-	fpu__clear(tsk);
+	fpu__clear(&tsk->thread.fpu);
 }
 
 static void hard_disable_TSC(void)

commit 5033861575df08a04090cc7b785b2b7aadcbde82
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 29 19:04:31 2015 +0200

    x86/fpu: Synchronize the naming of drop_fpu() and fpu_reset_state()
    
    drop_fpu() and fpu_reset_state() are similar in functionality
    and in scope, yet this is not apparent from their names.
    
    drop_fpu() deactivates FPU contents (both the fpregs and the fpstate),
    but leaves register contents intact in the eager-FPU case, mostly as an
    optimization. It disables fpregs in the lazy FPU case. The drop_fpu()
    method can be used to destroy FPU state in an optimized way, when we
    know that a new state will be loaded before user-space might see
    any remains of the old FPU state:
    
         - such as in sys_exit()'s exit_thread() where we know this task
           won't execute any user-space instructions anymore and the
           next context switch cleans up the FPU. The old FPU state
           might still be around in the eagerfpu case but won't be
           saved.
    
         - in __restore_xstate_sig(), where we use drop_fpu() before
           copying a new state into the fpstate and activating that one.
           No user-pace instructions can execute between those steps.
    
         - in sys_execve()'s fpu__clear(): there we use drop_fpu() in
           the !eagerfpu case, where it's equivalent to a full reinit.
    
    fpu_reset_state() is a stronger version of drop_fpu(): both in
    the eagerfpu and the lazy-FPU case it guarantees that fpregs
    are reinitialized to init state. This method is used in cases
    where we need a full reset:
    
         - handle_signal() uses fpu_reset_state() to reset the FPU state
           to init before executing a user-space signal handler. While we
           have already saved the original FPU state at this point, and
           always restore the original state, the signal handling code
           still has to do this reinit, because signals may interrupt
           any user-space instruction, and the FPU might be in various
           intermediate states (such as an unbalanced x87 stack) that is
           not immediately usable for general C signal handler code.
    
         - __restore_xstate_sig() uses fpu_reset_state() when the signal
           frame has no FP context. Since the signal handler may have
           modified the FPU state, it gets reset back to init state.
    
         - in another branch __restore_xstate_sig() uses fpu_reset_state()
           to handle a restoration error: when restore_user_xstate() fails
           to restore FPU state and we might have inconsistent FPU data,
           fpu_reset_state() is used to reset it back to a known good
           state.
    
         - __kernel_fpu_end() uses fpu_reset_state() in an error branch.
           This is in a 'must not trigger' error branch, so on bug-free
           kernels this never triggers.
    
         - fpu__restore() uses fpu_reset_state() in an error path
           as well: if the fpstate was set up with invalid FPU state
           (via ptrace or via a signal handler), then it's reset back
           to init state.
    
         - likewise, the scheduler's switch_fpu_finish() uses it in a
           restoration error path too.
    
    Move both drop_fpu() and fpu_reset_state() to the fpu__*() namespace
    and harmonize their naming with their function:
    
        fpu__drop()
        fpu__reset()
    
    This clearly shows that both methods operate on the full state of the
    FPU, just like fpu__restore().
    
    Also add comments to explain what each function does.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 5d37c26fa89f..dde263fb2031 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -110,7 +110,7 @@ void exit_thread(void)
 		kfree(bp);
 	}
 
-	drop_fpu(fpu);
+	fpu__drop(fpu);
 }
 
 void flush_thread(void)

commit c4d6ee6e2e52ec604cc1d76877791f8e8f5c79b5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 05:52:40 2015 +0200

    x86/fpu: Remove failure paths from fpstate-alloc low level functions
    
    Now that we always allocate the FPU context as part of task_struct there's
    no need for separate allocations - remove them and their primary failure
    handling code.
    
    ( Note that there's still secondary error codes that have become superfluous,
      those will be removed in separate patches. )
    
    Move the somewhat misplaced setup_xstate_comp() call to the core.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 71f4b4d2f1fd..5d37c26fa89f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -86,16 +86,6 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	return fpu__copy(&dst->thread.fpu, &src->thread.fpu);
 }
 
-void arch_release_task_struct(struct task_struct *tsk)
-{
-	fpstate_free(&tsk->thread.fpu);
-}
-
-void arch_task_cache_init(void)
-{
-	fpstate_cache_init();
-}
-
 /*
  * Free current thread data structures etc..
  */

commit 78f7f1e54bac032b98956862a5bcf8c28ed22d07
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 02:54:44 2015 +0200

    x86/fpu: Rename fpu-internal.h to fpu/internal.h
    
    This unifies all the FPU related header files under a unified, hiearchical
    naming scheme:
    
     - asm/fpu/types.h:      FPU related data types, needed for 'struct task_struct',
                             widely included in almost all kernel code, and hence kept
                             as small as possible.
    
     - asm/fpu/api.h:        FPU related 'public' methods exported to other subsystems.
    
     - asm/fpu/internal.h:   FPU subsystem internal methods
    
     - asm/fpu/xsave.h:      XSAVE support internal methods
    
    (Also standardize the header guard in asm/fpu/internal.h.)
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 51ad3422e728..71f4b4d2f1fd 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -25,7 +25,7 @@
 #include <asm/idle.h>
 #include <asm/uaccess.h>
 #include <asm/mwait.h>
-#include <asm/fpu-internal.h>
+#include <asm/fpu/internal.h>
 #include <asm/debugreg.h>
 #include <asm/nmi.h>
 #include <asm/tlbflush.h>

commit 2e8a3102662233dfac92fe70f56429b4050f674a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 02:28:23 2015 +0200

    x86/fpu: Rename fpu__flush_thread() to fpu__clear()
    
    The primary purpose of this function is to clear the current task's
    FPU before an exec(), to not leak information from the previous task,
    and to allow the new task to start with freshly initialized FPU
    registers.
    
    Rename the function to reflect this primary purpose.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e97266b18ad3..51ad3422e728 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -130,7 +130,7 @@ void flush_thread(void)
 	flush_ptrace_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
 
-	fpu__flush_thread(tsk);
+	fpu__clear(tsk);
 }
 
 static void hard_disable_TSC(void)

commit c69e098b1f90c0d520c4d5b5bff9f2ede95b13a8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 02:07:15 2015 +0200

    x86/fpu: Use 'struct fpu' in fpu__copy()
    
    Migrate this function to pure 'struct fpu' usage.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 50d503a2d8c3..e97266b18ad3 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -83,7 +83,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	*dst = *src;
 
-	return fpu__copy(dst, src);
+	return fpu__copy(&dst->thread.fpu, &src->thread.fpu);
 }
 
 void arch_release_task_struct(struct task_struct *tsk)

commit ca6787ba0fcc875cfb06dc2a538ac23210b7d251
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 23 12:33:50 2015 +0200

    x86/fpu: Remove 'struct task_struct' usage from drop_fpu()
    
    Migrate this function to pure 'struct fpu' usage.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index bb7d4abcdad6..50d503a2d8c3 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -104,6 +104,7 @@ void exit_thread(void)
 	struct task_struct *me = current;
 	struct thread_struct *t = &me->thread;
 	unsigned long *bp = t->io_bitmap_ptr;
+	struct fpu *fpu = &t->fpu;
 
 	if (bp) {
 		struct tss_struct *tss = &per_cpu(cpu_tss, get_cpu());
@@ -119,7 +120,7 @@ void exit_thread(void)
 		kfree(bp);
 	}
 
-	drop_fpu(me);
+	drop_fpu(fpu);
 }
 
 void flush_thread(void)

commit a752b53d9dcbae28a3a22b5577f0571acf53d5aa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 15:47:05 2015 +0200

    x86/fpu: Factor out fpu__copy()
    
    Introduce fpu__copy() and use it in arch_dup_task_struct(),
    thus moving another chunk of FPU logic to fpu/core.c.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 36d9f737f278..bb7d4abcdad6 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -83,17 +83,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	*dst = *src;
 
-	dst->thread.fpu.counter = 0;
-	dst->thread.fpu.has_fpu = 0;
-	dst->thread.fpu.state = NULL;
-	task_disable_lazy_fpu_restore(dst);
-	if (tsk_used_math(src)) {
-		int err = fpstate_alloc(&dst->thread.fpu);
-		if (err)
-			return err;
-		fpu_copy(dst, src);
-	}
-	return 0;
+	return fpu__copy(dst, src);
 }
 
 void arch_release_task_struct(struct task_struct *tsk)

commit 8ffb53ab986ccb4421b1060182c6e084edd7b9d8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 15:41:56 2015 +0200

    x86/fpu: Move task_xstate_cachep handling to core.c
    
    This code was historically in process.c, now we have FPU core internals in
    fpu/core.c instead - move it there.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 52fd8f6f44c7..36d9f737f278 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -75,9 +75,6 @@ void idle_notifier_unregister(struct notifier_block *n)
 EXPORT_SYMBOL_GPL(idle_notifier_unregister);
 #endif
 
-struct kmem_cache *task_xstate_cachep;
-EXPORT_SYMBOL_GPL(task_xstate_cachep);
-
 /*
  * this gets called so that we can store lazy state into memory and copy the
  * current task into the new thread.
@@ -106,11 +103,7 @@ void arch_release_task_struct(struct task_struct *tsk)
 
 void arch_task_cache_init(void)
 {
-        task_xstate_cachep =
-        	kmem_cache_create("task_xstate", xstate_size,
-				  __alignof__(union thread_xstate),
-				  SLAB_PANIC | SLAB_NOTRACK, NULL);
-	setup_xstate_comp();
+	fpstate_cache_init();
 }
 
 /*

commit 81683cc8277e79decff4d0cf82ae0e17d2fe465f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 11:52:13 2015 +0200

    x86/fpu: Factor out fpu__flush_thread() from flush_thread()
    
    flush_thread() open codes a lot of FPU internals - create a separate
    function for it in fpu/core.c.
    
    Turns out that this does not hurt performance:
    
       text    data     bss     dec     hex filename
       11843039        1884440 1130496 14857975         e2b6f7 vmlinux.before
       11843039        1884440 1130496 14857975         e2b6f7 vmlinux.after
    
    and since this is a slowpath clarity comes first anyway.
    
    We can reconsider inlining decisions after the FPU code has been cleaned up.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6ab180f40a7e..52fd8f6f44c7 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -146,19 +146,7 @@ void flush_thread(void)
 	flush_ptrace_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
 
-	if (!use_eager_fpu()) {
-		/* FPU state will be reallocated lazily at the first use. */
-		drop_fpu(tsk);
-		fpstate_free(&tsk->thread.fpu);
-	} else {
-		if (!tsk_used_math(tsk)) {
-			/* kthread execs. TODO: cleanup this horror. */
-		if (WARN_ON(fpstate_alloc_init(tsk)))
-				force_sig(SIGKILL, tsk);
-			user_fpu_begin();
-		}
-		restore_init_xstate();
-	}
+	fpu__flush_thread(tsk);
 }
 
 static void hard_disable_TSC(void)

commit 11ad19277e025f914518bc2943a240cdd37cf844
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 11:44:46 2015 +0200

    x86/fpu: Remove the free_thread_xstate() complication
    
    Use fpstate_free() directly to manage FPU state.
    
    Only process.c was using this method, so this is a speedup as well,
    as it removes the extra function call and related clobbers.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 35d0f1925524..6ab180f40a7e 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -99,14 +99,9 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	return 0;
 }
 
-void free_thread_xstate(struct task_struct *tsk)
-{
-	fpstate_free(&tsk->thread.fpu);
-}
-
 void arch_release_task_struct(struct task_struct *tsk)
 {
-	free_thread_xstate(tsk);
+	fpstate_free(&tsk->thread.fpu);
 }
 
 void arch_task_cache_init(void)
@@ -154,7 +149,7 @@ void flush_thread(void)
 	if (!use_eager_fpu()) {
 		/* FPU state will be reallocated lazily at the first use. */
 		drop_fpu(tsk);
-		free_thread_xstate(tsk);
+		fpstate_free(&tsk->thread.fpu);
 	} else {
 		if (!tsk_used_math(tsk)) {
 			/* kthread execs. TODO: cleanup this horror. */

commit f89e32e0a3df2f29d61fdc120ac62654ef267111
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 10:58:10 2015 +0200

    x86/fpu: Fix header file dependencies of fpu-internal.h
    
    Fix a minor header file dependency bug in asm/fpu-internal.h: it
    relies on i387.h but does not include it. All users of fpu-internal.h
    included it explicitly.
    
    Also remove unnecessary includes, to reduce compilation time.
    
    This also makes it easier to use it as a standalone header file
    for FPU internals, such as an upcoming C module in arch/x86/kernel/fpu/.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c7793addc237..35d0f1925524 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -25,7 +25,6 @@
 #include <asm/idle.h>
 #include <asm/uaccess.h>
 #include <asm/mwait.h>
-#include <asm/i387.h>
 #include <asm/fpu-internal.h>
 #include <asm/debugreg.h>
 #include <asm/nmi.h>

commit c0c2803dee21bef08ef5aacdf96fe2f1759ccc62
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 09:52:56 2015 +0200

    x86/fpu: Move thread_info::fpu_counter into thread_info::fpu.counter
    
    This field is kept separate from the main FPU state structure for
    no good reason.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index fd4aa56335de..c7793addc237 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -87,7 +87,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	*dst = *src;
 
-	dst->thread.fpu_counter = 0;
+	dst->thread.fpu.counter = 0;
 	dst->thread.fpu.has_fpu = 0;
 	dst->thread.fpu.state = NULL;
 	task_disable_lazy_fpu_restore(dst);

commit a7c2a83364c0d882a2d850c4c183c6de42d5c02b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 3 12:55:22 2015 +0200

    x86/fpu: Rename fpu_free() to fpstate_free()
    
    Use the fpu__*() namespace.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index a0ed7c06a12c..fd4aa56335de 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -102,7 +102,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 
 void free_thread_xstate(struct task_struct *tsk)
 {
-	fpu_free(&tsk->thread.fpu);
+	fpstate_free(&tsk->thread.fpu);
 }
 
 void arch_release_task_struct(struct task_struct *tsk)

commit ed97b085461ca6bdc22162b68b4cba69459ce1c8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 3 12:41:14 2015 +0200

    x86/fpu: Rename fpu_alloc() to fpstate_alloc()
    
    Use the fpu__*() namespace for fpstate_alloc() as well.
    
    Also add a comment about FPU state alignment.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index abdb81d07423..a0ed7c06a12c 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -92,7 +92,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	dst->thread.fpu.state = NULL;
 	task_disable_lazy_fpu_restore(dst);
 	if (tsk_used_math(src)) {
-		int err = fpu_alloc(&dst->thread.fpu);
+		int err = fpstate_alloc(&dst->thread.fpu);
 		if (err)
 			return err;
 		fpu_copy(dst, src);

commit 97185c95f7ab7f752473c34672dab0925758094b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 3 12:02:02 2015 +0200

    x86/fpu: Split an fpstate_alloc_init() function out of init_fpu()
    
    Most init_fpu() users don't want the register-saving aspect of the
    function, they are calling it for 'current' and when FPU registers
    are not allocated and initialized yet.
    
    Split out a simplified API that does just that (and add debug-checks
    for these conditions): fpstate_alloc_init().
    
    Use it where appropriate.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6e338e3b1dc0..abdb81d07423 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -159,7 +159,7 @@ void flush_thread(void)
 	} else {
 		if (!tsk_used_math(tsk)) {
 			/* kthread execs. TODO: cleanup this horror. */
-			if (WARN_ON(init_fpu(tsk)))
+		if (WARN_ON(fpstate_alloc_init(tsk)))
 				force_sig(SIGKILL, tsk);
 			user_fpu_begin();
 		}

commit c88d47480d300eaad80c213d50c9bf6077fc49bc
Author: Bobby Powers <bobbypowers@gmail.com>
Date:   Mon Apr 27 08:10:41 2015 -0700

    x86/fpu: Always restore_xinit_state() when use_eager_cpu()
    
    The following commit:
    
      f893959b0898 ("x86/fpu: Don't abuse drop_init_fpu() in flush_thread()")
    
    removed drop_init_fpu() usage from flush_thread(). This seems to break
    things for me - the Go 1.4 test suite fails all over the place with
    floating point comparision errors (offending commit found through
    bisection).
    
    The functional change was that flush_thread() after this commit
    only calls restore_init_xstate() when both use_eager_fpu() and
    !used_math() are true. drop_init_fpu() (now fpu_reset_state()) calls
    restore_init_xstate() regardless of whether current used_math() - apply
    the same logic here.
    
    Switch used_math() -> tsk_used_math(tsk) to consistently use the grabbed
    tsk instead of current, like in the rest of flush_thread().
    
    Tested-by: Dave Hansen <dave.hansen@intel.com>
    Signed-off-by: Bobby Powers <bobbypowers@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pekka Riikonen <priikone@iki.fi>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suresh Siddha <sbsiddha@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: f893959b ("x86/fpu: Don't abuse drop_init_fpu() in flush_thread()")
    Link: http://lkml.kernel.org/r/1430147441-9820-1-git-send-email-bobbypowers@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index bfc99b3b6522..6e338e3b1dc0 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -156,11 +156,13 @@ void flush_thread(void)
 		/* FPU state will be reallocated lazily at the first use. */
 		drop_fpu(tsk);
 		free_thread_xstate(tsk);
-	} else if (!used_math()) {
-		/* kthread execs. TODO: cleanup this horror. */
-		if (WARN_ON(init_fpu(tsk)))
-			force_sig(SIGKILL, tsk);
-		user_fpu_begin();
+	} else {
+		if (!tsk_used_math(tsk)) {
+			/* kthread execs. TODO: cleanup this horror. */
+			if (WARN_ON(init_fpu(tsk)))
+				force_sig(SIGKILL, tsk);
+			user_fpu_begin();
+		}
 		restore_init_xstate();
 	}
 }

commit de71ad2c97862eae1516aa36528cc3b317c17b2f
Author: Marc Dionne <marc.c.dionne@gmail.com>
Date:   Mon May 4 15:16:44 2015 -0300

    x86: Make cpu_tss available to external modules
    
    Commit 75182b1632 ("x86/asm/entry: Switch all C consumers of
    kernel_stack to this_cpu_sp0()") changed current_thread_info
    to use this_cpu_sp0, and indirectly made it rely on init_tss
    which was exported with EXPORT_PER_CPU_SYMBOL_GPL.
    As a result some macros and inline functions such as set/get_fs,
    test_thread_flag and variants have been made unusable for
    external modules.
    
    Make cpu_tss exported with EXPORT_PER_CPU_SYMBOL so that these
    functions are accessible again, as they were previously.
    
    Signed-off-by: Marc Dionne <marc.dionne@your-file-system.com>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/r/1430763404-21221-1-git-send-email-marc.dionne@your-file-system.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 8213da62b1b7..bfc99b3b6522 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -57,7 +57,7 @@ __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
 	.io_bitmap		= { [0 ... IO_BITMAP_LONGS] = ~0 },
 #endif
 };
-EXPORT_PER_CPU_SYMBOL_GPL(cpu_tss);
+EXPORT_PER_CPU_SYMBOL(cpu_tss);
 
 #ifdef CONFIG_X86_64
 static DEFINE_PER_CPU(unsigned char, is_idle);

commit 421ec9017f3a1f3f032d894c55c15870f3d474aa
Merge: 64f004a2ab68 7fc253e277ec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 13:24:23 2015 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fpu changes from Ingo Molnar:
     "Various x86 FPU handling cleanups, refactorings and fixes (Borislav
      Petkov, Oleg Nesterov, Rik van Riel)"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (21 commits)
      x86/fpu: Kill eager_fpu_init_bp()
      x86/fpu: Don't allocate fpu->state for swapper/0
      x86/fpu: Rename drop_init_fpu() to fpu_reset_state()
      x86/fpu: Fold __drop_fpu() into its sole user
      x86/fpu: Don't abuse drop_init_fpu() in flush_thread()
      x86/fpu: Use restore_init_xstate() instead of math_state_restore() on kthread exec
      x86/fpu: Introduce restore_init_xstate()
      x86/fpu: Document user_fpu_begin()
      x86/fpu: Factor out memset(xstate, 0) in fpu_finit() paths
      x86/fpu: Change xstateregs_get()/set() to use ->xsave.i387 rather than ->fxsave
      x86/fpu: Don't abuse FPU in kernel threads if use_eager_fpu()
      x86/fpu: Always allow FPU in interrupt if use_eager_fpu()
      x86/fpu: __kernel_fpu_begin() should clear fpu_owner_task even if use_eager_fpu()
      x86/fpu: Also check fpu_lazy_restore() when use_eager_fpu()
      x86/fpu: Use task_disable_lazy_fpu_restore() helper
      x86/fpu: Use an explicit if/else in switch_fpu_prepare()
      x86/fpu: Introduce task_disable_lazy_fpu_restore() helper
      x86/fpu: Move lazy restore functions up a few lines
      x86/fpu: Change math_error() to use unlazy_fpu(), kill (now) unused save_init_fpu()
      x86/fpu: Don't do __thread_fpu_end() if use_eager_fpu()
      ...

commit 60f898eeaaa1c5d0162a4240bacf33a6c87ecef6
Merge: 977e1ba50893 3b75232d5568
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 13:16:36 2015 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm changes from Ingo Molnar:
     "There were lots of changes in this development cycle:
    
       - over 100 separate cleanups, restructuring changes, speedups and
         fixes in the x86 system call, irq, trap and other entry code, part
         of a heroic effort to deobfuscate a decade old spaghetti asm code
         and its C code dependencies (Denys Vlasenko, Andy Lutomirski)
    
       - alternatives code fixes and enhancements (Borislav Petkov)
    
       - simplifications and cleanups to the compat code (Brian Gerst)
    
       - signal handling fixes and new x86 testcases (Andy Lutomirski)
    
       - various other fixes and cleanups
    
      By their nature many of these changes are risky - we tried to test
      them well on many different x86 systems (there are no known
      regressions), and they are split up finely to help bisection - but
      there's still a fair bit of residual risk left so caveat emptor"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (148 commits)
      perf/x86/64: Report regs_user->ax too in get_regs_user()
      perf/x86/64: Simplify regs_user->abi setting code in get_regs_user()
      perf/x86/64: Do report user_regs->cx while we are in syscall, in get_regs_user()
      perf/x86/64: Do not guess user_regs->cs, ss, sp in get_regs_user()
      x86/asm/entry/32: Tidy up JNZ instructions after TESTs
      x86/asm/entry/64: Reduce padding in execve stubs
      x86/asm/entry/64: Remove GET_THREAD_INFO() in ret_from_fork
      x86/asm/entry/64: Simplify jumps in ret_from_fork
      x86/asm/entry/64: Remove a redundant jump
      x86/asm/entry/64: Optimize [v]fork/clone stubs
      x86/asm/entry: Zero EXTRA_REGS for stub32_execve() too
      x86/asm/entry/64: Move stub_x32_execvecloser() to stub_execveat()
      x86/asm/entry/64: Use common code for rt_sigreturn() epilogue
      x86/asm/entry/64: Add forgotten CFI annotation
      x86/asm/entry/irq: Simplify interrupt dispatch table (IDT) layout
      x86/asm/entry/64: Move opportunistic sysret code to syscall code path
      x86, selftests: Add sigreturn selftest
      x86/alternatives: Guard NOPs optimization
      x86/asm/entry: Clear EXTRA_REGS for all executable formats
      x86/signal: Remove pax argument from restore_sigcontext
      ...

commit 7fd56474db326f7a6df0e2a4e3a9600cc083ab9b
Merge: 49d2953c72c6 def747087e83
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 11:08:28 2015 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - clockevents state machine cleanups and enhancements (Viresh Kumar)
    
       - clockevents broadcast notifier horror to state machine conversion
         and related cleanups (Thomas Gleixner, Rafael J Wysocki)
    
       - clocksource and timekeeping core updates (John Stultz)
    
       - clocksource driver updates and fixes (Ben Dooks, Dmitry Osipenko,
         Hans de Goede, Laurent Pinchart, Maxime Ripard, Xunlei Pang)
    
       - y2038 fixes (Xunlei Pang, John Stultz)
    
       - NMI-safe ktime_get_raw_fast() and general refactoring of the clock
         code, in preparation to perf's per event clock ID support (Peter
         Zijlstra)
    
       - generic sched/clock fixes, optimizations and cleanups (Daniel
         Thompson)
    
       - clockevents cpu_down() race fix (Preeti U Murthy)"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (94 commits)
      timers/PM: Drop unnecessary braces from tick_freeze()
      timers/PM: Fix up tick_unfreeze()
      timekeeping: Get rid of stale comment
      clockevents: Cleanup dead cpu explicitely
      clockevents: Make tick handover explicit
      clockevents: Remove broadcast oneshot control leftovers
      sched/idle: Use explicit broadcast oneshot control function
      ARM: Tegra: Use explicit broadcast oneshot control function
      ARM: OMAP: Use explicit broadcast oneshot control function
      intel_idle: Use explicit broadcast oneshot control function
      ACPI/idle: Use explicit broadcast control function
      ACPI/PAD: Use explicit broadcast oneshot control function
      x86/amd/idle, clockevents: Use explicit broadcast oneshot control functions
      clockevents: Provide explicit broadcast oneshot control functions
      clockevents: Remove the broadcast control leftovers
      ARM: OMAP: Use explicit broadcast control function
      intel_idle: Use explicit broadcast control function
      cpuidle: Use explicit broadcast control function
      ACPI/processor: Use explicit broadcast control function
      ACPI/PAD: Use explicit broadcast control function
      ...

commit 435c350e8197488f12c97e7df28a9c2199bd1673
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:05:53 2015 +0200

    x86/amd/idle, clockevents: Use explicit broadcast oneshot control functions
    
    Replace the clockevents_notify() call with an explicit function call.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/8569669.lgxIty9PKW@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e94b733de302..2f43c62fcd6d 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -381,7 +381,7 @@ static void amd_e400_idle(void)
 			tick_broadcast_force();
 			pr_info("Switch to broadcast mode on CPU%d\n", cpu);
 		}
-		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &cpu);
+		tick_broadcast_enter();
 
 		default_idle();
 
@@ -390,7 +390,7 @@ static void amd_e400_idle(void)
 		 * called with interrupts disabled.
 		 */
 		local_irq_disable();
-		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &cpu);
+		tick_broadcast_exit();
 		local_irq_enable();
 	} else
 		default_idle();

commit 162a688e84df49c5bcc855a5e5bf812d0ec89ad5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:01:28 2015 +0200

    x86/amd/idle, clockevents: Use explicit broadcast control function
    
    Replace the clockevents_notify() call with an explicit function call.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1528188.S1pjqkSL1P@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 046e2d620bbe..e94b733de302 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -9,7 +9,7 @@
 #include <linux/sched.h>
 #include <linux/module.h>
 #include <linux/pm.h>
-#include <linux/clockchips.h>
+#include <linux/tick.h>
 #include <linux/random.h>
 #include <linux/user-return-notifier.h>
 #include <linux/dmi.h>
@@ -377,11 +377,8 @@ static void amd_e400_idle(void)
 
 		if (!cpumask_test_cpu(cpu, amd_e400_c1e_mask)) {
 			cpumask_set_cpu(cpu, amd_e400_c1e_mask);
-			/*
-			 * Force broadcast so ACPI can not interfere.
-			 */
-			clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_FORCE,
-					   &cpu);
+			/* Force broadcast so ACPI can not interfere. */
+			tick_broadcast_force();
 			pr_info("Switch to broadcast mode on CPU%d\n", cpu);
 		}
 		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &cpu);

commit e1b63dec2ddba654c7ca75996284e453f32d1af7
Merge: f8e617f45829 746db9443ea5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Mar 23 10:50:29 2015 +0100

    Merge branch 'sched/urgent' into sched/core, to pick up fixes before applying new patches
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f893959b0898bd876673adbeb6798bdf25c034d7
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Mar 13 18:30:30 2015 +0100

    x86/fpu: Don't abuse drop_init_fpu() in flush_thread()
    
    flush_thread() -> drop_init_fpu() is suboptimal and confusing. It does
    drop_fpu() or restore_init_xstate() depending on !use_eager_fpu(). But
    flush_thread() too checks eagerfpu right after that, and if it is true
    then restore_init_xstate() just burns CPU for no reason. We are going to
    load init_xstate_buf again after we set used_math()/user_has_fpu(), until
    then the FPU state can't survive after switch_to().
    
    Remove it, and change the "if (!use_eager_fpu())" to call drop_fpu().
    While at it, clean up the tsk/current usage.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pekka Riikonen <priikone@iki.fi>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suresh Siddha <sbsiddha@gmail.com>
    Link: http://lkml.kernel.org/r/20150313173030.GA31217@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6b058296a456..1d2ebadba7ac 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -132,17 +132,14 @@ void flush_thread(void)
 	flush_ptrace_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
 
-	drop_init_fpu(tsk);
-	/*
-	 * Free the FPU state for non xsave platforms. They get reallocated
-	 * lazily at the first use.
-	 */
-	if (!use_eager_fpu())
+	if (!use_eager_fpu()) {
+		/* FPU state will be reallocated lazily at the first use. */
+		drop_fpu(tsk);
 		free_thread_xstate(tsk);
-	else if (!used_math()) {
+	} else if (!used_math()) {
 		/* kthread execs. TODO: cleanup this horror. */
-		if (WARN_ON(init_fpu(current)))
-			force_sig(SIGKILL, current);
+		if (WARN_ON(init_fpu(tsk)))
+			force_sig(SIGKILL, tsk);
 		user_fpu_begin();
 		restore_init_xstate();
 	}

commit 9cb6ce823bbd1adbe15e30bd1435c84c2e271767
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Mar 11 18:34:49 2015 +0100

    x86/fpu: Use restore_init_xstate() instead of math_state_restore() on kthread exec
    
    Change flush_thread() to do user_fpu_begin() and restore_init_xstate()
    instead of math_state_restore().
    
    Note: "TODO: cleanup this horror" is still valid. We do not need
    init_fpu() at all, we only need fpu_alloc() and memset(0). But this
    needs other changes, in particular user_fpu_begin() should set
    used_math().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pekka Riikonen <priikone@iki.fi>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suresh Siddha <sbsiddha@gmail.com>
    Link: http://lkml.kernel.org/r/20150311173449.GE5032@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index dcaf4b00d0b4..6b058296a456 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -143,7 +143,8 @@ void flush_thread(void)
 		/* kthread execs. TODO: cleanup this horror. */
 		if (WARN_ON(init_fpu(current)))
 			force_sig(SIGKILL, current);
-		math_state_restore();
+		user_fpu_begin();
+		restore_init_xstate();
 	}
 }
 

commit eda2360ad18b7cde87728fad85c6735a52c2576e
Merge: 1d23c4518b1f bc465aa9d045
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Mar 23 10:13:36 2015 +0100

    Merge tag 'v4.0-rc5' into x86/fpu, to prevent conflicts
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d9e05cc5a53246e074dc2b84956252e4bbe392cd
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Tue Mar 10 11:05:59 2015 -0700

    x86/asm/entry: Unify and fix initial thread_struct::sp0 values
    
    x86_32 and x86_64 need slightly different thread_struct::sp0 values, and
    x86_32's was incorrect for init.
    
    This never mattered -- the init thread never runs user code, so we never
    used thread_struct::sp0 for anything.
    
    Fix it and mostly unify them.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1b810c1d2e797e27bb4a7708c426101161edd1f6.1426009661.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index f4c0af7fc3a0..12b1cf606ddf 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -39,7 +39,7 @@
  */
 __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
 	.x86_tss = {
-		.sp0 = (unsigned long)&init_stack + sizeof(init_stack),
+		.sp0 = TOP_OF_INIT_STACK,
 #ifdef CONFIG_X86_32
 		.ss0 = __KERNEL_DS,
 		.ss1 = __KERNEL_CS,

commit f8e617f4582995f7c25ef25b4167213120ad122b
Author: Mike Galbraith <bitbucket@online.de>
Date:   Sat Jan 18 17:14:44 2014 +0100

    sched/idle/x86: Optimize unnecessary mwait_idle() resched IPIs
    
    To fully take advantage of MWAIT, apparently the CLFLUSH instruction needs
    another quirk on certain CPUs: proper barriers around it on certain machines.
    
    On a Q6600 SMP system, pipe-test scheduling performance, cross core,
    improves significantly:
    
      3.8.13                   487.2 KHz    1.000
      3.13.0-master            415.5 KHz     .852
      3.13.0-master+           415.2 KHz     .852     + restore mwait_idle
      3.13.0-master++          488.5 KHz    1.002     + restore mwait_idle + IPI fix
    
    Since X86_BUG_CLFLUSH_MONITOR is already a quirk, don't create a separate
    quirk for the extra smp_mb()s.
    
    Signed-off-by: Mike Galbraith <bitbucket@online.de>
    Cc: <stable@vger.kernel.org> # 3.10+
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ian Malone <ibmalone@gmail.com>
    Cc: Josh Boyer <jwboyer@redhat.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1390061684.5566.4.camel@marge.simpson.net
    [ Ported to recent kernel, added comments about the quirk. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index da06f741d2a6..6ad8a6396b75 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -428,18 +428,22 @@ static int prefer_mwait_c1_over_halt(const struct cpuinfo_x86 *c)
 
 static void mwait_idle(void)
 {
-	if (!need_resched()) {
-		if (this_cpu_has(X86_BUG_CLFLUSH_MONITOR))
+	if (!current_set_polling_and_test()) {
+		if (this_cpu_has(X86_BUG_CLFLUSH_MONITOR)) {
+			smp_mb(); /* quirk */
 			clflush((void *)&current_thread_info()->flags);
+			smp_mb(); /* quirk */
+		}
 
 		__monitor((void *)&current_thread_info()->flags, 0, 0);
-		smp_mb();
 		if (!need_resched())
 			__sti_mwait(0, 0);
 		else
 			local_irq_enable();
-	} else
+	} else {
 		local_irq_enable();
+	}
+	__current_clr_polling();
 }
 
 void select_idle_routine(const struct cpuinfo_x86 *c)

commit b253149b843f89cd300cbdbea27ce1f847506f99
Author: Len Brown <len.brown@intel.com>
Date:   Wed Jan 15 00:37:34 2014 -0500

    sched/idle/x86: Restore mwait_idle() to fix boot hangs, to improve power savings and to improve performance
    
    In Linux-3.9 we removed the mwait_idle() loop:
    
      69fb3676df33 ("x86 idle: remove mwait_idle() and "idle=mwait" cmdline param")
    
    The reasoning was that modern machines should be sufficiently
    happy during the boot process using the default_idle() HALT
    loop, until cpuidle loads and either acpi_idle or intel_idle
    invoke the newer MWAIT-with-hints idle loop.
    
    But two machines reported problems:
    
     1. Certain Core2-era machines support MWAIT-C1 and HALT only.
        MWAIT-C1 is preferred for optimal power and performance.
        But if they support just C1, cpuidle never loads and
        so they use the boot-time default idle loop forever.
    
     2. Some laptops will boot-hang if HALT is used,
        but will boot successfully if MWAIT is used.
        This appears to be a hidden assumption in BIOS SMI,
        that is presumably valid on the proprietary OS
        where the BIOS was validated.
    
           https://bugzilla.kernel.org/show_bug.cgi?id=60770
    
    So here we effectively revert the patch above, restoring
    the mwait_idle() loop.  However, we don't bother restoring
    the idle=mwait cmdline parameter, since it appears to add
    no value.
    
    Maintainer notes:
    
      For 3.9, simply revert 69fb3676df
      for 3.10, patch -F3 applies, fuzz needed due to __cpuinit use in
      context For 3.11, 3.12, 3.13, this patch applies cleanly
    
    Tested-by: Mike Galbraith <bitbucket@online.de>
    Signed-off-by: Len Brown <len.brown@intel.com>
    Acked-by: Mike Galbraith <bitbucket@online.de>
    Cc: <stable@vger.kernel.org> # 3.9+
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ian Malone <ibmalone@gmail.com>
    Cc: Josh Boyer <jwboyer@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/345254a551eb5a6a866e048d7ab570fd2193aca4.1389763084.git.len.brown@intel.com
    [ Ported to recent kernels. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e127ddaa2d5a..da06f741d2a6 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -24,6 +24,7 @@
 #include <asm/syscalls.h>
 #include <asm/idle.h>
 #include <asm/uaccess.h>
+#include <asm/mwait.h>
 #include <asm/i387.h>
 #include <asm/fpu-internal.h>
 #include <asm/debugreg.h>
@@ -398,6 +399,49 @@ static void amd_e400_idle(void)
 		default_idle();
 }
 
+/*
+ * Intel Core2 and older machines prefer MWAIT over HALT for C1.
+ * We can't rely on cpuidle installing MWAIT, because it will not load
+ * on systems that support only C1 -- so the boot default must be MWAIT.
+ *
+ * Some AMD machines are the opposite, they depend on using HALT.
+ *
+ * So for default C1, which is used during boot until cpuidle loads,
+ * use MWAIT-C1 on Intel HW that has it, else use HALT.
+ */
+static int prefer_mwait_c1_over_halt(const struct cpuinfo_x86 *c)
+{
+	if (c->x86_vendor != X86_VENDOR_INTEL)
+		return 0;
+
+	if (!cpu_has(c, X86_FEATURE_MWAIT))
+		return 0;
+
+	return 1;
+}
+
+/*
+ * MONITOR/MWAIT with no hints, used for default default C1 state.
+ * This invokes MWAIT with interrutps enabled and no flags,
+ * which is backwards compatible with the original MWAIT implementation.
+ */
+
+static void mwait_idle(void)
+{
+	if (!need_resched()) {
+		if (this_cpu_has(X86_BUG_CLFLUSH_MONITOR))
+			clflush((void *)&current_thread_info()->flags);
+
+		__monitor((void *)&current_thread_info()->flags, 0, 0);
+		smp_mb();
+		if (!need_resched())
+			__sti_mwait(0, 0);
+		else
+			local_irq_enable();
+	} else
+		local_irq_enable();
+}
+
 void select_idle_routine(const struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
@@ -411,6 +455,9 @@ void select_idle_routine(const struct cpuinfo_x86 *c)
 		/* E400: APIC timer interrupt does not wake up CPU from C1e */
 		pr_info("using AMD E400 aware idle routine\n");
 		x86_idle = amd_e400_idle;
+	} else if (prefer_mwait_c1_over_halt(c)) {
+		pr_info("using mwait in idle threads\n");
+		x86_idle = mwait_idle;
 	} else
 		x86_idle = default_idle;
 }

commit d0a0de21f82bbc1737ea3c831f018d0c2bc6b9c2
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:06 2015 -0800

    x86/asm/entry: Remove INIT_TSS and fold the definitions into 'cpu_tss'
    
    The INIT_TSS is unnecessary.  Just define the initial TSS where
    'cpu_tss' is defined.
    
    While we're at it, merge the 32-bit and 64-bit definitions.  The
    only syntactic change is that 32-bit kernels were computing sp0
    as long, but now they compute it as unsigned long.
    
    Verified by objdump: the contents and relocations of
    .data..percpu..shared_aligned are unchanged on 32-bit and 64-bit
    kernels.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/8fc39fa3f6c5d635e93afbdd1a0fe0678a6d7913.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6f6087349231..f4c0af7fc3a0 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -37,7 +37,25 @@
  * section. Since TSS's are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
-__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = INIT_TSS;
+__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
+	.x86_tss = {
+		.sp0 = (unsigned long)&init_stack + sizeof(init_stack),
+#ifdef CONFIG_X86_32
+		.ss0 = __KERNEL_DS,
+		.ss1 = __KERNEL_CS,
+		.io_bitmap_base	= INVALID_IO_BITMAP_OFFSET,
+#endif
+	 },
+#ifdef CONFIG_X86_32
+	 /*
+	  * Note that the .io_bitmap member must be extra-big. This is because
+	  * the CPU will access an additional byte beyond the end of the IO
+	  * permission bitmap. The extra byte must be all 1 bits, and must
+	  * be within the limit.
+	  */
+	.io_bitmap		= { [0 ... IO_BITMAP_LONGS] = ~0 },
+#endif
+};
 EXPORT_PER_CPU_SYMBOL_GPL(cpu_tss);
 
 #ifdef CONFIG_X86_64

commit 24933b82c0d9a711475a5ef7904eb733f561e637
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:05 2015 -0800

    x86/asm/entry: Rename 'init_tss' to 'cpu_tss'
    
    It has nothing to do with init -- there's only one TSS per cpu.
    
    Other names considered include:
    
     - current_tss: Confusing because we never switch the tss.
     - singleton_tss: Too long.
    
    This patch was generated with 's/init_tss/cpu_tss/g'.  Followup
    patches will fix INIT_TSS and INIT_TSS_IST by hand.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/da29fb2a793e4f649d93ce2d1ed320ebe8516262.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ff5c9088b1c5..6f6087349231 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -37,8 +37,8 @@
  * section. Since TSS's are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
-__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
-EXPORT_PER_CPU_SYMBOL_GPL(init_tss);
+__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = INIT_TSS;
+EXPORT_PER_CPU_SYMBOL_GPL(cpu_tss);
 
 #ifdef CONFIG_X86_64
 static DEFINE_PER_CPU(unsigned char, is_idle);
@@ -110,7 +110,7 @@ void exit_thread(void)
 	unsigned long *bp = t->io_bitmap_ptr;
 
 	if (bp) {
-		struct tss_struct *tss = &per_cpu(init_tss, get_cpu());
+		struct tss_struct *tss = &per_cpu(cpu_tss, get_cpu());
 
 		t->io_bitmap_ptr = NULL;
 		clear_thread_flag(TIF_IO_BITMAP);

commit 8ef46a672a7d852709561d10672b6eaa8a4acd82
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:02 2015 -0800

    x86/asm/entry: Add this_cpu_sp0() to read sp0 for the current cpu
    
    We currently store references to the top of the kernel stack in
    multiple places: kernel_stack (with an offset) and
    init_tss.x86_tss.sp0 (no offset).  The latter is defined by
    hardware and is a clean canonical way to find the top of the
    stack.  Add an accessor so we can start using it.
    
    This needs minor paravirt tweaks.  On native, sp0 defines the
    top of the kernel stack and is therefore always correct.  On Xen
    and lguest, the hypervisor tracks the top of the stack, but we
    want to start reading sp0 in the kernel.  Fixing this is simple:
    just update our local copy of sp0 as well as the hypervisor's
    copy on task switches.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/8d675581859712bee09a055ed8f785d80dac1eca.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 046e2d620bbe..ff5c9088b1c5 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -38,6 +38,7 @@
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
 __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
+EXPORT_PER_CPU_SYMBOL_GPL(init_tss);
 
 #ifdef CONFIG_X86_64
 static DEFINE_PER_CPU(unsigned char, is_idle);

commit 110d7f7513bbb916b8654da9e2973ac5bed929a9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jan 19 19:52:12 2015 +0100

    x86/fpu: Don't abuse FPU in kernel threads if use_eager_fpu()
    
    AFAICS, there is no reason why kernel threads should have FPU context
    even if use_eager_fpu() == T. Now that interrupted_kernel_fpu_idle()
    does not check __thread_has_fpu() in the use_eager_fpu() case, we
    can remove the init_fpu() code from eager_fpu_init() and change
    flush_thread() called by do_execve() to initialize FPU.
    
    Note: of course, the change in flush_thread() is horrible and must be
    cleanuped. We need the new helper, and flush_thread() should return the
    error if init_fpu() fails.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Suresh Siddha <sbsiddha@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/r/20150119185212.GD16427@redhat.com
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ce8b10351e28..83480373a642 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -130,6 +130,7 @@ void flush_thread(void)
 
 	flush_ptrace_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
+
 	drop_init_fpu(tsk);
 	/*
 	 * Free the FPU state for non xsave platforms. They get reallocated
@@ -137,6 +138,12 @@ void flush_thread(void)
 	 */
 	if (!use_eager_fpu())
 		free_thread_xstate(tsk);
+	else if (!used_math()) {
+		/* kthread execs. TODO: cleanup this horror. */
+		if (WARN_ON(init_fpu(current)))
+			force_sig(SIGKILL, current);
+		math_state_restore();
+	}
 }
 
 static void hard_disable_TSC(void)

commit 6a5fe8952bd676baf382d14df21e7b32b5d8943e
Author: Rik van Riel <riel@redhat.com>
Date:   Fri Feb 6 15:02:04 2015 -0500

    x86/fpu: Use task_disable_lazy_fpu_restore() helper
    
    Replace magic assignments of fpu.last_cpu = ~0 with more explicit
    task_disable_lazy_fpu_restore() calls.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1423252925-14451-8-git-send-email-riel@redhat.com
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e127ddaa2d5a..ce8b10351e28 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -68,8 +68,8 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 
 	dst->thread.fpu_counter = 0;
 	dst->thread.fpu.has_fpu = 0;
-	dst->thread.fpu.last_cpu = ~0;
 	dst->thread.fpu.state = NULL;
+	task_disable_lazy_fpu_restore(dst);
 	if (tsk_used_math(src)) {
 		int err = fpu_alloc(&dst->thread.fpu);
 		if (err)

commit 375074cc736ab1d89a708c0a8d7baa4a70d5d476
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Oct 24 15:58:07 2014 -0700

    x86: Clean up cr4 manipulation
    
    CR4 manipulation was split, seemingly at random, between direct
    (write_cr4) and using a helper (set/clear_in_cr4).  Unfortunately,
    the set_in_cr4 and clear_in_cr4 helpers also poke at the boot code,
    which only a small subset of users actually wanted.
    
    This patch replaces all cr4 access in functions that don't leave cr4
    exactly the way they found it with new helpers cr4_set_bits,
    cr4_clear_bits, and cr4_set_bits_and_update_boot.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Vince Weaver <vince@deater.net>
    Cc: "hillf.zj" <hillf.zj@alibaba-inc.com>
    Cc: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/495a10bdc9e67016b8fd3945700d46cfd5c12c2f.1414190806.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e127ddaa2d5a..046e2d620bbe 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -28,6 +28,7 @@
 #include <asm/fpu-internal.h>
 #include <asm/debugreg.h>
 #include <asm/nmi.h>
+#include <asm/tlbflush.h>
 
 /*
  * per-CPU TSS segments. Threads are completely 'soft' on Linux,
@@ -141,7 +142,7 @@ void flush_thread(void)
 
 static void hard_disable_TSC(void)
 {
-	write_cr4(read_cr4() | X86_CR4_TSD);
+	cr4_set_bits(X86_CR4_TSD);
 }
 
 void disable_TSC(void)
@@ -158,7 +159,7 @@ void disable_TSC(void)
 
 static void hard_enable_TSC(void)
 {
-	write_cr4(read_cr4() & ~X86_CR4_TSD);
+	cr4_clear_bits(X86_CR4_TSD);
 }
 
 static void enable_TSC(void)

commit dc56c0f9b870fba7a4eef2bb463db6881284152b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Sep 2 19:57:30 2014 +0200

    x86, fpu: Shift "fpu_counter = 0" from copy_thread() to arch_dup_task_struct()
    
    Cosmetic, but I think thread.fpu_counter should be initialized in
    arch_dup_task_struct() too, along with other "fpu" variables. And
    probably it make sense to turn it into thread.fpu->counter.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20140902175730.GA21669@redhat.com
    Reviewed-by: Suresh Siddha <sbsiddha@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index a44268c897bd..e127ddaa2d5a 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -66,6 +66,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	*dst = *src;
 
+	dst->thread.fpu_counter = 0;
 	dst->thread.fpu.has_fpu = 0;
 	dst->thread.fpu.last_cpu = ~0;
 	dst->thread.fpu.state = NULL;

commit 5e23fee23ea10730c752edce1777e6b7e727290f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Sep 2 19:57:27 2014 +0200

    x86, fpu: copy_process: Sanitize fpu->last_cpu initialization
    
    Cosmetic, but imho memset(&dst->thread.fpu, 0) is not good simply
    because it hides the (important) usage of ->has_fpu/etc from grep.
    Change this code to initialize the members explicitly.
    
    And note that ->last_cpu = 0 looks simply wrong, this can confuse
    fpu_lazy_restore() if per_cpu(fpu_owner_task, 0) has already exited
    and copy_process() re-allocated the same task_struct. Fortunately
    this is not actually possible because child->fpu_counter == 0 and
    thus fpu_lazy_restore() will not be called, but still this is not
    clean/robust.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20140902175727.GA21666@redhat.com
    Reviewed-by: Suresh Siddha <sbsiddha@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index b9ba9d52020e..a44268c897bd 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -66,7 +66,9 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	*dst = *src;
 
-	memset(&dst->thread.fpu, 0, sizeof(dst->thread.fpu));
+	dst->thread.fpu.has_fpu = 0;
+	dst->thread.fpu.last_cpu = ~0;
+	dst->thread.fpu.state = NULL;
 	if (tsk_used_math(src)) {
 		int err = fpu_alloc(&dst->thread.fpu);
 		if (err)

commit f1853505d9ca1c3ea27c29cf83c24661531c527b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Sep 2 19:57:23 2014 +0200

    x86, fpu: copy_process: Avoid fpu_alloc/copy if !used_math()
    
    arch_dup_task_struct() copies thread.fpu if fpu_allocated(), this
    looks suboptimal and misleading. Say, a forking process could use
    FPU only once in a signal handler but now tsk_used_math(src) == F,
    in this case the child gets a copy of fpu->state for no reason. The
    child won't use the saved registers anyway even if it starts to use
    FPU, this can only avoid fpu_alloc() in do_device_not_available().
    
    Change this code to check tsk_used_math(current) instead. We still
    need to clear fpu->has_fpu/state, we could do this memset(0) under
    fpu_allocated() check but I think this doesn't make sense. See also
    the next change.
    
    use_eager_fpu() assumes that fpu_allocated() is always true, but a
    forking task (and thus its child) must always have PF_USED_MATH set,
    otherwise the child can either use FPU without used_math() (note that
    switch_fpu_prepare() doesn't do stts() in this case), or it will be
    killed by do_device_not_available()->BUG_ON(use_eager_fpu).
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20140902175723.GA21659@redhat.com
    Reviewed-by: Suresh Siddha <sbsiddha@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index f804dc935d2a..b9ba9d52020e 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -64,14 +64,13 @@ EXPORT_SYMBOL_GPL(task_xstate_cachep);
  */
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
-	int ret;
-
 	*dst = *src;
-	if (fpu_allocated(&src->thread.fpu)) {
-		memset(&dst->thread.fpu, 0, sizeof(dst->thread.fpu));
-		ret = fpu_alloc(&dst->thread.fpu);
-		if (ret)
-			return ret;
+
+	memset(&dst->thread.fpu, 0, sizeof(dst->thread.fpu));
+	if (tsk_used_math(src)) {
+		int err = fpu_alloc(&dst->thread.fpu);
+		if (err)
+			return err;
 		fpu_copy(dst, src);
 	}
 	return 0;

commit 7496d6458fe3219d63848ce4a9afbd86245cab22
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Thu May 29 11:12:44 2014 -0700

    Define kernel API to get address of each state in xsave area
    
    In standard form, each state is saved in the xsave area in fixed offset.
    But in compacted form, offset of each saved state only can be calculated during
    run time because some xstates may not be enabled and saved.
    
    We define kernel API get_xsave_addr() returns address of a given state saved in a xsave area.
    
    It can be called in kernel to get address of each xstate in xsave area in
    either standard format or compacted format.
    
    It's useful when kernel wants to directly access each state in xsave area.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1401387164-43416-17-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 4505e2a950d8..f804dc935d2a 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -93,6 +93,7 @@ void arch_task_cache_init(void)
         	kmem_cache_create("task_xstate", xstate_size,
 				  __alignof__(union thread_xstate),
 				  SLAB_PANIC | SLAB_NOTRACK, NULL);
+	setup_xstate_comp();
 }
 
 /*

commit 16f8b05abe33575b5fc0833cab1286bd6227f255
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Jan 29 12:45:12 2014 -0500

    sched/idle, x86: Remove redundant cpuidle_idle_call()
    
    The core idle loop now takes care of it.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-sh@vger.kernel.org
    Cc: linux-pm@vger.kernel.org
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: linaro-kernel@lists.linaro.org
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-3ioazimg4j5iq6kdefks04i8@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 3fb8d95ab8b5..4505e2a950d8 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -298,10 +298,7 @@ void arch_cpu_idle_dead(void)
  */
 void arch_cpu_idle(void)
 {
-	if (cpuidle_idle_call())
-		x86_idle();
-	else
-		local_irq_enable();
+	x86_idle();
 }
 
 /*

commit ea8117478918a4734586d35ff530721b682425be
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 11 12:43:13 2013 +0200

    sched, idle: Fix the idle polling state logic
    
    Mike reported that commit 7d1a9417 ("x86: Use generic idle loop")
    regressed several workloads and caused excessive reschedule
    interrupts.
    
    The patch in question failed to notice that the x86 code had an
    inverted sense of the polling state versus the new generic code (x86:
    default polling, generic: default !polling).
    
    Fix the two prominent x86 mwait based idle drivers and introduce a few
    new generic polling helpers (fixing the wrong smp_mb__after_clear_bit
    usage).
    
    Also switch the idle routines to using tif_need_resched() which is an
    immediate TIF_NEED_RESCHED test as opposed to need_resched which will
    end up being slightly different.
    
    Reported-by: Mike Galbraith <bitbucket@online.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: lenb@kernel.org
    Cc: tglx@linutronix.de
    Link: http://lkml.kernel.org/n/tip-nc03imb0etuefmzybzj7sprf@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c83516be1052..3fb8d95ab8b5 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -391,9 +391,9 @@ static void amd_e400_idle(void)
 		 * The switch back from broadcast mode needs to be
 		 * called with interrupts disabled.
 		 */
-		 local_irq_disable();
-		 clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &cpu);
-		 local_irq_enable();
+		local_irq_disable();
+		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &cpu);
+		local_irq_enable();
 	} else
 		default_idle();
 }

commit 277d5b40b7bf495d2d4193746181b17dd98441b2
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Aug 5 15:02:43 2013 -0700

    x86, asmlinkage: Make several variables used from assembler/linker script visible
    
    Plus one function, load_gs_index().
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1375740170-7446-10-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 83369e5a1d27..c83516be1052 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -36,7 +36,7 @@
  * section. Since TSS's are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
-DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
+__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
 
 #ifdef CONFIG_X86_64
 static DEFINE_PER_CPU(unsigned char, is_idle);

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 81a5f5e8f142..83369e5a1d27 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -398,7 +398,7 @@ static void amd_e400_idle(void)
 		default_idle();
 }
 
-void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
+void select_idle_routine(const struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
 	if (boot_option_idle_override == IDLE_POLL && smp_num_siblings > 1)

commit d7880812b3594d3c6dcbe3cfd71dabb17347d082
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 10 16:52:03 2013 +0200

    idle: Add the stack canary init to cpu_startup_entry()
    
    Moving x86 to the generic idle implementation (commit 7d1a9417 "x86:
    Use generic idle loop") wreckaged the stack protector.
    
    I stupidly missed that boot_init_stack_canary() must be inlined from a
    function which never returns, but I put that call into
    arch_cpu_idle_prepare() which of course returns.
    
    I pondered to play tricks with arch_cpu_idle_prepare() first, but then
    I noticed, that the other archs which have implemented the
    stackprotector (ARM and SH) do not initialize the canary for the
    non-boot cpus.
    
    So I decided to move the boot_init_stack_canary() call into
    cpu_startup_entry() ifdeffed with an CONFIG_X86 for now. This #ifdef
    is just a temporary measure as I don't want to inflict the
    boot_init_stack_canary() call on ARM and SH that late in the cycle.
    
    I'll queue a patch for 3.11 which removes the #ifdef if the ARM/SH
    maintainers have no objection.
    
    Reported-by: Wouter van Kesteren <woutershep@gmail.com>
    Cc: x86@kernel.org
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 4e7a37ff03ab..81a5f5e8f142 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -277,18 +277,6 @@ void exit_idle(void)
 }
 #endif
 
-void arch_cpu_idle_prepare(void)
-{
-	/*
-	 * If we're the non-boot CPU, nothing set the stack canary up
-	 * for us.  CPU0 already has it initialized but no harm in
-	 * doing it again.  This is a good place for updating it, as
-	 * we wont ever return from this function (so the invalid
-	 * canaries already on the stack wont ever trigger).
-	 */
-	boot_init_stack_canary();
-}
-
 void arch_cpu_idle_enter(void)
 {
 	local_touch_nmi();

commit 97a5b81fa4d3a11dcdf224befc577f2e0abadc0b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 2 16:21:15 2013 +0200

    x86: Fix idle consolidation fallout
    
    The core code expects the arch idle code to return with interrupts
    enabled. The conversion missed two x86 cases which fail to do that.
    
    Reported-and-tested-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Tested-by: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1305021557030.3972@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 607af0d4d5ef..4e7a37ff03ab 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -312,6 +312,8 @@ void arch_cpu_idle(void)
 {
 	if (cpuidle_idle_call())
 		x86_idle();
+	else
+		local_irq_enable();
 }
 
 /*
@@ -368,9 +370,6 @@ void amd_e400_remove_cpu(int cpu)
  */
 static void amd_e400_idle(void)
 {
-	if (need_resched())
-		return;
-
 	if (!amd_e400_c1e_detected) {
 		u32 lo, hi;
 

commit a43cb95d547a061ed5bf1acb28e0f5fd575e26c1
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 30 15:27:17 2013 -0700

    dump_stack: unify debug information printed by show_regs()
    
    show_regs() is inherently arch-dependent but it does make sense to print
    generic debug information and some archs already do albeit in slightly
    different forms.  This patch introduces a generic function to print debug
    information from show_regs() so that different archs print out the same
    information and it's much easier to modify what's printed.
    
    show_regs_print_info() prints out the same debug info as dump_stack()
    does plus task and thread_info pointers.
    
    * Archs which didn't print debug info now do.
    
      alpha, arc, blackfin, c6x, cris, frv, h8300, hexagon, ia64, m32r,
      metag, microblaze, mn10300, openrisc, parisc, score, sh64, sparc,
      um, xtensa
    
    * Already prints debug info.  Replaced with show_regs_print_info().
      The printed information is superset of what used to be there.
    
      arm, arm64, avr32, mips, powerpc, sh32, tile, unicore32, x86
    
    * s390 is special in that it used to print arch-specific information
      along with generic debug info.  Heiko and Martin think that the
      arch-specific extra isn't worth keeping s390 specfic implementation.
      Converted to use the generic version.
    
    Note that now all archs print the debug info before actual register
    dumps.
    
    An example BUG() dump follows.
    
     kernel BUG at /work/os/work/kernel/workqueue.c:4841!
     invalid opcode: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC
     Modules linked in:
     CPU: 0 PID: 1 Comm: swapper/0 Not tainted 3.9.0-rc1-work+ #7
     Hardware name: empty empty/S3992, BIOS 080011  10/26/2007
     task: ffff88007c85e040 ti: ffff88007c860000 task.ti: ffff88007c860000
     RIP: 0010:[<ffffffff8234a07e>]  [<ffffffff8234a07e>] init_workqueues+0x4/0x6
     RSP: 0000:ffff88007c861ec8  EFLAGS: 00010246
     RAX: ffff88007c861fd8 RBX: ffffffff824466a8 RCX: 0000000000000001
     RDX: 0000000000000046 RSI: 0000000000000001 RDI: ffffffff8234a07a
     RBP: ffff88007c861ec8 R08: 0000000000000000 R09: 0000000000000000
     R10: 0000000000000001 R11: 0000000000000000 R12: ffffffff8234a07a
     R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
     FS:  0000000000000000(0000) GS:ffff88007dc00000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
     CR2: ffff88015f7ff000 CR3: 00000000021f1000 CR4: 00000000000007f0
     DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
     DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
     Stack:
      ffff88007c861ef8 ffffffff81000312 ffffffff824466a8 ffff88007c85e650
      0000000000000003 0000000000000000 ffff88007c861f38 ffffffff82335e5d
      ffff88007c862080 ffffffff8223d8c0 ffff88007c862080 ffffffff81c47760
     Call Trace:
      [<ffffffff81000312>] do_one_initcall+0x122/0x170
      [<ffffffff82335e5d>] kernel_init_freeable+0x9b/0x1c8
      [<ffffffff81c47760>] ? rest_init+0x140/0x140
      [<ffffffff81c4776e>] kernel_init+0xe/0xf0
      [<ffffffff81c6be9c>] ret_from_fork+0x7c/0xb0
      [<ffffffff81c47760>] ? rest_init+0x140/0x140
      ...
    
    v2: Typo fix in x86-32.
    
    v3: CPU number dropped from show_regs_print_info() as
        dump_stack_print_info() has been updated to print it.  s390
        specific implementation dropped as requested by s390 maintainers.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Acked-by: Chris Metcalf <cmetcalf@tilera.com>           [tile bits]
    Acked-by: Richard Kuo <rkuo@codeaurora.org>             [hexagon bits]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 14fcf55a5c5b..607af0d4d5ef 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -121,30 +121,6 @@ void exit_thread(void)
 	drop_fpu(me);
 }
 
-void show_regs_common(void)
-{
-	const char *vendor, *product, *board;
-
-	vendor = dmi_get_system_info(DMI_SYS_VENDOR);
-	if (!vendor)
-		vendor = "";
-	product = dmi_get_system_info(DMI_PRODUCT_NAME);
-	if (!product)
-		product = "";
-
-	/* Board Name is optional */
-	board = dmi_get_system_info(DMI_BOARD_NAME);
-
-	printk(KERN_DEFAULT "Pid: %d, comm: %.20s %s %s %.*s %s %s%s%s\n",
-	       current->pid, current->comm, print_tainted(),
-	       init_utsname()->release,
-	       (int)strcspn(init_utsname()->version, " "),
-	       init_utsname()->version,
-	       vendor, product,
-	       board ? "/" : "",
-	       board ? board : "");
-}
-
 void flush_thread(void)
 {
 	struct task_struct *tsk = current;

commit df8edfa9af5b2160549ed1a79b72e3ed13b6c7e2
Merge: 874f6d1be769 1077c932db63
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 08:34:38 2013 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpuid changes from Ingo Molnar:
     "The biggest change is x86 CPU bug handling refactoring and cleanups,
      by Borislav Petkov"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, CPU, AMD: Drop useless label
      x86, AMD: Correct {rd,wr}msr_amd_safe warnings
      x86: Fold-in trivial check_config function
      x86, cpu: Convert AMD Erratum 400
      x86, cpu: Convert AMD Erratum 383
      x86, cpu: Convert Cyrix coma bug detection
      x86, cpu: Convert FDIV bug detection
      x86, cpu: Convert F00F bug detection
      x86, cpu: Expand cpufeature facility to include cpu bugs

commit 7d1a941731fabf27e5fb6edbebb79fe856edb4e5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 22:50:03 2013 +0100

    x86: Use generic idle loop
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130321215235.486594473@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 14ae10031ff0..6833bffaadb7 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -301,13 +301,7 @@ void exit_idle(void)
 }
 #endif
 
-/*
- * The idle thread. There's no useful work to be
- * done, so just try to conserve power and have a
- * low exit latency (ie sit in a loop waiting for
- * somebody to say that they'd like to reschedule)
- */
-void cpu_idle(void)
+void arch_cpu_idle_prepare(void)
 {
 	/*
 	 * If we're the non-boot CPU, nothing set the stack canary up
@@ -317,71 +311,40 @@ void cpu_idle(void)
 	 * canaries already on the stack wont ever trigger).
 	 */
 	boot_init_stack_canary();
-	current_thread_info()->status |= TS_POLLING;
-
-	while (1) {
-		tick_nohz_idle_enter();
-
-		while (!need_resched()) {
-			rmb();
-
-			if (cpu_is_offline(smp_processor_id()))
-				play_dead();
-
-			/*
-			 * Idle routines should keep interrupts disabled
-			 * from here on, until they go to idle.
-			 * Otherwise, idle callbacks can misfire.
-			 */
-			local_touch_nmi();
-			local_irq_disable();
-
-			enter_idle();
-
-			/* Don't trace irqs off for idle */
-			stop_critical_timings();
-
-			/* enter_idle() needs rcu for notifiers */
-			rcu_idle_enter();
+}
 
-			if (cpuidle_idle_call())
-				x86_idle();
+void arch_cpu_idle_enter(void)
+{
+	local_touch_nmi();
+	enter_idle();
+}
 
-			rcu_idle_exit();
-			start_critical_timings();
+void arch_cpu_idle_exit(void)
+{
+	__exit_idle();
+}
 
-			/* In many cases the interrupt that ended idle
-			   has already called exit_idle. But some idle
-			   loops can be woken up without interrupt. */
-			__exit_idle();
-		}
+void arch_cpu_idle_dead(void)
+{
+	play_dead();
+}
 
-		tick_nohz_idle_exit();
-		preempt_enable_no_resched();
-		schedule();
-		preempt_disable();
-	}
+/*
+ * Called from the generic idle code.
+ */
+void arch_cpu_idle(void)
+{
+	if (cpuidle_idle_call())
+		x86_idle();
 }
 
 /*
- * We use this if we don't have any better
- * idle routine..
+ * We use this if we don't have any better idle routine..
  */
 void default_idle(void)
 {
 	trace_cpu_idle_rcuidle(1, smp_processor_id());
-	current_thread_info()->status &= ~TS_POLLING;
-	/*
-	 * TS_POLLING-cleared state must be visible before we
-	 * test NEED_RESCHED:
-	 */
-	smp_mb();
-
-	if (!need_resched())
-		safe_halt();	/* enables interrupts racelessly */
-	else
-		local_irq_enable();
-	current_thread_info()->status |= TS_POLLING;
+	safe_halt();
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
 #ifdef CONFIG_APM_MODULE
@@ -411,20 +374,6 @@ void stop_this_cpu(void *dummy)
 		halt();
 }
 
-/*
- * On SMP it's slightly faster (but much more power-consuming!)
- * to poll the ->work.need_resched flag instead of waiting for the
- * cross-CPU IPI to arrive. Use this option with caution.
- */
-static void poll_idle(void)
-{
-	trace_cpu_idle_rcuidle(0, smp_processor_id());
-	local_irq_enable();
-	while (!need_resched())
-		cpu_relax();
-	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
-}
-
 bool amd_e400_c1e_detected;
 EXPORT_SYMBOL(amd_e400_c1e_detected);
 
@@ -489,10 +438,10 @@ static void amd_e400_idle(void)
 void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
-	if (x86_idle == poll_idle && smp_num_siblings > 1)
+	if (boot_option_idle_override == IDLE_POLL && smp_num_siblings > 1)
 		pr_warn_once("WARNING: polling idle and HT enabled, performance may degrade\n");
 #endif
-	if (x86_idle)
+	if (x86_idle || boot_option_idle_override == IDLE_POLL)
 		return;
 
 	if (cpu_has_amd_erratum(amd_erratum_400)) {
@@ -517,8 +466,8 @@ static int __init idle_setup(char *str)
 
 	if (!strcmp(str, "poll")) {
 		pr_info("using polling idle threads\n");
-		x86_idle = poll_idle;
 		boot_option_idle_override = IDLE_POLL;
+		cpu_idle_poll_ctrl(true);
 	} else if (!strcmp(str, "halt")) {
 		/*
 		 * When the boot option of idle=halt is added, halt is

commit 7d7dc116e56c8a1ba4beb36d06a77a48fe5f750b
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 20 15:07:28 2013 +0100

    x86, cpu: Convert AMD Erratum 400
    
    Convert AMD erratum 400 to the bug infrastructure. Then, retract all
    exports for modules since they're not needed now and make the AMD
    erratum checking machinery local to amd.c. Use forward declarations to
    avoid shuffling too much code around needlessly.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1363788448-31325-7-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 14ae10031ff0..e718f150c880 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -495,7 +495,7 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 	if (x86_idle)
 		return;
 
-	if (cpu_has_amd_erratum(amd_erratum_400)) {
+	if (cpu_has_bug(c, X86_BUG_AMD_APIC_C1E)) {
 		/* E400: APIC timer interrupt does not wake up CPU from C1e */
 		pr_info("using AMD E400 aware idle routine\n");
 		x86_idle = amd_e400_idle;

commit 10baf04e95fbf7eb6089410220a547211dd2ffa7
Merge: fdbe0946d4c3 ca62cf59ceef
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Feb 18 22:34:11 2013 +0100

    Merge branch 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux
    
    * 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux: (35 commits)
      PM idle: remove global declaration of pm_idle
      unicore32 idle: delete stray pm_idle comment
      openrisc idle: delete pm_idle
      mn10300 idle: delete pm_idle
      microblaze idle: delete pm_idle
      m32r idle: delete pm_idle, and other dead idle code
      ia64 idle: delete pm_idle
      cris idle: delete idle and pm_idle
      ARM64 idle: delete pm_idle
      ARM idle: delete pm_idle
      blackfin idle: delete pm_idle
      sparc idle: rename pm_idle to sparc_idle
      sh idle: rename global pm_idle to static sh_idle
      x86 idle: rename global pm_idle to static x86_idle
      APM idle: register apm_cpu_idle via cpuidle
      tools/power turbostat: display SMI count by default
      intel_idle: export both C1 and C1E
      cpuidle: remove vestage definition of cpuidle_state_usage.driver_data
      x86 idle: remove 32-bit-only "no-hlt" parameter, hlt_works_ok flag
      x86 idle: remove mwait_idle() and "idle=mwait" cmdline param
      ...
    
    Conflicts:
            arch/x86/kernel/process.c (with PM / tracing commit 43720bd)
            drivers/acpi/processor_idle.c (with ACPICA commit 4f84291)

commit ca62cf59ceef10ff2ebca0e7f764507186870270
Merge: 2e7d0f60d8b9 27be45700021
Author: Len Brown <len.brown@intel.com>
Date:   Mon Feb 18 00:25:53 2013 -0500

    Merge branch 'misc' into release
    
    Conflicts:
            arch/x86/kernel/process.c
    
    Signed-off-by: Len Brown <len.brown@intel.com>

commit a476bda30baf7efa7f305793a340aae07b6e5780
Author: Len Brown <len.brown@intel.com>
Date:   Sat Feb 9 21:45:03 2013 -0500

    x86 idle: rename global pm_idle to static x86_idle
    
    (pm_idle)() is being removed from linux/pm.h
    because Linux does not have such a cross-architecture concept.
    
    x86 uses an idle function pointer in its architecture
    specific code as a backup to cpuidle.  So we re-name
    x86 use of pm_idle to x86_idle, and make it static to x86.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index f571a6e08710..ceb05db59be1 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -268,10 +268,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 unsigned long boot_option_idle_override = IDLE_NO_OVERRIDE;
 EXPORT_SYMBOL(boot_option_idle_override);
 
-/*
- * Powermanagement idle function, if any..
- */
-void (*pm_idle)(void);
+static void (*x86_idle)(void);
 
 #ifndef CONFIG_SMP
 static inline void play_dead(void)
@@ -348,7 +345,7 @@ void cpu_idle(void)
 			rcu_idle_enter();
 
 			if (cpuidle_idle_call())
-				pm_idle();
+				x86_idle();
 
 			rcu_idle_exit();
 			start_critical_timings();
@@ -395,9 +392,9 @@ EXPORT_SYMBOL(default_idle);
 
 bool set_pm_idle_to_default(void)
 {
-	bool ret = !!pm_idle;
+	bool ret = !!x86_idle;
 
-	pm_idle = default_idle;
+	x86_idle = default_idle;
 
 	return ret;
 }
@@ -564,11 +561,10 @@ static void amd_e400_idle(void)
 void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
-	if (pm_idle == poll_idle && smp_num_siblings > 1) {
+	if (x86_idle == poll_idle && smp_num_siblings > 1)
 		pr_warn_once("WARNING: polling idle and HT enabled, performance may degrade\n");
-	}
 #endif
-	if (pm_idle)
+	if (x86_idle)
 		return;
 
 	if (cpu_has(c, X86_FEATURE_MWAIT) && mwait_usable(c)) {
@@ -576,19 +572,19 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 		 * One CPU supports mwait => All CPUs supports mwait
 		 */
 		pr_info("using mwait in idle threads\n");
-		pm_idle = mwait_idle;
+		x86_idle = mwait_idle;
 	} else if (cpu_has_amd_erratum(amd_erratum_400)) {
 		/* E400: APIC timer interrupt does not wake up CPU from C1e */
 		pr_info("using AMD E400 aware idle routine\n");
-		pm_idle = amd_e400_idle;
+		x86_idle = amd_e400_idle;
 	} else
-		pm_idle = default_idle;
+		x86_idle = default_idle;
 }
 
 void __init init_amd_e400_c1e_mask(void)
 {
 	/* If we're using amd_e400_idle, we need to allocate amd_e400_c1e_mask. */
-	if (pm_idle == amd_e400_idle)
+	if (x86_idle == amd_e400_idle)
 		zalloc_cpumask_var(&amd_e400_c1e_mask, GFP_KERNEL);
 }
 
@@ -599,7 +595,7 @@ static int __init idle_setup(char *str)
 
 	if (!strcmp(str, "poll")) {
 		pr_info("using polling idle threads\n");
-		pm_idle = poll_idle;
+		x86_idle = poll_idle;
 		boot_option_idle_override = IDLE_POLL;
 	} else if (!strcmp(str, "mwait")) {
 		boot_option_idle_override = IDLE_FORCE_MWAIT;
@@ -612,7 +608,7 @@ static int __init idle_setup(char *str)
 		 * To continue to load the CPU idle driver, don't touch
 		 * the boot_option_idle_override.
 		 */
-		pm_idle = default_idle;
+		x86_idle = default_idle;
 		boot_option_idle_override = IDLE_HALT;
 	} else if (!strcmp(str, "nomwait")) {
 		/*

commit dd8af076262cc1ff85a8d5e0c5b1a4716d19fe25
Author: Len Brown <len.brown@intel.com>
Date:   Sat Feb 9 21:10:04 2013 -0500

    APM idle: register apm_cpu_idle via cpuidle
    
    Update APM to register its local idle routine with cpuidle.
    
    This allows us to stop exporting pm_idle to modules on x86.
    
    The Kconfig sub-option, APM_CPU_IDLE, now depends on on CPU_IDLE.
    
    Compile-tested only.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Reviewed-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 2ed787f15bf0..f571a6e08710 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -272,9 +272,6 @@ EXPORT_SYMBOL(boot_option_idle_override);
  * Powermanagement idle function, if any..
  */
 void (*pm_idle)(void);
-#ifdef CONFIG_APM_MODULE
-EXPORT_SYMBOL(pm_idle);
-#endif
 
 #ifndef CONFIG_SMP
 static inline void play_dead(void)

commit 27be457000211a6903968dfce06d5f73f051a217
Author: Len Brown <len.brown@intel.com>
Date:   Sun Feb 10 02:28:46 2013 -0500

    x86 idle: remove 32-bit-only "no-hlt" parameter, hlt_works_ok flag
    
    Remove 32-bit x86 a cmdline param "no-hlt",
    and the cpuinfo_x86.hlt_works_ok that it sets.
    
    If a user wants to avoid HLT, then "idle=poll"
    is much more useful, as it avoids invocation of HLT
    in idle, while "no-hlt" failed to do so.
    
    Indeed, hlt_works_ok was consulted in only 3 places.
    
    First, in /proc/cpuinfo where "hlt_bug yes"
    would be printed if and only if the user booted
    the system with "no-hlt" -- as there was no other code
    to set that flag.
    
    Second, check_hlt() would not invoke halt() if "no-hlt"
    were on the cmdline.
    
    Third, it was consulted in stop_this_cpu(), which is invoked
    by native_machine_halt()/reboot_interrupt()/smp_stop_nmi_callback() --
    all cases where the machine is being shutdown/reset.
    The flag was not consulted in the more frequently invoked
    play_dead()/hlt_play_dead() used in processor offline and suspend.
    
    Since Linux-3.0 there has been a run-time notice upon "no-hlt" invocations
    indicating that it would be removed in 2012.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index cd5a4c9ef835..aef852eac292 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -415,10 +415,8 @@ void stop_this_cpu(void *dummy)
 	set_cpu_online(smp_processor_id(), false);
 	disable_local_APIC();
 
-	for (;;) {
-		if (hlt_works(smp_processor_id()))
-			halt();
-	}
+	for (;;)
+		halt();
 }
 
 /*

commit 69fb3676df3329a7142803bb3502fa59dc0db2e3
Author: Len Brown <len.brown@intel.com>
Date:   Sun Feb 10 01:38:39 2013 -0500

    x86 idle: remove mwait_idle() and "idle=mwait" cmdline param
    
    mwait_idle() is a C1-only idle loop intended to be more efficient
    than HLT, starting on Pentium-4 HT-enabled processors.
    
    But mwait_idle() has been replaced by the more general
    mwait_idle_with_hints(), which handles both C1 and deeper C-states.
    ACPI processor_idle and intel_idle use only mwait_idle_with_hints(),
    and no longer use mwait_idle().
    
    Here we simplify the x86 native idle code by removing mwait_idle(),
    and the "idle=mwait" bootparam used to invoke it.
    
    Since Linux 3.0 there has been a boot-time warning when "idle=mwait"
    was invoked saying it would be removed in 2012.  This removal
    was also noted in the (now removed:-) feature-removal-schedule.txt.
    
    After this change, kernels configured with
    (CONFIG_ACPI=n && CONFIG_INTEL_IDLE=n) when run on hardware
    that supports MWAIT will simply use HLT.  If MWAIT is desired
    on those systems, cpuidle and the cpuidle drivers above
    can be enabled.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7ed9f6b08ba0..cd5a4c9ef835 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -421,27 +421,6 @@ void stop_this_cpu(void *dummy)
 	}
 }
 
-/* Default MONITOR/MWAIT with no hints, used for default C1 state */
-static void mwait_idle(void)
-{
-	if (!need_resched()) {
-		trace_power_start_rcuidle(POWER_CSTATE, 1, smp_processor_id());
-		trace_cpu_idle_rcuidle(1, smp_processor_id());
-		if (this_cpu_has(X86_FEATURE_CLFLUSH_MONITOR))
-			clflush((void *)&current_thread_info()->flags);
-
-		__monitor((void *)&current_thread_info()->flags, 0, 0);
-		smp_mb();
-		if (!need_resched())
-			__sti_mwait(0, 0);
-		else
-			local_irq_enable();
-		trace_power_end_rcuidle(smp_processor_id());
-		trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
-	} else
-		local_irq_enable();
-}
-
 /*
  * On SMP it's slightly faster (but much more power-consuming!)
  * to poll the ->work.need_resched flag instead of waiting for the
@@ -458,53 +437,6 @@ static void poll_idle(void)
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
 
-/*
- * mwait selection logic:
- *
- * It depends on the CPU. For AMD CPUs that support MWAIT this is
- * wrong. Family 0x10 and 0x11 CPUs will enter C1 on HLT. Powersavings
- * then depend on a clock divisor and current Pstate of the core. If
- * all cores of a processor are in halt state (C1) the processor can
- * enter the C1E (C1 enhanced) state. If mwait is used this will never
- * happen.
- *
- * idle=mwait overrides this decision and forces the usage of mwait.
- */
-
-#define MWAIT_INFO			0x05
-#define MWAIT_ECX_EXTENDED_INFO		0x01
-#define MWAIT_EDX_C1			0xf0
-
-int mwait_usable(const struct cpuinfo_x86 *c)
-{
-	u32 eax, ebx, ecx, edx;
-
-	/* Use mwait if idle=mwait boot option is given */
-	if (boot_option_idle_override == IDLE_FORCE_MWAIT)
-		return 1;
-
-	/*
-	 * Any idle= boot option other than idle=mwait means that we must not
-	 * use mwait. Eg: idle=halt or idle=poll or idle=nomwait
-	 */
-	if (boot_option_idle_override != IDLE_NO_OVERRIDE)
-		return 0;
-
-	if (c->cpuid_level < MWAIT_INFO)
-		return 0;
-
-	cpuid(MWAIT_INFO, &eax, &ebx, &ecx, &edx);
-	/* Check, whether EDX has extended info about MWAIT */
-	if (!(ecx & MWAIT_ECX_EXTENDED_INFO))
-		return 1;
-
-	/*
-	 * edx enumeratios MONITOR/MWAIT extensions. Check, whether
-	 * C1  supports MWAIT
-	 */
-	return (edx & MWAIT_EDX_C1);
-}
-
 bool amd_e400_c1e_detected;
 EXPORT_SYMBOL(amd_e400_c1e_detected);
 
@@ -576,13 +508,7 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 	if (pm_idle)
 		return;
 
-	if (cpu_has(c, X86_FEATURE_MWAIT) && mwait_usable(c)) {
-		/*
-		 * One CPU supports mwait => All CPUs supports mwait
-		 */
-		pr_info("using mwait in idle threads\n");
-		pm_idle = mwait_idle;
-	} else if (cpu_has_amd_erratum(amd_erratum_400)) {
+	if (cpu_has_amd_erratum(amd_erratum_400)) {
 		/* E400: APIC timer interrupt does not wake up CPU from C1e */
 		pr_info("using AMD E400 aware idle routine\n");
 		pm_idle = amd_e400_idle;
@@ -606,9 +532,6 @@ static int __init idle_setup(char *str)
 		pr_info("using polling idle threads\n");
 		pm_idle = poll_idle;
 		boot_option_idle_override = IDLE_POLL;
-	} else if (!strcmp(str, "mwait")) {
-		boot_option_idle_override = IDLE_FORCE_MWAIT;
-		WARN_ONCE(1, "\"idle=mwait\" will be removed in 2012\n");
 	} else if (!strcmp(str, "halt")) {
 		/*
 		 * When the boot option of idle=halt is added, halt is

commit 6a377ddc4e4ede2eeb9cd46ada23bbe417704fc9
Author: Len Brown <len.brown@intel.com>
Date:   Sat Feb 9 23:08:07 2013 -0500

    xen idle: make xen-specific macro xen-specific
    
    This macro is only invoked by Xen,
    so make its definition specific to Xen.
    
    > set_pm_idle_to_default()
    < xen_set_default_idle()
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: xen-devel@lists.xensource.com

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 2ed787f15bf0..7ed9f6b08ba0 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -396,7 +396,8 @@ void default_idle(void)
 EXPORT_SYMBOL(default_idle);
 #endif
 
-bool set_pm_idle_to_default(void)
+#ifdef CONFIG_XEN
+bool xen_set_default_idle(void)
 {
 	bool ret = !!pm_idle;
 
@@ -404,6 +405,7 @@ bool set_pm_idle_to_default(void)
 
 	return ret;
 }
+#endif
 void stop_this_cpu(void *dummy)
 {
 	local_irq_disable();

commit 43720bd6014327ac454434496cb953edcdb9f8d6
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jan 11 13:43:45 2013 +0100

    PM / tracing: remove deprecated power trace API
    
    The text in Documentation said it would be removed in 2.6.41;
    the text in the Kconfig said removal in the 3.1 release.  Either
    way you look at it, we are well past both, so push it off a cliff.
    
    Note that the POWER_CSTATE and the POWER_PSTATE are part of the
    legacy tracing API.  Remove all tracepoints which use these flags.
    As can be seen from context, most already have a trace entry via
    trace_cpu_idle anyways.
    
    Also, the cpufreq/cpufreq.c PSTATE one is actually unpaired, as
    compared to the CSTATE ones which all have a clear start/stop.
    As part of this, the trace_power_frequency also becomes orphaned,
    so it too is deleted.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 2ed787f15bf0..dcfc1f410dc4 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -375,7 +375,6 @@ void cpu_idle(void)
  */
 void default_idle(void)
 {
-	trace_power_start_rcuidle(POWER_CSTATE, 1, smp_processor_id());
 	trace_cpu_idle_rcuidle(1, smp_processor_id());
 	current_thread_info()->status &= ~TS_POLLING;
 	/*
@@ -389,7 +388,6 @@ void default_idle(void)
 	else
 		local_irq_enable();
 	current_thread_info()->status |= TS_POLLING;
-	trace_power_end_rcuidle(smp_processor_id());
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
 #ifdef CONFIG_APM_MODULE
@@ -423,7 +421,6 @@ void stop_this_cpu(void *dummy)
 static void mwait_idle(void)
 {
 	if (!need_resched()) {
-		trace_power_start_rcuidle(POWER_CSTATE, 1, smp_processor_id());
 		trace_cpu_idle_rcuidle(1, smp_processor_id());
 		if (this_cpu_has(X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
@@ -434,7 +431,6 @@ static void mwait_idle(void)
 			__sti_mwait(0, 0);
 		else
 			local_irq_enable();
-		trace_power_end_rcuidle(smp_processor_id());
 		trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 	} else
 		local_irq_enable();
@@ -447,12 +443,10 @@ static void mwait_idle(void)
  */
 static void poll_idle(void)
 {
-	trace_power_start_rcuidle(POWER_CSTATE, 0, smp_processor_id());
 	trace_cpu_idle_rcuidle(0, smp_processor_id());
 	local_irq_enable();
 	while (!need_resched())
 		cpu_relax();
-	trace_power_end_rcuidle(smp_processor_id());
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
 

commit 9977d9b379cb77e0f67bd6f4563618106e58e11d
Merge: cf4af0122157 541880d9a2c7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 12 12:22:13 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull big execve/kernel_thread/fork unification series from Al Viro:
     "All architectures are converted to new model.  Quite a bit of that
      stuff is actually shared with architecture trees; in such cases it's
      literally shared branch pulled by both, not a cherry-pick.
    
      A lot of ugliness and black magic is gone (-3KLoC total in this one):
    
       - kernel_thread()/kernel_execve()/sys_execve() redesign.
    
         We don't do syscalls from kernel anymore for either kernel_thread()
         or kernel_execve():
    
         kernel_thread() is essentially clone(2) with callback run before we
         return to userland, the callbacks either never return or do
         successful do_execve() before returning.
    
         kernel_execve() is a wrapper for do_execve() - it doesn't need to
         do transition to user mode anymore.
    
         As a result kernel_thread() and kernel_execve() are
         arch-independent now - they live in kernel/fork.c and fs/exec.c
         resp.  sys_execve() is also in fs/exec.c and it's completely
         architecture-independent.
    
       - daemonize() is gone, along with its parts in fs/*.c
    
       - struct pt_regs * is no longer passed to do_fork/copy_process/
         copy_thread/do_execve/search_binary_handler/->load_binary/do_coredump.
    
       - sys_fork()/sys_vfork()/sys_clone() unified; some architectures
         still need wrappers (ones with callee-saved registers not saved in
         pt_regs on syscall entry), but the main part of those suckers is in
         kernel/fork.c now."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (113 commits)
      do_coredump(): get rid of pt_regs argument
      print_fatal_signal(): get rid of pt_regs argument
      ptrace_signal(): get rid of unused arguments
      get rid of ptrace_signal_deliver() arguments
      new helper: signal_pt_regs()
      unify default ptrace_signal_deliver
      flagday: kill pt_regs argument of do_fork()
      death to idle_regs()
      don't pass regs to copy_process()
      flagday: don't pass regs to copy_thread()
      bfin: switch to generic vfork, get rid of pointless wrappers
      xtensa: switch to generic clone()
      openrisc: switch to use of generic fork and clone
      unicore32: switch to generic clone(2)
      score: switch to generic fork/vfork/clone
      c6x: sanitize copy_thread(), get rid of clone(2) wrapper, switch to generic clone()
      take sys_fork/sys_vfork/sys_clone prototypes to linux/syscalls.h
      mn10300: switch to generic fork/vfork/clone
      h8300: switch to generic fork/vfork/clone
      tile: switch to generic clone()
      ...
    
    Conflicts:
            arch/microblaze/include/asm/Kbuild

commit 1d4b4b2994b5fc208963c0b795291f8c1f18becf
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 22:34:11 2012 -0400

    x86, um: switch to generic fork/vfork/clone
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index b644e1c765dc..fe9459593257 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -262,36 +262,6 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 	propagate_user_return_notify(prev_p, next_p);
 }
 
-int sys_fork(struct pt_regs *regs)
-{
-	return do_fork(SIGCHLD, regs->sp, regs, 0, NULL, NULL);
-}
-
-/*
- * This is trivial, and on the face of it looks like it
- * could equally well be done in user mode.
- *
- * Not so, for quite unobvious reasons - register pressure.
- * In user mode vfork() cannot have a stack frame, and if
- * done by calling the "clone()" system call directly, you
- * do not have enough call-clobbered registers to hold all
- * the information you need.
- */
-int sys_vfork(struct pt_regs *regs)
-{
-	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->sp, regs, 0,
-		       NULL, NULL);
-}
-
-long
-sys_clone(unsigned long clone_flags, unsigned long newsp,
-	  void __user *parent_tid, void __user *child_tid, struct pt_regs *regs)
-{
-	if (!newsp)
-		newsp = regs->sp;
-	return do_fork(clone_flags, newsp, regs, 0, parent_tid, child_tid);
-}
-
 /*
  * Idle related variables and functions
  */

commit 4d0e42cc66f4e7e0bf08b29da1ae6ebd60549c4e
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Thu Oct 25 18:13:11 2012 +0200

    x86: Remove dead hlt_use_halt code
    
    The hlt_use_halt function returns always true and there is only
    one definition of it.
    
    The default_idle function can then get ride of the if ...
    statement and we can remove the else branch.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: linaro-dev@lists.linaro.org
    Cc: patches@linaro.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1351181591-8710-1-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index b644e1c765dc..2f99e3121875 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -306,11 +306,6 @@ void (*pm_idle)(void);
 EXPORT_SYMBOL(pm_idle);
 #endif
 
-static inline int hlt_use_halt(void)
-{
-	return 1;
-}
-
 #ifndef CONFIG_SMP
 static inline void play_dead(void)
 {
@@ -410,28 +405,22 @@ void cpu_idle(void)
  */
 void default_idle(void)
 {
-	if (hlt_use_halt()) {
-		trace_power_start_rcuidle(POWER_CSTATE, 1, smp_processor_id());
-		trace_cpu_idle_rcuidle(1, smp_processor_id());
-		current_thread_info()->status &= ~TS_POLLING;
-		/*
-		 * TS_POLLING-cleared state must be visible before we
-		 * test NEED_RESCHED:
-		 */
-		smp_mb();
+	trace_power_start_rcuidle(POWER_CSTATE, 1, smp_processor_id());
+	trace_cpu_idle_rcuidle(1, smp_processor_id());
+	current_thread_info()->status &= ~TS_POLLING;
+	/*
+	 * TS_POLLING-cleared state must be visible before we
+	 * test NEED_RESCHED:
+	 */
+	smp_mb();
 
-		if (!need_resched())
-			safe_halt();	/* enables interrupts racelessly */
-		else
-			local_irq_enable();
-		current_thread_info()->status |= TS_POLLING;
-		trace_power_end_rcuidle(smp_processor_id());
-		trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
-	} else {
+	if (!need_resched())
+		safe_halt();	/* enables interrupts racelessly */
+	else
 		local_irq_enable();
-		/* loop is done by the caller */
-		cpu_relax();
-	}
+	current_thread_info()->status |= TS_POLLING;
+	trace_power_end_rcuidle(smp_processor_id());
+	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
 #ifdef CONFIG_APM_MODULE
 EXPORT_SYMBOL(default_idle);

commit 42859eea96ba6beabfb0369a1eeffa3c7d2bd9cb
Merge: f59b51fe3d30 f322220d6159
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 10 12:02:25 2012 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull generic execve() changes from Al Viro:
     "This introduces the generic kernel_thread() and kernel_execve()
      functions, and switches x86, arm, alpha, um and s390 over to them."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (26 commits)
      s390: convert to generic kernel_execve()
      s390: switch to generic kernel_thread()
      s390: fold kernel_thread_helper() into ret_from_fork()
      s390: fold execve_tail() into start_thread(), convert to generic sys_execve()
      um: switch to generic kernel_thread()
      x86, um/x86: switch to generic sys_execve and kernel_execve
      x86: split ret_from_fork
      alpha: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      alpha: switch to generic kernel_thread()
      alpha: switch to generic sys_execve()
      arm: get rid of execve wrapper, switch to generic execve() implementation
      arm: optimized current_pt_regs()
      arm: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      arm: split ret_from_fork, simplify kernel_thread() [based on patch by rmk]
      generic sys_execve()
      generic kernel_execve()
      new helper: current_pt_regs()
      preparation for generic kernel_thread()
      um: kill thread->forking
      um: let signal_delivered() do SIGTRAP on singlestepping into handler
      ...

commit 6783eaa2e1253fbcbe2c2f6bb4c843abf1343caf
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Aug 2 23:05:11 2012 +0400

    x86, um/x86: switch to generic sys_execve and kernel_execve
    
    32bit wrapper is lost on that; 64bit one is *not*, since
    we need to arrange for full pt_regs on stack when we call
    sys_execve() and we need to load callee-saved ones from
    there afterwards.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6947ec968bf8..eae2dd5cd5a0 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -298,25 +298,6 @@ sys_clone(unsigned long clone_flags, unsigned long newsp,
 	return do_fork(clone_flags, newsp, regs, 0, parent_tid, child_tid);
 }
 
-/*
- * sys_execve() executes a new program.
- */
-long sys_execve(const char __user *name,
-		const char __user *const __user *argv,
-		const char __user *const __user *envp, struct pt_regs *regs)
-{
-	long error;
-	char *filename;
-
-	filename = getname(name);
-	error = PTR_ERR(filename);
-	if (IS_ERR(filename))
-		return error;
-	error = do_execve(filename, argv, envp, regs);
-	putname(filename);
-	return error;
-}
-
 /*
  * Idle related variables and functions
  */

commit 7076aada1040de4ed79a5977dbabdb5e5ea5e249
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Sep 10 16:44:54 2012 -0400

    x86: split ret_from_fork
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7162e9c1f598..6947ec968bf8 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -298,44 +298,6 @@ sys_clone(unsigned long clone_flags, unsigned long newsp,
 	return do_fork(clone_flags, newsp, regs, 0, parent_tid, child_tid);
 }
 
-/*
- * This gets run with %si containing the
- * function to call, and %di containing
- * the "args".
- */
-extern void kernel_thread_helper(void);
-
-/*
- * Create a kernel thread
- */
-int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
-{
-	struct pt_regs regs;
-
-	memset(&regs, 0, sizeof(regs));
-
-	regs.si = (unsigned long) fn;
-	regs.di = (unsigned long) arg;
-
-#ifdef CONFIG_X86_32
-	regs.ds = __USER_DS;
-	regs.es = __USER_DS;
-	regs.fs = __KERNEL_PERCPU;
-	regs.gs = __KERNEL_STACK_CANARY;
-#else
-	regs.ss = __KERNEL_DS;
-#endif
-
-	regs.orig_ax = -1;
-	regs.ip = (unsigned long) kernel_thread_helper;
-	regs.cs = __KERNEL_CS | get_kernel_rpl();
-	regs.flags = X86_EFLAGS_IF | X86_EFLAGS_BIT1;
-
-	/* Ok, create the new process.. */
-	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
-}
-EXPORT_SYMBOL(kernel_thread);
-
 /*
  * sys_execve() executes a new program.
  */

commit e76623d69408d0bd66a296c6ee5eae1b17a6adfc
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Aug 2 22:12:06 2012 +0400

    x86: get rid of TIF_IRET hackery
    
    TIF_NOTIFY_RESUME will work in precisely the same way; all that
    is achieved by TIF_IRET is appearing that there's some work to be
    done, so we end up on the iret exit path.  Just use NOTIFY_RESUME.
    And for execve() do that in 32bit start_thread(), not sys_execve()
    itself.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ef6a8456f719..7162e9c1f598 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -351,14 +351,6 @@ long sys_execve(const char __user *name,
 	if (IS_ERR(filename))
 		return error;
 	error = do_execve(filename, argv, envp, regs);
-
-#ifdef CONFIG_X86_32
-	if (error == 0) {
-		/* Make sure we don't return using sysenter.. */
-                set_thread_flag(TIF_IRET);
-        }
-#endif
-
 	putname(filename);
 	return error;
 }

commit 5d2bd7009f306c82afddd1ca4d9763ad8473c216
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Sep 6 14:58:52 2012 -0700

    x86, fpu: decouple non-lazy/eager fpu restore from xsave
    
    Decouple non-lazy/eager fpu restore policy from the existence of the xsave
    feature. Introduce a synthetic CPUID flag to represent the eagerfpu
    policy. "eagerfpu=on" boot paramter will enable the policy.
    
    Requested-by: H. Peter Anvin <hpa@zytor.com>
    Requested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1347300665-6209-2-git-send-email-suresh.b.siddha@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c21e30f8923b..dc3567e083f9 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -156,7 +156,7 @@ void flush_thread(void)
 	 * Free the FPU state for non xsave platforms. They get reallocated
 	 * lazily at the first use.
 	 */
-	if (!use_xsave())
+	if (!use_eager_fpu())
 		free_thread_xstate(tsk);
 }
 

commit 304bceda6a18ae0b0240b8aac9a6bdf8ce2d2469
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Aug 24 14:13:02 2012 -0700

    x86, fpu: use non-lazy fpu restore for processors supporting xsave
    
    Fundamental model of the current Linux kernel is to lazily init and
    restore FPU instead of restoring the task state during context switch.
    This changes that fundamental lazy model to the non-lazy model for
    the processors supporting xsave feature.
    
    Reasons driving this model change are:
    
    i. Newer processors support optimized state save/restore using xsaveopt and
    xrstor by tracking the INIT state and MODIFIED state during context-switch.
    This is faster than modifying the cr0.TS bit which has serializing semantics.
    
    ii. Newer glibc versions use SSE for some of the optimized copy/clear routines.
    With certain workloads (like boot, kernel-compilation etc), application
    completes its work with in the first 5 task switches, thus taking upto 5 #DNA
    traps with the kernel not getting a chance to apply the above mentioned
    pre-load heuristic.
    
    iii. Some xstate features (like AMD's LWP feature) don't honor the cr0.TS bit
    and thus will not work correctly in the presence of lazy restore. Non-lazy
    state restore is needed for enabling such features.
    
    Some data on a two socket SNB system:
     * Saved 20K DNA exceptions during boot on a two socket SNB system.
     * Saved 50K DNA exceptions during kernel-compilation workload.
     * Improved throughput of the AVX based checksumming function inside the
       kernel by ~15% as xsave/xrstor is faster than the serializing clts/stts
       pair.
    
    Also now kernel_fpu_begin/end() relies on the patched
    alternative instructions. So move check_fpu() which uses the
    kernel_fpu_begin/end() after alternative_instructions().
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1345842782-24175-7-git-send-email-suresh.b.siddha@intel.com
    Merge 32-bit boot fix from,
    Link: http://lkml.kernel.org/r/1347300665-6209-4-git-send-email-suresh.b.siddha@intel.com
    Cc: Jim Kukunas <james.t.kukunas@linux.intel.com>
    Cc: NeilBrown <neilb@suse.de>
    Cc: Avi Kivity <avi@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 30069d1a6a4d..c21e30f8923b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -66,15 +66,13 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	int ret;
 
-	unlazy_fpu(src);
-
 	*dst = *src;
 	if (fpu_allocated(&src->thread.fpu)) {
 		memset(&dst->thread.fpu, 0, sizeof(dst->thread.fpu));
 		ret = fpu_alloc(&dst->thread.fpu);
 		if (ret)
 			return ret;
-		fpu_copy(&dst->thread.fpu, &src->thread.fpu);
+		fpu_copy(dst, src);
 	}
 	return 0;
 }
@@ -153,7 +151,13 @@ void flush_thread(void)
 
 	flush_ptrace_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
-	drop_fpu(tsk);
+	drop_init_fpu(tsk);
+	/*
+	 * Free the FPU state for non xsave platforms. They get reallocated
+	 * lazily at the first use.
+	 */
+	if (!use_xsave())
+		free_thread_xstate(tsk);
 }
 
 static void hard_disable_TSC(void)

commit 72a671ced66db6d1c2bfff1c930a101ac8d08204
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Tue Jul 24 16:05:29 2012 -0700

    x86, fpu: Unify signal handling code paths for x86 and x86_64 kernels
    
    Currently for x86 and x86_32 binaries, fpstate in the user sigframe is copied
    to/from the fpstate in the task struct.
    
    And in the case of signal delivery for x86_64 binaries, if the fpstate is live
    in the CPU registers, then the live state is copied directly to the user
    sigframe. Otherwise  fpstate in the task struct is copied to the user sigframe.
    During restore, fpstate in the user sigframe is restored directly to the live
    CPU registers.
    
    Historically, different code paths led to different bugs. For example,
    x86_64 code path was not preemption safe till recently. Also there is lot
    of code duplication for support of new features like xsave etc.
    
    Unify signal handling code paths for x86 and x86_64 kernels.
    
    New strategy is as follows:
    
    Signal delivery: Both for 32/64-bit frames, align the core math frame area to
    64bytes as needed by xsave (this where the main fpu/extended state gets copied
    to and excludes the legacy compatibility fsave header for the 32-bit [f]xsave
    frames). If the state is live, copy the register state directly to the user
    frame. If not live, copy the state in the thread struct to the user frame. And
    for 32-bit [f]xsave frames, construct the fsave header separately before
    the actual [f]xsave area.
    
    Signal return: As the 32-bit frames with [f]xstate has an additional
    'fsave' header, copy everything back from the user sigframe to the
    fpstate in the task structure and reconstruct the fxstate from the 'fsave'
    header (Also user passed pointers may not be correctly aligned for
    any attempt to directly restore any partial state). At the next fpstate usage,
    everything will be restored to the live CPU registers.
    For all the 64-bit frames and the 32-bit fsave frame, restore the state from
    the user sigframe directly to the live CPU registers. 64-bit signals always
    restored the math frame directly, so we can expect the math frame pointer
    to be correctly aligned. For 32-bit fsave frames, there are no alignment
    requirements, so we can restore the state directly.
    
    "lat_sig catch" microbenchmark numbers (for x86, x86_64, x86_32 binaries) are
    with in the noise range with this change.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1343171129-2747-4-git-send-email-suresh.b.siddha@intel.com
    [ Merged in compilation fix ]
    Link: http://lkml.kernel.org/r/1344544736.8326.17.camel@sbsiddha-desk.sc.intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ef6a8456f719..30069d1a6a4d 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -97,16 +97,6 @@ void arch_task_cache_init(void)
 				  SLAB_PANIC | SLAB_NOTRACK, NULL);
 }
 
-static inline void drop_fpu(struct task_struct *tsk)
-{
-	/*
-	 * Forget coprocessor state..
-	 */
-	tsk->fpu_counter = 0;
-	clear_fpu(tsk);
-	clear_used_math();
-}
-
 /*
  * Free current thread data structures etc..
  */

commit c767a54ba0657e52e6edaa97cbe0b0a8bf1c1655
Author: Joe Perches <joe@perches.com>
Date:   Mon May 21 19:50:07 2012 -0700

    x86/debug: Add KERN_<LEVEL> to bare printks, convert printks to pr_<level>
    
    Use a more current logging style:
    
     - Bare printks should have a KERN_<LEVEL> for consistency's sake
     - Add pr_fmt where appropriate
     - Neaten some macro definitions
     - Convert some Ok output to OK
     - Use "%s: ", __func__ in pr_fmt for summit
     - Convert some printks to pr_<level>
    
    Message output is not identical in all cases.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: levinsasha928@gmail.com
    Link: http://lkml.kernel.org/r/1337655007.24226.10.camel@joe2Laptop
    [ merged two similar patches, tidied up the changelog ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 735279e54e59..ef6a8456f719 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -1,3 +1,5 @@
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
 #include <linux/errno.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
@@ -145,16 +147,14 @@ void show_regs_common(void)
 	/* Board Name is optional */
 	board = dmi_get_system_info(DMI_BOARD_NAME);
 
-	printk(KERN_CONT "\n");
-	printk(KERN_DEFAULT "Pid: %d, comm: %.20s %s %s %.*s",
-		current->pid, current->comm, print_tainted(),
-		init_utsname()->release,
-		(int)strcspn(init_utsname()->version, " "),
-		init_utsname()->version);
-	printk(KERN_CONT " %s %s", vendor, product);
-	if (board)
-		printk(KERN_CONT "/%s", board);
-	printk(KERN_CONT "\n");
+	printk(KERN_DEFAULT "Pid: %d, comm: %.20s %s %s %.*s %s %s%s%s\n",
+	       current->pid, current->comm, print_tainted(),
+	       init_utsname()->release,
+	       (int)strcspn(init_utsname()->version, " "),
+	       init_utsname()->version,
+	       vendor, product,
+	       board ? "/" : "",
+	       board ? board : "");
 }
 
 void flush_thread(void)
@@ -645,7 +645,7 @@ static void amd_e400_idle(void)
 			amd_e400_c1e_detected = true;
 			if (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))
 				mark_tsc_unstable("TSC halt in AMD C1E");
-			printk(KERN_INFO "System has AMD C1E enabled\n");
+			pr_info("System has AMD C1E enabled\n");
 		}
 	}
 
@@ -659,8 +659,7 @@ static void amd_e400_idle(void)
 			 */
 			clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_FORCE,
 					   &cpu);
-			printk(KERN_INFO "Switch to broadcast mode on CPU%d\n",
-			       cpu);
+			pr_info("Switch to broadcast mode on CPU%d\n", cpu);
 		}
 		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &cpu);
 
@@ -681,8 +680,7 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
 	if (pm_idle == poll_idle && smp_num_siblings > 1) {
-		printk_once(KERN_WARNING "WARNING: polling idle and HT enabled,"
-			" performance may degrade.\n");
+		pr_warn_once("WARNING: polling idle and HT enabled, performance may degrade\n");
 	}
 #endif
 	if (pm_idle)
@@ -692,11 +690,11 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 		/*
 		 * One CPU supports mwait => All CPUs supports mwait
 		 */
-		printk(KERN_INFO "using mwait in idle threads.\n");
+		pr_info("using mwait in idle threads\n");
 		pm_idle = mwait_idle;
 	} else if (cpu_has_amd_erratum(amd_erratum_400)) {
 		/* E400: APIC timer interrupt does not wake up CPU from C1e */
-		printk(KERN_INFO "using AMD E400 aware idle routine\n");
+		pr_info("using AMD E400 aware idle routine\n");
 		pm_idle = amd_e400_idle;
 	} else
 		pm_idle = default_idle;
@@ -715,7 +713,7 @@ static int __init idle_setup(char *str)
 		return -EINVAL;
 
 	if (!strcmp(str, "poll")) {
-		printk("using polling idle threads.\n");
+		pr_info("using polling idle threads\n");
 		pm_idle = poll_idle;
 		boot_option_idle_override = IDLE_POLL;
 	} else if (!strcmp(str, "mwait")) {

commit ec0d7f18ab7b5097d7c0c8f3d909ca1031b9d5cd
Merge: 269af9a1a08d 1dcc8d7ba235
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 23 10:59:07 2012 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull fpu state cleanups from Ingo Molnar:
     "This tree streamlines further aspects of FPU handling by eliminating
      the prepare_to_copy() complication and moving that logic to
      arch_dup_task_struct().
    
      It also fixes the FPU dumps in threaded core dumps, removes and old
      (and now invalid) assumption plus micro-optimizes the exit path by
      avoiding an FPU save for dead tasks."
    
    Fixed up trivial add-add conflict in arch/sh/kernel/process.c that came
    in because we now do the FPU handling in arch_dup_task_struct() rather
    than the legacy (and now gone) prepare_to_copy().
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, fpu: drop the fpu state during thread exit
      x86, xsave: remove thread_has_fpu() bug check in __sanitize_i387_state()
      coredump: ensure the fpu state is flushed for proper multi-threaded core dump
      fork: move the real prepare_to_copy() users to arch_dup_task_struct()

commit 19bec32d7f26f263dba13f2797d9c3245de2020b
Merge: 514b1923e154 fba60c620a6a 74bc49179542 ddc5681ed33a 57da8b960b9a e826abd52391
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 23 10:09:50 2012 -0700

    Merge branches 'x86-asm-for-linus', 'x86-cleanups-for-linus', 'x86-cpu-for-linus', 'x86-debug-for-linus' and 'x86-microcode-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull initial trivial x86 stuff from Ingo Molnar.
    
    Various random cleanups and trivial fixes.
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86-64: Eliminate dead ia32 syscall handlers
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/pci-calgary_64.c: Remove obsoleted simple_strtoul() usage
      x86: Don't continue booting if we can't load the specified initrd
      x86: kernel/dumpstack.c simple_strtoul cleanup
      x86: kernel/check.c simple_strtoul cleanup
      debug: Add CONFIG_READABLE_ASM
      x86: spinlock.h: Remove REG_PTR_MODE
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/cache_info: Fix setup of l2/l3 ids
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Avoid double stack traces with show_regs()
    
    * 'x86-microcode-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, microcode: microcode_core.c simple_strtoul cleanup

commit d79ee93de909dfb252279b9a95978bbda9a814a9
Merge: 2ff2b289a695 1c2927f18576
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 22 18:27:32 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The biggest change is the cleanup/simplification of the load-balancer:
      instead of the current practice of architectures twiddling scheduler
      internal data structures and providing the scheduler domains in
      colorfully inconsistent ways, we now have generic scheduler code in
      kernel/sched/core.c:sched_init_numa() that looks at the architecture's
      node_distance() parameters and (while not fully trusting it) deducts a
      NUMA topology from it.
    
      This inevitably changes balancing behavior - hopefully for the better.
    
      There are various smaller optimizations, cleanups and fixlets as well"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Taint kernel with TAINT_WARN after sleep-in-atomic bug
      sched: Remove stale power aware scheduling remnants and dysfunctional knobs
      sched/debug: Fix printing large integers on 32-bit platforms
      sched/fair: Improve the ->group_imb logic
      sched/nohz: Fix rq->cpu_load[] calculations
      sched/numa: Don't scale the imbalance
      sched/fair: Revert sched-domain iteration breakage
      sched/x86: Rewrite set_cpu_sibling_map()
      sched/numa: Fix the new NUMA topology bits
      sched/numa: Rewrite the CONFIG_NUMA sched domain support
      sched/fair: Propagate 'struct lb_env' usage into find_busiest_group
      sched/fair: Add some serialization to the sched_domain load-balance walk
      sched/fair: Let minimally loaded cpu balance the group
      sched: Change rq->nr_running to unsigned int
      x86/numa: Check for nonsensical topologies on real hw as well
      x86/numa: Hard partition cpu topology masks on node boundaries
      x86/numa: Allow specifying node_distance() for numa=fake
      x86/sched: Make mwait_usable() heed to "idle=" kernel parameters properly
      sched: Update documentation and comments
      sched_rt: Avoid unnecessary dequeue and enqueue of pushable tasks in set_cpus_allowed_rt()

commit f5c101892fbd3d2f6d2729bc7eb7b3f6c31dbddd
Merge: c54894cd4672 641b695c2f11
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 22 17:37:47 2012 -0700

    Merge branch 'for-3.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    Pull percpu updates from Tejun Heo:
     "Contains Alex Shi's three patches to remove percpu_xxx() which overlap
      with this_cpu_xxx().  There shouldn't be any functional change."
    
    * 'for-3.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu:
      percpu: remove percpu_xxx() functions
      x86: replace percpu_xxx funcs with this_cpu_xxx
      net: replace percpu_xxx funcs with this_cpu_xxx or __this_cpu_xxx

commit 1dcc8d7ba235a316a056f993e88f0d18b92c60d9
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed May 16 15:03:54 2012 -0700

    x86, fpu: drop the fpu state during thread exit
    
    There is no need to save any active fpu state to the task structure
    memory if the task is dead. Just drop the state instead.
    
    For example, this saved some 1770 xsave's during the system boot
    of a two socket Xeon system.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1336692811-30576-4-git-send-email-suresh.b.siddha@intel.com
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index b7e1e0e53987..1219fe2be8f3 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -87,6 +87,16 @@ void arch_task_cache_init(void)
 				  SLAB_PANIC | SLAB_NOTRACK, NULL);
 }
 
+static inline void drop_fpu(struct task_struct *tsk)
+{
+	/*
+	 * Forget coprocessor state..
+	 */
+	tsk->fpu_counter = 0;
+	clear_fpu(tsk);
+	clear_used_math();
+}
+
 /*
  * Free current thread data structures etc..
  */
@@ -109,6 +119,8 @@ void exit_thread(void)
 		put_cpu();
 		kfree(bp);
 	}
+
+	drop_fpu(me);
 }
 
 void show_regs(struct pt_regs *regs)
@@ -149,12 +161,7 @@ void flush_thread(void)
 
 	flush_ptrace_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
-	/*
-	 * Forget coprocessor state..
-	 */
-	tsk->fpu_counter = 0;
-	clear_fpu(tsk);
-	clear_used_math();
+	drop_fpu(tsk);
 }
 
 static void hard_disable_TSC(void)

commit 55ccf3fe3f9a3441731aa79cf42a628fc4ecace9
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed May 16 15:03:51 2012 -0700

    fork: move the real prepare_to_copy() users to arch_dup_task_struct()
    
    Historical prepare_to_copy() is mostly a no-op, duplicated for majority of
    the architectures and the rest following the x86 model of flushing the extended
    register state like fpu there.
    
    Remove it and use the arch_dup_task_struct() instead.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1336692811-30576-1-git-send-email-suresh.b.siddha@intel.com
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chen Liqin <liqin.chen@sunplusct.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 1d92a5ab6e8b..b7e1e0e53987 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -47,10 +47,16 @@ EXPORT_SYMBOL_GPL(idle_notifier_unregister);
 struct kmem_cache *task_xstate_cachep;
 EXPORT_SYMBOL_GPL(task_xstate_cachep);
 
+/*
+ * this gets called so that we can store lazy state into memory and copy the
+ * current task into the new thread.
+ */
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	int ret;
 
+	unlazy_fpu(src);
+
 	*dst = *src;
 	if (fpu_allocated(&src->thread.fpu)) {
 		memset(&dst->thread.fpu, 0, sizeof(dst->thread.fpu));

commit c6ae41e7d469f00d9c92a2b2887c7235d121c009
Author: Alex Shi <alex.shi@intel.com>
Date:   Fri May 11 15:35:27 2012 +0800

    x86: replace percpu_xxx funcs with this_cpu_xxx
    
    Since percpu_xxx() serial functions are duplicated with this_cpu_xxx().
    Removing percpu_xxx() definition and replacing them by this_cpu_xxx()
    in code. There is no function change in this patch, just preparation for
    later percpu_xxx serial function removing.
    
    On x86 machine the this_cpu_xxx() serial functions are same as
    __this_cpu_xxx() without no unnecessary premmpt enable/disable.
    
    Thanks for Stephen Rothwell, he found and fixed a i386 build error in
    the patch.
    
    Also thanks for Andrew Morton, he kept updating the patchset in Linus'
    tree.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Acked-by: Christoph Lameter <cl@gentwo.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 1d92a5ab6e8b..857adffb7080 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -377,7 +377,7 @@ static inline void play_dead(void)
 #ifdef CONFIG_X86_64
 void enter_idle(void)
 {
-	percpu_write(is_idle, 1);
+	this_cpu_write(is_idle, 1);
 	atomic_notifier_call_chain(&idle_notifier, IDLE_START, NULL);
 }
 

commit 57da8b960b9a25646a8ddb5a9c1d0b5978e69bec
Author: Jan Beulich <JBeulich@suse.com>
Date:   Wed May 9 08:47:37 2012 +0100

    x86: Avoid double stack traces with show_regs()
    
    What was called show_registers() so far already showed a stack
    trace for kernel faults, and kernel_stack_pointer() isn't even
    valid to be used for faults from user mode, hence it was
    pointless for show_regs() to call show_trace() after
    show_registers().
    
    Simply rename show_registers() to show_regs() and eliminate
    the old definition.
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/4FAA3D3902000078000826E1@nat28.tlf.novell.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 1d92a5ab6e8b..856d5bcae5b2 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -105,12 +105,6 @@ void exit_thread(void)
 	}
 }
 
-void show_regs(struct pt_regs *regs)
-{
-	show_registers(regs);
-	show_trace(NULL, regs, (unsigned long *)kernel_stack_pointer(regs), 0);
-}
-
 void show_regs_common(void)
 {
 	const char *vendor, *product, *board;

commit 38e7c572ce7310def003d8bb7c34260f5d8118cb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 5 15:05:42 2012 +0000

    x86: Use common threadinfo allocator
    
    The only difference is the free_thread_info function, which frees
    xstate.
    
    Use the new arch_release_task_struct() function instead and switch
    over to the core allocator.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120505150141.559556763@linutronix.de
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 8215458f6af5..e8173154800d 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -76,10 +76,9 @@ void free_thread_xstate(struct task_struct *tsk)
 	fpu_free(&tsk->thread.fpu);
 }
 
-void free_thread_info(struct thread_info *ti)
+void arch_release_task_struct(struct task_struct *tsk)
 {
-	free_thread_xstate(ti->task);
-	free_pages((unsigned long)ti, THREAD_ORDER);
+	free_thread_xstate(tsk);
 }
 
 void arch_task_cache_init(void)

commit 85f7f656274fa0ba109dd8774db3887d42de5c6b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 7 17:59:49 2012 +0000

    x86: Use kick_all_cpus_sync()
    
    Use kick_all_cpus_sync() and remove cpu_idle_wait().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120507175652.190382227@linutronix.de
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 8aa532fa015d..8215458f6af5 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -525,26 +525,6 @@ void stop_this_cpu(void *dummy)
 	}
 }
 
-static void do_nothing(void *unused)
-{
-}
-
-/*
- * cpu_idle_wait - Used to ensure that all the CPUs discard old value of
- * pm_idle and update to new pm_idle value. Required while changing pm_idle
- * handler on SMP systems.
- *
- * Caller must have changed pm_idle to the new value before the call. Old
- * pm_idle value will not be used by any CPU after the return of this function.
- */
-void cpu_idle_wait(void)
-{
-	smp_mb();
-	/* kick all the CPUs so that they exit out of pm_idle */
-	smp_call_function(do_nothing, NULL, 1);
-}
-EXPORT_SYMBOL_GPL(cpu_idle_wait);
-
 /* Default MONITOR/MWAIT with no hints, used for default C1 state */
 static void mwait_idle(void)
 {

commit 19209bbb8612004bc20a1f70ff12926f99fe2643
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Mon Apr 30 12:26:56 2012 +0530

    x86/sched: Make mwait_usable() heed to "idle=" kernel parameters properly
    
    The checks that exist in mwait_usable() for "idle=" kernel
    parameters are insufficient. As a result, mwait_usable() can
    return 1 even if "idle=nomwait" or "idle=poll" or "idle=halt"
    parameters are passed.
    
    Of these cases, incorrect handling of idle=nomwait is a
    universal problem since mwait can get used for usual CPU idling.
    However the rest of the cases are problematic only during CPU
    Hotplug (offline) because, in the CPU offline path, the function
    mwait_play_dead() is called, which might result in mwait being
    used in the offline CPUs, if mwait_usable() happens to return 1.
    
    Fix these issues by checking for the boot time "idle=" kernel
    parameter properly in mwait_usable().
    
    The first issue (usual cpu idling) is demonstrated below:
    
    Before applying the patch (dmesg snippet):
    
     [    0.000000] Command line: [...] idle=nomwait
     [    0.000000] Kernel command line: [...] idle=nomwait
     [    0.000000]  RCU dyntick-idle grace-period acceleration is enabled.
     [    0.140606] using mwait in idle threads.  <======= mwait being used
     [    4.303986] cpuidle: using governor ladder
     [    4.308232] cpuidle: using governor menu
    
    After applying the patch:
    
     [    0.000000] Command line: [...] idle=nomwait
     [    0.000000] Kernel command line: [...] idle=nomwait
     [    0.000000]  RCU dyntick-idle grace-period acceleration is enabled.
     [    4.264100] cpuidle: using governor ladder
     [    4.268342] cpuidle: using governor menu
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: venki@google.com
    Cc: suresh.b.siddha@intel.com
    Cc: Borislav Petkov <bp@amd64.org>
    Cc: lenb@kernel.org
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Link: http://lkml.kernel.org/r/4F9E37B8.30001@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 1d92a5ab6e8b..ad57d832d96f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -594,9 +594,17 @@ int mwait_usable(const struct cpuinfo_x86 *c)
 {
 	u32 eax, ebx, ecx, edx;
 
+	/* Use mwait if idle=mwait boot option is given */
 	if (boot_option_idle_override == IDLE_FORCE_MWAIT)
 		return 1;
 
+	/*
+	 * Any idle= boot option other than idle=mwait means that we must not
+	 * use mwait. Eg: idle=halt or idle=poll or idle=nomwait
+	 */
+	if (boot_option_idle_override != IDLE_NO_OVERRIDE)
+		return 0;
+
 	if (c->cpuid_level < MWAIT_INFO)
 		return 0;
 

commit 45046892ef89c1e0caad66a03c8c1e14ad478d23
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 3 09:03:01 2012 +0000

    x86: Use generic init_task
    
    Same code. Use the generic version. The special Makefile treatment is
    pointless anyway as init_task.o contains only data which is handled by
    the linker script. So no point on being treated like head text.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120503085035.739963562@linutronix.de
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 1d92a5ab6e8b..8aa532fa015d 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -27,6 +27,15 @@
 #include <asm/debugreg.h>
 #include <asm/nmi.h>
 
+/*
+ * per-CPU TSS segments. Threads are completely 'soft' on Linux,
+ * no more per-task TSS's. The TSS size is kept cacheline-aligned
+ * so they are allowed to end up in the .data..cacheline_aligned
+ * section. Since TSS's are completely CPU-local, we want them
+ * on exact cacheline boundaries, to eliminate cacheline ping-pong.
+ */
+DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
+
 #ifdef CONFIG_X86_64
 static DEFINE_PER_CPU(unsigned char, is_idle);
 static ATOMIC_NOTIFIER_HEAD(idle_notifier);

commit f6365201d8a21fb347260f89d6e9b3e718d63c70
Author: Len Brown <len.brown@intel.com>
Date:   Thu Mar 29 14:49:17 2012 -0700

    x86: Remove the ancient and deprecated disable_hlt() and enable_hlt() facility
    
    The X86_32-only disable_hlt/enable_hlt mechanism was used by the
    32-bit floppy driver. Its effect was to replace the use of the
    HLT instruction inside default_idle() with cpu_relax() - essentially
    it turned off the use of HLT.
    
    This workaround was commented in the code as:
    
     "disable hlt during certain critical i/o operations"
    
     "This halt magic was a workaround for ancient floppy DMA
      wreckage. It should be safe to remove."
    
    H. Peter Anvin additionally adds:
    
     "To the best of my knowledge, no-hlt only existed because of
      flaky power distributions on 386/486 systems which were sold to
      run DOS.  Since DOS did no power management of any kind,
      including HLT, the power draw was fairly uniform; when exposed
      to the much hhigher noise levels you got when Linux used HLT
      caused some of these systems to fail.
    
      They were by far in the minority even back then."
    
    Alan Cox further says:
    
     "Also for the Cyrix 5510 which tended to go castors up if a HLT
      occurred during a DMA cycle and on a few other boxes HLT during
      DMA tended to go astray.
    
      Do we care ? I doubt it. The 5510 was pretty obscure, the 5520
      fixed it, the 5530 is probably the oldest still in any kind of
      use."
    
    So, let's finally drop this.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Josh Boyer <jwboyer@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Acked-by: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Stephen Hemminger <shemminger@vyatta.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: <stable@kernel.org>
    Link: http://lkml.kernel.org/n/tip-3rhk9bzf0x9rljkv488tloib@git.kernel.org
    [ If anyone cares then alternative instruction patching could be
      used to replace HLT with a one-byte NOP instruction. Much simpler. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index a33afaa5ddb7..1d92a5ab6e8b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -362,34 +362,10 @@ void (*pm_idle)(void);
 EXPORT_SYMBOL(pm_idle);
 #endif
 
-#ifdef CONFIG_X86_32
-/*
- * This halt magic was a workaround for ancient floppy DMA
- * wreckage. It should be safe to remove.
- */
-static int hlt_counter;
-void disable_hlt(void)
-{
-	hlt_counter++;
-}
-EXPORT_SYMBOL(disable_hlt);
-
-void enable_hlt(void)
-{
-	hlt_counter--;
-}
-EXPORT_SYMBOL(enable_hlt);
-
-static inline int hlt_use_halt(void)
-{
-	return (!hlt_counter && boot_cpu_data.hlt_works_ok);
-}
-#else
 static inline int hlt_use_halt(void)
 {
 	return 1;
 }
-#endif
 
 #ifndef CONFIG_SMP
 static inline void play_dead(void)

commit 6b8212a313dae341ef3a2e413dfec5c4dea59617
Merge: bcd550745fc5 8abc3122aa02
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 14:28:26 2012 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 updates from Ingo Molnar.
    
    This touches some non-x86 files due to the sanitized INLINE_SPIN_UNLOCK
    config usage.
    
    Fixed up trivial conflicts due to just header include changes (removing
    headers due to cpu_idle() merge clashing with the <asm/system.h> split).
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/apic/amd: Be more verbose about LVT offset assignments
      x86, tls: Off by one limit check
      x86/ioapic: Add io_apic_ops driver layer to allow interception
      x86/olpc: Add debugfs interface for EC commands
      x86: Merge the x86_32 and x86_64 cpu_idle() functions
      x86/kconfig: Remove CONFIG_TR=y from the defconfigs
      x86: Stop recursive fault in print_context_stack after stack overflow
      x86/io_apic: Move and reenable irq only when CONFIG_GENERIC_PENDING_IRQ=y
      x86/apic: Add separate apic_id_valid() functions for selected apic drivers
      locking/kconfig: Simplify INLINE_SPIN_UNLOCK usage
      x86/kconfig: Update defconfigs
      x86: Fix excessive MSR print out when show_msr is not specified

commit f05e798ad4c09255f590f5b2c00a7ca6c172f983
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:11:12 2012 +0100

    Disintegrate asm/system.h for X86
    
    Disintegrate asm/system.h for X86.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    cc: x86@kernel.org

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 14baf78d5a1f..9b24f36eb55f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -15,7 +15,6 @@
 #include <trace/events/power.h>
 #include <linux/hw_breakpoint.h>
 #include <asm/cpu.h>
-#include <asm/system.h>
 #include <asm/apic.h>
 #include <asm/syscalls.h>
 #include <asm/idle.h>

commit 90e240142bd31ff10aeda5a280a53153f4eff004
Author: Richard Weinberger <richard@nod.at>
Date:   Sun Mar 25 23:00:04 2012 +0200

    x86: Merge the x86_32 and x86_64 cpu_idle() functions
    
    Both functions are mostly identical.
    The differences are:
    
    - x86_32's cpu_idle() makes use of check_pgt_cache(), which is a
      nop on both x86_32 and x86_64.
    
    - x86_64's cpu_idle() uses enter/__exit_idle/(), on x86_32 these
      function are a nop.
    
    - In contrast to x86_32, x86_64 calls rcu_idle_enter/exit() in
      the innermost loop because idle notifications need RCU.
      Calling these function on x86_32 also in the innermost loop
      does not hurt.
    
    So we can merge both functions.
    
    Signed-off-by: Richard Weinberger <richard@nod.at>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: paulmck@linux.vnet.ibm.com
    Cc: josh@joshtriplett.org
    Cc: tj@kernel.org
    Link: http://lkml.kernel.org/r/1332709204-22496-1-git-send-email-richard@nod.at
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 14baf78d5a1f..29309c42b9e5 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -12,6 +12,9 @@
 #include <linux/user-return-notifier.h>
 #include <linux/dmi.h>
 #include <linux/utsname.h>
+#include <linux/stackprotector.h>
+#include <linux/tick.h>
+#include <linux/cpuidle.h>
 #include <trace/events/power.h>
 #include <linux/hw_breakpoint.h>
 #include <asm/cpu.h>
@@ -23,6 +26,24 @@
 #include <asm/i387.h>
 #include <asm/fpu-internal.h>
 #include <asm/debugreg.h>
+#include <asm/nmi.h>
+
+#ifdef CONFIG_X86_64
+static DEFINE_PER_CPU(unsigned char, is_idle);
+static ATOMIC_NOTIFIER_HEAD(idle_notifier);
+
+void idle_notifier_register(struct notifier_block *n)
+{
+	atomic_notifier_chain_register(&idle_notifier, n);
+}
+EXPORT_SYMBOL_GPL(idle_notifier_register);
+
+void idle_notifier_unregister(struct notifier_block *n)
+{
+	atomic_notifier_chain_unregister(&idle_notifier, n);
+}
+EXPORT_SYMBOL_GPL(idle_notifier_unregister);
+#endif
 
 struct kmem_cache *task_xstate_cachep;
 EXPORT_SYMBOL_GPL(task_xstate_cachep);
@@ -371,6 +392,99 @@ static inline int hlt_use_halt(void)
 }
 #endif
 
+#ifndef CONFIG_SMP
+static inline void play_dead(void)
+{
+	BUG();
+}
+#endif
+
+#ifdef CONFIG_X86_64
+void enter_idle(void)
+{
+	percpu_write(is_idle, 1);
+	atomic_notifier_call_chain(&idle_notifier, IDLE_START, NULL);
+}
+
+static void __exit_idle(void)
+{
+	if (x86_test_and_clear_bit_percpu(0, is_idle) == 0)
+		return;
+	atomic_notifier_call_chain(&idle_notifier, IDLE_END, NULL);
+}
+
+/* Called from interrupts to signify idle end */
+void exit_idle(void)
+{
+	/* idle loop has pid 0 */
+	if (current->pid)
+		return;
+	__exit_idle();
+}
+#endif
+
+/*
+ * The idle thread. There's no useful work to be
+ * done, so just try to conserve power and have a
+ * low exit latency (ie sit in a loop waiting for
+ * somebody to say that they'd like to reschedule)
+ */
+void cpu_idle(void)
+{
+	/*
+	 * If we're the non-boot CPU, nothing set the stack canary up
+	 * for us.  CPU0 already has it initialized but no harm in
+	 * doing it again.  This is a good place for updating it, as
+	 * we wont ever return from this function (so the invalid
+	 * canaries already on the stack wont ever trigger).
+	 */
+	boot_init_stack_canary();
+	current_thread_info()->status |= TS_POLLING;
+
+	while (1) {
+		tick_nohz_idle_enter();
+
+		while (!need_resched()) {
+			rmb();
+
+			if (cpu_is_offline(smp_processor_id()))
+				play_dead();
+
+			/*
+			 * Idle routines should keep interrupts disabled
+			 * from here on, until they go to idle.
+			 * Otherwise, idle callbacks can misfire.
+			 */
+			local_touch_nmi();
+			local_irq_disable();
+
+			enter_idle();
+
+			/* Don't trace irqs off for idle */
+			stop_critical_timings();
+
+			/* enter_idle() needs rcu for notifiers */
+			rcu_idle_enter();
+
+			if (cpuidle_idle_call())
+				pm_idle();
+
+			rcu_idle_exit();
+			start_critical_timings();
+
+			/* In many cases the interrupt that ended idle
+			   has already called exit_idle. But some idle
+			   loops can be woken up without interrupt. */
+			__exit_idle();
+		}
+
+		tick_nohz_idle_exit();
+		preempt_enable_no_resched();
+		schedule();
+		preempt_disable();
+	}
+}
+
 /*
  * We use this if we don't have any better
  * idle routine..

commit 35cb8d9e18c0bb33b90d7e574abadbe23b65427d
Merge: 02c502566ef5 1361b83a13d4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 09:41:22 2012 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/fpu changes from Ingo Molnar.
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      i387: Split up <asm/i387.h> into exported and internal interfaces
      i387: Uninline the generic FP helpers that we expose to kernel modules

commit 1361b83a13d4d92e53fbb6c877528713e118b821
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 21 13:19:22 2012 -0800

    i387: Split up <asm/i387.h> into exported and internal interfaces
    
    While various modules include <asm/i387.h> to get access to things we
    actually *intend* for them to use, most of that header file was really
    pretty low-level internal stuff that we really don't want to expose to
    others.
    
    So split the header file into two: the small exported interfaces remain
    in <asm/i387.h>, while the internal definitions that are only used by
    core architecture code are now in <asm/fpu-internal.h>.
    
    The guiding principle for this was to expose functions that we export to
    modules, and leave them in <asm/i387.h>, while stuff that is used by
    task switching or was marked GPL-only is in <asm/fpu-internal.h>.
    
    The fpu-internal.h file could be further split up too, especially since
    arch/x86/kvm/ uses some of the remaining stuff for its module.  But that
    kvm usage should probably be abstracted out a bit, and at least now the
    internal FPU accessor functions are much more contained.  Even if it
    isn't perhaps as contained as it _could_ be.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1202211340330.5354@i5.linux-foundation.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 15763af7bfe3..c38d84e01022 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -21,6 +21,7 @@
 #include <asm/idle.h>
 #include <asm/uaccess.h>
 #include <asm/i387.h>
+#include <asm/fpu-internal.h>
 #include <asm/debugreg.h>
 
 struct kmem_cache *task_xstate_cachep;

commit 484546509ce5d49d43ec0a6eb2141c6bf3362bfc
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Feb 7 09:40:30 2012 -0500

    x86/tracing: Denote the power and cpuidle tracepoints as _rcuidle()
    
    The power and cpuidle tracepoints are called within a rcu_idle_exit()
    section, and must be denoted with the _rcuidle() version of the tracepoint.
    
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 15763af7bfe3..44eefde92109 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -377,8 +377,8 @@ static inline int hlt_use_halt(void)
 void default_idle(void)
 {
 	if (hlt_use_halt()) {
-		trace_power_start(POWER_CSTATE, 1, smp_processor_id());
-		trace_cpu_idle(1, smp_processor_id());
+		trace_power_start_rcuidle(POWER_CSTATE, 1, smp_processor_id());
+		trace_cpu_idle_rcuidle(1, smp_processor_id());
 		current_thread_info()->status &= ~TS_POLLING;
 		/*
 		 * TS_POLLING-cleared state must be visible before we
@@ -391,8 +391,8 @@ void default_idle(void)
 		else
 			local_irq_enable();
 		current_thread_info()->status |= TS_POLLING;
-		trace_power_end(smp_processor_id());
-		trace_cpu_idle(PWR_EVENT_EXIT, smp_processor_id());
+		trace_power_end_rcuidle(smp_processor_id());
+		trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 	} else {
 		local_irq_enable();
 		/* loop is done by the caller */
@@ -450,8 +450,8 @@ EXPORT_SYMBOL_GPL(cpu_idle_wait);
 static void mwait_idle(void)
 {
 	if (!need_resched()) {
-		trace_power_start(POWER_CSTATE, 1, smp_processor_id());
-		trace_cpu_idle(1, smp_processor_id());
+		trace_power_start_rcuidle(POWER_CSTATE, 1, smp_processor_id());
+		trace_cpu_idle_rcuidle(1, smp_processor_id());
 		if (this_cpu_has(X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
 
@@ -461,8 +461,8 @@ static void mwait_idle(void)
 			__sti_mwait(0, 0);
 		else
 			local_irq_enable();
-		trace_power_end(smp_processor_id());
-		trace_cpu_idle(PWR_EVENT_EXIT, smp_processor_id());
+		trace_power_end_rcuidle(smp_processor_id());
+		trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 	} else
 		local_irq_enable();
 }
@@ -474,13 +474,13 @@ static void mwait_idle(void)
  */
 static void poll_idle(void)
 {
-	trace_power_start(POWER_CSTATE, 0, smp_processor_id());
-	trace_cpu_idle(0, smp_processor_id());
+	trace_power_start_rcuidle(POWER_CSTATE, 0, smp_processor_id());
+	trace_cpu_idle_rcuidle(0, smp_processor_id());
 	local_irq_enable();
 	while (!need_resched())
 		cpu_relax();
-	trace_power_end(smp_processor_id());
-	trace_cpu_idle(PWR_EVENT_EXIT, smp_processor_id());
+	trace_power_end_rcuidle(smp_processor_id());
+	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
 
 /*

commit 1cf8343f55525c09c88da0a494a96e1b034f84e2
Author: Seiichi Ikarashi <s.ikarashi@jp.fujitsu.com>
Date:   Tue Dec 6 17:58:14 2011 +0900

    x86: Fix rflags in FAKE_STACK_FRAME
    
    The x86_64 kernel pushes the fake kernel stack in
    arch/x86/kernel/entry_64.S:FAKE_STACK_FRAME, and
    rflags register in it does not conform to the specification.
    
    Although Intel's manual[1] says bit 1 of it shall be set to 1,
    this bit is cleared to 0 on pushing the fake stack.
    
    [1] Intel(R) 64 and IA-32 Architectures Software Developer's Manual
        Vol.1 3-21 Figure 3-8. EFLAGS Register
    
    If it is not on purpose, it is better to be fixed, because
    it can lead some tools misunderstanding the stack frame. For example,
    "crash" utility[2] actually detects it and warns you like
    below:
    
           RIP: ffffffff8005dfa2  RSP: ffff8104ce0c7f58  RFLAGS: 00000200
           [...]
    
           bt: WARNING: possibly bogus exception frame
    
    Signed-off-by: Seiichi Ikarashi <s.ikarashi@jp.fujitsu.com>
    Tested-by: Masayoshi MIZUMA <m.mizuma@jp.fujitsu.com>
    Cc: Jan Beulich <JBeulich@suse.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ee5d4fbd53b4..15763af7bfe3 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -293,7 +293,7 @@ int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 	regs.orig_ax = -1;
 	regs.ip = (unsigned long) kernel_thread_helper;
 	regs.cs = __KERNEL_CS | get_kernel_rpl();
-	regs.flags = X86_EFLAGS_IF | 0x2;
+	regs.flags = X86_EFLAGS_IF | X86_EFLAGS_BIT1;
 
 	/* Ok, create the new process.. */
 	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);

commit e5fd47bfab2df0c2184cc0bf4245d8e1bb7724fb
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Nov 21 18:02:02 2011 -0500

    xen/pm_idle: Make pm_idle be default_idle under Xen.
    
    The idea behind commit d91ee5863b71 ("cpuidle: replace xen access to x86
    pm_idle and default_idle") was to have one call - disable_cpuidle()
    which would make pm_idle not be molested by other code.  It disallows
    cpuidle_idle_call to be set to pm_idle (which is excellent).
    
    But in the select_idle_routine() and idle_setup(), the pm_idle can still
    be set to either: amd_e400_idle, mwait_idle or default_idle.  This
    depends on some CPU flags (MWAIT) and in AMD case on the type of CPU.
    
    In case of mwait_idle we can hit some instances where the hypervisor
    (Amazon EC2 specifically) sets the MWAIT and we get:
    
      Brought up 2 CPUs
      invalid opcode: 0000 [#1] SMP
    
      Pid: 0, comm: swapper Not tainted 3.1.0-0.rc6.git0.3.fc16.x86_64 #1
      RIP: e030:[<ffffffff81015d1d>]  [<ffffffff81015d1d>] mwait_idle+0x6f/0xb4
      ...
      Call Trace:
       [<ffffffff8100e2ed>] cpu_idle+0xae/0xe8
       [<ffffffff8149ee78>] cpu_bringup_and_idle+0xe/0x10
      RIP  [<ffffffff81015d1d>] mwait_idle+0x6f/0xb4
       RSP <ffff8801d28ddf10>
    
    In the case of amd_e400_idle we don't get so spectacular crashes, but we
    do end up making an MSR which is trapped in the hypervisor, and then
    follow it up with a yield hypercall.  Meaning we end up going to
    hypervisor twice instead of just once.
    
    The previous behavior before v3.0 was that pm_idle was set to
    default_idle regardless of select_idle_routine/idle_setup.
    
    We want to do that, but only for one specific case: Xen.  This patch
    does that.
    
    Fixes RH BZ #739499 and Ubuntu #881076
    Reported-by: Stefan Bader <stefan.bader@canonical.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index b9b3b1a51643..ee5d4fbd53b4 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -403,6 +403,14 @@ void default_idle(void)
 EXPORT_SYMBOL(default_idle);
 #endif
 
+bool set_pm_idle_to_default(void)
+{
+	bool ret = !!pm_idle;
+
+	pm_idle = default_idle;
+
+	return ret;
+}
 void stop_this_cpu(void *dummy)
 {
 	local_irq_disable();

commit c812d8f7ad27bd5f1ed1c790eea8527b6f8ff1c7
Author: Zhao Jin <cronozhj@gmail.com>
Date:   Sat Aug 20 21:24:57 2011 +0800

    x86, mm, trivial: Remove unnecessary get_order() in free_thread_info()
    
    Because THREAD_SIZE is defined as PAGE_SIZE << THREAD_ORDER on x86, the
    call of get_order(THREAD_SIZE) can be replaced with THREAD_ORDER.
    
    Signed-off-by: Zhao Jin <cronozhj@gmail.com>
    Link: http://lkml.kernel.org/r/4E4FB5A9.700@gmail.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e7e3b019c439..b9b3b1a51643 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -49,7 +49,7 @@ void free_thread_xstate(struct task_struct *tsk)
 void free_thread_info(struct thread_info *ti)
 {
 	free_thread_xstate(ti->task);
-	free_pages((unsigned long)ti, get_order(THREAD_SIZE));
+	free_pages((unsigned long)ti, THREAD_ORDER);
 }
 
 void arch_task_cache_init(void)

commit 4bfc8288bc4a64529c5547d17349a2a1f4675507
Author: Len Brown <len.brown@intel.com>
Date:   Wed Mar 30 23:52:29 2011 -0400

    x86 idle: move mwait_idle_with_hints() to where it is used
    
    ...and make it static
    
    no functional change
    
    cc: x86@kernel.org
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e1ba8cb24e4e..e7e3b019c439 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -438,29 +438,6 @@ void cpu_idle_wait(void)
 }
 EXPORT_SYMBOL_GPL(cpu_idle_wait);
 
-/*
- * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
- * which can obviate IPI to trigger checking of need_resched.
- * We execute MONITOR against need_resched and enter optimized wait state
- * through MWAIT. Whenever someone changes need_resched, we would be woken
- * up from MWAIT (without an IPI).
- *
- * New with Core Duo processors, MWAIT can take some hints based on CPU
- * capability.
- */
-void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
-{
-	if (!need_resched()) {
-		if (this_cpu_has(X86_FEATURE_CLFLUSH_MONITOR))
-			clflush((void *)&current_thread_info()->flags);
-
-		__monitor((void *)&current_thread_info()->flags, 0, 0);
-		smp_mb();
-		if (!need_resched())
-			__mwait(ax, cx);
-	}
-}
-
 /* Default MONITOR/MWAIT with no hints, used for default C1 state */
 static void mwait_idle(void)
 {

commit 60b8b1de0dd2bf246f0e074d287bb3f0bc42a755
Author: Andy Whitcroft <apw@canonical.com>
Date:   Tue Jun 14 12:45:10 2011 -0700

    x86 idle: APM requires pm_idle/default_idle unconditionally when a module
    
    [ Also from Ben Hutchings <ben@decadent.org.uk> and Vitaliy Ivanov
      <vitalivanov@gmail.com> ]
    
    Commit 06ae40ce073d ("x86 idle: EXPORT_SYMBOL(default_idle, pm_idle)
    only when APM demands it") removed the export for pm_idle/default_idle
    unless the apm module was modularised and CONFIG_APM_CPU_IDLE was set.
    
    But the apm module uses pm_idle/default_idle unconditionally,
    CONFIG_APM_CPU_IDLE only affects the bios idle threshold.  Adjust the
    export accordingly.
    
    [ Used #ifdef instead of #if defined() as it's shorter, and what both
      Ben and Vitaliy used.. Andy, you're out-voted ;)    - Linus ]
    
    Reported-by: Randy Dunlap <randy.dunlap@oracle.com>
    Acked-by: Jiri Kosina <jkosina@suse.cz>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Andy Whitcroft <apw@canonical.com>
    Signed-off-by: Vitaliy Ivanov <vitalivanov@gmail.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 2e4928d45a2d..e1ba8cb24e4e 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -337,7 +337,7 @@ EXPORT_SYMBOL(boot_option_idle_override);
  * Powermanagement idle function, if any..
  */
 void (*pm_idle)(void);
-#if defined(CONFIG_APM_MODULE) && defined(CONFIG_APM_CPU_IDLE)
+#ifdef CONFIG_APM_MODULE
 EXPORT_SYMBOL(pm_idle);
 #endif
 
@@ -399,7 +399,7 @@ void default_idle(void)
 		cpu_relax();
 	}
 }
-#if defined(CONFIG_APM_MODULE) && defined(CONFIG_APM_CPU_IDLE)
+#ifdef CONFIG_APM_MODULE
 EXPORT_SYMBOL(default_idle);
 #endif
 

commit af0d6a0a3a30946f7df69c764791f1b0643f7cd6
Merge: 07ef3c3b4489 4f3c125c7420
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 1 02:07:22 2011 +0900

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Fix mwait_play_dead() faulting on mwait-incapable cpus
      x86 idle: Fix mwait deprecation warning message
    
    Evil merge to remove extra quote noticed by Joe Perches

commit 598e887d8b01655780c81cc86a9e7820ed091580
Author: Borislav Petkov <bp@alien8.de>
Date:   Mon May 30 11:38:06 2011 +0200

    x86 idle: Fix mwait deprecation warning message
    
    Fix:
    
      arch/x86/kernel/process.c:645:1: warning: unknown escape sequence '\i'
    
    due to missing escape backslash, introduced by this commit:
    
      5d4c47e0195b: x86 idle: deprecate mwait_idle() and "idle=mwait" cmdline param
    
    Signed-off-by: Borislav Petkov <bp@alien8.de>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1306748286-24701-1-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 426a5b66f7e4..3d83a71af7d5 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -642,7 +642,7 @@ static int __init idle_setup(char *str)
 		boot_option_idle_override = IDLE_POLL;
 	} else if (!strcmp(str, "mwait")) {
 		boot_option_idle_override = IDLE_FORCE_MWAIT;
-		WARN_ONCE(1, "\idle=mwait\" will be removed in 2012\"\n");
+		WARN_ONCE(1, "\"idle=mwait\" will be removed in 2012\"\n");
 	} else if (!strcmp(str, "halt")) {
 		/*
 		 * When the boot option of idle=halt is added, halt is

commit f310642123e0d32d919c60ca3fab5acd130c4ba3
Merge: ef1d57599dc9 5d4c47e0195b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 29 11:18:09 2011 -0700

    Merge branch 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6
    
    * 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6:
      x86 idle: deprecate mwait_idle() and "idle=mwait" cmdline param
      x86 idle: deprecate "no-hlt" cmdline param
      x86 idle APM: deprecate CONFIG_APM_CPU_IDLE
      x86 idle floppy: deprecate disable_hlt()
      x86 idle: EXPORT_SYMBOL(default_idle, pm_idle) only when APM demands it
      x86 idle: clarify AMD erratum 400 workaround
      idle governor: Avoid lock acquisition to read pm_qos before entering idle
      cpuidle: menu: fixed wrapping timers at 4.294 seconds

commit 5d4c47e0195b989f284907358bd5c268a44b91c7
Author: Len Brown <len.brown@intel.com>
Date:   Fri Apr 1 15:46:09 2011 -0400

    x86 idle: deprecate mwait_idle() and "idle=mwait" cmdline param
    
    mwait_idle() is a C1-only idle loop intended to be more efficient
    than HLT on SMP hardware that supports it.
    
    But mwait_idle() has been replaced by the more general
    mwait_idle_with_hints(), which handles both C1 and deeper C-states.
    ACPI uses only mwait_idle_with_hints(), and never uses mwait_idle().
    
    Deprecate mwait_idle() and the "idle=mwait" cmdline param
    to simplify the x86 idle code.
    
    After this change, kernels configured with
    (!CONFIG_ACPI=n && !CONFIG_INTEL_IDLE=n) when run on hardware
    that support MWAIT will simply use HLT.  If MWAIT is desired
    on those systems, cpuidle and the cpuidle drivers above
    can be used.
    
    cc: x86@kernel.org
    cc: stable@kernel.org # .39.x
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 84f3cdae4407..8fb182956cbc 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -645,6 +645,7 @@ static int __init idle_setup(char *str)
 		boot_option_idle_override = IDLE_POLL;
 	} else if (!strcmp(str, "mwait")) {
 		boot_option_idle_override = IDLE_FORCE_MWAIT;
+		WARN_ONCE(1, "\idle=mwait\" will be removed in 2012\"\n");
 	} else if (!strcmp(str, "halt")) {
 		/*
 		 * When the boot option of idle=halt is added, halt is

commit 06ae40ce073daf233607a3c54a489f2c1e44683e
Author: Len Brown <len.brown@intel.com>
Date:   Fri Apr 1 15:28:09 2011 -0400

    x86 idle: EXPORT_SYMBOL(default_idle, pm_idle) only when APM demands it
    
    In the long run, we don't want default_idle() or (pm_idle)() to
    be exported outside of process.c.  Start by not exporting them
    to modules, unless the APM build demands it.
    
    cc: x86@kernel.org
    cc: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 2efbfb712fb7..84f3cdae4407 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -340,7 +340,9 @@ EXPORT_SYMBOL(boot_option_idle_override);
  * Powermanagement idle function, if any..
  */
 void (*pm_idle)(void);
+#if defined(CONFIG_APM_MODULE) && defined(CONFIG_APM_CPU_IDLE)
 EXPORT_SYMBOL(pm_idle);
+#endif
 
 #ifdef CONFIG_X86_32
 /*
@@ -400,7 +402,7 @@ void default_idle(void)
 		cpu_relax();
 	}
 }
-#ifdef CONFIG_APM_MODULE
+#if defined(CONFIG_APM_MODULE) && defined(CONFIG_APM_CPU_IDLE)
 EXPORT_SYMBOL(default_idle);
 #endif
 

commit 02c68a02018669d1817c43c42de800975cbec467
Author: Len Brown <len.brown@intel.com>
Date:   Fri Apr 1 16:59:53 2011 -0400

    x86 idle: clarify AMD erratum 400 workaround
    
    The workaround for AMD erratum 400 uses the term "c1e" falsely suggesting:
    1. Intel C1E is somehow involved
    2. All AMD processors with C1E are involved
    
    Use the string "amd_c1e" instead of simply "c1e" to clarify that
    this workaround is specific to AMD's version of C1E.
    Use the string "e400" to clarify that the workaround is specific
    to AMD processors with Erratum 400.
    
    This patch is text-substitution only, with no functional change.
    
    cc: x86@kernel.org
    Acked-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ff4554198981..2efbfb712fb7 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -538,45 +538,45 @@ int mwait_usable(const struct cpuinfo_x86 *c)
 	return (edx & MWAIT_EDX_C1);
 }
 
-bool c1e_detected;
-EXPORT_SYMBOL(c1e_detected);
+bool amd_e400_c1e_detected;
+EXPORT_SYMBOL(amd_e400_c1e_detected);
 
-static cpumask_var_t c1e_mask;
+static cpumask_var_t amd_e400_c1e_mask;
 
-void c1e_remove_cpu(int cpu)
+void amd_e400_remove_cpu(int cpu)
 {
-	if (c1e_mask != NULL)
-		cpumask_clear_cpu(cpu, c1e_mask);
+	if (amd_e400_c1e_mask != NULL)
+		cpumask_clear_cpu(cpu, amd_e400_c1e_mask);
 }
 
 /*
- * C1E aware idle routine. We check for C1E active in the interrupt
+ * AMD Erratum 400 aware idle routine. We check for C1E active in the interrupt
  * pending message MSR. If we detect C1E, then we handle it the same
  * way as C3 power states (local apic timer and TSC stop)
  */
-static void c1e_idle(void)
+static void amd_e400_idle(void)
 {
 	if (need_resched())
 		return;
 
-	if (!c1e_detected) {
+	if (!amd_e400_c1e_detected) {
 		u32 lo, hi;
 
 		rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
 
 		if (lo & K8_INTP_C1E_ACTIVE_MASK) {
-			c1e_detected = true;
+			amd_e400_c1e_detected = true;
 			if (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))
 				mark_tsc_unstable("TSC halt in AMD C1E");
 			printk(KERN_INFO "System has AMD C1E enabled\n");
 		}
 	}
 
-	if (c1e_detected) {
+	if (amd_e400_c1e_detected) {
 		int cpu = smp_processor_id();
 
-		if (!cpumask_test_cpu(cpu, c1e_mask)) {
-			cpumask_set_cpu(cpu, c1e_mask);
+		if (!cpumask_test_cpu(cpu, amd_e400_c1e_mask)) {
+			cpumask_set_cpu(cpu, amd_e400_c1e_mask);
 			/*
 			 * Force broadcast so ACPI can not interfere.
 			 */
@@ -619,17 +619,17 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 		pm_idle = mwait_idle;
 	} else if (cpu_has_amd_erratum(amd_erratum_400)) {
 		/* E400: APIC timer interrupt does not wake up CPU from C1e */
-		printk(KERN_INFO "using C1E aware idle routine\n");
-		pm_idle = c1e_idle;
+		printk(KERN_INFO "using AMD E400 aware idle routine\n");
+		pm_idle = amd_e400_idle;
 	} else
 		pm_idle = default_idle;
 }
 
-void __init init_c1e_mask(void)
+void __init init_amd_e400_c1e_mask(void)
 {
-	/* If we're using c1e_idle, we need to allocate c1e_mask. */
-	if (pm_idle == c1e_idle)
-		zalloc_cpumask_var(&c1e_mask, GFP_KERNEL);
+	/* If we're using amd_e400_idle, we need to allocate amd_e400_c1e_mask. */
+	if (pm_idle == amd_e400_idle)
+		zalloc_cpumask_var(&amd_e400_c1e_mask, GFP_KERNEL);
 }
 
 static int __init idle_setup(char *str)

commit 349c004e3d31fda23ad225b61861be38047fff16
Author: Christoph Lameter <cl@linux.com>
Date:   Sat Mar 12 12:50:10 2011 +0100

    x86: A fast way to check capabilities of the current cpu
    
    Add this_cpu_has() which determines if the current cpu has a certain
    ability using a segment prefix and a bit test operation.
    
    For that we need to add bit operations to x86s percpu.h.
    
    Many uses of cpu_has use a pointer passed to a function to determine
    the current flags. That is no longer necessary after this patch.
    
    However, this patch only converts the straightforward cases where
    cpu_has is used with this_cpu_ptr. The rest is work for later.
    
    -tj: Rolled up patch to add x86_ prefix and use percpu_read() instead
         of percpu_read_stable().
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index d46cbe46b7ab..88a90a977f8e 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -449,7 +449,7 @@ EXPORT_SYMBOL_GPL(cpu_idle_wait);
 void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 {
 	if (!need_resched()) {
-		if (cpu_has(__this_cpu_ptr(&cpu_info), X86_FEATURE_CLFLUSH_MONITOR))
+		if (this_cpu_has(X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
 
 		__monitor((void *)&current_thread_info()->flags, 0, 0);
@@ -465,7 +465,7 @@ static void mwait_idle(void)
 	if (!need_resched()) {
 		trace_power_start(POWER_CSTATE, 1, smp_processor_id());
 		trace_cpu_idle(1, smp_processor_id());
-		if (cpu_has(__this_cpu_ptr(&cpu_info), X86_FEATURE_CLFLUSH_MONITOR))
+		if (this_cpu_has(X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
 
 		__monitor((void *)&current_thread_info()->flags, 0, 0);

commit e8e999cf3cc733482e390b02ff25a64cecdc0b64
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Fri Mar 18 11:40:06 2011 +0900

    x86, dumpstack: Correct stack dump info when frame pointer is available
    
    Current stack dump code scans entire stack and check each entry
    contains a pointer to kernel code. If CONFIG_FRAME_POINTER=y it
    could mark whether the pointer is valid or not based on value of
    the frame pointer. Invalid entries could be preceded by '?' sign.
    
    However this was not going to happen because scan start point
    was always higher than the frame pointer so that they could not
    meet.
    
    Commit 9c0729dc8062 ("x86: Eliminate bp argument from the stack
    tracing routines") delayed bp acquisition point, so the bp was
    read in lower frame, thus all of the entries were marked
    invalid.
    
    This patch fixes this by reverting above commit while retaining
    stack_frame() helper as suggested by Frederic Weisbecker.
    
    End result looks like below:
    
    before:
    
     [    3.508329] Call Trace:
     [    3.508551]  [<ffffffff814f35c9>] ? panic+0x91/0x199
     [    3.508662]  [<ffffffff814f3739>] ? printk+0x68/0x6a
     [    3.508770]  [<ffffffff81a981b2>] ? mount_block_root+0x257/0x26e
     [    3.508876]  [<ffffffff81a9821f>] ? mount_root+0x56/0x5a
     [    3.508975]  [<ffffffff81a98393>] ? prepare_namespace+0x170/0x1a9
     [    3.509216]  [<ffffffff81a9772b>] ? kernel_init+0x1d2/0x1e2
     [    3.509335]  [<ffffffff81003894>] ? kernel_thread_helper+0x4/0x10
     [    3.509442]  [<ffffffff814f6880>] ? restore_args+0x0/0x30
     [    3.509542]  [<ffffffff81a97559>] ? kernel_init+0x0/0x1e2
     [    3.509641]  [<ffffffff81003890>] ? kernel_thread_helper+0x0/0x10
    
    after:
    
     [    3.522991] Call Trace:
     [    3.523351]  [<ffffffff814f35b9>] panic+0x91/0x199
     [    3.523468]  [<ffffffff814f3729>] ? printk+0x68/0x6a
     [    3.523576]  [<ffffffff81a981b2>] mount_block_root+0x257/0x26e
     [    3.523681]  [<ffffffff81a9821f>] mount_root+0x56/0x5a
     [    3.523780]  [<ffffffff81a98393>] prepare_namespace+0x170/0x1a9
     [    3.523885]  [<ffffffff81a9772b>] kernel_init+0x1d2/0x1e2
     [    3.523987]  [<ffffffff81003894>] kernel_thread_helper+0x4/0x10
     [    3.524228]  [<ffffffff814f6880>] ? restore_args+0x0/0x30
     [    3.524345]  [<ffffffff81a97559>] ? kernel_init+0x0/0x1e2
     [    3.524445]  [<ffffffff81003890>] ? kernel_thread_helper+0x0/0x10
    
     -v5:
       * fix build breakage with oprofile
    
     -v4:
       * use 0 instead of regs->bp
       * separate out printk changes
    
     -v3:
       * apply comment from Frederic
       * add a couple of printk fixes
    
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Soren Sandmann <ssp@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Robert Richter <robert.richter@amd.com>
    LKML-Reference: <1300416006-3163-1-git-send-email-namhyung@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 99fa3adf0141..d46cbe46b7ab 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -87,7 +87,7 @@ void exit_thread(void)
 void show_regs(struct pt_regs *regs)
 {
 	show_registers(regs);
-	show_trace(NULL, regs, (unsigned long *)kernel_stack_pointer(regs));
+	show_trace(NULL, regs, (unsigned long *)kernel_stack_pointer(regs), 0);
 }
 
 void show_regs_common(void)

commit fd8fa4d3ddc4cc04ec8097e632b995d535c52beb
Author: Jan Beulich <JBeulich@novell.com>
Date:   Thu Feb 17 15:56:58 2011 +0000

    x86: Combine printk()s in show_regs_common()
    
    Printing a single character alone when there's an immediately
    following printk() is pretty pointless (and wasteful).
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    LKML-Reference: <4D5D535A0200007800032730@vpn.id2.novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ff4554198981..99fa3adf0141 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -110,12 +110,9 @@ void show_regs_common(void)
 		init_utsname()->release,
 		(int)strcspn(init_utsname()->version, " "),
 		init_utsname()->version);
-	printk(KERN_CONT " ");
-	printk(KERN_CONT "%s %s", vendor, product);
-	if (board) {
-		printk(KERN_CONT "/");
-		printk(KERN_CONT "%s", board);
-	}
+	printk(KERN_CONT " %s %s", vendor, product);
+	if (board)
+		printk(KERN_CONT "/%s", board);
 	printk(KERN_CONT "\n");
 }
 

commit 84e383b322e5348db03be54ff64cc6da87003717
Author: Naga Chumbalkar <nagananda.chumbalkar@hp.com>
Date:   Mon Feb 14 22:47:17 2011 +0000

    x86, dmi, debug: Log board name (when present) in dmesg/oops output
    
    The "Type 2" SMBIOS record that contains Board Name is not
    strictly required and may be absent in the SMBIOS on some
    platforms.
    
    ( Please note that Type 2 is not listed in Table 3 in Sec 6.2
      ("Required Structures and Data") of the SMBIOS v2.7
      Specification. )
    
    Use the Manufacturer Name (aka System Vendor) name.
    Print Board Name only when it is present.
    
    Before the fix:
      (i) dmesg output: DMI: /ProLiant DL380 G6, BIOS P62 01/29/2011
     (ii) oops output:  Pid: 2170, comm: bash Not tainted 2.6.38-rc4+ #3 /ProLiant DL380 G6
    
    After the fix:
      (i) dmesg output: DMI: HP ProLiant DL380 G6, BIOS P62 01/29/2011
     (ii) oops output:  Pid: 2278, comm: bash Not tainted 2.6.38-rc4+ #4 HP ProLiant DL380 G6
    
    Signed-off-by: Naga Chumbalkar <nagananda.chumbalkar@hp.com>
    Reviewed-by: Bjorn Helgaas <bjorn.helgaas@hp.com>
    Cc: <stable@kernel.org> # .3x - good for debugging, please apply as far back as it applies cleanly
    LKML-Reference: <20110214224423.2182.13929.sendpatchset@nchumbalkar.americas.hpqcorp.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 3c189e9accd3..ff4554198981 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -92,21 +92,31 @@ void show_regs(struct pt_regs *regs)
 
 void show_regs_common(void)
 {
-	const char *board, *product;
+	const char *vendor, *product, *board;
 
-	board = dmi_get_system_info(DMI_BOARD_NAME);
-	if (!board)
-		board = "";
+	vendor = dmi_get_system_info(DMI_SYS_VENDOR);
+	if (!vendor)
+		vendor = "";
 	product = dmi_get_system_info(DMI_PRODUCT_NAME);
 	if (!product)
 		product = "";
 
+	/* Board Name is optional */
+	board = dmi_get_system_info(DMI_BOARD_NAME);
+
 	printk(KERN_CONT "\n");
-	printk(KERN_DEFAULT "Pid: %d, comm: %.20s %s %s %.*s %s/%s\n",
+	printk(KERN_DEFAULT "Pid: %d, comm: %.20s %s %s %.*s",
 		current->pid, current->comm, print_tainted(),
 		init_utsname()->release,
 		(int)strcspn(init_utsname()->version, " "),
-		init_utsname()->version, board, product);
+		init_utsname()->version);
+	printk(KERN_CONT " ");
+	printk(KERN_CONT "%s %s", vendor, product);
+	if (board) {
+		printk(KERN_CONT "/");
+		printk(KERN_CONT "%s", board);
+	}
+	printk(KERN_CONT "\n");
 }
 
 void flush_thread(void)

commit 1c9d16e35911090dee3f9313e6af13af623d66ee
Author: Borislav Petkov <bp@amd64.org>
Date:   Fri Feb 11 18:17:54 2011 +0100

    x86: Fix mwait_usable section mismatch
    
    We use it in non __cpuinit code now too so drop marker.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <20110211171754.GA21047@aftab>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e764fc05d700..3c189e9accd3 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -506,7 +506,7 @@ static void poll_idle(void)
 #define MWAIT_ECX_EXTENDED_INFO		0x01
 #define MWAIT_EDX_C1			0xf0
 
-int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
+int mwait_usable(const struct cpuinfo_x86 *c)
 {
 	u32 eax, ebx, ecx, edx;
 

commit 93789b32dbf355e70f18b17a82e8661677a7f7fb
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Thu Jan 20 15:42:52 2011 +0100

    x86, hotplug: Fix powersavings with offlined cores on AMD
    
    ea53069231f9317062910d6e772cca4ce93de8c8 made a CPU use monitor/mwait
    when offline. This is not the optimal choice for AMD wrt to powersavings
    and we'd prefer our cores to halt (i.e. enter C1) instead. For this, the
    same selection whether to use monitor/mwait has to be used as when we
    select the idle routine for the machine.
    
    With this patch, offlining cores 1-5 on a X6 machine allows core0 to
    boost again.
    
    [ hpa: putting this in urgent since it is a (power) regression fix ]
    
    Reported-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: stable@kernel.org # 37.x
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.hl>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <1295534572-10730-1-git-send-email-bp@amd64.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index d8286ed54ffa..e764fc05d700 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -14,6 +14,7 @@
 #include <linux/utsname.h>
 #include <trace/events/power.h>
 #include <linux/hw_breakpoint.h>
+#include <asm/cpu.h>
 #include <asm/system.h>
 #include <asm/apic.h>
 #include <asm/syscalls.h>
@@ -505,7 +506,7 @@ static void poll_idle(void)
 #define MWAIT_ECX_EXTENDED_INFO		0x01
 #define MWAIT_EDX_C1			0xf0
 
-static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
+int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
 {
 	u32 eax, ebx, ecx, edx;
 

commit 43952886f0b8b3c344c3392b88de067d5fa5419a
Merge: 56dbed129df3 f77cfe4ea217
Author: Len Brown <len.brown@intel.com>
Date:   Wed Jan 12 18:06:19 2011 -0500

    Merge branch 'cpuidle-perf-events' into idle-test

commit 56dbed129df3fdd4caf9018b6e7599ee258a5420
Merge: 2a2d31c8dc6f f878133bf022
Author: Len Brown <len.brown@intel.com>
Date:   Wed Jan 12 18:06:06 2011 -0500

    Merge branch 'linus' into idle-test

commit f77cfe4ea21760268c0277fa3e4b02dfd2a2c2f4
Author: Thomas Renninger <trenn@suse.de>
Date:   Fri Jan 7 11:29:44 2011 +0100

    cpuidle/x86/perf: fix power:cpu_idle double end events and throw cpu_idle events from the cpuidle layer
    
    Currently intel_idle and acpi_idle driver show double cpu_idle "exit idle"
    events -> this patch fixes it and makes cpu_idle events throwing less complex.
    
    It also introduces cpu_idle events for all architectures which use
    the cpuidle subsystem, namely:
      - arch/arm/mach-at91/cpuidle.c
      - arch/arm/mach-davinci/cpuidle.c
      - arch/arm/mach-kirkwood/cpuidle.c
      - arch/arm/mach-omap2/cpuidle34xx.c
      - arch/drivers/acpi/processor_idle.c (for all cases, not only mwait)
      - arch/x86/kernel/process.c (did throw events before, but was a mess)
      - drivers/idle/intel_idle.c (did throw events before)
    
    Convention should be:
    Fire cpu_idle events inside the current pm_idle function (not somewhere
    down the the callee tree) to keep things easy.
    
    Current possible pm_idle functions in X86:
    c1e_idle, poll_idle, cpuidle_idle_call, mwait_idle, default_idle
    -> this is really easy is now.
    
    This affects userspace:
    The type field of the cpu_idle power event can now direclty get
    mapped to:
    /sys/devices/system/cpu/cpuX/cpuidle/stateX/{name,desc,usage,time,...}
    instead of throwing very CPU/mwait specific values.
    This change is not visible for the intel_idle driver.
    For the acpi_idle driver it should only be visible if the vendor
    misses out C-states in his BIOS.
    Another (perf timechart) patch reads out cpuidle info of cpu_idle
    events from:
    /sys/.../cpuidle/stateX/*, then the cpuidle events are mapped
    to the correct C-/cpuidle state again, even if e.g. vendors miss
    out C-states in their BIOS and for example only export C1 and C3.
    -> everything is fine.
    
    Signed-off-by: Thomas Renninger <trenn@suse.de>
    CC: Robert Schoene <robert.schoene@tu-dresden.de>
    CC: Jean Pihet <j-pihet@ti.com>
    CC: Arjan van de Ven <arjan@linux.intel.com>
    CC: Ingo Molnar <mingo@elte.hu>
    CC: Frederic Weisbecker <fweisbec@gmail.com>
    CC: linux-pm@lists.linux-foundation.org
    CC: linux-acpi@vger.kernel.org
    CC: linux-kernel@vger.kernel.org
    CC: linux-perf-users@vger.kernel.org
    CC: linux-omap@vger.kernel.org
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 09c08a1c706f..67e96e67157b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -386,6 +386,8 @@ void default_idle(void)
 		else
 			local_irq_enable();
 		current_thread_info()->status |= TS_POLLING;
+		trace_power_end(smp_processor_id());
+		trace_cpu_idle(PWR_EVENT_EXIT, smp_processor_id());
 	} else {
 		local_irq_enable();
 		/* loop is done by the caller */
@@ -443,8 +445,6 @@ EXPORT_SYMBOL_GPL(cpu_idle_wait);
  */
 void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 {
-	trace_power_start(POWER_CSTATE, (ax>>4)+1, smp_processor_id());
-	trace_cpu_idle((ax>>4)+1, smp_processor_id());
 	if (!need_resched()) {
 		if (cpu_has(__this_cpu_ptr(&cpu_info), X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
@@ -471,6 +471,8 @@ static void mwait_idle(void)
 			__sti_mwait(0, 0);
 		else
 			local_irq_enable();
+		trace_power_end(smp_processor_id());
+		trace_cpu_idle(PWR_EVENT_EXIT, smp_processor_id());
 	} else
 		local_irq_enable();
 }

commit d18960494f65ca4fa0d67c865aaca99452070d15
Author: Thomas Renninger <trenn@suse.de>
Date:   Wed Nov 3 17:06:14 2010 +0100

    ACPI, intel_idle: Cleanup idle= internal variables
    
    Having four variables for the same thing:
      idle_halt, idle_nomwait, force_mwait and boot_option_idle_overrides
    is rather confusing and unnecessary complex.
    
    if idle= boot param is passed, only set up one variable:
    boot_option_idle_overrides
    
    Introduces following functional changes/fixes:
      - intel_idle driver does not register if any idle=xy
        boot param is passed.
      - processor_idle.c will also not register a cpuidle driver
        and get active if idle=halt is passed.
        Before a cpuidle driver with one (C1, halt) state got registered
        Now the default_idle function will be used which finally uses
        the same idle call to enter sleep state (safe_halt()), but
        without registering a whole cpuidle driver.
    
    That means idle= param will always avoid cpuidle drivers to register
    with one exception (same behavior as before):
    idle=nomwait
    may still register acpi_idle cpuidle driver, but C1 will not use
    mwait, but hlt. This can be a workaround for IO based deeper sleep
    states where C1 mwait causes problems.
    
    Signed-off-by: Thomas Renninger <trenn@suse.de>
    cc: x86@kernel.org
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 57d1868a86aa..b6472153e45b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -22,11 +22,6 @@
 #include <asm/i387.h>
 #include <asm/debugreg.h>
 
-unsigned long idle_halt;
-EXPORT_SYMBOL(idle_halt);
-unsigned long idle_nomwait;
-EXPORT_SYMBOL(idle_nomwait);
-
 struct kmem_cache *task_xstate_cachep;
 EXPORT_SYMBOL_GPL(task_xstate_cachep);
 
@@ -328,7 +323,7 @@ long sys_execve(const char __user *name,
 /*
  * Idle related variables and functions
  */
-unsigned long boot_option_idle_override = 0;
+unsigned long boot_option_idle_override = IDLE_NO_OVERRIDE;
 EXPORT_SYMBOL(boot_option_idle_override);
 
 /*
@@ -499,7 +494,6 @@ static void poll_idle(void)
  *
  * idle=mwait overrides this decision and forces the usage of mwait.
  */
-static int __cpuinitdata force_mwait;
 
 #define MWAIT_INFO			0x05
 #define MWAIT_ECX_EXTENDED_INFO		0x01
@@ -509,7 +503,7 @@ static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
 {
 	u32 eax, ebx, ecx, edx;
 
-	if (force_mwait)
+	if (boot_option_idle_override == IDLE_FORCE_MWAIT)
 		return 1;
 
 	if (c->cpuid_level < MWAIT_INFO)
@@ -629,9 +623,10 @@ static int __init idle_setup(char *str)
 	if (!strcmp(str, "poll")) {
 		printk("using polling idle threads.\n");
 		pm_idle = poll_idle;
-	} else if (!strcmp(str, "mwait"))
-		force_mwait = 1;
-	else if (!strcmp(str, "halt")) {
+		boot_option_idle_override = IDLE_POLL;
+	} else if (!strcmp(str, "mwait")) {
+		boot_option_idle_override = IDLE_FORCE_MWAIT;
+	} else if (!strcmp(str, "halt")) {
 		/*
 		 * When the boot option of idle=halt is added, halt is
 		 * forced to be used for CPU idle. In such case CPU C2/C3
@@ -640,8 +635,7 @@ static int __init idle_setup(char *str)
 		 * the boot_option_idle_override.
 		 */
 		pm_idle = default_idle;
-		idle_halt = 1;
-		return 0;
+		boot_option_idle_override = IDLE_HALT;
 	} else if (!strcmp(str, "nomwait")) {
 		/*
 		 * If the boot option of "idle=nomwait" is added,
@@ -649,12 +643,10 @@ static int __init idle_setup(char *str)
 		 * states. In such case it won't touch the variable
 		 * of boot_option_idle_override.
 		 */
-		idle_nomwait = 1;
-		return 0;
+		boot_option_idle_override = IDLE_NOMWAIT;
 	} else
 		return -1;
 
-	boot_option_idle_override = 1;
 	return 0;
 }
 early_param("idle", idle_setup);

commit 72eb6a791459c87a0340318840bb3bd9252b627b
Merge: 23d69b09b78c 55ee4ef30241
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 7 17:02:58 2011 -0800

    Merge branch 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (30 commits)
      gameport: use this_cpu_read instead of lookup
      x86: udelay: Use this_cpu_read to avoid address calculation
      x86: Use this_cpu_inc_return for nmi counter
      x86: Replace uses of current_cpu_data with this_cpu ops
      x86: Use this_cpu_ops to optimize code
      vmstat: User per cpu atomics to avoid interrupt disable / enable
      irq_work: Use per cpu atomics instead of regular atomics
      cpuops: Use cmpxchg for xchg to avoid lock semantics
      x86: this_cpu_cmpxchg and this_cpu_xchg operations
      percpu: Generic this_cpu_cmpxchg() and this_cpu_xchg support
      percpu,x86: relocate this_cpu_add_return() and friends
      connector: Use this_cpu operations
      xen: Use this_cpu_inc_return
      taskstats: Use this_cpu_ops
      random: Use this_cpu_inc_return
      fs: Use this_cpu_inc_return in buffer.c
      highmem: Use this_cpu_xx_return() operations
      vmstat: Use this_cpu_inc_return for vm statistics
      x86: Support for this_cpu_add, sub, dec, inc_return
      percpu: Generic support for this_cpu_add, sub, dec, inc_return
      ...
    
    Fixed up conflicts: in arch/x86/kernel/{apic/nmi.c, apic/x2apic_uv_x.c, process.c}
    as per Tejun.

commit 25e41933b58777f2d020c3b0186b430ea004ec28
Author: Thomas Renninger <trenn@suse.de>
Date:   Mon Jan 3 17:50:44 2011 +0100

    perf: Clean up power events by introducing new, more generic ones
    
    Add these new power trace events:
    
     power:cpu_idle
     power:cpu_frequency
     power:machine_suspend
    
    The old C-state/idle accounting events:
      power:power_start
      power:power_end
    
    Have now a replacement (but we are still keeping the old
    tracepoints for compatibility):
    
      power:cpu_idle
    
    and
      power:power_frequency
    
    is replaced with:
      power:cpu_frequency
    
    power:machine_suspend is newly introduced.
    
    Jean Pihet has a patch integrated into the generic layer
    (kernel/power/suspend.c) which will make use of it.
    
    the type= field got removed from both, it was never
    used and the type is differed by the event type itself.
    
    perf timechart userspace tool gets adjusted in a separate patch.
    
    Signed-off-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Acked-by: Jean Pihet <jean.pihet@newoldbits.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: rjw@sisk.pl
    LKML-Reference: <1294073445-14812-3-git-send-email-trenn@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    LKML-Reference: <1290072314-31155-2-git-send-email-trenn@suse.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 96ed1aac543a..c852041bfc3d 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -373,6 +373,7 @@ void default_idle(void)
 {
 	if (hlt_use_halt()) {
 		trace_power_start(POWER_CSTATE, 1, smp_processor_id());
+		trace_cpu_idle(1, smp_processor_id());
 		current_thread_info()->status &= ~TS_POLLING;
 		/*
 		 * TS_POLLING-cleared state must be visible before we
@@ -443,6 +444,7 @@ EXPORT_SYMBOL_GPL(cpu_idle_wait);
 void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 {
 	trace_power_start(POWER_CSTATE, (ax>>4)+1, smp_processor_id());
+	trace_cpu_idle((ax>>4)+1, smp_processor_id());
 	if (!need_resched()) {
 		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
@@ -459,6 +461,7 @@ static void mwait_idle(void)
 {
 	if (!need_resched()) {
 		trace_power_start(POWER_CSTATE, 1, smp_processor_id());
+		trace_cpu_idle(1, smp_processor_id());
 		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
 
@@ -480,10 +483,12 @@ static void mwait_idle(void)
 static void poll_idle(void)
 {
 	trace_power_start(POWER_CSTATE, 0, smp_processor_id());
+	trace_cpu_idle(0, smp_processor_id());
 	local_irq_enable();
 	while (!need_resched())
 		cpu_relax();
-	trace_power_end(0);
+	trace_power_end(smp_processor_id());
+	trace_cpu_idle(PWR_EVENT_EXIT, smp_processor_id());
 }
 
 /*

commit 7b543a5334ff4ea2e3ad3b777fc23cdb8072a988
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Dec 18 16:30:05 2010 +0100

    x86: Replace uses of current_cpu_data with this_cpu ops
    
    Replace all uses of current_cpu_data with this_cpu operations on the
    per cpu structure cpu_info.  The scala accesses are replaced with the
    matching this_cpu ops which results in smaller and more efficient
    code.
    
    In the long run, it might be a good idea to remove cpu_data() macro
    too and use per_cpu macro directly.
    
    tj: updated description
    
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 57d1868a86aa..dae1c0766d9a 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -445,7 +445,7 @@ void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 {
 	trace_power_start(POWER_CSTATE, (ax>>4)+1, smp_processor_id());
 	if (!need_resched()) {
-		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
+		if (cpu_has(__this_cpu_ptr(&cpu_info), X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
 
 		__monitor((void *)&current_thread_info()->flags, 0, 0);
@@ -460,7 +460,7 @@ static void mwait_idle(void)
 {
 	if (!need_resched()) {
 		trace_power_start(POWER_CSTATE, 1, smp_processor_id());
-		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
+		if (cpu_has(__this_cpu_ptr(&cpu_info), X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
 
 		__monitor((void *)&current_thread_info()->flags, 0, 0);

commit 9c0729dc8062bed96189bd14ac6d4920f3958743
Author: Soeren Sandmann Pedersen <sandmann@redhat.com>
Date:   Fri Nov 5 05:59:39 2010 -0400

    x86: Eliminate bp argument from the stack tracing routines
    
    The various stack tracing routines take a 'bp' argument in which the
    caller is supposed to provide the base pointer to use, or 0 if doesn't
    have one. Since bp is garbage whenever CONFIG_FRAME_POINTER is not
    defined, this means all callers in principle should either always pass
    0, or be conditional on CONFIG_FRAME_POINTER.
    
    However, there are only really three use cases for stack tracing:
    
    (a) Trace the current task, including IRQ stack if any
    (b) Trace the current task, but skip IRQ stack
    (c) Trace some other task
    
    In all cases, if CONFIG_FRAME_POINTER is not defined, bp should just
    be 0.  If it _is_ defined, then
    
    - in case (a) bp should be gotten directly from the CPU's register, so
      the caller should pass NULL for regs,
    
    - in case (b) the caller should should pass the IRQ registers to
      dump_trace(),
    
    - in case (c) bp should be gotten from the top of the task's stack, so
      the caller should pass NULL for regs.
    
    Hence, the bp argument is not necessary because the combination of
    task and regs is sufficient to determine an appropriate value for bp.
    
    This patch introduces a new inline function stack_frame(task, regs)
    that computes the desired bp. This function is then called from the
    two versions of dump_stack().
    
    Signed-off-by: Soren Sandmann <ssp@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arjan van de Ven <arjan@infradead.org>,
    Cc: Frederic Weisbecker <fweisbec@gmail.com>,
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>,
    LKML-Reference: <m3oc9rop28.fsf@dhcp-100-3-82.bos.redhat.com>>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 57d1868a86aa..96ed1aac543a 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -91,8 +91,7 @@ void exit_thread(void)
 void show_regs(struct pt_regs *regs)
 {
 	show_registers(regs);
-	show_trace(NULL, regs, (unsigned long *)kernel_stack_pointer(regs),
-		   regs->bp);
+	show_trace(NULL, regs, (unsigned long *)kernel_stack_pointer(regs));
 }
 
 void show_regs_common(void)

commit d7627467b7a8dd6944885290a03a07ceb28c10eb
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 17 23:52:56 2010 +0100

    Make do_execve() take a const filename pointer
    
    Make do_execve() take a const filename pointer so that kernel_execve() compiles
    correctly on ARM:
    
    arch/arm/kernel/sys_arm.c:88: warning: passing argument 1 of 'do_execve' discards qualifiers from pointer target type
    
    This also requires the argv and envp arguments to be consted twice, once for
    the pointer array and once for the strings the array points to.  This is
    because do_execve() passes a pointer to the filename (now const) to
    copy_strings_kernel().  A simpler alternative would be to cast the filename
    pointer in do_execve() when it's passed to copy_strings_kernel().
    
    do_execve() may not change any of the strings it is passed as part of the argv
    or envp lists as they are some of them in .rodata, so marking these strings as
    const should be fine.
    
    Further kernel_execve() and sys_execve() need to be changed to match.
    
    This has been test built on x86_64, frv, arm and mips.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Ralf Baechle <ralf@linux-mips.org>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 64ecaf0af9af..57d1868a86aa 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -301,8 +301,9 @@ EXPORT_SYMBOL(kernel_thread);
 /*
  * sys_execve() executes a new program.
  */
-long sys_execve(const char __user *name, char __user * __user *argv,
-		char __user * __user *envp, struct pt_regs *regs)
+long sys_execve(const char __user *name,
+		const char __user *const __user *argv,
+		const char __user *const __user *envp, struct pt_regs *regs)
 {
 	long error;
 	char *filename;

commit c7887325230aec47d47a32562a6e26014a0fafca
Author: David Howells <dhowells@redhat.com>
Date:   Wed Aug 11 11:26:22 2010 +0100

    Mark arguments to certain syscalls as being const
    
    Mark arguments to certain system calls as being const where they should be but
    aren't.  The list includes:
    
     (*) The filename arguments of various stat syscalls, execve(), various utimes
         syscalls and some mount syscalls.
    
     (*) The filename arguments of some syscall helpers relating to the above.
    
     (*) The buffer argument of various write syscalls.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index d401f1d2d06e..64ecaf0af9af 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -301,7 +301,7 @@ EXPORT_SYMBOL(kernel_thread);
 /*
  * sys_execve() executes a new program.
  */
-long sys_execve(char __user *name, char __user * __user *argv,
+long sys_execve(const char __user *name, char __user * __user *argv,
 		char __user * __user *envp, struct pt_regs *regs)
 {
 	long error;

commit 0f477dd0851bdcee82923da66a7fc4a44cb1bc3d
Merge: c4efd6b569b2 e8c534ec068a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 6 10:02:36 2010 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Fix keeping track of AMD C1E
      x86, cpu: Package Level Thermal Control, Power Limit Notification definitions
      x86, cpu: Export AMD errata definitions
      x86, cpu: Use AMD errata checking framework for erratum 383
      x86, cpu: Clean up AMD erratum 400 workaround
      x86, cpu: AMD errata checking framework
      x86, cpu: Split addon_cpuid_features.c
      x86, cpu: Clean up formatting in cpufeature.h, remove override
      x86, cpu: Enumerate xsaveopt
      x86, cpu: Add xsaveopt cpufeature
      x86, cpu: Make init_scattered_cpuid_features() consider cpuid subleaves
      x86, cpu: Support the features flags in new CPUID leaf 7
      x86, cpu: Add CPU flags for F16C and RDRND
      x86: Look for IA32_ENERGY_PERF_BIAS support
      x86, AMD: Extend support to future families
      x86, cacheinfo: Carve out L3 cache slot accessors
      x86, xsave: Cleanup return codes in check_for_xstate()

commit 8d91530c5fd7f0b1e8c4ddfea2905e55a178569b
Merge: c145307a110c 9d1f44ee206a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 4 11:13:36 2010 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/davej/cpufreq
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/davej/cpufreq:
      [CPUFREQ] Remove pointless printk from p4-clockmod.
      [CPUFREQ] Fix section mismatch for powernow_cpu_init in powernow-k7.c
      [CPUFREQ] Fix section mismatch for longhaul_cpu_init.
      [CPUFREQ] Fix section mismatch for longrun_cpu_init.
      [CPUFREQ] powernow-k8: Fix misleading variable naming
      [CPUFREQ] Convert pci_table entries to PCI_VDEVICE (if PCI_ANY_ID is used)
      [CPUFREQ] arch/x86/kernel/cpu/cpufreq: use for_each_pci_dev()
      [CPUFREQ] fix brace coding style issue.
      [CPUFREQ] x86 cpufreq: Make trace_power_frequency cpufreq driver independent
      [CPUFREQ] acpi-cpufreq: Fix CPU_ANY CPUFREQ_{PRE,POST}CHANGE notification
      [CPUFREQ] ondemand: don't synchronize sample rate unless multiple cpus present
      [CPUFREQ] unexport (un)lock_policy_rwsem* functions
      [CPUFREQ] ondemand: Refactor frequency increase code
      [CPUFREQ] powernow-k8: On load failure, remind the user to enable support in BIOS setup
      [CPUFREQ] powernow-k8: Limit Pstate transition latency check
      [CPUFREQ] Fix PCC driver error path
      [CPUFREQ] fix double freeing in error path of pcc-cpufreq
      [CPUFREQ] pcc driver should check for pcch method before calling _OSC
      [CPUFREQ] fix memory leak in cpufreq_add_dev
      [CPUFREQ] revert "[CPUFREQ] remove rwsem lock from CPUFREQ_GOV_STOP call (second call site)"
    
    Manually fix up non-data merge conflict introduced by new calling
    conventions for trace_power_start() in commit 6f4f2723d085 ("x86
    cpufreq: Make trace_power_frequency cpufreq driver independent"), which
    didn't update the intel_idle native hardware cpuidle driver.

commit 6f4f2723d08534fd4e407e1ef8500b0f4d12c30c
Author: Thomas Renninger <trenn@suse.de>
Date:   Tue Apr 20 13:17:36 2010 +0200

    [CPUFREQ] x86 cpufreq: Make trace_power_frequency cpufreq driver independent
    
    and fix the broken case if a core's frequency depends on others.
    
    trace_power_frequency was only implemented in a rather ungeneric way
    in acpi-cpufreq driver's target() function only.
    -> Move the call to trace_power_frequency to
       cpufreq.c:cpufreq_notify_transition() where CPUFREQ_POSTCHANGE
       notifier is triggered.
       This will support power frequency tracing by all cpufreq drivers
    
    trace_power_frequency did not trace frequency changes correctly when
    the userspace governor was used or when CPU cores' frequency depend
    on each other.
    -> Moving this into the CPUFREQ_POSTCHANGE notifier and pass the cpu
       which gets switched automatically fixes this.
    
    Robert Schoene provided some important fixes on top of my initial
    quick shot version which are integrated in this patch:
    - Forgot some changes in power_end trace (TP_printk/variable names)
    - Variable dummy in power_end must now be cpu_id
    - Use static 64 bit variable instead of unsigned int for cpu_id
    
    Signed-off-by: Thomas Renninger <trenn@suse.de>
    CC: davej@redhat.com
    CC: arjan@infradead.org
    CC: linux-kernel@vger.kernel.org
    CC: robert.schoene@tu-dresden.de
    Tested-by: robert.schoene@tu-dresden.de
    Signed-off-by: Dave Jones <davej@redhat.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e7e35219b32f..787572d43d9c 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -371,7 +371,7 @@ static inline int hlt_use_halt(void)
 void default_idle(void)
 {
 	if (hlt_use_halt()) {
-		trace_power_start(POWER_CSTATE, 1);
+		trace_power_start(POWER_CSTATE, 1, smp_processor_id());
 		current_thread_info()->status &= ~TS_POLLING;
 		/*
 		 * TS_POLLING-cleared state must be visible before we
@@ -441,7 +441,7 @@ EXPORT_SYMBOL_GPL(cpu_idle_wait);
  */
 void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 {
-	trace_power_start(POWER_CSTATE, (ax>>4)+1);
+	trace_power_start(POWER_CSTATE, (ax>>4)+1, smp_processor_id());
 	if (!need_resched()) {
 		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
@@ -457,7 +457,7 @@ void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 static void mwait_idle(void)
 {
 	if (!need_resched()) {
-		trace_power_start(POWER_CSTATE, 1);
+		trace_power_start(POWER_CSTATE, 1, smp_processor_id());
 		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
 
@@ -478,7 +478,7 @@ static void mwait_idle(void)
  */
 static void poll_idle(void)
 {
-	trace_power_start(POWER_CSTATE, 0);
+	trace_power_start(POWER_CSTATE, 0, smp_processor_id());
 	local_irq_enable();
 	while (!need_resched())
 		cpu_relax();

commit e8c534ec068af1a0845aceda373a9bfd2de62030
Author: Michal Schmidt <mschmidt@redhat.com>
Date:   Tue Jul 27 18:53:35 2010 +0200

    x86: Fix keeping track of AMD C1E
    
    Accomodate the original C1E-aware idle routine to the different times
    during boot when the BIOS enables C1E. While at it, remove the synthetic
    CPUID flag in favor of a single global setting which denotes C1E status
    on the system.
    
    [ hpa: changed c1e_enabled to be a bool; clarified cpu bit 3:21 comment ]
    
    Signed-off-by: Michal Schmidt <mschmidt@redhat.com>
    LKML-Reference: <20100727165335.GA11630@aftab>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 553b02f13094..b944f89c4e6e 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -525,8 +525,10 @@ static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
 	return (edx & MWAIT_EDX_C1);
 }
 
+bool c1e_detected;
+EXPORT_SYMBOL(c1e_detected);
+
 static cpumask_var_t c1e_mask;
-static int c1e_detected;
 
 void c1e_remove_cpu(int cpu)
 {
@@ -548,12 +550,12 @@ static void c1e_idle(void)
 		u32 lo, hi;
 
 		rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
+
 		if (lo & K8_INTP_C1E_ACTIVE_MASK) {
-			c1e_detected = 1;
+			c1e_detected = true;
 			if (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))
 				mark_tsc_unstable("TSC halt in AMD C1E");
 			printk(KERN_INFO "System has AMD C1E enabled\n");
-			set_cpu_cap(&boot_cpu_data, X86_FEATURE_AMDC1E);
 		}
 	}
 

commit 5ee481da7b62a992b91f958bf26aaaa92354c170
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Mon May 17 17:22:23 2010 +0800

    x86: Export FPU API for KVM use
    
    Also add some constants.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e7e35219b32f..ebcfcceccc72 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -28,6 +28,7 @@ unsigned long idle_nomwait;
 EXPORT_SYMBOL(idle_nomwait);
 
 struct kmem_cache *task_xstate_cachep;
+EXPORT_SYMBOL_GPL(task_xstate_cachep);
 
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {

commit 9d8888c2a214aece2494a49e699a097c2ba9498b
Author: Hans Rosenfeld <hans.rosenfeld@amd.com>
Date:   Wed Jul 28 19:09:31 2010 +0200

    x86, cpu: Clean up AMD erratum 400 workaround
    
    Remove check_c1e_idle() and use the new AMD errata checking framework
    instead.
    
    Signed-off-by: Hans Rosenfeld <hans.rosenfeld@amd.com>
    LKML-Reference: <1280336972-865982-2-git-send-email-hans.rosenfeld@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e7e35219b32f..553b02f13094 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -525,42 +525,6 @@ static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
 	return (edx & MWAIT_EDX_C1);
 }
 
-/*
- * Check for AMD CPUs, where APIC timer interrupt does not wake up CPU from C1e.
- * For more information see
- * - Erratum #400 for NPT family 0xf and family 0x10 CPUs
- * - Erratum #365 for family 0x11 (not affected because C1e not in use)
- */
-static int __cpuinit check_c1e_idle(const struct cpuinfo_x86 *c)
-{
-	u64 val;
-	if (c->x86_vendor != X86_VENDOR_AMD)
-		goto no_c1e_idle;
-
-	/* Family 0x0f models < rev F do not have C1E */
-	if (c->x86 == 0x0F && c->x86_model >= 0x40)
-		return 1;
-
-	if (c->x86 == 0x10) {
-		/*
-		 * check OSVW bit for CPUs that are not affected
-		 * by erratum #400
-		 */
-		if (cpu_has(c, X86_FEATURE_OSVW)) {
-			rdmsrl(MSR_AMD64_OSVW_ID_LENGTH, val);
-			if (val >= 2) {
-				rdmsrl(MSR_AMD64_OSVW_STATUS, val);
-				if (!(val & BIT(1)))
-					goto no_c1e_idle;
-			}
-		}
-		return 1;
-	}
-
-no_c1e_idle:
-	return 0;
-}
-
 static cpumask_var_t c1e_mask;
 static int c1e_detected;
 
@@ -638,7 +602,8 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 		 */
 		printk(KERN_INFO "using mwait in idle threads.\n");
 		pm_idle = mwait_idle;
-	} else if (check_c1e_idle(c)) {
+	} else if (cpu_has_amd_erratum(amd_erratum_400)) {
+		/* E400: APIC timer interrupt does not wake up CPU from C1e */
 		printk(KERN_INFO "using C1E aware idle routine\n");
 		pm_idle = c1e_idle;
 	} else

commit 41d59102e146a4423a490b8eca68a5860af4fe1c
Merge: 3e1dd193edef c9775b4cc522
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:58:16 2010 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, fpu: Use static_cpu_has() to implement use_xsave()
      x86: Add new static_cpu_has() function using alternatives
      x86, fpu: Use the proper asm constraint in use_xsave()
      x86, fpu: Unbreak FPU emulation
      x86: Introduce 'struct fpu' and related API
      x86: Eliminate TS_XSAVE
      x86-32: Don't set ignore_fpu_irq in simd exception
      x86: Merge kernel_math_error() into math_error()
      x86: Merge simd_math_error() into math_error()
      x86-32: Rework cache flush denied handler
    
    Fix trivial conflict in arch/x86/kernel/process.c

commit 4d7b4ac22fbec1a03206c6cde353f2fd6942f828
Merge: 3aaf51ace597 94f3ca95787a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:19:03 2010 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (311 commits)
      perf tools: Add mode to build without newt support
      perf symbols: symbol inconsistency message should be done only at verbose=1
      perf tui: Add explicit -lslang option
      perf options: Type check all the remaining OPT_ variants
      perf options: Type check OPT_BOOLEAN and fix the offenders
      perf options: Check v type in OPT_U?INTEGER
      perf options: Introduce OPT_UINTEGER
      perf tui: Add workaround for slang < 2.1.4
      perf record: Fix bug mismatch with -c option definition
      perf options: Introduce OPT_U64
      perf tui: Add help window to show key associations
      perf tui: Make <- exit menus too
      perf newt: Add single key shortcuts for zoom into DSO and threads
      perf newt: Exit browser unconditionally when CTRL+C, q or Q is pressed
      perf newt: Fix the 'A'/'a' shortcut for annotate
      perf newt: Make <- exit the ui_browser
      x86, perf: P4 PMU - fix counters management logic
      perf newt: Make <- zoom out filters
      perf report: Report number of events, not samples
      perf hist: Clarify events_stats fields usage
      ...
    
    Fix up trivial conflicts in kernel/fork.c and tools/perf/builtin-record.c

commit f01487119dda3d9f58c9729c7361ecc50a61c188
Author: Andreas Herrmann <herrmann.der.user@googlemail.com>
Date:   Tue Apr 27 12:13:48 2010 +0200

    x86, amd: Check X86_FEATURE_OSVW bit before accessing OSVW MSRs
    
    If host CPU is exposed to a guest the OSVW MSRs are not guaranteed
    to be present and a GP fault occurs. Thus checking the feature flag is
    essential.
    
    Cc: <stable@kernel.org> # .32.x .33.x
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    LKML-Reference: <20100427101348.GC4489@alberich.amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 28ad9f4d8b94..0415c3ef91b5 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -546,11 +546,13 @@ static int __cpuinit check_c1e_idle(const struct cpuinfo_x86 *c)
 		 * check OSVW bit for CPUs that are not affected
 		 * by erratum #400
 		 */
-		rdmsrl(MSR_AMD64_OSVW_ID_LENGTH, val);
-		if (val >= 2) {
-			rdmsrl(MSR_AMD64_OSVW_STATUS, val);
-			if (!(val & BIT(1)))
-				goto no_c1e_idle;
+		if (cpu_has(c, X86_FEATURE_OSVW)) {
+			rdmsrl(MSR_AMD64_OSVW_ID_LENGTH, val);
+			if (val >= 2) {
+				rdmsrl(MSR_AMD64_OSVW_STATUS, val);
+				if (!(val & BIT(1)))
+					goto no_c1e_idle;
+			}
 		}
 		return 1;
 	}

commit 86603283326c9e95e5ad4e9fdddeec93cac5d9ad
Author: Avi Kivity <avi@redhat.com>
Date:   Thu May 6 11:45:46 2010 +0300

    x86: Introduce 'struct fpu' and related API
    
    Currently all fpu state access is through tsk->thread.xstate.  Since we wish
    to generalize fpu access to non-task contexts, wrap the state in a new
    'struct fpu' and convert existing access to use an fpu API.
    
    Signal frame handlers are not converted to the API since they will remain
    task context only things.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1273135546-29690-3-git-send-email-avi@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 28ad9f4d8b94..f18fd9c15247 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -32,25 +32,22 @@ struct kmem_cache *task_xstate_cachep;
 
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
+	int ret;
+
 	*dst = *src;
-	if (src->thread.xstate) {
-		dst->thread.xstate = kmem_cache_alloc(task_xstate_cachep,
-						      GFP_KERNEL);
-		if (!dst->thread.xstate)
-			return -ENOMEM;
-		WARN_ON((unsigned long)dst->thread.xstate & 15);
-		memcpy(dst->thread.xstate, src->thread.xstate, xstate_size);
+	if (fpu_allocated(&src->thread.fpu)) {
+		memset(&dst->thread.fpu, 0, sizeof(dst->thread.fpu));
+		ret = fpu_alloc(&dst->thread.fpu);
+		if (ret)
+			return ret;
+		fpu_copy(&dst->thread.fpu, &src->thread.fpu);
 	}
 	return 0;
 }
 
 void free_thread_xstate(struct task_struct *tsk)
 {
-	if (tsk->thread.xstate) {
-		kmem_cache_free(task_xstate_cachep, tsk->thread.xstate);
-		tsk->thread.xstate = NULL;
-	}
-
+	fpu_free(&tsk->thread.fpu);
 	WARN(tsk->thread.ds_ctx, "leaking DS context\n");
 }
 

commit ec5e61aabeac58670691bd0613388d16697d0d81
Merge: 75ec5a245c77 8bb39f9aa068
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 2 19:37:50 2010 +0200

    Merge branch 'perf/urgent' into perf/core
    
    Conflicts:
            arch/x86/kernel/cpu/perf_event.c
    
    Merge reason: Resolve the conflict, pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ea8e61b7bbc4a2faef77db34eb2db2a2c2372ff6
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:51 2010 +0100

    x86, ptrace: Fix block-step
    
    Implement ptrace-block-step using TIF_BLOCKSTEP which will set
    DEBUGCTLMSR_BTF when set for a task while preserving any other
    DEBUGCTLMSR bits.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100325135414.017536066@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 1a60beb32ede..8328009416d7 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -195,6 +195,17 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 	prev = &prev_p->thread;
 	next = &next_p->thread;
 
+	if (test_tsk_thread_flag(prev_p, TIF_BLOCKSTEP) ^
+	    test_tsk_thread_flag(next_p, TIF_BLOCKSTEP)) {
+		unsigned long debugctl = get_debugctlmsr();
+
+		debugctl &= ~DEBUGCTLMSR_BTF;
+		if (test_tsk_thread_flag(next_p, TIF_BLOCKSTEP))
+			debugctl |= DEBUGCTLMSR_BTF;
+
+		update_debugctlmsr(debugctl);
+	}
+
 	if (test_tsk_thread_flag(prev_p, TIF_NOTSC) ^
 	    test_tsk_thread_flag(next_p, TIF_NOTSC)) {
 		/* prev and next are different */

commit faa4602e47690fb11221e00f9b9697c8dc0d4b19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:50 2010 +0100

    x86, perf, bts, mm: Delete the never used BTS-ptrace code
    
    Support for the PMU's BTS features has been upstreamed in
    v2.6.32, but we still have the old and disabled ptrace-BTS,
    as Linus noticed it not so long ago.
    
    It's buggy: TIF_DEBUGCTLMSR is trampling all over that MSR without
    regard for other uses (perf) and doesn't provide the flexibility
    needed for perf either.
    
    Its users are ptrace-block-step and ptrace-bts, since ptrace-bts
    was never used and ptrace-block-step can be implemented using a
    much simpler approach.
    
    So axe all 3000 lines of it. That includes the *locked_memory*()
    APIs in mm/mlock.c as well.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20100325135413.938004390@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ad9540676fcc..1a60beb32ede 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -20,7 +20,6 @@
 #include <asm/idle.h>
 #include <asm/uaccess.h>
 #include <asm/i387.h>
-#include <asm/ds.h>
 #include <asm/debugreg.h>
 
 unsigned long idle_halt;
@@ -50,8 +49,6 @@ void free_thread_xstate(struct task_struct *tsk)
 		kmem_cache_free(task_xstate_cachep, tsk->thread.xstate);
 		tsk->thread.xstate = NULL;
 	}
-
-	WARN(tsk->thread.ds_ctx, "leaking DS context\n");
 }
 
 void free_thread_info(struct thread_info *ti)
@@ -198,12 +195,6 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 	prev = &prev_p->thread;
 	next = &next_p->thread;
 
-	if (test_tsk_thread_flag(next_p, TIF_DS_AREA_MSR) ||
-	    test_tsk_thread_flag(prev_p, TIF_DS_AREA_MSR))
-		ds_switch_to(prev_p, next_p);
-	else if (next->debugctlmsr != prev->debugctlmsr)
-		update_debugctlmsr(next->debugctlmsr);
-
 	if (test_tsk_thread_flag(prev_p, TIF_NOTSC) ^
 	    test_tsk_thread_flag(next_p, TIF_NOTSC)) {
 		/* prev and next are different */

commit 035a02c1e1de31888e8b6adac0ff667971ac04db
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Fri Mar 19 12:09:22 2010 +0100

    x86, amd: Restrict usage of c1e_idle()
    
    Currently c1e_idle returns true for all CPUs greater than or equal to
    family 0xf model 0x40. This covers too many CPUs.
    
    Meanwhile a respective erratum for the underlying problem was filed
    (#400). This patch adds the logic to check whether erratum #400
    applies to a given CPU.
    Especially for CPUs where SMI/HW triggered C1e is not supported,
    c1e_idle() doesn't need to be used. We can check this by looking at
    the respective OSVW bit for erratum #400.
    
    Cc: <stable@kernel.org> # .32.x .33.x
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    LKML-Reference: <20100319110922.GA19614@alberich.amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ad9540676fcc..28ad9f4d8b94 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -526,21 +526,37 @@ static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
 }
 
 /*
- * Check for AMD CPUs, which have potentially C1E support
+ * Check for AMD CPUs, where APIC timer interrupt does not wake up CPU from C1e.
+ * For more information see
+ * - Erratum #400 for NPT family 0xf and family 0x10 CPUs
+ * - Erratum #365 for family 0x11 (not affected because C1e not in use)
  */
 static int __cpuinit check_c1e_idle(const struct cpuinfo_x86 *c)
 {
+	u64 val;
 	if (c->x86_vendor != X86_VENDOR_AMD)
-		return 0;
-
-	if (c->x86 < 0x0F)
-		return 0;
+		goto no_c1e_idle;
 
 	/* Family 0x0f models < rev F do not have C1E */
-	if (c->x86 == 0x0f && c->x86_model < 0x40)
-		return 0;
+	if (c->x86 == 0x0F && c->x86_model >= 0x40)
+		return 1;
 
-	return 1;
+	if (c->x86 == 0x10) {
+		/*
+		 * check OSVW bit for CPUs that are not affected
+		 * by erratum #400
+		 */
+		rdmsrl(MSR_AMD64_OSVW_ID_LENGTH, val);
+		if (val >= 2) {
+			rdmsrl(MSR_AMD64_OSVW_STATUS, val);
+			if (!(val & BIT(1)))
+				goto no_c1e_idle;
+		}
+		return 1;
+	}
+
+no_c1e_idle:
+	return 0;
 }
 
 static cpumask_var_t c1e_mask;

commit 15c989d4d14b82f43cae7efc6a7794cdd556f274
Merge: 4e3eaddd142e 0e152cd7c168
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 13 14:45:49 2010 -0800

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, k8 nb: Fix boot crash: enable k8_northbridges unconditionally on AMD systems
      x86, UV: Fix target_cpus() in x2apic_uv_x.c
      x86: Reduce per cpu warning boot up messages
      x86: Reduce per cpu MCA boot up messages
      x86_64, cpa: Don't work hard in preserving kernel 2M mappings when using 4K already

commit d6dd692168c049196f54edc2e8227c60702bb1d2
Author: Mike Travis <travis@sgi.com>
Date:   Fri Mar 5 13:10:38 2010 -0600

    x86: Reduce per cpu warning boot up messages
    
    Reduce warning message output to one line only instead of per
    cpu.
    
    Signed-of-by: Mike Travis <travis@sgi.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: x86@kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c9b3522b6b46..4e8cb4ee9fcb 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -600,7 +600,7 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
 	if (pm_idle == poll_idle && smp_num_siblings > 1) {
-		printk(KERN_WARNING "WARNING: polling idle and HT enabled,"
+		printk_once(KERN_WARNING "WARNING: polling idle and HT enabled,"
 			" performance may degrade.\n");
 	}
 #endif

commit a7f16d10b510f9ee3500af7831f2e3094fab3dca
Merge: f66ffdedbf0f 17c0e7107bed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 28 10:35:09 2010 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Mark atomic irq ops raw for 32bit legacy
      x86: Merge show_regs()
      x86: Macroise x86 cache descriptors
      x86-32: clean up rwsem inline asm statements
      x86: Merge asm/atomic_{32,64}.h
      x86: Sync asm/atomic_32.h and asm/atomic_64.h
      x86: Split atomic64_t functions into seperate headers
      x86-64: Modify memcpy()/memset() alternatives mechanism
      x86-64: Modify copy_user_generic() alternatives mechanism
      x86: Lift restriction on the location of FIX_BTMAP_*
      x86, core: Optimize hweight32()

commit 05d43ed8a89c159ff641d472f970e3f1baa66318
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Jan 28 22:14:43 2010 -0800

    x86: get rid of the insane TIF_ABI_PENDING bit
    
    Now that the previous commit made it possible to do the personality
    setting at the point of no return, we do just that for ELF binaries.
    And suddenly all the reasons for that insane TIF_ABI_PENDING bit go
    away, and we can just make SET_PERSONALITY() just do the obvious thing
    for a 32-bit compat process.
    
    Everything becomes much more straightforward this way.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 02c3ee013ccd..c9b3522b6b46 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -115,18 +115,6 @@ void flush_thread(void)
 {
 	struct task_struct *tsk = current;
 
-#ifdef CONFIG_X86_64
-	if (test_tsk_thread_flag(tsk, TIF_ABI_PENDING)) {
-		clear_tsk_thread_flag(tsk, TIF_ABI_PENDING);
-		if (test_tsk_thread_flag(tsk, TIF_IA32)) {
-			clear_tsk_thread_flag(tsk, TIF_IA32);
-		} else {
-			set_tsk_thread_flag(tsk, TIF_IA32);
-			current_thread_info()->status |= TS_COMPAT;
-		}
-	}
-#endif
-
 	flush_ptrace_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
 	/*

commit 3bef444797f7624f8fbd27f4e0334ce96a108725
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Jan 13 10:45:55 2010 -0500

    x86: Merge show_regs()
    
    Using kernel_stack_pointer() allows 32-bit and 64-bit versions to
    be merged.  This is more correct for 64-bit, since the old %rsp is
    always saved on the stack.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1263397555-27695-1-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 98c2cdeb599e..cf1e04b2ad65 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -92,6 +92,13 @@ void exit_thread(void)
 	}
 }
 
+void show_regs(struct pt_regs *regs)
+{
+	show_registers(regs);
+	show_trace(NULL, regs, (unsigned long *)kernel_stack_pointer(regs),
+		   regs->bp);
+}
+
 void show_regs_common(void)
 {
 	const char *board, *product;

commit 864a0922dd128392467611d9857e5138c6a91999
Author: Cyrill Gorcunov <gorcunov@openvz.org>
Date:   Wed Jan 13 10:16:07 2010 +0000

    x86: kernel_thread() -- initialize SS to a known state
    
    Before the kernel_thread was converted into "C" we had
    pt_regs::ss set to __KERNEL_DS (by SAVE_ALL asm macro).
    
    Though I must admit I didn't find any *explicit* load of
    %ss from this structure the better to be on a safe side
    and set it to a known value.
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Cc: Christian Kujau <lists@nerdbynature.de>
    Cc: Jeremy Fitzhardinge <Jeremy.Fitzhardinge@citrix.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1263377768-19600-1-git-send-email-ian.campbell@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c6ee241c8a98..02c3ee013ccd 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -288,6 +288,8 @@ int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 	regs.es = __USER_DS;
 	regs.fs = __KERNEL_PERCPU;
 	regs.gs = __KERNEL_STACK_CANARY;
+#else
+	regs.ss = __KERNEL_DS;
 #endif
 
 	regs.orig_ax = -1;

commit d015a092989d673df44a5ad6866dc5d5006b7a2a
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Mon Dec 28 10:26:59 2009 +0200

    x86: Use KERN_DEFAULT log-level in __show_regs()
    
    Andrew Morton reported a strange looking kmemcheck warning:
    
      WARNING: kmemcheck: Caught 32-bit read from uninitialized memory (ffff88004fba6c20)
      0000000000000000310000000000000000000000000000002413000000c9ffff
       u u u u u u u u u u u u u u u u i i i i i i i i u u u u u u u u
    
       [<ffffffff810af3aa>] kmemleak_scan+0x25a/0x540
       [<ffffffff810afbcb>] kmemleak_scan_thread+0x5b/0xe0
       [<ffffffff8104d0fe>] kthread+0x9e/0xb0
       [<ffffffff81003074>] kernel_thread_helper+0x4/0x10
       [<ffffffffffffffff>] 0xffffffffffffffff
    
    The above printout is missing register dump completely. The
    problem here is that the output comes from syslog which doesn't
    show KERN_INFO log-level messages. We didn't see this before
    because both of us were testing on 32-bit kernels which use the
    _default_ log-level.
    
    Fix that up by explicitly using KERN_DEFAULT log-level for
    __show_regs() printks.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <1261988819.4641.2.camel@penberg-laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 98c2cdeb599e..c6ee241c8a98 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -103,8 +103,8 @@ void show_regs_common(void)
 	if (!product)
 		product = "";
 
-	printk("\n");
-	printk(KERN_INFO "Pid: %d, comm: %.20s %s %s %.*s %s/%s\n",
+	printk(KERN_CONT "\n");
+	printk(KERN_DEFAULT "Pid: %d, comm: %.20s %s %s %.*s %s/%s\n",
 		current->pid, current->comm, print_tainted(),
 		init_utsname()->release,
 		(int)strcspn(init_utsname()->version, " "),

commit ab1eebe77dd08b26585860d534e07810d1cd274d
Merge: 186a25026c44 df59e7bf4399
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 15 20:33:22 2009 +0100

    Merge branch 'x86/asm' into x86/urgent
    
    Merge reason: it's stable so lets push it upstream.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit df59e7bf439918f523ac29e996ec1eebbed60440
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Dec 9 12:34:44 2009 -0500

    x86: Merge kernel_thread()
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1260380084-3707-6-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index f3c1a6b3a65e..8705ccedd447 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -243,6 +243,41 @@ sys_clone(unsigned long clone_flags, unsigned long newsp,
 	return do_fork(clone_flags, newsp, regs, 0, parent_tid, child_tid);
 }
 
+/*
+ * This gets run with %si containing the
+ * function to call, and %di containing
+ * the "args".
+ */
+extern void kernel_thread_helper(void);
+
+/*
+ * Create a kernel thread
+ */
+int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
+{
+	struct pt_regs regs;
+
+	memset(&regs, 0, sizeof(regs));
+
+	regs.si = (unsigned long) fn;
+	regs.di = (unsigned long) arg;
+
+#ifdef CONFIG_X86_32
+	regs.ds = __USER_DS;
+	regs.es = __USER_DS;
+	regs.fs = __KERNEL_PERCPU;
+	regs.gs = __KERNEL_STACK_CANARY;
+#endif
+
+	regs.orig_ax = -1;
+	regs.ip = (unsigned long) kernel_thread_helper;
+	regs.cs = __KERNEL_CS | get_kernel_rpl();
+	regs.flags = X86_EFLAGS_IF | 0x2;
+
+	/* Ok, create the new process.. */
+	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+}
+EXPORT_SYMBOL(kernel_thread);
 
 /*
  * sys_execve() executes a new program.

commit f839bbc5c81b1c92ff8e81c360e9564f7b961b2e
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Dec 9 19:01:56 2009 -0500

    x86: Merge sys_clone
    
    Change 32-bit sys_clone to new PTREGSCALL stub, and merge with 64-bit.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1260403316-5679-7-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index bb17bd9334fb..f3c1a6b3a65e 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -234,6 +234,15 @@ int sys_vfork(struct pt_regs *regs)
 		       NULL, NULL);
 }
 
+long
+sys_clone(unsigned long clone_flags, unsigned long newsp,
+	  void __user *parent_tid, void __user *child_tid, struct pt_regs *regs)
+{
+	if (!newsp)
+		newsp = regs->sp;
+	return do_fork(clone_flags, newsp, regs, 0, parent_tid, child_tid);
+}
+
 
 /*
  * sys_execve() executes a new program.

commit 11cf88bd0b8165b65aaabaee0977e9a3ad474ab7
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Dec 9 19:01:53 2009 -0500

    x86: Merge sys_execve
    
    Change 32-bit sys_execve to PTREGSCALL3, and merge with 64-bit.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1260403316-5679-4-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 5e2ba634ea15..bb17bd9334fb 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -235,6 +235,32 @@ int sys_vfork(struct pt_regs *regs)
 }
 
 
+/*
+ * sys_execve() executes a new program.
+ */
+long sys_execve(char __user *name, char __user * __user *argv,
+		char __user * __user *envp, struct pt_regs *regs)
+{
+	long error;
+	char *filename;
+
+	filename = getname(name);
+	error = PTR_ERR(filename);
+	if (IS_ERR(filename))
+		return error;
+	error = do_execve(filename, argv, envp, regs);
+
+#ifdef CONFIG_X86_32
+	if (error == 0) {
+		/* Make sure we don't return using sysenter.. */
+                set_thread_flag(TIF_IRET);
+        }
+#endif
+
+	putname(filename);
+	return error;
+}
+
 /*
  * Idle related variables and functions
  */

commit a1884b8e558ef6395f6033f9e1b69b332dd040e0
Author: Andy Isaacson <adi@hexapodia.org>
Date:   Tue Dec 8 00:30:21 2009 -0800

    x86: Print DMI_BOARD_NAME as well as DMI_PRODUCT_NAME from __show_regs()
    
    Robert Hancock observes that DMI_BOARD_NAME is often more useful
    than DMI_PRODUCT_NAME, especially on standalone motherboards.
    So, print both.
    
    Signed-off-by: Andy Isaacson <adi@hexapodia.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Robert Hancock <hancockrwd@gmail.com>
    Cc: Richard Zidlicky <rz@linux-m68k.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20091208083021.GB27174@hexapodia.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 90cf1250a005..7a7bd4e3ec49 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -94,18 +94,21 @@ void exit_thread(void)
 
 void show_regs_common(void)
 {
-	const char *board;
+	const char *board, *product;
 
-	board = dmi_get_system_info(DMI_PRODUCT_NAME);
+	board = dmi_get_system_info(DMI_BOARD_NAME);
 	if (!board)
 		board = "";
+	product = dmi_get_system_info(DMI_PRODUCT_NAME);
+	if (!product)
+		product = "";
 
 	printk("\n");
-	printk(KERN_INFO "Pid: %d, comm: %.20s %s %s %.*s %s\n",
+	printk(KERN_INFO "Pid: %d, comm: %.20s %s %s %.*s %s/%s\n",
 		current->pid, current->comm, print_tainted(),
 		init_utsname()->release,
 		(int)strcspn(init_utsname()->version, " "),
-		init_utsname()->version, board);
+		init_utsname()->version, board, product);
 }
 
 void flush_thread(void)

commit 814e2c84a722c45650a9b8f52285d7ba6874f63b
Author: Andy Isaacson <adi@hexapodia.org>
Date:   Tue Dec 8 00:29:42 2009 -0800

    x86: Factor duplicated code out of __show_regs() into show_regs_common()
    
    Unify x86_32 and x86_64 implementations of __show_regs() header,
    standardizing on the x86_64 format string in the process. Also,
    32-bit will now call print_modules.
    
    Signed-off-by: Andy Isaacson <adi@hexapodia.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Robert Hancock <hancockrwd@gmail.com>
    Cc: Richard Zidlicky <rz@linux-m68k.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20091208082942.GA27174@hexapodia.org>
    [ v2: resolved conflict ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 5e2ba634ea15..90cf1250a005 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -10,6 +10,8 @@
 #include <linux/clockchips.h>
 #include <linux/random.h>
 #include <linux/user-return-notifier.h>
+#include <linux/dmi.h>
+#include <linux/utsname.h>
 #include <trace/events/power.h>
 #include <linux/hw_breakpoint.h>
 #include <asm/system.h>
@@ -90,6 +92,22 @@ void exit_thread(void)
 	}
 }
 
+void show_regs_common(void)
+{
+	const char *board;
+
+	board = dmi_get_system_info(DMI_PRODUCT_NAME);
+	if (!board)
+		board = "";
+
+	printk("\n");
+	printk(KERN_INFO "Pid: %d, comm: %.20s %s %s %.*s %s\n",
+		current->pid, current->comm, print_tainted(),
+		init_utsname()->release,
+		(int)strcspn(init_utsname()->version, " "),
+		init_utsname()->version, board);
+}
+
 void flush_thread(void)
 {
 	struct task_struct *tsk = current;

commit ed9216c1717a3f3738a77908aff78995ea69e7ff
Merge: d7fc02c7bae7 d5696725b2a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 8 08:02:38 2009 -0800

    Merge branch 'kvm-updates/2.6.33' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    * 'kvm-updates/2.6.33' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (84 commits)
      KVM: VMX: Fix comparison of guest efer with stale host value
      KVM: s390: Fix prefix register checking in arch/s390/kvm/sigp.c
      KVM: Drop user return notifier when disabling virtualization on a cpu
      KVM: VMX: Disable unrestricted guest when EPT disabled
      KVM: x86 emulator: limit instructions to 15 bytes
      KVM: s390: Make psw available on all exits, not just a subset
      KVM: x86: Add KVM_GET/SET_VCPU_EVENTS
      KVM: VMX: Report unexpected simultaneous exceptions as internal errors
      KVM: Allow internal errors reported to userspace to carry extra data
      KVM: Reorder IOCTLs in main kvm.h
      KVM: x86: Polish exception injection via KVM_SET_GUEST_DEBUG
      KVM: only clear irq_source_id if irqchip is present
      KVM: x86: disallow KVM_{SET,GET}_LAPIC without allocated in-kernel lapic
      KVM: x86: disallow multiple KVM_CREATE_IRQCHIP
      KVM: VMX: Remove vmx->msr_offset_efer
      KVM: MMU: update invlpg handler comment
      KVM: VMX: move CR3/PDPTR update to vmx_set_cr3
      KVM: remove duplicated task_switch check
      KVM: powerpc: Fix BUILD_BUG_ON condition
      KVM: VMX: Use shared msr infrastructure
      ...
    
    Trivial conflicts due to new Kconfig options in arch/Kconfig and kernel/Makefile

commit 24f1e32c60c45c89a997c73395b69c8af6f0a84e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Sep 9 19:22:48 2009 +0200

    hw-breakpoints: Rewrite the hw-breakpoints layer on top of perf events
    
    This patch rebase the implementation of the breakpoints API on top of
    perf events instances.
    
    Each breakpoints are now perf events that handle the
    register scheduling, thread/cpu attachment, etc..
    
    The new layering is now made as follows:
    
           ptrace       kgdb      ftrace   perf syscall
              \          |          /         /
               \         |         /         /
                                            /
                Core breakpoint API        /
                                          /
                         |               /
                         |              /
    
                  Breakpoints perf events
    
                         |
                         |
    
                   Breakpoints PMU ---- Debug Register constraints handling
                                        (Part of core breakpoint API)
                         |
                         |
    
                 Hardware debug registers
    
    Reasons of this rewrite:
    
    - Use the centralized/optimized pmu registers scheduling,
      implying an easier arch integration
    - More powerful register handling: perf attributes (pinned/flexible
      events, exclusive/non-exclusive, tunable period, etc...)
    
    Impact:
    
    - New perf ABI: the hardware breakpoints counters
    - Ptrace breakpoints setting remains tricky and still needs some per
      thread breakpoints references.
    
    Todo (in the order):
    
    - Support breakpoints perf counter events for perf tools (ie: implement
      perf_bpcounter_event())
    - Support from perf tools
    
    Changes in v2:
    
    - Follow the perf "event " rename
    - The ptrace regression have been fixed (ptrace breakpoint perf events
      weren't released when a task ended)
    - Drop the struct hw_breakpoint and store generic fields in
      perf_event_attr.
    - Separate core and arch specific headers, drop
      asm-generic/hw_breakpoint.h and create linux/hw_breakpoint.h
    - Use new generic len/type for breakpoint
    - Handle off case: when breakpoints api is not supported by an arch
    
    Changes in v3:
    
    - Fix broken CONFIG_KVM, we need to propagate the breakpoint api
      changes to kvm when we exit the guest and restore the bp registers
      to the host.
    
    Changes in v4:
    
    - Drop the hw_breakpoint_restore() stub as it is only used by KVM
    - EXPORT_SYMBOL_GPL hw_breakpoint_restore() as KVM can be built as a
      module
    - Restore the breakpoints unconditionally on kvm guest exit:
      TIF_DEBUG_THREAD doesn't anymore cover every cases of running
      breakpoints and vcpu->arch.switch_db_regs might not always be
      set when the guest used debug registers.
      (Waiting for a reliable optimization)
    
    Changes in v5:
    
    - Split-up the asm-generic/hw-breakpoint.h moving to
      linux/hw_breakpoint.h into a separate patch
    - Optimize the breakpoints restoring while switching from kvm guest
      to host. We only want to restore the state if we have active
      breakpoints to the host, otherwise we don't care about messed-up
      address registers.
    - Add asm/hw_breakpoint.h to Kbuild
    - Fix bad breakpoint type in trace_selftest.c
    
    Changes in v6:
    
    - Fix wrong header inclusion in trace.h (triggered a build
      error with CONFIG_FTRACE_SELFTEST
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jan Kiszka <jan.kiszka@web.de>
    Cc: Jiri Slaby <jirislaby@gmail.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index cf8ee0016307..744508e7cfdd 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -10,6 +10,7 @@
 #include <linux/clockchips.h>
 #include <linux/random.h>
 #include <trace/events/power.h>
+#include <linux/hw_breakpoint.h>
 #include <asm/system.h>
 #include <asm/apic.h>
 #include <asm/syscalls.h>
@@ -18,7 +19,6 @@
 #include <asm/i387.h>
 #include <asm/ds.h>
 #include <asm/debugreg.h>
-#include <asm/hw_breakpoint.h>
 
 unsigned long idle_halt;
 EXPORT_SYMBOL(idle_halt);
@@ -47,8 +47,6 @@ void free_thread_xstate(struct task_struct *tsk)
 		kmem_cache_free(task_xstate_cachep, tsk->thread.xstate);
 		tsk->thread.xstate = NULL;
 	}
-	if (unlikely(test_tsk_thread_flag(tsk, TIF_DEBUG)))
-		flush_thread_hw_breakpoint(tsk);
 
 	WARN(tsk->thread.ds_ctx, "leaking DS context\n");
 }
@@ -107,8 +105,7 @@ void flush_thread(void)
 	}
 #endif
 
-	if (unlikely(test_tsk_thread_flag(tsk, TIF_DEBUG)))
-		flush_thread_hw_breakpoint(tsk);
+	flush_ptrace_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
 	/*
 	 * Forget coprocessor state..

commit 41a48d14f6991020c9bb6b93e289ca5b411ed09a
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Oct 5 19:23:06 2009 +0900

    x86/hw-breakpoints: Actually flush thread breakpoints in flush_thread().
    
    flush_thread() tries to do a TIF_DEBUG check before calling in to
    flush_thread_hw_breakpoint() (which subsequently clears the thread flag),
    but for some reason, the x86 code is manually clearing TIF_DEBUG
    immediately before the test, so this path will never be taken.
    
    This kills off the erroneous clear_tsk_thread_flag() and lets
    flush_thread_hw_breakpoint() actually get invoked.
    
    Presumably folks were getting lucky with testing and the
    free_thread_info() -> free_thread_xstate() path was taking care of the
    flush there.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>
    Acked-by: "K.Prasad" <prasad@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    LKML-Reference: <20091005102306.GA7889@linux-sh.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 2275ce5776de..cf8ee0016307 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -107,8 +107,6 @@ void flush_thread(void)
 	}
 #endif
 
-	clear_tsk_thread_flag(tsk, TIF_DEBUG);
-
 	if (unlikely(test_tsk_thread_flag(tsk, TIF_DEBUG)))
 		flush_thread_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));

commit 0f8f86c7bdd1c954fbe153af437a0d91a6c5721a
Merge: dca2d6ac09d9 f39cdf25bf77
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Oct 18 01:09:09 2009 +0200

    Merge commit 'perf/core' into perf/hw-breakpoint
    
    Conflicts:
            kernel/Makefile
            kernel/trace/Makefile
            kernel/trace/trace.h
            samples/Makefile
    
    Merge reason: We need to be uptodate with the perf events development
    branch because we plan to rewrite the breakpoints API on top of
    perf events.

commit 7c68af6e32c73992bad24107311f3433c89016e2
Author: Avi Kivity <avi@redhat.com>
Date:   Sat Sep 19 09:40:22 2009 +0300

    core, x86: Add user return notifiers
    
    Add a general per-cpu notifier that is called whenever the kernel is
    about to return to userspace.  The notifier uses a thread_info flag
    and existing checks, so there is no impact on user return or context
    switch fast paths.
    
    This will be used initially to speed up KVM task switching by lazily
    updating MSRs.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    LKML-Reference: <1253342422-13811-1-git-send-email-avi@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 5284cd2b5776..e51b056fc88f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -9,6 +9,7 @@
 #include <linux/pm.h>
 #include <linux/clockchips.h>
 #include <linux/random.h>
+#include <linux/user-return-notifier.h>
 #include <trace/events/power.h>
 #include <asm/system.h>
 #include <asm/apic.h>
@@ -224,6 +225,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 		 */
 		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
 	}
+	propagate_user_return_notify(prev_p, next_p);
 }
 
 int sys_fork(struct pt_regs *regs)

commit 79f5599772ac2f138d7a75b8f3f06a93f09c75f7
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon Jun 15 14:58:26 2009 +0800

    cpumask: use zalloc_cpumask_var() where possible
    
    Remove open-coded zalloc_cpumask_var() and zalloc_cpumask_var_node().
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 847ab4160315..5284cd2b5776 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -555,10 +555,8 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 void __init init_c1e_mask(void)
 {
 	/* If we're using c1e_idle, we need to allocate c1e_mask. */
-	if (pm_idle == c1e_idle) {
-		alloc_cpumask_var(&c1e_mask, GFP_KERNEL);
-		cpumask_clear(c1e_mask);
-	}
+	if (pm_idle == c1e_idle)
+		zalloc_cpumask_var(&c1e_mask, GFP_KERNEL);
 }
 
 static int __init idle_setup(char *str)

commit 288f023e708efd89d77ce9acf977a33a623ae83d
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Sat Sep 19 13:35:33 2009 +0200

    tracing, x86, cpuidle: Move the end point of a C state in the power tracer
    
    The "end of a C state" trace point currently happens before
    the code runs that corrects the TSC for having stopped during idle.
    
    The result of this is that the timestamp of the end-of-C-state event
    is garbage on cpus where the TSC stops during idle.
    
    This patch moves the end point of the C state to after the timekeeping
    engine of the kernel has been corrected.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: fweisbec@gmail.com
    Cc: peterz@infradead.org
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <20090919133533.139c2a46@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7b60e3906889..847ab4160315 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -309,7 +309,6 @@ void default_idle(void)
 		else
 			local_irq_enable();
 		current_thread_info()->status |= TS_POLLING;
-		trace_power_end(0);
 	} else {
 		local_irq_enable();
 		/* loop is done by the caller */
@@ -377,7 +376,6 @@ void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 		if (!need_resched())
 			__mwait(ax, cx);
 	}
-	trace_power_end(0);
 }
 
 /* Default MONITOR/MWAIT with no hints, used for default C1 state */
@@ -394,7 +392,6 @@ static void mwait_idle(void)
 			__sti_mwait(0, 0);
 		else
 			local_irq_enable();
-		trace_power_end(0);
 	} else
 		local_irq_enable();
 }

commit 6161352142d5fed4cd753b32e5ccde66e705b14e
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Thu Sep 17 16:11:28 2009 +0200

    tracing, perf: Convert the power tracer into an event tracer
    
    This patch converts the existing power tracer into an event tracer,
    so that power events (C states and frequency changes) can be
    tracked via "perf".
    
    This also removes the perl script that was used to demo the tracer;
    its functionality is being replaced entirely with timechart.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <20090912130542.6d314860@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 071166a4ba83..7b60e3906889 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -9,7 +9,7 @@
 #include <linux/pm.h>
 #include <linux/clockchips.h>
 #include <linux/random.h>
-#include <trace/power.h>
+#include <trace/events/power.h>
 #include <asm/system.h>
 #include <asm/apic.h>
 #include <asm/syscalls.h>
@@ -25,9 +25,6 @@ EXPORT_SYMBOL(idle_nomwait);
 
 struct kmem_cache *task_xstate_cachep;
 
-DEFINE_TRACE(power_start);
-DEFINE_TRACE(power_end);
-
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	*dst = *src;
@@ -299,9 +296,7 @@ static inline int hlt_use_halt(void)
 void default_idle(void)
 {
 	if (hlt_use_halt()) {
-		struct power_trace it;
-
-		trace_power_start(&it, POWER_CSTATE, 1);
+		trace_power_start(POWER_CSTATE, 1);
 		current_thread_info()->status &= ~TS_POLLING;
 		/*
 		 * TS_POLLING-cleared state must be visible before we
@@ -314,7 +309,7 @@ void default_idle(void)
 		else
 			local_irq_enable();
 		current_thread_info()->status |= TS_POLLING;
-		trace_power_end(&it);
+		trace_power_end(0);
 	} else {
 		local_irq_enable();
 		/* loop is done by the caller */
@@ -372,9 +367,7 @@ EXPORT_SYMBOL_GPL(cpu_idle_wait);
  */
 void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 {
-	struct power_trace it;
-
-	trace_power_start(&it, POWER_CSTATE, (ax>>4)+1);
+	trace_power_start(POWER_CSTATE, (ax>>4)+1);
 	if (!need_resched()) {
 		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
@@ -384,15 +377,14 @@ void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 		if (!need_resched())
 			__mwait(ax, cx);
 	}
-	trace_power_end(&it);
+	trace_power_end(0);
 }
 
 /* Default MONITOR/MWAIT with no hints, used for default C1 state */
 static void mwait_idle(void)
 {
-	struct power_trace it;
 	if (!need_resched()) {
-		trace_power_start(&it, POWER_CSTATE, 1);
+		trace_power_start(POWER_CSTATE, 1);
 		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
 			clflush((void *)&current_thread_info()->flags);
 
@@ -402,7 +394,7 @@ static void mwait_idle(void)
 			__sti_mwait(0, 0);
 		else
 			local_irq_enable();
-		trace_power_end(&it);
+		trace_power_end(0);
 	} else
 		local_irq_enable();
 }
@@ -414,13 +406,11 @@ static void mwait_idle(void)
  */
 static void poll_idle(void)
 {
-	struct power_trace it;
-
-	trace_power_start(&it, POWER_CSTATE, 0);
+	trace_power_start(POWER_CSTATE, 0);
 	local_irq_enable();
 	while (!need_resched())
 		cpu_relax();
-	trace_power_end(&it);
+	trace_power_end(0);
 }
 
 /*

commit a1922ed661ab2c1637d0b10cde933bd9cd33d965
Merge: 75e33751ca8b d28daf923ac5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 7 08:19:51 2009 +0200

    Merge branch 'tracing/core' into tracing/hw-breakpoints
    
    Conflicts:
            arch/Kconfig
            kernel/trace/trace.h
    
    Merge reason: resolve the conflicts, plus adopt to the new
                  ring-buffer APIs.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f833bab87fca5c3ce13778421b1365845843b976
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Aug 17 14:34:59 2009 -0700

    clockevent: Prevent dead lock on clockevents_lock
    
    Currently clockevents_notify() is called with interrupts enabled at
    some places and interrupts disabled at some other places.
    
    This results in a deadlock in this scenario.
    
    cpu A holds clockevents_lock in clockevents_notify() with irqs enabled
    cpu B waits for clockevents_lock in clockevents_notify() with irqs disabled
    cpu C doing set_mtrr() which will try to rendezvous of all the cpus.
    
    This will result in C and A come to the rendezvous point and waiting
    for B. B is stuck forever waiting for the spinlock and thus not
    reaching the rendezvous point.
    
    Fix the clockevents code so that clockevents_lock is taken with
    interrupts disabled and thus avoid the above deadlock.
    
    Also call lapic_timer_propagate_broadcast() on the destination cpu so
    that we avoid calling smp_call_function() in the clockevents notifier
    chain.
    
    This issue left us wondering if we need to change the MTRR rendezvous
    logic to use stop machine logic (instead of smp_call_function) or add
    a check in spinlock debug code to see if there are other spinlocks
    which gets taken under both interrupts enabled/disabled conditions.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: "Pallipadi Venkatesh" <venkatesh.pallipadi@intel.com>
    Cc: "Brown Len" <len.brown@intel.com>
    LKML-Reference: <1250544899.2709.210.camel@sbs-t61.sc.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 994dd6a4a2a0..071166a4ba83 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -519,16 +519,12 @@ static void c1e_idle(void)
 		if (!cpumask_test_cpu(cpu, c1e_mask)) {
 			cpumask_set_cpu(cpu, c1e_mask);
 			/*
-			 * Force broadcast so ACPI can not interfere. Needs
-			 * to run with interrupts enabled as it uses
-			 * smp_function_call.
+			 * Force broadcast so ACPI can not interfere.
 			 */
-			local_irq_enable();
 			clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_FORCE,
 					   &cpu);
 			printk(KERN_INFO "Switch to broadcast mode on CPU%d\n",
 			       cpu);
-			local_irq_disable();
 		}
 		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &cpu);
 

commit eadb8a091b27a840de7450f84ecff5ef13476424
Merge: 73874005cd88 65795efbd380
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 17 12:52:15 2009 +0200

    Merge branch 'linus' into tracing/hw-breakpoints
    
    Conflicts:
            arch/x86/Kconfig
            arch/x86/kernel/traps.c
            arch/x86/power/cpu.c
            arch/x86/power/cpu_32.c
            kernel/Makefile
    
    Semantic conflict:
            arch/x86/kernel/hw_breakpoint.c
    
    Merge reason: Resolve the conflicts, move from put_cpu_no_sched() to
                  put_cpu() in arch/x86/kernel/hw_breakpoint.c.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 2dff440525f8faba8836e9f05297b76f23b4af30
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Sat May 31 15:56:17 2008 +0200

    kmemcheck: add mm functions
    
    With kmemcheck enabled, the slab allocator needs to do this:
    
    1. Tell kmemcheck to allocate the shadow memory which stores the status of
       each byte in the allocation proper, e.g. whether it is initialized or
       uninitialized.
    2. Tell kmemcheck which parts of memory that should be marked uninitialized.
       There are actually a few more states, such as "not yet allocated" and
       "recently freed".
    
    If a slab cache is set up using the SLAB_NOTRACK flag, it will never return
    memory that can take page faults because of kmemcheck.
    
    If a slab cache is NOT set up using the SLAB_NOTRACK flag, callers can still
    request memory with the __GFP_NOTRACK flag. This does not prevent the page
    faults from occuring, however, but marks the object in question as being
    initialized so that no warnings will ever be produced for this object.
    
    In addition to (and in contrast to) __GFP_NOTRACK, the
    __GFP_NOTRACK_FALSE_POSITIVE flag indicates that the allocation should
    not be tracked _because_ it would produce a false positive. Their values
    are identical, but need not be so in the future (for example, we could now
    enable/disable false positives with a config option).
    
    Parts of this patch were contributed by Pekka Enberg but merged for
    atomicity.
    
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    
    [rebased for mainline inclusion]
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 3bb2be1649bd..994dd6a4a2a0 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -63,7 +63,7 @@ void arch_task_cache_init(void)
         task_xstate_cachep =
         	kmem_cache_create("task_xstate", xstate_size,
 				  __alignof__(union thread_xstate),
-				  SLAB_PANIC, NULL);
+				  SLAB_PANIC | SLAB_NOTRACK, NULL);
 }
 
 /*

commit 862366118026a358882eefc70238dbcc3db37aac
Merge: 57eee9ae7bbc 511b01bdf64a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 19:53:40 2009 -0700

    Merge branch 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (244 commits)
      Revert "x86, bts: reenable ptrace branch trace support"
      tracing: do not translate event helper macros in print format
      ftrace/documentation: fix typo in function grapher name
      tracing/events: convert block trace points to TRACE_EVENT(), fix !CONFIG_BLOCK
      tracing: add protection around module events unload
      tracing: add trace_seq_vprint interface
      tracing: fix the block trace points print size
      tracing/events: convert block trace points to TRACE_EVENT()
      ring-buffer: fix ret in rb_add_time_stamp
      ring-buffer: pass in lockdep class key for reader_lock
      tracing: add annotation to what type of stack trace is recorded
      tracing: fix multiple use of __print_flags and __print_symbolic
      tracing/events: fix output format of user stack
      tracing/events: fix output format of kernel stack
      tracing/trace_stack: fix the number of entries in the header
      ring-buffer: discard timestamps that are at the start of the buffer
      ring-buffer: try to discard unneeded timestamps
      ring-buffer: fix bug in ring_buffer_discard_commit
      ftrace: do not profile functions when disabled
      tracing: make trace pipe recognize latency format flag
      ...

commit bb7762961d3ce745688e9050e914c1d3f980268d
Merge: 48c72d1ab4ec 35d5a9a61490
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 16:13:20 2009 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (22 commits)
      x86: fix system without memory on node0
      x86, mm: Fix node_possible_map logic
      mm, x86: remove MEMORY_HOTPLUG_RESERVE related code
      x86: make sparse mem work in non-NUMA mode
      x86: process.c, remove useless headers
      x86: merge process.c a bit
      x86: use sparse_memory_present_with_active_regions() on UMA
      x86: unify 64-bit UMA and NUMA paging_init()
      x86: Allow 1MB of slack between the e820 map and SRAT, not 4GB
      x86: Sanity check the e820 against the SRAT table using e820 map only
      x86: clean up and and print out initial max_pfn_mapped
      x86/pci: remove rounding quirk from e820_setup_gap()
      x86, e820, pci: reserve extra free space near end of RAM
      x86: fix typo in address space documentation
      x86: 46 bit physical address support on 64 bits
      x86, mm: fault.c, use printk_once() in is_errata93()
      x86: move per-cpu mmu_gathers to mm/init.c
      x86: move max_pfn_mapped and max_low_pfn_mapped to setup.c
      x86: unify noexec handling
      x86: remove (null) in /sys kernel_page_tables
      ...

commit 66cb5917295958652ff6ba36d83f98f2379c46b4
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Mon Jun 1 23:44:55 2009 +0530

    hw-breakpoints: use the new wrapper routines to access debug registers in process/thread code
    
    This patch enables the use of abstract debug registers in
    process-handling routines, according to the new hardware breakpoint
    Api.
    
    [ Impact: adapt thread breakpoints handling code to the new breakpoint Api ]
    
    Original-patch-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Reviewed-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 291527cb438a..19a686c401b5 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -15,6 +15,8 @@
 #include <asm/uaccess.h>
 #include <asm/i387.h>
 #include <asm/ds.h>
+#include <asm/debugreg.h>
+#include <asm/hw_breakpoint.h>
 
 unsigned long idle_halt;
 EXPORT_SYMBOL(idle_halt);
@@ -46,6 +48,8 @@ void free_thread_xstate(struct task_struct *tsk)
 		kmem_cache_free(task_xstate_cachep, tsk->thread.xstate);
 		tsk->thread.xstate = NULL;
 	}
+	if (unlikely(test_tsk_thread_flag(tsk, TIF_DEBUG)))
+		flush_thread_hw_breakpoint(tsk);
 
 	WARN(tsk->thread.ds_ctx, "leaking DS context\n");
 }
@@ -106,12 +110,8 @@ void flush_thread(void)
 
 	clear_tsk_thread_flag(tsk, TIF_DEBUG);
 
-	tsk->thread.debugreg[0] = 0;
-	tsk->thread.debugreg[1] = 0;
-	tsk->thread.debugreg[2] = 0;
-	tsk->thread.debugreg[3] = 0;
-	tsk->thread.debugreg6 = 0;
-	tsk->thread.debugreg7 = 0;
+	if (unlikely(test_tsk_thread_flag(tsk, TIF_DEBUG)))
+		flush_thread_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
 	/*
 	 * Forget coprocessor state..
@@ -193,16 +193,6 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 	else if (next->debugctlmsr != prev->debugctlmsr)
 		update_debugctlmsr(next->debugctlmsr);
 
-	if (test_tsk_thread_flag(next_p, TIF_DEBUG)) {
-		set_debugreg(next->debugreg[0], 0);
-		set_debugreg(next->debugreg[1], 1);
-		set_debugreg(next->debugreg[2], 2);
-		set_debugreg(next->debugreg[3], 3);
-		/* no 4 and 5 */
-		set_debugreg(next->debugreg6, 6);
-		set_debugreg(next->debugreg7, 7);
-	}
-
 	if (test_tsk_thread_flag(prev_p, TIF_NOTSC) ^
 	    test_tsk_thread_flag(next_p, TIF_NOTSC)) {
 		/* prev and next are different */

commit b332828c39326b1dca617f387dd15d12e81cd5f0
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Mon Jun 1 23:43:10 2009 +0530

    hw-breakpoints: prepare the code for Hardware Breakpoint interfaces
    
    The generic hardware breakpoint interface provides an abstraction of
    hardware breakpoints in front of specific arch implementations for both kernel
    and user side breakpoints.
    This includes execution breakpoints and read/write breakpoints, also known as
    "watchpoints".
    
    This patch introduces header files containing constants, structure definitions
    and declaration of functions used by the hardware breakpoint core and x86
    specific code.
    It also introduces an array based storage for the debug-register values in
    'struct thread_struct', while modifying all users of debugreg<n> member in the
    structure.
    
    [ Impact: add headers for new hardware breakpoint interface ]
    
    Original-patch-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Reviewed-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index fb5dfb891f0f..291527cb438a 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -106,10 +106,10 @@ void flush_thread(void)
 
 	clear_tsk_thread_flag(tsk, TIF_DEBUG);
 
-	tsk->thread.debugreg0 = 0;
-	tsk->thread.debugreg1 = 0;
-	tsk->thread.debugreg2 = 0;
-	tsk->thread.debugreg3 = 0;
+	tsk->thread.debugreg[0] = 0;
+	tsk->thread.debugreg[1] = 0;
+	tsk->thread.debugreg[2] = 0;
+	tsk->thread.debugreg[3] = 0;
 	tsk->thread.debugreg6 = 0;
 	tsk->thread.debugreg7 = 0;
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
@@ -194,10 +194,10 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 		update_debugctlmsr(next->debugctlmsr);
 
 	if (test_tsk_thread_flag(next_p, TIF_DEBUG)) {
-		set_debugreg(next->debugreg0, 0);
-		set_debugreg(next->debugreg1, 1);
-		set_debugreg(next->debugreg2, 2);
-		set_debugreg(next->debugreg3, 3);
+		set_debugreg(next->debugreg[0], 0);
+		set_debugreg(next->debugreg[1], 1);
+		set_debugreg(next->debugreg[2], 2);
+		set_debugreg(next->debugreg[3], 3);
 		/* no 4 and 5 */
 		set_debugreg(next->debugreg6, 6);
 		set_debugreg(next->debugreg7, 7);

commit 9d62dcdfa6f6fc843f7d9b494bcd48f00b94f883
Author: Amerigo Wang <amwang@redhat.com>
Date:   Mon May 11 22:05:28 2009 -0400

    x86: merge process.c a bit
    
    Merge arch_align_stack() and arch_randomize_brk(), since
    they are the same.
    
    Tested on x86_64.
    
    [ Impact: cleanup ]
    
    Signed-off-by: Amerigo Wang <amwang@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ca989158e847..2b9a8d0fb474 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -8,6 +8,7 @@
 #include <linux/module.h>
 #include <linux/pm.h>
 #include <linux/clockchips.h>
+#include <linux/random.h>
 #include <trace/power.h>
 #include <asm/system.h>
 #include <asm/apic.h>
@@ -613,3 +614,16 @@ static int __init idle_setup(char *str)
 }
 early_param("idle", idle_setup);
 
+unsigned long arch_align_stack(unsigned long sp)
+{
+	if (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)
+		sp -= get_random_int() % 8192;
+	return sp & ~0xf;
+}
+
+unsigned long arch_randomize_brk(struct mm_struct *mm)
+{
+	unsigned long range_end = mm->brk + 0x02000000;
+	return randomize_range(mm->brk, range_end, 0) ? : mm->brk;
+}
+

commit 2c1b284e4fa260fd922b9a65c99169e2630c6862
Author: Jaswinder Singh Rajput <jaswinder@kernel.org>
Date:   Sat Apr 11 00:03:10 2009 +0530

    x86: clean up declarations and variables
    
    Impact: cleanup, no code changed
    
     - syscalls.h       update declarations due to unifications
     - irq.c            declare smp_generic_interrupt() before it gets used
     - process.c        declare sys_fork() and sys_vfork() before they get used
     - tsc.c            rename tsc_khz shadowed variable
     - apic/probe_32.c  declare apic_default before it gets used
     - apic/nmi.c       prev_nmi_count should be unsigned
     - apic/io_apic.c   declare smp_irq_move_cleanup_interrupt() before it gets used
     - mm/init.c        declare direct_gbpages and free_initrd_mem before they get used
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinder@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ca989158e847..3e21e38d7e37 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -11,6 +11,7 @@
 #include <trace/power.h>
 #include <asm/system.h>
 #include <asm/apic.h>
+#include <asm/syscalls.h>
 #include <asm/idle.h>
 #include <asm/uaccess.h>
 #include <asm/i387.h>

commit 2311f0de21c17b2a8b960677a9cccfbfa52beb35
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Apr 3 16:43:46 2009 +0200

    x86, ds: add leakage warning
    
    Add a warning in case a debug store context is not removed before
    the task it is attached to is freed.
    
    Remove the old warning at thread exit. It is too early.
    
    Declare the debug store context field in thread_struct unconditionally.
    
    Remove ds_copy_thread() and ds_exit_thread() and do the work directly
    in process*.c.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Cc: roland@redhat.com
    Cc: eranian@googlemail.com
    Cc: oleg@redhat.com
    Cc: juan.villacis@intel.com
    Cc: ak@linux.jf.intel.com
    LKML-Reference: <20090403144601.254472000@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ca989158e847..fb5dfb891f0f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -14,6 +14,7 @@
 #include <asm/idle.h>
 #include <asm/uaccess.h>
 #include <asm/i387.h>
+#include <asm/ds.h>
 
 unsigned long idle_halt;
 EXPORT_SYMBOL(idle_halt);
@@ -45,6 +46,8 @@ void free_thread_xstate(struct task_struct *tsk)
 		kmem_cache_free(task_xstate_cachep, tsk->thread.xstate);
 		tsk->thread.xstate = NULL;
 	}
+
+	WARN(tsk->thread.ds_ctx, "leaking DS context\n");
 }
 
 void free_thread_info(struct thread_info *ti)
@@ -83,8 +86,6 @@ void exit_thread(void)
 		put_cpu();
 		kfree(bp);
 	}
-
-	ds_exit_thread(current);
 }
 
 void flush_thread(void)

commit 714f83d5d9f7c785f622259dad1f4fad12d64664
Merge: 8901e7ffc2fa 645dae969c3b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 5 11:04:19 2009 -0700

    Merge branch 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (413 commits)
      tracing, net: fix net tree and tracing tree merge interaction
      tracing, powerpc: fix powerpc tree and tracing tree interaction
      ring-buffer: do not remove reader page from list on ring buffer free
      function-graph: allow unregistering twice
      trace: make argument 'mem' of trace_seq_putmem() const
      tracing: add missing 'extern' keywords to trace_output.h
      tracing: provide trace_seq_reserve()
      blktrace: print out BLK_TN_MESSAGE properly
      blktrace: extract duplidate code
      blktrace: fix memory leak when freeing struct blk_io_trace
      blktrace: fix blk_probes_ref chaos
      blktrace: make classic output more classic
      blktrace: fix off-by-one bug
      blktrace: fix the original blktrace
      blktrace: fix a race when creating blk_tree_root in debugfs
      blktrace: fix timestamp in binary output
      tracing, Text Edit Lock: cleanup
      tracing: filter fix for TRACE_EVENT_FORMAT events
      ftrace: Using FTRACE_WARN_ON() to check "freed record" in ftrace_release()
      x86: kretprobe-booster interrupt emulation code fix
      ...
    
    Fix up trivial conflicts in
     arch/parisc/include/asm/ftrace.h
     include/linux/memory.h
     kernel/extable.c
     kernel/module.c

commit 8302294f43250dc337108c51882a6007f2b1e2e0
Merge: 4fe70410d9a2 2e572895bf32
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 1 21:54:19 2009 +0200

    Merge branch 'tracing/core-v2' into tracing-for-linus
    
    Conflicts:
            include/linux/slub_def.h
            lib/Kconfig.debug
            mm/slob.c
            mm/slub.c

commit 65fb0d23fcddd8697c871047b700c78817bdaa43
Merge: 8c083f081d00 dfbbe89e197a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 30 23:53:32 2009 +0200

    Merge branch 'linus' into cpumask-for-linus
    
    Conflicts:
            arch/x86/kernel/cpu/common.c

commit 30e1e6d1af2b67558bccf322af2b3e0676b209ae
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Mar 17 14:50:34 2009 +1030

    cpumask: fix CONFIG_CPUMASK_OFFSTACK=y cpu hotunplug crash
    
    Impact: Fix cpu offline when CONFIG_MAXSMP=y
    
    Changeset bc9b83dd1f66402b870301c3c7117b9c1484abb4 "cpumask: convert
    c1e_mask in arch/x86/kernel/process.c to cpumask_var_t" contained a
    bug: c1e_mask is manipulated even if C1E isn't detected (and hence
    not allocated).
    
    This is simply fixed by checking for NULL (which gcc optimizes out
    anyway of CONFIG_CPUMASK_OFFSTACK=n, since it knows ce1_mask can never
    be NULL).
    
    In addition, fix a leak where select_idle_routine re-allocates
    (and re-clears) c1e_mask on every cpu init.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Mike Travis <travis@sgi.com>
    LKML-Reference: <200903171450.34549.rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6638294cec8d..78533a519d8f 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -479,7 +479,8 @@ static int c1e_detected;
 
 void c1e_remove_cpu(int cpu)
 {
-	cpumask_clear_cpu(cpu, c1e_mask);
+	if (c1e_mask != NULL)
+		cpumask_clear_cpu(cpu, c1e_mask);
 }
 
 /*
@@ -556,13 +557,20 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 		pm_idle = mwait_idle;
 	} else if (check_c1e_idle(c)) {
 		printk(KERN_INFO "using C1E aware idle routine\n");
-		alloc_cpumask_var(&c1e_mask, GFP_KERNEL);
-		cpumask_clear(c1e_mask);
 		pm_idle = c1e_idle;
 	} else
 		pm_idle = default_idle;
 }
 
+void __init init_c1e_mask(void)
+{
+	/* If we're using c1e_idle, we need to allocate c1e_mask. */
+	if (pm_idle == c1e_idle) {
+		alloc_cpumask_var(&c1e_mask, GFP_KERNEL);
+		cpumask_clear(c1e_mask);
+	}
+}
+
 static int __init idle_setup(char *str)
 {
 	if (!str)

commit 250981e6e1ef04947eccaa55e8c25ddcaa47a02e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 16 13:07:21 2009 +0100

    x86: reduce preemption off section in exit thread
    
    Impact: latency improvement
    
    No need to keep preemption disabled over the kfree call.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6afa5232dbb7..156f87582c6c 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -65,11 +65,11 @@ void exit_thread(void)
 {
 	struct task_struct *me = current;
 	struct thread_struct *t = &me->thread;
+	unsigned long *bp = t->io_bitmap_ptr;
 
-	if (me->thread.io_bitmap_ptr) {
+	if (bp) {
 		struct tss_struct *tss = &per_cpu(init_tss, get_cpu());
 
-		kfree(t->io_bitmap_ptr);
 		t->io_bitmap_ptr = NULL;
 		clear_thread_flag(TIF_IO_BITMAP);
 		/*
@@ -78,6 +78,7 @@ void exit_thread(void)
 		memset(tss->io_bitmap, 0xff, t->io_bitmap_max);
 		t->io_bitmap_max = 0;
 		put_cpu();
+		kfree(bp);
 	}
 
 	ds_exit_thread(current);

commit 4f0628963c86d2f97b8cb9acc024a7fe288a6a57
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:54 2009 +1030

    cpumask: use new cpumask functions throughout x86
    
    Impact: cleanup
    
    1) &cpu_online_map -> cpu_online_mask
    2) first_cpu/next_cpu_nr -> cpumask_first/cpumask_next
    3) cpu_*_map manipulation -> init_cpu_* / set_cpu_*
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index cad5431951aa..6638294cec8d 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -324,7 +324,7 @@ void stop_this_cpu(void *dummy)
 	/*
 	 * Remove this CPU:
 	 */
-	cpu_clear(smp_processor_id(), cpu_online_map);
+	set_cpu_online(smp_processor_id(), false);
 	disable_local_APIC();
 
 	for (;;) {

commit bc9b83dd1f66402b870301c3c7117b9c1484abb4
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:49 2009 +1030

    cpumask: convert c1e_mask in arch/x86/kernel/process.c to cpumask_var_t.
    
    Impact: reduce kernel size when CONFIG_CPUMASK_OFFSTACK=y
    
    Simple conversion.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6afa5232dbb7..cad5431951aa 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -474,12 +474,12 @@ static int __cpuinit check_c1e_idle(const struct cpuinfo_x86 *c)
 	return 1;
 }
 
-static cpumask_t c1e_mask = CPU_MASK_NONE;
+static cpumask_var_t c1e_mask;
 static int c1e_detected;
 
 void c1e_remove_cpu(int cpu)
 {
-	cpu_clear(cpu, c1e_mask);
+	cpumask_clear_cpu(cpu, c1e_mask);
 }
 
 /*
@@ -508,8 +508,8 @@ static void c1e_idle(void)
 	if (c1e_detected) {
 		int cpu = smp_processor_id();
 
-		if (!cpu_isset(cpu, c1e_mask)) {
-			cpu_set(cpu, c1e_mask);
+		if (!cpumask_test_cpu(cpu, c1e_mask)) {
+			cpumask_set_cpu(cpu, c1e_mask);
 			/*
 			 * Force broadcast so ACPI can not interfere. Needs
 			 * to run with interrupts enabled as it uses
@@ -556,6 +556,8 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 		pm_idle = mwait_idle;
 	} else if (check_c1e_idle(c)) {
 		printk(KERN_INFO "using C1E aware idle routine\n");
+		alloc_cpumask_var(&c1e_mask, GFP_KERNEL);
+		cpumask_clear(c1e_mask);
 		pm_idle = c1e_idle;
 	} else
 		pm_idle = default_idle;

commit f0ef03985130287c6c84ebe69416cf790e6cc00e
Merge: 16097439703b 31bbed527e70
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 6 16:44:14 2009 +0100

    Merge branch 'x86/core' into tracing/textedit
    
    Conflicts:
            arch/x86/Kconfig
            block/blktrace.c
            kernel/irq/handle.c
    
    Semantic conflict:
            kernel/trace/blktrace.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 389d1fb11e5f2a16b5e34c547756f0c4dec641f7
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Feb 27 13:25:28 2009 -0800

    x86: unify chunks of kernel/process*.c
    
    With x86-32 and -64 using the same mechanism for managing the
    tss io permissions bitmap, large chunks of process*.c are
    trivially unifyable, including:
    
     - exit_thread
     - flush_thread
     - __switch_to_xtra (along with tsc enable/disable)
    
    and as bonus pickups:
    
     - sys_fork
     - sys_vfork
    
    (Note: asmlinkage expands to empty on x86-64)
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 87b69d4fac16..6afa5232dbb7 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -1,8 +1,8 @@
 #include <linux/errno.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
-#include <asm/idle.h>
 #include <linux/smp.h>
+#include <linux/prctl.h>
 #include <linux/slab.h>
 #include <linux/sched.h>
 #include <linux/module.h>
@@ -11,6 +11,9 @@
 #include <linux/ftrace.h>
 #include <asm/system.h>
 #include <asm/apic.h>
+#include <asm/idle.h>
+#include <asm/uaccess.h>
+#include <asm/i387.h>
 
 unsigned long idle_halt;
 EXPORT_SYMBOL(idle_halt);
@@ -55,6 +58,192 @@ void arch_task_cache_init(void)
 				  SLAB_PANIC, NULL);
 }
 
+/*
+ * Free current thread data structures etc..
+ */
+void exit_thread(void)
+{
+	struct task_struct *me = current;
+	struct thread_struct *t = &me->thread;
+
+	if (me->thread.io_bitmap_ptr) {
+		struct tss_struct *tss = &per_cpu(init_tss, get_cpu());
+
+		kfree(t->io_bitmap_ptr);
+		t->io_bitmap_ptr = NULL;
+		clear_thread_flag(TIF_IO_BITMAP);
+		/*
+		 * Careful, clear this in the TSS too:
+		 */
+		memset(tss->io_bitmap, 0xff, t->io_bitmap_max);
+		t->io_bitmap_max = 0;
+		put_cpu();
+	}
+
+	ds_exit_thread(current);
+}
+
+void flush_thread(void)
+{
+	struct task_struct *tsk = current;
+
+#ifdef CONFIG_X86_64
+	if (test_tsk_thread_flag(tsk, TIF_ABI_PENDING)) {
+		clear_tsk_thread_flag(tsk, TIF_ABI_PENDING);
+		if (test_tsk_thread_flag(tsk, TIF_IA32)) {
+			clear_tsk_thread_flag(tsk, TIF_IA32);
+		} else {
+			set_tsk_thread_flag(tsk, TIF_IA32);
+			current_thread_info()->status |= TS_COMPAT;
+		}
+	}
+#endif
+
+	clear_tsk_thread_flag(tsk, TIF_DEBUG);
+
+	tsk->thread.debugreg0 = 0;
+	tsk->thread.debugreg1 = 0;
+	tsk->thread.debugreg2 = 0;
+	tsk->thread.debugreg3 = 0;
+	tsk->thread.debugreg6 = 0;
+	tsk->thread.debugreg7 = 0;
+	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
+	/*
+	 * Forget coprocessor state..
+	 */
+	tsk->fpu_counter = 0;
+	clear_fpu(tsk);
+	clear_used_math();
+}
+
+static void hard_disable_TSC(void)
+{
+	write_cr4(read_cr4() | X86_CR4_TSD);
+}
+
+void disable_TSC(void)
+{
+	preempt_disable();
+	if (!test_and_set_thread_flag(TIF_NOTSC))
+		/*
+		 * Must flip the CPU state synchronously with
+		 * TIF_NOTSC in the current running context.
+		 */
+		hard_disable_TSC();
+	preempt_enable();
+}
+
+static void hard_enable_TSC(void)
+{
+	write_cr4(read_cr4() & ~X86_CR4_TSD);
+}
+
+static void enable_TSC(void)
+{
+	preempt_disable();
+	if (test_and_clear_thread_flag(TIF_NOTSC))
+		/*
+		 * Must flip the CPU state synchronously with
+		 * TIF_NOTSC in the current running context.
+		 */
+		hard_enable_TSC();
+	preempt_enable();
+}
+
+int get_tsc_mode(unsigned long adr)
+{
+	unsigned int val;
+
+	if (test_thread_flag(TIF_NOTSC))
+		val = PR_TSC_SIGSEGV;
+	else
+		val = PR_TSC_ENABLE;
+
+	return put_user(val, (unsigned int __user *)adr);
+}
+
+int set_tsc_mode(unsigned int val)
+{
+	if (val == PR_TSC_SIGSEGV)
+		disable_TSC();
+	else if (val == PR_TSC_ENABLE)
+		enable_TSC();
+	else
+		return -EINVAL;
+
+	return 0;
+}
+
+void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
+		      struct tss_struct *tss)
+{
+	struct thread_struct *prev, *next;
+
+	prev = &prev_p->thread;
+	next = &next_p->thread;
+
+	if (test_tsk_thread_flag(next_p, TIF_DS_AREA_MSR) ||
+	    test_tsk_thread_flag(prev_p, TIF_DS_AREA_MSR))
+		ds_switch_to(prev_p, next_p);
+	else if (next->debugctlmsr != prev->debugctlmsr)
+		update_debugctlmsr(next->debugctlmsr);
+
+	if (test_tsk_thread_flag(next_p, TIF_DEBUG)) {
+		set_debugreg(next->debugreg0, 0);
+		set_debugreg(next->debugreg1, 1);
+		set_debugreg(next->debugreg2, 2);
+		set_debugreg(next->debugreg3, 3);
+		/* no 4 and 5 */
+		set_debugreg(next->debugreg6, 6);
+		set_debugreg(next->debugreg7, 7);
+	}
+
+	if (test_tsk_thread_flag(prev_p, TIF_NOTSC) ^
+	    test_tsk_thread_flag(next_p, TIF_NOTSC)) {
+		/* prev and next are different */
+		if (test_tsk_thread_flag(next_p, TIF_NOTSC))
+			hard_disable_TSC();
+		else
+			hard_enable_TSC();
+	}
+
+	if (test_tsk_thread_flag(next_p, TIF_IO_BITMAP)) {
+		/*
+		 * Copy the relevant range of the IO bitmap.
+		 * Normally this is 128 bytes or less:
+		 */
+		memcpy(tss->io_bitmap, next->io_bitmap_ptr,
+		       max(prev->io_bitmap_max, next->io_bitmap_max));
+	} else if (test_tsk_thread_flag(prev_p, TIF_IO_BITMAP)) {
+		/*
+		 * Clear any possible leftover bits:
+		 */
+		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
+	}
+}
+
+int sys_fork(struct pt_regs *regs)
+{
+	return do_fork(SIGCHLD, regs->sp, regs, 0, NULL, NULL);
+}
+
+/*
+ * This is trivial, and on the face of it looks like it
+ * could equally well be done in user mode.
+ *
+ * Not so, for quite unobvious reasons - register pressure.
+ * In user mode vfork() cannot have a stack frame, and if
+ * done by calling the "clone()" system call directly, you
+ * do not have enough call-clobbered registers to hold all
+ * the information you need.
+ */
+int sys_vfork(struct pt_regs *regs)
+{
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->sp, regs, 0,
+		       NULL, NULL);
+}
+
+
 /*
  * Idle related variables and functions
  */

commit 72b623c73685e86b70a51855e1058ebc98a9f6ed
Merge: 9abd60304816 b5f9fd0f8a05
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Feb 15 20:43:03 2009 +0100

    Merge branch 'tip/tracing/ftrace' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into tracing/power-tracer

commit b5f9fd0f8a05c9bafb91a9a85b9110938d8e585b
Author: Jason Baron <jbaron@redhat.com>
Date:   Wed Feb 11 13:57:25 2009 -0500

    tracing: convert c/p state power tracer to use tracepoints
    
    Convert the c/p state "power" tracer to use tracepoints. Avoids a
    function call when the tracer is disabled.
    
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 026819ffcb0c..e0d0fd7ab514 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -19,6 +19,9 @@ EXPORT_SYMBOL(idle_nomwait);
 
 struct kmem_cache *task_xstate_cachep;
 
+DEFINE_TRACE(power_start);
+DEFINE_TRACE(power_end);
+
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	*dst = *src;

commit 1c511f740fe7031867f51831854360e8be1ba34c
Merge: e7669b8e3292 00f62f614bb7 b22f4858126a 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 10:25:18 2009 +0100

    Merge branches 'tracing/ftrace', 'tracing/ring-buffer', 'tracing/sysprof', 'tracing/urgent' and 'linus' into tracing/core

commit f8a6b2b9cee298a9663cbe38ce1eb5240987cb62
Merge: ba1511bf7fbd 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:44:22 2009 +0100

    Merge branch 'linus' into x86/apic
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c
            arch/x86/mm/fault.c

commit e736ad548db152776de61d7a26805cfae77ce5ce
Author: Pallipadi, Venkatesh <venkatesh.pallipadi@intel.com>
Date:   Fri Feb 6 16:52:05 2009 -0800

    x86: add clflush before monitor for Intel 7400 series
    
    For Intel 7400 series CPUs, the recommendation is to use a clflush on the
    monitored address just before monitor and mwait pair [1].
    
    This clflush makes sure that there are no false wakeups from mwait when the
    monitored address was recently written to.
    
    [1] "MONITOR/MWAIT Recommendations for Intel Xeon Processor 7400 series"
        section in specification update document of 7400 series
        http://download.intel.com/design/xeon/specupdt/32033601.pdf
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e68bb9e30864..6d12f7e37f8c 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -180,6 +180,9 @@ void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 
 	trace_power_start(&it, POWER_CSTATE, (ax>>4)+1);
 	if (!need_resched()) {
+		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
+			clflush((void *)&current_thread_info()->flags);
+
 		__monitor((void *)&current_thread_info()->flags, 0, 0);
 		smp_mb();
 		if (!need_resched())
@@ -194,6 +197,9 @@ static void mwait_idle(void)
 	struct power_trace it;
 	if (!need_resched()) {
 		trace_power_start(&it, POWER_CSTATE, 1);
+		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
+			clflush((void *)&current_thread_info()->flags);
+
 		__monitor((void *)&current_thread_info()->flags, 0, 0);
 		smp_mb();
 		if (!need_resched())

commit 1292211058aaf872eeb2a0e2677d237916b4501f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Feb 7 22:16:12 2009 +0100

    tracing/power: move the power trace headers to a dedicated file
    
    Impact: cleanup
    
    Move the power tracer headers to trace/power.h to keep ftrace.h and power bits
    more easy to maintain as separated topics.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e68bb9e30864..026819ffcb0c 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -8,7 +8,7 @@
 #include <linux/module.h>
 #include <linux/pm.h>
 #include <linux/clockchips.h>
-#include <linux/ftrace.h>
+#include <trace/power.h>
 #include <asm/system.h>
 #include <asm/apic.h>
 

commit 3e5095d15276efd14a45393666b1bb7536bf179f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 27 17:07:08 2009 +0100

    x86: replace CONFIG_X86_SMP with CONFIG_SMP
    
    The x86/Voyager subarch used to have this distinction between
     'x86 SMP support' and 'Voyager SMP support':
    
     config X86_SMP
            bool
            depends on SMP && ((X86_32 && !X86_VOYAGER) || X86_64)
    
    This is a pointless distinction - Voyager can (and already does) use
    smp_ops to implement various SMP quirks it has - and it can be extended
    more to cover all the specialities of Voyager.
    
    So remove this complication in the Kconfig space.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e68bb9e30864..89537f678b2d 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -344,7 +344,7 @@ static void c1e_idle(void)
 
 void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 {
-#ifdef CONFIG_X86_SMP
+#ifdef CONFIG_SMP
 	if (pm_idle == poll_idle && smp_num_siblings > 1) {
 		printk(KERN_WARNING "WARNING: polling idle and HT enabled,"
 			" performance may degrade.\n");

commit b0f4b285d7ed174804658539129a834270f4829a
Merge: be9c5ae4eeec 5250d329e38c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 28 12:21:10 2008 -0800

    Merge branch 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (241 commits)
      sched, trace: update trace_sched_wakeup()
      tracing/ftrace: don't trace on early stage of a secondary cpu boot, v3
      Revert "x86: disable X86_PTRACE_BTS"
      ring-buffer: prevent false positive warning
      ring-buffer: fix dangling commit race
      ftrace: enable format arguments checking
      x86, bts: memory accounting
      x86, bts: add fork and exit handling
      ftrace: introduce tracing_reset_online_cpus() helper
      tracing: fix warnings in kernel/trace/trace_sched_switch.c
      tracing: fix warning in kernel/trace/trace.c
      tracing/ring-buffer: remove unused ring_buffer size
      trace: fix task state printout
      ftrace: add not to regex on filtering functions
      trace: better use of stack_trace_enabled for boot up code
      trace: add a way to enable or disable the stack tracer
      x86: entry_64 - introduce FTRACE_ frame macro v2
      tracing/ftrace: add the printk-msg-only option
      tracing/ftrace: use preempt_enable_no_resched_notrace in ring_buffer_time_stamp()
      x86, bts: correctly report invalid bts records
      ...
    
    Fixed up trivial conflict in scripts/recordmcount.pl due to SH bits
    being already partly merged by the SH merge.

commit a3eeeefbf1cd1d142c52238cc19c75d14c3bc8d5
Merge: 30cd324e9787 7e3cbc3f774f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 25 12:48:18 2008 +0100

    Merge branch 'x86/tsc' into tracing/core
    
    Merge it to resolve this incidental conflict between the BTS fixes/cleanups
    and changes in x86/tsc:
    
    Conflicts:
            arch/x86/kernel/cpu/intel.c

commit be9a1d3c2e559b267983bcf8b003997b83befb49
Merge: fa623d1b0222 7e3cbc3f774f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 23 16:30:20 2008 +0100

    Merge branch 'x86/tsc' into x86/core

commit fa623d1b0222adbe8f822e53c08003b9679a410c
Merge: 3d44cc3e01ee 1ccedb7cdba6 34945ede3107 d43779740621 c415b3dce30d beeb4195cbc8 f269b07e862c 4e42ebd57b2e e1286f2c686f 878719e831d9 fd28a5b58ddd adf77bac052b 8f2466f45f75 93093d099e5d bb5574608a83 f34a10bd9f8c b6fd6f26733e 30604bb410b5 5b9a0e14eb4b 67bac792cd0c 7a9787e1eba9 f4166c54bfe0 69b88afa8d11 8daa19051e1c 3e1e9002aa8b 8403295e0fa4 4db646b1af8f 205516c12dbb c8182f0016fb ecbf29cdb399
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 23 16:27:23 2008 +0100

    Merge branches 'x86/apic', 'x86/cleanups', 'x86/cpufeature', 'x86/crashdump', 'x86/debug', 'x86/defconfig', 'x86/detect-hyper', 'x86/doc', 'x86/dumpstack', 'x86/early-printk', 'x86/fpu', 'x86/idle', 'x86/io', 'x86/memory-corruption-check', 'x86/microcode', 'x86/mm', 'x86/mtrr', 'x86/nmi-watchdog', 'x86/pat2', 'x86/pci-ioapic-boot-irq-quirks', 'x86/ptrace', 'x86/quirks', 'x86/reboot', 'x86/setup-memory', 'x86/signal', 'x86/sparse-fixes', 'x86/time', 'x86/uv' and 'x86/xen' into x86/core

commit f0bc2202e0373eb8e9b1ddbec930e2e681357db8
Author: Jaswinder Singh <jaswinder@infradead.org>
Date:   Wed Dec 17 23:20:05 2008 +0530

    x86: process.c declare c1e_remove_cpu before they get used
    
    Impact: cleanup, avoid sparse warning
    
    Included asm/idle.h for c1e_remove_cpu() declaration. Fixes this
    sparse warning:
    
      CHECK   arch/x86/kernel/process.c
      arch/x86/kernel/process.c:284:6: warning: symbol 'c1e_remove_cpu' was not declared. Should it be static?
    
    Signed-off-by: Jaswinder Singh <jaswinder@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c622772744d8..b06100f1d612 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -1,6 +1,7 @@
 #include <linux/errno.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
+#include <asm/idle.h>
 #include <linux/smp.h>
 #include <linux/slab.h>
 #include <linux/sched.h>

commit 40fb17152c50a69dc304dd632131c2f41281ce44
Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Mon Nov 17 16:11:37 2008 -0800

    x86: support always running TSC on Intel CPUs
    
    Impact: reward non-stop TSCs with good TSC-based clocksources, etc.
    
    Add support for CPUID_0x80000007_Bit8 on Intel CPUs as well. This bit means
    that the TSC is invariant with C/P/T states and always runs at constant
    frequency.
    
    With Intel CPUs, we have 3 classes
    * CPUs where TSC runs at constant rate and does not stop n C-states
    * CPUs where TSC runs at constant rate, but will stop in deep C-states
    * CPUs where TSC rate will vary based on P/T-states and TSC will stop in deep
      C-states.
    
    To cover these 3, one feature bit (CONSTANT_TSC) is not enough. So, add a
    second bit (NONSTOP_TSC). CONSTANT_TSC indicates that the TSC runs at
    constant frequency irrespective of P/T-states, and NONSTOP_TSC indicates
    that TSC does not stop in deep C-states.
    
    CPUID_0x8000000_Bit8 indicates both these feature bit can be set.
    We still have CONSTANT_TSC _set_ and NONSTOP_TSC _not_set_ on some older Intel
    CPUs, based on model checks. We can use TSC on such CPUs for time, as long as
    those CPUs do not support/enter deep C-states.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c622772744d8..18c70fedba32 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -270,7 +270,7 @@ static void c1e_idle(void)
 		rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
 		if (lo & K8_INTP_C1E_ACTIVE_MASK) {
 			c1e_detected = 1;
-			if (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC))
+			if (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))
 				mark_tsc_unstable("TSC halt in AMD C1E");
 			printk(KERN_INFO "System has AMD C1E enabled\n");
 			set_cpu_cap(&boot_cpu_data, X86_FEATURE_AMDC1E);

commit f3f47a6768a29448866da4422b6f6bee485c947f
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Sun Nov 23 16:49:58 2008 -0800

    tracing: add "power-tracer": C/P state tracer to help power optimization
    
    Impact: new "power-tracer" ftrace plugin
    
    This patch adds a C/P-state ftrace plugin that will generate
    detailed statistics about the C/P-states that are being used,
    so that we can look at detailed decisions that the C/P-state
    code is making, rather than the too high level "average"
    that we have today.
    
    An example way of using this is:
    
     mount -t debugfs none /sys/kernel/debug
     echo cstate > /sys/kernel/debug/tracing/current_tracer
     echo 1 > /sys/kernel/debug/tracing/tracing_enabled
     sleep 1
     echo 0 > /sys/kernel/debug/tracing/tracing_enabled
     cat /sys/kernel/debug/tracing/trace | perl scripts/trace/cstate.pl > out.svg
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c622772744d8..c27af49a4ede 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -7,6 +7,7 @@
 #include <linux/module.h>
 #include <linux/pm.h>
 #include <linux/clockchips.h>
+#include <linux/ftrace.h>
 #include <asm/system.h>
 
 unsigned long idle_halt;
@@ -100,6 +101,9 @@ static inline int hlt_use_halt(void)
 void default_idle(void)
 {
 	if (hlt_use_halt()) {
+		struct power_trace it;
+
+		trace_power_start(&it, POWER_CSTATE, 1);
 		current_thread_info()->status &= ~TS_POLLING;
 		/*
 		 * TS_POLLING-cleared state must be visible before we
@@ -112,6 +116,7 @@ void default_idle(void)
 		else
 			local_irq_enable();
 		current_thread_info()->status |= TS_POLLING;
+		trace_power_end(&it);
 	} else {
 		local_irq_enable();
 		/* loop is done by the caller */
@@ -154,24 +159,31 @@ EXPORT_SYMBOL_GPL(cpu_idle_wait);
  */
 void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 {
+	struct power_trace it;
+
+	trace_power_start(&it, POWER_CSTATE, (ax>>4)+1);
 	if (!need_resched()) {
 		__monitor((void *)&current_thread_info()->flags, 0, 0);
 		smp_mb();
 		if (!need_resched())
 			__mwait(ax, cx);
 	}
+	trace_power_end(&it);
 }
 
 /* Default MONITOR/MWAIT with no hints, used for default C1 state */
 static void mwait_idle(void)
 {
+	struct power_trace it;
 	if (!need_resched()) {
+		trace_power_start(&it, POWER_CSTATE, 1);
 		__monitor((void *)&current_thread_info()->flags, 0, 0);
 		smp_mb();
 		if (!need_resched())
 			__sti_mwait(0, 0);
 		else
 			local_irq_enable();
+		trace_power_end(&it);
 	} else
 		local_irq_enable();
 }
@@ -183,9 +195,13 @@ static void mwait_idle(void)
  */
 static void poll_idle(void)
 {
+	struct power_trace it;
+
+	trace_power_start(&it, POWER_CSTATE, 0);
 	local_irq_enable();
 	while (!need_resched())
 		cpu_relax();
+	trace_power_end(&it);
 }
 
 /*

commit d3ec5cae0921611ceae06464ef6291012dd9849f
Author: Ivan Vecera <ivecera@redhat.com>
Date:   Tue Nov 11 14:33:44 2008 +0100

    x86: call machine_shutdown and stop all CPUs in native_machine_halt
    
    Impact: really halt all CPUs on halt
    
    Function machine_halt (resp. native_machine_halt) is empty for x86
    architectures. When command 'halt -f' is invoked, the message "System
    halted." is displayed but this is not really true because all CPUs are
    still running.
    
    There are also similar inconsistencies for other arches (some uses
    power-off for halt or forever-loop with IRQs enabled/disabled).
    
    IMO there should be used the same approach for all architectures OR
    what does the message "System halted" really mean?
    
    This patch fixes it for x86.
    
    Signed-off-by: Ivan Vecera <ivecera@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c622772744d8..a4da7c4b3129 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -8,6 +8,7 @@
 #include <linux/pm.h>
 #include <linux/clockchips.h>
 #include <asm/system.h>
+#include <asm/apic.h>
 
 unsigned long idle_halt;
 EXPORT_SYMBOL(idle_halt);
@@ -122,6 +123,21 @@ void default_idle(void)
 EXPORT_SYMBOL(default_idle);
 #endif
 
+void stop_this_cpu(void *dummy)
+{
+	local_irq_disable();
+	/*
+	 * Remove this CPU:
+	 */
+	cpu_clear(smp_processor_id(), cpu_online_map);
+	disable_local_APIC();
+
+	for (;;) {
+		if (hlt_works(smp_processor_id()))
+			halt();
+	}
+}
+
 static void do_nothing(void *unused)
 {
 }

commit 0afe2db21394820d32646a695eccf3fbfe6ab5c7
Merge: d84705969f89 43603c8df97f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Oct 11 20:23:20 2008 +0200

    Merge branch 'x86/unify-cpu-detect' into x86-v28-for-linus-phase4-D
    
    Conflicts:
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/signal_64.c
            include/asm-x86/cpufeature.h

commit e496e3d645c93206faf61ff6005995ebd08cc39c
Merge: b159d7a989e5 5bbd4c372400 175e438f7a2d 516cbf3730c4 af2d237bf574 9b1568458a3e 5b7e41ff3726 1befdefcf476 a03352d2c1dc 7b22ff5344fd 2c7e9fd4c6cb 91030ca1e739 dd5523552c28 b3e15bdef689 20211e4d3447 efd327a2d412 c7ffa6c26277 e51a1ac2dfca 5df455155124 d99e90164e6c e621bd18958e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 6 18:17:07 2008 +0200

    Merge branches 'x86/alternatives', 'x86/cleanups', 'x86/commandline', 'x86/crashdump', 'x86/debug', 'x86/defconfig', 'x86/doc', 'x86/exports', 'x86/fpu', 'x86/gart', 'x86/idle', 'x86/mm', 'x86/mtrr', 'x86/nmi-watchdog', 'x86/oprofile', 'x86/paravirt', 'x86/reboot', 'x86/sparse-fixes', 'x86/tsc', 'x86/urgent' and 'x86/vmalloc' into x86-v28-for-linus-phase1

commit 09bfeea13cea843fb03eaa96b5d891fa0abdcc90
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Thu Sep 18 21:12:10 2008 +0200

    x86: c1e_idle: don't mark TSC unstable if CPU has invariant TSC
    
    Impact: Functional TSC is marked unstable on AMD family 0x10 and 0x11 CPUs.
    
    This would be wrong because for those CPUs "invariant TSC" means:
    
       "The TSC counts at the same rate in all P-states, all C states, S0,
       or S1"
    
    (See "Processor BIOS and Kernel Developer's Guides" for those CPUs.)
    
    [ tglx: Changed C1E to AMD C1E in the printks to avoid confusion
            with Intel C1E ]
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index d8c2a299bfe5..876e91890777 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -270,8 +270,9 @@ static void c1e_idle(void)
 		rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
 		if (lo & K8_INTP_C1E_ACTIVE_MASK) {
 			c1e_detected = 1;
-			mark_tsc_unstable("TSC halt in C1E");
-			printk(KERN_INFO "System has C1E enabled\n");
+			if (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC))
+				mark_tsc_unstable("TSC halt in AMD C1E");
+			printk(KERN_INFO "System has AMD C1E enabled\n");
 			set_cpu_cap(&boot_cpu_data, X86_FEATURE_AMDC1E);
 		}
 	}

commit a8d6829044901a67732904be5f1eacdf8539604f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 22 19:02:25 2008 +0200

    x86: prevent C-states hang on AMD C1E enabled machines
    
    Impact: System hang when AMD C1E machines switch into C2/C3
    
    AMD C1E enabled systems do not work with normal ACPI C-states
    even if the BIOS is advertising them. Limit the C-states to
    C1 for the ACPI processor idle code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 2e2247117f6e..d8c2a299bfe5 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -272,6 +272,7 @@ static void c1e_idle(void)
 			c1e_detected = 1;
 			mark_tsc_unstable("TSC halt in C1E");
 			printk(KERN_INFO "System has C1E enabled\n");
+			set_cpu_cap(&boot_cpu_data, X86_FEATURE_AMDC1E);
 		}
 	}
 

commit 4faac97d44ac27bdbb010a9c3597401a8f89341f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 22 18:54:29 2008 +0200

    x86: prevent stale state of c1e_mask across CPU offline/online
    
    Impact: hang which happens across CPU offline/online on AMD C1E systems.
    
    When a CPU goes offline then the corresponding bit in the broadcast
    mask is cleared. For AMD C1E enabled CPUs we do not reenable the
    broadcast when the CPU comes online again as we do not clear the
    corresponding bit in the c1e_mask, which keeps track which CPUs
    have been switched to broadcast already. So on those !$@#& machines
    we never switch back to broadcasting after a CPU offline/online cycle.
    
    Clear the bit when the CPU plays dead.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7fc4d5b0a6a0..2e2247117f6e 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -246,6 +246,14 @@ static int __cpuinit check_c1e_idle(const struct cpuinfo_x86 *c)
 	return 1;
 }
 
+static cpumask_t c1e_mask = CPU_MASK_NONE;
+static int c1e_detected;
+
+void c1e_remove_cpu(int cpu)
+{
+	cpu_clear(cpu, c1e_mask);
+}
+
 /*
  * C1E aware idle routine. We check for C1E active in the interrupt
  * pending message MSR. If we detect C1E, then we handle it the same
@@ -253,9 +261,6 @@ static int __cpuinit check_c1e_idle(const struct cpuinfo_x86 *c)
  */
 static void c1e_idle(void)
 {
-	static cpumask_t c1e_mask = CPU_MASK_NONE;
-	static int c1e_detected;
-
 	if (need_resched())
 		return;
 

commit c58606ad551e9a1c85a9bd4f1698ca41ec279b2c
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun Sep 7 17:58:51 2008 -0700

    x86: remove duplicated force_mwait
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7fc4d5b0a6a0..9f94bb1c8117 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -15,7 +15,6 @@ unsigned long idle_nomwait;
 EXPORT_SYMBOL(idle_nomwait);
 
 struct kmem_cache *task_xstate_cachep;
-static int force_mwait __cpuinitdata;
 
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {

commit 2c7e9fd4c6cb7f4b0bc7162e9a30847e51a1ca1b
Author: Joe Korty <joe.korty@ccur.com>
Date:   Wed Aug 27 10:35:06 2008 -0400

    x86: make poll_idle behave more like the other idle methods
    
    Make poll_idle() behave more like the other idle methods.
    
    Currently, poll_idle() returns immediately.  The other
    idle methods all wait indefinately for some condition
    to come true before returning.  poll_idle should emulate
    these other methods and also wait for a return condition,
    in this case, for need_resched() to become 'true'.
    
    Without this delay the idle loop spends all of its time
    in the outer loop that calls poll_idle.  This outer loop,
    these days, does real work, some of it under rcu locks.
    That work should only be done when idle is entered and
    when idle exits, not continuously while idle is spinning.
    
    Signed-off-by: Joe Korty <joe.korty@ccur.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7fc4d5b0a6a0..4e09d26748cf 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -185,7 +185,8 @@ static void mwait_idle(void)
 static void poll_idle(void)
 {
 	local_irq_enable();
-	cpu_relax();
+	while (!need_resched())
+		cpu_relax();
 }
 
 /*

commit acee709cab689ec7703770e8b8cb5cc3a4abcb31
Merge: 33a37eb411d1 5ff4789d045c 35b680557f95 c4dc59ae7af8 7edf8891ad7a 9781f39fd209 48fe4a76e27d be54f9d1c8df 77e442461c74 caadbdce240c 5e5a29bf2624 e3a61b0a8c0e fec0962e0bed fab3b58d3b24 f2ba93929fdb 48ae74443403 3cabf37f6167 7019cc2dd6fa 2ddf9b7b3e66 e66d90fb4abd
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 21 16:37:17 2008 +0200

    Merge branches 'x86/urgent', 'x86/amd-iommu', 'x86/apic', 'x86/cleanups', 'x86/core', 'x86/cpu', 'x86/fixmap', 'x86/gart', 'x86/kprobes', 'x86/memtest', 'x86/modules', 'x86/nmi', 'x86/pat', 'x86/reboot', 'x86/setup', 'x86/step', 'x86/unify-pci', 'x86/uv', 'x86/xen' and 'xen-64bit' into x86/for-linus

commit 08ad8afaa0f7343e9c64eec5dbbb178e390e03a2
Author: Jan Beulich <jbeulich@novell.com>
Date:   Fri Jul 18 13:45:20 2008 +0100

    x86: reduce force_mwait visibility
    
    It's not used anywhere outside its single referencing file.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 158bd6a16f6a..9f94bb1c8117 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -199,6 +199,7 @@ static void poll_idle(void)
  *
  * idle=mwait overrides this decision and forces the usage of mwait.
  */
+static int __cpuinitdata force_mwait;
 
 #define MWAIT_INFO			0x05
 #define MWAIT_ECX_EXTENDED_INFO		0x01

commit 9781f39fd209cd93ab98b669814191acc67f32fd
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Thu Jul 10 17:13:19 2008 +0200

    x86: consolidate the definition of the force_mwait variable
    
    The force_mwait variable iss defined either in
    arch/x86/kernel/cpu/amd.c or in arch/x86/kernel/setup_64.c, but it is
    only initialized and used in arch/x86/kernel/process.c. This patch
    moves the declaration to arch/x86/kernel/process.c.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Cc: michael@free-electrons.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 4d629c62f4f8..74f2d196adb4 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -15,6 +15,7 @@ unsigned long idle_nomwait;
 EXPORT_SYMBOL(idle_nomwait);
 
 struct kmem_cache *task_xstate_cachep;
+static int force_mwait __cpuinitdata;
 
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {

commit cdbfc557c43ea1f1f9b7062300ecb1254969814b
Merge: 4d8cc874d7ed 5b664cb235e9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 18 13:53:16 2008 +0200

    Merge branch 'linus' into x86/cleanups

commit da5e09a1b3e5a9fc0b15a3feb64e921ccc55ba74
Author: Zhao Yakui <yakui.zhao@intel.com>
Date:   Tue Jun 24 18:01:09 2008 +0800

    ACPI : Create "idle=nomwait" bootparam
    
    "idle=nomwait" disables the use of the MWAIT
    instruction from both C1 (C1_FFH) and deeper (C2C3_FFH)
    C-states.
    
    When MWAIT is unavailable, the BIOS and OS generally
    negotiate to use the HALT instruction for C1,
    and use IO accesses for deeper C-states.
    
    This option is useful for power and performance
    comparisons, and also to work around BIOS bugs
    where broken MWAIT support is advertised.
    
    http://bugzilla.kernel.org/show_bug.cgi?id=10807
    http://bugzilla.kernel.org/show_bug.cgi?id=10914
    
    Signed-off-by: Zhao Yakui <yakui.zhao@intel.com>
    Signed-off-by: Li Shaohua <shaohua.li@intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7fc729498760..4d629c62f4f8 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -11,6 +11,8 @@
 
 unsigned long idle_halt;
 EXPORT_SYMBOL(idle_halt);
+unsigned long idle_nomwait;
+EXPORT_SYMBOL(idle_nomwait);
 
 struct kmem_cache *task_xstate_cachep;
 
@@ -340,6 +342,15 @@ static int __init idle_setup(char *str)
 		pm_idle = default_idle;
 		idle_halt = 1;
 		return 0;
+	} else if (!strcmp(str, "nomwait")) {
+		/*
+		 * If the boot option of "idle=nomwait" is added,
+		 * it means that mwait will be disabled for CPU C2/C3
+		 * states. In such case it won't touch the variable
+		 * of boot_option_idle_override.
+		 */
+		idle_nomwait = 1;
+		return 0;
 	} else
 		return -1;
 

commit c1e3b377ad48febba6f91b8ae42c44ee4d4ab45e
Author: Zhao Yakui <yakui.zhao@intel.com>
Date:   Tue Jun 24 17:58:53 2008 +0800

    ACPI: Create "idle=halt" bootparam
    
    "idle=halt" limits the idle loop to using
    the halt instruction.  No MWAIT, no IO accesses,
    no C-states deeper than C1.
    
    If something is broken in the idle code,
    "idle=halt" is a less severe workaround
    than "idle=poll" which disables all power savings.
    
    Signed-off-by: Zhao Yakui <yakui.zhao@intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 7dceea947232..7fc729498760 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -7,6 +7,10 @@
 #include <linux/module.h>
 #include <linux/pm.h>
 #include <linux/clockchips.h>
+#include <asm/system.h>
+
+unsigned long idle_halt;
+EXPORT_SYMBOL(idle_halt);
 
 struct kmem_cache *task_xstate_cachep;
 
@@ -325,7 +329,18 @@ static int __init idle_setup(char *str)
 		pm_idle = poll_idle;
 	} else if (!strcmp(str, "mwait"))
 		force_mwait = 1;
-	else
+	else if (!strcmp(str, "halt")) {
+		/*
+		 * When the boot option of idle=halt is added, halt is
+		 * forced to be used for CPU idle. In such case CPU C2/C3
+		 * won't be used again.
+		 * To continue to load the CPU idle driver, don't touch
+		 * the boot_option_idle_override.
+		 */
+		pm_idle = default_idle;
+		idle_halt = 1;
+		return 0;
+	} else
 		return -1;
 
 	boot_option_idle_override = 1;

commit 1a781a777b2f6ac46523fe92396215762ced624d
Merge: b9d2252c1e44 42a2f217a5e3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 15 21:55:59 2008 +0200

    Merge branch 'generic-ipi' into generic-ipi-for-linus
    
    Conflicts:
    
            arch/powerpc/Kconfig
            arch/s390/kernel/time.c
            arch/x86/kernel/apic_32.c
            arch/x86/kernel/cpu/perfctr-watchdog.c
            arch/x86/kernel/i8259_64.c
            arch/x86/kernel/ldt.c
            arch/x86/kernel/nmi_64.c
            arch/x86/kernel/smpboot.c
            arch/x86/xen/smp.c
            include/asm-x86/hw_irq_32.h
            include/asm-x86/hw_irq_64.h
            include/asm-x86/mach-default/irq_vectors.h
            include/asm-x86/mach-voyager/irq_vectors.h
            include/asm-x86/smp.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ab6bc3e343fbe3be4a0f67225e849d0db6b4b7ac
Author: Cyrill Gorcunov <gorcunov@gmail.com>
Date:   Sat Jul 5 15:53:36 2008 +0400

    x86: idle process - add checking for NULL early param
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: akpm@linux-foundation.org
    Cc: andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ba370dc8685b..58325a6604a4 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -164,6 +164,9 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 
 static int __init idle_setup(char *str)
 {
+	if (!str)
+		return -EINVAL;
+
 	if (!strcmp(str, "poll")) {
 		printk("using polling idle threads.\n");
 		pm_idle = poll_idle;

commit 0beefa208bb3a9e581a60125703409ebe6f7fa53
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 17 09:12:03 2008 +0200

    x86: add C1E aware idle function, fix
    
    On Tue, 17 Jun 2008, Rafael J. Wysocki wrote:
    >
    > BTW, with the C1E patches reverted I don't get the
    > WARNING: at /home/rafael/src/linux-next/kernel/smp.c:215 smp_call_function_single+0x3d/0xa2
    > in the log.  Thomas?
    
    The BROADCAST_FORCE notification uses smp_function_call and therefor
    must be run with interrupts enabled.
    
    While at it, add a comment for the BROADCAST_EXIT notifier as well.
    
    Reported-and-bisected-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 68ad3539b143..4061d63aabe7 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -267,17 +267,29 @@ static void c1e_idle(void)
 
 		if (!cpu_isset(cpu, c1e_mask)) {
 			cpu_set(cpu, c1e_mask);
-			/* Force broadcast so ACPI can not interfere */
+			/*
+			 * Force broadcast so ACPI can not interfere. Needs
+			 * to run with interrupts enabled as it uses
+			 * smp_function_call.
+			 */
+			local_irq_enable();
 			clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_FORCE,
 					   &cpu);
 			printk(KERN_INFO "Switch to broadcast mode on CPU%d\n",
 			       cpu);
+			local_irq_disable();
 		}
 		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &cpu);
+
 		default_idle();
-		local_irq_disable();
-		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &cpu);
-		local_irq_enable();
+
+		/*
+		 * The switch back from broadcast mode needs to be
+		 * called with interrupts disabled.
+		 */
+		 local_irq_disable();
+		 clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &cpu);
+		 local_irq_enable();
 	} else
 		default_idle();
 }

commit aa276e1cafb3ce9d01d1e837bcd67e92616013ac
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 9 19:15:00 2008 +0200

    x86, clockevents: add C1E aware idle function
    
    C1E on AMD machines is like C3 but without control from the OS. Up to
    now we disabled the local apic timer for those machines as it stops
    when the CPU goes into C1E. This excludes those machines from high
    resolution timers / dynamic ticks, which hurts especially X2 based
    laptops.
    
    The current boot time C1E detection has another, more serious flaw
    as well: some BIOSes do not enable C1E until the ACPI processor module
    is loaded. This causes systems to stop working after that point.
    
    To work nicely with C1E enabled machines we use a separate idle
    function, which checks on idle entry whether C1E was enabled in the
    Interrupt Pending Message MSR. This allows us to do timer broadcasting
    for C1E and covers the late enablement of C1E as well.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 9fea14607dfe..68ad3539b143 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -6,6 +6,7 @@
 #include <linux/sched.h>
 #include <linux/module.h>
 #include <linux/pm.h>
+#include <linux/clockchips.h>
 
 struct kmem_cache *task_xstate_cachep;
 
@@ -219,6 +220,68 @@ static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
 	return (edx & MWAIT_EDX_C1);
 }
 
+/*
+ * Check for AMD CPUs, which have potentially C1E support
+ */
+static int __cpuinit check_c1e_idle(const struct cpuinfo_x86 *c)
+{
+	if (c->x86_vendor != X86_VENDOR_AMD)
+		return 0;
+
+	if (c->x86 < 0x0F)
+		return 0;
+
+	/* Family 0x0f models < rev F do not have C1E */
+	if (c->x86 == 0x0f && c->x86_model < 0x40)
+		return 0;
+
+	return 1;
+}
+
+/*
+ * C1E aware idle routine. We check for C1E active in the interrupt
+ * pending message MSR. If we detect C1E, then we handle it the same
+ * way as C3 power states (local apic timer and TSC stop)
+ */
+static void c1e_idle(void)
+{
+	static cpumask_t c1e_mask = CPU_MASK_NONE;
+	static int c1e_detected;
+
+	if (need_resched())
+		return;
+
+	if (!c1e_detected) {
+		u32 lo, hi;
+
+		rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
+		if (lo & K8_INTP_C1E_ACTIVE_MASK) {
+			c1e_detected = 1;
+			mark_tsc_unstable("TSC halt in C1E");
+			printk(KERN_INFO "System has C1E enabled\n");
+		}
+	}
+
+	if (c1e_detected) {
+		int cpu = smp_processor_id();
+
+		if (!cpu_isset(cpu, c1e_mask)) {
+			cpu_set(cpu, c1e_mask);
+			/* Force broadcast so ACPI can not interfere */
+			clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_FORCE,
+					   &cpu);
+			printk(KERN_INFO "Switch to broadcast mode on CPU%d\n",
+			       cpu);
+		}
+		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &cpu);
+		default_idle();
+		local_irq_disable();
+		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &cpu);
+		local_irq_enable();
+	} else
+		default_idle();
+}
+
 void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_SMP
@@ -236,6 +299,9 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 		 */
 		printk(KERN_INFO "using mwait in idle threads.\n");
 		pm_idle = mwait_idle;
+	} else if (check_c1e_idle(c)) {
+		printk(KERN_INFO "using C1E aware idle routine\n");
+		pm_idle = c1e_idle;
 	} else
 		pm_idle = default_idle;
 }

commit 127a237a1ff49fa5b8e00af91e841598aeea3513
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jun 27 11:48:22 2008 +0200

    fix "smp_call_function: get rid of the unused nonatomic/retry argument"
    
    fix:
    
    arch/x86/kernel/process.c: In function 'cpu_idle_wait':
    arch/x86/kernel/process.c:64: error: too many arguments to function 'smp_call_function'
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ba370dc8685b..2dad8fef391c 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -61,7 +61,7 @@ void cpu_idle_wait(void)
 {
 	smp_mb();
 	/* kick all the CPUs so that they exit out of pm_idle */
-	smp_call_function(do_nothing, NULL, 0, 1);
+	smp_call_function(do_nothing, NULL, 1);
 }
 EXPORT_SYMBOL_GPL(cpu_idle_wait);
 

commit 00dba56465228825ea806e3a7fc0aa6bba7bdc6c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 9 18:35:28 2008 +0200

    x86: move more common idle functions/variables to process.c
    
    more unification. Should cause no change in functionality.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index fe415ba606e0..9fea14607dfe 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -45,6 +45,76 @@ void arch_task_cache_init(void)
 				  SLAB_PANIC, NULL);
 }
 
+/*
+ * Idle related variables and functions
+ */
+unsigned long boot_option_idle_override = 0;
+EXPORT_SYMBOL(boot_option_idle_override);
+
+/*
+ * Powermanagement idle function, if any..
+ */
+void (*pm_idle)(void);
+EXPORT_SYMBOL(pm_idle);
+
+#ifdef CONFIG_X86_32
+/*
+ * This halt magic was a workaround for ancient floppy DMA
+ * wreckage. It should be safe to remove.
+ */
+static int hlt_counter;
+void disable_hlt(void)
+{
+	hlt_counter++;
+}
+EXPORT_SYMBOL(disable_hlt);
+
+void enable_hlt(void)
+{
+	hlt_counter--;
+}
+EXPORT_SYMBOL(enable_hlt);
+
+static inline int hlt_use_halt(void)
+{
+	return (!hlt_counter && boot_cpu_data.hlt_works_ok);
+}
+#else
+static inline int hlt_use_halt(void)
+{
+	return 1;
+}
+#endif
+
+/*
+ * We use this if we don't have any better
+ * idle routine..
+ */
+void default_idle(void)
+{
+	if (hlt_use_halt()) {
+		current_thread_info()->status &= ~TS_POLLING;
+		/*
+		 * TS_POLLING-cleared state must be visible before we
+		 * test NEED_RESCHED:
+		 */
+		smp_mb();
+
+		if (!need_resched())
+			safe_halt();	/* enables interrupts racelessly */
+		else
+			local_irq_enable();
+		current_thread_info()->status |= TS_POLLING;
+	} else {
+		local_irq_enable();
+		/* loop is done by the caller */
+		cpu_relax();
+	}
+}
+#ifdef CONFIG_APM_MODULE
+EXPORT_SYMBOL(default_idle);
+#endif
+
 static void do_nothing(void *unused)
 {
 }

commit 09fd4b4ef5bc7e5222c13acec1bee8cd252fb63f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 9 18:04:27 2008 +0200

    x86: use cpuid to check MWAIT support for C1
    
    cpuid(0x05) provides extended information about MWAIT in EDX when bit
    0 of ECX is set. Bit 4-7 of EDX determine whether MWAIT is supported
    for C1. C1E enabled CPUs have these bits set to 0.
    
    Based on an earlier patch from Andi Kleen.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index b3078f4ce25b..fe415ba606e0 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -122,19 +122,31 @@ static void poll_idle(void)
  *
  * idle=mwait overrides this decision and forces the usage of mwait.
  */
+
+#define MWAIT_INFO			0x05
+#define MWAIT_ECX_EXTENDED_INFO		0x01
+#define MWAIT_EDX_C1			0xf0
+
 static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
 {
+	u32 eax, ebx, ecx, edx;
+
 	if (force_mwait)
 		return 1;
 
-	if (c->x86_vendor == X86_VENDOR_AMD) {
-		switch(c->x86) {
-		case 0x10:
-		case 0x11:
-			return 0;
-		}
-	}
-	return 1;
+	if (c->cpuid_level < MWAIT_INFO)
+		return 0;
+
+	cpuid(MWAIT_INFO, &eax, &ebx, &ecx, &edx);
+	/* Check, whether EDX has extended info about MWAIT */
+	if (!(ecx & MWAIT_ECX_EXTENDED_INFO))
+		return 1;
+
+	/*
+	 * edx enumeratios MONITOR/MWAIT extensions. Check, whether
+	 * C1  supports MWAIT
+	 */
+	return (edx & MWAIT_EDX_C1);
 }
 
 void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)

commit 6ddd2a27948f0bd02a2ad001e8a6816898eba0dc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 9 16:59:53 2008 +0200

    x86: simplify idle selection
    
    default_idle is selected in cpu_idle(), when no other idle routine is
    selected. Select it in select_idle_routine() when mwait is not
    selected.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ba370dc8685b..b3078f4ce25b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -139,27 +139,23 @@ static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
 
 void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 {
-	static int selected;
-
-	if (selected)
-		return;
 #ifdef CONFIG_X86_SMP
 	if (pm_idle == poll_idle && smp_num_siblings > 1) {
 		printk(KERN_WARNING "WARNING: polling idle and HT enabled,"
 			" performance may degrade.\n");
 	}
 #endif
+	if (pm_idle)
+		return;
+
 	if (cpu_has(c, X86_FEATURE_MWAIT) && mwait_usable(c)) {
 		/*
-		 * Skip, if setup has overridden idle.
 		 * One CPU supports mwait => All CPUs supports mwait
 		 */
-		if (!pm_idle) {
-			printk(KERN_INFO "using mwait in idle threads.\n");
-			pm_idle = mwait_idle;
-		}
-	}
-	selected = 1;
+		printk(KERN_INFO "using mwait in idle threads.\n");
+		pm_idle = mwait_idle;
+	} else
+		pm_idle = default_idle;
 }
 
 static int __init idle_setup(char *str)

commit e9623b35599fcdbc00c16535cbefbb4d5578f4ab
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 16 22:55:26 2008 +0200

    x86: disable mwait for AMD family 10H/11H CPUs
    
    The previous revert of 0c07ee38c9d4eb081758f5ad14bbffa7197e1aec left
    out the mwait disable condition for AMD family 10H/11H CPUs.
    
    Andreas Herrman said:
    
    It depends on the CPU. For AMD CPUs that support MWAIT this is wrong.
    Family 0x10 and 0x11 CPUs will enter C1 on HLT. Powersavings then
    depend on a clock divisor and current Pstate of the core.
    
    If all cores of a processor are in halt state (C1) the processor can
    enter the C1E (C1 enhanced) state. If mwait is used this will never
    happen.
    
    Thus HLT saves more power than MWAIT here.
    
    It might be best to switch off the mwait flag for these AMD CPU
    families like it was introduced with commit
    f039b754714a422959027cb18bb33760eb8153f0 (x86: Don't use MWAIT on AMD
    Family 10)
    
    Re-add the AMD families 10H/11H check and disable the mwait usage for
    those.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index c7b6a694ca22..ba370dc8685b 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -110,6 +110,33 @@ static void poll_idle(void)
 	cpu_relax();
 }
 
+/*
+ * mwait selection logic:
+ *
+ * It depends on the CPU. For AMD CPUs that support MWAIT this is
+ * wrong. Family 0x10 and 0x11 CPUs will enter C1 on HLT. Powersavings
+ * then depend on a clock divisor and current Pstate of the core. If
+ * all cores of a processor are in halt state (C1) the processor can
+ * enter the C1E (C1 enhanced) state. If mwait is used this will never
+ * happen.
+ *
+ * idle=mwait overrides this decision and forces the usage of mwait.
+ */
+static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
+{
+	if (force_mwait)
+		return 1;
+
+	if (c->x86_vendor == X86_VENDOR_AMD) {
+		switch(c->x86) {
+		case 0x10:
+		case 0x11:
+			return 0;
+		}
+	}
+	return 1;
+}
+
 void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 {
 	static int selected;
@@ -122,7 +149,7 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 			" performance may degrade.\n");
 	}
 #endif
-	if (cpu_has(c, X86_FEATURE_MWAIT)) {
+	if (cpu_has(c, X86_FEATURE_MWAIT) && mwait_usable(c)) {
 		/*
 		 * Skip, if setup has overridden idle.
 		 * One CPU supports mwait => All CPUs supports mwait

commit a738d897b7b03b83488ae74a9bc03d26a2875dc6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed May 14 08:47:40 2008 +0200

    x86: remove mwait capability C-state check
    
    Vegard Nossum reports:
    
    | powertop shows between 200-400 wakeups/second with the description
    | "<kernel IPI>: Rescheduling interrupts" when all processors have load (e.g.
    | I need to run two busy-loops on my 2-CPU system for this to show up).
    |
    | The bisect resulted in this commit:
    |
    | commit 0c07ee38c9d4eb081758f5ad14bbffa7197e1aec
    | Date:   Wed Jan 30 13:33:16 2008 +0100
    |
    |     x86: use the correct cpuid method to detect MWAIT support for C states
    
    remove the functional effects of this patch and make mwait unconditional.
    
    A future patch will turn off mwait on specific CPUs where that causes
    power to be wasted.
    
    Bisected-by: Vegard Nossum <vegard.nossum@gmail.com>
    Tested-by: Vegard Nossum <vegard.nossum@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 67e9b4a1e89d..c7b6a694ca22 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -99,15 +99,6 @@ static void mwait_idle(void)
 		local_irq_enable();
 }
 
-
-static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
-{
-	if (force_mwait)
-		return 1;
-	/* Any C1 states supported? */
-	return c->cpuid_level >= 5 && ((cpuid_edx(5) >> 4) & 0xf) > 0;
-}
-
 /*
  * On SMP it's slightly faster (but much more power-consuming!)
  * to poll the ->work.need_resched flag instead of waiting for the
@@ -131,7 +122,7 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 			" performance may degrade.\n");
 	}
 #endif
-	if (cpu_has(c, X86_FEATURE_MWAIT) && mwait_usable(c)) {
+	if (cpu_has(c, X86_FEATURE_MWAIT)) {
 		/*
 		 * Skip, if setup has overridden idle.
 		 * One CPU supports mwait => All CPUs supports mwait

commit 7f424a8b08c26dc14ac5c17164014539ac9a5c65
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 25 17:39:01 2008 +0200

    fix idle (arch, acpi and apm) and lockdep
    
    OK, so 25-mm1 gave a lockdep error which made me look into this.
    
    The first thing that I noticed was the horrible mess; the second thing I
    saw was hacks like: 71e93d15612c61c2e26a169567becf088e71b8ff
    
    The problem is that arch idle routines are somewhat inconsitent with
    their IRQ state handling and instead of fixing _that_, we go paper over
    the problem.
    
    So the thing I've tried to do is set a standard for idle routines and
    fix them all up to adhere to that. So the rules are:
    
      idle routines are entered with IRQs disabled
      idle routines will exit with IRQs enabled
    
    Nearly all already did this in one form or another.
    
    Merge the 32 and 64 bit bits so they no longer have different bugs.
    
    As for the actual lockdep warning; __sti_mwait() did a plainly un-annotated
    irq-enable.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Bob Copeland <me@bobcopeland.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 3004d716539d..67e9b4a1e89d 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -4,6 +4,8 @@
 #include <linux/smp.h>
 #include <linux/slab.h>
 #include <linux/sched.h>
+#include <linux/module.h>
+#include <linux/pm.h>
 
 struct kmem_cache *task_xstate_cachep;
 
@@ -42,3 +44,118 @@ void arch_task_cache_init(void)
 				  __alignof__(union thread_xstate),
 				  SLAB_PANIC, NULL);
 }
+
+static void do_nothing(void *unused)
+{
+}
+
+/*
+ * cpu_idle_wait - Used to ensure that all the CPUs discard old value of
+ * pm_idle and update to new pm_idle value. Required while changing pm_idle
+ * handler on SMP systems.
+ *
+ * Caller must have changed pm_idle to the new value before the call. Old
+ * pm_idle value will not be used by any CPU after the return of this function.
+ */
+void cpu_idle_wait(void)
+{
+	smp_mb();
+	/* kick all the CPUs so that they exit out of pm_idle */
+	smp_call_function(do_nothing, NULL, 0, 1);
+}
+EXPORT_SYMBOL_GPL(cpu_idle_wait);
+
+/*
+ * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
+ * which can obviate IPI to trigger checking of need_resched.
+ * We execute MONITOR against need_resched and enter optimized wait state
+ * through MWAIT. Whenever someone changes need_resched, we would be woken
+ * up from MWAIT (without an IPI).
+ *
+ * New with Core Duo processors, MWAIT can take some hints based on CPU
+ * capability.
+ */
+void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
+{
+	if (!need_resched()) {
+		__monitor((void *)&current_thread_info()->flags, 0, 0);
+		smp_mb();
+		if (!need_resched())
+			__mwait(ax, cx);
+	}
+}
+
+/* Default MONITOR/MWAIT with no hints, used for default C1 state */
+static void mwait_idle(void)
+{
+	if (!need_resched()) {
+		__monitor((void *)&current_thread_info()->flags, 0, 0);
+		smp_mb();
+		if (!need_resched())
+			__sti_mwait(0, 0);
+		else
+			local_irq_enable();
+	} else
+		local_irq_enable();
+}
+
+
+static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
+{
+	if (force_mwait)
+		return 1;
+	/* Any C1 states supported? */
+	return c->cpuid_level >= 5 && ((cpuid_edx(5) >> 4) & 0xf) > 0;
+}
+
+/*
+ * On SMP it's slightly faster (but much more power-consuming!)
+ * to poll the ->work.need_resched flag instead of waiting for the
+ * cross-CPU IPI to arrive. Use this option with caution.
+ */
+static void poll_idle(void)
+{
+	local_irq_enable();
+	cpu_relax();
+}
+
+void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
+{
+	static int selected;
+
+	if (selected)
+		return;
+#ifdef CONFIG_X86_SMP
+	if (pm_idle == poll_idle && smp_num_siblings > 1) {
+		printk(KERN_WARNING "WARNING: polling idle and HT enabled,"
+			" performance may degrade.\n");
+	}
+#endif
+	if (cpu_has(c, X86_FEATURE_MWAIT) && mwait_usable(c)) {
+		/*
+		 * Skip, if setup has overridden idle.
+		 * One CPU supports mwait => All CPUs supports mwait
+		 */
+		if (!pm_idle) {
+			printk(KERN_INFO "using mwait in idle threads.\n");
+			pm_idle = mwait_idle;
+		}
+	}
+	selected = 1;
+}
+
+static int __init idle_setup(char *str)
+{
+	if (!strcmp(str, "poll")) {
+		printk("using polling idle threads.\n");
+		pm_idle = poll_idle;
+	} else if (!strcmp(str, "mwait"))
+		force_mwait = 1;
+	else
+		return -1;
+
+	boot_option_idle_override = 1;
+	return 0;
+}
+early_param("idle", idle_setup);
+

commit 1679f2710ac58df580d3716fab1f42ae50a226eb
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed Apr 16 10:27:53 2008 +0200

    x86: fpu xstate split cleanup
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 0e613e7e7b5e..3004d716539d 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -29,11 +29,10 @@ void free_thread_xstate(struct task_struct *tsk)
 	}
 }
 
-
 void free_thread_info(struct thread_info *ti)
 {
 	free_thread_xstate(ti->task);
-	free_pages((unsigned long)(ti), get_order(THREAD_SIZE));
+	free_pages((unsigned long)ti, get_order(THREAD_SIZE));
 }
 
 void arch_task_cache_init(void)

commit aa283f49276e7d840a40fb01eee6de97eaa7e012
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Mar 10 15:28:05 2008 -0700

    x86, fpu: lazy allocation of FPU area - v5
    
    Only allocate the FPU area when the application actually uses FPU, i.e., in the
    first lazy FPU trap. This could save memory for non-fpu using apps.
    
    for example: on my system after boot, there are around 300 processes, with
    only 17 using FPU.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index ead24efbcba0..0e613e7e7b5e 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -5,24 +5,34 @@
 #include <linux/slab.h>
 #include <linux/sched.h>
 
-static struct kmem_cache *task_xstate_cachep;
+struct kmem_cache *task_xstate_cachep;
 
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	*dst = *src;
-	dst->thread.xstate = kmem_cache_alloc(task_xstate_cachep, GFP_KERNEL);
-	if (!dst->thread.xstate)
-		return -ENOMEM;
-	WARN_ON((unsigned long)dst->thread.xstate & 15);
-	memcpy(dst->thread.xstate, src->thread.xstate, xstate_size);
+	if (src->thread.xstate) {
+		dst->thread.xstate = kmem_cache_alloc(task_xstate_cachep,
+						      GFP_KERNEL);
+		if (!dst->thread.xstate)
+			return -ENOMEM;
+		WARN_ON((unsigned long)dst->thread.xstate & 15);
+		memcpy(dst->thread.xstate, src->thread.xstate, xstate_size);
+	}
 	return 0;
 }
 
-void free_thread_info(struct thread_info *ti)
+void free_thread_xstate(struct task_struct *tsk)
 {
-	kmem_cache_free(task_xstate_cachep, ti->task->thread.xstate);
-	ti->task->thread.xstate = NULL;
+	if (tsk->thread.xstate) {
+		kmem_cache_free(task_xstate_cachep, tsk->thread.xstate);
+		tsk->thread.xstate = NULL;
+	}
+}
+
 
+void free_thread_info(struct thread_info *ti)
+{
+	free_thread_xstate(ti->task);
 	free_pages((unsigned long)(ti), get_order(THREAD_SIZE));
 }
 

commit 61c4628b538608c1a85211ed8438136adfeb9a95
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Mar 10 15:28:04 2008 -0700

    x86, fpu: split FPU state from task struct - v5
    
    Split the FPU save area from the task struct. This allows easy migration
    of FPU context, and it's generally cleaner. It also allows the following
    two optimizations:
    
    1) only allocate when the application actually uses FPU, so in the first
    lazy FPU trap. This could save memory for non-fpu using apps. Next patch
    does this lazy allocation.
    
    2) allocate the right size for the actual cpu rather than 512 bytes always.
    Patches enabling xsave/xrstor support (coming shortly) will take advantage
    of this.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
new file mode 100644
index 000000000000..ead24efbcba0
--- /dev/null
+++ b/arch/x86/kernel/process.c
@@ -0,0 +1,35 @@
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+
+static struct kmem_cache *task_xstate_cachep;
+
+int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
+{
+	*dst = *src;
+	dst->thread.xstate = kmem_cache_alloc(task_xstate_cachep, GFP_KERNEL);
+	if (!dst->thread.xstate)
+		return -ENOMEM;
+	WARN_ON((unsigned long)dst->thread.xstate & 15);
+	memcpy(dst->thread.xstate, src->thread.xstate, xstate_size);
+	return 0;
+}
+
+void free_thread_info(struct thread_info *ti)
+{
+	kmem_cache_free(task_xstate_cachep, ti->task->thread.xstate);
+	ti->task->thread.xstate = NULL;
+
+	free_pages((unsigned long)(ti), get_order(THREAD_SIZE));
+}
+
+void arch_task_cache_init(void)
+{
+        task_xstate_cachep =
+        	kmem_cache_create("task_xstate", xstate_size,
+				  __alignof__(union thread_xstate),
+				  SLAB_PANIC, NULL);
+}
