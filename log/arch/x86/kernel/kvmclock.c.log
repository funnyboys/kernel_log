commit b95a8a27c300d1a39a4e36f63a518ef36e4b966c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 7 13:38:56 2020 +0100

    x86/vdso: Use generic VDSO clock mode storage
    
    Switch to the generic VDSO clock mode storage.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com> (VDSO parts)
    Acked-by: Juergen Gross <jgross@suse.com> (Xen parts)
    Acked-by: Paolo Bonzini <pbonzini@redhat.com> (KVM parts)
    Link: https://lkml.kernel.org/r/20200207124403.152039903@linutronix.de

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 33f2cac25f13..34b18f6eeb2c 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -161,7 +161,7 @@ bool kvm_check_and_clear_guest_paused(void)
 
 static int kvm_cs_enable(struct clocksource *cs)
 {
-	vclocks_set_used(VCLOCK_PVCLOCK);
+	vclocks_set_used(VDSO_CLOCKMODE_PVCLOCK);
 	return 0;
 }
 
@@ -279,7 +279,7 @@ static int __init kvm_setup_vsyscall_timeinfo(void)
 	if (!(flags & PVCLOCK_TSC_STABLE_BIT))
 		return 0;
 
-	kvm_clock.archdata.vclock_mode = VCLOCK_PVCLOCK;
+	kvm_clock.vdso_clock_mode = VDSO_CLOCKMODE_PVCLOCK;
 #endif
 
 	kvmclock_init_mem();

commit eec399dd862762b9594df3659f15839a4e12f17a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 7 13:38:54 2020 +0100

    x86/vdso: Move VDSO clocksource state tracking to callback
    
    All architectures which use the generic VDSO code have their own storage
    for the VDSO clock mode. That's pointless and just requires duplicate code.
    
    X86 abuses the function which retrieves the architecture specific clock
    mode storage to mark the clocksource as used in the VDSO. That's silly
    because this is invoked on every tick when the VDSO data is updated.
    
    Move this functionality to the clocksource::enable() callback so it gets
    invoked once when the clocksource is installed. This allows to make the
    clock mode storage generic.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>  (Hyper-V parts)
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com> (VDSO parts)
    Acked-by: Juergen Gross <jgross@suse.com> (Xen parts)
    Link: https://lkml.kernel.org/r/20200207124402.934519777@linutronix.de

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 904494b924c1..33f2cac25f13 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -159,12 +159,19 @@ bool kvm_check_and_clear_guest_paused(void)
 	return ret;
 }
 
+static int kvm_cs_enable(struct clocksource *cs)
+{
+	vclocks_set_used(VCLOCK_PVCLOCK);
+	return 0;
+}
+
 struct clocksource kvm_clock = {
 	.name	= "kvm-clock",
 	.read	= kvm_clock_get_cycles,
 	.rating	= 400,
 	.mask	= CLOCKSOURCE_MASK(64),
 	.flags	= CLOCK_SOURCE_IS_CONTINUOUS,
+	.enable	= kvm_cs_enable,
 };
 EXPORT_SYMBOL_GPL(kvm_clock);
 

commit 7539b174aef405d9d57db48c58390ba360c91312
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Jan 4 15:54:14 2019 -0200

    x86: kvmguest: use TSC clocksource if invariant TSC is exposed
    
    The invariant TSC bit has the following meaning:
    
    "The time stamp counter in newer processors may support an enhancement,
    referred to as invariant TSC. Processor's support for invariant TSC
    is indicated by CPUID.80000007H:EDX[8]. The invariant TSC will run
    at a constant rate in all ACPI P-, C-. and T-states. This is the
    architectural behavior moving forward. On processors with invariant TSC
    support, the OS may use the TSC for wall clock timer services (instead
    of ACPI or HPET timers). TSC reads are much more efficient and do not
    incur the overhead associated with a ring transition or access to a
    platform resource."
    
    IOW, TSC does not change frequency. In such case, and with
    TSC scaling hardware available to handle migration, it is possible
    to use the TSC clocksource directly, whose system calls are
    faster.
    
    Reduce the rating of kvmclock clocksource to allow TSC clocksource
    to be the default if invariant TSC is exposed.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    
    v2: Use feature bits and tsc_unstable() check (Sean Christopherson)
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index d908a37bf3f3..904494b924c1 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -351,6 +351,20 @@ void __init kvmclock_init(void)
 	machine_ops.crash_shutdown  = kvm_crash_shutdown;
 #endif
 	kvm_get_preset_lpj();
+
+	/*
+	 * X86_FEATURE_NONSTOP_TSC is TSC runs at constant rate
+	 * with P/T states and does not stop in deep C-states.
+	 *
+	 * Invariant TSC exposed by host means kvmclock is not necessary:
+	 * can use TSC as clocksource.
+	 *
+	 */
+	if (boot_cpu_has(X86_FEATURE_CONSTANT_TSC) &&
+	    boot_cpu_has(X86_FEATURE_NONSTOP_TSC) &&
+	    !check_tsc_unstable())
+		kvm_clock.rating = 299;
+
 	clocksource_register_hz(&kvm_clock, NSEC_PER_SEC);
 	pv_info.name = "KVM";
 }

commit b5179ec4187251a751832193693d6e474d3445ac
Author: Pavel Tatashin <pasha.tatashin@soleen.com>
Date:   Sat Jan 26 12:49:56 2019 -0500

    x86/kvmclock: set offset for kvm unstable clock
    
    VMs may show incorrect uptime and dmesg printk offsets on hypervisors with
    unstable clock. The problem is produced when VM is rebooted without exiting
    from qemu.
    
    The fix is to calculate clock offset not only for stable clock but for
    unstable clock as well, and use kvm_sched_clock_read() which substracts
    the offset for both clocks.
    
    This is safe, because pvclock_clocksource_read() does the right thing and
    makes sure that clock always goes forward, so once offset is calculated
    with unstable clock, we won't get new reads that are smaller than offset,
    and thus won't get negative results.
    
    Thank you Jon DeVree for helping to reproduce this issue.
    
    Fixes: 857baa87b642 ("sched/clock: Enable sched clock early")
    Cc: stable@vger.kernel.org
    Reported-by: Dominique Martinet <asmadeus@codewreck.org>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index e811d4d1c824..d908a37bf3f3 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -104,12 +104,8 @@ static u64 kvm_sched_clock_read(void)
 
 static inline void kvm_sched_clock_init(bool stable)
 {
-	if (!stable) {
-		pv_ops.time.sched_clock = kvm_clock_read;
+	if (!stable)
 		clear_sched_clock_stable();
-		return;
-	}
-
 	kvm_sched_clock_offset = kvm_clock_read();
 	pv_ops.time.sched_clock = kvm_sched_clock_read;
 

commit 649472a1694fb72a9e586bc40bfcf9282a4e8ccc
Author: Peng Hao <peng.hao2@zte.com.cn>
Date:   Fri Nov 2 17:05:17 2018 +0800

    x86/kvmclock: convert to SPDX identifiers
    
    Update the verbose license text with the matching SPDX
    license identifier.
    
    Signed-off-by: Peng Hao <peng.hao2@zte.com.cn>
    [Changed deprecated GPL-2.0+ to GPL-2.0-or-later. - Radim]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 30084ecaa20f..e811d4d1c824 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -1,19 +1,6 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*  KVM paravirtual clock driver. A clocksource implementation
     Copyright (C) 2008 Glauber de Oliveira Costa, Red Hat Inc.
-
-    This program is free software; you can redistribute it and/or modify
-    it under the terms of the GNU General Public License as published by
-    the Free Software Foundation; either version 2 of the License, or
-    (at your option) any later version.
-
-    This program is distributed in the hope that it will be useful,
-    but WITHOUT ANY WARRANTY; without even the implied warranty of
-    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-    GNU General Public License for more details.
-
-    You should have received a copy of the GNU General Public License
-    along with this program; if not, write to the Free Software
-    Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
 */
 
 #include <linux/clocksource.h>

commit f682a7920baf7b721d01dd317f3b532265357cbb
Merge: 99792e0cea1e 3a025de64bf8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 17:54:58 2018 +0100

    Merge branch 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 paravirt updates from Ingo Molnar:
     "Two main changes:
    
       - Remove no longer used parts of the paravirt infrastructure and put
         large quantities of paravirt ops under a new config option
         PARAVIRT_XXL=y, which is selected by XEN_PV only. (Joergen Gross)
    
       - Enable PV spinlocks on Hyperv (Yi Sun)"
    
    * 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/hyperv: Enable PV qspinlock for Hyper-V
      x86/hyperv: Add GUEST_IDLE_MSR support
      x86/paravirt: Clean up native_patch()
      x86/paravirt: Prevent redefinition of SAVE_FLAGS macro
      x86/xen: Make xen_reservation_lock static
      x86/paravirt: Remove unneeded mmu related paravirt ops bits
      x86/paravirt: Move the Xen-only pv_mmu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the pv_irq_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the Xen-only pv_cpu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move items in pv_info under PARAVIRT_XXL umbrella
      x86/paravirt: Introduce new config option PARAVIRT_XXL
      x86/paravirt: Remove unused paravirt bits
      x86/paravirt: Use a single ops structure
      x86/paravirt: Remove clobbers from struct paravirt_patch_site
      x86/paravirt: Remove clobbers parameter from paravirt patch functions
      x86/paravirt: Make paravirt_patch_call() and paravirt_patch_jmp() static
      x86/xen: Add SPDX identifier in arch/x86/xen files
      x86/xen: Link platform-pci-unplug.o only if CONFIG_XEN_PVHVM
      x86/xen: Move pv specific parts of arch/x86/xen/mmu.c to mmu_pv.c
      x86/xen: Move pv irq related functions under CONFIG_XEN_PV umbrella

commit 6a1cac56f41f9ea94e440dfcc1cac44b41a1b194
Author: Brijesh Singh <brijesh.singh@amd.com>
Date:   Fri Sep 14 08:45:59 2018 -0500

    x86/kvm: Use __bss_decrypted attribute in shared variables
    
    The recent removal of the memblock dependency from kvmclock caused a SEV
    guest regression because the wall_clock and hv_clock_boot variables are
    no longer mapped decrypted when SEV is active.
    
    Use the __bss_decrypted attribute to put the static wall_clock and
    hv_clock_boot in the .bss..decrypted section so that they are mapped
    decrypted during boot.
    
    In the preparatory stage of CPU hotplug, the per-cpu pvclock data pointer
    assigns either an element of the static array or dynamically allocated
    memory for the pvclock data pointer. The static array are now mapped
    decrypted but the dynamically allocated memory is not mapped decrypted.
    However, when SEV is active this memory range must be mapped decrypted.
    
    Add a function which is called after the page allocator is up, and
    allocate memory for the pvclock data pointers for the all possible cpus.
    Map this memory range as decrypted when SEV is active.
    
    Fixes: 368a540e0232 ("x86/kvmclock: Remove memblock dependency")
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: kvm@vger.kernel.org
    Link: https://lkml.kernel.org/r/1536932759-12905-3-git-send-email-brijesh.singh@amd.com

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 1e6764648af3..013fe3d21dbb 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -28,6 +28,7 @@
 #include <linux/sched/clock.h>
 #include <linux/mm.h>
 #include <linux/slab.h>
+#include <linux/set_memory.h>
 
 #include <asm/hypervisor.h>
 #include <asm/mem_encrypt.h>
@@ -61,9 +62,10 @@ early_param("no-kvmclock-vsyscall", parse_no_kvmclock_vsyscall);
 	(PAGE_SIZE / sizeof(struct pvclock_vsyscall_time_info))
 
 static struct pvclock_vsyscall_time_info
-			hv_clock_boot[HVC_BOOT_ARRAY_SIZE] __aligned(PAGE_SIZE);
-static struct pvclock_wall_clock wall_clock;
+			hv_clock_boot[HVC_BOOT_ARRAY_SIZE] __bss_decrypted __aligned(PAGE_SIZE);
+static struct pvclock_wall_clock wall_clock __bss_decrypted;
 static DEFINE_PER_CPU(struct pvclock_vsyscall_time_info *, hv_clock_per_cpu);
+static struct pvclock_vsyscall_time_info *hvclock_mem;
 
 static inline struct pvclock_vcpu_time_info *this_cpu_pvti(void)
 {
@@ -236,6 +238,45 @@ static void kvm_shutdown(void)
 	native_machine_shutdown();
 }
 
+static void __init kvmclock_init_mem(void)
+{
+	unsigned long ncpus;
+	unsigned int order;
+	struct page *p;
+	int r;
+
+	if (HVC_BOOT_ARRAY_SIZE >= num_possible_cpus())
+		return;
+
+	ncpus = num_possible_cpus() - HVC_BOOT_ARRAY_SIZE;
+	order = get_order(ncpus * sizeof(*hvclock_mem));
+
+	p = alloc_pages(GFP_KERNEL, order);
+	if (!p) {
+		pr_warn("%s: failed to alloc %d pages", __func__, (1U << order));
+		return;
+	}
+
+	hvclock_mem = page_address(p);
+
+	/*
+	 * hvclock is shared between the guest and the hypervisor, must
+	 * be mapped decrypted.
+	 */
+	if (sev_active()) {
+		r = set_memory_decrypted((unsigned long) hvclock_mem,
+					 1UL << order);
+		if (r) {
+			__free_pages(p, order);
+			hvclock_mem = NULL;
+			pr_warn("kvmclock: set_memory_decrypted() failed. Disabling\n");
+			return;
+		}
+	}
+
+	memset(hvclock_mem, 0, PAGE_SIZE << order);
+}
+
 static int __init kvm_setup_vsyscall_timeinfo(void)
 {
 #ifdef CONFIG_X86_64
@@ -250,6 +291,9 @@ static int __init kvm_setup_vsyscall_timeinfo(void)
 
 	kvm_clock.archdata.vclock_mode = VCLOCK_PVCLOCK;
 #endif
+
+	kvmclock_init_mem();
+
 	return 0;
 }
 early_initcall(kvm_setup_vsyscall_timeinfo);
@@ -269,8 +313,10 @@ static int kvmclock_setup_percpu(unsigned int cpu)
 	/* Use the static page for the first CPUs, allocate otherwise */
 	if (cpu < HVC_BOOT_ARRAY_SIZE)
 		p = &hv_clock_boot[cpu];
+	else if (hvclock_mem)
+		p = hvclock_mem + cpu - HVC_BOOT_ARRAY_SIZE;
 	else
-		p = kzalloc(sizeof(*p), GFP_KERNEL);
+		return -ENOMEM;
 
 	per_cpu(hv_clock_per_cpu, cpu) = p;
 	return p ? 0 : -ENOMEM;

commit 5c83511bdb9832c86be20fb86b783356e2f58062
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Aug 28 09:40:19 2018 +0200

    x86/paravirt: Use a single ops structure
    
    Instead of using six globally visible paravirt ops structures combine
    them in a single structure, keeping the original structures as
    sub-structures.
    
    This avoids the need to assemble struct paravirt_patch_template at
    runtime on the stack each time apply_paravirt() is being called (i.e.
    when loading a module).
    
    [ tglx: Made the struct and the initializer tabular for readability sake ]
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel@lists.xenproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: boris.ostrovsky@oracle.com
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/20180828074026.820-9-jgross@suse.com

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 1e6764648af3..a36b93a722a2 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -116,13 +116,13 @@ static u64 kvm_sched_clock_read(void)
 static inline void kvm_sched_clock_init(bool stable)
 {
 	if (!stable) {
-		pv_time_ops.sched_clock = kvm_clock_read;
+		pv_ops.time.sched_clock = kvm_clock_read;
 		clear_sched_clock_stable();
 		return;
 	}
 
 	kvm_sched_clock_offset = kvm_clock_read();
-	pv_time_ops.sched_clock = kvm_sched_clock_read;
+	pv_ops.time.sched_clock = kvm_sched_clock_read;
 
 	pr_info("kvm-clock: using sched offset of %llu cycles",
 		kvm_sched_clock_offset);

commit 958f338e96f874a0d29442396d6adf9c1e17aa2d
Merge: 781fca5b1046 07d981ad4cf1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 09:46:06 2018 -0700

    Merge branch 'l1tf-final' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Merge L1 Terminal Fault fixes from Thomas Gleixner:
     "L1TF, aka L1 Terminal Fault, is yet another speculative hardware
      engineering trainwreck. It's a hardware vulnerability which allows
      unprivileged speculative access to data which is available in the
      Level 1 Data Cache when the page table entry controlling the virtual
      address, which is used for the access, has the Present bit cleared or
      other reserved bits set.
    
      If an instruction accesses a virtual address for which the relevant
      page table entry (PTE) has the Present bit cleared or other reserved
      bits set, then speculative execution ignores the invalid PTE and loads
      the referenced data if it is present in the Level 1 Data Cache, as if
      the page referenced by the address bits in the PTE was still present
      and accessible.
    
      While this is a purely speculative mechanism and the instruction will
      raise a page fault when it is retired eventually, the pure act of
      loading the data and making it available to other speculative
      instructions opens up the opportunity for side channel attacks to
      unprivileged malicious code, similar to the Meltdown attack.
    
      While Meltdown breaks the user space to kernel space protection, L1TF
      allows to attack any physical memory address in the system and the
      attack works across all protection domains. It allows an attack of SGX
      and also works from inside virtual machines because the speculation
      bypasses the extended page table (EPT) protection mechanism.
    
      The assoicated CVEs are: CVE-2018-3615, CVE-2018-3620, CVE-2018-3646
    
      The mitigations provided by this pull request include:
    
       - Host side protection by inverting the upper address bits of a non
         present page table entry so the entry points to uncacheable memory.
    
       - Hypervisor protection by flushing L1 Data Cache on VMENTER.
    
       - SMT (HyperThreading) control knobs, which allow to 'turn off' SMT
         by offlining the sibling CPU threads. The knobs are available on
         the kernel command line and at runtime via sysfs
    
       - Control knobs for the hypervisor mitigation, related to L1D flush
         and SMT control. The knobs are available on the kernel command line
         and at runtime via sysfs
    
       - Extensive documentation about L1TF including various degrees of
         mitigations.
    
      Thanks to all people who have contributed to this in various ways -
      patches, review, testing, backporting - and the fruitful, sometimes
      heated, but at the end constructive discussions.
    
      There is work in progress to provide other forms of mitigations, which
      might be less horrible performance wise for a particular kind of
      workloads, but this is not yet ready for consumption due to their
      complexity and limitations"
    
    * 'l1tf-final' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (75 commits)
      x86/microcode: Allow late microcode loading with SMT disabled
      tools headers: Synchronise x86 cpufeatures.h for L1TF additions
      x86/mm/kmmio: Make the tracer robust against L1TF
      x86/mm/pat: Make set_memory_np() L1TF safe
      x86/speculation/l1tf: Make pmd/pud_mknotpresent() invert
      x86/speculation/l1tf: Invert all not present mappings
      cpu/hotplug: Fix SMT supported evaluation
      KVM: VMX: Tell the nested hypervisor to skip L1D flush on vmentry
      x86/speculation: Use ARCH_CAPABILITIES to skip L1D flush on vmentry
      x86/speculation: Simplify sysfs report of VMX L1TF vulnerability
      Documentation/l1tf: Remove Yonah processors from not vulnerable list
      x86/KVM/VMX: Don't set l1tf_flush_l1d from vmx_handle_external_intr()
      x86/irq: Let interrupt handlers set kvm_cpu_l1tf_flush_l1d
      x86: Don't include linux/irq.h from asm/hardirq.h
      x86/KVM/VMX: Introduce per-host-cpu analogue of l1tf_flush_l1d
      x86/irq: Demote irq_cpustat_t::__softirq_pending to u16
      x86/KVM/VMX: Move the l1tf_flush_l1d test to vmx_l1d_flush()
      x86/KVM/VMX: Replace 'vmx_l1d_flush_always' with 'vmx_l1d_flush_cond'
      x86/KVM/VMX: Don't set l1tf_flush_l1d to true from vmx_l1d_flush()
      cpu/hotplug: detect SMT disabled by BIOS
      ...

commit 1088c6eef261939bda8346ec35b513790a2111d5
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Mon Jul 30 15:54:21 2018 +0800

    x86/kvmclock: Mark kvm_get_preset_lpj() as __init
    
    kvm_get_preset_lpj() is only called from kvmclock_init(), so mark it __init
    as well.
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc:   Radim Krčmář<rkrcmar@redhat.com>
    Cc:  <kvm@vger.kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: kvm@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180730075421.22830-3-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 91b94c0ae4e3..d2edd7e6c294 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -145,7 +145,7 @@ static unsigned long kvm_get_tsc_khz(void)
 	return pvclock_tsc_khz(this_cpu_pvti());
 }
 
-static void kvm_get_preset_lpj(void)
+static void __init kvm_get_preset_lpj(void)
 {
 	unsigned long khz;
 	u64 lpj;

commit 95a3d4454bb1cf5bfd666c27fdd2dc188e17c14d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 19 16:55:26 2018 -0400

    x86/kvmclock: Switch kvmclock data to a PER_CPU variable
    
    The previous removal of the memblock dependency from kvmclock introduced a
    static data array sized 64bytes * CONFIG_NR_CPUS. That's wasteful on large
    systems when kvmclock is not used.
    
    Replace it with:
    
     - A static page sized array of pvclock data. It's page sized because the
       pvclock data of the boot cpu is mapped into the VDSO so otherwise random
       other data would be exposed to the vDSO
    
     - A PER_CPU variable of pvclock data pointers. This is used to access the
       pcvlock data storage on each CPU.
    
    The setup is done in two stages:
    
     - Early boot stores the pointer to the static page for the boot CPU in
       the per cpu data.
    
     - In the preparatory stage of CPU hotplug assign either an element of
       the static array (when the CPU number is in that range) or allocate
       memory and initialize the per cpu pointer.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-8-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 7d690d2238f8..91b94c0ae4e3 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -23,6 +23,7 @@
 #include <asm/apic.h>
 #include <linux/percpu.h>
 #include <linux/hardirq.h>
+#include <linux/cpuhotplug.h>
 #include <linux/sched.h>
 #include <linux/sched/clock.h>
 #include <linux/mm.h>
@@ -55,12 +56,23 @@ early_param("no-kvmclock-vsyscall", parse_no_kvmclock_vsyscall);
 
 /* Aligned to page sizes to match whats mapped via vsyscalls to userspace */
 #define HV_CLOCK_SIZE	(sizeof(struct pvclock_vsyscall_time_info) * NR_CPUS)
+#define HVC_BOOT_ARRAY_SIZE \
+	(PAGE_SIZE / sizeof(struct pvclock_vsyscall_time_info))
 
-static u8 hv_clock_mem[PAGE_ALIGN(HV_CLOCK_SIZE)] __aligned(PAGE_SIZE);
-
-/* The hypervisor will put information about time periodically here */
-static struct pvclock_vsyscall_time_info *hv_clock __ro_after_init;
+static struct pvclock_vsyscall_time_info
+			hv_clock_boot[HVC_BOOT_ARRAY_SIZE] __aligned(PAGE_SIZE);
 static struct pvclock_wall_clock wall_clock;
+static DEFINE_PER_CPU(struct pvclock_vsyscall_time_info *, hv_clock_per_cpu);
+
+static inline struct pvclock_vcpu_time_info *this_cpu_pvti(void)
+{
+	return &this_cpu_read(hv_clock_per_cpu)->pvti;
+}
+
+static inline struct pvclock_vsyscall_time_info *this_cpu_hvclock(void)
+{
+	return this_cpu_read(hv_clock_per_cpu);
+}
 
 /*
  * The wallclock is the time of day when we booted. Since then, some time may
@@ -69,17 +81,10 @@ static struct pvclock_wall_clock wall_clock;
  */
 static void kvm_get_wallclock(struct timespec64 *now)
 {
-	struct pvclock_vcpu_time_info *vcpu_time;
-	int cpu;
-
 	wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
-
-	cpu = get_cpu();
-
-	vcpu_time = &hv_clock[cpu].pvti;
-	pvclock_read_wallclock(&wall_clock, vcpu_time, now);
-
-	put_cpu();
+	preempt_disable();
+	pvclock_read_wallclock(&wall_clock, this_cpu_pvti(), now);
+	preempt_enable();
 }
 
 static int kvm_set_wallclock(const struct timespec64 *now)
@@ -89,14 +94,10 @@ static int kvm_set_wallclock(const struct timespec64 *now)
 
 static u64 kvm_clock_read(void)
 {
-	struct pvclock_vcpu_time_info *src;
 	u64 ret;
-	int cpu;
 
 	preempt_disable_notrace();
-	cpu = smp_processor_id();
-	src = &hv_clock[cpu].pvti;
-	ret = pvclock_clocksource_read(src);
+	ret = pvclock_clocksource_read(this_cpu_pvti());
 	preempt_enable_notrace();
 	return ret;
 }
@@ -141,7 +142,7 @@ static inline void kvm_sched_clock_init(bool stable)
 static unsigned long kvm_get_tsc_khz(void)
 {
 	setup_force_cpu_cap(X86_FEATURE_TSC_KNOWN_FREQ);
-	return pvclock_tsc_khz(&hv_clock[0].pvti);
+	return pvclock_tsc_khz(this_cpu_pvti());
 }
 
 static void kvm_get_preset_lpj(void)
@@ -158,15 +159,14 @@ static void kvm_get_preset_lpj(void)
 
 bool kvm_check_and_clear_guest_paused(void)
 {
-	struct pvclock_vcpu_time_info *src;
+	struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
 	bool ret = false;
 
-	if (!hv_clock)
+	if (!src)
 		return ret;
 
-	src = &hv_clock[smp_processor_id()].pvti;
-	if ((src->flags & PVCLOCK_GUEST_STOPPED) != 0) {
-		src->flags &= ~PVCLOCK_GUEST_STOPPED;
+	if ((src->pvti.flags & PVCLOCK_GUEST_STOPPED) != 0) {
+		src->pvti.flags &= ~PVCLOCK_GUEST_STOPPED;
 		pvclock_touch_watchdogs();
 		ret = true;
 	}
@@ -184,17 +184,15 @@ EXPORT_SYMBOL_GPL(kvm_clock);
 
 static void kvm_register_clock(char *txt)
 {
-	struct pvclock_vcpu_time_info *src;
-	int cpu = smp_processor_id();
+	struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
 	u64 pa;
 
-	if (!hv_clock)
+	if (!src)
 		return;
 
-	src = &hv_clock[cpu].pvti;
-	pa = slow_virt_to_phys(src) | 0x01ULL;
+	pa = slow_virt_to_phys(&src->pvti) | 0x01ULL;
 	wrmsrl(msr_kvm_system_time, pa);
-	pr_info("kvm-clock: cpu %d, msr %llx, %s", cpu, pa, txt);
+	pr_info("kvm-clock: cpu %d, msr %llx, %s", smp_processor_id(), pa, txt);
 }
 
 static void kvm_save_sched_clock_state(void)
@@ -242,12 +240,12 @@ static int __init kvm_setup_vsyscall_timeinfo(void)
 #ifdef CONFIG_X86_64
 	u8 flags;
 
-	if (!hv_clock || !kvmclock_vsyscall)
+	if (!per_cpu(hv_clock_per_cpu, 0) || !kvmclock_vsyscall)
 		return 0;
 
-	flags = pvclock_read_flags(&hv_clock[0].pvti);
+	flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
 	if (!(flags & PVCLOCK_TSC_STABLE_BIT))
-		return 1;
+		return 0;
 
 	kvm_clock.archdata.vclock_mode = VCLOCK_PVCLOCK;
 #endif
@@ -255,6 +253,28 @@ static int __init kvm_setup_vsyscall_timeinfo(void)
 }
 early_initcall(kvm_setup_vsyscall_timeinfo);
 
+static int kvmclock_setup_percpu(unsigned int cpu)
+{
+	struct pvclock_vsyscall_time_info *p = per_cpu(hv_clock_per_cpu, cpu);
+
+	/*
+	 * The per cpu area setup replicates CPU0 data to all cpu
+	 * pointers. So carefully check. CPU0 has been set up in init
+	 * already.
+	 */
+	if (!cpu || (p && p != per_cpu(hv_clock_per_cpu, 0)))
+		return 0;
+
+	/* Use the static page for the first CPUs, allocate otherwise */
+	if (cpu < HVC_BOOT_ARRAY_SIZE)
+		p = &hv_clock_boot[cpu];
+	else
+		p = kzalloc(sizeof(*p), GFP_KERNEL);
+
+	per_cpu(hv_clock_per_cpu, cpu) = p;
+	return p ? 0 : -ENOMEM;
+}
+
 void __init kvmclock_init(void)
 {
 	u8 flags;
@@ -269,17 +289,22 @@ void __init kvmclock_init(void)
 		return;
 	}
 
+	if (cpuhp_setup_state(CPUHP_BP_PREPARE_DYN, "kvmclock:setup_percpu",
+			      kvmclock_setup_percpu, NULL) < 0) {
+		return;
+	}
+
 	pr_info("kvm-clock: Using msrs %x and %x",
 		msr_kvm_system_time, msr_kvm_wall_clock);
 
-	hv_clock = (struct pvclock_vsyscall_time_info *)hv_clock_mem;
+	this_cpu_write(hv_clock_per_cpu, &hv_clock_boot[0]);
 	kvm_register_clock("primary cpu clock");
-	pvclock_set_pvti_cpu0_va(hv_clock);
+	pvclock_set_pvti_cpu0_va(hv_clock_boot);
 
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
 		pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
 
-	flags = pvclock_read_flags(&hv_clock[0].pvti);
+	flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
 	kvm_sched_clock_init(flags & PVCLOCK_TSC_STABLE_BIT);
 
 	x86_platform.calibrate_tsc = kvm_get_tsc_khz;

commit e499a9b6dc488aff7f284bee51936f510ab7ad15
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 19 16:55:25 2018 -0400

    x86/kvmclock: Move kvmclock vsyscall param and init to kvmclock
    
    There is no point to have this in the kvm code itself and call it from
    there. This can be called from an initcall and the parameter is cleared
    when the hypervisor is not KVM.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-7-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 78aec160f5e0..7d690d2238f8 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -27,12 +27,14 @@
 #include <linux/sched/clock.h>
 #include <linux/mm.h>
 
+#include <asm/hypervisor.h>
 #include <asm/mem_encrypt.h>
 #include <asm/x86_init.h>
 #include <asm/reboot.h>
 #include <asm/kvmclock.h>
 
 static int kvmclock __initdata = 1;
+static int kvmclock_vsyscall __initdata = 1;
 static int msr_kvm_system_time __ro_after_init = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock __ro_after_init = MSR_KVM_WALL_CLOCK;
 static u64 kvm_sched_clock_offset __ro_after_init;
@@ -44,6 +46,13 @@ static int __init parse_no_kvmclock(char *arg)
 }
 early_param("no-kvmclock", parse_no_kvmclock);
 
+static int __init parse_no_kvmclock_vsyscall(char *arg)
+{
+	kvmclock_vsyscall = 0;
+	return 0;
+}
+early_param("no-kvmclock-vsyscall", parse_no_kvmclock_vsyscall);
+
 /* Aligned to page sizes to match whats mapped via vsyscalls to userspace */
 #define HV_CLOCK_SIZE	(sizeof(struct pvclock_vsyscall_time_info) * NR_CPUS)
 
@@ -228,6 +237,24 @@ static void kvm_shutdown(void)
 	native_machine_shutdown();
 }
 
+static int __init kvm_setup_vsyscall_timeinfo(void)
+{
+#ifdef CONFIG_X86_64
+	u8 flags;
+
+	if (!hv_clock || !kvmclock_vsyscall)
+		return 0;
+
+	flags = pvclock_read_flags(&hv_clock[0].pvti);
+	if (!(flags & PVCLOCK_TSC_STABLE_BIT))
+		return 1;
+
+	kvm_clock.archdata.vclock_mode = VCLOCK_PVCLOCK;
+#endif
+	return 0;
+}
+early_initcall(kvm_setup_vsyscall_timeinfo);
+
 void __init kvmclock_init(void)
 {
 	u8 flags;
@@ -272,20 +299,3 @@ void __init kvmclock_init(void)
 	clocksource_register_hz(&kvm_clock, NSEC_PER_SEC);
 	pv_info.name = "KVM";
 }
-
-int __init kvm_setup_vsyscall_timeinfo(void)
-{
-#ifdef CONFIG_X86_64
-	u8 flags;
-
-	if (!hv_clock)
-		return 0;
-
-	flags = pvclock_read_flags(&hv_clock[0].pvti);
-	if (!(flags & PVCLOCK_TSC_STABLE_BIT))
-		return 1;
-
-	kvm_clock.archdata.vclock_mode = VCLOCK_PVCLOCK;
-#endif
-	return 0;
-}

commit 42f8df935efefba51d0c5321b1325436523e3377
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 19 16:55:24 2018 -0400

    x86/kvmclock: Mark variables __initdata and __ro_after_init
    
    The kvmclock parameter is init data and the other variables are not
    modified after init.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-6-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 4afb03e49a4f..78aec160f5e0 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -32,10 +32,10 @@
 #include <asm/reboot.h>
 #include <asm/kvmclock.h>
 
-static int kvmclock __ro_after_init = 1;
-static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
-static int msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
-static u64 kvm_sched_clock_offset;
+static int kvmclock __initdata = 1;
+static int msr_kvm_system_time __ro_after_init = MSR_KVM_SYSTEM_TIME;
+static int msr_kvm_wall_clock __ro_after_init = MSR_KVM_WALL_CLOCK;
+static u64 kvm_sched_clock_offset __ro_after_init;
 
 static int __init parse_no_kvmclock(char *arg)
 {
@@ -50,7 +50,7 @@ early_param("no-kvmclock", parse_no_kvmclock);
 static u8 hv_clock_mem[PAGE_ALIGN(HV_CLOCK_SIZE)] __aligned(PAGE_SIZE);
 
 /* The hypervisor will put information about time periodically here */
-static struct pvclock_vsyscall_time_info *hv_clock;
+static struct pvclock_vsyscall_time_info *hv_clock __ro_after_init;
 static struct pvclock_wall_clock wall_clock;
 
 /*

commit 146c394d0c3c8e88df433a179c2b0b85fd8cf247
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 19 16:55:23 2018 -0400

    x86/kvmclock: Cleanup the code
    
    - Cleanup the mrs write for wall clock. The type casts to (int) are sloppy
      because the wrmsr parameters are u32 and aside of that wrmsrl() already
      provides the high/low split for free.
    
    - Remove the pointless get_cpu()/put_cpu() dance from various
      functions. Either they are called during early init where CPU is
      guaranteed to be 0 or they are already called from non preemptible
      context where smp_processor_id() can be used safely
    
    - Simplify the convoluted check for kvmclock in the init function.
    
    - Mark the parameter parsing function __init. No point in keeping it
      around.
    
    - Convert to pr_info()
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-5-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index f0a0aef5e9fa..4afb03e49a4f 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -37,7 +37,7 @@ static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
 static u64 kvm_sched_clock_offset;
 
-static int parse_no_kvmclock(char *arg)
+static int __init parse_no_kvmclock(char *arg)
 {
 	kvmclock = 0;
 	return 0;
@@ -61,13 +61,9 @@ static struct pvclock_wall_clock wall_clock;
 static void kvm_get_wallclock(struct timespec64 *now)
 {
 	struct pvclock_vcpu_time_info *vcpu_time;
-	int low, high;
 	int cpu;
 
-	low = (int)slow_virt_to_phys(&wall_clock);
-	high = ((u64)slow_virt_to_phys(&wall_clock) >> 32);
-
-	native_write_msr(msr_kvm_wall_clock, low, high);
+	wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
 
 	cpu = get_cpu();
 
@@ -117,11 +113,11 @@ static inline void kvm_sched_clock_init(bool stable)
 	kvm_sched_clock_offset = kvm_clock_read();
 	pv_time_ops.sched_clock = kvm_sched_clock_read;
 
-	printk(KERN_INFO "kvm-clock: using sched offset of %llu cycles\n",
-			kvm_sched_clock_offset);
+	pr_info("kvm-clock: using sched offset of %llu cycles",
+		kvm_sched_clock_offset);
 
 	BUILD_BUG_ON(sizeof(kvm_sched_clock_offset) >
-	         sizeof(((struct pvclock_vcpu_time_info *)NULL)->system_time));
+		sizeof(((struct pvclock_vcpu_time_info *)NULL)->system_time));
 }
 
 /*
@@ -135,16 +131,8 @@ static inline void kvm_sched_clock_init(bool stable)
  */
 static unsigned long kvm_get_tsc_khz(void)
 {
-	struct pvclock_vcpu_time_info *src;
-	int cpu;
-	unsigned long tsc_khz;
-
-	cpu = get_cpu();
-	src = &hv_clock[cpu].pvti;
-	tsc_khz = pvclock_tsc_khz(src);
-	put_cpu();
 	setup_force_cpu_cap(X86_FEATURE_TSC_KNOWN_FREQ);
-	return tsc_khz;
+	return pvclock_tsc_khz(&hv_clock[0].pvti);
 }
 
 static void kvm_get_preset_lpj(void)
@@ -161,29 +149,27 @@ static void kvm_get_preset_lpj(void)
 
 bool kvm_check_and_clear_guest_paused(void)
 {
-	bool ret = false;
 	struct pvclock_vcpu_time_info *src;
-	int cpu = smp_processor_id();
+	bool ret = false;
 
 	if (!hv_clock)
 		return ret;
 
-	src = &hv_clock[cpu].pvti;
+	src = &hv_clock[smp_processor_id()].pvti;
 	if ((src->flags & PVCLOCK_GUEST_STOPPED) != 0) {
 		src->flags &= ~PVCLOCK_GUEST_STOPPED;
 		pvclock_touch_watchdogs();
 		ret = true;
 	}
-
 	return ret;
 }
 
 struct clocksource kvm_clock = {
-	.name = "kvm-clock",
-	.read = kvm_clock_get_cycles,
-	.rating = 400,
-	.mask = CLOCKSOURCE_MASK(64),
-	.flags = CLOCK_SOURCE_IS_CONTINUOUS,
+	.name	= "kvm-clock",
+	.read	= kvm_clock_get_cycles,
+	.rating	= 400,
+	.mask	= CLOCKSOURCE_MASK(64),
+	.flags	= CLOCK_SOURCE_IS_CONTINUOUS,
 };
 EXPORT_SYMBOL_GPL(kvm_clock);
 
@@ -199,7 +185,7 @@ static void kvm_register_clock(char *txt)
 	src = &hv_clock[cpu].pvti;
 	pa = slow_virt_to_phys(src) | 0x01ULL;
 	wrmsrl(msr_kvm_system_time, pa);
-	pr_info("kvm-clock: cpu %d, msr %llx, %s\n", cpu, pa, txt);
+	pr_info("kvm-clock: cpu %d, msr %llx, %s", cpu, pa, txt);
 }
 
 static void kvm_save_sched_clock_state(void)
@@ -244,20 +230,19 @@ static void kvm_shutdown(void)
 
 void __init kvmclock_init(void)
 {
-	struct pvclock_vcpu_time_info *vcpu_time;
-	int cpu;
 	u8 flags;
 
-	if (!kvm_para_available())
+	if (!kvm_para_available() || !kvmclock)
 		return;
 
-	if (kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE2)) {
+	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE2)) {
 		msr_kvm_system_time = MSR_KVM_SYSTEM_TIME_NEW;
 		msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
-	} else if (!(kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)))
+	} else if (!kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)) {
 		return;
+	}
 
-	printk(KERN_INFO "kvm-clock: Using msrs %x and %x",
+	pr_info("kvm-clock: Using msrs %x and %x",
 		msr_kvm_system_time, msr_kvm_wall_clock);
 
 	hv_clock = (struct pvclock_vsyscall_time_info *)hv_clock_mem;
@@ -267,20 +252,15 @@ void __init kvmclock_init(void)
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
 		pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
 
-	cpu = get_cpu();
-	vcpu_time = &hv_clock[cpu].pvti;
-	flags = pvclock_read_flags(vcpu_time);
-
+	flags = pvclock_read_flags(&hv_clock[0].pvti);
 	kvm_sched_clock_init(flags & PVCLOCK_TSC_STABLE_BIT);
-	put_cpu();
 
 	x86_platform.calibrate_tsc = kvm_get_tsc_khz;
 	x86_platform.calibrate_cpu = kvm_get_tsc_khz;
 	x86_platform.get_wallclock = kvm_get_wallclock;
 	x86_platform.set_wallclock = kvm_set_wallclock;
 #ifdef CONFIG_X86_LOCAL_APIC
-	x86_cpuinit.early_percpu_clock_init =
-		kvm_setup_secondary_clock;
+	x86_cpuinit.early_percpu_clock_init = kvm_setup_secondary_clock;
 #endif
 	x86_platform.save_sched_clock_state = kvm_save_sched_clock_state;
 	x86_platform.restore_sched_clock_state = kvm_restore_sched_clock_state;
@@ -296,20 +276,12 @@ void __init kvmclock_init(void)
 int __init kvm_setup_vsyscall_timeinfo(void)
 {
 #ifdef CONFIG_X86_64
-	int cpu;
 	u8 flags;
-	struct pvclock_vcpu_time_info *vcpu_time;
 
 	if (!hv_clock)
 		return 0;
 
-	cpu = get_cpu();
-
-	vcpu_time = &hv_clock[cpu].pvti;
-	flags = pvclock_read_flags(vcpu_time);
-
-	put_cpu();
-
+	flags = pvclock_read_flags(&hv_clock[0].pvti);
 	if (!(flags & PVCLOCK_TSC_STABLE_BIT))
 		return 1;
 

commit 7a5ddc8fe0ea9518cd7fb6a929cac7d864c6f300
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 19 16:55:22 2018 -0400

    x86/kvmclock: Decrapify kvm_register_clock()
    
    The return value is pointless because the wrmsr cannot fail if
    KVM_FEATURE_CLOCKSOURCE or KVM_FEATURE_CLOCKSOURCE2 are set.
    
    kvm_register_clock() is only called locally so wants to be static.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-4-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index a995d7d7164c..f0a0aef5e9fa 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -187,23 +187,19 @@ struct clocksource kvm_clock = {
 };
 EXPORT_SYMBOL_GPL(kvm_clock);
 
-int kvm_register_clock(char *txt)
+static void kvm_register_clock(char *txt)
 {
-	int cpu = smp_processor_id();
-	int low, high, ret;
 	struct pvclock_vcpu_time_info *src;
+	int cpu = smp_processor_id();
+	u64 pa;
 
 	if (!hv_clock)
-		return 0;
+		return;
 
 	src = &hv_clock[cpu].pvti;
-	low = (int)slow_virt_to_phys(src) | 1;
-	high = ((u64)slow_virt_to_phys(src) >> 32);
-	ret = native_write_msr_safe(msr_kvm_system_time, low, high);
-	printk(KERN_INFO "kvm-clock: cpu %d, msr %x:%x, %s\n",
-	       cpu, high, low, txt);
-
-	return ret;
+	pa = slow_virt_to_phys(src) | 0x01ULL;
+	wrmsrl(msr_kvm_system_time, pa);
+	pr_info("kvm-clock: cpu %d, msr %llx, %s\n", cpu, pa, txt);
 }
 
 static void kvm_save_sched_clock_state(void)
@@ -218,11 +214,7 @@ static void kvm_restore_sched_clock_state(void)
 #ifdef CONFIG_X86_LOCAL_APIC
 static void kvm_setup_secondary_clock(void)
 {
-	/*
-	 * Now that the first cpu already had this clocksource initialized,
-	 * we shouldn't fail.
-	 */
-	WARN_ON(kvm_register_clock("secondary cpu clock"));
+	kvm_register_clock("secondary cpu clock");
 }
 #endif
 
@@ -265,16 +257,11 @@ void __init kvmclock_init(void)
 	} else if (!(kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)))
 		return;
 
-	hv_clock = (struct pvclock_vsyscall_time_info *)hv_clock_mem;
-
-	if (kvm_register_clock("primary cpu clock")) {
-		hv_clock = NULL;
-		return;
-	}
-
 	printk(KERN_INFO "kvm-clock: Using msrs %x and %x",
 		msr_kvm_system_time, msr_kvm_wall_clock);
 
+	hv_clock = (struct pvclock_vsyscall_time_info *)hv_clock_mem;
+	kvm_register_clock("primary cpu clock");
 	pvclock_set_pvti_cpu0_va(hv_clock);
 
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))

commit 7ef363a39514ed8a6f2333fbae1875ac0953715a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 19 16:55:21 2018 -0400

    x86/kvmclock: Remove page size requirement from wall_clock
    
    There is no requirement for wall_clock data to be page aligned or page
    sized.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-3-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 1f6ac5aaa904..a995d7d7164c 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -46,14 +46,12 @@ early_param("no-kvmclock", parse_no_kvmclock);
 
 /* Aligned to page sizes to match whats mapped via vsyscalls to userspace */
 #define HV_CLOCK_SIZE	(sizeof(struct pvclock_vsyscall_time_info) * NR_CPUS)
-#define WALL_CLOCK_SIZE	(sizeof(struct pvclock_wall_clock))
 
 static u8 hv_clock_mem[PAGE_ALIGN(HV_CLOCK_SIZE)] __aligned(PAGE_SIZE);
-static u8 wall_clock_mem[PAGE_ALIGN(WALL_CLOCK_SIZE)] __aligned(PAGE_SIZE);
 
 /* The hypervisor will put information about time periodically here */
 static struct pvclock_vsyscall_time_info *hv_clock;
-static struct pvclock_wall_clock *wall_clock;
+static struct pvclock_wall_clock wall_clock;
 
 /*
  * The wallclock is the time of day when we booted. Since then, some time may
@@ -66,15 +64,15 @@ static void kvm_get_wallclock(struct timespec64 *now)
 	int low, high;
 	int cpu;
 
-	low = (int)slow_virt_to_phys(wall_clock);
-	high = ((u64)slow_virt_to_phys(wall_clock) >> 32);
+	low = (int)slow_virt_to_phys(&wall_clock);
+	high = ((u64)slow_virt_to_phys(&wall_clock) >> 32);
 
 	native_write_msr(msr_kvm_wall_clock, low, high);
 
 	cpu = get_cpu();
 
 	vcpu_time = &hv_clock[cpu].pvti;
-	pvclock_read_wallclock(wall_clock, vcpu_time, now);
+	pvclock_read_wallclock(&wall_clock, vcpu_time, now);
 
 	put_cpu();
 }
@@ -267,12 +265,10 @@ void __init kvmclock_init(void)
 	} else if (!(kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)))
 		return;
 
-	wall_clock = (struct pvclock_wall_clock *)wall_clock_mem;
 	hv_clock = (struct pvclock_vsyscall_time_info *)hv_clock_mem;
 
 	if (kvm_register_clock("primary cpu clock")) {
 		hv_clock = NULL;
-		wall_clock = NULL;
 		return;
 	}
 

commit 368a540e0232ad446931f5a4e8a5e06f69f21343
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:20 2018 -0400

    x86/kvmclock: Remove memblock dependency
    
    KVM clock is initialized later compared to other hypervisor clocks because
    it has a dependency on the memblock allocator.
    
    Bring it in line with other hypervisors by using memory from the BSS
    instead of allocating it.
    
    The benefits:
    
      - Remove ifdef from common code
      - Earlier availability of the clock
      - Remove dependency on memblock, and reduce code
    
    The downside:
    
      - Static allocation of the per cpu data structures sized NR_CPUS * 64byte
        Will be addressed in follow up patches.
    
    [ tglx: Split out from larger series ]
    
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-2-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 3b8e7c13c614..1f6ac5aaa904 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -23,9 +23,9 @@
 #include <asm/apic.h>
 #include <linux/percpu.h>
 #include <linux/hardirq.h>
-#include <linux/memblock.h>
 #include <linux/sched.h>
 #include <linux/sched/clock.h>
+#include <linux/mm.h>
 
 #include <asm/mem_encrypt.h>
 #include <asm/x86_init.h>
@@ -44,6 +44,13 @@ static int parse_no_kvmclock(char *arg)
 }
 early_param("no-kvmclock", parse_no_kvmclock);
 
+/* Aligned to page sizes to match whats mapped via vsyscalls to userspace */
+#define HV_CLOCK_SIZE	(sizeof(struct pvclock_vsyscall_time_info) * NR_CPUS)
+#define WALL_CLOCK_SIZE	(sizeof(struct pvclock_wall_clock))
+
+static u8 hv_clock_mem[PAGE_ALIGN(HV_CLOCK_SIZE)] __aligned(PAGE_SIZE);
+static u8 wall_clock_mem[PAGE_ALIGN(WALL_CLOCK_SIZE)] __aligned(PAGE_SIZE);
+
 /* The hypervisor will put information about time periodically here */
 static struct pvclock_vsyscall_time_info *hv_clock;
 static struct pvclock_wall_clock *wall_clock;
@@ -245,43 +252,12 @@ static void kvm_shutdown(void)
 	native_machine_shutdown();
 }
 
-static phys_addr_t __init kvm_memblock_alloc(phys_addr_t size,
-					     phys_addr_t align)
-{
-	phys_addr_t mem;
-
-	mem = memblock_alloc(size, align);
-	if (!mem)
-		return 0;
-
-	if (sev_active()) {
-		if (early_set_memory_decrypted((unsigned long)__va(mem), size))
-			goto e_free;
-	}
-
-	return mem;
-e_free:
-	memblock_free(mem, size);
-	return 0;
-}
-
-static void __init kvm_memblock_free(phys_addr_t addr, phys_addr_t size)
-{
-	if (sev_active())
-		early_set_memory_encrypted((unsigned long)__va(addr), size);
-
-	memblock_free(addr, size);
-}
-
 void __init kvmclock_init(void)
 {
 	struct pvclock_vcpu_time_info *vcpu_time;
-	unsigned long mem, mem_wall_clock;
-	int size, cpu, wall_clock_size;
+	int cpu;
 	u8 flags;
 
-	size = PAGE_ALIGN(sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS);
-
 	if (!kvm_para_available())
 		return;
 
@@ -291,28 +267,11 @@ void __init kvmclock_init(void)
 	} else if (!(kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)))
 		return;
 
-	wall_clock_size = PAGE_ALIGN(sizeof(struct pvclock_wall_clock));
-	mem_wall_clock = kvm_memblock_alloc(wall_clock_size, PAGE_SIZE);
-	if (!mem_wall_clock)
-		return;
-
-	wall_clock = __va(mem_wall_clock);
-	memset(wall_clock, 0, wall_clock_size);
-
-	mem = kvm_memblock_alloc(size, PAGE_SIZE);
-	if (!mem) {
-		kvm_memblock_free(mem_wall_clock, wall_clock_size);
-		wall_clock = NULL;
-		return;
-	}
-
-	hv_clock = __va(mem);
-	memset(hv_clock, 0, size);
+	wall_clock = (struct pvclock_wall_clock *)wall_clock_mem;
+	hv_clock = (struct pvclock_vsyscall_time_info *)hv_clock_mem;
 
 	if (kvm_register_clock("primary cpu clock")) {
 		hv_clock = NULL;
-		kvm_memblock_free(mem, size);
-		kvm_memblock_free(mem_wall_clock, wall_clock_size);
 		wall_clock = NULL;
 		return;
 	}
@@ -357,13 +316,10 @@ int __init kvm_setup_vsyscall_timeinfo(void)
 	int cpu;
 	u8 flags;
 	struct pvclock_vcpu_time_info *vcpu_time;
-	unsigned int size;
 
 	if (!hv_clock)
 		return 0;
 
-	size = PAGE_ALIGN(sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS);
-
 	cpu = get_cpu();
 
 	vcpu_time = &hv_clock[cpu].pvti;

commit 47f7dc4b845a9fe60c53b84b8c88cf14efd0de7f
Merge: 3c53776e29f8 e10f78050323
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 18 11:08:44 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm fixes from Paolo Bonzini:
     "Miscellaneous bugfixes, plus a small patchlet related to Spectre v2"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm:
      kvmclock: fix TSC calibration for nested guests
      KVM: VMX: Mark VMXArea with revision_id of physical CPU even when eVMCS enabled
      KVM: irqfd: fix race between EPOLLHUP and irq_bypass_register_consumer
      KVM/Eventfd: Avoid crash when assign and deassign specific eventfd in parallel.
      x86/kvmclock: set pvti_cpu0_va after enabling kvmclock
      x86/kvm/Kconfig: Ensure CRYPTO_DEV_CCP_DD state at minimum matches KVM_AMD
      kvm: nVMX: Restore exit qual for VM-entry failure due to MSR loading
      x86/kvm/vmx: don't read current->thread.{fs,gs}base of legacy tasks
      KVM: VMX: support MSR_IA32_ARCH_CAPABILITIES as a feature MSR

commit e10f7805032365cc11c739a97f226ebb48aee042
Author: Peng Hao <peng.hao2@zte.com.cn>
Date:   Sat Jul 14 23:28:29 2018 +0800

    kvmclock: fix TSC calibration for nested guests
    
    Inside a nested guest, access to hardware can be slow enough that
    tsc_read_refs always return ULLONG_MAX, causing tsc_refine_calibration_work
    to be called periodically and the nested guest to spend a lot of time
    reading the ACPI timer.
    
    However, if the TSC frequency is available from the pvclock page,
    we can just set X86_FEATURE_TSC_KNOWN_FREQ and avoid the recalibration.
    'refine' operation.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Peng Hao <peng.hao2@zte.com.cn>
    [Commit message rewritten. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index d79a18b4cf9d..4c53d12ca933 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -138,6 +138,7 @@ static unsigned long kvm_get_tsc_khz(void)
 	src = &hv_clock[cpu].pvti;
 	tsc_khz = pvclock_tsc_khz(src);
 	put_cpu();
+	setup_force_cpu_cap(X86_FEATURE_TSC_KNOWN_FREQ);
 	return tsc_khz;
 }
 

commit 94ffba484663ab3fc695ce2a34871e8c3db499f7
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Sun Jul 15 17:43:11 2018 +0200

    x86/kvmclock: set pvti_cpu0_va after enabling kvmclock
    
    pvti_cpu0_va is the address of shared kvmclock data structure.
    
    pvti_cpu0_va is currently kept unset (1) on 32 bit systems, (2) when
    kvmclock vsyscall is disabled, and (3) if kvmclock is not stable.
    This poses a problem, because kvm_ptp needs pvti_cpu0_va, but (1) can
    work on 32 bit, (2) has little relation to the vsyscall, and (3) does
    not need stable kvmclock (although kvmclock won't be used for system
    clock if it's not stable, so kvm_ptp is pointless in that case).
    
    Expose pvti_cpu0_va whenever kvmclock is enabled to allow all users to
    work with it.
    
    This fixes a regression found on Gentoo: https://bugs.gentoo.org/658544.
    
    Fixes: 9f08890ab906 ("x86/pvclock: add setter for pvclock_pvti_cpu0_va")
    Cc: stable@vger.kernel.org
    Reported-by: Andreas Steinmetz <ast@domdv.de>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 8b26c9e01cc4..d79a18b4cf9d 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -319,6 +319,8 @@ void __init kvmclock_init(void)
 	printk(KERN_INFO "kvm-clock: Using msrs %x and %x",
 		msr_kvm_system_time, msr_kvm_wall_clock);
 
+	pvclock_set_pvti_cpu0_va(hv_clock);
+
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
 		pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
 
@@ -366,14 +368,11 @@ int __init kvm_setup_vsyscall_timeinfo(void)
 	vcpu_time = &hv_clock[cpu].pvti;
 	flags = pvclock_read_flags(vcpu_time);
 
-	if (!(flags & PVCLOCK_TSC_STABLE_BIT)) {
-		put_cpu();
-		return 1;
-	}
-
-	pvclock_set_pvti_cpu0_va(hv_clock);
 	put_cpu();
 
+	if (!(flags & PVCLOCK_TSC_STABLE_BIT))
+		return 1;
+
 	kvm_clock.archdata.vclock_mode = VCLOCK_PVCLOCK;
 #endif
 	return 0;

commit e27c49291a7fe9dc415c9fcab5bd781ec82dfe04
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Apr 27 22:13:23 2018 +0200

    x86: Convert x86_platform_ops to timespec64
    
    The x86 platform operations are fairly isolated, so it's easy to change
    them from using timespec to timespec64. It has been checked that all the
    users and callers are safe, and there is only one critical function that is
    broken beyond 2106:
    
      pvclock_read_wallclock() uses a 32-bit number of seconds since the epoch
      to communicate the boot time between host and guest in a virtual
      environment. This will work until 2106, but fixing this is outside the
      scope of this change, Add a comment at least.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Acked-by: Radim Krčmář <rkrcmar@redhat.com>
    Acked-by: Jan Kiszka <jan.kiszka@siemens.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: jailhouse-dev@googlegroups.com
    Cc: Borislav Petkov <bp@suse.de>
    Cc: kvm@vger.kernel.org
    Cc: y2038@lists.linaro.org
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: xen-devel@lists.xenproject.org
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Joao Martins <joao.m.martins@oracle.com>
    Link: https://lkml.kernel.org/r/20180427201435.3194219-1-arnd@arndb.de

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 8b26c9e01cc4..bf8d1eb7fca3 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -53,7 +53,7 @@ static struct pvclock_wall_clock *wall_clock;
  * have elapsed since the hypervisor wrote the data. So we try to account for
  * that with system time
  */
-static void kvm_get_wallclock(struct timespec *now)
+static void kvm_get_wallclock(struct timespec64 *now)
 {
 	struct pvclock_vcpu_time_info *vcpu_time;
 	int low, high;
@@ -72,7 +72,7 @@ static void kvm_get_wallclock(struct timespec *now)
 	put_cpu();
 }
 
-static int kvm_set_wallclock(const struct timespec *now)
+static int kvm_set_wallclock(const struct timespec64 *now)
 {
 	return -ENODEV;
 }

commit 051089a2eed9a9977080774f3793ff2688cd3878
Merge: 974aa5630b31 646d944c2ef5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 13:06:27 2017 -0800

    Merge tag 'for-linus-4.15-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from Juergen Gross:
     "Xen features and fixes for v4.15-rc1
    
      Apart from several small fixes it contains the following features:
    
       - a series by Joao Martins to add vdso support of the pv clock
         interface
    
       - a series by Juergen Gross to add support for Xen pv guests to be
         able to run on 5 level paging hosts
    
       - a series by Stefano Stabellini adding the Xen pvcalls frontend
         driver using a paravirtualized socket interface"
    
    * tag 'for-linus-4.15-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip: (34 commits)
      xen/pvcalls: fix potential endless loop in pvcalls-front.c
      xen/pvcalls: Add MODULE_LICENSE()
      MAINTAINERS: xen, kvm: track pvclock-abi.h changes
      x86/xen/time: setup vcpu 0 time info page
      x86/xen/time: set pvclock flags on xen_time_init()
      x86/pvclock: add setter for pvclock_pvti_cpu0_va
      ptp_kvm: probe for kvm guest availability
      xen/privcmd: remove unused variable pageidx
      xen: select grant interface version
      xen: update arch/x86/include/asm/xen/cpuid.h
      xen: add grant interface version dependent constants to gnttab_ops
      xen: limit grant v2 interface to the v1 functionality
      xen: re-introduce support for grant v2 interface
      xen: support priv-mapping in an HVM tools domain
      xen/pvcalls: remove redundant check for irq >= 0
      xen/pvcalls: fix unsigned less than zero error check
      xen/time: Return -ENODEV from xen_get_wallclock()
      xen/pvcalls-front: mark expected switch fall-through
      xen: xenbus_probe_frontend: mark expected switch fall-throughs
      xen/time: do not decrease steal time after live migration on xen
      ...

commit 9f08890ab906abaf9d4c1bad8111755cbd302260
Author: Joao Martins <joao.m.martins@oracle.com>
Date:   Wed Nov 8 17:19:55 2017 +0000

    x86/pvclock: add setter for pvclock_pvti_cpu0_va
    
    Right now there is only a pvclock_pvti_cpu0_va() which is defined
    on kvmclock since:
    
    commit dac16fba6fc5
    ("x86/vdso: Get pvclock data from the vvar VMA instead of the fixmap")
    
    The only user of this interface so far is kvm. This commit adds a
    setter function for the pvti page and moves pvclock_pvti_cpu0_va
    to pvclock, which is a more generic place to have it; and would
    allow other PV clocksources to use it, such as Xen.
    
    While moving pvclock_pvti_cpu0_va into pvclock, rename also this
    function to pvclock_get_pvti_cpu0_va (including its call sites)
    to be symmetric with the setter (pvclock_set_pvti_cpu0_va).
    
    Signed-off-by: Joao Martins <joao.m.martins@oracle.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index d88967659098..538738047ff5 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -47,12 +47,6 @@ early_param("no-kvmclock", parse_no_kvmclock);
 static struct pvclock_vsyscall_time_info *hv_clock;
 static struct pvclock_wall_clock wall_clock;
 
-struct pvclock_vsyscall_time_info *pvclock_pvti_cpu0_va(void)
-{
-	return hv_clock;
-}
-EXPORT_SYMBOL_GPL(pvclock_pvti_cpu0_va);
-
 /*
  * The wallclock is the time of day when we booted. Since then, some time may
  * have elapsed since the hypervisor wrote the data. So we try to account for
@@ -334,6 +328,7 @@ int __init kvm_setup_vsyscall_timeinfo(void)
 		return 1;
 	}
 
+	pvclock_set_pvti_cpu0_va(hv_clock);
 	put_cpu();
 
 	kvm_clock.archdata.vclock_mode = VCLOCK_PVCLOCK;

commit 819aeee065e5d1b417ecd633897427c89f3253ec
Author: Brijesh Singh <brijesh.singh@amd.com>
Date:   Fri Oct 20 09:30:59 2017 -0500

    X86/KVM: Clear encryption attribute when SEV is active
    
    The guest physical memory area holding the struct pvclock_wall_clock and
    struct pvclock_vcpu_time_info are shared with the hypervisor. It
    periodically updates the contents of the memory.
    
    When SEV is active, the encryption attributes from the shared memory pages
    must be cleared so that both hypervisor and guest can access the data.
    
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Tested-by: Borislav Petkov <bp@suse.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: https://lkml.kernel.org/r/20171020143059.3291-18-brijesh.singh@amd.com

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 5b609e28ce3f..77b492c2d658 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -27,6 +27,7 @@
 #include <linux/sched.h>
 #include <linux/sched/clock.h>
 
+#include <asm/mem_encrypt.h>
 #include <asm/x86_init.h>
 #include <asm/reboot.h>
 #include <asm/kvmclock.h>
@@ -45,7 +46,7 @@ early_param("no-kvmclock", parse_no_kvmclock);
 
 /* The hypervisor will put information about time periodically here */
 static struct pvclock_vsyscall_time_info *hv_clock;
-static struct pvclock_wall_clock wall_clock;
+static struct pvclock_wall_clock *wall_clock;
 
 struct pvclock_vsyscall_time_info *pvclock_pvti_cpu0_va(void)
 {
@@ -64,15 +65,15 @@ static void kvm_get_wallclock(struct timespec *now)
 	int low, high;
 	int cpu;
 
-	low = (int)__pa_symbol(&wall_clock);
-	high = ((u64)__pa_symbol(&wall_clock) >> 32);
+	low = (int)slow_virt_to_phys(wall_clock);
+	high = ((u64)slow_virt_to_phys(wall_clock) >> 32);
 
 	native_write_msr(msr_kvm_wall_clock, low, high);
 
 	cpu = get_cpu();
 
 	vcpu_time = &hv_clock[cpu].pvti;
-	pvclock_read_wallclock(&wall_clock, vcpu_time, now);
+	pvclock_read_wallclock(wall_clock, vcpu_time, now);
 
 	put_cpu();
 }
@@ -249,11 +250,39 @@ static void kvm_shutdown(void)
 	native_machine_shutdown();
 }
 
+static phys_addr_t __init kvm_memblock_alloc(phys_addr_t size,
+					     phys_addr_t align)
+{
+	phys_addr_t mem;
+
+	mem = memblock_alloc(size, align);
+	if (!mem)
+		return 0;
+
+	if (sev_active()) {
+		if (early_set_memory_decrypted((unsigned long)__va(mem), size))
+			goto e_free;
+	}
+
+	return mem;
+e_free:
+	memblock_free(mem, size);
+	return 0;
+}
+
+static void __init kvm_memblock_free(phys_addr_t addr, phys_addr_t size)
+{
+	if (sev_active())
+		early_set_memory_encrypted((unsigned long)__va(addr), size);
+
+	memblock_free(addr, size);
+}
+
 void __init kvmclock_init(void)
 {
 	struct pvclock_vcpu_time_info *vcpu_time;
-	unsigned long mem;
-	int size, cpu;
+	unsigned long mem, mem_wall_clock;
+	int size, cpu, wall_clock_size;
 	u8 flags;
 
 	size = PAGE_ALIGN(sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS);
@@ -267,21 +296,35 @@ void __init kvmclock_init(void)
 	} else if (!(kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)))
 		return;
 
-	printk(KERN_INFO "kvm-clock: Using msrs %x and %x",
-		msr_kvm_system_time, msr_kvm_wall_clock);
+	wall_clock_size = PAGE_ALIGN(sizeof(struct pvclock_wall_clock));
+	mem_wall_clock = kvm_memblock_alloc(wall_clock_size, PAGE_SIZE);
+	if (!mem_wall_clock)
+		return;
 
-	mem = memblock_alloc(size, PAGE_SIZE);
-	if (!mem)
+	wall_clock = __va(mem_wall_clock);
+	memset(wall_clock, 0, wall_clock_size);
+
+	mem = kvm_memblock_alloc(size, PAGE_SIZE);
+	if (!mem) {
+		kvm_memblock_free(mem_wall_clock, wall_clock_size);
+		wall_clock = NULL;
 		return;
+	}
+
 	hv_clock = __va(mem);
 	memset(hv_clock, 0, size);
 
 	if (kvm_register_clock("primary cpu clock")) {
 		hv_clock = NULL;
-		memblock_free(mem, size);
+		kvm_memblock_free(mem, size);
+		kvm_memblock_free(mem_wall_clock, wall_clock_size);
+		wall_clock = NULL;
 		return;
 	}
 
+	printk(KERN_INFO "kvm-clock: Using msrs %x and %x",
+		msr_kvm_system_time, msr_kvm_wall_clock);
+
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
 		pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
 

commit 008755209ce1ae86c0f562a44810b57d9ea31a71
Author: Jason Gunthorpe <jgg@ziepe.ca>
Date:   Tue Oct 31 14:28:09 2017 -0600

    kvm: Return -ENODEV from update_persistent_clock
    
    kvm does not support setting the RTC, so the correct result is -ENODEV.
    Returning -1 will cause sync_cmos_clock to keep trying to set the RTC
    every second.
    
    Signed-off-by: Jason Gunthorpe <jgg@ziepe.ca>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index d88967659098..5b609e28ce3f 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -79,7 +79,7 @@ static void kvm_get_wallclock(struct timespec *now)
 
 static int kvm_set_wallclock(const struct timespec *now)
 {
-	return -1;
+	return -ENODEV;
 }
 
 static u64 kvm_clock_read(void)

commit e601757102cfd3eeae068f53b3bc1234f3a2b2e9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/clock.h>
    
    We are going to split <linux/sched/clock.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and .c files.
    
    Create a trivial placeholder <linux/sched/clock.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index bae6ea6cfb94..d88967659098 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -25,6 +25,7 @@
 #include <linux/hardirq.h>
 #include <linux/memblock.h>
 #include <linux/sched.h>
+#include <linux/sched/clock.h>
 
 #include <asm/x86_init.h>
 #include <asm/reboot.h>

commit fd7e9a88348472521d999434ee02f25735c7dadf
Merge: 5066e4a34081 dd0fd8bca185
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 18:22:53 2017 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "4.11 is going to be a relatively large release for KVM, with a little
      over 200 commits and noteworthy changes for most architectures.
    
      ARM:
       - GICv3 save/restore
       - cache flushing fixes
       - working MSI injection for GICv3 ITS
       - physical timer emulation
    
      MIPS:
       - various improvements under the hood
       - support for SMP guests
       - a large rewrite of MMU emulation. KVM MIPS can now use MMU
         notifiers to support copy-on-write, KSM, idle page tracking,
         swapping, ballooning and everything else. KVM_CAP_READONLY_MEM is
         also supported, so that writes to some memory regions can be
         treated as MMIO. The new MMU also paves the way for hardware
         virtualization support.
    
      PPC:
       - support for POWER9 using the radix-tree MMU for host and guest
       - resizable hashed page table
       - bugfixes.
    
      s390:
       - expose more features to the guest
       - more SIMD extensions
       - instruction execution protection
       - ESOP2
    
      x86:
       - improved hashing in the MMU
       - faster PageLRU tracking for Intel CPUs without EPT A/D bits
       - some refactoring of nested VMX entry/exit code, preparing for live
         migration support of nested hypervisors
       - expose yet another AVX512 CPUID bit
       - host-to-guest PTP support
       - refactoring of interrupt injection, with some optimizations thrown
         in and some duct tape removed.
       - remove lazy FPU handling
       - optimizations of user-mode exits
       - optimizations of vcpu_is_preempted() for KVM guests
    
      generic:
       - alternative signaling mechanism that doesn't pound on
         tsk->sighand->siglock"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (195 commits)
      x86/kvm: Provide optimized version of vcpu_is_preempted() for x86-64
      x86/paravirt: Change vcp_is_preempted() arg type to long
      KVM: VMX: use correct vmcs_read/write for guest segment selector/base
      x86/kvm/vmx: Defer TR reload after VM exit
      x86/asm/64: Drop __cacheline_aligned from struct x86_hw_tss
      x86/kvm/vmx: Simplify segment_base()
      x86/kvm/vmx: Get rid of segment_base() on 64-bit kernels
      x86/kvm/vmx: Don't fetch the TSS base from the GDT
      x86/asm: Define the kernel TSS limit in a macro
      kvm: fix page struct leak in handle_vmon
      KVM: PPC: Book3S HV: Disable HPT resizing on POWER9 for now
      KVM: Return an error code only as a constant in kvm_get_dirty_log()
      KVM: Return an error code only as a constant in kvm_get_dirty_log_protect()
      KVM: Return directly after a failed copy_from_user() in kvm_vm_compat_ioctl()
      KVM: x86: remove code for lazy FPU handling
      KVM: race-free exit from KVM_RUN without POSIX signals
      KVM: PPC: Book3S HV: Turn "KVM guest htab" message into a debug message
      KVM: PPC: Book3S PR: Ratelimit copy data failure error messages
      KVM: Support vCPU-based gfn->hva cache
      KVM: use separate generations for each address space
      ...

commit f4066c2bc4d0de4e5dcbff21dae41e89fe8f38c0
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Jan 24 15:09:41 2017 -0200

    kvmclock: export kvmclock clocksource and data pointers
    
    To be used by KVM PTP driver.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 2a5cafdf8808..995fa260a6da 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -28,6 +28,7 @@
 
 #include <asm/x86_init.h>
 #include <asm/reboot.h>
+#include <asm/kvmclock.h>
 
 static int kvmclock __ro_after_init = 1;
 static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
@@ -49,6 +50,7 @@ struct pvclock_vsyscall_time_info *pvclock_pvti_cpu0_va(void)
 {
 	return hv_clock;
 }
+EXPORT_SYMBOL_GPL(pvclock_pvti_cpu0_va);
 
 /*
  * The wallclock is the time of day when we booted. Since then, some time may
@@ -174,13 +176,14 @@ bool kvm_check_and_clear_guest_paused(void)
 	return ret;
 }
 
-static struct clocksource kvm_clock = {
+struct clocksource kvm_clock = {
 	.name = "kvm-clock",
 	.read = kvm_clock_get_cycles,
 	.rating = 400,
 	.mask = CLOCKSOURCE_MASK(64),
 	.flags = CLOCK_SOURCE_IS_CONTINUOUS,
 };
+EXPORT_SYMBOL_GPL(kvm_clock);
 
 int kvm_register_clock(char *txt)
 {

commit acb04058de49458010c44bb35b849d45113fd668
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jan 19 14:36:33 2017 +0100

    sched/clock: Fix hotplug crash
    
    Mike reported that he could trigger the WARN_ON_ONCE() in
    set_sched_clock_stable() using hotplug.
    
    This exposed a fundamental problem with the interface, we should never
    mark the TSC stable if we ever find it to be unstable. Therefore
    set_sched_clock_stable() is a broken interface.
    
    The reason it existed is that not having it is a pain, it means all
    relevant architecture code needs to call clear_sched_clock_stable()
    where appropriate.
    
    Of the three architectures that select HAVE_UNSTABLE_SCHED_CLOCK ia64
    and parisc are trivial in that they never called
    set_sched_clock_stable(), so add an unconditional call to
    clear_sched_clock_stable() to them.
    
    For x86 the story is a lot more involved, and what this patch tries to
    do is ensure we preserve the status quo. So even is Cyrix or Transmeta
    have usable TSC they never called set_sched_clock_stable() so they now
    get an explicit mark unstable.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 9881b024b7d7 ("sched/clock: Delay switching sched_clock to stable")
    Link: http://lkml.kernel.org/r/20170119133633.GB6536@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 2a5cafdf8808..542710b99f52 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -107,12 +107,12 @@ static inline void kvm_sched_clock_init(bool stable)
 {
 	if (!stable) {
 		pv_time_ops.sched_clock = kvm_clock_read;
+		clear_sched_clock_stable();
 		return;
 	}
 
 	kvm_sched_clock_offset = kvm_clock_read();
 	pv_time_ops.sched_clock = kvm_sched_clock_read;
-	set_sched_clock_stable();
 
 	printk(KERN_INFO "kvm-clock: using sched offset of %llu cycles\n",
 			kvm_sched_clock_offset);

commit a5a1d1c2914b5316924c7893eb683a5420ebd3be
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:32:01 2016 +0100

    clocksource: Use a plain u64 instead of cycle_t
    
    There is no point in having an extra type for extra confusion. u64 is
    unambiguous.
    
    Conversion was done with the following coccinelle script:
    
    @rem@
    @@
    -typedef u64 cycle_t;
    
    @fix@
    typedef cycle_t;
    @@
    -cycle_t
    +u64
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 60b9949f1e65..2a5cafdf8808 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -32,7 +32,7 @@
 static int kvmclock __ro_after_init = 1;
 static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
-static cycle_t kvm_sched_clock_offset;
+static u64 kvm_sched_clock_offset;
 
 static int parse_no_kvmclock(char *arg)
 {
@@ -79,10 +79,10 @@ static int kvm_set_wallclock(const struct timespec *now)
 	return -1;
 }
 
-static cycle_t kvm_clock_read(void)
+static u64 kvm_clock_read(void)
 {
 	struct pvclock_vcpu_time_info *src;
-	cycle_t ret;
+	u64 ret;
 	int cpu;
 
 	preempt_disable_notrace();
@@ -93,12 +93,12 @@ static cycle_t kvm_clock_read(void)
 	return ret;
 }
 
-static cycle_t kvm_clock_get_cycles(struct clocksource *cs)
+static u64 kvm_clock_get_cycles(struct clocksource *cs)
 {
 	return kvm_clock_read();
 }
 
-static cycle_t kvm_sched_clock_read(void)
+static u64 kvm_sched_clock_read(void)
 {
 	return kvm_clock_read() - kvm_sched_clock_offset;
 }

commit d4b80afbba49e968623330f1336da8c724da8aad
Merge: fcd709ef20a9 4cea8776571b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Sep 15 08:24:53 2016 +0200

    Merge branch 'linus' into x86/asm, to pick up recent fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a4497a86fb9b855c5ac8503fdc959393b00bb643
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Thu Sep 8 10:15:28 2016 -0400

    x86, clock: Fix kvm guest tsc initialization
    
    When booting a kvm guest on AMD with the latest kernel the following
    messages are displayed in the boot log:
    
     tsc: Unable to calibrate against PIT
     tsc: HPET/PMTIMER calibration failed
    
    aa297292d708 ("x86/tsc: Enumerate SKL cpu_khz and tsc_khz via CPUID")
    introduced a change to account for a difference in cpu and tsc frequencies for
    Intel SKL processors. Before this change the native tsc set
    x86_platform.calibrate_tsc to native_calibrate_tsc() which is a hardware
    calibration of the tsc, and in tsc_init() executed
    
            tsc_khz = x86_platform.calibrate_tsc();
            cpu_khz = tsc_khz;
    
    The kvm code changed x86_platform.calibrate_tsc to kvm_get_tsc_khz() and
    executed the same tsc_init() function.  This meant that KVM guests did not
    execute the native hardware calibration function.
    
    After aa297292d708, there are separate native calibrations for cpu_khz and
    tsc_khz.  The code sets x86_platform.calibrate_tsc to native_calibrate_tsc()
    which is now an Intel specific calibration function, and
    x86_platform.calibrate_cpu to native_calibrate_cpu() which is the "old"
    native_calibrate_tsc() function (ie, the native hardware calibration
    function).
    
    tsc_init() now does
    
            cpu_khz = x86_platform.calibrate_cpu();
            tsc_khz = x86_platform.calibrate_tsc();
            if (tsc_khz == 0)
                    tsc_khz = cpu_khz;
            else if (abs(cpu_khz - tsc_khz) * 10 > tsc_khz)
                    cpu_khz = tsc_khz;
    
    The kvm code should not call the hardware initialization in
    native_calibrate_cpu(), as it isn't applicable for kvm and it didn't do that
    prior to aa297292d708.
    
    This patch resolves this issue by setting x86_platform.calibrate_cpu to
    kvm_get_tsc_khz().
    
    v2: I had originally set x86_platform.calibrate_cpu to
    cpu_khz_from_cpuid(), however, pbonzini pointed out that the CPUID leaf
    in that function is not available in KVM.  I have changed the function
    pointer to kvm_get_tsc_khz().
    
    Fixes: aa297292d708 ("x86/tsc: Enumerate SKL cpu_khz and tsc_khz via CPUID")
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Cc: Len Brown <len.brown@intel.com>
    Cc: "Peter Zijlstra (Intel)" <peterz@infradead.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: "Christopher S. Hall" <christopher.s.hall@intel.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: kvm@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 1d39bfbd26bb..3692249a70f1 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -289,6 +289,7 @@ void __init kvmclock_init(void)
 	put_cpu();
 
 	x86_platform.calibrate_tsc = kvm_get_tsc_khz;
+	x86_platform.calibrate_cpu = kvm_get_tsc_khz;
 	x86_platform.get_wallclock = kvm_get_wallclock;
 	x86_platform.set_wallclock = kvm_set_wallclock;
 #ifdef CONFIG_X86_LOCAL_APIC

commit 404f6aac9b3ef595735feca99979db084ea48315
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Aug 8 16:29:06 2016 -0700

    x86: Apply more __ro_after_init and const
    
    Guided by grsecurity's analogous __read_only markings in arch/x86,
    this applies several uses of __ro_after_init to structures that are
    only updated during __init, and const for some structures that are
    never updated.  Additionally extends __init markings to some functions
    that are only used during __init, and cleans up some missing C99 style
    static initializers.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Brown <david.brown@linaro.org>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Emese Revfy <re.emese@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathias Krause <minipli@googlemail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: PaX Team <pageexec@freemail.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-hardening@lists.openwall.com
    Link: http://lkml.kernel.org/r/20160808232906.GA29731@www.outflux.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 1d39bfbd26bb..0964399ef942 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -29,7 +29,7 @@
 #include <asm/x86_init.h>
 #include <asm/reboot.h>
 
-static int kvmclock = 1;
+static int kvmclock __ro_after_init = 1;
 static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
 static cycle_t kvm_sched_clock_offset;

commit 6a6256f9e0ebaabf7ded1fef8977a4352dbe7784
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Tue Feb 23 15:34:30 2016 -0800

    x86: Fix misspellings in comments
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: trivial@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 72cef58693c7..1d39bfbd26bb 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -226,7 +226,7 @@ static void kvm_setup_secondary_clock(void)
  * registered memory location. If the guest happens to shutdown, this memory
  * won't be valid. In cases like kexec, in which you install a new kernel, this
  * means a random memory location will be kept being written. So before any
- * kind of shutdown from our side, we unregister the clock by writting anything
+ * kind of shutdown from our side, we unregister the clock by writing anything
  * that does not have the 'enable' bit set in the msr
  */
 #ifdef CONFIG_KEXEC_CORE

commit cc1e24fdb064d3126a494716f22ad4fc39306742
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Dec 10 19:20:21 2015 -0800

    x86/vdso: Remove pvclock fixmap machinery
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/4933029991103ae44672c82b97a20035f5c1fe4f.1449702533.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index ec1b06dc82d2..72cef58693c7 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -310,7 +310,6 @@ int __init kvm_setup_vsyscall_timeinfo(void)
 {
 #ifdef CONFIG_X86_64
 	int cpu;
-	int ret;
 	u8 flags;
 	struct pvclock_vcpu_time_info *vcpu_time;
 	unsigned int size;
@@ -330,11 +329,6 @@ int __init kvm_setup_vsyscall_timeinfo(void)
 		return 1;
 	}
 
-	if ((ret = pvclock_init_vsyscall(hv_clock, size))) {
-		put_cpu();
-		return ret;
-	}
-
 	put_cpu();
 
 	kvm_clock.archdata.vclock_mode = VCLOCK_PVCLOCK;

commit dac16fba6fc590fa7239676b35ed75dae4c4cd2b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Dec 10 19:20:20 2015 -0800

    x86/vdso: Get pvclock data from the vvar VMA instead of the fixmap
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/9d37826fdc7e2d2809efe31d5345f97186859284.1449702533.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 2bd81e302427..ec1b06dc82d2 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -45,6 +45,11 @@ early_param("no-kvmclock", parse_no_kvmclock);
 static struct pvclock_vsyscall_time_info *hv_clock;
 static struct pvclock_wall_clock wall_clock;
 
+struct pvclock_vsyscall_time_info *pvclock_pvti_cpu0_va(void)
+{
+	return hv_clock;
+}
+
 /*
  * The wallclock is the time of day when we booted. Since then, some time may
  * have elapsed since the hypervisor wrote the data. So we try to account for

commit 72c930dcfc2b49404ee9e20f6c868402e9c71166
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Fri Sep 18 17:54:29 2015 +0200

    x86: kvmclock: abolish PVCLOCK_COUNTS_FROM_ZERO
    
    Newer KVM won't be exposing PVCLOCK_COUNTS_FROM_ZERO anymore.
    The purpose of that flags was to start counting system time from 0 when
    the KVM clock has been initialized.
    We can achieve the same by selecting one read as the initial point.
    
    A simple subtraction will work unless the KVM clock count overflows
    earlier (has smaller width) than scheduler's cycle count.  We should be
    safe till x86_128.
    
    Because PVCLOCK_COUNTS_FROM_ZERO was enabled only on new hypervisors,
    setting sched clock as stable based on PVCLOCK_TSC_STABLE_BIT might
    regress on older ones.
    
    I presume we don't need to change kvm_clock_read instead of introducing
    kvm_sched_clock_read.  A problem could arise in case sched_clock is
    expected to return the same value as get_cycles, but we should have
    merged those clocks in that case.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Acked-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 2c7aafa70702..2bd81e302427 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -32,6 +32,7 @@
 static int kvmclock = 1;
 static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
+static cycle_t kvm_sched_clock_offset;
 
 static int parse_no_kvmclock(char *arg)
 {
@@ -92,6 +93,29 @@ static cycle_t kvm_clock_get_cycles(struct clocksource *cs)
 	return kvm_clock_read();
 }
 
+static cycle_t kvm_sched_clock_read(void)
+{
+	return kvm_clock_read() - kvm_sched_clock_offset;
+}
+
+static inline void kvm_sched_clock_init(bool stable)
+{
+	if (!stable) {
+		pv_time_ops.sched_clock = kvm_clock_read;
+		return;
+	}
+
+	kvm_sched_clock_offset = kvm_clock_read();
+	pv_time_ops.sched_clock = kvm_sched_clock_read;
+	set_sched_clock_stable();
+
+	printk(KERN_INFO "kvm-clock: using sched offset of %llu cycles\n",
+			kvm_sched_clock_offset);
+
+	BUILD_BUG_ON(sizeof(kvm_sched_clock_offset) >
+	         sizeof(((struct pvclock_vcpu_time_info *)NULL)->system_time));
+}
+
 /*
  * If we don't do that, there is the possibility that the guest
  * will calibrate under heavy load - thus, getting a lower lpj -
@@ -248,7 +272,17 @@ void __init kvmclock_init(void)
 		memblock_free(mem, size);
 		return;
 	}
-	pv_time_ops.sched_clock = kvm_clock_read;
+
+	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
+		pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
+
+	cpu = get_cpu();
+	vcpu_time = &hv_clock[cpu].pvti;
+	flags = pvclock_read_flags(vcpu_time);
+
+	kvm_sched_clock_init(flags & PVCLOCK_TSC_STABLE_BIT);
+	put_cpu();
+
 	x86_platform.calibrate_tsc = kvm_get_tsc_khz;
 	x86_platform.get_wallclock = kvm_get_wallclock;
 	x86_platform.set_wallclock = kvm_set_wallclock;
@@ -265,16 +299,6 @@ void __init kvmclock_init(void)
 	kvm_get_preset_lpj();
 	clocksource_register_hz(&kvm_clock, NSEC_PER_SEC);
 	pv_info.name = "KVM";
-
-	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
-		pvclock_set_flags(~0);
-
-	cpu = get_cpu();
-	vcpu_time = &hv_clock[cpu].pvti;
-	flags = pvclock_read_flags(vcpu_time);
-	if (flags & PVCLOCK_COUNTS_FROM_ZERO)
-		set_sched_clock_stable();
-	put_cpu();
 }
 
 int __init kvm_setup_vsyscall_timeinfo(void)

commit 2965faa5e03d1e71e9ff9aa143fff39e0a77543a
Author: Dave Young <dyoung@redhat.com>
Date:   Wed Sep 9 15:38:55 2015 -0700

    kexec: split kexec_load syscall from kexec core code
    
    There are two kexec load syscalls, kexec_load another and kexec_file_load.
     kexec_file_load has been splited as kernel/kexec_file.c.  In this patch I
    split kexec_load syscall code to kernel/kexec.c.
    
    And add a new kconfig option KEXEC_CORE, so we can disable kexec_load and
    use kexec_file_load only, or vice verse.
    
    The original requirement is from Ted Ts'o, he want kexec kernel signature
    being checked with CONFIG_KEXEC_VERIFY_SIG enabled.  But kexec-tools use
    kexec_load syscall can bypass the checking.
    
    Vivek Goyal proposed to create a common kconfig option so user can compile
    in only one syscall for loading kexec kernel.  KEXEC/KEXEC_FILE selects
    KEXEC_CORE so that old config files still work.
    
    Because there's general code need CONFIG_KEXEC_CORE, so I updated all the
    architecture Kconfig with a new option KEXEC_CORE, and let KEXEC selects
    KEXEC_CORE in arch Kconfig.  Also updated general kernel code with to
    kexec_load syscall.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Dave Young <dyoung@redhat.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Petr Tesarik <ptesarik@suse.cz>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Josh Boyer <jwboyer@fedoraproject.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 49487b488061..2c7aafa70702 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -200,7 +200,7 @@ static void kvm_setup_secondary_clock(void)
  * kind of shutdown from our side, we unregister the clock by writting anything
  * that does not have the 'enable' bit set in the msr
  */
-#ifdef CONFIG_KEXEC
+#ifdef CONFIG_KEXEC_CORE
 static void kvm_crash_shutdown(struct pt_regs *regs)
 {
 	native_write_msr(msr_kvm_system_time, 0, 0);
@@ -259,7 +259,7 @@ void __init kvmclock_init(void)
 	x86_platform.save_sched_clock_state = kvm_save_sched_clock_state;
 	x86_platform.restore_sched_clock_state = kvm_restore_sched_clock_state;
 	machine_ops.shutdown  = kvm_shutdown;
-#ifdef CONFIG_KEXEC
+#ifdef CONFIG_KEXEC_CORE
 	machine_ops.crash_shutdown  = kvm_crash_shutdown;
 #endif
 	kvm_get_preset_lpj();

commit 0ad83caa21bff9f383c10e73b452425709573042
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Thu May 28 20:20:40 2015 -0300

    x86: kvmclock: set scheduler clock stable
    
    If you try to enable NOHZ_FULL on a guest today, you'll get
    the following error when the guest tries to deactivate the
    scheduler tick:
    
     WARNING: CPU: 3 PID: 2182 at kernel/time/tick-sched.c:192 can_stop_full_tick+0xb9/0x290()
     NO_HZ FULL will not work with unstable sched clock
     CPU: 3 PID: 2182 Comm: kworker/3:1 Not tainted 4.0.0-10545-gb9bb6fb #204
     Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011
     Workqueue: events flush_to_ldisc
      ffffffff8162a0c7 ffff88011f583e88 ffffffff814e6ba0 0000000000000002
      ffff88011f583ed8 ffff88011f583ec8 ffffffff8104d095 ffff88011f583eb8
      0000000000000000 0000000000000003 0000000000000001 0000000000000001
     Call Trace:
      <IRQ>  [<ffffffff814e6ba0>] dump_stack+0x4f/0x7b
      [<ffffffff8104d095>] warn_slowpath_common+0x85/0xc0
      [<ffffffff8104d146>] warn_slowpath_fmt+0x46/0x50
      [<ffffffff810bd2a9>] can_stop_full_tick+0xb9/0x290
      [<ffffffff810bd9ed>] tick_nohz_irq_exit+0x8d/0xb0
      [<ffffffff810511c5>] irq_exit+0xc5/0x130
      [<ffffffff814f180a>] smp_apic_timer_interrupt+0x4a/0x60
      [<ffffffff814eff5e>] apic_timer_interrupt+0x6e/0x80
      <EOI>  [<ffffffff814ee5d1>] ? _raw_spin_unlock_irqrestore+0x31/0x60
      [<ffffffff8108bbc8>] __wake_up+0x48/0x60
      [<ffffffff8134836c>] n_tty_receive_buf_common+0x49c/0xba0
      [<ffffffff8134a6bf>] ? tty_ldisc_ref+0x1f/0x70
      [<ffffffff81348a84>] n_tty_receive_buf2+0x14/0x20
      [<ffffffff8134b390>] flush_to_ldisc+0xe0/0x120
      [<ffffffff81064d05>] process_one_work+0x1d5/0x540
      [<ffffffff81064c81>] ? process_one_work+0x151/0x540
      [<ffffffff81065191>] worker_thread+0x121/0x470
      [<ffffffff81065070>] ? process_one_work+0x540/0x540
      [<ffffffff8106b4df>] kthread+0xef/0x110
      [<ffffffff8106b3f0>] ? __kthread_parkme+0xa0/0xa0
      [<ffffffff814ef4f2>] ret_from_fork+0x42/0x70
      [<ffffffff8106b3f0>] ? __kthread_parkme+0xa0/0xa0
     ---[ end trace 06e3507544a38866 ]---
    
    However, it turns out that kvmclock does provide a stable
    sched_clock callback. So, let the scheduler know this which
    in turn makes NOHZ_FULL work in the guest.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 42caaef897c8..49487b488061 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -24,6 +24,7 @@
 #include <linux/percpu.h>
 #include <linux/hardirq.h>
 #include <linux/memblock.h>
+#include <linux/sched.h>
 
 #include <asm/x86_init.h>
 #include <asm/reboot.h>
@@ -217,8 +218,10 @@ static void kvm_shutdown(void)
 
 void __init kvmclock_init(void)
 {
+	struct pvclock_vcpu_time_info *vcpu_time;
 	unsigned long mem;
-	int size;
+	int size, cpu;
+	u8 flags;
 
 	size = PAGE_ALIGN(sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS);
 
@@ -264,7 +267,14 @@ void __init kvmclock_init(void)
 	pv_info.name = "KVM";
 
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
-		pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
+		pvclock_set_flags(~0);
+
+	cpu = get_cpu();
+	vcpu_time = &hv_clock[cpu].pvti;
+	flags = pvclock_read_flags(vcpu_time);
+	if (flags & PVCLOCK_COUNTS_FROM_ZERO)
+		set_sched_clock_stable();
+	put_cpu();
 }
 
 int __init kvm_setup_vsyscall_timeinfo(void)

commit c35ebbeade127e7bca1f21ef5bf1a39deffba9de
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 13 15:59:26 2015 +0200

    Revert "kvmclock: set scheduler clock stable"
    
    This reverts commit ff7bbb9c6ab6e6620429daeff39424bbde1a94b4.
    Sasha Levin is seeing odd jump in time values during boot of a KVM guest:
    
    [...]
    [    0.000000] tsc: Detected 2260.998 MHz processor
    [3376355.247558] Calibrating delay loop (skipped) preset value..
    [...]
    
    and bisected them to this commit.
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 4e03921761c4..42caaef897c8 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -24,7 +24,6 @@
 #include <linux/percpu.h>
 #include <linux/hardirq.h>
 #include <linux/memblock.h>
-#include <linux/sched.h>
 
 #include <asm/x86_init.h>
 #include <asm/reboot.h>
@@ -266,8 +265,6 @@ void __init kvmclock_init(void)
 
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
 		pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
-
-	set_sched_clock_stable();
 }
 
 int __init kvm_setup_vsyscall_timeinfo(void)

commit ff7bbb9c6ab6e6620429daeff39424bbde1a94b4
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Thu Apr 23 17:12:42 2015 -0400

    kvmclock: set scheduler clock stable
    
    If you try to enable NOHZ_FULL on a guest today, you'll get
    the following error when the guest tries to deactivate the
    scheduler tick:
    
     WARNING: CPU: 3 PID: 2182 at kernel/time/tick-sched.c:192 can_stop_full_tick+0xb9/0x290()
     NO_HZ FULL will not work with unstable sched clock
     CPU: 3 PID: 2182 Comm: kworker/3:1 Not tainted 4.0.0-10545-gb9bb6fb #204
     Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011
     Workqueue: events flush_to_ldisc
      ffffffff8162a0c7 ffff88011f583e88 ffffffff814e6ba0 0000000000000002
      ffff88011f583ed8 ffff88011f583ec8 ffffffff8104d095 ffff88011f583eb8
      0000000000000000 0000000000000003 0000000000000001 0000000000000001
     Call Trace:
      <IRQ>  [<ffffffff814e6ba0>] dump_stack+0x4f/0x7b
      [<ffffffff8104d095>] warn_slowpath_common+0x85/0xc0
      [<ffffffff8104d146>] warn_slowpath_fmt+0x46/0x50
      [<ffffffff810bd2a9>] can_stop_full_tick+0xb9/0x290
      [<ffffffff810bd9ed>] tick_nohz_irq_exit+0x8d/0xb0
      [<ffffffff810511c5>] irq_exit+0xc5/0x130
      [<ffffffff814f180a>] smp_apic_timer_interrupt+0x4a/0x60
      [<ffffffff814eff5e>] apic_timer_interrupt+0x6e/0x80
      <EOI>  [<ffffffff814ee5d1>] ? _raw_spin_unlock_irqrestore+0x31/0x60
      [<ffffffff8108bbc8>] __wake_up+0x48/0x60
      [<ffffffff8134836c>] n_tty_receive_buf_common+0x49c/0xba0
      [<ffffffff8134a6bf>] ? tty_ldisc_ref+0x1f/0x70
      [<ffffffff81348a84>] n_tty_receive_buf2+0x14/0x20
      [<ffffffff8134b390>] flush_to_ldisc+0xe0/0x120
      [<ffffffff81064d05>] process_one_work+0x1d5/0x540
      [<ffffffff81064c81>] ? process_one_work+0x151/0x540
      [<ffffffff81065191>] worker_thread+0x121/0x470
      [<ffffffff81065070>] ? process_one_work+0x540/0x540
      [<ffffffff8106b4df>] kthread+0xef/0x110
      [<ffffffff8106b3f0>] ? __kthread_parkme+0xa0/0xa0
      [<ffffffff814ef4f2>] ret_from_fork+0x42/0x70
      [<ffffffff8106b3f0>] ? __kthread_parkme+0xa0/0xa0
     ---[ end trace 06e3507544a38866 ]---
    
    However, it turns out that kvmclock does provide a stable
    sched_clock callback. So, let the scheduler know this which
    in turn makes NOHZ_FULL work in the guest.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 42caaef897c8..4e03921761c4 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -24,6 +24,7 @@
 #include <linux/percpu.h>
 #include <linux/hardirq.h>
 #include <linux/memblock.h>
+#include <linux/sched.h>
 
 #include <asm/x86_init.h>
 #include <asm/reboot.h>
@@ -265,6 +266,8 @@ void __init kvmclock_init(void)
 
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
 		pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
+
+	set_sched_clock_stable();
 }
 
 int __init kvm_setup_vsyscall_timeinfo(void)

commit 29fa6825463c97e5157284db80107d1bfac5d77b
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Dec 5 19:03:28 2014 -0800

    x86, kvm: Clear paravirt_enabled on KVM guests for espfix32's benefit
    
    paravirt_enabled has the following effects:
    
     - Disables the F00F bug workaround warning.  There is no F00F bug
       workaround any more because Linux's standard IDT handling already
       works around the F00F bug, but the warning still exists.  This
       is only cosmetic, and, in any event, there is no such thing as
       KVM on a CPU with the F00F bug.
    
     - Disables 32-bit APM BIOS detection.  On a KVM paravirt system,
       there should be no APM BIOS anyway.
    
     - Disables tboot.  I think that the tboot code should check the
       CPUID hypervisor bit directly if it matters.
    
     - paravirt_enabled disables espfix32.  espfix32 should *not* be
       disabled under KVM paravirt.
    
    The last point is the purpose of this patch.  It fixes a leak of the
    high 16 bits of the kernel stack address on 32-bit KVM paravirt
    guests.  Fixes CVE-2014-8134.
    
    Cc: stable@vger.kernel.org
    Suggested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 0bf3467d7f30..42caaef897c8 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -261,7 +261,6 @@ void __init kvmclock_init(void)
 #endif
 	kvm_get_preset_lpj();
 	clocksource_register_hz(&kvm_clock, NSEC_PER_SEC);
-	pv_info.paravirt_enabled = 1;
 	pv_info.name = "KVM";
 
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))

commit c6338ce494456ed9c92ef10f63c0a8408bfeac6d
Author: Tiejun Chen <tiejun.chen@intel.com>
Date:   Fri Sep 26 14:00:04 2014 +0800

    kvm: kvmclock: use get_cpu() and put_cpu()
    
    We can use get_cpu() and put_cpu() to replace
    preempt_disable()/cpu = smp_processor_id() and
    preempt_enable() for slightly better code.
    
    Signed-off-by: Tiejun Chen <tiejun.chen@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index d9156ceecdff..0bf3467d7f30 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -59,13 +59,12 @@ static void kvm_get_wallclock(struct timespec *now)
 
 	native_write_msr(msr_kvm_wall_clock, low, high);
 
-	preempt_disable();
-	cpu = smp_processor_id();
+	cpu = get_cpu();
 
 	vcpu_time = &hv_clock[cpu].pvti;
 	pvclock_read_wallclock(&wall_clock, vcpu_time, now);
 
-	preempt_enable();
+	put_cpu();
 }
 
 static int kvm_set_wallclock(const struct timespec *now)
@@ -107,11 +106,10 @@ static unsigned long kvm_get_tsc_khz(void)
 	int cpu;
 	unsigned long tsc_khz;
 
-	preempt_disable();
-	cpu = smp_processor_id();
+	cpu = get_cpu();
 	src = &hv_clock[cpu].pvti;
 	tsc_khz = pvclock_tsc_khz(src);
-	preempt_enable();
+	put_cpu();
 	return tsc_khz;
 }
 
@@ -284,23 +282,22 @@ int __init kvm_setup_vsyscall_timeinfo(void)
 
 	size = PAGE_ALIGN(sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS);
 
-	preempt_disable();
-	cpu = smp_processor_id();
+	cpu = get_cpu();
 
 	vcpu_time = &hv_clock[cpu].pvti;
 	flags = pvclock_read_flags(vcpu_time);
 
 	if (!(flags & PVCLOCK_TSC_STABLE_BIT)) {
-		preempt_enable();
+		put_cpu();
 		return 1;
 	}
 
 	if ((ret = pvclock_init_vsyscall(hv_clock, size))) {
-		preempt_enable();
+		put_cpu();
 		return ret;
 	}
 
-	preempt_enable();
+	put_cpu();
 
 	kvm_clock.archdata.vclock_mode = VCLOCK_PVCLOCK;
 #endif

commit 0d75de4a65d99ba042b050620d479ab74b1919d4
Author: Fernando Luis Vázquez Cao <fernando_b1@lab.ntt.co.jp>
Date:   Tue Feb 18 19:09:11 2014 +0900

    kvm: remove redundant registration of BSP's hv_clock area
    
    These days hv_clock allocation is memblock based (i.e. the percpu
    allocator is not involved), which means that the physical address
    of each of the per-cpu hv_clock areas is guaranteed to remain
    unchanged through all its lifetime and we do not need to update
    its location after CPU bring-up.
    
    Signed-off-by: Fernando Luis Vazquez Cao <fernando@oss.ntt.co.jp>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index e6041094ff26..d9156ceecdff 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -242,7 +242,7 @@ void __init kvmclock_init(void)
 	hv_clock = __va(mem);
 	memset(hv_clock, 0, size);
 
-	if (kvm_register_clock("boot clock")) {
+	if (kvm_register_clock("primary cpu clock")) {
 		hv_clock = NULL;
 		memblock_free(mem, size);
 		return;

commit d63285e94af3ade4fa8b10b0d9a22bcf72baf2f9
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Oct 11 21:39:25 2013 -0300

    pvclock: detect watchdog reset at pvclock read
    
    Implement reset of kernel watchdogs at pvclock read time. This avoids
    adding special code to every watchdog.
    
    This is possible for watchdogs which measure time based on sched_clock() or
    ktime_get() variants.
    
    Suggested by Don Zickus.
    
    Acked-by: Don Zickus <dzickus@redhat.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 1570e0741344..e6041094ff26 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -139,6 +139,7 @@ bool kvm_check_and_clear_guest_paused(void)
 	src = &hv_clock[cpu].pvti;
 	if ((src->flags & PVCLOCK_GUEST_STOPPED) != 0) {
 		src->flags &= ~PVCLOCK_GUEST_STOPPED;
+		pvclock_touch_watchdogs();
 		ret = true;
 	}
 

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 1f354f4b602b..1570e0741344 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -182,7 +182,7 @@ static void kvm_restore_sched_clock_state(void)
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC
-static void __cpuinit kvm_setup_secondary_clock(void)
+static void kvm_setup_secondary_clock(void)
 {
 	/*
 	 * Now that the first cpu already had this clocksource initialized,

commit 2b0f89317e99735bbf32eaede81f707f98ab1b5e
Merge: 07bd1172902e fa18f7bde3ad
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 4 23:11:22 2013 +0200

    Merge branch 'timers/posix-cpu-timers-for-tglx' of
    git://git.kernel.org/pub/scm/linux/kernel/git/frederic/linux-dynticks into timers/core
    
    Frederic sayed: "Most of these patches have been hanging around for
    several month now, in -mmotm for a significant chunk. They already
    missed a few releases."
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 07868fc6aaf57847b0f3a3d53086b7556eb83f4a
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Mon Jun 10 18:31:11 2013 +0200

    x86: kvmclock: zero initialize pvclock shared memory area
    
    kernel might hung in pvclock_clocksource_read() due to
    uninitialized memory might contain odd version value in
    following cycle:
    
            do {
                    version = __pvclock_read_cycles(src, &ret, &flags);
            } while ((src->version & 1) || version != src->version);
    
    if secondary kvmclock is accessed before it's registered with kvm.
    
    Clear garbage in pvclock shared memory area right after it's
    allocated to avoid this issue.
    
    Ref: https://bugzilla.kernel.org/show_bug.cgi?id=59521
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    [See BZ for analysis.  We may want a different fix for 3.11, but
     this is the safest for now - Paolo]
    Cc: <stable@vger.kernel.org> # 3.8
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index d2c381280e3c..3dd37ebd591b 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -242,6 +242,7 @@ void __init kvmclock_init(void)
 	if (!mem)
 		return;
 	hv_clock = __va(mem);
+	memset(hv_clock, 0, size);
 
 	if (kvm_register_clock("boot clock")) {
 		hv_clock = NULL;

commit 3565184ed0c1ea46bea5b792da5f72a83c43e49b
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon May 13 18:56:06 2013 +0100

    x86: Increase precision of x86_platform.get/set_wallclock()
    
    All the virtualized platforms (KVM, lguest and Xen) have persistent
    wallclocks that have more than one second of precision.
    
    read_persistent_wallclock() and update_persistent_wallclock() allow
    for nanosecond precision but their implementation on x86 with
    x86_platform.get/set_wallclock() only allows for one second precision.
    This means guests may see a wallclock time that is off by up to 1
    second.
    
    Make set_wallclock() and get_wallclock() take a struct timespec
    parameter (which allows for nanosecond precision) so KVM and Xen
    guests may start with a more accurate wallclock time and a Xen dom0
    can maintain a more accurate wallclock for guests.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index d2c381280e3c..0db81ab511cc 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -48,10 +48,9 @@ static struct pvclock_wall_clock wall_clock;
  * have elapsed since the hypervisor wrote the data. So we try to account for
  * that with system time
  */
-static unsigned long kvm_get_wallclock(void)
+static void kvm_get_wallclock(struct timespec *now)
 {
 	struct pvclock_vcpu_time_info *vcpu_time;
-	struct timespec ts;
 	int low, high;
 	int cpu;
 
@@ -64,14 +63,12 @@ static unsigned long kvm_get_wallclock(void)
 	cpu = smp_processor_id();
 
 	vcpu_time = &hv_clock[cpu].pvti;
-	pvclock_read_wallclock(&wall_clock, vcpu_time, &ts);
+	pvclock_read_wallclock(&wall_clock, vcpu_time, now);
 
 	preempt_enable();
-
-	return ts.tv_sec;
 }
 
-static int kvm_set_wallclock(unsigned long now)
+static int kvm_set_wallclock(const struct timespec *now)
 {
 	return -1;
 }

commit ee2c25efdd46d7ed5605d6fe877bdf4b47a4ab2e
Merge: 3ab66e8a455a 6dbe51c251a3
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Mar 4 20:10:32 2013 -0300

    Merge branch 'master' into queue
    
    * master: (15791 commits)
      Linux 3.9-rc1
      btrfs/raid56: Add missing #include <linux/vmalloc.h>
      fix compat_sys_rt_sigprocmask()
      SUNRPC: One line comment fix
      ext4: enable quotas before orphan cleanup
      ext4: don't allow quota mount options when quota feature enabled
      ext4: fix a warning from sparse check for ext4_dir_llseek
      ext4: convert number of blocks to clusters properly
      ext4: fix possible memory leak in ext4_remount()
      jbd2: fix ERR_PTR dereference in jbd2__journal_start
      metag: Provide dma_get_sgtable()
      metag: prom.h: remove declaration of metag_dt_memblock_reserve()
      metag: copy devicetree to non-init memory
      metag: cleanup metag_ksyms.c includes
      metag: move mm/init.c exports out of metag_ksyms.c
      metag: move usercopy.c exports out of metag_ksyms.c
      metag: move setup.c exports out of metag_ksyms.c
      metag: move kick.c exports out of metag_ksyms.c
      metag: move traps.c exports out of metag_ksyms.c
      metag: move irq enable out of irqflags.h on SMP
      ...
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    
    Conflicts:
            arch/x86/kernel/kvmclock.c

commit fe1140cc369410a9c206fdb7aaabc644bd213dc2
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Sat Feb 23 17:05:29 2013 +0100

    x86: kvmclock: Do not setup kvmclock vsyscall in the absence of that clock
    
    This fixes boot lockups with "no-kvmclock", when the host is not
    exposing this particular feature (QEMU: -cpu ...,-kvmclock) or when
    the kvmclock initialization failed for whatever reason.
    
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 5bedbdddf1f2..b730efad6fe9 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -160,8 +160,12 @@ int kvm_register_clock(char *txt)
 {
 	int cpu = smp_processor_id();
 	int low, high, ret;
-	struct pvclock_vcpu_time_info *src = &hv_clock[cpu].pvti;
+	struct pvclock_vcpu_time_info *src;
+
+	if (!hv_clock)
+		return 0;
 
+	src = &hv_clock[cpu].pvti;
 	low = (int)__pa(src) | 1;
 	high = ((u64)__pa(src) >> 32);
 	ret = native_write_msr_safe(msr_kvm_system_time, low, high);
@@ -276,6 +280,9 @@ int __init kvm_setup_vsyscall_timeinfo(void)
 	struct pvclock_vcpu_time_info *vcpu_time;
 	unsigned int size;
 
+	if (!hv_clock)
+		return 0;
+
 	size = PAGE_ALIGN(sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS);
 
 	preempt_disable();

commit 89f883372fa60f604d136924baf3e89ff1870e9e
Merge: 9e2d59ad580d 6b73a96065e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 24 13:07:18 2013 -0800

    Merge tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Marcelo Tosatti:
     "KVM updates for the 3.9 merge window, including x86 real mode
      emulation fixes, stronger memory slot interface restrictions, mmu_lock
      spinlock hold time reduction, improved handling of large page faults
      on shadow, initial APICv HW acceleration support, s390 channel IO
      based virtio, amongst others"
    
    * tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (143 commits)
      Revert "KVM: MMU: lazily drop large spte"
      x86: pvclock kvm: align allocation size to page size
      KVM: nVMX: Remove redundant get_vmcs12 from nested_vmx_exit_handled_msr
      x86 emulator: fix parity calculation for AAD instruction
      KVM: PPC: BookE: Handle alignment interrupts
      booke: Added DBCR4 SPR number
      KVM: PPC: booke: Allow multiple exception types
      KVM: PPC: booke: use vcpu reference from thread_struct
      KVM: Remove user_alloc from struct kvm_memory_slot
      KVM: VMX: disable apicv by default
      KVM: s390: Fix handling of iscs.
      KVM: MMU: cleanup __direct_map
      KVM: MMU: remove pt_access in mmu_set_spte
      KVM: MMU: cleanup mapping-level
      KVM: MMU: lazily drop large spte
      KVM: VMX: cleanup vmx_set_cr0().
      KVM: VMX: add missing exit names to VMX_EXIT_REASONS array
      KVM: VMX: disable SMEP feature when guest is in non-paging mode
      KVM: Remove duplicate text in api.txt
      Revert "KVM: MMU: split kvm_mmu_free_page"
      ...

commit ed55705dd5008b408c48a8459b8b34b01f3de985
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Feb 18 22:58:14 2013 -0300

    x86: pvclock kvm: align allocation size to page size
    
    To match whats mapped via vsyscalls to userspace.
    
    Reported-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 220a360010f8..5bedbdddf1f2 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -218,6 +218,9 @@ static void kvm_shutdown(void)
 void __init kvmclock_init(void)
 {
 	unsigned long mem;
+	int size;
+
+	size = PAGE_ALIGN(sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS);
 
 	if (!kvm_para_available())
 		return;
@@ -231,16 +234,14 @@ void __init kvmclock_init(void)
 	printk(KERN_INFO "kvm-clock: Using msrs %x and %x",
 		msr_kvm_system_time, msr_kvm_wall_clock);
 
-	mem = memblock_alloc(sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS,
-			     PAGE_SIZE);
+	mem = memblock_alloc(size, PAGE_SIZE);
 	if (!mem)
 		return;
 	hv_clock = __va(mem);
 
 	if (kvm_register_clock("boot clock")) {
 		hv_clock = NULL;
-		memblock_free(mem,
-			sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS);
+		memblock_free(mem, size);
 		return;
 	}
 	pv_time_ops.sched_clock = kvm_clock_read;
@@ -275,7 +276,7 @@ int __init kvm_setup_vsyscall_timeinfo(void)
 	struct pvclock_vcpu_time_info *vcpu_time;
 	unsigned int size;
 
-	size = sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS;
+	size = PAGE_ALIGN(sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS);
 
 	preempt_disable();
 	cpu = smp_processor_id();

commit 5dfd486c4750c9278c63fa96e6e85bdd2fb58e9d
Author: Dave Hansen <dave@linux.vnet.ibm.com>
Date:   Tue Jan 22 13:24:35 2013 -0800

    x86, kvm: Fix kvm's use of __pa() on percpu areas
    
    In short, it is illegal to call __pa() on an address holding
    a percpu variable.  This replaces those __pa() calls with
    slow_virt_to_phys().  All of the cases in this patch are
    in boot time (or CPU hotplug time at worst) code, so the
    slow pagetable walking in slow_virt_to_phys() is not expected
    to have a performance impact.
    
    The times when this actually matters are pretty obscure
    (certain 32-bit NUMA systems), but it _does_ happen.  It is
    important to keep KVM guests working on these systems because
    the real hardware is getting harder and harder to find.
    
    This bug manifested first by me seeing a plain hang at boot
    after this message:
    
            CPU 0 irqstacks, hard=f3018000 soft=f301a000
    
    or, sometimes, it would actually make it out to the console:
    
    [    0.000000] BUG: unable to handle kernel paging request at ffffffff
    
    I eventually traced it down to the KVM async pagefault code.
    This can be worked around by disabling that code either at
    compile-time, or on the kernel command-line.
    
    The kvm async pagefault code was injecting page faults in
    to the guest which the guest misinterpreted because its
    "reason" was not being properly sent from the host.
    
    The guest passes a physical address of an per-cpu async page
    fault structure via an MSR to the host.  Since __pa() is
    broken on percpu data, the physical address it sent was
    bascially bogus and the host went scribbling on random data.
    The guest never saw the real reason for the page fault (it
    was injected by the host), assumed that the kernel had taken
    a _real_ page fault, and panic()'d.  The behavior varied,
    though, depending on what got corrupted by the bad write.
    
    Signed-off-by: Dave Hansen <dave@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20130122212435.4905663F@kernel.stglabs.ibm.com
    Acked-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 220a360010f8..9f966dc0b9e4 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -162,8 +162,8 @@ int kvm_register_clock(char *txt)
 	int low, high, ret;
 	struct pvclock_vcpu_time_info *src = &hv_clock[cpu].pvti;
 
-	low = (int)__pa(src) | 1;
-	high = ((u64)__pa(src) >> 32);
+	low = (int)slow_virt_to_phys(src) | 1;
+	high = ((u64)slow_virt_to_phys(src) >> 32);
 	ret = native_write_msr_safe(msr_kvm_system_time, low, high);
 	printk(KERN_INFO "kvm-clock: cpu %d, msr %x:%x, %s\n",
 	       cpu, high, low, txt);

commit 3dc4f7cfb7441e5e0fed3a02fc81cdaabd28300a
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Nov 27 23:28:56 2012 -0200

    x86: kvm guest: pvclock vsyscall support
    
    Hook into generic pvclock vsyscall code, with the aim to
    allow userspace to have visibility into pvclock data.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index c7d75678886e..220a360010f8 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -40,11 +40,7 @@ static int parse_no_kvmclock(char *arg)
 early_param("no-kvmclock", parse_no_kvmclock);
 
 /* The hypervisor will put information about time periodically here */
-struct pvclock_aligned_vcpu_time_info {
-	struct pvclock_vcpu_time_info clock;
-} __attribute__((__aligned__(SMP_CACHE_BYTES)));
-
-static struct pvclock_aligned_vcpu_time_info *hv_clock;
+static struct pvclock_vsyscall_time_info *hv_clock;
 static struct pvclock_wall_clock wall_clock;
 
 /*
@@ -67,7 +63,7 @@ static unsigned long kvm_get_wallclock(void)
 	preempt_disable();
 	cpu = smp_processor_id();
 
-	vcpu_time = &hv_clock[cpu].clock;
+	vcpu_time = &hv_clock[cpu].pvti;
 	pvclock_read_wallclock(&wall_clock, vcpu_time, &ts);
 
 	preempt_enable();
@@ -88,7 +84,7 @@ static cycle_t kvm_clock_read(void)
 
 	preempt_disable_notrace();
 	cpu = smp_processor_id();
-	src = &hv_clock[cpu].clock;
+	src = &hv_clock[cpu].pvti;
 	ret = pvclock_clocksource_read(src);
 	preempt_enable_notrace();
 	return ret;
@@ -116,7 +112,7 @@ static unsigned long kvm_get_tsc_khz(void)
 
 	preempt_disable();
 	cpu = smp_processor_id();
-	src = &hv_clock[cpu].clock;
+	src = &hv_clock[cpu].pvti;
 	tsc_khz = pvclock_tsc_khz(src);
 	preempt_enable();
 	return tsc_khz;
@@ -143,7 +139,7 @@ bool kvm_check_and_clear_guest_paused(void)
 	if (!hv_clock)
 		return ret;
 
-	src = &hv_clock[cpu].clock;
+	src = &hv_clock[cpu].pvti;
 	if ((src->flags & PVCLOCK_GUEST_STOPPED) != 0) {
 		src->flags &= ~PVCLOCK_GUEST_STOPPED;
 		ret = true;
@@ -164,7 +160,7 @@ int kvm_register_clock(char *txt)
 {
 	int cpu = smp_processor_id();
 	int low, high, ret;
-	struct pvclock_vcpu_time_info *src = &hv_clock[cpu].clock;
+	struct pvclock_vcpu_time_info *src = &hv_clock[cpu].pvti;
 
 	low = (int)__pa(src) | 1;
 	high = ((u64)__pa(src) >> 32);
@@ -235,7 +231,7 @@ void __init kvmclock_init(void)
 	printk(KERN_INFO "kvm-clock: Using msrs %x and %x",
 		msr_kvm_system_time, msr_kvm_wall_clock);
 
-	mem = memblock_alloc(sizeof(struct pvclock_aligned_vcpu_time_info) * NR_CPUS,
+	mem = memblock_alloc(sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS,
 			     PAGE_SIZE);
 	if (!mem)
 		return;
@@ -244,7 +240,7 @@ void __init kvmclock_init(void)
 	if (kvm_register_clock("boot clock")) {
 		hv_clock = NULL;
 		memblock_free(mem,
-			sizeof(struct pvclock_aligned_vcpu_time_info)*NR_CPUS);
+			sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS);
 		return;
 	}
 	pv_time_ops.sched_clock = kvm_clock_read;
@@ -269,3 +265,37 @@ void __init kvmclock_init(void)
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
 		pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
 }
+
+int __init kvm_setup_vsyscall_timeinfo(void)
+{
+#ifdef CONFIG_X86_64
+	int cpu;
+	int ret;
+	u8 flags;
+	struct pvclock_vcpu_time_info *vcpu_time;
+	unsigned int size;
+
+	size = sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS;
+
+	preempt_disable();
+	cpu = smp_processor_id();
+
+	vcpu_time = &hv_clock[cpu].pvti;
+	flags = pvclock_read_flags(vcpu_time);
+
+	if (!(flags & PVCLOCK_TSC_STABLE_BIT)) {
+		preempt_enable();
+		return 1;
+	}
+
+	if ((ret = pvclock_init_vsyscall(hv_clock, size))) {
+		preempt_enable();
+		return ret;
+	}
+
+	preempt_enable();
+
+	kvm_clock.archdata.vclock_mode = VCLOCK_PVCLOCK;
+#endif
+	return 0;
+}

commit 7069ed67635b8d574541af426d752cd7fbd465a6
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Nov 27 23:28:48 2012 -0200

    x86: kvmclock: allocate pvclock shared memory area
    
    We want to expose the pvclock shared memory areas, which
    the hypervisor periodically updates, to userspace.
    
    For a linear mapping from userspace, it is necessary that
    entire page sized regions are used for array of pvclock
    structures.
    
    There is no such guarantee with per cpu areas, therefore move
    to memblock_alloc based allocation.
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index f1b42b3a186c..c7d75678886e 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -23,6 +23,7 @@
 #include <asm/apic.h>
 #include <linux/percpu.h>
 #include <linux/hardirq.h>
+#include <linux/memblock.h>
 
 #include <asm/x86_init.h>
 #include <asm/reboot.h>
@@ -39,7 +40,11 @@ static int parse_no_kvmclock(char *arg)
 early_param("no-kvmclock", parse_no_kvmclock);
 
 /* The hypervisor will put information about time periodically here */
-static DEFINE_PER_CPU_SHARED_ALIGNED(struct pvclock_vcpu_time_info, hv_clock);
+struct pvclock_aligned_vcpu_time_info {
+	struct pvclock_vcpu_time_info clock;
+} __attribute__((__aligned__(SMP_CACHE_BYTES)));
+
+static struct pvclock_aligned_vcpu_time_info *hv_clock;
 static struct pvclock_wall_clock wall_clock;
 
 /*
@@ -52,15 +57,20 @@ static unsigned long kvm_get_wallclock(void)
 	struct pvclock_vcpu_time_info *vcpu_time;
 	struct timespec ts;
 	int low, high;
+	int cpu;
 
 	low = (int)__pa_symbol(&wall_clock);
 	high = ((u64)__pa_symbol(&wall_clock) >> 32);
 
 	native_write_msr(msr_kvm_wall_clock, low, high);
 
-	vcpu_time = &get_cpu_var(hv_clock);
+	preempt_disable();
+	cpu = smp_processor_id();
+
+	vcpu_time = &hv_clock[cpu].clock;
 	pvclock_read_wallclock(&wall_clock, vcpu_time, &ts);
-	put_cpu_var(hv_clock);
+
+	preempt_enable();
 
 	return ts.tv_sec;
 }
@@ -74,9 +84,11 @@ static cycle_t kvm_clock_read(void)
 {
 	struct pvclock_vcpu_time_info *src;
 	cycle_t ret;
+	int cpu;
 
 	preempt_disable_notrace();
-	src = &__get_cpu_var(hv_clock);
+	cpu = smp_processor_id();
+	src = &hv_clock[cpu].clock;
 	ret = pvclock_clocksource_read(src);
 	preempt_enable_notrace();
 	return ret;
@@ -99,8 +111,15 @@ static cycle_t kvm_clock_get_cycles(struct clocksource *cs)
 static unsigned long kvm_get_tsc_khz(void)
 {
 	struct pvclock_vcpu_time_info *src;
-	src = &per_cpu(hv_clock, 0);
-	return pvclock_tsc_khz(src);
+	int cpu;
+	unsigned long tsc_khz;
+
+	preempt_disable();
+	cpu = smp_processor_id();
+	src = &hv_clock[cpu].clock;
+	tsc_khz = pvclock_tsc_khz(src);
+	preempt_enable();
+	return tsc_khz;
 }
 
 static void kvm_get_preset_lpj(void)
@@ -119,10 +138,14 @@ bool kvm_check_and_clear_guest_paused(void)
 {
 	bool ret = false;
 	struct pvclock_vcpu_time_info *src;
+	int cpu = smp_processor_id();
+
+	if (!hv_clock)
+		return ret;
 
-	src = &__get_cpu_var(hv_clock);
+	src = &hv_clock[cpu].clock;
 	if ((src->flags & PVCLOCK_GUEST_STOPPED) != 0) {
-		__this_cpu_and(hv_clock.flags, ~PVCLOCK_GUEST_STOPPED);
+		src->flags &= ~PVCLOCK_GUEST_STOPPED;
 		ret = true;
 	}
 
@@ -141,9 +164,10 @@ int kvm_register_clock(char *txt)
 {
 	int cpu = smp_processor_id();
 	int low, high, ret;
+	struct pvclock_vcpu_time_info *src = &hv_clock[cpu].clock;
 
-	low = (int)__pa(&per_cpu(hv_clock, cpu)) | 1;
-	high = ((u64)__pa(&per_cpu(hv_clock, cpu)) >> 32);
+	low = (int)__pa(src) | 1;
+	high = ((u64)__pa(src) >> 32);
 	ret = native_write_msr_safe(msr_kvm_system_time, low, high);
 	printk(KERN_INFO "kvm-clock: cpu %d, msr %x:%x, %s\n",
 	       cpu, high, low, txt);
@@ -197,6 +221,8 @@ static void kvm_shutdown(void)
 
 void __init kvmclock_init(void)
 {
+	unsigned long mem;
+
 	if (!kvm_para_available())
 		return;
 
@@ -209,8 +235,18 @@ void __init kvmclock_init(void)
 	printk(KERN_INFO "kvm-clock: Using msrs %x and %x",
 		msr_kvm_system_time, msr_kvm_wall_clock);
 
-	if (kvm_register_clock("boot clock"))
+	mem = memblock_alloc(sizeof(struct pvclock_aligned_vcpu_time_info) * NR_CPUS,
+			     PAGE_SIZE);
+	if (!mem)
+		return;
+	hv_clock = __va(mem);
+
+	if (kvm_register_clock("boot clock")) {
+		hv_clock = NULL;
+		memblock_free(mem,
+			sizeof(struct pvclock_aligned_vcpu_time_info)*NR_CPUS);
 		return;
+	}
 	pv_time_ops.sched_clock = kvm_clock_read;
 	x86_platform.calibrate_tsc = kvm_get_tsc_khz;
 	x86_platform.get_wallclock = kvm_get_wallclock;

commit e32025a56403df4386cd61a741c0a36afe79ae8a
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Jun 11 23:18:33 2012 -0300

    x86: kvmclock: remove check_and_clear_guest_paused warning
    
    CPU offline path calls the hrtimer interrupt handler with interrupts
    disabled, without touching preempt_count, triggering this warning.
    
    Remove the warning since it is supposed to be used from hrtimer
    interrupt context only.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 086eb58c6e80..f1b42b3a186c 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -120,11 +120,6 @@ bool kvm_check_and_clear_guest_paused(void)
 	bool ret = false;
 	struct pvclock_vcpu_time_info *src;
 
-	/*
-	 * per_cpu() is safe here because this function is only called from
-	 * timer functions where preemption is already disabled.
-	 */
-	WARN_ON(!in_atomic());
 	src = &__get_cpu_var(hv_clock);
 	if ((src->flags & PVCLOCK_GUEST_STOPPED) != 0) {
 		__this_cpu_and(hv_clock.flags, ~PVCLOCK_GUEST_STOPPED);

commit 248997095d652576f1213028a95ca5fff85d089f
Author: Eric B Munson <emunson@mgebm.net>
Date:   Thu Mar 15 18:16:49 2012 -0400

    kvmclock: remove unneeded EXPORT macro
    
    check_and_clear_guest_paused does not need to be exported as it isn't used
    by any modules, remove the export.
    
    Signed-off-by: Eric B Munson <emunson@mgebm.net>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 4ba090ca689d..086eb58c6e80 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -133,7 +133,6 @@ bool kvm_check_and_clear_guest_paused(void)
 
 	return ret;
 }
-EXPORT_SYMBOL_GPL(kvm_check_and_clear_guest_paused);
 
 static struct clocksource kvm_clock = {
 	.name = "kvm-clock",

commit 3b5d56b9317fa7b5407dff1aa7b115bf6cdbd494
Author: Eric B Munson <emunson@mgebm.net>
Date:   Sat Mar 10 14:37:26 2012 -0500

    kvmclock: Add functions to check if the host has stopped the vm
    
    When a host stops or suspends a VM it will set a flag to show this.  The
    watchdog will use these functions to determine if a softlockup is real, or the
    result of a suspended VM.
    
    Signed-off-by: Eric B Munson <emunson@mgebm.net>
    asm-generic changes Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index f8492da65bfc..4ba090ca689d 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -22,6 +22,7 @@
 #include <asm/msr.h>
 #include <asm/apic.h>
 #include <linux/percpu.h>
+#include <linux/hardirq.h>
 
 #include <asm/x86_init.h>
 #include <asm/reboot.h>
@@ -114,6 +115,26 @@ static void kvm_get_preset_lpj(void)
 	preset_lpj = lpj;
 }
 
+bool kvm_check_and_clear_guest_paused(void)
+{
+	bool ret = false;
+	struct pvclock_vcpu_time_info *src;
+
+	/*
+	 * per_cpu() is safe here because this function is only called from
+	 * timer functions where preemption is already disabled.
+	 */
+	WARN_ON(!in_atomic());
+	src = &__get_cpu_var(hv_clock);
+	if ((src->flags & PVCLOCK_GUEST_STOPPED) != 0) {
+		__this_cpu_and(hv_clock.flags, ~PVCLOCK_GUEST_STOPPED);
+		ret = true;
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(kvm_check_and_clear_guest_paused);
+
 static struct clocksource kvm_clock = {
 	.name = "kvm-clock",
 	.read = kvm_clock_get_cycles,

commit b74f05d61b73af584d0c39121980171389ecfaaa
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Feb 13 11:07:27 2012 -0200

    x86: kvmclock: abstract save/restore sched_clock_state
    
    Upon resume from hibernation, CPU 0's hvclock area contains the old
    values for system_time and tsc_timestamp. It is necessary for the
    hypervisor to update these values with uptodate ones before the CPU uses
    them.
    
    Abstract TSC's save/restore sched_clock_state functions and use
    restore_state to write to KVM_SYSTEM_TIME MSR, forcing an update.
    
    Also move restore_sched_clock_state before __restore_processor_state,
    since the later calls CONFIG_LOCK_STAT's lockstat_clock (also for TSC).
    Thanks to Igor Mammedov for tracking it down.
    
    Fixes suspend-to-disk with kvmclock.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index ca4e735adc54..f8492da65bfc 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -136,6 +136,15 @@ int kvm_register_clock(char *txt)
 	return ret;
 }
 
+static void kvm_save_sched_clock_state(void)
+{
+}
+
+static void kvm_restore_sched_clock_state(void)
+{
+	kvm_register_clock("primary cpu clock, resume");
+}
+
 #ifdef CONFIG_X86_LOCAL_APIC
 static void __cpuinit kvm_setup_secondary_clock(void)
 {
@@ -195,6 +204,8 @@ void __init kvmclock_init(void)
 	x86_cpuinit.early_percpu_clock_init =
 		kvm_setup_secondary_clock;
 #endif
+	x86_platform.save_sched_clock_state = kvm_save_sched_clock_state;
+	x86_platform.restore_sched_clock_state = kvm_restore_sched_clock_state;
 	machine_ops.shutdown  = kvm_shutdown;
 #ifdef CONFIG_KEXEC
 	machine_ops.crash_shutdown  = kvm_crash_shutdown;

commit df156f90a0f90649dd38b7667901ef85478f3d2b
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Tue Feb 7 15:52:44 2012 +0100

    x86: Introduce x86_cpuinit.early_percpu_clock_init hook
    
    When kvm guest uses kvmclock, it may hang on vcpu hot-plug.
    This is caused by an overflow in pvclock_get_nsec_offset,
    
        u64 delta = tsc - shadow->tsc_timestamp;
    
    which in turn is caused by an undefined values from percpu
    hv_clock that hasn't been initialized yet.
    Uninitialized clock on being booted cpu is accessed from
       start_secondary
        -> smp_callin
          ->  smp_store_cpu_info
            -> identify_secondary_cpu
              -> mtrr_ap_init
                -> mtrr_restore
                  -> stop_machine_from_inactive_cpu
                    -> queue_stop_cpus_work
                      ...
                        -> sched_clock
                          -> kvm_clock_read
    which is well before x86_cpuinit.setup_percpu_clockev call in
    start_secondary, where percpu clock is initialized.
    
    This patch introduces a hook that allows to setup/initialize
    per_cpu clock early and avoid overflow due to reading
      - undefined values
      - old values if cpu was offlined and then onlined again
    
    Another possible early user of this clock source is ftrace that
    accesses it to get timestamps for ring buffer entries. So if
    mtrr_ap_init is moved from identify_secondary_cpu to past
    x86_cpuinit.setup_percpu_clockev in start_secondary, ftrace
    may cause the same overflow/hang on cpu hot-plug anyway.
    
    More complete description of the problem:
      https://lkml.org/lkml/2012/2/2/101
    
    Credits to Marcelo Tosatti <mtosatti@redhat.com> for hook idea.
    
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 44842d756b29..ca4e735adc54 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -144,8 +144,6 @@ static void __cpuinit kvm_setup_secondary_clock(void)
 	 * we shouldn't fail.
 	 */
 	WARN_ON(kvm_register_clock("secondary cpu clock"));
-	/* ok, done with our trickery, call native */
-	setup_secondary_APIC_clock();
 }
 #endif
 
@@ -194,7 +192,7 @@ void __init kvmclock_init(void)
 	x86_platform.get_wallclock = kvm_get_wallclock;
 	x86_platform.set_wallclock = kvm_set_wallclock;
 #ifdef CONFIG_X86_LOCAL_APIC
-	x86_cpuinit.setup_percpu_clockev =
+	x86_cpuinit.early_percpu_clock_init =
 		kvm_setup_secondary_clock;
 #endif
 	machine_ops.shutdown  = kvm_shutdown;

commit 95ef1e52922cf75b1ea2eae54ef886f2cc47eecb
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Nov 15 14:59:07 2011 +0200

    KVM guest: prevent tracing recursion with kvmclock
    
    Prevent tracing of preempt_disable() in get_cpu_var() in
    kvm_clock_read(). When CONFIG_DEBUG_PREEMPT is enabled,
    preempt_disable/enable() are traced and this causes the function_graph
    tracer to go into an infinite recursion. By open coding the
    preempt_disable() around the get_cpu_var(), we can use the notrace
    version which prevents preempt_disable/enable() from being traced and
    prevents the recursion.
    
    Based on a similar patch for Xen from Jeremy Fitzhardinge.
    
    Tested-by: Gleb Natapov <gleb@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index c1a0188e29ae..44842d756b29 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -74,9 +74,10 @@ static cycle_t kvm_clock_read(void)
 	struct pvclock_vcpu_time_info *src;
 	cycle_t ret;
 
-	src = &get_cpu_var(hv_clock);
+	preempt_disable_notrace();
+	src = &__get_cpu_var(hv_clock);
 	ret = pvclock_clocksource_read(src);
-	put_cpu_var(hv_clock);
+	preempt_enable_notrace();
 	return ret;
 }
 

commit d910f5c1064d7ff09c31b0191564f9f99e210f91
Author: Glauber Costa <glommer@redhat.com>
Date:   Mon Jul 11 15:28:19 2011 -0400

    KVM guest: KVM Steal time registration
    
    This patch implements the kvm bits of the steal time infrastructure.
    The most important part of it, is the steal time clock. It is an
    continuous clock that shows the accumulated amount of steal time
    since vcpu creation. It is supposed to survive cpu offlining/onlining.
    
    [marcelo: fix build with CONFIG_KVM_GUEST=n]
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Tested-by: Eric B Munson <emunson@mgebm.net>
    CC: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Avi Kivity <avi@redhat.com>
    CC: Anthony Liguori <aliguori@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 6389a6bca11b..c1a0188e29ae 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -160,6 +160,7 @@ static void __cpuinit kvm_setup_secondary_clock(void)
 static void kvm_crash_shutdown(struct pt_regs *regs)
 {
 	native_write_msr(msr_kvm_system_time, 0, 0);
+	kvm_disable_steal_time();
 	native_machine_crash_shutdown(regs);
 }
 #endif
@@ -167,6 +168,7 @@ static void kvm_crash_shutdown(struct pt_regs *regs)
 static void kvm_shutdown(void)
 {
 	native_write_msr(msr_kvm_system_time, 0, 0);
+	kvm_disable_steal_time();
 	native_machine_shutdown();
 }
 

commit b01cc1b0eae0dea19257b29347116505fbedf679
Author: John Stultz <johnstul@us.ibm.com>
Date:   Mon Apr 26 19:03:05 2010 -0700

    x86: Convert remaining x86 clocksources to clocksource_register_hz/khz
    
    This converts the remaining x86 clocksources to use
    clocksource_register_hz/khz.
    
    CC: jacob.jun.pan@intel.com
    CC: Glauber Costa <glommer@redhat.com>
    CC: Dimitri Sivanich <sivanich@sgi.com>
    CC: Rusty Russell <rusty@rustcorp.com.au>
    CC: Jeremy Fitzhardinge <jeremy@xensource.com>
    CC: Chris McDermott <lcm@us.ibm.com>
    CC: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com> [xen]
    Signed-off-by: John Stultz <johnstul@us.ibm.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index f98d3eafe07a..6389a6bca11b 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -26,8 +26,6 @@
 #include <asm/x86_init.h>
 #include <asm/reboot.h>
 
-#define KVM_SCALE 22
-
 static int kvmclock = 1;
 static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
@@ -120,8 +118,6 @@ static struct clocksource kvm_clock = {
 	.read = kvm_clock_get_cycles,
 	.rating = 400,
 	.mask = CLOCKSOURCE_MASK(64),
-	.mult = 1 << KVM_SCALE,
-	.shift = KVM_SCALE,
 	.flags = CLOCK_SOURCE_IS_CONTINUOUS,
 };
 
@@ -203,7 +199,7 @@ void __init kvmclock_init(void)
 	machine_ops.crash_shutdown  = kvm_crash_shutdown;
 #endif
 	kvm_get_preset_lpj();
-	clocksource_register(&kvm_clock);
+	clocksource_register_hz(&kvm_clock, NSEC_PER_SEC);
 	pv_info.paravirt_enabled = 1;
 	pv_info.name = "KVM";
 

commit ca3f10172eea9b95bbb66487656f3c3e93855702
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:49 2010 +0200

    KVM paravirt: Move kvm_smp_prepare_boot_cpu() from kvmclock.c to kvm.c.
    
    Async PF also needs to hook into smp_prepare_boot_cpu so move the hook
    into generic code.
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index ca43ce31a19c..f98d3eafe07a 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -125,7 +125,7 @@ static struct clocksource kvm_clock = {
 	.flags = CLOCK_SOURCE_IS_CONTINUOUS,
 };
 
-static int kvm_register_clock(char *txt)
+int kvm_register_clock(char *txt)
 {
 	int cpu = smp_processor_id();
 	int low, high, ret;
@@ -152,14 +152,6 @@ static void __cpuinit kvm_setup_secondary_clock(void)
 }
 #endif
 
-#ifdef CONFIG_SMP
-static void __init kvm_smp_prepare_boot_cpu(void)
-{
-	WARN_ON(kvm_register_clock("primary cpu clock"));
-	native_smp_prepare_boot_cpu();
-}
-#endif
-
 /*
  * After the clock is registered, the host will keep writing to the
  * registered memory location. If the guest happens to shutdown, this memory
@@ -205,9 +197,6 @@ void __init kvmclock_init(void)
 #ifdef CONFIG_X86_LOCAL_APIC
 	x86_cpuinit.setup_percpu_clockev =
 		kvm_setup_secondary_clock;
-#endif
-#ifdef CONFIG_SMP
-	smp_ops.smp_prepare_boot_cpu = kvm_smp_prepare_boot_cpu;
 #endif
 	machine_ops.shutdown  = kvm_shutdown;
 #ifdef CONFIG_KEXEC

commit 19b6a85b78a5d4b466c537bdbf0eaecae5e2c4e2
Author: Arjan Koers <0h61vkll2ly8@xutrox.com>
Date:   Mon Aug 2 23:35:28 2010 +0200

    KVM guest: Move a printk that's using the clock before it's ready
    
    Fix a hang during SMP kernel boot on KVM that showed up
    after commit 489fb490dbf8dab0249ad82b56688ae3842a79e8
    (2.6.35) and 59aab522154a2f17b25335b63c1cf68a51fb6ae0
    (2.6.34.1). The problem only occurs when
    CONFIG_PRINTK_TIME is set.
    
    KVM-Stable-Tag.
    Signed-off-by: Arjan Koers <0h61vkll2ly8@xutrox.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index eb9b76c716c2..ca43ce31a19c 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -128,13 +128,15 @@ static struct clocksource kvm_clock = {
 static int kvm_register_clock(char *txt)
 {
 	int cpu = smp_processor_id();
-	int low, high;
+	int low, high, ret;
+
 	low = (int)__pa(&per_cpu(hv_clock, cpu)) | 1;
 	high = ((u64)__pa(&per_cpu(hv_clock, cpu)) >> 32);
+	ret = native_write_msr_safe(msr_kvm_system_time, low, high);
 	printk(KERN_INFO "kvm-clock: cpu %d, msr %x:%x, %s\n",
 	       cpu, high, low, txt);
 
-	return native_write_msr_safe(msr_kvm_system_time, low, high);
+	return ret;
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC

commit 3a0d7256a6fb8c13f9fac6cd63250f97a8f0d8de
Author: Glauber Costa <glommer@redhat.com>
Date:   Tue May 11 12:17:45 2010 -0400

    x86, paravirt: don't compute pvclock adjustments if we trust the tsc
    
    If the HV told us we can fully trust the TSC, skip any
    correction
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Acked-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 59c740fcb9b3..eb9b76c716c2 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -215,4 +215,7 @@ void __init kvmclock_init(void)
 	clocksource_register(&kvm_clock);
 	pv_info.paravirt_enabled = 1;
 	pv_info.name = "KVM";
+
+	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
+		pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
 }

commit 838815a78785022f6611e5c48386567aea7b818b
Author: Glauber Costa <glommer@redhat.com>
Date:   Tue May 11 12:17:44 2010 -0400

    x86: KVM guest: Try using new kvm clock msrs
    
    We now added a new set of clock-related msrs in replacement of the old
    ones. In theory, we could just try to use them and get a return value
    indicating they do not exist, due to our use of kvm_write_msr_save.
    
    However, kvm clock registration happens very early, and if we ever
    try to write to a non-existant MSR, we raise a lethal #GP, since our
    idt handlers are not in place yet.
    
    So this patch tests for a cpuid feature exported by the host to
    decide which set of msrs are supported.
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Acked-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index feaeb0d3aa4f..59c740fcb9b3 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -29,6 +29,8 @@
 #define KVM_SCALE 22
 
 static int kvmclock = 1;
+static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
+static int msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
 
 static int parse_no_kvmclock(char *arg)
 {
@@ -54,7 +56,8 @@ static unsigned long kvm_get_wallclock(void)
 
 	low = (int)__pa_symbol(&wall_clock);
 	high = ((u64)__pa_symbol(&wall_clock) >> 32);
-	native_write_msr(MSR_KVM_WALL_CLOCK, low, high);
+
+	native_write_msr(msr_kvm_wall_clock, low, high);
 
 	vcpu_time = &get_cpu_var(hv_clock);
 	pvclock_read_wallclock(&wall_clock, vcpu_time, &ts);
@@ -130,7 +133,8 @@ static int kvm_register_clock(char *txt)
 	high = ((u64)__pa(&per_cpu(hv_clock, cpu)) >> 32);
 	printk(KERN_INFO "kvm-clock: cpu %d, msr %x:%x, %s\n",
 	       cpu, high, low, txt);
-	return native_write_msr_safe(MSR_KVM_SYSTEM_TIME, low, high);
+
+	return native_write_msr_safe(msr_kvm_system_time, low, high);
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC
@@ -165,14 +169,14 @@ static void __init kvm_smp_prepare_boot_cpu(void)
 #ifdef CONFIG_KEXEC
 static void kvm_crash_shutdown(struct pt_regs *regs)
 {
-	native_write_msr_safe(MSR_KVM_SYSTEM_TIME, 0, 0);
+	native_write_msr(msr_kvm_system_time, 0, 0);
 	native_machine_crash_shutdown(regs);
 }
 #endif
 
 static void kvm_shutdown(void)
 {
-	native_write_msr_safe(MSR_KVM_SYSTEM_TIME, 0, 0);
+	native_write_msr(msr_kvm_system_time, 0, 0);
 	native_machine_shutdown();
 }
 
@@ -181,27 +185,34 @@ void __init kvmclock_init(void)
 	if (!kvm_para_available())
 		return;
 
-	if (kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)) {
-		if (kvm_register_clock("boot clock"))
-			return;
-		pv_time_ops.sched_clock = kvm_clock_read;
-		x86_platform.calibrate_tsc = kvm_get_tsc_khz;
-		x86_platform.get_wallclock = kvm_get_wallclock;
-		x86_platform.set_wallclock = kvm_set_wallclock;
+	if (kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE2)) {
+		msr_kvm_system_time = MSR_KVM_SYSTEM_TIME_NEW;
+		msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
+	} else if (!(kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)))
+		return;
+
+	printk(KERN_INFO "kvm-clock: Using msrs %x and %x",
+		msr_kvm_system_time, msr_kvm_wall_clock);
+
+	if (kvm_register_clock("boot clock"))
+		return;
+	pv_time_ops.sched_clock = kvm_clock_read;
+	x86_platform.calibrate_tsc = kvm_get_tsc_khz;
+	x86_platform.get_wallclock = kvm_get_wallclock;
+	x86_platform.set_wallclock = kvm_set_wallclock;
 #ifdef CONFIG_X86_LOCAL_APIC
-		x86_cpuinit.setup_percpu_clockev =
-			kvm_setup_secondary_clock;
+	x86_cpuinit.setup_percpu_clockev =
+		kvm_setup_secondary_clock;
 #endif
 #ifdef CONFIG_SMP
-		smp_ops.smp_prepare_boot_cpu = kvm_smp_prepare_boot_cpu;
+	smp_ops.smp_prepare_boot_cpu = kvm_smp_prepare_boot_cpu;
 #endif
-		machine_ops.shutdown  = kvm_shutdown;
+	machine_ops.shutdown  = kvm_shutdown;
 #ifdef CONFIG_KEXEC
-		machine_ops.crash_shutdown  = kvm_crash_shutdown;
+	machine_ops.crash_shutdown  = kvm_crash_shutdown;
 #endif
-		kvm_get_preset_lpj();
-		clocksource_register(&kvm_clock);
-		pv_info.paravirt_enabled = 1;
-		pv_info.name = "KVM";
-	}
+	kvm_get_preset_lpj();
+	clocksource_register(&kvm_clock);
+	pv_info.paravirt_enabled = 1;
+	pv_info.name = "KVM";
 }

commit 78f28b7c555359c67c2a0d23f7436e915329421e
Merge: 3240a77b515f 7bd867dfb4e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 18 14:05:47 2009 -0700

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (38 commits)
      x86: Move get/set_wallclock to x86_platform_ops
      x86: platform: Fix section annotations
      x86: apic namespace cleanup
      x86: Distangle ioapic and i8259
      x86: Add Moorestown early detection
      x86: Add hardware_subarch ID for Moorestown
      x86: Add early platform detection
      x86: Move tsc_init to late_time_init
      x86: Move tsc_calibration to x86_init_ops
      x86: Replace the now identical time_32/64.c by time.c
      x86: time_32/64.c unify profile_pc
      x86: Move calibrate_cpu to tsc.c
      x86: Make timer setup and global variables the same in time_32/64.c
      x86: Remove mca bus ifdef from timer interrupt
      x86: Simplify timer_ack magic in time_32.c
      x86: Prepare unification of time_32/64.c
      x86: Remove do_timer hook
      x86: Add timer_init to x86_init_ops
      x86: Move percpu clockevents setup to x86_init_ops
      x86: Move xen_post_allocator_init into xen_pagetable_setup_done
      ...
    
    Fix up conflicts in arch/x86/include/asm/io_apic.h

commit 7bd867dfb4e0357e06a3211ab2bd0e714110def3
Author: Feng Tang <feng.tang@intel.com>
Date:   Thu Sep 10 10:48:56 2009 +0800

    x86: Move get/set_wallclock to x86_platform_ops
    
    get/set_wallclock() have already a set of platform dependent
    implementations (default, EFI, paravirt). MRST will add another
    variant.
    
    Moving them to platform ops simplifies the existing code and minimizes
    the effort to integrate new variants.
    
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 75a21b61b863..59ab94db12ea 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -184,10 +184,10 @@ void __init kvmclock_init(void)
 	if (kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)) {
 		if (kvm_register_clock("boot clock"))
 			return;
-		pv_time_ops.get_wallclock = kvm_get_wallclock;
-		pv_time_ops.set_wallclock = kvm_set_wallclock;
 		pv_time_ops.sched_clock = kvm_clock_read;
 		x86_platform.calibrate_tsc = kvm_get_tsc_khz;
+		x86_platform.get_wallclock = kvm_get_wallclock;
+		x86_platform.set_wallclock = kvm_set_wallclock;
 #ifdef CONFIG_X86_LOCAL_APIC
 		x86_cpuinit.setup_percpu_clockev =
 			kvm_setup_secondary_clock;

commit a20316d2aa41a8f4fd171648bad8f044f6060826
Author: Glauber Costa <glommer@redhat.com>
Date:   Mon Aug 31 03:04:31 2009 -0400

    KVM guest: fix bogus wallclock physical address calculation
    
    The use of __pa() to calculate the address of a C-visible symbol
    is wrong, and can lead to unpredictable results. See arch/x86/include/asm/page.h
    for details.
    
    It should be replaced with __pa_symbol(), that does the correct math here,
    by taking relocations into account.  This ensures the correct wallclock data
    structure physical address is passed to the hypervisor.
    
    Cc: stable@kernel.org
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 223af43f1526..e5efcdcca31b 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -50,8 +50,8 @@ static unsigned long kvm_get_wallclock(void)
 	struct timespec ts;
 	int low, high;
 
-	low = (int)__pa(&wall_clock);
-	high = ((u64)__pa(&wall_clock) >> 32);
+	low = (int)__pa_symbol(&wall_clock);
+	high = ((u64)__pa_symbol(&wall_clock) >> 32);
 	native_write_msr(MSR_KVM_WALL_CLOCK, low, high);
 
 	vcpu_time = &get_cpu_var(hv_clock);

commit 2d826404f0bdcac2a4dd7e3c446b70d6a3b63b78
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 17:06:25 2009 +0200

    x86: Move tsc_calibration to x86_init_ops
    
    TSC calibration is modified by the vmware hypervisor and paravirt by
    separate means. Moorestown wants to add its own calibration routine as
    well. So make calibrate_tsc a proper x86_init_ops function and
    override it by paravirt or by the early setup of the vmware
    hypervisor.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 64e9b5f59d2d..75a21b61b863 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -187,7 +187,7 @@ void __init kvmclock_init(void)
 		pv_time_ops.get_wallclock = kvm_get_wallclock;
 		pv_time_ops.set_wallclock = kvm_set_wallclock;
 		pv_time_ops.sched_clock = kvm_clock_read;
-		pv_time_ops.get_tsc_khz = kvm_get_tsc_khz;
+		x86_platform.calibrate_tsc = kvm_get_tsc_khz;
 #ifdef CONFIG_X86_LOCAL_APIC
 		x86_cpuinit.setup_percpu_clockev =
 			kvm_setup_secondary_clock;

commit 736decac643e8982655e22ac7f0e5e61c5b7f9bd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 19 12:35:53 2009 +0200

    x86: Move percpu clockevents setup to x86_init_ops
    
    paravirt overrides the setup of the default apic timers as per cpu
    timers. Moorestown needs to override that as well.
    
    Move it to x86_init_ops setup and create a separate x86_cpuinit struct
    which holds the function for the secondary evtl. hotplugabble CPUs.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 223af43f1526..64e9b5f59d2d 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -22,6 +22,8 @@
 #include <asm/msr.h>
 #include <asm/apic.h>
 #include <linux/percpu.h>
+
+#include <asm/x86_init.h>
 #include <asm/reboot.h>
 
 #define KVM_SCALE 22
@@ -187,7 +189,8 @@ void __init kvmclock_init(void)
 		pv_time_ops.sched_clock = kvm_clock_read;
 		pv_time_ops.get_tsc_khz = kvm_get_tsc_khz;
 #ifdef CONFIG_X86_LOCAL_APIC
-		pv_apic_ops.setup_secondary_clock = kvm_setup_secondary_clock;
+		x86_cpuinit.setup_percpu_clockev =
+			kvm_setup_secondary_clock;
 #endif
 #ifdef CONFIG_SMP
 		smp_ops.smp_prepare_boot_cpu = kvm_smp_prepare_boot_cpu;

commit 8e19608e8b5c001e4a66ce482edc474f05fb7355
Author: Magnus Damm <damm@igel.co.jp>
Date:   Tue Apr 21 12:24:00 2009 -0700

    clocksource: pass clocksource to read() callback
    
    Pass clocksource pointer to the read() callback for clocksources.  This
    allows us to share the callback between multiple instances.
    
    [hugh@veritas.com: fix powerpc build of clocksource pass clocksource mods]
    [akpm@linux-foundation.org: cleanup]
    Signed-off-by: Magnus Damm <damm@igel.co.jp>
    Acked-by: John Stultz <johnstul@us.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 137f2e8132df..223af43f1526 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -77,6 +77,11 @@ static cycle_t kvm_clock_read(void)
 	return ret;
 }
 
+static cycle_t kvm_clock_get_cycles(struct clocksource *cs)
+{
+	return kvm_clock_read();
+}
+
 /*
  * If we don't do that, there is the possibility that the guest
  * will calibrate under heavy load - thus, getting a lower lpj -
@@ -107,7 +112,7 @@ static void kvm_get_preset_lpj(void)
 
 static struct clocksource kvm_clock = {
 	.name = "kvm-clock",
-	.read = kvm_clock_read,
+	.read = kvm_clock_get_cycles,
 	.rating = 400,
 	.mask = CLOCKSOURCE_MASK(64),
 	.mult = 1 << KVM_SCALE,

commit 8e6dafd6c741cd4679b4de3c5d9698851e4fa59c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 23 00:34:39 2009 +0100

    x86: refactor x86_quirks support
    
    Impact: cleanup
    
    Make x86_quirks support more transparent. The highlevel
    methods are now named:
    
      extern void x86_quirk_pre_intr_init(void);
      extern void x86_quirk_intr_init(void);
    
      extern void x86_quirk_trap_init(void);
    
      extern void x86_quirk_pre_time_init(void);
      extern void x86_quirk_time_init(void);
    
    This makes it clear that if some platform extension has to
    do something here that it is considered ... weird, and is
    discouraged.
    
    Also remove arch_hooks.h and move it into setup.h (and other
    header files where appropriate).
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 652fce6d2cce..137f2e8132df 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -19,7 +19,6 @@
 #include <linux/clocksource.h>
 #include <linux/kvm_para.h>
 #include <asm/pvclock.h>
-#include <asm/arch_hooks.h>
 #include <asm/msr.h>
 #include <asm/apic.h>
 #include <linux/percpu.h>

commit e93353c93a3ba4215633ce930784f40a4e94e3f9
Author: Eduardo Habkost <ehabkost@redhat.com>
Date:   Fri Dec 5 18:36:45 2008 -0200

    x86: KVM guest: kvm_get_tsc_khz: return khz, not lpj
    
    kvm_get_tsc_khz() currently returns the previously-calculated preset_lpj
    value, but it is in loops-per-jiffy, not kHz. The current code works
    correctly only when HZ=1000.
    
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index b38e801014e3..652fce6d2cce 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -89,17 +89,17 @@ static cycle_t kvm_clock_read(void)
  */
 static unsigned long kvm_get_tsc_khz(void)
 {
-	return preset_lpj;
+	struct pvclock_vcpu_time_info *src;
+	src = &per_cpu(hv_clock, 0);
+	return pvclock_tsc_khz(src);
 }
 
 static void kvm_get_preset_lpj(void)
 {
-	struct pvclock_vcpu_time_info *src;
 	unsigned long khz;
 	u64 lpj;
 
-	src = &per_cpu(hv_clock, 0);
-	khz = pvclock_tsc_khz(src);
+	khz = kvm_get_tsc_khz();
 
 	lpj = ((u64)khz * 1000);
 	do_div(lpj, HZ);

commit 423cd25a5ade17b8a5cc85e6f0a0f37028d2c4a2
Author: Glauber Costa <glommer@redhat.com>
Date:   Mon Nov 24 15:45:23 2008 -0200

    x86: KVM guest: sign kvmclock as paravirt
    
    Currently, we only set the KVM paravirt signature in case
    of CONFIG_KVM_GUEST. However, it is possible to have it turned
    off, while CONFIG_KVM_CLOCK is turned on. This is also a paravirt
    case, and should be shown accordingly.
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index e169ae9b6a62..b38e801014e3 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -194,5 +194,7 @@ void __init kvmclock_init(void)
 #endif
 		kvm_get_preset_lpj();
 		clocksource_register(&kvm_clock);
+		pv_info.paravirt_enabled = 1;
+		pv_info.name = "KVM";
 	}
 }

commit 23a14b9e9db49ed5f7683857557c26c874d4abb6
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Sat Nov 22 17:37:44 2008 +0000

    kvm_setup_secondary_clock() is cpuinit
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 1c9cc431ea4f..e169ae9b6a62 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -128,7 +128,7 @@ static int kvm_register_clock(char *txt)
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC
-static void __devinit kvm_setup_secondary_clock(void)
+static void __cpuinit kvm_setup_secondary_clock(void)
 {
 	/*
 	 * Now that the first cpu already had this clocksource initialized,

commit a29a2af378f3f6362b68e126e2541c8bde885ead
Author: Rakib Mullick <rakib.mullick@gmail.com>
Date:   Wed Oct 29 14:13:39 2008 -0700

    x86: KVM guest: fix section mismatch warning in kvmclock.c
    
    WARNING: arch/x86/kernel/built-in.o(.text+0x1722c): Section mismatch
    in reference from the function kvm_setup_secondary_clock() to the
    function .devinit.text:setup_secondary_APIC_clock()
    The function kvm_setup_secondary_clock() references
    the function __devinit setup_secondary_APIC_clock().
    This is often because kvm_setup_secondary_clock lacks a __devinit
    annotation or the annotation of setup_secondary_APIC_clock is wrong.
    
    Signed-off-by: Md.Rakib H. Mullick <rakib.mullick@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 774ac4991568..1c9cc431ea4f 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -128,7 +128,7 @@ static int kvm_register_clock(char *txt)
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC
-static void kvm_setup_secondary_clock(void)
+static void __devinit kvm_setup_secondary_clock(void)
 {
 	/*
 	 * Now that the first cpu already had this clocksource initialized,

commit 0293615f3fb9886b6b23800c121be293bb7483e9
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Jul 28 11:47:53 2008 -0300

    x86: KVM guest: use paravirt function to calculate cpu khz
    
    We're currently facing timing problems in guests that do
    calibration under heavy load, and then the load vanishes.
    This means we'll have a much lower lpj than we actually should,
    and delays end up taking less time than they should, which is a
    nasty bug.
    
    Solution is to pass on the lpj value from host to guest, and have it
    preset.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index d02def06ca91..774ac4991568 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -78,6 +78,34 @@ static cycle_t kvm_clock_read(void)
 	return ret;
 }
 
+/*
+ * If we don't do that, there is the possibility that the guest
+ * will calibrate under heavy load - thus, getting a lower lpj -
+ * and execute the delays themselves without load. This is wrong,
+ * because no delay loop can finish beforehand.
+ * Any heuristics is subject to fail, because ultimately, a large
+ * poll of guests can be running and trouble each other. So we preset
+ * lpj here
+ */
+static unsigned long kvm_get_tsc_khz(void)
+{
+	return preset_lpj;
+}
+
+static void kvm_get_preset_lpj(void)
+{
+	struct pvclock_vcpu_time_info *src;
+	unsigned long khz;
+	u64 lpj;
+
+	src = &per_cpu(hv_clock, 0);
+	khz = pvclock_tsc_khz(src);
+
+	lpj = ((u64)khz * 1000);
+	do_div(lpj, HZ);
+	preset_lpj = lpj;
+}
+
 static struct clocksource kvm_clock = {
 	.name = "kvm-clock",
 	.read = kvm_clock_read,
@@ -153,6 +181,7 @@ void __init kvmclock_init(void)
 		pv_time_ops.get_wallclock = kvm_get_wallclock;
 		pv_time_ops.set_wallclock = kvm_set_wallclock;
 		pv_time_ops.sched_clock = kvm_clock_read;
+		pv_time_ops.get_tsc_khz = kvm_get_tsc_khz;
 #ifdef CONFIG_X86_LOCAL_APIC
 		pv_apic_ops.setup_secondary_clock = kvm_setup_secondary_clock;
 #endif
@@ -163,6 +192,7 @@ void __init kvmclock_init(void)
 #ifdef CONFIG_KEXEC
 		machine_ops.crash_shutdown  = kvm_crash_shutdown;
 #endif
+		kvm_get_preset_lpj();
 		clocksource_register(&kvm_clock);
 	}
 }

commit 7e37c2998a5a0b00134f6227167694b710f57ac0
Author: Adrian Bunk <bunk@kernel.org>
Date:   Tue Jul 1 01:19:19 2008 +0300

    x86: KVM guest: make kvm_smp_prepare_boot_cpu() static
    
    This patch makes the needlessly global kvm_smp_prepare_boot_cpu() static.
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 87edf1ceb1df..d02def06ca91 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -113,7 +113,7 @@ static void kvm_setup_secondary_clock(void)
 #endif
 
 #ifdef CONFIG_SMP
-void __init kvm_smp_prepare_boot_cpu(void)
+static void __init kvm_smp_prepare_boot_cpu(void)
 {
 	WARN_ON(kvm_register_clock("primary cpu clock"));
 	native_smp_prepare_boot_cpu();

commit f6e16d5ad463d15f285666f588cfe49495c692d9
Author: Gerd Hoffmann <kraxel@redhat.com>
Date:   Tue Jun 3 16:17:32 2008 +0200

    x86: KVM guest: Use the paravirt clocksource structs and functions
    
    This patch updates the kvm host code to use the pvclock structs
    and functions, thereby making it compatible with Xen.
    
    The patch also fixes an initialization bug: on SMP systems the
    per-cpu has two different locations early at boot and after CPU
    bringup.  kvmclock must take that in account when registering the
    physical address within the host.
    
    Signed-off-by: Gerd Hoffmann <kraxel@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 08a30986d472..87edf1ceb1df 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -18,6 +18,7 @@
 
 #include <linux/clocksource.h>
 #include <linux/kvm_para.h>
+#include <asm/pvclock.h>
 #include <asm/arch_hooks.h>
 #include <asm/msr.h>
 #include <asm/apic.h>
@@ -36,18 +37,9 @@ static int parse_no_kvmclock(char *arg)
 early_param("no-kvmclock", parse_no_kvmclock);
 
 /* The hypervisor will put information about time periodically here */
-static DEFINE_PER_CPU_SHARED_ALIGNED(struct kvm_vcpu_time_info, hv_clock);
-#define get_clock(cpu, field) per_cpu(hv_clock, cpu).field
+static DEFINE_PER_CPU_SHARED_ALIGNED(struct pvclock_vcpu_time_info, hv_clock);
+static struct pvclock_wall_clock wall_clock;
 
-static inline u64 kvm_get_delta(u64 last_tsc)
-{
-	int cpu = smp_processor_id();
-	u64 delta = native_read_tsc() - last_tsc;
-	return (delta * get_clock(cpu, tsc_to_system_mul)) >> KVM_SCALE;
-}
-
-static struct kvm_wall_clock wall_clock;
-static cycle_t kvm_clock_read(void);
 /*
  * The wallclock is the time of day when we booted. Since then, some time may
  * have elapsed since the hypervisor wrote the data. So we try to account for
@@ -55,64 +47,37 @@ static cycle_t kvm_clock_read(void);
  */
 static unsigned long kvm_get_wallclock(void)
 {
-	u32 wc_sec, wc_nsec;
-	u64 delta;
+	struct pvclock_vcpu_time_info *vcpu_time;
 	struct timespec ts;
-	int version, nsec;
 	int low, high;
 
 	low = (int)__pa(&wall_clock);
 	high = ((u64)__pa(&wall_clock) >> 32);
+	native_write_msr(MSR_KVM_WALL_CLOCK, low, high);
 
-	delta = kvm_clock_read();
+	vcpu_time = &get_cpu_var(hv_clock);
+	pvclock_read_wallclock(&wall_clock, vcpu_time, &ts);
+	put_cpu_var(hv_clock);
 
-	native_write_msr(MSR_KVM_WALL_CLOCK, low, high);
-	do {
-		version = wall_clock.wc_version;
-		rmb();
-		wc_sec = wall_clock.wc_sec;
-		wc_nsec = wall_clock.wc_nsec;
-		rmb();
-	} while ((wall_clock.wc_version != version) || (version & 1));
-
-	delta = kvm_clock_read() - delta;
-	delta += wc_nsec;
-	nsec = do_div(delta, NSEC_PER_SEC);
-	set_normalized_timespec(&ts, wc_sec + delta, nsec);
-	/*
-	 * Of all mechanisms of time adjustment I've tested, this one
-	 * was the champion!
-	 */
-	return ts.tv_sec + 1;
+	return ts.tv_sec;
 }
 
 static int kvm_set_wallclock(unsigned long now)
 {
-	return 0;
+	return -1;
 }
 
-/*
- * This is our read_clock function. The host puts an tsc timestamp each time
- * it updates a new time. Without the tsc adjustment, we can have a situation
- * in which a vcpu starts to run earlier (smaller system_time), but probes
- * time later (compared to another vcpu), leading to backwards time
- */
 static cycle_t kvm_clock_read(void)
 {
-	u64 last_tsc, now;
-	int cpu;
+	struct pvclock_vcpu_time_info *src;
+	cycle_t ret;
 
-	preempt_disable();
-	cpu = smp_processor_id();
-
-	last_tsc = get_clock(cpu, tsc_timestamp);
-	now = get_clock(cpu, system_time);
-
-	now += kvm_get_delta(last_tsc);
-	preempt_enable();
-
-	return now;
+	src = &get_cpu_var(hv_clock);
+	ret = pvclock_clocksource_read(src);
+	put_cpu_var(hv_clock);
+	return ret;
 }
+
 static struct clocksource kvm_clock = {
 	.name = "kvm-clock",
 	.read = kvm_clock_read,
@@ -123,13 +88,14 @@ static struct clocksource kvm_clock = {
 	.flags = CLOCK_SOURCE_IS_CONTINUOUS,
 };
 
-static int kvm_register_clock(void)
+static int kvm_register_clock(char *txt)
 {
 	int cpu = smp_processor_id();
 	int low, high;
 	low = (int)__pa(&per_cpu(hv_clock, cpu)) | 1;
 	high = ((u64)__pa(&per_cpu(hv_clock, cpu)) >> 32);
-
+	printk(KERN_INFO "kvm-clock: cpu %d, msr %x:%x, %s\n",
+	       cpu, high, low, txt);
 	return native_write_msr_safe(MSR_KVM_SYSTEM_TIME, low, high);
 }
 
@@ -140,12 +106,20 @@ static void kvm_setup_secondary_clock(void)
 	 * Now that the first cpu already had this clocksource initialized,
 	 * we shouldn't fail.
 	 */
-	WARN_ON(kvm_register_clock());
+	WARN_ON(kvm_register_clock("secondary cpu clock"));
 	/* ok, done with our trickery, call native */
 	setup_secondary_APIC_clock();
 }
 #endif
 
+#ifdef CONFIG_SMP
+void __init kvm_smp_prepare_boot_cpu(void)
+{
+	WARN_ON(kvm_register_clock("primary cpu clock"));
+	native_smp_prepare_boot_cpu();
+}
+#endif
+
 /*
  * After the clock is registered, the host will keep writing to the
  * registered memory location. If the guest happens to shutdown, this memory
@@ -174,13 +148,16 @@ void __init kvmclock_init(void)
 		return;
 
 	if (kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)) {
-		if (kvm_register_clock())
+		if (kvm_register_clock("boot clock"))
 			return;
 		pv_time_ops.get_wallclock = kvm_get_wallclock;
 		pv_time_ops.set_wallclock = kvm_set_wallclock;
 		pv_time_ops.sched_clock = kvm_clock_read;
 #ifdef CONFIG_X86_LOCAL_APIC
 		pv_apic_ops.setup_secondary_clock = kvm_setup_secondary_clock;
+#endif
+#ifdef CONFIG_SMP
+		smp_ops.smp_prepare_boot_cpu = kvm_smp_prepare_boot_cpu;
 #endif
 		machine_ops.shutdown  = kvm_shutdown;
 #ifdef CONFIG_KEXEC

commit 2ddfd20e7c55421435cbf95a5ed3dd6e423cf934
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu May 22 10:37:48 2008 +0200

    namespacecheck: automated fixes
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 4bc1be5d5472..08a30986d472 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -53,7 +53,7 @@ static cycle_t kvm_clock_read(void);
  * have elapsed since the hypervisor wrote the data. So we try to account for
  * that with system time
  */
-unsigned long kvm_get_wallclock(void)
+static unsigned long kvm_get_wallclock(void)
 {
 	u32 wc_sec, wc_nsec;
 	u64 delta;
@@ -86,7 +86,7 @@ unsigned long kvm_get_wallclock(void)
 	return ts.tv_sec + 1;
 }
 
-int kvm_set_wallclock(unsigned long now)
+static int kvm_set_wallclock(unsigned long now)
 {
 	return 0;
 }

commit b8ba5f10c5956d2b297766fda8f4f5ab8ad1e2cc
Author: Glauber Costa <gcosta@redhat.com>
Date:   Wed Apr 30 12:39:05 2008 -0300

    x86: KVM geust: make setup_secondary_clock definition dependent on local apic
    
    Since the pv_apic_ops are only present if CONFIG_X86_LOCAL_APIC is compiled
    in, kvmclock failed to build without this option.  This patch fixes this.
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index ddee04043aeb..4bc1be5d5472 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -133,6 +133,7 @@ static int kvm_register_clock(void)
 	return native_write_msr_safe(MSR_KVM_SYSTEM_TIME, low, high);
 }
 
+#ifdef CONFIG_X86_LOCAL_APIC
 static void kvm_setup_secondary_clock(void)
 {
 	/*
@@ -143,6 +144,7 @@ static void kvm_setup_secondary_clock(void)
 	/* ok, done with our trickery, call native */
 	setup_secondary_APIC_clock();
 }
+#endif
 
 /*
  * After the clock is registered, the host will keep writing to the
@@ -177,7 +179,9 @@ void __init kvmclock_init(void)
 		pv_time_ops.get_wallclock = kvm_get_wallclock;
 		pv_time_ops.set_wallclock = kvm_set_wallclock;
 		pv_time_ops.sched_clock = kvm_clock_read;
+#ifdef CONFIG_X86_LOCAL_APIC
 		pv_apic_ops.setup_secondary_clock = kvm_setup_secondary_clock;
+#endif
 		machine_ops.shutdown  = kvm_shutdown;
 #ifdef CONFIG_KEXEC
 		machine_ops.crash_shutdown  = kvm_crash_shutdown;

commit 1e977aa12dd4f80688b1f243762212e75c6d7fe8
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Mar 17 16:08:40 2008 -0300

    x86: KVM guest: disable clock before rebooting.
    
    This patch writes 0 (actually, what really matters is that the
    LSB is cleared) to the system time msr before shutting down
    the machine for kexec.
    
    Without it, we can have a random memory location being written
    when the guest comes back
    
    It overrides the functions shutdown, used in the path of kernel_kexec() (sys.c)
    and crash_shutdown, used in the path of crash_kexec() (kexec.c)
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index b999f5e5b3bf..ddee04043aeb 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -22,6 +22,7 @@
 #include <asm/msr.h>
 #include <asm/apic.h>
 #include <linux/percpu.h>
+#include <asm/reboot.h>
 
 #define KVM_SCALE 22
 
@@ -143,6 +144,28 @@ static void kvm_setup_secondary_clock(void)
 	setup_secondary_APIC_clock();
 }
 
+/*
+ * After the clock is registered, the host will keep writing to the
+ * registered memory location. If the guest happens to shutdown, this memory
+ * won't be valid. In cases like kexec, in which you install a new kernel, this
+ * means a random memory location will be kept being written. So before any
+ * kind of shutdown from our side, we unregister the clock by writting anything
+ * that does not have the 'enable' bit set in the msr
+ */
+#ifdef CONFIG_KEXEC
+static void kvm_crash_shutdown(struct pt_regs *regs)
+{
+	native_write_msr_safe(MSR_KVM_SYSTEM_TIME, 0, 0);
+	native_machine_crash_shutdown(regs);
+}
+#endif
+
+static void kvm_shutdown(void)
+{
+	native_write_msr_safe(MSR_KVM_SYSTEM_TIME, 0, 0);
+	native_machine_shutdown();
+}
+
 void __init kvmclock_init(void)
 {
 	if (!kvm_para_available())
@@ -155,6 +178,10 @@ void __init kvmclock_init(void)
 		pv_time_ops.set_wallclock = kvm_set_wallclock;
 		pv_time_ops.sched_clock = kvm_clock_read;
 		pv_apic_ops.setup_secondary_clock = kvm_setup_secondary_clock;
+		machine_ops.shutdown  = kvm_shutdown;
+#ifdef CONFIG_KEXEC
+		machine_ops.crash_shutdown  = kvm_crash_shutdown;
+#endif
 		clocksource_register(&kvm_clock);
 	}
 }

commit 790c73f6289a204f858ffdcbe4a2b38e91657ec6
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Fri Feb 15 17:52:48 2008 -0200

    x86: KVM guest: paravirtualized clocksource
    
    This is the guest part of kvm clock implementation
    It does not do tsc-only timing, as tsc can have deltas
    between cpus, and it did not seem worthy to me to keep
    adjusting them.
    
    We do use it, however, for fine-grained adjustment.
    
    Other than that, time comes from the host.
    
    [randy dunlap: add missing include]
    [randy dunlap: disallow on Voyager or Visual WS]
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
new file mode 100644
index 000000000000..b999f5e5b3bf
--- /dev/null
+++ b/arch/x86/kernel/kvmclock.c
@@ -0,0 +1,160 @@
+/*  KVM paravirtual clock driver. A clocksource implementation
+    Copyright (C) 2008 Glauber de Oliveira Costa, Red Hat Inc.
+
+    This program is free software; you can redistribute it and/or modify
+    it under the terms of the GNU General Public License as published by
+    the Free Software Foundation; either version 2 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU General Public License for more details.
+
+    You should have received a copy of the GNU General Public License
+    along with this program; if not, write to the Free Software
+    Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+*/
+
+#include <linux/clocksource.h>
+#include <linux/kvm_para.h>
+#include <asm/arch_hooks.h>
+#include <asm/msr.h>
+#include <asm/apic.h>
+#include <linux/percpu.h>
+
+#define KVM_SCALE 22
+
+static int kvmclock = 1;
+
+static int parse_no_kvmclock(char *arg)
+{
+	kvmclock = 0;
+	return 0;
+}
+early_param("no-kvmclock", parse_no_kvmclock);
+
+/* The hypervisor will put information about time periodically here */
+static DEFINE_PER_CPU_SHARED_ALIGNED(struct kvm_vcpu_time_info, hv_clock);
+#define get_clock(cpu, field) per_cpu(hv_clock, cpu).field
+
+static inline u64 kvm_get_delta(u64 last_tsc)
+{
+	int cpu = smp_processor_id();
+	u64 delta = native_read_tsc() - last_tsc;
+	return (delta * get_clock(cpu, tsc_to_system_mul)) >> KVM_SCALE;
+}
+
+static struct kvm_wall_clock wall_clock;
+static cycle_t kvm_clock_read(void);
+/*
+ * The wallclock is the time of day when we booted. Since then, some time may
+ * have elapsed since the hypervisor wrote the data. So we try to account for
+ * that with system time
+ */
+unsigned long kvm_get_wallclock(void)
+{
+	u32 wc_sec, wc_nsec;
+	u64 delta;
+	struct timespec ts;
+	int version, nsec;
+	int low, high;
+
+	low = (int)__pa(&wall_clock);
+	high = ((u64)__pa(&wall_clock) >> 32);
+
+	delta = kvm_clock_read();
+
+	native_write_msr(MSR_KVM_WALL_CLOCK, low, high);
+	do {
+		version = wall_clock.wc_version;
+		rmb();
+		wc_sec = wall_clock.wc_sec;
+		wc_nsec = wall_clock.wc_nsec;
+		rmb();
+	} while ((wall_clock.wc_version != version) || (version & 1));
+
+	delta = kvm_clock_read() - delta;
+	delta += wc_nsec;
+	nsec = do_div(delta, NSEC_PER_SEC);
+	set_normalized_timespec(&ts, wc_sec + delta, nsec);
+	/*
+	 * Of all mechanisms of time adjustment I've tested, this one
+	 * was the champion!
+	 */
+	return ts.tv_sec + 1;
+}
+
+int kvm_set_wallclock(unsigned long now)
+{
+	return 0;
+}
+
+/*
+ * This is our read_clock function. The host puts an tsc timestamp each time
+ * it updates a new time. Without the tsc adjustment, we can have a situation
+ * in which a vcpu starts to run earlier (smaller system_time), but probes
+ * time later (compared to another vcpu), leading to backwards time
+ */
+static cycle_t kvm_clock_read(void)
+{
+	u64 last_tsc, now;
+	int cpu;
+
+	preempt_disable();
+	cpu = smp_processor_id();
+
+	last_tsc = get_clock(cpu, tsc_timestamp);
+	now = get_clock(cpu, system_time);
+
+	now += kvm_get_delta(last_tsc);
+	preempt_enable();
+
+	return now;
+}
+static struct clocksource kvm_clock = {
+	.name = "kvm-clock",
+	.read = kvm_clock_read,
+	.rating = 400,
+	.mask = CLOCKSOURCE_MASK(64),
+	.mult = 1 << KVM_SCALE,
+	.shift = KVM_SCALE,
+	.flags = CLOCK_SOURCE_IS_CONTINUOUS,
+};
+
+static int kvm_register_clock(void)
+{
+	int cpu = smp_processor_id();
+	int low, high;
+	low = (int)__pa(&per_cpu(hv_clock, cpu)) | 1;
+	high = ((u64)__pa(&per_cpu(hv_clock, cpu)) >> 32);
+
+	return native_write_msr_safe(MSR_KVM_SYSTEM_TIME, low, high);
+}
+
+static void kvm_setup_secondary_clock(void)
+{
+	/*
+	 * Now that the first cpu already had this clocksource initialized,
+	 * we shouldn't fail.
+	 */
+	WARN_ON(kvm_register_clock());
+	/* ok, done with our trickery, call native */
+	setup_secondary_APIC_clock();
+}
+
+void __init kvmclock_init(void)
+{
+	if (!kvm_para_available())
+		return;
+
+	if (kvmclock && kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)) {
+		if (kvm_register_clock())
+			return;
+		pv_time_ops.get_wallclock = kvm_get_wallclock;
+		pv_time_ops.set_wallclock = kvm_set_wallclock;
+		pv_time_ops.sched_clock = kvm_clock_read;
+		pv_apic_ops.setup_secondary_clock = kvm_setup_secondary_clock;
+		clocksource_register(&kvm_clock);
+	}
+}
