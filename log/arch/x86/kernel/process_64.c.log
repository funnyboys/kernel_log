commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 0c169a5687e1..9a97415b2139 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -40,7 +40,6 @@
 #include <linux/ftrace.h>
 #include <linux/syscalls.h>
 
-#include <asm/pgtable.h>
 #include <asm/processor.h>
 #include <asm/fpu/internal.h>
 #include <asm/mmu_context.h>

commit 8dd97c65185c5a63c668e5bd8a861c04f47a35ed
Author: Reinette Chatre <reinette.chatre@intel.com>
Date:   Tue May 5 15:36:12 2020 -0700

    x86/resctrl: Rename asm/resctrl_sched.h to asm/resctrl.h
    
    asm/resctrl_sched.h is dedicated to the code used for configuration
    of the CPU resource control state when a task is scheduled.
    
    Rename resctrl_sched.h to resctrl.h in preparation of additions that
    will no longer make this file dedicated to work done during scheduling.
    
    No functional change.
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/6914e0ef880b539a82a6d889f9423496d471ad1d.1588715690.git.reinette.chatre@intel.com

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 5ef9d8f25b0e..0c169a5687e1 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -52,7 +52,7 @@
 #include <asm/switch_to.h>
 #include <asm/xen/hypervisor.h>
 #include <asm/vdso.h>
-#include <asm/resctrl_sched.h>
+#include <asm/resctrl.h>
 #include <asm/unistd.h>
 #include <asm/fsgsbase.h>
 #ifdef CONFIG_IA32_EMULATION

commit ffd75b373f3656cbb593c5221cc36ce232b7bbc1
Author: Brian Gerst <brgerst@gmail.com>
Date:   Fri Mar 13 15:51:44 2020 -0400

    x86: Remove unneeded includes
    
    Clean up includes of and in <asm/syscalls.h>
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200313195144.164260-19-brgerst@gmail.com

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ffd497804dbc..5ef9d8f25b0e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -48,7 +48,6 @@
 #include <asm/desc.h>
 #include <asm/proto.h>
 #include <asm/ia32.h>
-#include <asm/syscalls.h>
 #include <asm/debugreg.h>
 #include <asm/switch_to.h>
 #include <asm/xen/hypervisor.h>

commit 2b10906f2d25515bba58070b8183babc89063597
Author: Brian Gerst <brgerst@gmail.com>
Date:   Thu Dec 19 06:58:12 2019 -0500

    x86: Remove force_iret()
    
    force_iret() was originally intended to prevent the return to user mode with
    the SYSRET or SYSEXIT instructions, in cases where the register state could
    have been changed to be incompatible with those instructions.  The entry code
    has been significantly reworked since then, and register state is validated
    before SYSRET or SYSEXIT are used.  force_iret() no longer serves its original
    purpose and can be eliminated.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Link: https://lkml.kernel.org/r/20191219115812.102620-1-brgerst@gmail.com

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 506d66830d4d..ffd497804dbc 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -394,7 +394,6 @@ start_thread_common(struct pt_regs *regs, unsigned long new_ip,
 	regs->cs		= _cs;
 	regs->ss		= _ss;
 	regs->flags		= X86_EFLAGS_IF;
-	force_iret();
 }
 
 void

commit a24ca9976843156eabbc5f2d798954b5674d1b61
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:29 2019 +0100

    x86/iopl: Remove legacy IOPL option
    
    The IOPL emulation via the I/O bitmap is sufficient. Remove the legacy
    cruft dealing with the (e)flags based IOPL mechanism.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Juergen Gross <jgross@suse.com> (Paravirt and Xen parts)
    Acked-by: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e93a1b8fd7f9..506d66830d4d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -497,17 +497,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 
 	switch_to_extra(prev_p, next_p);
 
-#ifdef CONFIG_XEN_PV
-	/*
-	 * On Xen PV, IOPL bits in pt_regs->flags have no effect, and
-	 * current_pt_regs()->flags may not match the current task's
-	 * intended IOPL.  We need to switch it manually.
-	 */
-	if (unlikely(static_cpu_has(X86_FEATURE_XENPV) &&
-		     prev->iopl != next->iopl))
-		xen_set_iopl_mask(next->iopl);
-#endif
-
 	if (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {
 		/*
 		 * AMD CPUs have a misfeature: SYSRET sets the SS selector but

commit 2fff071d28b54f050f62654dad4ec111b8416d8e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:16 2019 +0100

    x86/process: Unify copy_thread_tls()
    
    While looking at the TSS io bitmap it turned out that any change in that
    area would require identical changes to copy_thread_tls(). The 32 and 64
    bit variants share sufficient code to consolidate them into a common
    function to avoid duplication of upcoming modifications.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index af64519b2695..e93a1b8fd7f9 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -371,81 +371,6 @@ void x86_gsbase_write_task(struct task_struct *task, unsigned long gsbase)
 	task->thread.gsbase = gsbase;
 }
 
-int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
-		unsigned long arg, struct task_struct *p, unsigned long tls)
-{
-	int err;
-	struct pt_regs *childregs;
-	struct fork_frame *fork_frame;
-	struct inactive_task_frame *frame;
-	struct task_struct *me = current;
-
-	childregs = task_pt_regs(p);
-	fork_frame = container_of(childregs, struct fork_frame, regs);
-	frame = &fork_frame->frame;
-
-	frame->bp = 0;
-	frame->ret_addr = (unsigned long) ret_from_fork;
-	p->thread.sp = (unsigned long) fork_frame;
-	p->thread.io_bitmap_ptr = NULL;
-
-	savesegment(gs, p->thread.gsindex);
-	p->thread.gsbase = p->thread.gsindex ? 0 : me->thread.gsbase;
-	savesegment(fs, p->thread.fsindex);
-	p->thread.fsbase = p->thread.fsindex ? 0 : me->thread.fsbase;
-	savesegment(es, p->thread.es);
-	savesegment(ds, p->thread.ds);
-	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));
-
-	if (unlikely(p->flags & PF_KTHREAD)) {
-		/* kernel thread */
-		memset(childregs, 0, sizeof(struct pt_regs));
-		frame->bx = sp;		/* function */
-		frame->r12 = arg;
-		return 0;
-	}
-	frame->bx = 0;
-	*childregs = *current_pt_regs();
-
-	childregs->ax = 0;
-	if (sp)
-		childregs->sp = sp;
-
-	err = -ENOMEM;
-	if (unlikely(test_tsk_thread_flag(me, TIF_IO_BITMAP))) {
-		p->thread.io_bitmap_ptr = kmemdup(me->thread.io_bitmap_ptr,
-						  IO_BITMAP_BYTES, GFP_KERNEL);
-		if (!p->thread.io_bitmap_ptr) {
-			p->thread.io_bitmap_max = 0;
-			return -ENOMEM;
-		}
-		set_tsk_thread_flag(p, TIF_IO_BITMAP);
-	}
-
-	/*
-	 * Set a new TLS for the child thread?
-	 */
-	if (clone_flags & CLONE_SETTLS) {
-#ifdef CONFIG_IA32_EMULATION
-		if (in_ia32_syscall())
-			err = do_set_thread_area(p, -1,
-				(struct user_desc __user *)tls, 0);
-		else
-#endif
-			err = do_arch_prctl_64(p, ARCH_SET_FS, tls);
-		if (err)
-			goto out;
-	}
-	err = 0;
-out:
-	if (err && p->thread.io_bitmap_ptr) {
-		kfree(p->thread.io_bitmap_ptr);
-		p->thread.io_bitmap_max = 0;
-	}
-
-	return err;
-}
-
 static void
 start_thread_common(struct pt_regs *regs, unsigned long new_ip,
 		    unsigned long new_sp,

commit 50e04acf2990d0d93983720b0a85b11ef805df60
Author: Jann Horn <jannh@google.com>
Date:   Sat Jul 13 00:41:52 2019 +0200

    x86/process: Delete useless check for dead process with LDT
    
    At release_thread(), ->mm is NULL; and it is fine for the former mm to
    still have an LDT. Delete this check in process_64.c, similar to
    commit 2684927c6b93 ("[PATCH] x86: Deprecate useless bug"), which did the
    same in process_32.c.
    
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190712224152.13129-1-jannh@google.com

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 250e4c4ac6d9..af64519b2695 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -143,17 +143,7 @@ void __show_regs(struct pt_regs *regs, enum show_regs_mode mode)
 
 void release_thread(struct task_struct *dead_task)
 {
-	if (dead_task->mm) {
-#ifdef CONFIG_MODIFY_LDT_SYSCALL
-		if (dead_task->mm->context.ldt) {
-			pr_warn("WARNING: dead process %s still has LDT? <%p/%d>\n",
-				dead_task->comm,
-				dead_task->mm->context.ldt->entries,
-				dead_task->mm->context.ldt->nr_entries);
-			BUG();
-		}
-#endif
-	}
+	WARN_ON(dead_task->mm);
 }
 
 enum which_selector {

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index f8e1af380cdf..250e4c4ac6d9 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  Copyright (C) 1995  Linus Torvalds
  *

commit 8ff468c29e9a9c3afe9152c10c7b141343270bf3
Merge: 68253e718c27 d9c9ce34ed5c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 7 10:24:10 2019 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU state handling updates from Borislav Petkov:
     "This contains work started by Rik van Riel and brought to fruition by
      Sebastian Andrzej Siewior with the main goal to optimize when to load
      FPU registers: only when returning to userspace and not on every
      context switch (while the task remains in the kernel).
    
      In addition, this optimization makes kernel_fpu_begin() cheaper by
      requiring registers saving only on the first invocation and skipping
      that in following ones.
    
      What is more, this series cleans up and streamlines many aspects of
      the already complex FPU code, hopefully making it more palatable for
      future improvements and simplifications.
    
      Finally, there's a __user annotations fix from Jann Horn"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (29 commits)
      x86/fpu: Fault-in user stack if copy_fpstate_to_sigframe() fails
      x86/pkeys: Add PKRU value to init_fpstate
      x86/fpu: Restore regs in copy_fpstate_to_sigframe() in order to use the fastpath
      x86/fpu: Add a fastpath to copy_fpstate_to_sigframe()
      x86/fpu: Add a fastpath to __fpu__restore_sig()
      x86/fpu: Defer FPU state load until return to userspace
      x86/fpu: Merge the two code paths in __fpu__restore_sig()
      x86/fpu: Restore from kernel memory on the 64-bit path too
      x86/fpu: Inline copy_user_to_fpregs_zeroing()
      x86/fpu: Update xstate's PKRU value on write_pkru()
      x86/fpu: Prepare copy_fpstate_to_sigframe() for TIF_NEED_FPU_LOAD
      x86/fpu: Always store the registers in copy_fpstate_to_sigframe()
      x86/entry: Add TIF_NEED_FPU_LOAD
      x86/fpu: Eager switch PKRU state
      x86/pkeys: Don't check if PKRU is zero before writing it
      x86/fpu: Only write PKRU if it is different from current
      x86/pkeys: Provide *pkru() helpers
      x86/fpu: Use a feature number instead of mask in two more helpers
      x86/fpu: Make __raw_xsave_addr() use a feature number instead of mask
      x86/fpu: Add an __fpregs_load_activate() internal helper
      ...

commit 5f409e20b794565e2d60ad333e79334630a6c798
Author: Rik van Riel <riel@surriel.com>
Date:   Wed Apr 3 18:41:52 2019 +0200

    x86/fpu: Defer FPU state load until return to userspace
    
    Defer loading of FPU state until return to userspace. This gives
    the kernel the potential to skip loading FPU state for tasks that
    stay in kernel mode, or for tasks that end up with repeated
    invocations of kernel_fpu_begin() & kernel_fpu_end().
    
    The fpregs_lock/unlock() section ensures that the registers remain
    unchanged. Otherwise a context switch or a bottom half could save the
    registers to its FPU context and the processor's FPU registers would
    became random if modified at the same time.
    
    KVM swaps the host/guest registers on entry/exit path. This flow has
    been kept as is. First it ensures that the registers are loaded and then
    saves the current (host) state before it loads the guest's registers. The
    swap is done at the very end with disabled interrupts so it should not
    change anymore before theg guest is entered. The read/save version seems
    to be cheaper compared to memcpy() in a micro benchmark.
    
    Each thread gets TIF_NEED_FPU_LOAD set as part of fork() / fpu__copy().
    For kernel threads, this flag gets never cleared which avoids saving /
    restoring the FPU state for kernel threads and during in-kernel usage of
    the FPU registers.
    
     [
       bp: Correct and update commit message and fix checkpatch warnings.
       s/register/registers/ where it is used in plural.
       minor comment corrections.
       remove unused trace_x86_fpu_activate_state() TP.
     ]
    
    Signed-off-by: Rik van Riel <riel@surriel.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Babu Moger <Babu.Moger@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Waiman Long <longman@redhat.com>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yi Wang <wang.yi59@zte.com.cn>
    Link: https://lkml.kernel.org/r/20190403164156.19645-24-bigeasy@linutronix.de

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ffea7c557963..37b2ecef041e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -520,7 +520,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	WARN_ON_ONCE(IS_ENABLED(CONFIG_DEBUG_ENTRY) &&
 		     this_cpu_read(irq_count) != -1);
 
-	switch_fpu_prepare(prev_fpu, cpu);
+	if (!test_thread_flag(TIF_NEED_FPU_LOAD))
+		switch_fpu_prepare(prev_fpu, cpu);
 
 	/* We must save %fs and %gs before load_TLS() because
 	 * %fs and %gs may be cleared by load_TLS().
@@ -572,7 +573,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	this_cpu_write(current_task, next_p);
 	this_cpu_write(cpu_current_top_of_stack, task_top_of_stack(next_p));
 
-	switch_fpu_finish(next_fpu, cpu);
+	switch_fpu_finish(next_fpu);
 
 	/* Reload sp0. */
 	update_task_stack(next_p);

commit 2722146eb78451b30e4717a267a3a2b44e4ad317
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 3 18:41:36 2019 +0200

    x86/fpu: Remove fpu->initialized
    
    The struct fpu.initialized member is always set to one for user tasks
    and zero for kernel tasks. This avoids saving/restoring the FPU
    registers for kernel threads.
    
    The ->initialized = 0 case for user tasks has been removed in previous
    changes, for instance, by doing an explicit unconditional init at fork()
    time for FPU-less systems which was otherwise delayed until the emulated
    opcode.
    
    The context switch code (switch_fpu_prepare() + switch_fpu_finish())
    can't unconditionally save/restore registers for kernel threads. Not
    only would it slow down the switch but also load a zeroed xcomp_bv for
    XSAVES.
    
    For kernel_fpu_begin() (+end) the situation is similar: EFI with runtime
    services uses this before alternatives_patched is true. Which means that
    this function is used too early and it wasn't the case before.
    
    For those two cases, use current->mm to distinguish between user and
    kernel thread. For kernel_fpu_begin() skip save/restore of the FPU
    registers.
    
    During the context switch into a kernel thread don't do anything. There
    is no reason to save the FPU state of a kernel thread.
    
    The reordering in __switch_to() is important because the current()
    pointer needs to be valid before switch_fpu_finish() is invoked so ->mm
    is seen of the new task instead the old one.
    
    N.B.: fpu__save() doesn't need to check ->mm because it is called by
    user tasks only.
    
     [ bp: Massage. ]
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Babu Moger <Babu.Moger@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-8-bigeasy@linutronix.de

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e1983b3a16c4..ffea7c557963 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -566,14 +566,14 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 
 	x86_fsgsbase_load(prev, next);
 
-	switch_fpu_finish(next_fpu, cpu);
-
 	/*
 	 * Switch the PDA and FPU contexts.
 	 */
 	this_cpu_write(current_task, next_p);
 	this_cpu_write(cpu_current_top_of_stack, task_top_of_stack(next_p));
 
+	switch_fpu_finish(next_fpu, cpu);
+
 	/* Reload sp0. */
 	update_task_stack(next_p);
 

commit 6dd677a044e606fd343e31c2108b13d74aec1ca5
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 3 18:41:31 2019 +0200

    x86/fpu: Remove fpu__restore()
    
    There are no users of fpu__restore() so it is time to remove it. The
    comment regarding fpu__restore() and TS bit is stale since commit
    
      b3b0870ef3ffe ("i387: do not preload FPU state at task switch time")
    
    and has no meaning since.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Babu Moger <Babu.Moger@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: linux-doc@vger.kernel.org
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-3-bigeasy@linutronix.de

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6a62f4af9fcf..e1983b3a16c4 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -538,9 +538,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/*
 	 * Leave lazy mode, flushing any hypercalls made here.  This
 	 * must be done after loading TLS entries in the GDT but before
-	 * loading segments that might reference them, and and it must
-	 * be done before fpu__restore(), so the TS bit is up to
-	 * date.
+	 * loading segments that might reference them.
 	 */
 	arch_end_context_switch(next_p);
 

commit 64604d54d3115fee89598bfb6d8d2252f8a2d114
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Mar 19 11:35:46 2019 +0100

    sched/x86_64: Don't save flags on context switch
    
    Now that we have objtool validating AC=1 state for all x86_64 code,
    we can once again guarantee clean flags on schedule.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 026a43be9bd1..844a28b29967 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -393,13 +393,6 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 	fork_frame = container_of(childregs, struct fork_frame, regs);
 	frame = &fork_frame->frame;
 
-	/*
-	 * For a new task use the RESET flags value since there is no before.
-	 * All the status flags are zero; DF and all the system flags must also
-	 * be 0, specifically IF must be 0 because we context switch to the new
-	 * task with interrupts disabled.
-	 */
-	frame->flags = X86_EFLAGS_FIXED;
 	frame->bp = 0;
 	frame->ret_addr = (unsigned long) ret_from_fork;
 	p->thread.sp = (unsigned long) fork_frame;

commit 6690e86be83ac75832e461c141055b5d601c0a6d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Feb 14 10:30:52 2019 +0100

    sched/x86: Save [ER]FLAGS on context switch
    
    Effectively reverts commit:
    
      2c7577a75837 ("sched/x86_64: Don't save flags on context switch")
    
    Specifically because SMAP uses FLAGS.AC which invalidates the claim
    that the kernel has clean flags.
    
    In particular; while preemption from interrupt return is fine (the
    IRET frame on the exception stack contains FLAGS) it breaks any code
    that does synchonous scheduling, including preempt_enable().
    
    This has become a significant issue ever since commit:
    
      5b24a7a2aa20 ("Add 'unsafe' user access functions for batched accesses")
    
    provided for means of having 'normal' C code between STAC / CLAC,
    exposing the FLAGS.AC state. So far this hasn't led to trouble,
    however fix it before it comes apart.
    
    Reported-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@kernel.org
    Fixes: 5b24a7a2aa20 ("Add 'unsafe' user access functions for batched accesses")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6a62f4af9fcf..026a43be9bd1 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -392,6 +392,14 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 	childregs = task_pt_regs(p);
 	fork_frame = container_of(childregs, struct fork_frame, regs);
 	frame = &fork_frame->frame;
+
+	/*
+	 * For a new task use the RESET flags value since there is no before.
+	 * All the status flags are zero; DF and all the system flags must also
+	 * be 0, specifically IF must be 0 because we context switch to the new
+	 * task with interrupts disabled.
+	 */
+	frame->flags = X86_EFLAGS_FIXED;
 	frame->bp = 0;
 	frame->ret_addr = (unsigned long) ret_from_fork;
 	p->thread.sp = (unsigned long) fork_frame;

commit e57d9f638af9673f38d9f09de66fa0a28303127d
Merge: d6e867a6ae13 6848ac7ca39a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 26 18:08:18 2018 -0800

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Update and clean up x86 fault handling, by Andy Lutomirski.
    
       - Drop usage of __flush_tlb_all() in kernel_physical_mapping_init()
         and related fallout, by Dan Williams.
    
       - CPA cleanups and reorganization by Peter Zijlstra: simplify the
         flow and remove a few warts.
    
       - Other misc cleanups"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (29 commits)
      x86/mm/dump_pagetables: Use DEFINE_SHOW_ATTRIBUTE()
      x86/mm/cpa: Rename @addrinarray to @numpages
      x86/mm/cpa: Better use CLFLUSHOPT
      x86/mm/cpa: Fold cpa_flush_range() and cpa_flush_array() into a single cpa_flush() function
      x86/mm/cpa: Make cpa_data::numpages invariant
      x86/mm/cpa: Optimize cpa_flush_array() TLB invalidation
      x86/mm/cpa: Simplify the code after making cpa->vaddr invariant
      x86/mm/cpa: Make cpa_data::vaddr invariant
      x86/mm/cpa: Add __cpa_addr() helper
      x86/mm/cpa: Add ARRAY and PAGES_ARRAY selftests
      x86/mm: Drop usage of __flush_tlb_all() in kernel_physical_mapping_init()
      x86/mm: Validate kernel_physical_mapping_init() PTE population
      generic/pgtable: Introduce set_pte_safe()
      generic/pgtable: Introduce {p4d,pgd}_same()
      generic/pgtable: Make {pmd, pud}_same() unconditionally available
      x86/fault: Clean up the page fault oops decoder a bit
      x86/fault: Decode page fault OOPSes better
      x86/vsyscall/64: Use X86_PF constants in the simulated #PF error code
      x86/oops: Show the correct CS value in show_regs()
      x86/fault: Don't try to recover from an implicit supervisor access
      ...

commit 312a466155108329c458049dc76a21ad56106960
Merge: 6e54df001ac9 4b1bacab61aa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 26 17:03:51 2018 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc cleanups"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/kprobes: Remove trampoline_handler() prototype
      x86/kernel: Fix more -Wmissing-prototypes warnings
      x86: Fix various typos in comments
      x86/headers: Fix -Wmissing-prototypes warning
      x86/process: Avoid unnecessary NULL check in get_wchan()
      x86/traps: Complete prototype declarations
      x86/mce: Fix -Wmissing-prototypes warnings
      x86/gart: Rewrite early_gart_iommu_check() comment

commit a52fb43a5faa40507cb164a793a7fa08da863ac7
Merge: 42b00f122cfb 52eb74339a62
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 26 12:17:43 2018 -0800

    Merge branch 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cache control updates from Borislav Petkov:
    
     - The generalization of the RDT code to accommodate the addition of
       AMD's very similar implementation of the cache monitoring feature.
    
       This entails a subsystem move into a separate and generic
       arch/x86/kernel/cpu/resctrl/ directory along with adding
       vendor-specific initialization and feature detection helpers.
    
       Ontop of that is the unification of user-visible strings, both in the
       resctrl filesystem error handling and Kconfig.
    
       Provided by Babu Moger and Sherry Hurwitz.
    
     - Code simplifications and error handling improvements by Reinette
       Chatre.
    
    * 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/resctrl: Fix rdt_find_domain() return value and checks
      x86/resctrl: Remove unnecessary check for cbm_validate()
      x86/resctrl: Use rdt_last_cmd_puts() where possible
      MAINTAINERS: Update resctrl filename patterns
      Documentation: Rename and update intel_rdt_ui.txt to resctrl_ui.txt
      x86/resctrl: Introduce AMD QOS feature
      x86/resctrl: Fixup the user-visible strings
      x86/resctrl: Add AMD's X86_FEATURE_MBA to the scattered CPUID features
      x86/resctrl: Rename the config option INTEL_RDT to RESCTRL
      x86/resctrl: Add vendor check for the MBA software controller
      x86/resctrl: Bring cbm_validate() into the resource structure
      x86/resctrl: Initialize the vendor-specific resource functions
      x86/resctrl: Move all the macros to resctrl/internal.h
      x86/resctrl: Re-arrange the RDT init code
      x86/resctrl: Rename the RDT functions and definitions
      x86/resctrl: Rename and move rdt files to a separate directory

commit 87ab4689ca6526079ab6f5150219ee88b42000ae
Author: Chang S. Bae <chang.seok.bae@intel.com>
Date:   Mon Nov 26 11:55:24 2018 -0800

    x86/fsgsbase/64: Fix the base write helper functions
    
    Andy spotted a regression in the fs/gs base helpers after the patch series
    was committed. The helper functions which write fs/gs base are not just
    writing the base, they are also changing the index. That's wrong and needs
    to be separated because writing the base has not to modify the index.
    
    While the regression is not causing any harm right now because the only
    caller depends on that behaviour, it's a guarantee for subtle breakage down
    the road.
    
    Make the index explicitly changed from the caller, instead of including
    the code in the helpers.
    
    Subsequently, the task write helpers do not handle for the current task
    anymore. The range check for a base value is also factored out, to minimize
    code redundancy from the caller.
    
    Fixes: b1378a561fd1 ("x86/fsgsbase/64: Introduce FS/GS base helper functions")
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/20181126195524.32179-1-chang.seok.bae@intel.com

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index bbfbf017065c..ddd4fa718c43 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -339,24 +339,6 @@ static unsigned long x86_fsgsbase_read_task(struct task_struct *task,
 	return base;
 }
 
-void x86_fsbase_write_cpu(unsigned long fsbase)
-{
-	/*
-	 * Set the selector to 0 as a notion, that the segment base is
-	 * overwritten, which will be checked for skipping the segment load
-	 * during context switch.
-	 */
-	loadseg(FS, 0);
-	wrmsrl(MSR_FS_BASE, fsbase);
-}
-
-void x86_gsbase_write_cpu_inactive(unsigned long gsbase)
-{
-	/* Set the selector to 0 for the same reason as %fs above. */
-	loadseg(GS, 0);
-	wrmsrl(MSR_KERNEL_GS_BASE, gsbase);
-}
-
 unsigned long x86_fsbase_read_task(struct task_struct *task)
 {
 	unsigned long fsbase;
@@ -385,38 +367,18 @@ unsigned long x86_gsbase_read_task(struct task_struct *task)
 	return gsbase;
 }
 
-int x86_fsbase_write_task(struct task_struct *task, unsigned long fsbase)
+void x86_fsbase_write_task(struct task_struct *task, unsigned long fsbase)
 {
-	/*
-	 * Not strictly needed for %fs, but do it for symmetry
-	 * with %gs
-	 */
-	if (unlikely(fsbase >= TASK_SIZE_MAX))
-		return -EPERM;
+	WARN_ON_ONCE(task == current);
 
-	preempt_disable();
 	task->thread.fsbase = fsbase;
-	if (task == current)
-		x86_fsbase_write_cpu(fsbase);
-	task->thread.fsindex = 0;
-	preempt_enable();
-
-	return 0;
 }
 
-int x86_gsbase_write_task(struct task_struct *task, unsigned long gsbase)
+void x86_gsbase_write_task(struct task_struct *task, unsigned long gsbase)
 {
-	if (unlikely(gsbase >= TASK_SIZE_MAX))
-		return -EPERM;
+	WARN_ON_ONCE(task == current);
 
-	preempt_disable();
 	task->thread.gsbase = gsbase;
-	if (task == current)
-		x86_gsbase_write_cpu_inactive(gsbase);
-	task->thread.gsindex = 0;
-	preempt_enable();
-
-	return 0;
 }
 
 int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
@@ -754,11 +716,60 @@ long do_arch_prctl_64(struct task_struct *task, int option, unsigned long arg2)
 
 	switch (option) {
 	case ARCH_SET_GS: {
-		ret = x86_gsbase_write_task(task, arg2);
+		if (unlikely(arg2 >= TASK_SIZE_MAX))
+			return -EPERM;
+
+		preempt_disable();
+		/*
+		 * ARCH_SET_GS has always overwritten the index
+		 * and the base. Zero is the most sensible value
+		 * to put in the index, and is the only value that
+		 * makes any sense if FSGSBASE is unavailable.
+		 */
+		if (task == current) {
+			loadseg(GS, 0);
+			x86_gsbase_write_cpu_inactive(arg2);
+
+			/*
+			 * On non-FSGSBASE systems, save_base_legacy() expects
+			 * that we also fill in thread.gsbase.
+			 */
+			task->thread.gsbase = arg2;
+
+		} else {
+			task->thread.gsindex = 0;
+			x86_gsbase_write_task(task, arg2);
+		}
+		preempt_enable();
 		break;
 	}
 	case ARCH_SET_FS: {
-		ret = x86_fsbase_write_task(task, arg2);
+		/*
+		 * Not strictly needed for %fs, but do it for symmetry
+		 * with %gs
+		 */
+		if (unlikely(arg2 >= TASK_SIZE_MAX))
+			return -EPERM;
+
+		preempt_disable();
+		/*
+		 * Set the selector to 0 for the same reason
+		 * as %gs above.
+		 */
+		if (task == current) {
+			loadseg(FS, 0);
+			x86_fsbase_write_cpu(arg2);
+
+			/*
+			 * On non-FSGSBASE systems, save_base_legacy() expects
+			 * that we also fill in thread.fsbase.
+			 */
+			task->thread.fsbase = arg2;
+		} else {
+			task->thread.fsindex = 0;
+			x86_fsbase_write_task(task, arg2);
+		}
+		preempt_enable();
 		break;
 	}
 	case ARCH_GET_FS: {

commit 02117e42db7470e59910088b2b0ee42d581d2651
Merge: ba6f508d0ec4 721066dfd4d5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Dec 17 18:48:25 2018 +0100

    Merge branch 'x86/urgent' into x86/mm, to pick up dependent fix
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a97673a1c43d005a3ae215f4ca8b4bbb5691aea1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Dec 3 10:47:34 2018 +0100

    x86: Fix various typos in comments
    
    Go over arch/x86/ and fix common typos in comments,
    and a typo in an actual function argument name.
    
    No change in functionality intended.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index bbfbf017065c..6a567f7e315b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -684,7 +684,7 @@ void set_personality_64bit(void)
 	/* TBD: overwrites user setup. Should have two bits.
 	   But 64bit processes have always behaved this way,
 	   so it's not too bad. The main problem is just that
-	   32bit childs are affected again. */
+	   32bit children are affected again. */
 	current->personality &= ~READ_IMPLIES_EXEC;
 }
 

commit ff16701a29cba3aafa0bd1656d766813b2d0a811
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Nov 25 19:33:47 2018 +0100

    x86/process: Consolidate and simplify switch_to_xtra() code
    
    Move the conditional invocation of __switch_to_xtra() into an inline
    function so the logic can be shared between 32 and 64 bit.
    
    Remove the handthrough of the TSS pointer and retrieve the pointer directly
    in the bitmap handling function. Use this_cpu_ptr() instead of the
    per_cpu() indirection.
    
    This is a preparatory change so integration of conditional indirect branch
    speculation optimization happens only in one place.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181125185005.280855518@linutronix.de

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 0e0b4288a4b2..bbfbf017065c 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -60,6 +60,8 @@
 #include <asm/unistd_32_ia32.h>
 #endif
 
+#include "process.h"
+
 /* Prints also some state that isn't saved in the pt_regs */
 void __show_regs(struct pt_regs *regs, enum show_regs_mode mode)
 {
@@ -553,7 +555,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	struct fpu *prev_fpu = &prev->fpu;
 	struct fpu *next_fpu = &next->fpu;
 	int cpu = smp_processor_id();
-	struct tss_struct *tss = &per_cpu(cpu_tss_rw, cpu);
 
 	WARN_ON_ONCE(IS_ENABLED(CONFIG_DEBUG_ENTRY) &&
 		     this_cpu_read(irq_count) != -1);
@@ -617,12 +618,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/* Reload sp0. */
 	update_task_stack(next_p);
 
-	/*
-	 * Now maybe reload the debug registers and handle I/O bitmaps
-	 */
-	if (unlikely(task_thread_info(next_p)->flags & _TIF_WORK_CTXSW_NEXT ||
-		     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))
-		__switch_to_xtra(prev_p, next_p, tss);
+	switch_to_extra(prev_p, next_p);
 
 #ifdef CONFIG_XEN_PV
 	/*

commit 352940ececaca58536a7fc4ff6b41d181156fd65
Author: Babu Moger <Babu.Moger@amd.com>
Date:   Wed Nov 21 20:28:27 2018 +0000

    x86/resctrl: Rename the RDT functions and definitions
    
    As AMD is starting to support RESCTRL features, rename the RDT functions
    and definitions to more generic names.
    
    Replace "intel_rdt" with "resctrl" where applicable.
    
    Signed-off-by: Babu Moger <babu.moger@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: <linux-doc@vger.kernel.org>
    Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: <qianyue.zj@alibaba-inc.com>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Reinette Chatre <reinette.chatre@intel.com>
    Cc: Rian Hunter <rian@alum.mit.edu>
    Cc: Sherry Hurwitz <sherry.hurwitz@amd.com>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Lendacky <Thomas.Lendacky@amd.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: <xiaochen.shen@intel.com>
    Link: https://lkml.kernel.org/r/20181121202811.4492-3-babu.moger@amd.com

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 8f23562687fb..67f7f5b0b6b7 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -664,7 +664,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	}
 
 	/* Load the Intel cache allocation PQR MSR. */
-	intel_rdt_sched_in();
+	resctrl_sched_in();
 
 	return prev_p;
 }

commit fa7d949337ccad32c76740c88e0e0351c349053b
Author: Babu Moger <Babu.Moger@amd.com>
Date:   Wed Nov 21 20:28:25 2018 +0000

    x86/resctrl: Rename and move rdt files to a separate directory
    
    New generation of AMD processors add support for RDT (or QOS) features.
    Together, these features will be called RESCTRL. With more than one
    vendors supporting these features, it seems more appropriate to rename
    these files.
    
    Create a new directory with the name 'resctrl' and move all the
    intel_rdt files to the new directory. This way all the resctrl related
    code resides inside one directory.
    
     [ bp: Add SPDX identifier to the Makefile ]
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Babu Moger <babu.moger@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: <linux-doc@vger.kernel.org>
    Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: <qianyue.zj@alibaba-inc.com>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Reinette Chatre <reinette.chatre@intel.com>
    Cc: Rian Hunter <rian@alum.mit.edu>
    Cc: Sherry Hurwitz <sherry.hurwitz@amd.com>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Lendacky <Thomas.Lendacky@amd.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: <xiaochen.shen@intel.com>
    Link: https://lkml.kernel.org/r/20181121202811.4492-2-babu.moger@amd.com

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 0e0b4288a4b2..8f23562687fb 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -52,7 +52,7 @@
 #include <asm/switch_to.h>
 #include <asm/xen/hypervisor.h>
 #include <asm/vdso.h>
-#include <asm/intel_rdt_sched.h>
+#include <asm/resctrl_sched.h>
 #include <asm/unistd.h>
 #include <asm/fsgsbase.h>
 #ifdef CONFIG_IA32_EMULATION

commit d38bc89c72e7235ac889ae64fe7828e2e61a18af
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Nov 21 15:11:24 2018 -0800

    x86/oops: Show the correct CS value in show_regs()
    
    show_regs() shows the CS in the CPU register instead of the value in
    regs.  This means that we'll probably print "CS: 0010" almost all
    the time regardless of what was actually in CS when the kernel
    malfunctioned.  This gives a particularly confusing result if we
    OOPSed due to an implicit supervisor access from user mode.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/4e36812b6e1e95236a812021d35cbf22746b5af6.1542841400.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 0e0b4288a4b2..2b8e6324fa20 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -66,7 +66,7 @@ void __show_regs(struct pt_regs *regs, enum show_regs_mode mode)
 	unsigned long cr0 = 0L, cr2 = 0L, cr3 = 0L, cr4 = 0L, fs, gs, shadowgs;
 	unsigned long d0, d1, d2, d3, d6, d7;
 	unsigned int fsindex, gsindex;
-	unsigned int ds, cs, es;
+	unsigned int ds, es;
 
 	show_iret_regs(regs);
 
@@ -98,7 +98,6 @@ void __show_regs(struct pt_regs *regs, enum show_regs_mode mode)
 	}
 
 	asm("movl %%ds,%0" : "=r" (ds));
-	asm("movl %%cs,%0" : "=r" (cs));
 	asm("movl %%es,%0" : "=r" (es));
 	asm("movl %%fs,%0" : "=r" (fsindex));
 	asm("movl %%gs,%0" : "=r" (gsindex));
@@ -114,7 +113,7 @@ void __show_regs(struct pt_regs *regs, enum show_regs_mode mode)
 
 	printk(KERN_DEFAULT "FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n",
 	       fs, fsindex, gs, gsindex, shadowgs);
-	printk(KERN_DEFAULT "CS:  %04x DS: %04x ES: %04x CR0: %016lx\n", cs, ds,
+	printk(KERN_DEFAULT "CS:  %04lx DS: %04x ES: %04x CR0: %016lx\n", regs->cs, ds,
 			es, cr0);
 	printk(KERN_DEFAULT "CR2: %016lx CR3: %016lx CR4: %016lx\n", cr2, cr3,
 			cr4);

commit a846446b1914d1e3d996d657754f43fde89bab51
Author: Dmitry Safonov <dima@arista.com>
Date:   Fri Oct 12 14:42:52 2018 +0100

    x86/compat: Adjust in_compat_syscall() to generic code under !COMPAT
    
    The result of in_compat_syscall() can be pictured as:
    
    x86 platform:
        ---------------------------------------------------
        |  Arch\syscall  |  64-bit  |   ia32   |   x32    |
        |-------------------------------------------------|
        |     x86_64     |  false   |   true   |   true   |
        |-------------------------------------------------|
        |      i686      |          |  <true>  |          |
        ---------------------------------------------------
    
    Other platforms:
        -------------------------------------------
        |  Arch\syscall  |  64-bit  |   compat    |
        |-----------------------------------------|
        |     64-bit     |  false   |    true     |
        |-----------------------------------------|
        |    32-bit(?)   |          |   <false>   |
        -------------------------------------------
    
    As seen, the result of in_compat_syscall() on generic 32-bit platform
    differs from i686.
    
    There is no reason for in_compat_syscall() == true on native i686.  It also
    easy to misread code if the result on native 32-bit platform differs
    between arches.
    
    Because of that non arch-specific code has many places with:
        if (IS_ENABLED(CONFIG_COMPAT) && in_compat_syscall())
    in different variations.
    
    It looks-like the only non-x86 code which uses in_compat_syscall() not
    under CONFIG_COMPAT guard is in amd/amdkfd. But according to the commit
    a18069c132cb ("amdkfd: Disable support for 32-bit user processes"), it
    actually should be disabled on native i686.
    
    Rename in_compat_syscall() to in_32bit_syscall() for x86-specific code
    and make in_compat_syscall() false under !CONFIG_COMPAT.
    
    A follow on patch will clean up generic users which were forced to check
    IS_ENABLED(CONFIG_COMPAT) with in_compat_syscall().
    
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Dmitry Safonov <0x7f454c46@gmail.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: Stephen Boyd <sboyd@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: linux-efi@vger.kernel.org
    Cc: netdev@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181012134253.23266-2-dima@arista.com

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 31b4755369f0..0e0b4288a4b2 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -701,10 +701,10 @@ static void __set_personality_x32(void)
 		current->mm->context.ia32_compat = TIF_X32;
 	current->personality &= ~READ_IMPLIES_EXEC;
 	/*
-	 * in_compat_syscall() uses the presence of the x32 syscall bit
+	 * in_32bit_syscall() uses the presence of the x32 syscall bit
 	 * flag to determine compat status.  The x86 mmap() code relies on
 	 * the syscall bitness so set x32 syscall bit right here to make
-	 * in_compat_syscall() work during exec().
+	 * in_32bit_syscall() work during exec().
 	 *
 	 * Pretend to come from a x32 execve.
 	 */

commit d82924c3b8d0607094b94fab290a33c5ad7d586c
Merge: d7197a5ad852 bb4b3b776273
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 18:43:04 2018 +0100

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 pti updates from Ingo Molnar:
     "The main changes:
    
       - Make the IBPB barrier more strict and add STIBP support (Jiri
         Kosina)
    
       - Micro-optimize and clean up the entry code (Andy Lutomirski)
    
       - ... plus misc other fixes"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/speculation: Propagate information about RSB filling mitigation to sysfs
      x86/speculation: Enable cross-hyperthread spectre v2 STIBP mitigation
      x86/speculation: Apply IBPB more strictly to avoid cross-process data leak
      x86/speculation: Add RETPOLINE_AMD support to the inline asm CALL_NOSPEC variant
      x86/CPU: Fix unused variable warning when !CONFIG_IA32_EMULATION
      x86/pti/64: Remove the SYSCALL64 entry trampoline
      x86/entry/64: Use the TSS sp2 slot for SYSCALL/SYSRET scratch space
      x86/entry/64: Document idtentry

commit f4550b52e495e1b634d1f2c1004bcea5dc3321ea
Author: Chang S. Bae <chang.seok.bae@intel.com>
Date:   Tue Sep 18 16:08:56 2018 -0700

    x86/fsgsbase/64: Factor out FS/GS segment loading from __switch_to()
    
    Instead of open coding the calls to load_seg_legacy(), introduce
    x86_fsgsbase_load() to load FS/GS segments.
    
    This makes it more explicit that this is part of FSGSBASE functionality,
    and the new helper can be updated when FSGSBASE instructions are enabled.
    
    [ mingo: Wrote new changelog. ]
    
    Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Markus T Metzger <markus.t.metzger@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: Rik van Riel <riel@surriel.com>
    Link: http://lkml.kernel.org/r/1537312139-5580-6-git-send-email-chang.seok.bae@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e5fb0c3dee4d..d6674a425714 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -287,6 +287,15 @@ static __always_inline void load_seg_legacy(unsigned short prev_index,
 	}
 }
 
+static __always_inline void x86_fsgsbase_load(struct thread_struct *prev,
+					      struct thread_struct *next)
+{
+	load_seg_legacy(prev->fsindex, prev->fsbase,
+			next->fsindex, next->fsbase, FS);
+	load_seg_legacy(prev->gsindex, prev->gsbase,
+			next->gsindex, next->gsbase, GS);
+}
+
 static unsigned long x86_fsgsbase_read_task(struct task_struct *task,
 					    unsigned short selector)
 {
@@ -597,10 +606,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	if (unlikely(next->ds | prev->ds))
 		loadsegment(ds, next->ds);
 
-	load_seg_legacy(prev->fsindex, prev->fsbase,
-			next->fsindex, next->fsbase, FS);
-	load_seg_legacy(prev->gsindex, prev->gsbase,
-			next->gsindex, next->gsbase, GS);
+	x86_fsgsbase_load(prev, next);
 
 	switch_fpu_finish(next_fpu, cpu);
 

commit e696c231bebf5f17fe0c5e465c01511320668054
Author: Chang S. Bae <chang.seok.bae@intel.com>
Date:   Tue Sep 18 16:08:54 2018 -0700

    x86/fsgsbase/64: Make ptrace use the new FS/GS base helpers
    
    Use the new FS/GS base helper functions in <asm/fsgsbase.h> in the platform
    specific ptrace implementation of the following APIs:
    
      PTRACE_ARCH_PRCTL,
      PTRACE_SETREG,
      PTRACE_GETREG,
      etc.
    
    The fsgsbase code is more abstracted out this way and the FS/GS-update
    mechanism will be easier to change this way.
    
    [ mingo: Wrote new changelog. ]
    
    Based-on-code-from: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Markus T Metzger <markus.t.metzger@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1537312139-5580-4-git-send-email-chang.seok.bae@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 2a53ff8d1baf..e5fb0c3dee4d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -287,8 +287,8 @@ static __always_inline void load_seg_legacy(unsigned short prev_index,
 	}
 }
 
-unsigned long x86_fsgsbase_read_task(struct task_struct *task,
-				     unsigned short selector)
+static unsigned long x86_fsgsbase_read_task(struct task_struct *task,
+					    unsigned short selector)
 {
 	unsigned short idx = selector >> 3;
 	unsigned long base;
@@ -751,54 +751,25 @@ static long prctl_map_vdso(const struct vdso_image *image, unsigned long addr)
 long do_arch_prctl_64(struct task_struct *task, int option, unsigned long arg2)
 {
 	int ret = 0;
-	int doit = task == current;
-	int cpu;
 
 	switch (option) {
-	case ARCH_SET_GS:
-		if (arg2 >= TASK_SIZE_MAX)
-			return -EPERM;
-		cpu = get_cpu();
-		task->thread.gsindex = 0;
-		task->thread.gsbase = arg2;
-		if (doit) {
-			load_gs_index(0);
-			ret = wrmsrl_safe(MSR_KERNEL_GS_BASE, arg2);
-		}
-		put_cpu();
+	case ARCH_SET_GS: {
+		ret = x86_gsbase_write_task(task, arg2);
 		break;
-	case ARCH_SET_FS:
-		/* Not strictly needed for fs, but do it for symmetry
-		   with gs */
-		if (arg2 >= TASK_SIZE_MAX)
-			return -EPERM;
-		cpu = get_cpu();
-		task->thread.fsindex = 0;
-		task->thread.fsbase = arg2;
-		if (doit) {
-			/* set the selector to 0 to not confuse __switch_to */
-			loadsegment(fs, 0);
-			ret = wrmsrl_safe(MSR_FS_BASE, arg2);
-		}
-		put_cpu();
+	}
+	case ARCH_SET_FS: {
+		ret = x86_fsbase_write_task(task, arg2);
 		break;
+	}
 	case ARCH_GET_FS: {
-		unsigned long base;
+		unsigned long base = x86_fsbase_read_task(task);
 
-		if (doit)
-			rdmsrl(MSR_FS_BASE, base);
-		else
-			base = task->thread.fsbase;
 		ret = put_user(base, (unsigned long __user *)arg2);
 		break;
 	}
 	case ARCH_GET_GS: {
-		unsigned long base;
+		unsigned long base = x86_gsbase_read_task(task);
 
-		if (doit)
-			rdmsrl(MSR_KERNEL_GS_BASE, base);
-		else
-			base = task->thread.gsbase;
 		ret = put_user(base, (unsigned long __user *)arg2);
 		break;
 	}

commit b1378a561fd16afdd96ef0bc912b1bcd2b85a68e
Author: Chang S. Bae <chang.seok.bae@intel.com>
Date:   Tue Sep 18 16:08:53 2018 -0700

    x86/fsgsbase/64: Introduce FS/GS base helper functions
    
    Introduce FS/GS base access functionality via <asm/fsgsbase.h>,
    not yet used by anything directly.
    
    Factor out task_seg_base() from x86/ptrace.c and rename it to
    x86_fsgsbase_read_task() to make it part of the new helpers.
    
    This will allow us to enhance FSGSBASE support and eventually enable
    the FSBASE/GSBASE instructions.
    
    An "inactive" GS base refers to a base saved at kernel entry
    and being part of an inactive, non-running/stopped user-task.
    (The typical ptrace model.)
    
    Here are the new functions:
    
      x86_fsbase_read_task()
      x86_gsbase_read_task()
      x86_fsbase_write_task()
      x86_gsbase_write_task()
      x86_fsbase_read_cpu()
      x86_fsbase_write_cpu()
      x86_gsbase_read_cpu_inactive()
      x86_gsbase_write_cpu_inactive()
    
    As an advantage of the unified namespace we can now see all FS/GSBASE
    API use in the kernel via the following 'git grep' pattern:
    
      $ git grep x86_.*sbase
    
    [ mingo: Wrote new changelog. ]
    
    Based-on-code-from: Andy Lutomirski <luto@kernel.org>
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Markus T Metzger <markus.t.metzger@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1537312139-5580-3-git-send-email-chang.seok.bae@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ea5ea850348d..2a53ff8d1baf 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -54,6 +54,7 @@
 #include <asm/vdso.h>
 #include <asm/intel_rdt_sched.h>
 #include <asm/unistd.h>
+#include <asm/fsgsbase.h>
 #ifdef CONFIG_IA32_EMULATION
 /* Not included via unistd.h */
 #include <asm/unistd_32_ia32.h>
@@ -286,6 +287,129 @@ static __always_inline void load_seg_legacy(unsigned short prev_index,
 	}
 }
 
+unsigned long x86_fsgsbase_read_task(struct task_struct *task,
+				     unsigned short selector)
+{
+	unsigned short idx = selector >> 3;
+	unsigned long base;
+
+	if (likely((selector & SEGMENT_TI_MASK) == 0)) {
+		if (unlikely(idx >= GDT_ENTRIES))
+			return 0;
+
+		/*
+		 * There are no user segments in the GDT with nonzero bases
+		 * other than the TLS segments.
+		 */
+		if (idx < GDT_ENTRY_TLS_MIN || idx > GDT_ENTRY_TLS_MAX)
+			return 0;
+
+		idx -= GDT_ENTRY_TLS_MIN;
+		base = get_desc_base(&task->thread.tls_array[idx]);
+	} else {
+#ifdef CONFIG_MODIFY_LDT_SYSCALL
+		struct ldt_struct *ldt;
+
+		/*
+		 * If performance here mattered, we could protect the LDT
+		 * with RCU.  This is a slow path, though, so we can just
+		 * take the mutex.
+		 */
+		mutex_lock(&task->mm->context.lock);
+		ldt = task->mm->context.ldt;
+		if (unlikely(idx >= ldt->nr_entries))
+			base = 0;
+		else
+			base = get_desc_base(ldt->entries + idx);
+		mutex_unlock(&task->mm->context.lock);
+#else
+		base = 0;
+#endif
+	}
+
+	return base;
+}
+
+void x86_fsbase_write_cpu(unsigned long fsbase)
+{
+	/*
+	 * Set the selector to 0 as a notion, that the segment base is
+	 * overwritten, which will be checked for skipping the segment load
+	 * during context switch.
+	 */
+	loadseg(FS, 0);
+	wrmsrl(MSR_FS_BASE, fsbase);
+}
+
+void x86_gsbase_write_cpu_inactive(unsigned long gsbase)
+{
+	/* Set the selector to 0 for the same reason as %fs above. */
+	loadseg(GS, 0);
+	wrmsrl(MSR_KERNEL_GS_BASE, gsbase);
+}
+
+unsigned long x86_fsbase_read_task(struct task_struct *task)
+{
+	unsigned long fsbase;
+
+	if (task == current)
+		fsbase = x86_fsbase_read_cpu();
+	else if (task->thread.fsindex == 0)
+		fsbase = task->thread.fsbase;
+	else
+		fsbase = x86_fsgsbase_read_task(task, task->thread.fsindex);
+
+	return fsbase;
+}
+
+unsigned long x86_gsbase_read_task(struct task_struct *task)
+{
+	unsigned long gsbase;
+
+	if (task == current)
+		gsbase = x86_gsbase_read_cpu_inactive();
+	else if (task->thread.gsindex == 0)
+		gsbase = task->thread.gsbase;
+	else
+		gsbase = x86_fsgsbase_read_task(task, task->thread.gsindex);
+
+	return gsbase;
+}
+
+int x86_fsbase_write_task(struct task_struct *task, unsigned long fsbase)
+{
+	/*
+	 * Not strictly needed for %fs, but do it for symmetry
+	 * with %gs
+	 */
+	if (unlikely(fsbase >= TASK_SIZE_MAX))
+		return -EPERM;
+
+	preempt_disable();
+	task->thread.fsbase = fsbase;
+	if (task == current)
+		x86_fsbase_write_cpu(fsbase);
+	task->thread.fsindex = 0;
+	preempt_enable();
+
+	return 0;
+}
+
+int x86_gsbase_write_task(struct task_struct *task, unsigned long gsbase)
+{
+	if (unlikely(gsbase >= TASK_SIZE_MAX))
+		return -EPERM;
+
+	preempt_disable();
+	task->thread.gsbase = gsbase;
+	if (task == current)
+		x86_gsbase_write_cpu_inactive(gsbase);
+	task->thread.gsindex = 0;
+	preempt_enable();
+
+	return 0;
+}
+
 int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 		unsigned long arg, struct task_struct *p, unsigned long tls)
 {

commit 98f05b5138f0a9b56022295cc1387e635b25635d
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Sep 3 15:59:43 2018 -0700

    x86/entry/64: Use the TSS sp2 slot for SYSCALL/SYSRET scratch space
    
    In the non-trampoline SYSCALL64 path, a percpu variable is used to
    temporarily store the user RSP value.
    
    Instead of a separate variable, use the otherwise unused sp2 slot in the
    TSS.  This will improve cache locality, as the sp1 slot is already used in
    the same code to find the kernel stack.  It will also simplify a future
    change to make the non-trampoline path work in PTI mode.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/08e769a0023dbad4bac6f34f3631dbaf8ad59f4f.1536015544.git.luto@kernel.org

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index a451bc374b9b..0fa7aa19f09e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -59,8 +59,6 @@
 #include <asm/unistd_32_ia32.h>
 #endif
 
-__visible DEFINE_PER_CPU(unsigned long, rsp_scratch);
-
 /* Prints also some state that isn't saved in the pt_regs */
 void __show_regs(struct pt_regs *regs, int all)
 {

commit 9fe6299dde587788f245e9f7a5a1b296fad4e8c7
Author: Jann Horn <jannh@google.com>
Date:   Fri Aug 31 21:41:51 2018 +0200

    x86/process: Don't mix user/kernel regs in 64bit __show_regs()
    
    When the kernel.print-fatal-signals sysctl has been enabled, a simple
    userspace crash will cause the kernel to write a crash dump that contains,
    among other things, the kernel gsbase into dmesg.
    
    As suggested by Andy, limit output to pt_regs, FS_BASE and KERNEL_GS_BASE
    in this case.
    
    This also moves the bitness-specific logic from show_regs() into
    process_{32,64}.c.
    
    Fixes: 45807a1df9f5 ("vdso: print fatal signals")
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180831194151.123586-1-jannh@google.com

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index a451bc374b9b..ea5ea850348d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -62,7 +62,7 @@
 __visible DEFINE_PER_CPU(unsigned long, rsp_scratch);
 
 /* Prints also some state that isn't saved in the pt_regs */
-void __show_regs(struct pt_regs *regs, int all)
+void __show_regs(struct pt_regs *regs, enum show_regs_mode mode)
 {
 	unsigned long cr0 = 0L, cr2 = 0L, cr3 = 0L, cr4 = 0L, fs, gs, shadowgs;
 	unsigned long d0, d1, d2, d3, d6, d7;
@@ -87,9 +87,17 @@ void __show_regs(struct pt_regs *regs, int all)
 	printk(KERN_DEFAULT "R13: %016lx R14: %016lx R15: %016lx\n",
 	       regs->r13, regs->r14, regs->r15);
 
-	if (!all)
+	if (mode == SHOW_REGS_SHORT)
 		return;
 
+	if (mode == SHOW_REGS_USER) {
+		rdmsrl(MSR_FS_BASE, fs);
+		rdmsrl(MSR_KERNEL_GS_BASE, shadowgs);
+		printk(KERN_DEFAULT "FS:  %016lx GS:  %016lx\n",
+		       fs, shadowgs);
+		return;
+	}
+
 	asm("movl %%ds,%0" : "=r" (ds));
 	asm("movl %%cs,%0" : "=r" (cs));
 	asm("movl %%es,%0" : "=r" (es));

commit dc76803e57cc86589c4efcb5362918f9b0c0436f
Author: Rian Hunter <rian@alum.mit.edu>
Date:   Sun Aug 19 16:08:53 2018 -0700

    x86/process: Re-export start_thread()
    
    The consolidation of the start_thread() functions removed the export
    unintentionally. This breaks binfmt handlers built as a module.
    
    Add it back.
    
    Fixes: e634d8fc792c ("x86-64: merge the standard and compat start_thread() functions")
    Signed-off-by: Rian Hunter <rian@alum.mit.edu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180819230854.7275-1-rian@alum.mit.edu

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 476e3ddf8890..a451bc374b9b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -384,6 +384,7 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 	start_thread_common(regs, new_ip, new_sp,
 			    __USER_CS, __USER_DS, 0);
 }
+EXPORT_SYMBOL_GPL(start_thread);
 
 #ifdef CONFIG_COMPAT
 void compat_start_thread(struct pt_regs *regs, u32 new_ip, u32 new_sp)

commit 252e1a0526304f0f3f6888fc09e81cb220f957f3
Author: Joerg Roedel <jroedel@suse.de>
Date:   Wed Jul 18 11:40:51 2018 +0200

    x86/entry: Rename update_sp0 to update_task_stack
    
    The function does not update sp0 anymore but updates makes the task-stack
    visible for entry code. This is by either writing it to sp1 or by doing a
    hypercall. Rename the function to get rid of the misleading name.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Pavel Machek <pavel@ucw.cz>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: linux-mm@kvack.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Waiman Long <llong@redhat.com>
    Cc: "David H . Gutteridge" <dhgutteridge@sympatico.ca>
    Cc: joro@8bytes.org
    Link: https://lkml.kernel.org/r/1531906876-13451-15-git-send-email-joro@8bytes.org

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 12bb445fb98d..476e3ddf8890 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -478,7 +478,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	this_cpu_write(cpu_current_top_of_stack, task_top_of_stack(next_p));
 
 	/* Reload sp0. */
-	update_sp0(next_p);
+	update_task_stack(next_p);
 
 	/*
 	 * Now maybe reload the debug registers and handle I/O bitmaps

commit acf46020012ccbca1172e9c7aeab399c950d9212
Author: Dmitry Safonov <dima@arista.com>
Date:   Fri May 18 00:35:10 2018 +0100

    x86/mm: Drop TS_COMPAT on 64-bit exec() syscall
    
    The x86 mmap() code selects the mmap base for an allocation depending on
    the bitness of the syscall. For 64bit sycalls it select mm->mmap_base and
    for 32bit mm->mmap_compat_base.
    
    exec() calls mmap() which in turn uses in_compat_syscall() to check whether
    the mapping is for a 32bit or a 64bit task. The decision is made on the
    following criteria:
    
      ia32    child->thread.status & TS_COMPAT
       x32    child->pt_regs.orig_ax & __X32_SYSCALL_BIT
      ia64    !ia32 && !x32
    
    __set_personality_x32() was dropping TS_COMPAT flag, but
    set_personality_64bit() has kept compat syscall flag making
    in_compat_syscall() return true during the first exec() syscall.
    
    Which in result has user-visible effects, mentioned by Alexey:
    1) It breaks ASAN
    $ gcc -fsanitize=address wrap.c -o wrap-asan
    $ ./wrap32 ./wrap-asan true
    ==1217==Shadow memory range interleaves with an existing memory mapping. ASan cannot proceed correctly. ABORTING.
    ==1217==ASan shadow was supposed to be located in the [0x00007fff7000-0x10007fff7fff] range.
    ==1217==Process memory map follows:
            0x000000400000-0x000000401000   /home/izbyshev/test/gcc/asan-exec-from-32bit/wrap-asan
            0x000000600000-0x000000601000   /home/izbyshev/test/gcc/asan-exec-from-32bit/wrap-asan
            0x000000601000-0x000000602000   /home/izbyshev/test/gcc/asan-exec-from-32bit/wrap-asan
            0x0000f7dbd000-0x0000f7de2000   /lib64/ld-2.27.so
            0x0000f7fe2000-0x0000f7fe3000   /lib64/ld-2.27.so
            0x0000f7fe3000-0x0000f7fe4000   /lib64/ld-2.27.so
            0x0000f7fe4000-0x0000f7fe5000
            0x7fed9abff000-0x7fed9af54000
            0x7fed9af54000-0x7fed9af6b000   /lib64/libgcc_s.so.1
    [snip]
    
    2) It doesn't seem to be great for security if an attacker always knows
    that ld.so is going to be mapped into the first 4GB in this case
    (the same thing happens for PIEs as well).
    
    The testcase:
    $ cat wrap.c
    
    int main(int argc, char *argv[]) {
      execvp(argv[1], &argv[1]);
      return 127;
    }
    
    $ gcc wrap.c -o wrap
    $ LD_SHOW_AUXV=1 ./wrap ./wrap true |& grep AT_BASE
    AT_BASE:         0x7f63b8309000
    AT_BASE:         0x7faec143c000
    AT_BASE:         0x7fbdb25fa000
    
    $ gcc -m32 wrap.c -o wrap32
    $ LD_SHOW_AUXV=1 ./wrap32 ./wrap true |& grep AT_BASE
    AT_BASE:         0xf7eff000
    AT_BASE:         0xf7cee000
    AT_BASE:         0x7f8b9774e000
    
    Fixes: 1b028f784e8c ("x86/mm: Introduce mmap_compat_base() for 32-bit mmap()")
    Fixes: ada26481dfe6 ("x86/mm: Make in_compat_syscall() work during exec")
    Reported-by: Alexey Izbyshev <izbyshev@ispras.ru>
    Bisected-by: Alexander Monakov <amonakov@ispras.ru>
    Investigated-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Alexander Monakov <amonakov@ispras.ru>
    Cc: Dmitry Safonov <0x7f454c46@gmail.com>
    Cc: stable@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Link: https://lkml.kernel.org/r/20180517233510.24996-1-dima@arista.com

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4b100fe0f508..12bb445fb98d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -542,6 +542,7 @@ void set_personality_64bit(void)
 	clear_thread_flag(TIF_X32);
 	/* Pretend that this comes from a 64bit execve */
 	task_pt_regs(current)->orig_ax = __NR_execve;
+	current_thread_info()->status &= ~TS_COMPAT;
 
 	/* Ensure the corresponding mm is not marked. */
 	if (current->mm)

commit 42b933b59721f288e3ce23ca79a17a973808dab9
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 13 18:48:04 2018 +0100

    x86/kvm/vmx: read MSR_{FS,KERNEL_GS}_BASE from current->thread
    
    vmx_save_host_state() is only called from kvm_arch_vcpu_ioctl_run() so
    the context is pretty well defined. Read MSR_{FS,KERNEL_GS}_BASE from
    current->thread after calling save_fsgs() which takes care of
    X86_BUG_NULL_SEG case now and will do RD[FG,GS]BASE when FSGSBASE
    extensions are exposed to userspace (currently they are not).
    
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 9eb448c7859d..4b100fe0f508 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -205,6 +205,20 @@ static __always_inline void save_fsgs(struct task_struct *task)
 	save_base_legacy(task, task->thread.gsindex, GS);
 }
 
+#if IS_ENABLED(CONFIG_KVM)
+/*
+ * While a process is running,current->thread.fsbase and current->thread.gsbase
+ * may not match the corresponding CPU registers (see save_base_legacy()). KVM
+ * wants an efficient way to save and restore FSBASE and GSBASE.
+ * When FSGSBASE extensions are enabled, this will have to use RD{FS,GS}BASE.
+ */
+void save_fsgs_for_kvm(void)
+{
+	save_fsgs(current);
+}
+EXPORT_SYMBOL_GPL(save_fsgs_for_kvm);
+#endif
+
 static __always_inline void loadseg(enum which_selector which,
 				    unsigned short sel)
 {

commit 37a8f7c38339b22b69876d6f5a0ab851565284e3
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Jan 28 10:38:50 2018 -0800

    x86/asm: Move 'status' from thread_struct to thread_info
    
    The TS_COMPAT bit is very hot and is accessed from code paths that mostly
    also touch thread_info::flags.  Move it into struct thread_info to improve
    cache locality.
    
    The only reason it was in thread_struct is that there was a brief period
    during which arch-specific fields were not allowed in struct thread_info.
    
    Linus suggested further changing:
    
      ti->status &= ~(TS_COMPAT|TS_I386_REGS_POKED);
    
    to:
    
      if (unlikely(ti->status & (TS_COMPAT|TS_I386_REGS_POKED)))
              ti->status &= ~(TS_COMPAT|TS_I386_REGS_POKED);
    
    on the theory that frequently dirtying the cacheline even in pure 64-bit
    code that never needs to modify status hurts performance.  That could be a
    reasonable followup patch, but I suspect it matters less on top of this
    patch.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Kernel Hardening <kernel-hardening@lists.openwall.com>
    Link: https://lkml.kernel.org/r/03148bcc1b217100e6e8ecf6a5468c45cf4304b6.1517164461.git.luto@kernel.org

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index c75466232016..9eb448c7859d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -557,7 +557,7 @@ static void __set_personality_x32(void)
 	 * Pretend to come from a x32 execve.
 	 */
 	task_pt_regs(current)->orig_ax = __NR_x32_execve | __X32_SYSCALL_BIT;
-	current->thread.status &= ~TS_COMPAT;
+	current_thread_info()->status &= ~TS_COMPAT;
 #endif
 }
 
@@ -571,7 +571,7 @@ static void __set_personality_ia32(void)
 	current->personality |= force_personality32;
 	/* Prepare the first "return" to user space */
 	task_pt_regs(current)->orig_ax = __NR_ia32_execve;
-	current->thread.status |= TS_COMPAT;
+	current_thread_info()->status |= TS_COMPAT;
 #endif
 }
 

commit c482feefe1aeb150156248ba0fd3e029bc886605
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:29 2017 +0100

    x86/entry/64: Make cpu_entry_area.tss read-only
    
    The TSS is a fairly juicy target for exploits, and, now that the TSS
    is in the cpu_entry_area, it's no longer protected by kASLR.  Make it
    read-only on x86_64.
    
    On x86_32, it can't be RO because it's written by the CPU during task
    switches, and we use a task gate for double faults.  I'd also be
    nervous about errata if we tried to make it RO even on configurations
    without double fault handling.
    
    [ tglx: AMD confirmed that there is no problem on 64-bit with TSS RO.  So
            it's probably safe to assume that it's a non issue, though Intel
            might have been creative in that area. Still waiting for
            confirmation. ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.733700132@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 157f81816915..c75466232016 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -399,7 +399,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	struct fpu *prev_fpu = &prev->fpu;
 	struct fpu *next_fpu = &next->fpu;
 	int cpu = smp_processor_id();
-	struct tss_struct *tss = &per_cpu(cpu_tss, cpu);
+	struct tss_struct *tss = &per_cpu(cpu_tss_rw, cpu);
 
 	WARN_ON_ONCE(IS_ENABLED(CONFIG_DEBUG_ENTRY) &&
 		     this_cpu_read(irq_count) != -1);

commit 9aaefe7b59ae00605256a7d6bd1c1456432495fc
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:21 2017 +0100

    x86/entry/64: Separate cpu_current_top_of_stack from TSS.sp0
    
    On 64-bit kernels, we used to assume that TSS.sp0 was the current
    top of stack.  With the addition of an entry trampoline, this will
    no longer be the case.  Store the current top of stack in TSS.sp1,
    which is otherwise unused but shares the same cacheline.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.050864668@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 01b119bebb68..157f81816915 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -461,6 +461,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 * Switch the PDA and FPU contexts.
 	 */
 	this_cpu_write(current_task, next_p);
+	this_cpu_write(cpu_current_top_of_stack, task_top_of_stack(next_p));
 
 	/* Reload sp0. */
 	update_sp0(next_p);

commit b02fcf9ba1211097754b286043cd87a8b4907e75
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Dec 4 15:07:09 2017 +0100

    x86/unwinder: Handle stack overflows more gracefully
    
    There are at least two unwinder bugs hindering the debugging of
    stack-overflow crashes:
    
    - It doesn't deal gracefully with the case where the stack overflows and
      the stack pointer itself isn't on a valid stack but the
      to-be-dereferenced data *is*.
    
    - The ORC oops dump code doesn't know how to print partial pt_regs, for the
      case where if we get an interrupt/exception in *early* entry code
      before the full pt_regs have been saved.
    
    Fix both issues.
    
    http://lkml.kernel.org/r/20171126024031.uxi4numpbjm5rlbr@treble
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.071425003@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index eeeb34f85c25..01b119bebb68 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -69,9 +69,8 @@ void __show_regs(struct pt_regs *regs, int all)
 	unsigned int fsindex, gsindex;
 	unsigned int ds, cs, es;
 
-	printk(KERN_DEFAULT "RIP: %04lx:%pS\n", regs->cs, (void *)regs->ip);
-	printk(KERN_DEFAULT "RSP: %04lx:%016lx EFLAGS: %08lx", regs->ss,
-		regs->sp, regs->flags);
+	show_iret_regs(regs);
+
 	if (regs->orig_ax != -1)
 		pr_cont(" ORIG_RAX: %016lx\n", regs->orig_ax);
 	else
@@ -88,6 +87,9 @@ void __show_regs(struct pt_regs *regs, int all)
 	printk(KERN_DEFAULT "R13: %016lx R14: %016lx R15: %016lx\n",
 	       regs->r13, regs->r14, regs->r15);
 
+	if (!all)
+		return;
+
 	asm("movl %%ds,%0" : "=r" (ds));
 	asm("movl %%cs,%0" : "=r" (cs));
 	asm("movl %%es,%0" : "=r" (es));
@@ -98,9 +100,6 @@ void __show_regs(struct pt_regs *regs, int all)
 	rdmsrl(MSR_GS_BASE, gs);
 	rdmsrl(MSR_KERNEL_GS_BASE, shadowgs);
 
-	if (!all)
-		return;
-
 	cr0 = read_cr0();
 	cr2 = read_cr2();
 	cr3 = __read_cr3();

commit d375cf1530595e33961a8844192cddab913650e3
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:16 2017 -0700

    x86/entry/64: Remove thread_struct::sp0
    
    On x86_64, we can easily calculate sp0 when needed instead of
    storing it in thread_struct.
    
    On x86_32, a similar cleanup would be possible, but it would require
    cleaning up the vm86 code first, and that can wait for a later
    cleanup series.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/719cd9c66c548c4350d98a90f050aee8b17f8919.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 45e380958392..eeeb34f85c25 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -274,7 +274,6 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 	struct inactive_task_frame *frame;
 	struct task_struct *me = current;
 
-	p->thread.sp0 = (unsigned long)task_stack_page(p) + THREAD_SIZE;
 	childregs = task_pt_regs(p);
 	fork_frame = container_of(childregs, struct fork_frame, regs);
 	frame = &fork_frame->frame;

commit 46f5a10a721ce8dce8cc8fe55279b49e1c6b3288
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:14 2017 -0700

    x86/entry/64: Remove all remaining direct thread_struct::sp0 reads
    
    The only remaining readers in context switch code or vm86(), and
    they all just want to update TSS.sp0 to match the current task.
    Replace them all with a new helper update_sp0().
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/2d231687f4ff288c9d9e98d7861b7df374246ac3.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 2124304fb77a..45e380958392 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -465,7 +465,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	this_cpu_write(current_task, next_p);
 
 	/* Reload sp0. */
-	load_sp0(next->sp0);
+	update_sp0(next_p);
 
 	/*
 	 * Now maybe reload the debug registers and handle I/O bitmaps

commit da51da189a24bb9b7e2d5a123be096e51a4695a5
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:10 2017 -0700

    x86/entry/64: Pass SP0 directly to load_sp0()
    
    load_sp0() had an odd signature:
    
      void load_sp0(struct tss_struct *tss, struct thread_struct *thread);
    
    Simplify it to:
    
      void load_sp0(unsigned long sp0);
    
    Also simplify a few get_cpu()/put_cpu() sequences to
    preempt_disable()/preempt_enable().
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/2655d8b42ed940aa384fe18ee1129bbbcf730a08.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index a6ff6d1a0110..2124304fb77a 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -465,7 +465,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	this_cpu_write(current_task, next_p);
 
 	/* Reload sp0. */
-	load_sp0(tss, next);
+	load_sp0(next->sp0);
 
 	/*
 	 * Now maybe reload the debug registers and handle I/O bitmaps

commit bd7dc5a6afac719d8ce4092391eef2c7e83c2a75
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:09 2017 -0700

    x86/entry/32: Pull the MSR_IA32_SYSENTER_CS update code out of native_load_sp0()
    
    This causes the MSR_IA32_SYSENTER_CS write to move out of the
    paravirt callback.  This shouldn't affect Xen PV: Xen already ignores
    MSR_IA32_SYSENTER_ESP writes.  In any event, Xen doesn't support
    vm86() in a useful way.
    
    Note to any potential backporters: This patch won't break lguest, as
    lguest didn't have any SYSENTER support at all.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/75cf09fe03ae778532d0ca6c65aa58e66bc2f90c.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 302e7b2572d1..a6ff6d1a0110 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -464,7 +464,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 */
 	this_cpu_write(current_task, next_p);
 
-	/* Reload esp0 and ss1.  This changes current_thread_info(). */
+	/* Reload sp0. */
 	load_sp0(tss, next);
 
 	/*

commit f57091767add2b79d76aac41b83b192d8ba1dce7
Merge: d725c7ac8b96 d56593eb5eda
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 13:56:37 2017 -0700

    Merge branch 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cache quality monitoring update from Thomas Gleixner:
     "This update provides a complete rewrite of the Cache Quality
      Monitoring (CQM) facility.
    
      The existing CQM support was duct taped into perf with a lot of issues
      and the attempts to fix those turned out to be incomplete and
      horrible.
    
      After lengthy discussions it was decided to integrate the CQM support
      into the Resource Director Technology (RDT) facility, which is the
      obvious choise as in hardware CQM is part of RDT. This allowed to add
      Memory Bandwidth Monitoring support on top.
    
      As a result the mechanisms for allocating cache/memory bandwidth and
      the corresponding monitoring mechanisms are integrated into a single
      management facility with a consistent user interface"
    
    * 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      x86/intel_rdt: Turn off most RDT features on Skylake
      x86/intel_rdt: Add command line options for resource director technology
      x86/intel_rdt: Move special case code for Haswell to a quirk function
      x86/intel_rdt: Remove redundant ternary operator on return
      x86/intel_rdt/cqm: Improve limbo list processing
      x86/intel_rdt/mbm: Fix MBM overflow handler during CPU hotplug
      x86/intel_rdt: Modify the intel_pqr_state for better performance
      x86/intel_rdt/cqm: Clear the default RMID during hotcpu
      x86/intel_rdt: Show bitmask of shareable resource with other executing units
      x86/intel_rdt/mbm: Handle counter overflow
      x86/intel_rdt/mbm: Add mbm counter initialization
      x86/intel_rdt/mbm: Basic counting of MBM events (total and local)
      x86/intel_rdt/cqm: Add CPU hotplug support
      x86/intel_rdt/cqm: Add sched_in support
      x86/intel_rdt: Introduce rdt_enable_key for scheduling
      x86/intel_rdt/cqm: Add mount,umount support
      x86/intel_rdt/cqm: Add rmdir support
      x86/intel_rdt: Separate the ctrl bits from rmdir
      x86/intel_rdt/cqm: Add mon_data
      x86/intel_rdt: Prepare for RDT monitor data support
      ...

commit e137a4d8f4dd2e277e355495b6b2cb241a8693c3
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Aug 1 07:11:37 2017 -0700

    x86/switch_to/64: Rewrite FS/GS switching yet again to fix AMD CPUs
    
    Switching FS and GS is a mess, and the current code is still subtly
    wrong: it assumes that "Loading a nonzero value into FS sets the
    index and base", which is false on AMD CPUs if the value being
    loaded is 1, 2, or 3.
    
    (The current code came from commit 3e2b68d752c9 ("x86/asm,
    sched/x86: Rewrite the FS and GS context switch code"), which made
    it better but didn't fully fix it.)
    
    Rewrite it to be much simpler and more obviously correct.  This
    should fix it fully on AMD CPUs and shouldn't adversely affect
    performance.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Chang Seok <chang.seok.bae@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index d8305ab3d61e..c85269a76511 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -148,6 +148,123 @@ void release_thread(struct task_struct *dead_task)
 	}
 }
 
+enum which_selector {
+	FS,
+	GS
+};
+
+/*
+ * Saves the FS or GS base for an outgoing thread if FSGSBASE extensions are
+ * not available.  The goal is to be reasonably fast on non-FSGSBASE systems.
+ * It's forcibly inlined because it'll generate better code and this function
+ * is hot.
+ */
+static __always_inline void save_base_legacy(struct task_struct *prev_p,
+					     unsigned short selector,
+					     enum which_selector which)
+{
+	if (likely(selector == 0)) {
+		/*
+		 * On Intel (without X86_BUG_NULL_SEG), the segment base could
+		 * be the pre-existing saved base or it could be zero.  On AMD
+		 * (with X86_BUG_NULL_SEG), the segment base could be almost
+		 * anything.
+		 *
+		 * This branch is very hot (it's hit twice on almost every
+		 * context switch between 64-bit programs), and avoiding
+		 * the RDMSR helps a lot, so we just assume that whatever
+		 * value is already saved is correct.  This matches historical
+		 * Linux behavior, so it won't break existing applications.
+		 *
+		 * To avoid leaking state, on non-X86_BUG_NULL_SEG CPUs, if we
+		 * report that the base is zero, it needs to actually be zero:
+		 * see the corresponding logic in load_seg_legacy.
+		 */
+	} else {
+		/*
+		 * If the selector is 1, 2, or 3, then the base is zero on
+		 * !X86_BUG_NULL_SEG CPUs and could be anything on
+		 * X86_BUG_NULL_SEG CPUs.  In the latter case, Linux
+		 * has never attempted to preserve the base across context
+		 * switches.
+		 *
+		 * If selector > 3, then it refers to a real segment, and
+		 * saving the base isn't necessary.
+		 */
+		if (which == FS)
+			prev_p->thread.fsbase = 0;
+		else
+			prev_p->thread.gsbase = 0;
+	}
+}
+
+static __always_inline void save_fsgs(struct task_struct *task)
+{
+	savesegment(fs, task->thread.fsindex);
+	savesegment(gs, task->thread.gsindex);
+	save_base_legacy(task, task->thread.fsindex, FS);
+	save_base_legacy(task, task->thread.gsindex, GS);
+}
+
+static __always_inline void loadseg(enum which_selector which,
+				    unsigned short sel)
+{
+	if (which == FS)
+		loadsegment(fs, sel);
+	else
+		load_gs_index(sel);
+}
+
+static __always_inline void load_seg_legacy(unsigned short prev_index,
+					    unsigned long prev_base,
+					    unsigned short next_index,
+					    unsigned long next_base,
+					    enum which_selector which)
+{
+	if (likely(next_index <= 3)) {
+		/*
+		 * The next task is using 64-bit TLS, is not using this
+		 * segment at all, or is having fun with arcane CPU features.
+		 */
+		if (next_base == 0) {
+			/*
+			 * Nasty case: on AMD CPUs, we need to forcibly zero
+			 * the base.
+			 */
+			if (static_cpu_has_bug(X86_BUG_NULL_SEG)) {
+				loadseg(which, __USER_DS);
+				loadseg(which, next_index);
+			} else {
+				/*
+				 * We could try to exhaustively detect cases
+				 * under which we can skip the segment load,
+				 * but there's really only one case that matters
+				 * for performance: if both the previous and
+				 * next states are fully zeroed, we can skip
+				 * the load.
+				 *
+				 * (This assumes that prev_base == 0 has no
+				 * false positives.  This is the case on
+				 * Intel-style CPUs.)
+				 */
+				if (likely(prev_index | next_index | prev_base))
+					loadseg(which, next_index);
+			}
+		} else {
+			if (prev_index != next_index)
+				loadseg(which, next_index);
+			wrmsrl(which == FS ? MSR_FS_BASE : MSR_KERNEL_GS_BASE,
+			       next_base);
+		}
+	} else {
+		/*
+		 * The next task is using a real segment.  Loading the selector
+		 * is sufficient.
+		 */
+		loadseg(which, next_index);
+	}
+}
+
 int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 		unsigned long arg, struct task_struct *p, unsigned long tls)
 {
@@ -285,7 +402,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	struct fpu *next_fpu = &next->fpu;
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &per_cpu(cpu_tss, cpu);
-	unsigned prev_fsindex, prev_gsindex;
 
 	WARN_ON_ONCE(IS_ENABLED(CONFIG_DEBUG_ENTRY) &&
 		     this_cpu_read(irq_count) != -1);
@@ -297,8 +413,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 *
 	 * (e.g. xen_load_tls())
 	 */
-	savesegment(fs, prev_fsindex);
-	savesegment(gs, prev_gsindex);
+	save_fsgs(prev_p);
 
 	/*
 	 * Load TLS before restoring any segments so that segment loads
@@ -337,108 +452,10 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	if (unlikely(next->ds | prev->ds))
 		loadsegment(ds, next->ds);
 
-	/*
-	 * Switch FS and GS.
-	 *
-	 * These are even more complicated than DS and ES: they have
-	 * 64-bit bases are that controlled by arch_prctl.  The bases
-	 * don't necessarily match the selectors, as user code can do
-	 * any number of things to cause them to be inconsistent.
-	 *
-	 * We don't promise to preserve the bases if the selectors are
-	 * nonzero.  We also don't promise to preserve the base if the
-	 * selector is zero and the base doesn't match whatever was
-	 * most recently passed to ARCH_SET_FS/GS.  (If/when the
-	 * FSGSBASE instructions are enabled, we'll need to offer
-	 * stronger guarantees.)
-	 *
-	 * As an invariant,
-	 * (fsbase != 0 && fsindex != 0) || (gsbase != 0 && gsindex != 0) is
-	 * impossible.
-	 */
-	if (next->fsindex) {
-		/* Loading a nonzero value into FS sets the index and base. */
-		loadsegment(fs, next->fsindex);
-	} else {
-		if (next->fsbase) {
-			/* Next index is zero but next base is nonzero. */
-			if (prev_fsindex)
-				loadsegment(fs, 0);
-			wrmsrl(MSR_FS_BASE, next->fsbase);
-		} else {
-			/* Next base and index are both zero. */
-			if (static_cpu_has_bug(X86_BUG_NULL_SEG)) {
-				/*
-				 * We don't know the previous base and can't
-				 * find out without RDMSR.  Forcibly clear it.
-				 */
-				loadsegment(fs, __USER_DS);
-				loadsegment(fs, 0);
-			} else {
-				/*
-				 * If the previous index is zero and ARCH_SET_FS
-				 * didn't change the base, then the base is
-				 * also zero and we don't need to do anything.
-				 */
-				if (prev->fsbase || prev_fsindex)
-					loadsegment(fs, 0);
-			}
-		}
-	}
-	/*
-	 * Save the old state and preserve the invariant.
-	 * NB: if prev_fsindex == 0, then we can't reliably learn the base
-	 * without RDMSR because Intel user code can zero it without telling
-	 * us and AMD user code can program any 32-bit value without telling
-	 * us.
-	 */
-	if (prev_fsindex)
-		prev->fsbase = 0;
-	prev->fsindex = prev_fsindex;
-
-	if (next->gsindex) {
-		/* Loading a nonzero value into GS sets the index and base. */
-		load_gs_index(next->gsindex);
-	} else {
-		if (next->gsbase) {
-			/* Next index is zero but next base is nonzero. */
-			if (prev_gsindex)
-				load_gs_index(0);
-			wrmsrl(MSR_KERNEL_GS_BASE, next->gsbase);
-		} else {
-			/* Next base and index are both zero. */
-			if (static_cpu_has_bug(X86_BUG_NULL_SEG)) {
-				/*
-				 * We don't know the previous base and can't
-				 * find out without RDMSR.  Forcibly clear it.
-				 *
-				 * This contains a pointless SWAPGS pair.
-				 * Fixing it would involve an explicit check
-				 * for Xen or a new pvop.
-				 */
-				load_gs_index(__USER_DS);
-				load_gs_index(0);
-			} else {
-				/*
-				 * If the previous index is zero and ARCH_SET_GS
-				 * didn't change the base, then the base is
-				 * also zero and we don't need to do anything.
-				 */
-				if (prev->gsbase || prev_gsindex)
-					load_gs_index(0);
-			}
-		}
-	}
-	/*
-	 * Save the old state and preserve the invariant.
-	 * NB: if prev_gsindex == 0, then we can't reliably learn the base
-	 * without RDMSR because Intel user code can zero it without telling
-	 * us and AMD user code can program any 32-bit value without telling
-	 * us.
-	 */
-	if (prev_gsindex)
-		prev->gsbase = 0;
-	prev->gsindex = prev_gsindex;
+	load_seg_legacy(prev->fsindex, prev->fsbase,
+			next->fsindex, next->fsbase, FS);
+	load_seg_legacy(prev->gsindex, prev->gsbase,
+			next->gsindex, next->gsbase, GS);
 
 	switch_fpu_finish(next_fpu, cpu);
 

commit 767d035d838f4fd6b5a5bbd7a3f6d293b7f65a49
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Aug 1 07:11:34 2017 -0700

    x86/fsgsbase/64: Fully initialize FS and GS state in start_thread_common
    
    execve used to leak FSBASE and GSBASE on AMD CPUs.  Fix it.
    
    The security impact of this bug is small but not quite zero -- it
    could weaken ASLR when a privileged task execs a less privileged
    program, but only if program changed bitness across the exec, or the
    child binary was highly unusual or actively malicious.  A child
    program that was compromised after the exec would not have access to
    the leaked base.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Chang Seok <chang.seok.bae@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e04941fb67fb..d8305ab3d61e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -228,10 +228,19 @@ start_thread_common(struct pt_regs *regs, unsigned long new_ip,
 		    unsigned long new_sp,
 		    unsigned int _cs, unsigned int _ss, unsigned int _ds)
 {
+	WARN_ON_ONCE(regs != current_pt_regs());
+
+	if (static_cpu_has(X86_BUG_NULL_SEG)) {
+		/* Loading zero below won't clear the base. */
+		loadsegment(fs, __USER_DS);
+		load_gs_index(__USER_DS);
+	}
+
 	loadsegment(fs, 0);
 	loadsegment(es, _ds);
 	loadsegment(ds, _ds);
 	load_gs_index(0);
+
 	regs->ip		= new_ip;
 	regs->sp		= new_sp;
 	regs->cs		= _cs;

commit 0583020456cea9fcf43b84bb13a41eab059ae0a8
Author: Vikas Shivappa <vikas.shivappa@linux.intel.com>
Date:   Tue Jul 25 14:14:23 2017 -0700

    x86/intel_rdt: Change file names to accommodate RDT monitor code
    
    Because the "perf cqm" and resctrl code were separately added and
    indivdually configurable, there seem to be separate context switch code
    and also things on global .h which are not really needed.
    
    Move only the scheduling specific code and definitions to
    <asm/intel_rdt_sched.h> and the put all the other declarations to a
    local intel_rdt.h.
    
    h/t to Reinette Chatre for pointing out that we should separate the
    public interfaces used by other parts of the kernel from private
    objects shared between the various files comprising RDT.
    
    No functional change.
    
    Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: ravi.v.shankar@intel.com
    Cc: tony.luck@intel.com
    Cc: fenghua.yu@intel.com
    Cc: peterz@infradead.org
    Cc: eranian@google.com
    Cc: vikas.shivappa@intel.com
    Cc: ak@linux.intel.com
    Cc: davidcc@google.com
    Cc: reinette.chatre@intel.com
    Link: http://lkml.kernel.org/r/1501017287-28083-5-git-send-email-vikas.shivappa@linux.intel.com

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index c3169be4c596..77a35c817b2b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -52,7 +52,7 @@
 #include <asm/switch_to.h>
 #include <asm/xen/hypervisor.h>
 #include <asm/vdso.h>
-#include <asm/intel_rdt.h>
+#include <asm/intel_rdt_sched.h>
 #include <asm/unistd.h>
 #ifdef CONFIG_IA32_EMULATION
 /* Not included via unistd.h */

commit 99504819fc643160afd6813921b1d42b18e52a49
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Jul 28 06:00:32 2017 -0700

    x86/asm/32: Remove a bunch of '& 0xffff' from pt_regs segment reads
    
    Now that pt_regs properly defines segment fields as 16-bit on 32-bit
    CPUs, there's no need to mask off the high word.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 2987e3991c2b..e04941fb67fb 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -69,8 +69,7 @@ void __show_regs(struct pt_regs *regs, int all)
 	unsigned int fsindex, gsindex;
 	unsigned int ds, cs, es;
 
-	printk(KERN_DEFAULT "RIP: %04lx:%pS\n", regs->cs & 0xffff,
-		(void *)regs->ip);
+	printk(KERN_DEFAULT "RIP: %04lx:%pS\n", regs->cs, (void *)regs->ip);
 	printk(KERN_DEFAULT "RSP: %04lx:%016lx EFLAGS: %08lx", regs->ss,
 		regs->sp, regs->flags);
 	if (regs->orig_ax != -1)

commit 1d3e53e8624a3ec85f4041ca6d973da7c1575938
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Jul 11 10:33:38 2017 -0500

    x86/entry/64: Refactor IRQ stacks and make them NMI-safe
    
    This will allow IRQ stacks to nest inside NMIs or similar entries
    that can happen during IRQ stack setup or teardown.
    
    The new macros won't work correctly if they're invoked with IRQs on.
    Add a check under CONFIG_DEBUG_ENTRY to detect that.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    [ Use %r10 instead of %r11 in xen_do_hypervisor_callback to make objtool
      and ORC unwinder's lives a little easier. ]
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: live-patching@vger.kernel.org
    Link: http://lkml.kernel.org/r/b0b2ff5fb97d2da2e1d7e1f380190c92545c8bb5.1499786555.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index c3169be4c596..2987e3991c2b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -279,6 +279,9 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	struct tss_struct *tss = &per_cpu(cpu_tss, cpu);
 	unsigned prev_fsindex, prev_gsindex;
 
+	WARN_ON_ONCE(IS_ENABLED(CONFIG_DEBUG_ENTRY) &&
+		     this_cpu_read(irq_count) != -1);
+
 	switch_fpu_prepare(prev_fpu, cpu);
 
 	/* We must save %fs and %gs before load_TLS() because

commit 6c690ee1039b251e583fc65b28da30e97d6a7385
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Jun 12 10:26:14 2017 -0700

    x86/mm: Split read_cr3() into read_cr3_pa() and __read_cr3()
    
    The kernel has several code paths that read CR3.  Most of them assume that
    CR3 contains the PGD's physical address, whereas some of them awkwardly
    use PHYSICAL_PAGE_MASK to mask off low bits.
    
    Add explicit mask macros for CR3 and convert all of the CR3 readers.
    This will keep them from breaking when PCID is enabled.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: xen-devel <xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/883f8fb121f4616c1c1427ad87350bb2f5ffeca1.1497288170.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 9c39ab8bcc41..c3169be4c596 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -104,7 +104,7 @@ void __show_regs(struct pt_regs *regs, int all)
 
 	cr0 = read_cr0();
 	cr2 = read_cr2();
-	cr3 = read_cr3();
+	cr3 = __read_cr3();
 	cr4 = __read_cr4();
 
 	printk(KERN_DEFAULT "FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n",

commit bbf79d21bd4627a01ca8721c9373752819f8e4cc
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jun 6 19:31:16 2017 +0200

    x86/ldt: Rename ldt_struct::size to ::nr_entries
    
    ... because this is exactly what it is: the number of entries in the
    LDT. Calling it "size" is simply confusing and it is actually begging
    to be called "nr_entries" or somesuch, especially if you see constructs
    like:
    
            alloc_size = size * LDT_ENTRY_SIZE;
    
    since LDT_ENTRY_SIZE is the size of a single entry.
    
    There should be no functionality change resulting from this patch, as
    the before/after output from tools/testing/selftests/x86/ldt_gdt.c
    shows.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170606173116.13977-1-bp@alien8.de
    [ Renamed 'n_entries' to 'nr_entries' ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b6840bf3940b..9c39ab8bcc41 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -142,7 +142,7 @@ void release_thread(struct task_struct *dead_task)
 			pr_warn("WARNING: dead process %s still has LDT? <%p/%d>\n",
 				dead_task->comm,
 				dead_task->mm->context.ldt->entries,
-				dead_task->mm->context.ldt->size);
+				dead_task->mm->context.ldt->nr_entries);
 			BUG();
 		}
 #endif

commit 5e57f1d607d1cc0f54611162525ca6436e17e8b7
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 14 18:35:38 2017 +0100

    x86/xen: add CONFIG_XEN_PV to Kconfig
    
    All code to support Xen PV will get under this new option. For the
    beginning, check for it in the common code.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 825a1e47cf3e..b6840bf3940b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -446,7 +446,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 		     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))
 		__switch_to_xtra(prev_p, next_p, tss);
 
-#ifdef CONFIG_XEN
+#ifdef CONFIG_XEN_PV
 	/*
 	 * On Xen PV, IOPL bits in pt_regs->flags have no effect, and
 	 * current_pt_regs()->flags may not match the current task's

commit ada26481dfe698ac64b4aaf19a726e66eb8508c6
Author: Dmitry Safonov <dsafonov@virtuozzo.com>
Date:   Fri Mar 31 14:11:37 2017 +0300

    x86/mm: Make in_compat_syscall() work during exec
    
    The x86 mmap() code selects the mmap base for an allocation depending on
    the bitness of the syscall. For 64bit sycalls it select mm->mmap_base and
    for 32bit mm->mmap_compat_base.
    
    On execve the registers of the task invoking exec() are copied to the child
    pt_regs. So child->pt_regs->orig_ax contains the execve syscall number of the
    parent.
    
    exec() calls mmap() which in turn uses in_compat_syscall() to check whether
    the mapping is for a 32bit or a 64bit task. The decision is made on the
    following criteria:
    
      ia32    child->thread.status & TS_COMPAT
       x32    child->pt_regs.orig_ax & __X32_SYSCALL_BIT
      ia64    !ia32 && !x32
    
    child->thread.status is corretly set up in set_personality_*(), but the
    syscall number in child->pt_regs.orig_ax is left unmodified.
    
    Therefore the parent/child combinations work or fail in the following way:
    
    Parent Child Child->thread_status  child->pt_regs.orig_ax  in_compat()  Works
    ia64    ia64   TS_COMPAT == 0      __X32_SYSCALL_BIT == 0     false       Y
    ia64    ia32   TS_COMPAT == 1      __X32_SYSCALL_BIT == 0     true        Y
    ia64     x32   TS_COMPAT == 0      __X32_SYSCALL_BIT == 0     false       N
    ia32    ia64   TS_COMPAT == 0      __X32_SYSCALL_BIT == 0     false       Y
    ia32    ia32   TS_COMPAT == 1      __X32_SYSCALL_BIT == 0     true        Y
    ia32     x32   TS_COMPAT == 0      __X32_SYSCALL_BIT == 0     false       N
     x32    ia64   TS_COMPAT == 0      __X32_SYSCALL_BIT == 1     true        N
     x32    ia32   TS_COMPAT == 1      __X32_SYSCALL_BIT == 1     true        Y
     x32     x32   TS_COMPAT == 0      __X32_SYSCALL_BIT == 1     true        Y
    
    Make set_personality_*() store the syscall number incl. __X32_SYSCALL_BIT
    which corresponds to the newly started ELF executable in the childs
    pt_regs, i.e. pretend that the exec was invoked from a task with the same
    executable format.
    
    So both thread.status and pt_regs.orig_ax correspond to the new ELF format
    and in_compat_syscall() returns the correct result.
    
    [ tglx: Rewrote changelog ]
    
    Fixes: commit 1b028f784e8c ("x86/mm: Introduce mmap_compat_base() for 32-bit mmap()")
    Reported-by: Adam Borowski <kilobyte@angband.pl>
    Suggested-by: H. Peter Anvin <hpa@zytor.com>
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: 0x7f454c46@gmail.com
    Cc: linux-mm@kvack.org
    Cc: Andrei Vagin <avagin@gmail.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Link: http://lkml.kernel.org/r/20170331111137.28170-1-dsafonov@virtuozzo.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ea1a6180bf39..825a1e47cf3e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -53,6 +53,11 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/vdso.h>
 #include <asm/intel_rdt.h>
+#include <asm/unistd.h>
+#ifdef CONFIG_IA32_EMULATION
+/* Not included via unistd.h */
+#include <asm/unistd_32_ia32.h>
+#endif
 
 __visible DEFINE_PER_CPU(unsigned long, rsp_scratch);
 
@@ -494,6 +499,8 @@ void set_personality_64bit(void)
 	clear_thread_flag(TIF_IA32);
 	clear_thread_flag(TIF_ADDR32);
 	clear_thread_flag(TIF_X32);
+	/* Pretend that this comes from a 64bit execve */
+	task_pt_regs(current)->orig_ax = __NR_execve;
 
 	/* Ensure the corresponding mm is not marked. */
 	if (current->mm)
@@ -506,32 +513,50 @@ void set_personality_64bit(void)
 	current->personality &= ~READ_IMPLIES_EXEC;
 }
 
-void set_personality_ia32(bool x32)
+static void __set_personality_x32(void)
 {
-	/* inherit personality from parent */
+#ifdef CONFIG_X86_X32
+	clear_thread_flag(TIF_IA32);
+	set_thread_flag(TIF_X32);
+	if (current->mm)
+		current->mm->context.ia32_compat = TIF_X32;
+	current->personality &= ~READ_IMPLIES_EXEC;
+	/*
+	 * in_compat_syscall() uses the presence of the x32 syscall bit
+	 * flag to determine compat status.  The x86 mmap() code relies on
+	 * the syscall bitness so set x32 syscall bit right here to make
+	 * in_compat_syscall() work during exec().
+	 *
+	 * Pretend to come from a x32 execve.
+	 */
+	task_pt_regs(current)->orig_ax = __NR_x32_execve | __X32_SYSCALL_BIT;
+	current->thread.status &= ~TS_COMPAT;
+#endif
+}
 
+static void __set_personality_ia32(void)
+{
+#ifdef CONFIG_IA32_EMULATION
+	set_thread_flag(TIF_IA32);
+	clear_thread_flag(TIF_X32);
+	if (current->mm)
+		current->mm->context.ia32_compat = TIF_IA32;
+	current->personality |= force_personality32;
+	/* Prepare the first "return" to user space */
+	task_pt_regs(current)->orig_ax = __NR_ia32_execve;
+	current->thread.status |= TS_COMPAT;
+#endif
+}
+
+void set_personality_ia32(bool x32)
+{
 	/* Make sure to be in 32bit mode */
 	set_thread_flag(TIF_ADDR32);
 
-	/* Mark the associated mm as containing 32-bit tasks. */
-	if (x32) {
-		clear_thread_flag(TIF_IA32);
-		set_thread_flag(TIF_X32);
-		if (current->mm)
-			current->mm->context.ia32_compat = TIF_X32;
-		current->personality &= ~READ_IMPLIES_EXEC;
-		/* in_compat_syscall() uses the presence of the x32
-		   syscall bit flag to determine compat status */
-		current->thread.status &= ~TS_COMPAT;
-	} else {
-		set_thread_flag(TIF_IA32);
-		clear_thread_flag(TIF_X32);
-		if (current->mm)
-			current->mm->context.ia32_compat = TIF_IA32;
-		current->personality |= force_personality32;
-		/* Prepare the first "return" to user space */
-		current->thread.status |= TS_COMPAT;
-	}
+	if (x32)
+		__set_personality_x32();
+	else
+		__set_personality_ia32();
 }
 EXPORT_SYMBOL_GPL(set_personality_ia32);
 

commit 79170fda313ed5be2394f87aa2a00d597f8ed4a1
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:24 2017 -0700

    x86/syscalls/32: Wire up arch_prctl on x86-32
    
    Hook up arch_prctl to call do_arch_prctl() on x86-32, and in 32 bit compat
    mode on x86-64. This allows to have arch_prctls that are not specific to 64
    bits.
    
    On UML, simply stub out this syscall.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-7-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index d81b0a60a45c..ea1a6180bf39 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -635,6 +635,13 @@ SYSCALL_DEFINE2(arch_prctl, int, option, unsigned long, arg2)
 	return ret;
 }
 
+#ifdef CONFIG_IA32_EMULATION
+COMPAT_SYSCALL_DEFINE2(arch_prctl, int, option, unsigned long, arg2)
+{
+	return do_arch_prctl_common(current, option, arg2);
+}
+#endif
+
 unsigned long KSTK_ESP(struct task_struct *task)
 {
 	return task_pt_regs(task)->sp;

commit b0b9b014016d16ca7a192da986aa8ebae21bb995
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:23 2017 -0700

    x86/arch_prctl: Add do_arch_prctl_common()
    
    Add do_arch_prctl_common() to handle arch_prctls that are not specific to 64
    bit mode. Call it from the syscall entry point, but not any of the other
    callsites in the kernel, which all want one of the existing 64 bit only
    arch_prctls.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-6-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e37f764c11cc..d81b0a60a45c 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -626,7 +626,13 @@ long do_arch_prctl_64(struct task_struct *task, int option, unsigned long arg2)
 
 SYSCALL_DEFINE2(arch_prctl, int, option, unsigned long, arg2)
 {
-	return do_arch_prctl_64(current, option, arg2);
+	long ret;
+
+	ret = do_arch_prctl_64(current, option, arg2);
+	if (ret == -EINVAL)
+		ret = do_arch_prctl_common(current, option, arg2);
+
+	return ret;
 }
 
 unsigned long KSTK_ESP(struct task_struct *task)

commit 17a6e1b8e8e8539f89156643f8c3073f09ec446a
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:22 2017 -0700

    x86/arch_prctl/64: Rename do_arch_prctl() to do_arch_prctl_64()
    
    In order to introduce new arch_prctls that are not 64 bit only, rename the
    existing 64 bit implementation to do_arch_prctl_64(). Also rename the
    second argument of that function from 'addr' to 'arg2', because it will no
    longer always be an address.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-5-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index bf9d7b6c0223..e37f764c11cc 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -205,7 +205,7 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 				(struct user_desc __user *)tls, 0);
 		else
 #endif
-			err = do_arch_prctl(p, ARCH_SET_FS, tls);
+			err = do_arch_prctl_64(p, ARCH_SET_FS, tls);
 		if (err)
 			goto out;
 	}
@@ -548,7 +548,7 @@ static long prctl_map_vdso(const struct vdso_image *image, unsigned long addr)
 }
 #endif
 
-long do_arch_prctl(struct task_struct *task, int option, unsigned long addr)
+long do_arch_prctl_64(struct task_struct *task, int option, unsigned long arg2)
 {
 	int ret = 0;
 	int doit = task == current;
@@ -556,62 +556,64 @@ long do_arch_prctl(struct task_struct *task, int option, unsigned long addr)
 
 	switch (option) {
 	case ARCH_SET_GS:
-		if (addr >= TASK_SIZE_MAX)
+		if (arg2 >= TASK_SIZE_MAX)
 			return -EPERM;
 		cpu = get_cpu();
 		task->thread.gsindex = 0;
-		task->thread.gsbase = addr;
+		task->thread.gsbase = arg2;
 		if (doit) {
 			load_gs_index(0);
-			ret = wrmsrl_safe(MSR_KERNEL_GS_BASE, addr);
+			ret = wrmsrl_safe(MSR_KERNEL_GS_BASE, arg2);
 		}
 		put_cpu();
 		break;
 	case ARCH_SET_FS:
 		/* Not strictly needed for fs, but do it for symmetry
 		   with gs */
-		if (addr >= TASK_SIZE_MAX)
+		if (arg2 >= TASK_SIZE_MAX)
 			return -EPERM;
 		cpu = get_cpu();
 		task->thread.fsindex = 0;
-		task->thread.fsbase = addr;
+		task->thread.fsbase = arg2;
 		if (doit) {
 			/* set the selector to 0 to not confuse __switch_to */
 			loadsegment(fs, 0);
-			ret = wrmsrl_safe(MSR_FS_BASE, addr);
+			ret = wrmsrl_safe(MSR_FS_BASE, arg2);
 		}
 		put_cpu();
 		break;
 	case ARCH_GET_FS: {
 		unsigned long base;
+
 		if (doit)
 			rdmsrl(MSR_FS_BASE, base);
 		else
 			base = task->thread.fsbase;
-		ret = put_user(base, (unsigned long __user *)addr);
+		ret = put_user(base, (unsigned long __user *)arg2);
 		break;
 	}
 	case ARCH_GET_GS: {
 		unsigned long base;
+
 		if (doit)
 			rdmsrl(MSR_KERNEL_GS_BASE, base);
 		else
 			base = task->thread.gsbase;
-		ret = put_user(base, (unsigned long __user *)addr);
+		ret = put_user(base, (unsigned long __user *)arg2);
 		break;
 	}
 
 #ifdef CONFIG_CHECKPOINT_RESTORE
 # ifdef CONFIG_X86_X32_ABI
 	case ARCH_MAP_VDSO_X32:
-		return prctl_map_vdso(&vdso_image_x32, addr);
+		return prctl_map_vdso(&vdso_image_x32, arg2);
 # endif
 # if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION
 	case ARCH_MAP_VDSO_32:
-		return prctl_map_vdso(&vdso_image_32, addr);
+		return prctl_map_vdso(&vdso_image_32, arg2);
 # endif
 	case ARCH_MAP_VDSO_64:
-		return prctl_map_vdso(&vdso_image_64, addr);
+		return prctl_map_vdso(&vdso_image_64, arg2);
 #endif
 
 	default:
@@ -622,9 +624,9 @@ long do_arch_prctl(struct task_struct *task, int option, unsigned long addr)
 	return ret;
 }
 
-SYSCALL_DEFINE2(arch_prctl, int, option, unsigned long, addr)
+SYSCALL_DEFINE2(arch_prctl, int, option, unsigned long, arg2)
 {
-	return do_arch_prctl(current, option, addr);
+	return do_arch_prctl_64(current, option, arg2);
 }
 
 unsigned long KSTK_ESP(struct task_struct *task)

commit ff3f097eef30151f5ee250859e0fe8a0ec02c160
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:21 2017 -0700

    x86/arch_prctl/64: Use SYSCALL_DEFINE2 to define sys_arch_prctl()
    
    Use the SYSCALL_DEFINE2 macro instead of manually defining it.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-4-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4377cfe8e449..bf9d7b6c0223 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -37,6 +37,7 @@
 #include <linux/uaccess.h>
 #include <linux/io.h>
 #include <linux/ftrace.h>
+#include <linux/syscalls.h>
 
 #include <asm/pgtable.h>
 #include <asm/processor.h>
@@ -621,7 +622,7 @@ long do_arch_prctl(struct task_struct *task, int option, unsigned long addr)
 	return ret;
 }
 
-long sys_arch_prctl(int option, unsigned long addr)
+SYSCALL_DEFINE2(arch_prctl, int, option, unsigned long, addr)
 {
 	return do_arch_prctl(current, option, addr);
 }

commit dd93938a92dc067aba70c401bdf2e50ed58083db
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:20 2017 -0700

    x86/arch_prctl: Rename 'code' argument to 'option'
    
    The x86 specific arch_prctl() arbitrarily changed prctl's 'option' to
    'code'. Before adding new options, rename it.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-3-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index d6b784a5520d..4377cfe8e449 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -547,13 +547,13 @@ static long prctl_map_vdso(const struct vdso_image *image, unsigned long addr)
 }
 #endif
 
-long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
+long do_arch_prctl(struct task_struct *task, int option, unsigned long addr)
 {
 	int ret = 0;
 	int doit = task == current;
 	int cpu;
 
-	switch (code) {
+	switch (option) {
 	case ARCH_SET_GS:
 		if (addr >= TASK_SIZE_MAX)
 			return -EPERM;
@@ -621,9 +621,9 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 	return ret;
 }
 
-long sys_arch_prctl(int code, unsigned long addr)
+long sys_arch_prctl(int option, unsigned long addr)
 {
-	return do_arch_prctl(current, code, addr);
+	return do_arch_prctl(current, option, addr);
 }
 
 unsigned long KSTK_ESP(struct task_struct *task)

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 124f5652ae3c..d6b784a5520d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -18,6 +18,7 @@
 #include <linux/errno.h>
 #include <linux/sched.h>
 #include <linux/sched/task.h>
+#include <linux/sched/task_stack.h>
 #include <linux/fs.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>

commit 299300258d1bc4e997b7db340a2e06636757fe2e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task.h>
    
    We are going to split <linux/sched/task.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index a61e141b6891..124f5652ae3c 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -17,6 +17,7 @@
 #include <linux/cpu.h>
 #include <linux/errno.h>
 #include <linux/sched.h>
+#include <linux/sched/task.h>
 #include <linux/fs.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>

commit eb254f323bd50ab7e3cc385f2fc641a595cc8b37
Merge: f79f7b1b4f91 76ae054c69a7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 22 09:25:45 2016 -0800

    Merge branch 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cache allocation interface from Thomas Gleixner:
     "This provides support for Intel's Cache Allocation Technology, a cache
      partitioning mechanism.
    
      The interface is odd, but the hardware interface of that CAT stuff is
      odd as well.
    
      We tried hard to come up with an abstraction, but that only allows
      rather simple partitioning, but no way of sharing and dealing with the
      per package nature of this mechanism.
    
      In the end we decided to expose the allocation bitmaps directly so all
      combinations of the hardware can be utilized.
    
      There are two ways of associating a cache partition:
    
       - Task
    
         A task can be added to a resource group. It uses the cache
         partition associated to the group.
    
       - CPU
    
         All tasks which are not member of a resource group use the group to
         which the CPU they are running on is associated with.
    
         That allows for simple CPU based partitioning schemes.
    
      The main expected user sare:
    
       - Virtualization so a VM can only trash only the associated part of
         the cash w/o disturbing others
    
       - Real-Time systems to seperate RT and general workloads.
    
       - Latency sensitive enterprise workloads
    
       - In theory this also can be used to protect against cache side
         channel attacks"
    
    [ Intel RDT is "Resource Director Technology". The interface really is
      rather odd and very specific, which delayed this pull request while I
      was thinking about it. The pull request itself came in early during
      the merge window, I just delayed it until things had calmed down and I
      had more time.
    
      But people tell me they'll use this, and the good news is that it is
      _so_ specific that it's rather independent of anything else, and no
      user is going to depend on the interface since it's pretty rare. So if
      push comes to shove, we can just remove the interface and nothing will
      break ]
    
    * 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (31 commits)
      x86/intel_rdt: Implement show_options() for resctrlfs
      x86/intel_rdt: Call intel_rdt_sched_in() with preemption disabled
      x86/intel_rdt: Update task closid immediately on CPU in rmdir and unmount
      x86/intel_rdt: Fix setting of closid when adding CPUs to a group
      x86/intel_rdt: Update percpu closid immeditately on CPUs affected by changee
      x86/intel_rdt: Reset per cpu closids on unmount
      x86/intel_rdt: Select KERNFS when enabling INTEL_RDT_A
      x86/intel_rdt: Prevent deadlock against hotplug lock
      x86/intel_rdt: Protect info directory from removal
      x86/intel_rdt: Add info files to Documentation
      x86/intel_rdt: Export the minimum number of set mask bits in sysfs
      x86/intel_rdt: Propagate error in rdt_mount() properly
      x86/intel_rdt: Add a missing #include
      MAINTAINERS: Add maintainer for Intel RDT resource allocation
      x86/intel_rdt: Add scheduler hook
      x86/intel_rdt: Add schemata file
      x86/intel_rdt: Add tasks files
      x86/intel_rdt: Add cpus file
      x86/intel_rdt: Add mkdir to resctrl file system
      x86/intel_rdt: Add "info" files to resctrl file system
      ...

commit 212f30008a284a9312d95dad6cc237ff81173d73
Merge: 6f3be0f04354 34bc3560c657
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 14:55:04 2016 -0800

    Merge branch 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 idle updates from Ingo Molnar:
     "There were two bigger changes in this development cycle:
    
       - remove idle notifiers:
    
           32 files changed, 74 insertions(+), 803 deletions(-)
    
         These notifiers were of questionable value and the main usecase,
         the i7300 driver, was essentially unmaintained and can be removed,
         plus modern power management concepts don't need the callback - so
         use this golden opportunity and get rid of this opaque and fragile
         callback from a latency sensitive code path.
    
         (Len Brown, Thomas Gleixner)
    
       - improve the AMD Erratum 400 workaround that used high overhead MSR
         polling in the idle loop (Borisla Petkov, Thomas Gleixner)"
    
    * 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Remove empty idle.h header
      x86/amd: Simplify AMD E400 aware idle routine
      x86/amd: Check for the C1E bug post ACPI subsystem init
      x86/bugs: Separate AMD E400 erratum and C1E bug
      x86/cpufeature: Provide helper to set bugs bits
      x86/idle: Remove enter_idle(), exit_idle()
      x86: Remove x86_test_and_clear_bit_percpu()
      x86/idle: Remove is_idle flag
      x86/idle: Remove idle_notifier
      i7300_idle: Remove this driver

commit 518bacf5a569d111e256d58b9fbc8d7b80ec42ea
Merge: 535b2f73f6f6 064e6a8ba61a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 14:27:49 2016 -0800

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - do a large round of simplifications after all CPUs do 'eager' FPU
         context switching in v4.9: remove CR0 twiddling, remove leftover
         eager/lazy bts, etc (Andy Lutomirski)
    
       - more FPU code simplifications: remove struct fpu::counter, clarify
         nomenclature, remove unnecessary arguments/functions and better
         structure the code (Rik van Riel)"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fpu: Remove clts()
      x86/fpu: Remove stts()
      x86/fpu: Handle #NM without FPU emulation as an error
      x86/fpu, lguest: Remove CR0.TS support
      x86/fpu, kvm: Remove host CR0.TS manipulation
      x86/fpu: Remove irq_ts_save() and irq_ts_restore()
      x86/fpu: Stop saving and restoring CR0.TS in fpu__init_check_bugs()
      x86/fpu: Get rid of two redundant clts() calls
      x86/fpu: Finish excising 'eagerfpu'
      x86/fpu: Split old_fpu & new_fpu handling into separate functions
      x86/fpu: Remove 'cpu' argument from __cpu_invalidate_fpregs_state()
      x86/fpu: Split old & new FPU code paths
      x86/fpu: Remove __fpregs_(de)activate()
      x86/fpu: Rename lazy restore functions to "register state valid"
      x86/fpu, kvm: Remove KVM vcpu->fpu_counter
      x86/fpu: Remove struct fpu::counter
      x86/fpu: Remove use_eager_fpu()
      x86/fpu: Remove the XFEATURE_MASK_EAGER/LAZY distinction
      x86/fpu: Hard-disable lazy FPU mode
      x86/crypto, x86/fpu: Remove X86_FEATURE_EAGER_FPU #ifdef from the crc32c code

commit 34bc3560c657d3d4fb17367ed9bfda803166dce0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Dec 9 19:29:12 2016 +0100

    x86: Remove empty idle.h header
    
    One include less is always a good thing(tm). Good riddance.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/20161209182912.2726-6-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b3760b3c1ca0..bb1517106ec1 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -44,7 +44,6 @@
 #include <asm/desc.h>
 #include <asm/proto.h>
 #include <asm/ia32.h>
-#include <asm/idle.h>
 #include <asm/syscalls.h>
 #include <asm/debugreg.h>
 #include <asm/switch_to.h>

commit 4f341a5e48443fcc2e2d935ca990e462c02bb1a6
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Fri Oct 28 15:04:48 2016 -0700

    x86/intel_rdt: Add scheduler hook
    
    Hook the x86 scheduler code to update closid based on whether the current
    task is assigned to a specific closid or running on a CPU assigned to a
    specific closid.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Cc: "Ravi V Shankar" <ravi.v.shankar@intel.com>
    Cc: "Tony Luck" <tony.luck@intel.com>
    Cc: "Shaohua Li" <shli@fb.com>
    Cc: "Sai Prakhya" <sai.praneeth.prakhya@intel.com>
    Cc: "Peter Zijlstra" <peterz@infradead.org>
    Cc: "Stephane Eranian" <eranian@google.com>
    Cc: "Dave Hansen" <dave.hansen@intel.com>
    Cc: "David Carrillo-Cisneros" <davidcc@google.com>
    Cc: "Nilay Vaish" <nilayvaish@gmail.com>
    Cc: "Vikas Shivappa" <vikas.shivappa@linux.intel.com>
    Cc: "Ingo Molnar" <mingo@elte.hu>
    Cc: "Borislav Petkov" <bp@suse.de>
    Cc: "H. Peter Anvin" <h.peter.anvin@intel.com>
    Link: http://lkml.kernel.org/r/1477692289-37412-10-git-send-email-fenghua.yu@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b3760b3c1ca0..acd7d6f507af 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -50,6 +50,7 @@
 #include <asm/switch_to.h>
 #include <asm/xen/hypervisor.h>
 #include <asm/vdso.h>
+#include <asm/intel_rdt.h>
 
 __visible DEFINE_PER_CPU(unsigned long, rsp_scratch);
 
@@ -473,6 +474,9 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 			loadsegment(ss, __KERNEL_DS);
 	}
 
+	/* Load the Intel cache allocation PQR MSR. */
+	intel_rdt_sched_in();
+
 	return prev_p;
 }
 

commit bb5e5ce545f2031c96f7901cd8d1698ea3ca4c9c
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Tue Oct 25 09:51:12 2016 -0500

    x86/dumpstack: Remove kernel text addresses from stack dump
    
    Printing kernel text addresses in stack dumps is of questionable value,
    especially now that address randomization is becoming common.
    
    It can be a security issue because it leaks kernel addresses.  It also
    affects the usefulness of the stack dump.  Linus says:
    
      "I actually spend time cleaning up commit messages in logs, because
      useless data that isn't actually information (random hex numbers) is
      actively detrimental.
    
      It makes commit logs less legible.
    
      It also makes it harder to parse dumps.
    
      It's not useful. That makes it actively bad.
    
      I probably look at more oops reports than most people. I have not
      found the hex numbers useful for the last five years, because they are
      just randomized crap.
    
      The stack content thing just makes code scroll off the screen etc, for
      example."
    
    The only real downside to removing these addresses is that they can be
    used to disambiguate duplicate symbol names.  However such cases are
    rare, and the context of the stack dump should be enough to be able to
    figure it out.
    
    There's now a 'faddr2line' script which can be used to convert a
    function address to a file name and line:
    
      $ ./scripts/faddr2line ~/k/vmlinux write_sysrq_trigger+0x51/0x60
      write_sysrq_trigger+0x51/0x60:
      write_sysrq_trigger at drivers/tty/sysrq.c:1098
    
    Or gdb can be used:
    
      $ echo "list *write_sysrq_trigger+0x51" |gdb ~/k/vmlinux |grep "is in"
      (gdb) 0xffffffff815b5d83 is in driver_probe_device (/home/jpoimboe/git/linux/drivers/base/dd.c:378).
    
    (But note that when there are duplicate symbol names, gdb will only show
    the first symbol it finds.  faddr2line is recommended over gdb because
    it handles duplicates and it also does function size checking.)
    
    Here's an example of what a stack dump looks like after this change:
    
      BUG: unable to handle kernel NULL pointer dereference at           (null)
      IP: sysrq_handle_crash+0x45/0x80
      PGD 36bfa067 [   29.650644] PUD 7aca3067
      Oops: 0002 [#1] PREEMPT SMP
      Modules linked in: ...
      CPU: 1 PID: 786 Comm: bash Tainted: G            E   4.9.0-rc1+ #1
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.9.1-1.fc24 04/01/2014
      task: ffff880078582a40 task.stack: ffffc90000ba8000
      RIP: 0010:sysrq_handle_crash+0x45/0x80
      RSP: 0018:ffffc90000babdc8 EFLAGS: 00010296
      RAX: ffff880078582a40 RBX: 0000000000000063 RCX: 0000000000000001
      RDX: 0000000000000001 RSI: 0000000000000000 RDI: 0000000000000292
      RBP: ffffc90000babdc8 R08: 0000000b31866061 R09: 0000000000000000
      R10: 0000000000000001 R11: 0000000000000000 R12: 0000000000000000
      R13: 0000000000000007 R14: ffffffff81ee8680 R15: 0000000000000000
      FS:  00007ffb43869700(0000) GS:ffff88007d400000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      CR2: 0000000000000000 CR3: 000000007a3e9000 CR4: 00000000001406e0
      Stack:
       ffffc90000babe00 ffffffff81572d08 ffffffff81572bd5 0000000000000002
       0000000000000000 ffff880079606600 00007ffb4386e000 ffffc90000babe20
       ffffffff81573201 ffff880036a3fd00 fffffffffffffffb ffffc90000babe40
      Call Trace:
       __handle_sysrq+0x138/0x220
       ? __handle_sysrq+0x5/0x220
       write_sysrq_trigger+0x51/0x60
       proc_reg_write+0x42/0x70
       __vfs_write+0x37/0x140
       ? preempt_count_sub+0xa1/0x100
       ? __sb_start_write+0xf5/0x210
       ? vfs_write+0x183/0x1a0
       vfs_write+0xb8/0x1a0
       SyS_write+0x58/0xc0
       entry_SYSCALL_64_fastpath+0x1f/0xc2
      RIP: 0033:0x7ffb42f55940
      RSP: 002b:00007ffd33bb6b18 EFLAGS: 00000246 ORIG_RAX: 0000000000000001
      RAX: ffffffffffffffda RBX: 0000000000000046 RCX: 00007ffb42f55940
      RDX: 0000000000000002 RSI: 00007ffb4386e000 RDI: 0000000000000001
      RBP: 0000000000000011 R08: 00007ffb4321ea40 R09: 00007ffb43869700
      R10: 00007ffb43869700 R11: 0000000000000246 R12: 0000000000778a10
      R13: 00007ffd33bb5c00 R14: 0000000000000007 R15: 0000000000000010
      Code: 34 e8 d0 34 bc ff 48 c7 c2 3b 2b 57 81 be 01 00 00 00 48 c7 c7 e0 dd e5 81 e8 a8 55 ba ff c7 05 0e 3f de 00 01 00 00 00 0f ae f8 <c6> 04 25 00 00 00 00 01 5d c3 e8 4c 49 bc ff 84 c0 75 c3 48 c7
      RIP: sysrq_handle_crash+0x45/0x80 RSP: ffffc90000babdc8
      CR2: 0000000000000000
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/69329cb29b8f324bb5fcea14d61d224807fb6488.1477405374.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index f1c36da4c9b5..c99f1ca35eb5 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -61,10 +61,10 @@ void __show_regs(struct pt_regs *regs, int all)
 	unsigned int fsindex, gsindex;
 	unsigned int ds, cs, es;
 
-	printk(KERN_DEFAULT "RIP: %04lx:[<%016lx>] %pS\n", regs->cs & 0xffff,
-			regs->ip, (void *)regs->ip);
+	printk(KERN_DEFAULT "RIP: %04lx:%pS\n", regs->cs & 0xffff,
+		(void *)regs->ip);
 	printk(KERN_DEFAULT "RSP: %04lx:%016lx EFLAGS: %08lx", regs->ss,
-			regs->sp, regs->flags);
+		regs->sp, regs->flags);
 	if (regs->orig_ax != -1)
 		pr_cont(" ORIG_RAX: %016lx\n", regs->orig_ax);
 	else

commit 6fa81a12b3f1834a22167aa2d7b24dcc4bf884e3
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Thu Oct 20 11:34:45 2016 -0500

    x86/dumpstack: Print orig_ax in __show_regs()
    
    The value of regs->orig_ax contains potentially useful debugging data:
    For syscalls it contains the syscall number.  For interrupts it contains
    the (negated) vector number.  To reduce noise, print it only if it has a
    useful value (i.e., something other than -1).
    
    Here's what it looks like for a write syscall:
    
      RIP: 0033:[<00007f53ad7b1940>] 0x7f53ad7b1940
      RSP: 002b:00007fff8de66558 EFLAGS: 00000246 ORIG_RAX: 0000000000000001
      RAX: ffffffffffffffda RBX: 0000000000000046 RCX: 00007f53ad7b1940
      RDX: 0000000000000002 RSI: 00007f53ae0ca000 RDI: 0000000000000001
      ...
    
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/93f0fe0307a4af884d3fca00edabcc8cff236002.1476973742.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b3b50ac6a302..f1c36da4c9b5 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -63,8 +63,13 @@ void __show_regs(struct pt_regs *regs, int all)
 
 	printk(KERN_DEFAULT "RIP: %04lx:[<%016lx>] %pS\n", regs->cs & 0xffff,
 			regs->ip, (void *)regs->ip);
-	printk(KERN_DEFAULT "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,
+	printk(KERN_DEFAULT "RSP: %04lx:%016lx EFLAGS: %08lx", regs->ss,
 			regs->sp, regs->flags);
+	if (regs->orig_ax != -1)
+		pr_cont(" ORIG_RAX: %016lx\n", regs->orig_ax);
+	else
+		pr_cont("\n");
+
 	printk(KERN_DEFAULT "RAX: %016lx RBX: %016lx RCX: %016lx\n",
 	       regs->ax, regs->bx, regs->cx);
 	printk(KERN_DEFAULT "RDX: %016lx RSI: %016lx RDI: %016lx\n",

commit 1141c3e39c64e4aba2d98cb3dcca95369c9dafbe
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Thu Oct 20 11:34:44 2016 -0500

    x86/dumpstack: Fix duplicate RIP address display in __show_regs()
    
    The RIP address is shown twice in __show_regs().  Before:
    
      RIP: 0010:[<ffffffff81070446>]  [<ffffffff81070446>] native_write_msr+0x6/0x30
    
    After:
    
      RIP: 0010:[<ffffffff81070446>] native_write_msr+0x6/0x30
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/b3fda66f36761759b000883b059cdd9a7649dcc1.1476973742.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b3760b3c1ca0..b3b50ac6a302 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -61,8 +61,8 @@ void __show_regs(struct pt_regs *regs, int all)
 	unsigned int fsindex, gsindex;
 	unsigned int ds, cs, es;
 
-	printk(KERN_DEFAULT "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
-	printk_address(regs->ip);
+	printk(KERN_DEFAULT "RIP: %04lx:[<%016lx>] %pS\n", regs->cs & 0xffff,
+			regs->ip, (void *)regs->ip);
 	printk(KERN_DEFAULT "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,
 			regs->sp, regs->flags);
 	printk(KERN_DEFAULT "RAX: %016lx RBX: %016lx RCX: %016lx\n",

commit 4d69f155d58d0f75c5404ea502178b1943a04755
Merge: c474e50711aa 1001354ca341
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Oct 16 13:04:34 2016 +0200

    Merge tag 'v4.9-rc1' into x86/fpu, to resolve conflict
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c474e50711aa79b7bd0ea30b44744baca5650375
Author: Rik van Riel <riel@redhat.com>
Date:   Fri Oct 14 08:15:31 2016 -0400

    x86/fpu: Split old_fpu & new_fpu handling into separate functions
    
    By moving all of the new_fpu state handling into switch_fpu_finish(),
    the code can be simplified some more.
    
    This gets rid of the prefetch, but given the size of the FPU register
    state on modern CPUs, and the amount of work done by __switch_to()
    inbetween both functions, the value of a single cache line prefetch
    seems somewhat dubious anyway.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Dave Hansen <dave.hansen@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1476447331-21566-3-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ee944bd2310d..705669efb762 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -264,9 +264,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &per_cpu(cpu_tss, cpu);
 	unsigned prev_fsindex, prev_gsindex;
-	fpu_switch_t fpu_switch;
 
-	fpu_switch = switch_fpu_prepare(prev_fpu, next_fpu, cpu);
+	switch_fpu_prepare(prev_fpu, cpu);
 
 	/* We must save %fs and %gs before load_TLS() because
 	 * %fs and %gs may be cleared by load_TLS().
@@ -416,7 +415,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 		prev->gsbase = 0;
 	prev->gsindex = prev_gsindex;
 
-	switch_fpu_finish(next_fpu, fpu_switch);
+	switch_fpu_finish(next_fpu, cpu);
 
 	/*
 	 * Switch the PDA and FPU contexts.

commit 93c26d7dc02380fe11e57ff0d152368743762169
Merge: 5fa0eb0b4d47 6679dac513fd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 10 11:01:51 2016 -0700

    Merge branch 'mm-pkeys-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull protection keys syscall interface from Thomas Gleixner:
     "This is the final step of Protection Keys support which adds the
      syscalls so user space can actually allocate keys and protect memory
      areas with them. Details and usage examples can be found in the
      documentation.
    
      The mm side of this has been acked by Mel"
    
    * 'mm-pkeys-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/pkeys: Update documentation
      x86/mm/pkeys: Do not skip PKRU register if debug registers are not used
      x86/pkeys: Fix pkeys build breakage for some non-x86 arches
      x86/pkeys: Add self-tests
      x86/pkeys: Allow configuration of init_pkru
      x86/pkeys: Default to a restrictive init PKRU
      pkeys: Add details of system call use to Documentation/
      generic syscalls: Wire up memory protection keys syscalls
      x86: Wire up protection keys system calls
      x86/pkeys: Allocation/free syscalls
      x86/pkeys: Make mprotect_key() mask off additional vm_flags
      mm: Implement new pkey_mprotect() system call
      x86/pkeys: Add fault handling for PF_PK page fault bit

commit 8e4ef6386703835f91898334b72e48649646ec00
Merge: 6aebe7f9e869 6e68b08728ce
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 17:29:01 2016 -0700

    Merge branch 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 vdso updates from Ingo Molnar:
     "The main changes in this cycle centered around adding support for
      32-bit compatible C/R of the vDSO on 64-bit kernels, by Dmitry
      Safonov"
    
    * 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/vdso: Use CONFIG_X86_X32_ABI to enable vdso prctl
      x86/vdso: Only define map_vdso_randomized() if CONFIG_X86_64
      x86/vdso: Only define prctl_map_vdso() if CONFIG_CHECKPOINT_RESTORE
      x86/signal: Add SA_{X32,IA32}_ABI sa_flags
      x86/ptrace: Down with test_thread_flag(TIF_IA32)
      x86/coredump: Use pr_reg size, rather that TIF_IA32 flag
      x86/arch_prctl/vdso: Add ARCH_MAP_VDSO_*
      x86/vdso: Replace calculate_addr in map_vdso() with addr
      x86/vdso: Unmap vdso blob on vvar mapping failure

commit 6e68b08728ce3365c713f8663c6b05a79e2bbca1
Author: Vinson Lee <vlee@freedesktop.org>
Date:   Sat Sep 17 00:51:53 2016 +0000

    x86/vdso: Use CONFIG_X86_X32_ABI to enable vdso prctl
    
    The prctl code which references vdso_image_x32 is built when CONFIG_X86_X32
    is set. This results in the following build failure:
    
      LD      init/built-in.o
    arch/x86/built-in.o: In function `do_arch_prctl':
    (.text+0x27466): undefined reference to `vdso_image_x32'
    
    vdso_image_x32 depends on CONFIG_X86_X32_ABI. So we need to make the prctl
    depend on that as well.
    
    [ tglx: Massaged changelog ]
    
    Fixes: 2eefd8789698 ("x86/arch_prctl/vdso: Add ARCH_MAP_VDSO_*")
    Signed-off-by: Vinson Lee <vlee@freedesktop.org>
    Reviewed-by: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Link: http://lkml.kernel.org/r/1474073513-6656-1-git-send-email-vlee@freedesktop.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b26a0092a01d..b4603b71a659 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -592,7 +592,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 	}
 
 #ifdef CONFIG_CHECKPOINT_RESTORE
-# ifdef CONFIG_X86_X32
+# ifdef CONFIG_X86_X32_ABI
 	case ARCH_MAP_VDSO_X32:
 		return prctl_map_vdso(&vdso_image_x32, addr);
 # endif

commit 91b7bd39e62e190700aa1398e451a6dfa6d24465
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Sep 15 08:42:51 2016 +0200

    x86/vdso: Only define prctl_map_vdso() if CONFIG_CHECKPOINT_RESTORE
    
    ... otherwise the compiler complains:
    
      arch/x86/kernel/process_64.c:528:13: warning: ‘prctl_map_vdso’ defined but not used [-Wunused-function]
    
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: gorcunov@openvz.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: oleg@redhat.com
    Cc: xemul@virtuozzo.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index f240a465920b..b26a0092a01d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -525,6 +525,7 @@ void set_personality_ia32(bool x32)
 }
 EXPORT_SYMBOL_GPL(set_personality_ia32);
 
+#ifdef CONFIG_CHECKPOINT_RESTORE
 static long prctl_map_vdso(const struct vdso_image *image, unsigned long addr)
 {
 	int ret;
@@ -535,6 +536,7 @@ static long prctl_map_vdso(const struct vdso_image *image, unsigned long addr)
 
 	return (long)image->size;
 }
+#endif
 
 long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 {
@@ -590,14 +592,14 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 	}
 
 #ifdef CONFIG_CHECKPOINT_RESTORE
-#ifdef CONFIG_X86_X32
+# ifdef CONFIG_X86_X32
 	case ARCH_MAP_VDSO_X32:
 		return prctl_map_vdso(&vdso_image_x32, addr);
-#endif
-#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION
+# endif
+# if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION
 	case ARCH_MAP_VDSO_32:
 		return prctl_map_vdso(&vdso_image_32, addr);
-#endif
+# endif
 	case ARCH_MAP_VDSO_64:
 		return prctl_map_vdso(&vdso_image_64, addr);
 #endif

commit b9d989c7218ac922185d82ad46f3e58b27a4bea9
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Sep 13 14:29:21 2016 -0700

    x86/asm: Move the thread_info::status field to thread_struct
    
    Because sched.h and thread_info.h are a tangled mess, I turned
    in_compat_syscall() into a macro.  If we had current_thread_struct()
    or similar and we could use it from thread_info.h, then this would
    be a bit cleaner.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/ccc8a1b2f41f9c264a41f771bb4a6539a642ad72.1473801993.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b812cd0d7889..de9acaf2d371 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -510,7 +510,7 @@ void set_personality_ia32(bool x32)
 		current->personality &= ~READ_IMPLIES_EXEC;
 		/* in_compat_syscall() uses the presence of the x32
 		   syscall bit flag to determine compat status */
-		current_thread_info()->status &= ~TS_COMPAT;
+		current->thread.status &= ~TS_COMPAT;
 	} else {
 		set_thread_flag(TIF_IA32);
 		clear_thread_flag(TIF_X32);
@@ -518,7 +518,7 @@ void set_personality_ia32(bool x32)
 			current->mm->context.ia32_compat = TIF_IA32;
 		current->personality |= force_personality32;
 		/* Prepare the first "return" to user space */
-		current_thread_info()->status |= TS_COMPAT;
+		current->thread.status |= TS_COMPAT;
 	}
 }
 EXPORT_SYMBOL_GPL(set_personality_ia32);

commit 2eefd8789698e89c4a5d610921dc3c1b66e3bd0d
Author: Dmitry Safonov <dsafonov@virtuozzo.com>
Date:   Mon Sep 5 16:33:05 2016 +0300

    x86/arch_prctl/vdso: Add ARCH_MAP_VDSO_*
    
    Add API to change vdso blob type with arch_prctl.
    As this is usefull only by needs of CRIU, expose
    this interface under CONFIG_CHECKPOINT_RESTORE.
    
    Signed-off-by: Dmitry Safonov <dsafonov@virtuozzo.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: 0x7f454c46@gmail.com
    Cc: oleg@redhat.com
    Cc: linux-mm@kvack.org
    Cc: gorcunov@openvz.org
    Cc: xemul@virtuozzo.com
    Link: http://lkml.kernel.org/r/20160905133308.28234-4-dsafonov@virtuozzo.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 63236d8f84bf..f240a465920b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -49,6 +49,7 @@
 #include <asm/debugreg.h>
 #include <asm/switch_to.h>
 #include <asm/xen/hypervisor.h>
+#include <asm/vdso.h>
 
 asmlinkage extern void ret_from_fork(void);
 
@@ -524,6 +525,17 @@ void set_personality_ia32(bool x32)
 }
 EXPORT_SYMBOL_GPL(set_personality_ia32);
 
+static long prctl_map_vdso(const struct vdso_image *image, unsigned long addr)
+{
+	int ret;
+
+	ret = map_vdso_once(image, addr);
+	if (ret)
+		return ret;
+
+	return (long)image->size;
+}
+
 long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 {
 	int ret = 0;
@@ -577,6 +589,19 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		break;
 	}
 
+#ifdef CONFIG_CHECKPOINT_RESTORE
+#ifdef CONFIG_X86_X32
+	case ARCH_MAP_VDSO_X32:
+		return prctl_map_vdso(&vdso_image_x32, addr);
+#endif
+#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION
+	case ARCH_MAP_VDSO_32:
+		return prctl_map_vdso(&vdso_image_32, addr);
+#endif
+	case ARCH_MAP_VDSO_64:
+		return prctl_map_vdso(&vdso_image_64, addr);
+#endif
+
 	default:
 		ret = -EINVAL;
 		break;

commit ba6d018e3d2f6a0fad58a668cadf66b2d1f80f59
Author: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
Date:   Sat Sep 10 20:30:45 2016 +0200

    x86/mm/pkeys: Do not skip PKRU register if debug registers are not used
    
    __show_regs() fails to dump the PKRU state when the debug registers are in
    their default state because there is a return statement on the debug
    register state.
    
    Change the logic to report PKRU value even when debug registers are in
    their default state.
    
    Fixes:c0b17b5bd4b7 ("x86/mm/pkeys: Dump PKRU with other kernel registers")
    Signed-off-by: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
    Acked-by: Dave Hansen <dave.hansen@linux.intel.com>
    Link: http://lkml.kernel.org/r/20160910183045.4618-1-nicolas.iooss_linux@m4x.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 63236d8f84bf..a21068e49dac 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -110,12 +110,13 @@ void __show_regs(struct pt_regs *regs, int all)
 	get_debugreg(d7, 7);
 
 	/* Only print out debug registers if they are in their non-default state. */
-	if ((d0 == 0) && (d1 == 0) && (d2 == 0) && (d3 == 0) &&
-	    (d6 == DR6_RESERVED) && (d7 == 0x400))
-		return;
-
-	printk(KERN_DEFAULT "DR0: %016lx DR1: %016lx DR2: %016lx\n", d0, d1, d2);
-	printk(KERN_DEFAULT "DR3: %016lx DR6: %016lx DR7: %016lx\n", d3, d6, d7);
+	if (!((d0 == 0) && (d1 == 0) && (d2 == 0) && (d3 == 0) &&
+	    (d6 == DR6_RESERVED) && (d7 == 0x400))) {
+		printk(KERN_DEFAULT "DR0: %016lx DR1: %016lx DR2: %016lx\n",
+		       d0, d1, d2);
+		printk(KERN_DEFAULT "DR3: %016lx DR6: %016lx DR7: %016lx\n",
+		       d3, d6, d7);
+	}
 
 	if (boot_cpu_has(X86_FEATURE_OSPKE))
 		printk(KERN_DEFAULT "PKRU: %08x\n", read_pkru());

commit 616d24835eeafa8ef3466479db028abfdfc77531
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sat Aug 13 12:38:20 2016 -0400

    sched/x86: Pass kernel thread parameters in 'struct fork_frame'
    
    Instead of setting up a fake pt_regs context, put the kernel thread
    function pointer and arg into the unused callee-restored registers
    of 'struct fork_frame'.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1471106302-10159-6-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 827eeed03e16..b812cd0d7889 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -50,8 +50,6 @@
 #include <asm/switch_to.h>
 #include <asm/xen/hypervisor.h>
 
-asmlinkage extern void ret_from_fork(void);
-
 __visible DEFINE_PER_CPU(unsigned long, rsp_scratch);
 
 /* Prints also some state that isn't saved in the pt_regs */
@@ -165,15 +163,11 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 	if (unlikely(p->flags & PF_KTHREAD)) {
 		/* kernel thread */
 		memset(childregs, 0, sizeof(struct pt_regs));
-		childregs->sp = (unsigned long)childregs;
-		childregs->ss = __KERNEL_DS;
-		childregs->bx = sp; /* function */
-		childregs->bp = arg;
-		childregs->orig_ax = -1;
-		childregs->cs = __KERNEL_CS | get_kernel_rpl();
-		childregs->flags = X86_EFLAGS_IF | X86_EFLAGS_FIXED;
+		frame->bx = sp;		/* function */
+		frame->r12 = arg;
 		return 0;
 	}
+	frame->bx = 0;
 	*childregs = *current_pt_regs();
 
 	childregs->ax = 0;

commit 0100301bfdf56a2a370c7157b5ab0fbf9313e1cd
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sat Aug 13 12:38:19 2016 -0400

    sched/x86: Rewrite the switch_to() code
    
    Move the low-level context switch code to an out-of-line asm stub instead of
    using complex inline asm.  This allows constructing a new stack frame for the
    child process to make it seamlessly flow to ret_from_fork without an extra
    test and branch in __switch_to().  It also improves code generation for
    __schedule() by using the C calling convention instead of clobbering all
    registers.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1471106302-10159-5-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 63236d8f84bf..827eeed03e16 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -141,12 +141,17 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 {
 	int err;
 	struct pt_regs *childregs;
+	struct fork_frame *fork_frame;
+	struct inactive_task_frame *frame;
 	struct task_struct *me = current;
 
 	p->thread.sp0 = (unsigned long)task_stack_page(p) + THREAD_SIZE;
 	childregs = task_pt_regs(p);
-	p->thread.sp = (unsigned long) childregs;
-	set_tsk_thread_flag(p, TIF_FORK);
+	fork_frame = container_of(childregs, struct fork_frame, regs);
+	frame = &fork_frame->frame;
+	frame->bp = 0;
+	frame->ret_addr = (unsigned long) ret_from_fork;
+	p->thread.sp = (unsigned long) fork_frame;
 	p->thread.io_bitmap_ptr = NULL;
 
 	savesegment(gs, p->thread.gsindex);

commit 186f43608a5c827f8284fe4559225b4dccaa49ef
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:56 2016 -0400

    x86/kernel: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed some implicit header usage that was fixed up accordingly.
    
    Note that some bool/obj-y instances remain since module.h is
    the header for some exception table entry stuff, and for things
    like __init_or_module (code that is tossed when MODULES=n).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-4-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6e789ca1f841..63236d8f84bf 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -26,7 +26,7 @@
 #include <linux/user.h>
 #include <linux/interrupt.h>
 #include <linux/delay.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/ptrace.h>
 #include <linux/notifier.h>
 #include <linux/kprobes.h>

commit d696ca016d579d43fc043f28ba656d9305fba651
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue May 10 09:18:46 2016 -0700

    x86/fsgsbase/64: Use TASK_SIZE_MAX for FSBASE/GSBASE upper limits
    
    The GSBASE upper limit exists to prevent user code from confusing
    the paranoid idtentry path.  The FSBASE upper limit is just for
    consistency.  There's no need to enforce a smaller limit for 32-bit
    tasks.
    
    Just use TASK_SIZE_MAX.  This simplifies the logic and will save a
    few bytes of code.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/5357f2fe0f103eabf005773b70722451eab09a89.1462897104.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6b16c36f0939..6e789ca1f841 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -532,7 +532,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 
 	switch (code) {
 	case ARCH_SET_GS:
-		if (addr >= TASK_SIZE_OF(task))
+		if (addr >= TASK_SIZE_MAX)
 			return -EPERM;
 		cpu = get_cpu();
 		task->thread.gsindex = 0;
@@ -546,7 +546,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 	case ARCH_SET_FS:
 		/* Not strictly needed for fs, but do it for symmetry
 		   with gs */
-		if (addr >= TASK_SIZE_OF(task))
+		if (addr >= TASK_SIZE_MAX)
 			return -EPERM;
 		cpu = get_cpu();
 		task->thread.fsindex = 0;

commit 4afd0565552c87f23834db9121dd9cf6955d0b43
Author: Mateusz Guzik <mguzik@redhat.com>
Date:   Tue May 10 22:56:43 2016 +0200

    x86/arch_prctl/64: Restore accidentally removed put_cpu() in ARCH_SET_GS
    
    This fixes an oversight in:
    
            731e33e39a5b95 ("Remove FSBASE/GSBASE < 4G optimization")
    
    Signed-off-by: Mateusz Guzik <mguzik@redhat.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/1462913803-29634-1-git-send-email-mguzik@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4285f6adcc5e..6b16c36f0939 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -541,6 +541,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 			load_gs_index(0);
 			ret = wrmsrl_safe(MSR_KERNEL_GS_BASE, addr);
 		}
+		put_cpu();
 		break;
 	case ARCH_SET_FS:
 		/* Not strictly needed for fs, but do it for symmetry

commit 296f781a4b7801ad9c1c0219f9e87b6c25e196fe
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Apr 26 12:23:29 2016 -0700

    x86/asm/64: Rename thread_struct's fs and gs to fsbase and gsbase
    
    Unlike ds and es, these are base addresses, not selectors.  Rename
    them so their meaning is more obvious.
    
    On x86_32, the field is still called fs.  Fixing that could make sense
    as a future cleanup.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/69a18a51c4cba0ce29a241e570fc618ad721d908.1461698311.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 864fe2cde3c6..4285f6adcc5e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -150,9 +150,9 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 	p->thread.io_bitmap_ptr = NULL;
 
 	savesegment(gs, p->thread.gsindex);
-	p->thread.gs = p->thread.gsindex ? 0 : me->thread.gs;
+	p->thread.gsbase = p->thread.gsindex ? 0 : me->thread.gsbase;
 	savesegment(fs, p->thread.fsindex);
-	p->thread.fs = p->thread.fsindex ? 0 : me->thread.fs;
+	p->thread.fsbase = p->thread.fsindex ? 0 : me->thread.fsbase;
 	savesegment(es, p->thread.es);
 	savesegment(ds, p->thread.ds);
 	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));
@@ -329,18 +329,18 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 * stronger guarantees.)
 	 *
 	 * As an invariant,
-	 * (fs != 0 && fsindex != 0) || (gs != 0 && gsindex != 0) is
+	 * (fsbase != 0 && fsindex != 0) || (gsbase != 0 && gsindex != 0) is
 	 * impossible.
 	 */
 	if (next->fsindex) {
 		/* Loading a nonzero value into FS sets the index and base. */
 		loadsegment(fs, next->fsindex);
 	} else {
-		if (next->fs) {
+		if (next->fsbase) {
 			/* Next index is zero but next base is nonzero. */
 			if (prev_fsindex)
 				loadsegment(fs, 0);
-			wrmsrl(MSR_FS_BASE, next->fs);
+			wrmsrl(MSR_FS_BASE, next->fsbase);
 		} else {
 			/* Next base and index are both zero. */
 			if (static_cpu_has_bug(X86_BUG_NULL_SEG)) {
@@ -356,7 +356,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 				 * didn't change the base, then the base is
 				 * also zero and we don't need to do anything.
 				 */
-				if (prev->fs || prev_fsindex)
+				if (prev->fsbase || prev_fsindex)
 					loadsegment(fs, 0);
 			}
 		}
@@ -369,18 +369,18 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 * us.
 	 */
 	if (prev_fsindex)
-		prev->fs = 0;
+		prev->fsbase = 0;
 	prev->fsindex = prev_fsindex;
 
 	if (next->gsindex) {
 		/* Loading a nonzero value into GS sets the index and base. */
 		load_gs_index(next->gsindex);
 	} else {
-		if (next->gs) {
+		if (next->gsbase) {
 			/* Next index is zero but next base is nonzero. */
 			if (prev_gsindex)
 				load_gs_index(0);
-			wrmsrl(MSR_KERNEL_GS_BASE, next->gs);
+			wrmsrl(MSR_KERNEL_GS_BASE, next->gsbase);
 		} else {
 			/* Next base and index are both zero. */
 			if (static_cpu_has_bug(X86_BUG_NULL_SEG)) {
@@ -400,7 +400,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 				 * didn't change the base, then the base is
 				 * also zero and we don't need to do anything.
 				 */
-				if (prev->gs || prev_gsindex)
+				if (prev->gsbase || prev_gsindex)
 					load_gs_index(0);
 			}
 		}
@@ -413,7 +413,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 * us.
 	 */
 	if (prev_gsindex)
-		prev->gs = 0;
+		prev->gsbase = 0;
 	prev->gsindex = prev_gsindex;
 
 	switch_fpu_finish(next_fpu, fpu_switch);
@@ -536,7 +536,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 			return -EPERM;
 		cpu = get_cpu();
 		task->thread.gsindex = 0;
-		task->thread.gs = addr;
+		task->thread.gsbase = addr;
 		if (doit) {
 			load_gs_index(0);
 			ret = wrmsrl_safe(MSR_KERNEL_GS_BASE, addr);
@@ -549,7 +549,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 			return -EPERM;
 		cpu = get_cpu();
 		task->thread.fsindex = 0;
-		task->thread.fs = addr;
+		task->thread.fsbase = addr;
 		if (doit) {
 			/* set the selector to 0 to not confuse __switch_to */
 			loadsegment(fs, 0);
@@ -562,7 +562,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		if (doit)
 			rdmsrl(MSR_FS_BASE, base);
 		else
-			base = task->thread.fs;
+			base = task->thread.fsbase;
 		ret = put_user(base, (unsigned long __user *)addr);
 		break;
 	}
@@ -571,7 +571,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		if (doit)
 			rdmsrl(MSR_KERNEL_GS_BASE, base);
 		else
-			base = task->thread.gs;
+			base = task->thread.gsbase;
 		ret = put_user(base, (unsigned long __user *)addr);
 		break;
 	}

commit 731e33e39a5b95ad77017811b3ced32ecf9dc666
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Apr 26 12:23:28 2016 -0700

    x86/arch_prctl/64: Remove FSBASE/GSBASE < 4G optimization
    
    As far as I know, the optimization doesn't work on any modern distro
    because modern distros use high addresses for ASLR.  Remove it.
    
    The ptrace code was either wrong or very strange, but the behavior
    with this patch should be essentially identical to the behavior
    without this patch unless user code goes out of its way to mislead
    ptrace.
    
    On newer CPUs, once the FSGSBASE instructions are enabled, we won't
    want to use the optimized variant anyway.
    
    This isn't actually much of a performance regression, it has no effect
    on normal dynamically linked programs, and it's a considerably
    simplification. It also removes some nasty special cases from code
    that is already way too full of special cases for comfort.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/dd1599b08866961dba9d2458faa6bbd7fba471d7.1461698311.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 24d1b7fb4399..864fe2cde3c6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -136,25 +136,6 @@ void release_thread(struct task_struct *dead_task)
 	}
 }
 
-static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
-{
-	struct user_desc ud = {
-		.base_addr = addr,
-		.limit = 0xfffff,
-		.seg_32bit = 1,
-		.limit_in_pages = 1,
-		.useable = 1,
-	};
-	struct desc_struct *desc = t->thread.tls_array;
-	desc += tls;
-	fill_ldt(desc, &ud);
-}
-
-static inline u32 read_32bit_tls(struct task_struct *t, int tls)
-{
-	return get_desc_base(&t->thread.tls_array[tls]);
-}
-
 int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 		unsigned long arg, struct task_struct *p, unsigned long tls)
 {
@@ -554,25 +535,12 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		if (addr >= TASK_SIZE_OF(task))
 			return -EPERM;
 		cpu = get_cpu();
-		/* handle small bases via the GDT because that's faster to
-		   switch. */
-		if (addr <= 0xffffffff) {
-			set_32bit_tls(task, GS_TLS, addr);
-			if (doit) {
-				load_TLS(&task->thread, cpu);
-				load_gs_index(GS_TLS_SEL);
-			}
-			task->thread.gsindex = GS_TLS_SEL;
-			task->thread.gs = 0;
-		} else {
-			task->thread.gsindex = 0;
-			task->thread.gs = addr;
-			if (doit) {
-				load_gs_index(0);
-				ret = wrmsrl_safe(MSR_KERNEL_GS_BASE, addr);
-			}
+		task->thread.gsindex = 0;
+		task->thread.gs = addr;
+		if (doit) {
+			load_gs_index(0);
+			ret = wrmsrl_safe(MSR_KERNEL_GS_BASE, addr);
 		}
-		put_cpu();
 		break;
 	case ARCH_SET_FS:
 		/* Not strictly needed for fs, but do it for symmetry
@@ -580,25 +548,12 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		if (addr >= TASK_SIZE_OF(task))
 			return -EPERM;
 		cpu = get_cpu();
-		/* handle small bases via the GDT because that's faster to
-		   switch. */
-		if (addr <= 0xffffffff) {
-			set_32bit_tls(task, FS_TLS, addr);
-			if (doit) {
-				load_TLS(&task->thread, cpu);
-				loadsegment(fs, FS_TLS_SEL);
-			}
-			task->thread.fsindex = FS_TLS_SEL;
-			task->thread.fs = 0;
-		} else {
-			task->thread.fsindex = 0;
-			task->thread.fs = addr;
-			if (doit) {
-				/* set the selector to 0 to not confuse
-				   __switch_to */
-				loadsegment(fs, 0);
-				ret = wrmsrl_safe(MSR_FS_BASE, addr);
-			}
+		task->thread.fsindex = 0;
+		task->thread.fs = addr;
+		if (doit) {
+			/* set the selector to 0 to not confuse __switch_to */
+			loadsegment(fs, 0);
+			ret = wrmsrl_safe(MSR_FS_BASE, addr);
 		}
 		put_cpu();
 		break;
@@ -606,8 +561,6 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		unsigned long base;
 		if (doit)
 			rdmsrl(MSR_FS_BASE, base);
-		else if (task->thread.fsindex == FS_TLS_SEL)
-			base = read_32bit_tls(task, FS_TLS);
 		else
 			base = task->thread.fs;
 		ret = put_user(base, (unsigned long __user *)addr);
@@ -617,8 +570,6 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		unsigned long base;
 		if (doit)
 			rdmsrl(MSR_KERNEL_GS_BASE, base);
-		else if (task->thread.gsindex == GS_TLS_SEL)
-			base = read_32bit_tls(task, GS_TLS);
 		else
 			base = task->thread.gs;
 		ret = put_user(base, (unsigned long __user *)addr);

commit abfb9498ee1327f534df92a7ecaea81a85913bae
Author: Dmitry Safonov <dsafonov@virtuozzo.com>
Date:   Mon Apr 18 16:43:43 2016 +0300

    x86/entry: Rename is_{ia32,x32}_task() to in_{ia32,x32}_syscall()
    
    The is_ia32_task()/is_x32_task() function names are a big misnomer: they
    suggests that the compat-ness of a system call is a task property, which
    is not true, the compatness of a system call purely depends on how it
    was invoked through the system call layer.
    
    A task may call 32-bit and 64-bit and x32 system calls without changing
    any of its kernel visible state.
    
    This specific minomer is also actively dangerous, as it might cause kernel
    developers to use the wrong kind of security checks within system calls.
    
    So rename it to in_{ia32,x32}_syscall().
    
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Dmitry Safonov <dsafonov@virtuozzo.com>
    [ Expanded the changelog. ]
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: 0x7f454c46@gmail.com
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/1460987025-30360-1-git-send-email-dsafonov@virtuozzo.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 50337eac1ca2..24d1b7fb4399 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -210,7 +210,7 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 	 */
 	if (clone_flags & CLONE_SETTLS) {
 #ifdef CONFIG_IA32_EMULATION
-		if (is_ia32_task())
+		if (in_ia32_syscall())
 			err = do_set_thread_area(p, -1,
 				(struct user_desc __user *)tls, 0);
 		else

commit 3e2b68d752c9e09c40d76442aa94d3b8e421b0f1
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Apr 7 17:31:47 2016 -0700

    x86/asm, sched/x86: Rewrite the FS and GS context switch code
    
    The old code was incomprehensible and was buggy on AMD CPUs.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rudolf Marek <r.marek@assembler.cz>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/5f6bde874c6fe6831c6711b5b1522a238ba035b4.1460075211.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index c671b9b015c0..50337eac1ca2 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -282,7 +282,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	struct fpu *next_fpu = &next->fpu;
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &per_cpu(cpu_tss, cpu);
-	unsigned fsindex, gsindex;
+	unsigned prev_fsindex, prev_gsindex;
 	fpu_switch_t fpu_switch;
 
 	fpu_switch = switch_fpu_prepare(prev_fpu, next_fpu, cpu);
@@ -292,8 +292,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 *
 	 * (e.g. xen_load_tls())
 	 */
-	savesegment(fs, fsindex);
-	savesegment(gs, gsindex);
+	savesegment(fs, prev_fsindex);
+	savesegment(gs, prev_gsindex);
 
 	/*
 	 * Load TLS before restoring any segments so that segment loads
@@ -336,66 +336,104 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 * Switch FS and GS.
 	 *
 	 * These are even more complicated than DS and ES: they have
-	 * 64-bit bases are that controlled by arch_prctl.  Those bases
-	 * only differ from the values in the GDT or LDT if the selector
-	 * is 0.
+	 * 64-bit bases are that controlled by arch_prctl.  The bases
+	 * don't necessarily match the selectors, as user code can do
+	 * any number of things to cause them to be inconsistent.
 	 *
-	 * Loading the segment register resets the hidden base part of
-	 * the register to 0 or the value from the GDT / LDT.  If the
-	 * next base address zero, writing 0 to the segment register is
-	 * much faster than using wrmsr to explicitly zero the base.
+	 * We don't promise to preserve the bases if the selectors are
+	 * nonzero.  We also don't promise to preserve the base if the
+	 * selector is zero and the base doesn't match whatever was
+	 * most recently passed to ARCH_SET_FS/GS.  (If/when the
+	 * FSGSBASE instructions are enabled, we'll need to offer
+	 * stronger guarantees.)
 	 *
-	 * The thread_struct.fs and thread_struct.gs values are 0
-	 * if the fs and gs bases respectively are not overridden
-	 * from the values implied by fsindex and gsindex.  They
-	 * are nonzero, and store the nonzero base addresses, if
-	 * the bases are overridden.
-	 *
-	 * (fs != 0 && fsindex != 0) || (gs != 0 && gsindex != 0) should
-	 * be impossible.
-	 *
-	 * Therefore we need to reload the segment registers if either
-	 * the old or new selector is nonzero, and we need to override
-	 * the base address if next thread expects it to be overridden.
-	 *
-	 * This code is unnecessarily slow in the case where the old and
-	 * new indexes are zero and the new base is nonzero -- it will
-	 * unnecessarily write 0 to the selector before writing the new
-	 * base address.
-	 *
-	 * Note: This all depends on arch_prctl being the only way that
-	 * user code can override the segment base.  Once wrfsbase and
-	 * wrgsbase are enabled, most of this code will need to change.
+	 * As an invariant,
+	 * (fs != 0 && fsindex != 0) || (gs != 0 && gsindex != 0) is
+	 * impossible.
 	 */
-	if (unlikely(fsindex | next->fsindex | prev->fs)) {
+	if (next->fsindex) {
+		/* Loading a nonzero value into FS sets the index and base. */
 		loadsegment(fs, next->fsindex);
-
-		/*
-		 * If user code wrote a nonzero value to FS, then it also
-		 * cleared the overridden base address.
-		 *
-		 * XXX: if user code wrote 0 to FS and cleared the base
-		 * address itself, we won't notice and we'll incorrectly
-		 * restore the prior base address next time we reschdule
-		 * the process.
-		 */
-		if (fsindex)
-			prev->fs = 0;
+	} else {
+		if (next->fs) {
+			/* Next index is zero but next base is nonzero. */
+			if (prev_fsindex)
+				loadsegment(fs, 0);
+			wrmsrl(MSR_FS_BASE, next->fs);
+		} else {
+			/* Next base and index are both zero. */
+			if (static_cpu_has_bug(X86_BUG_NULL_SEG)) {
+				/*
+				 * We don't know the previous base and can't
+				 * find out without RDMSR.  Forcibly clear it.
+				 */
+				loadsegment(fs, __USER_DS);
+				loadsegment(fs, 0);
+			} else {
+				/*
+				 * If the previous index is zero and ARCH_SET_FS
+				 * didn't change the base, then the base is
+				 * also zero and we don't need to do anything.
+				 */
+				if (prev->fs || prev_fsindex)
+					loadsegment(fs, 0);
+			}
+		}
 	}
-	if (next->fs)
-		wrmsrl(MSR_FS_BASE, next->fs);
-	prev->fsindex = fsindex;
+	/*
+	 * Save the old state and preserve the invariant.
+	 * NB: if prev_fsindex == 0, then we can't reliably learn the base
+	 * without RDMSR because Intel user code can zero it without telling
+	 * us and AMD user code can program any 32-bit value without telling
+	 * us.
+	 */
+	if (prev_fsindex)
+		prev->fs = 0;
+	prev->fsindex = prev_fsindex;
 
-	if (unlikely(gsindex | next->gsindex | prev->gs)) {
+	if (next->gsindex) {
+		/* Loading a nonzero value into GS sets the index and base. */
 		load_gs_index(next->gsindex);
-
-		/* This works (and fails) the same way as fsindex above. */
-		if (gsindex)
-			prev->gs = 0;
+	} else {
+		if (next->gs) {
+			/* Next index is zero but next base is nonzero. */
+			if (prev_gsindex)
+				load_gs_index(0);
+			wrmsrl(MSR_KERNEL_GS_BASE, next->gs);
+		} else {
+			/* Next base and index are both zero. */
+			if (static_cpu_has_bug(X86_BUG_NULL_SEG)) {
+				/*
+				 * We don't know the previous base and can't
+				 * find out without RDMSR.  Forcibly clear it.
+				 *
+				 * This contains a pointless SWAPGS pair.
+				 * Fixing it would involve an explicit check
+				 * for Xen or a new pvop.
+				 */
+				load_gs_index(__USER_DS);
+				load_gs_index(0);
+			} else {
+				/*
+				 * If the previous index is zero and ARCH_SET_GS
+				 * didn't change the base, then the base is
+				 * also zero and we don't need to do anything.
+				 */
+				if (prev->gs || prev_gsindex)
+					load_gs_index(0);
+			}
+		}
 	}
-	if (next->gs)
-		wrmsrl(MSR_KERNEL_GS_BASE, next->gs);
-	prev->gsindex = gsindex;
+	/*
+	 * Save the old state and preserve the invariant.
+	 * NB: if prev_gsindex == 0, then we can't reliably learn the base
+	 * without RDMSR because Intel user code can zero it without telling
+	 * us and AMD user code can program any 32-bit value without telling
+	 * us.
+	 */
+	if (prev_gsindex)
+		prev->gs = 0;
+	prev->gsindex = prev_gsindex;
 
 	switch_fpu_finish(next_fpu, fpu_switch);
 

commit d47b50e7a111bb7a56fb1c974728b56209d7f515
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Apr 7 17:31:45 2016 -0700

    x86/arch_prctl: Fix ARCH_GET_FS and ARCH_GET_GS
    
    ARCH_GET_FS and ARCH_GET_GS attempted to figure out the fsbase and
    gsbase respectively from saved thread state.  This was wrong: fsbase
    and gsbase live in registers while a thread is running, not in
    memory.
    
    For reasons I can't fathom, the fsbase and gsbase code were
    different.  Since neither was correct, I didn't try to figure out
    what the point of the difference was.
    
    Change it to simply read the MSRs.
    
    The code for reading the base for a remote thread is also completely
    wrong if the target thread uses its own descriptors (which is the case
    for all 32-bit threaded programs), but fixing that is a different
    story.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rudolf Marek <r.marek@assembler.cz>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/c6e7b507c72ca3bdbf6c7a8a3ceaa0334e873bd9.1460075211.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6cbab31ac23a..c671b9b015c0 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -566,10 +566,10 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		break;
 	case ARCH_GET_FS: {
 		unsigned long base;
-		if (task->thread.fsindex == FS_TLS_SEL)
-			base = read_32bit_tls(task, FS_TLS);
-		else if (doit)
+		if (doit)
 			rdmsrl(MSR_FS_BASE, base);
+		else if (task->thread.fsindex == FS_TLS_SEL)
+			base = read_32bit_tls(task, FS_TLS);
 		else
 			base = task->thread.fs;
 		ret = put_user(base, (unsigned long __user *)addr);
@@ -577,16 +577,11 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 	}
 	case ARCH_GET_GS: {
 		unsigned long base;
-		unsigned gsindex;
-		if (task->thread.gsindex == GS_TLS_SEL)
+		if (doit)
+			rdmsrl(MSR_KERNEL_GS_BASE, base);
+		else if (task->thread.gsindex == GS_TLS_SEL)
 			base = read_32bit_tls(task, GS_TLS);
-		else if (doit) {
-			savesegment(gs, gsindex);
-			if (gsindex)
-				rdmsrl(MSR_KERNEL_GS_BASE, base);
-			else
-				base = task->thread.gs;
-		} else
+		else
 			base = task->thread.gs;
 		ret = put_user(base, (unsigned long __user *)addr);
 		break;

commit d88f48e12821ab4b2244124d50ac094568f48db5
Merge: be53f58fa0fc 9da77666d697
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 09:47:32 2016 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Misc fixes:
    
       - fix hotplug bugs
       - fix irq live lock
       - fix various topology handling bugs
       - fix APIC ACK ordering
       - fix PV iopl handling
       - fix speling
       - fix/tweak memcpy_mcsafe() return value
       - fix fbcon bug
       - remove stray prototypes"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/msr: Remove unused native_read_tscp()
      x86/apic: Remove declaration of unused hw_nmi_is_cpu_stuck
      x86/oprofile/nmi: Add missing hotplug FROZEN handling
      x86/hpet: Use proper mask to modify hotplug action
      x86/apic/uv: Fix the hotplug notifier
      x86/apb/timer: Use proper mask to modify hotplug action
      x86/topology: Use total_cpus not nr_cpu_ids for logical packages
      x86/topology: Fix Intel HT disable
      x86/topology: Fix logical package mapping
      x86/irq: Cure live lock in fixup_irqs()
      x86/tsc: Prevent NULL pointer deref in calibrate_delay_is_known()
      x86/apic: Fix suspicious RCU usage in smp_trace_call_function_interrupt()
      x86/iopl: Fix iopl capability check on Xen PV
      x86/iopl/64: Properly context-switch IOPL on Xen PV
      selftests/x86: Add an iopl test
      x86/mm, x86/mce: Fix return type/value for memcpy_mcsafe()
      x86/video: Don't assume all FB devices are PCI devices
      arch/x86/irq: Purge useless handler declarations from hw_irq.h
      x86: Fix misspellings in comments

commit f970165beeaa4803438e435d68022b3680a1a0b0
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Mar 22 14:25:27 2016 -0700

    x86/compat: remove is_compat_task()
    
    x86's is_compat_task always checked the current syscall type, not the
    task type.  It has no non-arch users any more, so just remove it to
    avoid confusion.
    
    On x86, nothing should really be checking the task ABI.  There are
    legitimate users for the syscall ABI and for the mm ABI.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 776229e98202..dfa2781610e8 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -478,7 +478,7 @@ void set_personality_ia32(bool x32)
 		if (current->mm)
 			current->mm->context.ia32_compat = TIF_X32;
 		current->personality &= ~READ_IMPLIES_EXEC;
-		/* is_compat_task() uses the presence of the x32
+		/* in_compat_syscall() uses the presence of the x32
 		   syscall bit flag to determine compat status */
 		current_thread_info()->status &= ~TS_COMPAT;
 	} else {

commit b7a584598aea7ca73140cb87b40319944dd3393f
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 16 14:14:21 2016 -0700

    x86/iopl/64: Properly context-switch IOPL on Xen PV
    
    On Xen PV, regs->flags doesn't reliably reflect IOPL and the
    exit-to-userspace code doesn't change IOPL.  We need to context
    switch it manually.
    
    I'm doing this without going through paravirt because this is
    specific to Xen PV.  After the dust settles, we can merge this with
    the 32-bit code, tidy up the iopl syscall implementation, and remove
    the set_iopl pvop entirely.
    
    Fixes XSA-171.
    
    Reviewewd-by: Jan Beulich <JBeulich@suse.com>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan Beulich <JBeulich@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/693c3bd7aeb4d3c27c92c622b7d0f554a458173c.1458162709.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b9d99e0f82c4..9f751876066f 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -48,6 +48,7 @@
 #include <asm/syscalls.h>
 #include <asm/debugreg.h>
 #include <asm/switch_to.h>
+#include <asm/xen/hypervisor.h>
 
 asmlinkage extern void ret_from_fork(void);
 
@@ -411,6 +412,17 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 		     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))
 		__switch_to_xtra(prev_p, next_p, tss);
 
+#ifdef CONFIG_XEN
+	/*
+	 * On Xen PV, IOPL bits in pt_regs->flags have no effect, and
+	 * current_pt_regs()->flags may not match the current task's
+	 * intended IOPL.  We need to switch it manually.
+	 */
+	if (unlikely(static_cpu_has(X86_FEATURE_XENPV) &&
+		     prev->iopl != next->iopl))
+		xen_set_iopl_mask(next->iopl);
+#endif
+
 	if (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {
 		/*
 		 * AMD CPUs have a misfeature: SYSRET sets the SS selector but

commit c0b17b5bd4b7b98e7c6b67c9f69343b64711271b
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Feb 12 13:02:25 2016 -0800

    x86/mm/pkeys: Dump PKRU with other kernel registers
    
    Protection Keys never affect kernel mappings.  But, they can
    affect whether the kernel will fault when it touches a user
    mapping.  The kernel doesn't touch user mappings without some
    careful choreography and these accesses don't generally result in
    oopses.  But, if one does, we definitely want to have PKRU
    available so we can figure out if protection keys played a role.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20160212210225.BF0D4482@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b9d99e0f82c4..776229e98202 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -116,6 +116,8 @@ void __show_regs(struct pt_regs *regs, int all)
 	printk(KERN_DEFAULT "DR0: %016lx DR1: %016lx DR2: %016lx\n", d0, d1, d2);
 	printk(KERN_DEFAULT "DR3: %016lx DR6: %016lx DR7: %016lx\n", d3, d6, d7);
 
+	if (boot_cpu_has(X86_FEATURE_OSPKE))
+		printk(KERN_DEFAULT "PKRU: %08x\n", read_pkru());
 }
 
 void release_thread(struct task_struct *dead_task)

commit 0d430e3fb3f7cdc13c0d22078b820f682821b45a
Author: Jan Beulich <JBeulich@suse.com>
Date:   Tue Dec 22 08:42:44 2015 -0700

    x86/LDT: Print the real LDT base address
    
    This was meant to print base address and entry count; make it do so
    again.
    
    Fixes: 37868fe113ff "x86/ldt: Make modify_ldt synchronous"
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: http://lkml.kernel.org/r/56797D8402000078000C24F0@prv-mh.provo.novell.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e835d263a33b..b9d99e0f82c4 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -125,7 +125,7 @@ void release_thread(struct task_struct *dead_task)
 		if (dead_task->mm->context.ldt) {
 			pr_warn("WARNING: dead process %s still has LDT? <%p/%d>\n",
 				dead_task->comm,
-				dead_task->mm->context.ldt,
+				dead_task->mm->context.ldt->entries,
 				dead_task->mm->context.ldt->size);
 			BUG();
 		}

commit 558a65bc31a0c7811b34dad32f51f47c55a40000
Author: Chuck Ebbert <cebbert.lkml@gmail.com>
Date:   Wed Oct 14 14:31:19 2015 -0400

    sched/x86: Fix typo in __switch_to() comments
    
    Fix obvious mistake: FS/GS should be DS/ES.
    
    Signed-off-by: Chuck Ebbert <cebbert.lkml@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20151014143119.78858eeb@r5
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index d7f1d5c4387d..e835d263a33b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -332,7 +332,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/*
 	 * Switch FS and GS.
 	 *
-	 * These are even more complicated than FS and GS: they have
+	 * These are even more complicated than DS and ES: they have
 	 * 64-bit bases are that controlled by arch_prctl.  Those bases
 	 * only differ from the values in the GDT or LDT if the selector
 	 * is 0.

commit d87b7a33794f52226131f93cbc9db03274d9fecf
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Sep 28 18:11:18 2015 +0200

    sched/core, sched/x86: Kill thread_info::saved_preempt_count
    
    With the introduction of the context switch preempt_count invariant,
    and the demise of PREEMPT_ACTIVE, its pointless to save/restore the
    per-cpu preemption count, it must always be 2.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b35921a670b2..d7f1d5c4387d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -401,14 +401,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 */
 	this_cpu_write(current_task, next_p);
 
-	/*
-	 * If it were not for PREEMPT_ACTIVE we could guarantee that the
-	 * preempt_count of all tasks was equal here and this would not be
-	 * needed.
-	 */
-	task_thread_info(prev_p)->saved_preempt_count = this_cpu_read(__preempt_count);
-	this_cpu_write(__preempt_count, task_thread_info(next_p)->saved_preempt_count);
-
 	/* Reload esp0 and ss1.  This changes current_thread_info(). */
 	load_sp0(tss, next);
 

commit 7ba78053aacb89998a052843e3c56983c31d57f0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 30 08:38:23 2015 +0000

    x86/process: Unify 32bit and 64bit implementations of get_wchan()
    
    The stack layout and the functionality is identical. Use the 64bit
    version for all of x86.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: kasan-dev <kasan-dev@googlegroups.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Wolfram Gloger <wmglo@dent.med.uni-muenchen.de>
    Link: http://lkml.kernel.org/r/20150930083302.779694618@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index f1fd0889e8af..b35921a670b2 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -499,62 +499,6 @@ void set_personality_ia32(bool x32)
 }
 EXPORT_SYMBOL_GPL(set_personality_ia32);
 
-/*
- * Called from fs/proc with a reference on @p to find the function
- * which called into schedule(). This needs to be done carefully
- * because the task might wake up and we might look at a stack
- * changing under us.
- */
-unsigned long get_wchan(struct task_struct *p)
-{
-	unsigned long start, bottom, top, sp, fp, ip;
-	int count = 0;
-
-	if (!p || p == current || p->state == TASK_RUNNING)
-		return 0;
-
-	start = (unsigned long)task_stack_page(p);
-	if (!start)
-		return 0;
-
-	/*
-	 * Layout of the stack page:
-	 *
-	 * ----------- topmax = start + THREAD_SIZE - sizeof(unsigned long)
-	 * PADDING
-	 * ----------- top = topmax - TOP_OF_KERNEL_STACK_PADDING
-	 * stack
-	 * ----------- bottom = start + sizeof(thread_info)
-	 * thread_info
-	 * ----------- start
-	 *
-	 * The tasks stack pointer points at the location where the
-	 * framepointer is stored. The data on the stack is:
-	 * ... IP FP ... IP FP
-	 *
-	 * We need to read FP and IP, so we need to adjust the upper
-	 * bound by another unsigned long.
-	 */
-	top = start + THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;
-	top -= 2 * sizeof(unsigned long);
-	bottom = start + sizeof(struct thread_info);
-
-	sp = READ_ONCE(p->thread.sp);
-	if (sp < bottom || sp > top)
-		return 0;
-
-	fp = READ_ONCE(*(unsigned long *)sp);
-	do {
-		if (fp < bottom || fp > top)
-			return 0;
-		ip = READ_ONCE(*(unsigned long *)(fp + sizeof(unsigned long)));
-		if (!in_sched_functions(ip))
-			return ip;
-		fp = READ_ONCE(*(unsigned long *)fp);
-	} while (count++ < 16 && p->state != TASK_RUNNING);
-	return 0;
-}
-
 long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 {
 	int ret = 0;

commit eddd3826a1a0190e5235703d1e666affa4d13b96
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 30 08:38:22 2015 +0000

    x86/process: Add proper bound checks in 64bit get_wchan()
    
    Dmitry Vyukov reported the following using trinity and the memory
    error detector AddressSanitizer
    (https://code.google.com/p/address-sanitizer/wiki/AddressSanitizerForKernel).
    
    [ 124.575597] ERROR: AddressSanitizer: heap-buffer-overflow on
    address ffff88002e280000
    [ 124.576801] ffff88002e280000 is located 131938492886538 bytes to
    the left of 28857600-byte region [ffffffff81282e0a, ffffffff82e0830a)
    [ 124.578633] Accessed by thread T10915:
    [ 124.579295] inlined in describe_heap_address
    ./arch/x86/mm/asan/report.c:164
    [ 124.579295] #0 ffffffff810dd277 in asan_report_error
    ./arch/x86/mm/asan/report.c:278
    [ 124.580137] #1 ffffffff810dc6a0 in asan_check_region
    ./arch/x86/mm/asan/asan.c:37
    [ 124.581050] #2 ffffffff810dd423 in __tsan_read8 ??:0
    [ 124.581893] #3 ffffffff8107c093 in get_wchan
    ./arch/x86/kernel/process_64.c:444
    
    The address checks in the 64bit implementation of get_wchan() are
    wrong in several ways:
    
     - The lower bound of the stack is not the start of the stack
       page. It's the start of the stack page plus sizeof (struct
       thread_info)
    
     - The upper bound must be:
    
           top_of_stack - TOP_OF_KERNEL_STACK_PADDING - 2 * sizeof(unsigned long).
    
       The 2 * sizeof(unsigned long) is required because the stack pointer
       points at the frame pointer. The layout on the stack is: ... IP FP
       ... IP FP. So we need to make sure that both IP and FP are in the
       bounds.
    
    Fix the bound checks and get rid of the mix of numeric constants, u64
    and unsigned long. Making all unsigned long allows us to use the same
    function for 32bit as well.
    
    Use READ_ONCE() when accessing the stack. This does not prevent a
    concurrent wakeup of the task and the stack changing, but at least it
    avoids TOCTOU.
    
    Also check task state at the end of the loop. Again that does not
    prevent concurrent changes, but it avoids walking for nothing.
    
    Add proper comments while at it.
    
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Based-on-patch-from: Wolfram Gloger <wmglo@dent.med.uni-muenchen.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: kasan-dev <kasan-dev@googlegroups.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Wolfram Gloger <wmglo@dent.med.uni-muenchen.de>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/20150930083302.694788319@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 3c1bbcf12924..f1fd0889e8af 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -499,27 +499,59 @@ void set_personality_ia32(bool x32)
 }
 EXPORT_SYMBOL_GPL(set_personality_ia32);
 
+/*
+ * Called from fs/proc with a reference on @p to find the function
+ * which called into schedule(). This needs to be done carefully
+ * because the task might wake up and we might look at a stack
+ * changing under us.
+ */
 unsigned long get_wchan(struct task_struct *p)
 {
-	unsigned long stack;
-	u64 fp, ip;
+	unsigned long start, bottom, top, sp, fp, ip;
 	int count = 0;
 
 	if (!p || p == current || p->state == TASK_RUNNING)
 		return 0;
-	stack = (unsigned long)task_stack_page(p);
-	if (p->thread.sp < stack || p->thread.sp >= stack+THREAD_SIZE)
+
+	start = (unsigned long)task_stack_page(p);
+	if (!start)
+		return 0;
+
+	/*
+	 * Layout of the stack page:
+	 *
+	 * ----------- topmax = start + THREAD_SIZE - sizeof(unsigned long)
+	 * PADDING
+	 * ----------- top = topmax - TOP_OF_KERNEL_STACK_PADDING
+	 * stack
+	 * ----------- bottom = start + sizeof(thread_info)
+	 * thread_info
+	 * ----------- start
+	 *
+	 * The tasks stack pointer points at the location where the
+	 * framepointer is stored. The data on the stack is:
+	 * ... IP FP ... IP FP
+	 *
+	 * We need to read FP and IP, so we need to adjust the upper
+	 * bound by another unsigned long.
+	 */
+	top = start + THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;
+	top -= 2 * sizeof(unsigned long);
+	bottom = start + sizeof(struct thread_info);
+
+	sp = READ_ONCE(p->thread.sp);
+	if (sp < bottom || sp > top)
 		return 0;
-	fp = *(u64 *)(p->thread.sp);
+
+	fp = READ_ONCE(*(unsigned long *)sp);
 	do {
-		if (fp < (unsigned long)stack ||
-		    fp >= (unsigned long)stack+THREAD_SIZE)
+		if (fp < bottom || fp > top)
 			return 0;
-		ip = *(u64 *)(fp+8);
+		ip = READ_ONCE(*(unsigned long *)(fp + sizeof(unsigned long)));
 		if (!in_sched_functions(ip))
 			return ip;
-		fp = *(u64 *)fp;
-	} while (count++ < 16);
+		fp = READ_ONCE(*(unsigned long *)fp);
+	} while (count++ < 16 && p->state != TASK_RUNNING);
 	return 0;
 }
 

commit a5b9e5a2f14f25a8dae987494d50ad3aac7366b6
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 30 14:31:34 2015 -0700

    x86/ldt: Make modify_ldt() optional
    
    The modify_ldt syscall exposes a large attack surface and is
    unnecessary for modern userspace.  Make it optional.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan Beulich <jbeulich@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: security@kernel.org <security@kernel.org>
    Cc: xen-devel <xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/a605166a771c343fd64802dece77a903507333bd.1438291540.git.luto@kernel.org
    [ Made MATH_EMULATION dependent on MODIFY_LDT_SYSCALL. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 7ff035c6c54d..3c1bbcf12924 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -121,6 +121,7 @@ void __show_regs(struct pt_regs *regs, int all)
 void release_thread(struct task_struct *dead_task)
 {
 	if (dead_task->mm) {
+#ifdef CONFIG_MODIFY_LDT_SYSCALL
 		if (dead_task->mm->context.ldt) {
 			pr_warn("WARNING: dead process %s still has LDT? <%p/%d>\n",
 				dead_task->comm,
@@ -128,6 +129,7 @@ void release_thread(struct task_struct *dead_task)
 				dead_task->mm->context.ldt->size);
 			BUG();
 		}
+#endif
 	}
 }
 

commit 5b929bd11df23922daf1be5d52731cc3900c1d79
Merge: b2c51106c758 37868fe113ff
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jul 31 10:23:35 2015 +0200

    Merge branch 'x86/urgent' into x86/asm, before applying dependent patches
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 37868fe113ff2ba814b3b4eb12df214df555f8dc
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 30 14:31:32 2015 -0700

    x86/ldt: Make modify_ldt synchronous
    
    modify_ldt() has questionable locking and does not synchronize
    threads.  Improve it: redesign the locking and synchronize all
    threads' LDTs using an IPI on all modifications.
    
    This will dramatically slow down modify_ldt in multithreaded
    programs, but there shouldn't be any multithreaded programs that
    care about modify_ldt's performance in the first place.
    
    This fixes some fallout from the CVE-2015-5157 fixes.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan Beulich <jbeulich@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: security@kernel.org <security@kernel.org>
    Cc: <stable@vger.kernel.org>
    Cc: xen-devel <xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/4c6978476782160600471bd865b318db34c7b628.1438291540.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 71d7849a07f7..f6b916387590 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -121,11 +121,11 @@ void __show_regs(struct pt_regs *regs, int all)
 void release_thread(struct task_struct *dead_task)
 {
 	if (dead_task->mm) {
-		if (dead_task->mm->context.size) {
+		if (dead_task->mm->context.ldt) {
 			pr_warn("WARNING: dead process %s still has LDT? <%p/%d>\n",
 				dead_task->comm,
 				dead_task->mm->context.ldt,
-				dead_task->mm->context.size);
+				dead_task->mm->context.ldt->size);
 			BUG();
 		}
 	}

commit 7da770785f9740af1cb24b8fd63075543bd00711
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jun 22 07:55:13 2015 -0400

    x86/compat: Rename 'start_thread_ia32' to 'compat_start_thread'
    
    This function is shared between the 32-bit compat and x32 ABIs.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1434974121-32575-5-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 71d7849a07f7..0831ba3bcf95 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -248,8 +248,8 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 			    __USER_CS, __USER_DS, 0);
 }
 
-#ifdef CONFIG_IA32_EMULATION
-void start_thread_ia32(struct pt_regs *regs, u32 new_ip, u32 new_sp)
+#ifdef CONFIG_COMPAT
+void compat_start_thread(struct pt_regs *regs, u32 new_ip, u32 new_sp)
 {
 	start_thread_common(regs, new_ip, new_sp,
 			    test_thread_flag(TIF_X32)

commit c1bd55f922a2df6038060ed48a82d146d301ca12
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Tue Jun 30 15:00:00 2015 -0700

    x86: opt into HAVE_COPY_THREAD_TLS, for both 32-bit and 64-bit
    
    For 32-bit userspace on a 64-bit kernel, this requires modifying
    stub32_clone to actually swap the appropriate arguments to match
    CONFIG_CLONE_BACKWARDS, rather than just leaving the C argument for tls
    broken.
    
    Patch co-authored by Josh Triplett and Thiago Macieira.
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thiago Macieira <thiago.macieira@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 843f92e4c711..71d7849a07f7 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -150,8 +150,8 @@ static inline u32 read_32bit_tls(struct task_struct *t, int tls)
 	return get_desc_base(&t->thread.tls_array[tls]);
 }
 
-int copy_thread(unsigned long clone_flags, unsigned long sp,
-		unsigned long arg, struct task_struct *p)
+int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
+		unsigned long arg, struct task_struct *p, unsigned long tls)
 {
 	int err;
 	struct pt_regs *childregs;
@@ -207,10 +207,10 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 #ifdef CONFIG_IA32_EMULATION
 		if (is_ia32_task())
 			err = do_set_thread_area(p, -1,
-				(struct user_desc __user *)childregs->si, 0);
+				(struct user_desc __user *)tls, 0);
 		else
 #endif
-			err = do_arch_prctl(p, ARCH_SET_FS, childregs->r8);
+			err = do_arch_prctl(p, ARCH_SET_FS, tls);
 		if (err)
 			goto out;
 	}

commit d70b3ef54ceaf1c7c92209f5a662a670d04cbed9
Merge: 650ec5a6bd5d 7ef3d7d58d9d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 17:59:09 2015 -0700

    Merge branch 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 core updates from Ingo Molnar:
     "There were so many changes in the x86/asm, x86/apic and x86/mm topics
      in this cycle that the topical separation of -tip broke down somewhat -
      so the result is a more traditional architecture pull request,
      collected into the 'x86/core' topic.
    
      The topics were still maintained separately as far as possible, so
      bisectability and conceptual separation should still be pretty good -
      but there were a handful of merge points to avoid excessive
      dependencies (and conflicts) that would have been poorly tested in the
      end.
    
      The next cycle will hopefully be much more quiet (or at least will
      have fewer dependencies).
    
      The main changes in this cycle were:
    
       * x86/apic changes, with related IRQ core changes: (Jiang Liu, Thomas
         Gleixner)
    
         - This is the second and most intrusive part of changes to the x86
           interrupt handling - full conversion to hierarchical interrupt
           domains:
    
              [IOAPIC domain]   -----
                                     |
              [MSI domain]      --------[Remapping domain] ----- [ Vector domain ]
                                     |   (optional)          |
              [HPET MSI domain] -----                        |
                                                             |
              [DMAR domain]     -----------------------------
                                                             |
              [Legacy domain]   -----------------------------
    
           This now reflects the actual hardware and allowed us to distangle
           the domain specific code from the underlying parent domain, which
           can be optional in the case of interrupt remapping.  It's a clear
           separation of functionality and removes quite some duct tape
           constructs which plugged the remap code between ioapic/msi/hpet
           and the vector management.
    
         - Intel IOMMU IRQ remapping enhancements, to allow direct interrupt
           injection into guests (Feng Wu)
    
       * x86/asm changes:
    
         - Tons of cleanups and small speedups, micro-optimizations.  This
           is in preparation to move a good chunk of the low level entry
           code from assembly to C code (Denys Vlasenko, Andy Lutomirski,
           Brian Gerst)
    
         - Moved all system entry related code to a new home under
           arch/x86/entry/ (Ingo Molnar)
    
         - Removal of the fragile and ugly CFI dwarf debuginfo annotations.
           Conversion to C will reintroduce many of them - but meanwhile
           they are only getting in the way, and the upstream kernel does
           not rely on them (Ingo Molnar)
    
         - NOP handling refinements. (Borislav Petkov)
    
       * x86/mm changes:
    
         - Big PAT and MTRR rework: making the code more robust and
           preparing to phase out exposing direct MTRR interfaces to drivers -
           in favor of using PAT driven interfaces (Toshi Kani, Luis R
           Rodriguez, Borislav Petkov)
    
         - New ioremap_wt()/set_memory_wt() interfaces to support
           Write-Through cached memory mappings.  This is especially
           important for good performance on NVDIMM hardware (Toshi Kani)
    
       * x86/ras changes:
    
         - Add support for deferred errors on AMD (Aravind Gopalakrishnan)
    
           This is an important RAS feature which adds hardware support for
           poisoned data.  That means roughly that the hardware marks data
           which it has detected as corrupted but wasn't able to correct, as
           poisoned data and raises an APIC interrupt to signal that in the
           form of a deferred error.  It is the OS's responsibility then to
           take proper recovery action and thus prolonge system lifetime as
           far as possible.
    
         - Add support for Intel "Local MCE"s: upcoming CPUs will support
           CPU-local MCE interrupts, as opposed to the traditional system-
           wide broadcasted MCE interrupts (Ashok Raj)
    
         - Misc cleanups (Borislav Petkov)
    
       * x86/platform changes:
    
         - Intel Atom SoC updates
    
      ... and lots of other cleanups, fixlets and other changes - see the
      shortlog and the Git log for details"
    
    * 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (222 commits)
      x86/hpet: Use proper hpet device number for MSI allocation
      x86/hpet: Check for irq==0 when allocating hpet MSI interrupts
      x86/mm/pat, drivers/infiniband/ipath: Use arch_phys_wc_add() and require PAT disabled
      x86/mm/pat, drivers/media/ivtv: Use arch_phys_wc_add() and require PAT disabled
      x86/platform/intel/baytrail: Add comments about why we disabled HPET on Baytrail
      genirq: Prevent crash in irq_move_irq()
      genirq: Enhance irq_data_to_desc() to support hierarchy irqdomain
      iommu, x86: Properly handle posted interrupts for IOMMU hotplug
      iommu, x86: Provide irq_remapping_cap() interface
      iommu, x86: Setup Posted-Interrupts capability for Intel iommu
      iommu, x86: Add cap_pi_support() to detect VT-d PI capability
      iommu, x86: Avoid migrating VT-d posted interrupts
      iommu, x86: Save the mode (posted or remapped) of an IRTE
      iommu, x86: Implement irq_set_vcpu_affinity for intel_ir_chip
      iommu: dmar: Provide helper to copy shared irte fields
      iommu: dmar: Extend struct irte for VT-d Posted-Interrupts
      iommu: Add new member capability to struct irq_remap_ops
      x86/asm/entry/64: Disentangle error_entry/exit gsbase/ebx/usermode code
      x86/asm/entry/32: Shorten __audit_syscall_entry() args preparation
      x86/asm/entry/32: Explain reloading of registers after __audit_syscall_entry()
      ...

commit 78f7f1e54bac032b98956862a5bcf8c28ed22d07
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 02:54:44 2015 +0200

    x86/fpu: Rename fpu-internal.h to fpu/internal.h
    
    This unifies all the FPU related header files under a unified, hiearchical
    naming scheme:
    
     - asm/fpu/types.h:      FPU related data types, needed for 'struct task_struct',
                             widely included in almost all kernel code, and hence kept
                             as small as possible.
    
     - asm/fpu/api.h:        FPU related 'public' methods exported to other subsystems.
    
     - asm/fpu/internal.h:   FPU subsystem internal methods
    
     - asm/fpu/xsave.h:      XSAVE support internal methods
    
    (Also standardize the header guard in asm/fpu/internal.h.)
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4504569c6c4e..c50e013b57d2 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -38,7 +38,7 @@
 
 #include <asm/pgtable.h>
 #include <asm/processor.h>
-#include <asm/fpu-internal.h>
+#include <asm/fpu/internal.h>
 #include <asm/mmu_context.h>
 #include <asm/prctl.h>
 #include <asm/desc.h>

commit 384a23f939912d368d2b42e1b41992be09aaf266
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 23 17:43:27 2015 +0200

    x86/fpu: Use 'struct fpu' in switch_fpu_finish()
    
    Migrate this function to pure 'struct fpu' usage.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index fefe65efd9d6..4504569c6c4e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -273,12 +273,14 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 {
 	struct thread_struct *prev = &prev_p->thread;
 	struct thread_struct *next = &next_p->thread;
+	struct fpu *prev_fpu = &prev->fpu;
+	struct fpu *next_fpu = &next->fpu;
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &per_cpu(cpu_tss, cpu);
 	unsigned fsindex, gsindex;
-	fpu_switch_t fpu;
+	fpu_switch_t fpu_switch;
 
-	fpu = switch_fpu_prepare(&prev_p->thread.fpu, &next_p->thread.fpu, cpu);
+	fpu_switch = switch_fpu_prepare(prev_fpu, next_fpu, cpu);
 
 	/* We must save %fs and %gs before load_TLS() because
 	 * %fs and %gs may be cleared by load_TLS().
@@ -390,7 +392,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 		wrmsrl(MSR_KERNEL_GS_BASE, next->gs);
 	prev->gsindex = gsindex;
 
-	switch_fpu_finish(next_p, fpu);
+	switch_fpu_finish(next_fpu, fpu_switch);
 
 	/*
 	 * Switch the PDA and FPU contexts.

commit cb8818b6acb45a4e0acc2308df216f36cc5b950c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 23 17:39:04 2015 +0200

    x86/fpu: Use 'struct fpu' in switch_fpu_prepare()
    
    Migrate this function to pure 'struct fpu' usage.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 99cc4b8589ad..fefe65efd9d6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -278,7 +278,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	unsigned fsindex, gsindex;
 	fpu_switch_t fpu;
 
-	fpu = switch_fpu_prepare(prev_p, next_p, cpu);
+	fpu = switch_fpu_prepare(&prev_p->thread.fpu, &next_p->thread.fpu, cpu);
 
 	/* We must save %fs and %gs before load_TLS() because
 	 * %fs and %gs may be cleared by load_TLS().

commit 3a0aee4801d475b64a408539c01ec0d17d52192b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 13:16:47 2015 +0200

    x86/fpu: Rename math_state_restore() to fpu__restore()
    
    Move to the new fpu__*() namespace.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ae6efeccb46e..99cc4b8589ad 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -298,7 +298,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 * Leave lazy mode, flushing any hypercalls made here.  This
 	 * must be done after loading TLS entries in the GDT but before
 	 * loading segments that might reference them, and and it must
-	 * be done before math_state_restore, so the TS bit is up to
+	 * be done before fpu__restore(), so the TS bit is up to
 	 * date.
 	 */
 	arch_end_context_switch(next_p);

commit f89e32e0a3df2f29d61fdc120ac62654ef267111
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 10:58:10 2015 +0200

    x86/fpu: Fix header file dependencies of fpu-internal.h
    
    Fix a minor header file dependency bug in asm/fpu-internal.h: it
    relies on i387.h but does not include it. All users of fpu-internal.h
    included it explicitly.
    
    Also remove unnecessary includes, to reduce compilation time.
    
    This also makes it easier to use it as a standalone header file
    for FPU internals, such as an upcoming C module in arch/x86/kernel/fpu/.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ddfdbf74f174..ae6efeccb46e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -38,7 +38,6 @@
 
 #include <asm/pgtable.h>
 #include <asm/processor.h>
-#include <asm/i387.h>
 #include <asm/fpu-internal.h>
 #include <asm/mmu_context.h>
 #include <asm/prctl.h>

commit fed7c3f0f750f225317828d691e9eb76eec887b3
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Fri Apr 24 17:31:34 2015 +0200

    x86/entry: Remove unused 'kernel_stack' per-cpu variable
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1429889495-27850-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ddfdbf74f174..82134506faa8 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -409,9 +409,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/* Reload esp0 and ss1.  This changes current_thread_info(). */
 	load_sp0(tss, next);
 
-	this_cpu_write(kernel_stack,
-		(unsigned long)task_stack_page(next_p) + THREAD_SIZE);
-
 	/*
 	 * Now maybe reload the debug registers and handle I/O bitmaps
 	 */

commit 61f01dd941ba9e06d2bf05994450ecc3d61b6b8b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Apr 26 16:47:59 2015 -0700

    x86_64, asm: Work around AMD SYSRET SS descriptor attribute issue
    
    AMD CPUs don't reinitialize the SS descriptor on SYSRET, so SYSRET with
    SS == 0 results in an invalid usermode state in which SS is apparently
    equal to __USER_DS but causes #SS if used.
    
    Work around the issue by setting SS to __KERNEL_DS __switch_to, thus
    ensuring that SYSRET never happens with SS set to NULL.
    
    This was exposed by a recent vDSO cleanup.
    
    Fixes: e7d6eefaaa44 x86/vdso32/syscall.S: Do not load __USER32_DS to %ss
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Anvin <hpa@zytor.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <vda.linux@googlemail.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4baaa972f52a..ddfdbf74f174 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -419,6 +419,34 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 		     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))
 		__switch_to_xtra(prev_p, next_p, tss);
 
+	if (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {
+		/*
+		 * AMD CPUs have a misfeature: SYSRET sets the SS selector but
+		 * does not update the cached descriptor.  As a result, if we
+		 * do SYSRET while SS is NULL, we'll end up in user mode with
+		 * SS apparently equal to __USER_DS but actually unusable.
+		 *
+		 * The straightforward workaround would be to fix it up just
+		 * before SYSRET, but that would slow down the system call
+		 * fast paths.  Instead, we ensure that SS is never NULL in
+		 * system call context.  We do this by replacing NULL SS
+		 * selectors at every context switch.  SYSCALL sets up a valid
+		 * SS, so the only way to get NULL is to re-enter the kernel
+		 * from CPL 3 through an interrupt.  Since that can't happen
+		 * in the same task as a running syscall, we are guaranteed to
+		 * context switch between every interrupt vector entry and a
+		 * subsequent SYSRET.
+		 *
+		 * We read SS first because SS reads are much faster than
+		 * writes.  Out of caution, we force SS to __KERNEL_DS even if
+		 * it previously had a different non-NULL value.
+		 */
+		unsigned short ss_sel;
+		savesegment(ss, ss_sel);
+		if (ss_sel != __KERNEL_DS)
+			loadsegment(ss, __KERNEL_DS);
+	}
+
 	return prev_p;
 }
 

commit ef593260f0cae2699874f098fb5b19fb46502cb3
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Thu Mar 19 18:17:46 2015 +0100

    x86/asm/entry: Get rid of KERNEL_STACK_OFFSET
    
    PER_CPU_VAR(kernel_stack) was set up in a way where it points
    five stack slots below the top of stack.
    
    Presumably, it was done to avoid one "sub $5*8,%rsp"
    in syscall/sysenter code paths, where iret frame needs to be
    created by hand.
    
    Ironically, none of them benefits from this optimization,
    since all of them need to allocate additional data on stack
    (struct pt_regs), so they still have to perform subtraction.
    
    This patch eliminates KERNEL_STACK_OFFSET.
    
    PER_CPU_VAR(kernel_stack) now points directly to top of stack.
    pt_regs allocations are adjusted to allocate iret frame as well.
    Hopefully we can merge it later with 32-bit specific
    PER_CPU_VAR(cpu_current_top_of_stack) variable...
    
    Net result in generated code is that constants in several insns
    are changed.
    
    This change is necessary for changing struct pt_regs creation
    in SYSCALL64 code path from MOV to PUSH instructions.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1426785469-15125-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index da8b74598d90..4baaa972f52a 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -410,8 +410,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	load_sp0(tss, next);
 
 	this_cpu_write(kernel_stack,
-		  (unsigned long)task_stack_page(next_p) +
-		  THREAD_SIZE - KERNEL_STACK_OFFSET);
+		(unsigned long)task_stack_page(next_p) + THREAD_SIZE);
 
 	/*
 	 * Now maybe reload the debug registers and handle I/O bitmaps

commit 1daeaa315164c60b937f56fe3848d4328c358eba
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sat Mar 21 18:54:21 2015 -0400

    x86/asm/entry: Fix execve() and sigreturn() syscalls to always return via IRET
    
    Both the execve() and sigreturn() family of syscalls have the
    ability to change registers in ways that may not be compatabile
    with the syscall path they were called from.
    
    In particular, SYSRET and SYSEXIT can't handle non-default %cs and %ss,
    and some bits in eflags.
    
    These syscalls have stubs that are hardcoded to jump to the IRET path,
    and not return to the original syscall path.
    
    The following commit:
    
       76f5df43cab5e76 ("Always allocate a complete "struct pt_regs" on the kernel stack")
    
    recently changed this for some 32-bit compat syscalls, but introduced a bug where
    execve from a 32-bit program to a 64-bit program would fail because it still returned
    via SYSRETL. This caused Wine to fail when built for both 32-bit and 64-bit.
    
    This patch sets TIF_NOTIFY_RESUME for execve() and sigreturn() so
    that the IRET path is always taken on exit to userspace.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1426978461-32089-1-git-send-email-brgerst@gmail.com
    [ Improved the changelog and comments. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 97f5658290b7..da8b74598d90 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -239,6 +239,7 @@ start_thread_common(struct pt_regs *regs, unsigned long new_ip,
 	regs->cs		= _cs;
 	regs->ss		= _ss;
 	regs->flags		= X86_EFLAGS_IF;
+	force_iret();
 }
 
 void

commit c38e503804b0402c510f82437069f7769fa0cea9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 17 14:42:59 2015 +0100

    x86/asm/entry/64: Rename 'old_rsp' to 'rsp_scratch'
    
    Make clear that the usage of PER_CPU(old_rsp) is purely temporary,
    by renaming it to 'rsp_scratch'.
    
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 14df2be4711f..97f5658290b7 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -52,7 +52,7 @@
 
 asmlinkage extern void ret_from_fork(void);
 
-__visible DEFINE_PER_CPU(unsigned long, old_rsp);
+__visible DEFINE_PER_CPU(unsigned long, rsp_scratch);
 
 /* Prints also some state that isn't saved in the pt_regs */
 void __show_regs(struct pt_regs *regs, int all)

commit ac9af4983e77765a642b5a21086bc1fdc55418c4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 17 14:42:59 2015 +0100

    x86/asm/entry/64: Remove thread_struct::usersp
    
    Nothing uses thread_struct::usersp anymore, so remove it.
    
    Originally-from: Denys Vlasenko <dvlasenk@redhat.com>
    Tested-by: Borislav Petkov <bp@alien8.de>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 59696d76a4e3..14df2be4711f 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -161,7 +161,6 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 	p->thread.sp0 = (unsigned long)task_stack_page(p) + THREAD_SIZE;
 	childregs = task_pt_regs(p);
 	p->thread.sp = (unsigned long) childregs;
-	p->thread.usersp = me->thread.usersp;
 	set_tsk_thread_flag(p, TIF_FORK);
 	p->thread.io_bitmap_ptr = NULL;
 
@@ -235,7 +234,6 @@ start_thread_common(struct pt_regs *regs, unsigned long new_ip,
 	loadsegment(es, _ds);
 	loadsegment(ds, _ds);
 	load_gs_index(0);
-	current->thread.usersp	= new_sp;
 	regs->ip		= new_ip;
 	regs->sp		= new_sp;
 	regs->cs		= _cs;
@@ -397,7 +395,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/*
 	 * Switch the PDA and FPU contexts.
 	 */
-	prev->usersp = this_cpu_read(old_rsp);
 	this_cpu_write(current_task, next_p);
 
 	/*

commit 9854dd74c3f6af8d9d527de86c6074b7ed0495f1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 17 14:42:59 2015 +0100

    x86/asm/entry/64: Simplify 'old_rsp' usage
    
    Remove all manipulations of PER_CPU(old_rsp) in C code:
    
     - it is not used on SYSRET return anymore, and system entries
       are atomic, so updating it from the fork and context switch
       paths is pointless.
    
     - Tweak a few related comments as well: we no longer have a
       "partial stack frame" on entry, ever.
    
    Based on (split out of) patch from Denys Vlasenko.
    
    Originally-from: Denys Vlasenko <dvlasenk@redhat.com>
    Tested-by: Borislav Petkov <bp@alien8.de>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1426599779-8010-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e8c124a1f885..59696d76a4e3 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -238,7 +238,6 @@ start_thread_common(struct pt_regs *regs, unsigned long new_ip,
 	current->thread.usersp	= new_sp;
 	regs->ip		= new_ip;
 	regs->sp		= new_sp;
-	this_cpu_write(old_rsp, new_sp);
 	regs->cs		= _cs;
 	regs->ss		= _ss;
 	regs->flags		= X86_EFLAGS_IF;
@@ -399,7 +398,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 * Switch the PDA and FPU contexts.
 	 */
 	prev->usersp = this_cpu_read(old_rsp);
-	this_cpu_write(old_rsp, next->usersp);
 	this_cpu_write(current_task, next_p);
 
 	/*

commit 263042e4630a85e856b4a8cd72f28dab33ef4741
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Mon Mar 9 19:39:23 2015 +0100

    x86/asm/entry/64: Save user RSP in pt_regs->sp on SYSCALL64 fastpath
    
    Prepare for the removal of 'usersp', by simplifying PER_CPU(old_rsp) usage:
    
      - use it only as temp storage
    
      - store the userspace stack pointer immediately in pt_regs->sp
        on syscall entry, instead of using it later, on syscall exit.
    
      - change C code to use pt_regs->sp only, instead of PER_CPU(old_rsp)
        and task->thread.usersp.
    
    FIXUP/RESTORE_TOP_OF_STACK are simplified as well.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1425926364-9526-4-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 1e393d27d701..e8c124a1f885 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -602,6 +602,5 @@ long sys_arch_prctl(int code, unsigned long addr)
 
 unsigned long KSTK_ESP(struct task_struct *task)
 {
-	return (test_tsk_thread_flag(task, TIF_IA32)) ?
-			(task_pt_regs(task)->sp) : ((task)->thread.usersp);
+	return task_pt_regs(task)->sp;
 }

commit b27559a433bb6080d95c2593d4a2b81401197911
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Mar 6 17:50:18 2015 -0800

    x86/asm/entry: Delay loading sp0 slightly on task switch
    
    The change:
    
      75182b1632a8 ("x86/asm/entry: Switch all C consumers of kernel_stack to this_cpu_sp0()")
    
    had the unintended side effect of changing the return value of
    current_thread_info() during part of the context switch process.
    Change it back.
    
    This has no effect as far as I can tell -- it's just for
    consistency.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/9fcaa47dd8487db59eed7a3911b6ae409476763e.1425692936.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 2cd562f96c1f..1e393d27d701 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -283,9 +283,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 
 	fpu = switch_fpu_prepare(prev_p, next_p, cpu);
 
-	/* Reload esp0 and ss1. */
-	load_sp0(tss, next);
-
 	/* We must save %fs and %gs before load_TLS() because
 	 * %fs and %gs may be cleared by load_TLS().
 	 *
@@ -413,6 +410,9 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	task_thread_info(prev_p)->saved_preempt_count = this_cpu_read(__preempt_count);
 	this_cpu_write(__preempt_count, task_thread_info(next_p)->saved_preempt_count);
 
+	/* Reload esp0 and ss1.  This changes current_thread_info(). */
+	load_sp0(tss, next);
+
 	this_cpu_write(kernel_stack,
 		  (unsigned long)task_stack_page(next_p) +
 		  THREAD_SIZE - KERNEL_STACK_OFFSET);

commit 24933b82c0d9a711475a5ef7904eb733f561e637
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:05 2015 -0800

    x86/asm/entry: Rename 'init_tss' to 'cpu_tss'
    
    It has nothing to do with init -- there's only one TSS per cpu.
    
    Other names considered include:
    
     - current_tss: Confusing because we never switch the tss.
     - singleton_tss: Too long.
    
    This patch was generated with 's/init_tss/cpu_tss/g'.  Followup
    patches will fix INIT_TSS and INIT_TSS_IST by hand.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/da29fb2a793e4f649d93ce2d1ed320ebe8516262.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 854b5981b327..2cd562f96c1f 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -277,7 +277,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	struct thread_struct *prev = &prev_p->thread;
 	struct thread_struct *next = &next_p->thread;
 	int cpu = smp_processor_id();
-	struct tss_struct *tss = &per_cpu(init_tss, cpu);
+	struct tss_struct *tss = &per_cpu(cpu_tss, cpu);
 	unsigned fsindex, gsindex;
 	fpu_switch_t fpu;
 

commit d2c032e3dc58137a7261a7824d3acce435db1d66
Merge: 7e8e385aaf6e 13a7a6ac0a11
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Mar 4 06:35:43 2015 +0100

    Merge tag 'v4.0-rc2' into x86/asm, to refresh the tree
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 72c6fb4f74b6b3797f5b1abd6944d7a1d2adbf04
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Tue Feb 24 16:01:39 2015 -0800

    x86/ia32-compat: Fix CLONE_SETTLS bitness of copy_thread()
    
    CLONE_SETTLS is expected to write a TLS entry in the GDT for
    32-bit callers and to set FSBASE for 64-bit callers.
    
    The correct check is is_ia32_task(), which returns true in the
    context of a 32-bit syscall.  TIF_IA32 is set if the task itself
    has a 32-bit personality, which is not the same thing.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Link: http://lkml.kernel.org/r/45e2d0d695393d76406a0c7225b82c76223e0cc5.1424822291.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 5a2c02913af3..936d43461dca 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -207,7 +207,7 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 	 */
 	if (clone_flags & CLONE_SETTLS) {
 #ifdef CONFIG_IA32_EMULATION
-		if (test_thread_flag(TIF_IA32))
+		if (is_ia32_task())
 			err = do_set_thread_area(p, -1,
 				(struct user_desc __user *)childregs->si, 0);
 		else

commit 1e02ce4cccdcb9688386e5b8d2c9fa4660b45389
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Oct 24 15:58:08 2014 -0700

    x86: Store a per-cpu shadow copy of CR4
    
    Context switches and TLB flushes can change individual bits of CR4.
    CR4 reads take several cycles, so store a shadow copy of CR4 in a
    per-cpu variable.
    
    To avoid wasting a cache line, I added the CR4 shadow to
    cpu_tlbstate, which is already touched in switch_mm.  The heaviest
    users of the cr4 shadow will be switch_mm and __switch_to_xtra, and
    __switch_to_xtra is called shortly after switch_mm during context
    switch, so the cacheline is likely to be hot.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Vince Weaver <vince@deater.net>
    Cc: "hillf.zj" <hillf.zj@alibaba-inc.com>
    Cc: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/3a54dd3353fffbf84804398e00dfdc5b7c1afd7d.1414190806.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 5a2c02913af3..67fcc43577d2 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -93,7 +93,7 @@ void __show_regs(struct pt_regs *regs, int all)
 	cr0 = read_cr0();
 	cr2 = read_cr2();
 	cr3 = read_cr3();
-	cr4 = read_cr4();
+	cr4 = __read_cr4();
 
 	printk(KERN_DEFAULT "FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n",
 	       fs, fsindex, gs, gsindex, shadowgs);

commit f647d7c155f069c1a068030255c300663516420e
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon Dec 8 13:55:20 2014 -0800

    x86_64, switch_to(): Load TLS descriptors before switching DS and ES
    
    Otherwise, if buggy user code points DS or ES into the TLS
    array, they would be corrupted after a context switch.
    
    This also significantly improves the comments and documents some
    gotchas in the code.
    
    Before this patch, the both tests below failed.  With this
    patch, the es test passes, although the gsbase test still fails.
    
     ----- begin es test -----
    
    /*
     * Copyright (c) 2014 Andy Lutomirski
     * GPL v2
     */
    
    static unsigned short GDT3(int idx)
    {
            return (idx << 3) | 3;
    }
    
    static int create_tls(int idx, unsigned int base)
    {
            struct user_desc desc = {
                    .entry_number    = idx,
                    .base_addr       = base,
                    .limit           = 0xfffff,
                    .seg_32bit       = 1,
                    .contents        = 0, /* Data, grow-up */
                    .read_exec_only  = 0,
                    .limit_in_pages  = 1,
                    .seg_not_present = 0,
                    .useable         = 0,
            };
    
            if (syscall(SYS_set_thread_area, &desc) != 0)
                    err(1, "set_thread_area");
    
            return desc.entry_number;
    }
    
    int main()
    {
            int idx = create_tls(-1, 0);
            printf("Allocated GDT index %d\n", idx);
    
            unsigned short orig_es;
            asm volatile ("mov %%es,%0" : "=rm" (orig_es));
    
            int errors = 0;
            int total = 1000;
            for (int i = 0; i < total; i++) {
                    asm volatile ("mov %0,%%es" : : "rm" (GDT3(idx)));
                    usleep(100);
    
                    unsigned short es;
                    asm volatile ("mov %%es,%0" : "=rm" (es));
                    asm volatile ("mov %0,%%es" : : "rm" (orig_es));
                    if (es != GDT3(idx)) {
                            if (errors == 0)
                                    printf("[FAIL]\tES changed from 0x%hx to 0x%hx\n",
                                           GDT3(idx), es);
                            errors++;
                    }
            }
    
            if (errors) {
                    printf("[FAIL]\tES was corrupted %d/%d times\n", errors, total);
                    return 1;
            } else {
                    printf("[OK]\tES was preserved\n");
                    return 0;
            }
    }
    
     ----- end es test -----
    
     ----- begin gsbase test -----
    
    /*
     * gsbase.c, a gsbase test
     * Copyright (c) 2014 Andy Lutomirski
     * GPL v2
     */
    
    static unsigned char *testptr, *testptr2;
    
    static unsigned char read_gs_testvals(void)
    {
            unsigned char ret;
            asm volatile ("movb %%gs:%1, %0" : "=r" (ret) : "m" (*testptr));
            return ret;
    }
    
    int main()
    {
            int errors = 0;
    
            testptr = mmap((void *)0x200000000UL, 1, PROT_READ | PROT_WRITE,
                           MAP_PRIVATE | MAP_FIXED | MAP_ANONYMOUS, -1, 0);
            if (testptr == MAP_FAILED)
                    err(1, "mmap");
    
            testptr2 = mmap((void *)0x300000000UL, 1, PROT_READ | PROT_WRITE,
                           MAP_PRIVATE | MAP_FIXED | MAP_ANONYMOUS, -1, 0);
            if (testptr2 == MAP_FAILED)
                    err(1, "mmap");
    
            *testptr = 0;
            *testptr2 = 1;
    
            if (syscall(SYS_arch_prctl, ARCH_SET_GS,
                        (unsigned long)testptr2 - (unsigned long)testptr) != 0)
                    err(1, "ARCH_SET_GS");
    
            usleep(100);
    
            if (read_gs_testvals() == 1) {
                    printf("[OK]\tARCH_SET_GS worked\n");
            } else {
                    printf("[FAIL]\tARCH_SET_GS failed\n");
                    errors++;
            }
    
            asm volatile ("mov %0,%%gs" : : "r" (0));
    
            if (read_gs_testvals() == 0) {
                    printf("[OK]\tWriting 0 to gs worked\n");
            } else {
                    printf("[FAIL]\tWriting 0 to gs failed\n");
                    errors++;
            }
    
            usleep(100);
    
            if (read_gs_testvals() == 0) {
                    printf("[OK]\tgsbase is still zero\n");
            } else {
                    printf("[FAIL]\tgsbase was corrupted\n");
                    errors++;
            }
    
            return errors == 0 ? 0 : 1;
    }
    
     ----- end gsbase test -----
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: <stable@vger.kernel.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/509d27c9fec78217691c3dad91cec87e1006b34a.1418075657.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 3ed4a68d4013..5a2c02913af3 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -283,24 +283,9 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 
 	fpu = switch_fpu_prepare(prev_p, next_p, cpu);
 
-	/*
-	 * Reload esp0, LDT and the page table pointer:
-	 */
+	/* Reload esp0 and ss1. */
 	load_sp0(tss, next);
 
-	/*
-	 * Switch DS and ES.
-	 * This won't pick up thread selector changes, but I guess that is ok.
-	 */
-	savesegment(es, prev->es);
-	if (unlikely(next->es | prev->es))
-		loadsegment(es, next->es);
-
-	savesegment(ds, prev->ds);
-	if (unlikely(next->ds | prev->ds))
-		loadsegment(ds, next->ds);
-
-
 	/* We must save %fs and %gs before load_TLS() because
 	 * %fs and %gs may be cleared by load_TLS().
 	 *
@@ -309,41 +294,101 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	savesegment(fs, fsindex);
 	savesegment(gs, gsindex);
 
+	/*
+	 * Load TLS before restoring any segments so that segment loads
+	 * reference the correct GDT entries.
+	 */
 	load_TLS(next, cpu);
 
 	/*
-	 * Leave lazy mode, flushing any hypercalls made here.
-	 * This must be done before restoring TLS segments so
-	 * the GDT and LDT are properly updated, and must be
-	 * done before math_state_restore, so the TS bit is up
-	 * to date.
+	 * Leave lazy mode, flushing any hypercalls made here.  This
+	 * must be done after loading TLS entries in the GDT but before
+	 * loading segments that might reference them, and and it must
+	 * be done before math_state_restore, so the TS bit is up to
+	 * date.
 	 */
 	arch_end_context_switch(next_p);
 
+	/* Switch DS and ES.
+	 *
+	 * Reading them only returns the selectors, but writing them (if
+	 * nonzero) loads the full descriptor from the GDT or LDT.  The
+	 * LDT for next is loaded in switch_mm, and the GDT is loaded
+	 * above.
+	 *
+	 * We therefore need to write new values to the segment
+	 * registers on every context switch unless both the new and old
+	 * values are zero.
+	 *
+	 * Note that we don't need to do anything for CS and SS, as
+	 * those are saved and restored as part of pt_regs.
+	 */
+	savesegment(es, prev->es);
+	if (unlikely(next->es | prev->es))
+		loadsegment(es, next->es);
+
+	savesegment(ds, prev->ds);
+	if (unlikely(next->ds | prev->ds))
+		loadsegment(ds, next->ds);
+
 	/*
 	 * Switch FS and GS.
 	 *
-	 * Segment register != 0 always requires a reload.  Also
-	 * reload when it has changed.  When prev process used 64bit
-	 * base always reload to avoid an information leak.
+	 * These are even more complicated than FS and GS: they have
+	 * 64-bit bases are that controlled by arch_prctl.  Those bases
+	 * only differ from the values in the GDT or LDT if the selector
+	 * is 0.
+	 *
+	 * Loading the segment register resets the hidden base part of
+	 * the register to 0 or the value from the GDT / LDT.  If the
+	 * next base address zero, writing 0 to the segment register is
+	 * much faster than using wrmsr to explicitly zero the base.
+	 *
+	 * The thread_struct.fs and thread_struct.gs values are 0
+	 * if the fs and gs bases respectively are not overridden
+	 * from the values implied by fsindex and gsindex.  They
+	 * are nonzero, and store the nonzero base addresses, if
+	 * the bases are overridden.
+	 *
+	 * (fs != 0 && fsindex != 0) || (gs != 0 && gsindex != 0) should
+	 * be impossible.
+	 *
+	 * Therefore we need to reload the segment registers if either
+	 * the old or new selector is nonzero, and we need to override
+	 * the base address if next thread expects it to be overridden.
+	 *
+	 * This code is unnecessarily slow in the case where the old and
+	 * new indexes are zero and the new base is nonzero -- it will
+	 * unnecessarily write 0 to the selector before writing the new
+	 * base address.
+	 *
+	 * Note: This all depends on arch_prctl being the only way that
+	 * user code can override the segment base.  Once wrfsbase and
+	 * wrgsbase are enabled, most of this code will need to change.
 	 */
 	if (unlikely(fsindex | next->fsindex | prev->fs)) {
 		loadsegment(fs, next->fsindex);
+
 		/*
-		 * Check if the user used a selector != 0; if yes
-		 *  clear 64bit base, since overloaded base is always
-		 *  mapped to the Null selector
+		 * If user code wrote a nonzero value to FS, then it also
+		 * cleared the overridden base address.
+		 *
+		 * XXX: if user code wrote 0 to FS and cleared the base
+		 * address itself, we won't notice and we'll incorrectly
+		 * restore the prior base address next time we reschdule
+		 * the process.
 		 */
 		if (fsindex)
 			prev->fs = 0;
 	}
-	/* when next process has a 64bit base use it */
 	if (next->fs)
 		wrmsrl(MSR_FS_BASE, next->fs);
 	prev->fsindex = fsindex;
 
 	if (unlikely(gsindex | next->gsindex | prev->gs)) {
 		load_gs_index(next->gsindex);
+
+		/* This works (and fails) the same way as fsindex above. */
 		if (gsindex)
 			prev->gs = 0;
 	}

commit 6f46b3aef0031c08a7b439d63013dad2aeb093b2
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Sep 2 19:57:33 2014 +0200

    x86: copy_thread: Don't nullify ->ptrace_bps twice
    
    Both 32bit and 64bit versions of copy_thread() do memset(ptrace_bps)
    twice for no reason, kill the 2nd memset().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20140902175733.GA21676@redhat.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 593257def776..3ed4a68d4013 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -192,8 +192,6 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 		childregs->sp = sp;
 
 	err = -ENOMEM;
-	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));
-
 	if (unlikely(test_tsk_thread_flag(me, TIF_IO_BITMAP))) {
 		p->thread.io_bitmap_ptr = kmemdup(me->thread.io_bitmap_ptr,
 						  IO_BITMAP_BYTES, GFP_KERNEL);

commit dc56c0f9b870fba7a4eef2bb463db6881284152b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Sep 2 19:57:30 2014 +0200

    x86, fpu: Shift "fpu_counter = 0" from copy_thread() to arch_dup_task_struct()
    
    Cosmetic, but I think thread.fpu_counter should be initialized in
    arch_dup_task_struct() too, along with other "fpu" variables. And
    probably it make sense to turn it into thread.fpu->counter.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20140902175730.GA21669@redhat.com
    Reviewed-by: Suresh Siddha <sbsiddha@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ca5b02d405c3..593257def776 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -163,7 +163,6 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 	p->thread.sp = (unsigned long) childregs;
 	p->thread.usersp = me->thread.usersp;
 	set_tsk_thread_flag(p, TIF_FORK);
-	p->thread.fpu_counter = 0;
 	p->thread.io_bitmap_ptr = NULL;
 
 	savesegment(gs, p->thread.gsindex);

commit ec00010972a0971b2c1da4fbe4e5c7d8ed1ecb05
Merge: 8c6e549a447c e041e328c4b4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jun 6 07:55:06 2014 +0200

    Merge branch 'perf/urgent' into perf/core, to resolve conflict and to prepare for new patches
    
    Conflicts:
            arch/x86/kernel/traps.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 2605fc216fa492f9e7c488bdc7f687cd6dcc703b
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri May 2 00:44:37 2014 +0200

    asmlinkage, x86: Add explicit __visible to arch/x86/*
    
    As requested by Linus add explicit __visible to the asmlinkage users.
    This marks all functions visible to assembler.
    
    Tree sweep for arch/x86/*
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1398984278-29319-3-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 9c0280f93d05..898d077617a9 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -52,7 +52,7 @@
 
 asmlinkage extern void ret_from_fork(void);
 
-asmlinkage DEFINE_PER_CPU(unsigned long, old_rsp);
+__visible DEFINE_PER_CPU(unsigned long, old_rsp);
 
 /* Prints also some state that isn't saved in the pt_regs */
 void __show_regs(struct pt_regs *regs, int all)

commit b24dc8dace74708fd849312722090169c5da97d3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sat Apr 19 18:10:09 2014 +0200

    uprobes/x86: Fix is_64bit_mm() with CONFIG_X86_X32
    
    is_64bit_mm() assumes that mm->context.ia32_compat means the 32-bit
    instruction set, this is not true if the task is TIF_X32.
    
    Change set_personality_ia32() to initialize mm->context.ia32_compat
    by TIF_X32 or TIF_IA32 instead of 1. This allows to fix is_64bit_mm()
    without affecting other users, they all treat ia32_compat as "bool".
    
    TIF_ in ->ia32_compat looks a bit strange, but this is grep-friendly
    and avoids the new define's.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Jim Keniston <jkenisto@us.ibm.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 9c0280f93d05..9b53940981b7 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -413,12 +413,11 @@ void set_personality_ia32(bool x32)
 	set_thread_flag(TIF_ADDR32);
 
 	/* Mark the associated mm as containing 32-bit tasks. */
-	if (current->mm)
-		current->mm->context.ia32_compat = 1;
-
 	if (x32) {
 		clear_thread_flag(TIF_IA32);
 		set_thread_flag(TIF_X32);
+		if (current->mm)
+			current->mm->context.ia32_compat = TIF_X32;
 		current->personality &= ~READ_IMPLIES_EXEC;
 		/* is_compat_task() uses the presence of the x32
 		   syscall bit flag to determine compat status */
@@ -426,6 +425,8 @@ void set_personality_ia32(bool x32)
 	} else {
 		set_thread_flag(TIF_IA32);
 		clear_thread_flag(TIF_X32);
+		if (current->mm)
+			current->mm->context.ia32_compat = TIF_IA32;
 		current->personality |= force_personality32;
 		/* Prepare the first "return" to user space */
 		current_thread_info()->status |= TS_COMPAT;

commit d320e203bad4cfcef3613e83a52f8c70a77e8a60
Merge: fe8a45df3680 11f918d3e2d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 14 16:55:56 2013 +0900

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull two x86 fixes from Ingo Molnar.
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/microcode/amd: Tone down printk(), don't treat a missing firmware file as an error
      x86/dumpstack: Fix printk_address for direct addresses

commit c375f15a434db1867cb004bafba92aba739e4e39
Author: Vineet Gupta <Vineet.Gupta1@synopsys.com>
Date:   Tue Nov 12 15:08:46 2013 -0800

    x86: move fpu_counter into ARCH specific thread_struct
    
    Only a couple of arches (sh/x86) use fpu_counter in task_struct so it can
    be moved out into ARCH specific thread_struct, reducing the size of
    task_struct for other arches.
    
    Compile tested i386_defconfig + gcc 4.7.3
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Paul Mundt <paul.mundt@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 45ab4d6fc8a7..10fe4c189621 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -163,7 +163,7 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 	p->thread.sp = (unsigned long) childregs;
 	p->thread.usersp = me->thread.usersp;
 	set_tsk_thread_flag(p, TIF_FORK);
-	p->fpu_counter = 0;
+	p->thread.fpu_counter = 0;
 	p->thread.io_bitmap_ptr = NULL;
 
 	savesegment(gs, p->thread.gsindex);

commit 5f01c98859073cb512b01d4fad74b5f4e047be0b
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri Oct 25 15:06:58 2013 +0200

    x86/dumpstack: Fix printk_address for direct addresses
    
    Consider a kernel crash in a module, simulated the following way:
    
     static int my_init(void)
     {
             char *map = (void *)0x5;
             *map = 3;
             return 0;
     }
     module_init(my_init);
    
    When we turn off FRAME_POINTERs, the very first instruction in
    that function causes a BUG. The problem is that we print IP in
    the BUG report using %pB (from printk_address). And %pB
    decrements the pointer by one to fix printing addresses of
    functions with tail calls.
    
    This was added in commit 71f9e59800e5ad4 ("x86, dumpstack: Use
    %pB format specifier for stack trace") to fix the call stack
    printouts.
    
    So instead of correct output:
    
      BUG: unable to handle kernel NULL pointer dereference at 0000000000000005
      IP: [<ffffffffa01ac000>] my_init+0x0/0x10 [pb173]
    
    We get:
    
      BUG: unable to handle kernel NULL pointer dereference at 0000000000000005
      IP: [<ffffffffa0152000>] 0xffffffffa0151fff
    
    To fix that, we use %pS only for stack addresses printouts (via
    newly added printk_stack_address) and %pB for regs->ip (via
    printk_address). I.e. we revert to the old behaviour for all
    except call stacks. And since from all those reliable is 1, we
    remove that parameter from printk_address.
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: joe@perches.com
    Cc: jirislaby@gmail.com
    Link: http://lkml.kernel.org/r/1382706418-8435-1-git-send-email-jslaby@suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 45ab4d6fc8a7..176ad94e1d57 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -63,7 +63,7 @@ void __show_regs(struct pt_regs *regs, int all)
 	unsigned int ds, cs, es;
 
 	printk(KERN_DEFAULT "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
-	printk_address(regs->ip, 1);
+	printk_address(regs->ip);
 	printk(KERN_DEFAULT "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,
 			regs->sp, regs->flags);
 	printk(KERN_DEFAULT "RAX: %016lx RBX: %016lx RCX: %016lx\n",

commit c2daa3bed53a81171cf8c1a36db798e82b91afe8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Aug 14 14:51:00 2013 +0200

    sched, x86: Provide a per-cpu preempt_count implementation
    
    Convert x86 to use a per-cpu preemption count. The reason for doing so
    is that accessing per-cpu variables is a lot cheaper than accessing
    thread_info variables.
    
    We still need to save/restore the actual preemption count due to
    PREEMPT_ACTIVE so we place the per-cpu __preempt_count variable in the
    same cache-line as the other hot __switch_to() variables such as
    current_task.
    
    NOTE: this save/restore is required even for !PREEMPT kernels as
    cond_resched() also relies on preempt_count's PREEMPT_ACTIVE to ignore
    task_struct::state.
    
    Also rename thread_info::preempt_count to ensure nobody is
    'accidentally' still poking at it.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-gzn5rfsf8trgjoqx8hyayy3q@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index bb1dc51bab05..45ab4d6fc8a7 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -363,6 +363,14 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	this_cpu_write(old_rsp, next->usersp);
 	this_cpu_write(current_task, next_p);
 
+	/*
+	 * If it were not for PREEMPT_ACTIVE we could guarantee that the
+	 * preempt_count of all tasks was equal here and this would not be
+	 * needed.
+	 */
+	task_thread_info(prev_p)->saved_preempt_count = this_cpu_read(__preempt_count);
+	this_cpu_write(__preempt_count, task_thread_info(next_p)->saved_preempt_count);
+
 	this_cpu_write(kernel_stack,
 		  (unsigned long)task_stack_page(next_p) +
 		  THREAD_SIZE - KERNEL_STACK_OFFSET);

commit 277d5b40b7bf495d2d4193746181b17dd98441b2
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Aug 5 15:02:43 2013 -0700

    x86, asmlinkage: Make several variables used from assembler/linker script visible
    
    Plus one function, load_gs_index().
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1375740170-7446-10-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6e8c1d02ab4b..bb1dc51bab05 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -52,7 +52,7 @@
 
 asmlinkage extern void ret_from_fork(void);
 
-DEFINE_PER_CPU(unsigned long, old_rsp);
+asmlinkage DEFINE_PER_CPU(unsigned long, old_rsp);
 
 /* Prints also some state that isn't saved in the pt_regs */
 void __show_regs(struct pt_regs *regs, int all)

commit 35ea7903b8a97162e38da9da3b560df74713321d
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Aug 5 15:02:39 2013 -0700

    x86, asmlinkage: Make 32bit/64bit __switch_to visible
    
    This function is called from inline assembler, so has to be visible.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1375740170-7446-6-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 05646bab4ca6..6e8c1d02ab4b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -274,7 +274,7 @@ void start_thread_ia32(struct pt_regs *regs, u32 new_ip, u32 new_sp)
  * Kprobes not supported here. Set the probe on schedule instead.
  * Function graph tracer not supported too.
  */
-__notrace_funcgraph struct task_struct *
+__visible __notrace_funcgraph struct task_struct *
 __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 {
 	struct thread_struct *prev = &prev_p->thread;

commit 55a0d3ff603a69ea4ec7677effd457f838390afc
Merge: 35c23d5d7976 13bfd47a0ef6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 2 16:25:06 2013 -0700

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 debug update from Ingo Molnar:
     "Misc debuggability improvements:
    
       - Optimize the x86 CPU register printout a bit
       - Expose the tboot TXT log via debugfs
       - Small do_debug() cleanup"
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/tboot: Provide debugfs interfaces to access TXT log
      x86: Remove weird PTR_ERR() in do_debug
      x86/debug: Only print out DR registers if they are not power-on defaults

commit 1adfa76a95fe4444124a502f7cc858a39d5b8e01
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Sat Apr 27 16:10:11 2013 -0700

    x86, flags: Rename X86_EFLAGS_BIT1 to X86_EFLAGS_FIXED
    
    Bit 1 in the x86 EFLAGS is always set.  Name the macro something that
    actually tries to explain what it is all about, rather than being a
    tautology.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Gleb Natapov <gleb@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/n/tip-f10rx5vjjm6tfnt8o1wseb3v@git.kernel.org

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 355ae06dbf94..f99a242730e9 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -176,7 +176,7 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 		childregs->bp = arg;
 		childregs->orig_ax = -1;
 		childregs->cs = __KERNEL_CS | get_kernel_rpl();
-		childregs->flags = X86_EFLAGS_IF | X86_EFLAGS_BIT1;
+		childregs->flags = X86_EFLAGS_IF | X86_EFLAGS_FIXED;
 		return 0;
 	}
 	*childregs = *current_pt_regs();

commit 4338774cd41a6abf72aa76585ce2184cea8ff8a2
Author: Dave Jones <davej@redhat.com>
Date:   Tue Jun 18 12:09:11 2013 -0400

    x86/debug: Only print out DR registers if they are not power-on defaults
    
    The DR registers are rarely useful when decoding oopses.
    With screen real estate during oopses at a premium, we can save
    two lines by only printing out these registers when they are set
    to something other than they power-on state.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20130618160911.GA24487@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 355ae06dbf94..a8b9abc5459a 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -105,11 +105,18 @@ void __show_regs(struct pt_regs *regs, int all)
 	get_debugreg(d0, 0);
 	get_debugreg(d1, 1);
 	get_debugreg(d2, 2);
-	printk(KERN_DEFAULT "DR0: %016lx DR1: %016lx DR2: %016lx\n", d0, d1, d2);
 	get_debugreg(d3, 3);
 	get_debugreg(d6, 6);
 	get_debugreg(d7, 7);
+
+	/* Only print out debug registers if they are in their non-default state. */
+	if ((d0 == 0) && (d1 == 0) && (d2 == 0) && (d3 == 0) &&
+	    (d6 == DR6_RESERVED) && (d7 == 0x400))
+		return;
+
+	printk(KERN_DEFAULT "DR0: %016lx DR1: %016lx DR2: %016lx\n", d0, d1, d2);
 	printk(KERN_DEFAULT "DR3: %016lx DR6: %016lx DR7: %016lx\n", d3, d6, d7);
+
 }
 
 void release_thread(struct task_struct *dead_task)

commit a43cb95d547a061ed5bf1acb28e0f5fd575e26c1
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 30 15:27:17 2013 -0700

    dump_stack: unify debug information printed by show_regs()
    
    show_regs() is inherently arch-dependent but it does make sense to print
    generic debug information and some archs already do albeit in slightly
    different forms.  This patch introduces a generic function to print debug
    information from show_regs() so that different archs print out the same
    information and it's much easier to modify what's printed.
    
    show_regs_print_info() prints out the same debug info as dump_stack()
    does plus task and thread_info pointers.
    
    * Archs which didn't print debug info now do.
    
      alpha, arc, blackfin, c6x, cris, frv, h8300, hexagon, ia64, m32r,
      metag, microblaze, mn10300, openrisc, parisc, score, sh64, sparc,
      um, xtensa
    
    * Already prints debug info.  Replaced with show_regs_print_info().
      The printed information is superset of what used to be there.
    
      arm, arm64, avr32, mips, powerpc, sh32, tile, unicore32, x86
    
    * s390 is special in that it used to print arch-specific information
      along with generic debug info.  Heiko and Martin think that the
      arch-specific extra isn't worth keeping s390 specfic implementation.
      Converted to use the generic version.
    
    Note that now all archs print the debug info before actual register
    dumps.
    
    An example BUG() dump follows.
    
     kernel BUG at /work/os/work/kernel/workqueue.c:4841!
     invalid opcode: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC
     Modules linked in:
     CPU: 0 PID: 1 Comm: swapper/0 Not tainted 3.9.0-rc1-work+ #7
     Hardware name: empty empty/S3992, BIOS 080011  10/26/2007
     task: ffff88007c85e040 ti: ffff88007c860000 task.ti: ffff88007c860000
     RIP: 0010:[<ffffffff8234a07e>]  [<ffffffff8234a07e>] init_workqueues+0x4/0x6
     RSP: 0000:ffff88007c861ec8  EFLAGS: 00010246
     RAX: ffff88007c861fd8 RBX: ffffffff824466a8 RCX: 0000000000000001
     RDX: 0000000000000046 RSI: 0000000000000001 RDI: ffffffff8234a07a
     RBP: ffff88007c861ec8 R08: 0000000000000000 R09: 0000000000000000
     R10: 0000000000000001 R11: 0000000000000000 R12: ffffffff8234a07a
     R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
     FS:  0000000000000000(0000) GS:ffff88007dc00000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
     CR2: ffff88015f7ff000 CR3: 00000000021f1000 CR4: 00000000000007f0
     DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
     DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
     Stack:
      ffff88007c861ef8 ffffffff81000312 ffffffff824466a8 ffff88007c85e650
      0000000000000003 0000000000000000 ffff88007c861f38 ffffffff82335e5d
      ffff88007c862080 ffffffff8223d8c0 ffff88007c862080 ffffffff81c47760
     Call Trace:
      [<ffffffff81000312>] do_one_initcall+0x122/0x170
      [<ffffffff82335e5d>] kernel_init_freeable+0x9b/0x1c8
      [<ffffffff81c47760>] ? rest_init+0x140/0x140
      [<ffffffff81c4776e>] kernel_init+0xe/0xf0
      [<ffffffff81c6be9c>] ret_from_fork+0x7c/0xb0
      [<ffffffff81c47760>] ? rest_init+0x140/0x140
      ...
    
    v2: Typo fix in x86-32.
    
    v3: CPU number dropped from show_regs_print_info() as
        dump_stack_print_info() has been updated to print it.  s390
        specific implementation dropped as requested by s390 maintainers.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Acked-by: Chris Metcalf <cmetcalf@tilera.com>           [tile bits]
    Acked-by: Richard Kuo <rkuo@codeaurora.org>             [hexagon bits]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 0f49677da51e..355ae06dbf94 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -62,7 +62,6 @@ void __show_regs(struct pt_regs *regs, int all)
 	unsigned int fsindex, gsindex;
 	unsigned int ds, cs, es;
 
-	show_regs_common();
 	printk(KERN_DEFAULT "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
 	printk_address(regs->ip, 1);
 	printk(KERN_DEFAULT "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,

commit 349eab6eb07794c59e37703ccbfeb5920721885c
Author: Chen Gang <gang.chen@asianux.com>
Date:   Tue Nov 6 14:45:46 2012 +0800

    x86/process: Change %8s to %s for pr_warn() in release_thread()
    
    the length of dead_task->comm[] is 16 (TASK_COMM_LEN)
    on pr_warn(), it is not meaningful to use %8s for task->comm[].
    
    So change it to %s, since the line is not solid anyway.
    
    Additional information:
    
     %8s  limit the width, not for the original string output length
          if name length is more than 8, it still can be fully displayed.
          if name length is less than 8, the ' ' will be filled before name.
    
     %.8s truly limit the original string output length (precision)
    
    Signed-off-by: Chen Gang <gang.chen@asianux.com>
    Link: http://lkml.kernel.org/n/tip-nridm1zvreai1tgfLjuexDmd@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6e68a6194965..0f49677da51e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -117,7 +117,7 @@ void release_thread(struct task_struct *dead_task)
 {
 	if (dead_task->mm) {
 		if (dead_task->mm->context.size) {
-			pr_warn("WARNING: dead process %8s still has LDT? <%p/%d>\n",
+			pr_warn("WARNING: dead process %s still has LDT? <%p/%d>\n",
 				dead_task->comm,
 				dead_task->mm->context.ldt,
 				dead_task->mm->context.size);

commit afa86fc426ff7e7f5477f15da9c405d08d5cf790
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 22:51:14 2012 -0400

    flagday: don't pass regs to copy_thread()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 74aac76c6e34..6e68a6194965 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -146,8 +146,7 @@ static inline u32 read_32bit_tls(struct task_struct *t, int tls)
 }
 
 int copy_thread(unsigned long clone_flags, unsigned long sp,
-		unsigned long arg,
-	struct task_struct *p, struct pt_regs *regs)
+		unsigned long arg, struct task_struct *p)
 {
 	int err;
 	struct pt_regs *childregs;

commit 1d4b4b2994b5fc208963c0b795291f8c1f18becf
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 22:34:11 2012 -0400

    x86, um: switch to generic fork/vfork/clone
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 16c6365e2b86..74aac76c6e34 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -169,7 +169,7 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 	savesegment(ds, p->thread.ds);
 	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));
 
-	if (unlikely(!regs)) {
+	if (unlikely(p->flags & PF_KTHREAD)) {
 		/* kernel thread */
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->sp = (unsigned long)childregs;
@@ -181,10 +181,11 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 		childregs->flags = X86_EFLAGS_IF | X86_EFLAGS_BIT1;
 		return 0;
 	}
-	*childregs = *regs;
+	*childregs = *current_pt_regs();
 
 	childregs->ax = 0;
-	childregs->sp = sp;
+	if (sp)
+		childregs->sp = sp;
 
 	err = -ENOMEM;
 	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));

commit 42859eea96ba6beabfb0369a1eeffa3c7d2bd9cb
Merge: f59b51fe3d30 f322220d6159
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 10 12:02:25 2012 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull generic execve() changes from Al Viro:
     "This introduces the generic kernel_thread() and kernel_execve()
      functions, and switches x86, arm, alpha, um and s390 over to them."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (26 commits)
      s390: convert to generic kernel_execve()
      s390: switch to generic kernel_thread()
      s390: fold kernel_thread_helper() into ret_from_fork()
      s390: fold execve_tail() into start_thread(), convert to generic sys_execve()
      um: switch to generic kernel_thread()
      x86, um/x86: switch to generic sys_execve and kernel_execve
      x86: split ret_from_fork
      alpha: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      alpha: switch to generic kernel_thread()
      alpha: switch to generic sys_execve()
      arm: get rid of execve wrapper, switch to generic execve() implementation
      arm: optimized current_pt_regs()
      arm: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      arm: split ret_from_fork, simplify kernel_thread() [based on patch by rmk]
      generic sys_execve()
      generic kernel_execve()
      new helper: current_pt_regs()
      preparation for generic kernel_thread()
      um: kill thread->forking
      um: let signal_delivered() do SIGTRAP on singlestepping into handler
      ...

commit 7076aada1040de4ed79a5977dbabdb5e5ea5e249
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Sep 10 16:44:54 2012 -0400

    x86: split ret_from_fork
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 0a980c9d7cb8..937f2af6f2d4 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -146,29 +146,18 @@ static inline u32 read_32bit_tls(struct task_struct *t, int tls)
 }
 
 int copy_thread(unsigned long clone_flags, unsigned long sp,
-		unsigned long unused,
+		unsigned long arg,
 	struct task_struct *p, struct pt_regs *regs)
 {
 	int err;
 	struct pt_regs *childregs;
 	struct task_struct *me = current;
 
-	childregs = ((struct pt_regs *)
-			(THREAD_SIZE + task_stack_page(p))) - 1;
-	*childregs = *regs;
-
-	childregs->ax = 0;
-	if (user_mode(regs))
-		childregs->sp = sp;
-	else
-		childregs->sp = (unsigned long)childregs;
-
+	p->thread.sp0 = (unsigned long)task_stack_page(p) + THREAD_SIZE;
+	childregs = task_pt_regs(p);
 	p->thread.sp = (unsigned long) childregs;
-	p->thread.sp0 = (unsigned long) (childregs+1);
 	p->thread.usersp = me->thread.usersp;
-
 	set_tsk_thread_flag(p, TIF_FORK);
-
 	p->fpu_counter = 0;
 	p->thread.io_bitmap_ptr = NULL;
 
@@ -178,6 +167,24 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 	p->thread.fs = p->thread.fsindex ? 0 : me->thread.fs;
 	savesegment(es, p->thread.es);
 	savesegment(ds, p->thread.ds);
+	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));
+
+	if (unlikely(!regs)) {
+		/* kernel thread */
+		memset(childregs, 0, sizeof(struct pt_regs));
+		childregs->sp = (unsigned long)childregs;
+		childregs->ss = __KERNEL_DS;
+		childregs->bx = sp; /* function */
+		childregs->bp = arg;
+		childregs->orig_ax = -1;
+		childregs->cs = __KERNEL_CS | get_kernel_rpl();
+		childregs->flags = X86_EFLAGS_IF | X86_EFLAGS_BIT1;
+		return 0;
+	}
+	*childregs = *regs;
+
+	childregs->ax = 0;
+	childregs->sp = sp;
 
 	err = -ENOMEM;
 	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));

commit 304bceda6a18ae0b0240b8aac9a6bdf8ce2d2469
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Aug 24 14:13:02 2012 -0700

    x86, fpu: use non-lazy fpu restore for processors supporting xsave
    
    Fundamental model of the current Linux kernel is to lazily init and
    restore FPU instead of restoring the task state during context switch.
    This changes that fundamental lazy model to the non-lazy model for
    the processors supporting xsave feature.
    
    Reasons driving this model change are:
    
    i. Newer processors support optimized state save/restore using xsaveopt and
    xrstor by tracking the INIT state and MODIFIED state during context-switch.
    This is faster than modifying the cr0.TS bit which has serializing semantics.
    
    ii. Newer glibc versions use SSE for some of the optimized copy/clear routines.
    With certain workloads (like boot, kernel-compilation etc), application
    completes its work with in the first 5 task switches, thus taking upto 5 #DNA
    traps with the kernel not getting a chance to apply the above mentioned
    pre-load heuristic.
    
    iii. Some xstate features (like AMD's LWP feature) don't honor the cr0.TS bit
    and thus will not work correctly in the presence of lazy restore. Non-lazy
    state restore is needed for enabling such features.
    
    Some data on a two socket SNB system:
     * Saved 20K DNA exceptions during boot on a two socket SNB system.
     * Saved 50K DNA exceptions during kernel-compilation workload.
     * Improved throughput of the AVX based checksumming function inside the
       kernel by ~15% as xsave/xrstor is faster than the serializing clts/stts
       pair.
    
    Also now kernel_fpu_begin/end() relies on the patched
    alternative instructions. So move check_fpu() which uses the
    kernel_fpu_begin/end() after alternative_instructions().
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1345842782-24175-7-git-send-email-suresh.b.siddha@intel.com
    Merge 32-bit boot fix from,
    Link: http://lkml.kernel.org/r/1347300665-6209-4-git-send-email-suresh.b.siddha@intel.com
    Cc: Jim Kukunas <james.t.kukunas@linux.intel.com>
    Cc: NeilBrown <neilb@suse.de>
    Cc: Avi Kivity <avi@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 0a980c9d7cb8..8a6d20ce1978 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -232,10 +232,6 @@ start_thread_common(struct pt_regs *regs, unsigned long new_ip,
 	regs->cs		= _cs;
 	regs->ss		= _ss;
 	regs->flags		= X86_EFLAGS_IF;
-	/*
-	 * Free the old FP and other extended state
-	 */
-	free_thread_xstate(current);
 }
 
 void

commit 3fad0953a12f92289f1e35f091c4fa09d8e1884e
Merge: a065de0d2577 0fa0e2f02e8e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 22 12:04:44 2012 -0700

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull debug-for-linus git tree from Ingo Molnar.
    
    Fix up trivial conflict in arch/x86/kernel/cpu/perf_event_intel.c due to
    a printk() having changed to a pr_info() differently in the two branches.
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Move call to print_modules() out of show_regs()
      x86/mm: Mark free_initrd_mem() as __init
      x86/microcode: Mark microcode_id[] as __initconst
      x86/nmi: Clean up register_nmi_handler() usage
      x86: Save cr2 in NMI in case NMIs take a page fault (for i386)
      x86: Remove cmpxchg from i386 NMI nesting code
      x86: Save cr2 in NMI in case NMIs take a page fault
      x86/debug: Add KERN_<LEVEL> to bare printks, convert printks to pr_<level>

commit 715c85b1fc824e9cd0ea07d6ceb80d2262f32e90
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Jun 7 13:32:04 2012 -0700

    x86, cpu: Rename checking_wrmsrl() to wrmsrl_safe()
    
    Rename checking_wrmsrl() to wrmsrl_safe(), to match the naming
    convention used by all the other MSR access functions/macros.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 61cdf7fdf099..3e215ba68766 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -466,7 +466,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 			task->thread.gs = addr;
 			if (doit) {
 				load_gs_index(0);
-				ret = checking_wrmsrl(MSR_KERNEL_GS_BASE, addr);
+				ret = wrmsrl_safe(MSR_KERNEL_GS_BASE, addr);
 			}
 		}
 		put_cpu();
@@ -494,7 +494,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 				/* set the selector to 0 to not confuse
 				   __switch_to */
 				loadsegment(fs, 0);
-				ret = checking_wrmsrl(MSR_FS_BASE, addr);
+				ret = wrmsrl_safe(MSR_FS_BASE, addr);
 			}
 		}
 		put_cpu();

commit c767a54ba0657e52e6edaa97cbe0b0a8bf1c1655
Author: Joe Perches <joe@perches.com>
Date:   Mon May 21 19:50:07 2012 -0700

    x86/debug: Add KERN_<LEVEL> to bare printks, convert printks to pr_<level>
    
    Use a more current logging style:
    
     - Bare printks should have a KERN_<LEVEL> for consistency's sake
     - Add pr_fmt where appropriate
     - Neaten some macro definitions
     - Convert some Ok output to OK
     - Use "%s: ", __func__ in pr_fmt for summit
     - Convert some printks to pr_<level>
    
    Message output is not identical in all cases.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: levinsasha928@gmail.com
    Link: http://lkml.kernel.org/r/1337655007.24226.10.camel@joe2Laptop
    [ merged two similar patches, tidied up the changelog ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 61cdf7fdf099..85151f3e36e2 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -117,10 +117,10 @@ void release_thread(struct task_struct *dead_task)
 {
 	if (dead_task->mm) {
 		if (dead_task->mm->context.size) {
-			printk("WARNING: dead process %8s still has LDT? <%p/%d>\n",
-					dead_task->comm,
-					dead_task->mm->context.ldt,
-					dead_task->mm->context.size);
+			pr_warn("WARNING: dead process %8s still has LDT? <%p/%d>\n",
+				dead_task->comm,
+				dead_task->mm->context.ldt,
+				dead_task->mm->context.size);
 			BUG();
 		}
 	}

commit ec0d7f18ab7b5097d7c0c8f3d909ca1031b9d5cd
Merge: 269af9a1a08d 1dcc8d7ba235
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 23 10:59:07 2012 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull fpu state cleanups from Ingo Molnar:
     "This tree streamlines further aspects of FPU handling by eliminating
      the prepare_to_copy() complication and moving that logic to
      arch_dup_task_struct().
    
      It also fixes the FPU dumps in threaded core dumps, removes and old
      (and now invalid) assumption plus micro-optimizes the exit path by
      avoiding an FPU save for dead tasks."
    
    Fixed up trivial add-add conflict in arch/sh/kernel/process.c that came
    in because we now do the FPU handling in arch_dup_task_struct() rather
    than the legacy (and now gone) prepare_to_copy().
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, fpu: drop the fpu state during thread exit
      x86, xsave: remove thread_has_fpu() bug check in __sanitize_i387_state()
      coredump: ensure the fpu state is flushed for proper multi-threaded core dump
      fork: move the real prepare_to_copy() users to arch_dup_task_struct()

commit 55ccf3fe3f9a3441731aa79cf42a628fc4ecace9
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed May 16 15:03:51 2012 -0700

    fork: move the real prepare_to_copy() users to arch_dup_task_struct()
    
    Historical prepare_to_copy() is mostly a no-op, duplicated for majority of
    the architectures and the rest following the x86 model of flushing the extended
    register state like fpu there.
    
    Remove it and use the arch_dup_task_struct() instead.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1336692811-30576-1-git-send-email-suresh.b.siddha@intel.com
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chen Liqin <liqin.chen@sunplusct.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 43d8b48b23e6..c4c0645a4011 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -145,15 +145,6 @@ static inline u32 read_32bit_tls(struct task_struct *t, int tls)
 	return get_desc_base(&t->thread.tls_array[tls]);
 }
 
-/*
- * This gets called before we allocate a new thread and copy
- * the current task into it.
- */
-void prepare_to_copy(struct task_struct *tsk)
-{
-	unlazy_fpu(tsk);
-}
-
 int copy_thread(unsigned long clone_flags, unsigned long sp,
 		unsigned long unused,
 	struct task_struct *p, struct pt_regs *regs)

commit c6ae41e7d469f00d9c92a2b2887c7235d121c009
Author: Alex Shi <alex.shi@intel.com>
Date:   Fri May 11 15:35:27 2012 +0800

    x86: replace percpu_xxx funcs with this_cpu_xxx
    
    Since percpu_xxx() serial functions are duplicated with this_cpu_xxx().
    Removing percpu_xxx() definition and replacing them by this_cpu_xxx()
    in code. There is no function change in this patch, just preparation for
    later percpu_xxx serial function removing.
    
    On x86 machine the this_cpu_xxx() serial functions are same as
    __this_cpu_xxx() without no unnecessary premmpt enable/disable.
    
    Thanks for Stephen Rothwell, he found and fixed a i386 build error in
    the patch.
    
    Also thanks for Andrew Morton, he kept updating the patchset in Linus'
    tree.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Acked-by: Christoph Lameter <cl@gentwo.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 43d8b48b23e6..28e810255a0a 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -237,7 +237,7 @@ start_thread_common(struct pt_regs *regs, unsigned long new_ip,
 	current->thread.usersp	= new_sp;
 	regs->ip		= new_ip;
 	regs->sp		= new_sp;
-	percpu_write(old_rsp, new_sp);
+	this_cpu_write(old_rsp, new_sp);
 	regs->cs		= _cs;
 	regs->ss		= _ss;
 	regs->flags		= X86_EFLAGS_IF;
@@ -359,11 +359,11 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/*
 	 * Switch the PDA and FPU contexts.
 	 */
-	prev->usersp = percpu_read(old_rsp);
-	percpu_write(old_rsp, next->usersp);
-	percpu_write(current_task, next_p);
+	prev->usersp = this_cpu_read(old_rsp);
+	this_cpu_write(old_rsp, next->usersp);
+	this_cpu_write(current_task, next_p);
 
-	percpu_write(kernel_stack,
+	this_cpu_write(kernel_stack,
 		  (unsigned long)task_stack_page(next_p) +
 		  THREAD_SIZE - KERNEL_STACK_OFFSET);
 

commit febb72a6e4cc6c8cffcc1ea649a3fb364f1ea432
Author: Larry Finger <Larry.Finger@lwfinger.net>
Date:   Sun May 6 19:40:03 2012 -0500

    IA32 emulation: Fix build problem for modular ia32 a.out support
    
    Commit ce7e5d2d19bc ("x86: fix broken TASK_SIZE for ia32_aout") breaks
    kernel builds when "CONFIG_IA32_AOUT=m" with
    
      ERROR: "set_personality_ia32" [arch/x86/ia32/ia32_aout.ko] undefined!
      make[1]: *** [__modpost] Error 1
    
    The entry point needs to be exported.
    
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Acked-by: Al Viro <viro@zeniv.linux.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 733ca39f367e..43d8b48b23e6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -423,6 +423,7 @@ void set_personality_ia32(bool x32)
 		current_thread_info()->status |= TS_COMPAT;
 	}
 }
+EXPORT_SYMBOL_GPL(set_personality_ia32);
 
 unsigned long get_wchan(struct task_struct *p)
 {

commit a591afc01d9e48affbacb365558a31e53c85af45
Merge: 820d41cf0cd0 31796ac4e8f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 18:12:23 2012 -0700

    Merge branch 'x86-x32-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x32 support for x86-64 from Ingo Molnar:
     "This tree introduces the X32 binary format and execution mode for x86:
      32-bit data space binaries using 64-bit instructions and 64-bit kernel
      syscalls.
    
      This allows applications whose working set fits into a 32 bits address
      space to make use of 64-bit instructions while using a 32-bit address
      space with shorter pointers, more compressed data structures, etc."
    
    Fix up trivial context conflicts in arch/x86/{Kconfig,vdso/vma.c}
    
    * 'x86-x32-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (71 commits)
      x32: Fix alignment fail in struct compat_siginfo
      x32: Fix stupid ia32/x32 inversion in the siginfo format
      x32: Add ptrace for x32
      x32: Switch to a 64-bit clock_t
      x32: Provide separate is_ia32_task() and is_x32_task() predicates
      x86, mtrr: Use explicit sizing and padding for the 64-bit ioctls
      x86/x32: Fix the binutils auto-detect
      x32: Warn and disable rather than error if binutils too old
      x32: Only clear TIF_X32 flag once
      x32: Make sure TS_COMPAT is cleared for x32 tasks
      fs: Remove missed ->fds_bits from cessation use of fd_set structs internally
      fs: Fix close_on_exec pointer in alloc_fdtable
      x32: Drop non-__vdso weak symbols from the x32 VDSO
      x32: Fix coding style violations in the x32 VDSO code
      x32: Add x32 VDSO support
      x32: Allow x32 to be configured
      x32: If configured, add x32 system calls to system call tables
      x32: Handle process creation
      x32: Signal-related system calls
      x86: Add #ifdef CONFIG_COMPAT to <asm/sys_ia32.h>
      ...

commit 6b8212a313dae341ef3a2e413dfec5c4dea59617
Merge: bcd550745fc5 8abc3122aa02
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 14:28:26 2012 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 updates from Ingo Molnar.
    
    This touches some non-x86 files due to the sanitized INLINE_SPIN_UNLOCK
    config usage.
    
    Fixed up trivial conflicts due to just header include changes (removing
    headers due to cpu_idle() merge clashing with the <asm/system.h> split).
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/apic/amd: Be more verbose about LVT offset assignments
      x86, tls: Off by one limit check
      x86/ioapic: Add io_apic_ops driver layer to allow interception
      x86/olpc: Add debugfs interface for EC commands
      x86: Merge the x86_32 and x86_64 cpu_idle() functions
      x86/kconfig: Remove CONFIG_TR=y from the defconfigs
      x86: Stop recursive fault in print_context_stack after stack overflow
      x86/io_apic: Move and reenable irq only when CONFIG_GENERIC_PENDING_IRQ=y
      x86/apic: Add separate apic_id_valid() functions for selected apic drivers
      locking/kconfig: Simplify INLINE_SPIN_UNLOCK usage
      x86/kconfig: Update defconfigs
      x86: Fix excessive MSR print out when show_msr is not specified

commit f05e798ad4c09255f590f5b2c00a7ca6c172f983
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:11:12 2012 +0100

    Disintegrate asm/system.h for X86
    
    Disintegrate asm/system.h for X86.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    cc: x86@kernel.org

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 292da13fc5aa..61270e8d428a 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -40,7 +40,6 @@
 #include <linux/cpuidle.h>
 
 #include <asm/pgtable.h>
-#include <asm/system.h>
 #include <asm/processor.h>
 #include <asm/i387.h>
 #include <asm/fpu-internal.h>
@@ -53,6 +52,7 @@
 #include <asm/syscalls.h>
 #include <asm/debugreg.h>
 #include <asm/nmi.h>
+#include <asm/switch_to.h>
 
 asmlinkage extern void ret_from_fork(void);
 

commit 90e240142bd31ff10aeda5a280a53153f4eff004
Author: Richard Weinberger <richard@nod.at>
Date:   Sun Mar 25 23:00:04 2012 +0200

    x86: Merge the x86_32 and x86_64 cpu_idle() functions
    
    Both functions are mostly identical.
    The differences are:
    
    - x86_32's cpu_idle() makes use of check_pgt_cache(), which is a
      nop on both x86_32 and x86_64.
    
    - x86_64's cpu_idle() uses enter/__exit_idle/(), on x86_32 these
      function are a nop.
    
    - In contrast to x86_32, x86_64 calls rcu_idle_enter/exit() in
      the innermost loop because idle notifications need RCU.
      Calling these function on x86_32 also in the innermost loop
      does not hurt.
    
    So we can merge both functions.
    
    Signed-off-by: Richard Weinberger <richard@nod.at>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: paulmck@linux.vnet.ibm.com
    Cc: josh@joshtriplett.org
    Cc: tj@kernel.org
    Link: http://lkml.kernel.org/r/1332709204-22496-1-git-send-email-richard@nod.at
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 292da13fc5aa..ce5e34f2beca 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -14,7 +14,6 @@
  * This file handles the architecture-dependent parts of process handling..
  */
 
-#include <linux/stackprotector.h>
 #include <linux/cpu.h>
 #include <linux/errno.h>
 #include <linux/sched.h>
@@ -32,12 +31,10 @@
 #include <linux/notifier.h>
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
-#include <linux/tick.h>
 #include <linux/prctl.h>
 #include <linux/uaccess.h>
 #include <linux/io.h>
 #include <linux/ftrace.h>
-#include <linux/cpuidle.h>
 
 #include <asm/pgtable.h>
 #include <asm/system.h>
@@ -52,114 +49,10 @@
 #include <asm/idle.h>
 #include <asm/syscalls.h>
 #include <asm/debugreg.h>
-#include <asm/nmi.h>
 
 asmlinkage extern void ret_from_fork(void);
 
 DEFINE_PER_CPU(unsigned long, old_rsp);
-static DEFINE_PER_CPU(unsigned char, is_idle);
-
-static ATOMIC_NOTIFIER_HEAD(idle_notifier);
-
-void idle_notifier_register(struct notifier_block *n)
-{
-	atomic_notifier_chain_register(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_register);
-
-void idle_notifier_unregister(struct notifier_block *n)
-{
-	atomic_notifier_chain_unregister(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_unregister);
-
-void enter_idle(void)
-{
-	percpu_write(is_idle, 1);
-	atomic_notifier_call_chain(&idle_notifier, IDLE_START, NULL);
-}
-
-static void __exit_idle(void)
-{
-	if (x86_test_and_clear_bit_percpu(0, is_idle) == 0)
-		return;
-	atomic_notifier_call_chain(&idle_notifier, IDLE_END, NULL);
-}
-
-/* Called from interrupts to signify idle end */
-void exit_idle(void)
-{
-	/* idle loop has pid 0 */
-	if (current->pid)
-		return;
-	__exit_idle();
-}
-
-#ifndef CONFIG_SMP
-static inline void play_dead(void)
-{
-	BUG();
-}
-#endif
-
-/*
- * The idle thread. There's no useful work to be
- * done, so just try to conserve power and have a
- * low exit latency (ie sit in a loop waiting for
- * somebody to say that they'd like to reschedule)
- */
-void cpu_idle(void)
-{
-	current_thread_info()->status |= TS_POLLING;
-
-	/*
-	 * If we're the non-boot CPU, nothing set the stack canary up
-	 * for us.  CPU0 already has it initialized but no harm in
-	 * doing it again.  This is a good place for updating it, as
-	 * we wont ever return from this function (so the invalid
-	 * canaries already on the stack wont ever trigger).
-	 */
-	boot_init_stack_canary();
-
-	/* endless idle loop with no priority at all */
-	while (1) {
-		tick_nohz_idle_enter();
-		while (!need_resched()) {
-
-			rmb();
-
-			if (cpu_is_offline(smp_processor_id()))
-				play_dead();
-			/*
-			 * Idle routines should keep interrupts disabled
-			 * from here on, until they go to idle.
-			 * Otherwise, idle callbacks can misfire.
-			 */
-			local_touch_nmi();
-			local_irq_disable();
-			enter_idle();
-			/* Don't trace irqs off for idle */
-			stop_critical_timings();
-
-			/* enter_idle() needs rcu for notifiers */
-			rcu_idle_enter();
-
-			if (cpuidle_idle_call())
-				pm_idle();
-
-			rcu_idle_exit();
-			start_critical_timings();
-
-			/* In many cases the interrupt that ended idle
-			   has already called exit_idle. But some idle
-			   loops can be woken up without interrupt. */
-			__exit_idle();
-		}
-
-		tick_nohz_idle_exit();
-		schedule_preempt_disabled();
-	}
-}
 
 /* Prints also some state that isn't saved in the pt_regs */
 void __show_regs(struct pt_regs *regs, int all)

commit 35cb8d9e18c0bb33b90d7e574abadbe23b65427d
Merge: 02c502566ef5 1361b83a13d4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 09:41:22 2012 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/fpu changes from Ingo Molnar.
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      i387: Split up <asm/i387.h> into exported and internal interfaces
      i387: Uninline the generic FP helpers that we expose to kernel modules

commit c5c7fb8fbd7cd228132b6e2a17a10f246ffc06ee
Merge: 1b674bf106f5 140f190bc3a3 35f1790e6c6a 513c4ec6e475 42dfc43ee599 b0deca2e0270
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 09:28:15 2012 -0700

    Merge branches 'x86-cpu-for-linus', 'x86-boot-for-linus', 'x86-cpufeature-for-linus', 'x86-process-for-linus' and 'x86-uv-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull trivial x86 branches from Ingo Molnar: small one-liners to fix up
    details.
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Remove some noise from boot log when starting cpus
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, boot: Fix port argument to inl() function
    
    * 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, cpufeature: Add CPU features from Intel document 319433-012A
    
    * 'x86-process-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86_64: Record stack pointer before task execution begins
    
    * 'x86-uv-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/UV: Lower UV rtc clocksource rating

commit bd2f55361f18347e890d52ff9cfd8895455ec11b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 21 12:33:18 2011 +0100

    sched/rt: Use schedule_preempt_disabled()
    
    Coccinelle based conversion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-24swm5zut3h9c4a6s46x8rws@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index cfa5c90c01db..e34257c70c28 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -156,9 +156,7 @@ void cpu_idle(void)
 		}
 
 		tick_nohz_idle_exit();
-		preempt_enable_no_resched();
-		schedule();
-		preempt_disable();
+		schedule_preempt_disabled();
 	}
 }
 

commit 42dfc43ee5999ac64284476ea0ac6c937587cf2b
Author: Siddhesh Poyarekar <siddhesh.poyarekar@gmail.com>
Date:   Sun Feb 26 21:47:55 2012 +0530

    x86_64: Record stack pointer before task execution begins
    
    task->thread.usersp is unusable immediately after a binary is exec()'d
    until it undergoes a context switch cycle. The start_thread() function
    called during execve() saves the stack pointer into pt_regs and into
    old_rsp, but fails to record it into task->thread.usersp.
    
    Because of this, KSTK_ESP(task) returns an incorrect value for a
    64-bit program until the task is switched out and back in since
    switch_to swaps %rsp values in and out into task->thread.usersp.
    
    Signed-off-by: Siddhesh Poyarekar <siddhesh.poyarekar@gmail.com>
    Link: http://lkml.kernel.org/r/1330273075-2949-1-git-send-email-siddhesh.poyarekar@gmail.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 1fd94bc4279d..eb54dd0fbed6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -341,6 +341,7 @@ start_thread_common(struct pt_regs *regs, unsigned long new_ip,
 	loadsegment(es, _ds);
 	loadsegment(ds, _ds);
 	load_gs_index(0);
+	current->thread.usersp	= new_sp;
 	regs->ip		= new_ip;
 	regs->sp		= new_sp;
 	percpu_write(old_rsp, new_sp);

commit 00194b2e845da29395ad00c13a884d9acb9306b5
Author: Bobby Powers <bobbypowers@gmail.com>
Date:   Sat Feb 25 22:59:34 2012 -0500

    x32: Only clear TIF_X32 flag once
    
    Commits bb212724 and d1a797f3 both added a call to
    clear_thread_flag(TIF_X32) under set_personality_64bit() - only one is
    needed.
    
    Signed-off-by: Bobby Powers <bobbypowers@gmail.com>
    Link: http://lkml.kernel.org/r/1330228774-24223-1-git-send-email-bobbypowers@gmail.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 32e04120b2cd..a4659739e202 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -510,7 +510,6 @@ void set_personality_64bit(void)
 
 	/* Make sure to be in 64bit mode */
 	clear_thread_flag(TIF_IA32);
-	clear_thread_flag(TIF_X32);
 	clear_thread_flag(TIF_ADDR32);
 	clear_thread_flag(TIF_X32);
 

commit ce5f7a99df87918b5be4618a9386213a8e9a7146
Author: Bobby Powers <bobbypowers@gmail.com>
Date:   Sat Feb 25 23:25:38 2012 -0500

    x32: Make sure TS_COMPAT is cleared for x32 tasks
    
    If a process has a non-x32 ia32 personality and changes to x32, the
    process would keep its TS_COMPAT flag. x32 uses the presence of the
    x32 flag on a syscall to determine compat status, so make sure
    TS_COMPAT is cleared.
    
    Signed-off-by: Bobby Powers <bobbypowers@gmail.com>
    Link: http://lkml.kernel.org/r/1330230338-25077-1-git-send-email-bobbypowers@gmail.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index a0701da2bd18..32e04120b2cd 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -540,6 +540,9 @@ void set_personality_ia32(bool x32)
 		clear_thread_flag(TIF_IA32);
 		set_thread_flag(TIF_X32);
 		current->personality &= ~READ_IMPLIES_EXEC;
+		/* is_compat_task() uses the presence of the x32
+		   syscall bit flag to determine compat status */
+		current_thread_info()->status &= ~TS_COMPAT;
 	} else {
 		set_thread_flag(TIF_IA32);
 		clear_thread_flag(TIF_X32);

commit 1361b83a13d4d92e53fbb6c877528713e118b821
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 21 13:19:22 2012 -0800

    i387: Split up <asm/i387.h> into exported and internal interfaces
    
    While various modules include <asm/i387.h> to get access to things we
    actually *intend* for them to use, most of that header file was really
    pretty low-level internal stuff that we really don't want to expose to
    others.
    
    So split the header file into two: the small exported interfaces remain
    in <asm/i387.h>, while the internal definitions that are only used by
    core architecture code are now in <asm/fpu-internal.h>.
    
    The guiding principle for this was to expose functions that we export to
    modules, and leave them in <asm/i387.h>, while stuff that is used by
    task switching or was marked GPL-only is in <asm/fpu-internal.h>.
    
    The fpu-internal.h file could be further split up too, especially since
    arch/x86/kvm/ uses some of the remaining stuff for its module.  But that
    kvm usage should probably be abstracted out a bit, and at least now the
    internal FPU accessor functions are much more contained.  Even if it
    isn't perhaps as contained as it _could_ be.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1202211340330.5354@i5.linux-foundation.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index cfa5c90c01db..5bad3c71e48f 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -43,6 +43,7 @@
 #include <asm/system.h>
 #include <asm/processor.h>
 #include <asm/i387.h>
+#include <asm/fpu-internal.h>
 #include <asm/mmu_context.h>
 #include <asm/prctl.h>
 #include <asm/desc.h>

commit d1a797f388d6d30fa502915d1b9937ed758b7137
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Sun Feb 19 10:06:34 2012 -0800

    x32: Handle process creation
    
    Allow an x32 process to be started.
    
    Originally-by: H. J. Lu <hjl.tools@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 5fe2fbaa56ba..a0701da2bd18 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -364,7 +364,9 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 void start_thread_ia32(struct pt_regs *regs, u32 new_ip, u32 new_sp)
 {
 	start_thread_common(regs, new_ip, new_sp,
-			    __USER32_CS, __USER32_DS, __USER32_DS);
+			    test_thread_flag(TIF_X32)
+			    ? __USER_CS : __USER32_CS,
+			    __USER_DS, __USER_DS);
 }
 #endif
 
@@ -508,6 +510,7 @@ void set_personality_64bit(void)
 
 	/* Make sure to be in 64bit mode */
 	clear_thread_flag(TIF_IA32);
+	clear_thread_flag(TIF_X32);
 	clear_thread_flag(TIF_ADDR32);
 	clear_thread_flag(TIF_X32);
 
@@ -522,22 +525,28 @@ void set_personality_64bit(void)
 	current->personality &= ~READ_IMPLIES_EXEC;
 }
 
-void set_personality_ia32(void)
+void set_personality_ia32(bool x32)
 {
 	/* inherit personality from parent */
 
 	/* Make sure to be in 32bit mode */
-	set_thread_flag(TIF_IA32);
 	set_thread_flag(TIF_ADDR32);
-	clear_thread_flag(TIF_X32);
-	current->personality |= force_personality32;
 
 	/* Mark the associated mm as containing 32-bit tasks. */
 	if (current->mm)
 		current->mm->context.ia32_compat = 1;
 
-	/* Prepare the first "return" to user space */
-	current_thread_info()->status |= TS_COMPAT;
+	if (x32) {
+		clear_thread_flag(TIF_IA32);
+		set_thread_flag(TIF_X32);
+		current->personality &= ~READ_IMPLIES_EXEC;
+	} else {
+		set_thread_flag(TIF_IA32);
+		clear_thread_flag(TIF_X32);
+		current->personality |= force_personality32;
+		/* Prepare the first "return" to user space */
+		current_thread_info()->status |= TS_COMPAT;
+	}
 }
 
 unsigned long get_wchan(struct task_struct *p)

commit bb2127240c5595ae4ef7115494f51e973692f64e
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Tue Feb 14 13:56:49 2012 -0800

    x32: Add a thread flag for x32 processes
    
    An x32 process is *almost* the same thing as a 64-bit process with a
    32-bit address limit, but there are a few minor differences -- in
    particular core dumps are 32 bits and signal handling is different.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 0e900d09e232..5fe2fbaa56ba 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -509,6 +509,7 @@ void set_personality_64bit(void)
 	/* Make sure to be in 64bit mode */
 	clear_thread_flag(TIF_IA32);
 	clear_thread_flag(TIF_ADDR32);
+	clear_thread_flag(TIF_X32);
 
 	/* Ensure the corresponding mm is not marked. */
 	if (current->mm)
@@ -528,6 +529,7 @@ void set_personality_ia32(void)
 	/* Make sure to be in 32bit mode */
 	set_thread_flag(TIF_IA32);
 	set_thread_flag(TIF_ADDR32);
+	clear_thread_flag(TIF_X32);
 	current->personality |= force_personality32;
 
 	/* Mark the associated mm as containing 32-bit tasks. */

commit 6bd330083e0e97b7ddc053459190bf3d5768ca83
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Feb 6 13:03:09 2012 -0800

    x86: Factor out TIF_IA32 from 32-bit address space
    
    Factor out IA32 (compatibility instruction set) from 32-bit address
    space in the thread_info flags; this is a precondition patch for x32
    support.
    
    Originally-by: H. J. Lu <hjl.tools@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/n/tip-4pr1xnnksprt7t0h3w5fw4rv@git.kernel.org

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 9b9fe4a85c87..0e900d09e232 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -508,6 +508,7 @@ void set_personality_64bit(void)
 
 	/* Make sure to be in 64bit mode */
 	clear_thread_flag(TIF_IA32);
+	clear_thread_flag(TIF_ADDR32);
 
 	/* Ensure the corresponding mm is not marked. */
 	if (current->mm)
@@ -526,6 +527,7 @@ void set_personality_ia32(void)
 
 	/* Make sure to be in 32bit mode */
 	set_thread_flag(TIF_IA32);
+	set_thread_flag(TIF_ADDR32);
 	current->personality |= force_personality32;
 
 	/* Mark the associated mm as containing 32-bit tasks. */

commit 7e16838d94b566a17b65231073d179bc04d590c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 19 13:27:00 2012 -0800

    i387: support lazy restore of FPU state
    
    This makes us recognize when we try to restore FPU state that matches
    what we already have in the FPU on this CPU, and avoids the restore
    entirely if so.
    
    To do this, we add two new data fields:
    
     - a percpu 'fpu_owner_task' variable that gets written any time we
       update the "has_fpu" field, and thus acts as a kind of back-pointer
       to the task that owns the CPU.  The exception is when we save the FPU
       state as part of a context switch - if the save can keep the FPU
       state around, we leave the 'fpu_owner_task' variable pointing at the
       task whose FP state still remains on the CPU.
    
     - a per-thread 'last_cpu' field, that indicates which CPU that thread
       used its FPU on last.  We update this on every context switch
       (writing an invalid CPU number if the last context switch didn't
       leave the FPU in a lazily usable state), so we know that *that*
       thread has done nothing else with the FPU since.
    
    These two fields together can be used when next switching back to the
    task to see if the CPU still matches: if 'fpu_owner_task' matches the
    task we are switching to, we know that no other task (or kernel FPU
    usage) touched the FPU on this CPU in the meantime, and if the current
    CPU number matches the 'last_cpu' field, we know that this thread did no
    other FP work on any other CPU, so the FPU state on the CPU must match
    what was saved on last context switch.
    
    In that case, we can avoid the 'f[x]rstor' entirely, and just clear the
    CR0.TS bit.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 8ad880b3bc1c..cfa5c90c01db 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -389,7 +389,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	unsigned fsindex, gsindex;
 	fpu_switch_t fpu;
 
-	fpu = switch_fpu_prepare(prev_p, next_p);
+	fpu = switch_fpu_prepare(prev_p, next_p, cpu);
 
 	/*
 	 * Reload esp0, LDT and the page table pointer:

commit cea20ca3f3181fc36788a15bc65d1062b96a0a6c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 10:24:09 2012 -0800

    i387: fix up some fpu_counter confusion
    
    This makes sure we clear the FPU usage counter for newly created tasks,
    just so that we start off in a known state (for example, don't try to
    preload the FPU state on the first task switch etc).
    
    It also fixes a thinko in when we increment the fpu_counter at task
    switch time, introduced by commit 34ddc81a230b ("i387: re-introduce FPU
    state preloading at context switch time").  We should increment the
    *new* task fpu_counter, not the old task, and only if we decide to use
    that state (whether lazily or preloaded).
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 1fd94bc4279d..8ad880b3bc1c 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -286,6 +286,7 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 
 	set_tsk_thread_flag(p, TIF_FORK);
 
+	p->fpu_counter = 0;
 	p->thread.io_bitmap_ptr = NULL;
 
 	savesegment(gs, p->thread.gsindex);

commit 34ddc81a230b15c0e345b6b253049db731499f7e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 18 12:56:35 2012 -0800

    i387: re-introduce FPU state preloading at context switch time
    
    After all the FPU state cleanups and finally finding the problem that
    caused all our FPU save/restore problems, this re-introduces the
    preloading of FPU state that was removed in commit b3b0870ef3ff ("i387:
    do not preload FPU state at task switch time").
    
    However, instead of simply reverting the removal, this reimplements
    preloading with several fixes, most notably
    
     - properly abstracted as a true FPU state switch, rather than as
       open-coded save and restore with various hacks.
    
       In particular, implementing it as a proper FPU state switch allows us
       to optimize the CR0.TS flag accesses: there is no reason to set the
       TS bit only to then almost immediately clear it again.  CR0 accesses
       are quite slow and expensive, don't flip the bit back and forth for
       no good reason.
    
     - Make sure that the same model works for both x86-32 and x86-64, so
       that there are no gratuitous differences between the two due to the
       way they save and restore segment state differently due to
       architectural differences that really don't matter to the FPU state.
    
     - Avoid exposing the "preload" state to the context switch routines,
       and in particular allow the concept of lazy state restore: if nothing
       else has used the FPU in the meantime, and the process is still on
       the same CPU, we can avoid restoring state from memory entirely, just
       re-expose the state that is still in the FPU unit.
    
       That optimized lazy restore isn't actually implemented here, but the
       infrastructure is set up for it.  Of course, older CPU's that use
       'fnsave' to save the state cannot take advantage of this, since the
       state saving also trashes the state.
    
    In other words, there is now an actual _design_ to the FPU state saving,
    rather than just random historical baggage.  Hopefully it's easier to
    follow as a result.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 753e803f7197..1fd94bc4279d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -386,8 +386,9 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &per_cpu(init_tss, cpu);
 	unsigned fsindex, gsindex;
+	fpu_switch_t fpu;
 
-	__unlazy_fpu(prev_p);
+	fpu = switch_fpu_prepare(prev_p, next_p);
 
 	/*
 	 * Reload esp0, LDT and the page table pointer:
@@ -457,6 +458,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 		wrmsrl(MSR_KERNEL_GS_BASE, next->gs);
 	prev->gsindex = gsindex;
 
+	switch_fpu_finish(next_p, fpu);
+
 	/*
 	 * Switch the PDA and FPU contexts.
 	 */

commit 4903062b5485f0e2c286a23b44c9b59d9b017d53
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 16 19:11:15 2012 -0800

    i387: move AMD K7/K8 fpu fxsave/fxrstor workaround from save to restore
    
    The AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception is
    pending.  In order to not leak FIP state from one process to another, we
    need to do a floating point load after the fxsave of the old process,
    and before the fxrstor of the new FPU state.  That resets the state to
    the (uninteresting) kernel load, rather than some potentially sensitive
    user information.
    
    We used to do this directly after the FPU state save, but that is
    actually very inconvenient, since it
    
     (a) corrupts what is potentially perfectly good FPU state that we might
         want to lazy avoid restoring later and
    
     (b) on x86-64 it resulted in a very annoying ordering constraint, where
         "__unlazy_fpu()" in the task switch needs to be delayed until after
         the DS segment has been reloaded just to get the new DS value.
    
    Coupling it to the fxrstor instead of the fxsave automatically avoids
    both of these issues, and also ensures that we only do it when actually
    necessary (the FP state after a save may never actually get used).  It's
    simply a much more natural place for the leaked state cleanup.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 992b4e542bc3..753e803f7197 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -387,6 +387,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	struct tss_struct *tss = &per_cpu(init_tss, cpu);
 	unsigned fsindex, gsindex;
 
+	__unlazy_fpu(prev_p);
+
 	/*
 	 * Reload esp0, LDT and the page table pointer:
 	 */
@@ -415,9 +417,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 
 	load_TLS(next, cpu);
 
-	/* Must be after DS reload */
-	__unlazy_fpu(prev_p);
-
 	/*
 	 * Leave lazy mode, flushing any hypercalls made here.
 	 * This must be done before restoring TLS segments so

commit b3b0870ef3ffed72b92415423da864f440f57ad6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 16 15:45:23 2012 -0800

    i387: do not preload FPU state at task switch time
    
    Yes, taking the trap to re-load the FPU/MMX state is expensive, but so
    is spending several days looking for a bug in the state save/restore
    code.  And the preload code has some rather subtle interactions with
    both paravirtualization support and segment state restore, so it's not
    nearly as simple as it should be.
    
    Also, now that we no longer necessarily depend on a single bit (ie
    TS_USEDFPU) for keeping track of the state of the FPU, we migth be able
    to do better.  If we are really switching between two processes that
    keep touching the FP state, save/restore is inevitable, but in the case
    of having one process that does most of the FPU usage, we may actually
    be able to do much better than the preloading.
    
    In particular, we may be able to keep track of which CPU the process ran
    on last, and also per CPU keep track of which process' FP state that CPU
    has.  For modern CPU's that don't destroy the FPU contents on save time,
    that would allow us to do a lazy restore by just re-enabling the
    existing FPU state - with no restore cost at all!
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 9b9fe4a85c87..992b4e542bc3 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -386,18 +386,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &per_cpu(init_tss, cpu);
 	unsigned fsindex, gsindex;
-	bool preload_fpu;
-
-	/*
-	 * If the task has used fpu the last 5 timeslices, just do a full
-	 * restore of the math state immediately to avoid the trap; the
-	 * chances of needing FPU soon are obviously high now
-	 */
-	preload_fpu = tsk_used_math(next_p) && next_p->fpu_counter > 5;
-
-	/* we're going to use this soon, after a few expensive things */
-	if (preload_fpu)
-		prefetch(next->fpu.state);
 
 	/*
 	 * Reload esp0, LDT and the page table pointer:
@@ -430,10 +418,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/* Must be after DS reload */
 	__unlazy_fpu(prev_p);
 
-	/* Make sure cpu is ready for new context */
-	if (preload_fpu)
-		clts();
-
 	/*
 	 * Leave lazy mode, flushing any hypercalls made here.
 	 * This must be done before restoring TLS segments so
@@ -492,13 +476,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 		     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))
 		__switch_to_xtra(prev_p, next_p, tss);
 
-	/*
-	 * Preload the FPU context, now that we've determined that the
-	 * task is likely to be using it. 
-	 */
-	if (preload_fpu)
-		__math_state_restore();
-
 	return prev_p;
 }
 

commit cf3f33551b6d0acf8d21a53c9aa9cf8a0d73afa3
Merge: 69734b644bf1 1affc46cffad
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 6 14:00:12 2012 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Use "do { } while(0)" for empty lock_cmos()/unlock_cmos() macros
      x86: Use "do { } while(0)" for empty flush_tlb_fix_spurious_fault() macro
      x86, CPU: Drop superfluous get_cpu_cap() prototype
      arch/x86/mm/pageattr.c: Quiet sparse noise; local functions should be static
      arch/x86/kernel/ptrace.c: Quiet sparse noise
      x86: Use kmemdup() in copy_thread(), rather than duplicating its implementation
      x86: Replace the EVT_TO_HPET_DEV() macro with an inline function

commit e37e112de3ac64032df45c2db0dbe1e8f1af86b4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 7 18:22:08 2011 +0200

    x86: Enter rcu extended qs after idle notifier call
    
    The idle notifier, called by enter_idle(), enters into rcu read
    side critical section but at that time we already switched into
    the RCU-idle window (rcu_idle_enter() has been called). And it's
    illegal to use rcu_read_lock() in that state.
    
    This results in rcu reporting its bad mood:
    
    [    1.275635] WARNING: at include/linux/rcupdate.h:194 __atomic_notifier_call_chain+0xd2/0x110()
    [    1.275635] Hardware name: AMD690VM-FMH
    [    1.275635] Modules linked in:
    [    1.275635] Pid: 0, comm: swapper Not tainted 3.0.0-rc6+ #252
    [    1.275635] Call Trace:
    [    1.275635]  [<ffffffff81051c8a>] warn_slowpath_common+0x7a/0xb0
    [    1.275635]  [<ffffffff81051cd5>] warn_slowpath_null+0x15/0x20
    [    1.275635]  [<ffffffff817d6f22>] __atomic_notifier_call_chain+0xd2/0x110
    [    1.275635]  [<ffffffff817d6f71>] atomic_notifier_call_chain+0x11/0x20
    [    1.275635]  [<ffffffff810018a0>] enter_idle+0x20/0x30
    [    1.275635]  [<ffffffff81001995>] cpu_idle+0xa5/0x110
    [    1.275635]  [<ffffffff817a7465>] rest_init+0xe5/0x140
    [    1.275635]  [<ffffffff817a73c8>] ? rest_init+0x48/0x140
    [    1.275635]  [<ffffffff81cc5ca3>] start_kernel+0x3d1/0x3dc
    [    1.275635]  [<ffffffff81cc5321>] x86_64_start_reservations+0x131/0x135
    [    1.275635]  [<ffffffff81cc5412>] x86_64_start_kernel+0xed/0xf4
    [    1.275635] ---[ end trace a22d306b065d4a66 ]---
    
    Fix this by entering rcu extended quiescent state later, just before
    the CPU goes to sleep.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 18e8cf3581f6..64e926c89a6f 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -122,7 +122,7 @@ void cpu_idle(void)
 
 	/* endless idle loop with no priority at all */
 	while (1) {
-		tick_nohz_idle_enter_norcu();
+		tick_nohz_idle_enter();
 		while (!need_resched()) {
 
 			rmb();
@@ -139,8 +139,14 @@ void cpu_idle(void)
 			enter_idle();
 			/* Don't trace irqs off for idle */
 			stop_critical_timings();
+
+			/* enter_idle() needs rcu for notifiers */
+			rcu_idle_enter();
+
 			if (cpuidle_idle_call())
 				pm_idle();
+
+			rcu_idle_exit();
 			start_critical_timings();
 
 			/* In many cases the interrupt that ended idle
@@ -149,7 +155,7 @@ void cpu_idle(void)
 			__exit_idle();
 		}
 
-		tick_nohz_idle_exit_norcu();
+		tick_nohz_idle_exit();
 		preempt_enable_no_resched();
 		schedule();
 		preempt_disable();

commit 2bbb6817c0ac1b5f2a68d720f364f98eeb1ac4fd
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Oct 8 16:01:00 2011 +0200

    nohz: Allow rcu extended quiescent state handling seperately from tick stop
    
    It is assumed that rcu won't be used once we switch to tickless
    mode and until we restart the tick. However this is not always
    true, as in x86-64 where we dereference the idle notifiers after
    the tick is stopped.
    
    To prepare for fixing this, add two new APIs:
    tick_nohz_idle_enter_norcu() and tick_nohz_idle_exit_norcu().
    
    If no use of RCU is made in the idle loop between
    tick_nohz_enter_idle() and tick_nohz_exit_idle() calls, the arch
    must instead call the new *_norcu() version such that the arch doesn't
    need to call rcu_idle_enter() and rcu_idle_exit().
    
    Otherwise the arch must call tick_nohz_enter_idle() and
    tick_nohz_exit_idle() and also call explicitly:
    
    - rcu_idle_enter() after its last use of RCU before the CPU is put
    to sleep.
    - rcu_idle_exit() before the first use of RCU after the CPU is woken
    up.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Hans-Christian Egtvedt <hans-christian.egtvedt@atmel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b069e9d7875f..18e8cf3581f6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -122,7 +122,7 @@ void cpu_idle(void)
 
 	/* endless idle loop with no priority at all */
 	while (1) {
-		tick_nohz_idle_enter();
+		tick_nohz_idle_enter_norcu();
 		while (!need_resched()) {
 
 			rmb();
@@ -149,7 +149,7 @@ void cpu_idle(void)
 			__exit_idle();
 		}
 
-		tick_nohz_idle_exit();
+		tick_nohz_idle_exit_norcu();
 		preempt_enable_no_resched();
 		schedule();
 		preempt_disable();

commit 280f06774afedf849f0b34248ed6aff57d0f6908
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 7 18:22:06 2011 +0200

    nohz: Separate out irq exit and idle loop dyntick logic
    
    The tick_nohz_stop_sched_tick() function, which tries to delay
    the next timer tick as long as possible, can be called from two
    places:
    
    - From the idle loop to start the dytick idle mode
    - From interrupt exit if we have interrupted the dyntick
    idle mode, so that we reprogram the next tick event in
    case the irq changed some internal state that requires this
    action.
    
    There are only few minor differences between both that
    are handled by that function, driven by the ts->inidle
    cpu variable and the inidle parameter. The whole guarantees
    that we only update the dyntick mode on irq exit if we actually
    interrupted the dyntick idle mode, and that we enter in RCU extended
    quiescent state from idle loop entry only.
    
    Split this function into:
    
    - tick_nohz_idle_enter(), which sets ts->inidle to 1, enters
    dynticks idle mode unconditionally if it can, and enters into RCU
    extended quiescent state.
    
    - tick_nohz_irq_exit() which only updates the dynticks idle mode
    when ts->inidle is set (ie: if tick_nohz_idle_enter() has been called).
    
    To maintain symmetry, tick_nohz_restart_sched_tick() has been renamed
    into tick_nohz_idle_exit().
    
    This simplifies the code and micro-optimize the irq exit path (no need
    for local_irq_save there). This also prepares for the split between
    dynticks and rcu extended quiescent state logics. We'll need this split to
    further fix illegal uses of RCU in extended quiescent states in the idle
    loop.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Hans-Christian Egtvedt <hans-christian.egtvedt@atmel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 3bd7e6eebf31..b069e9d7875f 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -122,7 +122,7 @@ void cpu_idle(void)
 
 	/* endless idle loop with no priority at all */
 	while (1) {
-		tick_nohz_stop_sched_tick(1);
+		tick_nohz_idle_enter();
 		while (!need_resched()) {
 
 			rmb();
@@ -149,7 +149,7 @@ void cpu_idle(void)
 			__exit_idle();
 		}
 
-		tick_nohz_restart_sched_tick();
+		tick_nohz_idle_exit();
 		preempt_enable_no_resched();
 		schedule();
 		preempt_disable();

commit cced40229993bb63238299e48a22e4c8d1e13181
Author: Thomas Meyer <thomas@m3y3r.de>
Date:   Thu Nov 17 23:43:40 2011 +0100

    x86: Use kmemdup() in copy_thread(), rather than duplicating its implementation
    
    The semantic patch that makes this change is available
    in scripts/coccinelle/api/memdup.cocci.
    
    Signed-off-by: Thomas Meyer <thomas@m3y3r.de>
    Link: http://lkml.kernel.org/r/1321569820.1624.275.camel@localhost.localdomain
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 3bd7e6eebf31..d2c1f6208d62 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -293,13 +293,12 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));
 
 	if (unlikely(test_tsk_thread_flag(me, TIF_IO_BITMAP))) {
-		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
+		p->thread.io_bitmap_ptr = kmemdup(me->thread.io_bitmap_ptr,
+						  IO_BITMAP_BYTES, GFP_KERNEL);
 		if (!p->thread.io_bitmap_ptr) {
 			p->thread.io_bitmap_max = 0;
 			return -ENOMEM;
 		}
-		memcpy(p->thread.io_bitmap_ptr, me->thread.io_bitmap_ptr,
-				IO_BITMAP_BYTES);
 		set_tsk_thread_flag(p, TIF_IO_BITMAP);
 	}
 

commit b227e23399dc59977aa42c49bd668bdab7a61812
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri Sep 30 15:06:22 2011 -0400

    x86, nmi: Add in logic to handle multiple events and unknown NMIs
    
    Previous patches allow the NMI subsystem to process multipe NMI events
    in one NMI.  As previously discussed this can cause issues when an event
    triggered another NMI but is processed in the current NMI.  This causes the
    next NMI to go unprocessed and become an 'unknown' NMI.
    
    To handle this, we first have to flag whether or not the NMI handler handled
    more than one event or not.  If it did, then there exists a chance that
    the next NMI might be already processed.  Once the NMI is flagged as a
    candidate to be swallowed, we next look for a back-to-back NMI condition.
    
    This is determined by looking at the %rip from pt_regs.  If it is the same
    as the previous NMI, it is assumed the cpu did not have a chance to jump
    back into a non-NMI context and execute code and instead handled another NMI.
    
    If both of those conditions are true then we will swallow any unknown NMI.
    
    There still exists a chance that we accidentally swallow a real unknown NMI,
    but for now things seem better.
    
    An optimization has also been added to the nmi notifier rountine.  Because x86
    can latch up to one NMI while currently processing an NMI, we don't have to
    worry about executing _all_ the handlers in a standalone NMI.  The idea is
    if multiple NMIs come in, the second NMI will represent them.  For those
    back-to-back NMI cases, we have the potentail to drop NMIs.  Therefore only
    execute all the handlers in the second half of a detected back-to-back NMI.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1317409584-23662-5-git-send-email-dzickus@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index f693e44e1bf6..3bd7e6eebf31 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -51,6 +51,7 @@
 #include <asm/idle.h>
 #include <asm/syscalls.h>
 #include <asm/debugreg.h>
+#include <asm/nmi.h>
 
 asmlinkage extern void ret_from_fork(void);
 
@@ -133,6 +134,7 @@ void cpu_idle(void)
 			 * from here on, until they go to idle.
 			 * Otherwise, idle callbacks can misfire.
 			 */
+			local_touch_nmi();
 			local_irq_disable();
 			enter_idle();
 			/* Don't trace irqs off for idle */

commit a0bfa1373859e9d11dc92561a8667588803e42d8
Author: Len Brown <len.brown@intel.com>
Date:   Fri Apr 1 19:34:59 2011 -0400

    cpuidle: stop depending on pm_idle
    
    cpuidle users should call cpuidle_call_idle() directly
    rather than via (pm_idle)() function pointer.
    
    Architecture may choose to continue using (pm_idle)(),
    but cpuidle need not depend on it:
    
      my_arch_cpu_idle()
            ...
            if(cpuidle_call_idle())
                    pm_idle();
    
    cc: Kevin Hilman <khilman@deeprootsystems.com>
    cc: Paul Mundt <lethal@linux-sh.org>
    cc: x86@kernel.org
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ca6f7ab8df33..f693e44e1bf6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -37,6 +37,7 @@
 #include <linux/uaccess.h>
 #include <linux/io.h>
 #include <linux/ftrace.h>
+#include <linux/cpuidle.h>
 
 #include <asm/pgtable.h>
 #include <asm/system.h>
@@ -136,7 +137,8 @@ void cpu_idle(void)
 			enter_idle();
 			/* Don't trace irqs off for idle */
 			stop_critical_timings();
-			pm_idle();
+			if (cpuidle_idle_call())
+				pm_idle();
 			start_critical_timings();
 
 			/* In many cases the interrupt that ended idle

commit dac853ae89043f1b7752875300faf614de43c74b
Author: Mathias Krause <minipli@googlemail.com>
Date:   Thu Jun 9 20:05:18 2011 +0200

    exec: delay address limit change until point of no return
    
    Unconditionally changing the address limit to USER_DS and not restoring
    it to its old value in the error path is wrong because it prevents us
    using kernel memory on repeated calls to this function.  This, in fact,
    breaks the fallback of hard coded paths to the init program from being
    ever successful if the first candidate fails to load.
    
    With this patch applied switching to USER_DS is delayed until the point
    of no return is reached which makes it possible to have a multi-arch
    rootfs with one arch specific init binary for each of the (hard coded)
    probed paths.
    
    Since the address limit is already set to USER_DS when start_thread()
    will be invoked, this redundancy can be safely removed.
    
    Signed-off-by: Mathias Krause <minipli@googlemail.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6c9dd922ac0d..ca6f7ab8df33 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -338,7 +338,6 @@ start_thread_common(struct pt_regs *regs, unsigned long new_ip,
 	regs->cs		= _cs;
 	regs->ss		= _ss;
 	regs->flags		= X86_EFLAGS_IF;
-	set_fs(USER_DS);
 	/*
 	 * Free the old FP and other extended state
 	 */

commit 375906f8765e131a4a159b1ffebf78c15db7b3bf
Author: Stephen Wilson <wilsons@start.ca>
Date:   Sun Mar 13 15:49:14 2011 -0400

    x86: mark associated mm when running a task in 32 bit compatibility mode
    
    This patch simply follows the same practice as for setting the TIF_IA32 flag.
    In particular, an mm is marked as holding 32-bit tasks when a 32-bit binary is
    exec'ed.  Both ELF and a.out formats are updated.
    
    Signed-off-by: Stephen Wilson <wilsons@start.ca>
    Reviewed-by: Michel Lespinasse <walken@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index bd387e8f73b4..6c9dd922ac0d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -501,6 +501,10 @@ void set_personality_64bit(void)
 	/* Make sure to be in 64bit mode */
 	clear_thread_flag(TIF_IA32);
 
+	/* Ensure the corresponding mm is not marked. */
+	if (current->mm)
+		current->mm->context.ia32_compat = 0;
+
 	/* TBD: overwrites user setup. Should have two bits.
 	   But 64bit processes have always behaved this way,
 	   so it's not too bad. The main problem is just that
@@ -516,6 +520,10 @@ void set_personality_ia32(void)
 	set_thread_flag(TIF_IA32);
 	current->personality |= force_personality32;
 
+	/* Mark the associated mm as containing 32-bit tasks. */
+	if (current->mm)
+		current->mm->context.ia32_compat = 1;
+
 	/* Prepare the first "return" to user space */
 	current_thread_info()->status |= TS_COMPAT;
 }

commit f77cfe4ea21760268c0277fa3e4b02dfd2a2c2f4
Author: Thomas Renninger <trenn@suse.de>
Date:   Fri Jan 7 11:29:44 2011 +0100

    cpuidle/x86/perf: fix power:cpu_idle double end events and throw cpu_idle events from the cpuidle layer
    
    Currently intel_idle and acpi_idle driver show double cpu_idle "exit idle"
    events -> this patch fixes it and makes cpu_idle events throwing less complex.
    
    It also introduces cpu_idle events for all architectures which use
    the cpuidle subsystem, namely:
      - arch/arm/mach-at91/cpuidle.c
      - arch/arm/mach-davinci/cpuidle.c
      - arch/arm/mach-kirkwood/cpuidle.c
      - arch/arm/mach-omap2/cpuidle34xx.c
      - arch/drivers/acpi/processor_idle.c (for all cases, not only mwait)
      - arch/x86/kernel/process.c (did throw events before, but was a mess)
      - drivers/idle/intel_idle.c (did throw events before)
    
    Convention should be:
    Fire cpu_idle events inside the current pm_idle function (not somewhere
    down the the callee tree) to keep things easy.
    
    Current possible pm_idle functions in X86:
    c1e_idle, poll_idle, cpuidle_idle_call, mwait_idle, default_idle
    -> this is really easy is now.
    
    This affects userspace:
    The type field of the cpu_idle power event can now direclty get
    mapped to:
    /sys/devices/system/cpu/cpuX/cpuidle/stateX/{name,desc,usage,time,...}
    instead of throwing very CPU/mwait specific values.
    This change is not visible for the intel_idle driver.
    For the acpi_idle driver it should only be visible if the vendor
    misses out C-states in his BIOS.
    Another (perf timechart) patch reads out cpuidle info of cpu_idle
    events from:
    /sys/.../cpuidle/stateX/*, then the cpuidle events are mapped
    to the correct C-/cpuidle state again, even if e.g. vendors miss
    out C-states in their BIOS and for example only export C1 and C3.
    -> everything is fine.
    
    Signed-off-by: Thomas Renninger <trenn@suse.de>
    CC: Robert Schoene <robert.schoene@tu-dresden.de>
    CC: Jean Pihet <j-pihet@ti.com>
    CC: Arjan van de Ven <arjan@linux.intel.com>
    CC: Ingo Molnar <mingo@elte.hu>
    CC: Frederic Weisbecker <fweisbec@gmail.com>
    CC: linux-pm@lists.linux-foundation.org
    CC: linux-acpi@vger.kernel.org
    CC: linux-kernel@vger.kernel.org
    CC: linux-perf-users@vger.kernel.org
    CC: linux-omap@vger.kernel.org
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4c818a738396..bd387e8f73b4 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -51,8 +51,6 @@
 #include <asm/syscalls.h>
 #include <asm/debugreg.h>
 
-#include <trace/events/power.h>
-
 asmlinkage extern void ret_from_fork(void);
 
 DEFINE_PER_CPU(unsigned long, old_rsp);
@@ -141,10 +139,6 @@ void cpu_idle(void)
 			pm_idle();
 			start_critical_timings();
 
-			trace_power_end(smp_processor_id());
-			trace_cpu_idle(PWR_EVENT_EXIT,
-				       smp_processor_id());
-
 			/* In many cases the interrupt that ended idle
 			   has already called exit_idle. But some idle
 			   loops can be woken up without interrupt. */

commit 25e41933b58777f2d020c3b0186b430ea004ec28
Author: Thomas Renninger <trenn@suse.de>
Date:   Mon Jan 3 17:50:44 2011 +0100

    perf: Clean up power events by introducing new, more generic ones
    
    Add these new power trace events:
    
     power:cpu_idle
     power:cpu_frequency
     power:machine_suspend
    
    The old C-state/idle accounting events:
      power:power_start
      power:power_end
    
    Have now a replacement (but we are still keeping the old
    tracepoints for compatibility):
    
      power:cpu_idle
    
    and
      power:power_frequency
    
    is replaced with:
      power:cpu_frequency
    
    power:machine_suspend is newly introduced.
    
    Jean Pihet has a patch integrated into the generic layer
    (kernel/power/suspend.c) which will make use of it.
    
    the type= field got removed from both, it was never
    used and the type is differed by the event type itself.
    
    perf timechart userspace tool gets adjusted in a separate patch.
    
    Signed-off-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Acked-by: Jean Pihet <jean.pihet@newoldbits.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: rjw@sisk.pl
    LKML-Reference: <1294073445-14812-3-git-send-email-trenn@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    LKML-Reference: <1290072314-31155-2-git-send-email-trenn@suse.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b3d7a3a04f38..4c818a738396 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -142,6 +142,8 @@ void cpu_idle(void)
 			start_critical_timings();
 
 			trace_power_end(smp_processor_id());
+			trace_cpu_idle(PWR_EVENT_EXIT,
+				       smp_processor_id());
 
 			/* In many cases the interrupt that ended idle
 			   has already called exit_idle. But some idle

commit a4d4fbc7735bba6654b20f859135f9d3f8fe7f76
Author: Brian Gerst <brgerst@gmail.com>
Date:   Fri Sep 3 21:17:12 2010 -0400

    x86-64, fpu: Disable preemption when using TS_USEDFPU
    
    Consolidates code and fixes the below race for 64-bit.
    
    commit 9fa2f37bfeb798728241cc4a19578ce6e4258f25
    Author: torvalds <torvalds>
    Date:   Tue Sep 2 07:37:25 2003 +0000
    
        Be a lot more careful about TS_USEDFPU and preemption
    
        We had some races where we testecd (or set) TS_USEDFPU together
        with sequences that depended on the setting (like clearing or
        setting the TS flag in %cr0) and we could be preempted in between,
        which screws up the FPU state, since preemption will itself change
        USEDFPU and the TS flag.
    
        This makes it a lot more explicit: the "internal" low-level FPU
        functions ("__xxxx_fpu()") all require preemption to be disabled,
        and the exported "real" functions will make sure that is the case.
    
        One case - in __switch_to() - was switched to the non-preempt-safe
        internal version, since the scheduler itself has already disabled
        preemption.
    
        BKrev: 3f5448b5WRiQuyzAlbajs3qoQjSobw
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1283563039-3466-6-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 3d9ea531ddd1..b3d7a3a04f38 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -424,7 +424,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	load_TLS(next, cpu);
 
 	/* Must be after DS reload */
-	unlazy_fpu(prev_p);
+	__unlazy_fpu(prev_p);
 
 	/* Make sure cpu is ready for new context */
 	if (preload_fpu)

commit c882e0feb937af4e5b991cbd1c81536f37053e86
Author: Robert Schöne <robert.schoene@tu-dresden.de>
Date:   Mon Jun 14 13:37:20 2010 +0200

    x86, perf: Add power_end event to process_*.c cpu_idle routine
    
    Systems using the idle thread from process_32.c and process_64.c
    do not generate power_end events which could be traced using
    perf. This patch adds the event generation for such systems.
    
    Signed-off-by: Robert Schoene <robert.schoene@tu-dresden.de>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <1276515440.5441.45.camel@localhost>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 3c2422a99f1f..3d9ea531ddd1 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -51,6 +51,8 @@
 #include <asm/syscalls.h>
 #include <asm/debugreg.h>
 
+#include <trace/events/power.h>
+
 asmlinkage extern void ret_from_fork(void);
 
 DEFINE_PER_CPU(unsigned long, old_rsp);
@@ -138,6 +140,9 @@ void cpu_idle(void)
 			stop_critical_timings();
 			pm_idle();
 			start_critical_timings();
+
+			trace_power_end(smp_processor_id());
+
 			/* In many cases the interrupt that ended idle
 			   has already called exit_idle. But some idle
 			   loops can be woken up without interrupt. */

commit 41d59102e146a4423a490b8eca68a5860af4fe1c
Merge: 3e1dd193edef c9775b4cc522
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:58:16 2010 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, fpu: Use static_cpu_has() to implement use_xsave()
      x86: Add new static_cpu_has() function using alternatives
      x86, fpu: Use the proper asm constraint in use_xsave()
      x86, fpu: Unbreak FPU emulation
      x86: Introduce 'struct fpu' and related API
      x86: Eliminate TS_XSAVE
      x86-32: Don't set ignore_fpu_irq in simd exception
      x86: Merge kernel_math_error() into math_error()
      x86: Merge simd_math_error() into math_error()
      x86-32: Rework cache flush denied handler
    
    Fix trivial conflict in arch/x86/kernel/process.c

commit 86603283326c9e95e5ad4e9fdddeec93cac5d9ad
Author: Avi Kivity <avi@redhat.com>
Date:   Thu May 6 11:45:46 2010 +0300

    x86: Introduce 'struct fpu' and related API
    
    Currently all fpu state access is through tsk->thread.xstate.  Since we wish
    to generalize fpu access to non-task contexts, wrap the state in a new
    'struct fpu' and convert existing access to use an fpu API.
    
    Signal frame handlers are not converted to the API since they will remain
    task context only things.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1273135546-29690-3-git-send-email-avi@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 17cb3295cbf7..979215f51985 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -396,7 +396,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 
 	/* we're going to use this soon, after a few expensive things */
 	if (preload_fpu)
-		prefetch(next->xstate);
+		prefetch(next->fpu.state);
 
 	/*
 	 * Reload esp0, LDT and the page table pointer:

commit 3ca50496c2677a2b3fdd3ede86660fd1433beac6
Merge: 462b04e28a7e 66f41d4c5c8a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 30 09:56:41 2010 +0200

    Merge commit 'v2.6.34-rc6' into perf/core
    
    Merge reason: update to the latest -rc.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 7ce5a2b9bb2e92902230e3121d8c3047fab9cb47
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Fri Apr 23 16:17:40 2010 -0700

    x86-64: Clear a 64-bit FS/GS base on fork if selector is nonzero
    
    When we do a thread switch, we clear the outgoing FS/GS base if the
    corresponding selector is nonzero.  This is taken by __switch_to() as
    an entry invariant; it does not verify that it is true on entry.
    However, copy_thread() doesn't enforce this constraint, which can
    result in inconsistent results after fork().
    
    Make copy_thread() match the behavior of __switch_to().
    
    Reported-and-tested-by: Samuel Thibault <samuel.thibault@inria.fr>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    LKML-Reference: <4BD1E061.8030605@zytor.com>
    Cc: <stable@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index dc9690b4c4cc..17cb3295cbf7 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -276,12 +276,12 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 
 	set_tsk_thread_flag(p, TIF_FORK);
 
-	p->thread.fs = me->thread.fs;
-	p->thread.gs = me->thread.gs;
 	p->thread.io_bitmap_ptr = NULL;
 
 	savesegment(gs, p->thread.gsindex);
+	p->thread.gs = p->thread.gsindex ? 0 : me->thread.gs;
 	savesegment(fs, p->thread.fsindex);
+	p->thread.fs = p->thread.fsindex ? 0 : me->thread.fs;
 	savesegment(es, p->thread.es);
 	savesegment(ds, p->thread.ds);
 

commit faa4602e47690fb11221e00f9b9697c8dc0d4b19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:50 2010 +0100

    x86, perf, bts, mm: Delete the never used BTS-ptrace code
    
    Support for the PMU's BTS features has been upstreamed in
    v2.6.32, but we still have the old and disabled ptrace-BTS,
    as Linus noticed it not so long ago.
    
    It's buggy: TIF_DEBUGCTLMSR is trampling all over that MSR without
    regard for other uses (perf) and doesn't provide the flexibility
    needed for perf either.
    
    Its users are ptrace-block-step and ptrace-bts, since ptrace-bts
    was never used and ptrace-block-step can be implemented using a
    much simpler approach.
    
    So axe all 3000 lines of it. That includes the *locked_memory*()
    APIs in mm/mlock.c as well.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20100325135413.938004390@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index dc9690b4c4cc..cc4258f2beb5 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -49,7 +49,6 @@
 #include <asm/ia32.h>
 #include <asm/idle.h>
 #include <asm/syscalls.h>
-#include <asm/ds.h>
 #include <asm/debugreg.h>
 
 asmlinkage extern void ret_from_fork(void);
@@ -313,13 +312,6 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 		if (err)
 			goto out;
 	}
-
-	clear_tsk_thread_flag(p, TIF_DS_AREA_MSR);
-	p->thread.ds_ctx = NULL;
-
-	clear_tsk_thread_flag(p, TIF_DEBUGCTLMSR);
-	p->thread.debugctlmsr = 0;
-
 	err = 0;
 out:
 	if (err && p->thread.io_bitmap_ptr) {

commit a7f16d10b510f9ee3500af7831f2e3094fab3dca
Merge: f66ffdedbf0f 17c0e7107bed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 28 10:35:09 2010 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Mark atomic irq ops raw for 32bit legacy
      x86: Merge show_regs()
      x86: Macroise x86 cache descriptors
      x86-32: clean up rwsem inline asm statements
      x86: Merge asm/atomic_{32,64}.h
      x86: Sync asm/atomic_32.h and asm/atomic_64.h
      x86: Split atomic64_t functions into seperate headers
      x86-64: Modify memcpy()/memset() alternatives mechanism
      x86-64: Modify copy_user_generic() alternatives mechanism
      x86: Lift restriction on the location of FIX_BTMAP_*
      x86, core: Optimize hweight32()

commit 1252f238db48ec419f40c1bdf30fda649860eed9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Feb 16 15:02:13 2010 +0100

    x86: set_personality_ia32() misses force_personality32
    
    05d43ed8a "x86: get rid of the insane TIF_ABI_PENDING bit" forgot about
    force_personality32.  Fix.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 41a26a82470a..126f0b493d04 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -527,6 +527,7 @@ void set_personality_ia32(void)
 
 	/* Make sure to be in 32bit mode */
 	set_thread_flag(TIF_IA32);
+	current->personality |= force_personality32;
 
 	/* Prepare the first "return" to user space */
 	current_thread_info()->status |= TS_COMPAT;

commit 05d43ed8a89c159ff641d472f970e3f1baa66318
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Jan 28 22:14:43 2010 -0800

    x86: get rid of the insane TIF_ABI_PENDING bit
    
    Now that the previous commit made it possible to do the personality
    setting at the point of no return, we do just that for ELF binaries.
    And suddenly all the reasons for that insane TIF_ABI_PENDING bit go
    away, and we can just make SET_PERSONALITY() just do the obvious thing
    for a 32-bit compat process.
    
    Everything becomes much more straightforward this way.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index f9e033150cdf..41a26a82470a 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -521,6 +521,17 @@ void set_personality_64bit(void)
 	current->personality &= ~READ_IMPLIES_EXEC;
 }
 
+void set_personality_ia32(void)
+{
+	/* inherit personality from parent */
+
+	/* Make sure to be in 32bit mode */
+	set_thread_flag(TIF_IA32);
+
+	/* Prepare the first "return" to user space */
+	current_thread_info()->status |= TS_COMPAT;
+}
+
 unsigned long get_wchan(struct task_struct *p)
 {
 	unsigned long stack;

commit 3bef444797f7624f8fbd27f4e0334ce96a108725
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Jan 13 10:45:55 2010 -0500

    x86: Merge show_regs()
    
    Using kernel_stack_pointer() allows 32-bit and 64-bit versions to
    be merged.  This is more correct for 64-bit, since the old %rsp is
    always saved on the stack.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1263397555-27695-1-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 52fbd0c60198..418f860880a2 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -211,12 +211,6 @@ void __show_regs(struct pt_regs *regs, int all)
 	printk(KERN_INFO "DR3: %016lx DR6: %016lx DR7: %016lx\n", d3, d6, d7);
 }
 
-void show_regs(struct pt_regs *regs)
-{
-	show_registers(regs);
-	show_trace(NULL, regs, (void *)(regs + 1), regs->bp);
-}
-
 void release_thread(struct task_struct *dead_task)
 {
 	if (dead_task->mm) {

commit d015a092989d673df44a5ad6866dc5d5006b7a2a
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Mon Dec 28 10:26:59 2009 +0200

    x86: Use KERN_DEFAULT log-level in __show_regs()
    
    Andrew Morton reported a strange looking kmemcheck warning:
    
      WARNING: kmemcheck: Caught 32-bit read from uninitialized memory (ffff88004fba6c20)
      0000000000000000310000000000000000000000000000002413000000c9ffff
       u u u u u u u u u u u u u u u u i i i i i i i i u u u u u u u u
    
       [<ffffffff810af3aa>] kmemleak_scan+0x25a/0x540
       [<ffffffff810afbcb>] kmemleak_scan_thread+0x5b/0xe0
       [<ffffffff8104d0fe>] kthread+0x9e/0xb0
       [<ffffffff81003074>] kernel_thread_helper+0x4/0x10
       [<ffffffffffffffff>] 0xffffffffffffffff
    
    The above printout is missing register dump completely. The
    problem here is that the output comes from syslog which doesn't
    show KERN_INFO log-level messages. We didn't see this before
    because both of us were testing on 32-bit kernels which use the
    _default_ log-level.
    
    Fix that up by explicitly using KERN_DEFAULT log-level for
    __show_regs() printks.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <1261988819.4641.2.camel@penberg-laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 52fbd0c60198..f9e033150cdf 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -161,19 +161,19 @@ void __show_regs(struct pt_regs *regs, int all)
 	unsigned int ds, cs, es;
 
 	show_regs_common();
-	printk(KERN_INFO "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
+	printk(KERN_DEFAULT "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
 	printk_address(regs->ip, 1);
-	printk(KERN_INFO "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,
+	printk(KERN_DEFAULT "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,
 			regs->sp, regs->flags);
-	printk(KERN_INFO "RAX: %016lx RBX: %016lx RCX: %016lx\n",
+	printk(KERN_DEFAULT "RAX: %016lx RBX: %016lx RCX: %016lx\n",
 	       regs->ax, regs->bx, regs->cx);
-	printk(KERN_INFO "RDX: %016lx RSI: %016lx RDI: %016lx\n",
+	printk(KERN_DEFAULT "RDX: %016lx RSI: %016lx RDI: %016lx\n",
 	       regs->dx, regs->si, regs->di);
-	printk(KERN_INFO "RBP: %016lx R08: %016lx R09: %016lx\n",
+	printk(KERN_DEFAULT "RBP: %016lx R08: %016lx R09: %016lx\n",
 	       regs->bp, regs->r8, regs->r9);
-	printk(KERN_INFO "R10: %016lx R11: %016lx R12: %016lx\n",
+	printk(KERN_DEFAULT "R10: %016lx R11: %016lx R12: %016lx\n",
 	       regs->r10, regs->r11, regs->r12);
-	printk(KERN_INFO "R13: %016lx R14: %016lx R15: %016lx\n",
+	printk(KERN_DEFAULT "R13: %016lx R14: %016lx R15: %016lx\n",
 	       regs->r13, regs->r14, regs->r15);
 
 	asm("movl %%ds,%0" : "=r" (ds));
@@ -194,21 +194,21 @@ void __show_regs(struct pt_regs *regs, int all)
 	cr3 = read_cr3();
 	cr4 = read_cr4();
 
-	printk(KERN_INFO "FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n",
+	printk(KERN_DEFAULT "FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n",
 	       fs, fsindex, gs, gsindex, shadowgs);
-	printk(KERN_INFO "CS:  %04x DS: %04x ES: %04x CR0: %016lx\n", cs, ds,
+	printk(KERN_DEFAULT "CS:  %04x DS: %04x ES: %04x CR0: %016lx\n", cs, ds,
 			es, cr0);
-	printk(KERN_INFO "CR2: %016lx CR3: %016lx CR4: %016lx\n", cr2, cr3,
+	printk(KERN_DEFAULT "CR2: %016lx CR3: %016lx CR4: %016lx\n", cr2, cr3,
 			cr4);
 
 	get_debugreg(d0, 0);
 	get_debugreg(d1, 1);
 	get_debugreg(d2, 2);
-	printk(KERN_INFO "DR0: %016lx DR1: %016lx DR2: %016lx\n", d0, d1, d2);
+	printk(KERN_DEFAULT "DR0: %016lx DR1: %016lx DR2: %016lx\n", d0, d1, d2);
 	get_debugreg(d3, 3);
 	get_debugreg(d6, 6);
 	get_debugreg(d7, 7);
-	printk(KERN_INFO "DR3: %016lx DR6: %016lx DR7: %016lx\n", d3, d6, d7);
+	printk(KERN_DEFAULT "DR3: %016lx DR6: %016lx DR7: %016lx\n", d3, d6, d7);
 }
 
 void show_regs(struct pt_regs *regs)

commit ab1eebe77dd08b26585860d534e07810d1cd274d
Merge: 186a25026c44 df59e7bf4399
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 15 20:33:22 2009 +0100

    Merge branch 'x86/asm' into x86/urgent
    
    Merge reason: it's stable so lets push it upstream.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit df59e7bf439918f523ac29e996ec1eebbed60440
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Dec 9 12:34:44 2009 -0500

    x86: Merge kernel_thread()
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1260380084-3707-6-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index d49a9094f6f3..1a362c5bec37 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -229,42 +229,6 @@ void show_regs(struct pt_regs *regs)
 	show_trace(NULL, regs, (void *)(regs + 1), regs->bp);
 }
 
-/*
- * This gets run with %si containing the
- * function to call, and %di containing
- * the "args".
- */
-extern void kernel_thread_helper(void);
-
-/*
- * Create a kernel thread
- */
-int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
-{
-	struct pt_regs regs;
-
-	memset(&regs, 0, sizeof(regs));
-
-	regs.si = (unsigned long) fn;
-	regs.di = (unsigned long) arg;
-
-#ifdef CONFIG_X86_32
-	regs.ds = __USER_DS;
-	regs.es = __USER_DS;
-	regs.fs = __KERNEL_PERCPU;
-	regs.gs = __KERNEL_STACK_CANARY;
-#endif
-
-	regs.orig_ax = -1;
-	regs.ip = (unsigned long) kernel_thread_helper;
-	regs.cs = __KERNEL_CS | get_kernel_rpl();
-	regs.flags = X86_EFLAGS_IF | 0x2;
-
-	/* Ok, create the new process.. */
-	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
-}
-EXPORT_SYMBOL(kernel_thread);
-
 void release_thread(struct task_struct *dead_task)
 {
 	if (dead_task->mm) {

commit f443ff4201dd25cd4dec183f9919ecba90c8edc2
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Dec 9 12:34:43 2009 -0500

    x86: Sync 32/64-bit kernel_thread
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1260380084-3707-5-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 00ac66fa5c6b..d49a9094f6f3 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -248,10 +248,17 @@ int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 	regs.si = (unsigned long) fn;
 	regs.di = (unsigned long) arg;
 
+#ifdef CONFIG_X86_32
+	regs.ds = __USER_DS;
+	regs.es = __USER_DS;
+	regs.fs = __KERNEL_PERCPU;
+	regs.gs = __KERNEL_STACK_CANARY;
+#endif
+
 	regs.orig_ax = -1;
 	regs.ip = (unsigned long) kernel_thread_helper;
-	regs.cs = __KERNEL_CS;
-	regs.flags = X86_EFLAGS_IF;
+	regs.cs = __KERNEL_CS | get_kernel_rpl();
+	regs.flags = X86_EFLAGS_IF | 0x2;
 
 	/* Ok, create the new process.. */
 	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);

commit fa4b8f84383ae197e643a46c36bf58ab8dffc95c
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Dec 9 12:34:41 2009 -0500

    x86, 64-bit: Use user_mode() to determine new stack pointer in copy_thread()
    
    Use user_mode() instead of a magic value for sp to determine when returning
    to kernel mode.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1260380084-3707-3-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 92484c2130c6..00ac66fa5c6b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -254,7 +254,7 @@ int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 	regs.flags = X86_EFLAGS_IF;
 
 	/* Ok, create the new process.. */
-	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, ~0UL, &regs, 0, NULL, NULL);
+	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
 }
 EXPORT_SYMBOL(kernel_thread);
 
@@ -312,8 +312,9 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 	*childregs = *regs;
 
 	childregs->ax = 0;
-	childregs->sp = sp;
-	if (sp == ~0UL)
+	if (user_mode(regs))
+		childregs->sp = sp;
+	else
 		childregs->sp = (unsigned long)childregs;
 
 	p->thread.sp = (unsigned long) childregs;

commit 3bd95dfb182969dc6d2a317c150e0df7107608d3
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Dec 9 12:34:40 2009 -0500

    x86, 64-bit: Move kernel_thread to C
    
    Prepare for merging with 32-bit.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1260380084-3707-2-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 83019f94b83d..92484c2130c6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -59,8 +59,6 @@ asmlinkage extern void ret_from_fork(void);
 DEFINE_PER_CPU(unsigned long, old_rsp);
 static DEFINE_PER_CPU(unsigned char, is_idle);
 
-unsigned long kernel_thread_flags = CLONE_VM | CLONE_UNTRACED;
-
 static ATOMIC_NOTIFIER_HEAD(idle_notifier);
 
 void idle_notifier_register(struct notifier_block *n)
@@ -231,6 +229,35 @@ void show_regs(struct pt_regs *regs)
 	show_trace(NULL, regs, (void *)(regs + 1), regs->bp);
 }
 
+/*
+ * This gets run with %si containing the
+ * function to call, and %di containing
+ * the "args".
+ */
+extern void kernel_thread_helper(void);
+
+/*
+ * Create a kernel thread
+ */
+int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
+{
+	struct pt_regs regs;
+
+	memset(&regs, 0, sizeof(regs));
+
+	regs.si = (unsigned long) fn;
+	regs.di = (unsigned long) arg;
+
+	regs.orig_ax = -1;
+	regs.ip = (unsigned long) kernel_thread_helper;
+	regs.cs = __KERNEL_CS;
+	regs.flags = X86_EFLAGS_IF;
+
+	/* Ok, create the new process.. */
+	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, ~0UL, &regs, 0, NULL, NULL);
+}
+EXPORT_SYMBOL(kernel_thread);
+
 void release_thread(struct task_struct *dead_task)
 {
 	if (dead_task->mm) {

commit f839bbc5c81b1c92ff8e81c360e9564f7b961b2e
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Dec 9 19:01:56 2009 -0500

    x86: Merge sys_clone
    
    Change 32-bit sys_clone to new PTREGSCALL stub, and merge with 64-bit.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1260403316-5679-7-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 671960d82587..83019f94b83d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -534,15 +534,6 @@ void set_personality_64bit(void)
 	current->personality &= ~READ_IMPLIES_EXEC;
 }
 
-asmlinkage long
-sys_clone(unsigned long clone_flags, unsigned long newsp,
-	  void __user *parent_tid, void __user *child_tid, struct pt_regs *regs)
-{
-	if (!newsp)
-		newsp = regs->sp;
-	return do_fork(clone_flags, newsp, regs, 0, parent_tid, child_tid);
-}
-
 unsigned long get_wchan(struct task_struct *p)
 {
 	unsigned long stack;

commit 11cf88bd0b8165b65aaabaee0977e9a3ad474ab7
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Dec 9 19:01:53 2009 -0500

    x86: Merge sys_execve
    
    Change 32-bit sys_execve to PTREGSCALL3, and merge with 64-bit.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1260403316-5679-4-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index c95c8f4e790a..671960d82587 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -520,25 +520,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	return prev_p;
 }
 
-/*
- * sys_execve() executes a new program.
- */
-asmlinkage
-long sys_execve(char __user *name, char __user * __user *argv,
-		char __user * __user *envp, struct pt_regs *regs)
-{
-	long error;
-	char *filename;
-
-	filename = getname(name);
-	error = PTR_ERR(filename);
-	if (IS_ERR(filename))
-		return error;
-	error = do_execve(filename, argv, envp, regs);
-	putname(filename);
-	return error;
-}
-
 void set_personality_64bit(void)
 {
 	/* inherit personality from parent */

commit 814e2c84a722c45650a9b8f52285d7ba6874f63b
Author: Andy Isaacson <adi@hexapodia.org>
Date:   Tue Dec 8 00:29:42 2009 -0800

    x86: Factor duplicated code out of __show_regs() into show_regs_common()
    
    Unify x86_32 and x86_64 implementations of __show_regs() header,
    standardizing on the x86_64 format string in the process. Also,
    32-bit will now call print_modules.
    
    Signed-off-by: Andy Isaacson <adi@hexapodia.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Robert Hancock <hancockrwd@gmail.com>
    Cc: Richard Zidlicky <rz@linux-m68k.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20091208082942.GA27174@hexapodia.org>
    [ v2: resolved conflict ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index c95c8f4e790a..e5ab0cd0ef36 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -26,7 +26,6 @@
 #include <linux/slab.h>
 #include <linux/user.h>
 #include <linux/interrupt.h>
-#include <linux/utsname.h>
 #include <linux/delay.h>
 #include <linux/module.h>
 #include <linux/ptrace.h>
@@ -38,7 +37,6 @@
 #include <linux/uaccess.h>
 #include <linux/io.h>
 #include <linux/ftrace.h>
-#include <linux/dmi.h>
 
 #include <asm/pgtable.h>
 #include <asm/system.h>
@@ -163,18 +161,8 @@ void __show_regs(struct pt_regs *regs, int all)
 	unsigned long d0, d1, d2, d3, d6, d7;
 	unsigned int fsindex, gsindex;
 	unsigned int ds, cs, es;
-	const char *board;
-
-	printk("\n");
-	print_modules();
-	board = dmi_get_system_info(DMI_PRODUCT_NAME);
-	if (!board)
-		board = "";
-	printk(KERN_INFO "Pid: %d, comm: %.20s %s %s %.*s %s\n",
-		current->pid, current->comm, print_tainted(),
-		init_utsname()->release,
-		(int)strcspn(init_utsname()->version, " "),
-		init_utsname()->version, board);
+
+	show_regs_common();
 	printk(KERN_INFO "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
 	printk_address(regs->ip, 1);
 	printk(KERN_INFO "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,

commit e7522ed5c0140486f68bf3351b4e667f570b8c76
Merge: b391738bd1c7 e634d8fc792c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 8 13:34:53 2009 -0800

    Merge branch 'x86-process-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-process-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86-64: merge the standard and compat start_thread() functions
      x86-64: make compat_start_thread() match start_thread()

commit 6ec22f9b037fc0c2e00ddb7023fad279c365324d
Merge: 83be7d764dc4 9b3660a55a90
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 5 15:33:27 2009 -0800

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Limit number of per cpu TSC sync messages
      x86: dumpstack, 64-bit: Disable preemption when walking the IRQ/exception stacks
      x86: dumpstack: Clean up the x86_stack_ids[][] initalization and other details
      x86, cpu: mv display_cacheinfo -> cpu_detect_cache_sizes
      x86: Suppress stack overrun message for init_task
      x86: Fix cpu_devs[] initialization in early_cpu_init()
      x86: Remove CPU cache size output for non-Intel too
      x86: Minimise printk spew from per-vendor init code
      x86: Remove the CPU cache size printk's
      cpumask: Avoid cpumask_t in arch/x86/kernel/apic/nmi.c
      x86: Make sure we also print a Code: line for show_regs()

commit 96200591a34f8ecb98481c626125df43a2463b55
Merge: 7031281e02bf 68efa37df779
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Nov 21 14:07:23 2009 +0100

    Merge branch 'tracing/hw-breakpoints' into perf/core
    
    Conflicts:
            arch/x86/kernel/kprobes.c
            kernel/trace/Makefile
    
    Merge reason: hw-breakpoints perf integration is looking
                  good in testing and in reviews, plus conflicts
                  are mounting up - so merge & resolve.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 24f1e32c60c45c89a997c73395b69c8af6f0a84e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Sep 9 19:22:48 2009 +0200

    hw-breakpoints: Rewrite the hw-breakpoints layer on top of perf events
    
    This patch rebase the implementation of the breakpoints API on top of
    perf events instances.
    
    Each breakpoints are now perf events that handle the
    register scheduling, thread/cpu attachment, etc..
    
    The new layering is now made as follows:
    
           ptrace       kgdb      ftrace   perf syscall
              \          |          /         /
               \         |         /         /
                                            /
                Core breakpoint API        /
                                          /
                         |               /
                         |              /
    
                  Breakpoints perf events
    
                         |
                         |
    
                   Breakpoints PMU ---- Debug Register constraints handling
                                        (Part of core breakpoint API)
                         |
                         |
    
                 Hardware debug registers
    
    Reasons of this rewrite:
    
    - Use the centralized/optimized pmu registers scheduling,
      implying an easier arch integration
    - More powerful register handling: perf attributes (pinned/flexible
      events, exclusive/non-exclusive, tunable period, etc...)
    
    Impact:
    
    - New perf ABI: the hardware breakpoints counters
    - Ptrace breakpoints setting remains tricky and still needs some per
      thread breakpoints references.
    
    Todo (in the order):
    
    - Support breakpoints perf counter events for perf tools (ie: implement
      perf_bpcounter_event())
    - Support from perf tools
    
    Changes in v2:
    
    - Follow the perf "event " rename
    - The ptrace regression have been fixed (ptrace breakpoint perf events
      weren't released when a task ended)
    - Drop the struct hw_breakpoint and store generic fields in
      perf_event_attr.
    - Separate core and arch specific headers, drop
      asm-generic/hw_breakpoint.h and create linux/hw_breakpoint.h
    - Use new generic len/type for breakpoint
    - Handle off case: when breakpoints api is not supported by an arch
    
    Changes in v3:
    
    - Fix broken CONFIG_KVM, we need to propagate the breakpoint api
      changes to kvm when we exit the guest and restore the bp registers
      to the host.
    
    Changes in v4:
    
    - Drop the hw_breakpoint_restore() stub as it is only used by KVM
    - EXPORT_SYMBOL_GPL hw_breakpoint_restore() as KVM can be built as a
      module
    - Restore the breakpoints unconditionally on kvm guest exit:
      TIF_DEBUG_THREAD doesn't anymore cover every cases of running
      breakpoints and vcpu->arch.switch_db_regs might not always be
      set when the guest used debug registers.
      (Waiting for a reliable optimization)
    
    Changes in v5:
    
    - Split-up the asm-generic/hw-breakpoint.h moving to
      linux/hw_breakpoint.h into a separate patch
    - Optimize the breakpoints restoring while switching from kvm guest
      to host. We only want to restore the state if we have active
      breakpoints to the host, otherwise we don't care about messed-up
      address registers.
    - Add asm/hw_breakpoint.h to Kbuild
    - Fix bad breakpoint type in trace_selftest.c
    
    Changes in v6:
    
    - Fix wrong header inclusion in trace.h (triggered a build
      error with CONFIG_FTRACE_SELFTEST
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jan Kiszka <jan.kiszka@web.de>
    Cc: Jiri Slaby <jirislaby@gmail.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 72edac026a78..5bafdec34441 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -53,7 +53,6 @@
 #include <asm/syscalls.h>
 #include <asm/ds.h>
 #include <asm/debugreg.h>
-#include <asm/hw_breakpoint.h>
 
 asmlinkage extern void ret_from_fork(void);
 
@@ -244,8 +243,6 @@ void release_thread(struct task_struct *dead_task)
 			BUG();
 		}
 	}
-	if (unlikely(dead_task->thread.debugreg7))
-		flush_thread_hw_breakpoint(dead_task);
 }
 
 static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
@@ -309,9 +306,7 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 	savesegment(ds, p->thread.ds);
 
 	err = -ENOMEM;
-	if (unlikely(test_tsk_thread_flag(me, TIF_DEBUG)))
-		if (copy_thread_hw_breakpoint(me, p, clone_flags))
-			goto out;
+	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));
 
 	if (unlikely(test_tsk_thread_flag(me, TIF_IO_BITMAP))) {
 		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
@@ -351,8 +346,6 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 		kfree(p->thread.io_bitmap_ptr);
 		p->thread.io_bitmap_max = 0;
 	}
-	if (err)
-		flush_thread_hw_breakpoint(p);
 
 	return err;
 }
@@ -508,23 +501,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 */
 	if (preload_fpu)
 		__math_state_restore();
-	/*
-	 * There's a problem with moving the arch_install_thread_hw_breakpoint()
-	 * call before current is updated.  Suppose a kernel breakpoint is
-	 * triggered in between the two, the hw-breakpoint handler will see that
-	 * the 'current' task does not have TIF_DEBUG flag set and will think it
-	 * is leftover from an old task (lazy switching) and will erase it. Then
-	 * until the next context switch, no user-breakpoints will be installed.
-	 *
-	 * The real problem is that it's impossible to update both current and
-	 * physical debug registers at the same instant, so there will always be
-	 * a window in which they disagree and a breakpoint might get triggered.
-	 * Since we use lazy switching, we are forced to assume that a
-	 * disagreement means that current is correct and the exception is due
-	 * to lazy debug register switching.
-	 */
-	if (unlikely(test_tsk_thread_flag(next_p, TIF_DEBUG)))
-		arch_install_thread_hw_breakpoint(next_p);
 
 	return prev_p;
 }

commit 89240ba059ca468ae7a8346edf7f95082458c2fc
Author: Stefani Seibold <stefani@seibold.net>
Date:   Tue Nov 3 10:22:40 2009 +0100

    x86, fs: Fix x86 procfs stack information for threads on 64-bit
    
    This patch fixes two issues in the procfs stack information on
    x86-64 linux.
    
    The 32 bit loader compat_do_execve did not store stack
    start. (this was figured out by Alexey Dobriyan).
    
    The stack information on a x64_64 kernel always shows 0 kbyte
    stack usage, because of a missing implementation of the KSTK_ESP
    macro which always returned -1.
    
    The new implementation now returns the right value.
    
    Signed-off-by: Stefani Seibold <stefani@seibold.net>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <1257240160.4889.24.camel@wall-e>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ad535b683170..eb62cbcaa490 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -664,3 +664,8 @@ long sys_arch_prctl(int code, unsigned long addr)
 	return do_arch_prctl(current, code, addr);
 }
 
+unsigned long KSTK_ESP(struct task_struct *task)
+{
+	return (test_tsk_thread_flag(task, TIF_IA32)) ?
+			(task_pt_regs(task)->sp) : ((task)->thread.usersp);
+}

commit a489ca355efaf9efa4990b0f8f30ab650a206273
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Mon Nov 2 16:59:15 2009 -0800

    x86: Make sure we also print a Code: line for show_regs()
    
    show_regs() is called as a mini BUG() equivalent in some places,
    specifically for the "scheduling while atomic" case.
    
    Unfortunately right now it does not print a Code: line unlike
    a real bug/oops.
    
    This patch changes the x86 implementation of show_regs() so that
    it calls the same function as oopses do to print the registers
    as well as the Code: line.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    LKML-Reference: <20091102165915.4a980fc0@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ad535b683170..2386999bfcd2 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -226,8 +226,7 @@ void __show_regs(struct pt_regs *regs, int all)
 
 void show_regs(struct pt_regs *regs)
 {
-	printk(KERN_INFO "CPU %d:", smp_processor_id());
-	__show_regs(regs, 1);
+	show_registers(regs);
 	show_trace(NULL, regs, (void *)(regs + 1), regs->bp);
 }
 

commit e634d8fc792c66c3d4ff45518c04848c1e28f221
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Fri Oct 9 15:56:53 2009 -0700

    x86-64: merge the standard and compat start_thread() functions
    
    The only thing left that differs between the standard and compat
    start_thread functions is the actual segment numbers and the
    prototype, so have a single common function which contains the guts
    and two very small wrappers.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 7cf0a6b6d4bb..eb261c582a44 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -344,18 +344,20 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 	return err;
 }
 
-void
-start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
+static void
+start_thread_common(struct pt_regs *regs, unsigned long new_ip,
+		    unsigned long new_sp,
+		    unsigned int _cs, unsigned int _ss, unsigned int _ds)
 {
 	loadsegment(fs, 0);
-	loadsegment(es, 0);
-	loadsegment(ds, 0);
+	loadsegment(es, _ds);
+	loadsegment(ds, _ds);
 	load_gs_index(0);
 	regs->ip		= new_ip;
 	regs->sp		= new_sp;
 	percpu_write(old_rsp, new_sp);
-	regs->cs		= __USER_CS;
-	regs->ss		= __USER_DS;
+	regs->cs		= _cs;
+	regs->ss		= _ss;
 	regs->flags		= X86_EFLAGS_IF;
 	set_fs(USER_DS);
 	/*
@@ -363,26 +365,19 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 	 */
 	free_thread_xstate(current);
 }
-EXPORT_SYMBOL_GPL(start_thread);
+
+void
+start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
+{
+	start_thread_common(regs, new_ip, new_sp,
+			    __USER_CS, __USER_DS, 0);
+}
 
 #ifdef CONFIG_IA32_EMULATION
 void start_thread_ia32(struct pt_regs *regs, u32 new_ip, u32 new_sp)
 {
-	loadsegment(fs, 0);
-	loadsegment(ds, __USER32_DS);
-	loadsegment(es, __USER32_DS);
-	load_gs_index(0);
-	regs->ip		= new_ip;
-	regs->sp		= new_sp;
-	percpu_write(old_rsp, new_sp);
-	regs->cs		= __USER32_CS;
-	regs->ss		= __USER32_DS;
-	regs->flags		= X86_EFLAGS_IF;
-	set_fs(USER_DS);
-	/*
-	 * Free the old FP and other extended state
-	 */
-	free_thread_xstate(current);
+	start_thread_common(regs, new_ip, new_sp,
+			    __USER32_CS, __USER32_DS, __USER32_DS);
 }
 #endif
 

commit a6f05a6a0a1713d5b019f096799d49226807d3df
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Oct 8 18:02:54 2009 -0700

    x86-64: make compat_start_thread() match start_thread()
    
    For no real good reason, compat_start_thread() was embedded inline in
    <asm/elf.h> whereas the native start_thread() lives in process_*.c.
    Move compat_start_thread() to process_64.c, remove gratuitious
    differences, and fix a few items which mostly look like bit rot.
    
    In particular, compat_start_thread() didn't do free_thread_xstate(),
    which means it was hanging on to the xstate store area even when it
    was not needed.  It was also not setting old_rsp, but it looks like
    that generally shouldn't matter for a 32-bit process.
    
    Note: compat_start_thread *has* to be a macro, since it is tested with
    start_thread_ia32() as the out of line function name.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ad535b683170..7cf0a6b6d4bb 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -356,7 +356,7 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 	percpu_write(old_rsp, new_sp);
 	regs->cs		= __USER_CS;
 	regs->ss		= __USER_DS;
-	regs->flags		= 0x200;
+	regs->flags		= X86_EFLAGS_IF;
 	set_fs(USER_DS);
 	/*
 	 * Free the old FP and other extended state
@@ -365,6 +365,27 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 }
 EXPORT_SYMBOL_GPL(start_thread);
 
+#ifdef CONFIG_IA32_EMULATION
+void start_thread_ia32(struct pt_regs *regs, u32 new_ip, u32 new_sp)
+{
+	loadsegment(fs, 0);
+	loadsegment(ds, __USER32_DS);
+	loadsegment(es, __USER32_DS);
+	load_gs_index(0);
+	regs->ip		= new_ip;
+	regs->sp		= new_sp;
+	percpu_write(old_rsp, new_sp);
+	regs->cs		= __USER32_CS;
+	regs->ss		= __USER32_DS;
+	regs->flags		= X86_EFLAGS_IF;
+	set_fs(USER_DS);
+	/*
+	 * Free the old FP and other extended state
+	 */
+	free_thread_xstate(current);
+}
+#endif
+
 /*
  *	switch_to(x,y) should switch tasks from x to y.
  *

commit dca2d6ac09d9ef59ff46820d4f0c94b08a671202
Merge: d6a65dffb30d 18240904960a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Sep 15 12:18:15 2009 +0200

    Merge branch 'linus' into tracing/hw-breakpoints
    
    Conflicts:
            arch/x86/kernel/process_64.c
    
    Semantic conflict fixed in:
            arch/x86/kvm/x86.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 55e0715f612f19b44c17497929091df2f3357e5d
Merge: 7dfd54a905be bdf977b37418
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 14 08:01:28 2009 -0700

    Merge branch 'x86-percpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-percpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, percpu: Collect hot percpu variables into one cacheline
      x86, percpu: Fix DECLARE/DEFINE_PER_CPU_PAGE_ALIGNED()
      x86, percpu: Add 'percpu_read_stable()' interface for cacheable accesses

commit bdf977b37418cdf8a2252504779a7e12a09b7575
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 3 14:12:19 2009 +0900

    x86, percpu: Collect hot percpu variables into one cacheline
    
    On x86_64, percpu variables current_task and kernel_stack are used for
    get_current() and current_thread_info() respectively and thus are
    often used close to each other.  Move definition of current_task to
    kernel/cpu/common.c right above kernel_stack definition and align it
    to cacheline so that they always fall into the same cacheline.  Two
    percpu variables defined there together - irq_stack_ptr and irq_count
    - are also pretty hot and will benefit from sharing the cacheline.
    
    For consistency, current_task definition for x86_32 is also moved to
    kernel/cpu/common.c.
    
    Putting current_task and kernel_stack into the same cacheline was
    suggested by Linus Torvalds.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ebefb5407b9d..c4c675d5ba1a 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -55,9 +55,6 @@
 
 asmlinkage extern void ret_from_fork(void);
 
-DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
-EXPORT_PER_CPU_SYMBOL(current_task);
-
 DEFINE_PER_CPU(unsigned long, old_rsp);
 static DEFINE_PER_CPU(unsigned char, is_idle);
 

commit 17950c5b243f99cbabef173415ee988c52104d7e
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Apr 24 01:01:01 2009 -0700

    x86-64: move clts into batch cpu state updates when preloading fpu
    
    When a task is likely to be using the fpu, we preload its state during
    the context switch, rather than waiting for it to run an fpu instruction.
    Make sure the clts() happens while we're doing batched fpu state updates
    to optimise paravirtualized context switches.
    
    [ Impact: optimise paravirtual FPU context switch ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index c9b8904736db..a28279dbb07c 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -386,9 +386,17 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &per_cpu(init_tss, cpu);
 	unsigned fsindex, gsindex;
+	bool preload_fpu;
+
+	/*
+	 * If the task has used fpu the last 5 timeslices, just do a full
+	 * restore of the math state immediately to avoid the trap; the
+	 * chances of needing FPU soon are obviously high now
+	 */
+	preload_fpu = tsk_used_math(next_p) && next_p->fpu_counter > 5;
 
 	/* we're going to use this soon, after a few expensive things */
-	if (next_p->fpu_counter > 5)
+	if (preload_fpu)
 		prefetch(next->xstate);
 
 	/*
@@ -422,6 +430,10 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/* Must be after DS reload */
 	unlazy_fpu(prev_p);
 
+	/* Make sure cpu is ready for new context */
+	if (preload_fpu)
+		clts();
+
 	/*
 	 * Leave lazy mode, flushing any hypercalls made here.
 	 * This must be done before restoring TLS segments so
@@ -480,15 +492,12 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 		     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))
 		__switch_to_xtra(prev_p, next_p, tss);
 
-	/* If the task has used fpu the last 5 timeslices, just do a full
-	 * restore of the math state immediately to avoid the trap; the
-	 * chances of needing FPU soon are obviously high now
-	 *
-	 * tsk_used_math() checks prevent calling math_state_restore(),
-	 * which can sleep in the case of !tsk_used_math()
+	/*
+	 * Preload the FPU context, now that we've determined that the
+	 * task is likely to be using it. 
 	 */
-	if (tsk_used_math(next_p) && next_p->fpu_counter > 5)
-		math_state_restore();
+	if (preload_fpu)
+		__math_state_restore();
 	return prev_p;
 }
 

commit 16d9dbf0c2bd167fdd942b83592d59696c7b73bd
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Apr 24 00:50:27 2009 -0700

    x86-64: move unlazy_fpu() into lazy cpu state part of context switch
    
    Make sure that unlazy_fpu()'s stts gets batched along with the other
    cpu state changes during context switch.  (32-bit already does this.)
    
    This makes sure it gets batched when running paravirtualized.
    
    [ Impact: optimise paravirtual FPU context switch ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ebefb5407b9d..c9b8904736db 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -419,6 +419,9 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 
 	load_TLS(next, cpu);
 
+	/* Must be after DS reload */
+	unlazy_fpu(prev_p);
+
 	/*
 	 * Leave lazy mode, flushing any hypercalls made here.
 	 * This must be done before restoring TLS segments so
@@ -459,9 +462,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 		wrmsrl(MSR_KERNEL_GS_BASE, next->gs);
 	prev->gsindex = gsindex;
 
-	/* Must be after DS reload */
-	unlazy_fpu(prev_p);
-
 	/*
 	 * Switch the PDA and FPU contexts.
 	 */

commit eadb8a091b27a840de7450f84ecff5ef13476424
Merge: 73874005cd88 65795efbd380
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 17 12:52:15 2009 +0200

    Merge branch 'linus' into tracing/hw-breakpoints
    
    Conflicts:
            arch/x86/Kconfig
            arch/x86/kernel/traps.c
            arch/x86/power/cpu.c
            arch/x86/power/cpu_32.c
            kernel/Makefile
    
    Semantic conflict:
            arch/x86/kernel/hw_breakpoint.c
    
    Merge reason: Resolve the conflicts, move from put_cpu_no_sched() to
                  put_cpu() in arch/x86/kernel/hw_breakpoint.c.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 862366118026a358882eefc70238dbcc3db37aac
Merge: 57eee9ae7bbc 511b01bdf64a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 19:53:40 2009 -0700

    Merge branch 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (244 commits)
      Revert "x86, bts: reenable ptrace branch trace support"
      tracing: do not translate event helper macros in print format
      ftrace/documentation: fix typo in function grapher name
      tracing/events: convert block trace points to TRACE_EVENT(), fix !CONFIG_BLOCK
      tracing: add protection around module events unload
      tracing: add trace_seq_vprint interface
      tracing: fix the block trace points print size
      tracing/events: convert block trace points to TRACE_EVENT()
      ring-buffer: fix ret in rb_add_time_stamp
      ring-buffer: pass in lockdep class key for reader_lock
      tracing: add annotation to what type of stack trace is recorded
      tracing: fix multiple use of __print_flags and __print_symbolic
      tracing/events: fix output format of user stack
      tracing/events: fix output format of kernel stack
      tracing/trace_stack: fix the number of entries in the header
      ring-buffer: discard timestamps that are at the start of the buffer
      ring-buffer: try to discard unneeded timestamps
      ring-buffer: fix bug in ring_buffer_discard_commit
      ftrace: do not profile functions when disabled
      tracing: make trace pipe recognize latency format flag
      ...

commit be15f9d63b97da0065187696962331de6cd9de9e
Merge: 595dc54a1da9 a789ed5fb6d0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 16:16:27 2009 -0700

    Merge branch 'x86-xen-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-xen-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (42 commits)
      xen: cache cr0 value to avoid trap'n'emulate for read_cr0
      xen/x86-64: clean up warnings about IST-using traps
      xen/x86-64: fix breakpoints and hardware watchpoints
      xen: reserve Xen start_info rather than e820 reserving
      xen: add FIX_TEXT_POKE to fixmap
      lguest: update lazy mmu changes to match lguest's use of kvm hypercalls
      xen: honour VCPU availability on boot
      xen: add "capabilities" file
      xen: drop kexec bits from /sys/hypervisor since kexec isn't implemented yet
      xen/sys/hypervisor: change writable_pt to features
      xen: add /sys/hypervisor support
      xen/xenbus: export xenbus_dev_changed
      xen: use device model for suspending xenbus devices
      xen: remove suspend_cancel hook
      xen/dev-evtchn: clean up locking in evtchn
      xen: export ioctl headers to userspace
      xen: add /dev/xen/evtchn driver
      xen: add irq_from_evtchn
      xen: clean up gate trap/interrupt constants
      xen: set _PAGE_NX in __supported_pte_mask before pagetable construction
      ...

commit 66cb5917295958652ff6ba36d83f98f2379c46b4
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Mon Jun 1 23:44:55 2009 +0530

    hw-breakpoints: use the new wrapper routines to access debug registers in process/thread code
    
    This patch enables the use of abstract debug registers in
    process-handling routines, according to the new hardware breakpoint
    Api.
    
    [ Impact: adapt thread breakpoints handling code to the new breakpoint Api ]
    
    Original-patch-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Reviewed-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 5a1a1de292ec..f7b276d4b3fb 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -55,6 +55,8 @@
 #include <asm/idle.h>
 #include <asm/syscalls.h>
 #include <asm/ds.h>
+#include <asm/debugreg.h>
+#include <asm/hw_breakpoint.h>
 
 asmlinkage extern void ret_from_fork(void);
 
@@ -248,6 +250,8 @@ void release_thread(struct task_struct *dead_task)
 			BUG();
 		}
 	}
+	if (unlikely(dead_task->thread.debugreg7))
+		flush_thread_hw_breakpoint(dead_task);
 }
 
 static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
@@ -303,12 +307,18 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 
 	p->thread.fs = me->thread.fs;
 	p->thread.gs = me->thread.gs;
+	p->thread.io_bitmap_ptr = NULL;
 
 	savesegment(gs, p->thread.gsindex);
 	savesegment(fs, p->thread.fsindex);
 	savesegment(es, p->thread.es);
 	savesegment(ds, p->thread.ds);
 
+	err = -ENOMEM;
+	if (unlikely(test_tsk_thread_flag(me, TIF_DEBUG)))
+		if (copy_thread_hw_breakpoint(me, p, clone_flags))
+			goto out;
+
 	if (unlikely(test_tsk_thread_flag(me, TIF_IO_BITMAP))) {
 		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
 		if (!p->thread.io_bitmap_ptr) {
@@ -347,6 +357,9 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 		kfree(p->thread.io_bitmap_ptr);
 		p->thread.io_bitmap_max = 0;
 	}
+	if (err)
+		flush_thread_hw_breakpoint(p);
+
 	return err;
 }
 
@@ -492,6 +505,24 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 */
 	if (tsk_used_math(next_p) && next_p->fpu_counter > 5)
 		math_state_restore();
+	/*
+	 * There's a problem with moving the arch_install_thread_hw_breakpoint()
+	 * call before current is updated.  Suppose a kernel breakpoint is
+	 * triggered in between the two, the hw-breakpoint handler will see that
+	 * the 'current' task does not have TIF_DEBUG flag set and will think it
+	 * is leftover from an old task (lazy switching) and will erase it. Then
+	 * until the next context switch, no user-breakpoints will be installed.
+	 *
+	 * The real problem is that it's impossible to update both current and
+	 * physical debug registers at the same instant, so there will always be
+	 * a window in which they disagree and a breakpoint might get triggered.
+	 * Since we use lazy switching, we are forced to assume that a
+	 * disagreement means that current is correct and the exception is due
+	 * to lazy debug register switching.
+	 */
+	if (unlikely(test_tsk_thread_flag(next_p, TIF_DEBUG)))
+		arch_install_thread_hw_breakpoint(next_p);
+
 	return prev_p;
 }
 

commit bf78ad69cd351798b9447a269c6bd41ce1f111f4
Author: Amerigo Wang <amwang@redhat.com>
Date:   Mon May 11 23:29:09 2009 -0400

    x86: process.c, remove useless headers
    
    <stdarg.h> is not needed by these files, remove them.
    
    [ Impact: cleanup ]
    
    Signed-off-by: WANG Cong <amwang@redhat.com>
    Cc: akpm@linux-foundation.org
    LKML-Reference: <20090512032956.5040.77055.sendpatchset@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 34386f72bdcf..9d6b20e6cd80 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -14,8 +14,6 @@
  * This file handles the architecture-dependent parts of process handling..
  */
 
-#include <stdarg.h>
-
 #include <linux/stackprotector.h>
 #include <linux/cpu.h>
 #include <linux/errno.h>

commit 9d62dcdfa6f6fc843f7d9b494bcd48f00b94f883
Author: Amerigo Wang <amwang@redhat.com>
Date:   Mon May 11 22:05:28 2009 -0400

    x86: merge process.c a bit
    
    Merge arch_align_stack() and arch_randomize_brk(), since
    they are the same.
    
    Tested on x86_64.
    
    [ Impact: cleanup ]
    
    Signed-off-by: Amerigo Wang <amwang@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b751a41392b1..34386f72bdcf 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -32,7 +32,6 @@
 #include <linux/delay.h>
 #include <linux/module.h>
 #include <linux/ptrace.h>
-#include <linux/random.h>
 #include <linux/notifier.h>
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
@@ -660,15 +659,3 @@ long sys_arch_prctl(int code, unsigned long addr)
 	return do_arch_prctl(current, code, addr);
 }
 
-unsigned long arch_align_stack(unsigned long sp)
-{
-	if (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)
-		sp -= get_random_int() % 8192;
-	return sp & ~0xf;
-}
-
-unsigned long arch_randomize_brk(struct mm_struct *mm)
-{
-	unsigned long range_end = mm->brk + 0x02000000;
-	return randomize_range(mm->brk, range_end, 0) ? : mm->brk;
-}

commit 38f4b8c0da01ae7cd9b93386842ce272d6fde9ab
Merge: a81145402735 8e2c4f2844c0
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue Apr 7 13:34:16 2009 -0700

    Merge commit 'origin/master' into for-linus/xen/master
    
    * commit 'origin/master': (4825 commits)
      Fix build errors due to CONFIG_BRANCH_TRACER=y
      parport: Use the PCI IRQ if offered
      tty: jsm cleanups
      Adjust path to gpio headers
      KGDB_SERIAL_CONSOLE check for module
      Change KCONFIG name
      tty: Blackin CTS/RTS
      Change hardware flow control from poll to interrupt driven
      Add support for the MAX3100 SPI UART.
      lanana: assign a device name and numbering for MAX3100
      serqt: initial clean up pass for tty side
      tty: Use the generic RS485 ioctl on CRIS
      tty: Correct inline types for tty_driver_kref_get()
      splice: fix deadlock in splicing to file
      nilfs2: support nanosecond timestamp
      nilfs2: introduce secondary super block
      nilfs2: simplify handling of active state of segments
      nilfs2: mark minor flag for checkpoint created by internal operation
      nilfs2: clean up sketch file
      nilfs2: super block operations fix endian bug
      ...
    
    Conflicts:
            arch/x86/include/asm/thread_info.h
            arch/x86/lguest/boot.c
            drivers/xen/manage.c

commit 2311f0de21c17b2a8b960677a9cccfbfa52beb35
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Apr 3 16:43:46 2009 +0200

    x86, ds: add leakage warning
    
    Add a warning in case a debug store context is not removed before
    the task it is attached to is freed.
    
    Remove the old warning at thread exit. It is too early.
    
    Declare the debug store context field in thread_struct unconditionally.
    
    Remove ds_copy_thread() and ds_exit_thread() and do the work directly
    in process*.c.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Cc: roland@redhat.com
    Cc: eranian@googlemail.com
    Cc: oleg@redhat.com
    Cc: juan.villacis@intel.com
    Cc: ak@linux.jf.intel.com
    LKML-Reference: <20090403144601.254472000@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b751a41392b1..5a1a1de292ec 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -335,7 +335,8 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
 			goto out;
 	}
 
-	ds_copy_thread(p, me);
+	clear_tsk_thread_flag(p, TIF_DS_AREA_MSR);
+	p->thread.ds_ctx = NULL;
 
 	clear_tsk_thread_flag(p, TIF_DEBUGCTLMSR);
 	p->thread.debugctlmsr = 0;

commit 6f2c55b843836d26528c56a0968689accaedbc67
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 2 16:56:59 2009 -0700

    Simplify copy_thread()
    
    First argument unused since 2.3.11.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index abb7e6a7f0c6..b751a41392b1 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -278,7 +278,7 @@ void prepare_to_copy(struct task_struct *tsk)
 	unlazy_fpu(tsk);
 }
 
-int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
+int copy_thread(unsigned long clone_flags, unsigned long sp,
 		unsigned long unused,
 	struct task_struct *p, struct pt_regs *regs)
 {

commit 224101ed69d3fbb486868e0f6e0f9fa37302efb4
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Feb 18 11:18:57 2009 -0800

    x86/paravirt: finish change from lazy cpu to context switch start/end
    
    Impact: fix lazy context switch API
    
    Pass the previous and next tasks into the context switch start
    end calls, so that the called functions can properly access the
    task state (esp in end_context_switch, in which the next task
    is not yet completely current).
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 7115e6085326..e8a9aaf9df88 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -428,7 +428,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 * done before math_state_restore, so the TS bit is up
 	 * to date.
 	 */
-	arch_end_context_switch();
+	arch_end_context_switch(next_p);
 
 	/*
 	 * Switch FS and GS.

commit 7fd7d83d49914f03aefffba6aee09032fcd54cce
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue Feb 17 23:24:03 2009 -0800

    x86/pvops: replace arch_enter_lazy_cpu_mode with arch_start_context_switch
    
    Impact: simplification, prepare for later changes
    
    Make lazy cpu mode more specific to context switching, so that
    it makes sense to do more context-switch specific things in
    the callbacks.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index abb7e6a7f0c6..7115e6085326 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -428,7 +428,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 * done before math_state_restore, so the TS bit is up
 	 * to date.
 	 */
-	arch_leave_lazy_cpu_mode();
+	arch_end_context_switch();
 
 	/*
 	 * Switch FS and GS.

commit 389d1fb11e5f2a16b5e34c547756f0c4dec641f7
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Feb 27 13:25:28 2009 -0800

    x86: unify chunks of kernel/process*.c
    
    With x86-32 and -64 using the same mechanism for managing the
    tss io permissions bitmap, large chunks of process*.c are
    trivially unifyable, including:
    
     - exit_thread
     - flush_thread
     - __switch_to_xtra (along with tsc enable/disable)
    
    and as bonus pickups:
    
     - sys_fork
     - sys_vfork
    
    (Note: asmlinkage expands to empty on x86-64)
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 836ef6575f01..abb7e6a7f0c6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -237,61 +237,6 @@ void show_regs(struct pt_regs *regs)
 	show_trace(NULL, regs, (void *)(regs + 1), regs->bp);
 }
 
-/*
- * Free current thread data structures etc..
- */
-void exit_thread(void)
-{
-	struct task_struct *me = current;
-	struct thread_struct *t = &me->thread;
-
-	if (me->thread.io_bitmap_ptr) {
-		struct tss_struct *tss = &per_cpu(init_tss, get_cpu());
-
-		kfree(t->io_bitmap_ptr);
-		t->io_bitmap_ptr = NULL;
-		clear_thread_flag(TIF_IO_BITMAP);
-		/*
-		 * Careful, clear this in the TSS too:
-		 */
-		memset(tss->io_bitmap, 0xff, t->io_bitmap_max);
-		t->io_bitmap_max = 0;
-		put_cpu();
-	}
-
-	ds_exit_thread(current);
-}
-
-void flush_thread(void)
-{
-	struct task_struct *tsk = current;
-
-	if (test_tsk_thread_flag(tsk, TIF_ABI_PENDING)) {
-		clear_tsk_thread_flag(tsk, TIF_ABI_PENDING);
-		if (test_tsk_thread_flag(tsk, TIF_IA32)) {
-			clear_tsk_thread_flag(tsk, TIF_IA32);
-		} else {
-			set_tsk_thread_flag(tsk, TIF_IA32);
-			current_thread_info()->status |= TS_COMPAT;
-		}
-	}
-	clear_tsk_thread_flag(tsk, TIF_DEBUG);
-
-	tsk->thread.debugreg0 = 0;
-	tsk->thread.debugreg1 = 0;
-	tsk->thread.debugreg2 = 0;
-	tsk->thread.debugreg3 = 0;
-	tsk->thread.debugreg6 = 0;
-	tsk->thread.debugreg7 = 0;
-	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
-	/*
-	 * Forget coprocessor state..
-	 */
-	tsk->fpu_counter = 0;
-	clear_fpu(tsk);
-	clear_used_math();
-}
-
 void release_thread(struct task_struct *dead_task)
 {
 	if (dead_task->mm) {
@@ -425,118 +370,6 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 }
 EXPORT_SYMBOL_GPL(start_thread);
 
-static void hard_disable_TSC(void)
-{
-	write_cr4(read_cr4() | X86_CR4_TSD);
-}
-
-void disable_TSC(void)
-{
-	preempt_disable();
-	if (!test_and_set_thread_flag(TIF_NOTSC))
-		/*
-		 * Must flip the CPU state synchronously with
-		 * TIF_NOTSC in the current running context.
-		 */
-		hard_disable_TSC();
-	preempt_enable();
-}
-
-static void hard_enable_TSC(void)
-{
-	write_cr4(read_cr4() & ~X86_CR4_TSD);
-}
-
-static void enable_TSC(void)
-{
-	preempt_disable();
-	if (test_and_clear_thread_flag(TIF_NOTSC))
-		/*
-		 * Must flip the CPU state synchronously with
-		 * TIF_NOTSC in the current running context.
-		 */
-		hard_enable_TSC();
-	preempt_enable();
-}
-
-int get_tsc_mode(unsigned long adr)
-{
-	unsigned int val;
-
-	if (test_thread_flag(TIF_NOTSC))
-		val = PR_TSC_SIGSEGV;
-	else
-		val = PR_TSC_ENABLE;
-
-	return put_user(val, (unsigned int __user *)adr);
-}
-
-int set_tsc_mode(unsigned int val)
-{
-	if (val == PR_TSC_SIGSEGV)
-		disable_TSC();
-	else if (val == PR_TSC_ENABLE)
-		enable_TSC();
-	else
-		return -EINVAL;
-
-	return 0;
-}
-
-/*
- * This special macro can be used to load a debugging register
- */
-#define loaddebug(thread, r) set_debugreg(thread->debugreg ## r, r)
-
-static inline void __switch_to_xtra(struct task_struct *prev_p,
-				    struct task_struct *next_p,
-				    struct tss_struct *tss)
-{
-	struct thread_struct *prev, *next;
-
-	prev = &prev_p->thread,
-	next = &next_p->thread;
-
-	if (test_tsk_thread_flag(next_p, TIF_DS_AREA_MSR) ||
-	    test_tsk_thread_flag(prev_p, TIF_DS_AREA_MSR))
-		ds_switch_to(prev_p, next_p);
-	else if (next->debugctlmsr != prev->debugctlmsr)
-		update_debugctlmsr(next->debugctlmsr);
-
-	if (test_tsk_thread_flag(next_p, TIF_DEBUG)) {
-		loaddebug(next, 0);
-		loaddebug(next, 1);
-		loaddebug(next, 2);
-		loaddebug(next, 3);
-		/* no 4 and 5 */
-		loaddebug(next, 6);
-		loaddebug(next, 7);
-	}
-
-	if (test_tsk_thread_flag(prev_p, TIF_NOTSC) ^
-	    test_tsk_thread_flag(next_p, TIF_NOTSC)) {
-		/* prev and next are different */
-		if (test_tsk_thread_flag(next_p, TIF_NOTSC))
-			hard_disable_TSC();
-		else
-			hard_enable_TSC();
-	}
-
-	if (test_tsk_thread_flag(next_p, TIF_IO_BITMAP)) {
-		/*
-		 * Copy the relevant range of the IO bitmap.
-		 * Normally this is 128 bytes or less:
-		 */
-		memcpy(tss->io_bitmap, next->io_bitmap_ptr,
-		       max(prev->io_bitmap_max, next->io_bitmap_max));
-	} else if (test_tsk_thread_flag(prev_p, TIF_IO_BITMAP)) {
-		/*
-		 * Clear any possible leftover bits:
-		 */
-		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
-	}
-}
-
 /*
  *	switch_to(x,y) should switch tasks from x to y.
  *
@@ -694,11 +527,6 @@ void set_personality_64bit(void)
 	current->personality &= ~READ_IMPLIES_EXEC;
 }
 
-asmlinkage long sys_fork(struct pt_regs *regs)
-{
-	return do_fork(SIGCHLD, regs->sp, regs, 0, NULL, NULL);
-}
-
 asmlinkage long
 sys_clone(unsigned long clone_flags, unsigned long newsp,
 	  void __user *parent_tid, void __user *child_tid, struct pt_regs *regs)
@@ -708,22 +536,6 @@ sys_clone(unsigned long clone_flags, unsigned long newsp,
 	return do_fork(clone_flags, newsp, regs, 0, parent_tid, child_tid);
 }
 
-/*
- * This is trivial, and on the face of it looks like it
- * could equally well be done in user mode.
- *
- * Not so, for quite unobvious reasons - register pressure.
- * In user mode vfork() cannot have a stack frame, and if
- * done by calling the "clone()" system call directly, you
- * do not have enough call-clobbered registers to hold all
- * the information you need.
- */
-asmlinkage long sys_vfork(struct pt_regs *regs)
-{
-	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->sp, regs, 0,
-		    NULL, NULL);
-}
-
 unsigned long get_wchan(struct task_struct *p)
 {
 	unsigned long stack;

commit 5c79d2a517a9905599d192db8ce77ab5f1a2faca
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 11 16:31:00 2009 +0900

    x86: fix x86_32 stack protector bugs
    
    Impact: fix x86_32 stack protector
    
    Brian Gerst found out that %gs was being initialized to stack_canary
    instead of stack_canary - 20, which basically gave the same canary
    value for all threads.  Fixing this also exposed the following bugs.
    
    * cpu_idle() didn't call boot_init_stack_canary()
    
    * stack canary switching in switch_to() was being done too late making
      the initial run of a new thread use the old stack canary value.
    
    Fix all of them and while at it update comment in cpu_idle() about
    calling boot_init_stack_canary().
    
    Reported-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 8eb169e45584..836ef6575f01 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -120,12 +120,11 @@ void cpu_idle(void)
 	current_thread_info()->status |= TS_POLLING;
 
 	/*
-	 * If we're the non-boot CPU, nothing set the PDA stack
-	 * canary up for us - and if we are the boot CPU we have
-	 * a 0 stack canary. This is a good place for updating
-	 * it, as we wont ever return from this function (so the
-	 * invalid canaries already on the stack wont ever
-	 * trigger):
+	 * If we're the non-boot CPU, nothing set the stack canary up
+	 * for us.  CPU0 already has it initialized but no harm in
+	 * doing it again.  This is a good place for updating it, as
+	 * we wont ever return from this function (so the invalid
+	 * canaries already on the stack wont ever trigger).
 	 */
 	boot_init_stack_canary();
 

commit 92e2d508464b6293ad274fb606f766a458894142
Merge: 5d96218b4a5e d315760ffa26
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 10 00:41:02 2009 +0100

    Merge branch 'x86/urgent' into core/percpu
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c

commit 48ec4d9537282a55d602136724f069faafcac8c8
Author: Kyle McMartin <kyle@infradead.org>
Date:   Wed Feb 4 15:54:45 2009 -0500

    x86, 64-bit: print DMI info in the oops trace
    
    This patch echoes what we already do on 32-bit since
    90f7d25c6b672137344f447a30a9159945ffea72, and prints the DMI
    product name in show_regs, so that system specific problems can be
    easily identified.
    
    Signed-off-by: Kyle McMartin <kyle@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 416fb9282f4f..85b4cb5c1980 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -40,6 +40,7 @@
 #include <linux/uaccess.h>
 #include <linux/io.h>
 #include <linux/ftrace.h>
+#include <linux/dmi.h>
 
 #include <asm/pgtable.h>
 #include <asm/system.h>
@@ -151,14 +152,18 @@ void __show_regs(struct pt_regs *regs, int all)
 	unsigned long d0, d1, d2, d3, d6, d7;
 	unsigned int fsindex, gsindex;
 	unsigned int ds, cs, es;
+	const char *board;
 
 	printk("\n");
 	print_modules();
-	printk(KERN_INFO "Pid: %d, comm: %.20s %s %s %.*s\n",
+	board = dmi_get_system_info(DMI_PRODUCT_NAME);
+	if (!board)
+		board = "";
+	printk(KERN_INFO "Pid: %d, comm: %.20s %s %s %.*s %s\n",
 		current->pid, current->comm, print_tainted(),
 		init_utsname()->release,
 		(int)strcspn(init_utsname()->version, " "),
-		init_utsname()->version);
+		init_utsname()->version, board);
 	printk(KERN_INFO "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
 	printk_address(regs->ip, 1);
 	printk(KERN_INFO "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,

commit 0d974d4592708f85044751817da4b7016e1b0602
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sun Jan 18 19:52:25 2009 -0500

    x86: remove pda.h
    
    Impact: cleanup
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 088bc9a0f82c..c422eebb0c58 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -47,7 +47,6 @@
 #include <asm/processor.h>
 #include <asm/i387.h>
 #include <asm/mmu_context.h>
-#include <asm/pda.h>
 #include <asm/prctl.h>
 #include <asm/desc.h>
 #include <asm/proto.h>

commit c6e50f93db5bd0895ec7c7d1b6f3886c6e1f11b6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 20 12:29:19 2009 +0900

    x86: cleanup stack protector
    
    Impact: cleanup
    
    Make the following cleanups.
    
    * remove duplicate comment from boot_init_stack_canary() which fits
      better in the other place - cpu_idle().
    
    * move stack_canary offset check from __switch_to() to
      boot_init_stack_canary().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index aa89eabf09e0..088bc9a0f82c 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -638,13 +638,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	percpu_write(kernel_stack,
 		  (unsigned long)task_stack_page(next_p) +
 		  THREAD_SIZE - KERNEL_STACK_OFFSET);
-#ifdef CONFIG_CC_STACKPROTECTOR
-	/*
-	 * Build time only check to make sure the stack_canary is at
-	 * offset 40 in the pda; this is a gcc ABI requirement
-	 */
-	BUILD_BUG_ON(offsetof(struct x8664_pda, stack_canary) != 40);
-#endif
 
 	/*
 	 * Now maybe reload the debug registers and handle I/O bitmaps

commit b2b062b8163391c42b3219d466ca1ac9742b9c7b
Merge: a9de18eb761f 99937d6455ce
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 18 18:37:14 2009 +0100

    Merge branch 'core/percpu' into stackprotector
    
    Conflicts:
            arch/x86/include/asm/pda.h
            arch/x86/include/asm/system.h
    
    Also, moved include/asm-x86/stackprotector.h to arch/x86/include/asm.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit c2558e0eba66b49993e619da66c95a50a97830a3
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:59 2009 +0900

    x86-64: Move isidle from PDA to per-cpu.
    
    tj: s/isidle/is_idle/
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 480128918926..4523ff88a69d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -61,6 +61,7 @@ DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
 
 DEFINE_PER_CPU(unsigned long, old_rsp);
+static DEFINE_PER_CPU(unsigned char, is_idle);
 
 unsigned long kernel_thread_flags = CLONE_VM | CLONE_UNTRACED;
 
@@ -80,13 +81,13 @@ EXPORT_SYMBOL_GPL(idle_notifier_unregister);
 
 void enter_idle(void)
 {
-	write_pda(isidle, 1);
+	percpu_write(is_idle, 1);
 	atomic_notifier_call_chain(&idle_notifier, IDLE_START, NULL);
 }
 
 static void __exit_idle(void)
 {
-	if (test_and_clear_bit_pda(0, isidle) == 0)
+	if (x86_test_and_clear_bit_percpu(0, is_idle) == 0)
 		return;
 	atomic_notifier_call_chain(&idle_notifier, IDLE_END, NULL);
 }

commit 3d1e42a7cf945e289d6ba26159aa0e2b0645401b
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Move oldrsp from PDA to per-cpu.
    
    tj: * in asm-offsets_64.c, pda.h inclusion shouldn't be removed as pda
          is still referenced in the file
        * s/oldrsp/old_rsp/
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6c5f57602108..480128918926 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -60,6 +60,8 @@ asmlinkage extern void ret_from_fork(void);
 DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
 
+DEFINE_PER_CPU(unsigned long, old_rsp);
+
 unsigned long kernel_thread_flags = CLONE_VM | CLONE_UNTRACED;
 
 static ATOMIC_NOTIFIER_HEAD(idle_notifier);
@@ -395,7 +397,7 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 	load_gs_index(0);
 	regs->ip		= new_ip;
 	regs->sp		= new_sp;
-	write_pda(oldrsp, new_sp);
+	percpu_write(old_rsp, new_sp);
 	regs->cs		= __USER_CS;
 	regs->ss		= __USER_DS;
 	regs->flags		= 0x200;
@@ -616,8 +618,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/*
 	 * Switch the PDA and FPU contexts.
 	 */
-	prev->usersp = read_pda(oldrsp);
-	write_pda(oldrsp, next->usersp);
+	prev->usersp = percpu_read(old_rsp);
+	percpu_write(old_rsp, next->usersp);
 	percpu_write(current_task, next_p);
 
 	percpu_write(kernel_stack,

commit 9af45651f1f7c89942e016a1a00a7ebddfa727f8
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Move kernelstack from PDA to per-cpu.
    
    Also clean up PER_CPU_VAR usage in xen-asm_64.S
    
    tj: * remove now unused stack_thread_info()
        * s/kernelstack/kernel_stack/
        * added FIXME comment in xen-asm_64.S
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e00c31a4b3c0..6c5f57602108 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -620,9 +620,9 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	write_pda(oldrsp, next->usersp);
 	percpu_write(current_task, next_p);
 
-	write_pda(kernelstack,
+	percpu_write(kernel_stack,
 		  (unsigned long)task_stack_page(next_p) +
-		  THREAD_SIZE - PDA_STACKOFFSET);
+		  THREAD_SIZE - KERNEL_STACK_OFFSET);
 #ifdef CONFIG_CC_STACKPROTECTOR
 	write_pda(stack_canary, next_p->stack_canary);
 	/*

commit c6f5e0acd5d12ee23f701f15889872e67b47caa6
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Move current task from PDA to per-cpu and consolidate with 32-bit.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 416fb9282f4f..e00c31a4b3c0 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -57,6 +57,9 @@
 
 asmlinkage extern void ret_from_fork(void);
 
+DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
+EXPORT_PER_CPU_SYMBOL(current_task);
+
 unsigned long kernel_thread_flags = CLONE_VM | CLONE_UNTRACED;
 
 static ATOMIC_NOTIFIER_HEAD(idle_notifier);
@@ -615,7 +618,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 */
 	prev->usersp = read_pda(oldrsp);
 	write_pda(oldrsp, next->usersp);
-	write_pda(pcurrent, next_p);
+	percpu_write(current_task, next_p);
 
 	write_pda(kernelstack,
 		  (unsigned long)task_stack_page(next_p) +

commit a9de18eb761f7c1c860964b2e5addc1a35c7e861
Merge: b2aaf8f74cdc 6a94cb73064c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 31 08:31:57 2008 +0100

    Merge branch 'linus' into stackprotector
    
    Conflicts:
            arch/x86/include/asm/pda.h
            kernel/fork.c

commit bf53de907dfdaac178c92d774aae7370d7b97d20
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Dec 19 15:10:24 2008 +0100

    x86, bts: add fork and exit handling
    
    Impact: introduce new ptrace facility
    
    Add arch_ptrace_untrace() function that is called when the tracer
    detaches (either voluntarily or when the tracing task dies);
    ptrace_disable() is only called on a voluntary detach.
    
    Add ptrace_fork() and arch_ptrace_fork(). They are called when a
    traced task is forked.
    
    Clear DS and BTS related fields on fork.
    
    Release DS resources and reclaim memory in ptrace_untrace(). This
    releases resources already when the tracing task dies. We used to do
    that when the traced task dies.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 1cfd2a4bf853..416fb9282f4f 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -53,6 +53,7 @@
 #include <asm/ia32.h>
 #include <asm/idle.h>
 #include <asm/syscalls.h>
+#include <asm/ds.h>
 
 asmlinkage extern void ret_from_fork(void);
 
@@ -236,17 +237,8 @@ void exit_thread(void)
 		t->io_bitmap_max = 0;
 		put_cpu();
 	}
-#ifdef CONFIG_X86_DS
-	/* Free any BTS tracers that have not been properly released. */
-	if (unlikely(current->bts)) {
-		ds_release_bts(current->bts);
-		current->bts = NULL;
-
-		kfree(current->bts_buffer);
-		current->bts_buffer = NULL;
-		current->bts_size = 0;
-	}
-#endif /* CONFIG_X86_DS */
+
+	ds_exit_thread(current);
 }
 
 void flush_thread(void)
@@ -376,6 +368,12 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
 		if (err)
 			goto out;
 	}
+
+	ds_copy_thread(p, me);
+
+	clear_tsk_thread_flag(p, TIF_DEBUGCTLMSR);
+	p->thread.debugctlmsr = 0;
+
 	err = 0;
 out:
 	if (err && p->thread.io_bitmap_ptr) {

commit c2724775ce57c98b8af9694857b941dc61056516
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Thu Dec 11 13:49:59 2008 +0100

    x86, bts: provide in-kernel branch-trace interface
    
    Impact: cleanup
    
    Move the BTS bits from ptrace.c into ds.c.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index fbb321d53d34..1cfd2a4bf853 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -237,11 +237,14 @@ void exit_thread(void)
 		put_cpu();
 	}
 #ifdef CONFIG_X86_DS
-	/* Free any DS contexts that have not been properly released. */
-	if (unlikely(t->ds_ctx)) {
-		/* we clear debugctl to make sure DS is not used. */
-		update_debugctlmsr(0);
-		ds_free(t->ds_ctx);
+	/* Free any BTS tracers that have not been properly released. */
+	if (unlikely(current->bts)) {
+		ds_release_bts(current->bts);
+		current->bts = NULL;
+
+		kfree(current->bts_buffer);
+		current->bts_buffer = NULL;
+		current->bts_size = 0;
 	}
 #endif /* CONFIG_X86_DS */
 }
@@ -471,35 +474,14 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 				    struct tss_struct *tss)
 {
 	struct thread_struct *prev, *next;
-	unsigned long debugctl;
 
 	prev = &prev_p->thread,
 	next = &next_p->thread;
 
-	debugctl = prev->debugctlmsr;
-
-#ifdef CONFIG_X86_DS
-	{
-		unsigned long ds_prev = 0, ds_next = 0;
-
-		if (prev->ds_ctx)
-			ds_prev = (unsigned long)prev->ds_ctx->ds;
-		if (next->ds_ctx)
-			ds_next = (unsigned long)next->ds_ctx->ds;
-
-		if (ds_next != ds_prev) {
-			/*
-			 * We clear debugctl to make sure DS
-			 * is not in use when we change it:
-			 */
-			debugctl = 0;
-			update_debugctlmsr(0);
-			wrmsrl(MSR_IA32_DS_AREA, ds_next);
-		}
-	}
-#endif /* CONFIG_X86_DS */
-
-	if (next->debugctlmsr != debugctl)
+	if (test_tsk_thread_flag(next_p, TIF_DS_AREA_MSR) ||
+	    test_tsk_thread_flag(prev_p, TIF_DS_AREA_MSR))
+		ds_switch_to(prev_p, next_p);
+	else if (next->debugctlmsr != prev->debugctlmsr)
 		update_debugctlmsr(next->debugctlmsr);
 
 	if (test_tsk_thread_flag(next_p, TIF_DEBUG)) {
@@ -534,14 +516,6 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 		 */
 		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
 	}
-
-#ifdef CONFIG_X86_PTRACE_BTS
-	if (test_tsk_thread_flag(prev_p, TIF_BTS_TRACE_TS))
-		ptrace_bts_take_timestamp(prev_p, BTS_TASK_DEPARTS);
-
-	if (test_tsk_thread_flag(next_p, TIF_BTS_TRACE_TS))
-		ptrace_bts_take_timestamp(next_p, BTS_TASK_ARRIVES);
-#endif /* CONFIG_X86_PTRACE_BTS */
 }
 
 /*

commit 8b96f0119818964e4944fd1c423bf6770027d3ac
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Dec 6 03:40:00 2008 +0100

    tracing/function-graph-tracer: introduce __notrace_funcgraph to filter special functions
    
    Impact: trace more functions
    
    When the function graph tracer is configured, three more files are not
    traced to prevent only four functions to be traced. And this impacts the
    normal function tracer too.
    
    arch/x86/kernel/process_64/32.c:
    
    I had crashes when I let this file traced. After some debugging, I saw
    that the "current" task point was changed inside__swtich_to(), ie:
    "write_pda(pcurrent, next_p);" inside process_64.c Since the tracer store
    the original return address of the function inside current, we had
    crashes. Only __switch_to() has to be excluded from tracing.
    
    kernel/module.c and kernel/extable.c:
    
    Because of a function used internally by the function graph tracer:
    __kernel_text_address()
    
    To let the other functions inside these files to be traced, this patch
    introduces the __notrace_funcgraph function prefix which is __notrace if
    function graph tracer is configured and nothing if not.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index c958120fb1b6..fbb321d53d34 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -39,6 +39,7 @@
 #include <linux/prctl.h>
 #include <linux/uaccess.h>
 #include <linux/io.h>
+#include <linux/ftrace.h>
 
 #include <asm/pgtable.h>
 #include <asm/system.h>
@@ -551,8 +552,9 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
  * - could test fs/gs bitsliced
  *
  * Kprobes not supported here. Set the probe on schedule instead.
+ * Function graph tracer not supported too.
  */
-struct task_struct *
+__notrace_funcgraph struct task_struct *
 __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 {
 	struct thread_struct *prev = &prev_p->thread;

commit 057316cc6a5b521b332a1d7ccc871cd60c904c74
Merge: 3e2dab9a1c2d 2515ddc6db8e
Author: Len Brown <len.brown@intel.com>
Date:   Wed Oct 22 23:57:26 2008 -0400

    Merge branch 'linus' into test
    
    Conflicts:
            MAINTAINERS
            arch/x86/kernel/acpi/boot.c
            arch/x86/kernel/acpi/sleep.c
            drivers/acpi/Kconfig
            drivers/pnp/Makefile
            drivers/pnp/quirks.c
    
    Signed-off-by: Len Brown <len.brown@intel.com>

commit c7d87d79d14cecab7a34dedf1b133377cf5a0203
Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Thu Oct 16 16:34:43 2008 -0400

    x86 allow modules to register idle notifiers
    
    needed if the i7300_idle driver is to be modular.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e12e0e4dd256..3e3d503eadcf 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -62,6 +62,13 @@ void idle_notifier_register(struct notifier_block *n)
 {
 	atomic_notifier_chain_register(&idle_notifier, n);
 }
+EXPORT_SYMBOL_GPL(idle_notifier_register);
+
+void idle_notifier_unregister(struct notifier_block *n)
+{
+	atomic_notifier_chain_unregister(&idle_notifier, n);
+}
+EXPORT_SYMBOL_GPL(idle_notifier_unregister);
 
 void enter_idle(void)
 {

commit b2aaf8f74cdc84a9182f6cabf198b7763bcb9d40
Merge: 4f962d4d6592 278429cff880
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Oct 15 13:46:29 2008 +0200

    Merge branch 'linus' into stackprotector
    
    Conflicts:
            arch/x86/kernel/Makefile
            include/asm-x86/pda.h

commit e2ce07c8042975e52df4cec1f41faf15b83f2e42
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Thu Apr 3 16:40:48 2008 +0300

    x86: __show_registers() and __show_regs() API unification
    
    Currently the low-level function to dump user-passed registers on i386 is
    called __show_registers() whereas on x86-64 it's called __show_regs(). Unify
    the API to simplify porting of kmemcheck to x86-64.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Vegard Nossum <vegard.nossum@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ca80394ef5b8..cd8c0ed02b7e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -136,7 +136,7 @@ void cpu_idle(void)
 }
 
 /* Prints also some state that isn't saved in the pt_regs */
-void __show_regs(struct pt_regs *regs)
+void __show_regs(struct pt_regs *regs, int all)
 {
 	unsigned long cr0 = 0L, cr2 = 0L, cr3 = 0L, cr4 = 0L, fs, gs, shadowgs;
 	unsigned long d0, d1, d2, d3, d6, d7;
@@ -175,6 +175,9 @@ void __show_regs(struct pt_regs *regs)
 	rdmsrl(MSR_GS_BASE, gs);
 	rdmsrl(MSR_KERNEL_GS_BASE, shadowgs);
 
+	if (!all)
+		return;
+
 	cr0 = read_cr0();
 	cr2 = read_cr2();
 	cr3 = read_cr3();
@@ -200,7 +203,7 @@ void __show_regs(struct pt_regs *regs)
 void show_regs(struct pt_regs *regs)
 {
 	printk(KERN_INFO "CPU %d:", smp_processor_id());
-	__show_regs(regs);
+	__show_regs(regs, 1);
 	show_trace(NULL, regs, (void *)(regs + 1), regs->bp);
 }
 

commit eceb1383361c6327cef4de01d278cd6722ebceeb
Merge: 365d46dc9be9 84e9c95ad92f 4c7145a1ec1b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Oct 12 13:20:25 2008 +0200

    Merge branches 'core/signal' and 'x86/spinlocks' into x86/xen
    
    Conflicts:
            include/asm-x86/spinlock.h

commit 365d46dc9be9b3c833990a06f3994b1987eda578
Merge: 5dc64a3442b9 fd0480883066
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Oct 12 12:35:23 2008 +0200

    Merge branch 'linus' into x86/xen
    
    Conflicts:
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/process_64.c
            arch/x86/xen/enlighten.c

commit e1e23bb0513520035ec934fa3483507cb6648b7c
Author: David Rientjes <rientjes@google.com>
Date:   Tue Oct 7 14:15:11 2008 -0700

    x86: avoid dereferencing beyond stack + THREAD_SIZE
    
    It's possible for get_wchan() to dereference past task->stack + THREAD_SIZE
    while iterating through instruction pointers if fp equals the upper boundary,
    causing a kernel panic.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 2a8ccb9238b4..b6b508ea7110 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -754,12 +754,12 @@ unsigned long get_wchan(struct task_struct *p)
 	if (!p || p == current || p->state == TASK_RUNNING)
 		return 0;
 	stack = (unsigned long)task_stack_page(p);
-	if (p->thread.sp < stack || p->thread.sp > stack+THREAD_SIZE)
+	if (p->thread.sp < stack || p->thread.sp >= stack+THREAD_SIZE)
 		return 0;
 	fp = *(u64 *)(p->thread.sp);
 	do {
 		if (fp < (unsigned long)stack ||
-		    fp > (unsigned long)stack+THREAD_SIZE)
+		    fp >= (unsigned long)stack+THREAD_SIZE)
 			return 0;
 		ip = *(u64 *)(fp+8);
 		if (!in_sched_functions(ip))

commit e496e3d645c93206faf61ff6005995ebd08cc39c
Merge: b159d7a989e5 5bbd4c372400 175e438f7a2d 516cbf3730c4 af2d237bf574 9b1568458a3e 5b7e41ff3726 1befdefcf476 a03352d2c1dc 7b22ff5344fd 2c7e9fd4c6cb 91030ca1e739 dd5523552c28 b3e15bdef689 20211e4d3447 efd327a2d412 c7ffa6c26277 e51a1ac2dfca 5df455155124 d99e90164e6c e621bd18958e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 6 18:17:07 2008 +0200

    Merge branches 'x86/alternatives', 'x86/cleanups', 'x86/commandline', 'x86/crashdump', 'x86/debug', 'x86/defconfig', 'x86/doc', 'x86/exports', 'x86/fpu', 'x86/gart', 'x86/idle', 'x86/mm', 'x86/mtrr', 'x86/nmi-watchdog', 'x86/oprofile', 'x86/paravirt', 'x86/reboot', 'x86/sparse-fixes', 'x86/tsc', 'x86/urgent' and 'x86/vmalloc' into x86-v28-for-linus-phase1

commit 0962f402af1bb0b53ccee626785d202a10c12fff
Merge: 19268ed7449c 8d7ccaa54549
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 6 16:18:26 2008 +0200

    Merge branch 'x86/prototypes' into x86-v28-for-linus-phase1
    
    Conflicts:
            arch/x86/kernel/process_32.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 19268ed7449c561694d048a34601a30e2d1aaf79
Merge: b8cd9d056bbc 493cd9122af5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 6 16:17:23 2008 +0200

    Merge branch 'x86/pebs' into x86-v28-for-linus-phase1
    
    Conflicts:
            include/asm-x86/ds.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ebdd90a8cb2e3963f55499850f02ce6003558b55
Merge: 3c9339049df5 72d31053f62c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Sep 24 09:56:20 2008 +0200

    Merge commit 'v2.6.27-rc7' into x86/pebs

commit 4faac97d44ac27bdbb010a9c3597401a8f89341f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 22 18:54:29 2008 +0200

    x86: prevent stale state of c1e_mask across CPU offline/online
    
    Impact: hang which happens across CPU offline/online on AMD C1E systems.
    
    When a CPU goes offline then the corresponding bit in the broadcast
    mask is cleared. For AMD C1E enabled CPUs we do not reenable the
    broadcast when the CPU comes online again as we do not clear the
    corresponding bit in the c1e_mask, which keeps track which CPUs
    have been switched to broadcast already. So on those !$@#& machines
    we never switch back to broadcasting after a CPU offline/online cycle.
    
    Clear the bit when the CPU plays dead.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 71553b664e2a..e12e0e4dd256 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -93,6 +93,8 @@ DECLARE_PER_CPU(int, cpu_state);
 static inline void play_dead(void)
 {
 	idle_task_exit();
+	c1e_remove_cpu(raw_smp_processor_id());
+
 	mb();
 	/* Ack it */
 	__get_cpu_var(cpu_state) = CPU_DEAD;

commit 913da64b54b2b3bb212a59aba2e6f2b8294ca1fa
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Wed Sep 3 14:30:23 2008 +0100

    x86: build fix for !CONFIG_SMP
    
    Move reset_lazy_tlbstate into tlb_32.c, and define noop versions of
    play_dead() in process_{32,64}.c when !CONFIG_SMP.
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index aa28ed01cdf3..ec27afa43d7e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -85,6 +85,13 @@ void exit_idle(void)
 	__exit_idle();
 }
 
+#ifndef CONFIG_SMP
+static inline void play_dead(void)
+{
+	BUG();
+}
+#endif
+
 /*
  * The idle thread. There's no useful work to be
  * done, so just try to conserve power and have a

commit a21f5d88c17a40941f6239d1959d89e8493e8e01
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Fri Aug 22 11:52:13 2008 +0100

    x86: unify x86_32 and x86_64 play_dead into one function
    
    Add the new play_dead into smpboot.c, as it fits more cleanly in there
    alongside other CONFIG_HOTPLUG functions.
    
    Separate out the common code into its own function.
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index dfd3f5752085..aa28ed01cdf3 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -85,29 +85,6 @@ void exit_idle(void)
 	__exit_idle();
 }
 
-#ifdef CONFIG_HOTPLUG_CPU
-DECLARE_PER_CPU(int, cpu_state);
-
-#include <asm/nmi.h>
-/* We halt the CPU with physical CPU hotplug */
-void native_play_dead(void)
-{
-	idle_task_exit();
-	mb();
-	/* Ack it */
-	__get_cpu_var(cpu_state) = CPU_DEAD;
-
-	local_irq_disable();
-	/* mask all interrupts, flush any and all caches, and halt */
-	wbinvd_halt();
-}
-#else
-void native_play_dead(void)
-{
-	BUG();
-}
-#endif /* CONFIG_HOTPLUG_CPU */
-
 /*
  * The idle thread. There's no useful work to be
  * done, so just try to conserve power and have a

commit 93be71b672f167b1e8c23725114f86305354f0ac
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Fri Aug 22 11:52:11 2008 +0100

    x86: add cpu hotplug hooks into smp_ops
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 71553b664e2a..dfd3f5752085 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -90,7 +90,7 @@ DECLARE_PER_CPU(int, cpu_state);
 
 #include <asm/nmi.h>
 /* We halt the CPU with physical CPU hotplug */
-static inline void play_dead(void)
+void native_play_dead(void)
 {
 	idle_task_exit();
 	mb();
@@ -102,7 +102,7 @@ static inline void play_dead(void)
 	wbinvd_halt();
 }
 #else
-static inline void play_dead(void)
+void native_play_dead(void)
 {
 	BUG();
 }

commit 7393423dd9b5790a3115873be355e9fc862bce8f
Merge: 8df9676d6402 1fca25427482
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Aug 20 11:52:15 2008 +0200

    Merge branch 'linus' into x86/cleanups

commit 394a15051c33f2b18e72f42283b36a9388fa414b
Author: Mark Langsdorf <mark.langsdorf@amd.com>
Date:   Thu Aug 14 09:11:26 2008 -0500

    x86: invalidate caches before going into suspend
    
    When a CPU core is shut down, all of its caches need to be flushed
    to prevent stale data from causing errors if the core is resumed.
    Current Linux suspend code performs an assignment after the flush,
    which can add dirty data back to the cache.  On some AMD platforms,
    additional speculative reads have caused crashes on resume because
    of this dirty data.
    
    Relocate the cache flush to be the very last thing done before
    halting.  Tie into an assembly line so the compile will not
    reorder it.  Add some documentation explaining what is going
    on and why we're doing this.
    
    Signed-off-by: Mark Langsdorf <mark.langsdorf@amd.com>
    Acked-by: Mark Borden <mark.borden@amd.com>
    Acked-by: Michael Hohmuth <michael.hohmuth@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 3fb62a7d9a16..71553b664e2a 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -93,14 +93,13 @@ DECLARE_PER_CPU(int, cpu_state);
 static inline void play_dead(void)
 {
 	idle_task_exit();
-	wbinvd();
 	mb();
 	/* Ack it */
 	__get_cpu_var(cpu_state) = CPU_DEAD;
 
 	local_irq_disable();
-	while (1)
-		halt();
+	/* mask all interrupts, flush any and all caches, and halt */
+	wbinvd_halt();
 }
 #else
 static inline void play_dead(void)

commit 8d7ccaa545490cdffdfaff0842436a8dd85cf47b
Merge: b2139aa0eec3 30a2f3c60a84
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 14 12:19:59 2008 +0200

    Merge commit 'v2.6.27-rc3' into x86/prototypes
    
    Conflicts:
    
            include/asm-x86/dma-mapping.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 8092c654de9a964c14d89da56834f73a80548a58
Author: Gustavo F. Padovan <gustavo@las.ic.unicamp.br>
Date:   Tue Jul 29 02:48:52 2008 -0300

    x86: add KERN_INFO to printks on process_64.c
    
    Fix many coding style warnings.
    
    Signed-off-by: Gustavo F. Padovan <gustavo@las.ic.unicamp.br>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4da8514dd25c..3560d7f4d74e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -161,24 +161,24 @@ void __show_regs(struct pt_regs *regs)
 
 	printk("\n");
 	print_modules();
-	printk("Pid: %d, comm: %.20s %s %s %.*s\n",
+	printk(KERN_INFO "Pid: %d, comm: %.20s %s %s %.*s\n",
 		current->pid, current->comm, print_tainted(),
 		init_utsname()->release,
 		(int)strcspn(init_utsname()->version, " "),
 		init_utsname()->version);
-	printk("RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
+	printk(KERN_INFO "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
 	printk_address(regs->ip, 1);
-	printk("RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss, regs->sp,
-		regs->flags);
-	printk("RAX: %016lx RBX: %016lx RCX: %016lx\n",
+	printk(KERN_INFO "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,
+			regs->sp, regs->flags);
+	printk(KERN_INFO "RAX: %016lx RBX: %016lx RCX: %016lx\n",
 	       regs->ax, regs->bx, regs->cx);
-	printk("RDX: %016lx RSI: %016lx RDI: %016lx\n",
+	printk(KERN_INFO "RDX: %016lx RSI: %016lx RDI: %016lx\n",
 	       regs->dx, regs->si, regs->di);
-	printk("RBP: %016lx R08: %016lx R09: %016lx\n",
+	printk(KERN_INFO "RBP: %016lx R08: %016lx R09: %016lx\n",
 	       regs->bp, regs->r8, regs->r9);
-	printk("R10: %016lx R11: %016lx R12: %016lx\n",
+	printk(KERN_INFO "R10: %016lx R11: %016lx R12: %016lx\n",
 	       regs->r10, regs->r11, regs->r12);
-	printk("R13: %016lx R14: %016lx R15: %016lx\n",
+	printk(KERN_INFO "R13: %016lx R14: %016lx R15: %016lx\n",
 	       regs->r13, regs->r14, regs->r15);
 
 	asm("movl %%ds,%0" : "=r" (ds));
@@ -196,24 +196,26 @@ void __show_regs(struct pt_regs *regs)
 	cr3 = read_cr3();
 	cr4 = read_cr4();
 
-	printk("FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n",
+	printk(KERN_INFO "FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n",
 	       fs, fsindex, gs, gsindex, shadowgs);
-	printk("CS:  %04x DS: %04x ES: %04x CR0: %016lx\n", cs, ds, es, cr0);
-	printk("CR2: %016lx CR3: %016lx CR4: %016lx\n", cr2, cr3, cr4);
+	printk(KERN_INFO "CS:  %04x DS: %04x ES: %04x CR0: %016lx\n", cs, ds,
+			es, cr0);
+	printk(KERN_INFO "CR2: %016lx CR3: %016lx CR4: %016lx\n", cr2, cr3,
+			cr4);
 
 	get_debugreg(d0, 0);
 	get_debugreg(d1, 1);
 	get_debugreg(d2, 2);
-	printk("DR0: %016lx DR1: %016lx DR2: %016lx\n", d0, d1, d2);
+	printk(KERN_INFO "DR0: %016lx DR1: %016lx DR2: %016lx\n", d0, d1, d2);
 	get_debugreg(d3, 3);
 	get_debugreg(d6, 6);
 	get_debugreg(d7, 7);
-	printk("DR3: %016lx DR6: %016lx DR7: %016lx\n", d3, d6, d7);
+	printk(KERN_INFO "DR3: %016lx DR6: %016lx DR7: %016lx\n", d3, d6, d7);
 }
 
 void show_regs(struct pt_regs *regs)
 {
-	printk("CPU %d:", smp_processor_id());
+	printk(KERN_INFO "CPU %d:", smp_processor_id());
 	__show_regs(regs);
 	show_trace(NULL, regs, (void *)(regs + 1), regs->bp);
 }

commit 7de08b4e1ed8d80e6086f71b7e99fc4b397aae39
Author: Gustavo F. Padovan <gustavo@las.ic.unicamp.br>
Date:   Tue Jul 29 02:48:51 2008 -0300

    x86: coding styles fixes to arch/x86/kernel/process_64.c
    
    Fix about 50 errors and many warnings without change process_64.o
    
    arch/x86/kernel/process_64.o:
    text    data     bss     dec     hex filename
    5236       8      24    5268    1494 process_64.o.after
    5236       8      24    5268    1494 process_64.o.before
    md5:
    9c35e9debdea4e471288c6e8ca267a75  process_64.o.after
    9c35e9debdea4e471288c6e8ca267a75  process_64.o.before
    
    Signed-off-by: Gustavo F. Padovan <gustavo@las.ic.unicamp.br>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 3fb62a7d9a16..4da8514dd25c 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -37,11 +37,11 @@
 #include <linux/kdebug.h>
 #include <linux/tick.h>
 #include <linux/prctl.h>
+#include <linux/uaccess.h>
+#include <linux/io.h>
 
-#include <asm/uaccess.h>
 #include <asm/pgtable.h>
 #include <asm/system.h>
-#include <asm/io.h>
 #include <asm/processor.h>
 #include <asm/i387.h>
 #include <asm/mmu_context.h>
@@ -88,7 +88,7 @@ void exit_idle(void)
 #ifdef CONFIG_HOTPLUG_CPU
 DECLARE_PER_CPU(int, cpu_state);
 
-#include <asm/nmi.h>
+#include <linux/nmi.h>
 /* We halt the CPU with physical CPU hotplug */
 static inline void play_dead(void)
 {
@@ -152,7 +152,7 @@ void cpu_idle(void)
 }
 
 /* Prints also some state that isn't saved in the pt_regs */
-void __show_regs(struct pt_regs * regs)
+void __show_regs(struct pt_regs *regs)
 {
 	unsigned long cr0 = 0L, cr2 = 0L, cr3 = 0L, cr4 = 0L, fs, gs, shadowgs;
 	unsigned long d0, d1, d2, d3, d6, d7;
@@ -177,28 +177,28 @@ void __show_regs(struct pt_regs * regs)
 	printk("RBP: %016lx R08: %016lx R09: %016lx\n",
 	       regs->bp, regs->r8, regs->r9);
 	printk("R10: %016lx R11: %016lx R12: %016lx\n",
-	       regs->r10, regs->r11, regs->r12); 
+	       regs->r10, regs->r11, regs->r12);
 	printk("R13: %016lx R14: %016lx R15: %016lx\n",
-	       regs->r13, regs->r14, regs->r15); 
+	       regs->r13, regs->r14, regs->r15);
 
-	asm("movl %%ds,%0" : "=r" (ds)); 
-	asm("movl %%cs,%0" : "=r" (cs)); 
-	asm("movl %%es,%0" : "=r" (es)); 
+	asm("movl %%ds,%0" : "=r" (ds));
+	asm("movl %%cs,%0" : "=r" (cs));
+	asm("movl %%es,%0" : "=r" (es));
 	asm("movl %%fs,%0" : "=r" (fsindex));
 	asm("movl %%gs,%0" : "=r" (gsindex));
 
 	rdmsrl(MSR_FS_BASE, fs);
-	rdmsrl(MSR_GS_BASE, gs); 
-	rdmsrl(MSR_KERNEL_GS_BASE, shadowgs); 
+	rdmsrl(MSR_GS_BASE, gs);
+	rdmsrl(MSR_KERNEL_GS_BASE, shadowgs);
 
 	cr0 = read_cr0();
 	cr2 = read_cr2();
 	cr3 = read_cr3();
 	cr4 = read_cr4();
 
-	printk("FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n", 
-	       fs,fsindex,gs,gsindex,shadowgs); 
-	printk("CS:  %04x DS: %04x ES: %04x CR0: %016lx\n", cs, ds, es, cr0); 
+	printk("FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n",
+	       fs, fsindex, gs, gsindex, shadowgs);
+	printk("CS:  %04x DS: %04x ES: %04x CR0: %016lx\n", cs, ds, es, cr0);
 	printk("CR2: %016lx CR3: %016lx CR4: %016lx\n", cr2, cr3, cr4);
 
 	get_debugreg(d0, 0);
@@ -314,10 +314,10 @@ void prepare_to_copy(struct task_struct *tsk)
 
 int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
 		unsigned long unused,
-	struct task_struct * p, struct pt_regs * regs)
+	struct task_struct *p, struct pt_regs *regs)
 {
 	int err;
-	struct pt_regs * childregs;
+	struct pt_regs *childregs;
 	struct task_struct *me = current;
 
 	childregs = ((struct pt_regs *)
@@ -362,10 +362,10 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
 		if (test_thread_flag(TIF_IA32))
 			err = do_set_thread_area(p, -1,
 				(struct user_desc __user *)childregs->si, 0);
-		else 			
-#endif	 
-			err = do_arch_prctl(p, ARCH_SET_FS, childregs->r8); 
-		if (err) 
+		else
+#endif
+			err = do_arch_prctl(p, ARCH_SET_FS, childregs->r8);
+		if (err)
 			goto out;
 	}
 	err = 0;
@@ -544,7 +544,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	unsigned fsindex, gsindex;
 
 	/* we're going to use this soon, after a few expensive things */
-	if (next_p->fpu_counter>5)
+	if (next_p->fpu_counter > 5)
 		prefetch(next->xstate);
 
 	/*
@@ -552,13 +552,13 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 */
 	load_sp0(tss, next);
 
-	/* 
+	/*
 	 * Switch DS and ES.
 	 * This won't pick up thread selector changes, but I guess that is ok.
 	 */
 	savesegment(es, prev->es);
 	if (unlikely(next->es | prev->es))
-		loadsegment(es, next->es); 
+		loadsegment(es, next->es);
 
 	savesegment(ds, prev->ds);
 	if (unlikely(next->ds | prev->ds))
@@ -584,7 +584,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 */
 	arch_leave_lazy_cpu_mode();
 
-	/* 
+	/*
 	 * Switch FS and GS.
 	 *
 	 * Segment register != 0 always requires a reload.  Also
@@ -593,13 +593,13 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 */
 	if (unlikely(fsindex | next->fsindex | prev->fs)) {
 		loadsegment(fs, next->fsindex);
-		/* 
+		/*
 		 * Check if the user used a selector != 0; if yes
 		 *  clear 64bit base, since overloaded base is always
 		 *  mapped to the Null selector
 		 */
 		if (fsindex)
-			prev->fs = 0;				
+			prev->fs = 0;
 	}
 	/* when next process has a 64bit base use it */
 	if (next->fs)
@@ -609,7 +609,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	if (unlikely(gsindex | next->gsindex | prev->gs)) {
 		load_gs_index(next->gsindex);
 		if (gsindex)
-			prev->gs = 0;				
+			prev->gs = 0;
 	}
 	if (next->gs)
 		wrmsrl(MSR_KERNEL_GS_BASE, next->gs);
@@ -618,12 +618,12 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/* Must be after DS reload */
 	unlazy_fpu(prev_p);
 
-	/* 
+	/*
 	 * Switch the PDA and FPU contexts.
 	 */
 	prev->usersp = read_pda(oldrsp);
 	write_pda(oldrsp, next->usersp);
-	write_pda(pcurrent, next_p); 
+	write_pda(pcurrent, next_p);
 
 	write_pda(kernelstack,
 		  (unsigned long)task_stack_page(next_p) +
@@ -664,7 +664,7 @@ long sys_execve(char __user *name, char __user * __user *argv,
 		char __user * __user *envp, struct pt_regs *regs)
 {
 	long error;
-	char * filename;
+	char *filename;
 
 	filename = getname(name);
 	error = PTR_ERR(filename);
@@ -722,55 +722,55 @@ asmlinkage long sys_vfork(struct pt_regs *regs)
 unsigned long get_wchan(struct task_struct *p)
 {
 	unsigned long stack;
-	u64 fp,ip;
+	u64 fp, ip;
 	int count = 0;
 
-	if (!p || p == current || p->state==TASK_RUNNING)
-		return 0; 
+	if (!p || p == current || p->state == TASK_RUNNING)
+		return 0;
 	stack = (unsigned long)task_stack_page(p);
 	if (p->thread.sp < stack || p->thread.sp > stack+THREAD_SIZE)
 		return 0;
 	fp = *(u64 *)(p->thread.sp);
-	do { 
+	do {
 		if (fp < (unsigned long)stack ||
 		    fp > (unsigned long)stack+THREAD_SIZE)
-			return 0; 
+			return 0;
 		ip = *(u64 *)(fp+8);
 		if (!in_sched_functions(ip))
 			return ip;
-		fp = *(u64 *)fp; 
-	} while (count++ < 16); 
+		fp = *(u64 *)fp;
+	} while (count++ < 16);
 	return 0;
 }
 
 long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
-{ 
-	int ret = 0; 
+{
+	int ret = 0;
 	int doit = task == current;
 	int cpu;
 
-	switch (code) { 
+	switch (code) {
 	case ARCH_SET_GS:
 		if (addr >= TASK_SIZE_OF(task))
-			return -EPERM; 
+			return -EPERM;
 		cpu = get_cpu();
-		/* handle small bases via the GDT because that's faster to 
+		/* handle small bases via the GDT because that's faster to
 		   switch. */
-		if (addr <= 0xffffffff) {  
-			set_32bit_tls(task, GS_TLS, addr); 
-			if (doit) { 
+		if (addr <= 0xffffffff) {
+			set_32bit_tls(task, GS_TLS, addr);
+			if (doit) {
 				load_TLS(&task->thread, cpu);
-				load_gs_index(GS_TLS_SEL); 
+				load_gs_index(GS_TLS_SEL);
 			}
-			task->thread.gsindex = GS_TLS_SEL; 
+			task->thread.gsindex = GS_TLS_SEL;
 			task->thread.gs = 0;
-		} else { 
+		} else {
 			task->thread.gsindex = 0;
 			task->thread.gs = addr;
 			if (doit) {
 				load_gs_index(0);
 				ret = checking_wrmsrl(MSR_KERNEL_GS_BASE, addr);
-			} 
+			}
 		}
 		put_cpu();
 		break;
@@ -824,8 +824,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 				rdmsrl(MSR_KERNEL_GS_BASE, base);
 			else
 				base = task->thread.gs;
-		}
-		else
+		} else
 			base = task->thread.gs;
 		ret = put_user(base, (unsigned long __user *)addr);
 		break;

commit 0e2f65ee30eee2db054f7fd73f462c5da33ec963
Merge: da7878d75b85 fb2e405fc1fc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 25 11:37:07 2008 +0200

    Merge branch 'linus' into x86/pebs
    
    Conflicts:
    
            arch/x86/Kconfig.cpu
            arch/x86/kernel/cpu/intel.c
            arch/x86/kernel/setup_64.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ecc8b655b38a880b578146895e0e1e2d477ca2c0
Merge: 2528ce3237be e338125b8a88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 24 12:55:01 2008 -0700

    Merge branch 'timers-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      nohz: adjust tick_nohz_stop_sched_tick() call of s390 as well
      nohz: prevent tick stop outside of the idle loop

commit bbc1f698a508927d21324b57500e863f9bd562b9
Author: Jaswinder Singh <jaswinder@infradead.org>
Date:   Mon Jul 21 21:34:13 2008 +0530

    x86: Introducing asm/syscalls.h
    
    Declaring arch-dependent syscalls for x86 architecture
    
    Signed-off-by: Jaswinder Singh <jaswinder@infradead.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e8a8e1b99817..c78090dd0c52 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -51,6 +51,7 @@
 #include <asm/proto.h>
 #include <asm/ia32.h>
 #include <asm/idle.h>
+#include <asm/syscalls.h>
 
 asmlinkage extern void ret_from_fork(void);
 

commit 9b610fda0df5d0f0b0c64242e37441ad1b384aac
Merge: b8f8c3cf0a4a 5b664cb235e9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 18 19:53:16 2008 +0200

    Merge branch 'linus' into timers/nohz

commit b8f8c3cf0a4ac0632ec3f0e15e9dc0c29de917af
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 18 17:27:28 2008 +0200

    nohz: prevent tick stop outside of the idle loop
    
    Jack Ren and Eric Miao tracked down the following long standing
    problem in the NOHZ code:
    
            scheduler switch to idle task
            enable interrupts
    
    Window starts here
    
            ----> interrupt happens (does not set NEED_RESCHED)
                    irq_exit() stops the tick
    
            ----> interrupt happens (does set NEED_RESCHED)
    
            return from schedule()
    
            cpu_idle(): preempt_disable();
    
    Window ends here
    
    The interrupts can happen at any point inside the race window. The
    first interrupt stops the tick, the second one causes the scheduler to
    rerun and switch away from idle again and we end up with the tick
    disabled.
    
    The fact that it needs two interrupts where the first one does not set
    NEED_RESCHED and the second one does made the bug obscure and extremly
    hard to reproduce and analyse. Kudos to Jack and Eric.
    
    Solution: Limit the NOHZ functionality to the idle loop to make sure
    that we can not run into such a situation ever again.
    
    cpu_idle()
    {
            preempt_disable();
    
            while(1) {
                     tick_nohz_stop_sched_tick(1); <- tell NOHZ code that we
                                                      are in the idle loop
    
                     while (!need_resched())
                           halt();
    
                     tick_nohz_restart_sched_tick(); <- disables NOHZ mode
                     preempt_enable_no_resched();
                     schedule();
                     preempt_disable();
            }
    }
    
    In hindsight we should have done this forever, but ...
    
    /me grabs a large brown paperbag.
    
    Debugged-by: Jack Ren <jack.ren@marvell.com>,
    Debugged-by: eric miao <eric.y.miao@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e2319f39988b..c0a5c2a687e6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -148,7 +148,7 @@ void cpu_idle(void)
 	current_thread_info()->status |= TS_POLLING;
 	/* endless idle loop with no priority at all */
 	while (1) {
-		tick_nohz_stop_sched_tick();
+		tick_nohz_stop_sched_tick(1);
 		while (!need_resched()) {
 			void (*idle)(void);
 

commit 87b935a0ef9a1ddf62f2f0c0fc17b10654ff41cd
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:26 2008 -0700

    x86: clean up formatting of __switch_to
    
    process_64.c:__switch_to has some very old strange formatting, some of
    it dating back to pre-git.  Fix it up.
    
    No functional changes.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index a8e53626ac9a..e8a8e1b99817 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -537,8 +537,8 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 struct task_struct *
 __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 {
-	struct thread_struct *prev = &prev_p->thread,
-				 *next = &next_p->thread;
+	struct thread_struct *prev = &prev_p->thread;
+	struct thread_struct *next = &next_p->thread;
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &per_cpu(init_tss, cpu);
 	unsigned fsindex, gsindex;
@@ -586,35 +586,34 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 
 	/* 
 	 * Switch FS and GS.
+	 *
+	 * Segment register != 0 always requires a reload.  Also
+	 * reload when it has changed.  When prev process used 64bit
+	 * base always reload to avoid an information leak.
 	 */
-	{ 
-		/* segment register != 0 always requires a reload. 
-		   also reload when it has changed. 
-		   when prev process used 64bit base always reload
-		   to avoid an information leak. */
-		if (unlikely(fsindex | next->fsindex | prev->fs)) {
-			loadsegment(fs, next->fsindex);
-			/* check if the user used a selector != 0
-	                 * if yes clear 64bit base, since overloaded base
-                         * is always mapped to the Null selector
-                         */
-			if (fsindex)
+	if (unlikely(fsindex | next->fsindex | prev->fs)) {
+		loadsegment(fs, next->fsindex);
+		/* 
+		 * Check if the user used a selector != 0; if yes
+		 *  clear 64bit base, since overloaded base is always
+		 *  mapped to the Null selector
+		 */
+		if (fsindex)
 			prev->fs = 0;				
-		}
-		/* when next process has a 64bit base use it */
-		if (next->fs) 
-			wrmsrl(MSR_FS_BASE, next->fs); 
-		prev->fsindex = fsindex;
-
-		if (unlikely(gsindex | next->gsindex | prev->gs)) {
-			load_gs_index(next->gsindex);
-			if (gsindex)
+	}
+	/* when next process has a 64bit base use it */
+	if (next->fs)
+		wrmsrl(MSR_FS_BASE, next->fs);
+	prev->fsindex = fsindex;
+
+	if (unlikely(gsindex | next->gsindex | prev->gs)) {
+		load_gs_index(next->gsindex);
+		if (gsindex)
 			prev->gs = 0;				
-		}
-		if (next->gs)
-			wrmsrl(MSR_KERNEL_GS_BASE, next->gs); 
-		prev->gsindex = gsindex;
 	}
+	if (next->gs)
+		wrmsrl(MSR_KERNEL_GS_BASE, next->gs);
+	prev->gsindex = gsindex;
 
 	/* Must be after DS reload */
 	unlazy_fpu(prev_p);
@@ -627,7 +626,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	write_pda(pcurrent, next_p); 
 
 	write_pda(kernelstack,
-	(unsigned long)task_stack_page(next_p) + THREAD_SIZE - PDA_STACKOFFSET);
+		  (unsigned long)task_stack_page(next_p) +
+		  THREAD_SIZE - PDA_STACKOFFSET);
 #ifdef CONFIG_CC_STACKPROTECTOR
 	write_pda(stack_canary, next_p->stack_canary);
 	/*

commit 5806b81ac1c0c52665b91723fd4146a4f86e386b
Merge: d14c8a680ccf 6712e299b7dc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 14 16:11:52 2008 +0200

    Merge branch 'auto-ftrace-next' into tracing/for-linus
    
    Conflicts:
    
            arch/x86/kernel/entry_32.S
            arch/x86/kernel/process_32.c
            arch/x86/kernel/process_64.c
            arch/x86/lib/Makefile
            include/asm-x86/irqflags.h
            kernel/Makefile
            kernel/sched.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 478de5a9d691dd0c048ddce62dbec23722515636
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jun 25 00:19:24 2008 -0400

    x86: save %fs and %gs before load_TLS() and arch_leave_lazy_cpu_mode()
    
    We must do this because load_TLS() may need to clear %fs and %gs.
    (e.g. under Xen).
    
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: xen-devel <xen-devel@lists.xensource.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 488eaca47bd8..db5eb963e4df 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -538,6 +538,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 				 *next = &next_p->thread;
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &per_cpu(init_tss, cpu);
+	unsigned fsindex, gsindex;
 
 	/* we're going to use this soon, after a few expensive things */
 	if (next_p->fpu_counter>5)
@@ -560,6 +561,15 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	if (unlikely(next->ds | prev->ds))
 		loadsegment(ds, next->ds);
 
+
+	/* We must save %fs and %gs before load_TLS() because
+	 * %fs and %gs may be cleared by load_TLS().
+	 *
+	 * (e.g. xen_load_tls())
+	 */
+	savesegment(fs, fsindex);
+	savesegment(gs, gsindex);
+
 	load_TLS(next, cpu);
 
 	/*
@@ -575,8 +585,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 * Switch FS and GS.
 	 */
 	{ 
-		unsigned fsindex;
-		savesegment(fs, fsindex);
 		/* segment register != 0 always requires a reload. 
 		   also reload when it has changed. 
 		   when prev process used 64bit base always reload
@@ -594,10 +602,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 		if (next->fs) 
 			wrmsrl(MSR_FS_BASE, next->fs); 
 		prev->fsindex = fsindex;
-	}
-	{ 
-		unsigned gsindex;
-		savesegment(gs, gsindex);
+
 		if (unlikely(gsindex | next->gsindex | prev->gs)) {
 			load_gs_index(next->gsindex);
 			if (gsindex)

commit 3fe0a63efd4437f6438ce5f2708929b1108873b6
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jun 25 00:19:23 2008 -0400

    x86, 64-bit: __switch_to(): move arch_leave_lazy_cpu_mode() to the right place
    
    We must leave lazy mode before switching the %fs and %gs selectors.
    
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: xen-devel <xen-devel@lists.xensource.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ddc6fcc73dc6..488eaca47bd8 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -562,6 +562,15 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 
 	load_TLS(next, cpu);
 
+	/*
+	 * Leave lazy mode, flushing any hypercalls made here.
+	 * This must be done before restoring TLS segments so
+	 * the GDT and LDT are properly updated, and must be
+	 * done before math_state_restore, so the TS bit is up
+	 * to date.
+	 */
+	arch_leave_lazy_cpu_mode();
+
 	/* 
 	 * Switch FS and GS.
 	 */

commit ada857082317e6883cfcf7deb4e0c54d3c447cb0
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jun 25 00:19:00 2008 -0400

    x86: remove open-coded save/load segment operations
    
    This removes a pile of buggy open-coded implementations of savesegment
    and loadsegment.
    
    (They are buggy because they don't have memory barriers to prevent
    them from being reordered with respect to memory accesses.)
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: xen-devel <xen-devel@lists.xensource.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 290183e9731a..ddc6fcc73dc6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -335,10 +335,10 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
 	p->thread.fs = me->thread.fs;
 	p->thread.gs = me->thread.gs;
 
-	asm("mov %%gs,%0" : "=m" (p->thread.gsindex));
-	asm("mov %%fs,%0" : "=m" (p->thread.fsindex));
-	asm("mov %%es,%0" : "=m" (p->thread.es));
-	asm("mov %%ds,%0" : "=m" (p->thread.ds));
+	savesegment(gs, p->thread.gsindex);
+	savesegment(fs, p->thread.fsindex);
+	savesegment(es, p->thread.es);
+	savesegment(ds, p->thread.ds);
 
 	if (unlikely(test_tsk_thread_flag(me, TIF_IO_BITMAP))) {
 		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
@@ -377,7 +377,9 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
 void
 start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 {
-	asm volatile("movl %0, %%fs; movl %0, %%es; movl %0, %%ds" :: "r"(0));
+	loadsegment(fs, 0);
+	loadsegment(es, 0);
+	loadsegment(ds, 0);
 	load_gs_index(0);
 	regs->ip		= new_ip;
 	regs->sp		= new_sp;
@@ -550,11 +552,11 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 * Switch DS and ES.
 	 * This won't pick up thread selector changes, but I guess that is ok.
 	 */
-	asm volatile("mov %%es,%0" : "=m" (prev->es));
+	savesegment(es, prev->es);
 	if (unlikely(next->es | prev->es))
 		loadsegment(es, next->es); 
-	
-	asm volatile ("mov %%ds,%0" : "=m" (prev->ds));
+
+	savesegment(ds, prev->ds);
 	if (unlikely(next->ds | prev->ds))
 		loadsegment(ds, next->ds);
 
@@ -565,7 +567,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 */
 	{ 
 		unsigned fsindex;
-		asm volatile("movl %%fs,%0" : "=r" (fsindex)); 
+		savesegment(fs, fsindex);
 		/* segment register != 0 always requires a reload. 
 		   also reload when it has changed. 
 		   when prev process used 64bit base always reload
@@ -586,7 +588,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	}
 	{ 
 		unsigned gsindex;
-		asm volatile("movl %%gs,%0" : "=r" (gsindex)); 
+		savesegment(gs, gsindex);
 		if (unlikely(gsindex | next->gsindex | prev->gs)) {
 			load_gs_index(next->gsindex);
 			if (gsindex)
@@ -767,7 +769,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 			set_32bit_tls(task, FS_TLS, addr);
 			if (doit) {
 				load_TLS(&task->thread, cpu);
-				asm volatile("movl %0,%%fs" :: "r"(FS_TLS_SEL));
+				loadsegment(fs, FS_TLS_SEL);
 			}
 			task->thread.fsindex = FS_TLS_SEL;
 			task->thread.fs = 0;
@@ -777,7 +779,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 			if (doit) {
 				/* set the selector to 0 to not confuse
 				   __switch_to */
-				asm volatile("movl %0,%%fs" :: "r" (0));
+				loadsegment(fs, 0);
 				ret = checking_wrmsrl(MSR_FS_BASE, addr);
 			}
 		}
@@ -800,7 +802,7 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		if (task->thread.gsindex == GS_TLS_SEL)
 			base = read_32bit_tls(task, GS_TLS);
 		else if (doit) {
-			asm("movl %%gs,%0" : "=r" (gsindex));
+			savesegment(gs, gsindex);
 			if (gsindex)
 				rdmsrl(MSR_KERNEL_GS_BASE, base);
 			else

commit 93022136fff9e6130aa128a5ed8a599e93ac813c
Merge: c49c412a47b5 b7279469d66b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 8 07:47:47 2008 +0200

    Merge commit 'v2.6.26-rc9' into x86/cpu

commit da7878d75b8520c9ae00d27dfbbce546a7bfdfbb
Merge: 0e50a4c6ab94 543cf4cb3fe6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 25 12:32:01 2008 +0200

    Merge branch 'linus' into x86/pebs

commit 5ce001b0e56638c726270d4f9e05d46d4250dfbb
Merge: 7c9f8861e6c9 543cf4cb3fe6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 25 12:27:29 2008 +0200

    Merge branch 'linus' into stackprotector

commit f34bfb1beef8a17ba3d46b60f8fa19ffedc1ed8d
Merge: ee4311adf105 481c5346d098
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jun 23 11:11:42 2008 +0200

    Merge branch 'linus' into tracing/ftrace

commit 75118a82e21cafb4a82b53bb85d1c7689787e046
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Jun 13 15:47:12 2008 -0700

    x86: fix NULL pointer deref in __switch_to
    
    Patrick McHardy reported a crash:
    
    > > I get this oops once a day, its apparently triggered by something
    > > run by cron, but the process is a different one each time.
    > >
    > > Kernel is -git from yesterday shortly before the -rc6 release
    > > (last commit is the usb-2.6 merge, the x86 patches are missing),
    > > .config is attached.
    > >
    > > I'll retry with current -git, but the patches that have gone in
    > > since I last updated don't look related.
    > >
    > > [62060.043009] BUG: unable to handle kernel NULL pointer dereference at
    > > 000001ff
    > > [62060.043009] IP: [<c0102a9b>] __switch_to+0x2f/0x118
    > > [62060.043009] *pde = 00000000
    > > [62060.043009] Oops: 0002 [#1] PREEMPT
    
    Vegard Nossum analyzed it:
    
    > This decodes to
    >
    >    0:   0f ae 00                fxsave (%eax)
    >
    > so it's related to the floating-point context. This is the exact
    > location of the crash:
    >
    > $ addr2line -e arch/x86/kernel/process_32.o -i ab0
    > include/asm/i387.h:232
    > include/asm/i387.h:262
    > arch/x86/kernel/process_32.c:595
    >
    > ...so it looks like prev_task->thread.xstate->fxsave has become NULL.
    > Or maybe it never had any other value.
    
    Somehow (as described below) TS_USEDFPU is set but the fpu is not
    allocated or freed.
    
    Another possible FPU pre-emption issue with the sleazy FPU optimization
    which was benign before but not so anymore, with the dynamic FPU allocation
    patch.
    
    New task is getting exec'd and it is prempted at the below point.
    
    flush_thread() {
            ...
            /*
            * Forget coprocessor state..
            */
            clear_fpu(tsk);
                    <----- Preemption point
            clear_used_math();
            ...
    }
    
    Now when it context switches in again, as the used_math() is still set
    and fpu_counter can be > 5, we will do a math_state_restore() which sets
    the task's TS_USEDFPU. After it continues from the above preemption point
    it does clear_used_math() and much later free_thread_xstate().
    
    Now, at the next context switch, it is quite possible that xstate is
    null, used_math() is not set and TS_USEDFPU is still set. This will
    trigger unlazy_fpu() causing kernel oops.
    
    Fix this  by clearing tsk's fpu_counter before clearing task's fpu.
    
    Reported-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ac54ff56df80..c6eb5c91e5f6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -294,6 +294,7 @@ void flush_thread(void)
 	/*
 	 * Forget coprocessor state..
 	 */
+	tsk->fpu_counter = 0;
 	clear_fpu(tsk);
 	clear_used_math();
 }

commit e765ee90da62535ac7d7a97f2464f9646539d683
Merge: a4500b84c516 066519068ad2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jun 16 11:15:58 2008 +0200

    Merge branch 'linus' into tracing/ftrace

commit 00dba56465228825ea806e3a7fc0aa6bba7bdc6c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 9 18:35:28 2008 +0200

    x86: move more common idle functions/variables to process.c
    
    more unification. Should cause no change in functionality.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index db3d89a04399..9fb3a6fe863b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -56,15 +56,6 @@ asmlinkage extern void ret_from_fork(void);
 
 unsigned long kernel_thread_flags = CLONE_VM | CLONE_UNTRACED;
 
-unsigned long boot_option_idle_override = 0;
-EXPORT_SYMBOL(boot_option_idle_override);
-
-/*
- * Powermanagement idle function, if any..
- */
-void (*pm_idle)(void);
-EXPORT_SYMBOL(pm_idle);
-
 static ATOMIC_NOTIFIER_HEAD(idle_notifier);
 
 void idle_notifier_register(struct notifier_block *n)
@@ -94,25 +85,6 @@ void exit_idle(void)
 	__exit_idle();
 }
 
-/*
- * We use this if we don't have any better
- * idle routine..
- */
-void default_idle(void)
-{
-	current_thread_info()->status &= ~TS_POLLING;
-	/*
-	 * TS_POLLING-cleared state must be visible before we
-	 * test NEED_RESCHED:
-	 */
-	smp_mb();
-	if (!need_resched())
-		safe_halt();	/* enables interrupts racelessly */
-	else
-		local_irq_enable();
-	current_thread_info()->status |= TS_POLLING;
-}
-
 #ifdef CONFIG_HOTPLUG_CPU
 DECLARE_PER_CPU(int, cpu_state);
 

commit 6ddd2a27948f0bd02a2ad001e8a6816898eba0dc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 9 16:59:53 2008 +0200

    x86: simplify idle selection
    
    default_idle is selected in cpu_idle(), when no other idle routine is
    selected. Select it in select_idle_routine() when mwait is not
    selected.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e2319f39988b..db3d89a04399 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -150,12 +150,9 @@ void cpu_idle(void)
 	while (1) {
 		tick_nohz_stop_sched_tick();
 		while (!need_resched()) {
-			void (*idle)(void);
 
 			rmb();
-			idle = pm_idle;
-			if (!idle)
-				idle = default_idle;
+
 			if (cpu_is_offline(smp_processor_id()))
 				play_dead();
 			/*
@@ -165,7 +162,7 @@ void cpu_idle(void)
 			 */
 			local_irq_disable();
 			enter_idle();
-			idle();
+			pm_idle();
 			/* In many cases the interrupt that ended idle
 			   has already called exit_idle. But some idle
 			   loops can be woken up without interrupt. */

commit 870568b39064cab2dd971fe57969916036982862
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Jun 2 15:57:27 2008 -0700

    x86, fpu: fix CONFIG_PREEMPT=y corruption of application's FPU stack
    
    Jürgen Mell reported an FPU state corruption bug under CONFIG_PREEMPT,
    and bisected it to commit v2.6.19-1363-gacc2076, "i386: add sleazy FPU
    optimization".
    
    Add tsk_used_math() checks to prevent calling math_state_restore()
    which can sleep in the case of !tsk_used_math(). This prevents
    making a blocking call in __switch_to().
    
    Apparently "fpu_counter > 5" check is not enough, as in some signal handling
    and fork/exec scenarios, fpu_counter > 5 and !tsk_used_math() is possible.
    
    It's a side effect though. This is the failing scenario:
    
    process 'A' in save_i387_ia32() just after clear_used_math()
    
    Got an interrupt and pre-empted out.
    
    At the next context switch to process 'A' again, kernel tries to restore
    the math state proactively and sees a fpu_counter > 0 and !tsk_used_math()
    
    This results in init_fpu() during the __switch_to()'s math_state_restore()
    
    And resulting in fpu corruption which will be saved/restored
    (save_i387_fxsave and restore_i387_fxsave) during the remaining
    part of the signal handling after the context switch.
    
    Bisected-by: Jürgen Mell <j.mell@t-online.de>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Tested-by: Jürgen Mell <j.mell@t-online.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@kernel.org

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e2319f39988b..ac54ff56df80 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -658,8 +658,11 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/* If the task has used fpu the last 5 timeslices, just do a full
 	 * restore of the math state immediately to avoid the trap; the
 	 * chances of needing FPU soon are obviously high now
+	 *
+	 * tsk_used_math() checks prevent calling math_state_restore(),
+	 * which can sleep in the case of !tsk_used_math()
 	 */
-	if (next_p->fpu_counter>5)
+	if (tsk_used_math(next_p) && next_p->fpu_counter > 5)
 		math_state_restore();
 	return prev_p;
 }

commit 420594296838fdc9a674470d710cda7d1487f9f4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 14 09:44:08 2008 +0100

    x86: fix the stackprotector canary of the boot CPU
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 5107cb214c7b..cce47f7fbf22 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -16,6 +16,7 @@
 
 #include <stdarg.h>
 
+#include <linux/stackprotector.h>
 #include <linux/cpu.h>
 #include <linux/errno.h>
 #include <linux/sched.h>

commit 18aa8bb12dcb10adc3d7c9d69714d53667c0ab7f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 14 09:42:02 2008 +0100

    stackprotector: add boot_init_stack_canary()
    
    add the boot_init_stack_canary() and make the secondary idle threads
    use it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index d4c7ac7aa430..5107cb214c7b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -147,7 +147,6 @@ void cpu_idle(void)
 {
 	current_thread_info()->status |= TS_POLLING;
 
-#ifdef CONFIG_CC_STACKPROTECTOR
 	/*
 	 * If we're the non-boot CPU, nothing set the PDA stack
 	 * canary up for us - and if we are the boot CPU we have
@@ -156,9 +155,8 @@ void cpu_idle(void)
 	 * invalid canaries already on the stack wont ever
 	 * trigger):
 	 */
-	current->stack_canary = get_random_int();
-	write_pda(stack_canary, current->stack_canary);
-#endif
+	boot_init_stack_canary();
+
 	/* endless idle loop with no priority at all */
 	while (1) {
 		tick_nohz_stop_sched_tick();

commit 7e09b2a02dae4616a5a26000169964b32f86cd35
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 14 09:22:34 2008 +0100

    x86: fix canary of the boot CPU's idle task
    
    the boot CPU's idle task has a zero stackprotector canary value.
    
    this is a special task that is never forked, so the fork code
    does not randomize its canary. Do it when we hit cpu_idle().
    
    Academic sidenote: this means that the early init code runs with a
    zero canary and hence the canary becomes predictable for this short,
    boot-only amount of time.
    
    Although attack vectors against early init code are very rare, it might
    make sense to move this initialization to an earlier point.
    (to one of the early init functions that never return - such as
    start_kernel())
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 9e69e223023e..d4c7ac7aa430 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -150,9 +150,13 @@ void cpu_idle(void)
 #ifdef CONFIG_CC_STACKPROTECTOR
 	/*
 	 * If we're the non-boot CPU, nothing set the PDA stack
-	 * canary up for us. This is as good a place as any for
-	 * doing that.
+	 * canary up for us - and if we are the boot CPU we have
+	 * a 0 stack canary. This is a good place for updating
+	 * it, as we wont ever return from this function (so the
+	 * invalid canaries already on the stack wont ever
+	 * trigger):
 	 */
+	current->stack_canary = get_random_int();
 	write_pda(stack_canary, current->stack_canary);
 #endif
 	/* endless idle loop with no priority at all */

commit ce22bd92cba0958e052cb1ce0f89f1d3a02b60a7
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Mon May 12 15:44:31 2008 +0200

    x86: setup stack canary for the idle threads
    
    The idle threads for non-boot CPUs are a bit special in how they
    are created; the result is that these don't have the stack canary
    set up properly in their PDA. Easiest fix is to just always set
    the PDA up correctly when entering the idle thread; this is a NOP
    for the boot cpu.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index d8640388039e..9e69e223023e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -146,6 +146,15 @@ static inline void play_dead(void)
 void cpu_idle(void)
 {
 	current_thread_info()->status |= TS_POLLING;
+
+#ifdef CONFIG_CC_STACKPROTECTOR
+	/*
+	 * If we're the non-boot CPU, nothing set the PDA stack
+	 * canary up for us. This is as good a place as any for
+	 * doing that.
+	 */
+	write_pda(stack_canary, current->stack_canary);
+#endif
 	/* endless idle loop with no priority at all */
 	while (1) {
 		tick_nohz_stop_sched_tick();

commit e00320875d0cc5f8099a7227b2f25fbb3231268d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 14 08:48:23 2008 +0100

    x86: fix stackprotector canary updates during context switches
    
    fix a bug noticed and fixed by pageexec@freemail.hu.
    
    if built with -fstack-protector-all then we'll have canary checks built
    into the __switch_to() function. That does not work well with the
    canary-switching code there: while we already use the %rsp of the
    new task, we still call __switch_to() whith the previous task's canary
    value in the PDA, hence the __switch_to() ssp prologue instructions
    will store the previous canary. Then we update the PDA and upon return
    from __switch_to() the canary check triggers and we panic.
    
    so update the canary after we have called __switch_to(), where we are
    at the same stackframe level as the last stackframe of the next
    (and now freshly current) task.
    
    Note: this means that we call __switch_to() [and its sub-functions]
    still with the old canary, but that is not a problem, both the previous
    and the next task has a high-quality canary. The only (mostly academic)
    disadvantage is that the canary of one task may leak onto the stack of
    another task, increasing the risk of information leaks, were an attacker
    able to read the stack of specific tasks (but not that of others).
    
    To solve this we'll have to reorganize the way we switch tasks, and move
    the PDA setting into the switch_to() assembly code. That will happen in
    another patch.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e2319f39988b..d8640388039e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -640,7 +640,6 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	write_pda(kernelstack,
 	(unsigned long)task_stack_page(next_p) + THREAD_SIZE - PDA_STACKOFFSET);
 #ifdef CONFIG_CC_STACKPROTECTOR
-	write_pda(stack_canary, next_p->stack_canary);
 	/*
 	 * Build time only check to make sure the stack_canary is at
 	 * offset 40 in the pda; this is a gcc ABI requirement

commit 81d68a96a39844853b37f20cc8282d9b65b78ef3
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:42 2008 +0200

    ftrace: trace irq disabled critical timings
    
    This patch adds latency tracing for critical timings
    (how long interrupts are disabled for).
    
     "irqsoff" is added to /debugfs/tracing/available_tracers
    
    Note:
      tracing_max_latency
        also holds the max latency for irqsoff (in usecs).
       (default to large number so one must start latency tracing)
    
      tracing_thresh
        threshold (in usecs) to always print out if irqs off
        is detected to be longer than stated here.
        If irq_thresh is non-zero, then max_irq_latency
        is ignored.
    
    Here's an example of a trace with ftrace_enabled = 0
    
    =======
    preemption latency trace v1.1.5 on 2.6.24-rc7
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    --------------------------------------------------------------------
     latency: 100 us, #3/3, CPU#1 | (M:rt VP:0, KP:0, SP:0 HP:0 #P:2)
        -----------------
        | task: swapper-0 (uid:0 nice:0 policy:0 rt_prio:0)
        -----------------
     => started at: _spin_lock_irqsave+0x2a/0xb7
     => ended at:   _spin_unlock_irqrestore+0x32/0x5f
    
                     _------=> CPU#
                    / _-----=> irqs-off
                   | / _----=> need-resched
                   || / _---=> hardirq/softirq
                   ||| / _--=> preempt-depth
                   |||| /
                   |||||     delay
       cmd     pid ||||| time  |   caller
          \   /    |||||   \   |   /
     swapper-0     1d.s3    0us+: _spin_lock_irqsave+0x2a/0xb7 (e1000_update_stats+0x47/0x64c [e1000])
     swapper-0     1d.s3  100us : _spin_unlock_irqrestore+0x32/0x5f (e1000_update_stats+0x641/0x64c [e1000])
     swapper-0     1d.s3  100us : trace_hardirqs_on_caller+0x75/0x89 (_spin_unlock_irqrestore+0x32/0x5f)
    
    vim:ft=help
    =======
    
    And this is a trace with ftrace_enabled == 1
    
    =======
    preemption latency trace v1.1.5 on 2.6.24-rc7
    --------------------------------------------------------------------
     latency: 102 us, #12/12, CPU#1 | (M:rt VP:0, KP:0, SP:0 HP:0 #P:2)
        -----------------
        | task: swapper-0 (uid:0 nice:0 policy:0 rt_prio:0)
        -----------------
     => started at: _spin_lock_irqsave+0x2a/0xb7
     => ended at:   _spin_unlock_irqrestore+0x32/0x5f
    
                     _------=> CPU#
                    / _-----=> irqs-off
                   | / _----=> need-resched
                   || / _---=> hardirq/softirq
                   ||| / _--=> preempt-depth
                   |||| /
                   |||||     delay
       cmd     pid ||||| time  |   caller
          \   /    |||||   \   |   /
     swapper-0     1dNs3    0us+: _spin_lock_irqsave+0x2a/0xb7 (e1000_update_stats+0x47/0x64c [e1000])
     swapper-0     1dNs3   46us : e1000_read_phy_reg+0x16/0x225 [e1000] (e1000_update_stats+0x5e2/0x64c [e1000])
     swapper-0     1dNs3   46us : e1000_swfw_sync_acquire+0x10/0x99 [e1000] (e1000_read_phy_reg+0x49/0x225 [e1000])
     swapper-0     1dNs3   46us : e1000_get_hw_eeprom_semaphore+0x12/0xa6 [e1000] (e1000_swfw_sync_acquire+0x36/0x99 [e1000])
     swapper-0     1dNs3   47us : __const_udelay+0x9/0x47 (e1000_read_phy_reg+0x116/0x225 [e1000])
     swapper-0     1dNs3   47us+: __delay+0x9/0x50 (__const_udelay+0x45/0x47)
     swapper-0     1dNs3   97us : preempt_schedule+0xc/0x84 (__delay+0x4e/0x50)
     swapper-0     1dNs3   98us : e1000_swfw_sync_release+0xc/0x55 [e1000] (e1000_read_phy_reg+0x211/0x225 [e1000])
     swapper-0     1dNs3   99us+: e1000_put_hw_eeprom_semaphore+0x9/0x35 [e1000] (e1000_swfw_sync_release+0x50/0x55 [e1000])
     swapper-0     1dNs3  101us : _spin_unlock_irqrestore+0xe/0x5f (e1000_update_stats+0x641/0x64c [e1000])
     swapper-0     1dNs3  102us : _spin_unlock_irqrestore+0x32/0x5f (e1000_update_stats+0x641/0x64c [e1000])
     swapper-0     1dNs3  102us : trace_hardirqs_on_caller+0x75/0x89 (_spin_unlock_irqrestore+0x32/0x5f)
    
    vim:ft=help
    =======
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e2319f39988b..dd349c92f051 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -165,7 +165,10 @@ void cpu_idle(void)
 			 */
 			local_irq_disable();
 			enter_idle();
+			/* Don't trace irqs off for idle */
+			stop_critical_timings();
 			idle();
+			start_critical_timings();
 			/* In many cases the interrupt that ended idle
 			   has already called exit_idle. But some idle
 			   loops can be woken up without interrupt. */

commit 34b2cd5b688b012975fcfc3b3970fc3508fa82c4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat May 17 08:30:07 2008 +0200

    x86: PEBS cleanup
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ad213494a22f..4a93c98a60a2 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -500,7 +500,6 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 {
 	struct thread_struct *prev, *next;
 	unsigned long debugctl;
-	unsigned long ds_prev = 0, ds_next = 0;
 
 	prev = &prev_p->thread,
 	next = &next_p->thread;
@@ -508,17 +507,23 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 	debugctl = prev->debugctlmsr;
 
 #ifdef CONFIG_X86_DS
-	if (prev->ds_ctx)
-		ds_prev = (unsigned long)prev->ds_ctx->ds;
-	if (next->ds_ctx)
-		ds_next = (unsigned long)next->ds_ctx->ds;
-
-	if (ds_next != ds_prev) {
-		/* we clear debugctl to make sure DS
-		 * is not in use when we change it */
-		debugctl = 0;
-		update_debugctlmsr(0);
-		wrmsrl(MSR_IA32_DS_AREA, ds_next);
+	{
+		unsigned long ds_prev = 0, ds_next = 0;
+
+		if (prev->ds_ctx)
+			ds_prev = (unsigned long)prev->ds_ctx->ds;
+		if (next->ds_ctx)
+			ds_next = (unsigned long)next->ds_ctx->ds;
+
+		if (ds_next != ds_prev) {
+			/*
+			 * We clear debugctl to make sure DS
+			 * is not in use when we change it:
+			 */
+			debugctl = 0;
+			update_debugctlmsr(0);
+			wrmsrl(MSR_IA32_DS_AREA, ds_next);
+		}
 	}
 #endif /* CONFIG_X86_DS */
 

commit 93fa7636dfdc059b25df148f230c0991096afdef
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Tue Apr 8 11:01:58 2008 +0200

    x86, ptrace: PEBS support
    
    Polish the ds.h interface and add support for PEBS.
    
    Ds.c is meant to be the resource allocator for per-thread and per-cpu
    BTS and PEBS recording.
    It is used by ptrace/utrace to provide execution tracing of debugged tasks.
    It will be used by profilers (e.g. perfmon2).
    It may be used by kernel debuggers to provide a kernel execution trace.
    
    Changes in detail:
    - guard DS and ptrace by CONFIG macros
    - separate DS and BTS more clearly
    - simplify field accesses
    - add functions to manage PEBS buffers
    - add simple protection/allocation mechanism
    - added support for Atom
    
    Opens:
    - buffer overflow handling
      Currently, only circular buffers are supported. This is all we need
      for debugging. Profilers would want an overflow notification.
      This is planned to be added when perfmon2 is made to use the ds.h
      interface.
    - utrace intermediate layer
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e2319f39988b..ad213494a22f 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -267,6 +267,14 @@ void exit_thread(void)
 		t->io_bitmap_max = 0;
 		put_cpu();
 	}
+#ifdef CONFIG_X86_DS
+	/* Free any DS contexts that have not been properly released. */
+	if (unlikely(t->ds_ctx)) {
+		/* we clear debugctl to make sure DS is not used. */
+		update_debugctlmsr(0);
+		ds_free(t->ds_ctx);
+	}
+#endif /* CONFIG_X86_DS */
 }
 
 void flush_thread(void)
@@ -492,18 +500,27 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 {
 	struct thread_struct *prev, *next;
 	unsigned long debugctl;
+	unsigned long ds_prev = 0, ds_next = 0;
 
 	prev = &prev_p->thread,
 	next = &next_p->thread;
 
 	debugctl = prev->debugctlmsr;
-	if (next->ds_area_msr != prev->ds_area_msr) {
+
+#ifdef CONFIG_X86_DS
+	if (prev->ds_ctx)
+		ds_prev = (unsigned long)prev->ds_ctx->ds;
+	if (next->ds_ctx)
+		ds_next = (unsigned long)next->ds_ctx->ds;
+
+	if (ds_next != ds_prev) {
 		/* we clear debugctl to make sure DS
 		 * is not in use when we change it */
 		debugctl = 0;
 		update_debugctlmsr(0);
-		wrmsrl(MSR_IA32_DS_AREA, next->ds_area_msr);
+		wrmsrl(MSR_IA32_DS_AREA, ds_next);
 	}
+#endif /* CONFIG_X86_DS */
 
 	if (next->debugctlmsr != debugctl)
 		update_debugctlmsr(next->debugctlmsr);
@@ -541,13 +558,13 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
 	}
 
-#ifdef X86_BTS
+#ifdef CONFIG_X86_PTRACE_BTS
 	if (test_tsk_thread_flag(prev_p, TIF_BTS_TRACE_TS))
 		ptrace_bts_take_timestamp(prev_p, BTS_TASK_DEPARTS);
 
 	if (test_tsk_thread_flag(next_p, TIF_BTS_TRACE_TS))
 		ptrace_bts_take_timestamp(next_p, BTS_TASK_ARRIVES);
-#endif
+#endif /* CONFIG_X86_PTRACE_BTS */
 }
 
 /*

commit 7f424a8b08c26dc14ac5c17164014539ac9a5c65
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 25 17:39:01 2008 +0200

    fix idle (arch, acpi and apm) and lockdep
    
    OK, so 25-mm1 gave a lockdep error which made me look into this.
    
    The first thing that I noticed was the horrible mess; the second thing I
    saw was hacks like: 71e93d15612c61c2e26a169567becf088e71b8ff
    
    The problem is that arch idle routines are somewhat inconsitent with
    their IRQ state handling and instead of fixing _that_, we go paper over
    the problem.
    
    So the thing I've tried to do is set a standard for idle routines and
    fix them all up to adhere to that. So the rules are:
    
      idle routines are entered with IRQs disabled
      idle routines will exit with IRQs enabled
    
    Nearly all already did this in one form or another.
    
    Merge the 32 and 64 bit bits so they no longer have different bugs.
    
    As for the actual lockdep warning; __sti_mwait() did a plainly un-annotated
    irq-enable.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Bob Copeland <me@bobcopeland.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 131c2ee7ac56..e2319f39988b 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -106,26 +106,13 @@ void default_idle(void)
 	 * test NEED_RESCHED:
 	 */
 	smp_mb();
-	local_irq_disable();
-	if (!need_resched()) {
+	if (!need_resched())
 		safe_halt();	/* enables interrupts racelessly */
-		local_irq_disable();
-	}
-	local_irq_enable();
+	else
+		local_irq_enable();
 	current_thread_info()->status |= TS_POLLING;
 }
 
-/*
- * On SMP it's slightly faster (but much more power-consuming!)
- * to poll the ->need_resched flag instead of waiting for the
- * cross-CPU IPI to arrive. Use this option with caution.
- */
-static void poll_idle(void)
-{
-	local_irq_enable();
-	cpu_relax();
-}
-
 #ifdef CONFIG_HOTPLUG_CPU
 DECLARE_PER_CPU(int, cpu_state);
 
@@ -192,110 +179,6 @@ void cpu_idle(void)
 	}
 }
 
-static void do_nothing(void *unused)
-{
-}
-
-/*
- * cpu_idle_wait - Used to ensure that all the CPUs discard old value of
- * pm_idle and update to new pm_idle value. Required while changing pm_idle
- * handler on SMP systems.
- *
- * Caller must have changed pm_idle to the new value before the call. Old
- * pm_idle value will not be used by any CPU after the return of this function.
- */
-void cpu_idle_wait(void)
-{
-	smp_mb();
-	/* kick all the CPUs so that they exit out of pm_idle */
-	smp_call_function(do_nothing, NULL, 0, 1);
-}
-EXPORT_SYMBOL_GPL(cpu_idle_wait);
-
-/*
- * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
- * which can obviate IPI to trigger checking of need_resched.
- * We execute MONITOR against need_resched and enter optimized wait state
- * through MWAIT. Whenever someone changes need_resched, we would be woken
- * up from MWAIT (without an IPI).
- *
- * New with Core Duo processors, MWAIT can take some hints based on CPU
- * capability.
- */
-void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
-{
-	if (!need_resched()) {
-		__monitor((void *)&current_thread_info()->flags, 0, 0);
-		smp_mb();
-		if (!need_resched())
-			__mwait(ax, cx);
-	}
-}
-
-/* Default MONITOR/MWAIT with no hints, used for default C1 state */
-static void mwait_idle(void)
-{
-	if (!need_resched()) {
-		__monitor((void *)&current_thread_info()->flags, 0, 0);
-		smp_mb();
-		if (!need_resched())
-			__sti_mwait(0, 0);
-		else
-			local_irq_enable();
-	} else {
-		local_irq_enable();
-	}
-}
-
-
-static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
-{
-	if (force_mwait)
-		return 1;
-	/* Any C1 states supported? */
-	return c->cpuid_level >= 5 && ((cpuid_edx(5) >> 4) & 0xf) > 0;
-}
-
-void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
-{
-	static int selected;
-
-	if (selected)
-		return;
-#ifdef CONFIG_X86_SMP
-	if (pm_idle == poll_idle && smp_num_siblings > 1) {
-		printk(KERN_WARNING "WARNING: polling idle and HT enabled,"
-			" performance may degrade.\n");
-	}
-#endif
-	if (cpu_has(c, X86_FEATURE_MWAIT) && mwait_usable(c)) {
-		/*
-		 * Skip, if setup has overridden idle.
-		 * One CPU supports mwait => All CPUs supports mwait
-		 */
-		if (!pm_idle) {
-			printk(KERN_INFO "using mwait in idle threads.\n");
-			pm_idle = mwait_idle;
-		}
-	}
-	selected = 1;
-}
-
-static int __init idle_setup(char *str)
-{
-	if (!strcmp(str, "poll")) {
-		printk("using polling idle threads.\n");
-		pm_idle = poll_idle;
-	} else if (!strcmp(str, "mwait"))
-		force_mwait = 1;
-	else
-		return -1;
-
-	boot_option_idle_override = 1;
-	return 0;
-}
-early_param("idle", idle_setup);
-
 /* Prints also some state that isn't saved in the pt_regs */
 void __show_regs(struct pt_regs * regs)
 {

commit a4928cffe6435caf427ae673131a633c1329dbf3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 23 13:20:56 2008 +0200

    "make namespacecheck" fixes
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 891af1a1b48a..131c2ee7ac56 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -562,7 +562,7 @@ static void hard_enable_TSC(void)
 	write_cr4(read_cr4() & ~X86_CR4_TSD);
 }
 
-void enable_TSC(void)
+static void enable_TSC(void)
 {
 	preempt_disable();
 	if (test_and_clear_thread_flag(TIF_NOTSC))

commit aa283f49276e7d840a40fb01eee6de97eaa7e012
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Mar 10 15:28:05 2008 -0700

    x86, fpu: lazy allocation of FPU area - v5
    
    Only allocate the FPU area when the application actually uses FPU, i.e., in the
    first lazy FPU trap. This could save memory for non-fpu using apps.
    
    for example: on my system after boot, there are around 300 processes, with
    only 17 using FPU.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b795e831afd6..891af1a1b48a 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -533,6 +533,10 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 	regs->ss		= __USER_DS;
 	regs->flags		= 0x200;
 	set_fs(USER_DS);
+	/*
+	 * Free the old FP and other extended state
+	 */
+	free_thread_xstate(current);
 }
 EXPORT_SYMBOL_GPL(start_thread);
 

commit 61c4628b538608c1a85211ed8438136adfeb9a95
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Mar 10 15:28:04 2008 -0700

    x86, fpu: split FPU state from task struct - v5
    
    Split the FPU save area from the task struct. This allows easy migration
    of FPU context, and it's generally cleaner. It also allows the following
    two optimizations:
    
    1) only allocate when the application actually uses FPU, so in the first
    lazy FPU trap. This could save memory for non-fpu using apps. Next patch
    does this lazy allocation.
    
    2) allocate the right size for the actual cpu rather than 512 bytes always.
    Patches enabling xsave/xrstor support (coming shortly) will take advantage
    of this.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4c13b1406c70..b795e831afd6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -682,7 +682,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 
 	/* we're going to use this soon, after a few expensive things */
 	if (next_p->fpu_counter>5)
-		prefetch(&next->i387.fxsave);
+		prefetch(next->xstate);
 
 	/*
 	 * Reload esp0, LDT and the page table pointer:

commit 529e25f646e08901a6dad5768f681efffd77225e
Author: Erik Bosman <ejbosman@cs.vu.nl>
Date:   Mon Apr 14 00:24:18 2008 +0200

    x86: implement prctl PR_GET_TSC and PR_SET_TSC
    
    This patch implements the PR_GET_TSC and PR_SET_TSC prctl()
    commands on the x86 platform (both 32 and 64 bit.) These
    commands control the ability to read the timestamp counter
    from userspace (the RDTSC instruction.)
    
    While the RDTSC instuction is a useful profiling tool,
    it is also the source of some non-determinism in ring-3.
    For deterministic replay applications it is useful to be
    able to trap and emulate (and record the outcome of) this
    instruction.
    
    This patch uses code earlier used to disable the timestamp
    counter for the SECCOMP framework. A side-effect of this
    patch is that the SECCOMP environment will now also disable
    the timestamp counter on x86_64 due to the addition of the
    TIF_NOTSC define on this platform.
    
    The code which enables/disables the RDTSC instruction during
    context switches is in the __switch_to_xtra function, which
    already handles other unusual conditions, so normal
    performance should not have to suffer from this change.
    
    Signed-off-by: Erik Bosman <ejbosman@cs.vu.nl>
    Acked-by: Arjan van de Ven  <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e75ccc8a2b87..4c13b1406c70 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -36,6 +36,7 @@
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
 #include <linux/tick.h>
+#include <linux/prctl.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -535,6 +536,64 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 }
 EXPORT_SYMBOL_GPL(start_thread);
 
+static void hard_disable_TSC(void)
+{
+	write_cr4(read_cr4() | X86_CR4_TSD);
+}
+
+void disable_TSC(void)
+{
+	preempt_disable();
+	if (!test_and_set_thread_flag(TIF_NOTSC))
+		/*
+		 * Must flip the CPU state synchronously with
+		 * TIF_NOTSC in the current running context.
+		 */
+		hard_disable_TSC();
+	preempt_enable();
+}
+
+static void hard_enable_TSC(void)
+{
+	write_cr4(read_cr4() & ~X86_CR4_TSD);
+}
+
+void enable_TSC(void)
+{
+	preempt_disable();
+	if (test_and_clear_thread_flag(TIF_NOTSC))
+		/*
+		 * Must flip the CPU state synchronously with
+		 * TIF_NOTSC in the current running context.
+		 */
+		hard_enable_TSC();
+	preempt_enable();
+}
+
+int get_tsc_mode(unsigned long adr)
+{
+	unsigned int val;
+
+	if (test_thread_flag(TIF_NOTSC))
+		val = PR_TSC_SIGSEGV;
+	else
+		val = PR_TSC_ENABLE;
+
+	return put_user(val, (unsigned int __user *)adr);
+}
+
+int set_tsc_mode(unsigned int val)
+{
+	if (val == PR_TSC_SIGSEGV)
+		disable_TSC();
+	else if (val == PR_TSC_ENABLE)
+		enable_TSC();
+	else
+		return -EINVAL;
+
+	return 0;
+}
+
 /*
  * This special macro can be used to load a debugging register
  */
@@ -572,6 +631,15 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 		loaddebug(next, 7);
 	}
 
+	if (test_tsk_thread_flag(prev_p, TIF_NOTSC) ^
+	    test_tsk_thread_flag(next_p, TIF_NOTSC)) {
+		/* prev and next are different */
+		if (test_tsk_thread_flag(next_p, TIF_NOTSC))
+			hard_disable_TSC();
+		else
+			hard_enable_TSC();
+	}
+
 	if (test_tsk_thread_flag(next_p, TIF_IO_BITMAP)) {
 		/*
 		 * Copy the relevant range of the IO bitmap.

commit 13af4836b3914b23946f6a8982934e2c828c183f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 2 13:23:22 2008 +0200

    x86: improve default idle
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4f40272474dd..e75ccc8a2b87 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -107,16 +107,8 @@ void default_idle(void)
 	smp_mb();
 	local_irq_disable();
 	if (!need_resched()) {
-		ktime_t t0, t1;
-		u64 t0n, t1n;
-
-		t0 = ktime_get();
-		t0n = ktime_to_ns(t0);
 		safe_halt();	/* enables interrupts racelessly */
 		local_irq_disable();
-		t1 = ktime_get();
-		t1n = ktime_to_ns(t1);
-		sched_clock_idle_wakeup_event(t1n - t0n);
 	}
 	local_irq_enable();
 	current_thread_info()->status |= TS_POLLING;

commit 5b0e508415a83989fe704b4718a1a214bc333ca7
Author: Jan Beulich <jbeulich@novell.com>
Date:   Mon Mar 10 13:11:17 2008 +0000

    x86: prevent unconditional writes to DebugCtl MSR
    
    Otherwise, enabling (or better, subsequent disabling) of single
    stepping would cause a kernel oops on CPUs not having this MSR.
    
    The patch could have been added a conditional to the MSR write in
    user_disable_single_step(), but centralizing the updates seems safer
    and (looking forward) better manageable.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 1ffce14cff6e..4f40272474dd 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -563,12 +563,12 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 		/* we clear debugctl to make sure DS
 		 * is not in use when we change it */
 		debugctl = 0;
-		wrmsrl(MSR_IA32_DEBUGCTLMSR, 0);
+		update_debugctlmsr(0);
 		wrmsrl(MSR_IA32_DS_AREA, next->ds_area_msr);
 	}
 
 	if (next->debugctlmsr != debugctl)
-		wrmsrl(MSR_IA32_DEBUGCTLMSR, next->debugctlmsr);
+		update_debugctlmsr(next->debugctlmsr);
 
 	if (test_tsk_thread_flag(next_p, TIF_DEBUG)) {
 		loaddebug(next, 0);

commit 513ad84bf60d96a6998bca10ed07c3d340449be8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 21 05:18:40 2008 +0100

    x86: de-macro start_thread()
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 46c4c546b499..1ffce14cff6e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -528,6 +528,21 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
 	return err;
 }
 
+void
+start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
+{
+	asm volatile("movl %0, %%fs; movl %0, %%es; movl %0, %%ds" :: "r"(0));
+	load_gs_index(0);
+	regs->ip		= new_ip;
+	regs->sp		= new_sp;
+	write_pda(oldrsp, new_sp);
+	regs->cs		= __USER_CS;
+	regs->ss		= __USER_DS;
+	regs->flags		= 0x200;
+	set_fs(USER_DS);
+}
+EXPORT_SYMBOL_GPL(start_thread);
+
 /*
  * This special macro can be used to load a debugging register
  */

commit 783e391b7b5b273cd20856d8f6f4878da8ec31b3
Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Thu Apr 10 09:49:58 2008 -0700

    x86: Simplify cpu_idle_wait
    
    This patch also resolves hangs on boot:
            http://lkml.org/lkml/2008/2/23/263
            http://bugzilla.kernel.org/show_bug.cgi?id=10093
    
    The bug was causing once-in-few-reboots 10-15 sec wait during boot on
    certain laptops.
    
    Earlier commit 40d6a146629b98d8e322b6f9332b182c7cbff3df added
    smp_call_function in cpu_idle_wait() to kick cpus that are in tickless
    idle.  Looking at cpu_idle_wait code at that time, code seemed to be
    over-engineered for a case which is rarely used (while changing idle
    handler).
    
    Below is a simplified version of cpu_idle_wait, which just makes a dummy
    smp_call_function to all cpus, to make them come out of old idle handler
    and start using the new idle handler.  It eliminates code in the idle
    loop to handle cpu_idle_wait.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 3baf9b9f4c87..46c4c546b499 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -63,7 +63,6 @@ EXPORT_SYMBOL(boot_option_idle_override);
  */
 void (*pm_idle)(void);
 EXPORT_SYMBOL(pm_idle);
-static DEFINE_PER_CPU(unsigned int, cpu_idle_state);
 
 static ATOMIC_NOTIFIER_HEAD(idle_notifier);
 
@@ -173,9 +172,6 @@ void cpu_idle(void)
 		while (!need_resched()) {
 			void (*idle)(void);
 
-			if (__get_cpu_var(cpu_idle_state))
-				__get_cpu_var(cpu_idle_state) = 0;
-
 			rmb();
 			idle = pm_idle;
 			if (!idle)
@@ -207,40 +203,19 @@ static void do_nothing(void *unused)
 {
 }
 
+/*
+ * cpu_idle_wait - Used to ensure that all the CPUs discard old value of
+ * pm_idle and update to new pm_idle value. Required while changing pm_idle
+ * handler on SMP systems.
+ *
+ * Caller must have changed pm_idle to the new value before the call. Old
+ * pm_idle value will not be used by any CPU after the return of this function.
+ */
 void cpu_idle_wait(void)
 {
-	unsigned int cpu, this_cpu = get_cpu();
-	cpumask_t map, tmp = current->cpus_allowed;
-
-	set_cpus_allowed(current, cpumask_of_cpu(this_cpu));
-	put_cpu();
-
-	cpus_clear(map);
-	for_each_online_cpu(cpu) {
-		per_cpu(cpu_idle_state, cpu) = 1;
-		cpu_set(cpu, map);
-	}
-
-	__get_cpu_var(cpu_idle_state) = 0;
-
-	wmb();
-	do {
-		ssleep(1);
-		for_each_online_cpu(cpu) {
-			if (cpu_isset(cpu, map) && !per_cpu(cpu_idle_state, cpu))
-				cpu_clear(cpu, map);
-		}
-		cpus_and(map, map, cpu_online_map);
-		/*
-		 * We waited 1 sec, if a CPU still did not call idle
-		 * it may be because it is in idle and not waking up
-		 * because it has nothing to do.
-		 * Give all the remaining CPUS a kick.
-		 */
-		smp_call_function_mask(map, do_nothing, 0, 0);
-	} while (!cpus_empty(map));
-
-	set_cpus_allowed(current, tmp);
+	smp_mb();
+	/* kick all the CPUs so that they exit out of pm_idle */
+	smp_call_function(do_nothing, NULL, 0, 1);
 }
 EXPORT_SYMBOL_GPL(cpu_idle_wait);
 

commit b4ef95de00be4c2c30feccf607a45093c8c118b7
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 26 09:40:27 2008 +0100

    x86: disable BTS ptrace extensions for now
    
    revert the BTS ptrace extension for now.
    
    based on general objections from Roland McGrath:
    
        http://lkml.org/lkml/2008/2/21/323
    
    we'll let the BTS functionality cook some more and re-enable
    it in v2.6.26. We'll leave the dead code around to help the
    development of this code.
    
    (X86_BTS is not defined at the moment)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 43f287744f9f..3baf9b9f4c87 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -604,11 +604,13 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
 	}
 
+#ifdef X86_BTS
 	if (test_tsk_thread_flag(prev_p, TIF_BTS_TRACE_TS))
 		ptrace_bts_take_timestamp(prev_p, BTS_TASK_DEPARTS);
 
 	if (test_tsk_thread_flag(next_p, TIF_BTS_TRACE_TS))
 		ptrace_bts_take_timestamp(next_p, BTS_TASK_ARRIVES);
+#endif
 }
 
 /*

commit 5d119b2c9a490e2d647eae134211b32a18a04c7d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 26 12:55:57 2008 +0100

    x86: fix execve with -fstack-protect
    
    pointed out by pageexec@freemail.hu:
    
    > what happens here is that gcc treats the argument area as owned by the
    > callee, not the caller and is allowed to do certain tricks. for ssp it
    > will make a copy of the struct passed by value into the local variable
    > area and pass *its* address down, and it won't copy it back into the
    > original instance stored in the argument area.
    >
    > so once sys_execve returns, the pt_regs passed by value hasn't at all
    > changed and its default content will cause a nice double fault (FWIW,
    > this part took me the longest to debug, being down with cold didn't
    > help it either ;).
    
    To fix this we pass in pt_regs by pointer.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b0cc8f0136d8..43f287744f9f 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -730,16 +730,16 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
  */
 asmlinkage
 long sys_execve(char __user *name, char __user * __user *argv,
-		char __user * __user *envp, struct pt_regs regs)
+		char __user * __user *envp, struct pt_regs *regs)
 {
 	long error;
 	char * filename;
 
 	filename = getname(name);
 	error = PTR_ERR(filename);
-	if (IS_ERR(filename)) 
+	if (IS_ERR(filename))
 		return error;
-	error = do_execve(filename, argv, envp, &regs); 
+	error = do_execve(filename, argv, envp, regs);
 	putname(filename);
 	return error;
 }

commit 1eb114112381eb66ebacdace1b6e70d30d603f9c
Author: David Howells <dhowells@redhat.com>
Date:   Fri Feb 8 04:19:29 2008 -0800

    aout: remove unnecessary inclusions of {asm, linux}/a.out.h
    
    Remove now unnecessary inclusions of {asm,linux}/a.out.h.
    
    [akpm@linux-foundation.org: fix alpha build]
    Signed-off-by: David Howells <dhowells@redhat.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 137a86171c39..b0cc8f0136d8 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -26,7 +26,6 @@
 #include <linux/smp.h>
 #include <linux/slab.h>
 #include <linux/user.h>
-#include <linux/a.out.h>
 #include <linux/interrupt.h>
 #include <linux/utsname.h>
 #include <linux/delay.h>

commit 4c02ad1efdd1293d6fdd453a2f27ad993458dcd1
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Wed Jan 30 13:33:37 2008 +0100

    x86: fix section mismatch warning in process_*.c
    
    Fix the following warning:
    WARNING: arch/x86/kernel/built-in.o(.text+0x3): Section mismatch: reference to .cpuinit.data:force_mwait in 'mwait_usable'
    [Seen on 64 bit only but similar pattern exist on 32 bit so fix it there too]
    
    mwait_usable() were only used by a function annotated __cpuinit
    so annotate mwait_usable() with __cpuinit to fix the warning.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index b4c470658a8a..137a86171c39 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -281,7 +281,7 @@ static void mwait_idle(void)
 }
 
 
-static int mwait_usable(const struct cpuinfo_x86 *c)
+static int __cpuinit mwait_usable(const struct cpuinfo_x86 *c)
 {
 	if (force_mwait)
 		return 1;

commit ade1af77129dea6e335b525ed3be3b846bc1ec13
Author: Jan Engelhardt <jengelh@computergmbh.de>
Date:   Wed Jan 30 13:33:23 2008 +0100

    x86: remove unneded casts
    
    x86: remove unneeded casts
    
    Signed-off-by: Jan Engelhardt <jengelh@computergmbh.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 95313532b2e0..b4c470658a8a 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -470,7 +470,7 @@ static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
 		.limit_in_pages = 1,
 		.useable = 1,
 	};
-	struct desc_struct *desc = (void *)t->thread.tls_array;
+	struct desc_struct *desc = t->thread.tls_array;
 	desc += tls;
 	fill_ldt(desc, &ud);
 }

commit 27415a4fe369e07a1393ae52c8ed8e48aabed5a9
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Wed Jan 30 13:33:18 2008 +0100

    x86: move warning message of polling idle and HT enabled
    
    The warning message at idle_setup() is never shown because
    smp_num_sibling hasn't been updated at this point yet.
    
    Move this polling idle and HT enabled warning to select_idle_routine().
    I also implement this warning on 64-bit kernel.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index dbe0a846ec52..95313532b2e0 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -291,20 +291,27 @@ static int mwait_usable(const struct cpuinfo_x86 *c)
 
 void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 {
-	static int printed;
+	static int selected;
+
+	if (selected)
+		return;
+#ifdef CONFIG_X86_SMP
+	if (pm_idle == poll_idle && smp_num_siblings > 1) {
+		printk(KERN_WARNING "WARNING: polling idle and HT enabled,"
+			" performance may degrade.\n");
+	}
+#endif
 	if (cpu_has(c, X86_FEATURE_MWAIT) && mwait_usable(c)) {
 		/*
 		 * Skip, if setup has overridden idle.
 		 * One CPU supports mwait => All CPUs supports mwait
 		 */
 		if (!pm_idle) {
-			if (!printed) {
-				printk(KERN_INFO "using mwait in idle threads.\n");
-				printed = 1;
-			}
+			printk(KERN_INFO "using mwait in idle threads.\n");
 			pm_idle = mwait_idle;
 		}
 	}
+	selected = 1;
 }
 
 static int __init idle_setup(char *str)

commit 0c07ee38c9d4eb081758f5ad14bbffa7197e1aec
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:33:16 2008 +0100

    x86: use the correct cpuid method to detect MWAIT support for C states
    
    Previously there was a AMD specific quirk to handle the case of
    AMD Fam10h MWAIT not supporting any C states. But it turns out
    that CPUID already has ways to detectly detect that without
    using special quirks.
    
    The new code simply checks if MWAIT supports at least C1 and doesn't
    use it if it doesn't. No more vendor specific code.
    
    Note this is does not simply clear MWAIT because MWAIT can be still
    useful even without C states.
    
    Credit goes to Ben Serebrin for pointing out the (nearly) obvious.
    
    Cc: "Andreas Herrmann" <andreas.herrmann3@amd.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4e65ae8a54bf..dbe0a846ec52 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -280,10 +280,19 @@ static void mwait_idle(void)
 	}
 }
 
+
+static int mwait_usable(const struct cpuinfo_x86 *c)
+{
+	if (force_mwait)
+		return 1;
+	/* Any C1 states supported? */
+	return c->cpuid_level >= 5 && ((cpuid_edx(5) >> 4) & 0xf) > 0;
+}
+
 void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 {
 	static int printed;
-	if (cpu_has(c, X86_FEATURE_MWAIT)) {
+	if (cpu_has(c, X86_FEATURE_MWAIT) && mwait_usable(c)) {
 		/*
 		 * Skip, if setup has overridden idle.
 		 * One CPU supports mwait => All CPUs supports mwait

commit aafbd7eb2057edfc9a17b258e3f0258a4e6d8198
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Wed Jan 30 13:33:08 2008 +0100

    x86: make printk_address regs->ip always reliable
    
    printk_address()'s second parameter is the reliability indication,
    not the ebp. If we're printing regs->ip we're reliable by definition,
    so pass a 1 here.
    
    Signed-off-by: Arjan van de Ven
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 383760bfd283..4e65ae8a54bf 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -329,7 +329,7 @@ void __show_regs(struct pt_regs * regs)
 		(int)strcspn(init_utsname()->version, " "),
 		init_utsname()->version);
 	printk("RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
-	printk_address(regs->ip, regs->bp);
+	printk_address(regs->ip, 1);
 	printk("RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss, regs->sp,
 		regs->flags);
 	printk("RAX: %016lx RBX: %016lx RCX: %016lx\n",

commit bc850d6b374fffd08336996f4b4d3bbd6bf427f6
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Wed Jan 30 13:33:07 2008 +0100

    x86: add the capability to print fuzzy backtraces
    
    For enhancing the 32 bit EBP based backtracer, I need the capability
    for the backtracer to tell it's customer that an entry is either
    reliable or unreliable, and the backtrace printing code then needs to
    print the unreliable ones slightly different.
    
    This patch adds the basic capability, the next patch will add a user
    of this capability.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index a0130eb2fa50..383760bfd283 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -329,7 +329,7 @@ void __show_regs(struct pt_regs * regs)
 		(int)strcspn(init_utsname()->version, " "),
 		init_utsname()->version);
 	printk("RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
-	printk_address(regs->ip);
+	printk_address(regs->ip, regs->bp);
 	printk("RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss, regs->sp,
 		regs->flags);
 	printk("RAX: %016lx RBX: %016lx RCX: %016lx\n",
@@ -377,7 +377,7 @@ void show_regs(struct pt_regs *regs)
 {
 	printk("CPU %d:", smp_processor_id());
 	__show_regs(regs);
-	show_trace(NULL, regs, (void *)(regs + 1));
+	show_trace(NULL, regs, (void *)(regs + 1), regs->bp);
 }
 
 /*

commit 3d97775a80a03013abe1fd681620925f884ad18a
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Wed Jan 30 13:33:00 2008 +0100

    x86: move out tick_nohz_stop_sched_tick() call from the loop
    
    Move out tick_nohz_stop_sched_tick() call from the loop in cpu_idle
    same as 32-bit version.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 78d80067b7f9..a0130eb2fa50 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -170,14 +170,13 @@ void cpu_idle(void)
 	current_thread_info()->status |= TS_POLLING;
 	/* endless idle loop with no priority at all */
 	while (1) {
+		tick_nohz_stop_sched_tick();
 		while (!need_resched()) {
 			void (*idle)(void);
 
 			if (__get_cpu_var(cpu_idle_state))
 				__get_cpu_var(cpu_idle_state) = 0;
 
-			tick_nohz_stop_sched_tick();
-
 			rmb();
 			idle = pm_idle;
 			if (!idle)

commit 60b3b9af35aad66345e395be911e46fb8443f0c5
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Jan 30 13:31:55 2008 +0100

    x86: x86 user_regset cleanup
    
    This removes a bunch of dead code that is no longer needed now
    that the user_regset interfaces are being used for all these jobs.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e3a3610ade10..78d80067b7f9 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -544,24 +544,6 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
  */
 #define loaddebug(thread, r) set_debugreg(thread->debugreg ## r, r)
 
-/*
- * Capture the user space registers if the task is not running (in user space)
- */
-int dump_task_regs(struct task_struct *tsk, elf_gregset_t *regs)
-{
-	struct pt_regs *pp, ptregs;
-
-	pp = task_pt_regs(tsk);
-
-	ptregs = *pp;
-	ptregs.cs &= 0xffff;
-	ptregs.ss &= 0xffff;
-
-	elf_core_copy_regs(regs, &ptregs);
-
-	return 1;
-}
-
 static inline void __switch_to_xtra(struct task_struct *prev_p,
 				    struct task_struct *next_p,
 				    struct tss_struct *tss)
@@ -929,4 +911,3 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 	unsigned long range_end = mm->brk + 0x02000000;
 	return randomize_range(mm->brk, range_end, 0) ? : mm->brk;
 }
-

commit 7818a1e0294debee02d5135e17b89f28b8871887
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:31 2008 +0100

    x86: provide 64-bit with a load_sp0 function.
    
    Paravirt guests need to inform the underlying hypervisor whenever the sp0
    tss field changes. i386 already has such a function, and we use it for
    x86_64 too. There's an unnecessary (for 64-bit) msr handling part in the original
    version, and it is placed around an ifdef. Making no more sense in
    processor_32.h, it is moved to the common header
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index af56104b73ff..e3a3610ade10 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -639,7 +639,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/*
 	 * Reload esp0, LDT and the page table pointer:
 	 */
-	tss->x86_tss.sp0 = next->sp0;
+	load_sp0(tss, next);
 
 	/* 
 	 * Switch DS and ES.

commit ca241c75037b32e0216a68e39ad2801d04fa1f87
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:31 2008 +0100

    x86: unify tss_struct
    
    Although slighly different, the tss_struct is very similar in x86_64 and
    i386. The really different part, which matchs the hardware vision of it, is
    now called x86_hw_tss, and each of the architectures provides yours.
    It's then used as a field in the outter tss_struct.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 836a71adfa11..af56104b73ff 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -639,7 +639,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/*
 	 * Reload esp0, LDT and the page table pointer:
 	 */
-	tss->sp0 = next->sp0;
+	tss->x86_tss.sp0 = next->sp0;
 
 	/* 
 	 * Switch DS and ES.

commit 2f4aaf53c21e644ba0f581ce62b985d767388c64
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Wed Jan 30 13:31:20 2008 +0100

    x86, ptrace: remove bad comment
    
    Remove no longer correct comment.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index f91521e26335..836a71adfa11 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -608,10 +608,6 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
 	}
 
-	/*
-	 * Last branch recording recofiguration of trace hardware and
-	 * disentangling of trace data per task.
-	 */
 	if (test_tsk_thread_flag(prev_p, TIF_BTS_TRACE_TS))
 		ptrace_bts_take_timestamp(prev_p, BTS_TASK_DEPARTS);
 

commit 80fbb69a8d1268ef48dfe21da80e68cb01922f31
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:13 2008 +0100

    x86: introduce fill_ldt
    
    This patch introduces fill_ldt(), which populates a ldt descriptor
    from a user_desc in once, instead of relying in the LDT_entry_a and
    LDT_entry_b macros
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 86c310acc989..f91521e26335 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -457,8 +457,7 @@ static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
 	};
 	struct desc_struct *desc = (void *)t->thread.tls_array;
 	desc += tls;
-	desc->a = LDT_entry_a(&ud);
-	desc->b = LDT_entry_b(&ud);
+	fill_ldt(desc, &ud);
 }
 
 static inline u32 read_32bit_tls(struct task_struct *t, int tls)

commit 6842ef0e85a9cc1295f3ef933a230f863b01eb0f
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:11 2008 +0100

    x86: unify desc_struct
    
    This patch aims to make the access of struct desc_struct variables
    equal across architectures. In this patch, I unify the i386 and x86_64
    versions under an anonymous union, keeping the way they are accessed
    untouched (a and b for 32-bit code, individual bit-fields for 64-bit).
    
    This solution is not beautiful, but will allow us to integrate common
    code that differed by the way descriptors were used. This is to be viewed
    incrementally. There's simply too much code to be fixed at once.
    
    In the future, goal is to set up in a single way of acessing
    the desc_struct fields.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 843bf0c978a4..86c310acc989 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -455,7 +455,7 @@ static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
 		.limit_in_pages = 1,
 		.useable = 1,
 	};
-	struct n_desc_struct *desc = (void *)t->thread.tls_array;
+	struct desc_struct *desc = (void *)t->thread.tls_array;
 	desc += tls;
 	desc->a = LDT_entry_a(&ud);
 	desc->b = LDT_entry_b(&ud);

commit eee3af4a2c83a97fff107ddc445d9df6fded9ce4
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Wed Jan 30 13:31:09 2008 +0100

    x86, ptrace: support for branch trace store(BTS)
    
    Resend using different mail client
    
    Changes to the last version:
    - split implementation into two layers: ds/bts and ptrace
    - renamed TIF's
    - save/restore ds save area msr in __switch_to_xtra()
    - make block-stepping only look at BTF bit
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 057b5442ffda..843bf0c978a4 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -568,11 +568,21 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 				    struct tss_struct *tss)
 {
 	struct thread_struct *prev, *next;
+	unsigned long debugctl;
 
 	prev = &prev_p->thread,
 	next = &next_p->thread;
 
-	if (next->debugctlmsr != prev->debugctlmsr)
+	debugctl = prev->debugctlmsr;
+	if (next->ds_area_msr != prev->ds_area_msr) {
+		/* we clear debugctl to make sure DS
+		 * is not in use when we change it */
+		debugctl = 0;
+		wrmsrl(MSR_IA32_DEBUGCTLMSR, 0);
+		wrmsrl(MSR_IA32_DS_AREA, next->ds_area_msr);
+	}
+
+	if (next->debugctlmsr != debugctl)
 		wrmsrl(MSR_IA32_DEBUGCTLMSR, next->debugctlmsr);
 
 	if (test_tsk_thread_flag(next_p, TIF_DEBUG)) {
@@ -598,6 +608,16 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 		 */
 		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
 	}
+
+	/*
+	 * Last branch recording recofiguration of trace hardware and
+	 * disentangling of trace data per task.
+	 */
+	if (test_tsk_thread_flag(prev_p, TIF_BTS_TRACE_TS))
+		ptrace_bts_take_timestamp(prev_p, BTS_TASK_DEPARTS);
+
+	if (test_tsk_thread_flag(next_p, TIF_BTS_TRACE_TS))
+		ptrace_bts_take_timestamp(next_p, BTS_TASK_ARRIVES);
 }
 
 /*
@@ -701,8 +721,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/*
 	 * Now maybe reload the debug registers and handle I/O bitmaps
 	 */
-	if (unlikely((task_thread_info(next_p)->flags & _TIF_WORK_CTXSW))
-	    || test_tsk_thread_flag(prev_p, TIF_IO_BITMAP))
+	if (unlikely(task_thread_info(next_p)->flags & _TIF_WORK_CTXSW_NEXT ||
+		     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))
 		__switch_to_xtra(prev_p, next_p, tss);
 
 	/* If the task has used fpu the last 5 timeslices, just do a full

commit d89542229b657bdcce6a6f76168f9098ee3e9344
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:08 2008 +0100

    x86: put together equal pieces of system.h
    
    This patch puts together pieces of system_{32,64}.h that
    looks like the same. It's the first step towards integration
    of this file.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 4c4d8b3f046e..057b5442ffda 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -99,7 +99,7 @@ void exit_idle(void)
  * We use this if we don't have any better
  * idle routine..
  */
-static void default_idle(void)
+void default_idle(void)
 {
 	current_thread_info()->status &= ~TS_POLLING;
 	/*

commit 6612538ca9b38f0f45d0aec2aae8992c43313705
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Wed Jan 30 13:31:03 2008 +0100

    x86: clean up process_32/64.c
    
    White space and coding style clean up.
    Make process_32/64.c similar.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 238193822e23..4c4d8b3f046e 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -3,7 +3,7 @@
  *
  *  Pentium III FXSR, SSE support
  *	Gareth Hughes <gareth@valinux.com>, May 2000
- * 
+ *
  *  X86-64 port
  *	Andi Kleen.
  *
@@ -19,19 +19,19 @@
 #include <linux/cpu.h>
 #include <linux/errno.h>
 #include <linux/sched.h>
+#include <linux/fs.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
-#include <linux/fs.h>
 #include <linux/elfcore.h>
 #include <linux/smp.h>
 #include <linux/slab.h>
 #include <linux/user.h>
-#include <linux/module.h>
 #include <linux/a.out.h>
 #include <linux/interrupt.h>
+#include <linux/utsname.h>
 #include <linux/delay.h>
+#include <linux/module.h>
 #include <linux/ptrace.h>
-#include <linux/utsname.h>
 #include <linux/random.h>
 #include <linux/notifier.h>
 #include <linux/kprobes.h>
@@ -129,54 +129,12 @@ static void default_idle(void)
  * to poll the ->need_resched flag instead of waiting for the
  * cross-CPU IPI to arrive. Use this option with caution.
  */
-static void poll_idle (void)
+static void poll_idle(void)
 {
 	local_irq_enable();
 	cpu_relax();
 }
 
-static void do_nothing(void *unused)
-{
-}
-
-void cpu_idle_wait(void)
-{
-	unsigned int cpu, this_cpu = get_cpu();
-	cpumask_t map, tmp = current->cpus_allowed;
-
-	set_cpus_allowed(current, cpumask_of_cpu(this_cpu));
-	put_cpu();
-
-	cpus_clear(map);
-	for_each_online_cpu(cpu) {
-		per_cpu(cpu_idle_state, cpu) = 1;
-		cpu_set(cpu, map);
-	}
-
-	__get_cpu_var(cpu_idle_state) = 0;
-
-	wmb();
-	do {
-		ssleep(1);
-		for_each_online_cpu(cpu) {
-			if (cpu_isset(cpu, map) &&
-					!per_cpu(cpu_idle_state, cpu))
-				cpu_clear(cpu, map);
-		}
-		cpus_and(map, map, cpu_online_map);
-		/*
-		 * We waited 1 sec, if a CPU still did not call idle
-		 * it may be because it is in idle and not waking up
-		 * because it has nothing to do.
-		 * Give all the remaining CPUS a kick.
-		 */
-		smp_call_function_mask(map, do_nothing, 0, 0);
-	} while (!cpus_empty(map));
-
-	set_cpus_allowed(current, tmp);
-}
-EXPORT_SYMBOL_GPL(cpu_idle_wait);
-
 #ifdef CONFIG_HOTPLUG_CPU
 DECLARE_PER_CPU(int, cpu_state);
 
@@ -247,6 +205,47 @@ void cpu_idle(void)
 	}
 }
 
+static void do_nothing(void *unused)
+{
+}
+
+void cpu_idle_wait(void)
+{
+	unsigned int cpu, this_cpu = get_cpu();
+	cpumask_t map, tmp = current->cpus_allowed;
+
+	set_cpus_allowed(current, cpumask_of_cpu(this_cpu));
+	put_cpu();
+
+	cpus_clear(map);
+	for_each_online_cpu(cpu) {
+		per_cpu(cpu_idle_state, cpu) = 1;
+		cpu_set(cpu, map);
+	}
+
+	__get_cpu_var(cpu_idle_state) = 0;
+
+	wmb();
+	do {
+		ssleep(1);
+		for_each_online_cpu(cpu) {
+			if (cpu_isset(cpu, map) && !per_cpu(cpu_idle_state, cpu))
+				cpu_clear(cpu, map);
+		}
+		cpus_and(map, map, cpu_online_map);
+		/*
+		 * We waited 1 sec, if a CPU still did not call idle
+		 * it may be because it is in idle and not waking up
+		 * because it has nothing to do.
+		 * Give all the remaining CPUS a kick.
+		 */
+		smp_call_function_mask(map, do_nothing, 0, 0);
+	} while (!cpus_empty(map));
+
+	set_cpus_allowed(current, tmp);
+}
+EXPORT_SYMBOL_GPL(cpu_idle_wait);
+
 /*
  * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
  * which can obviate IPI to trigger checking of need_resched.
@@ -300,7 +299,7 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 	}
 }
 
-static int __init idle_setup (char *str)
+static int __init idle_setup(char *str)
 {
 	if (!strcmp(str, "poll")) {
 		printk("using polling idle threads.\n");
@@ -315,13 +314,13 @@ static int __init idle_setup (char *str)
 }
 early_param("idle", idle_setup);
 
-/* Prints also some state that isn't saved in the pt_regs */ 
+/* Prints also some state that isn't saved in the pt_regs */
 void __show_regs(struct pt_regs * regs)
 {
 	unsigned long cr0 = 0L, cr2 = 0L, cr3 = 0L, cr4 = 0L, fs, gs, shadowgs;
 	unsigned long d0, d1, d2, d3, d6, d7;
-	unsigned int fsindex,gsindex;
-	unsigned int ds,cs,es; 
+	unsigned int fsindex, gsindex;
+	unsigned int ds, cs, es;
 
 	printk("\n");
 	print_modules();
@@ -390,7 +389,7 @@ void exit_thread(void)
 	struct task_struct *me = current;
 	struct thread_struct *t = &me->thread;
 
-	if (me->thread.io_bitmap_ptr) { 
+	if (me->thread.io_bitmap_ptr) {
 		struct tss_struct *tss = &per_cpu(init_tss, get_cpu());
 
 		kfree(t->io_bitmap_ptr);
@@ -426,7 +425,7 @@ void flush_thread(void)
 	tsk->thread.debugreg3 = 0;
 	tsk->thread.debugreg6 = 0;
 	tsk->thread.debugreg7 = 0;
-	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));	
+	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
 	/*
 	 * Forget coprocessor state..
 	 */
@@ -449,7 +448,7 @@ void release_thread(struct task_struct *dead_task)
 
 static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
 {
-	struct user_desc ud = { 
+	struct user_desc ud = {
 		.base_addr = addr,
 		.limit = 0xfffff,
 		.seg_32bit = 1,
@@ -458,8 +457,8 @@ static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
 	};
 	struct n_desc_struct *desc = (void *)t->thread.tls_array;
 	desc += tls;
-	desc->a = LDT_entry_a(&ud); 
-	desc->b = LDT_entry_b(&ud); 
+	desc->a = LDT_entry_a(&ud);
+	desc->b = LDT_entry_b(&ud);
 }
 
 static inline u32 read_32bit_tls(struct task_struct *t, int tls)
@@ -516,7 +515,7 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
 		memcpy(p->thread.io_bitmap_ptr, me->thread.io_bitmap_ptr,
 				IO_BITMAP_BYTES);
 		set_tsk_thread_flag(p, TIF_IO_BITMAP);
-	} 
+	}
 
 	/*
 	 * Set a new TLS for the child thread?
@@ -544,11 +543,29 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
 /*
  * This special macro can be used to load a debugging register
  */
-#define loaddebug(thread,r) set_debugreg(thread->debugreg ## r, r)
+#define loaddebug(thread, r) set_debugreg(thread->debugreg ## r, r)
+
+/*
+ * Capture the user space registers if the task is not running (in user space)
+ */
+int dump_task_regs(struct task_struct *tsk, elf_gregset_t *regs)
+{
+	struct pt_regs *pp, ptregs;
+
+	pp = task_pt_regs(tsk);
+
+	ptregs = *pp;
+	ptregs.cs &= 0xffff;
+	ptregs.ss &= 0xffff;
+
+	elf_core_copy_regs(regs, &ptregs);
+
+	return 1;
+}
 
 static inline void __switch_to_xtra(struct task_struct *prev_p,
-			     	    struct task_struct *next_p,
-			     	    struct tss_struct *tss)
+				    struct task_struct *next_p,
+				    struct tss_struct *tss)
 {
 	struct thread_struct *prev, *next;
 
@@ -586,7 +603,7 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 /*
  *	switch_to(x,y) should switch tasks from x to y.
  *
- * This could still be optimized: 
+ * This could still be optimized:
  * - fold all the options into a flag word and test it with a single test.
  * - could test fs/gs bitsliced
  *
@@ -597,7 +614,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 {
 	struct thread_struct *prev = &prev_p->thread,
 				 *next = &next_p->thread;
-	int cpu = smp_processor_id();  
+	int cpu = smp_processor_id();
 	struct tss_struct *tss = &per_cpu(init_tss, cpu);
 
 	/* we're going to use this soon, after a few expensive things */
@@ -700,7 +717,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 /*
  * sys_execve() executes a new program.
  */
-asmlinkage 
+asmlinkage
 long sys_execve(char __user *name, char __user * __user *argv,
 		char __user * __user *envp, struct pt_regs regs)
 {
@@ -721,12 +738,12 @@ void set_personality_64bit(void)
 	/* inherit personality from parent */
 
 	/* Make sure to be in 64bit mode */
-	clear_thread_flag(TIF_IA32); 
+	clear_thread_flag(TIF_IA32);
 
 	/* TBD: overwrites user setup. Should have two bits.
 	   But 64bit processes have always behaved this way,
 	   so it's not too bad. The main problem is just that
-   	   32bit childs are affected again. */
+	   32bit childs are affected again. */
 	current->personality &= ~READ_IMPLIES_EXEC;
 }
 
@@ -819,19 +836,19 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		/* Not strictly needed for fs, but do it for symmetry
 		   with gs */
 		if (addr >= TASK_SIZE_OF(task))
-			return -EPERM; 
+			return -EPERM;
 		cpu = get_cpu();
-		/* handle small bases via the GDT because that's faster to 
+		/* handle small bases via the GDT because that's faster to
 		   switch. */
-		if (addr <= 0xffffffff) { 
+		if (addr <= 0xffffffff) {
 			set_32bit_tls(task, FS_TLS, addr);
-			if (doit) { 
-				load_TLS(&task->thread, cpu); 
+			if (doit) {
+				load_TLS(&task->thread, cpu);
 				asm volatile("movl %0,%%fs" :: "r"(FS_TLS_SEL));
 			}
 			task->thread.fsindex = FS_TLS_SEL;
 			task->thread.fs = 0;
-		} else { 
+		} else {
 			task->thread.fsindex = 0;
 			task->thread.fs = addr;
 			if (doit) {
@@ -843,24 +860,24 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		}
 		put_cpu();
 		break;
-	case ARCH_GET_FS: { 
-		unsigned long base; 
+	case ARCH_GET_FS: {
+		unsigned long base;
 		if (task->thread.fsindex == FS_TLS_SEL)
 			base = read_32bit_tls(task, FS_TLS);
 		else if (doit)
 			rdmsrl(MSR_FS_BASE, base);
 		else
 			base = task->thread.fs;
-		ret = put_user(base, (unsigned long __user *)addr); 
-		break; 
+		ret = put_user(base, (unsigned long __user *)addr);
+		break;
 	}
-	case ARCH_GET_GS: { 
+	case ARCH_GET_GS: {
 		unsigned long base;
 		unsigned gsindex;
 		if (task->thread.gsindex == GS_TLS_SEL)
 			base = read_32bit_tls(task, GS_TLS);
 		else if (doit) {
- 			asm("movl %%gs,%0" : "=r" (gsindex));
+			asm("movl %%gs,%0" : "=r" (gsindex));
 			if (gsindex)
 				rdmsrl(MSR_KERNEL_GS_BASE, base);
 			else
@@ -868,39 +885,21 @@ long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
 		}
 		else
 			base = task->thread.gs;
-		ret = put_user(base, (unsigned long __user *)addr); 
+		ret = put_user(base, (unsigned long __user *)addr);
 		break;
 	}
 
 	default:
 		ret = -EINVAL;
 		break;
-	} 
+	}
 
-	return ret;	
-} 
+	return ret;
+}
 
 long sys_arch_prctl(int code, unsigned long addr)
 {
 	return do_arch_prctl(current, code, addr);
-} 
-
-/* 
- * Capture the user space registers if the task is not running (in user space)
- */
-int dump_task_regs(struct task_struct *tsk, elf_gregset_t *regs)
-{
-	struct pt_regs *pp, ptregs;
-
-	pp = task_pt_regs(tsk);
-
-	ptregs = *pp; 
-	ptregs.cs &= 0xffff;
-	ptregs.ss &= 0xffff;
-
-	elf_core_copy_regs(regs, &ptregs);
- 
-	return 1;
 }
 
 unsigned long arch_align_stack(unsigned long sp)

commit faca62273b602ab482fb7d3d940dbf41ef08b00e
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Jan 30 13:31:02 2008 +0100

    x86: use generic register name in the thread and tss structures
    
    This changes size-specific register names (eip/rip, esp/rsp, etc.) to
    generic names in the thread and tss structures.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index efbb1a2eab97..238193822e23 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -493,9 +493,9 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
 	if (sp == ~0UL)
 		childregs->sp = (unsigned long)childregs;
 
-	p->thread.rsp = (unsigned long) childregs;
-	p->thread.rsp0 = (unsigned long) (childregs+1);
-	p->thread.userrsp = me->thread.userrsp; 
+	p->thread.sp = (unsigned long) childregs;
+	p->thread.sp0 = (unsigned long) (childregs+1);
+	p->thread.usersp = me->thread.usersp;
 
 	set_tsk_thread_flag(p, TIF_FORK);
 
@@ -607,7 +607,7 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/*
 	 * Reload esp0, LDT and the page table pointer:
 	 */
-	tss->rsp0 = next->rsp0;
+	tss->sp0 = next->sp0;
 
 	/* 
 	 * Switch DS and ES.
@@ -666,8 +666,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/* 
 	 * Switch the PDA and FPU contexts.
 	 */
-	prev->userrsp = read_pda(oldrsp); 
-	write_pda(oldrsp, next->userrsp); 
+	prev->usersp = read_pda(oldrsp);
+	write_pda(oldrsp, next->usersp);
 	write_pda(pcurrent, next_p); 
 
 	write_pda(kernelstack,
@@ -769,9 +769,9 @@ unsigned long get_wchan(struct task_struct *p)
 	if (!p || p == current || p->state==TASK_RUNNING)
 		return 0; 
 	stack = (unsigned long)task_stack_page(p);
-	if (p->thread.rsp < stack || p->thread.rsp > stack+THREAD_SIZE)
+	if (p->thread.sp < stack || p->thread.sp > stack+THREAD_SIZE)
 		return 0;
-	fp = *(u64 *)(p->thread.rsp);
+	fp = *(u64 *)(p->thread.sp);
 	do { 
 		if (fp < (unsigned long)stack ||
 		    fp > (unsigned long)stack+THREAD_SIZE)

commit 65ea5b0349903585bfed9720fa06f5edb4f1cd25
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Jan 30 13:30:56 2008 +0100

    x86: rename the struct pt_regs members for 32/64-bit consistency
    
    We have a lot of code which differs only by the naming of specific
    members of structures that contain registers.  In order to enable
    additional unifications, this patch drops the e- or r- size prefix
    from the register names in struct pt_regs, and drops the x- prefixes
    for segment registers on the 32-bit side.
    
    This patch also performs the equivalent renames in some additional
    places that might be candidates for unification in the future.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ae5eca17aa3c..efbb1a2eab97 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -257,13 +257,13 @@ void cpu_idle(void)
  * New with Core Duo processors, MWAIT can take some hints based on CPU
  * capability.
  */
-void mwait_idle_with_hints(unsigned long eax, unsigned long ecx)
+void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 {
 	if (!need_resched()) {
 		__monitor((void *)&current_thread_info()->flags, 0, 0);
 		smp_mb();
 		if (!need_resched())
-			__mwait(eax, ecx);
+			__mwait(ax, cx);
 	}
 }
 
@@ -330,16 +330,16 @@ void __show_regs(struct pt_regs * regs)
 		init_utsname()->release,
 		(int)strcspn(init_utsname()->version, " "),
 		init_utsname()->version);
-	printk("RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->rip);
-	printk_address(regs->rip); 
-	printk("RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss, regs->rsp,
-		regs->eflags);
+	printk("RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
+	printk_address(regs->ip);
+	printk("RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss, regs->sp,
+		regs->flags);
 	printk("RAX: %016lx RBX: %016lx RCX: %016lx\n",
-	       regs->rax, regs->rbx, regs->rcx);
+	       regs->ax, regs->bx, regs->cx);
 	printk("RDX: %016lx RSI: %016lx RDI: %016lx\n",
-	       regs->rdx, regs->rsi, regs->rdi); 
+	       regs->dx, regs->si, regs->di);
 	printk("RBP: %016lx R08: %016lx R09: %016lx\n",
-	       regs->rbp, regs->r8, regs->r9); 
+	       regs->bp, regs->r8, regs->r9);
 	printk("R10: %016lx R11: %016lx R12: %016lx\n",
 	       regs->r10, regs->r11, regs->r12); 
 	printk("R13: %016lx R14: %016lx R15: %016lx\n",
@@ -476,7 +476,7 @@ void prepare_to_copy(struct task_struct *tsk)
 	unlazy_fpu(tsk);
 }
 
-int copy_thread(int nr, unsigned long clone_flags, unsigned long rsp, 
+int copy_thread(int nr, unsigned long clone_flags, unsigned long sp,
 		unsigned long unused,
 	struct task_struct * p, struct pt_regs * regs)
 {
@@ -488,10 +488,10 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long rsp,
 			(THREAD_SIZE + task_stack_page(p))) - 1;
 	*childregs = *regs;
 
-	childregs->rax = 0;
-	childregs->rsp = rsp;
-	if (rsp == ~0UL)
-		childregs->rsp = (unsigned long)childregs;
+	childregs->ax = 0;
+	childregs->sp = sp;
+	if (sp == ~0UL)
+		childregs->sp = (unsigned long)childregs;
 
 	p->thread.rsp = (unsigned long) childregs;
 	p->thread.rsp0 = (unsigned long) (childregs+1);
@@ -525,7 +525,7 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long rsp,
 #ifdef CONFIG_IA32_EMULATION
 		if (test_thread_flag(TIF_IA32))
 			err = do_set_thread_area(p, -1,
-				(struct user_desc __user *)childregs->rsi, 0);
+				(struct user_desc __user *)childregs->si, 0);
 		else 			
 #endif	 
 			err = do_arch_prctl(p, ARCH_SET_FS, childregs->r8); 
@@ -732,7 +732,7 @@ void set_personality_64bit(void)
 
 asmlinkage long sys_fork(struct pt_regs *regs)
 {
-	return do_fork(SIGCHLD, regs->rsp, regs, 0, NULL, NULL);
+	return do_fork(SIGCHLD, regs->sp, regs, 0, NULL, NULL);
 }
 
 asmlinkage long
@@ -740,7 +740,7 @@ sys_clone(unsigned long clone_flags, unsigned long newsp,
 	  void __user *parent_tid, void __user *child_tid, struct pt_regs *regs)
 {
 	if (!newsp)
-		newsp = regs->rsp;
+		newsp = regs->sp;
 	return do_fork(clone_flags, newsp, regs, 0, parent_tid, child_tid);
 }
 
@@ -756,14 +756,14 @@ sys_clone(unsigned long clone_flags, unsigned long newsp,
  */
 asmlinkage long sys_vfork(struct pt_regs *regs)
 {
-	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->rsp, regs, 0,
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->sp, regs, 0,
 		    NULL, NULL);
 }
 
 unsigned long get_wchan(struct task_struct *p)
 {
 	unsigned long stack;
-	u64 fp,rip;
+	u64 fp,ip;
 	int count = 0;
 
 	if (!p || p == current || p->state==TASK_RUNNING)
@@ -776,9 +776,9 @@ unsigned long get_wchan(struct task_struct *p)
 		if (fp < (unsigned long)stack ||
 		    fp > (unsigned long)stack+THREAD_SIZE)
 			return 0; 
-		rip = *(u64 *)(fp+8); 
-		if (!in_sched_functions(rip))
-			return rip; 
+		ip = *(u64 *)(fp+8);
+		if (!in_sched_functions(ip))
+			return ip;
 		fp = *(u64 *)fp; 
 	} while (count++ < 16); 
 	return 0;

commit 7e9916040b3020d0f36d68bb7512e3b80b623097
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Jan 30 13:30:54 2008 +0100

    x86: debugctlmsr context switch
    
    This adds low-level support for a per-thread value of MSR_IA32_DEBUGCTLMSR.
    The per-thread value is switched in when TIF_DEBUGCTLMSR is set.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index f7356e5517f6..ae5eca17aa3c 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -555,6 +555,9 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
 	prev = &prev_p->thread,
 	next = &next_p->thread;
 
+	if (next->debugctlmsr != prev->debugctlmsr)
+		wrmsrl(MSR_IA32_DEBUGCTLMSR, next->debugctlmsr);
+
 	if (test_tsk_thread_flag(next_p, TIF_DEBUG)) {
 		loaddebug(next, 0);
 		loaddebug(next, 1);

commit e1f287735c1e58c653b516931b5d3dd899edcb77
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Jan 30 13:30:50 2008 +0100

    x86 single_step: TIF_FORCED_TF
    
    This changes the single-step support to use a new thread_info flag
    TIF_FORCED_TF instead of the PT_DTRACE flag in task_struct.ptrace.
    This keeps arch implementation uses out of this non-arch field.
    
    This changes the ptrace access to eflags to mask TF and maintain
    the TIF_FORCED_TF flag directly if userland sets TF, instead of
    relying on ptrace_signal_deliver.  The 64-bit and 32-bit kernels
    are harmonized on this same behavior.  The ptrace_signal_deliver
    approach works now, but this change makes the low-level register
    access code reliable when called from different contexts than a
    ptrace stop, which will be possible in the future.
    
    The 64-bit do_debug exception handler is also changed not to clear TF
    from user-mode registers.  This matches the 32-bit kernel's behavior.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ccc9d68d5a58..f7356e5517f6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -709,11 +709,6 @@ long sys_execve(char __user *name, char __user * __user *argv,
 	if (IS_ERR(filename)) 
 		return error;
 	error = do_execve(filename, argv, envp, &regs); 
-	if (error == 0) {
-		task_lock(current);
-		current->ptrace &= ~PT_DTRACE;
-		task_unlock(current);
-	}
 	putname(filename);
 	return error;
 }

commit efd1ca52d04d2f6df337a3332cee56cd60e6d4c4
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Jan 30 13:30:46 2008 +0100

    x86: TLS cleanup
    
    This consolidates the four different places that implemented the same
    encoding magic for the GDT-slot 32-bit TLS support.  The old tls32.c was
    renamed and is now only slightly modified to be the shared implementation.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Zachary Amsden <zach@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 9ea1d7546f80..ccc9d68d5a58 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -524,7 +524,8 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long rsp,
 	if (clone_flags & CLONE_SETTLS) {
 #ifdef CONFIG_IA32_EMULATION
 		if (test_thread_flag(TIF_IA32))
-			err = ia32_child_tls(p, childregs); 
+			err = do_set_thread_area(p, -1,
+				(struct user_desc __user *)childregs->rsi, 0);
 		else 			
 #endif	 
 			err = do_arch_prctl(p, ARCH_SET_FS, childregs->r8); 

commit 91394eb0975b3771dde7071a0825c6df6c20ff8a
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Jan 30 13:30:45 2008 +0100

    x86: use get_desc_base
    
    This changes a couple of places to use the get_desc_base function.
    They were duplicating the same calculation with different equivalent code.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index aa9414ed74c7..9ea1d7546f80 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -464,11 +464,7 @@ static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
 
 static inline u32 read_32bit_tls(struct task_struct *t, int tls)
 {
-	struct desc_struct *desc = (void *)t->thread.tls_array;
-	desc += tls;
-	return desc->base0 | 
-		(((u32)desc->base1) << 16) | 
-		(((u32)desc->base2) << 24);
+	return get_desc_base(&t->thread.tls_array[tls]);
 }
 
 /*

commit c1d171a002942ea2d93b4fbd0c9583c56fce0772
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Wed Jan 30 13:30:40 2008 +0100

    x86: randomize brk
    
    Randomize the location of the heap (brk) for i386 and x86_64.  The range is
    randomized in the range starting at current brk location up to 0x02000000
    offset for both architectures.  This, together with
    pie-executable-randomization.patch and
    pie-executable-randomization-fix.patch, should make the address space
    randomization on i386 and x86_64 complete.
    
    Arjan says:
    
    This is known to break older versions of some emacs variants, whose dumper
    code assumed that the last variable declared in the program is equal to the
    start of the dynamically allocated memory region.
    
    (The dumper is the code where emacs effectively dumps core at the end of it's
    compilation stage; this coredump is then loaded as the main program during
    normal use)
    
    iirc this was 5 years or so; we found this way back when I was at RH and we
    first did the security stuff there (including this brk randomization).  It
    wasn't all variants of emacs, and it got fixed as a result (I vaguely remember
    that emacs already had code to deal with it for other archs/oses, just
    ifdeffed wrongly).
    
    It's a rare and wrong assumption as a general thing, just on x86 it mostly
    happened to be true (but to be honest, it'll break too if gcc does
    something fancy or if the linker does a non-standard order).  Still its
    something we should at least document.
    
    Note 2: afaik it only broke the emacs *build*.  I'm not 100% sure about that
    (it IS 5 years ago) though.
    
    [ akpm@linux-foundation.org: deuglification ]
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Jakub Jelinek <jakub@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 98d85952f574..aa9414ed74c7 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -914,3 +914,10 @@ unsigned long arch_align_stack(unsigned long sp)
 		sp -= get_random_int() % 8192;
 	return sp & ~0xf;
 }
+
+unsigned long arch_randomize_brk(struct mm_struct *mm)
+{
+	unsigned long range_end = mm->brk + 0x02000000;
+	return randomize_range(mm->brk, range_end, 0) ? : mm->brk;
+}
+

commit a72368dd37f6ae333fbab03598e46a995d91decc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 30 13:30:28 2008 +0100

    x86: remove dead code and exports
    
    No users.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 2c9e59448f4c..98d85952f574 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -72,13 +72,6 @@ void idle_notifier_register(struct notifier_block *n)
 {
 	atomic_notifier_chain_register(&idle_notifier, n);
 }
-EXPORT_SYMBOL_GPL(idle_notifier_register);
-
-void idle_notifier_unregister(struct notifier_block *n)
-{
-	atomic_notifier_chain_unregister(&idle_notifier, n);
-}
-EXPORT_SYMBOL(idle_notifier_unregister);
 
 void enter_idle(void)
 {

commit 39d44a51474a52bec6d72d30ebc76f5159101d90
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Wed Jan 30 13:30:06 2008 +0100

    x86: enable irq in default_idle on 64-bit
    
    local_irq_enable() is missing after sched_clock_idle_wakeup_event().
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 40fed477f3e5..2c9e59448f4c 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -126,8 +126,8 @@ static void default_idle(void)
 		t1 = ktime_get();
 		t1n = ktime_to_ns(t1);
 		sched_clock_idle_wakeup_event(t1n - t0n);
-	} else
-		local_irq_enable();
+	}
+	local_irq_enable();
 	current_thread_info()->status |= TS_POLLING;
 }
 

commit 5ee613b6751cd91db4b6bd7c1dc9d2f9cf65cde2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 30 13:30:06 2008 +0100

    x86: idle wakeup event in the HLT loop
    
    do a proper idle-wakeup event on HLT as well - some CPUs stop the TSC
    in HLT too, not just when going through the ACPI methods.
    
    (the ACPI idle code already does this.)
    
    [ update the 64-bit side too, as noticed by Jiri Slaby. ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index c2db7ef93565..40fed477f3e5 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -116,9 +116,16 @@ static void default_idle(void)
 	smp_mb();
 	local_irq_disable();
 	if (!need_resched()) {
-		/* Enables interrupts one instruction before HLT.
-		   x86 special cases this so there is no race. */
-		safe_halt();
+		ktime_t t0, t1;
+		u64 t0n, t1n;
+
+		t0 = ktime_get();
+		t0n = ktime_to_ns(t0);
+		safe_halt();	/* enables interrupts racelessly */
+		local_irq_disable();
+		t1 = ktime_get();
+		t1n = ktime_to_ns(t1);
+		sched_clock_idle_wakeup_event(t1n - t0n);
 	} else
 		local_irq_enable();
 	current_thread_info()->status |= TS_POLLING;

commit b10db7f0d2b589a7f88dc3026e150756cb437a28
Author: Pavel Machek <pavel@ucw.cz>
Date:   Wed Jan 30 13:30:00 2008 +0100

    time: more timer related cleanups
    
    I was confused by FSEC = 10^15 NSEC statement, plus small whitespace
    fixes. When there's copyright, there should be GPL.
    
    Signed-off-by: Pavel Machek <pavel@suse.cz>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index ab79e1dfa023..c2db7ef93565 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -207,7 +207,7 @@ static inline void play_dead(void)
  * low exit latency (ie sit in a loop waiting for
  * somebody to say that they'd like to reschedule)
  */
-void cpu_idle (void)
+void cpu_idle(void)
 {
 	current_thread_info()->status |= TS_POLLING;
 	/* endless idle loop with no priority at all */

commit 40d6a146629b98d8e322b6f9332b182c7cbff3df
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Jan 14 00:55:10 2008 -0800

    Kick CPUS that might be sleeping in cpus_idle_wait
    
    Sometimes cpu_idle_wait gets stuck because it might miss CPUS that are
    already in idle, have no tasks waiting to run and have no interrupts going
    to them.  This is common on bootup when switching cpu idle governors.
    
    This patch gives those CPUS that don't check in an IPI kick.
    
     Background:
     -----------
    I notice this while developing the mcount patches, that every once in a
    while the system would hang. Looking deeper, the hang was always at boot
    up when registering init_menu of the cpu_idle menu governor. Talking
    with Thomas Gliexner, we discovered that one of the CPUS had no timer
    events scheduled for it and it was in idle (running with NO_HZ). So the
    CPU would not set the cpu_idle_state bit.
    
    Hitting sysrq-t a few times would eventually route the interrupt to the
    stuck CPU and the system would continue.
    
    Note, I would have used the PDA isidle but that is set after the
    cpu_idle_state bit is cleared, and would leave a window open where we
    may miss being kicked.
    
    hmm, looking closer at this, we still have a small race window between
    clearing the cpu_idle_state and disabling interrupts (hence the RFC).
    
        CPU0:                          CPU 1:
      ---------                       ---------
     cpu_idle_wait():                 cpu_idle():
          |                           __cpu_cpu_var(is_idle) = 1;
          |                           if (__get_cpu_var(cpu_idle_state)) /* == 0 */
     per_cpu(cpu_idle_state, 1) = 1;         |
     if (per_cpu(is_idle, 1)) /* == 1 */     |
     smp_call_function(1)                    |
          |                             receives ipi and runs do_nothing.
     wait on map == empty               idle();
       /* waits forever */
    
    So really we need interrupts off for most of this then. One might think
    that we could simply clear the cpu_idle_state from do_nothing, but I'm
    assuming that cpu_idle governors can be removed, and this might cause a
    race that a governor might be used after the module was removed.
    
    Venki said:
    
      I think your RFC patch is the right solution here.  As I see it, there is
      no race with your RFC patch.  As long as you call a dummy smp_call_function
      on all CPUs, we should be OK.  We can get rid of cpu_idle_state and the
      current wait forever logic altogether with dummy smp_call_function.  And so
      there wont be any wait forever scenario.
    
      The whole point of cpu_idle_wait() is to make all CPUs come out of idle
      loop atleast once.  The caller will use cpu_idle_wait something like this.
    
      // Want to change idle handler
    
      - Switch global idle handler to always present default_idle
    
      - call cpu_idle_wait so that all cpus come out of idle for an instant
        and stop using old idle pointer and start using default idle
    
      - Change the idle handler to a new handler
    
      - optional cpu_idle_wait if you want all cpus to start using the new
        handler immediately.
    
    Maybe the below 1s patch is safe bet for .24.  But for .25, I would say we
    just replace all complicated logic by simple dummy smp_call_function and
    remove cpu_idle_state altogether.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Len Brown <lenb@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6309b275cb9c..ab79e1dfa023 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -135,6 +135,10 @@ static void poll_idle (void)
 	cpu_relax();
 }
 
+static void do_nothing(void *unused)
+{
+}
+
 void cpu_idle_wait(void)
 {
 	unsigned int cpu, this_cpu = get_cpu();
@@ -160,6 +164,13 @@ void cpu_idle_wait(void)
 				cpu_clear(cpu, map);
 		}
 		cpus_and(map, map, cpu_online_map);
+		/*
+		 * We waited 1 sec, if a CPU still did not call idle
+		 * it may be because it is in idle and not waking up
+		 * because it has nothing to do.
+		 * Give all the remaining CPUS a kick.
+		 */
+		smp_call_function_mask(map, do_nothing, 0, 0);
 	} while (!cpus_empty(map));
 
 	set_cpus_allowed(current, tmp);

commit f438d914b220051d4cbc65cbc5d98e163c85c93b
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Tue Oct 16 01:27:49 2007 -0700

    kprobes: support kretprobe blacklist
    
    Introduce architecture dependent kretprobe blacklists to prohibit users
    from inserting return probes on the function in which kprobes can be
    inserted but kretprobes can not.
    
    This patch also removes "__kprobes" mark from "__switch_to" on x86_64 and
    registers "__switch_to" to the blacklist on x86-64, because that mark is to
    prohibit user from inserting only kretprobe.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 7352d4b377e6..6309b275cb9c 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -581,7 +581,7 @@ static inline void __switch_to_xtra(struct task_struct *prev_p,
  *
  * Kprobes not supported here. Set the probe on schedule instead.
  */
-__kprobes struct task_struct *
+struct task_struct *
 __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 {
 	struct thread_struct *prev = &prev_p->thread,

commit 835c34a1687f524c37d4fb8bad18d642c74bed8d
Author: Dave Jones <davej@redhat.com>
Date:   Fri Oct 12 21:10:53 2007 -0400

    Delete filenames in comments.
    
    Since the x86 merge, lots of files that referenced their own filenames
    are no longer correct.  Rather than keep them up to date, just delete
    them, as they add no real value.
    
    Additionally:
    - fix up comment formatting in scx200_32.c
    - Remove a credit from myself in setup_64.c from a time when we had no SCM
    - remove longwinded history from tsc_32.c which can be figured out from
      git.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6f9dbbe65eef..7352d4b377e6 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -1,6 +1,4 @@
 /*
- *  linux/arch/x86-64/kernel/process.c
- *
  *  Copyright (C) 1995  Linus Torvalds
  *
  *  Pentium III FXSR, SSE support

commit 02290683343391a50f45599710295dafa2ddd018
Author: Chris Wright <chrisw@sous-sol.org>
Date:   Fri Oct 12 23:04:07 2007 +0200

    x86_64: prepare idle loop for dynamic ticks
    
    Add tick_nohz_{stop,restart}_sched_tick to idle loop in prepartion for turning
    on dynticks.  These are just noops until NO_HZ is enabled.
    
    Signed-off-by: Chris Wright <chrisw@sous-sol.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 98956555450b..6f9dbbe65eef 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -38,6 +38,7 @@
 #include <linux/notifier.h>
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
+#include <linux/tick.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -208,6 +209,8 @@ void cpu_idle (void)
 			if (__get_cpu_var(cpu_idle_state))
 				__get_cpu_var(cpu_idle_state) = 0;
 
+			tick_nohz_stop_sched_tick();
+
 			rmb();
 			idle = pm_idle;
 			if (!idle)
@@ -228,6 +231,7 @@ void cpu_idle (void)
 			__exit_idle();
 		}
 
+		tick_nohz_restart_sched_tick();
 		preempt_enable_no_resched();
 		schedule();
 		preempt_disable();

commit 250c22777fe1ccd7ac588579a6c16db4c0161cc5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:17:24 2007 +0200

    x86_64: move kernel
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
new file mode 100644
index 000000000000..98956555450b
--- /dev/null
+++ b/arch/x86/kernel/process_64.c
@@ -0,0 +1,903 @@
+/*
+ *  linux/arch/x86-64/kernel/process.c
+ *
+ *  Copyright (C) 1995  Linus Torvalds
+ *
+ *  Pentium III FXSR, SSE support
+ *	Gareth Hughes <gareth@valinux.com>, May 2000
+ * 
+ *  X86-64 port
+ *	Andi Kleen.
+ *
+ *	CPU hotplug support - ashok.raj@intel.com
+ */
+
+/*
+ * This file handles the architecture-dependent parts of process handling..
+ */
+
+#include <stdarg.h>
+
+#include <linux/cpu.h>
+#include <linux/errno.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/fs.h>
+#include <linux/elfcore.h>
+#include <linux/smp.h>
+#include <linux/slab.h>
+#include <linux/user.h>
+#include <linux/module.h>
+#include <linux/a.out.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/ptrace.h>
+#include <linux/utsname.h>
+#include <linux/random.h>
+#include <linux/notifier.h>
+#include <linux/kprobes.h>
+#include <linux/kdebug.h>
+
+#include <asm/uaccess.h>
+#include <asm/pgtable.h>
+#include <asm/system.h>
+#include <asm/io.h>
+#include <asm/processor.h>
+#include <asm/i387.h>
+#include <asm/mmu_context.h>
+#include <asm/pda.h>
+#include <asm/prctl.h>
+#include <asm/desc.h>
+#include <asm/proto.h>
+#include <asm/ia32.h>
+#include <asm/idle.h>
+
+asmlinkage extern void ret_from_fork(void);
+
+unsigned long kernel_thread_flags = CLONE_VM | CLONE_UNTRACED;
+
+unsigned long boot_option_idle_override = 0;
+EXPORT_SYMBOL(boot_option_idle_override);
+
+/*
+ * Powermanagement idle function, if any..
+ */
+void (*pm_idle)(void);
+EXPORT_SYMBOL(pm_idle);
+static DEFINE_PER_CPU(unsigned int, cpu_idle_state);
+
+static ATOMIC_NOTIFIER_HEAD(idle_notifier);
+
+void idle_notifier_register(struct notifier_block *n)
+{
+	atomic_notifier_chain_register(&idle_notifier, n);
+}
+EXPORT_SYMBOL_GPL(idle_notifier_register);
+
+void idle_notifier_unregister(struct notifier_block *n)
+{
+	atomic_notifier_chain_unregister(&idle_notifier, n);
+}
+EXPORT_SYMBOL(idle_notifier_unregister);
+
+void enter_idle(void)
+{
+	write_pda(isidle, 1);
+	atomic_notifier_call_chain(&idle_notifier, IDLE_START, NULL);
+}
+
+static void __exit_idle(void)
+{
+	if (test_and_clear_bit_pda(0, isidle) == 0)
+		return;
+	atomic_notifier_call_chain(&idle_notifier, IDLE_END, NULL);
+}
+
+/* Called from interrupts to signify idle end */
+void exit_idle(void)
+{
+	/* idle loop has pid 0 */
+	if (current->pid)
+		return;
+	__exit_idle();
+}
+
+/*
+ * We use this if we don't have any better
+ * idle routine..
+ */
+static void default_idle(void)
+{
+	current_thread_info()->status &= ~TS_POLLING;
+	/*
+	 * TS_POLLING-cleared state must be visible before we
+	 * test NEED_RESCHED:
+	 */
+	smp_mb();
+	local_irq_disable();
+	if (!need_resched()) {
+		/* Enables interrupts one instruction before HLT.
+		   x86 special cases this so there is no race. */
+		safe_halt();
+	} else
+		local_irq_enable();
+	current_thread_info()->status |= TS_POLLING;
+}
+
+/*
+ * On SMP it's slightly faster (but much more power-consuming!)
+ * to poll the ->need_resched flag instead of waiting for the
+ * cross-CPU IPI to arrive. Use this option with caution.
+ */
+static void poll_idle (void)
+{
+	local_irq_enable();
+	cpu_relax();
+}
+
+void cpu_idle_wait(void)
+{
+	unsigned int cpu, this_cpu = get_cpu();
+	cpumask_t map, tmp = current->cpus_allowed;
+
+	set_cpus_allowed(current, cpumask_of_cpu(this_cpu));
+	put_cpu();
+
+	cpus_clear(map);
+	for_each_online_cpu(cpu) {
+		per_cpu(cpu_idle_state, cpu) = 1;
+		cpu_set(cpu, map);
+	}
+
+	__get_cpu_var(cpu_idle_state) = 0;
+
+	wmb();
+	do {
+		ssleep(1);
+		for_each_online_cpu(cpu) {
+			if (cpu_isset(cpu, map) &&
+					!per_cpu(cpu_idle_state, cpu))
+				cpu_clear(cpu, map);
+		}
+		cpus_and(map, map, cpu_online_map);
+	} while (!cpus_empty(map));
+
+	set_cpus_allowed(current, tmp);
+}
+EXPORT_SYMBOL_GPL(cpu_idle_wait);
+
+#ifdef CONFIG_HOTPLUG_CPU
+DECLARE_PER_CPU(int, cpu_state);
+
+#include <asm/nmi.h>
+/* We halt the CPU with physical CPU hotplug */
+static inline void play_dead(void)
+{
+	idle_task_exit();
+	wbinvd();
+	mb();
+	/* Ack it */
+	__get_cpu_var(cpu_state) = CPU_DEAD;
+
+	local_irq_disable();
+	while (1)
+		halt();
+}
+#else
+static inline void play_dead(void)
+{
+	BUG();
+}
+#endif /* CONFIG_HOTPLUG_CPU */
+
+/*
+ * The idle thread. There's no useful work to be
+ * done, so just try to conserve power and have a
+ * low exit latency (ie sit in a loop waiting for
+ * somebody to say that they'd like to reschedule)
+ */
+void cpu_idle (void)
+{
+	current_thread_info()->status |= TS_POLLING;
+	/* endless idle loop with no priority at all */
+	while (1) {
+		while (!need_resched()) {
+			void (*idle)(void);
+
+			if (__get_cpu_var(cpu_idle_state))
+				__get_cpu_var(cpu_idle_state) = 0;
+
+			rmb();
+			idle = pm_idle;
+			if (!idle)
+				idle = default_idle;
+			if (cpu_is_offline(smp_processor_id()))
+				play_dead();
+			/*
+			 * Idle routines should keep interrupts disabled
+			 * from here on, until they go to idle.
+			 * Otherwise, idle callbacks can misfire.
+			 */
+			local_irq_disable();
+			enter_idle();
+			idle();
+			/* In many cases the interrupt that ended idle
+			   has already called exit_idle. But some idle
+			   loops can be woken up without interrupt. */
+			__exit_idle();
+		}
+
+		preempt_enable_no_resched();
+		schedule();
+		preempt_disable();
+	}
+}
+
+/*
+ * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
+ * which can obviate IPI to trigger checking of need_resched.
+ * We execute MONITOR against need_resched and enter optimized wait state
+ * through MWAIT. Whenever someone changes need_resched, we would be woken
+ * up from MWAIT (without an IPI).
+ *
+ * New with Core Duo processors, MWAIT can take some hints based on CPU
+ * capability.
+ */
+void mwait_idle_with_hints(unsigned long eax, unsigned long ecx)
+{
+	if (!need_resched()) {
+		__monitor((void *)&current_thread_info()->flags, 0, 0);
+		smp_mb();
+		if (!need_resched())
+			__mwait(eax, ecx);
+	}
+}
+
+/* Default MONITOR/MWAIT with no hints, used for default C1 state */
+static void mwait_idle(void)
+{
+	if (!need_resched()) {
+		__monitor((void *)&current_thread_info()->flags, 0, 0);
+		smp_mb();
+		if (!need_resched())
+			__sti_mwait(0, 0);
+		else
+			local_irq_enable();
+	} else {
+		local_irq_enable();
+	}
+}
+
+void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
+{
+	static int printed;
+	if (cpu_has(c, X86_FEATURE_MWAIT)) {
+		/*
+		 * Skip, if setup has overridden idle.
+		 * One CPU supports mwait => All CPUs supports mwait
+		 */
+		if (!pm_idle) {
+			if (!printed) {
+				printk(KERN_INFO "using mwait in idle threads.\n");
+				printed = 1;
+			}
+			pm_idle = mwait_idle;
+		}
+	}
+}
+
+static int __init idle_setup (char *str)
+{
+	if (!strcmp(str, "poll")) {
+		printk("using polling idle threads.\n");
+		pm_idle = poll_idle;
+	} else if (!strcmp(str, "mwait"))
+		force_mwait = 1;
+	else
+		return -1;
+
+	boot_option_idle_override = 1;
+	return 0;
+}
+early_param("idle", idle_setup);
+
+/* Prints also some state that isn't saved in the pt_regs */ 
+void __show_regs(struct pt_regs * regs)
+{
+	unsigned long cr0 = 0L, cr2 = 0L, cr3 = 0L, cr4 = 0L, fs, gs, shadowgs;
+	unsigned long d0, d1, d2, d3, d6, d7;
+	unsigned int fsindex,gsindex;
+	unsigned int ds,cs,es; 
+
+	printk("\n");
+	print_modules();
+	printk("Pid: %d, comm: %.20s %s %s %.*s\n",
+		current->pid, current->comm, print_tainted(),
+		init_utsname()->release,
+		(int)strcspn(init_utsname()->version, " "),
+		init_utsname()->version);
+	printk("RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->rip);
+	printk_address(regs->rip); 
+	printk("RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss, regs->rsp,
+		regs->eflags);
+	printk("RAX: %016lx RBX: %016lx RCX: %016lx\n",
+	       regs->rax, regs->rbx, regs->rcx);
+	printk("RDX: %016lx RSI: %016lx RDI: %016lx\n",
+	       regs->rdx, regs->rsi, regs->rdi); 
+	printk("RBP: %016lx R08: %016lx R09: %016lx\n",
+	       regs->rbp, regs->r8, regs->r9); 
+	printk("R10: %016lx R11: %016lx R12: %016lx\n",
+	       regs->r10, regs->r11, regs->r12); 
+	printk("R13: %016lx R14: %016lx R15: %016lx\n",
+	       regs->r13, regs->r14, regs->r15); 
+
+	asm("movl %%ds,%0" : "=r" (ds)); 
+	asm("movl %%cs,%0" : "=r" (cs)); 
+	asm("movl %%es,%0" : "=r" (es)); 
+	asm("movl %%fs,%0" : "=r" (fsindex));
+	asm("movl %%gs,%0" : "=r" (gsindex));
+
+	rdmsrl(MSR_FS_BASE, fs);
+	rdmsrl(MSR_GS_BASE, gs); 
+	rdmsrl(MSR_KERNEL_GS_BASE, shadowgs); 
+
+	cr0 = read_cr0();
+	cr2 = read_cr2();
+	cr3 = read_cr3();
+	cr4 = read_cr4();
+
+	printk("FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n", 
+	       fs,fsindex,gs,gsindex,shadowgs); 
+	printk("CS:  %04x DS: %04x ES: %04x CR0: %016lx\n", cs, ds, es, cr0); 
+	printk("CR2: %016lx CR3: %016lx CR4: %016lx\n", cr2, cr3, cr4);
+
+	get_debugreg(d0, 0);
+	get_debugreg(d1, 1);
+	get_debugreg(d2, 2);
+	printk("DR0: %016lx DR1: %016lx DR2: %016lx\n", d0, d1, d2);
+	get_debugreg(d3, 3);
+	get_debugreg(d6, 6);
+	get_debugreg(d7, 7);
+	printk("DR3: %016lx DR6: %016lx DR7: %016lx\n", d3, d6, d7);
+}
+
+void show_regs(struct pt_regs *regs)
+{
+	printk("CPU %d:", smp_processor_id());
+	__show_regs(regs);
+	show_trace(NULL, regs, (void *)(regs + 1));
+}
+
+/*
+ * Free current thread data structures etc..
+ */
+void exit_thread(void)
+{
+	struct task_struct *me = current;
+	struct thread_struct *t = &me->thread;
+
+	if (me->thread.io_bitmap_ptr) { 
+		struct tss_struct *tss = &per_cpu(init_tss, get_cpu());
+
+		kfree(t->io_bitmap_ptr);
+		t->io_bitmap_ptr = NULL;
+		clear_thread_flag(TIF_IO_BITMAP);
+		/*
+		 * Careful, clear this in the TSS too:
+		 */
+		memset(tss->io_bitmap, 0xff, t->io_bitmap_max);
+		t->io_bitmap_max = 0;
+		put_cpu();
+	}
+}
+
+void flush_thread(void)
+{
+	struct task_struct *tsk = current;
+
+	if (test_tsk_thread_flag(tsk, TIF_ABI_PENDING)) {
+		clear_tsk_thread_flag(tsk, TIF_ABI_PENDING);
+		if (test_tsk_thread_flag(tsk, TIF_IA32)) {
+			clear_tsk_thread_flag(tsk, TIF_IA32);
+		} else {
+			set_tsk_thread_flag(tsk, TIF_IA32);
+			current_thread_info()->status |= TS_COMPAT;
+		}
+	}
+	clear_tsk_thread_flag(tsk, TIF_DEBUG);
+
+	tsk->thread.debugreg0 = 0;
+	tsk->thread.debugreg1 = 0;
+	tsk->thread.debugreg2 = 0;
+	tsk->thread.debugreg3 = 0;
+	tsk->thread.debugreg6 = 0;
+	tsk->thread.debugreg7 = 0;
+	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));	
+	/*
+	 * Forget coprocessor state..
+	 */
+	clear_fpu(tsk);
+	clear_used_math();
+}
+
+void release_thread(struct task_struct *dead_task)
+{
+	if (dead_task->mm) {
+		if (dead_task->mm->context.size) {
+			printk("WARNING: dead process %8s still has LDT? <%p/%d>\n",
+					dead_task->comm,
+					dead_task->mm->context.ldt,
+					dead_task->mm->context.size);
+			BUG();
+		}
+	}
+}
+
+static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
+{
+	struct user_desc ud = { 
+		.base_addr = addr,
+		.limit = 0xfffff,
+		.seg_32bit = 1,
+		.limit_in_pages = 1,
+		.useable = 1,
+	};
+	struct n_desc_struct *desc = (void *)t->thread.tls_array;
+	desc += tls;
+	desc->a = LDT_entry_a(&ud); 
+	desc->b = LDT_entry_b(&ud); 
+}
+
+static inline u32 read_32bit_tls(struct task_struct *t, int tls)
+{
+	struct desc_struct *desc = (void *)t->thread.tls_array;
+	desc += tls;
+	return desc->base0 | 
+		(((u32)desc->base1) << 16) | 
+		(((u32)desc->base2) << 24);
+}
+
+/*
+ * This gets called before we allocate a new thread and copy
+ * the current task into it.
+ */
+void prepare_to_copy(struct task_struct *tsk)
+{
+	unlazy_fpu(tsk);
+}
+
+int copy_thread(int nr, unsigned long clone_flags, unsigned long rsp, 
+		unsigned long unused,
+	struct task_struct * p, struct pt_regs * regs)
+{
+	int err;
+	struct pt_regs * childregs;
+	struct task_struct *me = current;
+
+	childregs = ((struct pt_regs *)
+			(THREAD_SIZE + task_stack_page(p))) - 1;
+	*childregs = *regs;
+
+	childregs->rax = 0;
+	childregs->rsp = rsp;
+	if (rsp == ~0UL)
+		childregs->rsp = (unsigned long)childregs;
+
+	p->thread.rsp = (unsigned long) childregs;
+	p->thread.rsp0 = (unsigned long) (childregs+1);
+	p->thread.userrsp = me->thread.userrsp; 
+
+	set_tsk_thread_flag(p, TIF_FORK);
+
+	p->thread.fs = me->thread.fs;
+	p->thread.gs = me->thread.gs;
+
+	asm("mov %%gs,%0" : "=m" (p->thread.gsindex));
+	asm("mov %%fs,%0" : "=m" (p->thread.fsindex));
+	asm("mov %%es,%0" : "=m" (p->thread.es));
+	asm("mov %%ds,%0" : "=m" (p->thread.ds));
+
+	if (unlikely(test_tsk_thread_flag(me, TIF_IO_BITMAP))) {
+		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
+		if (!p->thread.io_bitmap_ptr) {
+			p->thread.io_bitmap_max = 0;
+			return -ENOMEM;
+		}
+		memcpy(p->thread.io_bitmap_ptr, me->thread.io_bitmap_ptr,
+				IO_BITMAP_BYTES);
+		set_tsk_thread_flag(p, TIF_IO_BITMAP);
+	} 
+
+	/*
+	 * Set a new TLS for the child thread?
+	 */
+	if (clone_flags & CLONE_SETTLS) {
+#ifdef CONFIG_IA32_EMULATION
+		if (test_thread_flag(TIF_IA32))
+			err = ia32_child_tls(p, childregs); 
+		else 			
+#endif	 
+			err = do_arch_prctl(p, ARCH_SET_FS, childregs->r8); 
+		if (err) 
+			goto out;
+	}
+	err = 0;
+out:
+	if (err && p->thread.io_bitmap_ptr) {
+		kfree(p->thread.io_bitmap_ptr);
+		p->thread.io_bitmap_max = 0;
+	}
+	return err;
+}
+
+/*
+ * This special macro can be used to load a debugging register
+ */
+#define loaddebug(thread,r) set_debugreg(thread->debugreg ## r, r)
+
+static inline void __switch_to_xtra(struct task_struct *prev_p,
+			     	    struct task_struct *next_p,
+			     	    struct tss_struct *tss)
+{
+	struct thread_struct *prev, *next;
+
+	prev = &prev_p->thread,
+	next = &next_p->thread;
+
+	if (test_tsk_thread_flag(next_p, TIF_DEBUG)) {
+		loaddebug(next, 0);
+		loaddebug(next, 1);
+		loaddebug(next, 2);
+		loaddebug(next, 3);
+		/* no 4 and 5 */
+		loaddebug(next, 6);
+		loaddebug(next, 7);
+	}
+
+	if (test_tsk_thread_flag(next_p, TIF_IO_BITMAP)) {
+		/*
+		 * Copy the relevant range of the IO bitmap.
+		 * Normally this is 128 bytes or less:
+		 */
+		memcpy(tss->io_bitmap, next->io_bitmap_ptr,
+		       max(prev->io_bitmap_max, next->io_bitmap_max));
+	} else if (test_tsk_thread_flag(prev_p, TIF_IO_BITMAP)) {
+		/*
+		 * Clear any possible leftover bits:
+		 */
+		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
+	}
+}
+
+/*
+ *	switch_to(x,y) should switch tasks from x to y.
+ *
+ * This could still be optimized: 
+ * - fold all the options into a flag word and test it with a single test.
+ * - could test fs/gs bitsliced
+ *
+ * Kprobes not supported here. Set the probe on schedule instead.
+ */
+__kprobes struct task_struct *
+__switch_to(struct task_struct *prev_p, struct task_struct *next_p)
+{
+	struct thread_struct *prev = &prev_p->thread,
+				 *next = &next_p->thread;
+	int cpu = smp_processor_id();  
+	struct tss_struct *tss = &per_cpu(init_tss, cpu);
+
+	/* we're going to use this soon, after a few expensive things */
+	if (next_p->fpu_counter>5)
+		prefetch(&next->i387.fxsave);
+
+	/*
+	 * Reload esp0, LDT and the page table pointer:
+	 */
+	tss->rsp0 = next->rsp0;
+
+	/* 
+	 * Switch DS and ES.
+	 * This won't pick up thread selector changes, but I guess that is ok.
+	 */
+	asm volatile("mov %%es,%0" : "=m" (prev->es));
+	if (unlikely(next->es | prev->es))
+		loadsegment(es, next->es); 
+	
+	asm volatile ("mov %%ds,%0" : "=m" (prev->ds));
+	if (unlikely(next->ds | prev->ds))
+		loadsegment(ds, next->ds);
+
+	load_TLS(next, cpu);
+
+	/* 
+	 * Switch FS and GS.
+	 */
+	{ 
+		unsigned fsindex;
+		asm volatile("movl %%fs,%0" : "=r" (fsindex)); 
+		/* segment register != 0 always requires a reload. 
+		   also reload when it has changed. 
+		   when prev process used 64bit base always reload
+		   to avoid an information leak. */
+		if (unlikely(fsindex | next->fsindex | prev->fs)) {
+			loadsegment(fs, next->fsindex);
+			/* check if the user used a selector != 0
+	                 * if yes clear 64bit base, since overloaded base
+                         * is always mapped to the Null selector
+                         */
+			if (fsindex)
+			prev->fs = 0;				
+		}
+		/* when next process has a 64bit base use it */
+		if (next->fs) 
+			wrmsrl(MSR_FS_BASE, next->fs); 
+		prev->fsindex = fsindex;
+	}
+	{ 
+		unsigned gsindex;
+		asm volatile("movl %%gs,%0" : "=r" (gsindex)); 
+		if (unlikely(gsindex | next->gsindex | prev->gs)) {
+			load_gs_index(next->gsindex);
+			if (gsindex)
+			prev->gs = 0;				
+		}
+		if (next->gs)
+			wrmsrl(MSR_KERNEL_GS_BASE, next->gs); 
+		prev->gsindex = gsindex;
+	}
+
+	/* Must be after DS reload */
+	unlazy_fpu(prev_p);
+
+	/* 
+	 * Switch the PDA and FPU contexts.
+	 */
+	prev->userrsp = read_pda(oldrsp); 
+	write_pda(oldrsp, next->userrsp); 
+	write_pda(pcurrent, next_p); 
+
+	write_pda(kernelstack,
+	(unsigned long)task_stack_page(next_p) + THREAD_SIZE - PDA_STACKOFFSET);
+#ifdef CONFIG_CC_STACKPROTECTOR
+	write_pda(stack_canary, next_p->stack_canary);
+	/*
+	 * Build time only check to make sure the stack_canary is at
+	 * offset 40 in the pda; this is a gcc ABI requirement
+	 */
+	BUILD_BUG_ON(offsetof(struct x8664_pda, stack_canary) != 40);
+#endif
+
+	/*
+	 * Now maybe reload the debug registers and handle I/O bitmaps
+	 */
+	if (unlikely((task_thread_info(next_p)->flags & _TIF_WORK_CTXSW))
+	    || test_tsk_thread_flag(prev_p, TIF_IO_BITMAP))
+		__switch_to_xtra(prev_p, next_p, tss);
+
+	/* If the task has used fpu the last 5 timeslices, just do a full
+	 * restore of the math state immediately to avoid the trap; the
+	 * chances of needing FPU soon are obviously high now
+	 */
+	if (next_p->fpu_counter>5)
+		math_state_restore();
+	return prev_p;
+}
+
+/*
+ * sys_execve() executes a new program.
+ */
+asmlinkage 
+long sys_execve(char __user *name, char __user * __user *argv,
+		char __user * __user *envp, struct pt_regs regs)
+{
+	long error;
+	char * filename;
+
+	filename = getname(name);
+	error = PTR_ERR(filename);
+	if (IS_ERR(filename)) 
+		return error;
+	error = do_execve(filename, argv, envp, &regs); 
+	if (error == 0) {
+		task_lock(current);
+		current->ptrace &= ~PT_DTRACE;
+		task_unlock(current);
+	}
+	putname(filename);
+	return error;
+}
+
+void set_personality_64bit(void)
+{
+	/* inherit personality from parent */
+
+	/* Make sure to be in 64bit mode */
+	clear_thread_flag(TIF_IA32); 
+
+	/* TBD: overwrites user setup. Should have two bits.
+	   But 64bit processes have always behaved this way,
+	   so it's not too bad. The main problem is just that
+   	   32bit childs are affected again. */
+	current->personality &= ~READ_IMPLIES_EXEC;
+}
+
+asmlinkage long sys_fork(struct pt_regs *regs)
+{
+	return do_fork(SIGCHLD, regs->rsp, regs, 0, NULL, NULL);
+}
+
+asmlinkage long
+sys_clone(unsigned long clone_flags, unsigned long newsp,
+	  void __user *parent_tid, void __user *child_tid, struct pt_regs *regs)
+{
+	if (!newsp)
+		newsp = regs->rsp;
+	return do_fork(clone_flags, newsp, regs, 0, parent_tid, child_tid);
+}
+
+/*
+ * This is trivial, and on the face of it looks like it
+ * could equally well be done in user mode.
+ *
+ * Not so, for quite unobvious reasons - register pressure.
+ * In user mode vfork() cannot have a stack frame, and if
+ * done by calling the "clone()" system call directly, you
+ * do not have enough call-clobbered registers to hold all
+ * the information you need.
+ */
+asmlinkage long sys_vfork(struct pt_regs *regs)
+{
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->rsp, regs, 0,
+		    NULL, NULL);
+}
+
+unsigned long get_wchan(struct task_struct *p)
+{
+	unsigned long stack;
+	u64 fp,rip;
+	int count = 0;
+
+	if (!p || p == current || p->state==TASK_RUNNING)
+		return 0; 
+	stack = (unsigned long)task_stack_page(p);
+	if (p->thread.rsp < stack || p->thread.rsp > stack+THREAD_SIZE)
+		return 0;
+	fp = *(u64 *)(p->thread.rsp);
+	do { 
+		if (fp < (unsigned long)stack ||
+		    fp > (unsigned long)stack+THREAD_SIZE)
+			return 0; 
+		rip = *(u64 *)(fp+8); 
+		if (!in_sched_functions(rip))
+			return rip; 
+		fp = *(u64 *)fp; 
+	} while (count++ < 16); 
+	return 0;
+}
+
+long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
+{ 
+	int ret = 0; 
+	int doit = task == current;
+	int cpu;
+
+	switch (code) { 
+	case ARCH_SET_GS:
+		if (addr >= TASK_SIZE_OF(task))
+			return -EPERM; 
+		cpu = get_cpu();
+		/* handle small bases via the GDT because that's faster to 
+		   switch. */
+		if (addr <= 0xffffffff) {  
+			set_32bit_tls(task, GS_TLS, addr); 
+			if (doit) { 
+				load_TLS(&task->thread, cpu);
+				load_gs_index(GS_TLS_SEL); 
+			}
+			task->thread.gsindex = GS_TLS_SEL; 
+			task->thread.gs = 0;
+		} else { 
+			task->thread.gsindex = 0;
+			task->thread.gs = addr;
+			if (doit) {
+				load_gs_index(0);
+				ret = checking_wrmsrl(MSR_KERNEL_GS_BASE, addr);
+			} 
+		}
+		put_cpu();
+		break;
+	case ARCH_SET_FS:
+		/* Not strictly needed for fs, but do it for symmetry
+		   with gs */
+		if (addr >= TASK_SIZE_OF(task))
+			return -EPERM; 
+		cpu = get_cpu();
+		/* handle small bases via the GDT because that's faster to 
+		   switch. */
+		if (addr <= 0xffffffff) { 
+			set_32bit_tls(task, FS_TLS, addr);
+			if (doit) { 
+				load_TLS(&task->thread, cpu); 
+				asm volatile("movl %0,%%fs" :: "r"(FS_TLS_SEL));
+			}
+			task->thread.fsindex = FS_TLS_SEL;
+			task->thread.fs = 0;
+		} else { 
+			task->thread.fsindex = 0;
+			task->thread.fs = addr;
+			if (doit) {
+				/* set the selector to 0 to not confuse
+				   __switch_to */
+				asm volatile("movl %0,%%fs" :: "r" (0));
+				ret = checking_wrmsrl(MSR_FS_BASE, addr);
+			}
+		}
+		put_cpu();
+		break;
+	case ARCH_GET_FS: { 
+		unsigned long base; 
+		if (task->thread.fsindex == FS_TLS_SEL)
+			base = read_32bit_tls(task, FS_TLS);
+		else if (doit)
+			rdmsrl(MSR_FS_BASE, base);
+		else
+			base = task->thread.fs;
+		ret = put_user(base, (unsigned long __user *)addr); 
+		break; 
+	}
+	case ARCH_GET_GS: { 
+		unsigned long base;
+		unsigned gsindex;
+		if (task->thread.gsindex == GS_TLS_SEL)
+			base = read_32bit_tls(task, GS_TLS);
+		else if (doit) {
+ 			asm("movl %%gs,%0" : "=r" (gsindex));
+			if (gsindex)
+				rdmsrl(MSR_KERNEL_GS_BASE, base);
+			else
+				base = task->thread.gs;
+		}
+		else
+			base = task->thread.gs;
+		ret = put_user(base, (unsigned long __user *)addr); 
+		break;
+	}
+
+	default:
+		ret = -EINVAL;
+		break;
+	} 
+
+	return ret;	
+} 
+
+long sys_arch_prctl(int code, unsigned long addr)
+{
+	return do_arch_prctl(current, code, addr);
+} 
+
+/* 
+ * Capture the user space registers if the task is not running (in user space)
+ */
+int dump_task_regs(struct task_struct *tsk, elf_gregset_t *regs)
+{
+	struct pt_regs *pp, ptregs;
+
+	pp = task_pt_regs(tsk);
+
+	ptregs = *pp; 
+	ptregs.cs &= 0xffff;
+	ptregs.ss &= 0xffff;
+
+	elf_core_copy_regs(regs, &ptregs);
+ 
+	return 1;
+}
+
+unsigned long arch_align_stack(unsigned long sp)
+{
+	if (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)
+		sp -= get_random_int() % 8192;
+	return sp & ~0xf;
+}
