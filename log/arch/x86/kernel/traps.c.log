commit efb9666e900b6f8f079adb2ab18a7be266434cd3
Merge: 66e4b63624fc da05b143a308
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 19 12:16:09 2020 -0700

    Merge tag 'x86-urgent-2020-07-19' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip into master
    
    Pull x86 fixes from Thomas Gleixner:
     "A pile of fixes for x86:
    
       - Fix the I/O bitmap invalidation on XEN PV, which was overlooked in
         the recent ioperm/iopl rework. This caused the TSS and XEN's I/O
         bitmap to get out of sync.
    
       - Use the proper vectors for HYPERV.
    
       - Make disabling of stack protector for the entry code work with GCC
         builds which enable stack protector by default. Removing the option
         is not sufficient, it needs an explicit -fno-stack-protector to
         shut it off.
    
       - Mark check_user_regs() noinstr as it is called from noinstr code.
         The missing annotation causes it to be placed in the text section
         which makes it instrumentable.
    
       - Add the missing interrupt disable in exc_alignment_check()
    
       - Fixup a XEN_PV build dependency in the 32bit entry code
    
       - A few fixes to make the Clang integrated assembler happy
    
       - Move EFI stub build to the right place for out of tree builds
    
       - Make prepare_exit_to_usermode() static. It's not longer called from
         ASM code"
    
    * tag 'x86-urgent-2020-07-19' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/boot: Don't add the EFI stub to targets
      x86/entry: Actually disable stack protector
      x86/ioperm: Fix io bitmap invalidation on Xen PV
      x86: math-emu: Fix up 'cmp' insn for clang ias
      x86/entry: Fix vectors to IDTENTRY_SYSVEC for CONFIG_HYPERV
      x86/entry: Add compatibility with IAS
      x86/entry/common: Make prepare_exit_to_usermode() static
      x86/entry: Mark check_user_regs() noinstr
      x86/traps: Disable interrupts in exc_aligment_check()
      x86/entry/32: Fix XEN_PV build dependency

commit bce9b042ec73e8662b8119d4ca47e7c78b20d0bf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 8 21:28:05 2020 +0200

    x86/traps: Disable interrupts in exc_aligment_check()
    
    exc_alignment_check() fails to disable interrupts before returning to the
    entry code.
    
    Fixes: ca4c6a9858c2 ("x86/traps: Make interrupt enable/disable symmetric in C code")
    Reported-by: syzbot+0889df9502bc0f112b31@syzkaller.appspotmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200708192934.076519438@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 6ed8cc5fbe8f..4f3a509e5547 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -299,6 +299,8 @@ DEFINE_IDTENTRY_ERRORCODE(exc_alignment_check)
 
 	do_trap(X86_TRAP_AC, SIGBUS, "alignment check", regs,
 		error_code, BUS_ADRALN, NULL);
+
+	local_irq_disable();
 }
 
 #ifdef CONFIG_VMAP_STACK

commit 72674d480076067d627e708c0a062dd900438bd7
Merge: f23dbe18930b cc801833a171
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 5 12:23:49 2020 -0700

    Merge tag 'x86-urgent-2020-07-05' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Thomas Gleixner:
     "A series of fixes for x86:
    
       - Reset MXCSR in kernel_fpu_begin() to prevent using a stale user
         space value.
    
       - Prevent writing MSR_TEST_CTRL on CPUs which are not explicitly
         whitelisted for split lock detection. Some CPUs which do not
         support it crash even when the MSR is written to 0 which is the
         default value.
    
       - Fix the XEN PV fallout of the entry code rework
    
       - Fix the 32bit fallout of the entry code rework
    
       - Add more selftests to ensure that these entry problems don't come
         back.
    
       - Disable 16 bit segments on XEN PV. It's not supported because XEN
         PV does not implement ESPFIX64"
    
    * tag 'x86-urgent-2020-07-05' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/ldt: Disable 16-bit segments on Xen PV
      x86/entry/32: Fix #MC and #DB wiring on x86_32
      x86/entry/xen: Route #DB correctly on Xen PV
      x86/entry, selftests: Further improve user entry sanity checks
      x86/entry/compat: Clear RAX high bits on Xen PV SYSENTER
      selftests/x86: Consolidate and fix get/set_eflags() helpers
      selftests/x86/syscall_nt: Clear weird flags after each test
      selftests/x86/syscall_nt: Add more flag combinations
      x86/entry/64/compat: Fix Xen PV SYSENTER frame setup
      x86/entry: Move SYSENTER's regs->sp and regs->flags fixups into C
      x86/entry: Assert that syscalls are on the right stack
      x86/split_lock: Don't write MSR_TEST_CTRL on CPUs that aren't whitelisted
      x86/fpu: Reset MXCSR to default in kernel_fpu_begin()

commit 13cbc0cd4a30c815984ad88e3a2e5976493516a3
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Jul 3 10:02:56 2020 -0700

    x86/entry/32: Fix #MC and #DB wiring on x86_32
    
    DEFINE_IDTENTRY_MCE and DEFINE_IDTENTRY_DEBUG were wired up as non-RAW
    on x86_32, but the code expected them to be RAW.
    
    Get rid of all the macro indirection for them on 32-bit and just use
    DECLARE_IDTENTRY_RAW and DEFINE_IDTENTRY_RAW directly.
    
    Also add a warning to make sure that we only hit the _kernel paths
    in kernel mode.
    
    Reported-by: Naresh Kamboju <naresh.kamboju@linaro.org>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/9e90a7ee8e72fd757db6d92e1e5ff16339c1ecf9.1593795633.git.luto@kernel.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index c17f9b57171f..6ed8cc5fbe8f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -925,7 +925,7 @@ DEFINE_IDTENTRY_DEBUG_USER(exc_debug)
 }
 #else
 /* 32 bit does not have separate entry points. */
-DEFINE_IDTENTRY_DEBUG(exc_debug)
+DEFINE_IDTENTRY_RAW(exc_debug)
 {
 	unsigned long dr6, dr7;
 

commit f41f0824224eb12ad84de8972962dd54be5abe3b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Jul 3 10:02:55 2020 -0700

    x86/entry/xen: Route #DB correctly on Xen PV
    
    On Xen PV, #DB doesn't use IST. It still needs to be correctly routed
    depending on whether it came from user or kernel mode.
    
    Get rid of DECLARE/DEFINE_IDTENTRY_XEN -- it was too hard to follow the
    logic.  Instead, route #DB and NMI through DECLARE/DEFINE_IDTENTRY_RAW on
    Xen, and do the right thing for #DB.  Also add more warnings to the
    exc_debug* handlers to make this type of failure more obvious.
    
    This fixes various forms of corruption that happen when usermode
    triggers #DB on Xen PV.
    
    Fixes: 4c0dcd8350a0 ("x86/entry: Implement user mode C entry points for #DB and #MCE")
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/4163e733cce0b41658e252c6c6b3464f33fdff17.1593795633.git.luto@kernel.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index f9727b96961f..c17f9b57171f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -865,6 +865,12 @@ static __always_inline void exc_debug_kernel(struct pt_regs *regs,
 	instrumentation_begin();
 	trace_hardirqs_off_finish();
 
+	/*
+	 * If something gets miswired and we end up here for a user mode
+	 * #DB, we will malfunction.
+	 */
+	WARN_ON_ONCE(user_mode(regs));
+
 	/*
 	 * Catch SYSENTER with TF set and clear DR_STEP. If this hit a
 	 * watchpoint at the same time then that will still be handled.
@@ -883,6 +889,12 @@ static __always_inline void exc_debug_kernel(struct pt_regs *regs,
 static __always_inline void exc_debug_user(struct pt_regs *regs,
 					   unsigned long dr6)
 {
+	/*
+	 * If something gets miswired and we end up here for a kernel mode
+	 * #DB, we will malfunction.
+	 */
+	WARN_ON_ONCE(!user_mode(regs));
+
 	idtentry_enter_user(regs);
 	instrumentation_begin();
 

commit 2c92d787cc9fad57d05c96bd117782183768258a
Merge: 145a773aef83 8be3a53e18e0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jun 26 12:24:42 2020 +0200

    Merge branch 'linus' into x86/entry, to resolve conflicts
    
    Conflicts:
            arch/x86/kernel/traps.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 145a773aef83181d47ebab21bb33c89233aadb1e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 16 13:28:36 2020 +0200

    x86/entry: Fix #UD vs WARN more
    
    vmlinux.o: warning: objtool: exc_invalid_op()+0x47: call to probe_kernel_read() leaves .noinstr.text section
    
    Since we use UD2 as a short-cut for 'CALL __WARN', treat it as such.
    Have the bare exception handler do the report_bug() thing.
    
    Fixes: 15a416e8aaa7 ("x86/entry: Treat BUG/WARN as NMI-like entries")
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200622114713.GE577403@hirez.programming.kicks-ass.net

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a7d157090572..1d9ea2101b97 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -84,17 +84,16 @@ static inline void cond_local_irq_disable(struct pt_regs *regs)
 		local_irq_disable();
 }
 
-int is_valid_bugaddr(unsigned long addr)
+__always_inline int is_valid_bugaddr(unsigned long addr)
 {
-	unsigned short ud;
-
 	if (addr < TASK_SIZE_MAX)
 		return 0;
 
-	if (probe_kernel_address((unsigned short *)addr, ud))
-		return 0;
-
-	return ud == INSN_UD0 || ud == INSN_UD2;
+	/*
+	 * We got #UD, if the text isn't readable we'd have gotten
+	 * a different exception.
+	 */
+	return *(unsigned short *)addr == INSN_UD2;
 }
 
 static nokprobe_inline int
@@ -216,40 +215,45 @@ static inline void handle_invalid_op(struct pt_regs *regs)
 		      ILL_ILLOPN, error_get_trap_addr(regs));
 }
 
-DEFINE_IDTENTRY_RAW(exc_invalid_op)
+static noinstr bool handle_bug(struct pt_regs *regs)
 {
-	bool rcu_exit;
+	bool handled = false;
+
+	if (!is_valid_bugaddr(regs->ip))
+		return handled;
 
 	/*
-	 * Handle BUG/WARN like NMIs instead of like normal idtentries:
-	 * if we bugged/warned in a bad RCU context, for example, the last
-	 * thing we want is to BUG/WARN again in the idtentry code, ad
-	 * infinitum.
+	 * All lies, just get the WARN/BUG out.
 	 */
-	if (!user_mode(regs) && is_valid_bugaddr(regs->ip)) {
-		enum bug_trap_type type;
+	instrumentation_begin();
+	/*
+	 * Since we're emulating a CALL with exceptions, restore the interrupt
+	 * state to what it was at the exception site.
+	 */
+	if (regs->flags & X86_EFLAGS_IF)
+		raw_local_irq_enable();
+	if (report_bug(regs->ip, regs) == BUG_TRAP_TYPE_WARN) {
+		regs->ip += LEN_UD2;
+		handled = true;
+	}
+	if (regs->flags & X86_EFLAGS_IF)
+		raw_local_irq_disable();
+	instrumentation_end();
 
-		nmi_enter();
-		instrumentation_begin();
-		trace_hardirqs_off_finish();
-		type = report_bug(regs->ip, regs);
-		if (regs->flags & X86_EFLAGS_IF)
-			trace_hardirqs_on_prepare();
-		instrumentation_end();
-		nmi_exit();
+	return handled;
+}
 
-		if (type == BUG_TRAP_TYPE_WARN) {
-			/* Skip the ud2. */
-			regs->ip += LEN_UD2;
-			return;
-		}
+DEFINE_IDTENTRY_RAW(exc_invalid_op)
+{
+	bool rcu_exit;
 
-		/*
-		 * Else, if this was a BUG and report_bug returns or if this
-		 * was just a normal #UD, we want to continue onward and
-		 * crash.
-		 */
-	}
+	/*
+	 * We use UD2 as a short encoding for 'CALL __WARN', as such
+	 * handle it before exception entry to avoid recursive WARN
+	 * in case exception entry is the one triggering WARNs.
+	 */
+	if (!user_mode(regs) && handle_bug(regs))
+		return;
 
 	rcu_exit = idtentry_enter_cond_rcu(regs);
 	instrumentation_begin();

commit e3a9e681adb779b39565a28b3252c3be1033f994
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 17 18:21:16 2020 +0200

    x86/entry: Fixup bad_iret vs noinstr
    
    vmlinux.o: warning: objtool: fixup_bad_iret()+0x8e: call to memcpy() leaves .noinstr.text section
    
    Worse, when KASAN there is no telling what memcpy() actually is. Force
    the use of __memcpy() which is our assmebly implementation.
    
    Reported-by: Marco Elver <elver@google.com>
    Suggested-by: Marco Elver <elver@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Marco Elver <elver@google.com>
    Link: https://lkml.kernel.org/r/20200618144801.760070502@infradead.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index af75109485c2..a7d157090572 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -690,13 +690,13 @@ struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)
 		(struct bad_iret_stack *)__this_cpu_read(cpu_tss_rw.x86_tss.sp0) - 1;
 
 	/* Copy the IRET target to the temporary storage. */
-	memcpy(&tmp.regs.ip, (void *)s->regs.sp, 5*8);
+	__memcpy(&tmp.regs.ip, (void *)s->regs.sp, 5*8);
 
 	/* Copy the remainder of the stack from the current stack. */
-	memcpy(&tmp, s, offsetof(struct bad_iret_stack, regs.ip));
+	__memcpy(&tmp, s, offsetof(struct bad_iret_stack, regs.ip));
 
 	/* Update the entry stack */
-	memcpy(new_stack, &tmp, sizeof(tmp));
+	__memcpy(new_stack, &tmp, sizeof(tmp));
 
 	BUG_ON(!user_mode(&new_stack->regs));
 	return new_stack;

commit 25f12ae45fc1931a1dce3cc59f9989a9d87834b0
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 17 09:37:55 2020 +0200

    maccess: rename probe_kernel_address to get_kernel_nofault
    
    Better describe what this helper does, and match the naming of
    copy_from_kernel_nofault.
    
    Also switch the argument order around, so that it acts and looks
    like get_user().
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7003f2e7b163..f9727b96961f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -91,7 +91,7 @@ int is_valid_bugaddr(unsigned long addr)
 	if (addr < TASK_SIZE_MAX)
 		return 0;
 
-	if (probe_kernel_address((unsigned short *)addr, ud))
+	if (get_kernel_nofault(ud, (unsigned short *)addr))
 		return 0;
 
 	return ud == INSN_UD0 || ud == INSN_UD2;

commit fe557319aa06c23cffc9346000f119547e0f289a
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 17 09:37:53 2020 +0200

    maccess: rename probe_kernel_{read,write} to copy_{from,to}_kernel_nofault
    
    Better describe what these functions do.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index af75109485c2..7003f2e7b163 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -488,7 +488,8 @@ static enum kernel_gp_hint get_kernel_gp_address(struct pt_regs *regs,
 	u8 insn_buf[MAX_INSN_SIZE];
 	struct insn insn;
 
-	if (probe_kernel_read(insn_buf, (void *)regs->ip, MAX_INSN_SIZE))
+	if (copy_from_kernel_nofault(insn_buf, (void *)regs->ip,
+			MAX_INSN_SIZE))
 		return GP_NO_HINT;
 
 	kernel_insn_init(&insn, insn_buf, MAX_INSN_SIZE);

commit 15a416e8aaa758b5534f64a3972dae05275bc225
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 11 20:26:38 2020 -0700

    x86/entry: Treat BUG/WARN as NMI-like entries
    
    BUG/WARN are cleverly optimized using UD2 to handle the BUG/WARN out of
    line in an exception fixup.
    
    But if BUG or WARN is issued in a funny RCU context, then the
    idtentry_enter...() path might helpfully WARN that the RCU context is
    invalid, which results in infinite recursion.
    
    Split the BUG/WARN handling into an nmi_enter()/nmi_exit() path in
    exc_invalid_op() to increase the chance to survive the experience.
    
    [ tglx: Make the declaration match the implementation ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/f8fe40e0088749734b4435b554f73eee53dcf7a8.1591932307.git.luto@kernel.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7febae381b91..af75109485c2 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -97,24 +97,6 @@ int is_valid_bugaddr(unsigned long addr)
 	return ud == INSN_UD0 || ud == INSN_UD2;
 }
 
-int fixup_bug(struct pt_regs *regs, int trapnr)
-{
-	if (trapnr != X86_TRAP_UD)
-		return 0;
-
-	switch (report_bug(regs->ip, regs)) {
-	case BUG_TRAP_TYPE_NONE:
-	case BUG_TRAP_TYPE_BUG:
-		break;
-
-	case BUG_TRAP_TYPE_WARN:
-		regs->ip += LEN_UD2;
-		return 1;
-	}
-
-	return 0;
-}
-
 static nokprobe_inline int
 do_trap_no_signal(struct task_struct *tsk, int trapnr, const char *str,
 		  struct pt_regs *regs,	long error_code)
@@ -190,13 +172,6 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 {
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 
-	/*
-	 * WARN*()s end up here; fix them up before we call the
-	 * notifier chain.
-	 */
-	if (!user_mode(regs) && fixup_bug(regs, trapnr))
-		return;
-
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
 			NOTIFY_STOP) {
 		cond_local_irq_enable(regs);
@@ -241,9 +216,46 @@ static inline void handle_invalid_op(struct pt_regs *regs)
 		      ILL_ILLOPN, error_get_trap_addr(regs));
 }
 
-DEFINE_IDTENTRY(exc_invalid_op)
+DEFINE_IDTENTRY_RAW(exc_invalid_op)
 {
+	bool rcu_exit;
+
+	/*
+	 * Handle BUG/WARN like NMIs instead of like normal idtentries:
+	 * if we bugged/warned in a bad RCU context, for example, the last
+	 * thing we want is to BUG/WARN again in the idtentry code, ad
+	 * infinitum.
+	 */
+	if (!user_mode(regs) && is_valid_bugaddr(regs->ip)) {
+		enum bug_trap_type type;
+
+		nmi_enter();
+		instrumentation_begin();
+		trace_hardirqs_off_finish();
+		type = report_bug(regs->ip, regs);
+		if (regs->flags & X86_EFLAGS_IF)
+			trace_hardirqs_on_prepare();
+		instrumentation_end();
+		nmi_exit();
+
+		if (type == BUG_TRAP_TYPE_WARN) {
+			/* Skip the ud2. */
+			regs->ip += LEN_UD2;
+			return;
+		}
+
+		/*
+		 * Else, if this was a BUG and report_bug returns or if this
+		 * was just a normal #UD, we want to continue onward and
+		 * crash.
+		 */
+	}
+
+	rcu_exit = idtentry_enter_cond_rcu(regs);
+	instrumentation_begin();
 	handle_invalid_op(regs);
+	instrumentation_end();
+	idtentry_exit_cond_rcu(regs, rcu_exit);
 }
 
 DEFINE_IDTENTRY(exc_coproc_segment_overrun)

commit 5ef227933117085f1320b3421ef43a26bf624b4c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 3 13:40:20 2020 +0200

    x86/entry: Re-order #DB handler to avoid *SAN instrumentation
    
    vmlinux.o: warning: objtool: exc_debug()+0xbb: call to clear_ti_thread_flag.constprop.0() leaves .noinstr.text section
    vmlinux.o: warning: objtool: noist_exc_debug()+0x55: call to clear_ti_thread_flag.constprop.0() leaves .noinstr.text section
    
    Rework things so that handle_debug() looses the noinstr and move the
    clear_thread_flag() into that.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200603114052.127756554@infradead.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 5566fe50ef98..7febae381b91 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -775,26 +775,44 @@ static __always_inline void debug_exit(unsigned long dr7)
  *
  * May run on IST stack.
  */
-static void noinstr handle_debug(struct pt_regs *regs, unsigned long dr6,
-				 bool user_icebp)
+static void handle_debug(struct pt_regs *regs, unsigned long dr6, bool user)
 {
 	struct task_struct *tsk = current;
+	bool user_icebp;
 	int si_code;
 
+	/*
+	 * The SDM says "The processor clears the BTF flag when it
+	 * generates a debug exception."  Clear TIF_BLOCKSTEP to keep
+	 * TIF_BLOCKSTEP in sync with the hardware BTF flag.
+	 */
+	clear_thread_flag(TIF_BLOCKSTEP);
+
+	/*
+	 * If DR6 is zero, no point in trying to handle it. The kernel is
+	 * not using INT1.
+	 */
+	if (!user && !dr6)
+		return;
+
+	/*
+	 * If dr6 has no reason to give us about the origin of this trap,
+	 * then it's very likely the result of an icebp/int01 trap.
+	 * User wants a sigtrap for that.
+	 */
+	user_icebp = user && !dr6;
+
 	/* Store the virtualized DR6 value */
 	tsk->thread.debugreg6 = dr6;
 
-	instrumentation_begin();
 #ifdef CONFIG_KPROBES
 	if (kprobe_debug_handler(regs)) {
-		instrumentation_end();
 		return;
 	}
 #endif
 
 	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, 0,
 		       SIGTRAP) == NOTIFY_STOP) {
-		instrumentation_end();
 		return;
 	}
 
@@ -825,7 +843,6 @@ static void noinstr handle_debug(struct pt_regs *regs, unsigned long dr6,
 
 out:
 	cond_local_irq_disable(regs);
-	instrumentation_end();
 }
 
 static __always_inline void exc_debug_kernel(struct pt_regs *regs,
@@ -834,14 +851,6 @@ static __always_inline void exc_debug_kernel(struct pt_regs *regs,
 	nmi_enter();
 	instrumentation_begin();
 	trace_hardirqs_off_finish();
-	instrumentation_end();
-
-	/*
-	 * The SDM says "The processor clears the BTF flag when it
-	 * generates a debug exception."  Clear TIF_BLOCKSTEP to keep
-	 * TIF_BLOCKSTEP in sync with the hardware BTF flag.
-	 */
-	clear_thread_flag(TIF_BLOCKSTEP);
 
 	/*
 	 * Catch SYSENTER with TF set and clear DR_STEP. If this hit a
@@ -850,14 +859,8 @@ static __always_inline void exc_debug_kernel(struct pt_regs *regs,
 	if ((dr6 & DR_STEP) && is_sysenter_singlestep(regs))
 		dr6 &= ~DR_STEP;
 
-	/*
-	 * If DR6 is zero, no point in trying to handle it. The kernel is
-	 * not using INT1.
-	 */
-	if (dr6)
-		handle_debug(regs, dr6, false);
+	handle_debug(regs, dr6, false);
 
-	instrumentation_begin();
 	if (regs->flags & X86_EFLAGS_IF)
 		trace_hardirqs_on_prepare();
 	instrumentation_end();
@@ -868,14 +871,10 @@ static __always_inline void exc_debug_user(struct pt_regs *regs,
 					   unsigned long dr6)
 {
 	idtentry_enter_user(regs);
-	clear_thread_flag(TIF_BLOCKSTEP);
+	instrumentation_begin();
 
-	/*
-	 * If dr6 has no reason to give us about the origin of this trap,
-	 * then it's very likely the result of an icebp/int01 trap.
-	 * User wants a sigtrap for that.
-	 */
-	handle_debug(regs, dr6, !dr6);
+	handle_debug(regs, dr6, true);
+	instrumentation_end();
 	idtentry_exit_user(regs);
 }
 

commit 00229a54300108502f68c8777faca2d13f805f1a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 28 16:53:19 2020 +0200

    x86/idt: Cleanup trap_init()
    
    No point in having all the IDT cruft in trap_init(). Move it into the IDT
    code and fixup the comments.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200528145522.992376498@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 79af913e78a3..5566fe50ef98 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -1055,15 +1055,6 @@ void __init trap_init(void)
 
 	idt_setup_traps();
 
-	/*
-	 * Set the IDT descriptor to a fixed read-only location, so that the
-	 * "sidt" instruction will not leak the location of the kernel, and
-	 * to defend the IDT against arbitrary memory write vulnerabilities.
-	 * It will be reloaded in cpu_init() */
-	cea_set_pte(CPU_ENTRY_AREA_RO_IDT_VADDR, __pa_symbol(idt_table),
-		    PAGE_KERNEL_RO);
-	idt_descr.address = CPU_ENTRY_AREA_RO_IDT;
-
 	/*
 	 * Should be a barrier for any external CPU state:
 	 */

commit bf2b3008440072068580c609d79a079656af0588
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 29 23:27:40 2020 +0200

    x86/entry: Rename trace_hardirqs_off_prepare()
    
    The typical pattern for trace_hardirqs_off_prepare() is:
    
      ENTRY
        lockdep_hardirqs_off(); // because hardware
        ... do entry magic
        instrumentation_begin();
        trace_hardirqs_off_prepare();
        ... do actual work
        trace_hardirqs_on_prepare();
        lockdep_hardirqs_on_prepare();
        instrumentation_end();
        ... do exit magic
        lockdep_hardirqs_on();
    
    which shows that it's named wrong, rename it to
    trace_hardirqs_off_finish(), as it concludes the hardirq_off transition.
    
    Also, given that the above is the only correct order, make the traditional
    all-in-one trace_hardirqs_off() follow suit.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200529213321.415774872@infradead.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 6f887be1ac0c..79af913e78a3 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -634,7 +634,7 @@ DEFINE_IDTENTRY_RAW(exc_int3)
 	} else {
 		nmi_enter();
 		instrumentation_begin();
-		trace_hardirqs_off_prepare();
+		trace_hardirqs_off_finish();
 		if (!do_int3(regs))
 			die("int3", regs, 0);
 		if (regs->flags & X86_EFLAGS_IF)
@@ -833,7 +833,7 @@ static __always_inline void exc_debug_kernel(struct pt_regs *regs,
 {
 	nmi_enter();
 	instrumentation_begin();
-	trace_hardirqs_off_prepare();
+	trace_hardirqs_off_finish();
 	instrumentation_end();
 
 	/*

commit f9912ada82862df341b3e86864cbd532d0d24b84
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 29 23:27:37 2020 +0200

    x86/entry: Remove debug IDT frobbing
    
    This is all unused now.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200529213321.245019500@infradead.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index bcb9dd961c6d..6f887be1ac0c 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -798,12 +798,6 @@ static void noinstr handle_debug(struct pt_regs *regs, unsigned long dr6,
 		return;
 	}
 
-	/*
-	 * Let others (NMI) know that the debug stack is in use
-	 * as we may switch to the interrupt stack.
-	 */
-	debug_stack_usage_inc();
-
 	/* It's safe to allow irq's after DR6 has been saved */
 	cond_local_irq_enable(regs);
 
@@ -831,7 +825,6 @@ static void noinstr handle_debug(struct pt_regs *regs, unsigned long dr6,
 
 out:
 	cond_local_irq_disable(regs);
-	debug_stack_usage_dec();
 	instrumentation_end();
 }
 
@@ -1077,6 +1070,4 @@ void __init trap_init(void)
 	cpu_init();
 
 	idt_setup_ist_traps();
-
-	idt_setup_debugidt_traps();
 }

commit e1de11d4d1a64ac1b90b9833f1a3629dae18facb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 29 23:27:33 2020 +0200

    x86/entry: Introduce local_db_{save,restore}()
    
    In order to allow other exceptions than #DB to disable breakpoints,
    provide common helpers.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200529213321.012060983@infradead.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 50fb9cd5be97..bcb9dd961c6d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -727,15 +727,7 @@ static __always_inline void debug_enter(unsigned long *dr6, unsigned long *dr7)
 	 * Entry text is excluded for HW_BP_X and cpu_entry_area, which
 	 * includes the entry stack is excluded for everything.
 	 */
-	get_debugreg(*dr7, 7);
-	set_debugreg(0, 7);
-
-	/*
-	 * Ensure the compiler doesn't lower the above statements into
-	 * the critical section; disabling breakpoints late would not
-	 * be good.
-	 */
-	barrier();
+	*dr7 = local_db_save();
 
 	/*
 	 * The Intel SDM says:
@@ -756,13 +748,7 @@ static __always_inline void debug_enter(unsigned long *dr6, unsigned long *dr7)
 
 static __always_inline void debug_exit(unsigned long dr7)
 {
-	/*
-	 * Ensure the compiler doesn't raise this statement into
-	 * the critical section; enabling breakpoints early would
-	 * not be good.
-	 */
-	barrier();
-	set_debugreg(dr7, 7);
+	local_db_restore(dr7);
 }
 
 /*

commit 3ffdfdcec1bae39b68b990762350b3cd3127f23f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 21 22:05:51 2020 +0200

    x86/entry: Move paranoid irq tracing out of ASM code
    
    The last step to remove the irq tracing cruft from ASM. Ignore #DF as the
    maschine is going to die anyway.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lore.kernel.org/r/20200521202120.414043330@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index f28be3e51cca..50fb9cd5be97 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -634,8 +634,11 @@ DEFINE_IDTENTRY_RAW(exc_int3)
 	} else {
 		nmi_enter();
 		instrumentation_begin();
+		trace_hardirqs_off_prepare();
 		if (!do_int3(regs))
 			die("int3", regs, 0);
+		if (regs->flags & X86_EFLAGS_IF)
+			trace_hardirqs_on_prepare();
 		instrumentation_end();
 		nmi_exit();
 	}
@@ -850,6 +853,10 @@ static __always_inline void exc_debug_kernel(struct pt_regs *regs,
 					     unsigned long dr6)
 {
 	nmi_enter();
+	instrumentation_begin();
+	trace_hardirqs_off_prepare();
+	instrumentation_end();
+
 	/*
 	 * The SDM says "The processor clears the BTF flag when it
 	 * generates a debug exception."  Clear TIF_BLOCKSTEP to keep
@@ -871,6 +878,10 @@ static __always_inline void exc_debug_kernel(struct pt_regs *regs,
 	if (dr6)
 		handle_debug(regs, dr6, false);
 
+	instrumentation_begin();
+	if (regs->flags & X86_EFLAGS_IF)
+		trace_hardirqs_on_prepare();
+	instrumentation_end();
 	nmi_exit();
 }
 

commit fa95d7dc1abceb288db2959badb9aaf558eb0530
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 21 22:05:19 2020 +0200

    x86/idtentry: Switch to conditional RCU handling
    
    Switch all idtentry_enter/exit() users over to the new conditional RCU
    handling scheme and make the user mode entries in #DB, #INT3 and #MCE use
    the user mode idtentry functions.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lore.kernel.org/r/20200521202117.382387286@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 9e5d81cb94ba..f28be3e51cca 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -619,18 +619,18 @@ DEFINE_IDTENTRY_RAW(exc_int3)
 		return;
 
 	/*
-	 * idtentry_enter() uses static_branch_{,un}likely() and therefore
+	 * idtentry_enter_user() uses static_branch_{,un}likely() and therefore
 	 * can trigger INT3, hence poke_int3_handler() must be done
 	 * before. If the entry came from kernel mode, then use nmi_enter()
 	 * because the INT3 could have been hit in any context including
 	 * NMI.
 	 */
 	if (user_mode(regs)) {
-		idtentry_enter(regs);
+		idtentry_enter_user(regs);
 		instrumentation_begin();
 		do_int3_user(regs);
 		instrumentation_end();
-		idtentry_exit(regs);
+		idtentry_exit_user(regs);
 	} else {
 		nmi_enter();
 		instrumentation_begin();
@@ -877,7 +877,7 @@ static __always_inline void exc_debug_kernel(struct pt_regs *regs,
 static __always_inline void exc_debug_user(struct pt_regs *regs,
 					   unsigned long dr6)
 {
-	idtentry_enter(regs);
+	idtentry_enter_user(regs);
 	clear_thread_flag(TIF_BLOCKSTEP);
 
 	/*
@@ -886,7 +886,7 @@ static __always_inline void exc_debug_user(struct pt_regs *regs,
 	 * User wants a sigtrap for that.
 	 */
 	handle_debug(regs, dr6, !dr6);
-	idtentry_exit(regs);
+	idtentry_exit_user(regs);
 }
 
 #ifdef CONFIG_X86_64

commit 7102cb07132624cdc09aa8e40c03ae34b4cbb74a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 25 09:42:41 2020 +0200

    x86/entry: Fix allnoconfig build warning
    
    The following commit:
    
      095b7a3e7745 ("x86/entry: Convert double fault exception to IDTENTRY_DF")
    
    introduced a new build warning on 64-bit allnoconfig kernels, that have CONFIG_VMAP_STACK disabled:
    
      arch/x86/kernel/traps.c:332:16: warning: unused variable ‘address’ [-Wunused-variable]
    
    This variable is only used if CONFIG_VMAP_STACK is defined, so make it
    dependent on that, not CONFIG_X86_64.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Alexandre Chartre <alexandre.chartre@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 35298c1df32f..9e5d81cb94ba 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -328,7 +328,7 @@ DEFINE_IDTENTRY_DF(exc_double_fault)
 	static const char str[] = "double fault";
 	struct task_struct *tsk = current;
 
-#ifdef CONFIG_X86_64
+#ifdef CONFIG_VMAP_STACK
 	unsigned long address = read_cr2();
 #endif
 

commit c29c775a554f7060b6fb31b68f88a3c9087cf1c5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:33:31 2020 +0100

    x86/entry: Convert double fault exception to IDTENTRY_DF
    
    Convert #DF to IDTENTRY_DF
      - Implement the C entry point with DEFINE_IDTENTRY_DF
      - Emit the ASM stub with DECLARE_IDTENTRY_DF on 64bit
      - Remove the ASM idtentry in 64bit
      - Adjust the 32bit shim code
      - Fixup the XEN/PV code
      - Remove the old prototypes
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505135315.583415264@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 41bb0cb9df84..35298c1df32f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -319,12 +319,19 @@ __visible void __noreturn handle_stack_overflow(const char *message,
  * from the TSS.  Returning is, in principle, okay, but changes to regs will
  * be lost.  If, for some reason, we need to return to a context with modified
  * regs, the shim code could be adjusted to synchronize the registers.
+ *
+ * The 32bit #DF shim provides CR2 already as an argument. On 64bit it needs
+ * to be read before doing anything else.
  */
-dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsigned long cr2)
+DEFINE_IDTENTRY_DF(exc_double_fault)
 {
 	static const char str[] = "double fault";
 	struct task_struct *tsk = current;
 
+#ifdef CONFIG_X86_64
+	unsigned long address = read_cr2();
+#endif
+
 #ifdef CONFIG_X86_ESPFIX64
 	extern unsigned char native_irq_return_iret[];
 
@@ -381,6 +388,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 #endif
 
 	nmi_enter();
+	instrumentation_begin();
 	notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);
 
 	tsk->thread.error_code = error_code;
@@ -424,13 +432,16 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 	 * stack even if the actual trigger for the double fault was
 	 * something else.
 	 */
-	if ((unsigned long)task_stack_page(tsk) - 1 - cr2 < PAGE_SIZE)
-		handle_stack_overflow("kernel stack overflow (double-fault)", regs, cr2);
+	if ((unsigned long)task_stack_page(tsk) - 1 - address < PAGE_SIZE) {
+		handle_stack_overflow("kernel stack overflow (double-fault)",
+				      regs, address);
+	}
 #endif
 
 	pr_emerg("PANIC: double fault, error_code: 0x%lx\n", error_code);
 	die("double fault", regs, error_code);
 	panic("Machine halted.");
+	instrumentation_end();
 }
 
 DEFINE_IDTENTRY(exc_bounds)

commit 75347bb2535a6d5549cc3e436467b7c40d7bb874
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 30 11:07:20 2020 +0200

    x86/traps: Address objtool noinstr complaints in #DB
    
    The functions invoked from handle_debug() can be instrumented. Tell objtool
    about it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505135315.380927730@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b62e962871f2..41bb0cb9df84 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -784,14 +784,19 @@ static void noinstr handle_debug(struct pt_regs *regs, unsigned long dr6,
 	/* Store the virtualized DR6 value */
 	tsk->thread.debugreg6 = dr6;
 
+	instrumentation_begin();
 #ifdef CONFIG_KPROBES
-	if (kprobe_debug_handler(regs))
+	if (kprobe_debug_handler(regs)) {
+		instrumentation_end();
 		return;
+	}
 #endif
 
 	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, 0,
-		       SIGTRAP) == NOTIFY_STOP)
+		       SIGTRAP) == NOTIFY_STOP) {
+		instrumentation_end();
 		return;
+	}
 
 	/*
 	 * Let others (NMI) know that the debug stack is in use
@@ -827,6 +832,7 @@ static void noinstr handle_debug(struct pt_regs *regs, unsigned long dr6,
 out:
 	cond_local_irq_disable(regs);
 	debug_stack_usage_dec();
+	instrumentation_end();
 }
 
 static __always_inline void exc_debug_kernel(struct pt_regs *regs,

commit 9347f41352181bf4a7e663f7b5f4a4bb32244d73
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 4 19:56:26 2020 +0200

    x86/traps: Restructure #DB handling
    
    Now that there are separate entry points, move the kernel/user_mode specifc
    checks into the entry functions so the common handling code does not need
    the extra mode checks. Make the code more readable while at it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505135315.283276272@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 4f248c5d5cab..b62e962871f2 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -775,39 +775,12 @@ static __always_inline void debug_exit(unsigned long dr7)
  *
  * May run on IST stack.
  */
-static noinstr void handle_debug(struct pt_regs *regs, unsigned long dr6)
+static void noinstr handle_debug(struct pt_regs *regs, unsigned long dr6,
+				 bool user_icebp)
 {
 	struct task_struct *tsk = current;
-	int user_icebp = 0;
 	int si_code;
 
-	/*
-	 * The SDM says "The processor clears the BTF flag when it
-	 * generates a debug exception."  Clear TIF_BLOCKSTEP to keep
-	 * TIF_BLOCKSTEP in sync with the hardware BTF flag.
-	 */
-	clear_tsk_thread_flag(tsk, TIF_BLOCKSTEP);
-
-	if (unlikely(!user_mode(regs) && (dr6 & DR_STEP) &&
-		     is_sysenter_singlestep(regs))) {
-		dr6 &= ~DR_STEP;
-		if (!dr6)
-			return;
-		/*
-		 * else we might have gotten a single-step trap and hit a
-		 * watchpoint at the same time, in which case we should fall
-		 * through and handle the watchpoint.
-		 */
-	}
-
-	/*
-	 * If dr6 has no reason to give us about the origin of this trap,
-	 * then it's very likely the result of an icebp/int01 trap.
-	 * User wants a sigtrap for that.
-	 */
-	if (!dr6 && user_mode(regs))
-		user_icebp = 1;
-
 	/* Store the virtualized DR6 value */
 	tsk->thread.debugreg6 = dr6;
 
@@ -832,9 +805,7 @@ static noinstr void handle_debug(struct pt_regs *regs, unsigned long dr6)
 	if (v8086_mode(regs)) {
 		handle_vm86_trap((struct kernel_vm86_regs *) regs, 0,
 				 X86_TRAP_DB);
-		cond_local_irq_disable(regs);
-		debug_stack_usage_dec();
-		return;
+		goto out;
 	}
 
 	if (WARN_ON_ONCE((dr6 & DR_STEP) && !user_mode(regs))) {
@@ -848,9 +819,12 @@ static noinstr void handle_debug(struct pt_regs *regs, unsigned long dr6)
 		set_tsk_thread_flag(tsk, TIF_SINGLESTEP);
 		regs->flags &= ~X86_EFLAGS_TF;
 	}
+
 	si_code = get_si_code(tsk->thread.debugreg6);
 	if (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)
 		send_sigtrap(regs, 0, si_code);
+
+out:
 	cond_local_irq_disable(regs);
 	debug_stack_usage_dec();
 }
@@ -859,7 +833,27 @@ static __always_inline void exc_debug_kernel(struct pt_regs *regs,
 					     unsigned long dr6)
 {
 	nmi_enter();
-	handle_debug(regs, dr6);
+	/*
+	 * The SDM says "The processor clears the BTF flag when it
+	 * generates a debug exception."  Clear TIF_BLOCKSTEP to keep
+	 * TIF_BLOCKSTEP in sync with the hardware BTF flag.
+	 */
+	clear_thread_flag(TIF_BLOCKSTEP);
+
+	/*
+	 * Catch SYSENTER with TF set and clear DR_STEP. If this hit a
+	 * watchpoint at the same time then that will still be handled.
+	 */
+	if ((dr6 & DR_STEP) && is_sysenter_singlestep(regs))
+		dr6 &= ~DR_STEP;
+
+	/*
+	 * If DR6 is zero, no point in trying to handle it. The kernel is
+	 * not using INT1.
+	 */
+	if (dr6)
+		handle_debug(regs, dr6, false);
+
 	nmi_exit();
 }
 
@@ -867,7 +861,14 @@ static __always_inline void exc_debug_user(struct pt_regs *regs,
 					   unsigned long dr6)
 {
 	idtentry_enter(regs);
-	handle_debug(regs, dr6);
+	clear_thread_flag(TIF_BLOCKSTEP);
+
+	/*
+	 * If dr6 has no reason to give us about the origin of this trap,
+	 * then it's very likely the result of an icebp/int01 trap.
+	 * User wants a sigtrap for that.
+	 */
+	handle_debug(regs, dr6, !dr6);
 	idtentry_exit(regs);
 }
 

commit 4c0dcd8350a03cb65f645a039f2772be880ee74a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:33:29 2020 +0100

    x86/entry: Implement user mode C entry points for #DB and #MCE
    
    The MCE entry point uses the same mechanism as the IST entry point for
    now. For #DB split the inner workings and just keep the nmi_enter/exit()
    magic in the IST variant. Fixup the ASM code to emit the proper
    noist_##cfunc call.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505135315.177564104@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 569408a681b6..4f248c5d5cab 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -775,20 +775,12 @@ static __always_inline void debug_exit(unsigned long dr7)
  *
  * May run on IST stack.
  */
-DEFINE_IDTENTRY_DEBUG(exc_debug)
+static noinstr void handle_debug(struct pt_regs *regs, unsigned long dr6)
 {
 	struct task_struct *tsk = current;
-	unsigned long dr6, dr7;
 	int user_icebp = 0;
 	int si_code;
 
-	debug_enter(&dr6, &dr7);
-
-	if (user_mode(regs))
-		idtentry_enter(regs);
-	else
-		nmi_enter();
-
 	/*
 	 * The SDM says "The processor clears the BTF flag when it
 	 * generates a debug exception."  Clear TIF_BLOCKSTEP to keep
@@ -800,7 +792,7 @@ DEFINE_IDTENTRY_DEBUG(exc_debug)
 		     is_sysenter_singlestep(regs))) {
 		dr6 &= ~DR_STEP;
 		if (!dr6)
-			goto exit;
+			return;
 		/*
 		 * else we might have gotten a single-step trap and hit a
 		 * watchpoint at the same time, in which case we should fall
@@ -821,12 +813,12 @@ DEFINE_IDTENTRY_DEBUG(exc_debug)
 
 #ifdef CONFIG_KPROBES
 	if (kprobe_debug_handler(regs))
-		goto exit;
+		return;
 #endif
 
 	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, 0,
 		       SIGTRAP) == NOTIFY_STOP)
-		goto exit;
+		return;
 
 	/*
 	 * Let others (NMI) know that the debug stack is in use
@@ -842,7 +834,7 @@ DEFINE_IDTENTRY_DEBUG(exc_debug)
 				 X86_TRAP_DB);
 		cond_local_irq_disable(regs);
 		debug_stack_usage_dec();
-		goto exit;
+		return;
 	}
 
 	if (WARN_ON_ONCE((dr6 & DR_STEP) && !user_mode(regs))) {
@@ -861,14 +853,60 @@ DEFINE_IDTENTRY_DEBUG(exc_debug)
 		send_sigtrap(regs, 0, si_code);
 	cond_local_irq_disable(regs);
 	debug_stack_usage_dec();
+}
+
+static __always_inline void exc_debug_kernel(struct pt_regs *regs,
+					     unsigned long dr6)
+{
+	nmi_enter();
+	handle_debug(regs, dr6);
+	nmi_exit();
+}
+
+static __always_inline void exc_debug_user(struct pt_regs *regs,
+					   unsigned long dr6)
+{
+	idtentry_enter(regs);
+	handle_debug(regs, dr6);
+	idtentry_exit(regs);
+}
+
+#ifdef CONFIG_X86_64
+/* IST stack entry */
+DEFINE_IDTENTRY_DEBUG(exc_debug)
+{
+	unsigned long dr6, dr7;
+
+	debug_enter(&dr6, &dr7);
+	exc_debug_kernel(regs, dr6);
+	debug_exit(dr7);
+}
+
+/* User entry, runs on regular task stack */
+DEFINE_IDTENTRY_DEBUG_USER(exc_debug)
+{
+	unsigned long dr6, dr7;
+
+	debug_enter(&dr6, &dr7);
+	exc_debug_user(regs, dr6);
+	debug_exit(dr7);
+}
+#else
+/* 32 bit does not have separate entry points. */
+DEFINE_IDTENTRY_DEBUG(exc_debug)
+{
+	unsigned long dr6, dr7;
+
+	debug_enter(&dr6, &dr7);
 
-exit:
 	if (user_mode(regs))
-		idtentry_exit(regs);
+		exc_debug_user(regs, dr6);
 	else
-		nmi_exit();
+		exc_debug_kernel(regs, dr6);
+
 	debug_exit(dr7);
 }
+#endif
 
 /*
  * Note that we play around with the 'TS' bit in an attempt to get

commit 2bbc68f8373c0631ebf137f376fbea00e8086be7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:33:26 2020 +0100

    x86/entry: Convert Debug exception to IDTENTRY_DB
    
    Convert #DB to IDTENTRY_ERRORCODE:
      - Implement the C entry point with DEFINE_IDTENTRY_DB
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505135314.900297476@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index de5120e2fbe1..569408a681b6 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -775,7 +775,7 @@ static __always_inline void debug_exit(unsigned long dr7)
  *
  * May run on IST stack.
  */
-dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
+DEFINE_IDTENTRY_DEBUG(exc_debug)
 {
 	struct task_struct *tsk = current;
 	unsigned long dr6, dr7;
@@ -784,7 +784,10 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 
 	debug_enter(&dr6, &dr7);
 
-	nmi_enter();
+	if (user_mode(regs))
+		idtentry_enter(regs);
+	else
+		nmi_enter();
 
 	/*
 	 * The SDM says "The processor clears the BTF flag when it
@@ -821,7 +824,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 		goto exit;
 #endif
 
-	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, error_code,
+	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, 0,
 		       SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
@@ -835,8 +838,8 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	cond_local_irq_enable(regs);
 
 	if (v8086_mode(regs)) {
-		handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code,
-					X86_TRAP_DB);
+		handle_vm86_trap((struct kernel_vm86_regs *) regs, 0,
+				 X86_TRAP_DB);
 		cond_local_irq_disable(regs);
 		debug_stack_usage_dec();
 		goto exit;
@@ -855,15 +858,17 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	}
 	si_code = get_si_code(tsk->thread.debugreg6);
 	if (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)
-		send_sigtrap(regs, error_code, si_code);
+		send_sigtrap(regs, 0, si_code);
 	cond_local_irq_disable(regs);
 	debug_stack_usage_dec();
 
 exit:
-	nmi_exit();
+	if (user_mode(regs))
+		idtentry_exit(regs);
+	else
+		nmi_exit();
 	debug_exit(dr7);
 }
-NOKPROBE_SYMBOL(do_debug);
 
 /*
  * Note that we play around with the 'TS' bit in an attempt to get

commit 9f58fdde95c9509a4ded27a6d0035e79294002b4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Apr 6 21:02:56 2020 +0200

    x86/db: Split out dr6/7 handling
    
    DR6/7 should be handled before nmi_enter() is invoked and restore after
    nmi_exit() to minimize the exposure.
    
    Split it out into helper inlines and bring it into the correct order.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505135314.808628211@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 21c8cfce24d3..de5120e2fbe1 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -700,6 +700,57 @@ static bool is_sysenter_singlestep(struct pt_regs *regs)
 #endif
 }
 
+static __always_inline void debug_enter(unsigned long *dr6, unsigned long *dr7)
+{
+	/*
+	 * Disable breakpoints during exception handling; recursive exceptions
+	 * are exceedingly 'fun'.
+	 *
+	 * Since this function is NOKPROBE, and that also applies to
+	 * HW_BREAKPOINT_X, we can't hit a breakpoint before this (XXX except a
+	 * HW_BREAKPOINT_W on our stack)
+	 *
+	 * Entry text is excluded for HW_BP_X and cpu_entry_area, which
+	 * includes the entry stack is excluded for everything.
+	 */
+	get_debugreg(*dr7, 7);
+	set_debugreg(0, 7);
+
+	/*
+	 * Ensure the compiler doesn't lower the above statements into
+	 * the critical section; disabling breakpoints late would not
+	 * be good.
+	 */
+	barrier();
+
+	/*
+	 * The Intel SDM says:
+	 *
+	 *   Certain debug exceptions may clear bits 0-3. The remaining
+	 *   contents of the DR6 register are never cleared by the
+	 *   processor. To avoid confusion in identifying debug
+	 *   exceptions, debug handlers should clear the register before
+	 *   returning to the interrupted task.
+	 *
+	 * Keep it simple: clear DR6 immediately.
+	 */
+	get_debugreg(*dr6, 6);
+	set_debugreg(0, 6);
+	/* Filter out all the reserved bits which are preset to 1 */
+	*dr6 &= ~DR6_RESERVED;
+}
+
+static __always_inline void debug_exit(unsigned long dr7)
+{
+	/*
+	 * Ensure the compiler doesn't raise this statement into
+	 * the critical section; enabling breakpoints early would
+	 * not be good.
+	 */
+	barrier();
+	set_debugreg(dr7, 7);
+}
+
 /*
  * Our handling of the processor debug registers is non-trivial.
  * We do not clear them on entry and exit from the kernel. Therefore
@@ -727,28 +778,13 @@ static bool is_sysenter_singlestep(struct pt_regs *regs)
 dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk = current;
+	unsigned long dr6, dr7;
 	int user_icebp = 0;
-	unsigned long dr6;
 	int si_code;
 
-	nmi_enter();
-
-	get_debugreg(dr6, 6);
-	/*
-	 * The Intel SDM says:
-	 *
-	 *   Certain debug exceptions may clear bits 0-3. The remaining
-	 *   contents of the DR6 register are never cleared by the
-	 *   processor. To avoid confusion in identifying debug
-	 *   exceptions, debug handlers should clear the register before
-	 *   returning to the interrupted task.
-	 *
-	 * Keep it simple: clear DR6 immediately.
-	 */
-	set_debugreg(0, 6);
+	debug_enter(&dr6, &dr7);
 
-	/* Filter out all the reserved bits which are preset to 1 */
-	dr6 &= ~DR6_RESERVED;
+	nmi_enter();
 
 	/*
 	 * The SDM says "The processor clears the BTF flag when it
@@ -786,7 +822,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 #endif
 
 	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, error_code,
-							SIGTRAP) == NOTIFY_STOP)
+		       SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
 	/*
@@ -825,6 +861,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 
 exit:
 	nmi_exit();
+	debug_exit(dr7);
 }
 NOKPROBE_SYMBOL(do_debug);
 

commit 21e28290b31708b72763641604e239eb369c230d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Mar 5 16:09:52 2020 +0100

    x86/traps: Split int3 handler up
    
    For code simplicity split up the int3 handler into a kernel and user part
    which makes the code flow simpler to understand.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Link: https://lkml.kernel.org/r/20200505135314.045220765@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 0ad12dffde22..21c8cfce24d3 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -568,6 +568,35 @@ DEFINE_IDTENTRY_ERRORCODE(exc_general_protection)
 	cond_local_irq_disable(regs);
 }
 
+static bool do_int3(struct pt_regs *regs)
+{
+	int res;
+
+#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
+	if (kgdb_ll_trap(DIE_INT3, "int3", regs, 0, X86_TRAP_BP,
+			 SIGTRAP) == NOTIFY_STOP)
+		return true;
+#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */
+
+#ifdef CONFIG_KPROBES
+	if (kprobe_int3_handler(regs))
+		return true;
+#endif
+	res = notify_die(DIE_INT3, "int3", regs, 0, X86_TRAP_BP, SIGTRAP);
+
+	return res == NOTIFY_STOP;
+}
+
+static void do_int3_user(struct pt_regs *regs)
+{
+	if (do_int3(regs))
+		return;
+
+	cond_local_irq_enable(regs);
+	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, 0, 0, NULL);
+	cond_local_irq_disable(regs);
+}
+
 DEFINE_IDTENTRY_RAW(exc_int3)
 {
 	/*
@@ -585,37 +614,20 @@ DEFINE_IDTENTRY_RAW(exc_int3)
 	 * because the INT3 could have been hit in any context including
 	 * NMI.
 	 */
-	if (user_mode(regs))
+	if (user_mode(regs)) {
 		idtentry_enter(regs);
-	else
-		nmi_enter();
-
-	instrumentation_begin();
-#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
-	if (kgdb_ll_trap(DIE_INT3, "int3", regs, 0, X86_TRAP_BP,
-				SIGTRAP) == NOTIFY_STOP)
-		goto exit;
-#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */
-
-#ifdef CONFIG_KPROBES
-	if (kprobe_int3_handler(regs))
-		goto exit;
-#endif
-
-	if (notify_die(DIE_INT3, "int3", regs, 0, X86_TRAP_BP,
-			SIGTRAP) == NOTIFY_STOP)
-		goto exit;
-
-	cond_local_irq_enable(regs);
-	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, 0, 0, NULL);
-	cond_local_irq_disable(regs);
-
-exit:
-	instrumentation_end();
-	if (user_mode(regs))
+		instrumentation_begin();
+		do_int3_user(regs);
+		instrumentation_end();
 		idtentry_exit(regs);
-	else
+	} else {
+		nmi_enter();
+		instrumentation_begin();
+		if (!do_int3(regs))
+			die("int3", regs, 0);
+		instrumentation_end();
 		nmi_exit();
+	}
 }
 
 #ifdef CONFIG_X86_64

commit 8edd7e37aed8b9df938a63f0b0259c70569ce3d2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:16 2020 +0100

    x86/entry: Convert INT3 exception to IDTENTRY_RAW
    
    Convert #BP to IDTENTRY_RAW:
      - Implement the C entry point with DEFINE_IDTENTRY_RAW
      - Invoke idtentry_enter/exit() from the function body
      - Emit the ASM stub with DECLARE_IDTENTRY_RAW
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
    
    No functional change.
    
    This could be a plain IDTENTRY, but as Peter pointed out INT3 is broken
    vs. the static key in the context tracking code as this static key might be
    in the state of being patched and has an int3 which would recurse forever.
    IDTENTRY_RAW is therefore chosen to allow addressing this issue without
    lots of code churn.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505135313.938474960@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 280c290f414f..0ad12dffde22 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -568,7 +568,7 @@ DEFINE_IDTENTRY_ERRORCODE(exc_general_protection)
 	cond_local_irq_disable(regs);
 }
 
-dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
+DEFINE_IDTENTRY_RAW(exc_int3)
 {
 	/*
 	 * poke_int3_handler() is completely self contained code; it does (and
@@ -579,16 +579,20 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 		return;
 
 	/*
-	 * Unlike any other non-IST entry, we can be called from pretty much
-	 * any location in the kernel through kprobes -- text_poke() will most
-	 * likely be handled by poke_int3_handler() above. This means this
-	 * handler is effectively NMI-like.
+	 * idtentry_enter() uses static_branch_{,un}likely() and therefore
+	 * can trigger INT3, hence poke_int3_handler() must be done
+	 * before. If the entry came from kernel mode, then use nmi_enter()
+	 * because the INT3 could have been hit in any context including
+	 * NMI.
 	 */
-	if (!user_mode(regs))
+	if (user_mode(regs))
+		idtentry_enter(regs);
+	else
 		nmi_enter();
 
+	instrumentation_begin();
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
-	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
+	if (kgdb_ll_trap(DIE_INT3, "int3", regs, 0, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 #endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */
@@ -598,19 +602,21 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 		goto exit;
 #endif
 
-	if (notify_die(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
+	if (notify_die(DIE_INT3, "int3", regs, 0, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
 	cond_local_irq_enable(regs);
-	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, 0, NULL);
+	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, 0, 0, NULL);
 	cond_local_irq_disable(regs);
 
 exit:
-	if (!user_mode(regs))
+	instrumentation_end();
+	if (user_mode(regs))
+		idtentry_exit(regs);
+	else
 		nmi_exit();
 }
-NOKPROBE_SYMBOL(do_int3);
 
 #ifdef CONFIG_X86_64
 /*

commit f64366efd8c60b93138b813d071d2cd201fd0f6e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Feb 20 13:28:06 2020 +0100

    x86/int3: Inline bsearch()
    
    Avoid calling out to bsearch() by inlining it, for normal kernel configs
    this was the last external call and poke_int3_handler() is now fully self
    sufficient -- no calls to external code.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505135313.731774429@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b28a64d7691f..280c290f414f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -570,6 +570,11 @@ DEFINE_IDTENTRY_ERRORCODE(exc_general_protection)
 
 dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
+	/*
+	 * poke_int3_handler() is completely self contained code; it does (and
+	 * must) *NOT* call out to anything, lest it hits upon yet another
+	 * INT3.
+	 */
 	if (poke_int3_handler(regs))
 		return;
 

commit d77290507ab2ac691d50389e255ebd11a6cbc35a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:30 2020 +0100

    x86/entry/32: Convert IRET exception to IDTENTRY_SW
    
    Convert the IRET exception handler to IDTENTRY_SW. This is slightly
    different than the conversions of hardware exceptions as the IRET exception
    is invoked via an exception table when IRET faults. So it just uses the
    IDTENTRY_SW mechanism for consistency. It does not emit ASM code as it does
    not fit the other idtentry exceptions.
    
      - Implement the C entry point with DEFINE_IDTENTRY_SW() which maps to
        DEFINE_IDTENTRY()
      - Fixup the XEN/PV code
      - Remove the old prototypes
      - Remove the RCU warning as the new entry macro ensures correctness
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134906.128769226@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1702922ebd9c..b28a64d7691f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -925,14 +925,12 @@ DEFINE_IDTENTRY(exc_device_not_available)
 }
 
 #ifdef CONFIG_X86_32
-dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
+DEFINE_IDTENTRY_SW(iret_error)
 {
-	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	local_irq_enable();
-
-	if (notify_die(DIE_TRAP, "iret exception", regs, error_code,
+	if (notify_die(DIE_TRAP, "iret exception", regs, 0,
 			X86_TRAP_IRET, SIGILL) != NOTIFY_STOP) {
-		do_trap(X86_TRAP_IRET, SIGILL, "iret exception", regs, error_code,
+		do_trap(X86_TRAP_IRET, SIGILL, "iret exception", regs, 0,
 			ILL_BADSTK, (void __user *)NULL);
 	}
 	local_irq_disable();

commit 48227e21f7430e31042f63e078a45cd230e9fdfc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:29 2020 +0100

    x86/entry: Convert SIMD coprocessor error exception to IDTENTRY
    
    Convert #XF to IDTENTRY_ERRORCODE:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Handle INVD_BUG in C
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
      - Remove the RCU warning as the new entry macro ensures correctness
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134906.021552202@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 9f156c84195d..1702922ebd9c 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -810,7 +810,7 @@ NOKPROBE_SYMBOL(do_debug);
  * the correct behaviour even in the presence of the asynchronous
  * IRQ13 behaviour
  */
-static void math_error(struct pt_regs *regs, int error_code, int trapnr)
+static void math_error(struct pt_regs *regs, int trapnr)
 {
 	struct task_struct *task = current;
 	struct fpu *fpu = &task->thread.fpu;
@@ -821,15 +821,15 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	cond_local_irq_enable(regs);
 
 	if (!user_mode(regs)) {
-		if (fixup_exception(regs, trapnr, error_code, 0))
+		if (fixup_exception(regs, trapnr, 0, 0))
 			goto exit;
 
-		task->thread.error_code = error_code;
+		task->thread.error_code = 0;
 		task->thread.trap_nr = trapnr;
 
-		if (notify_die(DIE_TRAP, str, regs, error_code,
-					trapnr, SIGFPE) != NOTIFY_STOP)
-			die(str, regs, error_code);
+		if (notify_die(DIE_TRAP, str, regs, 0, trapnr,
+			       SIGFPE) != NOTIFY_STOP)
+			die(str, regs, 0);
 		goto exit;
 	}
 
@@ -839,7 +839,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	fpu__save(fpu);
 
 	task->thread.trap_nr	= trapnr;
-	task->thread.error_code = error_code;
+	task->thread.error_code = 0;
 
 	si_code = fpu__exception_code(fpu, trapnr);
 	/* Retry when we get spurious exceptions: */
@@ -854,14 +854,19 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 
 DEFINE_IDTENTRY(exc_coprocessor_error)
 {
-	math_error(regs, 0, X86_TRAP_MF);
+	math_error(regs, X86_TRAP_MF);
 }
 
-dotraplinkage void
-do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
+DEFINE_IDTENTRY(exc_simd_coprocessor_error)
 {
-	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
-	math_error(regs, error_code, X86_TRAP_XF);
+	if (IS_ENABLED(CONFIG_X86_INVD_BUG)) {
+		/* AMD 486 bug: INVD in CPL 0 raises #XF instead of #GP */
+		if (!static_cpu_has(X86_FEATURE_XMM)) {
+			__exc_general_protection(regs, 0);
+			return;
+		}
+	}
+	math_error(regs, X86_TRAP_XF);
 }
 
 DEFINE_IDTENTRY(exc_spurious_interrupt_bug)

commit 436608bb00a59f5457cee26f416067860ca88d9d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:28 2020 +0100

    x86/entry: Convert Alignment check exception to IDTENTRY
    
    Convert #AC to IDTENTRY_ERRORCODE:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
      - Remove the RCU warning as the new entry macro ensures correctness
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134905.928967113@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ba26bebfed72..9f156c84195d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -270,12 +270,10 @@ DEFINE_IDTENTRY_ERRORCODE(exc_stack_segment)
 		      0, NULL);
 }
 
-dotraplinkage void do_alignment_check(struct pt_regs *regs, long error_code)
+DEFINE_IDTENTRY_ERRORCODE(exc_alignment_check)
 {
 	char *str = "alignment check";
 
-	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
-
 	if (notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_AC, SIGBUS) == NOTIFY_STOP)
 		return;
 

commit 14a8bd2aa7c355b3a8879618a4f70f9c2b0004f7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:27 2020 +0100

    x86/entry: Convert Coprocessor error exception to IDTENTRY
    
    Convert #MF to IDTENTRY_ERRORCODE:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
      - Remove the RCU warning as the new entry macro ensures correctness
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134905.838823510@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 2c638b9bc827..ba26bebfed72 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -854,10 +854,9 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	cond_local_irq_disable(regs);
 }
 
-dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
+DEFINE_IDTENTRY(exc_coprocessor_error)
 {
-	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
-	math_error(regs, error_code, X86_TRAP_MF);
+	math_error(regs, 0, X86_TRAP_MF);
 }
 
 dotraplinkage void

commit dad7106f8194df1b096666c5499ef732497ddb15
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:26 2020 +0100

    x86/entry: Convert Spurious interrupt bug exception to IDTENTRY
    
    Convert #SPURIOUS to IDTENTRY_ERRORCODE:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134905.728077036@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index e65c7612ecf3..2c638b9bc827 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -867,8 +867,7 @@ do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 	math_error(regs, error_code, X86_TRAP_XF);
 }
 
-dotraplinkage void
-do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
+DEFINE_IDTENTRY(exc_spurious_interrupt_bug)
 {
 	/*
 	 * This addresses a Pentium Pro Erratum:

commit be4c11afbb6d5317274e61fda0edf744080fb72b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:25 2020 +0100

    x86/entry: Convert General protection exception to IDTENTRY
    
    Convert #GP to IDTENTRY_ERRORCODE:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
      - Remove the RCU warning as the new entry macro ensures correctness
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134905.637269946@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 3dfdc4d3de87..e65c7612ecf3 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -145,7 +145,7 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, const char *str,
 	 * process no chance to handle the signal and notice the
 	 * kernel fault information, so that won't result in polluting
 	 * the information about previously queued, but not yet
-	 * delivered, faults.  See also do_general_protection below.
+	 * delivered, faults.  See also exc_general_protection below.
 	 */
 	tsk->thread.error_code = error_code;
 	tsk->thread.trap_nr = trapnr;
@@ -375,7 +375,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 		 * which is what the stub expects, given that the faulting
 		 * RIP will be the IRET instruction.
 		 */
-		regs->ip = (unsigned long)general_protection;
+		regs->ip = (unsigned long)asm_exc_general_protection;
 		regs->sp = (unsigned long)&gpregs->orig_ax;
 
 		return;
@@ -494,7 +494,7 @@ static enum kernel_gp_hint get_kernel_gp_address(struct pt_regs *regs,
 
 #define GPFSTR "general protection fault"
 
-dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
+DEFINE_IDTENTRY_ERRORCODE(exc_general_protection)
 {
 	char desc[sizeof(GPFSTR) + 50 + 2*sizeof(unsigned long) + 1] = GPFSTR;
 	enum kernel_gp_hint hint = GP_NO_HINT;
@@ -502,7 +502,6 @@ dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
 	unsigned long gp_addr;
 	int ret;
 
-	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	cond_local_irq_enable(regs);
 
 	if (static_cpu_has(X86_FEATURE_UMIP)) {
@@ -570,7 +569,6 @@ dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
 exit:
 	cond_local_irq_disable(regs);
 }
-NOKPROBE_SYMBOL(do_general_protection);
 
 dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {

commit fd9689bf91131c4bea5ea54f828af5267f5ed6a0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:24 2020 +0100

    x86/entry: Convert Stack segment exception to IDTENTRY
    
    Convert #SS to IDTENTRY_ERRORCODE:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134905.539867572@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 88ba5f0400fd..3dfdc4d3de87 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -264,16 +264,12 @@ DEFINE_IDTENTRY_ERRORCODE(exc_segment_not_present)
 		      SIGBUS, 0, NULL);
 }
 
-#define IP ((void __user *)uprobe_get_trap_addr(regs))
-#define DO_ERROR(trapnr, signr, sicode, addr, str, name)		   \
-dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
-{									   \
-	do_error_trap(regs, error_code, str, trapnr, signr, sicode, addr); \
+DEFINE_IDTENTRY_ERRORCODE(exc_stack_segment)
+{
+	do_error_trap(regs, error_code, "stack segment", X86_TRAP_SS, SIGBUS,
+		      0, NULL);
 }
 
-DO_ERROR(X86_TRAP_SS,     SIGBUS,           0, NULL, "stack segment",       stack_segment)
-#undef IP
-
 dotraplinkage void do_alignment_check(struct pt_regs *regs, long error_code)
 {
 	char *str = "alignment check";

commit 99a3fb8d01af1085f16a417a748e0a462dc92d29
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:23 2020 +0100

    x86/entry: Convert Segment not present exception to IDTENTRY
    
    Convert #NP to IDTENTRY_ERRORCODE:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134905.443591450@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 10ab0837668f..88ba5f0400fd 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -258,6 +258,12 @@ DEFINE_IDTENTRY_ERRORCODE(exc_invalid_tss)
 		      0, NULL);
 }
 
+DEFINE_IDTENTRY_ERRORCODE(exc_segment_not_present)
+{
+	do_error_trap(regs, error_code, "segment not present", X86_TRAP_NP,
+		      SIGBUS, 0, NULL);
+}
+
 #define IP ((void __user *)uprobe_get_trap_addr(regs))
 #define DO_ERROR(trapnr, signr, sicode, addr, str, name)		   \
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
@@ -265,7 +271,6 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
 	do_error_trap(regs, error_code, str, trapnr, signr, sicode, addr); \
 }
 
-DO_ERROR(X86_TRAP_NP,     SIGBUS,           0, NULL, "segment not present", segment_not_present)
 DO_ERROR(X86_TRAP_SS,     SIGBUS,           0, NULL, "stack segment",       stack_segment)
 #undef IP
 

commit 97b3d290b865cf9115f7d37d40b7482efba4d46d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:22 2020 +0100

    x86/entry: Convert Invalid TSS exception to IDTENTRY
    
    Convert #TS to IDTENTRY_ERRORCODE:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134905.350676449@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 3ce1f667d078..10ab0837668f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -252,6 +252,12 @@ DEFINE_IDTENTRY(exc_coproc_segment_overrun)
 		      X86_TRAP_OLD_MF, SIGFPE, 0, NULL);
 }
 
+DEFINE_IDTENTRY_ERRORCODE(exc_invalid_tss)
+{
+	do_error_trap(regs, error_code, "invalid TSS", X86_TRAP_TS, SIGSEGV,
+		      0, NULL);
+}
+
 #define IP ((void __user *)uprobe_get_trap_addr(regs))
 #define DO_ERROR(trapnr, signr, sicode, addr, str, name)		   \
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
@@ -259,7 +265,6 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
 	do_error_trap(regs, error_code, str, trapnr, signr, sicode, addr); \
 }
 
-DO_ERROR(X86_TRAP_TS,     SIGSEGV,          0, NULL, "invalid TSS",         invalid_TSS)
 DO_ERROR(X86_TRAP_NP,     SIGBUS,           0, NULL, "segment not present", segment_not_present)
 DO_ERROR(X86_TRAP_SS,     SIGBUS,           0, NULL, "stack segment",       stack_segment)
 #undef IP

commit f95658fdb575233f79e3e7ed7ecf990389d31319
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:20 2020 +0100

    x86/entry: Convert Coprocessor segment overrun exception to IDTENTRY
    
    Convert #OLD_MF to IDTENTRY:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134905.838823510@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b8af5eb6a929..3ce1f667d078 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -246,6 +246,12 @@ DEFINE_IDTENTRY(exc_invalid_op)
 	handle_invalid_op(regs);
 }
 
+DEFINE_IDTENTRY(exc_coproc_segment_overrun)
+{
+	do_error_trap(regs, 0, "coprocessor segment overrun",
+		      X86_TRAP_OLD_MF, SIGFPE, 0, NULL);
+}
+
 #define IP ((void __user *)uprobe_get_trap_addr(regs))
 #define DO_ERROR(trapnr, signr, sicode, addr, str, name)		   \
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
@@ -253,7 +259,6 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
 	do_error_trap(regs, error_code, str, trapnr, signr, sicode, addr); \
 }
 
-DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,           0, NULL, "coprocessor segment overrun", coprocessor_segment_overrun)
 DO_ERROR(X86_TRAP_TS,     SIGSEGV,          0, NULL, "invalid TSS",         invalid_TSS)
 DO_ERROR(X86_TRAP_NP,     SIGBUS,           0, NULL, "segment not present", segment_not_present)
 DO_ERROR(X86_TRAP_SS,     SIGBUS,           0, NULL, "stack segment",       stack_segment)

commit 866ae2ccee4ac092fea14f18d537205e14c5a904
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:19 2020 +0100

    x86/entry: Convert Device not available exception to IDTENTRY
    
    Convert #NM to IDTENTRY:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
      - Remove the RCU warning as the new entry macro ensures correctness
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134905.056243863@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 71ac43dc036a..b8af5eb6a929 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -882,13 +882,10 @@ do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 	 */
 }
 
-dotraplinkage void
-do_device_not_available(struct pt_regs *regs, long error_code)
+DEFINE_IDTENTRY(exc_device_not_available)
 {
 	unsigned long cr0 = read_cr0();
 
-	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
-
 #ifdef CONFIG_MATH_EMULATION
 	if (!boot_cpu_has(X86_FEATURE_FPU) && (cr0 & X86_CR0_EM)) {
 		struct math_emu_info info = { };
@@ -913,10 +910,9 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 		 * to kill the task than getting stuck in a never-ending
 		 * loop of #NM faults.
 		 */
-		die("unexpected #NM exception", regs, error_code);
+		die("unexpected #NM exception", regs, 0);
 	}
 }
-NOKPROBE_SYMBOL(do_device_not_available);
 
 #ifdef CONFIG_X86_32
 dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)

commit 49893c5cb281f8691dcbe53e6f85a963f47a4b9b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:18 2020 +0100

    x86/entry: Convert Invalid Opcode exception to IDTENTRY
    
    Convert #UD to IDTENTRY:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Fixup the FOOF bug call in fault.c
      - Remove the old prototypes
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134904.955511913@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7a9fb8b9e1a8..71ac43dc036a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -231,6 +231,21 @@ DEFINE_IDTENTRY(exc_overflow)
 	do_error_trap(regs, 0, "overflow", X86_TRAP_OF, SIGSEGV, 0, NULL);
 }
 
+#ifdef CONFIG_X86_F00F_BUG
+void handle_invalid_op(struct pt_regs *regs)
+#else
+static inline void handle_invalid_op(struct pt_regs *regs)
+#endif
+{
+	do_error_trap(regs, 0, "invalid opcode", X86_TRAP_UD, SIGILL,
+		      ILL_ILLOPN, error_get_trap_addr(regs));
+}
+
+DEFINE_IDTENTRY(exc_invalid_op)
+{
+	handle_invalid_op(regs);
+}
+
 #define IP ((void __user *)uprobe_get_trap_addr(regs))
 #define DO_ERROR(trapnr, signr, sicode, addr, str, name)		   \
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
@@ -238,7 +253,6 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
 	do_error_trap(regs, error_code, str, trapnr, signr, sicode, addr); \
 }
 
-DO_ERROR(X86_TRAP_UD,     SIGILL,  ILL_ILLOPN,   IP, "invalid opcode",      invalid_op)
 DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,           0, NULL, "coprocessor segment overrun", coprocessor_segment_overrun)
 DO_ERROR(X86_TRAP_TS,     SIGSEGV,          0, NULL, "invalid TSS",         invalid_TSS)
 DO_ERROR(X86_TRAP_NP,     SIGBUS,           0, NULL, "segment not present", segment_not_present)

commit 58d9c81facf55dbd1836d114ce360a048e3a0582
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:17 2020 +0100

    x86/entry: Convert Bounds exception to IDTENTRY
    
    Convert #BR to IDTENTRY:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
      - Remove the RCU warning as the new entry macro ensures correctness
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134904.863001309@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b522e2aa9e9a..7a9fb8b9e1a8 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -410,18 +410,17 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 	panic("Machine halted.");
 }
 
-dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
+DEFINE_IDTENTRY(exc_bounds)
 {
-	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
-	if (notify_die(DIE_TRAP, "bounds", regs, error_code,
+	if (notify_die(DIE_TRAP, "bounds", regs, 0,
 			X86_TRAP_BR, SIGSEGV) == NOTIFY_STOP)
 		return;
 	cond_local_irq_enable(regs);
 
 	if (!user_mode(regs))
-		die("bounds", regs, error_code);
+		die("bounds", regs, 0);
 
-	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, 0, NULL);
+	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, 0, 0, NULL);
 
 	cond_local_irq_disable(regs);
 }

commit 4b6b9111c0b9aa4c3b319f1c5a3b1d5850792167
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:15 2020 +0100

    x86/entry: Convert Overflow exception to IDTENTRY
    
    Convert #OF to IDTENTRY:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
      - Remove the old prototypes
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134904.771457898@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 37092f74ec42..b522e2aa9e9a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -226,6 +226,11 @@ DEFINE_IDTENTRY(exc_divide_error)
 		      FPE_INTDIV, error_get_trap_addr(regs));
 }
 
+DEFINE_IDTENTRY(exc_overflow)
+{
+	do_error_trap(regs, 0, "overflow", X86_TRAP_OF, SIGSEGV, 0, NULL);
+}
+
 #define IP ((void __user *)uprobe_get_trap_addr(regs))
 #define DO_ERROR(trapnr, signr, sicode, addr, str, name)		   \
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
@@ -233,7 +238,6 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
 	do_error_trap(regs, error_code, str, trapnr, signr, sicode, addr); \
 }
 
-DO_ERROR(X86_TRAP_OF,     SIGSEGV,          0, NULL, "overflow",            overflow)
 DO_ERROR(X86_TRAP_UD,     SIGILL,  ILL_ILLOPN,   IP, "invalid opcode",      invalid_op)
 DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,           0, NULL, "coprocessor segment overrun", coprocessor_segment_overrun)
 DO_ERROR(X86_TRAP_TS,     SIGSEGV,          0, NULL, "invalid TSS",         invalid_TSS)

commit 9d06c4027f21fcfa60221bd7203eda3c82568467
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:14 2020 +0100

    x86/entry: Convert Divide Error to IDTENTRY
    
    Convert #DE to IDTENTRY:
      - Implement the C entry point with DEFINE_IDTENTRY
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64bit
      - Remove the open coded ASM entry code in 32bit
      - Fixup the XEN/PV code
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134904.663914713@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 3857c0fd3306..37092f74ec42 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -220,6 +220,12 @@ static __always_inline void __user *error_get_trap_addr(struct pt_regs *regs)
 	return (void __user *)uprobe_get_trap_addr(regs);
 }
 
+DEFINE_IDTENTRY(exc_divide_error)
+{
+	do_error_trap(regs, 0, "divide_error", X86_TRAP_DE, SIGFPE,
+		      FPE_INTDIV, error_get_trap_addr(regs));
+}
+
 #define IP ((void __user *)uprobe_get_trap_addr(regs))
 #define DO_ERROR(trapnr, signr, sicode, addr, str, name)		   \
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
@@ -227,7 +233,6 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
 	do_error_trap(regs, error_code, str, trapnr, signr, sicode, addr); \
 }
 
-DO_ERROR(X86_TRAP_DE,     SIGFPE,  FPE_INTDIV,   IP, "divide error",        divide_error)
 DO_ERROR(X86_TRAP_OF,     SIGSEGV,          0, NULL, "overflow",            overflow)
 DO_ERROR(X86_TRAP_UD,     SIGILL,  ILL_ILLOPN,   IP, "invalid opcode",      invalid_op)
 DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,           0, NULL, "coprocessor segment overrun", coprocessor_segment_overrun)

commit 218e31b6e7a33c9b5e5d608aa79d51665bb84e62
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 23:16:13 2020 +0100

    x86/traps: Prepare for using DEFINE_IDTENTRY
    
    Prepare for using IDTENTRY to define the C exception/trap entry points. It
    would be possible to glue this into the existing macro maze, but it's
    simpler and better to read at the end to just make them distinct.
    
    Provide a trivial inline helper to read the trap address and add a comment
    explaining the logic behind it.
    
    The existing macros will be removed once all instances are converted.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134904.556327833@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index f5f4a76fb516..3857c0fd3306 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -205,6 +205,21 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 	}
 }
 
+/*
+ * Posix requires to provide the address of the faulting instruction for
+ * SIGILL (#UD) and SIGFPE (#DE) in the si_addr member of siginfo_t.
+ *
+ * This address is usually regs->ip, but when an uprobe moved the code out
+ * of line then regs->ip points to the XOL code which would confuse
+ * anything which analyzes the fault address vs. the unmodified binary. If
+ * a trap happened in XOL code then uprobe maps regs->ip back to the
+ * original instruction address.
+ */
+static __always_inline void __user *error_get_trap_addr(struct pt_regs *regs)
+{
+	return (void __user *)uprobe_get_trap_addr(regs);
+}
+
 #define IP ((void __user *)uprobe_get_trap_addr(regs))
 #define DO_ERROR(trapnr, signr, sicode, addr, str, name)		   \
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \

commit ca4c6a9858c23b4f330113f391f2eadc983e780f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 23 14:27:10 2019 +0200

    x86/traps: Make interrupt enable/disable symmetric in C code
    
    Traps enable interrupts conditionally but rely on the ASM return code to
    disable them again. That results in redundant interrupt disable and trace
    calls.
    
    Make the trap handlers disable interrupts before returning to avoid that,
    which allows simplification of the ASM entry code in follow up changes.
    
    Originally-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134903.622702796@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index adcc62380ece..f5f4a76fb516 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -201,6 +201,7 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 			NOTIFY_STOP) {
 		cond_local_irq_enable(regs);
 		do_trap(trapnr, signr, str, regs, error_code, sicode, addr);
+		cond_local_irq_disable(regs);
 	}
 }
 
@@ -397,6 +398,8 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 		die("bounds", regs, error_code);
 
 	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, 0, NULL);
+
+	cond_local_irq_disable(regs);
 }
 
 enum kernel_gp_hint {
@@ -456,12 +459,13 @@ dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
 
 	if (static_cpu_has(X86_FEATURE_UMIP)) {
 		if (user_mode(regs) && fixup_umip_exception(regs))
-			return;
+			goto exit;
 	}
 
 	if (v8086_mode(regs)) {
 		local_irq_enable();
 		handle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);
+		local_irq_disable();
 		return;
 	}
 
@@ -473,12 +477,11 @@ dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
 
 		show_signal(tsk, SIGSEGV, "", desc, regs, error_code);
 		force_sig(SIGSEGV);
-
-		return;
+		goto exit;
 	}
 
 	if (fixup_exception(regs, X86_TRAP_GP, error_code, 0))
-		return;
+		goto exit;
 
 	tsk->thread.error_code = error_code;
 	tsk->thread.trap_nr = X86_TRAP_GP;
@@ -490,11 +493,11 @@ dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
 	if (!preemptible() &&
 	    kprobe_running() &&
 	    kprobe_fault_handler(regs, X86_TRAP_GP))
-		return;
+		goto exit;
 
 	ret = notify_die(DIE_GPF, desc, regs, error_code, X86_TRAP_GP, SIGSEGV);
 	if (ret == NOTIFY_STOP)
-		return;
+		goto exit;
 
 	if (error_code)
 		snprintf(desc, sizeof(desc), "segment-related " GPFSTR);
@@ -516,6 +519,8 @@ dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
 
 	die_addr(desc, regs, error_code, gp_addr);
 
+exit:
+	cond_local_irq_disable(regs);
 }
 NOKPROBE_SYMBOL(do_general_protection);
 
@@ -773,7 +778,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 
 	if (!user_mode(regs)) {
 		if (fixup_exception(regs, trapnr, error_code, 0))
-			return;
+			goto exit;
 
 		task->thread.error_code = error_code;
 		task->thread.trap_nr = trapnr;
@@ -781,7 +786,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 		if (notify_die(DIE_TRAP, str, regs, error_code,
 					trapnr, SIGFPE) != NOTIFY_STOP)
 			die(str, regs, error_code);
-		return;
+		goto exit;
 	}
 
 	/*
@@ -795,10 +800,12 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	si_code = fpu__exception_code(fpu, trapnr);
 	/* Retry when we get spurious exceptions: */
 	if (!si_code)
-		return;
+		goto exit;
 
 	force_sig_fault(SIGFPE, si_code,
 			(void __user *)uprobe_get_trap_addr(regs));
+exit:
+	cond_local_irq_disable(regs);
 }
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
@@ -853,6 +860,8 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 
 		info.regs = regs;
 		math_emulate(&info);
+
+		cond_local_irq_disable(regs);
 		return;
 	}
 #endif
@@ -883,6 +892,7 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 		do_trap(X86_TRAP_IRET, SIGILL, "iret exception", regs, error_code,
 			ILL_BADSTK, (void __user *)NULL);
 	}
+	local_irq_disable();
 }
 #endif
 

commit daf7a69787b587d454adea73377a904e09fd54a9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 25 23:47:51 2020 +0100

    x86/traps: Mark sync_regs() noinstr
    
    Replace the notrace and NOKPROBE annotations with noinstr.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134903.439765290@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b2b36561a569..adcc62380ece 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -564,14 +564,13 @@ NOKPROBE_SYMBOL(do_int3);
  * to switch to the normal thread stack if the interrupted code was in
  * user mode. The actual stack switch is done in entry_64.S
  */
-asmlinkage __visible notrace struct pt_regs *sync_regs(struct pt_regs *eregs)
+asmlinkage __visible noinstr struct pt_regs *sync_regs(struct pt_regs *eregs)
 {
 	struct pt_regs *regs = (struct pt_regs *)this_cpu_read(cpu_current_top_of_stack) - 1;
 	if (regs != eregs)
 		*regs = *eregs;
 	return regs;
 }
-NOKPROBE_SYMBOL(sync_regs);
 
 struct bad_iret_stack {
 	void *error_entry_ret;

commit d73a332936a6d33be3aa3fa4bee959efab09e431
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 25 19:53:38 2020 +0100

    x86/traps: Mark fixup_bad_iret() noinstr
    
    This is called from deep entry ASM in a situation where instrumentation
    will cause more harm than providing useful information.
    
    Switch from memmove() to memcpy() because memmove() can't be called
    from noinstr code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134903.346741553@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 48468f61202c..b2b36561a569 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -578,7 +578,7 @@ struct bad_iret_stack {
 	struct pt_regs regs;
 };
 
-asmlinkage __visible notrace
+asmlinkage __visible noinstr
 struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)
 {
 	/*
@@ -589,19 +589,21 @@ struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)
 	 * just below the IRET frame) and we want to pretend that the
 	 * exception came from the IRET target.
 	 */
-	struct bad_iret_stack *new_stack =
-		(struct bad_iret_stack *)this_cpu_read(cpu_tss_rw.x86_tss.sp0) - 1;
+	struct bad_iret_stack tmp, *new_stack =
+		(struct bad_iret_stack *)__this_cpu_read(cpu_tss_rw.x86_tss.sp0) - 1;
 
-	/* Copy the IRET target to the new stack. */
-	memmove(&new_stack->regs.ip, (void *)s->regs.sp, 5*8);
+	/* Copy the IRET target to the temporary storage. */
+	memcpy(&tmp.regs.ip, (void *)s->regs.sp, 5*8);
 
 	/* Copy the remainder of the stack from the current stack. */
-	memmove(new_stack, s, offsetof(struct bad_iret_stack, regs.ip));
+	memcpy(&tmp, s, offsetof(struct bad_iret_stack, regs.ip));
+
+	/* Update the entry stack */
+	memcpy(new_stack, &tmp, sizeof(tmp));
 
 	BUG_ON(!user_mode(&new_stack->regs));
 	return new_stack;
 }
-NOKPROBE_SYMBOL(fixup_bad_iret);
 #endif
 
 static bool is_sysenter_singlestep(struct pt_regs *regs)

commit e9660391d0ebd174b169af3d6de680c2f836027c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Feb 20 13:17:27 2020 +0100

    x86/doublefault: Remove memmove() call
    
    Use of memmove() in #DF is problematic considered tracing and other
    instrumentation.
    
    Remove the memmove() call and simply write out what needs doing; this
    even clarifies the code, win-win! The code copies from the espfix64
    stack to the normal task stack, there is no possible way for that to
    overlap.
    
    Survives selftests/x86, specifically sigreturn_64.
    
    Suggested-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505134058.863038566@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 4cc541051994..48468f61202c 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -299,6 +299,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 		regs->ip == (unsigned long)native_irq_return_iret)
 	{
 		struct pt_regs *gpregs = (struct pt_regs *)this_cpu_read(cpu_tss_rw.x86_tss.sp0) - 1;
+		unsigned long *p = (unsigned long *)regs->sp;
 
 		/*
 		 * regs->sp points to the failing IRET frame on the
@@ -306,7 +307,11 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 		 * in gpregs->ss through gpregs->ip.
 		 *
 		 */
-		memmove(&gpregs->ip, (void *)regs->sp, 5*8);
+		gpregs->ip	= p[0];
+		gpregs->cs	= p[1];
+		gpregs->flags	= p[2];
+		gpregs->sp	= p[3];
+		gpregs->ss	= p[4];
 		gpregs->orig_ax = 0;  /* Missing (lost) #GP error code */
 
 		/*

commit 039aeb9deb9291f3b19c375a8bc6fa7f768996cc
Merge: 6b2591c21273 13ffbd8db1dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 15:13:47 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - Move the arch-specific code into arch/arm64/kvm
    
       - Start the post-32bit cleanup
    
       - Cherry-pick a few non-invasive pre-NV patches
    
      x86:
       - Rework of TLB flushing
    
       - Rework of event injection, especially with respect to nested
         virtualization
    
       - Nested AMD event injection facelift, building on the rework of
         generic code and fixing a lot of corner cases
    
       - Nested AMD live migration support
    
       - Optimization for TSC deadline MSR writes and IPIs
    
       - Various cleanups
    
       - Asynchronous page fault cleanups (from tglx, common topic branch
         with tip tree)
    
       - Interrupt-based delivery of asynchronous "page ready" events (host
         side)
    
       - Hyper-V MSRs and hypercalls for guest debugging
    
       - VMX preemption timer fixes
    
      s390:
       - Cleanups
    
      Generic:
       - switch vCPU thread wakeup from swait to rcuwait
    
      The other architectures, and the guest side of the asynchronous page
      fault work, will come next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (256 commits)
      KVM: selftests: fix rdtsc() for vmx_tsc_adjust_test
      KVM: check userspace_addr for all memslots
      KVM: selftests: update hyperv_cpuid with SynDBG tests
      x86/kvm/hyper-v: Add support for synthetic debugger via hypercalls
      x86/kvm/hyper-v: enable hypercalls regardless of hypercall page
      x86/kvm/hyper-v: Add support for synthetic debugger interface
      x86/hyper-v: Add synthetic debugger definitions
      KVM: selftests: VMX preemption timer migration test
      KVM: nVMX: Fix VMX preemption timer migration
      x86/kvm/hyper-v: Explicitly align hcall param for kvm_hyperv_exit
      KVM: x86/pmu: Support full width counting
      KVM: x86/pmu: Tweak kvm_pmu_get_msr to pass 'struct msr_data' in
      KVM: x86: announce KVM_FEATURE_ASYNC_PF_INT
      KVM: x86: acknowledgment mechanism for async pf page ready notifications
      KVM: x86: interrupt based APF 'page ready' event delivery
      KVM: introduce kvm_read_guest_offset_cached()
      KVM: rename kvm_arch_can_inject_async_page_present() to kvm_arch_can_dequeue_async_page_present()
      KVM: x86: extend struct kvm_vcpu_pv_apf_data with token info
      Revert "KVM: async_pf: Fix #DF due to inject "Page not Present" and "Page Ready" exceptions simultaneously"
      KVM: VMX: Replace zero-length array with flexible-array
      ...

commit 533b220f7be4e461a5222a223d169b42856741ef
Merge: 3ee3723b40d5 082af5ec5080
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 15:18:27 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "A sizeable pile of arm64 updates for 5.8.
    
      Summary below, but the big two features are support for Branch Target
      Identification and Clang's Shadow Call stack. The latter is currently
      arm64-only, but the high-level parts are all in core code so it could
      easily be adopted by other architectures pending toolchain support
    
      Branch Target Identification (BTI):
    
       - Support for ARMv8.5-BTI in both user- and kernel-space. This allows
         branch targets to limit the types of branch from which they can be
         called and additionally prevents branching to arbitrary code,
         although kernel support requires a very recent toolchain.
    
       - Function annotation via SYM_FUNC_START() so that assembly functions
         are wrapped with the relevant "landing pad" instructions.
    
       - BPF and vDSO updates to use the new instructions.
    
       - Addition of a new HWCAP and exposure of BTI capability to userspace
         via ID register emulation, along with ELF loader support for the
         BTI feature in .note.gnu.property.
    
       - Non-critical fixes to CFI unwind annotations in the sigreturn
         trampoline.
    
      Shadow Call Stack (SCS):
    
       - Support for Clang's Shadow Call Stack feature, which reserves
         platform register x18 to point at a separate stack for each task
         that holds only return addresses. This protects function return
         control flow from buffer overruns on the main stack.
    
       - Save/restore of x18 across problematic boundaries (user-mode,
         hypervisor, EFI, suspend, etc).
    
       - Core support for SCS, should other architectures want to use it
         too.
    
       - SCS overflow checking on context-switch as part of the existing
         stack limit check if CONFIG_SCHED_STACK_END_CHECK=y.
    
      CPU feature detection:
    
       - Removed numerous "SANITY CHECK" errors when running on a system
         with mismatched AArch32 support at EL1. This is primarily a concern
         for KVM, which disabled support for 32-bit guests on such a system.
    
       - Addition of new ID registers and fields as the architecture has
         been extended.
    
      Perf and PMU drivers:
    
       - Minor fixes and cleanups to system PMU drivers.
    
      Hardware errata:
    
       - Unify KVM workarounds for VHE and nVHE configurations.
    
       - Sort vendor errata entries in Kconfig.
    
      Secure Monitor Call Calling Convention (SMCCC):
    
       - Update to the latest specification from Arm (v1.2).
    
       - Allow PSCI code to query the SMCCC version.
    
      Software Delegated Exception Interface (SDEI):
    
       - Unexport a bunch of unused symbols.
    
       - Minor fixes to handling of firmware data.
    
      Pointer authentication:
    
       - Add support for dumping the kernel PAC mask in vmcoreinfo so that
         the stack can be unwound by tools such as kdump.
    
       - Simplification of key initialisation during CPU bringup.
    
      BPF backend:
    
       - Improve immediate generation for logical and add/sub instructions.
    
      vDSO:
    
       - Minor fixes to the linker flags for consistency with other
         architectures and support for LLVM's unwinder.
    
       - Clean up logic to initialise and map the vDSO into userspace.
    
      ACPI:
    
       - Work around for an ambiguity in the IORT specification relating to
         the "num_ids" field.
    
       - Support _DMA method for all named components rather than only PCIe
         root complexes.
    
       - Minor other IORT-related fixes.
    
      Miscellaneous:
    
       - Initialise debug traps early for KGDB and fix KDB cacheflushing
         deadlock.
    
       - Minor tweaks to early boot state (documentation update, set
         TEXT_OFFSET to 0x0, increase alignment of PE/COFF sections).
    
       - Refactoring and cleanup"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (148 commits)
      KVM: arm64: Move __load_guest_stage2 to kvm_mmu.h
      KVM: arm64: Check advertised Stage-2 page size capability
      arm64/cpufeature: Add get_arm64_ftr_reg_nowarn()
      ACPI/IORT: Remove the unused __get_pci_rid()
      arm64/cpuinfo: Add ID_MMFR4_EL1 into the cpuinfo_arm64 context
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR1 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR0 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64ISAR0 register
      arm64/cpufeature: Add remaining feature bits in ID_MMFR4 register
      arm64/cpufeature: Add remaining feature bits in ID_PFR0 register
      arm64/cpufeature: Introduce ID_MMFR5 CPU register
      arm64/cpufeature: Introduce ID_DFR1 CPU register
      arm64/cpufeature: Introduce ID_PFR2 CPU register
      arm64/cpufeature: Make doublelock a signed feature in ID_AA64DFR0
      arm64/cpufeature: Drop TraceFilt feature exposure from ID_DFR0 register
      arm64/cpufeature: Add explicit ftr_id_isar0[] for ID_ISAR0 register
      arm64: mm: Add asid_gen_match() helper
      firmware: smccc: Fix missing prototype warning for arm_smccc_version_init
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      ...

commit ef68017eb5704eb2b0577c3aa6619e13caf2b59f
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Feb 28 10:42:48 2020 -0800

    x86/kvm: Handle async page faults directly through do_page_fault()
    
    KVM overloads #PF to indicate two types of not-actually-page-fault
    events.  Right now, the KVM guest code intercepts them by modifying
    the IDT and hooking the #PF vector.  This makes the already fragile
    fault code even harder to understand, and it also pollutes call
    traces with async_page_fault and do_async_page_fault for normal page
    faults.
    
    Clean it up by moving the logic into do_page_fault() using a static
    branch.  This gets rid of the platform trap_init override mechanism
    completely.
    
    [ tglx: Fixed up 32bit, removed error code from the async functions and
            massaged coding style ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134059.169270470@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index d54cffdc7cac..821fac47eef6 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -983,7 +983,5 @@ void __init trap_init(void)
 
 	idt_setup_ist_traps();
 
-	x86_init.irqs.trap_init();
-
 	idt_setup_debugidt_traps();
 }

commit 0d00449c7a28a1514595630735df383dec606812
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Feb 19 09:46:43 2020 +0100

    x86: Replace ist_enter() with nmi_enter()
    
    A few exceptions (like #DB and #BP) can happen at any location in the code,
    this then means that tracers should treat events from these exceptions as
    NMI-like. The interrupted context could be holding locks with interrupts
    disabled for instance.
    
    Similarly, #MC is an actual NMI-like exception.
    
    All of them use ist_enter() which only concerns itself with RCU, but does
    not do any of the other setup that NMIs need. This means things like:
    
            printk()
              raw_spin_lock_irq(&logbuf_lock);
              <#DB/#BP/#MC>
                 printk()
                   raw_spin_lock_irq(&logbuf_lock);
    
    are entirely possible (well, not really since printk tries hard to
    play nice, but the concept stands).
    
    So replace ist_enter() with nmi_enter(). Also observe that any nmi_enter()
    caller must be both notrace and NOKPROBE, or in the noinstr text section.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Link: https://lkml.kernel.org/r/20200505134101.525508608@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 6740e8351486..f7cfb9d0ad02 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -37,10 +37,12 @@
 #include <linux/mm.h>
 #include <linux/smp.h>
 #include <linux/io.h>
+#include <linux/hardirq.h>
+#include <linux/atomic.h>
+
 #include <asm/stacktrace.h>
 #include <asm/processor.h>
 #include <asm/debugreg.h>
-#include <linux/atomic.h>
 #include <asm/text-patching.h>
 #include <asm/ftrace.h>
 #include <asm/traps.h>
@@ -82,41 +84,6 @@ static inline void cond_local_irq_disable(struct pt_regs *regs)
 		local_irq_disable();
 }
 
-/*
- * In IST context, we explicitly disable preemption.  This serves two
- * purposes: it makes it much less likely that we would accidentally
- * schedule in IST context and it will force a warning if we somehow
- * manage to schedule by accident.
- */
-void ist_enter(struct pt_regs *regs)
-{
-	if (user_mode(regs)) {
-		RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
-	} else {
-		/*
-		 * We might have interrupted pretty much anything.  In
-		 * fact, if we're a machine check, we can even interrupt
-		 * NMI processing.  We don't want in_nmi() to return true,
-		 * but we need to notify RCU.
-		 */
-		rcu_nmi_enter();
-	}
-
-	preempt_disable();
-
-	/* This code is a bit fragile.  Test it. */
-	RCU_LOCKDEP_WARN(!rcu_is_watching(), "ist_enter didn't work");
-}
-NOKPROBE_SYMBOL(ist_enter);
-
-void ist_exit(struct pt_regs *regs)
-{
-	preempt_enable_no_resched();
-
-	if (!user_mode(regs))
-		rcu_nmi_exit();
-}
-
 int is_valid_bugaddr(unsigned long addr)
 {
 	unsigned short ud;
@@ -326,7 +293,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 	 * The net result is that our #GP handler will think that we
 	 * entered from usermode with the bad user context.
 	 *
-	 * No need for ist_enter here because we don't use RCU.
+	 * No need for nmi_enter() here because we don't use RCU.
 	 */
 	if (((long)regs->sp >> P4D_SHIFT) == ESPFIX_PGD_ENTRY &&
 		regs->cs == __KERNEL_CS &&
@@ -361,7 +328,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 	}
 #endif
 
-	ist_enter(regs);
+	nmi_enter();
 	notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);
 
 	tsk->thread.error_code = error_code;
@@ -555,19 +522,13 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 		return;
 
 	/*
-	 * Unlike any other non-IST entry, we can be called from a kprobe in
-	 * non-CONTEXT_KERNEL kernel mode or even during context tracking
-	 * state changes.  Make sure that we wake up RCU even if we're coming
-	 * from kernel code.
-	 *
-	 * This means that we can't schedule even if we came from a
-	 * preemptible kernel context.  That's okay.
+	 * Unlike any other non-IST entry, we can be called from pretty much
+	 * any location in the kernel through kprobes -- text_poke() will most
+	 * likely be handled by poke_int3_handler() above. This means this
+	 * handler is effectively NMI-like.
 	 */
-	if (!user_mode(regs)) {
-		rcu_nmi_enter();
-		preempt_disable();
-	}
-	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
+	if (!user_mode(regs))
+		nmi_enter();
 
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
@@ -589,10 +550,8 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	cond_local_irq_disable(regs);
 
 exit:
-	if (!user_mode(regs)) {
-		preempt_enable_no_resched();
-		rcu_nmi_exit();
-	}
+	if (!user_mode(regs))
+		nmi_exit();
 }
 NOKPROBE_SYMBOL(do_int3);
 
@@ -696,7 +655,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	unsigned long dr6;
 	int si_code;
 
-	ist_enter(regs);
+	nmi_enter();
 
 	get_debugreg(dr6, 6);
 	/*
@@ -789,7 +748,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	debug_stack_usage_dec();
 
 exit:
-	ist_exit(regs);
+	nmi_exit();
 }
 NOKPROBE_SYMBOL(do_debug);
 

commit b052df3da821adfd6be26a6eb16624fb50e90e56
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 5 00:52:41 2020 +0100

    x86/entry: Get rid of ist_begin/end_non_atomic()
    
    This is completely overengineered and definitely not an interface which
    should be made available to anything else than this particular MCE case.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134059.462640294@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index d54cffdc7cac..6740e8351486 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -117,43 +117,6 @@ void ist_exit(struct pt_regs *regs)
 		rcu_nmi_exit();
 }
 
-/**
- * ist_begin_non_atomic() - begin a non-atomic section in an IST exception
- * @regs:	regs passed to the IST exception handler
- *
- * IST exception handlers normally cannot schedule.  As a special
- * exception, if the exception interrupted userspace code (i.e.
- * user_mode(regs) would return true) and the exception was not
- * a double fault, it can be safe to schedule.  ist_begin_non_atomic()
- * begins a non-atomic section within an ist_enter()/ist_exit() region.
- * Callers are responsible for enabling interrupts themselves inside
- * the non-atomic section, and callers must call ist_end_non_atomic()
- * before ist_exit().
- */
-void ist_begin_non_atomic(struct pt_regs *regs)
-{
-	BUG_ON(!user_mode(regs));
-
-	/*
-	 * Sanity check: we need to be on the normal thread stack.  This
-	 * will catch asm bugs and any attempt to use ist_preempt_enable
-	 * from double_fault.
-	 */
-	BUG_ON(!on_thread_stack());
-
-	preempt_enable_no_resched();
-}
-
-/**
- * ist_end_non_atomic() - begin a non-atomic section in an IST exception
- *
- * Ends a non-atomic section started with ist_begin_non_atomic().
- */
-void ist_end_non_atomic(void)
-{
-	preempt_disable();
-}
-
 int is_valid_bugaddr(unsigned long addr)
 {
 	unsigned short ud;

commit 593309423cbad0fab659a685834416cf12d8f581
Author: Borislav Petkov <bp@suse.de>
Date:   Sat Apr 4 01:33:05 2020 +0200

    x86/32: Remove CONFIG_DOUBLEFAULT
    
    Make the doublefault exception handler unconditional on 32-bit. Yes,
    it is important to be able to catch #DF exceptions instead of silent
    reboots. Yes, the code size increase is worth every byte. And one less
    CONFIG symbol is just the cherry on top.
    
    No functional changes.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200404083646.8897-1-bp@alien8.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index d54cffdc7cac..e85561fc0dc8 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -326,7 +326,6 @@ __visible void __noreturn handle_stack_overflow(const char *message,
 }
 #endif
 
-#if defined(CONFIG_X86_64) || defined(CONFIG_DOUBLEFAULT)
 /*
  * Runs on an IST stack for x86_64 and on a special task stack for x86_32.
  *
@@ -450,7 +449,6 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 	die("double fault", regs, error_code);
 	panic("Machine halted.");
 }
-#endif
 
 dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 {

commit 2853d5fafb1e21707e89aa2e227c90bb2c1ea4a9
Merge: d5f744f9a2ac a6a60741035b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 19:35:52 2020 -0700

    Merge tag 'x86-splitlock-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 splitlock updates from Thomas Gleixner:
     "Support for 'split lock' detection:
    
      Atomic operations (lock prefixed instructions) which span two cache
      lines have to acquire the global bus lock. This is at least 1k cycles
      slower than an atomic operation within a cache line and disrupts
      performance on other cores. Aside of performance disruption this is a
      unpriviledged form of DoS.
    
      Some newer CPUs have the capability to raise an #AC trap when such an
      operation is attempted. The detection is by default enabled in warning
      mode which will warn once when a user space application is caught. A
      command line option allows to disable the detection or to select fatal
      mode which will terminate offending applications with SIGBUS"
    
    * tag 'x86-splitlock-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/split_lock: Avoid runtime reads of the TEST_CTRL MSR
      x86/split_lock: Rework the initialization flow of split lock detection
      x86/split_lock: Enable split lock detection by kernel

commit 65c668f5faebf549db086b7a6841b6f4187b4e4f
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Feb 25 22:36:46 2020 +0100

    x86/traps: Stop using ist_enter/exit() in do_int3()
    
    #BP is not longer using IST and using ist_enter() and ist_exit() makes it
    harder to change ist_enter() and ist_exit()'s behavior.  Instead open-code
    the very small amount of required logic.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200225220217.150607679@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7ffb6f4dea1c..c0bc9df8634d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -572,14 +572,20 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 		return;
 
 	/*
-	 * Use ist_enter despite the fact that we don't use an IST stack.
-	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
-	 * mode or even during context tracking state changes.
+	 * Unlike any other non-IST entry, we can be called from a kprobe in
+	 * non-CONTEXT_KERNEL kernel mode or even during context tracking
+	 * state changes.  Make sure that we wake up RCU even if we're coming
+	 * from kernel code.
 	 *
-	 * This means that we can't schedule.  That's okay.
+	 * This means that we can't schedule even if we came from a
+	 * preemptible kernel context.  That's okay.
 	 */
-	ist_enter(regs);
+	if (!user_mode(regs)) {
+		rcu_nmi_enter();
+		preempt_disable();
+	}
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
+
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
@@ -600,7 +606,10 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	cond_local_irq_disable(regs);
 
 exit:
-	ist_exit(regs);
+	if (!user_mode(regs)) {
+		preempt_enable_no_resched();
+		rcu_nmi_exit();
+	}
 }
 NOKPROBE_SYMBOL(do_int3);
 

commit d244d0e195bc12964bcf3b8eef45e715a7f203b0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 22:36:41 2020 +0100

    x86/traps: Document do_spurious_interrupt_bug()
    
    Add a comment which explains why this empty handler for a reserved vector
    exists.
    
    Requested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200225220216.624165786@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 474b8cb54899..7ffb6f4dea1c 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -862,6 +862,25 @@ do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 {
+	/*
+	 * This addresses a Pentium Pro Erratum:
+	 *
+	 * PROBLEM: If the APIC subsystem is configured in mixed mode with
+	 * Virtual Wire mode implemented through the local APIC, an
+	 * interrupt vector of 0Fh (Intel reserved encoding) may be
+	 * generated by the local APIC (Int 15).  This vector may be
+	 * generated upon receipt of a spurious interrupt (an interrupt
+	 * which is removed before the system receives the INTA sequence)
+	 * instead of the programmed 8259 spurious interrupt vector.
+	 *
+	 * IMPLICATION: The spurious interrupt vector programmed in the
+	 * 8259 is normally handled by an operating system's spurious
+	 * interrupt handler. However, a vector of 0Fh is unknown to some
+	 * operating systems, which would crash if this erratum occurred.
+	 *
+	 * In theory this could be limited to 32bit, but the handler is not
+	 * hurting and who knows which other CPUs suffer from this.
+	 */
 }
 
 dotraplinkage void

commit e039dd815941e785203142261397da6ec64d20cc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 25 22:36:40 2020 +0100

    x86/traps: Remove pointless irq enable from do_spurious_interrupt_bug()
    
    That function returns immediately after conditionally reenabling interrupts which
    is more than pointless and requires the ASM code to disable interrupts again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lore.kernel.org/r/20191023123117.871608831@linutronix.de
    Link: https://lkml.kernel.org/r/20200225220216.518575042@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 6ef00eb6fbb9..474b8cb54899 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -862,7 +862,6 @@ do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 {
-	cond_local_irq_enable(regs);
 }
 
 dotraplinkage void

commit 6650cdd9a8ccf00555dbbe743d58541ad8feb6a7
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Sun Jan 26 12:05:35 2020 -0800

    x86/split_lock: Enable split lock detection by kernel
    
    A split-lock occurs when an atomic instruction operates on data that spans
    two cache lines. In order to maintain atomicity the core takes a global bus
    lock.
    
    This is typically >1000 cycles slower than an atomic operation within a
    cache line. It also disrupts performance on other cores (which must wait
    for the bus lock to be released before their memory operations can
    complete). For real-time systems this may mean missing deadlines. For other
    systems it may just be very annoying.
    
    Some CPUs have the capability to raise an #AC trap when a split lock is
    attempted.
    
    Provide a command line option to give the user choices on how to handle
    this:
    
    split_lock_detect=
            off     - not enabled (no traps for split locks)
            warn    - warn once when an application does a
                      split lock, but allow it to continue
                      running.
            fatal   - Send SIGBUS to applications that cause split lock
    
    On systems that support split lock detection the default is "warn". Note
    that if the kernel hits a split lock in any mode other than "off" it will
    OOPs.
    
    One implementation wrinkle is that the MSR to control the split lock
    detection is per-core, not per thread. This might result in some short
    lived races on HT systems in "warn" mode if Linux tries to enable on one
    thread while disabling on the other. Race analysis by Sean Christopherson:
    
      - Toggling of split-lock is only done in "warn" mode.  Worst case
        scenario of a race is that a misbehaving task will generate multiple
        #AC exceptions on the same instruction.  And this race will only occur
        if both siblings are running tasks that generate split-lock #ACs, e.g.
        a race where sibling threads are writing different values will only
        occur if CPUx is disabling split-lock after an #AC and CPUy is
        re-enabling split-lock after *its* previous task generated an #AC.
      - Transitioning between off/warn/fatal modes at runtime isn't supported
        and disabling is tracked per task, so hardware will always reach a steady
        state that matches the configured mode.  I.e. split-lock is guaranteed to
        be enabled in hardware once all _TIF_SLD threads have been scheduled out.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Co-developed-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/20200126200535.GB30377@agluck-desk2.amr.corp.intel.com

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 6ef00eb6fbb9..0ef5befaed7d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -46,6 +46,7 @@
 #include <asm/traps.h>
 #include <asm/desc.h>
 #include <asm/fpu/internal.h>
+#include <asm/cpu.h>
 #include <asm/cpu_entry_area.h>
 #include <asm/mce.h>
 #include <asm/fixmap.h>
@@ -242,7 +243,6 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 {
 	struct task_struct *tsk = current;
 
-
 	if (!do_trap_no_signal(tsk, trapnr, str, regs, error_code))
 		return;
 
@@ -288,9 +288,29 @@ DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,           0, NULL, "coprocessor segment overru
 DO_ERROR(X86_TRAP_TS,     SIGSEGV,          0, NULL, "invalid TSS",         invalid_TSS)
 DO_ERROR(X86_TRAP_NP,     SIGBUS,           0, NULL, "segment not present", segment_not_present)
 DO_ERROR(X86_TRAP_SS,     SIGBUS,           0, NULL, "stack segment",       stack_segment)
-DO_ERROR(X86_TRAP_AC,     SIGBUS,  BUS_ADRALN, NULL, "alignment check",     alignment_check)
 #undef IP
 
+dotraplinkage void do_alignment_check(struct pt_regs *regs, long error_code)
+{
+	char *str = "alignment check";
+
+	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
+
+	if (notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_AC, SIGBUS) == NOTIFY_STOP)
+		return;
+
+	if (!user_mode(regs))
+		die("Split lock detected\n", regs, error_code);
+
+	local_irq_enable();
+
+	if (handle_user_split_lock(regs, error_code))
+		return;
+
+	do_trap(X86_TRAP_AC, SIGBUS, "alignment check", regs,
+		error_code, BUS_ADRALN, NULL);
+}
+
 #ifdef CONFIG_VMAP_STACK
 __visible void __noreturn handle_stack_overflow(const char *message,
 						struct pt_regs *regs,

commit ccaaaf6fe5a5e1fffca5cca0f3fc4ec84d7ae752
Merge: 35c222fd3236 45fc24e89b7c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 30 16:11:50 2020 -0800

    Merge tag 'mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/daveh/x86-mpx
    
    Pull x86 MPX removal from Dave Hansen:
     "MPX requires recompiling applications, which requires compiler
      support. Unfortunately, GCC 9.1 is expected to be be released without
      support for MPX. This means that there was only a relatively small
      window where folks could have ever used MPX. It failed to gain wide
      adoption in the industry, and Linux was the only mainstream OS to ever
      support it widely.
    
      Support for the feature may also disappear on future processors.
    
      This set completes the process that we started during the 5.4 merge
      window when the MPX prctl()s were removed. XSAVE support is left in
      place, which allows MPX-using KVM guests to continue to function"
    
    * tag 'mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/daveh/x86-mpx:
      x86/mpx: remove MPX from arch/x86
      mm: remove arch_bprm_mm_init() hook
      x86/mpx: remove bounds exception code
      x86/mpx: remove build infrastructure
      x86/alternatives: add missing insn.h include

commit f6170f0afbe23ad82b4a1195168949c31e3a2527
Merge: 6da49d1abd2c 248ed51048c4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 28 12:28:06 2020 -0800

    Merge branch 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull misc x86 updates from Ingo Molnar:
     "Misc changes:
    
       - Enhance #GP fault printouts by distinguishing between canonical and
         non-canonical address faults, and also add KASAN fault decoding.
    
       - Fix/enhance the x86 NMI handler by putting the duration check into
         a direct function call instead of an irq_work which we know to be
         broken in some cases.
    
       - Clean up do_general_protection() a bit"
    
    * 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/nmi: Remove irq_work from the long duration NMI handler
      x86/traps: Cleanup do_general_protection()
      x86/kasan: Print original address on #GP
      x86/dumpstack: Introduce die_addr() for die() with #GP fault address
      x86/traps: Print address on #GP
      x86/insn-eval: Add support for 64-bit kernel mode

commit aa9ccb7b47f909e2e7206ebb47e4c63e477199fc
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Jan 23 10:41:15 2020 -0800

    x86/mpx: remove bounds exception code
    
    From: Dave Hansen <dave.hansen@linux.intel.com>
    
    MPX is being removed from the kernel due to a lack of support
    in the toolchain going forward (gcc).
    
    Remove the other user-visible ABI: signal handling.  This code
    should basically have been inactive after the prctl()s were
    removed, but there may be some small ABI remnants from this code.
    Remove it.
    
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: x86@kernel.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 4bb0f8447112..6fd5b7561444 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -57,8 +57,6 @@
 #include <asm/mach_traps.h>
 #include <asm/alternative.h>
 #include <asm/fpu/xstate.h>
-#include <asm/trace/mpx.h>
-#include <asm/mpx.h>
 #include <asm/vm86.h>
 #include <asm/umip.h>
 
@@ -430,8 +428,6 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 
 dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 {
-	const struct mpx_bndcsr *bndcsr;
-
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	if (notify_die(DIE_TRAP, "bounds", regs, error_code,
 			X86_TRAP_BR, SIGSEGV) == NOTIFY_STOP)
@@ -441,76 +437,6 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	if (!user_mode(regs))
 		die("bounds", regs, error_code);
 
-	if (!cpu_feature_enabled(X86_FEATURE_MPX)) {
-		/* The exception is not from Intel MPX */
-		goto exit_trap;
-	}
-
-	/*
-	 * We need to look at BNDSTATUS to resolve this exception.
-	 * A NULL here might mean that it is in its 'init state',
-	 * which is all zeros which indicates MPX was not
-	 * responsible for the exception.
-	 */
-	bndcsr = get_xsave_field_ptr(XFEATURE_BNDCSR);
-	if (!bndcsr)
-		goto exit_trap;
-
-	trace_bounds_exception_mpx(bndcsr);
-	/*
-	 * The error code field of the BNDSTATUS register communicates status
-	 * information of a bound range exception #BR or operation involving
-	 * bound directory.
-	 */
-	switch (bndcsr->bndstatus & MPX_BNDSTA_ERROR_CODE) {
-	case 2:	/* Bound directory has invalid entry. */
-		if (mpx_handle_bd_fault())
-			goto exit_trap;
-		break; /* Success, it was handled */
-	case 1: /* Bound violation. */
-	{
-		struct task_struct *tsk = current;
-		struct mpx_fault_info mpx;
-
-		if (mpx_fault_info(&mpx, regs)) {
-			/*
-			 * We failed to decode the MPX instruction.  Act as if
-			 * the exception was not caused by MPX.
-			 */
-			goto exit_trap;
-		}
-		/*
-		 * Success, we decoded the instruction and retrieved
-		 * an 'mpx' containing the address being accessed
-		 * which caused the exception.  This information
-		 * allows and application to possibly handle the
-		 * #BR exception itself.
-		 */
-		if (!do_trap_no_signal(tsk, X86_TRAP_BR, "bounds", regs,
-				       error_code))
-			break;
-
-		show_signal(tsk, SIGSEGV, "trap ", "bounds", regs, error_code);
-
-		force_sig_bnderr(mpx.addr, mpx.lower, mpx.upper);
-		break;
-	}
-	case 0: /* No exception caused by Intel MPX operations. */
-		goto exit_trap;
-	default:
-		die("bounds", regs, error_code);
-	}
-
-	return;
-
-exit_trap:
-	/*
-	 * This path out is for all the cases where we could not
-	 * handle the exception in some way (like allocating a
-	 * table or telling userspace about it.  We will also end
-	 * up here if the kernel has MPX turned off at compile
-	 * time..
-	 */
 	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, 0, NULL);
 }
 

commit 36209766cede1fe9d39f3d3418d93bbf71ad21c4
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Dec 31 17:15:35 2019 +0100

    x86/traps: Cleanup do_general_protection()
    
    Hoist the user_mode() case up because it is less code and can be dealt
    with up-front like the other special cases UMIP and vm86.
    
    This saves an indentation level for the kernel-mode #GP case and allows
    to "unfold" the code more so that it is more readable.
    
    No functional changes.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jann Horn <jannh@google.com>
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 2afd7d8d4007..ca395ad28b4e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -567,7 +567,10 @@ static enum kernel_gp_hint get_kernel_gp_address(struct pt_regs *regs,
 dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
 {
 	char desc[sizeof(GPFSTR) + 50 + 2*sizeof(unsigned long) + 1] = GPFSTR;
+	enum kernel_gp_hint hint = GP_NO_HINT;
 	struct task_struct *tsk;
+	unsigned long gp_addr;
+	int ret;
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	cond_local_irq_enable(regs);
@@ -584,58 +587,56 @@ dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
 	}
 
 	tsk = current;
-	if (!user_mode(regs)) {
-		enum kernel_gp_hint hint = GP_NO_HINT;
-		unsigned long gp_addr;
-
-		if (fixup_exception(regs, X86_TRAP_GP, error_code, 0))
-			return;
 
+	if (user_mode(regs)) {
 		tsk->thread.error_code = error_code;
 		tsk->thread.trap_nr = X86_TRAP_GP;
 
-		/*
-		 * To be potentially processing a kprobe fault and to
-		 * trust the result from kprobe_running(), we have to
-		 * be non-preemptible.
-		 */
-		if (!preemptible() && kprobe_running() &&
-		    kprobe_fault_handler(regs, X86_TRAP_GP))
-			return;
+		show_signal(tsk, SIGSEGV, "", desc, regs, error_code);
+		force_sig(SIGSEGV);
 
-		if (notify_die(DIE_GPF, desc, regs, error_code,
-			       X86_TRAP_GP, SIGSEGV) == NOTIFY_STOP)
-			return;
+		return;
+	}
 
-		if (error_code)
-			snprintf(desc, sizeof(desc), "segment-related " GPFSTR);
-		else
-			hint = get_kernel_gp_address(regs, &gp_addr);
+	if (fixup_exception(regs, X86_TRAP_GP, error_code, 0))
+		return;
 
-		if (hint != GP_NO_HINT)
-			snprintf(desc, sizeof(desc), GPFSTR ", %s 0x%lx",
-				 (hint == GP_NON_CANONICAL) ?
-				 "probably for non-canonical address" :
-				 "maybe for address",
-				 gp_addr);
+	tsk->thread.error_code = error_code;
+	tsk->thread.trap_nr = X86_TRAP_GP;
 
-		/*
-		 * KASAN is interested only in the non-canonical case, clear it
-		 * otherwise.
-		 */
-		if (hint != GP_NON_CANONICAL)
-			gp_addr = 0;
+	/*
+	 * To be potentially processing a kprobe fault and to trust the result
+	 * from kprobe_running(), we have to be non-preemptible.
+	 */
+	if (!preemptible() &&
+	    kprobe_running() &&
+	    kprobe_fault_handler(regs, X86_TRAP_GP))
+		return;
 
-		die_addr(desc, regs, error_code, gp_addr);
+	ret = notify_die(DIE_GPF, desc, regs, error_code, X86_TRAP_GP, SIGSEGV);
+	if (ret == NOTIFY_STOP)
 		return;
-	}
 
-	tsk->thread.error_code = error_code;
-	tsk->thread.trap_nr = X86_TRAP_GP;
+	if (error_code)
+		snprintf(desc, sizeof(desc), "segment-related " GPFSTR);
+	else
+		hint = get_kernel_gp_address(regs, &gp_addr);
+
+	if (hint != GP_NO_HINT)
+		snprintf(desc, sizeof(desc), GPFSTR ", %s 0x%lx",
+			 (hint == GP_NON_CANONICAL) ? "probably for non-canonical address"
+						    : "maybe for address",
+			 gp_addr);
+
+	/*
+	 * KASAN is interested only in the non-canonical case, clear it
+	 * otherwise.
+	 */
+	if (hint != GP_NON_CANONICAL)
+		gp_addr = 0;
 
-	show_signal(tsk, SIGSEGV, "", desc, regs, error_code);
+	die_addr(desc, regs, error_code, gp_addr);
 
-	force_sig(SIGSEGV);
 }
 NOKPROBE_SYMBOL(do_general_protection);
 

commit aa49f20462c90df4150f33d245cbcfe0d9c80350
Author: Jann Horn <jannh@google.com>
Date:   Thu Dec 19 00:11:49 2019 +0100

    x86/dumpstack: Introduce die_addr() for die() with #GP fault address
    
    Split __die() into __die_header() and __die_body(). This allows inserting
    extra information below the header line that initiates the bug report.
    
    Introduce a new function die_addr() that behaves like die(), but is for
    faults only and uses __die_header() and __die_body() so that a future
    commit can print extra information after the header line.
    
     [ bp: Comment the KASAN-specific usage of gp_addr. ]
    
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: kasan-dev@googlegroups.com
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: "Peter Zijlstra (Intel)" <peterz@infradead.org>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20191218231150.12139-3-jannh@google.com

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 108ab1ee069f..2afd7d8d4007 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -619,7 +619,14 @@ dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
 				 "maybe for address",
 				 gp_addr);
 
-		die(desc, regs, error_code);
+		/*
+		 * KASAN is interested only in the non-canonical case, clear it
+		 * otherwise.
+		 */
+		if (hint != GP_NON_CANONICAL)
+			gp_addr = 0;
+
+		die_addr(desc, regs, error_code, gp_addr);
 		return;
 	}
 

commit 59c1dcbed5b51cab543be8f47b6d7d9cf107ec94
Author: Jann Horn <jannh@google.com>
Date:   Thu Dec 19 00:11:48 2019 +0100

    x86/traps: Print address on #GP
    
    A frequent cause of #GP exceptions are memory accesses to non-canonical
    addresses. Unlike #PF, #GP doesn't report a fault address in CR2, so the
    kernel doesn't currently print the fault address for a #GP.
    
    Luckily, the necessary infrastructure for decoding x86 instructions and
    computing the memory address being accessed is already present. Hook
    it up to the #GP handler so that the address operand of the faulting
    instruction can be figured out and printed.
    
    Distinguish two cases:
    
      a) (Part of) the memory range being accessed lies in the non-canonical
         address range; in this case, it is likely that the decoded address
         is actually the one that caused the #GP.
    
      b) The entire memory range of the decoded operand lies in canonical
         address space; the #GP may or may not be related in some way to the
         computed address. Print it, but with hedging language in the message.
    
    While it is already possible to compute the faulting address manually by
    disassembling the opcode dump and evaluating the instruction against the
    register dump, this should make it slightly easier to identify crashes
    at a glance.
    
    Note that the operand length which comes from the instruction decoder
    and is used to determine whether the access straddles into non-canonical
    address space, is currently somewhat unreliable; but it should be good
    enough, considering that Linux on x86-64 never maps the page directly
    before the start of the non-canonical range anyway, and therefore the
    case where a memory range begins in that page and potentially straddles
    into the non-canonical range should be fairly uncommon.
    
    In the case the address is still computed wrongly, it only influences
    whether the error message claims that the access is canonical.
    
     [ bp: Remove ambiguous "we", massage, reflow comments and spacing. ]
    
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Tested-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: kasan-dev@googlegroups.com
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20191218231150.12139-2-jannh@google.com

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 05da6b5b167b..108ab1ee069f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -56,6 +56,8 @@
 #include <asm/mpx.h>
 #include <asm/vm86.h>
 #include <asm/umip.h>
+#include <asm/insn.h>
+#include <asm/insn-eval.h>
 
 #ifdef CONFIG_X86_64
 #include <asm/x86_init.h>
@@ -518,10 +520,53 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, 0, NULL);
 }
 
-dotraplinkage void
-do_general_protection(struct pt_regs *regs, long error_code)
+enum kernel_gp_hint {
+	GP_NO_HINT,
+	GP_NON_CANONICAL,
+	GP_CANONICAL
+};
+
+/*
+ * When an uncaught #GP occurs, try to determine the memory address accessed by
+ * the instruction and return that address to the caller. Also, try to figure
+ * out whether any part of the access to that address was non-canonical.
+ */
+static enum kernel_gp_hint get_kernel_gp_address(struct pt_regs *regs,
+						 unsigned long *addr)
 {
-	const char *desc = "general protection fault";
+	u8 insn_buf[MAX_INSN_SIZE];
+	struct insn insn;
+
+	if (probe_kernel_read(insn_buf, (void *)regs->ip, MAX_INSN_SIZE))
+		return GP_NO_HINT;
+
+	kernel_insn_init(&insn, insn_buf, MAX_INSN_SIZE);
+	insn_get_modrm(&insn);
+	insn_get_sib(&insn);
+
+	*addr = (unsigned long)insn_get_addr_ref(&insn, regs);
+	if (*addr == -1UL)
+		return GP_NO_HINT;
+
+#ifdef CONFIG_X86_64
+	/*
+	 * Check that:
+	 *  - the operand is not in the kernel half
+	 *  - the last byte of the operand is not in the user canonical half
+	 */
+	if (*addr < ~__VIRTUAL_MASK &&
+	    *addr + insn.opnd_bytes - 1 > __VIRTUAL_MASK)
+		return GP_NON_CANONICAL;
+#endif
+
+	return GP_CANONICAL;
+}
+
+#define GPFSTR "general protection fault"
+
+dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
+{
+	char desc[sizeof(GPFSTR) + 50 + 2*sizeof(unsigned long) + 1] = GPFSTR;
 	struct task_struct *tsk;
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
@@ -540,6 +585,9 @@ do_general_protection(struct pt_regs *regs, long error_code)
 
 	tsk = current;
 	if (!user_mode(regs)) {
+		enum kernel_gp_hint hint = GP_NO_HINT;
+		unsigned long gp_addr;
+
 		if (fixup_exception(regs, X86_TRAP_GP, error_code, 0))
 			return;
 
@@ -556,8 +604,22 @@ do_general_protection(struct pt_regs *regs, long error_code)
 			return;
 
 		if (notify_die(DIE_GPF, desc, regs, error_code,
-			       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)
-			die(desc, regs, error_code);
+			       X86_TRAP_GP, SIGSEGV) == NOTIFY_STOP)
+			return;
+
+		if (error_code)
+			snprintf(desc, sizeof(desc), "segment-related " GPFSTR);
+		else
+			hint = get_kernel_gp_address(regs, &gp_addr);
+
+		if (hint != GP_NO_HINT)
+			snprintf(desc, sizeof(desc), GPFSTR ", %s 0x%lx",
+				 (hint == GP_NON_CANONICAL) ?
+				 "probably for non-canonical address" :
+				 "maybe for address",
+				 gp_addr);
+
+		die(desc, regs, error_code);
 		return;
 	}
 

commit 2040cf9f59037aa8aec749363e69ead165b67b43
Merge: f66c0447cca1 e42617b825f8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Dec 10 10:11:00 2019 +0100

    Merge tag 'v5.5-rc1' into core/kprobes, to resolve conflicts
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 768ae4406a5cab7e8702550f2446dbeb377b798d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Aug 26 14:48:23 2019 +0200

    x86/ftrace: Use text_poke()
    
    Move ftrace over to using the generic x86 text_poke functions; this
    avoids having a second/different copy of that code around.
    
    This also avoids ftrace violating the (new) W^X rule and avoids
    fragmenting the kernel text page-tables, due to no longer having to
    toggle them RW.
    
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Tested-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel Bristot de Oliveira <bristot@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20191111132457.761255803@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index c90312146da0..3451a004e162 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -563,15 +563,6 @@ NOKPROBE_SYMBOL(do_general_protection);
 
 dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
-#ifdef CONFIG_DYNAMIC_FTRACE
-	/*
-	 * ftrace must be first, everything else may cause a recursive crash.
-	 * See note by declaration of modifying_ftrace_code in ftrace.c
-	 */
-	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
-	    ftrace_int3_handler(regs))
-		return;
-#endif
 	if (poke_int3_handler(regs))
 		return;
 

commit 0337b7ebfcb8efb4ea0a9f2b2f284217a1c0e62d
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Nov 25 22:37:44 2019 -0800

    x86/traps: die() instead of panicking on a double fault
    
    A double fault has a decent chance of being recoverable by killing
    the offending thread.  Use die() so that we at least try to recover.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a9b16c3a933d..05da6b5b167b 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -427,7 +427,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 #endif
 
 	pr_emerg("PANIC: double fault, error_code: 0x%lx\n", error_code);
-	show_regs(regs);
+	die("double fault", regs, error_code);
 	panic("Machine halted.");
 }
 #endif

commit 7d8d8cfdee9a7bd6f9682f253fa98efdd8048a9e
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Nov 20 23:06:41 2019 -0800

    x86/doublefault/32: Rewrite the x86_32 #DF handler and unify with 64-bit
    
    The old x86_32 doublefault_fn() was old and crufty, and it did not
    even try to recover.  do_double_fault() is much nicer.  Rewrite the
    32-bit double fault code to sanitize CPU state and call
    do_double_fault().  This is mostly an exercise i386 archaeology.
    
    With this patch applied, 32-bit double faults get a real stack trace,
    just like 64-bit double faults.
    
    [ mingo: merged the patch to a later kernel base. ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 76381b04dc93..a9b16c3a933d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -306,8 +306,23 @@ __visible void __noreturn handle_stack_overflow(const char *message,
 }
 #endif
 
-#ifdef CONFIG_X86_64
-/* Runs on IST stack */
+#if defined(CONFIG_X86_64) || defined(CONFIG_DOUBLEFAULT)
+/*
+ * Runs on an IST stack for x86_64 and on a special task stack for x86_32.
+ *
+ * On x86_64, this is more or less a normal kernel entry.  Notwithstanding the
+ * SDM's warnings about double faults being unrecoverable, returning works as
+ * expected.  Presumably what the SDM actually means is that the CPU may get
+ * the register state wrong on entry, so returning could be a bad idea.
+ *
+ * Various CPU engineers have promised that double faults due to an IRET fault
+ * while the stack is read-only are, in fact, recoverable.
+ *
+ * On x86_32, this is entered through a task gate, and regs are synthesized
+ * from the TSS.  Returning is, in principle, okay, but changes to regs will
+ * be lost.  If, for some reason, we need to return to a context with modified
+ * regs, the shim code could be adjusted to synchronize the registers.
+ */
 dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsigned long cr2)
 {
 	static const char str[] = "double fault";

commit 93efbde2c331004d8053f04b4bf0ca3e630b474a
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Nov 20 22:12:38 2019 -0800

    x86/traps: Disentangle the 32-bit and 64-bit doublefault code
    
    The 64-bit doublefault handler is much nicer than the 32-bit one.
    As a first step toward unifying them, make the 64-bit handler
    self-contained.  This should have no effect no functional effect
    except in the odd case of x86_64 with CONFIG_DOUBLEFAULT=n in which
    case it will change the logging a bit.
    
    This also gets rid of CONFIG_DOUBLEFAULT configurability on 64-bit
    kernels.  It didn't do anything useful -- CONFIG_DOUBLEFAULT=n
    didn't actually disable doublefault handling on x86_64.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index c90312146da0..76381b04dc93 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -411,15 +411,9 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 		handle_stack_overflow("kernel stack overflow (double-fault)", regs, cr2);
 #endif
 
-#ifdef CONFIG_DOUBLEFAULT
-	df_debug(regs, error_code);
-#endif
-	/*
-	 * This is always a kernel trap and never fixable (and thus must
-	 * never return).
-	 */
-	for (;;)
-		die(str, regs, error_code);
+	pr_emerg("PANIC: double fault, error_code: 0x%lx\n", error_code);
+	show_regs(regs);
+	panic("Machine halted.");
 }
 #endif
 

commit abaebe11dd07adf51b53bb7c56ba35eab267654d
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Aug 16 09:14:04 2018 +0200

    x86/nmi: Remove stale EDAC include leftover
    
    db47d5f85646 ("x86/nmi, EDAC: Get rid of DRAM error reporting thru PCI SERR NMI")
    
    forgot to remove it. Drop it.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190923193807.30896-1-bp@alien8.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 4bb0f8447112..c90312146da0 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -37,11 +37,6 @@
 #include <linux/mm.h>
 #include <linux/smp.h>
 #include <linux/io.h>
-
-#if defined(CONFIG_EDAC)
-#include <linux/edac.h>
-#endif
-
 #include <asm/stacktrace.h>
 #include <asm/processor.h>
 #include <asm/debugreg.h>

commit a0d14b8909de55139b8702fe0c7e80b69763dcfb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jul 11 13:40:59 2019 +0200

    x86/mm, tracing: Fix CR2 corruption
    
    Despite the current efforts to read CR2 before tracing happens there still
    exist a number of possible holes:
    
      idtentry page_fault             do_page_fault           has_error_code=1
        call error_entry
          TRACE_IRQS_OFF
            call trace_hardirqs_off*
              #PF // modifies CR2
    
          CALL_enter_from_user_mode
            __context_tracking_exit()
              trace_user_exit(0)
                #PF // modifies CR2
    
        call do_page_fault
          address = read_cr2(); /* whoopsie */
    
    And similar for i386.
    
    Fix it by pulling the CR2 read into the entry code, before any of that
    stuff gets a chance to run and ruin things.
    
    Reported-by: He Zhe <zhe.he@windriver.com>
    Reported-by: Eiichi Tsukata <devel@etsukata.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: bp@alien8.de
    Cc: rostedt@goodmis.org
    Cc: torvalds@linux-foundation.org
    Cc: hpa@zytor.com
    Cc: dave.hansen@linux.intel.com
    Cc: jgross@suse.com
    Cc: joel@joelfernandes.org
    Link: https://lkml.kernel.org/r/20190711114336.116812491@infradead.org
    
    Debugged-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 87095a477154..4bb0f8447112 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -313,13 +313,10 @@ __visible void __noreturn handle_stack_overflow(const char *message,
 
 #ifdef CONFIG_X86_64
 /* Runs on IST stack */
-dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
+dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsigned long cr2)
 {
 	static const char str[] = "double fault";
 	struct task_struct *tsk = current;
-#ifdef CONFIG_VMAP_STACK
-	unsigned long cr2;
-#endif
 
 #ifdef CONFIG_X86_ESPFIX64
 	extern unsigned char native_irq_return_iret[];
@@ -415,7 +412,6 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	 * stack even if the actual trigger for the double fault was
 	 * something else.
 	 */
-	cr2 = read_cr2();
 	if ((unsigned long)task_stack_page(tsk) - 1 - cr2 < PAGE_SIZE)
 		handle_stack_overflow("kernel stack overflow (double-fault)", regs, cr2);
 #endif

commit 2e1661d2673667d886cd40ad9f414cb6db48d8da
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 23 11:04:24 2019 -0500

    signal: Remove the task parameter from force_sig_fault
    
    As synchronous exceptions really only make sense against the current
    task (otherwise how are you synchronous) remove the task parameter
    from from force_sig_fault to make it explicit that is what is going
    on.
    
    The two known exceptions that deliver a synchronous exception to a
    stopped ptraced task have already been changed to
    force_sig_fault_to_task.
    
    The callers have been changed with the following emacs regular expression
    (with obvious variations on the architectures that take more arguments)
    to avoid typos:
    
    force_sig_fault[(]\([^,]+\)[,]\([^,]+\)[,]\([^,]+\)[,]\W+current[)]
    ->
    force_sig_fault(\1,\2,\3)
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 945b9a0719dd..87095a477154 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -256,7 +256,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	if (!sicode)
 		force_sig(signr);
 	else
-		force_sig_fault(signr, sicode, addr, current);
+		force_sig_fault(signr, sicode, addr);
 }
 NOKPROBE_SYMBOL(do_trap);
 
@@ -856,7 +856,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 		return;
 
 	force_sig_fault(SIGFPE, si_code,
-			(void __user *)uprobe_get_trap_addr(regs), current);
+			(void __user *)uprobe_get_trap_addr(regs));
 }
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)

commit 351b6825b3a9f70bab080fba67aec104ff9a41d6
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Feb 7 09:25:08 2019 -0600

    signal: Explicitly call force_sig_fault on current
    
    Update the calls of force_sig_fault that pass in a variable that is
    set to current earlier to explicitly use current.
    
    This is to make the next change that removes the task parameter
    from force_sig_fault easier to verify.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 30a9b843ef04..945b9a0719dd 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -256,7 +256,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	if (!sicode)
 		force_sig(signr);
 	else
-		force_sig_fault(signr, sicode, addr, tsk);
+		force_sig_fault(signr, sicode, addr, current);
 }
 NOKPROBE_SYMBOL(do_trap);
 
@@ -856,7 +856,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 		return;
 
 	force_sig_fault(SIGFPE, si_code,
-			(void __user *)uprobe_get_trap_addr(regs), task);
+			(void __user *)uprobe_get_trap_addr(regs), current);
 }
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)

commit 28d42ea14e489047caeaa89496a3ad7e0ae6a49f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Feb 5 18:25:11 2019 -0600

    signal/x86: Remove task parameter from send_sigtrap
    
    The send_sigtrap function is always called with task == current.  Make
    that explicit by removing the task parameter.
    
    This also makes it clear that the x86 send_sigtrap passes current
    into force_sig_fault.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index e54f0cad4b2e..30a9b843ef04 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -805,7 +805,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	}
 	si_code = get_si_code(tsk->thread.debugreg6);
 	if (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)
-		send_sigtrap(tsk, regs, error_code, si_code);
+		send_sigtrap(regs, error_code, si_code);
 	cond_local_irq_disable(regs);
 	debug_stack_usage_dec();
 

commit 3cf5d076fb4d48979f382bc9452765bf8b79e740
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 23 10:17:27 2019 -0500

    signal: Remove task parameter from force_sig
    
    All of the remaining callers pass current into force_sig so
    remove the task parameter to make this obvious and to make
    misuse more difficult in the future.
    
    This also makes it clear force_sig passes current into force_sig_info.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 8b6d03e55d2f..e54f0cad4b2e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -254,7 +254,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	show_signal(tsk, signr, "trap ", str, regs, error_code);
 
 	if (!sicode)
-		force_sig(signr, tsk);
+		force_sig(signr);
 	else
 		force_sig_fault(signr, sicode, addr, tsk);
 }
@@ -566,7 +566,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 
 	show_signal(tsk, SIGSEGV, "", desc, regs, error_code);
 
-	force_sig(SIGSEGV, tsk);
+	force_sig(SIGSEGV);
 }
 NOKPROBE_SYMBOL(do_general_protection);
 

commit 88640e1dcd089879530a49a8d212d1814678dfe7
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue May 14 13:24:39 2019 -0700

    x86/speculation/mds: Revert CPU buffer clear on double fault exit
    
    The double fault ESPFIX path doesn't return to user mode at all --
    it returns back to the kernel by simulating a #GP fault.
    prepare_exit_to_usermode() will run on the way out of
    general_protection before running user code.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Frederic Weisbecker <frederic@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Fixes: 04dcbdb80578 ("x86/speculation/mds: Clear CPU buffers on exit to user")
    Link: http://lkml.kernel.org/r/ac97612445c0a44ee10374f6ea79c222fe22a5c4.1557865329.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7de466eb960b..8b6d03e55d2f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -58,7 +58,6 @@
 #include <asm/alternative.h>
 #include <asm/fpu/xstate.h>
 #include <asm/trace/mpx.h>
-#include <asm/nospec-branch.h>
 #include <asm/mpx.h>
 #include <asm/vm86.h>
 #include <asm/umip.h>
@@ -368,13 +367,6 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 		regs->ip = (unsigned long)general_protection;
 		regs->sp = (unsigned long)&gpregs->orig_ax;
 
-		/*
-		 * This situation can be triggered by userspace via
-		 * modify_ldt(2) and the return does not take the regular
-		 * user space exit, so a CPU buffer clear is required when
-		 * MDS mitigation is enabled.
-		 */
-		mds_user_clear_cpu_buffers();
 		return;
 	}
 #endif

commit fa4bff165070dc40a3de35b78e4f8da8e8d85ec5
Merge: 63863ee8e2f6 95310e348a32
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 14 07:57:29 2019 -0700

    Merge branch 'x86-mds-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 MDS mitigations from Thomas Gleixner:
     "Microarchitectural Data Sampling (MDS) is a hardware vulnerability
      which allows unprivileged speculative access to data which is
      available in various CPU internal buffers. This new set of misfeatures
      has the following CVEs assigned:
    
         CVE-2018-12126  MSBDS  Microarchitectural Store Buffer Data Sampling
         CVE-2018-12130  MFBDS  Microarchitectural Fill Buffer Data Sampling
         CVE-2018-12127  MLPDS  Microarchitectural Load Port Data Sampling
         CVE-2019-11091  MDSUM  Microarchitectural Data Sampling Uncacheable Memory
    
      MDS attacks target microarchitectural buffers which speculatively
      forward data under certain conditions. Disclosure gadgets can expose
      this data via cache side channels.
    
      Contrary to other speculation based vulnerabilities the MDS
      vulnerability does not allow the attacker to control the memory target
      address. As a consequence the attacks are purely sampling based, but
      as demonstrated with the TLBleed attack samples can be postprocessed
      successfully.
    
      The mitigation is to flush the microarchitectural buffers on return to
      user space and before entering a VM. It's bolted on the VERW
      instruction and requires a microcode update. As some of the attacks
      exploit data structures shared between hyperthreads, full protection
      requires to disable hyperthreading. The kernel does not do that by
      default to avoid breaking unattended updates.
    
      The mitigation set comes with documentation for administrators and a
      deeper technical view"
    
    * 'x86-mds-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      x86/speculation/mds: Fix documentation typo
      Documentation: Correct the possible MDS sysfs values
      x86/mds: Add MDSUM variant to the MDS documentation
      x86/speculation/mds: Add 'mitigations=' support for MDS
      x86/speculation/mds: Print SMT vulnerable on MSBDS with mitigations off
      x86/speculation/mds: Fix comment
      x86/speculation/mds: Add SMT warning message
      x86/speculation: Move arch_smt_update() call to after mitigation decisions
      x86/speculation/mds: Add mds=full,nosmt cmdline option
      Documentation: Add MDS vulnerability documentation
      Documentation: Move L1TF to separate directory
      x86/speculation/mds: Add mitigation mode VMWERV
      x86/speculation/mds: Add sysfs reporting for MDS
      x86/speculation/mds: Add mitigation control for MDS
      x86/speculation/mds: Conditionally clear CPU buffers on idle entry
      x86/kvm/vmx: Add MDS protection when L1D Flush is not active
      x86/speculation/mds: Clear CPU buffers on exit to user
      x86/speculation/mds: Add mds_clear_cpu_buffers()
      x86/kvm: Expose X86_FEATURE_MD_CLEAR to guests
      x86/speculation/mds: Add BUG_MSBDS_ONLY
      ...

commit abd16d68d65229e5acafdadc32704239131bf2ea
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 3 18:41:40 2019 +0200

    x86/fpu: Use a feature number instead of mask in two more helpers
    
    After changing the argument of __raw_xsave_addr() from a mask to
    number Dave suggested to check if it makes sense to do the same for
    get_xsave_addr(). As it turns out it does.
    
    Only get_xsave_addr() needs the mask to check if the requested feature
    is part of what is supported/saved and then uses the number again. The
    shift operation is cheaper compared to fls64() (find last bit set).
    Also, the feature number uses less opcode space compared to the mask. :)
    
    Make the get_xsave_addr() argument a xfeature number instead of a mask
    and fix up its callers.
    
    Furthermore, use xfeature_nr and xfeature_mask consistently.
    
    This results in the following changes to the kvm code:
    
      feature -> xfeature_mask
      index -> xfeature_nr
    
    Suggested-by: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Siarhei Liakh <Siarhei.Liakh@concurrent-rt.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-12-bigeasy@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index d26f9e9c3d83..8b6d03e55d2f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -456,7 +456,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	 * which is all zeros which indicates MPX was not
 	 * responsible for the exception.
 	 */
-	bndcsr = get_xsave_field_ptr(XFEATURE_MASK_BNDCSR);
+	bndcsr = get_xsave_field_ptr(XFEATURE_BNDCSR);
 	if (!bndcsr)
 		goto exit_trap;
 

commit 35a738fb5fd0fdd1cc7e749e3a45f1876ecd1db8
Merge: bcd49c3dd172 2f7726f95557
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 7 17:09:28 2019 -0800

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fpu updates from Ingo Molnar:
     "Three changes:
    
       - preparatory patch for AVX state tracking that computing-cluster
         folks would like to use for user-space batching - but we are not
         happy about the related ABI yet so this is only the kernel tracking
         side
    
       - a cleanup for CR0 handling in do_device_not_available()
    
       - plus we removed a workaround for an ancient binutils version"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fpu: Track AVX-512 usage of tasks
      x86/fpu: Get rid of CONFIG_AS_FXSAVEQ
      x86/traps: Have read_cr0() only once in the #NM handler

commit 04dcbdb8057827b043b3c71aa397c4c63e67d086
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 18 23:42:51 2019 +0100

    x86/speculation/mds: Clear CPU buffers on exit to user
    
    Add a static key which controls the invocation of the CPU buffer clear
    mechanism on exit to user space and add the call into
    prepare_exit_to_usermode() and do_nmi() right before actually returning.
    
    Add documentation which kernel to user space transition this covers and
    explain why some corner cases are not mitigated.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Jon Masters <jcm@redhat.com>
    Tested-by: Jon Masters <jcm@redhat.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 9b7c4ca8f0a7..85fe1870f873 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -58,6 +58,7 @@
 #include <asm/alternative.h>
 #include <asm/fpu/xstate.h>
 #include <asm/trace/mpx.h>
+#include <asm/nospec-branch.h>
 #include <asm/mpx.h>
 #include <asm/vm86.h>
 #include <asm/umip.h>
@@ -366,6 +367,13 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 		regs->ip = (unsigned long)general_protection;
 		regs->sp = (unsigned long)&gpregs->orig_ax;
 
+		/*
+		 * This situation can be triggered by userspace via
+		 * modify_ldt(2) and the return does not take the regular
+		 * user space exit, so a CPU buffer clear is required when
+		 * MDS mitigation is enabled.
+		 */
+		mds_user_clear_cpu_buffers();
 		return;
 	}
 #endif

commit c13324a505c7790fe91a9df35be2e0462abccdb0
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Wed Feb 13 01:12:15 2019 +0900

    x86/kprobes: Prohibit probing on functions before kprobe_int3_handler()
    
    Prohibit probing on the functions called before kprobe_int3_handler()
    in do_int3(). More specifically, ftrace_int3_handler(),
    poke_int3_handler(), and ist_enter(). And since rcu_nmi_enter() is
    called by ist_enter(), it also should be marked as NOKPROBE_SYMBOL.
    
    Since those are handled before kprobe_int3_handler(), probing those
    functions can cause a breakpoint recursion and crash the kernel.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andrea Righi <righi.andrea@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/154998793571.31052.11301258949601150994.stgit@devbox
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 9b7c4ca8f0a7..e289ce1332ab 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -111,6 +111,7 @@ void ist_enter(struct pt_regs *regs)
 	/* This code is a bit fragile.  Test it. */
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "ist_enter didn't work");
 }
+NOKPROBE_SYMBOL(ist_enter);
 
 void ist_exit(struct pt_regs *regs)
 {

commit ee35b9b9f6d52ba134b4e75442531935f295be7a
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Jan 17 13:02:05 2019 +0100

    x86/traps: Have read_cr0() only once in the #NM handler
    
    ... instead of twice in the code. In any case, CR0 ends up being read
    once anyway:
    
    1. The CONFIG_MATH_EMULATION case does so and exits.
    2. The normal case does it once too.
    
    However, read it on function entry instead to make the code even simpler
    to follow.
    
    No functional changes.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: x86@kernel.org
    Link: https://lkml.kernel.org/r/20190117120728.3811-1-bp@alien8.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 9b7c4ca8f0a7..2684a9d11e66 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -880,12 +880,12 @@ do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
-	unsigned long cr0;
+	unsigned long cr0 = read_cr0();
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 
 #ifdef CONFIG_MATH_EMULATION
-	if (!boot_cpu_has(X86_FEATURE_FPU) && (read_cr0() & X86_CR0_EM)) {
+	if (!boot_cpu_has(X86_FEATURE_FPU) && (cr0 & X86_CR0_EM)) {
 		struct math_emu_info info = { };
 
 		cond_local_irq_enable(regs);
@@ -897,7 +897,6 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 #endif
 
 	/* This should not happen. */
-	cr0 = read_cr0();
 	if (WARN(cr0 & X86_CR0_TS, "CR0.TS was set")) {
 		/* Try to fix it up and carry on. */
 		write_cr0(cr0 & ~X86_CR0_TS);

commit 2022cceb4e30f1bb4c84d40ffa705aa8d8d68adb
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Sat Oct 27 00:20:04 2018 +0200

    x86/traps: Use format string with panic() call
    
    Building with -Wformat-nonliteral gives:
    
      arch/x86/kernel/traps.c:334:2: warning: format not a string literal and no format arguments [-Wformat-nonliteral]
        panic(message);
    
    handle_stack_overflow() can only be called from two places (kernel/traps.c
    and via inline asm in mm/fault.c), in both cases with a string not
    containing format specifiers, so we might as well silence this warning
    using "%s" as a format string.
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20181026222004.14193-1-linux@rasmusvillemoes.dk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 8f6dcd88202e..9b7c4ca8f0a7 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -306,7 +306,7 @@ __visible void __noreturn handle_stack_overflow(const char *message,
 	die(message, regs, 0);
 
 	/* Be absolutely certain we don't return. */
-	panic(message);
+	panic("%s", message);
 }
 #endif
 

commit ba9f6f8954afa5224e3ed60332f7b92242b7ed0f
Merge: a978a5b8d83f a36700589b85
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 24 11:22:39 2018 +0100

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull siginfo updates from Eric Biederman:
     "I have been slowly sorting out siginfo and this is the culmination of
      that work.
    
      The primary result is in several ways the signal infrastructure has
      been made less error prone. The code has been updated so that manually
      specifying SEND_SIG_FORCED is never necessary. The conversion to the
      new siginfo sending functions is now complete, which makes it
      difficult to send a signal without filling in the proper siginfo
      fields.
    
      At the tail end of the patchset comes the optimization of decreasing
      the size of struct siginfo in the kernel from 128 bytes to about 48
      bytes on 64bit. The fundamental observation that enables this is by
      definition none of the known ways to use struct siginfo uses the extra
      bytes.
    
      This comes at the cost of a small user space observable difference.
      For the rare case of siginfo being injected into the kernel only what
      can be copied into kernel_siginfo is delivered to the destination, the
      rest of the bytes are set to 0. For cases where the signal and the
      si_code are known this is safe, because we know those bytes are not
      used. For cases where the signal and si_code combination is unknown
      the bits that won't fit into struct kernel_siginfo are tested to
      verify they are zero, and the send fails if they are not.
    
      I made an extensive search through userspace code and I could not find
      anything that would break because of the above change. If it turns out
      I did break something it will take just the revert of a single change
      to restore kernel_siginfo to the same size as userspace siginfo.
    
      Testing did reveal dependencies on preferring the signo passed to
      sigqueueinfo over si->signo, so bit the bullet and added the
      complexity necessary to handle that case.
    
      Testing also revealed bad things can happen if a negative signal
      number is passed into the system calls. Something no sane application
      will do but something a malicious program or a fuzzer might do. So I
      have fixed the code that performs the bounds checks to ensure negative
      signal numbers are handled"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (80 commits)
      signal: Guard against negative signal numbers in copy_siginfo_from_user32
      signal: Guard against negative signal numbers in copy_siginfo_from_user
      signal: In sigqueueinfo prefer sig not si_signo
      signal: Use a smaller struct siginfo in the kernel
      signal: Distinguish between kernel_siginfo and siginfo
      signal: Introduce copy_siginfo_from_user and use it's return value
      signal: Remove the need for __ARCH_SI_PREABLE_SIZE and SI_PAD_SIZE
      signal: Fail sigqueueinfo if si_signo != sig
      signal/sparc: Move EMT_TAGOVF into the generic siginfo.h
      signal/unicore32: Use force_sig_fault where appropriate
      signal/unicore32: Generate siginfo in ucs32_notify_die
      signal/unicore32: Use send_sig_fault where appropriate
      signal/arc: Use force_sig_fault where appropriate
      signal/arc: Push siginfo generation into unhandled_exception
      signal/ia64: Use force_sig_fault where appropriate
      signal/ia64: Use the force_sig(SIGSEGV,...) in ia64_rt_sigreturn
      signal/ia64: Use the generic force_sigsegv in setup_frame
      signal/arm/kvm: Use send_sig_mceerr
      signal/arm: Use send_sig_fault where appropriate
      signal/arm: Use force_sig_fault where appropriate
      ...

commit d82924c3b8d0607094b94fab290a33c5ad7d586c
Merge: d7197a5ad852 bb4b3b776273
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 18:43:04 2018 +0100

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 pti updates from Ingo Molnar:
     "The main changes:
    
       - Make the IBPB barrier more strict and add STIBP support (Jiri
         Kosina)
    
       - Micro-optimize and clean up the entry code (Andy Lutomirski)
    
       - ... plus misc other fixes"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/speculation: Propagate information about RSB filling mitigation to sysfs
      x86/speculation: Enable cross-hyperthread spectre v2 STIBP mitigation
      x86/speculation: Apply IBPB more strictly to avoid cross-process data leak
      x86/speculation: Add RETPOLINE_AMD support to the inline asm CALL_NOSPEC variant
      x86/CPU: Fix unused variable warning when !CONFIG_IA32_EMULATION
      x86/pti/64: Remove the SYSCALL64 entry trampoline
      x86/entry/64: Use the TSS sp2 slot for SYSCALL/SYSRET scratch space
      x86/entry/64: Document idtentry

commit 0a996c1a3f260ba6fe1201a668d31fa8d8339793
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Sep 18 01:16:39 2018 +0200

    signal/x86: Use force_sig_fault where appropriate
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ce9a4112897a..34a327f35976 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -808,7 +808,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 {
 	struct task_struct *task = current;
 	struct fpu *fpu = &task->thread.fpu;
-	siginfo_t info;
+	int si_code;
 	char *str = (trapnr == X86_TRAP_MF) ? "fpu exception" :
 						"simd exception";
 
@@ -834,18 +834,14 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 
 	task->thread.trap_nr	= trapnr;
 	task->thread.error_code = error_code;
-	clear_siginfo(&info);
-	info.si_signo		= SIGFPE;
-	info.si_errno		= 0;
-	info.si_addr		= (void __user *)uprobe_get_trap_addr(regs);
-
-	info.si_code = fpu__exception_code(fpu, trapnr);
 
+	si_code = fpu__exception_code(fpu, trapnr);
 	/* Retry when we get spurious exceptions: */
-	if (!info.si_code)
+	if (!si_code)
 		return;
 
-	force_sig_info(SIGFPE, &info, task);
+	force_sig_fault(SIGFPE, si_code,
+			(void __user *)uprobe_get_trap_addr(regs), task);
 }
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)

commit 164881b61429d38717408fa5d175cb55a6291969
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Apr 16 14:29:39 2018 -0500

    signal/x86/traps: Simplify trap generation
    
    Update the DO_ERROR macro to take si_code and si_addr values for a siginfo,
    removing the need for the fill_trap_info function.
    
    Update do_trap to also take the sicode and si_addr values for a sigininfo
    and modify the code to call force_sig when a sicode is not passed in
    and to call force_sig_fault when all of the information is present.
    
    Making this a more obvious, simpler and less error prone construction.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1fa69bf27f4f..ce9a4112897a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -240,40 +240,9 @@ static void show_signal(struct task_struct *tsk, int signr,
 	}
 }
 
-static siginfo_t *fill_trap_info(struct pt_regs *regs, int signr, int trapnr,
-				siginfo_t *info)
-{
-	unsigned long siaddr;
-	int sicode;
-
-	switch (trapnr) {
-	default:
-		return SEND_SIG_PRIV;
-
-	case X86_TRAP_DE:
-		sicode = FPE_INTDIV;
-		siaddr = uprobe_get_trap_addr(regs);
-		break;
-	case X86_TRAP_UD:
-		sicode = ILL_ILLOPN;
-		siaddr = uprobe_get_trap_addr(regs);
-		break;
-	case X86_TRAP_AC:
-		sicode = BUS_ADRALN;
-		siaddr = 0;
-		break;
-	}
-
-	info->si_signo = signr;
-	info->si_errno = 0;
-	info->si_code = sicode;
-	info->si_addr = (void __user *)siaddr;
-	return info;
-}
-
 static void
 do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
-	long error_code, siginfo_t *info)
+	long error_code, int sicode, void __user *addr)
 {
 	struct task_struct *tsk = current;
 
@@ -283,15 +252,16 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 
 	show_signal(tsk, signr, "trap ", str, regs, error_code);
 
-	force_sig_info(signr, info ?: SEND_SIG_PRIV, tsk);
+	if (!sicode)
+		force_sig(signr, tsk);
+	else
+		force_sig_fault(signr, sicode, addr, tsk);
 }
 NOKPROBE_SYMBOL(do_trap);
 
 static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
-			  unsigned long trapnr, int signr)
+	unsigned long trapnr, int signr, int sicode, void __user *addr)
 {
-	siginfo_t info;
-
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 
 	/*
@@ -304,26 +274,26 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
 			NOTIFY_STOP) {
 		cond_local_irq_enable(regs);
-		clear_siginfo(&info);
-		do_trap(trapnr, signr, str, regs, error_code,
-			fill_trap_info(regs, signr, trapnr, &info));
+		do_trap(trapnr, signr, str, regs, error_code, sicode, addr);
 	}
 }
 
-#define DO_ERROR(trapnr, signr, str, name)				\
-dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
-{									\
-	do_error_trap(regs, error_code, str, trapnr, signr);		\
+#define IP ((void __user *)uprobe_get_trap_addr(regs))
+#define DO_ERROR(trapnr, signr, sicode, addr, str, name)		   \
+dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	   \
+{									   \
+	do_error_trap(regs, error_code, str, trapnr, signr, sicode, addr); \
 }
 
-DO_ERROR(X86_TRAP_DE,     SIGFPE,  "divide error",		divide_error)
-DO_ERROR(X86_TRAP_OF,     SIGSEGV, "overflow",			overflow)
-DO_ERROR(X86_TRAP_UD,     SIGILL,  "invalid opcode",		invalid_op)
-DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  "coprocessor segment overrun",coprocessor_segment_overrun)
-DO_ERROR(X86_TRAP_TS,     SIGSEGV, "invalid TSS",		invalid_TSS)
-DO_ERROR(X86_TRAP_NP,     SIGBUS,  "segment not present",	segment_not_present)
-DO_ERROR(X86_TRAP_SS,     SIGBUS,  "stack segment",		stack_segment)
-DO_ERROR(X86_TRAP_AC,     SIGBUS,  "alignment check",		alignment_check)
+DO_ERROR(X86_TRAP_DE,     SIGFPE,  FPE_INTDIV,   IP, "divide error",        divide_error)
+DO_ERROR(X86_TRAP_OF,     SIGSEGV,          0, NULL, "overflow",            overflow)
+DO_ERROR(X86_TRAP_UD,     SIGILL,  ILL_ILLOPN,   IP, "invalid opcode",      invalid_op)
+DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,           0, NULL, "coprocessor segment overrun", coprocessor_segment_overrun)
+DO_ERROR(X86_TRAP_TS,     SIGSEGV,          0, NULL, "invalid TSS",         invalid_TSS)
+DO_ERROR(X86_TRAP_NP,     SIGBUS,           0, NULL, "segment not present", segment_not_present)
+DO_ERROR(X86_TRAP_SS,     SIGBUS,           0, NULL, "stack segment",       stack_segment)
+DO_ERROR(X86_TRAP_AC,     SIGBUS,  BUS_ADRALN, NULL, "alignment check",     alignment_check)
+#undef IP
 
 #ifdef CONFIG_VMAP_STACK
 __visible void __noreturn handle_stack_overflow(const char *message,
@@ -540,7 +510,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	 * up here if the kernel has MPX turned off at compile
 	 * time..
 	 */
-	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, NULL);
+	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, 0, NULL);
 }
 
 dotraplinkage void
@@ -624,7 +594,7 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 		goto exit;
 
 	cond_local_irq_enable(regs);
-	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, NULL);
+	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, 0, NULL);
 	cond_local_irq_disable(regs);
 
 exit:
@@ -935,20 +905,13 @@ NOKPROBE_SYMBOL(do_device_not_available);
 #ifdef CONFIG_X86_32
 dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 {
-	siginfo_t info;
-
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	local_irq_enable();
 
-	clear_siginfo(&info);
-	info.si_signo = SIGILL;
-	info.si_errno = 0;
-	info.si_code = ILL_BADSTK;
-	info.si_addr = NULL;
 	if (notify_die(DIE_TRAP, "iret exception", regs, error_code,
 			X86_TRAP_IRET, SIGILL) != NOTIFY_STOP) {
 		do_trap(X86_TRAP_IRET, SIGILL, "iret exception", regs, error_code,
-			&info);
+			ILL_BADSTK, (void __user *)NULL);
 	}
 }
 #endif

commit afe8448c0d9db1e1ffaa4a130a7df5f5e63b7188
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Aug 8 10:55:19 2017 -0500

    signal/x86/traps: Use force_sig instead of open coding it.
    
    The function "force_sig(sig, tsk)" is equivalent to "
    force_sig_info(sig, SEND_SIG_PRIV, tsk)".  Using the siginfo variants can
    be error prone so use the simpler old fashioned force_sig variant,
    and with luck the force_sig_info variant can go away.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 2d44e63cb583..1fa69bf27f4f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -581,7 +581,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 
 	show_signal(tsk, SIGSEGV, "", desc, regs, error_code);
 
-	force_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);
+	force_sig(SIGSEGV, tsk);
 }
 NOKPROBE_SYMBOL(do_general_protection);
 

commit 851ce9e697b8a432ad1847a0c2d464b412353829
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Aug 19 12:14:49 2017 -0500

    signal/x86/traps: Use force_sig_bnderr
    
    Instead of generating the siginfo in x86 specific code use the new
    helper function force_sig_bnderr to separate the concerns of
    collecting the information and generating a proper siginfo.
    
    Making the code easier to understand and maintain.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 23ac69a61a95..2d44e63cb583 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -498,8 +498,8 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 		break; /* Success, it was handled */
 	case 1: /* Bound violation. */
 	{
+		struct task_struct *tsk = current;
 		struct mpx_fault_info mpx;
-		struct siginfo info;
 
 		if (mpx_fault_info(&mpx, regs)) {
 			/*
@@ -510,19 +510,18 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 		}
 		/*
 		 * Success, we decoded the instruction and retrieved
-		 * an 'info' containing the address being accessed
+		 * an 'mpx' containing the address being accessed
 		 * which caused the exception.  This information
 		 * allows and application to possibly handle the
 		 * #BR exception itself.
 		 */
-		clear_siginfo(&info);
-		info.si_signo = SIGSEGV;
-		info.si_errno = 0;
-		info.si_code  = SEGV_BNDERR;
-		info.si_addr  = mpx.addr;
-		info.si_lower = mpx.lower;
-		info.si_upper = mpx.upper;
-		do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, &info);
+		if (!do_trap_no_signal(tsk, X86_TRAP_BR, "bounds", regs,
+				       error_code))
+			break;
+
+		show_signal(tsk, SIGSEGV, "trap ", "bounds", regs, error_code);
+
+		force_sig_bnderr(mpx.addr, mpx.lower, mpx.upper);
 		break;
 	}
 	case 0: /* No exception caused by Intel MPX operations. */

commit 79e21d65409385a57d2c77aeb95825a28a8058c7
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Aug 4 14:01:50 2017 -0500

    signal/x86/traps: Move more code into do_trap_no_signal so it can be reused
    
    The function do_trap_no_signal embodies almost all of the work of the
    function do_trap.  The exceptions are setting of thread.error_code and
    thread.trap_nr in the case when the signal will be sent, and reporting
    which signal will be sent with show_signal.
    
    Filling in struct siginfo and then calling do_trap is problematic as
    filling in struct siginfo is an fiddly process that can through
    inattention has resulted in fields not initialized and the wrong
    fields being filled in.
    
    To avoid this error prone situation I am replacing force_sig_info with
    a set of functions that take as arguments the information needed to
    send a specific kind of signal.
    
    The function do_trap is called in the context of several different
    kinds of signals today.  Having a solid do_trap_no_signal that
    can be reused allows call sites that send different kinds of
    signals to reuse all of the code in do_trap_no_signal.
    
    Modify do_trap_no_signal to have a single exit there signals
    where be sent (aka returning -1) to allow more of the signal
    sending path to be moved to from do_trap to do_trap_no_signal.
    
    Move setting thread.trap_nr and thread.error_code into do_trap_no_signal
    so the code does not need to be duplicated.
    
    Make the type of the string that is passed into do_trap_no_signal to
    const.  The only user of that str is die and it already takes a const
    string, so this just makes it explicit that the string won't change.
    
    All of this prepares the way for using do_trap_no_signal outside
    of do_trap.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 31a689b67be3..23ac69a61a95 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -189,7 +189,7 @@ int fixup_bug(struct pt_regs *regs, int trapnr)
 }
 
 static nokprobe_inline int
-do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
+do_trap_no_signal(struct task_struct *tsk, int trapnr, const char *str,
 		  struct pt_regs *regs,	long error_code)
 {
 	if (v8086_mode(regs)) {
@@ -202,10 +202,7 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 						error_code, trapnr))
 				return 0;
 		}
-		return -1;
-	}
-
-	if (!user_mode(regs)) {
+	} else if (!user_mode(regs)) {
 		if (fixup_exception(regs, trapnr))
 			return 0;
 
@@ -214,6 +211,18 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 		die(str, regs, error_code);
 	}
 
+	/*
+	 * We want error_code and trap_nr set for userspace faults and
+	 * kernelspace faults which result in die(), but not
+	 * kernelspace faults which are fixed up.  die() gives the
+	 * process no chance to handle the signal and notice the
+	 * kernel fault information, so that won't result in polluting
+	 * the information about previously queued, but not yet
+	 * delivered, faults.  See also do_general_protection below.
+	 */
+	tsk->thread.error_code = error_code;
+	tsk->thread.trap_nr = trapnr;
+
 	return -1;
 }
 
@@ -271,17 +280,6 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 
 	if (!do_trap_no_signal(tsk, trapnr, str, regs, error_code))
 		return;
-	/*
-	 * We want error_code and trap_nr set for userspace faults and
-	 * kernelspace faults which result in die(), but not
-	 * kernelspace faults which are fixed up.  die() gives the
-	 * process no chance to handle the signal and notice the
-	 * kernel fault information, so that won't result in polluting
-	 * the information about previously queued, but not yet
-	 * delivered, faults.  See also do_general_protection below.
-	 */
-	tsk->thread.error_code = error_code;
-	tsk->thread.trap_nr = trapnr;
 
 	show_signal(tsk, signr, "trap ", str, regs, error_code);
 

commit 6ace1098a68cbf4cb570f73eb650b00f9a3efc97
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Aug 3 14:34:24 2017 -0500

    signal/x86/traps: Factor out show_signal
    
    The code for conditionally printing unhanded signals is duplicated twice
    in arch/x86/kernel/traps.c.  Factor it out into it's own subroutine
    called show_signal to make the code clearer and easier to maintain.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 2155d2c7f49b..31a689b67be3 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -217,6 +217,20 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 	return -1;
 }
 
+static void show_signal(struct task_struct *tsk, int signr,
+			const char *type, const char *desc,
+			struct pt_regs *regs, long error_code)
+{
+	if (show_unhandled_signals && unhandled_signal(tsk, signr) &&
+	    printk_ratelimit()) {
+		pr_info("%s[%d] %s%s ip:%lx sp:%lx error:%lx",
+			tsk->comm, task_pid_nr(tsk), type, desc,
+			regs->ip, regs->sp, error_code);
+		print_vma_addr(KERN_CONT " in ", regs->ip);
+		pr_cont("\n");
+	}
+}
+
 static siginfo_t *fill_trap_info(struct pt_regs *regs, int signr, int trapnr,
 				siginfo_t *info)
 {
@@ -269,14 +283,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	tsk->thread.error_code = error_code;
 	tsk->thread.trap_nr = trapnr;
 
-	if (show_unhandled_signals && unhandled_signal(tsk, signr) &&
-	    printk_ratelimit()) {
-		pr_info("%s[%d] trap %s ip:%lx sp:%lx error:%lx",
-			tsk->comm, tsk->pid, str,
-			regs->ip, regs->sp, error_code);
-		print_vma_addr(KERN_CONT " in ", regs->ip);
-		pr_cont("\n");
-	}
+	show_signal(tsk, signr, "trap ", str, regs, error_code);
 
 	force_sig_info(signr, info ?: SEND_SIG_PRIV, tsk);
 }
@@ -542,6 +549,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_general_protection(struct pt_regs *regs, long error_code)
 {
+	const char *desc = "general protection fault";
 	struct task_struct *tsk;
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
@@ -565,23 +573,16 @@ do_general_protection(struct pt_regs *regs, long error_code)
 
 		tsk->thread.error_code = error_code;
 		tsk->thread.trap_nr = X86_TRAP_GP;
-		if (notify_die(DIE_GPF, "general protection fault", regs, error_code,
+		if (notify_die(DIE_GPF, desc, regs, error_code,
 			       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)
-			die("general protection fault", regs, error_code);
+			die(desc, regs, error_code);
 		return;
 	}
 
 	tsk->thread.error_code = error_code;
 	tsk->thread.trap_nr = X86_TRAP_GP;
 
-	if (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&
-			printk_ratelimit()) {
-		pr_info("%s[%d] general protection ip:%lx sp:%lx error:%lx",
-			tsk->comm, task_pid_nr(tsk),
-			regs->ip, regs->sp, error_code);
-		print_vma_addr(KERN_CONT " in ", regs->ip);
-		pr_cont("\n");
-	}
+	show_signal(tsk, SIGSEGV, "", desc, regs, error_code);
 
 	force_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);
 }

commit 8d68fa0e083995b48be6b66e04afe746d4654b0c
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Jan 3 19:22:04 2018 -0600

    signal/x86: Move mpx siginfo generation into do_bounds
    
    This separates the logic of generating the signal from the logic of
    gathering the information about the bounds violation.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index e6db475164ed..2155d2c7f49b 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -455,7 +455,6 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 {
 	const struct mpx_bndcsr *bndcsr;
-	siginfo_t *info;
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	if (notify_die(DIE_TRAP, "bounds", regs, error_code,
@@ -493,8 +492,11 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 			goto exit_trap;
 		break; /* Success, it was handled */
 	case 1: /* Bound violation. */
-		info = mpx_generate_siginfo(regs);
-		if (IS_ERR(info)) {
+	{
+		struct mpx_fault_info mpx;
+		struct siginfo info;
+
+		if (mpx_fault_info(&mpx, regs)) {
 			/*
 			 * We failed to decode the MPX instruction.  Act as if
 			 * the exception was not caused by MPX.
@@ -508,9 +510,16 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 		 * allows and application to possibly handle the
 		 * #BR exception itself.
 		 */
-		do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, info);
-		kfree(info);
+		clear_siginfo(&info);
+		info.si_signo = SIGSEGV;
+		info.si_errno = 0;
+		info.si_code  = SEGV_BNDERR;
+		info.si_addr  = mpx.addr;
+		info.si_lower = mpx.lower;
+		info.si_upper = mpx.upper;
+		do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, &info);
 		break;
+	}
 	case 0: /* No exception caused by Intel MPX operations. */
 		goto exit_trap;
 	default:

commit bd7b1f7cbf9cb35dab8e1b99145d07afc5b7a132
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Sep 3 15:59:42 2018 -0700

    x86/entry/64: Document idtentry
    
    The idtentry macro is complicated and magical.  Document what it
    does to help future readers and to allow future patches to adjust
    the code and docs at the same time.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/6e56c3ad94879e41afe345750bc28ccc0e820ea8.1536015544.git.luto@kernel.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index e6db475164ed..1a90821c0b74 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -383,6 +383,10 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 		 * we won't enable interupts or schedule before we invoke
 		 * general_protection, so nothing will clobber the stack
 		 * frame we just set up.
+		 *
+		 * We will enter general_protection with kernel GSBASE,
+		 * which is what the stub expects, given that the faulting
+		 * RIP will be the IRET instruction.
 		 */
 		regs->ip = (unsigned long)general_protection;
 		regs->sp = (unsigned long)&gpregs->orig_ax;

commit 81fd9c18444ed1199b5a6f6776a395292d4256fb
Author: Jann Horn <jannh@google.com>
Date:   Tue Aug 28 22:14:19 2018 +0200

    x86/fault: Plumb error code and fault address through to fault handlers
    
    This is preparation for looking at trap number and fault address in the
    handlers for uaccess errors. No functional change.
    
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Kees Cook <keescook@chromium.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: kernel-hardening@lists.openwall.com
    Cc: linux-kernel@vger.kernel.org
    Cc: dvyukov@google.com
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: "Naveen N. Rao" <naveen.n.rao@linux.vnet.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Borislav Petkov <bp@alien8.de>
    Link: https://lkml.kernel.org/r/20180828201421.157735-6-jannh@google.com

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index bf9ab1aaa175..16c95cb90496 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -206,7 +206,7 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 	}
 
 	if (!user_mode(regs)) {
-		if (fixup_exception(regs, trapnr))
+		if (fixup_exception(regs, trapnr, error_code, 0))
 			return 0;
 
 		tsk->thread.error_code = error_code;
@@ -551,7 +551,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 
 	tsk = current;
 	if (!user_mode(regs)) {
-		if (fixup_exception(regs, X86_TRAP_GP))
+		if (fixup_exception(regs, X86_TRAP_GP, error_code, 0))
 			return;
 
 		tsk->thread.error_code = error_code;
@@ -848,7 +848,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	cond_local_irq_enable(regs);
 
 	if (!user_mode(regs)) {
-		if (fixup_exception(regs, trapnr))
+		if (fixup_exception(regs, trapnr, error_code, 0))
 			return;
 
 		task->thread.error_code = error_code;

commit 76dee4a72849561f6ffacc357cfd0aa33336081a
Author: Jann Horn <jannh@google.com>
Date:   Tue Aug 28 22:14:16 2018 +0200

    x86/kprobes: Inline kprobe_exceptions_notify() into do_general_protection()
    
    The opaque plumbing of #GP from do_general_protection() through
    notify_die() into kprobe_exceptions_notify() makes it hard to understand
    what's going on.
    
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Kees Cook <keescook@chromium.org>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: kernel-hardening@lists.openwall.com
    Cc: dvyukov@google.com
    Cc: "Naveen N. Rao" <naveen.n.rao@linux.vnet.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Borislav Petkov <bp@alien8.de>
    Link: https://lkml.kernel.org/r/20180828201421.157735-3-jannh@google.com

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index e6db475164ed..bf9ab1aaa175 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -556,6 +556,16 @@ do_general_protection(struct pt_regs *regs, long error_code)
 
 		tsk->thread.error_code = error_code;
 		tsk->thread.trap_nr = X86_TRAP_GP;
+
+		/*
+		 * To be potentially processing a kprobe fault and to
+		 * trust the result from kprobe_running(), we have to
+		 * be non-preemptible.
+		 */
+		if (!preemptible() && kprobe_running() &&
+		    kprobe_fault_handler(regs, X86_TRAP_GP))
+			return;
+
 		if (notify_die(DIE_GPF, "general protection fault", regs, error_code,
 			       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)
 			die("general protection fault", regs, error_code);

commit 7731b8bc94e599c9a79e428f3359ff2c34b7576a
Merge: 48e315618dc4 9ffc59d57228
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jun 22 21:20:35 2018 +0200

    Merge branch 'linus' into x86/urgent
    
    Required to queue a dependent fix.

commit 3ae6295ccb7cf6d344908209701badbbbb503e40
Author: Siarhei Liakh <Siarhei.Liakh@concurrent-rt.com>
Date:   Thu Jun 14 19:36:07 2018 +0000

    x86: Call fixup_exception() before notify_die() in math_error()
    
    fpu__drop() has an explicit fwait which under some conditions can trigger a
    fixable FPU exception while in kernel. Thus, we should attempt to fixup the
    exception first, and only call notify_die() if the fixup failed just like
    in do_general_protection(). The original call sequence incorrectly triggers
    KDB entry on debug kernels under particular FPU-intensive workloads.
    
    Andy noted, that this makes the whole conditional irq enable thing even
    more inconsistent, but fixing that it outside the scope of this.
    
    Signed-off-by: Siarhei Liakh <siarhei.liakh@concurrent-rt.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Borislav  Petkov" <bpetkov@suse.de>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/DM5PR11MB201156F1CAB2592B07C79A03B17D0@DM5PR11MB2011.namprd11.prod.outlook.com

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 03f3d7695dac..162a31d80ad5 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -834,16 +834,18 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	char *str = (trapnr == X86_TRAP_MF) ? "fpu exception" :
 						"simd exception";
 
-	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)
-		return;
 	cond_local_irq_enable(regs);
 
 	if (!user_mode(regs)) {
-		if (!fixup_exception(regs, trapnr)) {
-			task->thread.error_code = error_code;
-			task->thread.trap_nr = trapnr;
+		if (fixup_exception(regs, trapnr))
+			return;
+
+		task->thread.error_code = error_code;
+		task->thread.trap_nr = trapnr;
+
+		if (notify_die(DIE_TRAP, str, regs, error_code,
+					trapnr, SIGFPE) != NOTIFY_STOP)
 			die(str, regs, error_code);
-		}
 		return;
 	}
 

commit 3eb0f5193b497083391aa05d35210d5645211eef
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Apr 17 15:26:37 2018 -0500

    signal: Ensure every siginfo we send has all bits initialized
    
    Call clear_siginfo to ensure every stack allocated siginfo is properly
    initialized before being passed to the signal sending functions.
    
    Note: It is not safe to depend on C initializers to initialize struct
    siginfo on the stack because C is allowed to skip holes when
    initializing a structure.
    
    The initialization of struct siginfo in tracehook_report_syscall_exit
    was moved from the helper user_single_step_siginfo into
    tracehook_report_syscall_exit itself, to make it clear that the local
    variable siginfo gets fully initialized.
    
    In a few cases the scope of struct siginfo has been reduced to make it
    clear that siginfo siginfo is not used on other paths in the function
    in which it is declared.
    
    Instances of using memset to initialize siginfo have been replaced
    with calls clear_siginfo for clarity.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 03f3d7695dac..a535dd64de63 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -299,6 +299,7 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
 			NOTIFY_STOP) {
 		cond_local_irq_enable(regs);
+		clear_siginfo(&info);
 		do_trap(trapnr, signr, str, regs, error_code,
 			fill_trap_info(regs, signr, trapnr, &info));
 	}
@@ -854,6 +855,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 
 	task->thread.trap_nr	= trapnr;
 	task->thread.error_code = error_code;
+	clear_siginfo(&info);
 	info.si_signo		= SIGFPE;
 	info.si_errno		= 0;
 	info.si_addr		= (void __user *)uprobe_get_trap_addr(regs);
@@ -929,6 +931,7 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	local_irq_enable();
 
+	clear_siginfo(&info);
 	info.si_signo = SIGILL;
 	info.si_errno = 0;
 	info.si_code = ILL_BADSTK;

commit d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 23 15:37:48 2015 -0700

    x86/entry/64: Don't use IST entry for #BP stack
    
    There's nothing IST-worthy about #BP/int3.  We don't allow kprobes
    in the small handful of places in the kernel that run at CPL0 with
    an invalid stack, and 32-bit kernels have used normal interrupt
    gates for #BP forever.
    
    Furthermore, we don't allow kprobes in places that have usergs while
    in kernel mode, so "paranoid" is also unnecessary.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 3d9b2308e7fa..03f3d7695dac 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -577,7 +577,6 @@ do_general_protection(struct pt_regs *regs, long error_code)
 }
 NOKPROBE_SYMBOL(do_general_protection);
 
-/* May run on IST stack. */
 dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -592,6 +591,13 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	if (poke_int3_handler(regs))
 		return;
 
+	/*
+	 * Use ist_enter despite the fact that we don't use an IST stack.
+	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
+	 * mode or even during context tracking state changes.
+	 *
+	 * This means that we can't schedule.  That's okay.
+	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
@@ -609,15 +615,10 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
-	/*
-	 * Let others (NMI) know that the debug stack is in use
-	 * as we may switch to the interrupt stack.
-	 */
-	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
-	debug_stack_usage_dec();
+
 exit:
 	ist_exit(regs);
 }

commit 3b3a371cc9bc980429baabe0a8e5f307f3d1f463
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Feb 9 13:16:59 2018 +0100

    x86/debug: Use UD2 for WARN()
    
    Since the Intel SDM added an ModR/M byte to UD0 and binutils followed
    that specification, we now cannot disassemble our kernel anymore.
    
    This now means Intel and AMD disagree on the encoding of UD0. And instead
    of playing games with additional bytes that are valid ModR/M and single
    byte instructions (0xd6 for instance), simply use UD2 for both WARN() and
    BUG().
    
    Requested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180208194406.GD25181@hirez.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 446c9ef8cfc3..3d9b2308e7fa 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -181,7 +181,7 @@ int fixup_bug(struct pt_regs *regs, int trapnr)
 		break;
 
 	case BUG_TRAP_TYPE_WARN:
-		regs->ip += LEN_UD0;
+		regs->ip += LEN_UD2;
 		return 1;
 	}
 

commit f39d7d78b70e0f39facb1e4fab77ad3df5c52a35
Merge: 52c90f2d32bf a31e58e129f7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 31 13:13:56 2017 -0800

    Merge branch 'x86/urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Thomas Gleixner:
     "A couple of fixlets for x86:
    
       - Fix the ESPFIX double fault handling for 5-level pagetables
    
       - Fix the commandline parsing for 'apic=' on 32bit systems and update
         documentation
    
       - Make zombie stack traces reliable
    
       - Fix kexec with stack canary
    
       - Fix the delivery mode for APICs which was missed when the x86
         vector management was converted to single target delivery. Caused a
         regression due to the broken hardware which ignores affinity
         settings in lowest prio delivery mode.
    
       - Unbreak modules when AMD memory encryption is enabled
    
       - Remove an unused parameter of prepare_switch_to"
    
    * 'x86/urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/apic: Switch all APICs to Fixed delivery mode
      x86/apic: Update the 'apic=' description of setting APIC driver
      x86/apic: Avoid wrong warning when parsing 'apic=' in X86-32 case
      x86-32: Fix kexec with stack canary (CONFIG_CC_STACKPROTECTOR)
      x86: Remove unused parameter of prepare_switch_to
      x86/stacktrace: Make zombie stack traces reliable
      x86/mm: Unbreak modules that use the DMA API
      x86/build: Make isoimage work on Debian
      x86/espfix/64: Fix espfix double-fault handling on 5-level systems

commit caf9a82657b313106aae8f4a35936c116a152299
Merge: 9c294ec08408 f6c4fd506cb6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 23 11:53:04 2017 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 PTI preparatory patches from Thomas Gleixner:
     "Todays Advent calendar window contains twentyfour easy to digest
      patches. The original plan was to have twenty three matching the date,
      but a late fixup made that moot.
    
       - Move the cpu_entry_area mapping out of the fixmap into a separate
         address space. That's necessary because the fixmap becomes too big
         with NRCPUS=8192 and this caused already subtle and hard to
         diagnose failures.
    
         The top most patch is fresh from today and cures a brain slip of
         that tall grumpy german greybeard, who ignored the intricacies of
         32bit wraparounds.
    
       - Limit the number of CPUs on 32bit to 64. That's insane big already,
         but at least it's small enough to prevent address space issues with
         the cpu_entry_area map, which have been observed and debugged with
         the fixmap code
    
       - A few TLB flush fixes in various places plus documentation which of
         the TLB functions should be used for what.
    
       - Rename the SYSENTER stack to CPU_ENTRY_AREA stack as it is used for
         more than sysenter now and keeping the name makes backtraces
         confusing.
    
       - Prevent LDT inheritance on exec() by moving it to arch_dup_mmap(),
         which is only invoked on fork().
    
       - Make vysycall more robust.
    
       - A few fixes and cleanups of the debug_pagetables code. Check
         PAGE_PRESENT instead of checking the PTE for 0 and a cleanup of the
         C89 initialization of the address hint array which already was out
         of sync with the index enums.
    
       - Move the ESPFIX init to a different place to prepare for PTI.
    
       - Several code moves with no functional change to make PTI
         integration simpler and header files less convoluted.
    
       - Documentation fixes and clarifications"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      x86/cpu_entry_area: Prevent wraparound in setup_cpu_entry_area_ptes() on 32bit
      init: Invoke init_espfix_bsp() from mm_init()
      x86/cpu_entry_area: Move it out of the fixmap
      x86/cpu_entry_area: Move it to a separate unit
      x86/mm: Create asm/invpcid.h
      x86/mm: Put MMU to hardware ASID translation in one place
      x86/mm: Remove hard-coded ASID limit checks
      x86/mm: Move the CR3 construction functions to tlbflush.h
      x86/mm: Add comments to clarify which TLB-flush functions are supposed to flush what
      x86/mm: Remove superfluous barriers
      x86/mm: Use __flush_tlb_one() for kernel memory
      x86/microcode: Dont abuse the TLB-flush interface
      x86/uv: Use the right TLB-flush API
      x86/entry: Rename SYSENTER_stack to CPU_ENTRY_AREA_entry_stack
      x86/doc: Remove obvious weirdnesses from the x86 MM layout documentation
      x86/mm/64: Improve the memory map documentation
      x86/ldt: Prevent LDT inheritance on exec
      x86/ldt: Rework locking
      arch, mm: Allow arch_dup_mmap() to fail
      x86/vsyscall/64: Warn and fail vsyscall emulation in NATIVE mode
      ...

commit 92a0f81d89571e3e8759366e050ee05cc545ef99
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 20 18:51:31 2017 +0100

    x86/cpu_entry_area: Move it out of the fixmap
    
    Put the cpu_entry_area into a separate P4D entry. The fixmap gets too big
    and 0-day already hit a case where the fixmap PTEs were cleared by
    cleanup_highmap().
    
    Aside of that the fixmap API is a pain as it's all backwards.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 464daed6894f..7c16fe0b60c2 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -951,8 +951,9 @@ void __init trap_init(void)
 	 * "sidt" instruction will not leak the location of the kernel, and
 	 * to defend the IDT against arbitrary memory write vulnerabilities.
 	 * It will be reloaded in cpu_init() */
-	__set_fixmap(FIX_RO_IDT, __pa_symbol(idt_table), PAGE_KERNEL_RO);
-	idt_descr.address = fix_to_virt(FIX_RO_IDT);
+	cea_set_pte(CPU_ENTRY_AREA_RO_IDT_VADDR, __pa_symbol(idt_table),
+		    PAGE_KERNEL_RO);
+	idt_descr.address = CPU_ENTRY_AREA_RO_IDT;
 
 	/*
 	 * Should be a barrier for any external CPU state:

commit ed1bbc40a0d10e0c5c74fe7bdc6298295cf40255
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 20 18:28:54 2017 +0100

    x86/cpu_entry_area: Move it to a separate unit
    
    Separate the cpu_entry_area code out of cpu/common.c and the fixmap.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 74136fd16f49..464daed6894f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -52,6 +52,7 @@
 #include <asm/traps.h>
 #include <asm/desc.h>
 #include <asm/fpu/internal.h>
+#include <asm/cpu_entry_area.h>
 #include <asm/mce.h>
 #include <asm/fixmap.h>
 #include <asm/mach_traps.h>

commit 64a48099b3b31568ac45716b7fafcb74a0c2fcfe
Merge: 1291a0d5049d 6cbd2171e89b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 18 08:59:15 2017 -0800

    Merge branch 'WIP.x86-pti.entry-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 syscall entry code changes for PTI from Ingo Molnar:
     "The main changes here are Andy Lutomirski's changes to switch the
      x86-64 entry code to use the 'per CPU entry trampoline stack'. This,
      besides helping fix KASLR leaks (the pending Page Table Isolation
      (PTI) work), also robustifies the x86 entry code"
    
    * 'WIP.x86-pti.entry-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (26 commits)
      x86/cpufeatures: Make CPU bugs sticky
      x86/paravirt: Provide a way to check for hypervisors
      x86/paravirt: Dont patch flush_tlb_single
      x86/entry/64: Make cpu_entry_area.tss read-only
      x86/entry: Clean up the SYSENTER_stack code
      x86/entry/64: Remove the SYSENTER stack canary
      x86/entry/64: Move the IST stacks into struct cpu_entry_area
      x86/entry/64: Create a per-CPU SYSCALL entry trampoline
      x86/entry/64: Return to userspace from the trampoline stack
      x86/entry/64: Use a per-CPU trampoline stack for IDT entries
      x86/espfix/64: Stop assuming that pt_regs is on the entry stack
      x86/entry/64: Separate cpu_current_top_of_stack from TSS.sp0
      x86/entry: Remap the TSS into the CPU entry area
      x86/entry: Move SYSENTER_stack to the beginning of struct tss_struct
      x86/dumpstack: Handle stack overflow on all stacks
      x86/entry: Fix assumptions that the HW TSS is at the beginning of cpu_tss
      x86/kasan/64: Teach KASAN about the cpu_entry_area
      x86/mm/fixmap: Generalize the GDT fixmap mechanism, introduce struct cpu_entry_area
      x86/entry/gdt: Put per-CPU GDT remaps in ascending order
      x86/dumpstack: Add get_stack_info() support for the SYSENTER stack
      ...

commit c482feefe1aeb150156248ba0fd3e029bc886605
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:29 2017 +0100

    x86/entry/64: Make cpu_entry_area.tss read-only
    
    The TSS is a fairly juicy target for exploits, and, now that the TSS
    is in the cpu_entry_area, it's no longer protected by kASLR.  Make it
    read-only on x86_64.
    
    On x86_32, it can't be RO because it's written by the CPU during task
    switches, and we use a task gate for double faults.  I'd also be
    nervous about errata if we tried to make it RO even on configurations
    without double fault handling.
    
    [ tglx: AMD confirmed that there is no problem on 64-bit with TSS RO.  So
            it's probably safe to assume that it's a non issue, though Intel
            might have been creative in that area. Still waiting for
            confirmation. ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.733700132@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 5ade4f89a6d1..74136fd16f49 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -364,7 +364,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 		regs->cs == __KERNEL_CS &&
 		regs->ip == (unsigned long)native_irq_return_iret)
 	{
-		struct pt_regs *gpregs = (struct pt_regs *)this_cpu_read(cpu_tss.x86_tss.sp0) - 1;
+		struct pt_regs *gpregs = (struct pt_regs *)this_cpu_read(cpu_tss_rw.x86_tss.sp0) - 1;
 
 		/*
 		 * regs->sp points to the failing IRET frame on the
@@ -649,7 +649,7 @@ struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)
 	 * exception came from the IRET target.
 	 */
 	struct bad_iret_stack *new_stack =
-		(struct bad_iret_stack *)this_cpu_read(cpu_tss.x86_tss.sp0) - 1;
+		(struct bad_iret_stack *)this_cpu_read(cpu_tss_rw.x86_tss.sp0) - 1;
 
 	/* Copy the IRET target to the new stack. */
 	memmove(&new_stack->regs.ip, (void *)s->regs.sp, 5*8);

commit 7fbbd5cbebf118a9e09f5453f686656a167c3d1c
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:27 2017 +0100

    x86/entry/64: Remove the SYSENTER stack canary
    
    Now that the SYSENTER stack has a guard page, there's no need for a canary
    to detect overflow after the fact.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.572577316@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 3e29aad5c7cc..5ade4f89a6d1 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -814,13 +814,6 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	debug_stack_usage_dec();
 
 exit:
-	/*
-	 * This is the most likely code path that involves non-trivial use
-	 * of the SYSENTER stack.  Check that we haven't overrun it.
-	 */
-	WARN(this_cpu_read(cpu_tss.SYSENTER_stack_canary) != STACK_END_MAGIC,
-	     "Overran or corrupted SYSENTER stack\n");
-
 	ist_exit(regs);
 }
 NOKPROBE_SYMBOL(do_debug);

commit 40e7f949e0d9a33968ebde5d67f7e3a47c97742a
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:26 2017 +0100

    x86/entry/64: Move the IST stacks into struct cpu_entry_area
    
    The IST stacks are needed when an IST exception occurs and are accessed
    before any kernel code at all runs.  Move them into struct cpu_entry_area.
    
    The IST stacks are unlike the rest of cpu_entry_area: they're used even for
    entries from kernel mode.  This means that they should be set up before we
    load the final IDT.  Move cpu_entry_area setup to trap_init() for the boot
    CPU and set it up for all possible CPUs at once in native_smp_prepare_cpus().
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.480598743@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ee9ca0ad4388..3e29aad5c7cc 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -947,6 +947,9 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 
 void __init trap_init(void)
 {
+	/* Init cpu_entry_area before IST entries are set up */
+	setup_cpu_entry_areas();
+
 	idt_setup_traps();
 
 	/*

commit 7f2590a110b837af5679d08fc25c6227c5a8c497
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:23 2017 +0100

    x86/entry/64: Use a per-CPU trampoline stack for IDT entries
    
    Historically, IDT entries from usermode have always gone directly
    to the running task's kernel stack.  Rearrange it so that we enter on
    a per-CPU trampoline stack and then manually switch to the task's stack.
    This touches a couple of extra cachelines, but it gives us a chance
    to run some code before we touch the kernel stack.
    
    The asm isn't exactly beautiful, but I think that fully refactoring
    it can wait.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.225330557@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index f0029d17b14b..ee9ca0ad4388 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -619,14 +619,15 @@ NOKPROBE_SYMBOL(do_int3);
 
 #ifdef CONFIG_X86_64
 /*
- * Help handler running on IST stack to switch off the IST stack if the
- * interrupted code was in user mode. The actual stack switch is done in
- * entry_64.S
+ * Help handler running on a per-cpu (IST or entry trampoline) stack
+ * to switch to the normal thread stack if the interrupted code was in
+ * user mode. The actual stack switch is done in entry_64.S
  */
 asmlinkage __visible notrace struct pt_regs *sync_regs(struct pt_regs *eregs)
 {
-	struct pt_regs *regs = task_pt_regs(current);
-	*regs = *eregs;
+	struct pt_regs *regs = (struct pt_regs *)this_cpu_read(cpu_current_top_of_stack) - 1;
+	if (regs != eregs)
+		*regs = *eregs;
 	return regs;
 }
 NOKPROBE_SYMBOL(sync_regs);
@@ -642,13 +643,13 @@ struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)
 	/*
 	 * This is called from entry_64.S early in handling a fault
 	 * caused by a bad iret to user mode.  To handle the fault
-	 * correctly, we want move our stack frame to task_pt_regs
-	 * and we want to pretend that the exception came from the
-	 * iret target.
+	 * correctly, we want to move our stack frame to where it would
+	 * be had we entered directly on the entry stack (rather than
+	 * just below the IRET frame) and we want to pretend that the
+	 * exception came from the IRET target.
 	 */
 	struct bad_iret_stack *new_stack =
-		container_of(task_pt_regs(current),
-			     struct bad_iret_stack, regs);
+		(struct bad_iret_stack *)this_cpu_read(cpu_tss.x86_tss.sp0) - 1;
 
 	/* Copy the IRET target to the new stack. */
 	memmove(&new_stack->regs.ip, (void *)s->regs.sp, 5*8);

commit 6d9256f0a89eaff97fca6006100bcaea8d1d8bdb
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:22 2017 +0100

    x86/espfix/64: Stop assuming that pt_regs is on the entry stack
    
    When we start using an entry trampoline, a #GP from userspace will
    be delivered on the entry stack, not on the task stack.  Fix the
    espfix64 #DF fixup to set up #GP according to TSS.SP0, rather than
    assuming that pt_regs + 1 == SP0.  This won't change anything
    without an entry stack, but it will make the code continue to work
    when an entry stack is added.
    
    While we're at it, improve the comments to explain what's actually
    going on.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.130778051@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index d3e3bbd5d3a0..f0029d17b14b 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -348,9 +348,15 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 
 	/*
 	 * If IRET takes a non-IST fault on the espfix64 stack, then we
-	 * end up promoting it to a doublefault.  In that case, modify
-	 * the stack to make it look like we just entered the #GP
-	 * handler from user space, similar to bad_iret.
+	 * end up promoting it to a doublefault.  In that case, take
+	 * advantage of the fact that we're not using the normal (TSS.sp0)
+	 * stack right now.  We can write a fake #GP(0) frame at TSS.sp0
+	 * and then modify our own IRET frame so that, when we return,
+	 * we land directly at the #GP(0) vector with the stack already
+	 * set up according to its expectations.
+	 *
+	 * The net result is that our #GP handler will think that we
+	 * entered from usermode with the bad user context.
 	 *
 	 * No need for ist_enter here because we don't use RCU.
 	 */
@@ -358,13 +364,26 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 		regs->cs == __KERNEL_CS &&
 		regs->ip == (unsigned long)native_irq_return_iret)
 	{
-		struct pt_regs *normal_regs = task_pt_regs(current);
+		struct pt_regs *gpregs = (struct pt_regs *)this_cpu_read(cpu_tss.x86_tss.sp0) - 1;
+
+		/*
+		 * regs->sp points to the failing IRET frame on the
+		 * ESPFIX64 stack.  Copy it to the entry stack.  This fills
+		 * in gpregs->ss through gpregs->ip.
+		 *
+		 */
+		memmove(&gpregs->ip, (void *)regs->sp, 5*8);
+		gpregs->orig_ax = 0;  /* Missing (lost) #GP error code */
 
-		/* Fake a #GP(0) from userspace. */
-		memmove(&normal_regs->ip, (void *)regs->sp, 5*8);
-		normal_regs->orig_ax = 0;  /* Missing (lost) #GP error code */
+		/*
+		 * Adjust our frame so that we return straight to the #GP
+		 * vector with the expected RSP value.  This is safe because
+		 * we won't enable interupts or schedule before we invoke
+		 * general_protection, so nothing will clobber the stack
+		 * frame we just set up.
+		 */
 		regs->ip = (unsigned long)general_protection;
-		regs->sp = (unsigned long)&normal_regs->orig_ax;
+		regs->sp = (unsigned long)&gpregs->orig_ax;
 
 		return;
 	}
@@ -389,7 +408,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	 *
 	 *   Processors update CR2 whenever a page fault is detected. If a
 	 *   second page fault occurs while an earlier page fault is being
-	 *   deliv- ered, the faulting linear address of the second fault will
+	 *   delivered, the faulting linear address of the second fault will
 	 *   overwrite the contents of CR2 (replacing the previous
 	 *   address). These updates to CR2 occur even if the page fault
 	 *   results in a double fault or occurs during the delivery of a

commit 1a79797b58cddfa948420a7553241c79c013e3ca
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:12 2017 +0100

    x86/entry/64: Allocate and enable the SYSENTER stack
    
    This will simplify future changes that want scratch variables early in
    the SYSENTER handler -- they'll be able to spill registers to the
    stack.  It also lets us get rid of a SWAPGS_UNSAFE_STACK user.
    
    This does not depend on CONFIG_IA32_EMULATION=y because we'll want the
    stack space even without IA32 emulation.
    
    As far as I can tell, the reason that this wasn't done from day 1 is
    that we use IST for #DB and #BP, which is IMO rather nasty and causes
    a lot more problems than it solves.  But, since #DB uses IST, we don't
    actually need a real stack for SYSENTER (because SYSENTER with TF set
    will invoke #DB on the IST stack rather than the SYSENTER stack).
    
    I want to remove IST usage from these vectors some day, and this patch
    is a prerequisite for that as well.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.312726423@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index d366adfc61da..d3e3bbd5d3a0 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -794,14 +794,13 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	debug_stack_usage_dec();
 
 exit:
-#if defined(CONFIG_X86_32)
 	/*
 	 * This is the most likely code path that involves non-trivial use
 	 * of the SYSENTER stack.  Check that we haven't overrun it.
 	 */
 	WARN(this_cpu_read(cpu_tss.SYSENTER_stack_canary) != STACK_END_MAGIC,
 	     "Overran or corrupted SYSENTER stack\n");
-#endif
+
 	ist_exit(regs);
 }
 NOKPROBE_SYMBOL(do_debug);

commit 0fd2e9c53d82704a3ba87ea1980ec515188c5316
Merge: 1784f9144b14 1e4c4f610f77
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Dec 1 10:32:48 2017 +0100

    Merge commit 'upstream-x86-entry' into WIP.x86/mm
    
    Pull in a minimal set of v4.15 entry code changes, for a base for the MM isolation patches.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c739f930be1dd5fd949030e3475a884fe06dae9b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Dec 12 07:56:36 2017 -0800

    x86/espfix/64: Fix espfix double-fault handling on 5-level systems
    
    Using PGDIR_SHIFT to identify espfix64 addresses on 5-level systems
    was wrong, and it resulted in panics due to unhandled double faults.
    Use P4D_SHIFT instead, which is correct on 4-level and 5-level
    machines.
    
    This fixes a panic when running x86 selftests on 5-level machines.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Fixes: 1d33b219563f ("x86/espfix: Add support for 5-level paging")
    Link: http://lkml.kernel.org/r/24c898b4f44fdf8c22d93703850fb384ef87cfdc.1513035461.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b7b0f74a2150..c751518936ac 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -355,7 +355,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	 *
 	 * No need for ist_enter here because we don't use RCU.
 	 */
-	if (((long)regs->sp >> PGDIR_SHIFT) == ESPFIX_PGD_ENTRY &&
+	if (((long)regs->sp >> P4D_SHIFT) == ESPFIX_PGD_ENTRY &&
 		regs->cs == __KERNEL_CS &&
 		regs->ip == (unsigned long)native_irq_return_iret)
 	{

commit 4950276672fce5c241857540f8561c440663673d
Author: Levin, Alexander (Sasha Levin) <alexander.levin@verizon.com>
Date:   Wed Nov 15 17:35:51 2017 -0800

    kmemcheck: remove annotations
    
    Patch series "kmemcheck: kill kmemcheck", v2.
    
    As discussed at LSF/MM, kill kmemcheck.
    
    KASan is a replacement that is able to work without the limitation of
    kmemcheck (single CPU, slow).  KASan is already upstream.
    
    We are also not aware of any users of kmemcheck (or users who don't
    consider KASan as a suitable replacement).
    
    The only objection was that since KASAN wasn't supported by all GCC
    versions provided by distros at that time we should hold off for 2
    years, and try again.
    
    Now that 2 years have passed, and all distros provide gcc that supports
    KASAN, kill kmemcheck again for the very same reasons.
    
    This patch (of 4):
    
    Remove kmemcheck annotations, and calls to kmemcheck from the kernel.
    
    [alexander.levin@verizon.com: correctly remove kmemcheck call from dma_map_sg_attrs]
      Link: http://lkml.kernel.org/r/20171012192151.26531-1-alexander.levin@verizon.com
    Link: http://lkml.kernel.org/r/20171007030159.22241-2-alexander.levin@verizon.com
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tim Hansen <devtimhansen@gmail.com>
    Cc: Vegard Nossum <vegardno@ifi.uio.no>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b7b0f74a2150..989514c94a55 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -42,7 +42,6 @@
 #include <linux/edac.h>
 #endif
 
-#include <asm/kmemcheck.h>
 #include <asm/stacktrace.h>
 #include <asm/processor.h>
 #include <asm/debugreg.h>
@@ -749,10 +748,6 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	if (!dr6 && user_mode(regs))
 		user_icebp = 1;
 
-	/* Catch kmemcheck conditions! */
-	if ((dr6 & DR_STEP) && kmemcheck_trap(regs))
-		goto exit;
-
 	/* Store the virtualized DR6 value */
 	tsk->thread.debugreg6 = dr6;
 

commit b18d62891aaff49d0ee8367d4b6bb9452469f807
Merge: 7d58e1c9059e 141d3b1daacd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 18:29:23 2017 -0800

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 APIC updates from Thomas Gleixner:
     "This update provides a major overhaul of the APIC initialization and
      vector allocation code:
    
       - Unification of the APIC and interrupt mode setup which was
         scattered all over the place and was hard to follow. This also
         distangles the timer setup from the APIC initialization which
         brings a clear separation of functionality.
    
         Great detective work from Dou Lyiang!
    
       - Refactoring of the x86 vector allocation mechanism. The existing
         code was based on nested loops and rather convoluted APIC callbacks
         which had a horrible worst case behaviour and tried to serve all
         different use cases in one go. This led to quite odd hacks when
         supporting the new managed interupt facility for multiqueue devices
         and made it more or less impossible to deal with the vector space
         exhaustion which was a major roadblock for server hibernation.
    
         Aside of that the code dealing with cpu hotplug and the system
         vectors was disconnected from the actual vector management and
         allocation code, which made it hard to follow and maintain.
    
         Utilizing the new bitmap matrix allocator core mechanism, the new
         allocator and management code consolidates the handling of system
         vectors, legacy vectors, cpu hotplug mechanisms and the actual
         allocation which needs to be aware of system and legacy vectors and
         hotplug constraints into a single consistent entity.
    
         This has one visible change: The support for multi CPU targets of
         interrupts, which is only available on a certain subset of
         CPUs/APIC variants has been removed in favour of single interrupt
         targets. A proper analysis of the multi CPU target feature revealed
         that there is no real advantage as the vast majority of interrupts
         end up on the CPU with the lowest APIC id in the set of target CPUs
         anyway. That change was agreed on by the relevant folks and allowed
         to simplify the implementation significantly and to replace rather
         fragile constructs like the vector cleanup IPI with straight
         forward and solid code.
    
         Furthermore this allowed to cleanly separate the allocation details
         for legacy, normal and managed interrupts:
    
          * Legacy interrupts are not longer wasting 16 vectors
            unconditionally
    
          * Managed interrupts have now a guaranteed vector reservation, but
            the actual vector assignment happens when the interrupt is
            requested. It's guaranteed not to fail.
    
          * Normal interrupts no longer allocate vectors unconditionally
            when the interrupt is set up (IO/APIC init or MSI(X) enable).
            The mechanism has been switched to a best effort reservation
            mode. The actual allocation happens when the interrupt is
            requested. Contrary to managed interrupts the request can fail
            due to vector space exhaustion, but drivers must handle a fail
            of request_irq() anyway. When the interrupt is freed, the vector
            is handed back as well.
    
            This solves a long standing problem with large unconditional
            vector allocations for a certain class of enterprise devices
            which prevented server hibernation due to vector space
            exhaustion when the unused allocated vectors had to be migrated
            to CPU0 while unplugging all non boot CPUs.
    
         The code has been equipped with trace points and detailed debugfs
         information to aid analysis of the vector space"
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (60 commits)
      x86/vector/msi: Select CONFIG_GENERIC_IRQ_RESERVATION_MODE
      PCI/MSI: Set MSI_FLAG_MUST_REACTIVATE in core code
      genirq: Add config option for reservation mode
      x86/vector: Use correct per cpu variable in free_moved_vector()
      x86/apic/vector: Ignore set_affinity call for inactive interrupts
      x86/apic: Fix spelling mistake: "symmectic" -> "symmetric"
      x86/apic: Use dead_cpu instead of current CPU when cleaning up
      ACPI/init: Invoke early ACPI initialization earlier
      x86/vector: Respect affinity mask in irq descriptor
      x86/irq: Simplify hotplug vector accounting
      x86/vector: Switch IOAPIC to global reservation mode
      x86/vector/msi: Switch to global reservation mode
      x86/vector: Handle managed interrupts proper
      x86/io_apic: Reevaluate vector configuration on activate()
      iommu/amd: Reevaluate vector configuration on activate()
      iommu/vt-d: Reevaluate vector configuration on activate()
      x86/apic/msi: Force reactivation of interrupts at startup time
      x86/vector: Untangle internal state from irq_cfg
      x86/vector: Compile SMP only code conditionally
      x86/apic: Remove unused callbacks
      ...

commit d6ec9d9a4def52a5094237564eaf6f6979fd7a27
Merge: 3e2014637c50 91a6a6cfee8a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 14:13:48 2017 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 core updates from Ingo Molnar:
     "Note that in this cycle most of the x86 topics interacted at a level
      that caused them to be merged into tip:x86/asm - but this should be a
      temporary phenomenon, hopefully we'll back to the usual patterns in
      the next merge window.
    
      The main changes in this cycle were:
    
      Hardware enablement:
    
       - Add support for the Intel UMIP (User Mode Instruction Prevention)
         CPU feature. This is a security feature that disables certain
         instructions such as SGDT, SLDT, SIDT, SMSW and STR. (Ricardo Neri)
    
         [ Note that this is disabled by default for now, there are some
           smaller enhancements in the pipeline that I'll follow up with in
           the next 1-2 days, which allows this to be enabled by default.]
    
       - Add support for the AMD SEV (Secure Encrypted Virtualization) CPU
         feature, on top of SME (Secure Memory Encryption) support that was
         added in v4.14. (Tom Lendacky, Brijesh Singh)
    
       - Enable new SSE/AVX/AVX512 CPU features: AVX512_VBMI2, GFNI, VAES,
         VPCLMULQDQ, AVX512_VNNI, AVX512_BITALG. (Gayatri Kammela)
    
      Other changes:
    
       - A big series of entry code simplifications and enhancements (Andy
         Lutomirski)
    
       - Make the ORC unwinder default on x86 and various objtool
         enhancements. (Josh Poimboeuf)
    
       - 5-level paging enhancements (Kirill A. Shutemov)
    
       - Micro-optimize the entry code a bit (Borislav Petkov)
    
       - Improve the handling of interdependent CPU features in the early
         FPU init code (Andi Kleen)
    
       - Build system enhancements (Changbin Du, Masahiro Yamada)
    
       - ... plus misc enhancements, fixes and cleanups"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (118 commits)
      x86/build: Make the boot image generation less verbose
      selftests/x86: Add tests for the STR and SLDT instructions
      selftests/x86: Add tests for User-Mode Instruction Prevention
      x86/traps: Fix up general protection faults caused by UMIP
      x86/umip: Enable User-Mode Instruction Prevention at runtime
      x86/umip: Force a page fault when unable to copy emulated result to user
      x86/umip: Add emulation code for UMIP instructions
      x86/cpufeature: Add User-Mode Instruction Prevention definitions
      x86/insn-eval: Add support to resolve 16-bit address encodings
      x86/insn-eval: Handle 32-bit address encodings in virtual-8086 mode
      x86/insn-eval: Add wrapper function for 32 and 64-bit addresses
      x86/insn-eval: Add support to resolve 32-bit address encodings
      x86/insn-eval: Compute linear address in several utility functions
      resource: Fix resource_size.cocci warnings
      X86/KVM: Clear encryption attribute when SEV is active
      X86/KVM: Decrypt shared per-cpu variables when SEV is active
      percpu: Introduce DEFINE_PER_CPU_DECRYPTED
      x86: Add support for changing memory encryption attribute in early boot
      x86/io: Unroll string I/O when SEV is active
      x86/boot: Add early boot support when running with SEV active
      ...

commit b8347c2196492f4e1cccde3d92fda1cc2cc7de7e
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Mon Jul 24 13:04:28 2017 +0300

    x86/debug: Handle warnings before the notifier chain, to fix KGDB crash
    
    Commit:
    
      9a93848fe787 ("x86/debug: Implement __WARN() using UD0")
    
    turned warnings into UD0, but the fixup code only runs after the
    notify_die() chain. This is a problem, in particular, with kgdb,
    which kicks in as if it was a BUG().
    
    Fix this by running the fixup code before the notifier chain in
    the invalid op handler path.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Tested-by: Ilya Dryomov <idryomov@gmail.com>
    Acked-by: Daniel Thompson <daniel.thompson@linaro.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jason Wessel <jason.wessel@windriver.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Weinberger <richard.weinberger@gmail.com>
    Cc: <stable@vger.kernel.org> # v4.12+
    Link: http://lkml.kernel.org/r/20170724100428.19173-1-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 67db4f43309e..5a6b8f809792 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -209,9 +209,6 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 		if (fixup_exception(regs, trapnr))
 			return 0;
 
-		if (fixup_bug(regs, trapnr))
-			return 0;
-
 		tsk->thread.error_code = error_code;
 		tsk->thread.trap_nr = trapnr;
 		die(str, regs, error_code);
@@ -292,6 +289,13 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 
+	/*
+	 * WARN*()s end up here; fix them up before we call the
+	 * notifier chain.
+	 */
+	if (!user_mode(regs) && fixup_bug(regs, trapnr))
+		return;
+
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
 			NOTIFY_STOP) {
 		cond_local_irq_enable(regs);

commit 6fc9dc81bff0ea461db534e2672acfdaf76f3e4e
Author: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
Date:   Sun Nov 5 18:27:55 2017 -0800

    x86/traps: Fix up general protection faults caused by UMIP
    
    If the User-Mode Instruction Prevention CPU feature is available and
    enabled, a general protection fault will be issued if the instructions
    sgdt, sldt, sidt, str or smsw are executed from user-mode context
    (CPL > 0). If the fault was caused by any of the instructions protected
    by UMIP, fixup_umip_exception() will emulate dummy results for these
    instructions as follows: in virtual-8086 and protected modes, sgdt, sidt
    and smsw are emulated; str and sldt are not emulated. No emulation is done
    for user-space long mode processes.
    
    If emulation is successful, the emulated result is passed to the user space
    program and no SIGSEGV signal is emitted.
    
    Signed-off-by: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Chen Yucong <slaoub@gmail.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi V. Shankar <ravi.v.shankar@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: ricardo.neri@intel.com
    Link: http://lkml.kernel.org/r/1509935277-22138-11-git-send-email-ricardo.neri-calderon@linux.intel.com
    [ Added curly braces. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 42a9c4458f5d..ab54bf398803 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -60,6 +60,7 @@
 #include <asm/trace/mpx.h>
 #include <asm/mpx.h>
 #include <asm/vm86.h>
+#include <asm/umip.h>
 
 #ifdef CONFIG_X86_64
 #include <asm/x86_init.h>
@@ -513,6 +514,11 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	cond_local_irq_enable(regs);
 
+	if (static_cpu_has(X86_FEATURE_UMIP)) {
+		if (user_mode(regs) && fixup_umip_exception(regs))
+			return;
+	}
+
 	if (v8086_mode(regs)) {
 		local_irq_enable();
 		handle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);

commit 3383642c2f9d4f5b4fa37436db4a109a1a10018c
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:17 2017 -0700

    x86/traps: Use a new on_thread_stack() helper to clean up an assertion
    
    Let's keep the stack-related logic together rather than open-coding
    a comparison in an assertion in the traps code.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/856b15bee1f55017b8f79d3758b0d51c48a08cf8.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 67db4f43309e..42a9c4458f5d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -141,8 +141,7 @@ void ist_begin_non_atomic(struct pt_regs *regs)
 	 * will catch asm bugs and any attempt to use ist_preempt_enable
 	 * from double_fault.
 	 */
-	BUG_ON((unsigned long)(current_top_of_stack() -
-			       current_stack_pointer) >= THREAD_SIZE);
+	BUG_ON(!on_thread_stack());
 
 	preempt_enable_no_resched();
 }

commit 331b57d14829c49d75076779cdc54d7e4537bbf0
Merge: 79761ce80aa0 e43b3b585480
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 12 11:02:50 2017 +0200

    Merge branch 'irq/urgent' into x86/apic
    
    Pick up core changes which affect the vector rework.

commit 196bd485ee4f03ce4c690bfcf38138abfcd0a4bc
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Fri Sep 29 17:15:36 2017 +0300

    x86/asm: Use register variable to get stack pointer value
    
    Currently we use current_stack_pointer() function to get the value
    of the stack pointer register. Since commit:
    
      f5caf621ee35 ("x86/asm: Fix inline asm call constraints for Clang")
    
    ... we have a stack register variable declared. It can be used instead of
    current_stack_pointer() function which allows to optimize away some
    excessive "mov %rsp, %<dst>" instructions:
    
     -mov    %rsp,%rdx
     -sub    %rdx,%rax
     -cmp    $0x3fff,%rax
     -ja     ffffffff810722fd <ist_begin_non_atomic+0x2d>
    
     +sub    %rsp,%rax
     +cmp    $0x3fff,%rax
     +ja     ffffffff810722fa <ist_begin_non_atomic+0x2a>
    
    Remove current_stack_pointer(), rename __asm_call_sp to current_stack_pointer
    and use it instead of the removed function.
    
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170929141537.29167-1-aryabinin@virtuozzo.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 34ea3651362e..67db4f43309e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -142,7 +142,7 @@ void ist_begin_non_atomic(struct pt_regs *regs)
 	 * from double_fault.
 	 */
 	BUG_ON((unsigned long)(current_top_of_stack() -
-			       current_stack_pointer()) >= THREAD_SIZE);
+			       current_stack_pointer) >= THREAD_SIZE);
 
 	preempt_enable_no_resched();
 }

commit 7854f82293e99f6bb3df793a2f579db4670ba71b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 13 23:29:26 2017 +0200

    x86/vector: Rename used_vectors to system_vectors
    
    used_vectors is a nisnomer as it only has the system vectors which are
    excluded from the regular vector allocation marked. It's not what the name
    suggests storage for the actually used vectors.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Juergen Gross <jgross@suse.com>
    Tested-by: Yu Chen <yu.c.chen@intel.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Link: https://lkml.kernel.org/r/20170913213154.150209009@linutronix.de

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 34ea3651362e..321240f712e1 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -71,7 +71,7 @@
 #include <asm/proto.h>
 #endif
 
-DECLARE_BITMAP(used_vectors, NR_VECTORS);
+DECLARE_BITMAP(system_vectors, NR_VECTORS);
 
 static inline void cond_local_irq_enable(struct pt_regs *regs)
 {

commit b70543a0b2b680f8953b6917a83b9203b20d7abd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:53 2017 +0200

    x86/idt: Move regular trap init to tables
    
    Initialize the regular traps with a table.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170828064959.182128165@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 293f5bddd761..34ea3651362e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -925,46 +925,7 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 
 void __init trap_init(void)
 {
-	int i;
-
-	set_intr_gate(X86_TRAP_DE, divide_error);
-	set_intr_gate_ist(X86_TRAP_NMI, &nmi, NMI_STACK);
-	/* int4 can be called from all */
-	set_system_intr_gate(X86_TRAP_OF, &overflow);
-	set_intr_gate(X86_TRAP_BR, bounds);
-	set_intr_gate(X86_TRAP_UD, invalid_op);
-	set_intr_gate(X86_TRAP_NM, device_not_available);
-#ifdef CONFIG_X86_32
-	set_task_gate(X86_TRAP_DF, GDT_ENTRY_DOUBLEFAULT_TSS);
-#else
-	set_intr_gate_ist(X86_TRAP_DF, &double_fault, DOUBLEFAULT_STACK);
-#endif
-	set_intr_gate(X86_TRAP_OLD_MF, coprocessor_segment_overrun);
-	set_intr_gate(X86_TRAP_TS, invalid_TSS);
-	set_intr_gate(X86_TRAP_NP, segment_not_present);
-	set_intr_gate(X86_TRAP_SS, stack_segment);
-	set_intr_gate(X86_TRAP_GP, general_protection);
-	set_intr_gate(X86_TRAP_SPURIOUS, spurious_interrupt_bug);
-	set_intr_gate(X86_TRAP_MF, coprocessor_error);
-	set_intr_gate(X86_TRAP_AC, alignment_check);
-#ifdef CONFIG_X86_MCE
-	set_intr_gate_ist(X86_TRAP_MC, &machine_check, MCE_STACK);
-#endif
-	set_intr_gate(X86_TRAP_XF, simd_coprocessor_error);
-
-	/* Reserve all the builtin and the syscall vector: */
-	for (i = 0; i < FIRST_EXTERNAL_VECTOR; i++)
-		set_bit(i, used_vectors);
-
-#ifdef CONFIG_IA32_EMULATION
-	set_system_intr_gate(IA32_SYSCALL_VECTOR, entry_INT80_compat);
-	set_bit(IA32_SYSCALL_VECTOR, used_vectors);
-#endif
-
-#ifdef CONFIG_X86_32
-	set_system_intr_gate(IA32_SYSCALL_VECTOR, entry_INT80_32);
-	set_bit(IA32_SYSCALL_VECTOR, used_vectors);
-#endif
+	idt_setup_traps();
 
 	/*
 	 * Set the IDT descriptor to a fixed read-only location, so that the

commit 90f6225fba0c732f3f5f9f5e265bdefa021ff12d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:52 2017 +0200

    x86/idt: Move IST stack based traps to table init
    
    Initialize the IST based traps via a table.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170828064959.091328949@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1492bf5eb549..293f5bddd761 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -979,14 +979,7 @@ void __init trap_init(void)
 	 */
 	cpu_init();
 
-	/*
-	 * X86_TRAP_DB and X86_TRAP_BP have been set
-	 * in early_trap_init(). However, ITS works only after
-	 * cpu_init() loads TSS. See comments in early_trap_init().
-	 */
-	set_intr_gate_ist(X86_TRAP_DB, &debug, DEBUG_STACK);
-	/* int3 can be called from all */
-	set_system_intr_gate_ist(X86_TRAP_BP, &int3, DEBUG_STACK);
+	idt_setup_ist_traps();
 
 	x86_init.irqs.trap_init();
 

commit 0a30908b9149b2b332ccf817261125a634765566
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:51 2017 +0200

    x86/idt: Move debug stack init to table based
    
    Add the debug_idt init table and make use of it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170828064959.006502252@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 835c7e8a0eb1..1492bf5eb549 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -990,9 +990,5 @@ void __init trap_init(void)
 
 	x86_init.irqs.trap_init();
 
-#ifdef CONFIG_X86_64
-	memcpy(&debug_idt_table, &idt_table, IDT_ENTRIES * 16);
-	set_nmi_gate(X86_TRAP_DB, &debug);
-	set_nmi_gate(X86_TRAP_BP, &int3);
-#endif
+	idt_setup_debugidt_traps();
 }

commit 433f8924fa8e55a50ce57f3b8a33ed095c405644
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:50 2017 +0200

    x86/idt: Switch early trap init to IDT tables
    
    Add the initialization table for the early trap setup and replace the early
    trap init code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170828064958.929139008@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 41f4cd331cb2..835c7e8a0eb1 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -923,33 +923,6 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 }
 #endif
 
-/* Set of traps needed for early debugging. */
-void __init early_trap_init(void)
-{
-	/*
-	 * Don't use IST to set DEBUG_STACK as it doesn't work until TSS
-	 * is ready in cpu_init() <-- trap_init(). Before trap_init(),
-	 * CPU runs at ring 0 so it is impossible to hit an invalid
-	 * stack.  Using the original stack works well enough at this
-	 * early stage. DEBUG_STACK will be equipped after cpu_init() in
-	 * trap_init().
-	 */
-	set_intr_gate(X86_TRAP_DB, debug);
-	/* int3 can be called from all */
-	set_system_intr_gate(X86_TRAP_BP, &int3);
-#ifdef CONFIG_X86_32
-	set_intr_gate(X86_TRAP_PF, page_fault);
-#endif
-	load_idt(&idt_descr);
-}
-
-void __init early_trap_pf_init(void)
-{
-#ifdef CONFIG_X86_64
-	set_intr_gate(X86_TRAP_PF, page_fault);
-#endif
-}
-
 void __init trap_init(void)
 {
 	int i;

commit d8ed9d48266a27ab02a4bbcb81e755d63aec108a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:43 2017 +0200

    x86/idt: Create file for IDT related code
    
    IDT related code lives scattered around in various places. Create a new
    source file in arch/x86/kernel/idt.c to hold it.
    
    Move the idt_tables and descriptors to it for a start. Follow up patches
    will gradually move more code over.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170828064958.367081121@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 36c583625328..41f4cd331cb2 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -65,18 +65,12 @@
 #include <asm/x86_init.h>
 #include <asm/pgalloc.h>
 #include <asm/proto.h>
-
-/* No need to be aligned, but done to keep all IDTs defined the same way. */
-gate_desc debug_idt_table[NR_VECTORS] __page_aligned_bss;
 #else
 #include <asm/processor-flags.h>
 #include <asm/setup.h>
 #include <asm/proto.h>
 #endif
 
-/* Must be page-aligned because the real IDT is used in a fixmap. */
-gate_desc idt_table[NR_VECTORS] __page_aligned_bss;
-
 DECLARE_BITMAP(used_vectors, NR_VECTORS);
 
 static inline void cond_local_irq_enable(struct pt_regs *regs)

commit 4b9a8dca0e58b6fee229795f77e902a4dfd116fe
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:31 2017 +0200

    x86/idt: Remove the tracing IDT completely
    
    No more users of the tracing IDT. All exception tracepoints have been moved
    into the regular handlers. Get rid of the mess which shouldn't have been
    created in the first place.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170828064957.378851687@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 309532451d94..36c583625328 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -939,12 +939,8 @@ void __init early_trap_init(void)
 	 * stack.  Using the original stack works well enough at this
 	 * early stage. DEBUG_STACK will be equipped after cpu_init() in
 	 * trap_init().
-	 *
-	 * We don't need to set trace_idt_table like set_intr_gate(),
-	 * since we don't have trace_debug and it will be reset to
-	 * 'debug' in trap_init() by set_intr_gate_ist().
 	 */
-	set_intr_gate_notrace(X86_TRAP_DB, debug);
+	set_intr_gate(X86_TRAP_DB, debug);
 	/* int3 can be called from all */
 	set_system_intr_gate(X86_TRAP_BP, &int3);
 #ifdef CONFIG_X86_32

commit f7eaf6e00fd581043bb540dfe865f1d81769b189
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:20 2017 +0200

    x86/boot: Move EISA setup to a separate file
    
    EISA has absolutely nothing to do with traps, so move it out of traps.c
    into its own eisa.c file.
    
    Furthermore, the EISA bus detection does not need to run during
    very early boot, it's good enough to run it before the EISA bus
    and drivers are initialized.
    
    I.e. instead of calling it from the very early trap_init() code,
    make it a subsys_initcall().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170828064956.515322409@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 556f8f53965d..309532451d94 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -38,11 +38,6 @@
 #include <linux/smp.h>
 #include <linux/io.h>
 
-#ifdef CONFIG_EISA
-#include <linux/ioport.h>
-#include <linux/eisa.h>
-#endif
-
 #if defined(CONFIG_EDAC)
 #include <linux/edac.h>
 #endif
@@ -969,14 +964,6 @@ void __init trap_init(void)
 {
 	int i;
 
-#ifdef CONFIG_EISA
-	void __iomem *p = early_ioremap(0x0FFFD9, 4);
-
-	if (readl(p) == 'E' + ('I'<<8) + ('S'<<16) + ('A'<<24))
-		EISA_bus = 1;
-	early_iounmap(p, 4);
-#endif
-
 	set_intr_gate(X86_TRAP_DE, divide_error);
 	set_intr_gate_ist(X86_TRAP_NMI, &nmi, NMI_STACK);
 	/* int4 can be called from all */

commit fa4ab5774dfe58fd5e99462f625253659d41df09
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:17 2017 +0200

    x86/irq: Unexport used_vectors[]
    
    No modular users.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170828064956.278375986@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index bf54309b85da..556f8f53965d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -83,7 +83,6 @@ gate_desc debug_idt_table[NR_VECTORS] __page_aligned_bss;
 gate_desc idt_table[NR_VECTORS] __page_aligned_bss;
 
 DECLARE_BITMAP(used_vectors, NR_VECTORS);
-EXPORT_SYMBOL_GPL(used_vectors);
 
 static inline void cond_local_irq_enable(struct pt_regs *regs)
 {

commit 8a524f803a3e0290cdba6d373361b2cef9752934
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jun 12 13:52:46 2017 +0200

    x86/debug: Handle early WARN_ONs proper
    
    Hans managed to trigger a WARN very early in the boot which killed his
    (Virtual) box.
    
    The reason is that the recent rework of WARN() to use UD0 forgot to add the
    fixup_bug() call to early_fixup_exception(). As a result the kernel does
    not handle the WARN_ON injected UD0 exception and panics.
    
    Add the missing fixup call, so early UD's injected by WARN() get handled.
    
    Fixes: 9a93848fe787 ("x86/debug: Implement __WARN() using UD0")
    Reported-and-tested-by: Hans de Goede <hdegoede@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Frank Mehnert <frank.mehnert@oracle.com>
    Cc: Hans de Goede <hdegoede@redhat.com>
    Cc: Michael Thayer <michael.thayer@oracle.com>
    Link: http://lkml.kernel.org/r/20170612180108.w4vgu2ckucmllf3a@hirez.programming.kicks-ass.net

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 3995d3a777d4..bf54309b85da 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -182,7 +182,7 @@ int is_valid_bugaddr(unsigned long addr)
 	return ud == INSN_UD0 || ud == INSN_UD2;
 }
 
-static int fixup_bug(struct pt_regs *regs, int trapnr)
+int fixup_bug(struct pt_regs *regs, int trapnr)
 {
 	if (trapnr != X86_TRAP_UD)
 		return 0;

commit 3fb9268e43e7ab62adb5c6ddec58d3cb4767bd9a
Merge: 12ca7c8db30d 262fa734a002
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 1 22:07:51 2017 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - unwinder fixes and enhancements
    
       - improve ftrace interaction with the unwinder
    
       - optimize the code footprint of WARN() and related debugging
         constructs
    
       - ... plus misc updates, cleanups and fixes"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      x86/unwind: Dump all stacks in unwind_dump()
      x86/unwind: Silence more entry-code related warnings
      x86/ftrace: Fix ebp in ftrace_regs_caller that screws up unwinder
      x86/unwind: Remove unused 'sp' parameter in unwind_dump()
      x86/unwind: Prepend hex mask value with '0x' in unwind_dump()
      x86/unwind: Properly zero-pad 32-bit values in unwind_dump()
      x86/unwind: Ensure stack pointer is aligned
      debug: Avoid setting BUGFLAG_WARNING twice
      x86/unwind: Silence entry-related warnings
      x86/unwind: Read stack return address in update_stack_state()
      x86/unwind: Move common code into update_stack_state()
      debug: Fix __bug_table[] in arch linker scripts
      debug: Add _ONCE() logic to report_bug()
      x86/debug: Define BUG() again for !CONFIG_BUG
      x86/debug: Implement __WARN() using UD0
      x86/ftrace: Use Makefile logic instead of #ifdef for compiling ftrace_*.o
      x86/ftrace: Add -mfentry support to x86_32 with DYNAMIC_FTRACE set
      x86/ftrace: Clean up ftrace_regs_caller
      x86/ftrace: Add stack frame pointer to ftrace_caller
      x86/ftrace: Move the ftrace specific code out of entry_32.S
      ...

commit 1c99a6874133ebf4513504ef7c32ce1f532d323f
Author: Markus Trippelsdorf <markus@trippelsdorf.de>
Date:   Fri Apr 7 14:09:04 2017 +0200

    x86/debug: Fix the printk() debug output of signal_fault(), do_trap() and do_general_protection()
    
    Since commit:
    
      4bcc595ccd80 "printk: reinstate KERN_CONT for printing"
    
    ... the debug output of signal_fault(), do_trap() and do_general_protection()
    looks garbled, e.g.:
    
     traps: conftest[9335] trap invalid opcode ip:400428 sp:7ffeaba1b0d8 error:0
      in conftest[400000+1000]
    
    (note the unintended line break.)
    
    Fix the bug by adding KERN_CONTs.
    
    Signed-off-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 948443e115c1..4e496379a871 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -255,7 +255,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 		pr_info("%s[%d] trap %s ip:%lx sp:%lx error:%lx",
 			tsk->comm, tsk->pid, str,
 			regs->ip, regs->sp, error_code);
-		print_vma_addr(" in ", regs->ip);
+		print_vma_addr(KERN_CONT " in ", regs->ip);
 		pr_cont("\n");
 	}
 
@@ -519,7 +519,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		pr_info("%s[%d] general protection ip:%lx sp:%lx error:%lx",
 			tsk->comm, task_pid_nr(tsk),
 			regs->ip, regs->sp, error_code);
-		print_vma_addr(" in ", regs->ip);
+		print_vma_addr(KERN_CONT " in ", regs->ip);
 		pr_cont("\n");
 	}
 

commit 9a93848fe787a53aec404e4e00d8f7f9bbdaebb4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Feb 2 14:43:51 2017 +0100

    x86/debug: Implement __WARN() using UD0
    
    By using "UD0" for WARN()s we remove the function call and its possible
    __FILE__ and __LINE__ immediate arguments from the instruction stream.
    
    Total image size will not change much, what we win in the instruction
    stream we'll lose because of the __bug_table entries. Still, saves on
    I$ footprint and the total image size does go down a bit.
    
          text    data       filename
      10702123    4530992    defconfig-build/vmlinux.orig
      10682460    4530992    defconfig-build/vmlinux.patched
    
    (UML didn't seem to use GENERIC_BUG at all, so remove it)
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Weinberger <richard.weinberger@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 948443e115c1..3c0751b120de 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -169,6 +169,37 @@ void ist_end_non_atomic(void)
 	preempt_disable();
 }
 
+int is_valid_bugaddr(unsigned long addr)
+{
+	unsigned short ud;
+
+	if (addr < TASK_SIZE_MAX)
+		return 0;
+
+	if (probe_kernel_address((unsigned short *)addr, ud))
+		return 0;
+
+	return ud == INSN_UD0 || ud == INSN_UD2;
+}
+
+static int fixup_bug(struct pt_regs *regs, int trapnr)
+{
+	if (trapnr != X86_TRAP_UD)
+		return 0;
+
+	switch (report_bug(regs->ip, regs)) {
+	case BUG_TRAP_TYPE_NONE:
+	case BUG_TRAP_TYPE_BUG:
+		break;
+
+	case BUG_TRAP_TYPE_WARN:
+		regs->ip += LEN_UD0;
+		return 1;
+	}
+
+	return 0;
+}
+
 static nokprobe_inline int
 do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 		  struct pt_regs *regs,	long error_code)
@@ -187,12 +218,15 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 	}
 
 	if (!user_mode(regs)) {
-		if (!fixup_exception(regs, trapnr)) {
-			tsk->thread.error_code = error_code;
-			tsk->thread.trap_nr = trapnr;
-			die(str, regs, error_code);
-		}
-		return 0;
+		if (fixup_exception(regs, trapnr))
+			return 0;
+
+		if (fixup_bug(regs, trapnr))
+			return 0;
+
+		tsk->thread.error_code = error_code;
+		tsk->thread.trap_nr = trapnr;
+		die(str, regs, error_code);
 	}
 
 	return -1;

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1dc86ee60a03..948443e115c1 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -29,6 +29,7 @@
 #include <linux/errno.h>
 #include <linux/kexec.h>
 #include <linux/sched.h>
+#include <linux/sched/task_stack.h>
 #include <linux/timer.h>
 #include <linux/init.h>
 #include <linux/bug.h>

commit 07d495dae20717b00881798ef812f7aa53ca0eb3
Author: Alexander Kuleshov <kuleshovmail@gmail.com>
Date:   Mon Nov 28 13:50:57 2016 +0600

    x86/traps: Get rid of unnecessary preempt_disable/preempt_enable_no_resched
    
    Exception handlers which may run on IST stack call ist_enter() at the start
    of execution and ist_exit() in the end. ist_enter() disables preemption
    unconditionally and ist_exit() enables it.
    
    So the extra preempt_disable/enable() pairs nested inside the
    ist_enter/exit() regions are pointless and can be removed.
    
    Signed-off-by: Alexander Kuleshov <kuleshovmail@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Jianyu Zhan <nasa4836@gmail.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/20161128075057.7724-1-kuleshovmail@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index bf0c6d049080..1dc86ee60a03 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -563,11 +563,9 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	 * as we may switch to the interrupt stack.
 	 */
 	debug_stack_usage_inc();
-	preempt_disable();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
-	preempt_enable_no_resched();
 	debug_stack_usage_dec();
 exit:
 	ist_exit(regs);
@@ -742,14 +740,12 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	debug_stack_usage_inc();
 
 	/* It's safe to allow irq's after DR6 has been saved */
-	preempt_disable();
 	cond_local_irq_enable(regs);
 
 	if (v8086_mode(regs)) {
 		handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code,
 					X86_TRAP_DB);
 		cond_local_irq_disable(regs);
-		preempt_enable_no_resched();
 		debug_stack_usage_dec();
 		goto exit;
 	}
@@ -769,7 +765,6 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	if (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)
 		send_sigtrap(tsk, regs, error_code, si_code);
 	cond_local_irq_disable(regs);
-	preempt_enable_no_resched();
 	debug_stack_usage_dec();
 
 exit:

commit bef8b6da95229f780a0a7f63e23124058bfad6d3
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Oct 31 15:18:47 2016 -0700

    x86/fpu: Handle #NM without FPU emulation as an error
    
    Don't use CR0.TS.  Make it an error rather than making nonsensical
    changes to the FPU state.
    
    (The cond_local_irq_enable() appears to have been pointless, too.)
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm list <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/f1ee6bf73ed1025fccaab321ba43d0594245f927.1477951965.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index bd4e3d4d3625..bf0c6d049080 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -853,6 +853,8 @@ do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
+	unsigned long cr0;
+
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 
 #ifdef CONFIG_MATH_EMULATION
@@ -866,10 +868,20 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 		return;
 	}
 #endif
-	fpu__restore(&current->thread.fpu); /* interrupts still off */
-#ifdef CONFIG_X86_32
-	cond_local_irq_enable(regs);
-#endif
+
+	/* This should not happen. */
+	cr0 = read_cr0();
+	if (WARN(cr0 & X86_CR0_TS, "CR0.TS was set")) {
+		/* Try to fix it up and carry on. */
+		write_cr0(cr0 & ~X86_CR0_TS);
+	} else {
+		/*
+		 * Something terrible happened, and we're better off trying
+		 * to kill the task than getting stuck in a never-ending
+		 * loop of #NM faults.
+		 */
+		die("unexpected #NM exception", regs, error_code);
+	}
 }
 NOKPROBE_SYMBOL(do_device_not_available);
 

commit 6271cfdfc0e4731b76921ef02fdd87409d71dfdf
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Aug 30 17:27:57 2016 -0700

    x86/mm: Improve stack-overflow #PF handling
    
    If we get a page fault indicating kernel stack overflow, invoke
    handle_stack_overflow().  To prevent us from overflowing the stack
    again while handling the overflow (because we are likely to have
    very little stack space left), call handle_stack_overflow() on the
    double-fault stack.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/6d6cf96b3fb9b4c9aa303817e1dc4de0c7c36487.1472603235.git.luto@kernel.org
    [ Minor edit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 907b4e4aeb5e..bd4e3d4d3625 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -293,9 +293,9 @@ DO_ERROR(X86_TRAP_SS,     SIGBUS,  "stack segment",		stack_segment)
 DO_ERROR(X86_TRAP_AC,     SIGBUS,  "alignment check",		alignment_check)
 
 #ifdef CONFIG_VMAP_STACK
-static void __noreturn handle_stack_overflow(const char *message,
-					     struct pt_regs *regs,
-					     unsigned long fault_address)
+__visible void __noreturn handle_stack_overflow(const char *message,
+						struct pt_regs *regs,
+						unsigned long fault_address)
 {
 	printk(KERN_EMERG "BUG: stack guard page was hit at %p (stack is %p..%p)\n",
 		 (void *)fault_address, current->stack,

commit e37e43a497d5a8b7c0cc1736d56986f432c394c9
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Aug 11 02:35:23 2016 -0700

    x86/mm/64: Enable vmapped stacks (CONFIG_HAVE_ARCH_VMAP_STACK=y)
    
    This allows x86_64 kernels to enable vmapped stacks by setting
    HAVE_ARCH_VMAP_STACK=y - which enables the CONFIG_VMAP_STACK=y
    high level Kconfig option.
    
    There are a couple of interesting bits:
    
    First, x86 lazily faults in top-level paging entries for the vmalloc
    area.  This won't work if we get a page fault while trying to access
    the stack: the CPU will promote it to a double-fault and we'll die.
    To avoid this problem, probe the new stack when switching stacks and
    forcibly populate the pgd entry for the stack when switching mms.
    
    Second, once we have guard pages around the stack, we'll want to
    detect and handle stack overflow.
    
    I didn't enable it on x86_32.  We'd need to rework the double-fault
    code a bit and I'm concerned about running out of vmalloc virtual
    addresses under some workloads.
    
    This patch, by itself, will behave somewhat erratically when the
    stack overflows while RSP is still more than a few tens of bytes
    above the bottom of the stack.  Specifically, we'll get #PF and make
    it to no_context and them oops without reliably triggering a
    double-fault, and no_context doesn't know about stack overflows.
    The next patch will improve that case.
    
    Thank you to Nadav and Brian for helping me pay enough attention to
    the SDM to hopefully get this right.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/c88f3e2920b18e6cc621d772a04a62c06869037e.1470907718.git.luto@kernel.org
    [ Minor edits. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b70ca12dd389..907b4e4aeb5e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -292,12 +292,30 @@ DO_ERROR(X86_TRAP_NP,     SIGBUS,  "segment not present",	segment_not_present)
 DO_ERROR(X86_TRAP_SS,     SIGBUS,  "stack segment",		stack_segment)
 DO_ERROR(X86_TRAP_AC,     SIGBUS,  "alignment check",		alignment_check)
 
+#ifdef CONFIG_VMAP_STACK
+static void __noreturn handle_stack_overflow(const char *message,
+					     struct pt_regs *regs,
+					     unsigned long fault_address)
+{
+	printk(KERN_EMERG "BUG: stack guard page was hit at %p (stack is %p..%p)\n",
+		 (void *)fault_address, current->stack,
+		 (char *)current->stack + THREAD_SIZE - 1);
+	die(message, regs, 0);
+
+	/* Be absolutely certain we don't return. */
+	panic(message);
+}
+#endif
+
 #ifdef CONFIG_X86_64
 /* Runs on IST stack */
 dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 {
 	static const char str[] = "double fault";
 	struct task_struct *tsk = current;
+#ifdef CONFIG_VMAP_STACK
+	unsigned long cr2;
+#endif
 
 #ifdef CONFIG_X86_ESPFIX64
 	extern unsigned char native_irq_return_iret[];
@@ -332,6 +350,49 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	tsk->thread.error_code = error_code;
 	tsk->thread.trap_nr = X86_TRAP_DF;
 
+#ifdef CONFIG_VMAP_STACK
+	/*
+	 * If we overflow the stack into a guard page, the CPU will fail
+	 * to deliver #PF and will send #DF instead.  Similarly, if we
+	 * take any non-IST exception while too close to the bottom of
+	 * the stack, the processor will get a page fault while
+	 * delivering the exception and will generate a double fault.
+	 *
+	 * According to the SDM (footnote in 6.15 under "Interrupt 14 -
+	 * Page-Fault Exception (#PF):
+	 *
+	 *   Processors update CR2 whenever a page fault is detected. If a
+	 *   second page fault occurs while an earlier page fault is being
+	 *   deliv- ered, the faulting linear address of the second fault will
+	 *   overwrite the contents of CR2 (replacing the previous
+	 *   address). These updates to CR2 occur even if the page fault
+	 *   results in a double fault or occurs during the delivery of a
+	 *   double fault.
+	 *
+	 * The logic below has a small possibility of incorrectly diagnosing
+	 * some errors as stack overflows.  For example, if the IDT or GDT
+	 * gets corrupted such that #GP delivery fails due to a bad descriptor
+	 * causing #GP and we hit this condition while CR2 coincidentally
+	 * points to the stack guard page, we'll think we overflowed the
+	 * stack.  Given that we're going to panic one way or another
+	 * if this happens, this isn't necessarily worth fixing.
+	 *
+	 * If necessary, we could improve the test by only diagnosing
+	 * a stack overflow if the saved RSP points within 47 bytes of
+	 * the bottom of the stack: if RSP == tsk_stack + 48 and we
+	 * take an exception, the stack is already aligned and there
+	 * will be enough room SS, RSP, RFLAGS, CS, RIP, and a
+	 * possible error code, so a stack overflow would *not* double
+	 * fault.  With any less space left, exception delivery could
+	 * fail, and, as a practical matter, we've overflowed the
+	 * stack even if the actual trigger for the double fault was
+	 * something else.
+	 */
+	cr2 = read_cr2();
+	if ((unsigned long)task_stack_page(tsk) - 1 - cr2 < PAGE_SIZE)
+		handle_stack_overflow("kernel stack overflow (double-fault)", regs, cr2);
+#endif
+
 #ifdef CONFIG_DOUBLEFAULT
 	df_debug(regs, error_code);
 #endif

commit 186f43608a5c827f8284fe4559225b4dccaa49ef
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:56 2016 -0400

    x86/kernel: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed some implicit header usage that was fixed up accordingly.
    
    Note that some bool/obj-y instances remain since module.h is
    the header for some exception table entry stuff, and for things
    like __init_or_module (code that is tossed when MODULES=n).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-4-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 00f03d82e69a..b70ca12dd389 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -21,7 +21,7 @@
 #include <linux/kdebug.h>
 #include <linux/kgdb.h>
 #include <linux/kernel.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/ptrace.h>
 #include <linux/uprobes.h>
 #include <linux/string.h>

commit aaee8c3c5cce2d9107310dd9f3026b4f901d441c
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue May 24 15:54:04 2016 -0700

    x86/entry/traps: Don't force in_interrupt() to return true in IST handlers
    
    Forcing in_interrupt() to return true if we're not in a bona fide
    interrupt confuses the softirq code.  This fixes warnings like:
    
      NOHZ: local_softirq_pending 282
    
    ... which can happen when running things like selftests/x86.
    
    This will change perf's static percpu buffer usage in IST context.
    I think this is okay, and it's changing the behavior to match
    historical (pre-4.0) behavior.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Fixes: 959274753857 ("x86, traps: Track entry into and exit from IST context")
    Link: http://lkml.kernel.org/r/cdc215f94d118d691d73df35275022331156fb45.1464130360.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index d1590486204a..00f03d82e69a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -96,6 +96,12 @@ static inline void cond_local_irq_disable(struct pt_regs *regs)
 		local_irq_disable();
 }
 
+/*
+ * In IST context, we explicitly disable preemption.  This serves two
+ * purposes: it makes it much less likely that we would accidentally
+ * schedule in IST context and it will force a warning if we somehow
+ * manage to schedule by accident.
+ */
 void ist_enter(struct pt_regs *regs)
 {
 	if (user_mode(regs)) {
@@ -110,13 +116,7 @@ void ist_enter(struct pt_regs *regs)
 		rcu_nmi_enter();
 	}
 
-	/*
-	 * We are atomic because we're on the IST stack; or we're on
-	 * x86_32, in which case we still shouldn't schedule; or we're
-	 * on x86_64 and entered from user mode, in which case we're
-	 * still atomic unless ist_begin_non_atomic is called.
-	 */
-	preempt_count_add(HARDIRQ_OFFSET);
+	preempt_disable();
 
 	/* This code is a bit fragile.  Test it. */
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "ist_enter didn't work");
@@ -124,7 +124,7 @@ void ist_enter(struct pt_regs *regs)
 
 void ist_exit(struct pt_regs *regs)
 {
-	preempt_count_sub(HARDIRQ_OFFSET);
+	preempt_enable_no_resched();
 
 	if (!user_mode(regs))
 		rcu_nmi_exit();
@@ -155,7 +155,7 @@ void ist_begin_non_atomic(struct pt_regs *regs)
 	BUG_ON((unsigned long)(current_top_of_stack() -
 			       current_stack_pointer()) >= THREAD_SIZE);
 
-	preempt_count_sub(HARDIRQ_OFFSET);
+	preempt_enable_no_resched();
 }
 
 /**
@@ -165,7 +165,7 @@ void ist_begin_non_atomic(struct pt_regs *regs)
  */
 void ist_end_non_atomic(void)
 {
-	preempt_count_add(HARDIRQ_OFFSET);
+	preempt_disable();
 }
 
 static nokprobe_inline int

commit 35de5b0692aaa1f99803044526f2cc00ff864426
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Apr 26 12:23:24 2016 -0700

    x86/asm: Stop depending on ptrace.h in alternative.h
    
    alternative.h pulls in ptrace.h, which means that alternatives can't
    be used in anything referenced from ptrace.h, which is a mess.
    
    Break the dependency by pulling text patching helpers into their own
    header.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/99b93b13f2c9eb671f5c98bba4c2cbdc061293a2.1461698311.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 06cbe25861f1..d1590486204a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -51,6 +51,7 @@
 #include <asm/processor.h>
 #include <asm/debugreg.h>
 #include <linux/atomic.h>
+#include <asm/text-patching.h>
 #include <asm/ftrace.h>
 #include <asm/traps.h>
 #include <asm/desc.h>

commit ecc026bff6e8444c6b50dcde192e7acdaf42bf82
Merge: fa53c4893994 14ddde78c787
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 10:23:56 2016 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fpu updates from Ingo Molnar:
     "The biggest change in terms of impact is the changing of the FPU
      context switch model to 'eagerfpu' for all CPU types, via: commit
      58122bf1d856: "x86/fpu: Default eagerfpu=on on all CPUs"
    
      This makes all FPU saves and restores synchronous and makes the FPU
      code a lot more obvious to read.  In the next cycle, if this change is
      problem free, we'll remove the old lazy FPU restore code altogether.
    
      This change flushed out some old bugs, which should all be fixed by
      now, BYMMV"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fpu: Default eagerfpu=on on all CPUs
      x86/fpu: Speed up lazy FPU restores slightly
      x86/fpu: Fold fpu_copy() into fpu__copy()
      x86/fpu: Fix FNSAVE usage in eagerfpu mode
      x86/fpu: Fix math emulation in eager fpu mode

commit ba33ea811e1ff6726abb7f8f96df38c2d7b50304
Merge: e23604edac2a d05004944206
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 09:32:27 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "This is another big update. Main changes are:
    
       - lots of x86 system call (and other traps/exceptions) entry code
         enhancements.  In particular the complex parts of the 64-bit entry
         code have been migrated to C code as well, and a number of dusty
         corners have been refreshed.  (Andy Lutomirski)
    
       - vDSO special mapping robustification and general cleanups (Andy
         Lutomirski)
    
       - cpufeature refactoring, cleanups and speedups (Borislav Petkov)
    
       - lots of other changes ..."
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (64 commits)
      x86/cpufeature: Enable new AVX-512 features
      x86/entry/traps: Show unhandled signal for i386 in do_trap()
      x86/entry: Call enter_from_user_mode() with IRQs off
      x86/entry/32: Change INT80 to be an interrupt gate
      x86/entry: Improve system call entry comments
      x86/entry: Remove TIF_SINGLESTEP entry work
      x86/entry/32: Add and check a stack canary for the SYSENTER stack
      x86/entry/32: Simplify and fix up the SYSENTER stack #DB/NMI fixup
      x86/entry: Only allocate space for tss_struct::SYSENTER_stack if needed
      x86/entry: Vastly simplify SYSENTER TF (single-step) handling
      x86/entry/traps: Clear DR6 early in do_debug() and improve the comment
      x86/entry/traps: Clear TIF_BLOCKSTEP on all debug exceptions
      x86/entry/32: Restore FLAGS on SYSEXIT
      x86/entry/32: Filter NT and speed up AC filtering in SYSENTER
      x86/entry/compat: In SYSENTER, sink AC clearing below the existing FLAGS test
      selftests/x86: In syscall_nt, test NT|TF as well
      x86/asm-offsets: Remove PARAVIRT_enabled
      x86/entry/32: Introduce and use X86_BUG_ESPFIX instead of paravirt_enabled
      uprobes: __create_xol_area() must nullify xol_mapping.fault
      x86/cpufeature: Create a new synthetic cpu capability for machine check recovery
      ...

commit 10ee73865e9e4705ba8b3f4d6149e8e68d902bb7
Author: Jianyu Zhan <nasa4836@gmail.com>
Date:   Thu Mar 10 20:19:58 2016 +0800

    x86/entry/traps: Show unhandled signal for i386 in do_trap()
    
    Commit abd4f7505baf ("x86: i386-show-unhandled-signals-v3") did turn on
    the showing-unhandled-signal behaviour for i386 for some exception handlers,
    but for no reason do_trap() is left out (my naive guess is because turning it on
    for do_trap() would be too noisy since do_trap() is shared by several exceptions).
    
    And since the same commit make "show_unhandled_signals" a debug tunable(in
    /proc/sys/debug/exception-trace), and x86 by default turning it on.
    
    So it would be strange for i386 users who turing it on manually and expect
    seeing the unhandled signal output in log, but nothing.
    
    This patch turns it on for i386 in do_trap() as well.
    
    Signed-off-by: Jianyu Zhan <nasa4836@gmail.com>
    Reviewed-by: Jan Beulich <jbeulich@suse.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@suse.de
    Cc: dave.hansen@linux.intel.com
    Cc: heukelum@fastmail.fm
    Cc: jbeulich@novell.com
    Cc: jdike@addtoit.com
    Cc: joe@perches.com
    Cc: luto@kernel.org
    Link: http://lkml.kernel.org/r/1457612398-4568-1-git-send-email-nasa4836@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 5fae2d840e64..1baf08187ddf 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -248,7 +248,6 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	tsk->thread.error_code = error_code;
 	tsk->thread.trap_nr = trapnr;
 
-#ifdef CONFIG_X86_64
 	if (show_unhandled_signals && unhandled_signal(tsk, signr) &&
 	    printk_ratelimit()) {
 		pr_info("%s[%d] trap %s ip:%lx sp:%lx error:%lx",
@@ -257,7 +256,6 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 		print_vma_addr(" in ", regs->ip);
 		pr_cont("\n");
 	}
-#endif
 
 	force_sig_info(signr, info ?: SEND_SIG_PRIV, tsk);
 }

commit a798f091113ef4999277dbe0483d37d04fa35b2e
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 9 13:24:32 2016 -0800

    x86/entry/32: Change INT80 to be an interrupt gate
    
    We want all of the syscall entries to run with interrupts off so that
    we can efficiently run context tracking before enabling interrupts.
    
    This will regress int $0x80 performance on 32-bit kernels by a
    couple of cycles.  This shouldn't matter much -- int $0x80 is not a
    fast path.
    
    This effectively reverts:
    
      657c1eea0019 ("x86/entry/32: Fix entry_INT80_32() to expect interrupts to be on")
    
    ... and fixes the same issue differently.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/59b4f90c9ebfccd8c937305dbbbca680bc74b905.1457558566.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 49e2e775f507..5fae2d840e64 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -912,7 +912,7 @@ void __init trap_init(void)
 #endif
 
 #ifdef CONFIG_X86_32
-	set_system_trap_gate(IA32_SYSCALL_VECTOR, entry_INT80_32);
+	set_system_intr_gate(IA32_SYSCALL_VECTOR, entry_INT80_32);
 	set_bit(IA32_SYSCALL_VECTOR, used_vectors);
 #endif
 

commit 2a41aa4feb25af3ead60b740c43df80c576efea2
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 9 19:00:33 2016 -0800

    x86/entry/32: Add and check a stack canary for the SYSENTER stack
    
    The first instruction of the SYSENTER entry runs on its own tiny
    stack.  That stack can be used if a #DB or NMI is delivered before
    the SYSENTER prologue switches to a real stack.
    
    We have code in place to prevent us from overflowing the tiny stack.
    For added paranoia, add a canary to the stack and check it in
    do_debug() -- that way, if something goes wrong with the #DB logic,
    we'll eventually notice.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/6ff9a806f39098b166dc2c41c1db744df5272f29.1457578375.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b0ddb81926f9..49e2e775f507 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -713,6 +713,14 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	debug_stack_usage_dec();
 
 exit:
+#if defined(CONFIG_X86_32)
+	/*
+	 * This is the most likely code path that involves non-trivial use
+	 * of the SYSENTER stack.  Check that we haven't overrun it.
+	 */
+	WARN(this_cpu_read(cpu_tss.SYSENTER_stack_canary) != STACK_END_MAGIC,
+	     "Overran or corrupted SYSENTER stack\n");
+#endif
 	ist_exit(regs);
 }
 NOKPROBE_SYMBOL(do_debug);

commit f2b375756c839ff655a3e0b45e339f8fbf973093
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 9 19:00:30 2016 -0800

    x86/entry: Vastly simplify SYSENTER TF (single-step) handling
    
    Due to a blatant design error, SYSENTER doesn't clear TF (single-step).
    
    As a result, if a user does SYSENTER with TF set, we will single-step
    through the kernel until something clears TF.  There is absolutely
    nothing we can do to prevent this short of turning off SYSENTER [1].
    
    Simplify the handling considerably with two changes:
    
      1. We already sanitize EFLAGS in SYSENTER to clear NT and AC.  We can
         add TF to that list of flags to sanitize with no overhead whatsoever.
    
      2. Teach do_debug() to ignore single-step traps in the SYSENTER prologue.
    
    That's all we need to do.
    
    Don't get too excited -- our handling is still buggy on 32-bit
    kernels.  There's nothing wrong with the SYSENTER code itself, but
    the #DB prologue has a clever fixup for traps on the very first
    instruction of entry_SYSENTER_32, and the fixup doesn't work quite
    correctly.  The next two patches will fix that.
    
    [1] We could probably prevent it by forcing BTF on at all times and
        making sure we clear TF before any branches in the SYSENTER
        code.  Needless to say, this is a bad idea.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/a30d2ea06fe4b621fe6a9ef911b02c0f38feb6f2.1457578375.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7394be8b1a66..b0ddb81926f9 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -559,6 +559,29 @@ struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)
 NOKPROBE_SYMBOL(fixup_bad_iret);
 #endif
 
+static bool is_sysenter_singlestep(struct pt_regs *regs)
+{
+	/*
+	 * We don't try for precision here.  If we're anywhere in the region of
+	 * code that can be single-stepped in the SYSENTER entry path, then
+	 * assume that this is a useless single-step trap due to SYSENTER
+	 * being invoked with TF set.  (We don't know in advance exactly
+	 * which instructions will be hit because BTF could plausibly
+	 * be set.)
+	 */
+#ifdef CONFIG_X86_32
+	return (regs->ip - (unsigned long)__begin_SYSENTER_singlestep_region) <
+		(unsigned long)__end_SYSENTER_singlestep_region -
+		(unsigned long)__begin_SYSENTER_singlestep_region;
+#elif defined(CONFIG_IA32_EMULATION)
+	return (regs->ip - (unsigned long)entry_SYSENTER_compat) <
+		(unsigned long)__end_entry_SYSENTER_compat -
+		(unsigned long)entry_SYSENTER_compat;
+#else
+	return false;
+#endif
+}
+
 /*
  * Our handling of the processor debug registers is non-trivial.
  * We do not clear them on entry and exit from the kernel. Therefore
@@ -616,6 +639,18 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	 */
 	clear_tsk_thread_flag(tsk, TIF_BLOCKSTEP);
 
+	if (unlikely(!user_mode(regs) && (dr6 & DR_STEP) &&
+		     is_sysenter_singlestep(regs))) {
+		dr6 &= ~DR_STEP;
+		if (!dr6)
+			goto exit;
+		/*
+		 * else we might have gotten a single-step trap and hit a
+		 * watchpoint at the same time, in which case we should fall
+		 * through and handle the watchpoint.
+		 */
+	}
+
 	/*
 	 * If dr6 has no reason to give us about the origin of this trap,
 	 * then it's very likely the result of an icebp/int01 trap.
@@ -624,7 +659,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	if (!dr6 && user_mode(regs))
 		user_icebp = 1;
 
-	/* Catch kmemcheck conditions first of all! */
+	/* Catch kmemcheck conditions! */
 	if ((dr6 & DR_STEP) && kmemcheck_trap(regs))
 		goto exit;
 
@@ -659,14 +694,13 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 		goto exit;
 	}
 
-	/*
-	 * Single-stepping through system calls: ignore any exceptions in
-	 * kernel space, but re-enable TF when returning to user mode.
-	 *
-	 * We already checked v86 mode above, so we can check for kernel mode
-	 * by just checking the CPL of CS.
-	 */
-	if ((dr6 & DR_STEP) && !user_mode(regs)) {
+	if (WARN_ON_ONCE((dr6 & DR_STEP) && !user_mode(regs))) {
+		/*
+		 * Historical junk that used to handle SYSENTER single-stepping.
+		 * This should be unreachable now.  If we survive for a while
+		 * without anyone hitting this warning, we'll turn this into
+		 * an oops.
+		 */
 		tsk->thread.debugreg6 &= ~DR_STEP;
 		set_tsk_thread_flag(tsk, TIF_SINGLESTEP);
 		regs->flags &= ~X86_EFLAGS_TF;

commit 8bb5643686d2898bec181c407651cf84e77d413f
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 9 19:00:29 2016 -0800

    x86/entry/traps: Clear DR6 early in do_debug() and improve the comment
    
    Leaving any bits set in DR6 on return from a debug exception is
    asking for trouble.  Prevent it by writing zero right away and
    clarify the comment.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/3857676e1be8fb27db4b89bbb1e2052b7f435ff4.1457578375.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1af56217fb5d..7394be8b1a66 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -593,6 +593,18 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	ist_enter(regs);
 
 	get_debugreg(dr6, 6);
+	/*
+	 * The Intel SDM says:
+	 *
+	 *   Certain debug exceptions may clear bits 0-3. The remaining
+	 *   contents of the DR6 register are never cleared by the
+	 *   processor. To avoid confusion in identifying debug
+	 *   exceptions, debug handlers should clear the register before
+	 *   returning to the interrupted task.
+	 *
+	 * Keep it simple: clear DR6 immediately.
+	 */
+	set_debugreg(0, 6);
 
 	/* Filter out all the reserved bits which are preset to 1 */
 	dr6 &= ~DR6_RESERVED;
@@ -616,9 +628,6 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	if ((dr6 & DR_STEP) && kmemcheck_trap(regs))
 		goto exit;
 
-	/* DR6 may or may not be cleared by the CPU */
-	set_debugreg(0, 6);
-
 	/* Store the virtualized DR6 value */
 	tsk->thread.debugreg6 = dr6;
 

commit 81edd9f69a6fd214fdbe66b43de6aa1610c84c63
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 9 19:00:28 2016 -0800

    x86/entry/traps: Clear TIF_BLOCKSTEP on all debug exceptions
    
    The SDM says that debug exceptions clear BTF, and we need to keep
    TIF_BLOCKSTEP in sync with BTF.  Clear it unconditionally and improve
    the comment.
    
    I suspect that the fact that kmemcheck could cause TIF_BLOCKSTEP not
    to be cleared was just an oversight.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/fa86e55d196e6dde5b38839595bde2a292c52fdc.1457578375.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 410e8e2700c5..1af56217fb5d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -597,6 +597,13 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	/* Filter out all the reserved bits which are preset to 1 */
 	dr6 &= ~DR6_RESERVED;
 
+	/*
+	 * The SDM says "The processor clears the BTF flag when it
+	 * generates a debug exception."  Clear TIF_BLOCKSTEP to keep
+	 * TIF_BLOCKSTEP in sync with the hardware BTF flag.
+	 */
+	clear_tsk_thread_flag(tsk, TIF_BLOCKSTEP);
+
 	/*
 	 * If dr6 has no reason to give us about the origin of this trap,
 	 * then it's very likely the result of an icebp/int01 trap.
@@ -612,11 +619,6 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	/* DR6 may or may not be cleared by the CPU */
 	set_debugreg(0, 6);
 
-	/*
-	 * The processor cleared BTF, so don't mark that we need it set.
-	 */
-	clear_tsk_thread_flag(tsk, TIF_BLOCKSTEP);
-
 	/* Store the virtualized DR6 value */
 	tsk->thread.debugreg6 = dr6;
 

commit 548acf19234dbda5a52d5a8e7e205af46e9da840
Author: Tony Luck <tony.luck@intel.com>
Date:   Wed Feb 17 10:20:12 2016 -0800

    x86/mm: Expand the exception table logic to allow new handling options
    
    Huge amounts of help from  Andy Lutomirski and Borislav Petkov to
    produce this. Andy provided the inspiration to add classes to the
    exception table with a clever bit-squeezing trick, Boris pointed
    out how much cleaner it would all be if we just had a new field.
    
    Linus Torvalds blessed the expansion with:
    
      ' I'd rather not be clever in order to save just a tiny amount of space
        in the exception table, which isn't really criticial for anybody. '
    
    The third field is another relative function pointer, this one to a
    handler that executes the actions.
    
    We start out with three handlers:
    
     1: Legacy - just jumps the to fixup IP
     2: Fault - provide the trap number in %ax to the fixup code
     3: Cleaned up legacy for the uaccess error hack
    
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/f6af78fcbd348cf4939875cfda9c19689b5e50b8.1455732970.git.tony.luck@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ade185a46b1d..211c11c7bba4 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -199,7 +199,7 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 	}
 
 	if (!user_mode(regs)) {
-		if (!fixup_exception(regs)) {
+		if (!fixup_exception(regs, trapnr)) {
 			tsk->thread.error_code = error_code;
 			tsk->thread.trap_nr = trapnr;
 			die(str, regs, error_code);
@@ -453,7 +453,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 
 	tsk = current;
 	if (!user_mode(regs)) {
-		if (fixup_exception(regs))
+		if (fixup_exception(regs, X86_TRAP_GP))
 			return;
 
 		tsk->thread.error_code = error_code;
@@ -699,7 +699,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	conditional_sti(regs);
 
 	if (!user_mode(regs)) {
-		if (!fixup_exception(regs)) {
+		if (!fixup_exception(regs, trapnr)) {
 			task->thread.error_code = error_code;
 			task->thread.trap_nr = trapnr;
 			die(str, regs, error_code);

commit c6ab109f7e0eae3bae3bb10f8ddb0df67735c150
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Jan 24 14:38:09 2016 -0800

    x86/fpu: Speed up lazy FPU restores slightly
    
    If we have an FPU, there's no need to check CR0 for FPU emulation.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: yu-cheng yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/980004297e233c27066d54e71382c44cdd36ef7c.1453675014.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 87f80febf477..36a9c017540e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -752,7 +752,7 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 
 #ifdef CONFIG_MATH_EMULATION
-	if (read_cr0() & X86_CR0_EM) {
+	if (!boot_cpu_has(X86_FEATURE_FPU) && (read_cr0() & X86_CR0_EM)) {
 		struct math_emu_info info = { };
 
 		conditional_sti(regs);

commit 4ecd16ec7059390b430af34bd8bc3ca2b5dcef9a
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Jan 24 14:38:06 2016 -0800

    x86/fpu: Fix math emulation in eager fpu mode
    
    Systems without an FPU are generally old and therefore use lazy FPU
    switching. Unsurprisingly, math emulation in eager FPU mode is a
    bit buggy. Fix it.
    
    There were two bugs involving kernel code trying to use the FPU
    registers in eager mode even if they didn't exist and one BUG_ON()
    that was incorrect.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: yu-cheng yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/b4b8d112436bd6fab866e1b4011131507e8d7fbe.1453675014.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ade185a46b1d..87f80febf477 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -750,7 +750,6 @@ dotraplinkage void
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
-	BUG_ON(use_eager_fpu());
 
 #ifdef CONFIG_MATH_EMULATION
 	if (read_cr0() & X86_CR0_EM) {

commit d99e1bd175f4291ddb6e62b22bb5bdbe3976389a
Author: Alexander Kuleshov <kuleshovmail@gmail.com>
Date:   Mon Jan 25 20:41:46 2016 +0100

    x86/entry/traps: Refactor preemption and interrupt flag handling
    
    Make the preemption and interrupt flag handling more readable by
    removing preempt_conditional_sti() and preempt_conditional_cli()
    helpers and using preempt_disable() and
    preempt_enable_no_resched() instead.
    
    Rename contitional_sti() and conditional_cli() to the more
    understandable cond_local_irq_enable() and
    cond_local_irq_disable() respectively, while at it.
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Alexander Kuleshov <kuleshovmail@gmail.com>
    [ Boris: massage text. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/r/1453750913-4781-2-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ade185a46b1d..410e8e2700c5 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -83,30 +83,16 @@ gate_desc idt_table[NR_VECTORS] __page_aligned_bss;
 DECLARE_BITMAP(used_vectors, NR_VECTORS);
 EXPORT_SYMBOL_GPL(used_vectors);
 
-static inline void conditional_sti(struct pt_regs *regs)
+static inline void cond_local_irq_enable(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
 		local_irq_enable();
 }
 
-static inline void preempt_conditional_sti(struct pt_regs *regs)
-{
-	preempt_count_inc();
-	if (regs->flags & X86_EFLAGS_IF)
-		local_irq_enable();
-}
-
-static inline void conditional_cli(struct pt_regs *regs)
-{
-	if (regs->flags & X86_EFLAGS_IF)
-		local_irq_disable();
-}
-
-static inline void preempt_conditional_cli(struct pt_regs *regs)
+static inline void cond_local_irq_disable(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
 		local_irq_disable();
-	preempt_count_dec();
 }
 
 void ist_enter(struct pt_regs *regs)
@@ -286,7 +272,7 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
 			NOTIFY_STOP) {
-		conditional_sti(regs);
+		cond_local_irq_enable(regs);
 		do_trap(trapnr, signr, str, regs, error_code,
 			fill_trap_info(regs, signr, trapnr, &info));
 	}
@@ -368,7 +354,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	if (notify_die(DIE_TRAP, "bounds", regs, error_code,
 			X86_TRAP_BR, SIGSEGV) == NOTIFY_STOP)
 		return;
-	conditional_sti(regs);
+	cond_local_irq_enable(regs);
 
 	if (!user_mode(regs))
 		die("bounds", regs, error_code);
@@ -443,7 +429,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	struct task_struct *tsk;
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
-	conditional_sti(regs);
+	cond_local_irq_enable(regs);
 
 	if (v8086_mode(regs)) {
 		local_irq_enable();
@@ -517,9 +503,11 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	 * as we may switch to the interrupt stack.
 	 */
 	debug_stack_usage_inc();
-	preempt_conditional_sti(regs);
+	preempt_disable();
+	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, NULL);
-	preempt_conditional_cli(regs);
+	cond_local_irq_disable(regs);
+	preempt_enable_no_resched();
 	debug_stack_usage_dec();
 exit:
 	ist_exit(regs);
@@ -648,12 +636,14 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	debug_stack_usage_inc();
 
 	/* It's safe to allow irq's after DR6 has been saved */
-	preempt_conditional_sti(regs);
+	preempt_disable();
+	cond_local_irq_enable(regs);
 
 	if (v8086_mode(regs)) {
 		handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code,
 					X86_TRAP_DB);
-		preempt_conditional_cli(regs);
+		cond_local_irq_disable(regs);
+		preempt_enable_no_resched();
 		debug_stack_usage_dec();
 		goto exit;
 	}
@@ -673,7 +663,8 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	si_code = get_si_code(tsk->thread.debugreg6);
 	if (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)
 		send_sigtrap(tsk, regs, error_code, si_code);
-	preempt_conditional_cli(regs);
+	cond_local_irq_disable(regs);
+	preempt_enable_no_resched();
 	debug_stack_usage_dec();
 
 exit:
@@ -696,7 +687,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)
 		return;
-	conditional_sti(regs);
+	cond_local_irq_enable(regs);
 
 	if (!user_mode(regs)) {
 		if (!fixup_exception(regs)) {
@@ -743,7 +734,7 @@ do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 {
-	conditional_sti(regs);
+	cond_local_irq_enable(regs);
 }
 
 dotraplinkage void
@@ -756,7 +747,7 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 	if (read_cr0() & X86_CR0_EM) {
 		struct math_emu_info info = { };
 
-		conditional_sti(regs);
+		cond_local_irq_enable(regs);
 
 		info.regs = regs;
 		math_emulate(&info);
@@ -765,7 +756,7 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 #endif
 	fpu__restore(&current->thread.fpu); /* interrupts still off */
 #ifdef CONFIG_X86_32
-	conditional_sti(regs);
+	cond_local_irq_enable(regs);
 #endif
 }
 NOKPROBE_SYMBOL(do_device_not_available);

commit 1126cb4535c4ff172c37a412a6bd25d6b47a1901
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Wed Sep 2 16:31:29 2015 -0700

    x86/fpu/mpx: Rework MPX 'xstate' types
    
    MPX includes two separate "extended state components".  There is
    no real need to have an 'mpx_struct' because we never really
    manage the states together.
    
    We also separate out the actual data in 'mpx_bndcsr_state' from
    the padding.  We will shortly be checking the state sizes
    against our structures and need them to match.  For consistency,
    we also ensure to prefix these types with 'mpx_'.
    
    Lastly, we add some comments to mirror some of the descriptions
    in the Intel documents (SDM) of the various state components.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: dave@sr71.net
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20150902233129.384B73EB@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 0cd2ac5c0f28..ade185a46b1d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -361,7 +361,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 
 dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 {
-	const struct bndcsr *bndcsr;
+	const struct mpx_bndcsr *bndcsr;
 	siginfo_t *info;
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");

commit d91cab78133d33b1dfd3d3fa7167fcbf74fb5f99
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Wed Sep 2 16:31:26 2015 -0700

    x86/fpu: Rename XSAVE macros
    
    There are two concepts that have some confusing naming:
     1. Extended State Component numbers (currently called
        XFEATURE_BIT_*)
     2. Extended State Component masks (currently called XSTATE_*)
    
    The numbers are (currently) from 0-9.  State component 3 is the
    bounds registers for MPX, for instance.
    
    But when we want to enable "state component 3", we go set a bit
    in XCR0.  The bit we set is 1<<3.  We can check to see if a
    state component feature is enabled by looking at its bit.
    
    The current 'xfeature_bit's are at best xfeature bit _numbers_.
    Calling them bits is at best inconsistent with ending the enum
    list with 'XFEATURES_NR_MAX'.
    
    This patch renames the enum to be 'xfeature'.  These also
    happen to be what the Intel documentation calls a "state
    component".
    
    We also want to differentiate these from the "XSTATE_*" macros.
    The "XSTATE_*" macros are a mask, and we rename them to match.
    
    These macros are reasonably widely used so this patch is a
    wee bit big, but this really is just a rename.
    
    The only non-mechanical part of this is the
    
            s/XSTATE_EXTEND_MASK/XFEATURE_MASK_EXTEND/
    
    We need a better name for it, but that's another patch.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: dave@sr71.net
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20150902233126.38653250@viggo.jf.intel.com
    [ Ported to v4.3-rc1. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 346eec73f7db..0cd2ac5c0f28 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -384,7 +384,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	 * which is all zeros which indicates MPX was not
 	 * responsible for the exception.
 	 */
-	bndcsr = get_xsave_field_ptr(XSTATE_BNDCSR);
+	bndcsr = get_xsave_field_ptr(XFEATURE_MASK_BNDCSR);
 	if (!bndcsr)
 		goto exit_trap;
 

commit 5778077d03cb25aac9b6a428e18970642fc019e3
Merge: 65a99597f044 7e01ebffffed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 1 08:40:25 2015 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm changes from Ingo Molnar:
     "The biggest changes in this cycle were:
    
       - Revamp, simplify (and in some cases fix) Time Stamp Counter (TSC)
         primitives.  (Andy Lutomirski)
    
       - Add new, comprehensible entry and exit handlers written in C.
         (Andy Lutomirski)
    
       - vm86 mode cleanups and fixes.  (Brian Gerst)
    
       - 32-bit compat code cleanups.  (Brian Gerst)
    
      The amount of simplification in low level assembly code is already
      palpable:
    
         arch/x86/entry/entry_32.S                          | 130 +----
         arch/x86/entry/entry_64.S                          | 197 ++-----
    
      but more simplifications are planned.
    
      There's also the usual laudry mix of low level changes - see the
      changelog for details"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (83 commits)
      x86/asm: Drop repeated macro of X86_EFLAGS_AC definition
      x86/asm/msr: Make wrmsrl() a function
      x86/asm/delay: Introduce an MWAITX-based delay with a configurable timer
      x86/asm: Add MONITORX/MWAITX instruction support
      x86/traps: Weaken context tracking entry assertions
      x86/asm/tsc: Add rdtscll() merge helper
      selftests/x86: Add syscall_nt selftest
      selftests/x86: Disable sigreturn_64
      x86/vdso: Emit a GNU hash
      x86/entry: Remove do_notify_resume(), syscall_trace_leave(), and their TIF masks
      x86/entry/32: Migrate to C exit path
      x86/entry/32: Remove 32-bit syscall audit optimizations
      x86/vm86: Rename vm86->v86flags and v86mask
      x86/vm86: Rename vm86->vm86_info to user_vm86
      x86/vm86: Clean up vm86.h includes
      x86/vm86: Move the vm86 IRQ definitions to vm86.h
      x86/vm86: Use the normal pt_regs area for vm86
      x86/vm86: Eliminate 'struct kernel_vm86_struct'
      x86/vm86: Move fields from 'struct kernel_vm86_struct' to 'struct vm86'
      x86/vm86: Move vm86 fields out of 'thread_struct'
      ...

commit f0a97af83f6287357dcc100c859ec0066f164f32
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Aug 20 22:03:21 2015 -0700

    x86/traps: Weaken context tracking entry assertions
    
    We were asserting that we were all the way in CONTEXT_KERNEL
    when exception handlers were called.  While having this be true
    is, I think, a nice goal (or maybe a variant in which we assert
    that we're in CONTEXT_KERNEL or some new IRQ context), we're not
    quite there.
    
    In particular, if an IRQ interrupts the SYSCALL prologue and the
    IRQ handler in turn causes an exception, the exception entry
    will be called in RCU IRQ mode but with CONTEXT_USER.
    
    This is okay (nothing goes wrong), but until we fix up the
    SYSCALL prologue, we need to avoid warning.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/c81faf3916346c0e04346c441392974f49cd7184.1440133286.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 86a82eafb96f..45e8d9891fa3 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -112,7 +112,7 @@ static inline void preempt_conditional_cli(struct pt_regs *regs)
 void ist_enter(struct pt_regs *regs)
 {
 	if (user_mode(regs)) {
-		CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
+		rcu_lockdep_assert(rcu_is_watching(), "entry code didn't wake RCU");
 	} else {
 		/*
 		 * We might have interrupted pretty much anything.  In
@@ -282,7 +282,7 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 {
 	siginfo_t info;
 
-	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
+	rcu_lockdep_assert(rcu_is_watching(), "entry code didn't wake RCU");
 
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
 			NOTIFY_STOP) {
@@ -364,7 +364,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	const struct bndcsr *bndcsr;
 	siginfo_t *info;
 
-	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
+	rcu_lockdep_assert(rcu_is_watching(), "entry code didn't wake RCU");
 	if (notify_die(DIE_TRAP, "bounds", regs, error_code,
 			X86_TRAP_BR, SIGSEGV) == NOTIFY_STOP)
 		return;
@@ -442,7 +442,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk;
 
-	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
+	rcu_lockdep_assert(rcu_is_watching(), "entry code didn't wake RCU");
 	conditional_sti(regs);
 
 	if (v8086_mode(regs)) {
@@ -496,7 +496,7 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 		return;
 
 	ist_enter(regs);
-	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
+	rcu_lockdep_assert(rcu_is_watching(), "entry code didn't wake RCU");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
@@ -729,14 +729,14 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
+	rcu_lockdep_assert(rcu_is_watching(), "entry code didn't wake RCU");
 	math_error(regs, error_code, X86_TRAP_MF);
 }
 
 dotraplinkage void
 do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
+	rcu_lockdep_assert(rcu_is_watching(), "entry code didn't wake RCU");
 	math_error(regs, error_code, X86_TRAP_XF);
 }
 
@@ -749,7 +749,7 @@ do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
-	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
+	rcu_lockdep_assert(rcu_is_watching(), "entry code didn't wake RCU");
 	BUG_ON(use_eager_fpu());
 
 #ifdef CONFIG_MATH_EMULATION
@@ -775,7 +775,7 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 {
 	siginfo_t info;
 
-	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
+	rcu_lockdep_assert(rcu_is_watching(), "entry code didn't wake RCU");
 	local_irq_enable();
 
 	info.si_signo = SIGILL;

commit ba3e127ec105e790eeec4034d9769e018e4a1b54
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Jul 29 01:41:21 2015 -0400

    x86/vm86: Clean up vm86.h includes
    
    vm86.h was being implicitly included in alot of places via
    processor.h, which in turn got it from math_emu.h.  Break that
    chain and explicitly include vm86.h in all files that need it.
    Also remove unused vm86 field from math_emu_info.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1438148483-11932-7-git-send-email-brgerst@gmail.com
    [ Fixed build failure. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 8e65d8a9b8db..86a82eafb96f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -62,6 +62,7 @@
 #include <asm/fpu/xstate.h>
 #include <asm/trace/mpx.h>
 #include <asm/mpx.h>
+#include <asm/vm86.h>
 
 #ifdef CONFIG_X86_64
 #include <asm/x86_init.h>

commit f78f5b90c4ffa559e400c3919a02236101f29f3f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 18 15:50:02 2015 -0700

    rcu: Rename rcu_lockdep_assert() to RCU_LOCKDEP_WARN()
    
    This commit renames rcu_lockdep_assert() to RCU_LOCKDEP_WARN() for
    consistency with the WARN() series of macros.  This also requires
    inverting the sense of the conditional, which this commit also does.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index f5791927aa64..c5a5231d1d11 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -136,7 +136,7 @@ enum ctx_state ist_enter(struct pt_regs *regs)
 	preempt_count_add(HARDIRQ_OFFSET);
 
 	/* This code is a bit fragile.  Test it. */
-	rcu_lockdep_assert(rcu_is_watching(), "ist_enter didn't work");
+	RCU_LOCKDEP_WARN(!rcu_is_watching(), "ist_enter didn't work");
 
 	return prev_state;
 }

commit 8c84014f3bbb112d07e73f30a10ac8a3a72f8649
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Jul 3 12:44:32 2015 -0700

    x86/entry: Remove exception_enter() from most trap handlers
    
    On 64-bit kernels, we don't need it any more: we handle context
    tracking directly on entry from user mode and exit to user mode.
    
    On 32-bit kernels, we don't support context tracking at all, so
    these callbacks had no effect.
    
    Note: this doesn't change do_page_fault().  Before we do that,
    we need to make sure that there is no code that can page fault
    from kernel mode with CONTEXT_USER.  The 32-bit fast system call
    stack argument code is the only offender I'm aware of right now.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Denys Vlasenko <vda.linux@googlemail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/ae22f4dfebd799c916574089964592be218151f9.1435952415.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 2a783c4fe0e9..8e65d8a9b8db 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -108,13 +108,10 @@ static inline void preempt_conditional_cli(struct pt_regs *regs)
 	preempt_count_dec();
 }
 
-enum ctx_state ist_enter(struct pt_regs *regs)
+void ist_enter(struct pt_regs *regs)
 {
-	enum ctx_state prev_state;
-
 	if (user_mode(regs)) {
-		/* Other than that, we're just an exception. */
-		prev_state = exception_enter();
+		CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	} else {
 		/*
 		 * We might have interrupted pretty much anything.  In
@@ -123,32 +120,25 @@ enum ctx_state ist_enter(struct pt_regs *regs)
 		 * but we need to notify RCU.
 		 */
 		rcu_nmi_enter();
-		prev_state = CONTEXT_KERNEL;  /* the value is irrelevant. */
 	}
 
 	/*
-	 * We are atomic because we're on the IST stack (or we're on x86_32,
-	 * in which case we still shouldn't schedule).
-	 *
-	 * This must be after exception_enter(), because exception_enter()
-	 * won't do anything if in_interrupt() returns true.
+	 * We are atomic because we're on the IST stack; or we're on
+	 * x86_32, in which case we still shouldn't schedule; or we're
+	 * on x86_64 and entered from user mode, in which case we're
+	 * still atomic unless ist_begin_non_atomic is called.
 	 */
 	preempt_count_add(HARDIRQ_OFFSET);
 
 	/* This code is a bit fragile.  Test it. */
 	rcu_lockdep_assert(rcu_is_watching(), "ist_enter didn't work");
-
-	return prev_state;
 }
 
-void ist_exit(struct pt_regs *regs, enum ctx_state prev_state)
+void ist_exit(struct pt_regs *regs)
 {
-	/* Must be before exception_exit. */
 	preempt_count_sub(HARDIRQ_OFFSET);
 
-	if (user_mode(regs))
-		return exception_exit(prev_state);
-	else
+	if (!user_mode(regs))
 		rcu_nmi_exit();
 }
 
@@ -162,7 +152,7 @@ void ist_exit(struct pt_regs *regs, enum ctx_state prev_state)
  * a double fault, it can be safe to schedule.  ist_begin_non_atomic()
  * begins a non-atomic section within an ist_enter()/ist_exit() region.
  * Callers are responsible for enabling interrupts themselves inside
- * the non-atomic section, and callers must call is_end_non_atomic()
+ * the non-atomic section, and callers must call ist_end_non_atomic()
  * before ist_exit().
  */
 void ist_begin_non_atomic(struct pt_regs *regs)
@@ -289,7 +279,6 @@ NOKPROBE_SYMBOL(do_trap);
 static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 			  unsigned long trapnr, int signr)
 {
-	enum ctx_state prev_state = exception_enter();
 	siginfo_t info;
 
 	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
@@ -300,8 +289,6 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 		do_trap(trapnr, signr, str, regs, error_code,
 			fill_trap_info(regs, signr, trapnr, &info));
 	}
-
-	exception_exit(prev_state);
 }
 
 #define DO_ERROR(trapnr, signr, str, name)				\
@@ -353,7 +340,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	}
 #endif
 
-	ist_enter(regs);  /* Discard prev_state because we won't return. */
+	ist_enter(regs);
 	notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);
 
 	tsk->thread.error_code = error_code;
@@ -373,15 +360,13 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 
 dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 {
-	enum ctx_state prev_state;
 	const struct bndcsr *bndcsr;
 	siginfo_t *info;
 
-	prev_state = exception_enter();
 	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	if (notify_die(DIE_TRAP, "bounds", regs, error_code,
 			X86_TRAP_BR, SIGSEGV) == NOTIFY_STOP)
-		goto exit;
+		return;
 	conditional_sti(regs);
 
 	if (!user_mode(regs))
@@ -438,9 +423,8 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 		die("bounds", regs, error_code);
 	}
 
-exit:
-	exception_exit(prev_state);
 	return;
+
 exit_trap:
 	/*
 	 * This path out is for all the cases where we could not
@@ -450,36 +434,33 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	 * time..
 	 */
 	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, NULL);
-	exception_exit(prev_state);
 }
 
 dotraplinkage void
 do_general_protection(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk;
-	enum ctx_state prev_state;
 
-	prev_state = exception_enter();
 	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	conditional_sti(regs);
 
 	if (v8086_mode(regs)) {
 		local_irq_enable();
 		handle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);
-		goto exit;
+		return;
 	}
 
 	tsk = current;
 	if (!user_mode(regs)) {
 		if (fixup_exception(regs))
-			goto exit;
+			return;
 
 		tsk->thread.error_code = error_code;
 		tsk->thread.trap_nr = X86_TRAP_GP;
 		if (notify_die(DIE_GPF, "general protection fault", regs, error_code,
 			       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)
 			die("general protection fault", regs, error_code);
-		goto exit;
+		return;
 	}
 
 	tsk->thread.error_code = error_code;
@@ -495,16 +476,12 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	}
 
 	force_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);
-exit:
-	exception_exit(prev_state);
 }
 NOKPROBE_SYMBOL(do_general_protection);
 
 /* May run on IST stack. */
 dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
-	enum ctx_state prev_state;
-
 #ifdef CONFIG_DYNAMIC_FTRACE
 	/*
 	 * ftrace must be first, everything else may cause a recursive crash.
@@ -517,7 +494,7 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	if (poke_int3_handler(regs))
 		return;
 
-	prev_state = ist_enter(regs);
+	ist_enter(regs);
 	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
@@ -544,7 +521,7 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	preempt_conditional_cli(regs);
 	debug_stack_usage_dec();
 exit:
-	ist_exit(regs, prev_state);
+	ist_exit(regs);
 }
 NOKPROBE_SYMBOL(do_int3);
 
@@ -620,12 +597,11 @@ NOKPROBE_SYMBOL(fixup_bad_iret);
 dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk = current;
-	enum ctx_state prev_state;
 	int user_icebp = 0;
 	unsigned long dr6;
 	int si_code;
 
-	prev_state = ist_enter(regs);
+	ist_enter(regs);
 
 	get_debugreg(dr6, 6);
 
@@ -700,7 +676,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	debug_stack_usage_dec();
 
 exit:
-	ist_exit(regs, prev_state);
+	ist_exit(regs);
 }
 NOKPROBE_SYMBOL(do_debug);
 
@@ -752,23 +728,15 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-	enum ctx_state prev_state;
-
-	prev_state = exception_enter();
 	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	math_error(regs, error_code, X86_TRAP_MF);
-	exception_exit(prev_state);
 }
 
 dotraplinkage void
 do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-	enum ctx_state prev_state;
-
-	prev_state = exception_enter();
 	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	math_error(regs, error_code, X86_TRAP_XF);
-	exception_exit(prev_state);
 }
 
 dotraplinkage void
@@ -780,9 +748,6 @@ do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
-	enum ctx_state prev_state;
-
-	prev_state = exception_enter();
 	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	BUG_ON(use_eager_fpu());
 
@@ -794,7 +759,6 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 
 		info.regs = regs;
 		math_emulate(&info);
-		exception_exit(prev_state);
 		return;
 	}
 #endif
@@ -802,7 +766,6 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 #ifdef CONFIG_X86_32
 	conditional_sti(regs);
 #endif
-	exception_exit(prev_state);
 }
 NOKPROBE_SYMBOL(do_device_not_available);
 
@@ -810,9 +773,7 @@ NOKPROBE_SYMBOL(do_device_not_available);
 dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 {
 	siginfo_t info;
-	enum ctx_state prev_state;
 
-	prev_state = exception_enter();
 	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	local_irq_enable();
 
@@ -825,7 +786,6 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 		do_trap(X86_TRAP_IRET, SIGILL, "iret exception", regs, error_code,
 			&info);
 	}
-	exception_exit(prev_state);
 }
 #endif
 

commit 02fdcd5eac9d653d1addbd69b0c58d73650e1c00
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Jul 3 12:44:24 2015 -0700

    x86/traps, context_tracking: Assert that we're in CONTEXT_KERNEL in exception entries
    
    Other than the super-atomic exception entries, all exception
    entries are supposed to switch our context tracking state to
    CONTEXT_KERNEL. Assert that they do.  These assertions appear
    trivial at this point, as exception_enter() is the function
    responsible for switching context, but I'm planning on reworking
    x86's exception context tracking, and these assertions will help
    make sure that all of this code keeps working.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Denys Vlasenko <vda.linux@googlemail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/20fa1ee2d943233a184aaf96ff75394d3b34dfba.1435952415.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index f5791927aa64..2a783c4fe0e9 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -292,6 +292,8 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 	enum ctx_state prev_state = exception_enter();
 	siginfo_t info;
 
+	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
+
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
 			NOTIFY_STOP) {
 		conditional_sti(regs);
@@ -376,6 +378,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	siginfo_t *info;
 
 	prev_state = exception_enter();
+	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	if (notify_die(DIE_TRAP, "bounds", regs, error_code,
 			X86_TRAP_BR, SIGSEGV) == NOTIFY_STOP)
 		goto exit;
@@ -457,6 +460,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	enum ctx_state prev_state;
 
 	prev_state = exception_enter();
+	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	conditional_sti(regs);
 
 	if (v8086_mode(regs)) {
@@ -514,6 +518,7 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 		return;
 
 	prev_state = ist_enter(regs);
+	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
@@ -750,6 +755,7 @@ dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 	enum ctx_state prev_state;
 
 	prev_state = exception_enter();
+	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	math_error(regs, error_code, X86_TRAP_MF);
 	exception_exit(prev_state);
 }
@@ -760,6 +766,7 @@ do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 	enum ctx_state prev_state;
 
 	prev_state = exception_enter();
+	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	math_error(regs, error_code, X86_TRAP_XF);
 	exception_exit(prev_state);
 }
@@ -776,6 +783,7 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 	enum ctx_state prev_state;
 
 	prev_state = exception_enter();
+	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	BUG_ON(use_eager_fpu());
 
 #ifdef CONFIG_MATH_EMULATION
@@ -805,6 +813,7 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 	enum ctx_state prev_state;
 
 	prev_state = exception_enter();
+	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
 	local_irq_enable();
 
 	info.si_signo = SIGILL;

commit d70b3ef54ceaf1c7c92209f5a662a670d04cbed9
Merge: 650ec5a6bd5d 7ef3d7d58d9d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 17:59:09 2015 -0700

    Merge branch 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 core updates from Ingo Molnar:
     "There were so many changes in the x86/asm, x86/apic and x86/mm topics
      in this cycle that the topical separation of -tip broke down somewhat -
      so the result is a more traditional architecture pull request,
      collected into the 'x86/core' topic.
    
      The topics were still maintained separately as far as possible, so
      bisectability and conceptual separation should still be pretty good -
      but there were a handful of merge points to avoid excessive
      dependencies (and conflicts) that would have been poorly tested in the
      end.
    
      The next cycle will hopefully be much more quiet (or at least will
      have fewer dependencies).
    
      The main changes in this cycle were:
    
       * x86/apic changes, with related IRQ core changes: (Jiang Liu, Thomas
         Gleixner)
    
         - This is the second and most intrusive part of changes to the x86
           interrupt handling - full conversion to hierarchical interrupt
           domains:
    
              [IOAPIC domain]   -----
                                     |
              [MSI domain]      --------[Remapping domain] ----- [ Vector domain ]
                                     |   (optional)          |
              [HPET MSI domain] -----                        |
                                                             |
              [DMAR domain]     -----------------------------
                                                             |
              [Legacy domain]   -----------------------------
    
           This now reflects the actual hardware and allowed us to distangle
           the domain specific code from the underlying parent domain, which
           can be optional in the case of interrupt remapping.  It's a clear
           separation of functionality and removes quite some duct tape
           constructs which plugged the remap code between ioapic/msi/hpet
           and the vector management.
    
         - Intel IOMMU IRQ remapping enhancements, to allow direct interrupt
           injection into guests (Feng Wu)
    
       * x86/asm changes:
    
         - Tons of cleanups and small speedups, micro-optimizations.  This
           is in preparation to move a good chunk of the low level entry
           code from assembly to C code (Denys Vlasenko, Andy Lutomirski,
           Brian Gerst)
    
         - Moved all system entry related code to a new home under
           arch/x86/entry/ (Ingo Molnar)
    
         - Removal of the fragile and ugly CFI dwarf debuginfo annotations.
           Conversion to C will reintroduce many of them - but meanwhile
           they are only getting in the way, and the upstream kernel does
           not rely on them (Ingo Molnar)
    
         - NOP handling refinements. (Borislav Petkov)
    
       * x86/mm changes:
    
         - Big PAT and MTRR rework: making the code more robust and
           preparing to phase out exposing direct MTRR interfaces to drivers -
           in favor of using PAT driven interfaces (Toshi Kani, Luis R
           Rodriguez, Borislav Petkov)
    
         - New ioremap_wt()/set_memory_wt() interfaces to support
           Write-Through cached memory mappings.  This is especially
           important for good performance on NVDIMM hardware (Toshi Kani)
    
       * x86/ras changes:
    
         - Add support for deferred errors on AMD (Aravind Gopalakrishnan)
    
           This is an important RAS feature which adds hardware support for
           poisoned data.  That means roughly that the hardware marks data
           which it has detected as corrupted but wasn't able to correct, as
           poisoned data and raises an APIC interrupt to signal that in the
           form of a deferred error.  It is the OS's responsibility then to
           take proper recovery action and thus prolonge system lifetime as
           far as possible.
    
         - Add support for Intel "Local MCE"s: upcoming CPUs will support
           CPU-local MCE interrupts, as opposed to the traditional system-
           wide broadcasted MCE interrupts (Ashok Raj)
    
         - Misc cleanups (Borislav Petkov)
    
       * x86/platform changes:
    
         - Intel Atom SoC updates
    
      ... and lots of other cleanups, fixlets and other changes - see the
      shortlog and the Git log for details"
    
    * 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (222 commits)
      x86/hpet: Use proper hpet device number for MSI allocation
      x86/hpet: Check for irq==0 when allocating hpet MSI interrupts
      x86/mm/pat, drivers/infiniband/ipath: Use arch_phys_wc_add() and require PAT disabled
      x86/mm/pat, drivers/media/ivtv: Use arch_phys_wc_add() and require PAT disabled
      x86/platform/intel/baytrail: Add comments about why we disabled HPET on Baytrail
      genirq: Prevent crash in irq_move_irq()
      genirq: Enhance irq_data_to_desc() to support hierarchy irqdomain
      iommu, x86: Properly handle posted interrupts for IOMMU hotplug
      iommu, x86: Provide irq_remapping_cap() interface
      iommu, x86: Setup Posted-Interrupts capability for Intel iommu
      iommu, x86: Add cap_pi_support() to detect VT-d PI capability
      iommu, x86: Avoid migrating VT-d posted interrupts
      iommu, x86: Save the mode (posted or remapped) of an IRTE
      iommu, x86: Implement irq_set_vcpu_affinity for intel_ir_chip
      iommu: dmar: Provide helper to copy shared irte fields
      iommu: dmar: Extend struct irte for VT-d Posted-Interrupts
      iommu: Add new member capability to struct irq_remap_ops
      x86/asm/entry/64: Disentangle error_entry/exit gsbase/ebx/usermode code
      x86/asm/entry/32: Shorten __audit_syscall_entry() args preparation
      x86/asm/entry/32: Explain reloading of registers after __audit_syscall_entry()
      ...

commit e7126cf5f10aef1555cb99eddb7efff41bdf9566
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Sun Jun 7 11:37:03 2015 -0700

    x86/mpx: Trace #BR exceptions
    
    This is the first in a series of MPX tracing patches.
    I've found these extremely useful in the process of
    debugging applications and the kernel code itself.
    
    This exception hooks in to the bounds (#BR) exception
    very early and allows capturing the key registers which
    would influence how the exception is handled.
    
    Note that bndcfgu/bndstatus are technically still
    64-bit registers even in 32-bit mode.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20150607183703.5FE2619A@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index cffff669be3f..36cb15b7b367 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -60,6 +60,7 @@
 #include <asm/mach_traps.h>
 #include <asm/alternative.h>
 #include <asm/fpu/xstate.h>
+#include <asm/trace/mpx.h>
 #include <asm/mpx.h>
 
 #ifdef CONFIG_X86_64
@@ -399,6 +400,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	if (!bndcsr)
 		goto exit_trap;
 
+	trace_bounds_exception_mpx(bndcsr);
 	/*
 	 * The error code field of the BNDSTATUS register communicates status
 	 * information of a bound range exception #BR or operation involving

commit 46a6e0cf1c6665a8e867d8f7798d7a3538633f03
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Sun Jun 7 11:37:02 2015 -0700

    x86/mpx: Clean up the code by not passing a task pointer around when unnecessary
    
    The MPX code can only work on the current task.  You can not,
    for instance, enable MPX management in another process or
    thread. You can also not handle a fault for another process or
    thread.
    
    Despite this, we pass a task_struct around prolifically.  This
    patch removes all of the task struct passing for code paths
    where the code can not deal with another task (which turns out
    to be all of them).
    
    This has no functional changes.  It's just a cleanup.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: bp@alien8.de
    Link: http://lkml.kernel.org/r/20150607183702.6A81DA2C@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 42f15314b361..cffff669be3f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -371,7 +371,6 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 
 dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 {
-	struct task_struct *tsk = current;
 	enum ctx_state prev_state;
 	const struct bndcsr *bndcsr;
 	siginfo_t *info;
@@ -407,11 +406,11 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	 */
 	switch (bndcsr->bndstatus & MPX_BNDSTA_ERROR_CODE) {
 	case 2:	/* Bound directory has invalid entry. */
-		if (mpx_handle_bd_fault(tsk))
+		if (mpx_handle_bd_fault())
 			goto exit_trap;
 		break; /* Success, it was handled */
 	case 1: /* Bound violation. */
-		info = mpx_generate_siginfo(regs, tsk);
+		info = mpx_generate_siginfo(regs);
 		if (IS_ERR(info)) {
 			/*
 			 * We failed to decode the MPX instruction.  Act as if

commit a84eeaa96b36a03188e1423349669c108d3a4bd7
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Sun Jun 7 11:37:01 2015 -0700

    x86/mpx: Use the new get_xsave_field_ptr()API
    
    The MPX registers (bndcsr/bndcfgu/bndstatus) are not directly
    accessible via normal instructions.  They essentially act as
    if they were floating point registers and are saved/restored
    along with those registers.
    
    There are two main paths in the MPX code where we care about
    the contents of these registers:
    
            1. #BR (bounds) faults
            2. the prctl() code where we are setting MPX up
    
    Both of those paths _might_ be called without the FPU having
    been used.  That means that 'tsk->thread.fpu.state' might
    never be allocated.
    
    Also, fpu_save_init() is not preempt-safe.  It was a bug to
    call it without disabling preemption.  The new
    get_xsave_addr() calls unlazy_fpu() instead and properly
    disables preemption.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suresh Siddha <sbsiddha@gmail.com>
    Cc: bp@alien8.de
    Link: http://lkml.kernel.org/r/20150607183701.BC0D37CF@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a2510f230195..42f15314b361 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -59,6 +59,7 @@
 #include <asm/fixmap.h>
 #include <asm/mach_traps.h>
 #include <asm/alternative.h>
+#include <asm/fpu/xstate.h>
 #include <asm/mpx.h>
 
 #ifdef CONFIG_X86_64
@@ -371,9 +372,8 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk = current;
-	struct xregs_state *xsave_buf;
 	enum ctx_state prev_state;
-	struct bndcsr *bndcsr;
+	const struct bndcsr *bndcsr;
 	siginfo_t *info;
 
 	prev_state = exception_enter();
@@ -392,12 +392,11 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 
 	/*
 	 * We need to look at BNDSTATUS to resolve this exception.
-	 * It is not directly accessible, though, so we need to
-	 * do an xsave and then pull it out of the xsave buffer.
+	 * A NULL here might mean that it is in its 'init state',
+	 * which is all zeros which indicates MPX was not
+	 * responsible for the exception.
 	 */
-	copy_fpregs_to_fpstate(&tsk->thread.fpu);
-	xsave_buf = &(tsk->thread.fpu.state.xsave);
-	bndcsr = get_xsave_addr(xsave_buf, XSTATE_BNDCSR);
+	bndcsr = get_xsave_field_ptr(XSTATE_BNDCSR);
 	if (!bndcsr)
 		goto exit_trap;
 
@@ -408,11 +407,11 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	 */
 	switch (bndcsr->bndstatus & MPX_BNDSTA_ERROR_CODE) {
 	case 2:	/* Bound directory has invalid entry. */
-		if (mpx_handle_bd_fault(xsave_buf))
+		if (mpx_handle_bd_fault(tsk))
 			goto exit_trap;
 		break; /* Success, it was handled */
 	case 1: /* Bound violation. */
-		info = mpx_generate_siginfo(regs, xsave_buf);
+		info = mpx_generate_siginfo(regs, tsk);
 		if (IS_ERR(info)) {
 			/*
 			 * We failed to decode the MPX instruction.  Act as if

commit 9dda1658a9bd450d65da5153a2427955785d17c2
Merge: b72e7464e4cf a49976d14f78
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 8 20:48:01 2015 +0200

    Merge branch 'x86/asm' into x86/core, to prepare for new patch
    
    Collect all changes to arch/x86/entry/entry_64.S, before applying
    patch that changes most of the file.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b2502b418e63fcde0fe1857732a476b5aa3789b1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 8 08:42:03 2015 +0200

    x86/asm/entry: Untangle 'system_call' into two entry points: entry_SYSCALL_64 and entry_INT80_32
    
    The 'system_call' entry points differ starkly between native 32-bit and 64-bit
    kernels: on 32-bit kernels it defines the INT 0x80 entry point, while on
    64-bit it's the SYSCALL entry point.
    
    This is pretty confusing when looking at generic code, and it also obscures
    the nature of the entry point at the assembly level.
    
    So unangle this by splitting the name into its two uses:
    
            system_call (32) -> entry_INT80_32
            system_call (64) -> entry_SYSCALL_64
    
    As per the generic naming scheme for x86 system call entry points:
    
            entry_MNEMONIC_qualifier
    
    where 'qualifier' is one of _32, _64 or _compat.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index edf97986a53d..001ddac221a1 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -72,8 +72,7 @@ gate_desc debug_idt_table[NR_VECTORS] __page_aligned_bss;
 #else
 #include <asm/processor-flags.h>
 #include <asm/setup.h>
-
-asmlinkage int system_call(void);
+#include <asm/proto.h>
 #endif
 
 /* Must be page-aligned because the real IDT is used in a fixmap. */
@@ -997,7 +996,7 @@ void __init trap_init(void)
 #endif
 
 #ifdef CONFIG_X86_32
-	set_system_trap_gate(IA32_SYSCALL_VECTOR, &system_call);
+	set_system_trap_gate(IA32_SYSCALL_VECTOR, entry_INT80_32);
 	set_bit(IA32_SYSCALL_VECTOR, used_vectors);
 #endif
 

commit 2cd23553b488589f287457b7396470f5e3c40698
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 8 08:28:07 2015 +0200

    x86/asm/entry: Rename compat syscall entry points
    
    Rename the following system call entry points:
    
            ia32_cstar_target       -> entry_SYSCALL_compat
            ia32_syscall            -> entry_INT80_compat
    
    The generic naming scheme for x86 system call entry points is:
    
            entry_MNEMONIC_qualifier
    
    where 'qualifier' is one of _32, _64 or _compat.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 5e0791f9d3dc..edf97986a53d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -992,7 +992,7 @@ void __init trap_init(void)
 		set_bit(i, used_vectors);
 
 #ifdef CONFIG_IA32_EMULATION
-	set_system_intr_gate(IA32_SYSCALL_VECTOR, ia32_syscall);
+	set_system_intr_gate(IA32_SYSCALL_VECTOR, entry_INT80_compat);
 	set_bit(IA32_SYSCALL_VECTOR, used_vectors);
 #endif
 

commit c2f9b0af8bb4b2a5c020ae88efc9a624f59a7080
Merge: c8e56d20f2d1 243d657eaf54
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Jun 7 15:35:27 2015 +0200

    Merge branch 'x86/ras' into x86/core, to fix conflicts
    
    Conflicts:
            arch/x86/include/asm/irq_vectors.h
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e1884d69f643c743806ebb9bc9292863ef39e894
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 4 11:49:58 2015 +0200

    x86/fpu: Pass 'struct fpu' to fpu__restore()
    
    This cleans up the call sites and the function a bit,
    and also makes it more symmetric with the other high
    level FPU state handling functions.
    
    It's still only valid for the current task, as we copy
    to the FPU registers of the current CPU.
    
    No change in functionality.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 6f581c65c648..a2510f230195 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -803,7 +803,7 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 		return;
 	}
 #endif
-	fpu__restore(); /* interrupts still off */
+	fpu__restore(&current->thread.fpu); /* interrupts still off */
 #ifdef CONFIG_X86_32
 	conditional_sti(regs);
 #endif

commit c47ada305de3803517ae64aa50686f644c5456fa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 17:15:32 2015 +0200

    x86/fpu: Harmonize FPU register state types
    
    Use these consistent names:
    
        struct fregs_state           # was: i387_fsave_struct
        struct fxregs_state          # was: i387_fxsave_struct
        struct swregs_state          # was: i387_soft_struct
        struct xregs_state           # was: xsave_struct
        union  fpregs_state          # was: thread_xstate
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index cab397d0085f..6f581c65c648 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -371,7 +371,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk = current;
-	struct xsave_struct *xsave_buf;
+	struct xregs_state *xsave_buf;
 	enum ctx_state prev_state;
 	struct bndcsr *bndcsr;
 	siginfo_t *info;

commit e1cebad49c54e0241e101ebf63d5238fe1137749
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 09:29:38 2015 +0200

    x86/fpu: Factor out the exception error code handling code
    
    Factor out the FPU error code handling code from traps.c and fpu/internal.h
    and move them close to each other.
    
    Also convert the helper functions to 'struct fpu *', which further simplifies
    them.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 48dfcd9ed351..cab397d0085f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -708,8 +708,8 @@ NOKPROBE_SYMBOL(do_debug);
 static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 {
 	struct task_struct *task = current;
+	struct fpu *fpu = &task->thread.fpu;
 	siginfo_t info;
-	unsigned short err;
 	char *str = (trapnr == X86_TRAP_MF) ? "fpu exception" :
 						"simd exception";
 
@@ -717,8 +717,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 		return;
 	conditional_sti(regs);
 
-	if (!user_mode(regs))
-	{
+	if (!user_mode(regs)) {
 		if (!fixup_exception(regs)) {
 			task->thread.error_code = error_code;
 			task->thread.trap_nr = trapnr;
@@ -730,62 +729,20 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	/*
 	 * Save the info for the exception handler and clear the error.
 	 */
-	fpu__save(&task->thread.fpu);
-	task->thread.trap_nr = trapnr;
+	fpu__save(fpu);
+
+	task->thread.trap_nr	= trapnr;
 	task->thread.error_code = error_code;
-	info.si_signo = SIGFPE;
-	info.si_errno = 0;
-	info.si_addr = (void __user *)uprobe_get_trap_addr(regs);
-	if (trapnr == X86_TRAP_MF) {
-		unsigned short cwd, swd;
-		/*
-		 * (~cwd & swd) will mask out exceptions that are not set to unmasked
-		 * status.  0x3f is the exception bits in these regs, 0x200 is the
-		 * C1 reg you need in case of a stack fault, 0x040 is the stack
-		 * fault bit.  We should only be taking one exception at a time,
-		 * so if this combination doesn't produce any single exception,
-		 * then we have a bad program that isn't synchronizing its FPU usage
-		 * and it will suffer the consequences since we won't be able to
-		 * fully reproduce the context of the exception
-		 */
-		cwd = get_fpu_cwd(task);
-		swd = get_fpu_swd(task);
+	info.si_signo		= SIGFPE;
+	info.si_errno		= 0;
+	info.si_addr		= (void __user *)uprobe_get_trap_addr(regs);
 
-		err = swd & ~cwd;
-	} else {
-		/*
-		 * The SIMD FPU exceptions are handled a little differently, as there
-		 * is only a single status/control register.  Thus, to determine which
-		 * unmasked exception was caught we must mask the exception mask bits
-		 * at 0x1f80, and then use these to mask the exception bits at 0x3f.
-		 */
-		unsigned short mxcsr = get_fpu_mxcsr(task);
-		err = ~(mxcsr >> 7) & mxcsr;
-	}
+	info.si_code = fpu__exception_code(fpu, trapnr);
 
-	if (err & 0x001) {	/* Invalid op */
-		/*
-		 * swd & 0x240 == 0x040: Stack Underflow
-		 * swd & 0x240 == 0x240: Stack Overflow
-		 * User must clear the SF bit (0x40) if set
-		 */
-		info.si_code = FPE_FLTINV;
-	} else if (err & 0x004) { /* Divide by Zero */
-		info.si_code = FPE_FLTDIV;
-	} else if (err & 0x008) { /* Overflow */
-		info.si_code = FPE_FLTOVF;
-	} else if (err & 0x012) { /* Denormal, Underflow */
-		info.si_code = FPE_FLTUND;
-	} else if (err & 0x020) { /* Precision */
-		info.si_code = FPE_FLTRES;
-	} else {
-		/*
-		 * If we're using IRQ 13, or supposedly even some trap
-		 * X86_TRAP_MF implementations, it's possible
-		 * we get a spurious trap, which is not an error.
-		 */
+	/* Retry when we get spurious exceptions: */
+	if (!info.si_code)
 		return;
-	}
+
 	force_sig_info(SIGFPE, &info, task);
 }
 

commit 7366ed771f6ed95e4c4525c335722888a83b4b6c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 04:19:39 2015 +0200

    x86/fpu: Simplify FPU handling by embedding the fpstate in task_struct (again)
    
    So 6 years ago we made the FPU fpstate dynamically allocated:
    
      aa283f49276e ("x86, fpu: lazy allocation of FPU area - v5")
      61c4628b5386 ("x86, fpu: split FPU state from task struct - v5")
    
    In hindsight this was a mistake:
    
       - it complicated context allocation failure handling, such as:
    
                    /* kthread execs. TODO: cleanup this horror. */
                    if (WARN_ON(fpstate_alloc_init(fpu)))
                            force_sig(SIGKILL, tsk);
    
       - it caused us to enable irqs in fpu__restore():
    
                    local_irq_enable();
                    /*
                     * does a slab alloc which can sleep
                     */
                    if (fpstate_alloc_init(fpu)) {
                            /*
                             * ran out of memory!
                             */
                            do_group_exit(SIGKILL);
                            return;
                    }
                    local_irq_disable();
    
       - it (slightly) slowed down task creation/destruction by adding
         slab allocation/free pattens.
    
       - it made access to context contents (slightly) slower by adding
         one more pointer dereference.
    
    The motivation for the dynamic allocation was two-fold:
    
       - reduce memory consumption by non-FPU tasks
    
       - allocate and handle only the necessary amount of context for
         various XSAVE processors that have varying hardware frame
         sizes.
    
    These days, with glibc using SSE memcpy by default and GCC optimizing
    for SSE/AVX by default, the scope of FPU using apps on an x86 system is
    much larger than it was 6 years ago.
    
    For example on a freshly installed Fedora 21 desktop system, with a
    recent kernel, all non-kthread tasks have used the FPU shortly after
    bootup.
    
    Also, even modern embedded x86 CPUs try to support the latest vector
    instruction set - so they'll too often use the larger xstate frame
    sizes.
    
    So remove the dynamic allocation complication by embedding the FPU
    fpstate in task_struct again. This should make the FPU a lot more
    accessible to all sorts of atomic contexts.
    
    We could still optimize for the xstate frame size in the future,
    by moving the state structure to the last element of task_struct,
    and allocating only a part of that.
    
    This change is kept minimal by still keeping the ctx_alloc()/free()
    routines (that now do nothing substantial) - we'll remove them in
    the following patches.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index f028f1da3480..48dfcd9ed351 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -396,7 +396,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	 * do an xsave and then pull it out of the xsave buffer.
 	 */
 	copy_fpregs_to_fpstate(&tsk->thread.fpu);
-	xsave_buf = &(tsk->thread.fpu.state->xsave);
+	xsave_buf = &(tsk->thread.fpu.state.xsave);
 	bndcsr = get_xsave_addr(xsave_buf, XSTATE_BNDCSR);
 	if (!bndcsr)
 		goto exit_trap;

commit 4f83634710a1a7024b8acaa3b589dc5d8ca03ab0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 02:53:16 2015 +0200

    x86/fpu: Rename fpu_save_init() to copy_fpregs_to_fpstate()
    
    So fpu_save_init() is a historic name that got its name when the only
    way the FPU state was FNSAVE, which cleared (well, destroyed) the FPU
    state after saving it.
    
    Nowadays the name is misleading, because ever since the introduction of
    FXSAVE (and more modern FPU saving instructions) the 'we need to reload
    the FPU state' part is only true if there's a pending FPU exception [*],
    which is almost never the case.
    
    So rename it to copy_fpregs_to_fpstate() to make it clear what's
    happening. Also add a few comments about why we cannot keep registers
    in certain cases.
    
    Also clean up the control flow a bit, to make it more apparent when
    we are dropping/keeping FP registers, and to optimize the common
    case (of keeping fpregs) some more.
    
    [*] Probably not true anymore, modern instructions always leave the FPU
        state intact, even if exceptions are pending: because pending FP
        exceptions are posted on the next FP instruction, not asynchronously.
    
        They were truly asynchronous back in the IRQ13 case, and we had to
        synchronize with them, but that code is not working anymore: we don't
        have IRQ13 mapped in the IDT anymore.
    
        But a cleanup patch is obviously not the place to change subtle behavior.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a65586edbb57..f028f1da3480 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -395,7 +395,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	 * It is not directly accessible, though, so we need to
 	 * do an xsave and then pull it out of the xsave buffer.
 	 */
-	fpu_save_init(&tsk->thread.fpu);
+	copy_fpregs_to_fpstate(&tsk->thread.fpu);
 	xsave_buf = &(tsk->thread.fpu.state->xsave);
 	bndcsr = get_xsave_addr(xsave_buf, XSTATE_BNDCSR);
 	if (!bndcsr)

commit 78f7f1e54bac032b98956862a5bcf8c28ed22d07
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 02:54:44 2015 +0200

    x86/fpu: Rename fpu-internal.h to fpu/internal.h
    
    This unifies all the FPU related header files under a unified, hiearchical
    naming scheme:
    
     - asm/fpu/types.h:      FPU related data types, needed for 'struct task_struct',
                             widely included in almost all kernel code, and hence kept
                             as small as possible.
    
     - asm/fpu/api.h:        FPU related 'public' methods exported to other subsystems.
    
     - asm/fpu/internal.h:   FPU subsystem internal methods
    
     - asm/fpu/xsave.h:      XSAVE support internal methods
    
    (Also standardize the header guard in asm/fpu/internal.h.)
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 8abcd6a6f3dc..a65586edbb57 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -54,7 +54,7 @@
 #include <asm/ftrace.h>
 #include <asm/traps.h>
 #include <asm/desc.h>
-#include <asm/fpu-internal.h>
+#include <asm/fpu/internal.h>
 #include <asm/mce.h>
 #include <asm/fixmap.h>
 #include <asm/mach_traps.h>

commit 0c070595ceccb391100127a28ff837c50356ad67
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 23 17:57:24 2015 +0200

    x86/fpu: Use 'struct fpu' in fpu__save()
    
    Migrate this function to pure 'struct fpu' usage.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 22ad90a40dbf..8abcd6a6f3dc 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -730,7 +730,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	/*
 	 * Save the info for the exception handler and clear the error.
 	 */
-	fpu__save(task);
+	fpu__save(&task->thread.fpu);
 	task->thread.trap_nr = trapnr;
 	task->thread.error_code = error_code;
 	info.si_signo = SIGFPE;

commit 3a0aee4801d475b64a408539c01ec0d17d52192b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 13:16:47 2015 +0200

    x86/fpu: Rename math_state_restore() to fpu__restore()
    
    Move to the new fpu__*() namespace.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 63c7fc3677b4..22ad90a40dbf 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -846,7 +846,7 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 		return;
 	}
 #endif
-	math_state_restore(); /* interrupts still off */
+	fpu__restore(); /* interrupts still off */
 #ifdef CONFIG_X86_32
 	conditional_sti(regs);
 #endif

commit 93b90712c64ca2db4b39fcb2e7dffcf0d478468d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 12:50:13 2015 +0200

    x86/fpu: Move math_state_restore() to fpu/core.c
    
    It's another piece of FPU internals that is better off close to
    the other FPU internals.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 465b335e7491..63c7fc3677b4 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -826,48 +826,6 @@ asmlinkage __visible void __attribute__((weak)) smp_threshold_interrupt(void)
 {
 }
 
-/*
- * 'math_state_restore()' saves the current math information in the
- * old math state array, and gets the new ones from the current task
- *
- * Careful.. There are problems with IBM-designed IRQ13 behaviour.
- * Don't touch unless you *really* know how it works.
- *
- * Must be called with kernel preemption disabled (eg with local
- * local interrupts as in the case of do_device_not_available).
- */
-void math_state_restore(void)
-{
-	struct task_struct *tsk = current;
-
-	if (!tsk_used_math(tsk)) {
-		local_irq_enable();
-		/*
-		 * does a slab alloc which can sleep
-		 */
-		if (fpstate_alloc_init(tsk)) {
-			/*
-			 * ran out of memory!
-			 */
-			do_group_exit(SIGKILL);
-			return;
-		}
-		local_irq_disable();
-	}
-
-	/* Avoid __kernel_fpu_begin() right after __thread_fpu_begin() */
-	kernel_fpu_disable();
-	__thread_fpu_begin(tsk);
-	if (unlikely(restore_fpu_checking(tsk))) {
-		fpu_reset_state(tsk);
-		force_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);
-	} else {
-		tsk->thread.fpu.counter++;
-	}
-	kernel_fpu_enable();
-}
-EXPORT_SYMBOL_GPL(math_state_restore);
-
 dotraplinkage void
 do_device_not_available(struct pt_regs *regs, long error_code)
 {

commit f89e32e0a3df2f29d61fdc120ac62654ef267111
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 10:58:10 2015 +0200

    x86/fpu: Fix header file dependencies of fpu-internal.h
    
    Fix a minor header file dependency bug in asm/fpu-internal.h: it
    relies on i387.h but does not include it. All users of fpu-internal.h
    included it explicitly.
    
    Also remove unnecessary includes, to reduce compilation time.
    
    This also makes it easier to use it as a standalone header file
    for FPU internals, such as an upcoming C module in arch/x86/kernel/fpu/.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 231aa579d9cd..465b335e7491 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -54,7 +54,6 @@
 #include <asm/ftrace.h>
 #include <asm/traps.h>
 #include <asm/desc.h>
-#include <asm/i387.h>
 #include <asm/fpu-internal.h>
 #include <asm/mce.h>
 #include <asm/fixmap.h>

commit c0c2803dee21bef08ef5aacdf96fe2f1759ccc62
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 09:52:56 2015 +0200

    x86/fpu: Move thread_info::fpu_counter into thread_info::fpu.counter
    
    This field is kept separate from the main FPU state structure for
    no good reason.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index cf9c9627be19..231aa579d9cd 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -863,7 +863,7 @@ void math_state_restore(void)
 		fpu_reset_state(tsk);
 		force_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);
 	} else {
-		tsk->thread.fpu_counter++;
+		tsk->thread.fpu.counter++;
 	}
 	kernel_fpu_enable();
 }

commit 97185c95f7ab7f752473c34672dab0925758094b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 3 12:02:02 2015 +0200

    x86/fpu: Split an fpstate_alloc_init() function out of init_fpu()
    
    Most init_fpu() users don't want the register-saving aspect of the
    function, they are calling it for 'current' and when FPU registers
    are not allocated and initialized yet.
    
    Split out a simplified API that does just that (and add debug-checks
    for these conditions): fpstate_alloc_init().
    
    Use it where appropriate.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 12f29f9907cd..cf9c9627be19 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -846,7 +846,7 @@ void math_state_restore(void)
 		/*
 		 * does a slab alloc which can sleep
 		 */
-		if (init_fpu(tsk)) {
+		if (fpstate_alloc_init(tsk)) {
 			/*
 			 * ran out of memory!
 			 */

commit 0a7815515471a80379bfefc9f1913e0d8c87fbfb
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 3 10:58:52 2015 +0200

    x86/fpu: Rename unlazy_fpu() to fpu__save()
    
    This function is a misnomer on two levels:
    
    1) it doesn't really manipulate TS on modern CPUs anymore, its
       primary purpose is to save FPU state, used:
    
          - when executing fork()/clone(): to copy current FPU state
            to the child's FPU state.
    
          - when handling math exceptions: to generate the math error
            si_code in the signal frame.
    
    2) even on legacy CPUs it doesn't actually 'unlazy', if then
       it lazies the FPU state: as a side effect of the old FNSAVE
       instruction which clears (destroys) FPU state it's necessary
       to set CR0::TS.
    
    So rename it to fpu__save() to better reflect its purpose.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 324ab5247687..12f29f9907cd 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -731,7 +731,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	/*
 	 * Save the info for the exception handler and clear the error.
 	 */
-	unlazy_fpu(task);
+	fpu__save(task);
 	task->thread.trap_nr = trapnr;
 	task->thread.error_code = error_code;
 	info.si_signo = SIGFPE;

commit 51bb92843edcba5a58138cad25ced97923048add
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sat May 9 11:36:52 2015 -0400

    x86/asm/entry: Remove SYSCALL_VECTOR
    
    Use IA32_SYSCALL_VECTOR for both compat and native.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1431185813-15413-4-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 324ab5247687..5e0791f9d3dc 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -997,8 +997,8 @@ void __init trap_init(void)
 #endif
 
 #ifdef CONFIG_X86_32
-	set_system_trap_gate(SYSCALL_VECTOR, &system_call);
-	set_bit(SYSCALL_VECTOR, used_vectors);
+	set_system_trap_gate(IA32_SYSCALL_VECTOR, &system_call);
+	set_bit(IA32_SYSCALL_VECTOR, used_vectors);
 #endif
 
 	/*

commit 8cd161b1f755decd8b7f6c9c7144119281fe11a4
Author: Borislav Petkov <bp@suse.de>
Date:   Thu May 7 11:38:08 2015 +0200

    x86/traps: Remove superfluous weak definitions and dead code
    
    Those were leftovers of the x86 merge, see
    
      081f75bbdc86 ("traps: x86: make traps_32.c and traps_64.c equal")
    
    for example and are not needed now.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 68b1d5979a46..2768bb663f94 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -813,23 +813,6 @@ dotraplinkage void
 do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 {
 	conditional_sti(regs);
-#if 0
-	/* No need to warn about this any longer. */
-	pr_info("Ignoring P6 Local APIC Spurious Interrupt Bug...\n");
-#endif
-}
-
-asmlinkage __visible void __attribute__((weak)) smp_thermal_interrupt(void)
-{
-}
-
-asmlinkage __visible void __attribute__((weak)) smp_threshold_interrupt(void)
-{
-}
-
-asmlinkage __visible void __attribute__((weak))
-smp_deferred_error_interrupt(void)
-{
 }
 
 /*

commit 24fd78a81f6d3fe7f7a440c8629f9c52cd5f830e
Author: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
Date:   Wed May 6 06:58:56 2015 -0500

    x86/mce/amd: Introduce deferred error interrupt handler
    
    Deferred errors indicate error conditions that were not corrected, but
    require no action from S/W (or action is optional).These errors provide
    info about a latent UC MCE that can occur when a poisoned data is
    consumed by the processor.
    
    Processors that report these errors can be configured to generate APIC
    interrupts to notify OS about the error.
    
    Provide an interrupt handler in this patch so that OS can catch these
    errors as and when they happen. Currently, we simply log the errors and
    exit the handler as S/W action is not mandated.
    
    Signed-off-by: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: x86-ml <x86@kernel.org>
    Cc: linux-edac <linux-edac@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1430913538-1415-5-git-send-email-Aravind.Gopalakrishnan@amd.com
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 324ab5247687..68b1d5979a46 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -827,6 +827,11 @@ asmlinkage __visible void __attribute__((weak)) smp_threshold_interrupt(void)
 {
 }
 
+asmlinkage __visible void __attribute__((weak))
+smp_deferred_error_interrupt(void)
+{
+}
+
 /*
  * 'math_state_restore()' saves the current math information in the
  * old math state array, and gets the new ones from the current task

commit e95e7f627062be5e6ce971ce873e6234c91ffc50
Merge: 078838d56574 1524b745406a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 13:58:48 2015 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull NOHZ changes from Ingo Molnar:
     "This tree adds full dynticks support to KVM guests (support the
      disabling of the timer tick on the guest).  The main missing piece was
      the recognition of guest execution as RCU extended quiescent state and
      related changes"
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      kvm,rcu,nohz: use RCU extended quiescent state when running KVM guest
      context_tracking: Export context_tracking_user_enter/exit
      context_tracking: Run vtime_user_enter/exit only when state == CONTEXT_USER
      context_tracking: Add stub context_tracking_is_enabled
      context_tracking: Generalize context tracking APIs to support user and guest
      context_tracking: Rename context symbols to prepare for transition state
      ppc: Remove unused cpp symbols in kvm headers

commit 421ec9017f3a1f3f032d894c55c15870f3d474aa
Merge: 64f004a2ab68 7fc253e277ec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 13:24:23 2015 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fpu changes from Ingo Molnar:
     "Various x86 FPU handling cleanups, refactorings and fixes (Borislav
      Petkov, Oleg Nesterov, Rik van Riel)"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (21 commits)
      x86/fpu: Kill eager_fpu_init_bp()
      x86/fpu: Don't allocate fpu->state for swapper/0
      x86/fpu: Rename drop_init_fpu() to fpu_reset_state()
      x86/fpu: Fold __drop_fpu() into its sole user
      x86/fpu: Don't abuse drop_init_fpu() in flush_thread()
      x86/fpu: Use restore_init_xstate() instead of math_state_restore() on kthread exec
      x86/fpu: Introduce restore_init_xstate()
      x86/fpu: Document user_fpu_begin()
      x86/fpu: Factor out memset(xstate, 0) in fpu_finit() paths
      x86/fpu: Change xstateregs_get()/set() to use ->xsave.i387 rather than ->fxsave
      x86/fpu: Don't abuse FPU in kernel threads if use_eager_fpu()
      x86/fpu: Always allow FPU in interrupt if use_eager_fpu()
      x86/fpu: __kernel_fpu_begin() should clear fpu_owner_task even if use_eager_fpu()
      x86/fpu: Also check fpu_lazy_restore() when use_eager_fpu()
      x86/fpu: Use task_disable_lazy_fpu_restore() helper
      x86/fpu: Use an explicit if/else in switch_fpu_prepare()
      x86/fpu: Introduce task_disable_lazy_fpu_restore() helper
      x86/fpu: Move lazy restore functions up a few lines
      x86/fpu: Change math_error() to use unlazy_fpu(), kill (now) unused save_init_fpu()
      x86/fpu: Don't do __thread_fpu_end() if use_eager_fpu()
      ...

commit 55474c48b4726fd3914c1ec47fced0f931729979
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Mar 29 11:02:34 2015 +0200

    x86/asm/entry: Remove user_mode_ignore_vm86()
    
    user_mode_ignore_vm86() can be used instead of user_mode(), in
    places where we have already done a v8086_mode() security
    check of ptregs.
    
    But doing this check in the wrong place would be a bug that
    could result in security problems, and also the naming still
    isn't very clear.
    
    Furthermore, it only affects 32-bit kernels, while most
    development happens on 64-bit kernels.
    
    If we replace them with user_mode() checks then the cost is only
    a very minor increase in various slowpaths:
    
       text             data   bss     dec              hex    filename
       10573391         703562 1753042 13029995         c6d26b vmlinux.o.before
       10573423         703562 1753042 13030027         c6d28b vmlinux.o.after
    
    So lets get rid of this distinction once and for all.
    
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20150329090233.GA1963@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index c8eb469a94a4..6751c5c58eec 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -207,7 +207,7 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 		return -1;
 	}
 
-	if (!user_mode_ignore_vm86(regs)) {
+	if (!user_mode(regs)) {
 		if (!fixup_exception(regs)) {
 			tsk->thread.error_code = error_code;
 			tsk->thread.trap_nr = trapnr;
@@ -468,7 +468,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	}
 
 	tsk = current;
-	if (!user_mode_ignore_vm86(regs)) {
+	if (!user_mode(regs)) {
 		if (fixup_exception(regs))
 			goto exit;
 
@@ -685,7 +685,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	 * We already checked v86 mode above, so we can check for kernel mode
 	 * by just checking the CPL of CS.
 	 */
-	if ((dr6 & DR_STEP) && !user_mode_ignore_vm86(regs)) {
+	if ((dr6 & DR_STEP) && !user_mode(regs)) {
 		tsk->thread.debugreg6 &= ~DR_STEP;
 		set_tsk_thread_flag(tsk, TIF_SINGLESTEP);
 		regs->flags &= ~X86_EFLAGS_TF;

commit d74ef1118a146ae1135c8b26fff2bfee980fd7a4
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 18 18:33:35 2015 -0700

    x86/asm/entry: Replace some open-coded VM86 checks with v8086_mode() checks
    
    This allows us to remove some unnecessary ifdefs.  There should
    be no change to the generated code.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/f7e00f0d668e253abf0bd8bf36491ac47bd761ff.1426728647.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index d4e265952102..c8eb469a94a4 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -194,8 +194,7 @@ static nokprobe_inline int
 do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 		  struct pt_regs *regs,	long error_code)
 {
-#ifdef CONFIG_X86_32
-	if (regs->flags & X86_VM_MASK) {
+	if (v8086_mode(regs)) {
 		/*
 		 * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.
 		 * On nmi (interrupt 2), do_trap should not be called.
@@ -207,7 +206,7 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 		}
 		return -1;
 	}
-#endif
+
 	if (!user_mode_ignore_vm86(regs)) {
 		if (!fixup_exception(regs)) {
 			tsk->thread.error_code = error_code;
@@ -462,13 +461,11 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	prev_state = exception_enter();
 	conditional_sti(regs);
 
-#ifdef CONFIG_X86_32
-	if (regs->flags & X86_VM_MASK) {
+	if (v8086_mode(regs)) {
 		local_irq_enable();
 		handle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);
 		goto exit;
 	}
-#endif
 
 	tsk = current;
 	if (!user_mode_ignore_vm86(regs)) {
@@ -673,7 +670,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	/* It's safe to allow irq's after DR6 has been saved */
 	preempt_conditional_sti(regs);
 
-	if (regs->flags & X86_VM_MASK) {
+	if (v8086_mode(regs)) {
 		handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code,
 					X86_TRAP_DB);
 		preempt_conditional_cli(regs);

commit f39b6f0ef855a38ea17329a4e621ff97750dfcc2
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 18 18:33:33 2015 -0700

    x86/asm/entry: Change all 'user_mode_vm()' calls to 'user_mode()'
    
    user_mode_vm() and user_mode() are now the same.  Change all callers
    of user_mode_vm() to user_mode().
    
    The next patch will remove the definition of user_mode_vm.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/43b1f57f3df70df5a08b0925897c660725015554.1426728647.git.luto@kernel.org
    [ Merged to a more recent kernel. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1136961a679b..d4e265952102 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -112,7 +112,7 @@ enum ctx_state ist_enter(struct pt_regs *regs)
 {
 	enum ctx_state prev_state;
 
-	if (user_mode_vm(regs)) {
+	if (user_mode(regs)) {
 		/* Other than that, we're just an exception. */
 		prev_state = exception_enter();
 	} else {
@@ -146,7 +146,7 @@ void ist_exit(struct pt_regs *regs, enum ctx_state prev_state)
 	/* Must be before exception_exit. */
 	preempt_count_sub(HARDIRQ_OFFSET);
 
-	if (user_mode_vm(regs))
+	if (user_mode(regs))
 		return exception_exit(prev_state);
 	else
 		rcu_nmi_exit();
@@ -158,7 +158,7 @@ void ist_exit(struct pt_regs *regs, enum ctx_state prev_state)
  *
  * IST exception handlers normally cannot schedule.  As a special
  * exception, if the exception interrupted userspace code (i.e.
- * user_mode_vm(regs) would return true) and the exception was not
+ * user_mode(regs) would return true) and the exception was not
  * a double fault, it can be safe to schedule.  ist_begin_non_atomic()
  * begins a non-atomic section within an ist_enter()/ist_exit() region.
  * Callers are responsible for enabling interrupts themselves inside
@@ -167,7 +167,7 @@ void ist_exit(struct pt_regs *regs, enum ctx_state prev_state)
  */
 void ist_begin_non_atomic(struct pt_regs *regs)
 {
-	BUG_ON(!user_mode_vm(regs));
+	BUG_ON(!user_mode(regs));
 
 	/*
 	 * Sanity check: we need to be on the normal thread stack.  This
@@ -384,7 +384,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 		goto exit;
 	conditional_sti(regs);
 
-	if (!user_mode_vm(regs))
+	if (!user_mode(regs))
 		die("bounds", regs, error_code);
 
 	if (!cpu_feature_enabled(X86_FEATURE_MPX)) {
@@ -587,7 +587,7 @@ struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)
 	/* Copy the remainder of the stack from the current stack. */
 	memmove(new_stack, s, offsetof(struct bad_iret_stack, regs.ip));
 
-	BUG_ON(!user_mode_vm(&new_stack->regs));
+	BUG_ON(!user_mode(&new_stack->regs));
 	return new_stack;
 }
 NOKPROBE_SYMBOL(fixup_bad_iret);
@@ -637,7 +637,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	 * then it's very likely the result of an icebp/int01 trap.
 	 * User wants a sigtrap for that.
 	 */
-	if (!dr6 && user_mode_vm(regs))
+	if (!dr6 && user_mode(regs))
 		user_icebp = 1;
 
 	/* Catch kmemcheck conditions first of all! */
@@ -721,7 +721,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 		return;
 	conditional_sti(regs);
 
-	if (!user_mode_vm(regs))
+	if (!user_mode(regs))
 	{
 		if (!fixup_exception(regs)) {
 			task->thread.error_code = error_code;

commit ae60f0710ae6b33092267ef8ac853c498f6d3e5d
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 18 18:33:31 2015 -0700

    x86/asm/entry: Use user_mode_ignore_vm86() where appropriate
    
    A few of the user_mode() checks in traps.c are immediately after
    explicit checks for vm86 mode.  Change them to user_mode_ignore_vm86().
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/0b324d5b75c3402be07f8d3c6245ed7f4995029e.1426728647.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 277341128b47..1136961a679b 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -208,7 +208,7 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 		return -1;
 	}
 #endif
-	if (!user_mode(regs)) {
+	if (!user_mode_ignore_vm86(regs)) {
 		if (!fixup_exception(regs)) {
 			tsk->thread.error_code = error_code;
 			tsk->thread.trap_nr = trapnr;
@@ -471,7 +471,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 #endif
 
 	tsk = current;
-	if (!user_mode(regs)) {
+	if (!user_mode_ignore_vm86(regs)) {
 		if (fixup_exception(regs))
 			goto exit;
 
@@ -688,7 +688,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	 * We already checked v86 mode above, so we can check for kernel mode
 	 * by just checking the CPL of CS.
 	 */
-	if ((dr6 & DR_STEP) && !user_mode(regs)) {
+	if ((dr6 & DR_STEP) && !user_mode_ignore_vm86(regs)) {
 		tsk->thread.debugreg6 &= ~DR_STEP;
 		set_tsk_thread_flag(tsk, TIF_SINGLESTEP);
 		regs->flags &= ~X86_EFLAGS_TF;

commit e4518ab90f63cd5bea3ddc15341162debc34442b
Merge: c56716af8d27 bc465aa9d045
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Mar 23 11:13:15 2015 +0100

    Merge tag 'v4.0-rc5' into x86/asm, to resolve conflicts
    
    Conflicts:
            arch/x86/kernel/entry_64.S
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b85e67d1483c72b77d1bdc265aa8ba91590794c1
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Mar 16 10:21:55 2015 +0100

    x86/fpu: Rename drop_init_fpu() to fpu_reset_state()
    
    Call it what it does and in accordance with the context where it is
    used: we reset the FPU state either because we were unable to restore it
    from the one saved in the task or because we simply want to reset it.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7ee7369d5aec..edf66c066da9 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -863,7 +863,7 @@ void math_state_restore(void)
 	kernel_fpu_disable();
 	__thread_fpu_begin(tsk);
 	if (unlikely(restore_fpu_checking(tsk))) {
-		drop_init_fpu(tsk);
+		fpu_reset_state(tsk);
 		force_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);
 	} else {
 		tsk->thread.fpu_counter++;

commit eda2360ad18b7cde87728fad85c6735a52c2576e
Merge: 1d23c4518b1f bc465aa9d045
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Mar 23 10:13:36 2015 +0100

    Merge tag 'v4.0-rc5' into x86/fpu, to prevent conflicts
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 394838c96013ba414a24ffe7a2a593a9154daadf
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon Mar 9 17:42:31 2015 -0700

    x86/asm/entry/32: Fix user_mode() misuses
    
    The one in do_debug() is probably harmless, but better safe than sorry.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: <stable@vger.kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/d67deaa9df5458363623001f252d1aee3215d014.1425948056.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 9d2073e2ecc9..4ff5d162ff9f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -384,7 +384,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 		goto exit;
 	conditional_sti(regs);
 
-	if (!user_mode(regs))
+	if (!user_mode_vm(regs))
 		die("bounds", regs, error_code);
 
 	if (!cpu_feature_enabled(X86_FEATURE_MPX)) {
@@ -637,7 +637,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	 * then it's very likely the result of an icebp/int01 trap.
 	 * User wants a sigtrap for that.
 	 */
-	if (!dr6 && user_mode(regs))
+	if (!dr6 && user_mode_vm(regs))
 		user_icebp = 1;
 
 	/* Catch kmemcheck conditions first of all! */

commit c467ea763fd5d8795b7d1b5a78eb94b6ad8f66ad
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Mar 4 18:06:33 2015 +0100

    context_tracking: Rename context symbols to prepare for transition state
    
    Current context tracking symbols are designed to express living state.
    As such they are prefixed with "IN_": IN_USER, IN_KERNEL.
    
    Now we are going to use these symbols to also express state transitions
    such as context_tracking_enter(IN_USER) or context_tracking_exit(IN_USER).
    But while the "IN_" prefix works well to express entering a context, it's
    confusing to depict a context exit: context_tracking_exit(IN_USER)
    could mean two things:
            1) We are exiting the current context to enter user context.
            2) We are exiting the user context
    We want 2) but the reviewer may be confused and understand 1)
    
    So lets disambiguate these symbols and rename them to CONTEXT_USER and
    CONTEXT_KERNEL.
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Will deacon <will.deacon@arm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 9d2073e2ecc9..756f74eed35d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -123,7 +123,7 @@ enum ctx_state ist_enter(struct pt_regs *regs)
 		 * but we need to notify RCU.
 		 */
 		rcu_nmi_enter();
-		prev_state = IN_KERNEL;  /* the value is irrelevant. */
+		prev_state = CONTEXT_KERNEL;  /* the value is irrelevant. */
 	}
 
 	/*

commit a7fcf28d431ef70afaa91496e64e16dc51dccec4
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Mar 6 17:50:19 2015 -0800

    x86/asm/entry: Replace this_cpu_sp0() with current_top_of_stack() and fix it on x86_32
    
    I broke 32-bit kernels.  The implementation of sp0 was correct
    as far as I can tell, but sp0 was much weirder on x86_32 than I
    realized.  It has the following issues:
    
     - Init's sp0 is inconsistent with everything else's: non-init tasks
       are offset by 8 bytes.  (I have no idea why, and the comment is unhelpful.)
    
     - vm86 does crazy things to sp0.
    
    Fix it up by replacing this_cpu_sp0() with
    current_top_of_stack() and using a new percpu variable to track
    the top of the stack on x86_32.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 75182b1632a8 ("x86/asm/entry: Switch all C consumers of kernel_stack to this_cpu_sp0()")
    Link: http://lkml.kernel.org/r/d09dbe270883433776e0cbee3c7079433349e96d.1425692936.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index fa290586ed37..081252c44cde 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -174,8 +174,8 @@ void ist_begin_non_atomic(struct pt_regs *regs)
 	 * will catch asm bugs and any attempt to use ist_preempt_enable
 	 * from double_fault.
 	 */
-	BUG_ON((unsigned long)(this_cpu_sp0() - current_stack_pointer()) >=
-	       THREAD_SIZE);
+	BUG_ON((unsigned long)(current_top_of_stack() -
+			       current_stack_pointer()) >= THREAD_SIZE);
 
 	preempt_count_sub(HARDIRQ_OFFSET);
 }

commit 75182b1632a89f12540baa1806a7c5c180db620c
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:03 2015 -0800

    x86/asm/entry: Switch all C consumers of kernel_stack to this_cpu_sp0()
    
    This will make modifying the semantics of kernel_stack easier.
    
    The change to ist_begin_non_atomic() is necessary because sp0 no
    longer points to the same THREAD_SIZE-aligned region as RSP;
    it's one byte too high for that.  At Denys' suggestion, rather
    than offsetting it, just check explicitly that we're in the
    correct range ending at sp0.  This has the added benefit that we
    no longer assume that the thread stack is aligned to
    THREAD_SIZE.
    
    Suggested-by: Denys Vlasenko <dvlasenk@redhat.com>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/ef8254ad414cbb8034c9a56396eeb24f5dd5b0de.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 9965bd1916db..fa290586ed37 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -174,8 +174,8 @@ void ist_begin_non_atomic(struct pt_regs *regs)
 	 * will catch asm bugs and any attempt to use ist_preempt_enable
 	 * from double_fault.
 	 */
-	BUG_ON(((current_stack_pointer() ^ this_cpu_read_stable(kernel_stack))
-		& ~(THREAD_SIZE - 1)) != 0);
+	BUG_ON((unsigned long)(this_cpu_sp0() - current_stack_pointer()) >=
+	       THREAD_SIZE);
 
 	preempt_count_sub(HARDIRQ_OFFSET);
 }

commit 5eca7453d61003bf886992388f8cb407e6f0d051
Author: Wang Nan <wangnan0@huawei.com>
Date:   Fri Feb 27 12:19:49 2015 +0800

    x86/traps: Separate set_intr_gate() and clean up early_trap_init()
    
    As early_trap_init() doesn't use IST, replace
    set_intr_gate_ist() and set_system_intr_gate_ist() with their
    standard counterparts.
    
    set_intr_gate() requires a trace_debug symbol which we don't
    have and won't use. This patch separates set_intr_gate() into two
    parts, and uses base version in early_trap_init().
    
    Reported-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Cc: <dave.hansen@linux.intel.com>
    Cc: <lizefan@huawei.com>
    Cc: <masami.hiramatsu.pt@hitachi.com>
    Cc: <oleg@redhat.com>
    Cc: <rostedt@goodmis.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1425010789-13714-1-git-send-email-wangnan0@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 42819886be0c..9965bd1916db 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -926,16 +926,20 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 void __init early_trap_init(void)
 {
 	/*
-	 * Don't set ist to DEBUG_STACK as it doesn't work until TSS is
-	 * ready in cpu_init() <-- trap_init(). Before trap_init(), CPU
-	 * runs at ring 0 so it is impossible to hit an invalid stack.
-	 * Using the original stack works well enough at this early
-	 * stage. DEBUG_STACK will be equipped after cpu_init() in
+	 * Don't use IST to set DEBUG_STACK as it doesn't work until TSS
+	 * is ready in cpu_init() <-- trap_init(). Before trap_init(),
+	 * CPU runs at ring 0 so it is impossible to hit an invalid
+	 * stack.  Using the original stack works well enough at this
+	 * early stage. DEBUG_STACK will be equipped after cpu_init() in
 	 * trap_init().
+	 *
+	 * We don't need to set trace_idt_table like set_intr_gate(),
+	 * since we don't have trace_debug and it will be reset to
+	 * 'debug' in trap_init() by set_intr_gate_ist().
 	 */
-	set_intr_gate_ist(X86_TRAP_DB, &debug, 0);
+	set_intr_gate_notrace(X86_TRAP_DB, debug);
 	/* int3 can be called from all */
-	set_system_intr_gate_ist(X86_TRAP_BP, &int3, 0);
+	set_system_intr_gate(X86_TRAP_BP, &int3);
 #ifdef CONFIG_X86_32
 	set_intr_gate(X86_TRAP_PF, page_fault);
 #endif
@@ -1015,7 +1019,7 @@ void __init trap_init(void)
 
 	/*
 	 * X86_TRAP_DB and X86_TRAP_BP have been set
-	 * in early_trap_init(). However, DEBUG_STACK works only after
+	 * in early_trap_init(). However, ITS works only after
 	 * cpu_init() loads TSS. See comments in early_trap_init().
 	 */
 	set_intr_gate_ist(X86_TRAP_DB, &debug, DEBUG_STACK);

commit b4d8327024637cb2a1f7910dcb5d0ad7a096f473
Author: Wang Nan <wangnan0@huawei.com>
Date:   Thu Feb 26 13:49:39 2015 +0800

    x86/traps: Enable DEBUG_STACK after cpu_init() for TRAP_DB/BP
    
    Before this patch early_trap_init() installs DEBUG_STACK for
    X86_TRAP_BP and X86_TRAP_DB. However, DEBUG_STACK doesn't work
    correctly until cpu_init() <-- trap_init().
    
    This patch passes 0 to set_intr_gate_ist() and
    set_system_intr_gate_ist() instead of DEBUG_STACK to let it use
    same stack as kernel, and installs DEBUG_STACK for them in
    trap_init().
    
    As core runs at ring 0 between early_trap_init() and
    trap_init(), there is no chance to get a bad stack before
    trap_init().
    
    As NMI is also enabled in trap_init(), we don't need to care
    about is_debug_stack() and related things used in
    arch/x86/kernel/nmi.c.
    
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: <dave.hansen@linux.intel.com>
    Cc: <lizefan@huawei.com>
    Cc: <luto@amacapital.net>
    Cc: <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/1424929779-13174-1-git-send-email-wangnan0@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 9d2073e2ecc9..42819886be0c 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -925,9 +925,17 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 /* Set of traps needed for early debugging. */
 void __init early_trap_init(void)
 {
-	set_intr_gate_ist(X86_TRAP_DB, &debug, DEBUG_STACK);
+	/*
+	 * Don't set ist to DEBUG_STACK as it doesn't work until TSS is
+	 * ready in cpu_init() <-- trap_init(). Before trap_init(), CPU
+	 * runs at ring 0 so it is impossible to hit an invalid stack.
+	 * Using the original stack works well enough at this early
+	 * stage. DEBUG_STACK will be equipped after cpu_init() in
+	 * trap_init().
+	 */
+	set_intr_gate_ist(X86_TRAP_DB, &debug, 0);
 	/* int3 can be called from all */
-	set_system_intr_gate_ist(X86_TRAP_BP, &int3, DEBUG_STACK);
+	set_system_intr_gate_ist(X86_TRAP_BP, &int3, 0);
 #ifdef CONFIG_X86_32
 	set_intr_gate(X86_TRAP_PF, page_fault);
 #endif
@@ -1005,6 +1013,15 @@ void __init trap_init(void)
 	 */
 	cpu_init();
 
+	/*
+	 * X86_TRAP_DB and X86_TRAP_BP have been set
+	 * in early_trap_init(). However, DEBUG_STACK works only after
+	 * cpu_init() loads TSS. See comments in early_trap_init().
+	 */
+	set_intr_gate_ist(X86_TRAP_DB, &debug, DEBUG_STACK);
+	/* int3 can be called from all */
+	set_system_intr_gate_ist(X86_TRAP_BP, &int3, DEBUG_STACK);
+
 	x86_init.irqs.trap_init();
 
 #ifdef CONFIG_X86_64

commit f353e612304ba752d6b613dc02eae8116cd3e27b
Merge: e07e0d4cb0c4 728e53fef429
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 19 11:17:42 2015 +0100

    Merge branch 'tip-x86-fpu' of git://git.kernel.org/pub/scm/linux/kernel/git/bp/bp into x86/fpu
    
    Pull FPU updates from Borislav Petkov:
    
     "A round of updates to the FPU maze from Oleg and Rik. It should make
      the code a bit more understandable/readable/streamlined and a preparation
      for more cleanups and improvements in that area."
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 08a744c6bfded3d5fa66f94263f81773226113d1
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Feb 6 15:02:00 2015 -0500

    x86/fpu: Change math_error() to use unlazy_fpu(), kill (now) unused save_init_fpu()
    
    math_error() calls save_init_fpu() after conditional_sti(), this means
    that the caller can be preempted. If !use_eager_fpu() we can hit the
    WARN_ON_ONCE(!__thread_has_fpu(tsk)) and/or save the wrong FPU state.
    
    Change math_error() to use unlazy_fpu() and kill save_init_fpu().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1423252925-14451-4-git-send-email-riel@redhat.com
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 88900e288021..9d889f74e806 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -663,7 +663,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	/*
 	 * Save the info for the exception handler and clear the error.
 	 */
-	save_init_fpu(task);
+	unlazy_fpu(task);
 	task->thread.trap_nr = trapnr;
 	task->thread.error_code = error_code;
 	info.si_signo = SIGFPE;

commit c93ecedab35f5305542a9fe5cfbd37377189589e
Merge: 072bc448cc79 7575637ab293
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 9 18:01:52 2015 -0800

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fpu updates from Ingo Molnar:
     "Initial round of kernel_fpu_begin/end cleanups from Oleg Nesterov,
      plus a cleanup from Borislav Petkov"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, fpu: Fix math_state_restore() race with kernel_fpu_begin()
      x86, fpu: Don't abuse has_fpu in __kernel_fpu_begin/end()
      x86, fpu: Introduce per-cpu in_kernel_fpu state
      x86/fpu: Use a symbolic name for asm operand

commit b926e6f61a26036ee9eabe6761483954d481ad25
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Sat Jan 31 04:53:53 2015 -0800

    x86, traps: Fix ist_enter from userspace
    
    context_tracking_user_exit() has no effect if in_interrupt() returns true,
    so ist_enter() didn't work.  Fix it by calling exception_enter(), and thus
    context_tracking_user_exit(), before incrementing the preempt count.
    
    This also adds an assertion that will catch the problem reliably if
    CONFIG_PROVE_RCU=y to help prevent the bug from being reintroduced.
    
    Link: http://lkml.kernel.org/r/261ebee6aee55a4724746d0d7024697013c40a08.1422709102.git.luto@amacapital.net
    Fixes: 959274753857 x86, traps: Track entry into and exit from IST context
    Reported-and-tested-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7176f84f95a4..c74f2f5652da 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -110,15 +110,11 @@ static inline void preempt_conditional_cli(struct pt_regs *regs)
 
 enum ctx_state ist_enter(struct pt_regs *regs)
 {
-	/*
-	 * We are atomic because we're on the IST stack (or we're on x86_32,
-	 * in which case we still shouldn't schedule.
-	 */
-	preempt_count_add(HARDIRQ_OFFSET);
+	enum ctx_state prev_state;
 
 	if (user_mode_vm(regs)) {
 		/* Other than that, we're just an exception. */
-		return exception_enter();
+		prev_state = exception_enter();
 	} else {
 		/*
 		 * We might have interrupted pretty much anything.  In
@@ -127,12 +123,27 @@ enum ctx_state ist_enter(struct pt_regs *regs)
 		 * but we need to notify RCU.
 		 */
 		rcu_nmi_enter();
-		return IN_KERNEL;  /* the value is irrelevant. */
+		prev_state = IN_KERNEL;  /* the value is irrelevant. */
 	}
+
+	/*
+	 * We are atomic because we're on the IST stack (or we're on x86_32,
+	 * in which case we still shouldn't schedule).
+	 *
+	 * This must be after exception_enter(), because exception_enter()
+	 * won't do anything if in_interrupt() returns true.
+	 */
+	preempt_count_add(HARDIRQ_OFFSET);
+
+	/* This code is a bit fragile.  Test it. */
+	rcu_lockdep_assert(rcu_is_watching(), "ist_enter didn't work");
+
+	return prev_state;
 }
 
 void ist_exit(struct pt_regs *regs, enum ctx_state prev_state)
 {
+	/* Must be before exception_exit. */
 	preempt_count_sub(HARDIRQ_OFFSET);
 
 	if (user_mode_vm(regs))

commit 7575637ab293861a799f3bbafe0d8c597389f4e9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jan 15 20:20:28 2015 +0100

    x86, fpu: Fix math_state_restore() race with kernel_fpu_begin()
    
    math_state_restore() can race with kernel_fpu_begin() if irq comes
    right after __thread_fpu_begin(), __save_init_fpu() will overwrite
    fpu->state we are going to restore.
    
    Add 2 simple helpers, kernel_fpu_disable() and kernel_fpu_enable()
    which simply set/clear in_kernel_fpu, and change math_state_restore()
    to exclude kernel_fpu_begin() in between.
    
    Alternatively we could use local_irq_save/restore, but probably these
    new helpers can have more users.
    
    Perhaps they should disable/enable preemption themselves, in this case
    we can remove preempt_disable() in __restore_xstate_sig().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: matt.fleming@intel.com
    Cc: bp@suse.de
    Cc: pbonzini@redhat.com
    Cc: luto@amacapital.net
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Suresh Siddha <sbsiddha@gmail.com>
    Link: http://lkml.kernel.org/r/20150115192028.GD27332@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 88900e288021..fb4cb6adf225 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -788,18 +788,16 @@ void math_state_restore(void)
 		local_irq_disable();
 	}
 
+	/* Avoid __kernel_fpu_begin() right after __thread_fpu_begin() */
+	kernel_fpu_disable();
 	__thread_fpu_begin(tsk);
-
-	/*
-	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
-	 */
 	if (unlikely(restore_fpu_checking(tsk))) {
 		drop_init_fpu(tsk);
 		force_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);
-		return;
+	} else {
+		tsk->thread.fpu_counter++;
 	}
-
-	tsk->thread.fpu_counter++;
+	kernel_fpu_enable();
 }
 EXPORT_SYMBOL_GPL(math_state_restore);
 

commit bced35b65aefe53a6f77a9ed0ce1aea86e9d65a2
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Wed Nov 19 17:59:41 2014 -0800

    x86, traps: Add ist_begin_non_atomic and ist_end_non_atomic
    
    In some IST handlers, if the interrupt came from user mode,
    we can safely enable preemption.  Add helpers to do it safely.
    
    This is intended to be used my the memory failure code in
    do_machine_check.
    
    Acked-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b3a9d24dba25..7176f84f95a4 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -141,6 +141,44 @@ void ist_exit(struct pt_regs *regs, enum ctx_state prev_state)
 		rcu_nmi_exit();
 }
 
+/**
+ * ist_begin_non_atomic() - begin a non-atomic section in an IST exception
+ * @regs:	regs passed to the IST exception handler
+ *
+ * IST exception handlers normally cannot schedule.  As a special
+ * exception, if the exception interrupted userspace code (i.e.
+ * user_mode_vm(regs) would return true) and the exception was not
+ * a double fault, it can be safe to schedule.  ist_begin_non_atomic()
+ * begins a non-atomic section within an ist_enter()/ist_exit() region.
+ * Callers are responsible for enabling interrupts themselves inside
+ * the non-atomic section, and callers must call is_end_non_atomic()
+ * before ist_exit().
+ */
+void ist_begin_non_atomic(struct pt_regs *regs)
+{
+	BUG_ON(!user_mode_vm(regs));
+
+	/*
+	 * Sanity check: we need to be on the normal thread stack.  This
+	 * will catch asm bugs and any attempt to use ist_preempt_enable
+	 * from double_fault.
+	 */
+	BUG_ON(((current_stack_pointer() ^ this_cpu_read_stable(kernel_stack))
+		& ~(THREAD_SIZE - 1)) != 0);
+
+	preempt_count_sub(HARDIRQ_OFFSET);
+}
+
+/**
+ * ist_end_non_atomic() - begin a non-atomic section in an IST exception
+ *
+ * Ends a non-atomic section started with ist_begin_non_atomic().
+ */
+void ist_end_non_atomic(void)
+{
+	preempt_count_add(HARDIRQ_OFFSET);
+}
+
 static nokprobe_inline int
 do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 		  struct pt_regs *regs,	long error_code)

commit 959274753857efe9c5f1ba35fe727f51e9aa128d
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Wed Nov 19 17:41:09 2014 -0800

    x86, traps: Track entry into and exit from IST context
    
    We currently pretend that IST context is like standard exception
    context, but this is incorrect.  IST entries from userspace are like
    standard exceptions except that they use per-cpu stacks, so they are
    atomic.  IST entries from kernel space are like NMIs from RCU's
    perspective -- they are not quiescent states even if they
    interrupted the kernel during a quiescent state.
    
    Add and use ist_enter and ist_exit to track IST context.  Even
    though x86_32 has no IST stacks, we track these interrupts the same
    way.
    
    This fixes two issues:
    
     - Scheduling from an IST interrupt handler will now warn.  It would
       previously appear to work as long as we got lucky and nothing
       overwrote the stack frame.  (I don't know of any bugs in this
       that would trigger the warning, but it's good to be on the safe
       side.)
    
     - RCU handling in IST context was dangerous.  As far as I know,
       only machine checks were likely to trigger this, but it's good to
       be on the safe side.
    
    Note that the machine check handlers appears to have been missing
    any context tracking at all before this patch.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 28f3e5ffc55d..b3a9d24dba25 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -108,6 +108,39 @@ static inline void preempt_conditional_cli(struct pt_regs *regs)
 	preempt_count_dec();
 }
 
+enum ctx_state ist_enter(struct pt_regs *regs)
+{
+	/*
+	 * We are atomic because we're on the IST stack (or we're on x86_32,
+	 * in which case we still shouldn't schedule.
+	 */
+	preempt_count_add(HARDIRQ_OFFSET);
+
+	if (user_mode_vm(regs)) {
+		/* Other than that, we're just an exception. */
+		return exception_enter();
+	} else {
+		/*
+		 * We might have interrupted pretty much anything.  In
+		 * fact, if we're a machine check, we can even interrupt
+		 * NMI processing.  We don't want in_nmi() to return true,
+		 * but we need to notify RCU.
+		 */
+		rcu_nmi_enter();
+		return IN_KERNEL;  /* the value is irrelevant. */
+	}
+}
+
+void ist_exit(struct pt_regs *regs, enum ctx_state prev_state)
+{
+	preempt_count_sub(HARDIRQ_OFFSET);
+
+	if (user_mode_vm(regs))
+		return exception_exit(prev_state);
+	else
+		rcu_nmi_exit();
+}
+
 static nokprobe_inline int
 do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 		  struct pt_regs *regs,	long error_code)
@@ -251,6 +284,8 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	 * end up promoting it to a doublefault.  In that case, modify
 	 * the stack to make it look like we just entered the #GP
 	 * handler from user space, similar to bad_iret.
+	 *
+	 * No need for ist_enter here because we don't use RCU.
 	 */
 	if (((long)regs->sp >> PGDIR_SHIFT) == ESPFIX_PGD_ENTRY &&
 		regs->cs == __KERNEL_CS &&
@@ -263,12 +298,12 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 		normal_regs->orig_ax = 0;  /* Missing (lost) #GP error code */
 		regs->ip = (unsigned long)general_protection;
 		regs->sp = (unsigned long)&normal_regs->orig_ax;
+
 		return;
 	}
 #endif
 
-	exception_enter();
-	/* Return not checked because double check cannot be ignored */
+	ist_enter(regs);  /* Discard prev_state because we won't return. */
 	notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);
 
 	tsk->thread.error_code = error_code;
@@ -434,7 +469,7 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	if (poke_int3_handler(regs))
 		return;
 
-	prev_state = exception_enter();
+	prev_state = ist_enter(regs);
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
@@ -460,7 +495,7 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	preempt_conditional_cli(regs);
 	debug_stack_usage_dec();
 exit:
-	exception_exit(prev_state);
+	ist_exit(regs, prev_state);
 }
 NOKPROBE_SYMBOL(do_int3);
 
@@ -541,7 +576,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	unsigned long dr6;
 	int si_code;
 
-	prev_state = exception_enter();
+	prev_state = ist_enter(regs);
 
 	get_debugreg(dr6, 6);
 
@@ -616,7 +651,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	debug_stack_usage_dec();
 
 exit:
-	exception_exit(prev_state);
+	ist_exit(regs, prev_state);
 }
 NOKPROBE_SYMBOL(do_debug);
 

commit 48e08d0fb265b007ebbb29a72297ff7e40938969
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Tue Nov 11 12:49:41 2014 -0800

    x86, entry: Switch stacks on a paranoid entry from userspace
    
    This causes all non-NMI, non-double-fault kernel entries from
    userspace to run on the normal kernel stack.  Double-fault is
    exempt to minimize confusion if we double-fault directly from
    userspace due to a bad kernel stack.
    
    This is, suprisingly, simpler and shorter than the current code.  It
    removes the IMO rather frightening paranoid_userspace path, and it
    make sync_regs much simpler.
    
    There is no risk of stack overflow due to this change -- the kernel
    stack that we switch to is empty.
    
    This will also enable us to create non-atomic sections within
    machine checks from userspace, which will simplify memory failure
    handling.  It will also allow the upcoming fsgsbase code to be
    simplified, because it doesn't need to worry about usergs when
    scheduling in paranoid_exit, as that code no longer exists.
    
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 88900e288021..28f3e5ffc55d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -466,27 +466,14 @@ NOKPROBE_SYMBOL(do_int3);
 
 #ifdef CONFIG_X86_64
 /*
- * Help handler running on IST stack to switch back to user stack
- * for scheduling or signal handling. The actual stack switch is done in
- * entry.S
+ * Help handler running on IST stack to switch off the IST stack if the
+ * interrupted code was in user mode. The actual stack switch is done in
+ * entry_64.S
  */
 asmlinkage __visible notrace struct pt_regs *sync_regs(struct pt_regs *eregs)
 {
-	struct pt_regs *regs = eregs;
-	/* Did already sync */
-	if (eregs == (struct pt_regs *)eregs->sp)
-		;
-	/* Exception from user space */
-	else if (user_mode(eregs))
-		regs = task_pt_regs(current);
-	/*
-	 * Exception from kernel and interrupts are enabled. Move to
-	 * kernel process stack.
-	 */
-	else if (eregs->flags & X86_EFLAGS_IF)
-		regs = (struct pt_regs *)(eregs->sp -= sizeof(struct pt_regs));
-	if (eregs != regs)
-		*regs = *eregs;
+	struct pt_regs *regs = task_pt_regs(current);
+	*regs = *eregs;
 	return regs;
 }
 NOKPROBE_SYMBOL(sync_regs);

commit a54455766b9e3d3c27a6cef758355d2591d81d68
Merge: 1092b596a56b 72e9b5fe9bee
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 19 13:22:42 2014 -0800

    Merge branch 'x86-mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 MPX fixes from Thomas Gleixner:
     "Three updates for the new MPX infrastructure:
       - Use the proper error check in the trap handler
       - Add a proper config option for it
       - Bring documentation up to date"
    
    * 'x86-mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, mpx: Give MPX a real config option prompt
      x86, mpx: Update documentation
      x86_64/traps: Fix always true condition

commit 3eb5b893ebec7325ac9e6b8e4864af89a9ca1ed1
Merge: 9e66645d72d3 9f7789f845cc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 10 09:34:43 2014 -0800

    Merge branch 'x86-mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 MPX support from Thomas Gleixner:
     "This enables support for x86 MPX.
    
      MPX is a new debug feature for bound checking in user space.  It
      requires kernel support to handle the bound tables and decode the
      bound violating instruction in the trap handler"
    
    * 'x86-mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      asm-generic: Remove asm-generic arch_bprm_mm_init()
      mm: Make arch_unmap()/bprm_mm_init() available to all architectures
      x86: Cleanly separate use of asm-generic/mm_hooks.h
      x86 mpx: Change return type of get_reg_offset()
      fs: Do not include mpx.h in exec.c
      x86, mpx: Add documentation on Intel MPX
      x86, mpx: Cleanup unused bound tables
      x86, mpx: On-demand kernel allocation of bounds tables
      x86, mpx: Decode MPX instruction to get bound violation information
      x86, mpx: Add MPX-specific mmap interface
      x86, mpx: Introduce VM_MPX to indicate that a VMA is MPX specific
      x86, mpx: Add MPX to disabled features
      ia64: Sync struct siginfo with general version
      mips: Sync struct siginfo with general version
      mpx: Extend siginfo structure to include bound violation information
      x86, mpx: Rename cfg_reg_u and status_reg
      x86: mpx: Give bndX registers actual names
      x86: Remove arbitrary instruction size limit in instruction decoder

commit e10abb2f770eeb673d23c19980216b0fb62b4f50
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Tue Nov 25 20:21:14 2014 +0300

    x86_64/traps: Fix always true condition
    
    We should be checking IS_ERR() here.  PTR_ERR() is always true.
    
    Fixes: fe3d197f8431 ('x86, mpx: On-demand kernel allocation of
    bounds tables')
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Link: http://lkml.kernel.org/r/20141125172114.GA24535@mwanda
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 651d5d4f7558..f73fc4a22a4d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -323,7 +323,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 		break; /* Success, it was handled */
 	case 1: /* Bound violation. */
 		info = mpx_generate_siginfo(regs, xsave_buf);
-		if (PTR_ERR(info)) {
+		if (IS_ERR(info)) {
 			/*
 			 * We failed to decode the MPX instruction.  Act as if
 			 * the exception was not caused by MPX.

commit 7ddc6a2199f1da405a2fb68c40db8899b1a8cd87
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon Nov 24 17:39:06 2014 -0800

    x86/asm/traps: Disable tracing and kprobes in fixup_bad_iret and sync_regs
    
    These functions can be executed on the int3 stack, so kprobes
    are dangerous. Tracing is probably a bad idea, too.
    
    Fixes: b645af2d5905 ("x86_64, traps: Rework bad_iret")
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: <stable@vger.kernel.org> # Backport as far back as it would apply
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/50e33d26adca60816f3ba968875801652507d0c4.1416870125.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index de801f22128a..07ab8e9733c5 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -387,7 +387,7 @@ NOKPROBE_SYMBOL(do_int3);
  * for scheduling or signal handling. The actual stack switch is done in
  * entry.S
  */
-asmlinkage __visible struct pt_regs *sync_regs(struct pt_regs *eregs)
+asmlinkage __visible notrace struct pt_regs *sync_regs(struct pt_regs *eregs)
 {
 	struct pt_regs *regs = eregs;
 	/* Did already sync */
@@ -413,7 +413,7 @@ struct bad_iret_stack {
 	struct pt_regs regs;
 };
 
-asmlinkage __visible
+asmlinkage __visible notrace
 struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)
 {
 	/*
@@ -436,6 +436,7 @@ struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)
 	BUG_ON(!user_mode_vm(&new_stack->regs));
 	return new_stack;
 }
+NOKPROBE_SYMBOL(fixup_bad_iret);
 #endif
 
 /*

commit b645af2d5905c4e32399005b867987919cbfc3ae
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Sat Nov 22 18:00:33 2014 -0800

    x86_64, traps: Rework bad_iret
    
    It's possible for iretq to userspace to fail.  This can happen because
    of a bad CS, SS, or RIP.
    
    Historically, we've handled it by fixing up an exception from iretq to
    land at bad_iret, which pretends that the failed iret frame was really
    the hardware part of #GP(0) from userspace.  To make this work, there's
    an extra fixup to fudge the gs base into a usable state.
    
    This is suboptimal because it loses the original exception.  It's also
    buggy because there's no guarantee that we were on the kernel stack to
    begin with.  For example, if the failing iret happened on return from an
    NMI, then we'll end up executing general_protection on the NMI stack.
    This is bad for several reasons, the most immediate of which is that
    general_protection, as a non-paranoid idtentry, will try to deliver
    signals and/or schedule from the wrong stack.
    
    This patch throws out bad_iret entirely.  As a replacement, it augments
    the existing swapgs fudge into a full-blown iret fixup, mostly written
    in C.  It's should be clearer and more correct.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 48035e9cdde9..de801f22128a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -407,6 +407,35 @@ asmlinkage __visible struct pt_regs *sync_regs(struct pt_regs *eregs)
 	return regs;
 }
 NOKPROBE_SYMBOL(sync_regs);
+
+struct bad_iret_stack {
+	void *error_entry_ret;
+	struct pt_regs regs;
+};
+
+asmlinkage __visible
+struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)
+{
+	/*
+	 * This is called from entry_64.S early in handling a fault
+	 * caused by a bad iret to user mode.  To handle the fault
+	 * correctly, we want move our stack frame to task_pt_regs
+	 * and we want to pretend that the exception came from the
+	 * iret target.
+	 */
+	struct bad_iret_stack *new_stack =
+		container_of(task_pt_regs(current),
+			     struct bad_iret_stack, regs);
+
+	/* Copy the IRET target to the new stack. */
+	memmove(&new_stack->regs.ip, (void *)s->regs.sp, 5*8);
+
+	/* Copy the remainder of the stack from the current stack. */
+	memmove(new_stack, s, offsetof(struct bad_iret_stack, regs.ip));
+
+	BUG_ON(!user_mode_vm(&new_stack->regs));
+	return new_stack;
+}
 #endif
 
 /*

commit 6f442be2fb22be02cafa606f1769fa1e6f894441
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Sat Nov 22 18:00:32 2014 -0800

    x86_64, traps: Stop using IST for #SS
    
    On a 32-bit kernel, this has no effect, since there are no IST stacks.
    
    On a 64-bit kernel, #SS can only happen in user code, on a failed iret
    to user space, a canonical violation on access via RSP or RBP, or a
    genuine stack segment violation in 32-bit kernel code.  The first two
    cases don't need IST, and the latter two cases are unlikely fatal bugs,
    and promoting them to double faults would be fine.
    
    This fixes a bug in which the espfix64 code mishandles a stack segment
    violation.
    
    This saves 4k of memory per CPU and a tiny bit of code.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 819662746e23..48035e9cdde9 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -233,27 +233,11 @@ DO_ERROR(X86_TRAP_UD,     SIGILL,  "invalid opcode",		invalid_op)
 DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  "coprocessor segment overrun",coprocessor_segment_overrun)
 DO_ERROR(X86_TRAP_TS,     SIGSEGV, "invalid TSS",		invalid_TSS)
 DO_ERROR(X86_TRAP_NP,     SIGBUS,  "segment not present",	segment_not_present)
-#ifdef CONFIG_X86_32
 DO_ERROR(X86_TRAP_SS,     SIGBUS,  "stack segment",		stack_segment)
-#endif
 DO_ERROR(X86_TRAP_AC,     SIGBUS,  "alignment check",		alignment_check)
 
 #ifdef CONFIG_X86_64
 /* Runs on IST stack */
-dotraplinkage void do_stack_segment(struct pt_regs *regs, long error_code)
-{
-	enum ctx_state prev_state;
-
-	prev_state = exception_enter();
-	if (notify_die(DIE_TRAP, "stack segment", regs, error_code,
-		       X86_TRAP_SS, SIGBUS) != NOTIFY_STOP) {
-		preempt_conditional_sti(regs);
-		do_trap(X86_TRAP_SS, SIGBUS, "stack segment", regs, error_code, NULL);
-		preempt_conditional_cli(regs);
-	}
-	exception_exit(prev_state);
-}
-
 dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 {
 	static const char str[] = "double fault";
@@ -802,7 +786,7 @@ void __init trap_init(void)
 	set_intr_gate(X86_TRAP_OLD_MF, coprocessor_segment_overrun);
 	set_intr_gate(X86_TRAP_TS, invalid_TSS);
 	set_intr_gate(X86_TRAP_NP, segment_not_present);
-	set_intr_gate_ist(X86_TRAP_SS, &stack_segment, STACKFAULT_STACK);
+	set_intr_gate(X86_TRAP_SS, stack_segment);
 	set_intr_gate(X86_TRAP_GP, general_protection);
 	set_intr_gate(X86_TRAP_SPURIOUS, spurious_interrupt_bug);
 	set_intr_gate(X86_TRAP_MF, coprocessor_error);

commit af726f21ed8af2cdaa4e93098dc211521218ae65
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Sat Nov 22 18:00:31 2014 -0800

    x86_64, traps: Fix the espfix64 #DF fixup and rewrite it in C
    
    There's nothing special enough about the espfix64 double fault fixup to
    justify writing it in assembly.  Move it to C.
    
    This also fixes a bug: if the double fault came from an IST stack, the
    old asm code would return to a partially uninitialized stack frame.
    
    Fixes: 3891a04aafd668686239349ea58f3314ea2af86b
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 0d0e922fafc1..819662746e23 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -259,6 +259,30 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	static const char str[] = "double fault";
 	struct task_struct *tsk = current;
 
+#ifdef CONFIG_X86_ESPFIX64
+	extern unsigned char native_irq_return_iret[];
+
+	/*
+	 * If IRET takes a non-IST fault on the espfix64 stack, then we
+	 * end up promoting it to a doublefault.  In that case, modify
+	 * the stack to make it look like we just entered the #GP
+	 * handler from user space, similar to bad_iret.
+	 */
+	if (((long)regs->sp >> PGDIR_SHIFT) == ESPFIX_PGD_ENTRY &&
+		regs->cs == __KERNEL_CS &&
+		regs->ip == (unsigned long)native_irq_return_iret)
+	{
+		struct pt_regs *normal_regs = task_pt_regs(current);
+
+		/* Fake a #GP(0) from userspace. */
+		memmove(&normal_regs->ip, (void *)regs->sp, 5*8);
+		normal_regs->orig_ax = 0;  /* Missing (lost) #GP error code */
+		regs->ip = (unsigned long)general_protection;
+		regs->sp = (unsigned long)&normal_regs->orig_ax;
+		return;
+	}
+#endif
+
 	exception_enter();
 	/* Return not checked because double check cannot be ignored */
 	notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);

commit fe3d197f84319d3bce379a9c0dc17b1f48ad358c
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Nov 14 07:18:29 2014 -0800

    x86, mpx: On-demand kernel allocation of bounds tables
    
    This is really the meat of the MPX patch set.  If there is one patch to
    review in the entire series, this is the one.  There is a new ABI here
    and this kernel code also interacts with userspace memory in a
    relatively unusual manner.  (small FAQ below).
    
    Long Description:
    
    This patch adds two prctl() commands to provide enable or disable the
    management of bounds tables in kernel, including on-demand kernel
    allocation (See the patch "on-demand kernel allocation of bounds tables")
    and cleanup (See the patch "cleanup unused bound tables"). Applications
    do not strictly need the kernel to manage bounds tables and we expect
    some applications to use MPX without taking advantage of this kernel
    support. This means the kernel can not simply infer whether an application
    needs bounds table management from the MPX registers.  The prctl() is an
    explicit signal from userspace.
    
    PR_MPX_ENABLE_MANAGEMENT is meant to be a signal from userspace to
    require kernel's help in managing bounds tables.
    
    PR_MPX_DISABLE_MANAGEMENT is the opposite, meaning that userspace don't
    want kernel's help any more. With PR_MPX_DISABLE_MANAGEMENT, the kernel
    won't allocate and free bounds tables even if the CPU supports MPX.
    
    PR_MPX_ENABLE_MANAGEMENT will fetch the base address of the bounds
    directory out of a userspace register (bndcfgu) and then cache it into
    a new field (->bd_addr) in  the 'mm_struct'.  PR_MPX_DISABLE_MANAGEMENT
    will set "bd_addr" to an invalid address.  Using this scheme, we can
    use "bd_addr" to determine whether the management of bounds tables in
    kernel is enabled.
    
    Also, the only way to access that bndcfgu register is via an xsaves,
    which can be expensive.  Caching "bd_addr" like this also helps reduce
    the cost of those xsaves when doing table cleanup at munmap() time.
    Unfortunately, we can not apply this optimization to #BR fault time
    because we need an xsave to get the value of BNDSTATUS.
    
    ==== Why does the hardware even have these Bounds Tables? ====
    
    MPX only has 4 hardware registers for storing bounds information.
    If MPX-enabled code needs more than these 4 registers, it needs to
    spill them somewhere. It has two special instructions for this
    which allow the bounds to be moved between the bounds registers
    and some new "bounds tables".
    
    They are similar conceptually to a page fault and will be raised by
    the MPX hardware during both bounds violations or when the tables
    are not present. This patch handles those #BR exceptions for
    not-present tables by carving the space out of the normal processes
    address space (essentially calling the new mmap() interface indroduced
    earlier in this patch set.) and then pointing the bounds-directory
    over to it.
    
    The tables *need* to be accessed and controlled by userspace because
    the instructions for moving bounds in and out of them are extremely
    frequent. They potentially happen every time a register pointing to
    memory is dereferenced. Any direct kernel involvement (like a syscall)
    to access the tables would obviously destroy performance.
    
    ==== Why not do this in userspace? ====
    
    This patch is obviously doing this allocation in the kernel.
    However, MPX does not strictly *require* anything in the kernel.
    It can theoretically be done completely from userspace. Here are
    a few ways this *could* be done. I don't think any of them are
    practical in the real-world, but here they are.
    
    Q: Can virtual space simply be reserved for the bounds tables so
       that we never have to allocate them?
    A: As noted earlier, these tables are *HUGE*. An X-GB virtual
       area needs 4*X GB of virtual space, plus 2GB for the bounds
       directory. If we were to preallocate them for the 128TB of
       user virtual address space, we would need to reserve 512TB+2GB,
       which is larger than the entire virtual address space today.
       This means they can not be reserved ahead of time. Also, a
       single process's pre-popualated bounds directory consumes 2GB
       of virtual *AND* physical memory. IOW, it's completely
       infeasible to prepopulate bounds directories.
    
    Q: Can we preallocate bounds table space at the same time memory
       is allocated which might contain pointers that might eventually
       need bounds tables?
    A: This would work if we could hook the site of each and every
       memory allocation syscall. This can be done for small,
       constrained applications. But, it isn't practical at a larger
       scale since a given app has no way of controlling how all the
       parts of the app might allocate memory (think libraries). The
       kernel is really the only place to intercept these calls.
    
    Q: Could a bounds fault be handed to userspace and the tables
       allocated there in a signal handler instead of in the kernel?
    A: (thanks to tglx) mmap() is not on the list of safe async
       handler functions and even if mmap() would work it still
       requires locking or nasty tricks to keep track of the
       allocation state there.
    
    Having ruled out all of the userspace-only approaches for managing
    bounds tables that we could think of, we create them on demand in
    the kernel.
    
    Based-on-patch-by: Qiaowei Ren <qiaowei.ren@intel.com>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: linux-mm@kvack.org
    Cc: linux-mips@linux-mips.org
    Cc: Dave Hansen <dave@sr71.net>
    Link: http://lkml.kernel.org/r/20141114151829.AD4310DE@viggo.jf.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 0d0e922fafc1..651d5d4f7558 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -60,6 +60,7 @@
 #include <asm/fixmap.h>
 #include <asm/mach_traps.h>
 #include <asm/alternative.h>
+#include <asm/mpx.h>
 
 #ifdef CONFIG_X86_64
 #include <asm/x86_init.h>
@@ -228,7 +229,6 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 
 DO_ERROR(X86_TRAP_DE,     SIGFPE,  "divide error",		divide_error)
 DO_ERROR(X86_TRAP_OF,     SIGSEGV, "overflow",			overflow)
-DO_ERROR(X86_TRAP_BR,     SIGSEGV, "bounds",			bounds)
 DO_ERROR(X86_TRAP_UD,     SIGILL,  "invalid opcode",		invalid_op)
 DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  "coprocessor segment overrun",coprocessor_segment_overrun)
 DO_ERROR(X86_TRAP_TS,     SIGSEGV, "invalid TSS",		invalid_TSS)
@@ -278,6 +278,89 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 }
 #endif
 
+dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
+{
+	struct task_struct *tsk = current;
+	struct xsave_struct *xsave_buf;
+	enum ctx_state prev_state;
+	struct bndcsr *bndcsr;
+	siginfo_t *info;
+
+	prev_state = exception_enter();
+	if (notify_die(DIE_TRAP, "bounds", regs, error_code,
+			X86_TRAP_BR, SIGSEGV) == NOTIFY_STOP)
+		goto exit;
+	conditional_sti(regs);
+
+	if (!user_mode(regs))
+		die("bounds", regs, error_code);
+
+	if (!cpu_feature_enabled(X86_FEATURE_MPX)) {
+		/* The exception is not from Intel MPX */
+		goto exit_trap;
+	}
+
+	/*
+	 * We need to look at BNDSTATUS to resolve this exception.
+	 * It is not directly accessible, though, so we need to
+	 * do an xsave and then pull it out of the xsave buffer.
+	 */
+	fpu_save_init(&tsk->thread.fpu);
+	xsave_buf = &(tsk->thread.fpu.state->xsave);
+	bndcsr = get_xsave_addr(xsave_buf, XSTATE_BNDCSR);
+	if (!bndcsr)
+		goto exit_trap;
+
+	/*
+	 * The error code field of the BNDSTATUS register communicates status
+	 * information of a bound range exception #BR or operation involving
+	 * bound directory.
+	 */
+	switch (bndcsr->bndstatus & MPX_BNDSTA_ERROR_CODE) {
+	case 2:	/* Bound directory has invalid entry. */
+		if (mpx_handle_bd_fault(xsave_buf))
+			goto exit_trap;
+		break; /* Success, it was handled */
+	case 1: /* Bound violation. */
+		info = mpx_generate_siginfo(regs, xsave_buf);
+		if (PTR_ERR(info)) {
+			/*
+			 * We failed to decode the MPX instruction.  Act as if
+			 * the exception was not caused by MPX.
+			 */
+			goto exit_trap;
+		}
+		/*
+		 * Success, we decoded the instruction and retrieved
+		 * an 'info' containing the address being accessed
+		 * which caused the exception.  This information
+		 * allows and application to possibly handle the
+		 * #BR exception itself.
+		 */
+		do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, info);
+		kfree(info);
+		break;
+	case 0: /* No exception caused by Intel MPX operations. */
+		goto exit_trap;
+	default:
+		die("bounds", regs, error_code);
+	}
+
+exit:
+	exception_exit(prev_state);
+	return;
+exit_trap:
+	/*
+	 * This path out is for all the cases where we could not
+	 * handle the exception in some way (like allocating a
+	 * table or telling userspace about it.  We will also end
+	 * up here if the kernel has MPX turned off at compile
+	 * time..
+	 */
+	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, NULL);
+	exception_exit(prev_state);
+}
+
 dotraplinkage void
 do_general_protection(struct pt_regs *regs, long error_code)
 {

commit 4cdf77a828b056258f48a9f6078bd2f77d9704bb
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Sat Jun 14 06:47:12 2014 +0000

    x86/kprobes: Fix build errors and blacklist context_track_user
    
    This essentially reverts commit:
    
      ecd50f714c42 ("kprobes, x86: Call exception_enter after kprobes handled")
    
    since it causes build errors with CONFIG_CONTEXT_TRACKING and
    that has been made from misunderstandings;
    context_track_user_*() don't involve much in interrupt context,
    it just returns if in_interrupt() is true.
    
    Instead of changing the do_debug/int3(), this just adds
    context_track_user_*() to kprobes blacklist, since those are
    still can be called right before kprobes handles int3 and debug
    exceptions, and probing those will cause an infinite loop.
    
    Reported-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Kees Cook <keescook@chromium.org>
    Link: http://lkml.kernel.org/r/20140614064711.7865.45957.stgit@kbuild-fedora.novalocal
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index c6eb418c5627..0d0e922fafc1 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -343,6 +343,7 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	if (poke_int3_handler(regs))
 		return;
 
+	prev_state = exception_enter();
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
@@ -351,9 +352,8 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 
 #ifdef CONFIG_KPROBES
 	if (kprobe_int3_handler(regs))
-		return;
+		goto exit;
 #endif
-	prev_state = exception_enter();
 
 	if (notify_die(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
@@ -433,6 +433,8 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	unsigned long dr6;
 	int si_code;
 
+	prev_state = exception_enter();
+
 	get_debugreg(dr6, 6);
 
 	/* Filter out all the reserved bits which are preset to 1 */
@@ -465,7 +467,6 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	if (kprobe_debug_handler(regs))
 		goto exit;
 #endif
-	prev_state = exception_enter();
 
 	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, error_code,
 							SIGTRAP) == NOTIFY_STOP)

commit ec00010972a0971b2c1da4fbe4e5c7d8ed1ecb05
Merge: 8c6e549a447c e041e328c4b4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jun 6 07:55:06 2014 +0200

    Merge branch 'perf/urgent' into perf/core, to resolve conflict and to prepare for new patches
    
    Conflicts:
            arch/x86/kernel/traps.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 10b0256496fd6c162478547f7c6df1e052e87644
Merge: c56d34064b6e 69902c718c0b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Jun 5 11:18:34 2014 +0200

    Merge branch 'perf/kprobes' into perf/core
    
    Conflicts:
            arch/x86/kernel/traps.c
    
    The kprobes enhancements are fully cooked, ship them upstream.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b02ef20a9fba08948e643d3eec0efadf1da01a44
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon May 12 18:24:45 2014 +0200

    uprobes/x86: Fix the wrong ->si_addr when xol triggers a trap
    
    If the probed insn triggers a trap, ->si_addr = regs->ip is technically
    correct, but this is not what the signal handler wants; we need to pass
    the address of the probed insn, not the address of xol slot.
    
    Add the new arch-agnostic helper, uprobe_get_trap_addr(), and change
    fill_trap_info() and math_error() to use it. !CONFIG_UPROBES case in
    uprobes.h uses a macro to avoid include hell and ensure that it can be
    compiled even if an architecture doesn't define instruction_pointer().
    
    Test-case:
    
            #include <signal.h>
            #include <stdio.h>
            #include <unistd.h>
    
            extern void probe_div(void);
    
            void sigh(int sig, siginfo_t *info, void *c)
            {
                    int passed = (info->si_addr == probe_div);
                    printf(passed ? "PASS\n" : "FAIL\n");
                    _exit(!passed);
            }
    
            int main(void)
            {
                    struct sigaction sa = {
                            .sa_sigaction   = sigh,
                            .sa_flags       = SA_SIGINFO,
                    };
    
                    sigaction(SIGFPE, &sa, NULL);
    
                    asm (
                            "xor %ecx,%ecx\n"
                            ".globl probe_div; probe_div:\n"
                            "idiv %ecx\n"
                    );
    
                    return 0;
            }
    
    it fails if probe_div() is probed.
    
    Note: show_unhandled_signals users should probably use this helper too,
    but we need to cleanup them first.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 73b3ea32245a..3fdb20548c4b 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -23,6 +23,7 @@
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/ptrace.h>
+#include <linux/uprobes.h>
 #include <linux/string.h>
 #include <linux/delay.h>
 #include <linux/errno.h>
@@ -148,11 +149,11 @@ static siginfo_t *fill_trap_info(struct pt_regs *regs, int signr, int trapnr,
 
 	case X86_TRAP_DE:
 		sicode = FPE_INTDIV;
-		siaddr = regs->ip;
+		siaddr = uprobe_get_trap_addr(regs);
 		break;
 	case X86_TRAP_UD:
 		sicode = ILL_ILLOPN;
-		siaddr = regs->ip;
+		siaddr = uprobe_get_trap_addr(regs);
 		break;
 	case X86_TRAP_AC:
 		sicode = BUS_ADRALN;
@@ -531,7 +532,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	task->thread.error_code = error_code;
 	info.si_signo = SIGFPE;
 	info.si_errno = 0;
-	info.si_addr = (void __user *)regs->ip;
+	info.si_addr = (void __user *)uprobe_get_trap_addr(regs);
 	if (trapnr == X86_TRAP_MF) {
 		unsigned short cwd, swd;
 		/*

commit 0eb14833d5b1ea1accfeffb71be5de5929f85da9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu May 8 20:12:24 2014 +0200

    x86/traps: Kill DO_ERROR_INFO()
    
    Now that DO_ERROR_INFO() doesn't differ from DO_ERROR() we can remove
    it and use DO_ERROR() instead.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index c9dd74124038..73b3ea32245a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -224,23 +224,17 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 	do_error_trap(regs, error_code, str, trapnr, signr);		\
 }
 
-#define DO_ERROR_INFO(trapnr, signr, str, name)				\
-dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
-{									\
-	do_error_trap(regs, error_code, str, trapnr, signr);		\
-}
-
-DO_ERROR_INFO(X86_TRAP_DE,     SIGFPE,  "divide error",			divide_error)
-DO_ERROR     (X86_TRAP_OF,     SIGSEGV, "overflow",			overflow)
-DO_ERROR     (X86_TRAP_BR,     SIGSEGV, "bounds",			bounds)
-DO_ERROR_INFO(X86_TRAP_UD,     SIGILL,  "invalid opcode",		invalid_op)
-DO_ERROR     (X86_TRAP_OLD_MF, SIGFPE,  "coprocessor segment overrun",	coprocessor_segment_overrun)
-DO_ERROR     (X86_TRAP_TS,     SIGSEGV, "invalid TSS",			invalid_TSS)
-DO_ERROR     (X86_TRAP_NP,     SIGBUS,  "segment not present",		segment_not_present)
+DO_ERROR(X86_TRAP_DE,     SIGFPE,  "divide error",		divide_error)
+DO_ERROR(X86_TRAP_OF,     SIGSEGV, "overflow",			overflow)
+DO_ERROR(X86_TRAP_BR,     SIGSEGV, "bounds",			bounds)
+DO_ERROR(X86_TRAP_UD,     SIGILL,  "invalid opcode",		invalid_op)
+DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  "coprocessor segment overrun",coprocessor_segment_overrun)
+DO_ERROR(X86_TRAP_TS,     SIGSEGV, "invalid TSS",		invalid_TSS)
+DO_ERROR(X86_TRAP_NP,     SIGBUS,  "segment not present",	segment_not_present)
 #ifdef CONFIG_X86_32
-DO_ERROR     (X86_TRAP_SS,     SIGBUS,  "stack segment",		stack_segment)
+DO_ERROR(X86_TRAP_SS,     SIGBUS,  "stack segment",		stack_segment)
 #endif
-DO_ERROR_INFO(X86_TRAP_AC,     SIGBUS,  "alignment check",		alignment_check)
+DO_ERROR(X86_TRAP_AC,     SIGBUS,  "alignment check",		alignment_check)
 
 #ifdef CONFIG_X86_64
 /* Runs on IST stack */

commit 1c326c4dfe182a1c4c1e39f2c00f04c380d11692
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu May 8 20:04:11 2014 +0200

    x86/traps: Shift fill_trap_info() from DO_ERROR_INFO() to do_error_trap()
    
    Move the callsite of fill_trap_info() into do_error_trap() and remove
    the "siginfo_t *info" argument.
    
    This obviously breaks DO_ERROR() which passed info == NULL, we simply
    change fill_trap_info() to return "siginfo_t *" and add the "default"
    case which returns SEND_SIG_PRIV.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1cf8a4c409b6..c9dd74124038 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -136,13 +136,16 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 	return -1;
 }
 
-static void fill_trap_info(struct pt_regs *regs, int signr, int trapnr,
-			   siginfo_t *info)
+static siginfo_t *fill_trap_info(struct pt_regs *regs, int signr, int trapnr,
+				siginfo_t *info)
 {
 	unsigned long siaddr;
 	int sicode;
 
 	switch (trapnr) {
+	default:
+		return SEND_SIG_PRIV;
+
 	case X86_TRAP_DE:
 		sicode = FPE_INTDIV;
 		siaddr = regs->ip;
@@ -161,6 +164,7 @@ static void fill_trap_info(struct pt_regs *regs, int signr, int trapnr,
 	info->si_errno = 0;
 	info->si_code = sicode;
 	info->si_addr = (void __user *)siaddr;
+	return info;
 }
 
 static void __kprobes
@@ -199,14 +203,16 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 }
 
 static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
-			  unsigned long trapnr, int signr, siginfo_t *info)
+			  unsigned long trapnr, int signr)
 {
 	enum ctx_state prev_state = exception_enter();
+	siginfo_t info;
 
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
 			NOTIFY_STOP) {
 		conditional_sti(regs);
-		do_trap(trapnr, signr, str, regs, error_code, info);
+		do_trap(trapnr, signr, str, regs, error_code,
+			fill_trap_info(regs, signr, trapnr, &info));
 	}
 
 	exception_exit(prev_state);
@@ -215,16 +221,13 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 #define DO_ERROR(trapnr, signr, str, name)				\
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 {									\
-	do_error_trap(regs, error_code, str, trapnr, signr, NULL);	\
+	do_error_trap(regs, error_code, str, trapnr, signr);		\
 }
 
 #define DO_ERROR_INFO(trapnr, signr, str, name)				\
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 {									\
-	siginfo_t info;							\
-									\
-	fill_trap_info(regs, signr, trapnr, &info);			\
-	do_error_trap(regs, error_code, str, trapnr, signr, &info);	\
+	do_error_trap(regs, error_code, str, trapnr, signr);		\
 }
 
 DO_ERROR_INFO(X86_TRAP_DE,     SIGFPE,  "divide error",			divide_error)

commit 958d3d729802f7d741cbe8400e69b89baae580ee
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 7 17:59:39 2014 +0200

    x86/traps: Introduce fill_trap_info(), simplify DO_ERROR_INFO()
    
    Extract the fill-siginfo code from DO_ERROR_INFO() into the new helper,
    fill_trap_info().
    
    It can calculate si_code and si_addr looking at trapnr, so we can remove
    these arguments from DO_ERROR_INFO() and simplify the source code. The
    generated code is the same, __builtin_constant_p(trapnr) == T.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ab8dad719b9e..1cf8a4c409b6 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -136,6 +136,33 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 	return -1;
 }
 
+static void fill_trap_info(struct pt_regs *regs, int signr, int trapnr,
+			   siginfo_t *info)
+{
+	unsigned long siaddr;
+	int sicode;
+
+	switch (trapnr) {
+	case X86_TRAP_DE:
+		sicode = FPE_INTDIV;
+		siaddr = regs->ip;
+		break;
+	case X86_TRAP_UD:
+		sicode = ILL_ILLOPN;
+		siaddr = regs->ip;
+		break;
+	case X86_TRAP_AC:
+		sicode = BUS_ADRALN;
+		siaddr = 0;
+		break;
+	}
+
+	info->si_signo = signr;
+	info->si_errno = 0;
+	info->si_code = sicode;
+	info->si_addr = (void __user *)siaddr;
+}
+
 static void __kprobes
 do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	long error_code, siginfo_t *info)
@@ -191,30 +218,26 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 	do_error_trap(regs, error_code, str, trapnr, signr, NULL);	\
 }
 
-#define DO_ERROR_INFO(trapnr, signr, str, name, sicode, siaddr)		\
+#define DO_ERROR_INFO(trapnr, signr, str, name)				\
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 {									\
 	siginfo_t info;							\
 									\
-	info.si_signo = signr;						\
-	info.si_errno = 0;						\
-	info.si_code = sicode;						\
-	info.si_addr = (void __user *)siaddr;				\
-									\
+	fill_trap_info(regs, signr, trapnr, &info);			\
 	do_error_trap(regs, error_code, str, trapnr, signr, &info);	\
 }
 
-DO_ERROR_INFO(X86_TRAP_DE,     SIGFPE,  "divide error",			divide_error,		     FPE_INTDIV, regs->ip )
-DO_ERROR     (X86_TRAP_OF,     SIGSEGV, "overflow",			overflow					  )
-DO_ERROR     (X86_TRAP_BR,     SIGSEGV, "bounds",			bounds						  )
-DO_ERROR_INFO(X86_TRAP_UD,     SIGILL,  "invalid opcode",		invalid_op,		     ILL_ILLOPN, regs->ip )
-DO_ERROR     (X86_TRAP_OLD_MF, SIGFPE,  "coprocessor segment overrun",	coprocessor_segment_overrun			  )
-DO_ERROR     (X86_TRAP_TS,     SIGSEGV, "invalid TSS",			invalid_TSS					  )
-DO_ERROR     (X86_TRAP_NP,     SIGBUS,  "segment not present",		segment_not_present				  )
+DO_ERROR_INFO(X86_TRAP_DE,     SIGFPE,  "divide error",			divide_error)
+DO_ERROR     (X86_TRAP_OF,     SIGSEGV, "overflow",			overflow)
+DO_ERROR     (X86_TRAP_BR,     SIGSEGV, "bounds",			bounds)
+DO_ERROR_INFO(X86_TRAP_UD,     SIGILL,  "invalid opcode",		invalid_op)
+DO_ERROR     (X86_TRAP_OLD_MF, SIGFPE,  "coprocessor segment overrun",	coprocessor_segment_overrun)
+DO_ERROR     (X86_TRAP_TS,     SIGSEGV, "invalid TSS",			invalid_TSS)
+DO_ERROR     (X86_TRAP_NP,     SIGBUS,  "segment not present",		segment_not_present)
 #ifdef CONFIG_X86_32
-DO_ERROR     (X86_TRAP_SS,     SIGBUS,  "stack segment",		stack_segment					  )
+DO_ERROR     (X86_TRAP_SS,     SIGBUS,  "stack segment",		stack_segment)
 #endif
-DO_ERROR_INFO(X86_TRAP_AC,     SIGBUS,  "alignment check",		alignment_check,	     BUS_ADRALN, 0	  )
+DO_ERROR_INFO(X86_TRAP_AC,     SIGBUS,  "alignment check",		alignment_check)
 
 #ifdef CONFIG_X86_64
 /* Runs on IST stack */

commit dff0796e53c29147c9bd1f5567a261dcf0e528bc
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 7 17:21:34 2014 +0200

    x86/traps: Introduce do_error_trap()
    
    Move the common code from DO_ERROR() and DO_ERROR_INFO() into the new
    helper, do_error_trap(). This simplifies define's and shaves 527 bytes
    from traps.o.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 2cd429117a41..ab8dad719b9e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -171,41 +171,37 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	force_sig_info(signr, info ?: SEND_SIG_PRIV, tsk);
 }
 
+static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
+			  unsigned long trapnr, int signr, siginfo_t *info)
+{
+	enum ctx_state prev_state = exception_enter();
+
+	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
+			NOTIFY_STOP) {
+		conditional_sti(regs);
+		do_trap(trapnr, signr, str, regs, error_code, info);
+	}
+
+	exception_exit(prev_state);
+}
+
 #define DO_ERROR(trapnr, signr, str, name)				\
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 {									\
-	enum ctx_state prev_state;					\
-									\
-	prev_state = exception_enter();					\
-	if (notify_die(DIE_TRAP, str, regs, error_code,			\
-			trapnr, signr) == NOTIFY_STOP) {		\
-		exception_exit(prev_state);				\
-		return;							\
-	}								\
-	conditional_sti(regs);						\
-	do_trap(trapnr, signr, str, regs, error_code, NULL);		\
-	exception_exit(prev_state);					\
+	do_error_trap(regs, error_code, str, trapnr, signr, NULL);	\
 }
 
 #define DO_ERROR_INFO(trapnr, signr, str, name, sicode, siaddr)		\
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 {									\
 	siginfo_t info;							\
-	enum ctx_state prev_state;					\
 									\
 	info.si_signo = signr;						\
 	info.si_errno = 0;						\
 	info.si_code = sicode;						\
 	info.si_addr = (void __user *)siaddr;				\
-	prev_state = exception_enter();					\
-	if (notify_die(DIE_TRAP, str, regs, error_code,			\
-			trapnr, signr) == NOTIFY_STOP) {		\
-		exception_exit(prev_state);				\
-		return;							\
-	}								\
-	conditional_sti(regs);						\
-	do_trap(trapnr, signr, str, regs, error_code, &info);		\
-	exception_exit(prev_state);					\
+									\
+	do_error_trap(regs, error_code, str, trapnr, signr, &info);	\
 }
 
 DO_ERROR_INFO(X86_TRAP_DE,     SIGFPE,  "divide error",			divide_error,		     FPE_INTDIV, regs->ip )

commit 38cad57be9800e46c52a3612fb9d963eee4fd9c3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 7 16:47:09 2014 +0200

    x86/traps: Use SEND_SIG_PRIV instead of force_sig()
    
    force_sig() is just force_sig_info(SEND_SIG_PRIV). Imho it should die,
    we have too many ugly "send signal" helpers.
    
    And do_trap() looks just ugly because it uses force_sig_info() or
    force_sig() depending on info != NULL.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 8eddd628326e..2cd429117a41 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -168,10 +168,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	}
 #endif
 
-	if (info)
-		force_sig_info(signr, info, tsk);
-	else
-		force_sig(signr, tsk);
+	force_sig_info(signr, info ?: SEND_SIG_PRIV, tsk);
 }
 
 #define DO_ERROR(trapnr, signr, str, name)				\
@@ -305,7 +302,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		pr_cont("\n");
 	}
 
-	force_sig(SIGSEGV, tsk);
+	force_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);
 exit:
 	exception_exit(prev_state);
 }
@@ -645,7 +642,7 @@ void math_state_restore(void)
 	 */
 	if (unlikely(restore_fpu_checking(tsk))) {
 		drop_init_fpu(tsk);
-		force_sig(SIGSEGV, tsk);
+		force_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);
 		return;
 	}
 

commit 5e1b05beeca8139204324581a5b1ffb53d057f96
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu May 8 20:34:00 2014 +0200

    x86/traps: Make math_error() static
    
    Trivial, make math_error() static.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 57409f6b8c62..8eddd628326e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -488,7 +488,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
  * the correct behaviour even in the presence of the asynchronous
  * IRQ13 behaviour
  */
-void math_error(struct pt_regs *regs, int error_code, int trapnr)
+static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 {
 	struct task_struct *task = current;
 	siginfo_t info;

commit 2605fc216fa492f9e7c488bdc7f687cd6dcc703b
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri May 2 00:44:37 2014 +0200

    asmlinkage, x86: Add explicit __visible to arch/x86/*
    
    As requested by Linus add explicit __visible to the asmlinkage users.
    This marks all functions visible to assembler.
    
    Tree sweep for arch/x86/*
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1398984278-29319-3-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 57409f6b8c62..f73b5d435bdc 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -357,7 +357,7 @@ dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_co
  * for scheduling or signal handling. The actual stack switch is done in
  * entry.S
  */
-asmlinkage __kprobes struct pt_regs *sync_regs(struct pt_regs *eregs)
+asmlinkage __visible __kprobes struct pt_regs *sync_regs(struct pt_regs *eregs)
 {
 	struct pt_regs *regs = eregs;
 	/* Did already sync */
@@ -601,11 +601,11 @@ do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 #endif
 }
 
-asmlinkage void __attribute__((weak)) smp_thermal_interrupt(void)
+asmlinkage __visible void __attribute__((weak)) smp_thermal_interrupt(void)
 {
 }
 
-asmlinkage void __attribute__((weak)) smp_threshold_interrupt(void)
+asmlinkage __visible void __attribute__((weak)) smp_threshold_interrupt(void)
 {
 }
 

commit 9326638cbee2d36b051ed2a69f3e4e107e5f86bd
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Apr 17 17:18:14 2014 +0900

    kprobes, x86: Use NOKPROBE_SYMBOL() instead of __kprobes annotation
    
    Use NOKPROBE_SYMBOL macro for protecting functions
    from kprobes instead of __kprobes annotation under
    arch/x86.
    
    This applies nokprobe_inline annotation for some cases,
    because NOKPROBE_SYMBOL() will inhibit inlining by
    referring the symbol address.
    
    This just folds a bunch of previous NOKPROBE_SYMBOL()
    cleanup patches for x86 to one patch.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Link: http://lkml.kernel.org/r/20140417081814.26341.51656.stgit@ltc230.yrl.intra.hitachi.co.jp
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fernando Luis Vázquez Cao <fernando_b1@lab.ntt.co.jp>
    Cc: Gleb Natapov <gleb@redhat.com>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Lebon <jlebon@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Matt Fleming <matt.fleming@intel.com>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Cc: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ba9abe9cbce3..3c8ae7d83820 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -106,7 +106,7 @@ static inline void preempt_conditional_cli(struct pt_regs *regs)
 	preempt_count_dec();
 }
 
-static int __kprobes
+static nokprobe_inline int
 do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 		  struct pt_regs *regs,	long error_code)
 {
@@ -136,7 +136,7 @@ do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 	return -1;
 }
 
-static void __kprobes
+static void
 do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	long error_code, siginfo_t *info)
 {
@@ -173,6 +173,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	else
 		force_sig(signr, tsk);
 }
+NOKPROBE_SYMBOL(do_trap);
 
 #define DO_ERROR(trapnr, signr, str, name)				\
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
@@ -263,7 +264,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 }
 #endif
 
-dotraplinkage void __kprobes
+dotraplinkage void
 do_general_protection(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk;
@@ -309,9 +310,10 @@ do_general_protection(struct pt_regs *regs, long error_code)
 exit:
 	exception_exit(prev_state);
 }
+NOKPROBE_SYMBOL(do_general_protection);
 
 /* May run on IST stack. */
-dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_code)
+dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 	enum ctx_state prev_state;
 
@@ -355,6 +357,7 @@ dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_co
 exit:
 	exception_exit(prev_state);
 }
+NOKPROBE_SYMBOL(do_int3);
 
 #ifdef CONFIG_X86_64
 /*
@@ -362,7 +365,7 @@ dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_co
  * for scheduling or signal handling. The actual stack switch is done in
  * entry.S
  */
-asmlinkage __kprobes struct pt_regs *sync_regs(struct pt_regs *eregs)
+asmlinkage struct pt_regs *sync_regs(struct pt_regs *eregs)
 {
 	struct pt_regs *regs = eregs;
 	/* Did already sync */
@@ -381,6 +384,7 @@ asmlinkage __kprobes struct pt_regs *sync_regs(struct pt_regs *eregs)
 		*regs = *eregs;
 	return regs;
 }
+NOKPROBE_SYMBOL(sync_regs);
 #endif
 
 /*
@@ -407,7 +411,7 @@ asmlinkage __kprobes struct pt_regs *sync_regs(struct pt_regs *eregs)
  *
  * May run on IST stack.
  */
-dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
+dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk = current;
 	enum ctx_state prev_state;
@@ -491,6 +495,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 exit:
 	exception_exit(prev_state);
 }
+NOKPROBE_SYMBOL(do_debug);
 
 /*
  * Note that we play around with the 'TS' bit in an attempt to get
@@ -662,7 +667,7 @@ void math_state_restore(void)
 }
 EXPORT_SYMBOL_GPL(math_state_restore);
 
-dotraplinkage void __kprobes
+dotraplinkage void
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
 	enum ctx_state prev_state;
@@ -688,6 +693,7 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 #endif
 	exception_exit(prev_state);
 }
+NOKPROBE_SYMBOL(do_device_not_available);
 
 #ifdef CONFIG_X86_32
 dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)

commit ecd50f714c421c759354632dd00f70c718c95b10
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Apr 17 17:17:40 2014 +0900

    kprobes, x86: Call exception_enter after kprobes handled
    
    Move exception_enter() call after kprobes handler
    is done. Since the exception_enter() involves
    many other functions (like printk), it can cause
    recursive int3/break loop when kprobes probe such
    functions.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Link: http://lkml.kernel.org/r/20140417081740.26341.10894.stgit@ltc230.yrl.intra.hitachi.co.jp
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index e5d4a70814d7..ba9abe9cbce3 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -327,7 +327,6 @@ dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_co
 	if (poke_int3_handler(regs))
 		return;
 
-	prev_state = exception_enter();
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
@@ -338,6 +337,7 @@ dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_co
 	if (kprobe_int3_handler(regs))
 		return;
 #endif
+	prev_state = exception_enter();
 
 	if (notify_die(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
@@ -415,8 +415,6 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	unsigned long dr6;
 	int si_code;
 
-	prev_state = exception_enter();
-
 	get_debugreg(dr6, 6);
 
 	/* Filter out all the reserved bits which are preset to 1 */
@@ -449,6 +447,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	if (kprobe_debug_handler(regs))
 		goto exit;
 #endif
+	prev_state = exception_enter();
 
 	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, error_code,
 							SIGTRAP) == NOTIFY_STOP)

commit 6f6343f53d133bae516caf3d254bce37d8774625
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Apr 17 17:17:33 2014 +0900

    kprobes/x86: Call exception handlers directly from do_int3/do_debug
    
    To avoid a kernel crash by probing on lockdep code, call
    kprobe_int3_handler() and kprobe_debug_handler()(which was
    formerly called post_kprobe_handler()) directly from
    do_int3 and do_debug.
    
    Currently kprobes uses notify_die() to hook the int3/debug
    exceptoins. Since there is a locking code in notify_die,
    the lockdep code can be invoked. And because the lockdep
    involves printk() related things, theoretically, we need to
    prohibit probing on such code, which means much longer blacklist
    we'll have. Instead, hooking the int3/debug for kprobes before
    notify_die() can avoid this problem.
    
    Anyway, most of the int3 handlers in the kernel are already
    called from do_int3 directly, e.g. ftrace_int3_handler,
    poke_int3_handler, kgdb_ll_trap. Actually only
    kprobe_exceptions_notify is on the notifier_call_chain.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Jonathan Lebon <jlebon@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Link: http://lkml.kernel.org/r/20140417081733.26341.24423.stgit@ltc230.yrl.intra.hitachi.co.jp
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 57409f6b8c62..e5d4a70814d7 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -334,6 +334,11 @@ dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_co
 		goto exit;
 #endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */
 
+#ifdef CONFIG_KPROBES
+	if (kprobe_int3_handler(regs))
+		return;
+#endif
+
 	if (notify_die(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
@@ -440,6 +445,11 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	/* Store the virtualized DR6 value */
 	tsk->thread.debugreg6 = dr6;
 
+#ifdef CONFIG_KPROBES
+	if (kprobe_debug_handler(regs))
+		goto exit;
+#endif
+
 	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, error_code,
 							SIGTRAP) == NOTIFY_STOP)
 		goto exit;

commit d8af4ce490e92adfa123ae79899332bd829903e7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Dec 12 14:46:42 2013 +0100

    x86/traps: Clean up error exception handler definitions
    
    So I was reading the exception handler generation code and got a real
    headache looking at the unstructured mess that our DO_ERROR*()
    generation code is today.
    
    Make it more readable.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: http://lkml.kernel.org/n/tip-kuabysiykvUJpgus35lhnhvs@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b857ed890b4c..57409f6b8c62 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -211,21 +211,17 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 	exception_exit(prev_state);					\
 }
 
-DO_ERROR_INFO(X86_TRAP_DE, SIGFPE, "divide error", divide_error, FPE_INTDIV,
-		regs->ip)
-DO_ERROR(X86_TRAP_OF, SIGSEGV, "overflow", overflow)
-DO_ERROR(X86_TRAP_BR, SIGSEGV, "bounds", bounds)
-DO_ERROR_INFO(X86_TRAP_UD, SIGILL, "invalid opcode", invalid_op, ILL_ILLOPN,
-		regs->ip)
-DO_ERROR(X86_TRAP_OLD_MF, SIGFPE, "coprocessor segment overrun",
-		coprocessor_segment_overrun)
-DO_ERROR(X86_TRAP_TS, SIGSEGV, "invalid TSS", invalid_TSS)
-DO_ERROR(X86_TRAP_NP, SIGBUS, "segment not present", segment_not_present)
+DO_ERROR_INFO(X86_TRAP_DE,     SIGFPE,  "divide error",			divide_error,		     FPE_INTDIV, regs->ip )
+DO_ERROR     (X86_TRAP_OF,     SIGSEGV, "overflow",			overflow					  )
+DO_ERROR     (X86_TRAP_BR,     SIGSEGV, "bounds",			bounds						  )
+DO_ERROR_INFO(X86_TRAP_UD,     SIGILL,  "invalid opcode",		invalid_op,		     ILL_ILLOPN, regs->ip )
+DO_ERROR     (X86_TRAP_OLD_MF, SIGFPE,  "coprocessor segment overrun",	coprocessor_segment_overrun			  )
+DO_ERROR     (X86_TRAP_TS,     SIGSEGV, "invalid TSS",			invalid_TSS					  )
+DO_ERROR     (X86_TRAP_NP,     SIGBUS,  "segment not present",		segment_not_present				  )
 #ifdef CONFIG_X86_32
-DO_ERROR(X86_TRAP_SS, SIGBUS, "stack segment", stack_segment)
+DO_ERROR     (X86_TRAP_SS,     SIGBUS,  "stack segment",		stack_segment					  )
 #endif
-DO_ERROR_INFO(X86_TRAP_AC, SIGBUS, "alignment check", alignment_check,
-		BUS_ADRALN, 0)
+DO_ERROR_INFO(X86_TRAP_AC,     SIGBUS,  "alignment check",		alignment_check,	     BUS_ADRALN, 0	  )
 
 #ifdef CONFIG_X86_64
 /* Runs on IST stack */

commit 7971e23a66c94f1b9bd2d64a3e86dfbfa8c60121
Merge: f0d55cc1a658 a4f61dec55c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 14 16:25:10 2013 +0900

    Merge branch 'x86-trace-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/trace changes from Ingo Molnar:
     "This adds page fault tracepoints which have zero runtime cost in the
      disabled case via IDT trickery (no NOPs in the page fault hotpath)"
    
    * 'x86-trace-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, trace: Change user|kernel_page_fault to page_fault_user|kernel
      x86, trace: Add page fault tracepoints
      x86, trace: Delete __trace_alloc_intr_gate()
      x86, trace: Register exception handler to trace IDT
      x86, trace: Remove __alloc_intr_gate()

commit c375f15a434db1867cb004bafba92aba739e4e39
Author: Vineet Gupta <Vineet.Gupta1@synopsys.com>
Date:   Tue Nov 12 15:08:46 2013 -0800

    x86: move fpu_counter into ARCH specific thread_struct
    
    Only a couple of arches (sh/x86) use fpu_counter in task_struct so it can
    be moved out into ARCH specific thread_struct, reducing the size of
    task_struct for other arches.
    
    Compile tested i386_defconfig + gcc 4.7.3
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Paul Mundt <paul.mundt@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 729aa779ff75..996ce2313ce6 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -653,7 +653,7 @@ void math_state_restore(void)
 		return;
 	}
 
-	tsk->fpu_counter++;
+	tsk->thread.fpu_counter++;
 }
 EXPORT_SYMBOL_GPL(math_state_restore);
 

commit 25c74b10bacead867478480170083f69cfc0db48
Author: Seiji Aguchi <seiji.aguchi@hds.com>
Date:   Wed Oct 30 16:37:00 2013 -0400

    x86, trace: Register exception handler to trace IDT
    
    This patch registers exception handlers for tracing to a trace IDT.
    
    To implemented it in set_intr_gate(), this patch does followings.
     - Register the exception handlers to
       the trace IDT by prepending "trace_" to the handler's names.
     - Also, newly introduce trace_page_fault() to add tracepoints
       in a subsequent patch.
    
    Signed-off-by: Seiji Aguchi <seiji.aguchi@hds.com>
    Link: http://lkml.kernel.org/r/52716DEC.5050204@hds.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 8c8093b146ca..1c9d0ad5a193 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -713,7 +713,7 @@ void __init early_trap_init(void)
 	/* int3 can be called from all */
 	set_system_intr_gate_ist(X86_TRAP_BP, &int3, DEBUG_STACK);
 #ifdef CONFIG_X86_32
-	set_intr_gate(X86_TRAP_PF, &page_fault);
+	set_intr_gate(X86_TRAP_PF, page_fault);
 #endif
 	load_idt(&idt_descr);
 }
@@ -721,7 +721,7 @@ void __init early_trap_init(void)
 void __init early_trap_pf_init(void)
 {
 #ifdef CONFIG_X86_64
-	set_intr_gate(X86_TRAP_PF, &page_fault);
+	set_intr_gate(X86_TRAP_PF, page_fault);
 #endif
 }
 
@@ -737,30 +737,30 @@ void __init trap_init(void)
 	early_iounmap(p, 4);
 #endif
 
-	set_intr_gate(X86_TRAP_DE, &divide_error);
+	set_intr_gate(X86_TRAP_DE, divide_error);
 	set_intr_gate_ist(X86_TRAP_NMI, &nmi, NMI_STACK);
 	/* int4 can be called from all */
 	set_system_intr_gate(X86_TRAP_OF, &overflow);
-	set_intr_gate(X86_TRAP_BR, &bounds);
-	set_intr_gate(X86_TRAP_UD, &invalid_op);
-	set_intr_gate(X86_TRAP_NM, &device_not_available);
+	set_intr_gate(X86_TRAP_BR, bounds);
+	set_intr_gate(X86_TRAP_UD, invalid_op);
+	set_intr_gate(X86_TRAP_NM, device_not_available);
 #ifdef CONFIG_X86_32
 	set_task_gate(X86_TRAP_DF, GDT_ENTRY_DOUBLEFAULT_TSS);
 #else
 	set_intr_gate_ist(X86_TRAP_DF, &double_fault, DOUBLEFAULT_STACK);
 #endif
-	set_intr_gate(X86_TRAP_OLD_MF, &coprocessor_segment_overrun);
-	set_intr_gate(X86_TRAP_TS, &invalid_TSS);
-	set_intr_gate(X86_TRAP_NP, &segment_not_present);
+	set_intr_gate(X86_TRAP_OLD_MF, coprocessor_segment_overrun);
+	set_intr_gate(X86_TRAP_TS, invalid_TSS);
+	set_intr_gate(X86_TRAP_NP, segment_not_present);
 	set_intr_gate_ist(X86_TRAP_SS, &stack_segment, STACKFAULT_STACK);
-	set_intr_gate(X86_TRAP_GP, &general_protection);
-	set_intr_gate(X86_TRAP_SPURIOUS, &spurious_interrupt_bug);
-	set_intr_gate(X86_TRAP_MF, &coprocessor_error);
-	set_intr_gate(X86_TRAP_AC, &alignment_check);
+	set_intr_gate(X86_TRAP_GP, general_protection);
+	set_intr_gate(X86_TRAP_SPURIOUS, spurious_interrupt_bug);
+	set_intr_gate(X86_TRAP_MF, coprocessor_error);
+	set_intr_gate(X86_TRAP_AC, alignment_check);
 #ifdef CONFIG_X86_MCE
 	set_intr_gate_ist(X86_TRAP_MC, &machine_check, MCE_STACK);
 #endif
-	set_intr_gate(X86_TRAP_XF, &simd_coprocessor_error);
+	set_intr_gate(X86_TRAP_XF, simd_coprocessor_error);
 
 	/* Reserve all the builtin and the syscall vector: */
 	for (i = 0; i < FIRST_EXTERNAL_VECTOR; i++)

commit bdb43806589096ac4272fe1307e789846ac08d7c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 10 12:15:23 2013 +0200

    sched: Extract the basic add/sub preempt_count modifiers
    
    Rewrite the preempt_count macros in order to extract the 3 basic
    preempt_count value modifiers:
    
      __preempt_count_add()
      __preempt_count_sub()
    
    and the new:
    
      __preempt_count_dec_and_test()
    
    And since we're at it anyway, replace the unconventional
    $op_preempt_count names with the more conventional preempt_count_$op.
    
    Since these basic operators are equivalent to the previous _notrace()
    variants, do away with the _notrace() versions.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-ewbpdbupy9xpsjhg960zwbv8@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 8c8093b146ca..729aa779ff75 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -88,7 +88,7 @@ static inline void conditional_sti(struct pt_regs *regs)
 
 static inline void preempt_conditional_sti(struct pt_regs *regs)
 {
-	inc_preempt_count();
+	preempt_count_inc();
 	if (regs->flags & X86_EFLAGS_IF)
 		local_irq_enable();
 }
@@ -103,7 +103,7 @@ static inline void preempt_conditional_cli(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
 		local_irq_disable();
-	dec_preempt_count();
+	preempt_count_dec();
 }
 
 static int __kprobes

commit 17f41571bb2c4a398785452ac2718a6c5d77180e
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Tue Jul 23 10:09:28 2013 +0200

    kprobes/x86: Call out into INT3 handler directly instead of using notifier
    
    In fd4363fff3d96 ("x86: Introduce int3 (breakpoint)-based
    instruction patching"), the mechanism that was introduced for
    notifying alternatives code from int3 exception handler that and
    exception occured was die_notifier.
    
    This is however problematic, as early code might be using jump
    labels even before the notifier registration has been performed,
    which will then lead to an oops due to unhandled exception. One
    of such occurences has been encountered by Fengguang:
    
     int3: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC
     Modules linked in:
     CPU: 1 PID: 0 Comm: swapper/1 Not tainted 3.11.0-rc1-01429-g04bf576 #8
     task: ffff88000da1b040 ti: ffff88000da1c000 task.ti: ffff88000da1c000
     RIP: 0010:[<ffffffff811098cc>]  [<ffffffff811098cc>] ttwu_do_wakeup+0x28/0x225
     RSP: 0000:ffff88000dd03f10  EFLAGS: 00000006
     RAX: 0000000000000000 RBX: ffff88000dd12940 RCX: ffffffff81769c40
     RDX: 0000000000000002 RSI: 0000000000000000 RDI: 0000000000000001
     RBP: ffff88000dd03f28 R08: ffffffff8176a8c0 R09: 0000000000000002
     R10: ffffffff810ff484 R11: ffff88000dd129e8 R12: ffff88000dbc90c0
     R13: ffff88000dbc90c0 R14: ffff88000da1dfd8 R15: ffff88000da1dfd8
     FS:  0000000000000000(0000) GS:ffff88000dd00000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
     CR2: 00000000ffffffff CR3: 0000000001c88000 CR4: 00000000000006e0
     Stack:
      ffff88000dd12940 ffff88000dbc90c0 ffff88000da1dfd8 ffff88000dd03f48
      ffffffff81109e2b ffff88000dd12940 0000000000000000 ffff88000dd03f68
      ffffffff81109e9e 0000000000000000 0000000000012940 ffff88000dd03f98
     Call Trace:
      <IRQ>
      [<ffffffff81109e2b>] ttwu_do_activate.constprop.56+0x6d/0x79
      [<ffffffff81109e9e>] sched_ttwu_pending+0x67/0x84
      [<ffffffff8110c845>] scheduler_ipi+0x15a/0x2b0
      [<ffffffff8104dfb4>] smp_reschedule_interrupt+0x38/0x41
      [<ffffffff8173bf5d>] reschedule_interrupt+0x6d/0x80
      <EOI>
      [<ffffffff810ff484>] ? __atomic_notifier_call_chain+0x5/0xc1
      [<ffffffff8105cc30>] ? native_safe_halt+0xd/0x16
      [<ffffffff81015f10>] default_idle+0x147/0x282
      [<ffffffff81017026>] arch_cpu_idle+0x3d/0x5d
      [<ffffffff81127d6a>] cpu_idle_loop+0x46d/0x5db
      [<ffffffff81127f5c>] cpu_startup_entry+0x84/0x84
      [<ffffffff8104f4f8>] start_secondary+0x3c8/0x3d5
      [...]
    
    Fix this by directly calling poke_int3_handler() from the int3
    exception handler (analogically to what ftrace has been doing
    already), instead of relying on notifier, registration of which
    might not have yet been finalized by the time of the first trap.
    
    Reported-and-tested-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/alpine.LNX.2.00.1307231007490.14024@pobox.suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1b23a1c92746..8c8093b146ca 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -58,6 +58,7 @@
 #include <asm/mce.h>
 #include <asm/fixmap.h>
 #include <asm/mach_traps.h>
+#include <asm/alternative.h>
 
 #ifdef CONFIG_X86_64
 #include <asm/x86_init.h>
@@ -327,6 +328,9 @@ dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_co
 	    ftrace_int3_handler(regs))
 		return;
 #endif
+	if (poke_int3_handler(regs))
+		return;
+
 	prev_state = exception_enter();
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,

commit 4df05f361937ee86e5a8c9ead8aeb6a19ea9b7d7
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Jul 16 11:34:41 2013 -0700

    x86: Make sure IDT is page aligned
    
    Since the IDT is referenced from a fixmap, make sure it is page aligned.
    Merge with 32-bit one, since it was already aligned to deal with F00F
    bug. Since bss is cleared before IDT setup, it can live there. This also
    moves the other *_idt_table variables into common locations.
    
    This avoids the risk of the IDT ever being moved in the bss and having
    the mapping be offset, resulting in calling incorrect handlers. In the
    current upstream kernel this is not a manifested bug, but heavily patched
    kernels (such as those using the PaX patch series) did encounter this bug.
    
    The tables other than idt_table technically do not need to be page
    aligned, at least not at the current time, but using a common
    declaration avoids mistakes.  On 64 bits the table is exactly one page
    long, anyway.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Link: http://lkml.kernel.org/r/20130716183441.GA14232@www.outflux.net
    Reported-by: PaX Team <pageexec@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b0865e88d3cc..1b23a1c92746 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -63,19 +63,19 @@
 #include <asm/x86_init.h>
 #include <asm/pgalloc.h>
 #include <asm/proto.h>
+
+/* No need to be aligned, but done to keep all IDTs defined the same way. */
+gate_desc debug_idt_table[NR_VECTORS] __page_aligned_bss;
 #else
 #include <asm/processor-flags.h>
 #include <asm/setup.h>
 
 asmlinkage int system_call(void);
-
-/*
- * The IDT has to be page-aligned to simplify the Pentium
- * F0 0F bug workaround.
- */
-gate_desc idt_table[NR_VECTORS] __page_aligned_data = { { { { 0, 0 } } }, };
 #endif
 
+/* Must be page-aligned because the real IDT is used in a fixmap. */
+gate_desc idt_table[NR_VECTORS] __page_aligned_bss;
+
 DECLARE_BITMAP(used_vectors, NR_VECTORS);
 EXPORT_SYMBOL_GPL(used_vectors);
 

commit 96a3d998fb92c28b9862297fcf93a24d8a0eac1d
Merge: 3045f94a20cc 5236eb968ec2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 2 16:31:49 2013 -0700

    Merge branch 'x86-tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 tracing updates from Ingo Molnar:
     "This tree adds IRQ vector tracepoints that are named after the handler
      and which output the vector #, based on a zero-overhead approach that
      relies on changing the IDT entries, by Seiji Aguchi.
    
      The new tracepoints look like this:
    
       # perf list | grep -i irq_vector
        irq_vectors:local_timer_entry                      [Tracepoint event]
        irq_vectors:local_timer_exit                       [Tracepoint event]
        irq_vectors:reschedule_entry                       [Tracepoint event]
        irq_vectors:reschedule_exit                        [Tracepoint event]
        irq_vectors:spurious_apic_entry                    [Tracepoint event]
        irq_vectors:spurious_apic_exit                     [Tracepoint event]
        irq_vectors:error_apic_entry                       [Tracepoint event]
        irq_vectors:error_apic_exit                        [Tracepoint event]
       [...]"
    
    * 'x86-tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/tracing: Add config option checking to the definitions of mce handlers
      trace,x86: Do not call local_irq_save() in load_current_idt()
      trace,x86: Move creation of irq tracepoints from apic.c to irq.c
      x86, trace: Add irq vector tracepoints
      x86: Rename variables for debugging
      x86, trace: Introduce entering/exiting_irq()
      tracing: Add DEFINE_EVENT_FN() macro

commit 55a0d3ff603a69ea4ec7677effd457f838390afc
Merge: 35c23d5d7976 13bfd47a0ef6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 2 16:25:06 2013 -0700

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 debug update from Ingo Molnar:
     "Misc debuggability improvements:
    
       - Optimize the x86 CPU register printout a bit
       - Expose the tboot TXT log via debugfs
       - Small do_debug() cleanup"
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/tboot: Provide debugfs interfaces to access TXT log
      x86: Remove weird PTR_ERR() in do_debug
      x86/debug: Only print out DR registers if they are not power-on defaults

commit 629f4f9d59a27d8e58aa612e886e6a9a63ea7aeb
Author: Seiji Aguchi <seiji.aguchi@hds.com>
Date:   Thu Jun 20 11:45:44 2013 -0400

    x86: Rename variables for debugging
    
    Rename variables for debugging to describe meaning of them precisely.
    
    Also, introduce a generic way to switch IDT by checking a current state,
    debug on/off.
    
    Signed-off-by: Seiji Aguchi <seiji.aguchi@hds.com>
    Link: http://lkml.kernel.org/r/51C323A8.7050905@hds.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 772e2a846dec..d27182d6ed2a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -785,7 +785,7 @@ void __init trap_init(void)
 	x86_init.irqs.trap_init();
 
 #ifdef CONFIG_X86_64
-	memcpy(&nmi_idt_table, &idt_table, IDT_ENTRIES * 16);
+	memcpy(&debug_idt_table, &idt_table, IDT_ENTRIES * 16);
 	set_nmi_gate(X86_TRAP_DB, &debug);
 	set_nmi_gate(X86_TRAP_BP, &int3);
 #endif

commit 5a802e15308f152247433d87050d1bfbf9613483
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sun Jun 16 14:12:47 2013 +0930

    x86: Remove weird PTR_ERR() in do_debug
    
    62edab905 changed the argument to notify_die() from dr6 to &dr6,
    but weirdly, used PTR_ERR() to cast it to a long.  Since dr6 is
    on the stack, this is an abuse of PTR_ERR().  Cast to long, as
    per kernel standard.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: K.Prasad <prasad@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1371357768-4968-8-git-send-email-rusty@rustcorp.com.au
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 772e2a846dec..9340dfb7057f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -437,7 +437,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	/* Store the virtualized DR6 value */
 	tsk->thread.debugreg6 = dr6;
 
-	if (notify_die(DIE_DEBUG, "debug", regs, PTR_ERR(&dr6), error_code,
+	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, error_code,
 							SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 

commit 4d067d8e056d76a3327f0517c7722db55e7888fc
Author: Borislav Petkov <bp@suse.de>
Date:   Thu May 9 12:02:29 2013 +0200

    x86: Extend #DF debugging aid to 64-bit
    
    It is sometimes very helpful to be able to pinpoint the location which
    causes a double fault before it turns into a triple fault and the
    machine reboots. We have this for 32-bit already so extend it to 64-bit.
    On 64-bit we get the register snapshot at #DF time and not from the
    first exception which actually causes the #DF. It should be close
    enough, though.
    
    [ hpa: and definitely better than nothing, which is what we have now. ]
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1368093749-31296-1-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 772e2a846dec..167d481c5fd3 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -254,6 +254,9 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	tsk->thread.error_code = error_code;
 	tsk->thread.trap_nr = X86_TRAP_DF;
 
+#ifdef CONFIG_DOUBLEFAULT
+	df_debug(regs, error_code);
+#endif
 	/*
 	 * This is always a kernel trap and never fixable (and thus must
 	 * never return).

commit 01c7cd0ef5d98fdd007d8a04c9f834bead6e5ee1
Merge: 39b2f8656e2a c889ba801dc3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 08:37:24 2013 -0700

    Merge branch 'x86-kaslr-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perparatory x86 kasrl changes from Ingo Molnar:
     "This contains changes from the ongoing KASLR work, by Kees Cook.
    
      The main changes are the use of a read-only IDT on x86 (which
      decouples the userspace visible virtual IDT address from the physical
      address), and a rework of ELF relocation support, in preparation of
      random, boot-time kernel image relocation."
    
    * 'x86-kaslr-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, relocs: Refactor the relocs tool to merge 32- and 64-bit ELF
      x86, relocs: Build separate 32/64-bit tools
      x86, relocs: Add 64-bit ELF support to relocs tool
      x86, relocs: Consolidate processing logic
      x86, relocs: Generalize ELF structure names
      x86: Use a read-only IDT alias on all CPUs

commit 4eefbe792baedb474e256d35370849992fcf1c79
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Apr 10 12:24:22 2013 -0700

    x86: Use a read-only IDT alias on all CPUs
    
    Make a copy of the IDT (as seen via the "sidt" instruction) read-only.
    This primarily removes the IDT from being a target for arbitrary memory
    write attacks, and has the added benefit of also not leaking the kernel
    base offset, if it has been relocated.
    
    We already did this on vendor == Intel and family == 5 because of the
    F0 0F bug -- regardless of if a particular CPU had the F0 0F bug or
    not.  Since the workaround was so cheap, there simply was no reason to
    be very specific.  This patch extends the readonly alias to all CPUs,
    but does not activate the #PF to #UD conversion code needed to deliver
    the proper exception in the F0 0F case except on Intel family 5
    processors.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Link: http://lkml.kernel.org/r/20130410192422.GA17344@www.outflux.net
    Cc: Eric Northup <digitaleric@google.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 68bda7a84159..10e24462c058 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -56,6 +56,7 @@
 #include <asm/fpu-internal.h>
 #include <asm/mce.h>
 #include <asm/context_tracking.h>
+#include <asm/fixmap.h>
 
 #include <asm/mach_traps.h>
 
@@ -752,6 +753,14 @@ void __init trap_init(void)
 	set_bit(SYSCALL_VECTOR, used_vectors);
 #endif
 
+	/*
+	 * Set the IDT descriptor to a fixed read-only location, so that the
+	 * "sidt" instruction will not leak the location of the kernel, and
+	 * to defend the IDT against arbitrary memory write vulnerabilities.
+	 * It will be reloaded in cpu_init() */
+	__set_fixmap(FIX_RO_IDT, __pa_symbol(idt_table), PAGE_KERNEL_RO);
+	idt_descr.address = fix_to_virt(FIX_RO_IDT);
+
 	/*
 	 * Should be a barrier for any external CPU state:
 	 */

commit 6c1e0256fad84a843d915414e4b5973b7443d48d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Feb 24 01:19:14 2013 +0100

    context_tracking: Restore correct previous context state on exception exit
    
    On exception exit, we restore the previous context tracking state based on
    the regs of the interrupted frame. Iff that frame is in user mode as
    stated by user_mode() helper, we restore the context tracking user mode.
    
    However there is a tiny chunck of low level arch code after we pass through
    user_enter() and until the CPU eventually resumes userspace.
    If an exception happens in this tiny area, exception_enter() correctly
    exits the context tracking user mode but exception_exit() won't restore
    it because of the value returned by user_mode(regs).
    
    As a result we may return to userspace with the wrong context tracking
    state.
    
    To fix this, change exception_enter() to return the context tracking state
    prior to its call and pass this saved state to exception_exit(). This restores
    the real context tracking state of the interrupted frame.
    
    (May be this patch was suggested to me, I don't recall exactly. If so,
    sorry for the missing credit).
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Mats Liljegren <mats.liljegren@enea.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ecc4ccbdd0cf..ff6d2271cbe2 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -175,34 +175,38 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 #define DO_ERROR(trapnr, signr, str, name)				\
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 {									\
-	exception_enter(regs);						\
+	enum ctx_state prev_state;					\
+									\
+	prev_state = exception_enter();					\
 	if (notify_die(DIE_TRAP, str, regs, error_code,			\
 			trapnr, signr) == NOTIFY_STOP) {		\
-		exception_exit(regs);					\
+		exception_exit(prev_state);				\
 		return;							\
 	}								\
 	conditional_sti(regs);						\
 	do_trap(trapnr, signr, str, regs, error_code, NULL);		\
-	exception_exit(regs);						\
+	exception_exit(prev_state);					\
 }
 
 #define DO_ERROR_INFO(trapnr, signr, str, name, sicode, siaddr)		\
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 {									\
 	siginfo_t info;							\
+	enum ctx_state prev_state;					\
+									\
 	info.si_signo = signr;						\
 	info.si_errno = 0;						\
 	info.si_code = sicode;						\
 	info.si_addr = (void __user *)siaddr;				\
-	exception_enter(regs);						\
+	prev_state = exception_enter();					\
 	if (notify_die(DIE_TRAP, str, regs, error_code,			\
 			trapnr, signr) == NOTIFY_STOP) {		\
-		exception_exit(regs);					\
+		exception_exit(prev_state);				\
 		return;							\
 	}								\
 	conditional_sti(regs);						\
 	do_trap(trapnr, signr, str, regs, error_code, &info);		\
-	exception_exit(regs);						\
+	exception_exit(prev_state);					\
 }
 
 DO_ERROR_INFO(X86_TRAP_DE, SIGFPE, "divide error", divide_error, FPE_INTDIV,
@@ -225,14 +229,16 @@ DO_ERROR_INFO(X86_TRAP_AC, SIGBUS, "alignment check", alignment_check,
 /* Runs on IST stack */
 dotraplinkage void do_stack_segment(struct pt_regs *regs, long error_code)
 {
-	exception_enter(regs);
+	enum ctx_state prev_state;
+
+	prev_state = exception_enter();
 	if (notify_die(DIE_TRAP, "stack segment", regs, error_code,
 		       X86_TRAP_SS, SIGBUS) != NOTIFY_STOP) {
 		preempt_conditional_sti(regs);
 		do_trap(X86_TRAP_SS, SIGBUS, "stack segment", regs, error_code, NULL);
 		preempt_conditional_cli(regs);
 	}
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
@@ -240,7 +246,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	static const char str[] = "double fault";
 	struct task_struct *tsk = current;
 
-	exception_enter(regs);
+	exception_enter();
 	/* Return not checked because double check cannot be ignored */
 	notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);
 
@@ -260,8 +266,9 @@ dotraplinkage void __kprobes
 do_general_protection(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk;
+	enum ctx_state prev_state;
 
-	exception_enter(regs);
+	prev_state = exception_enter();
 	conditional_sti(regs);
 
 #ifdef CONFIG_X86_32
@@ -299,12 +306,14 @@ do_general_protection(struct pt_regs *regs, long error_code)
 
 	force_sig(SIGSEGV, tsk);
 exit:
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 /* May run on IST stack. */
 dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_code)
 {
+	enum ctx_state prev_state;
+
 #ifdef CONFIG_DYNAMIC_FTRACE
 	/*
 	 * ftrace must be first, everything else may cause a recursive crash.
@@ -314,7 +323,7 @@ dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_co
 	    ftrace_int3_handler(regs))
 		return;
 #endif
-	exception_enter(regs);
+	prev_state = exception_enter();
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
@@ -335,7 +344,7 @@ dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_co
 	preempt_conditional_cli(regs);
 	debug_stack_usage_dec();
 exit:
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 #ifdef CONFIG_X86_64
@@ -392,11 +401,12 @@ asmlinkage __kprobes struct pt_regs *sync_regs(struct pt_regs *eregs)
 dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk = current;
+	enum ctx_state prev_state;
 	int user_icebp = 0;
 	unsigned long dr6;
 	int si_code;
 
-	exception_enter(regs);
+	prev_state = exception_enter();
 
 	get_debugreg(dr6, 6);
 
@@ -466,7 +476,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	debug_stack_usage_dec();
 
 exit:
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 /*
@@ -560,17 +570,21 @@ void math_error(struct pt_regs *regs, int error_code, int trapnr)
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-	exception_enter(regs);
+	enum ctx_state prev_state;
+
+	prev_state = exception_enter();
 	math_error(regs, error_code, X86_TRAP_MF);
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 dotraplinkage void
 do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-	exception_enter(regs);
+	enum ctx_state prev_state;
+
+	prev_state = exception_enter();
 	math_error(regs, error_code, X86_TRAP_XF);
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 dotraplinkage void
@@ -638,7 +652,9 @@ EXPORT_SYMBOL_GPL(math_state_restore);
 dotraplinkage void __kprobes
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
-	exception_enter(regs);
+	enum ctx_state prev_state;
+
+	prev_state = exception_enter();
 	BUG_ON(use_eager_fpu());
 
 #ifdef CONFIG_MATH_EMULATION
@@ -649,7 +665,7 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 
 		info.regs = regs;
 		math_emulate(&info);
-		exception_exit(regs);
+		exception_exit(prev_state);
 		return;
 	}
 #endif
@@ -657,15 +673,16 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 #ifdef CONFIG_X86_32
 	conditional_sti(regs);
 #endif
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 #ifdef CONFIG_X86_32
 dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 {
 	siginfo_t info;
+	enum ctx_state prev_state;
 
-	exception_enter(regs);
+	prev_state = exception_enter();
 	local_irq_enable();
 
 	info.si_signo = SIGILL;
@@ -677,7 +694,7 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 		do_trap(X86_TRAP_IRET, SIGILL, "iret exception", regs, error_code,
 			&info);
 	}
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 #endif
 

commit 56dd9470d7c8734f055da2a6bac553caf4a468eb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Feb 24 00:23:25 2013 +0100

    context_tracking: Move exception handling to generic code
    
    Exceptions handling on context tracking should share common
    treatment: on entry we exit user mode if the exception triggered
    in that context. Then on exception exit we return to that previous
    context.
    
    Generalize this to avoid duplication across archs.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Mats Liljegren <mats.liljegren@enea.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 68bda7a84159..ecc4ccbdd0cf 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -12,6 +12,7 @@
 
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
+#include <linux/context_tracking.h>
 #include <linux/interrupt.h>
 #include <linux/kallsyms.h>
 #include <linux/spinlock.h>
@@ -55,8 +56,6 @@
 #include <asm/i387.h>
 #include <asm/fpu-internal.h>
 #include <asm/mce.h>
-#include <asm/context_tracking.h>
-
 #include <asm/mach_traps.h>
 
 #ifdef CONFIG_X86_64

commit 8170e6bed465b4b0c7687f93e9948aca4358a33b
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Jan 24 12:19:52 2013 -0800

    x86, 64bit: Use a #PF handler to materialize early mappings on demand
    
    Linear mode (CR0.PG = 0) is mutually exclusive with 64-bit mode; all
    64-bit code has to use page tables.  This makes it awkward before we
    have first set up properly all-covering page tables to access objects
    that are outside the static kernel range.
    
    So far we have dealt with that simply by mapping a fixed amount of
    low memory, but that fails in at least two upcoming use cases:
    
    1. We will support load and run kernel, struct boot_params, ramdisk,
       command line, etc. above the 4 GiB mark.
    2. need to access ramdisk early to get microcode to update that as
       early possible.
    
    We could use early_iomap to access them too, but it will make code to
    messy and hard to be unified with 32 bit.
    
    Hence, set up a #PF table and use a fixed number of buffers to set up
    page tables on demand.  If the buffers fill up then we simply flush
    them and start over.  These buffers are all in __initdata, so it does
    not increase RAM usage at runtime.
    
    Thus, with the help of the #PF handler, we can set the final kernel
    mapping from blank, and switch to init_level4_pgt later.
    
    During the switchover in head_64.S, before #PF handler is available,
    we use three pages to handle kernel crossing 1G, 512G boundaries with
    sharing page by playing games with page aliasing: the same page is
    mapped twice in the higher-level tables with appropriate wraparound.
    The kernel region itself will be properly mapped; other mappings may
    be spurious.
    
    early_make_pgtable is using kernel high mapping address to access pages
    to set page table.
    
    -v4: Add phys_base offset to make kexec happy, and add
            init_mapping_kernel()   - Yinghai
    -v5: fix compiling with xen, and add back ident level3 and level2 for xen
         also move back init_level4_pgt from BSS to DATA again.
         because we have to clear it anyway.  - Yinghai
    -v6: switch to init_level4_pgt in init_mem_mapping. - Yinghai
    -v7: remove not needed clear_page for init_level4_page
         it is with fill 512,8,0 already in head_64.S  - Yinghai
    -v8: we need to keep that handler alive until init_mem_mapping and don't
         let early_trap_init to trash that early #PF handler.
         So split early_trap_pf_init out and move it down. - Yinghai
    -v9: switchover only cover kernel space instead of 1G so could avoid
         touch possible mem holes. - Yinghai
    -v11: change far jmp back to far return to initial_code, that is needed
         to fix failure that is reported by Konrad on AMD systems.  - Yinghai
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/1359058816-7615-12-git-send-email-yinghai@kernel.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ecffca11f4e9..68bda7a84159 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -688,10 +688,19 @@ void __init early_trap_init(void)
 	set_intr_gate_ist(X86_TRAP_DB, &debug, DEBUG_STACK);
 	/* int3 can be called from all */
 	set_system_intr_gate_ist(X86_TRAP_BP, &int3, DEBUG_STACK);
+#ifdef CONFIG_X86_32
 	set_intr_gate(X86_TRAP_PF, &page_fault);
+#endif
 	load_idt(&idt_descr);
 }
 
+void __init early_trap_pf_init(void)
+{
+#ifdef CONFIG_X86_64
+	set_intr_gate(X86_TRAP_PF, &page_fault);
+#endif
+}
+
 void __init trap_init(void)
 {
 	int i;

commit 1bd12c91de35756129b8ffe28a4fe45177b86381
Merge: 3dbff0d5e8ec bc3eba606827
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 19 13:02:23 2012 -0800

    Merge branch 'x86/nuke386' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull one final 386 removal patch from Peter Anvin.
    
    IRQ 13 FPU error handling is gone.  That was not one of the proudest
    moments in PC history.
    
    * 'x86/nuke386' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, 386 removal: Remove support for IRQ 13 FPU error reporting

commit bc3eba60682750dd7b45ea616d65c926fc3b8be8
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Mon Dec 17 11:26:31 2012 -0800

    x86, 386 removal: Remove support for IRQ 13 FPU error reporting
    
    Remove support for FPU error reporting via IRQ 13, as opposed to
    exception 16 (#MF).  One last remnant of i386 gone.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Alan Cox <alan@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 8276dc6794cc..4e1478c12344 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -69,9 +69,6 @@
 
 asmlinkage int system_call(void);
 
-/* Do we ignore FPU interrupts ? */
-char ignore_fpu_irq;
-
 /*
  * The IDT has to be page-aligned to simplify the Pentium
  * F0 0F bug workaround.
@@ -564,9 +561,6 @@ void math_error(struct pt_regs *regs, int error_code, int trapnr)
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-#ifdef CONFIG_X86_32
-	ignore_fpu_irq = 1;
-#endif
 	exception_enter(regs);
 	math_error(regs, error_code, X86_TRAP_MF);
 	exception_exit(regs);

commit 91d1aa43d30505b0b825db8898ffc80a8eca96c7
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 27 19:33:25 2012 +0100

    context_tracking: New context tracking susbsystem
    
    Create a new subsystem that probes on kernel boundaries
    to keep track of the transitions between level contexts
    with two basic initial contexts: user or kernel.
    
    This is an abstraction of some RCU code that use such tracking
    to implement its userspace extended quiescent state.
    
    We need to pull this up from RCU into this new level of indirection
    because this tracking is also going to be used to implement an "on
    demand" generic virtual cputime accounting. A necessary step to
    shutdown the tick while still accounting the cputime.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Gilad Ben-Yossef <gilad@benyossef.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    [ paulmck: fix whitespace error and email address. ]
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 8276dc6794cc..eb8586693e0b 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -55,7 +55,7 @@
 #include <asm/i387.h>
 #include <asm/fpu-internal.h>
 #include <asm/mce.h>
-#include <asm/rcu.h>
+#include <asm/context_tracking.h>
 
 #include <asm/mach_traps.h>
 

commit ac07f5c3cb0cf19258c55cdf210aa4ac91ca7330
Merge: 3b29b03a4623 b1a74bf82123
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 11:10:52 2012 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/fpu update from Ingo Molnar:
     "The biggest change is the addition of the non-lazy (eager) FPU saving
      support model and enabling it on CPUs with optimized xsaveopt/xrstor
      FPU state saving instructions.
    
      There are also various Sparse fixes"
    
    Fix up trivial add-add conflict in arch/x86/kernel/traps.c
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, kvm: fix kvm's usage of kernel_fpu_begin/end()
      x86, fpu: remove cpu_has_xmm check in the fx_finit()
      x86, fpu: make eagerfpu= boot param tri-state
      x86, fpu: enable eagerfpu by default for xsaveopt
      x86, fpu: decouple non-lazy/eager fpu restore from xsave
      x86, fpu: use non-lazy fpu restore for processors supporting xsave
      lguest, x86: handle guest TS bit for lazy/non-lazy fpu host models
      x86, fpu: always use kernel_fpu_begin/end() for in-kernel FPU usage
      x86, kvm: use kernel_fpu_begin/end() in kvm_load/put_guest_fpu()
      x86, fpu: remove unnecessary user_fpu_end() in save_xstate_sig()
      x86, fpu: drop_fpu() before restoring new state from sigframe
      x86, fpu: Unify signal handling code paths for x86 and x86_64 kernels
      x86, fpu: Consolidate inline asm routines for saving/restoring fpu state
      x86, signal: Cleanup ifdefs and is_ia32, is_x32

commit da8347969f324db5f572581397d9b3a8e108cda4
Merge: 80749df4a149 c416ddf5b909
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 10:46:27 2012 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/asm changes from Ingo Molnar:
     "The one change that stands out is the alternatives patching change
      that prevents us from ever patching back instructions from SMP to UP:
      this simplifies things and speeds up CPU hotplug.
    
      Other than that it's smaller fixes, cleanups and improvements."
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Unspaghettize do_trap()
      x86_64: Work around old GAS bug
      x86: Use REP BSF unconditionally
      x86: Prefer TZCNT over BFS
      x86/64: Adjust types of temporaries used by ffs()/fls()/fls64()
      x86: Drop unnecessary kernel_eflags variable on 64-bit
      x86/smp: Don't ever patch back to UP if we unplug cpus

commit 6ba3c97a38803883c2eee489505796cb0a727122
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 11 20:26:35 2012 +0200

    x86: Exception hooks for userspace RCU extended QS
    
    Add necessary hooks to x86 exception for userspace
    RCU extended quiescent state support.
    
    This includes traps, page fault, debug exceptions, etc...
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 74850e559449..378967578f22 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -55,6 +55,7 @@
 #include <asm/i387.h>
 #include <asm/fpu-internal.h>
 #include <asm/mce.h>
+#include <asm/rcu.h>
 
 #include <asm/mach_traps.h>
 
@@ -180,11 +181,15 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 #define DO_ERROR(trapnr, signr, str, name)				\
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 {									\
-	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr)	\
-							== NOTIFY_STOP)	\
+	exception_enter(regs);						\
+	if (notify_die(DIE_TRAP, str, regs, error_code,			\
+			trapnr, signr) == NOTIFY_STOP) {		\
+		exception_exit(regs);					\
 		return;							\
+	}								\
 	conditional_sti(regs);						\
 	do_trap(trapnr, signr, str, regs, error_code, NULL);		\
+	exception_exit(regs);						\
 }
 
 #define DO_ERROR_INFO(trapnr, signr, str, name, sicode, siaddr)		\
@@ -195,11 +200,15 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 	info.si_errno = 0;						\
 	info.si_code = sicode;						\
 	info.si_addr = (void __user *)siaddr;				\
-	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr)	\
-							== NOTIFY_STOP)	\
+	exception_enter(regs);						\
+	if (notify_die(DIE_TRAP, str, regs, error_code,			\
+			trapnr, signr) == NOTIFY_STOP) {		\
+		exception_exit(regs);					\
 		return;							\
+	}								\
 	conditional_sti(regs);						\
 	do_trap(trapnr, signr, str, regs, error_code, &info);		\
+	exception_exit(regs);						\
 }
 
 DO_ERROR_INFO(X86_TRAP_DE, SIGFPE, "divide error", divide_error, FPE_INTDIV,
@@ -222,12 +231,14 @@ DO_ERROR_INFO(X86_TRAP_AC, SIGBUS, "alignment check", alignment_check,
 /* Runs on IST stack */
 dotraplinkage void do_stack_segment(struct pt_regs *regs, long error_code)
 {
+	exception_enter(regs);
 	if (notify_die(DIE_TRAP, "stack segment", regs, error_code,
-			X86_TRAP_SS, SIGBUS) == NOTIFY_STOP)
-		return;
-	preempt_conditional_sti(regs);
-	do_trap(X86_TRAP_SS, SIGBUS, "stack segment", regs, error_code, NULL);
-	preempt_conditional_cli(regs);
+		       X86_TRAP_SS, SIGBUS) != NOTIFY_STOP) {
+		preempt_conditional_sti(regs);
+		do_trap(X86_TRAP_SS, SIGBUS, "stack segment", regs, error_code, NULL);
+		preempt_conditional_cli(regs);
+	}
+	exception_exit(regs);
 }
 
 dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
@@ -235,6 +246,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	static const char str[] = "double fault";
 	struct task_struct *tsk = current;
 
+	exception_enter(regs);
 	/* Return not checked because double check cannot be ignored */
 	notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);
 
@@ -255,27 +267,28 @@ do_general_protection(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk;
 
+	exception_enter(regs);
 	conditional_sti(regs);
 
 #ifdef CONFIG_X86_32
 	if (regs->flags & X86_VM_MASK) {
 		local_irq_enable();
 		handle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);
-		return;
+		goto exit;
 	}
 #endif
 
 	tsk = current;
 	if (!user_mode(regs)) {
 		if (fixup_exception(regs))
-			return;
+			goto exit;
 
 		tsk->thread.error_code = error_code;
 		tsk->thread.trap_nr = X86_TRAP_GP;
-		if (!notify_die(DIE_GPF, "general protection fault", regs, error_code,
-			       X86_TRAP_GP, SIGSEGV) == NOTIFY_STOP)
+		if (notify_die(DIE_GPF, "general protection fault", regs, error_code,
+			       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)
 			die("general protection fault", regs, error_code);
-		return;
+		goto exit;
 	}
 
 	tsk->thread.error_code = error_code;
@@ -291,7 +304,8 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	}
 
 	force_sig(SIGSEGV, tsk);
-	return;
+exit:
+	exception_exit(regs);
 }
 
 /* May run on IST stack. */
@@ -306,15 +320,16 @@ dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_co
 	    ftrace_int3_handler(regs))
 		return;
 #endif
+	exception_enter(regs);
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
-		return;
+		goto exit;
 #endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */
 
 	if (notify_die(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
-		return;
+		goto exit;
 
 	/*
 	 * Let others (NMI) know that the debug stack is in use
@@ -325,6 +340,8 @@ dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_co
 	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, NULL);
 	preempt_conditional_cli(regs);
 	debug_stack_usage_dec();
+exit:
+	exception_exit(regs);
 }
 
 #ifdef CONFIG_X86_64
@@ -385,6 +402,8 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	unsigned long dr6;
 	int si_code;
 
+	exception_enter(regs);
+
 	get_debugreg(dr6, 6);
 
 	/* Filter out all the reserved bits which are preset to 1 */
@@ -400,7 +419,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 
 	/* Catch kmemcheck conditions first of all! */
 	if ((dr6 & DR_STEP) && kmemcheck_trap(regs))
-		return;
+		goto exit;
 
 	/* DR6 may or may not be cleared by the CPU */
 	set_debugreg(0, 6);
@@ -415,7 +434,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 
 	if (notify_die(DIE_DEBUG, "debug", regs, PTR_ERR(&dr6), error_code,
 							SIGTRAP) == NOTIFY_STOP)
-		return;
+		goto exit;
 
 	/*
 	 * Let others (NMI) know that the debug stack is in use
@@ -431,7 +450,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 					X86_TRAP_DB);
 		preempt_conditional_cli(regs);
 		debug_stack_usage_dec();
-		return;
+		goto exit;
 	}
 
 	/*
@@ -452,7 +471,8 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	preempt_conditional_cli(regs);
 	debug_stack_usage_dec();
 
-	return;
+exit:
+	exception_exit(regs);
 }
 
 /*
@@ -549,14 +569,17 @@ dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 #ifdef CONFIG_X86_32
 	ignore_fpu_irq = 1;
 #endif
-
+	exception_enter(regs);
 	math_error(regs, error_code, X86_TRAP_MF);
+	exception_exit(regs);
 }
 
 dotraplinkage void
 do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 {
+	exception_enter(regs);
 	math_error(regs, error_code, X86_TRAP_XF);
+	exception_exit(regs);
 }
 
 dotraplinkage void
@@ -623,6 +646,7 @@ EXPORT_SYMBOL_GPL(math_state_restore);
 dotraplinkage void __kprobes
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
+	exception_enter(regs);
 #ifdef CONFIG_MATH_EMULATION
 	if (read_cr0() & X86_CR0_EM) {
 		struct math_emu_info info = { };
@@ -631,6 +655,7 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 
 		info.regs = regs;
 		math_emulate(&info);
+		exception_exit(regs);
 		return;
 	}
 #endif
@@ -638,12 +663,15 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 #ifdef CONFIG_X86_32
 	conditional_sti(regs);
 #endif
+	exception_exit(regs);
 }
 
 #ifdef CONFIG_X86_32
 dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 {
 	siginfo_t info;
+
+	exception_enter(regs);
 	local_irq_enable();
 
 	info.si_signo = SIGILL;
@@ -651,10 +679,11 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 	info.si_code = ILL_BADSTK;
 	info.si_addr = NULL;
 	if (notify_die(DIE_TRAP, "iret exception", regs, error_code,
-			X86_TRAP_IRET, SIGILL) == NOTIFY_STOP)
-		return;
-	do_trap(X86_TRAP_IRET, SIGILL, "iret exception", regs, error_code,
-		&info);
+			X86_TRAP_IRET, SIGILL) != NOTIFY_STOP) {
+		do_trap(X86_TRAP_IRET, SIGILL, "iret exception", regs, error_code,
+			&info);
+	}
+	exception_exit(regs);
 }
 #endif
 

commit ef3f628872c838933a279d0d7e63e707783c9710
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Sep 24 21:05:52 2012 +0200

    x86: Unspaghettize do_general_protection()
    
    There is some unnatural label based layout in this function.
    Convert the unnecessary goto to readable conditional blocks.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b481341c9369..74850e559449 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -258,13 +258,25 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	conditional_sti(regs);
 
 #ifdef CONFIG_X86_32
-	if (regs->flags & X86_VM_MASK)
-		goto gp_in_vm86;
+	if (regs->flags & X86_VM_MASK) {
+		local_irq_enable();
+		handle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);
+		return;
+	}
 #endif
 
 	tsk = current;
-	if (!user_mode(regs))
-		goto gp_in_kernel;
+	if (!user_mode(regs)) {
+		if (fixup_exception(regs))
+			return;
+
+		tsk->thread.error_code = error_code;
+		tsk->thread.trap_nr = X86_TRAP_GP;
+		if (!notify_die(DIE_GPF, "general protection fault", regs, error_code,
+			       X86_TRAP_GP, SIGSEGV) == NOTIFY_STOP)
+			die("general protection fault", regs, error_code);
+		return;
+	}
 
 	tsk->thread.error_code = error_code;
 	tsk->thread.trap_nr = X86_TRAP_GP;
@@ -280,24 +292,6 @@ do_general_protection(struct pt_regs *regs, long error_code)
 
 	force_sig(SIGSEGV, tsk);
 	return;
-
-#ifdef CONFIG_X86_32
-gp_in_vm86:
-	local_irq_enable();
-	handle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);
-	return;
-#endif
-
-gp_in_kernel:
-	if (fixup_exception(regs))
-		return;
-
-	tsk->thread.error_code = error_code;
-	tsk->thread.trap_nr = X86_TRAP_GP;
-	if (notify_die(DIE_GPF, "general protection fault", regs, error_code,
-			X86_TRAP_GP, SIGSEGV) == NOTIFY_STOP)
-		return;
-	die("general protection fault", regs, error_code);
 }
 
 /* May run on IST stack. */

commit c416ddf5b909736f5b57d348f5de159693e699ad
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Sep 25 14:51:19 2012 +0200

    x86: Unspaghettize do_trap()
    
    Cleanup the label maze in this function. Having a
    seperate function to first handle the traps that don't
    generate a signal makes it easier to convert into
    more readable conditional paths.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1348577479-2564-1-git-send-email-fweisbec@gmail.com
    [ Fixed 32-bit build failure. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b481341c9369..6ff771559af3 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -107,30 +107,45 @@ static inline void preempt_conditional_cli(struct pt_regs *regs)
 	dec_preempt_count();
 }
 
-static void __kprobes
-do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
-	long error_code, siginfo_t *info)
+static int __kprobes
+do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
+		  struct pt_regs *regs,	long error_code)
 {
-	struct task_struct *tsk = current;
-
 #ifdef CONFIG_X86_32
 	if (regs->flags & X86_VM_MASK) {
 		/*
-		 * traps 0, 1, 3, 4, and 5 should be forwarded to vm86.
+		 * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.
 		 * On nmi (interrupt 2), do_trap should not be called.
 		 */
-		if (trapnr < X86_TRAP_UD)
-			goto vm86_trap;
-		goto trap_signal;
+		if (trapnr < X86_TRAP_UD) {
+			if (!handle_vm86_trap((struct kernel_vm86_regs *) regs,
+						error_code, trapnr))
+				return 0;
+		}
+		return -1;
 	}
 #endif
+	if (!user_mode(regs)) {
+		if (!fixup_exception(regs)) {
+			tsk->thread.error_code = error_code;
+			tsk->thread.trap_nr = trapnr;
+			die(str, regs, error_code);
+		}
+		return 0;
+	}
 
-	if (!user_mode(regs))
-		goto kernel_trap;
+	return -1;
+}
 
-#ifdef CONFIG_X86_32
-trap_signal:
-#endif
+static void __kprobes
+do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
+	long error_code, siginfo_t *info)
+{
+	struct task_struct *tsk = current;
+
+
+	if (!do_trap_no_signal(tsk, trapnr, str, regs, error_code))
+		return;
 	/*
 	 * We want error_code and trap_nr set for userspace faults and
 	 * kernelspace faults which result in die(), but not
@@ -158,23 +173,6 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 		force_sig_info(signr, info, tsk);
 	else
 		force_sig(signr, tsk);
-	return;
-
-kernel_trap:
-	if (!fixup_exception(regs)) {
-		tsk->thread.error_code = error_code;
-		tsk->thread.trap_nr = trapnr;
-		die(str, regs, error_code);
-	}
-	return;
-
-#ifdef CONFIG_X86_32
-vm86_trap:
-	if (handle_vm86_trap((struct kernel_vm86_regs *) regs,
-						error_code, trapnr))
-		goto trap_signal;
-	return;
-#endif
 }
 
 #define DO_ERROR(trapnr, signr, str, name)				\

commit 5d2bd7009f306c82afddd1ca4d9763ad8473c216
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Sep 6 14:58:52 2012 -0700

    x86, fpu: decouple non-lazy/eager fpu restore from xsave
    
    Decouple non-lazy/eager fpu restore policy from the existence of the xsave
    feature. Introduce a synthetic CPUID flag to represent the eagerfpu
    policy. "eagerfpu=on" boot paramter will enable the policy.
    
    Requested-by: H. Peter Anvin <hpa@zytor.com>
    Requested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1347300665-6209-2-git-send-email-suresh.b.siddha@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ac7d5275f6e8..4f4aba0551b0 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -630,7 +630,7 @@ EXPORT_SYMBOL_GPL(math_state_restore);
 dotraplinkage void __kprobes
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
-	BUG_ON(use_xsave());
+	BUG_ON(use_eager_fpu());
 
 #ifdef CONFIG_MATH_EMULATION
 	if (read_cr0() & X86_CR0_EM) {

commit 304bceda6a18ae0b0240b8aac9a6bdf8ce2d2469
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Aug 24 14:13:02 2012 -0700

    x86, fpu: use non-lazy fpu restore for processors supporting xsave
    
    Fundamental model of the current Linux kernel is to lazily init and
    restore FPU instead of restoring the task state during context switch.
    This changes that fundamental lazy model to the non-lazy model for
    the processors supporting xsave feature.
    
    Reasons driving this model change are:
    
    i. Newer processors support optimized state save/restore using xsaveopt and
    xrstor by tracking the INIT state and MODIFIED state during context-switch.
    This is faster than modifying the cr0.TS bit which has serializing semantics.
    
    ii. Newer glibc versions use SSE for some of the optimized copy/clear routines.
    With certain workloads (like boot, kernel-compilation etc), application
    completes its work with in the first 5 task switches, thus taking upto 5 #DNA
    traps with the kernel not getting a chance to apply the above mentioned
    pre-load heuristic.
    
    iii. Some xstate features (like AMD's LWP feature) don't honor the cr0.TS bit
    and thus will not work correctly in the presence of lazy restore. Non-lazy
    state restore is needed for enabling such features.
    
    Some data on a two socket SNB system:
     * Saved 20K DNA exceptions during boot on a two socket SNB system.
     * Saved 50K DNA exceptions during kernel-compilation workload.
     * Improved throughput of the AVX based checksumming function inside the
       kernel by ~15% as xsave/xrstor is faster than the serializing clts/stts
       pair.
    
    Also now kernel_fpu_begin/end() relies on the patched
    alternative instructions. So move check_fpu() which uses the
    kernel_fpu_begin/end() after alternative_instructions().
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1345842782-24175-7-git-send-email-suresh.b.siddha@intel.com
    Merge 32-bit boot fix from,
    Link: http://lkml.kernel.org/r/1347300665-6209-4-git-send-email-suresh.b.siddha@intel.com
    Cc: Jim Kukunas <james.t.kukunas@linux.intel.com>
    Cc: NeilBrown <neilb@suse.de>
    Cc: Avi Kivity <avi@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b481341c9369..ac7d5275f6e8 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -613,11 +613,12 @@ void math_state_restore(void)
 	}
 
 	__thread_fpu_begin(tsk);
+
 	/*
 	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
 	 */
 	if (unlikely(restore_fpu_checking(tsk))) {
-		__thread_fpu_end(tsk);
+		drop_init_fpu(tsk);
 		force_sig(SIGSEGV, tsk);
 		return;
 	}
@@ -629,6 +630,8 @@ EXPORT_SYMBOL_GPL(math_state_restore);
 dotraplinkage void __kprobes
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
+	BUG_ON(use_xsave());
+
 #ifdef CONFIG_MATH_EMULATION
 	if (read_cr0() & X86_CR0_EM) {
 		struct math_emu_info info = { };

commit c767a54ba0657e52e6edaa97cbe0b0a8bf1c1655
Author: Joe Perches <joe@perches.com>
Date:   Mon May 21 19:50:07 2012 -0700

    x86/debug: Add KERN_<LEVEL> to bare printks, convert printks to pr_<level>
    
    Use a more current logging style:
    
     - Bare printks should have a KERN_<LEVEL> for consistency's sake
     - Add pr_fmt where appropriate
     - Neaten some macro definitions
     - Convert some Ok output to OK
     - Use "%s: ", __func__ in pr_fmt for summit
     - Convert some printks to pr_<level>
    
    Message output is not identical in all cases.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: levinsasha928@gmail.com
    Link: http://lkml.kernel.org/r/1337655007.24226.10.camel@joe2Laptop
    [ merged two similar patches, tidied up the changelog ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 05b31d92f69c..b481341c9369 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -9,6 +9,9 @@
 /*
  * Handle hardware traps and faults.
  */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
 #include <linux/interrupt.h>
 #include <linux/kallsyms.h>
 #include <linux/spinlock.h>
@@ -143,12 +146,11 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 #ifdef CONFIG_X86_64
 	if (show_unhandled_signals && unhandled_signal(tsk, signr) &&
 	    printk_ratelimit()) {
-		printk(KERN_INFO
-		       "%s[%d] trap %s ip:%lx sp:%lx error:%lx",
-		       tsk->comm, tsk->pid, str,
-		       regs->ip, regs->sp, error_code);
+		pr_info("%s[%d] trap %s ip:%lx sp:%lx error:%lx",
+			tsk->comm, tsk->pid, str,
+			regs->ip, regs->sp, error_code);
 		print_vma_addr(" in ", regs->ip);
-		printk("\n");
+		pr_cont("\n");
 	}
 #endif
 
@@ -269,12 +271,11 @@ do_general_protection(struct pt_regs *regs, long error_code)
 
 	if (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&
 			printk_ratelimit()) {
-		printk(KERN_INFO
-			"%s[%d] general protection ip:%lx sp:%lx error:%lx",
+		pr_info("%s[%d] general protection ip:%lx sp:%lx error:%lx",
 			tsk->comm, task_pid_nr(tsk),
 			regs->ip, regs->sp, error_code);
 		print_vma_addr(" in ", regs->ip);
-		printk("\n");
+		pr_cont("\n");
 	}
 
 	force_sig(SIGSEGV, tsk);
@@ -570,7 +571,7 @@ do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 	conditional_sti(regs);
 #if 0
 	/* No need to warn about this any longer. */
-	printk(KERN_INFO "Ignoring P6 Local APIC Spurious Interrupt Bug...\n");
+	pr_info("Ignoring P6 Local APIC Spurious Interrupt Bug...\n");
 #endif
 }
 

commit a192cd0413b71c2a3e4e48dd365af704be72b748
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed May 30 13:26:37 2012 -0400

    ftrace: Synchronize variable setting with breakpoints
    
    When the function tracer starts modifying the code via breakpoints
    it sets a variable (modifying_ftrace_code) to inform the breakpoint
    handler to call the ftrace int3 code.
    
    But there's no synchronization between setting this code and the
    handler, thus it is possible for the handler to be called on another
    CPU before it sees the variable. This will cause a kernel crash as
    the int3 handler will not know what to do with it.
    
    I originally added smp_mb()'s to force the visibility of the variable
    but H. Peter Anvin suggested that I just make it atomic.
    
    [ Added comments as suggested by Peter Zijlstra ]
    
    Suggested-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ff08457a025d..05b31d92f69c 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -303,8 +303,12 @@ do_general_protection(struct pt_regs *regs, long error_code)
 dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
-	/* ftrace must be first, everything else may cause a recursive crash */
-	if (unlikely(modifying_ftrace_code) && ftrace_int3_handler(regs))
+	/*
+	 * ftrace must be first, everything else may cause a recursive crash.
+	 * See note by declaration of modifying_ftrace_code in ftrace.c
+	 */
+	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
+	    ftrace_int3_handler(regs))
 		return;
 #endif
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP

commit d5b4bb4d103cd601d8009f2d3a7e44586c9ae7cc
Merge: c80ddb526331 bb8187d35f82
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 23 17:12:06 2012 -0700

    Merge branch 'delete-mca' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    Pull the MCA deletion branch from Paul Gortmaker:
     "It was good that we could support MCA machines back in the day, but
      realistically, nobody is using them anymore.  They were mostly limited
      to 386-sx 16MHz CPU and some 486 class machines and never more than
      64MB of RAM.  Even the enthusiast hobbyist community seems to have
      dried up close to ten years ago, based on what you can find searching
      various websites dedicated to the relatively short lived hardware.
    
      So lets remove the support relating to CONFIG_MCA.  There is no point
      carrying this forward, wasting cycles doing routine maintenance on it;
      wasting allyesconfig build time on validating it, wasting I/O on git
      grep'ping over it, and so on."
    
    Let's see if anybody screams.  It generally has compiled, and James
    Bottomley pointed out that there was a MCA extension from NCR that
    allowed for up to 4GB of memory and PPro-class machines.  So in *theory*
    there may be users out there.
    
    But even James (technically listed as a maintainer) doesn't actually
    have a system, and while Alan Cox claims to have a machine in his cellar
    that he offered to anybody who wants to take it off his hands, he didn't
    argue for keeping MCA support either.
    
    So we could bring it back.  But somebody had better speak up and talk
    about how they have actually been using said MCA hardware with modern
    kernels for us to do that.  And David already took the patch to delete
    all the networking driver code (commit a5e371f61ad3: "drivers/net:
    delete all code/drivers depending on CONFIG_MCA").
    
    * 'delete-mca' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux:
      MCA: delete all remaining traces of microchannel bus support.
      scsi: delete the MCA specific drivers and driver code
      serial: delete the MCA specific 8250 support.
      arm: remove ability to select CONFIG_MCA

commit bb8187d35f820671d6dd76700d77a6b55f95e2c5
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu May 17 19:06:13 2012 -0400

    MCA: delete all remaining traces of microchannel bus support.
    
    Hardware with MCA bus is limited to 386 and 486 class machines
    that are now 20+ years old and typically with less than 32MB
    of memory.  A quick search on the internet, and you see that
    even the MCA hobbyist/enthusiast community has lost interest
    in the early 2000 era and never really even moved ahead from
    the 2.4 kernels to the 2.6 series.
    
    This deletes anything remaining related to CONFIG_MCA from core
    kernel code and from the x86 architecture.  There is no point in
    carrying this any further into the future.
    
    One complication to watch for is inadvertently scooping up
    stuff relating to machine check, since there is overlap in
    the TLA name space (e.g. arch/x86/boot/mca.c).
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: James Bottomley <JBottomley@Parallels.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ff9281f16029..4754f510b360 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -37,10 +37,6 @@
 #include <linux/eisa.h>
 #endif
 
-#ifdef CONFIG_MCA
-#include <linux/mca.h>
-#endif
-
 #if defined(CONFIG_EDAC)
 #include <linux/edac.h>
 #endif

commit 08d636b6d4fb80647fe8869ea1cd97b2c26a4751
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Aug 16 09:57:10 2011 -0400

    ftrace/x86: Have arch x86_64 use breakpoints instead of stop machine
    
    This method changes x86 to add a breakpoint to the mcount locations
    instead of calling stop machine.
    
    Now that iret can be handled by NMIs, we perform the following to
    update code:
    
    1) Add a breakpoint to all locations that will be modified
    
    2) Sync all cores
    
    3) Update all locations to be either a nop or call (except breakpoint
       op)
    
    4) Sync all cores
    
    5) Remove the breakpoint with the new code.
    
    6) Sync all cores
    
    [
      Added updates that Masami suggested:
       Use unlikely(modifying_ftrace_code) in int3 trap to keep kprobes efficient.
       Don't use NOTIFY_* in ftrace handler in int3 as it is not a notifier.
    ]
    
    Cc: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ff9281f16029..92d5756d85fc 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -50,6 +50,7 @@
 #include <asm/processor.h>
 #include <asm/debugreg.h>
 #include <linux/atomic.h>
+#include <asm/ftrace.h>
 #include <asm/traps.h>
 #include <asm/desc.h>
 #include <asm/i387.h>
@@ -303,8 +304,13 @@ do_general_protection(struct pt_regs *regs, long error_code)
 }
 
 /* May run on IST stack. */
-dotraplinkage void __kprobes do_int3(struct pt_regs *regs, long error_code)
+dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_code)
 {
+#ifdef CONFIG_DYNAMIC_FTRACE
+	/* ftrace must be first, everything else may cause a recursive crash */
+	if (unlikely(modifying_ftrace_code) && ftrace_int3_handler(regs))
+		return;
+#endif
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)

commit eb05df9e7e793f3134dbb574c7ccc05f7932bc59
Merge: a591afc01d9e ef334a20d84f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 18:21:35 2012 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Peter Anvin:
     "The biggest textual change is the cleanup to use symbolic constants
      for x86 trap values.
    
      The only *functional* change and the reason for the x86/x32 dependency
      is the move of is_ia32_task() into <asm/thread_info.h> so that it can
      be used in other code that needs to understand if a system call comes
      from the compat entry point (and therefore uses i386 system call
      numbers) or not.  One intended user for that is the BPF system call
      filter.  Moving it out of <asm/compat.h> means we can define it
      unconditionally, returning always true on i386."
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Move is_ia32_task to asm/thread_info.h from asm/compat.h
      x86: Rename trap_no to trap_nr in thread_struct
      x86: Use enum instead of literals for trap values

commit f05e798ad4c09255f590f5b2c00a7ca6c172f983
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:11:12 2012 +0100

    Disintegrate asm/system.h for X86
    
    Disintegrate asm/system.h for X86.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    cc: x86@kernel.org

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ec61d4c1b93b..860f126ca233 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -50,7 +50,6 @@
 #include <asm/processor.h>
 #include <asm/debugreg.h>
 #include <linux/atomic.h>
-#include <asm/system.h>
 #include <asm/traps.h>
 #include <asm/desc.h>
 #include <asm/i387.h>

commit 51e7dc7011c99e1e5294658c7b551b92ca069985
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Mon Mar 12 14:55:55 2012 +0530

    x86: Rename trap_no to trap_nr in thread_struct
    
    There are precedences of trap number being referred to as
    trap_nr. However thread struct refers trap number as trap_no.
    Change it to trap_nr.
    
    Also use enum instead of left-over literals for trap values.
    
    This is pure cleanup, no functional change intended.
    
    Suggested-by: Ingo Molnar <mingo@eltu.hu>
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@linux.vnet.ibm.com>
    Cc: Linux-mm <linux-mm@kvack.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120312092555.5379.942.sendpatchset@srdronam.in.ibm.com
    [ Fixed the math-emu build ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 037fc2bc5316..c6d17ad59b8a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -132,7 +132,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 trap_signal:
 #endif
 	/*
-	 * We want error_code and trap_no set for userspace faults and
+	 * We want error_code and trap_nr set for userspace faults and
 	 * kernelspace faults which result in die(), but not
 	 * kernelspace faults which are fixed up.  die() gives the
 	 * process no chance to handle the signal and notice the
@@ -141,7 +141,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	 * delivered, faults.  See also do_general_protection below.
 	 */
 	tsk->thread.error_code = error_code;
-	tsk->thread.trap_no = trapnr;
+	tsk->thread.trap_nr = trapnr;
 
 #ifdef CONFIG_X86_64
 	if (show_unhandled_signals && unhandled_signal(tsk, signr) &&
@@ -164,7 +164,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 kernel_trap:
 	if (!fixup_exception(regs)) {
 		tsk->thread.error_code = error_code;
-		tsk->thread.trap_no = trapnr;
+		tsk->thread.trap_nr = trapnr;
 		die(str, regs, error_code);
 	}
 	return;
@@ -240,7 +240,7 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);
 
 	tsk->thread.error_code = error_code;
-	tsk->thread.trap_no = X86_TRAP_DF;
+	tsk->thread.trap_nr = X86_TRAP_DF;
 
 	/*
 	 * This is always a kernel trap and never fixable (and thus must
@@ -268,7 +268,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		goto gp_in_kernel;
 
 	tsk->thread.error_code = error_code;
-	tsk->thread.trap_no = X86_TRAP_GP;
+	tsk->thread.trap_nr = X86_TRAP_GP;
 
 	if (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&
 			printk_ratelimit()) {
@@ -295,7 +295,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		return;
 
 	tsk->thread.error_code = error_code;
-	tsk->thread.trap_no = X86_TRAP_GP;
+	tsk->thread.trap_nr = X86_TRAP_GP;
 	if (notify_die(DIE_GPF, "general protection fault", regs, error_code,
 			X86_TRAP_GP, SIGSEGV) == NOTIFY_STOP)
 		return;
@@ -475,7 +475,7 @@ void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	{
 		if (!fixup_exception(regs)) {
 			task->thread.error_code = error_code;
-			task->thread.trap_no = trapnr;
+			task->thread.trap_nr = trapnr;
 			die(str, regs, error_code);
 		}
 		return;
@@ -485,7 +485,7 @@ void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	 * Save the info for the exception handler and clear the error.
 	 */
 	save_init_fpu(task);
-	task->thread.trap_no = trapnr;
+	task->thread.trap_nr = trapnr;
 	task->thread.error_code = error_code;
 	info.si_signo = SIGFPE;
 	info.si_errno = 0;

commit c94082656dac74257f63e91f78d5d458ac781fa5
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Mar 9 16:07:10 2012 -0800

    x86: Use enum instead of literals for trap values
    
    The traps are referred to by their numbers and it can be difficult to
    understand them while reading the code without context. This patch adds
    enumeration of the trap numbers and replaces the numbers with the correct
    enum for x86.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Link: http://lkml.kernel.org/r/20120310000710.GA32667@www.outflux.net
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 4bbe04d96744..037fc2bc5316 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -119,7 +119,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 		 * traps 0, 1, 3, 4, and 5 should be forwarded to vm86.
 		 * On nmi (interrupt 2), do_trap should not be called.
 		 */
-		if (trapnr < 6)
+		if (trapnr < X86_TRAP_UD)
 			goto vm86_trap;
 		goto trap_signal;
 	}
@@ -203,27 +203,31 @@ dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 	do_trap(trapnr, signr, str, regs, error_code, &info);		\
 }
 
-DO_ERROR_INFO(0, SIGFPE, "divide error", divide_error, FPE_INTDIV, regs->ip)
-DO_ERROR(4, SIGSEGV, "overflow", overflow)
-DO_ERROR(5, SIGSEGV, "bounds", bounds)
-DO_ERROR_INFO(6, SIGILL, "invalid opcode", invalid_op, ILL_ILLOPN, regs->ip)
-DO_ERROR(9, SIGFPE, "coprocessor segment overrun", coprocessor_segment_overrun)
-DO_ERROR(10, SIGSEGV, "invalid TSS", invalid_TSS)
-DO_ERROR(11, SIGBUS, "segment not present", segment_not_present)
+DO_ERROR_INFO(X86_TRAP_DE, SIGFPE, "divide error", divide_error, FPE_INTDIV,
+		regs->ip)
+DO_ERROR(X86_TRAP_OF, SIGSEGV, "overflow", overflow)
+DO_ERROR(X86_TRAP_BR, SIGSEGV, "bounds", bounds)
+DO_ERROR_INFO(X86_TRAP_UD, SIGILL, "invalid opcode", invalid_op, ILL_ILLOPN,
+		regs->ip)
+DO_ERROR(X86_TRAP_OLD_MF, SIGFPE, "coprocessor segment overrun",
+		coprocessor_segment_overrun)
+DO_ERROR(X86_TRAP_TS, SIGSEGV, "invalid TSS", invalid_TSS)
+DO_ERROR(X86_TRAP_NP, SIGBUS, "segment not present", segment_not_present)
 #ifdef CONFIG_X86_32
-DO_ERROR(12, SIGBUS, "stack segment", stack_segment)
+DO_ERROR(X86_TRAP_SS, SIGBUS, "stack segment", stack_segment)
 #endif
-DO_ERROR_INFO(17, SIGBUS, "alignment check", alignment_check, BUS_ADRALN, 0)
+DO_ERROR_INFO(X86_TRAP_AC, SIGBUS, "alignment check", alignment_check,
+		BUS_ADRALN, 0)
 
 #ifdef CONFIG_X86_64
 /* Runs on IST stack */
 dotraplinkage void do_stack_segment(struct pt_regs *regs, long error_code)
 {
 	if (notify_die(DIE_TRAP, "stack segment", regs, error_code,
-			12, SIGBUS) == NOTIFY_STOP)
+			X86_TRAP_SS, SIGBUS) == NOTIFY_STOP)
 		return;
 	preempt_conditional_sti(regs);
-	do_trap(12, SIGBUS, "stack segment", regs, error_code, NULL);
+	do_trap(X86_TRAP_SS, SIGBUS, "stack segment", regs, error_code, NULL);
 	preempt_conditional_cli(regs);
 }
 
@@ -233,10 +237,10 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	struct task_struct *tsk = current;
 
 	/* Return not checked because double check cannot be ignored */
-	notify_die(DIE_TRAP, str, regs, error_code, 8, SIGSEGV);
+	notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);
 
 	tsk->thread.error_code = error_code;
-	tsk->thread.trap_no = 8;
+	tsk->thread.trap_no = X86_TRAP_DF;
 
 	/*
 	 * This is always a kernel trap and never fixable (and thus must
@@ -264,7 +268,7 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		goto gp_in_kernel;
 
 	tsk->thread.error_code = error_code;
-	tsk->thread.trap_no = 13;
+	tsk->thread.trap_no = X86_TRAP_GP;
 
 	if (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&
 			printk_ratelimit()) {
@@ -291,9 +295,9 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		return;
 
 	tsk->thread.error_code = error_code;
-	tsk->thread.trap_no = 13;
-	if (notify_die(DIE_GPF, "general protection fault", regs,
-				error_code, 13, SIGSEGV) == NOTIFY_STOP)
+	tsk->thread.trap_no = X86_TRAP_GP;
+	if (notify_die(DIE_GPF, "general protection fault", regs, error_code,
+			X86_TRAP_GP, SIGSEGV) == NOTIFY_STOP)
 		return;
 	die("general protection fault", regs, error_code);
 }
@@ -302,13 +306,13 @@ do_general_protection(struct pt_regs *regs, long error_code)
 dotraplinkage void __kprobes do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
-	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, 3, SIGTRAP)
-			== NOTIFY_STOP)
+	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
+				SIGTRAP) == NOTIFY_STOP)
 		return;
 #endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */
 
-	if (notify_die(DIE_INT3, "int3", regs, error_code, 3, SIGTRAP)
-			== NOTIFY_STOP)
+	if (notify_die(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
+			SIGTRAP) == NOTIFY_STOP)
 		return;
 
 	/*
@@ -317,7 +321,7 @@ dotraplinkage void __kprobes do_int3(struct pt_regs *regs, long error_code)
 	 */
 	debug_stack_usage_inc();
 	preempt_conditional_sti(regs);
-	do_trap(3, SIGTRAP, "int3", regs, error_code, NULL);
+	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, NULL);
 	preempt_conditional_cli(regs);
 	debug_stack_usage_dec();
 }
@@ -422,8 +426,8 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	preempt_conditional_sti(regs);
 
 	if (regs->flags & X86_VM_MASK) {
-		handle_vm86_trap((struct kernel_vm86_regs *) regs,
-				error_code, 1);
+		handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code,
+					X86_TRAP_DB);
 		preempt_conditional_cli(regs);
 		debug_stack_usage_dec();
 		return;
@@ -460,7 +464,8 @@ void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	struct task_struct *task = current;
 	siginfo_t info;
 	unsigned short err;
-	char *str = (trapnr == 16) ? "fpu exception" : "simd exception";
+	char *str = (trapnr == X86_TRAP_MF) ? "fpu exception" :
+						"simd exception";
 
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)
 		return;
@@ -485,7 +490,7 @@ void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	info.si_signo = SIGFPE;
 	info.si_errno = 0;
 	info.si_addr = (void __user *)regs->ip;
-	if (trapnr == 16) {
+	if (trapnr == X86_TRAP_MF) {
 		unsigned short cwd, swd;
 		/*
 		 * (~cwd & swd) will mask out exceptions that are not set to unmasked
@@ -529,10 +534,11 @@ void math_error(struct pt_regs *regs, int error_code, int trapnr)
 		info.si_code = FPE_FLTRES;
 	} else {
 		/*
-		 * If we're using IRQ 13, or supposedly even some trap 16
-		 * implementations, it's possible we get a spurious trap...
+		 * If we're using IRQ 13, or supposedly even some trap
+		 * X86_TRAP_MF implementations, it's possible
+		 * we get a spurious trap, which is not an error.
 		 */
-		return;		/* Spurious trap, no error */
+		return;
 	}
 	force_sig_info(SIGFPE, &info, task);
 }
@@ -543,13 +549,13 @@ dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 	ignore_fpu_irq = 1;
 #endif
 
-	math_error(regs, error_code, 16);
+	math_error(regs, error_code, X86_TRAP_MF);
 }
 
 dotraplinkage void
 do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-	math_error(regs, error_code, 19);
+	math_error(regs, error_code, X86_TRAP_XF);
 }
 
 dotraplinkage void
@@ -643,20 +649,21 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 	info.si_errno = 0;
 	info.si_code = ILL_BADSTK;
 	info.si_addr = NULL;
-	if (notify_die(DIE_TRAP, "iret exception",
-			regs, error_code, 32, SIGILL) == NOTIFY_STOP)
+	if (notify_die(DIE_TRAP, "iret exception", regs, error_code,
+			X86_TRAP_IRET, SIGILL) == NOTIFY_STOP)
 		return;
-	do_trap(32, SIGILL, "iret exception", regs, error_code, &info);
+	do_trap(X86_TRAP_IRET, SIGILL, "iret exception", regs, error_code,
+		&info);
 }
 #endif
 
 /* Set of traps needed for early debugging. */
 void __init early_trap_init(void)
 {
-	set_intr_gate_ist(1, &debug, DEBUG_STACK);
+	set_intr_gate_ist(X86_TRAP_DB, &debug, DEBUG_STACK);
 	/* int3 can be called from all */
-	set_system_intr_gate_ist(3, &int3, DEBUG_STACK);
-	set_intr_gate(14, &page_fault);
+	set_system_intr_gate_ist(X86_TRAP_BP, &int3, DEBUG_STACK);
+	set_intr_gate(X86_TRAP_PF, &page_fault);
 	load_idt(&idt_descr);
 }
 
@@ -672,30 +679,30 @@ void __init trap_init(void)
 	early_iounmap(p, 4);
 #endif
 
-	set_intr_gate(0, &divide_error);
-	set_intr_gate_ist(2, &nmi, NMI_STACK);
+	set_intr_gate(X86_TRAP_DE, &divide_error);
+	set_intr_gate_ist(X86_TRAP_NMI, &nmi, NMI_STACK);
 	/* int4 can be called from all */
-	set_system_intr_gate(4, &overflow);
-	set_intr_gate(5, &bounds);
-	set_intr_gate(6, &invalid_op);
-	set_intr_gate(7, &device_not_available);
+	set_system_intr_gate(X86_TRAP_OF, &overflow);
+	set_intr_gate(X86_TRAP_BR, &bounds);
+	set_intr_gate(X86_TRAP_UD, &invalid_op);
+	set_intr_gate(X86_TRAP_NM, &device_not_available);
 #ifdef CONFIG_X86_32
-	set_task_gate(8, GDT_ENTRY_DOUBLEFAULT_TSS);
+	set_task_gate(X86_TRAP_DF, GDT_ENTRY_DOUBLEFAULT_TSS);
 #else
-	set_intr_gate_ist(8, &double_fault, DOUBLEFAULT_STACK);
+	set_intr_gate_ist(X86_TRAP_DF, &double_fault, DOUBLEFAULT_STACK);
 #endif
-	set_intr_gate(9, &coprocessor_segment_overrun);
-	set_intr_gate(10, &invalid_TSS);
-	set_intr_gate(11, &segment_not_present);
-	set_intr_gate_ist(12, &stack_segment, STACKFAULT_STACK);
-	set_intr_gate(13, &general_protection);
-	set_intr_gate(15, &spurious_interrupt_bug);
-	set_intr_gate(16, &coprocessor_error);
-	set_intr_gate(17, &alignment_check);
+	set_intr_gate(X86_TRAP_OLD_MF, &coprocessor_segment_overrun);
+	set_intr_gate(X86_TRAP_TS, &invalid_TSS);
+	set_intr_gate(X86_TRAP_NP, &segment_not_present);
+	set_intr_gate_ist(X86_TRAP_SS, &stack_segment, STACKFAULT_STACK);
+	set_intr_gate(X86_TRAP_GP, &general_protection);
+	set_intr_gate(X86_TRAP_SPURIOUS, &spurious_interrupt_bug);
+	set_intr_gate(X86_TRAP_MF, &coprocessor_error);
+	set_intr_gate(X86_TRAP_AC, &alignment_check);
 #ifdef CONFIG_X86_MCE
-	set_intr_gate_ist(18, &machine_check, MCE_STACK);
+	set_intr_gate_ist(X86_TRAP_MC, &machine_check, MCE_STACK);
 #endif
-	set_intr_gate(19, &simd_coprocessor_error);
+	set_intr_gate(X86_TRAP_XF, &simd_coprocessor_error);
 
 	/* Reserve all the builtin and the syscall vector: */
 	for (i = 0; i < FIRST_EXTERNAL_VECTOR; i++)
@@ -720,7 +727,7 @@ void __init trap_init(void)
 
 #ifdef CONFIG_X86_64
 	memcpy(&nmi_idt_table, &idt_table, IDT_ENTRIES * 16);
-	set_nmi_gate(1, &debug);
-	set_nmi_gate(3, &int3);
+	set_nmi_gate(X86_TRAP_DB, &debug);
+	set_nmi_gate(X86_TRAP_BP, &int3);
 #endif
 }

commit 1361b83a13d4d92e53fbb6c877528713e118b821
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 21 13:19:22 2012 -0800

    i387: Split up <asm/i387.h> into exported and internal interfaces
    
    While various modules include <asm/i387.h> to get access to things we
    actually *intend* for them to use, most of that header file was really
    pretty low-level internal stuff that we really don't want to expose to
    others.
    
    So split the header file into two: the small exported interfaces remain
    in <asm/i387.h>, while the internal definitions that are only used by
    core architecture code are now in <asm/fpu-internal.h>.
    
    The guiding principle for this was to expose functions that we export to
    modules, and leave them in <asm/i387.h>, while stuff that is used by
    task switching or was marked GPL-only is in <asm/fpu-internal.h>.
    
    The fpu-internal.h file could be further split up too, especially since
    arch/x86/kvm/ uses some of the remaining stuff for its module.  But that
    kvm usage should probably be abstracted out a bit, and at least now the
    internal FPU accessor functions are much more contained.  Even if it
    isn't perhaps as contained as it _could_ be.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1202211340330.5354@i5.linux-foundation.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 4bbe04d96744..ec61d4c1b93b 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -54,6 +54,7 @@
 #include <asm/traps.h>
 #include <asm/desc.h>
 #include <asm/i387.h>
+#include <asm/fpu-internal.h>
 #include <asm/mce.h>
 
 #include <asm/mach_traps.h>

commit 80ab6f1e8c981b1b6604b2f22e36c917526235cd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 19 11:48:44 2012 -0800

    i387: use 'restore_fpu_checking()' directly in task switching code
    
    This inlines what is usually just a couple of instructions, but more
    importantly it also fixes the theoretical error case (can that FPU
    restore really ever fail? Maybe we should remove the checking).
    
    We can't start sending signals from within the scheduler, we're much too
    deep in the kernel and are holding the runqueue lock etc.  So don't
    bother even trying.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 77da5b475ad2..4bbe04d96744 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -570,37 +570,6 @@ asmlinkage void __attribute__((weak)) smp_threshold_interrupt(void)
 {
 }
 
-/*
- * This gets called with the process already owning the
- * FPU state, and with CR0.TS cleared. It just needs to
- * restore the FPU register state.
- */
-void __math_state_restore(struct task_struct *tsk)
-{
-	/* We need a safe address that is cheap to find and that is already
-	   in L1. We've just brought in "tsk->thread.has_fpu", so use that */
-#define safe_address (tsk->thread.has_fpu)
-
-	/* AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception
-	   is pending.  Clear the x87 state here by setting it to fixed
-	   values. safe_address is a random variable that should be in L1 */
-	alternative_input(
-		ASM_NOP8 ASM_NOP2,
-		"emms\n\t"	  	/* clear stack tags */
-		"fildl %P[addr]",	/* set F?P to defined value */
-		X86_FEATURE_FXSAVE_LEAK,
-		[addr] "m" (safe_address));
-
-	/*
-	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
-	 */
-	if (unlikely(restore_fpu_checking(tsk))) {
-		__thread_fpu_end(tsk);
-		force_sig(SIGSEGV, tsk);
-		return;
-	}
-}
-
 /*
  * 'math_state_restore()' saves the current math information in the
  * old math state array, and gets the new ones from the current task
@@ -631,7 +600,14 @@ void math_state_restore(void)
 	}
 
 	__thread_fpu_begin(tsk);
-	__math_state_restore(tsk);
+	/*
+	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
+	 */
+	if (unlikely(restore_fpu_checking(tsk))) {
+		__thread_fpu_end(tsk);
+		force_sig(SIGSEGV, tsk);
+		return;
+	}
 
 	tsk->fpu_counter++;
 }

commit 34ddc81a230b15c0e345b6b253049db731499f7e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 18 12:56:35 2012 -0800

    i387: re-introduce FPU state preloading at context switch time
    
    After all the FPU state cleanups and finally finding the problem that
    caused all our FPU save/restore problems, this re-introduces the
    preloading of FPU state that was removed in commit b3b0870ef3ff ("i387:
    do not preload FPU state at task switch time").
    
    However, instead of simply reverting the removal, this reimplements
    preloading with several fixes, most notably
    
     - properly abstracted as a true FPU state switch, rather than as
       open-coded save and restore with various hacks.
    
       In particular, implementing it as a proper FPU state switch allows us
       to optimize the CR0.TS flag accesses: there is no reason to set the
       TS bit only to then almost immediately clear it again.  CR0 accesses
       are quite slow and expensive, don't flip the bit back and forth for
       no good reason.
    
     - Make sure that the same model works for both x86-32 and x86-64, so
       that there are no gratuitous differences between the two due to the
       way they save and restore segment state differently due to
       architectural differences that really don't matter to the FPU state.
    
     - Avoid exposing the "preload" state to the context switch routines,
       and in particular allow the concept of lazy state restore: if nothing
       else has used the FPU in the meantime, and the process is still on
       the same CPU, we can avoid restoring state from memory entirely, just
       re-expose the state that is still in the FPU unit.
    
       That optimized lazy restore isn't actually implemented here, but the
       infrastructure is set up for it.  Of course, older CPU's that use
       'fnsave' to save the state cannot take advantage of this, since the
       state saving also trashes the state.
    
    In other words, there is now an actual _design_ to the FPU state saving,
    rather than just random historical baggage.  Hopefully it's easier to
    follow as a result.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ad25e51f40c4..77da5b475ad2 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -570,6 +570,37 @@ asmlinkage void __attribute__((weak)) smp_threshold_interrupt(void)
 {
 }
 
+/*
+ * This gets called with the process already owning the
+ * FPU state, and with CR0.TS cleared. It just needs to
+ * restore the FPU register state.
+ */
+void __math_state_restore(struct task_struct *tsk)
+{
+	/* We need a safe address that is cheap to find and that is already
+	   in L1. We've just brought in "tsk->thread.has_fpu", so use that */
+#define safe_address (tsk->thread.has_fpu)
+
+	/* AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception
+	   is pending.  Clear the x87 state here by setting it to fixed
+	   values. safe_address is a random variable that should be in L1 */
+	alternative_input(
+		ASM_NOP8 ASM_NOP2,
+		"emms\n\t"	  	/* clear stack tags */
+		"fildl %P[addr]",	/* set F?P to defined value */
+		X86_FEATURE_FXSAVE_LEAK,
+		[addr] "m" (safe_address));
+
+	/*
+	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
+	 */
+	if (unlikely(restore_fpu_checking(tsk))) {
+		__thread_fpu_end(tsk);
+		force_sig(SIGSEGV, tsk);
+		return;
+	}
+}
+
 /*
  * 'math_state_restore()' saves the current math information in the
  * old math state array, and gets the new ones from the current task
@@ -584,10 +615,6 @@ void math_state_restore(void)
 {
 	struct task_struct *tsk = current;
 
-	/* We need a safe address that is cheap to find and that is already
-	   in L1. We're just bringing in "tsk->thread.has_fpu", so use that */
-#define safe_address (tsk->thread.has_fpu)
-
 	if (!tsk_used_math(tsk)) {
 		local_irq_enable();
 		/*
@@ -604,25 +631,7 @@ void math_state_restore(void)
 	}
 
 	__thread_fpu_begin(tsk);
-
-	/* AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception
-	   is pending.  Clear the x87 state here by setting it to fixed
-	   values. safe_address is a random variable that should be in L1 */
-	alternative_input(
-		ASM_NOP8 ASM_NOP2,
-		"emms\n\t"	  	/* clear stack tags */
-		"fildl %P[addr]",	/* set F?P to defined value */
-		X86_FEATURE_FXSAVE_LEAK,
-		[addr] "m" (safe_address));
-
-	/*
-	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
-	 */
-	if (unlikely(restore_fpu_checking(tsk))) {
-		__thread_fpu_end(tsk);
-		force_sig(SIGSEGV, tsk);
-		return;
-	}
+	__math_state_restore(tsk);
 
 	tsk->fpu_counter++;
 }

commit f94edacf998516ac9d849f7bc6949a703977a7f3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 17 21:48:54 2012 -0800

    i387: move TS_USEDFPU flag from thread_info to task_struct
    
    This moves the bit that indicates whether a thread has ownership of the
    FPU from the TS_USEDFPU bit in thread_info->status to a word of its own
    (called 'has_fpu') in task_struct->thread.has_fpu.
    
    This fixes two independent bugs at the same time:
    
     - changing 'thread_info->status' from the scheduler causes nasty
       problems for the other users of that variable, since it is defined to
       be thread-synchronous (that's what the "TS_" part of the naming was
       supposed to indicate).
    
       So perfectly valid code could (and did) do
    
            ti->status |= TS_RESTORE_SIGMASK;
    
       and the compiler was free to do that as separate load, or and store
       instructions.  Which can cause problems with preemption, since a task
       switch could happen in between, and change the TS_USEDFPU bit. The
       change to TS_USEDFPU would be overwritten by the final store.
    
       In practice, this seldom happened, though, because the 'status' field
       was seldom used more than once, so gcc would generally tend to
       generate code that used a read-modify-write instruction and thus
       happened to avoid this problem - RMW instructions are naturally low
       fat and preemption-safe.
    
     - On x86-32, the current_thread_info() pointer would, during interrupts
       and softirqs, point to a *copy* of the real thread_info, because
       x86-32 uses %esp to calculate the thread_info address, and thus the
       separate irq (and softirq) stacks would cause these kinds of odd
       thread_info copy aliases.
    
       This is normally not a problem, since interrupts aren't supposed to
       look at thread information anyway (what thread is running at
       interrupt time really isn't very well-defined), but it confused the
       heck out of irq_fpu_usable() and the code that tried to squirrel
       away the FPU state.
    
       (It also caused untold confusion for us poor kernel developers).
    
    It also turns out that using 'task_struct' is actually much more natural
    for most of the call sites that care about the FPU state, since they
    tend to work with the task struct for other reasons anyway (ie
    scheduling).  And the FPU data that we are going to save/restore is
    found there too.
    
    Thanks to Arjan Van De Ven <arjan@linux.intel.com> for pointing us to
    the %esp issue.
    
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Reported-and-tested-by: Raphael Prevost <raphael@buro.asia>
    Acked-and-tested-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Tested-by: Peter Anvin <hpa@zytor.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 4d42300dcd2c..ad25e51f40c4 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -582,12 +582,11 @@ asmlinkage void __attribute__((weak)) smp_threshold_interrupt(void)
  */
 void math_state_restore(void)
 {
-	struct thread_info *thread = current_thread_info();
-	struct task_struct *tsk = thread->task;
+	struct task_struct *tsk = current;
 
 	/* We need a safe address that is cheap to find and that is already
-	   in L1. We just brought in "thread->task", so use that */
-#define safe_address (thread->task)
+	   in L1. We're just bringing in "tsk->thread.has_fpu", so use that */
+#define safe_address (tsk->thread.has_fpu)
 
 	if (!tsk_used_math(tsk)) {
 		local_irq_enable();
@@ -604,7 +603,7 @@ void math_state_restore(void)
 		local_irq_disable();
 	}
 
-	__thread_fpu_begin(thread);
+	__thread_fpu_begin(tsk);
 
 	/* AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception
 	   is pending.  Clear the x87 state here by setting it to fixed
@@ -620,7 +619,7 @@ void math_state_restore(void)
 	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
 	 */
 	if (unlikely(restore_fpu_checking(tsk))) {
-		__thread_fpu_end(thread);
+		__thread_fpu_end(tsk);
 		force_sig(SIGSEGV, tsk);
 		return;
 	}

commit 4903062b5485f0e2c286a23b44c9b59d9b017d53
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 16 19:11:15 2012 -0800

    i387: move AMD K7/K8 fpu fxsave/fxrstor workaround from save to restore
    
    The AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception is
    pending.  In order to not leak FIP state from one process to another, we
    need to do a floating point load after the fxsave of the old process,
    and before the fxrstor of the new FPU state.  That resets the state to
    the (uninteresting) kernel load, rather than some potentially sensitive
    user information.
    
    We used to do this directly after the FPU state save, but that is
    actually very inconvenient, since it
    
     (a) corrupts what is potentially perfectly good FPU state that we might
         want to lazy avoid restoring later and
    
     (b) on x86-64 it resulted in a very annoying ordering constraint, where
         "__unlazy_fpu()" in the task switch needs to be delayed until after
         the DS segment has been reloaded just to get the new DS value.
    
    Coupling it to the fxrstor instead of the fxsave automatically avoids
    both of these issues, and also ensures that we only do it when actually
    necessary (the FP state after a save may never actually get used).  It's
    simply a much more natural place for the leaked state cleanup.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 5afe824c66e5..4d42300dcd2c 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -585,6 +585,10 @@ void math_state_restore(void)
 	struct thread_info *thread = current_thread_info();
 	struct task_struct *tsk = thread->task;
 
+	/* We need a safe address that is cheap to find and that is already
+	   in L1. We just brought in "thread->task", so use that */
+#define safe_address (thread->task)
+
 	if (!tsk_used_math(tsk)) {
 		local_irq_enable();
 		/*
@@ -602,6 +606,16 @@ void math_state_restore(void)
 
 	__thread_fpu_begin(thread);
 
+	/* AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception
+	   is pending.  Clear the x87 state here by setting it to fixed
+	   values. safe_address is a random variable that should be in L1 */
+	alternative_input(
+		ASM_NOP8 ASM_NOP2,
+		"emms\n\t"	  	/* clear stack tags */
+		"fildl %P[addr]",	/* set F?P to defined value */
+		X86_FEATURE_FXSAVE_LEAK,
+		[addr] "m" (safe_address));
+
 	/*
 	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
 	 */

commit b3b0870ef3ffed72b92415423da864f440f57ad6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 16 15:45:23 2012 -0800

    i387: do not preload FPU state at task switch time
    
    Yes, taking the trap to re-load the FPU/MMX state is expensive, but so
    is spending several days looking for a bug in the state save/restore
    code.  And the preload code has some rather subtle interactions with
    both paravirtualization support and segment state restore, so it's not
    nearly as simple as it should be.
    
    Also, now that we no longer necessarily depend on a single bit (ie
    TS_USEDFPU) for keeping track of the state of the FPU, we migth be able
    to do better.  If we are really switching between two processes that
    keep touching the FP state, save/restore is inevitable, but in the case
    of having one process that does most of the FPU usage, we may actually
    be able to do much better than the preloading.
    
    In particular, we may be able to keep track of which CPU the process ran
    on last, and also per CPU keep track of which process' FP state that CPU
    has.  For modern CPU's that don't destroy the FPU contents on save time,
    that would allow us to do a lazy restore by just re-enabling the
    existing FPU state - with no restore cost at all!
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index fc676e44c77f..5afe824c66e5 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -570,28 +570,6 @@ asmlinkage void __attribute__((weak)) smp_threshold_interrupt(void)
 {
 }
 
-/*
- * __math_state_restore assumes that cr0.TS is already clear and the
- * fpu state is all ready for use.  Used during context switch.
- */
-void __math_state_restore(void)
-{
-	struct thread_info *thread = current_thread_info();
-	struct task_struct *tsk = thread->task;
-
-	/*
-	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
-	 */
-	if (unlikely(restore_fpu_checking(tsk))) {
-		stts();
-		force_sig(SIGSEGV, tsk);
-		return;
-	}
-
-	__thread_set_has_fpu(thread);	/* clts in caller! */
-	tsk->fpu_counter++;
-}
-
 /*
  * 'math_state_restore()' saves the current math information in the
  * old math state array, and gets the new ones from the current task
@@ -622,9 +600,18 @@ void math_state_restore(void)
 		local_irq_disable();
 	}
 
-	clts();				/* Allow maths ops (or we recurse) */
+	__thread_fpu_begin(thread);
 
-	__math_state_restore();
+	/*
+	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
+	 */
+	if (unlikely(restore_fpu_checking(tsk))) {
+		__thread_fpu_end(thread);
+		force_sig(SIGSEGV, tsk);
+		return;
+	}
+
+	tsk->fpu_counter++;
 }
 EXPORT_SYMBOL_GPL(math_state_restore);
 

commit 6d59d7a9f5b723a7ac1925c136e93ec83c0c3043
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 16 13:33:12 2012 -0800

    i387: don't ever touch TS_USEDFPU directly, use helper functions
    
    This creates three helper functions that do the TS_USEDFPU accesses, and
    makes everybody that used to do it by hand use those helpers instead.
    
    In addition, there's a couple of helper functions for the "change both
    CR0.TS and TS_USEDFPU at the same time" case, and the places that do
    that together have been changed to use those.  That means that we have
    fewer random places that open-code this situation.
    
    The intent is partly to clarify the code without actually changing any
    semantics yet (since we clearly still have some hard to reproduce bug in
    this area), but also to make it much easier to use another approach
    entirely to caching the CR0.TS bit for software accesses.
    
    Right now we use a bit in the thread-info 'status' variable (this patch
    does not change that), but we might want to make it a full field of its
    own or even make it a per-cpu variable.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 982433b5da30..fc676e44c77f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -588,7 +588,7 @@ void __math_state_restore(void)
 		return;
 	}
 
-	thread->status |= TS_USEDFPU;	/* So we fnsave on switch_to() */
+	__thread_set_has_fpu(thread);	/* clts in caller! */
 	tsk->fpu_counter++;
 }
 

commit 15d8791cae75dca27bfda8ecfe87dca9379d6bb0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 16 09:15:04 2012 -0800

    i387: fix x86-64 preemption-unsafe user stack save/restore
    
    Commit 5b1cbac37798 ("i387: make irq_fpu_usable() tests more robust")
    added a sanity check to the #NM handler to verify that we never cause
    the "Device Not Available" exception in kernel mode.
    
    However, that check actually pinpointed a (fundamental) race where we do
    cause that exception as part of the signal stack FPU state save/restore
    code.
    
    Because we use the floating point instructions themselves to save and
    restore state directly from user mode, we cannot do that atomically with
    testing the TS_USEDFPU bit: the user mode access itself may cause a page
    fault, which causes a task switch, which saves and restores the FP/MMX
    state from the kernel buffers.
    
    This kind of "recursive" FP state save is fine per se, but it means that
    when the signal stack save/restore gets restarted, it will now take the
    '#NM' exception we originally tried to avoid.  With preemption this can
    happen even without the page fault - but because of the user access, we
    cannot just disable preemption around the save/restore instruction.
    
    There are various ways to solve this, including using the
    "enable/disable_page_fault()" helpers to not allow page faults at all
    during the sequence, and fall back to copying things by hand without the
    use of the native FP state save/restore instructions.
    
    However, the simplest thing to do is to just allow the #NM from kernel
    space, but fix the race in setting and clearing CR0.TS that this all
    exposed: the TS bit changes and the TS_USEDFPU bit absolutely have to be
    atomic wrt scheduling, so while the actual state save/restore can be
    interrupted and restarted, the act of actually clearing/setting CR0.TS
    and the TS_USEDFPU bit together must not.
    
    Instead of just adding random "preempt_disable/enable()" calls to what
    is already excessively ugly code, this introduces some helper functions
    that mostly mirror the "kernel_fpu_begin/end()" functionality, just for
    the user state instead.
    
    Those helper functions should probably eventually replace the other
    ad-hoc CR0.TS and TS_USEDFPU tests too, but I'll need to think about it
    some more: the task switching functionality in particular needs to
    expose the difference between the 'prev' and 'next' threads, while the
    new helper functions intentionally were written to only work with
    'current'.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 8ba27dbc107a..982433b5da30 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -631,7 +631,6 @@ EXPORT_SYMBOL_GPL(math_state_restore);
 dotraplinkage void __kprobes
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
-	WARN_ON_ONCE(!user_mode_vm(regs));
 #ifdef CONFIG_MATH_EMULATION
 	if (read_cr0() & X86_CR0_EM) {
 		struct math_emu_info info = { };

commit 5b1cbac37798805c1fee18c8cebe5c0a13975b17
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 13 13:56:14 2012 -0800

    i387: make irq_fpu_usable() tests more robust
    
    Some code - especially the crypto layer - wants to use the x86
    FP/MMX/AVX register set in what may be interrupt (typically softirq)
    context.
    
    That *can* be ok, but the tests for when it was ok were somewhat
    suspect.  We cannot touch the thread-specific status bits either, so
    we'd better check that we're not going to try to save FP state or
    anything like that.
    
    Now, it may be that the TS bit is always cleared *before* we set the
    USEDFPU bit (and only set when we had already cleared the USEDFP
    before), so the TS bit test may actually have been sufficient, but it
    certainly was not obviously so.
    
    So this explicitly verifies that we will not touch the TS_USEDFPU bit,
    and adds a few related sanity-checks.  Because it seems that somehow
    AES-NI is corrupting user FP state.  The cause is not clear, and this
    patch doesn't fix it, but while debugging it I really wanted the code to
    be more obviously correct and robust.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 982433b5da30..8ba27dbc107a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -631,6 +631,7 @@ EXPORT_SYMBOL_GPL(math_state_restore);
 dotraplinkage void __kprobes
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
+	WARN_ON_ONCE(!user_mode_vm(regs));
 #ifdef CONFIG_MATH_EMULATION
 	if (read_cr0() & X86_CR0_EM) {
 		struct math_emu_info info = { };

commit be98c2cdb15ba26148cd2bd58a857d4f7759ed38
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 13 13:47:25 2012 -0800

    i387: math_state_restore() isn't called from asm
    
    It was marked asmlinkage for some really old and stale legacy reasons.
    Fix that and the equally stale comment.
    
    Noticed when debugging the irq_fpu_usable() bugs.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 482ec3af2067..982433b5da30 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -599,10 +599,10 @@ void __math_state_restore(void)
  * Careful.. There are problems with IBM-designed IRQ13 behaviour.
  * Don't touch unless you *really* know how it works.
  *
- * Must be called with kernel preemption disabled (in this case,
- * local interrupts are disabled at the call-site in entry.S).
+ * Must be called with kernel preemption disabled (eg with local
+ * local interrupts as in the case of do_device_not_available).
  */
-asmlinkage void math_state_restore(void)
+void math_state_restore(void)
 {
 	struct thread_info *thread = current_thread_info();
 	struct task_struct *tsk = thread->task;

commit 83c2f912b43c3a7babbb6cb7ae2a5276c1ed2a3e
Merge: f0ed5b9a2853 172d1b0b7325
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 15 11:26:35 2012 -0800

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (39 commits)
      perf tools: Fix compile error on x86_64 Ubuntu
      perf report: Fix --stdio output alignment when --showcpuutilization used
      perf annotate: Get rid of field_sep check
      perf annotate: Fix usage string
      perf kmem: Fix a memory leak
      perf kmem: Add missing closedir() calls
      perf top: Add error message for EMFILE
      perf test: Change type of '-v' option to INCR
      perf script: Add missing closedir() calls
      tracing: Fix compile error when static ftrace is enabled
      recordmcount: Fix handling of elf64 big-endian objects.
      perf tools: Add const.h to MANIFEST to make perf-tar-src-pkg work again
      perf tools: Add support for guest/host-only profiling
      perf kvm: Do guest-only counting by default
      perf top: Don't update total_period on process_sample
      perf hists: Stop using 'self' for struct hist_entry
      perf hists: Rename total_session to total_period
      x86: Add counter when debug stack is used with interrupts enabled
      x86: Allow NMIs to hit breakpoints in i386
      x86: Keep current stack in NMI breakpoints
      ...

commit 42181186ad4db986fcaa40ca95c6e407e9e79372
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Dec 16 11:43:02 2011 -0500

    x86: Add counter when debug stack is used with interrupts enabled
    
    Mathieu Desnoyers pointed out a case that can cause issues with
    NMIs running on the debug stack:
    
      int3 -> interrupt -> NMI -> int3
    
    Because the interrupt changes the stack, the NMI will not see that
    it preempted the debug stack. Looking deeper at this case,
    interrupts only happen when the int3 is from userspace or in
    an a location in the exception table (fixup).
    
      userspace -> int3 -> interurpt -> NMI -> int3
    
    All other int3s that happen in the kernel should be processed
    without ever enabling interrupts, as the do_trap() call will
    panic the kernel if it is called to process any other location
    within the kernel.
    
    Adding a counter around the sections that enable interrupts while
    using the debug stack allows the NMI to also check that case.
    If the NMI sees that it either interrupted a task using the debug
    stack or the debug counter is non-zero, then it will have to
    change the IDT table to make the int3 not change stacks (which will
    corrupt the stack if it does).
    
    Note, I had to move the debug_usage functions out of processor.h
    and into debugreg.h because of the static inlined functions to
    inc and dec the debug_usage counter. __get_cpu_var() requires
    smp.h which includes processor.h, and would fail to build.
    
    Link: http://lkml.kernel.org/r/1323976535.23971.112.camel@gandalf.stny.rr.com
    
    Reported-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul Turner <pjt@google.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a93c5cabc36a..0072b38e3ea1 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -316,9 +316,15 @@ dotraplinkage void __kprobes do_int3(struct pt_regs *regs, long error_code)
 		return;
 #endif
 
+	/*
+	 * Let others (NMI) know that the debug stack is in use
+	 * as we may switch to the interrupt stack.
+	 */
+	debug_stack_usage_inc();
 	preempt_conditional_sti(regs);
 	do_trap(3, SIGTRAP, "int3", regs, error_code, NULL);
 	preempt_conditional_cli(regs);
+	debug_stack_usage_dec();
 }
 
 #ifdef CONFIG_X86_64
@@ -411,6 +417,12 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 							SIGTRAP) == NOTIFY_STOP)
 		return;
 
+	/*
+	 * Let others (NMI) know that the debug stack is in use
+	 * as we may switch to the interrupt stack.
+	 */
+	debug_stack_usage_inc();
+
 	/* It's safe to allow irq's after DR6 has been saved */
 	preempt_conditional_sti(regs);
 
@@ -418,6 +430,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 		handle_vm86_trap((struct kernel_vm86_regs *) regs,
 				error_code, 1);
 		preempt_conditional_cli(regs);
+		debug_stack_usage_dec();
 		return;
 	}
 
@@ -437,6 +450,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	if (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)
 		send_sigtrap(tsk, regs, error_code, si_code);
 	preempt_conditional_cli(regs);
+	debug_stack_usage_dec();
 
 	return;
 }

commit 228bdaa95fb830e08b6acd1afd4d2c55093cabfa
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Dec 9 03:02:19 2011 -0500

    x86: Keep current stack in NMI breakpoints
    
    We want to allow NMI handlers to have breakpoints to be able to
    remove stop_machine from ftrace, kprobes and jump_labels. But if
    an NMI interrupts a current breakpoint, and then it triggers a
    breakpoint itself, it will switch to the breakpoint stack and
    corrupt the data on it for the breakpoint processing that it
    interrupted.
    
    Instead, have the NMI check if it interrupted breakpoint processing
    by checking if the stack that is currently used is a breakpoint
    stack. If it is, then load a special IDT that changes the IST
    for the debug exception to keep the same stack in kernel context.
    When the NMI is done, it puts it back.
    
    This way, if the NMI does trigger a breakpoint, it will keep
    using the same stack and not stomp on the breakpoint data for
    the breakpoint it interrupted.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a8e3eb83466c..a93c5cabc36a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -723,4 +723,10 @@ void __init trap_init(void)
 	cpu_init();
 
 	x86_init.irqs.trap_init();
+
+#ifdef CONFIG_X86_64
+	memcpy(&nmi_idt_table, &idt_table, IDT_ENTRIES * 16);
+	set_nmi_gate(1, &debug);
+	set_nmi_gate(3, &int3);
+#endif
 }

commit cc3a1bf52a9d2808c7cd6e8f413b02b650b6b84b
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Tue Oct 25 19:51:59 2011 +0530

    x86: Clean up and extend do_int3()
    
    Since there is a possibility of !KPROBES int3 listeners
    (such as kgdb) and since DIE_TRAP is currently not being
    used by anybody, notify all listeners with DIE_INT3.
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20111025142159.GB21225@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a8e3eb83466c..fa1191fb679d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -306,15 +306,10 @@ dotraplinkage void __kprobes do_int3(struct pt_regs *regs, long error_code)
 			== NOTIFY_STOP)
 		return;
 #endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */
-#ifdef CONFIG_KPROBES
+
 	if (notify_die(DIE_INT3, "int3", regs, error_code, 3, SIGTRAP)
 			== NOTIFY_STOP)
 		return;
-#else
-	if (notify_die(DIE_TRAP, "int3", regs, error_code, 3, SIGTRAP)
-			== NOTIFY_STOP)
-		return;
-#endif
 
 	preempt_conditional_sti(regs);
 	do_trap(3, SIGTRAP, "int3", regs, error_code, NULL);

commit 1d48922c14b6363f6d5febb12464d804bb5cc53f
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri Sep 30 15:06:19 2011 -0400

    x86, nmi: Split out nmi from traps.c
    
    The nmi stuff is changing a lot and adding more functionality.  Split it
    out from the traps.c file so it doesn't continue to pollute that file.
    
    This makes it easier to find and expand all the future nmi related work.
    
    No real functional changes here.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1317409584-23662-2-git-send-email-dzickus@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 6913369c234c..a8e3eb83466c 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -81,15 +81,6 @@ gate_desc idt_table[NR_VECTORS] __page_aligned_data = { { { { 0, 0 } } }, };
 DECLARE_BITMAP(used_vectors, NR_VECTORS);
 EXPORT_SYMBOL_GPL(used_vectors);
 
-static int ignore_nmis;
-
-int unknown_nmi_panic;
-/*
- * Prevent NMI reason port (0x61) being accessed simultaneously, can
- * only be used in NMI handler.
- */
-static DEFINE_RAW_SPINLOCK(nmi_reason_lock);
-
 static inline void conditional_sti(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
@@ -307,152 +298,6 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	die("general protection fault", regs, error_code);
 }
 
-static int __init setup_unknown_nmi_panic(char *str)
-{
-	unknown_nmi_panic = 1;
-	return 1;
-}
-__setup("unknown_nmi_panic", setup_unknown_nmi_panic);
-
-static notrace __kprobes void
-pci_serr_error(unsigned char reason, struct pt_regs *regs)
-{
-	pr_emerg("NMI: PCI system error (SERR) for reason %02x on CPU %d.\n",
-		 reason, smp_processor_id());
-
-	/*
-	 * On some machines, PCI SERR line is used to report memory
-	 * errors. EDAC makes use of it.
-	 */
-#if defined(CONFIG_EDAC)
-	if (edac_handler_set()) {
-		edac_atomic_assert_error();
-		return;
-	}
-#endif
-
-	if (panic_on_unrecovered_nmi)
-		panic("NMI: Not continuing");
-
-	pr_emerg("Dazed and confused, but trying to continue\n");
-
-	/* Clear and disable the PCI SERR error line. */
-	reason = (reason & NMI_REASON_CLEAR_MASK) | NMI_REASON_CLEAR_SERR;
-	outb(reason, NMI_REASON_PORT);
-}
-
-static notrace __kprobes void
-io_check_error(unsigned char reason, struct pt_regs *regs)
-{
-	unsigned long i;
-
-	pr_emerg(
-	"NMI: IOCK error (debug interrupt?) for reason %02x on CPU %d.\n",
-		 reason, smp_processor_id());
-	show_registers(regs);
-
-	if (panic_on_io_nmi)
-		panic("NMI IOCK error: Not continuing");
-
-	/* Re-enable the IOCK line, wait for a few seconds */
-	reason = (reason & NMI_REASON_CLEAR_MASK) | NMI_REASON_CLEAR_IOCHK;
-	outb(reason, NMI_REASON_PORT);
-
-	i = 20000;
-	while (--i) {
-		touch_nmi_watchdog();
-		udelay(100);
-	}
-
-	reason &= ~NMI_REASON_CLEAR_IOCHK;
-	outb(reason, NMI_REASON_PORT);
-}
-
-static notrace __kprobes void
-unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
-{
-	if (notify_die(DIE_NMIUNKNOWN, "nmi", regs, reason, 2, SIGINT) ==
-			NOTIFY_STOP)
-		return;
-#ifdef CONFIG_MCA
-	/*
-	 * Might actually be able to figure out what the guilty party
-	 * is:
-	 */
-	if (MCA_bus) {
-		mca_handle_nmi();
-		return;
-	}
-#endif
-	pr_emerg("Uhhuh. NMI received for unknown reason %02x on CPU %d.\n",
-		 reason, smp_processor_id());
-
-	pr_emerg("Do you have a strange power saving mode enabled?\n");
-	if (unknown_nmi_panic || panic_on_unrecovered_nmi)
-		panic("NMI: Not continuing");
-
-	pr_emerg("Dazed and confused, but trying to continue\n");
-}
-
-static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
-{
-	unsigned char reason = 0;
-
-	/*
-	 * CPU-specific NMI must be processed before non-CPU-specific
-	 * NMI, otherwise we may lose it, because the CPU-specific
-	 * NMI can not be detected/processed on other CPUs.
-	 */
-	if (notify_die(DIE_NMI, "nmi", regs, 0, 2, SIGINT) == NOTIFY_STOP)
-		return;
-
-	/* Non-CPU-specific NMI: NMI sources can be processed on any CPU */
-	raw_spin_lock(&nmi_reason_lock);
-	reason = get_nmi_reason();
-
-	if (reason & NMI_REASON_MASK) {
-		if (reason & NMI_REASON_SERR)
-			pci_serr_error(reason, regs);
-		else if (reason & NMI_REASON_IOCHK)
-			io_check_error(reason, regs);
-#ifdef CONFIG_X86_32
-		/*
-		 * Reassert NMI in case it became active
-		 * meanwhile as it's edge-triggered:
-		 */
-		reassert_nmi();
-#endif
-		raw_spin_unlock(&nmi_reason_lock);
-		return;
-	}
-	raw_spin_unlock(&nmi_reason_lock);
-
-	unknown_nmi_error(reason, regs);
-}
-
-dotraplinkage notrace __kprobes void
-do_nmi(struct pt_regs *regs, long error_code)
-{
-	nmi_enter();
-
-	inc_irq_stat(__nmi_count);
-
-	if (!ignore_nmis)
-		default_do_nmi(regs);
-
-	nmi_exit();
-}
-
-void stop_nmi(void)
-{
-	ignore_nmis++;
-}
-
-void restart_nmi(void)
-{
-	ignore_nmis--;
-}
-
 /* May run on IST stack. */
 dotraplinkage void __kprobes do_int3(struct pt_regs *regs, long error_code)
 {

commit 06e727d2a5d9d889fabad35223ad77205a9bebb9
Merge: e68ff9cd1555 3ae36655b97a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 12 20:46:24 2011 -0700

    Merge branch 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-tip
    
    * 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-tip:
      x86-64: Rework vsyscall emulation and add vsyscall= parameter
      x86-64: Wire up getcpu syscall
      x86: Remove unnecessary compile flag tweaks for vsyscall code
      x86-64: Add vsyscall:emulate_vsyscall trace event
      x86-64: Add user_64bit_mode paravirt op
      x86-64, xen: Enable the vvar mapping
      x86-64: Work around gold bug 13023
      x86-64: Move the "user" vsyscall segment out of the data segment.
      x86-64: Pad vDSO to a page boundary

commit 3ae36655b97a03fa1decf72f04078ef945647c1a
Author: Andy Lutomirski <luto@mit.edu>
Date:   Wed Aug 10 11:15:32 2011 -0400

    x86-64: Rework vsyscall emulation and add vsyscall= parameter
    
    There are three choices:
    
    vsyscall=native: Vsyscalls are native code that issues the
    corresponding syscalls.
    
    vsyscall=emulate (default): Vsyscalls are emulated by instruction
    fault traps, tested in the bad_area path.  The actual contents of
    the vsyscall page is the same as the vsyscall=native case except
    that it's marked NX.  This way programs that make assumptions about
    what the code in the page does will not be confused when they read
    that code.
    
    vsyscall=none: Trying to execute a vsyscall will segfault.
    
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Link: http://lkml.kernel.org/r/8449fb3abf89851fd6b2260972666a6f82542284.1312988155.git.luto@mit.edu
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index fbc097a085ca..b9b67166f9de 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -872,12 +872,6 @@ void __init trap_init(void)
 	set_bit(SYSCALL_VECTOR, used_vectors);
 #endif
 
-#ifdef CONFIG_X86_64
-	BUG_ON(test_bit(VSYSCALL_EMU_VECTOR, used_vectors));
-	set_system_intr_gate(VSYSCALL_EMU_VECTOR, &emulate_vsyscall);
-	set_bit(VSYSCALL_EMU_VECTOR, used_vectors);
-#endif
-
 	/*
 	 * Should be a barrier for any external CPU state:
 	 */

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index fbc097a085ca..9682ec50180c 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -49,7 +49,7 @@
 #include <asm/stacktrace.h>
 #include <asm/processor.h>
 #include <asm/debugreg.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #include <asm/system.h>
 #include <asm/traps.h>
 #include <asm/desc.h>

commit 5cec93c216db77c45f7ce970d46283bcb1933884
Author: Andy Lutomirski <luto@MIT.EDU>
Date:   Sun Jun 5 13:50:24 2011 -0400

    x86-64: Emulate legacy vsyscalls
    
    There's a fair amount of code in the vsyscall page.  It contains
    a syscall instruction (in the gettimeofday fallback) and who
    knows what will happen if an exploit jumps into the middle of
    some other code.
    
    Reduce the risk by replacing the vsyscalls with short magic
    incantations that cause the kernel to emulate the real
    vsyscalls. These incantations are useless if entered in the
    middle.
    
    This causes vsyscalls to be a little more expensive than real
    syscalls.  Fortunately sensible programs don't use them.
    The only exception is time() which is still called by glibc
    through the vsyscall - but calling time() millions of times
    per second is not sensible. glibc has this fixed in the
    development tree.
    
    This patch is not perfect: the vread_tsc and vread_hpet
    functions are still at a fixed address.  Fixing that might
    involve making alternative patching work in the vDSO.
    
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jesper Juhl <jj@chaosbits.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Jan Beulich <JBeulich@novell.com>
    Cc: richard -rw- weinberger <richard.weinberger@gmail.com>
    Cc: Mikael Pettersson <mikpe@it.uu.se>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Louis Rilling <Louis.Rilling@kerlabs.com>
    Cc: Valdis.Kletnieks@vt.edu
    Cc: pageexec@freemail.hu
    Link: http://lkml.kernel.org/r/e64e1b3c64858820d12c48fa739efbd1485e79d5.1307292171.git.luto@mit.edu
    [ Removed the CONFIG option - it's simpler to just do it unconditionally. Tidied up the code as well. ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index b9b67166f9de..fbc097a085ca 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -872,6 +872,12 @@ void __init trap_init(void)
 	set_bit(SYSCALL_VECTOR, used_vectors);
 #endif
 
+#ifdef CONFIG_X86_64
+	BUG_ON(test_bit(VSYSCALL_EMU_VECTOR, used_vectors));
+	set_system_intr_gate(VSYSCALL_EMU_VECTOR, &emulate_vsyscall);
+	set_bit(VSYSCALL_EMU_VECTOR, used_vectors);
+#endif
+
 	/*
 	 * Should be a barrier for any external CPU state:
 	 */

commit f2fd43954abc058586e95d4eb91e7a5477070704
Author: Don Zickus <dzickus@redhat.com>
Date:   Thu Jan 6 16:18:52 2011 -0500

    x86, NMI: Clean-up default_do_nmi()
    
    Just re-arrange the code a bit to make it easier to follow what is
    going on.  Basically un-negating the if-statement and swapping the code
    inside the if-statement with code outside.
    
    No functional changes.
    
    Originally-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1294348732-15030-7-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 613b3d284a89..b9b67166f9de 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -410,26 +410,24 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	raw_spin_lock(&nmi_reason_lock);
 	reason = get_nmi_reason();
 
-	if (!(reason & NMI_REASON_MASK)) {
+	if (reason & NMI_REASON_MASK) {
+		if (reason & NMI_REASON_SERR)
+			pci_serr_error(reason, regs);
+		else if (reason & NMI_REASON_IOCHK)
+			io_check_error(reason, regs);
+#ifdef CONFIG_X86_32
+		/*
+		 * Reassert NMI in case it became active
+		 * meanwhile as it's edge-triggered:
+		 */
+		reassert_nmi();
+#endif
 		raw_spin_unlock(&nmi_reason_lock);
-		unknown_nmi_error(reason, regs);
-
 		return;
 	}
-
-	/* AK: following checks seem to be broken on modern chipsets. FIXME */
-	if (reason & NMI_REASON_SERR)
-		pci_serr_error(reason, regs);
-	if (reason & NMI_REASON_IOCHK)
-		io_check_error(reason, regs);
-#ifdef CONFIG_X86_32
-	/*
-	 * Reassert NMI in case it became active meanwhile
-	 * as it's edge-triggered:
-	 */
-	reassert_nmi();
-#endif
 	raw_spin_unlock(&nmi_reason_lock);
+
+	unknown_nmi_error(reason, regs);
 }
 
 dotraplinkage notrace __kprobes void

commit ab846f13f69fa64f8ed69ce0c3e239e075910d23
Author: Don Zickus <dzickus@redhat.com>
Date:   Thu Jan 6 16:18:51 2011 -0500

    x86, NMI: Allow NMI reason io port (0x61) to be processed on any CPU
    
    In original NMI handler, NMI reason io port (0x61) is only processed
    on BSP.  This makes it impossible to hot-remove BSP.  To solve the
    issue, a raw spinlock is used to allow the port to be processed on any
    CPU.
    
    Originally-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1294348732-15030-6-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 23f6ac05d04c..613b3d284a89 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -84,6 +84,11 @@ EXPORT_SYMBOL_GPL(used_vectors);
 static int ignore_nmis;
 
 int unknown_nmi_panic;
+/*
+ * Prevent NMI reason port (0x61) being accessed simultaneously, can
+ * only be used in NMI handler.
+ */
+static DEFINE_RAW_SPINLOCK(nmi_reason_lock);
 
 static inline void conditional_sti(struct pt_regs *regs)
 {
@@ -392,7 +397,6 @@ unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 {
 	unsigned char reason = 0;
-	int cpu;
 
 	/*
 	 * CPU-specific NMI must be processed before non-CPU-specific
@@ -402,13 +406,12 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	if (notify_die(DIE_NMI, "nmi", regs, 0, 2, SIGINT) == NOTIFY_STOP)
 		return;
 
-	cpu = smp_processor_id();
-
-	/* Only the BSP gets external NMIs from the system. */
-	if (!cpu)
-		reason = get_nmi_reason();
+	/* Non-CPU-specific NMI: NMI sources can be processed on any CPU */
+	raw_spin_lock(&nmi_reason_lock);
+	reason = get_nmi_reason();
 
 	if (!(reason & NMI_REASON_MASK)) {
+		raw_spin_unlock(&nmi_reason_lock);
 		unknown_nmi_error(reason, regs);
 
 		return;
@@ -426,6 +429,7 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	 */
 	reassert_nmi();
 #endif
+	raw_spin_unlock(&nmi_reason_lock);
 }
 
 dotraplinkage notrace __kprobes void

commit c410b8307702c1e1f35be3fd868ad18e4ba0410f
Author: Don Zickus <dzickus@redhat.com>
Date:   Thu Jan 6 16:18:50 2011 -0500

    x86, NMI: Remove DIE_NMI_IPI
    
    With priorities in place and no one really understanding the difference between
    DIE_NMI and DIE_NMI_IPI, just remove DIE_NMI_IPI and convert everyone to DIE_NMI.
    
    This also simplifies default_do_nmi() a little bit.  Instead of calling the
    die_notifier in both the if and else part, just pull it out and call it before
    the if-statement.  This has the side benefit of avoiding a call to the ioport
    to see if there is an external NMI sitting around until after the (more frequent)
    internal NMIs are dealt with.
    
    Patch-Inspired-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1294348732-15030-5-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index c7fd1cea0374..23f6ac05d04c 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -394,6 +394,14 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	unsigned char reason = 0;
 	int cpu;
 
+	/*
+	 * CPU-specific NMI must be processed before non-CPU-specific
+	 * NMI, otherwise we may lose it, because the CPU-specific
+	 * NMI can not be detected/processed on other CPUs.
+	 */
+	if (notify_die(DIE_NMI, "nmi", regs, 0, 2, SIGINT) == NOTIFY_STOP)
+		return;
+
 	cpu = smp_processor_id();
 
 	/* Only the BSP gets external NMIs from the system. */
@@ -401,21 +409,10 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 		reason = get_nmi_reason();
 
 	if (!(reason & NMI_REASON_MASK)) {
-		if (notify_die(DIE_NMI_IPI, "nmi_ipi", regs, reason, 2, SIGINT)
-								== NOTIFY_STOP)
-			return;
-
-#ifdef CONFIG_X86_LOCAL_APIC
-		if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT)
-							== NOTIFY_STOP)
-			return;
-#endif
 		unknown_nmi_error(reason, regs);
 
 		return;
 	}
-	if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT) == NOTIFY_STOP)
-		return;
 
 	/* AK: following checks seem to be broken on modern chipsets. FIXME */
 	if (reason & NMI_REASON_SERR)

commit 1c7b74d46fed530cca22a9a54140cdac2815c797
Author: Huang Ying <ying.huang@intel.com>
Date:   Thu Jan 6 16:18:47 2011 -0500

    x86, NMI: Add NMI symbol constants and rename memory parity to PCI SERR
    
    Replace the NMI related magic numbers with symbol constants.
    
    Memory parity error is only valid for IBM PC-AT, newer machine use
    bit 7 (0x80) of 0x61 port for PCI SERR. While memory error is usually
    reported via MCE. So corresponding function name and kernel log string
    is changed.
    
    But on some machines, PCI SERR line is still used to report memory
    errors. This is used by EDAC, so corresponding EDAC call is reserved.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1294348732-15030-2-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index c76aaca5694d..c7fd1cea0374 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -310,15 +310,15 @@ static int __init setup_unknown_nmi_panic(char *str)
 __setup("unknown_nmi_panic", setup_unknown_nmi_panic);
 
 static notrace __kprobes void
-mem_parity_error(unsigned char reason, struct pt_regs *regs)
+pci_serr_error(unsigned char reason, struct pt_regs *regs)
 {
-	printk(KERN_EMERG
-		"Uhhuh. NMI received for unknown reason %02x on CPU %d.\n",
-			reason, smp_processor_id());
-
-	printk(KERN_EMERG
-		"You have some hardware problem, likely on the PCI bus.\n");
+	pr_emerg("NMI: PCI system error (SERR) for reason %02x on CPU %d.\n",
+		 reason, smp_processor_id());
 
+	/*
+	 * On some machines, PCI SERR line is used to report memory
+	 * errors. EDAC makes use of it.
+	 */
 #if defined(CONFIG_EDAC)
 	if (edac_handler_set()) {
 		edac_atomic_assert_error();
@@ -329,11 +329,11 @@ mem_parity_error(unsigned char reason, struct pt_regs *regs)
 	if (panic_on_unrecovered_nmi)
 		panic("NMI: Not continuing");
 
-	printk(KERN_EMERG "Dazed and confused, but trying to continue\n");
+	pr_emerg("Dazed and confused, but trying to continue\n");
 
-	/* Clear and disable the memory parity error line. */
-	reason = (reason & 0xf) | 4;
-	outb(reason, 0x61);
+	/* Clear and disable the PCI SERR error line. */
+	reason = (reason & NMI_REASON_CLEAR_MASK) | NMI_REASON_CLEAR_SERR;
+	outb(reason, NMI_REASON_PORT);
 }
 
 static notrace __kprobes void
@@ -341,15 +341,17 @@ io_check_error(unsigned char reason, struct pt_regs *regs)
 {
 	unsigned long i;
 
-	printk(KERN_EMERG "NMI: IOCK error (debug interrupt?)\n");
+	pr_emerg(
+	"NMI: IOCK error (debug interrupt?) for reason %02x on CPU %d.\n",
+		 reason, smp_processor_id());
 	show_registers(regs);
 
 	if (panic_on_io_nmi)
 		panic("NMI IOCK error: Not continuing");
 
 	/* Re-enable the IOCK line, wait for a few seconds */
-	reason = (reason & 0xf) | 8;
-	outb(reason, 0x61);
+	reason = (reason & NMI_REASON_CLEAR_MASK) | NMI_REASON_CLEAR_IOCHK;
+	outb(reason, NMI_REASON_PORT);
 
 	i = 20000;
 	while (--i) {
@@ -357,8 +359,8 @@ io_check_error(unsigned char reason, struct pt_regs *regs)
 		udelay(100);
 	}
 
-	reason &= ~8;
-	outb(reason, 0x61);
+	reason &= ~NMI_REASON_CLEAR_IOCHK;
+	outb(reason, NMI_REASON_PORT);
 }
 
 static notrace __kprobes void
@@ -377,15 +379,14 @@ unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 		return;
 	}
 #endif
-	printk(KERN_EMERG
-		"Uhhuh. NMI received for unknown reason %02x on CPU %d.\n",
-			reason, smp_processor_id());
+	pr_emerg("Uhhuh. NMI received for unknown reason %02x on CPU %d.\n",
+		 reason, smp_processor_id());
 
-	printk(KERN_EMERG "Do you have a strange power saving mode enabled?\n");
+	pr_emerg("Do you have a strange power saving mode enabled?\n");
 	if (unknown_nmi_panic || panic_on_unrecovered_nmi)
 		panic("NMI: Not continuing");
 
-	printk(KERN_EMERG "Dazed and confused, but trying to continue\n");
+	pr_emerg("Dazed and confused, but trying to continue\n");
 }
 
 static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
@@ -399,7 +400,7 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	if (!cpu)
 		reason = get_nmi_reason();
 
-	if (!(reason & 0xc0)) {
+	if (!(reason & NMI_REASON_MASK)) {
 		if (notify_die(DIE_NMI_IPI, "nmi_ipi", regs, reason, 2, SIGINT)
 								== NOTIFY_STOP)
 			return;
@@ -417,9 +418,9 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 		return;
 
 	/* AK: following checks seem to be broken on modern chipsets. FIXME */
-	if (reason & 0x80)
-		mem_parity_error(reason, regs);
-	if (reason & 0x40)
+	if (reason & NMI_REASON_SERR)
+		pci_serr_error(reason, regs);
+	if (reason & NMI_REASON_IOCHK)
 		io_check_error(reason, regs);
 #ifdef CONFIG_X86_32
 	/*

commit 74d91e3c6a66359bb754fb5d8a5b54fb6ba2f9a6
Author: Huang Ying <ying.huang@intel.com>
Date:   Tue Jan 4 22:38:09 2011 -0500

    x86, NMI: Add touch_nmi_watchdog to io_check_error delay
    
    Prevent the long delay in io_check_error making NMI watchdog
    timeout.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    LKML-Reference: <1294198689-15447-3-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index bb6f04167361..c76aaca5694d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -351,9 +351,11 @@ io_check_error(unsigned char reason, struct pt_regs *regs)
 	reason = (reason & 0xf) | 8;
 	outb(reason, 0x61);
 
-	i = 2000;
-	while (--i)
-		udelay(1000);
+	i = 20000;
+	while (--i) {
+		touch_nmi_watchdog();
+		udelay(100);
+	}
 
 	reason &= ~8;
 	outb(reason, 0x61);

commit 5dc3055879b8f659f62abb7c3d1eaa4d02e36d65
Author: Don Zickus <dzickus@redhat.com>
Date:   Mon Nov 29 17:07:17 2010 -0500

    x86, NMI: Add back unknown_nmi_panic and nmi_watchdog sysctls
    
    Originally adapted from Huang Ying's patch which moved the
    unknown_nmi_panic to the traps.c file.  Because the old nmi
    watchdog was deleted before this change happened, the
    unknown_nmi_panic sysctl was lost.  This re-adds it.
    
    Also, the nmi_watchdog sysctl was re-implemented and its
    documentation updated accordingly.
    
    Patch-inspired-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Reviewed-by: Cyrill Gorcunov <gorcunov@gmail.com>
    Acked-by: Yinghai Lu <yinghai@kernel.org>
    Cc: fweisbec@gmail.com
    LKML-Reference: <1291068437-5331-3-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index f02c179c2552..bb6f04167361 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -83,6 +83,8 @@ EXPORT_SYMBOL_GPL(used_vectors);
 
 static int ignore_nmis;
 
+int unknown_nmi_panic;
+
 static inline void conditional_sti(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
@@ -300,6 +302,13 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	die("general protection fault", regs, error_code);
 }
 
+static int __init setup_unknown_nmi_panic(char *str)
+{
+	unknown_nmi_panic = 1;
+	return 1;
+}
+__setup("unknown_nmi_panic", setup_unknown_nmi_panic);
+
 static notrace __kprobes void
 mem_parity_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -371,7 +380,7 @@ unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 			reason, smp_processor_id());
 
 	printk(KERN_EMERG "Do you have a strange power saving mode enabled?\n");
-	if (panic_on_unrecovered_nmi)
+	if (unknown_nmi_panic || panic_on_unrecovered_nmi)
 		panic("NMI: Not continuing");
 
 	printk(KERN_EMERG "Dazed and confused, but trying to continue\n");
@@ -397,11 +406,8 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 		if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT)
 							== NOTIFY_STOP)
 			return;
-
-			unknown_nmi_error(reason, regs);
-#else
-		unknown_nmi_error(reason, regs);
 #endif
+		unknown_nmi_error(reason, regs);
 
 		return;
 	}

commit 072b198a4ad48bd722ec6d203d65422a4698eae7
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri Nov 12 11:22:24 2010 -0500

    x86, nmi_watchdog: Remove all stub function calls from old nmi_watchdog
    
    Now that the bulk of the old nmi_watchdog is gone, remove all
    the stub variables and hooks associated with it.
    
    This touches lots of files mainly because of how the io_apic
    nmi_watchdog was implemented.  Now that the io_apic nmi_watchdog
    is forever gone, remove all its fingers.
    
    Most of this code was not being exercised by virtue of
    nmi_watchdog != NMI_IO_APIC, so there shouldn't be anything to
    risky here.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: fweisbec@gmail.com
    Cc: gorcunov@openvz.org
    LKML-Reference: <1289578944-28564-3-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index db30d9cb9dd6..f02c179c2552 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -437,14 +437,12 @@ do_nmi(struct pt_regs *regs, long error_code)
 
 void stop_nmi(void)
 {
-	acpi_nmi_disable();
 	ignore_nmis++;
 }
 
 void restart_nmi(void)
 {
 	ignore_nmis--;
-	acpi_nmi_enable();
 }
 
 /* May run on IST stack. */

commit 5f2b0ba4d94b3ac23cbc4b7f675d98eb677a760a
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri Nov 12 11:22:23 2010 -0500

    x86, nmi_watchdog: Remove the old nmi_watchdog
    
    Now that we have a new nmi_watchdog that is more generic and
    sits on top of the perf subsystem, we really do not need the old
    nmi_watchdog any more.
    
    In addition, the old nmi_watchdog doesn't really work if you are
    using the default clocksource, hpet.  The old nmi_watchdog code
    relied on local apic interrupts to determine if the cpu is still
    alive.  With hpet as the clocksource, these interrupts don't
    increment any more and the old nmi_watchdog triggers false
    postives.
    
    This piece removes the old nmi_watchdog code and stubs out any
    variables and functions calls.  The stubs are the same ones used
    by the new nmi_watchdog code, so it should be well tested.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: fweisbec@gmail.com
    Cc: gorcunov@openvz.org
    LKML-Reference: <1289578944-28564-2-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index cb838ca42c96..db30d9cb9dd6 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -398,15 +398,6 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 							== NOTIFY_STOP)
 			return;
 
-#ifndef CONFIG_LOCKUP_DETECTOR
-		/*
-		 * Ok, so this is none of the documented NMI sources,
-		 * so it must be the NMI watchdog.
-		 */
-		if (nmi_watchdog_tick(regs, reason))
-			return;
-		if (!do_nmi_callback(regs, cpu))
-#endif /* !CONFIG_LOCKUP_DETECTOR */
 			unknown_nmi_error(reason, regs);
 #else
 		unknown_nmi_error(reason, regs);

commit 02f36038c568111ad4fc433f6fa760ff5e38fab4
Merge: 6c2754c28f23 676cb02dc32a 07bd8516a2f9 50f2d7f682f9 892df7f81c31 68f4d5a00ada b365a85c6816 6554287b1de0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 23 08:25:36 2010 -0700

    Merge branches 'softirq-for-linus', 'x86-debug-for-linus', 'x86-numa-for-linus', 'x86-quirks-for-linus', 'x86-setup-for-linus', 'x86-uv-for-linus' and 'x86-vm86-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'softirq-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      softirqs: Make wakeup_softirqd static
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, asm: Restore parentheses around one pushl_cfi argument
      x86, asm: Fix ancient-GAS workaround
      x86, asm: Fix CFI macro invocations to deal with shortcomings in gas
    
    * 'x86-numa-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, numa: Assign CPUs to nodes in round-robin manner on fake NUMA
    
    * 'x86-quirks-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: HPET force enable for CX700 / VIA Epia LT
    
    * 'x86-setup-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, setup: Use string copy operation to optimze copy in kernel compression
    
    * 'x86-uv-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, UV: Use allocated buffer in tlb_uv.c:tunables_read()
    
    * 'x86-vm86-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, vm86: Fix preemption bug for int1 debug and int3 breakpoint handlers.

commit 6554287b1de0448f1e02e200d02b43914e997d15
Author: Bart Oldeman <bartoldeman@gmail.com>
Date:   Thu Sep 23 13:16:58 2010 -0400

    x86, vm86: Fix preemption bug for int1 debug and int3 breakpoint handlers.
    
    Impact: fix kernel bug such as:
    BUG: scheduling while atomic: dosemu.bin/19680/0x00000004
    See also Ubuntu bug 455067 at
    https://bugs.launchpad.net/ubuntu/+source/linux/+bug/455067
    
    Commits 4915a35e35a037254550a2ba9f367a812bc37d40
    ("Use preempt_conditional_sti/cli in do_int3, like on x86_64.")
    and 3d2a71a596bd9c761c8487a2178e95f8a61da083
    ("x86, traps: converge do_debug handlers")
    started disabling preemption in int1 and int3 handlers on i386.
    The problem with vm86 is that the call to handle_vm86_trap() may jump
    straight to entry_32.S and never returns so preempt is never enabled
    again, and there is an imbalance in the preempt count.
    
    Commit be716615fe596ee117292dc615e95f707fb67fd1 ("x86, vm86:
    fix preemption bug"), which was later (accidentally?) reverted by commit
    08d68323d1f0c34452e614263b212ca556dae47f ("hw-breakpoints: modifying
    generic debug exception to use thread-specific debug registers")
    fixed the problem for debug exceptions but not for breakpoints.
    
    There are three solutions to this problem.
    
    1. Reenable preemption before calling handle_vm86_trap(). This
    was the approach that was later reverted.
    
    2. Do not disable preemption for i386 in breakpoint and debug handlers.
    This was the situation before October 2008. As far as I understand
    preemption only needs to be disabled on x86_64 because a seperate stack is
    used, but it's nice to have things work the same way on
    i386 and x86_64.
    
    3. Let handle_vm86_trap() return instead of jumping to assembly code.
    By setting a flag in _TIF_WORK_MASK, either TIF_IRET or TIF_NOTIFY_RESUME,
    the code in entry_32.S is instructed to return to 32 bit mode from
    V86 mode. The logic in entry_32.S was already present to handle signals.
    (I chose TIF_IRET because it's slightly more efficient in
    do_notify_resume() in signal.c, but in fact TIF_IRET can probably be
    replaced by TIF_NOTIFY_RESUME everywhere.)
    
    I'm submitting approach 3, because I believe it is the most elegant
    and prevents future confusion. Still, an obvious
    preempt_conditional_cli(regs); is necessary in traps.c to correct the
    bug.
    
    [ hpa: This is technically a regression, but because:
      1. the regression is so old,
      2. the patch seems relatively high risk, justifying more testing, and
      3. we're late in the 2.6.36-rc cycle,
    
      I'm queuing it up for the 2.6.37 merge window.  It might, however,
      justify as a -stable backport at a latter time, hence Cc: stable. ]
    
    Signed-off-by: Bart Oldeman <bartoldeman@users.sourceforge.net>
    LKML-Reference: <alpine.DEB.2.00.1009231312330.4732@localhost.localdomain>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: K.Prasad <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Alexander van Heukelum <heukelum@fastmail.fm>
    Cc: <stable@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 60788dee0f8a..9f4edeb21323 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -575,6 +575,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	if (regs->flags & X86_VM_MASK) {
 		handle_vm86_trap((struct kernel_vm86_regs *) regs,
 				error_code, 1);
+		preempt_conditional_cli(regs);
 		return;
 	}
 

commit a334fe43d85f570ae907acf988a053c5eff78d6e
Author: Brian Gerst <brgerst@gmail.com>
Date:   Fri Sep 3 21:17:15 2010 -0400

    x86-32, fpu: Remove math_emulate stub
    
    check_fpu() in bugs.c halts boot if no FPU is found and math emulation
    isn't enabled.  Therefore this stub will never be used.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1283563039-3466-9-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index d0029eb58589..d43968503dd2 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -776,21 +776,10 @@ asmlinkage void math_state_restore(void)
 }
 EXPORT_SYMBOL_GPL(math_state_restore);
 
-#ifndef CONFIG_MATH_EMULATION
-void math_emulate(struct math_emu_info *info)
-{
-	printk(KERN_EMERG
-		"math-emulation not enabled and no coprocessor found.\n");
-	printk(KERN_EMERG "killing %s.\n", current->comm);
-	force_sig(SIGFPE, current);
-	schedule();
-}
-#endif /* CONFIG_MATH_EMULATION */
-
 dotraplinkage void __kprobes
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
-#ifdef CONFIG_X86_32
+#ifdef CONFIG_MATH_EMULATION
 	if (read_cr0() & X86_CR0_EM) {
 		struct math_emu_info info = { };
 
@@ -798,12 +787,12 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 
 		info.regs = regs;
 		math_emulate(&info);
-	} else {
-		math_state_restore(); /* interrupts still off */
-		conditional_sti(regs);
+		return;
 	}
-#else
-	math_state_restore();
+#endif
+	math_state_restore(); /* interrupts still off */
+#ifdef CONFIG_X86_32
+	conditional_sti(regs);
 #endif
 }
 

commit 6ac8bac2684235f4caf22a410549c582aa7327d6
Author: Brian Gerst <brgerst@gmail.com>
Date:   Fri Sep 3 21:17:09 2010 -0400

    x86, fpu: Merge fpu_init()
    
    Make fpu_init() handle 32-bit setup.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1283563039-3466-3-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 60788dee0f8a..d0029eb58589 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -881,18 +881,6 @@ void __init trap_init(void)
 #endif
 
 #ifdef CONFIG_X86_32
-	if (cpu_has_fxsr) {
-		printk(KERN_INFO "Enabling fast FPU save and restore... ");
-		set_in_cr4(X86_CR4_OSFXSR);
-		printk("done.\n");
-	}
-	if (cpu_has_xmm) {
-		printk(KERN_INFO
-			"Enabling unmasked SIMD FPU exception support... ");
-		set_in_cr4(X86_CR4_OSXMMEXCPT);
-		printk("done.\n");
-	}
-
 	set_system_trap_gate(SYSCALL_VECTOR, &system_call);
 	set_bit(SYSCALL_VECTOR, used_vectors);
 #endif

commit 61be7fdec2f51b99570cd5dcc30c7848c8e56513
Merge: 12a81c8df13c eb703f98191a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 5 08:45:05 2010 +0200

    Merge branch 'perf/nmi' into perf/core
    
    Conflicts:
            kernel/Makefile
    
    Merge reason: Add the now complete topic, fix the conflict.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a1e80fafc9f0742a1776a0490258cb64912411b0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jun 30 15:09:06 2010 +0200

    x86: Send a SIGTRAP for user icebp traps
    
    Before we had a generic breakpoint layer, x86 used to send a
    sigtrap for any debug event that happened in userspace,
    except if it was caused by lazy dr7 switches.
    
    Currently we only send such signal for single step or breakpoint
    events.
    
    However, there are three other kind of debug exceptions:
    
    - debug register access detected: trigger an exception if the
      next instruction touches the debug registers. We don't use
      it.
    - task switch, but we don't use tss.
    - icebp/int01 trap. This instruction (0xf1) is undocumented and
      generates an int 1 exception. Unlike single step through TF
      flag, it doesn't set the single step origin of the exception
      in dr6.
    
    icebp then used to be reported in userspace using trap signals
    but this have been incidentally broken with the new breakpoint
    code. Reenable this. Since this is the only debug event that
    doesn't set anything in dr6, this is all we have to check.
    
    This fixes a regression in Wine where World Of Warcraft got broken
    as it uses this for software protection checks purposes. And
    probably other apps do.
    
    Reported-and-tested-by: Alexandre Julliard <julliard@winehq.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: 2.6.33.x 2.6.34.x <stable@kernel.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 142d70c74b02..725ef4d17cd5 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -526,6 +526,7 @@ asmlinkage __kprobes struct pt_regs *sync_regs(struct pt_regs *eregs)
 dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk = current;
+	int user_icebp = 0;
 	unsigned long dr6;
 	int si_code;
 
@@ -534,6 +535,14 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	/* Filter out all the reserved bits which are preset to 1 */
 	dr6 &= ~DR6_RESERVED;
 
+	/*
+	 * If dr6 has no reason to give us about the origin of this trap,
+	 * then it's very likely the result of an icebp/int01 trap.
+	 * User wants a sigtrap for that.
+	 */
+	if (!dr6 && user_mode(regs))
+		user_icebp = 1;
+
 	/* Catch kmemcheck conditions first of all! */
 	if ((dr6 & DR_STEP) && kmemcheck_trap(regs))
 		return;
@@ -575,7 +584,7 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 		regs->flags &= ~X86_EFLAGS_TF;
 	}
 	si_code = get_si_code(tsk->thread.debugreg6);
-	if (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS))
+	if (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)
 		send_sigtrap(tsk, regs, error_code, si_code);
 	preempt_conditional_cli(regs);
 

commit 29c843912a0baa7fa63033fe28e1ca7e796686a5
Author: Jan Kiszka <jan.kiszka@web.de>
Date:   Thu May 20 21:04:29 2010 -0500

    x86, kgdb: early trap init for early debug
    
    Allow the x86 arch to have early exception processing for the purpose
    of debugging via the kgdb.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@web.de>
    Signed-off-by: Jason Wessel <jason.wessel@windriver.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7eaad4c5110a..142d70c74b02 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -808,6 +808,16 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 }
 #endif
 
+/* Set of traps needed for early debugging. */
+void __init early_trap_init(void)
+{
+	set_intr_gate_ist(1, &debug, DEBUG_STACK);
+	/* int3 can be called from all */
+	set_system_intr_gate_ist(3, &int3, DEBUG_STACK);
+	set_intr_gate(14, &page_fault);
+	load_idt(&idt_descr);
+}
+
 void __init trap_init(void)
 {
 	int i;
@@ -821,10 +831,7 @@ void __init trap_init(void)
 #endif
 
 	set_intr_gate(0, &divide_error);
-	set_intr_gate_ist(1, &debug, DEBUG_STACK);
 	set_intr_gate_ist(2, &nmi, NMI_STACK);
-	/* int3 can be called from all */
-	set_system_intr_gate_ist(3, &int3, DEBUG_STACK);
 	/* int4 can be called from all */
 	set_system_intr_gate(4, &overflow);
 	set_intr_gate(5, &bounds);
@@ -840,7 +847,6 @@ void __init trap_init(void)
 	set_intr_gate(11, &segment_not_present);
 	set_intr_gate_ist(12, &stack_segment, STACKFAULT_STACK);
 	set_intr_gate(13, &general_protection);
-	set_intr_gate(14, &page_fault);
 	set_intr_gate(15, &spurious_interrupt_bug);
 	set_intr_gate(16, &coprocessor_error);
 	set_intr_gate(17, &alignment_check);

commit f503b5ae53cb557ac351a668fcac1baab1cef0db
Author: Jason Wessel <jason.wessel@windriver.com>
Date:   Thu May 20 21:04:25 2010 -0500

    x86,kgdb: Add low level debug hook
    
    The only way the debugger can handle a trap in inside rcu_lock,
    notify_die, or atomic_notifier_call_chain without a triple fault is
    to have a low level "first opportunity handler" in the int3 exception
    handler.
    
    Generally this will be something the vast majority of folks will not
    need, but for those who need it, it is added as a kernel .config
    option called KGDB_LOW_LEVEL_TRAP.
    
    CC: Ingo Molnar <mingo@elte.hu>
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: H. Peter Anvin <hpa@zytor.com>
    CC: x86@kernel.org
    Signed-off-by: Jason Wessel <jason.wessel@windriver.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 02cfb9b8f5b1..7eaad4c5110a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -15,6 +15,7 @@
 #include <linux/kprobes.h>
 #include <linux/uaccess.h>
 #include <linux/kdebug.h>
+#include <linux/kgdb.h>
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/ptrace.h>
@@ -451,6 +452,11 @@ void restart_nmi(void)
 /* May run on IST stack. */
 dotraplinkage void __kprobes do_int3(struct pt_regs *regs, long error_code)
 {
+#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
+	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, 3, SIGTRAP)
+			== NOTIFY_STOP)
+		return;
+#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */
 #ifdef CONFIG_KPROBES
 	if (notify_die(DIE_INT3, "int3", regs, error_code, 3, SIGTRAP)
 			== NOTIFY_STOP)

commit 41d59102e146a4423a490b8eca68a5860af4fe1c
Merge: 3e1dd193edef c9775b4cc522
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:58:16 2010 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, fpu: Use static_cpu_has() to implement use_xsave()
      x86: Add new static_cpu_has() function using alternatives
      x86, fpu: Use the proper asm constraint in use_xsave()
      x86, fpu: Unbreak FPU emulation
      x86: Introduce 'struct fpu' and related API
      x86: Eliminate TS_XSAVE
      x86-32: Don't set ignore_fpu_irq in simd exception
      x86: Merge kernel_math_error() into math_error()
      x86: Merge simd_math_error() into math_error()
      x86-32: Rework cache flush denied handler
    
    Fix trivial conflict in arch/x86/kernel/process.c

commit 58687acba59266735adb8ccd9b5b9aa2c7cd205b
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri May 7 17:11:44 2010 -0400

    lockup_detector: Combine nmi_watchdog and softlockup detector
    
    The new nmi_watchdog (which uses the perf event subsystem) is very
    similar in structure to the softlockup detector.  Using Ingo's
    suggestion, I combined the two functionalities into one file:
    kernel/watchdog.c.
    
    Now both the nmi_watchdog (or hardlockup detector) and softlockup
    detector sit on top of the perf event subsystem, which is run every
    60 seconds or so to see if there are any lockups.
    
    To detect hardlockups, cpus not responding to interrupts, I
    implemented an hrtimer that runs 5 times for every perf event
    overflow event.  If that stops counting on a cpu, then the cpu is
    most likely in trouble.
    
    To detect softlockups, tasks not yielding to the scheduler, I used the
    previous kthread idea that now gets kicked every time the hrtimer fires.
    If the kthread isn't being scheduled neither is anyone else and the
    warning is printed to the console.
    
    I tested this on x86_64 and both the softlockup and hardlockup paths
    work.
    
    V2:
    - cleaned up the Kconfig and softlockup combination
    - surrounded hardlockup cases with #ifdef CONFIG_PERF_EVENTS_NMI
    - seperated out the softlockup case from perf event subsystem
    - re-arranged the enabling/disabling nmi watchdog from proc space
    - added cpumasks for hardlockup failure cases
    - removed fallback to soft events if no PMU exists for hard events
    
    V3:
    - comment cleanups
    - drop support for older softlockup code
    - per_cpu cleanups
    - completely remove software clock base hardlockup detector
    - use per_cpu masking on hard/soft lockup detection
    - #ifdef cleanups
    - rename config option NMI_WATCHDOG to LOCKUP_DETECTOR
    - documentation additions
    
    V4:
    - documentation fixes
    - convert per_cpu to __get_cpu_var
    - powerpc compile fixes
    
    V5:
    - split apart warn flags for hard and soft lockups
    
    TODO:
    - figure out how to make an arch-agnostic clock2cycles call
      (if possible) to feed into perf events as a sample period
    
    [fweisbec: merged conflict patch]
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    LKML-Reference: <1273266711-18706-2-git-send-email-dzickus@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index bdc7fab3ef3e..bd347c2b34dc 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -406,7 +406,7 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 							== NOTIFY_STOP)
 			return;
 
-#ifndef CONFIG_NMI_WATCHDOG
+#ifndef CONFIG_LOCKUP_DETECTOR
 		/*
 		 * Ok, so this is none of the documented NMI sources,
 		 * so it must be the NMI watchdog.
@@ -414,7 +414,7 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 		if (nmi_watchdog_tick(regs, reason))
 			return;
 		if (!do_nmi_callback(regs, cpu))
-#endif /* !CONFIG_NMI_WATCHDOG */
+#endif /* !CONFIG_LOCKUP_DETECTOR */
 			unknown_nmi_error(reason, regs);
 #else
 		unknown_nmi_error(reason, regs);

commit 250825008f1f94887bc039e9227a8adfb5ba366e
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sun Mar 21 09:00:46 2010 -0400

    x86-32: Don't set ignore_fpu_irq in simd exception
    
    Any processor that supports simd will have an internal fpu, and the
    irq13 handler will not be enabled.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1269176446-2489-5-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 53ba86fc5dd8..00516e1de55d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -675,10 +675,6 @@ dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-#ifdef CONFIG_X86_32
-	ignore_fpu_irq = 1;
-#endif
-
 	math_error(regs, error_code, 19);
 }
 

commit e2e75c915de045f0785387dc32f55e92fab0614c
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sun Mar 21 09:00:45 2010 -0400

    x86: Merge kernel_math_error() into math_error()
    
    Clean up the kernel exception handling and make it more similar to
    the other traps.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1269176446-2489-4-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a472992cecdc..53ba86fc5dd8 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -576,20 +576,6 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	return;
 }
 
-#ifdef CONFIG_X86_64
-static int kernel_math_error(struct pt_regs *regs, const char *str, int trapnr)
-{
-	if (fixup_exception(regs))
-		return 1;
-
-	notify_die(DIE_GPF, str, regs, 0, trapnr, SIGFPE);
-	/* Illegal floating point operation in the kernel */
-	current->thread.trap_no = trapnr;
-	die(str, regs, 0);
-	return 0;
-}
-#endif
-
 /*
  * Note that we play around with the 'TS' bit in an attempt to get
  * the correct behaviour even in the presence of the asynchronous
@@ -597,14 +583,28 @@ static int kernel_math_error(struct pt_regs *regs, const char *str, int trapnr)
  */
 void math_error(struct pt_regs *regs, int error_code, int trapnr)
 {
-	struct task_struct *task;
+	struct task_struct *task = current;
 	siginfo_t info;
 	unsigned short err;
+	char *str = (trapnr == 16) ? "fpu exception" : "simd exception";
+
+	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)
+		return;
+	conditional_sti(regs);
+
+	if (!user_mode_vm(regs))
+	{
+		if (!fixup_exception(regs)) {
+			task->thread.error_code = error_code;
+			task->thread.trap_no = trapnr;
+			die(str, regs, error_code);
+		}
+		return;
+	}
 
 	/*
 	 * Save the info for the exception handler and clear the error.
 	 */
-	task = current;
 	save_init_fpu(task);
 	task->thread.trap_no = trapnr;
 	task->thread.error_code = error_code;
@@ -665,14 +665,8 @@ void math_error(struct pt_regs *regs, int error_code, int trapnr)
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-	conditional_sti(regs);
-
 #ifdef CONFIG_X86_32
 	ignore_fpu_irq = 1;
-#else
-	if (!user_mode(regs) &&
-	    kernel_math_error(regs, "kernel x87 math error", 16))
-		return;
 #endif
 
 	math_error(regs, error_code, 16);
@@ -681,14 +675,8 @@ dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-	conditional_sti(regs);
-
 #ifdef CONFIG_X86_32
 	ignore_fpu_irq = 1;
-#else
-	if (!user_mode(regs) &&
-			kernel_math_error(regs, "kernel simd math error", 19))
-		return;
 #endif
 
 	math_error(regs, error_code, 19);

commit 9b6dba9e0798325dab427b9d60c61630ccc39b28
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sun Mar 21 09:00:44 2010 -0400

    x86: Merge simd_math_error() into math_error()
    
    The only difference between FPU and SIMD exceptions is where the
    status bits are read from (cwd/swd vs. mxcsr).  This also fixes
    the discrepency introduced by commit adf77bac, which fixed FPU
    but not SIMD.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1269176446-2489-3-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a16c9dfe6b70..a472992cecdc 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -595,36 +595,48 @@ static int kernel_math_error(struct pt_regs *regs, const char *str, int trapnr)
  * the correct behaviour even in the presence of the asynchronous
  * IRQ13 behaviour
  */
-void math_error(void __user *ip)
+void math_error(struct pt_regs *regs, int error_code, int trapnr)
 {
 	struct task_struct *task;
 	siginfo_t info;
-	unsigned short cwd, swd, err;
+	unsigned short err;
 
 	/*
 	 * Save the info for the exception handler and clear the error.
 	 */
 	task = current;
 	save_init_fpu(task);
-	task->thread.trap_no = 16;
-	task->thread.error_code = 0;
+	task->thread.trap_no = trapnr;
+	task->thread.error_code = error_code;
 	info.si_signo = SIGFPE;
 	info.si_errno = 0;
-	info.si_addr = ip;
-	/*
-	 * (~cwd & swd) will mask out exceptions that are not set to unmasked
-	 * status.  0x3f is the exception bits in these regs, 0x200 is the
-	 * C1 reg you need in case of a stack fault, 0x040 is the stack
-	 * fault bit.  We should only be taking one exception at a time,
-	 * so if this combination doesn't produce any single exception,
-	 * then we have a bad program that isn't synchronizing its FPU usage
-	 * and it will suffer the consequences since we won't be able to
-	 * fully reproduce the context of the exception
-	 */
-	cwd = get_fpu_cwd(task);
-	swd = get_fpu_swd(task);
+	info.si_addr = (void __user *)regs->ip;
+	if (trapnr == 16) {
+		unsigned short cwd, swd;
+		/*
+		 * (~cwd & swd) will mask out exceptions that are not set to unmasked
+		 * status.  0x3f is the exception bits in these regs, 0x200 is the
+		 * C1 reg you need in case of a stack fault, 0x040 is the stack
+		 * fault bit.  We should only be taking one exception at a time,
+		 * so if this combination doesn't produce any single exception,
+		 * then we have a bad program that isn't synchronizing its FPU usage
+		 * and it will suffer the consequences since we won't be able to
+		 * fully reproduce the context of the exception
+		 */
+		cwd = get_fpu_cwd(task);
+		swd = get_fpu_swd(task);
 
-	err = swd & ~cwd;
+		err = swd & ~cwd;
+	} else {
+		/*
+		 * The SIMD FPU exceptions are handled a little differently, as there
+		 * is only a single status/control register.  Thus, to determine which
+		 * unmasked exception was caught we must mask the exception mask bits
+		 * at 0x1f80, and then use these to mask the exception bits at 0x3f.
+		 */
+		unsigned short mxcsr = get_fpu_mxcsr(task);
+		err = ~(mxcsr >> 7) & mxcsr;
+	}
 
 	if (err & 0x001) {	/* Invalid op */
 		/*
@@ -663,55 +675,7 @@ dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 		return;
 #endif
 
-	math_error((void __user *)regs->ip);
-}
-
-static void simd_math_error(void __user *ip)
-{
-	struct task_struct *task;
-	siginfo_t info;
-	unsigned short mxcsr;
-
-	/*
-	 * Save the info for the exception handler and clear the error.
-	 */
-	task = current;
-	save_init_fpu(task);
-	task->thread.trap_no = 19;
-	task->thread.error_code = 0;
-	info.si_signo = SIGFPE;
-	info.si_errno = 0;
-	info.si_code = __SI_FAULT;
-	info.si_addr = ip;
-	/*
-	 * The SIMD FPU exceptions are handled a little differently, as there
-	 * is only a single status/control register.  Thus, to determine which
-	 * unmasked exception was caught we must mask the exception mask bits
-	 * at 0x1f80, and then use these to mask the exception bits at 0x3f.
-	 */
-	mxcsr = get_fpu_mxcsr(task);
-	switch (~((mxcsr & 0x1f80) >> 7) & (mxcsr & 0x3f)) {
-	case 0x000:
-	default:
-		break;
-	case 0x001: /* Invalid Op */
-		info.si_code = FPE_FLTINV;
-		break;
-	case 0x002: /* Denormalize */
-	case 0x010: /* Underflow */
-		info.si_code = FPE_FLTUND;
-		break;
-	case 0x004: /* Zero Divide */
-		info.si_code = FPE_FLTDIV;
-		break;
-	case 0x008: /* Overflow */
-		info.si_code = FPE_FLTOVF;
-		break;
-	case 0x020: /* Precision */
-		info.si_code = FPE_FLTRES;
-		break;
-	}
-	force_sig_info(SIGFPE, &info, task);
+	math_error(regs, error_code, 16);
 }
 
 dotraplinkage void
@@ -727,7 +691,7 @@ do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 		return;
 #endif
 
-	simd_math_error((void __user *)regs->ip);
+	math_error(regs, error_code, 19);
 }
 
 dotraplinkage void

commit 40d2e76315da38993129090dc5d56377e573c312
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sun Mar 21 09:00:43 2010 -0400

    x86-32: Rework cache flush denied handler
    
    The cache flush denied error is an erratum on some AMD 486 clones.  If an invd
    instruction is executed in userspace, the processor calls exception 19 (13 hex)
    instead of #GP (13 decimal).  On cpus where XMM is not supported, redirect
    exception 19 to do_general_protection().  Also, remove die_if_kernel(), since
    this was the last user.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1269176446-2489-2-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1168e4454188..a16c9dfe6b70 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -108,15 +108,6 @@ static inline void preempt_conditional_cli(struct pt_regs *regs)
 	dec_preempt_count();
 }
 
-#ifdef CONFIG_X86_32
-static inline void
-die_if_kernel(const char *str, struct pt_regs *regs, long err)
-{
-	if (!user_mode_vm(regs))
-		die(str, regs, err);
-}
-#endif
-
 static void __kprobes
 do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 	long error_code, siginfo_t *info)
@@ -729,30 +720,14 @@ do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 	conditional_sti(regs);
 
 #ifdef CONFIG_X86_32
-	if (cpu_has_xmm) {
-		/* Handle SIMD FPU exceptions on PIII+ processors. */
-		ignore_fpu_irq = 1;
-		simd_math_error((void __user *)regs->ip);
-		return;
-	}
-	/*
-	 * Handle strange cache flush from user space exception
-	 * in all other cases.  This is undocumented behaviour.
-	 */
-	if (regs->flags & X86_VM_MASK) {
-		handle_vm86_fault((struct kernel_vm86_regs *)regs, error_code);
-		return;
-	}
-	current->thread.trap_no = 19;
-	current->thread.error_code = error_code;
-	die_if_kernel("cache flush denied", regs, error_code);
-	force_sig(SIGSEGV, current);
+	ignore_fpu_irq = 1;
 #else
 	if (!user_mode(regs) &&
 			kernel_math_error(regs, "kernel simd math error", 19))
 		return;
-	simd_math_error((void __user *)regs->ip);
 #endif
+
+	simd_math_error((void __user *)regs->ip);
 }
 
 dotraplinkage void

commit ea8e61b7bbc4a2faef77db34eb2db2a2c2372ff6
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:51 2010 +0100

    x86, ptrace: Fix block-step
    
    Implement ptrace-block-step using TIF_BLOCKSTEP which will set
    DEBUGCTLMSR_BTF when set for a task while preserving any other
    DEBUGCTLMSR bits.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100325135414.017536066@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index e3da5d726a37..36f1bd9f8e76 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -544,6 +544,11 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	/* DR6 may or may not be cleared by the CPU */
 	set_debugreg(0, 6);
 
+	/*
+	 * The processor cleared BTF, so don't mark that we need it set.
+	 */
+	clear_tsk_thread_flag(tsk, TIF_BLOCKSTEP);
+
 	/* Store the virtualized DR6 value */
 	tsk->thread.debugreg6 = dr6;
 

commit faa4602e47690fb11221e00f9b9697c8dc0d4b19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:50 2010 +0100

    x86, perf, bts, mm: Delete the never used BTS-ptrace code
    
    Support for the PMU's BTS features has been upstreamed in
    v2.6.32, but we still have the old and disabled ptrace-BTS,
    as Linus noticed it not so long ago.
    
    It's buggy: TIF_DEBUGCTLMSR is trampling all over that MSR without
    regard for other uses (perf) and doesn't provide the flexibility
    needed for perf either.
    
    Its users are ptrace-block-step and ptrace-bts, since ptrace-bts
    was never used and ptrace-block-step can be implemented using a
    much simpler approach.
    
    So axe all 3000 lines of it. That includes the *locked_memory*()
    APIs in mm/mlock.c as well.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20100325135413.938004390@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1168e4454188..e3da5d726a37 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -543,11 +543,6 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 
 	/* DR6 may or may not be cleared by the CPU */
 	set_debugreg(0, 6);
-	/*
-	 * The processor cleared BTF, so don't mark that we need it set.
-	 */
-	clear_tsk_thread_flag(tsk, TIF_DEBUGCTLMSR);
-	tsk->thread.debugctlmsr = 0;
 
 	/* Store the virtualized DR6 value */
 	tsk->thread.debugreg6 = dr6;

commit 47195d57636604ff6048b0d7aa3e4ed9643f6073
Author: Don Zickus <dzickus@redhat.com>
Date:   Mon Feb 22 18:09:03 2010 -0500

    nmi_watchdog: Clean up various small details
    
    Mostly copy/paste whitespace damage with a couple of nitpicks by
    the checkpatch script. Fix the struct definition as requested by Ingo too.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: peterz@infradead.org
    Cc: gorcunov@gmail.com
    Cc: aris@redhat.com
    LKML-Reference: <1266880143-24943-1-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    --
     arch/x86/kernel/apic/hw_nmi.c |   14 +++++------
     arch/x86/kernel/traps.c       |    6 ++--
     include/linux/nmi.h           |    2 -
     kernel/nmi_watchdog.c         |   51 ++++++++++++++++++++----------------------
     4 files changed, 36 insertions(+), 37 deletions(-)

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 973cbc4f044f..bdc7fab3ef3e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -402,9 +402,9 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 			return;
 
 #ifdef CONFIG_X86_LOCAL_APIC
-	        if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT)
-	        			                == NOTIFY_STOP)
-	                return;
+		if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT)
+							== NOTIFY_STOP)
+			return;
 
 #ifndef CONFIG_NMI_WATCHDOG
 		/*

commit 84e478c6f1eb9c4bfa1fff2f8108e9a061b46428
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri Feb 5 21:47:05 2010 -0500

    nmi_watchdog: Config option to enable new nmi_watchdog
    
    These are the bits that enable the new nmi_watchdog and safely
    isolate the old nmi_watchdog.  Only one or the other can run,
    not both at the same time.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: gorcunov@gmail.com
    Cc: aris@redhat.com
    Cc: peterz@infradead.org
    LKML-Reference: <1265424425-31562-4-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 51ef893ffa65..973cbc4f044f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -406,6 +406,7 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	        			                == NOTIFY_STOP)
 	                return;
 
+#ifndef CONFIG_NMI_WATCHDOG
 		/*
 		 * Ok, so this is none of the documented NMI sources,
 		 * so it must be the NMI watchdog.
@@ -413,6 +414,7 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 		if (nmi_watchdog_tick(regs, reason))
 			return;
 		if (!do_nmi_callback(regs, cpu))
+#endif /* !CONFIG_NMI_WATCHDOG */
 			unknown_nmi_error(reason, regs);
 #else
 		unknown_nmi_error(reason, regs);

commit e40b17208b6805be50ffe891878662b6076206b9
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri Feb 5 21:47:03 2010 -0500

    x86: Move notify_die from nmi.c to traps.c
    
    In order to handle a new nmi_watchdog approach, I need to move
    the notify_die() routine out of nmi_watchdog_tick() and into
    default_do_nmi(). This lets me easily swap out the old
    nmi_watchdog with the new one with just a config change.
    
    The change probably makes sense from a high level perspective
    because the nmi_watchdog shouldn't be handling notify_die
    routines anyway.  However, this move does change the semantics a
    little bit.  Instead of checking on every nmi interrupt if the
    cpus are stuck, only check them on the nmi_watchdog interrupts.
    
     v2: Move notify_die call into #idef block
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: gorcunov@gmail.com
    Cc: aris@redhat.com
    Cc: peterz@infradead.org
    LKML-Reference: <1265424425-31562-2-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1168e4454188..51ef893ffa65 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -400,7 +400,12 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 		if (notify_die(DIE_NMI_IPI, "nmi_ipi", regs, reason, 2, SIGINT)
 								== NOTIFY_STOP)
 			return;
+
 #ifdef CONFIG_X86_LOCAL_APIC
+	        if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT)
+	        			                == NOTIFY_STOP)
+	                return;
+
 		/*
 		 * Ok, so this is none of the documented NMI sources,
 		 * so it must be the NMI watchdog.

commit 40f9249a73f6c251adea492b1c3d19d39e2a9bda
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Thu Jan 28 16:44:01 2010 +0530

    x86/debug: Clear reserved bits of DR6 in do_debug()
    
    Clear the reserved bits from the stored copy of debug status
    register (DR6).
    This will help easy bitwise operations such as quick testing
    of a debug event origin.
    
    Signed-off-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Jan Kiszka <jan.kiszka@siemens.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Ingo Molnar <mingo@elte.hu>
    LKML-Reference: <20100128111401.GB13935@in.ibm.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 33399176512a..1168e4454188 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -534,6 +534,9 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 
 	get_debugreg(dr6, 6);
 
+	/* Filter out all the reserved bits which are preset to 1 */
+	dr6 &= ~DR6_RESERVED;
+
 	/* Catch kmemcheck conditions first of all! */
 	if ((dr6 & DR_STEP) && kmemcheck_trap(regs))
 		return;

commit 0f8f86c7bdd1c954fbe153af437a0d91a6c5721a
Merge: dca2d6ac09d9 f39cdf25bf77
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Oct 18 01:09:09 2009 +0200

    Merge commit 'perf/core' into perf/hw-breakpoint
    
    Conflicts:
            kernel/Makefile
            kernel/trace/Makefile
            kernel/trace/trace.h
            samples/Makefile
    
    Merge reason: We need to be uptodate with the perf events development
    branch because we plan to rewrite the breakpoints API on top of
    perf events.

commit 5bb241b325d7d91bc4ec0b394f31dffb17fe7978
Merge: 76e0134f4154 704daf55c729
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 26 10:13:35 2009 -0700

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Remove redundant non-NUMA topology functions
      x86: early_printk: Protect against using the same device twice
      x86: Reduce verbosity of "PAT enabled" kernel message
      x86: Reduce verbosity of "TSC is reliable" message
      x86: mce: Use safer ways to access MCE registers
      x86: mce, inject: Use real inject-msg in raise_local
      x86: mce: Fix thermal throttling message storm
      x86: mce: Clean up thermal throttling state tracking code
      x86: split NX setup into separate file to limit unstack-protected code
      xen: check EFER for NX before setting up GDT mapping
      x86: Cleanup linker script using new linker script macros.
      x86: Use section .data.page_aligned for the idt_table.
      x86: convert to use __HEAD and HEAD_TEXT macros.
      x86: convert compressed loader to use __HEAD and HEAD_TEXT macros.
      x86: fix fragile computation of vsyscall address

commit 704daf55c7297e727021063cb5d8ba1c55b84426
Merge: b0c6fbe45818 123f3e1d7695
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 25 10:45:32 2009 +0200

    Merge branch 'x86/asm' into x86/urgent
    
    Merge reason: The linker script cleanups are ready for upstream.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 2bcd57ab61e7cabed626226a3771617981c11ce1
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Sep 24 04:22:25 2009 +0400

    headers: utsname.h redux
    
    * remove asm/atomic.h inclusion from linux/utsname.h --
       not needed after kref conversion
     * remove linux/utsname.h inclusion from files which do not need it
    
    NOTE: it looks like fs/binfmt_elf.c do not need utsname.h, however
    due to some personality stuff it _is_ needed -- cowardly leave ELF-related
    headers and files alone.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 9346e102338d..a665c71352b8 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -14,7 +14,6 @@
 #include <linux/spinlock.h>
 #include <linux/kprobes.h>
 #include <linux/uaccess.h>
-#include <linux/utsname.h>
 #include <linux/kdebug.h>
 #include <linux/kernel.h>
 #include <linux/module.h>

commit 144374dcc3ad0436f0a1bb3095836cf0ec32eebe
Author: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
Date:   Sun Sep 20 15:09:26 2009 +0530

    includecheck fix: x86, traps.c
    
    fix the following 'make includecheck' warning:
    
      arch/x86/kernel/traps.c: asm/traps.h is included more than once.
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    LKML-Reference: <1247065094.4382.49.camel@ht.satnam>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7dc0de9d1ed9..9346e102338d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -65,7 +65,6 @@
 #else
 #include <asm/processor-flags.h>
 #include <asm/setup.h>
-#include <asm/traps.h>
 
 asmlinkage int system_call(void);
 

commit 78f28b7c555359c67c2a0d23f7436e915329421e
Merge: 3240a77b515f 7bd867dfb4e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 18 14:05:47 2009 -0700

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (38 commits)
      x86: Move get/set_wallclock to x86_platform_ops
      x86: platform: Fix section annotations
      x86: apic namespace cleanup
      x86: Distangle ioapic and i8259
      x86: Add Moorestown early detection
      x86: Add hardware_subarch ID for Moorestown
      x86: Add early platform detection
      x86: Move tsc_init to late_time_init
      x86: Move tsc_calibration to x86_init_ops
      x86: Replace the now identical time_32/64.c by time.c
      x86: time_32/64.c unify profile_pc
      x86: Move calibrate_cpu to tsc.c
      x86: Make timer setup and global variables the same in time_32/64.c
      x86: Remove mca bus ifdef from timer interrupt
      x86: Simplify timer_ack magic in time_32.c
      x86: Prepare unification of time_32/64.c
      x86: Remove do_timer hook
      x86: Add timer_init to x86_init_ops
      x86: Move percpu clockevents setup to x86_init_ops
      x86: Move xen_post_allocator_init into xen_pagetable_setup_done
      ...
    
    Fix up conflicts in arch/x86/include/asm/io_apic.h

commit 07e81d61605f885920f31634a65aace52beb97db
Author: Tim Abbott <tabbott@ksplice.com>
Date:   Wed Sep 16 16:44:29 2009 -0400

    x86: Use section .data.page_aligned for the idt_table.
    
    The .data.idt section is just squashed into the .data.page_aligned
    output section by the linker script anyway, so it might as well be in
    the .data.page_aligned section.
    
    This eliminates all references to .data.idt on x86.
    
    Signed-off-by: Tim Abbott <tabbott@ksplice.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 83264922a878..ea23d3b76f95 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -73,11 +73,9 @@ char ignore_fpu_irq;
 
 /*
  * The IDT has to be page-aligned to simplify the Pentium
- * F0 0F bug workaround.. We have a special link segment
- * for this.
+ * F0 0F bug workaround.
  */
-gate_desc idt_table[NR_VECTORS]
-	__attribute__((__section__(".data.idt"))) = { { { { 0, 0 } } }, };
+gate_desc idt_table[NR_VECTORS] __page_aligned_data = { { { { 0, 0 } } }, };
 #endif
 
 DECLARE_BITMAP(used_vectors, NR_VECTORS);

commit dca2d6ac09d9ef59ff46820d4f0c94b08a671202
Merge: d6a65dffb30d 18240904960a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Sep 15 12:18:15 2009 +0200

    Merge branch 'linus' into tracing/hw-breakpoints
    
    Conflicts:
            arch/x86/kernel/process_64.c
    
    Semantic conflict fixed in:
            arch/x86/kvm/x86.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 625037cc405eabbfd2a39e9297e583a94886225f
Merge: 8fafa0a789fa 17950c5b243f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 14 07:58:08 2009 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86-64: move clts into batch cpu state updates when preloading fpu
      x86-64: move unlazy_fpu() into lazy cpu state part of context switch
      x86-32: make sure clts is batched during context switch
      x86: split out core __math_state_restore

commit 15b0404272e1513940223cf9eefadfd22804a060
Merge: 9b74aec028e7 d535e4319a2e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 14 07:56:43 2009 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Make memtype_seq_ops const
      x86: uv: Clean up uv_ptc_init(), use proc_create()
      x86: Use printk_once()
      x86/cpu: Clean up various files a bit
      x86: Remove duplicated #include
      x86, ipi: Clean up safe_smp_processor_id() by using the cpu_has_apic() macro helper
      x86: Clean up idt_descr and idt_tableby using NR_VECTORS instead of hardcoded number
      x86: Further clean up of mtrr/generic.c
      x86: Clean up mtrr/main.c
      x86: Clean up mtrr/state.c
      x86: Clean up mtrr/mtrr.h
      x86: Clean up mtrr/if.c
      x86: Clean up mtrr/generic.c
      x86: Clean up mtrr/cyrix.c
      x86: Clean up mtrr/cleanup.c
      x86: Clean up mtrr/centaur.c
      x86: Clean up mtrr/amd.c:
      x86: ds.c fix invalid assignment

commit a1922ed661ab2c1637d0b10cde933bd9cd33d965
Merge: 75e33751ca8b d28daf923ac5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 7 08:19:51 2009 +0200

    Merge branch 'tracing/core' into tracing/hw-breakpoints
    
    Conflicts:
            arch/Kconfig
            kernel/trace/trace.h
    
    Merge reason: resolve the conflicts, plus adopt to the new
                  ring-buffer APIs.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 428cf9025b15573e16e658032f2b963283e34ae0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 10:35:46 2009 +0200

    x86: Move traps_init to x86_init_ops
    
    Replace the quirks by a simple x86_init_ops function.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7e4b1f5dec8e..ed96ed53f69e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -59,6 +59,7 @@
 #include <asm/mach_traps.h>
 
 #ifdef CONFIG_X86_64
+#include <asm/x86_init.h>
 #include <asm/pgalloc.h>
 #include <asm/proto.h>
 #else
@@ -980,7 +981,5 @@ void __init trap_init(void)
 	 */
 	cpu_init();
 
-#ifdef CONFIG_X86_32
-	x86_quirk_trap_init();
-#endif
+	x86_init.irqs.trap_init();
 }

commit fde0312d01b60a3fd5dc56e69a9613defbbc7097
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sun Jul 19 00:09:56 2009 +0900

    x86: Remove unused patch_espfix_desc()
    
    patch_espfix_desc() is not used after commit
    dc4c2a0aed3b09f6e255bd5c3faa50fe6e0b2ded
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    LKML-Reference: <20090718150955.GB11294@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 5204332f475d..236794110207 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -786,27 +786,6 @@ do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 #endif
 }
 
-#ifdef CONFIG_X86_32
-unsigned long patch_espfix_desc(unsigned long uesp, unsigned long kesp)
-{
-	struct desc_struct *gdt = get_cpu_gdt_table(smp_processor_id());
-	unsigned long base = (kesp - uesp) & -THREAD_SIZE;
-	unsigned long new_kesp = kesp - base;
-	unsigned long lim_pages = (new_kesp | (THREAD_SIZE - 1)) >> PAGE_SHIFT;
-	__u64 desc = *(__u64 *)&gdt[GDT_ENTRY_ESPFIX_SS];
-
-	/* Set up base for espfix segment */
-	desc &= 0x00f0ff0000000000ULL;
-	desc |=	((((__u64)base) << 16) & 0x000000ffffff0000ULL) |
-		((((__u64)base) << 32) & 0xff00000000000000ULL) |
-		((((__u64)lim_pages) << 32) & 0x000f000000000000ULL) |
-		(lim_pages & 0xffff);
-	*(__u64 *)&gdt[GDT_ENTRY_ESPFIX_SS] = desc;
-
-	return new_kesp;
-}
-#endif
-
 asmlinkage void __attribute__((weak)) smp_thermal_interrupt(void)
 {
 }

commit 9ff80942992cd5abd0779c815f310f65b7b83860
Author: Cyrill Gorcunov <gorcunov@openvz.org>
Date:   Wed Jul 8 22:03:53 2009 +0400

    x86: Clean up idt_descr and idt_tableby using NR_VECTORS instead of hardcoded number
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <20090708180353.GH5301@lenovo>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 5204332f475d..7e4b1f5dec8e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -76,7 +76,7 @@ char ignore_fpu_irq;
  * F0 0F bug workaround.. We have a special link segment
  * for this.
  */
-gate_desc idt_table[256]
+gate_desc idt_table[NR_VECTORS]
 	__attribute__((__section__(".data.idt"))) = { { { { 0, 0 } } }, };
 #endif
 

commit 5211a242d0cbdded372aee59da18f80552b0a80a
Author: Kurt Garloff <garloff@suse.de>
Date:   Wed Jun 24 14:32:11 2009 -0700

    x86: Add sysctl to allow panic on IOCK NMI error
    
    This patch introduces a new sysctl:
    
        /proc/sys/kernel/panic_on_io_nmi
    
    which defaults to 0 (off).
    
    When enabled, the kernel panics when the kernel receives an NMI
    caused by an IO error.
    
    The IO error triggered NMI indicates a serious system
    condition, which could result in IO data corruption. Rather
    than contiuing, panicing and dumping might be a better choice,
    so one can figure out what's causing the IO error.
    
    This could be especially important to companies running IO
    intensive applications where corruption must be avoided, e.g. a
    bank's databases.
    
    [ SuSE has been shipping it for a while, it was done at the
      request of a large database vendor, for their users. ]
    
    Signed-off-by: Kurt Garloff <garloff@suse.de>
    Signed-off-by: Roberto Angelino <robertangelino@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    LKML-Reference: <20090624213211.GA11291@kroah.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a0f48f5671c0..5204332f475d 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -346,6 +346,9 @@ io_check_error(unsigned char reason, struct pt_regs *regs)
 	printk(KERN_EMERG "NMI: IOCK error (debug interrupt?)\n");
 	show_registers(regs);
 
+	if (panic_on_io_nmi)
+		panic("NMI IOCK error: Not continuing");
+
 	/* Re-enable the IOCK line, wait for a few seconds */
 	reason = (reason & 0xf) | 8;
 	outb(reason, 0x61);

commit c4c5ab3089c8a794eb0bdaa9794d0f055dd82412
Merge: 7fd5b632db00 1d99100120ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 20 10:49:48 2009 -0700

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (45 commits)
      x86, mce: fix error path in mce_create_device()
      x86: use zalloc_cpumask_var for mce_dev_initialized
      x86: fix duplicated sysfs attribute
      x86: de-assembler-ize asm/desc.h
      i386: fix/simplify espfix stack switching, move it into assembly
      i386: fix return to 16-bit stack from NMI handler
      x86, ioapic: Don't call disconnect_bsp_APIC if no APIC present
      x86: Remove duplicated #include's
      x86: msr.h linux/types.h is only required for __KERNEL__
      x86: nmi: Add Intel processor 0x6f4 to NMI perfctr1 workaround
      x86, mce: mce_intel.c needs <asm/apic.h>
      x86: apic/io_apic.c: dmar_msi_type should be static
      x86, io_apic.c: Work around compiler warning
      x86: mce: Don't touch THERMAL_APIC_VECTOR if no active APIC present
      x86: mce: Handle banks == 0 case in K7 quirk
      x86, boot: use .code16gcc instead of .code16
      x86: correct the conversion of EFI memory types
      x86: cap iomem_resource to addressable physical memory
      x86, mce: rename _64.c files which are no longer 64-bit-specific
      x86, mce: mce.h cleanup
      ...
    
    Manually fix up trivial conflict in arch/x86/mm/fault.c

commit e6e9cac8c3417b43498b243c1f8f11780e157168
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Apr 24 00:40:59 2009 -0700

    x86: split out core __math_state_restore
    
    Split the core fpu state restoration out into __math_state_restore, which
    assumes that cr0.TS is clear and that the fpu context has been initialized.
    
    This will be used during context switch.  There are two reasons this is
    desireable:
    
    - There's a small clarification.  When __switch_to() calls math_state_restore,
      it relies on the fact that tsk_used_math() returns true, and so will
      never do a blocking init_fpu().  __math_state_restore() does not have
      (or need) that logic, so the question never arises.
    
    - It allows the clts() to be moved earler in __switch_to() so it can be performed
      while cpu context updates are batched (will be done in a later patch).
    
    [ Impact: refactor code to make reuse cleaner; no functional change ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 5f935f0d5861..71b91669ad19 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -813,6 +813,28 @@ asmlinkage void __attribute__((weak)) smp_threshold_interrupt(void)
 {
 }
 
+/*
+ * __math_state_restore assumes that cr0.TS is already clear and the
+ * fpu state is all ready for use.  Used during context switch.
+ */
+void __math_state_restore(void)
+{
+	struct thread_info *thread = current_thread_info();
+	struct task_struct *tsk = thread->task;
+
+	/*
+	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
+	 */
+	if (unlikely(restore_fpu_checking(tsk))) {
+		stts();
+		force_sig(SIGSEGV, tsk);
+		return;
+	}
+
+	thread->status |= TS_USEDFPU;	/* So we fnsave on switch_to() */
+	tsk->fpu_counter++;
+}
+
 /*
  * 'math_state_restore()' saves the current math information in the
  * old math state array, and gets the new ones from the current task
@@ -844,17 +866,8 @@ asmlinkage void math_state_restore(void)
 	}
 
 	clts();				/* Allow maths ops (or we recurse) */
-	/*
-	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
-	 */
-	if (unlikely(restore_fpu_checking(tsk))) {
-		stts();
-		force_sig(SIGSEGV, tsk);
-		return;
-	}
 
-	thread->status |= TS_USEDFPU;	/* So we fnsave on switch_to() */
-	tsk->fpu_counter++;
+	__math_state_restore();
 }
 EXPORT_SYMBOL_GPL(math_state_restore);
 

commit eadb8a091b27a840de7450f84ecff5ef13476424
Merge: 73874005cd88 65795efbd380
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 17 12:52:15 2009 +0200

    Merge branch 'linus' into tracing/hw-breakpoints
    
    Conflicts:
            arch/x86/Kconfig
            arch/x86/kernel/traps.c
            arch/x86/power/cpu.c
            arch/x86/power/cpu_32.c
            kernel/Makefile
    
    Semantic conflict:
            arch/x86/kernel/hw_breakpoint.c
    
    Merge reason: Resolve the conflicts, move from put_cpu_no_sched() to
                  put_cpu() in arch/x86/kernel/hw_breakpoint.c.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 9e55e44e39798541ba39d57f4b569deb555ae1ce
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Mon Jun 15 17:22:15 2009 +0900

    x86, mce: unify mce.h
    
    There are 2 headers:
            arch/x86/include/asm/mce.h
            arch/x86/kernel/cpu/mcheck/mce.h
    and in the latter small header:
            #include <asm/mce.h>
    
    This patch move all contents in the latter header into the former,
    and fix all files using the latter to include the former instead.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 1e1e27b7d438..71f9c74814d8 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -53,6 +53,7 @@
 #include <asm/traps.h>
 #include <asm/desc.h>
 #include <asm/i387.h>
+#include <asm/mce.h>
 
 #include <asm/mach_traps.h>
 
@@ -64,8 +65,6 @@
 #include <asm/setup.h>
 #include <asm/traps.h>
 
-#include "cpu/mcheck/mce.h"
-
 asmlinkage int system_call(void);
 
 /* Do we ignore FPU interrupts ? */

commit 722f2a6c87f34ee0fd0130a8cf45f81e0705594a
Merge: 7a0aeb14e18a 45e3e1935e28
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Mon Jun 15 15:50:49 2009 +0200

    Merge commit 'linus/master' into HEAD
    
    Conflicts:
            MAINTAINERS
    
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>

commit f85612967c93b67b10dd240e3e8bf8a0eee9def7
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Fri Apr 4 00:53:23 2008 +0200

    x86: add hooks for kmemcheck
    
    The hooks that we modify are:
    - Page fault handler (to handle kmemcheck faults)
    - Debug exception handler (to hide pages after single-stepping
      the instruction that caused the page fault)
    
    Also redefine memset() to use the optimized version if kmemcheck is
    enabled.
    
    (Thanks to Pekka Enberg for minimizing the impact on the page fault
    handler.)
    
    As kmemcheck doesn't handle MMX/SSE instructions (yet), we also disable
    the optimized xor code, and rely instead on the generic C implementation
    in order to avoid false-positive warnings.
    
    Signed-off-by: Vegard Nossum <vegardno@ifi.uio.no>
    
    [whitespace fixlet]
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    
    [rebased for mainline inclusion]
    Signed-off-by: Vegard Nossum <vegardno@ifi.uio.no>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 07d60c870ce2..e7a28e6aa4bc 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -45,6 +45,7 @@
 #include <linux/edac.h>
 #endif
 
+#include <asm/kmemcheck.h>
 #include <asm/stacktrace.h>
 #include <asm/processor.h>
 #include <asm/debugreg.h>
@@ -534,6 +535,10 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 
 	get_debugreg(condition, 6);
 
+	/* Catch kmemcheck conditions first of all! */
+	if (condition & DR_STEP && kmemcheck_trap(regs))
+		return;
+
 	/*
 	 * The processor cleared BTF, so don't mark that we need it set.
 	 */

commit 0d5959723e1db3fd7323c198a50c16cecf96c7a9
Merge: 62fdac5913f7 512626a04e72
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jun 11 23:31:52 2009 +0200

    Merge branch 'linus' into x86/mce3
    
    Conflicts:
            arch/x86/kernel/cpu/mcheck/mce_64.c
            arch/x86/kernel/irq.c
    
    Merge reason: Resolve the conflicts above.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 940010c5a314a7bd9b498593bc6ba1718ac5aec5
Merge: 8dc8e5e8bc0c 991ec02cdca3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jun 11 17:55:42 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/irqinit.c
            arch/x86/kernel/irqinit_64.c
            arch/x86/kernel/traps.c
            arch/x86/mm/fault.c
            include/linux/sched.h
            kernel/exit.c

commit 5301e0de34a29625c06b8eef115c1a9b4c1f72b1
Merge: c44e3ed539e4 4ecf458492c2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 15:55:04 2009 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86_64: fix incorrect comments
      x86: unify restore_fpu_checking
      x86_32: introduce restore_fpu_checking()

commit 62edab9056a6cf0c9207339c8892c923a5217e45
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Mon Jun 1 23:47:06 2009 +0530

    hw-breakpoints: reset bits in dr6 after the corresponding exception is handled
    
    This patch resets the bit in dr6 after the corresponding exception is
    handled in code, so that we keep a clean track of the current virtual debug
    status register.
    
    [ Impact: keep track of breakpoints triggering completion ]
    
    Signed-off-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index de9913247dd0..124a4d5a95b2 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -545,8 +545,8 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	/* Store the virtualized DR6 value */
 	tsk->thread.debugreg6 = dr6;
 
-	if (notify_die(DIE_DEBUG, "debug", regs, dr6, error_code,
-						SIGTRAP) == NOTIFY_STOP)
+	if (notify_die(DIE_DEBUG, "debug", regs, PTR_ERR(&dr6), error_code,
+							SIGTRAP) == NOTIFY_STOP)
 		return;
 
 	/* It's safe to allow irq's after DR6 has been saved */

commit 08d68323d1f0c34452e614263b212ca556dae47f
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Mon Jun 1 23:44:08 2009 +0530

    hw-breakpoints: modifying generic debug exception to use thread-specific debug registers
    
    This patch modifies the breakpoint exception handler code to use the new
    abstract debug register names.
    
    [ fweisbec@gmail.com: fix conflict against kmemcheck ]
    
    [ Impact: refactor and cleanup x86 debug exception handler ]
    
    Original-patch-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Reviewed-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a1d288327ff0..de9913247dd0 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -529,73 +529,52 @@ asmlinkage __kprobes struct pt_regs *sync_regs(struct pt_regs *eregs)
 dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk = current;
-	unsigned long condition;
+	unsigned long dr6;
 	int si_code;
 
-	get_debugreg(condition, 6);
+	get_debugreg(dr6, 6);
 
+	/* DR6 may or may not be cleared by the CPU */
+	set_debugreg(0, 6);
 	/*
 	 * The processor cleared BTF, so don't mark that we need it set.
 	 */
 	clear_tsk_thread_flag(tsk, TIF_DEBUGCTLMSR);
 	tsk->thread.debugctlmsr = 0;
 
-	if (notify_die(DIE_DEBUG, "debug", regs, condition, error_code,
+	/* Store the virtualized DR6 value */
+	tsk->thread.debugreg6 = dr6;
+
+	if (notify_die(DIE_DEBUG, "debug", regs, dr6, error_code,
 						SIGTRAP) == NOTIFY_STOP)
 		return;
 
 	/* It's safe to allow irq's after DR6 has been saved */
 	preempt_conditional_sti(regs);
 
-	/* Mask out spurious debug traps due to lazy DR7 setting */
-	if (condition & (DR_TRAP0|DR_TRAP1|DR_TRAP2|DR_TRAP3)) {
-		if (!tsk->thread.debugreg7)
-			goto clear_dr7;
+	if (regs->flags & X86_VM_MASK) {
+		handle_vm86_trap((struct kernel_vm86_regs *) regs,
+				error_code, 1);
+		return;
 	}
 
-#ifdef CONFIG_X86_32
-	if (regs->flags & X86_VM_MASK)
-		goto debug_vm86;
-#endif
-
-	/* Save debug status register where ptrace can see it */
-	tsk->thread.debugreg6 = condition;
-
 	/*
-	 * Single-stepping through TF: make sure we ignore any events in
-	 * kernel space (but re-enable TF when returning to user mode).
+	 * Single-stepping through system calls: ignore any exceptions in
+	 * kernel space, but re-enable TF when returning to user mode.
+	 *
+	 * We already checked v86 mode above, so we can check for kernel mode
+	 * by just checking the CPL of CS.
 	 */
-	if (condition & DR_STEP) {
-		if (!user_mode(regs))
-			goto clear_TF_reenable;
+	if ((dr6 & DR_STEP) && !user_mode(regs)) {
+		tsk->thread.debugreg6 &= ~DR_STEP;
+		set_tsk_thread_flag(tsk, TIF_SINGLESTEP);
+		regs->flags &= ~X86_EFLAGS_TF;
 	}
-
-	si_code = get_si_code(condition);
-	/* Ok, finally something we can handle */
-	send_sigtrap(tsk, regs, error_code, si_code);
-
-	/*
-	 * Disable additional traps. They'll be re-enabled when
-	 * the signal is delivered.
-	 */
-clear_dr7:
-	set_debugreg(0, 7);
+	si_code = get_si_code(tsk->thread.debugreg6);
+	if (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS))
+		send_sigtrap(tsk, regs, error_code, si_code);
 	preempt_conditional_cli(regs);
-	return;
 
-#ifdef CONFIG_X86_32
-debug_vm86:
-	/* reenable preemption: handle_vm86_trap() might sleep */
-	dec_preempt_count();
-	handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code, 1);
-	conditional_cli(regs);
-	return;
-#endif
-
-clear_TF_reenable:
-	set_tsk_thread_flag(tsk, TIF_SINGLESTEP);
-	regs->flags &= ~X86_EFLAGS_TF;
-	preempt_conditional_cli(regs);
 	return;
 }
 

commit 48b1fddbb100a64f3983ca9768b8ea629a09aa20
Merge: 38736072d454 ee4c24a5c9b5
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Jun 1 15:13:02 2009 -0700

    Merge branch 'irq/numa' into x86/mce3
    
    Merge reason: arch/x86/kernel/irqinit_{32,64}.c unified in irq/numa
    and modified in x86/mce3; this merge resolves the conflict.
    
    Conflicts:
            arch/x86/kernel/irqinit.c
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

commit 7856f6cce4a8cda8c1f94b99605c07d16b8d8dec
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Apr 28 23:32:56 2009 +0200

    x86, mce: enable MCE_INTEL for 32bit new MCE
    
    Enable the 64bit MCE_INTEL code (CMCI, thermal interrupts) for 32bit NEW_MCE.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ad771f15bddd..0d358c884b35 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -804,7 +804,7 @@ asmlinkage void __attribute__((weak)) smp_thermal_interrupt(void)
 {
 }
 
-asmlinkage void __attribute__((weak)) mce_threshold_interrupt(void)
+asmlinkage void __attribute__((weak)) smp_threshold_interrupt(void)
 {
 }
 

commit 4efc0670baf4b14bc95502e54a83ccf639146125
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Apr 28 19:07:31 2009 +0200

    x86, mce: use 64bit machine check code on 32bit
    
    The 64bit machine check code is in many ways much better than
    the 32bit machine check code: it is more specification compliant,
    is cleaner, only has a single code base versus one per CPU,
    has better infrastructure for recovery, has a cleaner way to communicate
    with user space etc. etc.
    
    Use the 64bit code for 32bit too.
    
    This is the second attempt to do this. There was one a couple of years
    ago to unify this code for 32bit and 64bit.  Back then this ran into some
    trouble with K7s and was reverted.
    
    I believe this time the K7 problems (and some others) are addressed.
    I went over the old handlers and was very careful to retain
    all quirks.
    
    But of course this needs a lot of testing on old systems. On newer
    64bit capable systems I don't expect much problems because they have been
    already tested with the 64bit kernel.
    
    I made this a CONFIG for now that still allows to select the old
    machine check code. This is mostly to make testing easier,
    if someone runs into a problem we can ask them to try
    with the CONFIG switched.
    
    The new code is default y for more coverage.
    
    Once there is confidence the 64bit code works well on older hardware
    too the CONFIG_X86_OLD_MCE and the associated code can be easily
    removed.
    
    This causes a behaviour change for 32bit installations. They now
    have to install the mcelog package to be able to log
    corrected machine checks.
    
    The 64bit machine check code only handles CPUs which support the
    standard Intel machine check architecture described in the IA32 SDM.
    The 32bit code has special support for some older CPUs which
    have non standard machine check architectures, in particular
    WinChip C3 and Intel P5.  I made those a separate CONFIG option
    and kept them for now. The WinChip variant could be probably
    removed without too much pain, it doesn't really do anything
    interesting. P5 is also disabled by default (like it
    was before) because many motherboards have it miswired, but
    according to Alan Cox a few embedded setups use that one.
    
    Forward ported/heavily changed version of old patch, original patch
    included review/fixes from Thomas Gleixner, Bert Wesarg.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a1d288327ff0..ad771f15bddd 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -798,7 +798,8 @@ unsigned long patch_espfix_desc(unsigned long uesp, unsigned long kesp)
 
 	return new_kesp;
 }
-#else
+#endif
+
 asmlinkage void __attribute__((weak)) smp_thermal_interrupt(void)
 {
 }
@@ -806,7 +807,6 @@ asmlinkage void __attribute__((weak)) smp_thermal_interrupt(void)
 asmlinkage void __attribute__((weak)) mce_threshold_interrupt(void)
 {
 }
-#endif
 
 /*
  * 'math_state_restore()' saves the current math information in the

commit ac3048dfd4740becf8d768844cf47ebee363c9f8
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Thu Apr 9 11:52:29 2009 +0300

    x86: define IA32_SYSCALL_VECTOR on 32-bit to reduce ifdefs
    
    Impact: cleanup
    
    We can remove some #ifdefs if we define IA32_SYSCALL_VECTOR on 32-bit.
    
    Reviewed-by Cyrill Gorcunov <gorcunov@openvz.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a1d288327ff0..2310700faca5 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -969,11 +969,8 @@ void __init trap_init(void)
 	for (i = 0; i < FIRST_EXTERNAL_VECTOR; i++)
 		set_bit(i, used_vectors);
 
-#ifdef CONFIG_X86_64
 	set_bit(IA32_SYSCALL_VECTOR, used_vectors);
-#else
-	set_bit(SYSCALL_VECTOR, used_vectors);
-#endif
+
 	/*
 	 * Should be a barrier for any external CPU state:
 	 */

commit fcb2ac5bdfa3a7a04fb9749b916f64400f4c35a8
Author: Jiri Slaby <jirislaby@gmail.com>
Date:   Wed Apr 8 13:31:58 2009 +0200

    x86_32: introduce restore_fpu_checking()
    
    Impact: cleanup, prepare FPU code unificaton
    
    Like on x86_64, return an error from restore_fpu and kill the task
    if it fails.
    
    Also rename restore_fpu to restore_fpu_checking which allows ifdefs
    to be removed in math_state_restore().
    
    Signed-off-by: Jiri Slaby <jirislaby@gmail.com>
    LKML-Reference: <1239190320-23952-1-git-send-email-jirislaby@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index a1d288327ff0..d696145855b5 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -839,9 +839,6 @@ asmlinkage void math_state_restore(void)
 	}
 
 	clts();				/* Allow maths ops (or we recurse) */
-#ifdef CONFIG_X86_32
-	restore_fpu(tsk);
-#else
 	/*
 	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
 	 */
@@ -850,7 +847,7 @@ asmlinkage void math_state_restore(void)
 		force_sig(SIGSEGV, tsk);
 		return;
 	}
-#endif
+
 	thread->status |= TS_USEDFPU;	/* So we fnsave on switch_to() */
 	tsk->fpu_counter++;
 }

commit f541ae326fa120fa5c57433e4d9a133df212ce41
Merge: e255357764f9 0221c81b1b8e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 6 09:02:57 2009 +0200

    Merge branch 'linus' into perfcounters/core-v2
    
    Merge reason: we have gathered quite a few conflicts, need to merge upstream
    
    Conflicts:
            arch/powerpc/kernel/Makefile
            arch/x86/ia32/ia32entry.S
            arch/x86/include/asm/hardirq.h
            arch/x86/include/asm/unistd_32.h
            arch/x86/include/asm/unistd_64.h
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/irq.c
            arch/x86/kernel/syscall_table_32.S
            arch/x86/mm/iomap_32.c
            include/linux/sched.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit db949bba3c7cf2e664ac12e237c6d4c914f0c69d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Feb 27 13:25:21 2009 -0800

    x86-32: use non-lazy io bitmap context switching
    
    Impact: remove 32-bit optimization to prepare unification
    
    x86-32 and -64 differ in the way they context-switch tasks
    with io permission bitmaps.  x86-64 simply copies the next
    tasks io bitmap into place (if any) on context switch.  x86-32
    invalidates the bitmap on context switch, so that the next
    IO instruction will fault; at that point it installs the
    appropriate IO bitmap.
    
    This makes context switching IO-bitmap-using tasks a bit more
    less expensive, at the cost of making the next IO instruction
    slower due to the extra fault.  This tradeoff only makes sense
    if IO-bitmap-using processes are relatively common, but they
    don't actually use IO instructions very often.
    
    However, in a typical desktop system, the only process likely
    to be using IO bitmaps is the X server, and nothing at all on
    a server.  Therefore the lazy context switch doesn't really win
    all that much, and its just a gratuitious difference from
    64-bit code.
    
    This patch removes the lazy context switch, with a view to
    unifying this code in a later change.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index c05430ac1b44..a1d288327ff0 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -118,47 +118,6 @@ die_if_kernel(const char *str, struct pt_regs *regs, long err)
 	if (!user_mode_vm(regs))
 		die(str, regs, err);
 }
-
-/*
- * Perform the lazy TSS's I/O bitmap copy. If the TSS has an
- * invalid offset set (the LAZY one) and the faulting thread has
- * a valid I/O bitmap pointer, we copy the I/O bitmap in the TSS,
- * we set the offset field correctly and return 1.
- */
-static int lazy_iobitmap_copy(void)
-{
-	struct thread_struct *thread;
-	struct tss_struct *tss;
-	int cpu;
-
-	cpu = get_cpu();
-	tss = &per_cpu(init_tss, cpu);
-	thread = &current->thread;
-
-	if (tss->x86_tss.io_bitmap_base == INVALID_IO_BITMAP_OFFSET_LAZY &&
-	    thread->io_bitmap_ptr) {
-		memcpy(tss->io_bitmap, thread->io_bitmap_ptr,
-		       thread->io_bitmap_max);
-		/*
-		 * If the previously set map was extending to higher ports
-		 * than the current one, pad extra space with 0xff (no access).
-		 */
-		if (thread->io_bitmap_max < tss->io_bitmap_max) {
-			memset((char *) tss->io_bitmap +
-				thread->io_bitmap_max, 0xff,
-				tss->io_bitmap_max - thread->io_bitmap_max);
-		}
-		tss->io_bitmap_max = thread->io_bitmap_max;
-		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
-		tss->io_bitmap_owner = thread;
-		put_cpu();
-
-		return 1;
-	}
-	put_cpu();
-
-	return 0;
-}
 #endif
 
 static void __kprobes
@@ -309,11 +268,6 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	conditional_sti(regs);
 
 #ifdef CONFIG_X86_32
-	if (lazy_iobitmap_copy()) {
-		/* restart the faulting instruction */
-		return;
-	}
-
 	if (regs->flags & X86_VM_MASK)
 		goto gp_in_vm86;
 #endif

commit 8e818179eb9e8f9e44d8410dd2a25077d026a08e
Merge: 742bd95ba96e ecc25fbd6b9e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 26 13:02:23 2009 +0100

    Merge branch 'x86/core' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/apic/apic.c
            arch/x86/kernel/irqinit_32.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a852cbfaaf8122827602027b1614971cfd832304
Merge: 0d3a9cf5ab04 ecda06289f82 9f331119a4f9 bda3a89745d7 b319eed0aa0a a967bb3fbe64 954a8b8162ec
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 24 21:50:43 2009 +0100

    Merge branches 'x86/acpi', 'x86/apic', 'x86/asm', 'x86/cleanups', 'x86/mm', 'x86/signal' and 'x86/urgent'; commit 'v2.6.29-rc6' into x86/core

commit 8e6dafd6c741cd4679b4de3c5d9698851e4fa59c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 23 00:34:39 2009 +0100

    x86: refactor x86_quirks support
    
    Impact: cleanup
    
    Make x86_quirks support more transparent. The highlevel
    methods are now named:
    
      extern void x86_quirk_pre_intr_init(void);
      extern void x86_quirk_intr_init(void);
    
      extern void x86_quirk_trap_init(void);
    
      extern void x86_quirk_pre_time_init(void);
      extern void x86_quirk_time_init(void);
    
    This makes it clear that if some platform extension has to
    do something here that it is considered ... weird, and is
    discouraged.
    
    Also remove arch_hooks.h and move it into setup.h (and other
    header files where appropriate).
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index acb8c0585ab9..c8c0a7e530be 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -61,7 +61,7 @@
 #include <asm/proto.h>
 #else
 #include <asm/processor-flags.h>
-#include <asm/arch_hooks.h>
+#include <asm/setup.h>
 #include <asm/traps.h>
 
 #include "cpu/mcheck/mce.h"
@@ -1026,6 +1026,6 @@ void __init trap_init(void)
 	cpu_init();
 
 #ifdef CONFIG_X86_32
-	trap_init_hook();
+	x86_quirk_trap_init();
 #endif
 }

commit fc6fcdfbb8f1b2553255170f221cd57a4108591f
Author: Hannes Eder <hannes@hanneseder.net>
Date:   Sun Feb 22 01:00:57 2009 +0100

    x86: kexec/i386: fix sparse warnings: Using plain integer as NULL pointer
    
    Fix these sparse warnings:
    
      arch/x86/kernel/machine_kexec_32.c:124:22: warning: Using plain integer as NULL pointer
      arch/x86/kernel/traps.c:950:24: warning: Using plain integer as NULL pointer
    
    Signed-off-by: Hannes Eder <hannes@hanneseder.net>
    Cc: trivial@kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index acb8c0585ab9..2df927b5003f 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -942,7 +942,7 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 	info.si_signo = SIGILL;
 	info.si_errno = 0;
 	info.si_code = ILL_BADSTK;
-	info.si_addr = 0;
+	info.si_addr = NULL;
 	if (notify_die(DIE_TRAP, "iret exception",
 			regs, error_code, 32, SIGILL) == NOTIFY_STOP)
 		return;

commit 0b6de0092244c98b5ba1abda34c92470a20e0d0c
Merge: 37a25424252b f62bae5009c1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 17:20:11 2009 +0100

    Merge branch 'x86/apic' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/cpu/perfctr-watchdog.c

commit 494df596f9c315e20523894caa2a2938db3e5d8d
Merge: 970ec1a8213c 98c061b6cf2e f6db44df5bd3 22796b157254 694aa960608d be716615fe59 c99608637eac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 12:07:00 2009 +0100

    Merge branches 'x86/acpi', 'x86/apic', 'x86/cpudetect', 'x86/headers', 'x86/paravirt', 'x86/urgent' and 'x86/xen'; commit 'v2.6.29-rc5' into x86/core

commit be716615fe596ee117292dc615e95f707fb67fd1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jan 13 23:36:34 2009 +0100

    x86, vm86: fix preemption bug
    
    Commit 3d2a71a596bd9c761c8487a2178e95f8a61da083 ("x86, traps: converge
    do_debug handlers") changed the preemption disable logic of do_debug()
    so vm86_handle_trap() is called with preemption disabled resulting in:
    
     BUG: sleeping function called from invalid context at include/linux/kernel.h:155
     in_atomic(): 1, irqs_disabled(): 0, pid: 3005, name: dosemu.bin
     Pid: 3005, comm: dosemu.bin Tainted: G        W  2.6.29-rc1 #51
     Call Trace:
      [<c050d669>] copy_to_user+0x33/0x108
      [<c04181f4>] save_v86_state+0x65/0x149
      [<c0418531>] handle_vm86_trap+0x20/0x8f
      [<c064e345>] do_debug+0x15b/0x1a4
      [<c064df1f>] debug_stack_correct+0x27/0x2c
      [<c040365b>] sysenter_do_call+0x12/0x2f
     BUG: scheduling while atomic: dosemu.bin/3005/0x10000001
    
    Restore the original calling convention and reenable preemption before
    calling handle_vm86_trap().
    
    Reported-by: Michal Suchanek <hramrach@centrum.cz>
    Cc: stable@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7932338d7cb3..a9e7548e1790 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -99,6 +99,12 @@ static inline void preempt_conditional_sti(struct pt_regs *regs)
 		local_irq_enable();
 }
 
+static inline void conditional_cli(struct pt_regs *regs)
+{
+	if (regs->flags & X86_EFLAGS_IF)
+		local_irq_disable();
+}
+
 static inline void preempt_conditional_cli(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
@@ -626,8 +632,10 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 
 #ifdef CONFIG_X86_32
 debug_vm86:
+	/* reenable preemption: handle_vm86_trap() might sleep */
+	dec_preempt_count();
 	handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code, 1);
-	preempt_conditional_cli(regs);
+	conditional_cli(regs);
 	return;
 #endif
 

commit b1864e9a1afef41709886072c6e6248def0386f4
Merge: e9c4ffb11f0b 7032e8696726
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:49:38 2009 +0100

    Merge branch 'x86/core' into perfcounters/core
    
    Conflicts:
            arch/x86/Kconfig
            arch/x86/kernel/apic.c
            arch/x86/kernel/setup_percpu.c

commit ab639f3593f0b5e4439d549831442c18c3baf989
Merge: f8a6b2b9cee2 58105ef18571
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:45:09 2009 +0100

    Merge branch 'core/percpu' into x86/core

commit f8a6b2b9cee298a9663cbe38ce1eb5240987cb62
Merge: ba1511bf7fbd 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:44:22 2009 +0100

    Merge branch 'linus' into x86/apic
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c
            arch/x86/mm/fault.c

commit e9c4ffb11f0b19005b5b9dc8481687a3637e5887
Merge: 4bcf349a0f90 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:34:07 2009 +0100

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c

commit aa78bcfa01dec3cdbde3cda098ce32abbd9c3bf6
Author: Brian Gerst <brgerst@gmail.com>
Date:   Tue Feb 10 09:51:45 2009 -0500

    x86: use pt_regs pointer in do_device_not_available()
    
    The generic exception handler (error_code) passes in the pt_regs
    pointer and the error code (unused in this case).  The commit
    "x86: fix math_emu register frame access" changed this to pass by
    value, which doesn't work correctly with stack protector enabled.
    Change it back to use the pt_regs pointer.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 3b7b2e190201..71a8f871331e 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -905,19 +905,20 @@ void math_emulate(struct math_emu_info *info)
 }
 #endif /* CONFIG_MATH_EMULATION */
 
-dotraplinkage void __kprobes do_device_not_available(struct pt_regs regs)
+dotraplinkage void __kprobes
+do_device_not_available(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_X86_32
 	if (read_cr0() & X86_CR0_EM) {
 		struct math_emu_info info = { };
 
-		conditional_sti(&regs);
+		conditional_sti(regs);
 
-		info.regs = &regs;
+		info.regs = regs;
 		math_emulate(&info);
 	} else {
 		math_state_restore(); /* interrupts still off */
-		conditional_sti(&regs);
+		conditional_sti(regs);
 	}
 #else
 	math_state_restore();

commit 92e2d508464b6293ad274fb606f766a458894142
Merge: 5d96218b4a5e d315760ffa26
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 10 00:41:02 2009 +0100

    Merge branch 'x86/urgent' into core/percpu
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c

commit d315760ffa261c15ff92699ac6f514112543d7ca
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 9 22:17:39 2009 +0900

    x86: fix math_emu register frame access
    
    do_device_not_available() is the handler for #NM and it declares that
    it takes a unsigned long and calls math_emu(), which takes a long
    argument and surprisingly expects the stack frame starting at the zero
    argument would match struct math_emu_info, which isn't true regardless
    of configuration in the current code.
    
    This patch makes do_device_not_available() take struct pt_regs like
    other exception handlers and initialize struct math_emu_info with
    pointer to it and pass pointer to the math_emu_info to math_emulate()
    like normal C functions do.  This way, unless gcc makes a copy of
    struct pt_regs in do_device_not_available(), the register frame is
    correctly accessed regardless of kernel configuration or compiler
    used.
    
    This doesn't fix all math_emu problems but it at least gets it
    somewhat working.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 98c2d055284b..7932338d7cb3 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -896,7 +896,7 @@ asmlinkage void math_state_restore(void)
 EXPORT_SYMBOL_GPL(math_state_restore);
 
 #ifndef CONFIG_MATH_EMULATION
-asmlinkage void math_emulate(long arg)
+void math_emulate(struct math_emu_info *info)
 {
 	printk(KERN_EMERG
 		"math-emulation not enabled and no coprocessor found.\n");
@@ -906,16 +906,19 @@ asmlinkage void math_emulate(long arg)
 }
 #endif /* CONFIG_MATH_EMULATION */
 
-dotraplinkage void __kprobes
-do_device_not_available(struct pt_regs *regs, long error)
+dotraplinkage void __kprobes do_device_not_available(struct pt_regs regs)
 {
 #ifdef CONFIG_X86_32
 	if (read_cr0() & X86_CR0_EM) {
-		conditional_sti(regs);
-		math_emulate(0);
+		struct math_emu_info info = { };
+
+		conditional_sti(&regs);
+
+		info.regs = &regs;
+		math_emulate(&info);
 	} else {
 		math_state_restore(); /* interrupts still off */
-		conditional_sti(regs);
+		conditional_sti(&regs);
 	}
 #else
 	math_state_restore();

commit 1164dd0099c0d79146a55319670f57ab7ad1d352
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 19:34:09 2009 +0100

    x86: move mach-default/*.h files to asm/
    
    We are getting rid of subarchitecture support - move the hook files
    to asm/. (These are now stale and should be replaced with more explicit
    runtime mechanisms - but the transition is simpler this way.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ed5aee5f3fcc..214bc327a0c3 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -54,7 +54,7 @@
 #include <asm/desc.h>
 #include <asm/i387.h>
 
-#include <mach_traps.h>
+#include <asm/mach_traps.h>
 
 #ifdef CONFIG_X86_64
 #include <asm/pgalloc.h>

commit bb3f0b59ad005d2d2ecbbe9bd048eab6d1ecbd31
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sun Jan 25 02:38:09 2009 -0800

    x86: make irqinit_32.c more like irqinit_64.c, v2
    
    Impact: cleanup
    
    1. add smp_intr_init and apic_intr_init for 32bit, the same as 64bit
    2. move the apic_intr_init() call before set gate with interrupt[i]
    3. for 64bit, if ia32_emulation is not used, will make per_cpu to use 0x80 vector.
    
    [ v2: should use !test_bit() instead of test_bit() with 32bit ]
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ed5aee5f3fcc..d36a502d87ab 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -979,8 +979,13 @@ void __init trap_init(void)
 #endif
 	set_intr_gate(19, &simd_coprocessor_error);
 
+	/* Reserve all the builtin and the syscall vector: */
+	for (i = 0; i < FIRST_EXTERNAL_VECTOR; i++)
+		set_bit(i, used_vectors);
+
 #ifdef CONFIG_IA32_EMULATION
 	set_system_intr_gate(IA32_SYSCALL_VECTOR, ia32_syscall);
+	set_bit(IA32_SYSCALL_VECTOR, used_vectors);
 #endif
 
 #ifdef CONFIG_X86_32
@@ -997,17 +1002,9 @@ void __init trap_init(void)
 	}
 
 	set_system_trap_gate(SYSCALL_VECTOR, &system_call);
-#endif
-
-	/* Reserve all the builtin and the syscall vector: */
-	for (i = 0; i < FIRST_EXTERNAL_VECTOR; i++)
-		set_bit(i, used_vectors);
-
-#ifdef CONFIG_X86_64
-	set_bit(IA32_SYSCALL_VECTOR, used_vectors);
-#else
 	set_bit(SYSCALL_VECTOR, used_vectors);
 #endif
+
 	/*
 	 * Should be a barrier for any external CPU state:
 	 */

commit 0d974d4592708f85044751817da4b7016e1b0602
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sun Jan 18 19:52:25 2009 -0500

    x86: remove pda.h
    
    Impact: cleanup
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 98c2d055284b..ed5aee5f3fcc 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -59,7 +59,6 @@
 #ifdef CONFIG_X86_64
 #include <asm/pgalloc.h>
 #include <asm/proto.h>
-#include <asm/pda.h>
 #else
 #include <asm/processor-flags.h>
 #include <asm/arch_hooks.h>

commit 3d14bdad40315b54470cb7812293d14c8af2bf7d
Merge: 4e9b1c184cad 51d7a1398d18
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 10 06:13:09 2009 -0800

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (36 commits)
      x86: fix section mismatch warnings in mcheck/mce_amd_64.c
      x86: offer frame pointers in all build modes
      x86: remove duplicated #include's
      x86: k8 numa register active regions later
      x86: update Alan Cox's email addresses
      x86: rename all fields of mpc_table mpc_X to X
      x86: rename all fields of mpc_oemtable oem_X to X
      x86: rename all fields of mpc_bus mpc_X to X
      x86: rename all fields of mpc_cpu mpc_X to X
      x86: rename all fields of mpc_intsrc mpc_X to X
      x86: rename all fields of mpc_lintsrc mpc_X to X
      x86: rename all fields of mpc_iopic mpc_X to X
      x86: irqinit_64.c init_ISA_irqs should be static
      Documentation/x86/boot.txt: payload length was changed to payload_length
      x86: setup_percpu.c fix style problems
      x86: irqinit_64.c fix style problems
      x86: irqinit_32.c fix style problems
      x86: i8259.c fix style problems
      x86: irq_32.c fix style problems
      x86: ioport.c fix style problems
      ...

commit f1883f86dea84fe47a71a39fc1afccc005915ed8
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Jan 6 14:40:45 2009 -0800

    Remove remaining unwinder code
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Gabor Gombas <gombasg@sztaki.hu>
    Cc: Jan Beulich <jbeulich@novell.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Ingo Molnar <mingo@elte.hu>,
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ce6650eb64e9..c9a666cdd3db 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -20,7 +20,6 @@
 #include <linux/module.h>
 #include <linux/ptrace.h>
 #include <linux/string.h>
-#include <linux/unwind.h>
 #include <linux/delay.h>
 #include <linux/errno.h>
 #include <linux/kexec.h>
@@ -51,7 +50,6 @@
 #include <asm/debugreg.h>
 #include <asm/atomic.h>
 #include <asm/system.h>
-#include <asm/unwind.h>
 #include <asm/traps.h>
 #include <asm/desc.h>
 #include <asm/i387.h>

commit 60d53c305805fcb612d3982dfef5c9e678e27f42
Author: Jaswinder Singh Rajput <jaswinder@infradead.org>
Date:   Sun Jan 4 16:20:40 2009 +0530

    x86: traps.c fix style problems
    
    Impact: cleanup
    
    Fix:
    
     WARNING: Use #include <linux/nmi.h> instead of <asm/nmi.h>
     WARNING: Use #include <linux/smp.h> instead of <asm/smp.h>
     WARNING: Use #include <linux/io.h> instead of <asm/io.h>
    
     total: 0 errors, 3 warnings
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ce6650eb64e9..25d5c3073582 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -65,9 +65,6 @@
 #else
 #include <asm/processor-flags.h>
 #include <asm/arch_hooks.h>
-#include <asm/nmi.h>
-#include <asm/smp.h>
-#include <asm/io.h>
 #include <asm/traps.h>
 
 #include "cpu/mcheck/mce.h"

commit 923a789b49c7269a0245d5af6afe486188d940df
Merge: 103ceffb9501 b840d79631c8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 2 22:41:36 2009 +0100

    Merge branch 'linus' into x86/cleanups
    
    Conflicts:
            arch/x86/kernel/reboot.c

commit b840d79631c882786925303c2b0f4fefc31845ed
Merge: 597b0d21626d c3d80000e3a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 2 11:44:09 2009 -0800

    Merge branch 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (66 commits)
      x86: export vector_used_by_percpu_irq
      x86: use logical apicid in x2apic_cluster's x2apic_cpu_mask_to_apicid_and()
      sched: nominate preferred wakeup cpu, fix
      x86: fix lguest used_vectors breakage, -v2
      x86: fix warning in arch/x86/kernel/io_apic.c
      sched: fix warning in kernel/sched.c
      sched: move test_sd_parent() to an SMP section of sched.h
      sched: add SD_BALANCE_NEWIDLE at MC and CPU level for sched_mc>0
      sched: activate active load balancing in new idle cpus
      sched: bias task wakeups to preferred semi-idle packages
      sched: nominate preferred wakeup cpu
      sched: favour lower logical cpu number for sched_mc balance
      sched: framework for sched_mc/smt_power_savings=N
      sched: convert BALANCE_FOR_xx_POWER to inline functions
      x86: use possible_cpus=NUM to extend the possible cpus allowed
      x86: fix cpu_mask_to_apicid_and to include cpu_online_mask
      x86: update io_apic.c to the new cpumask code
      x86: Introduce topology_core_cpumask()/topology_thread_cpumask()
      x86: xen: use smp_call_function_many()
      x86: use work_on_cpu in x86/kernel/cpu/mcheck/mce_amd_64.c
      ...
    
    Fixed up trivial conflict in kernel/time/tick-sched.c manually

commit 34bf5d0ff54d03f0a8ed690d47efb806ee2fffcb
Merge: bd8b96dfc216 79a66b96c339
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Dec 27 11:30:05 2008 +0100

    Merge branch 'x86/core' into x86/cleanups

commit bd8b96dfc216eebc72950a6c40da8d3eca8667df
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Dec 26 09:20:22 2008 +0100

    x86: clean up comment style in arch/x86/kernel/traps.c
    
    Impact: cleanup
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index f5a640ba04bc..dbfb80289928 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -292,8 +292,10 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 	tsk->thread.error_code = error_code;
 	tsk->thread.trap_no = 8;
 
-	/* This is always a kernel trap and never fixable (and thus must
-	   never return). */
+	/*
+	 * This is always a kernel trap and never fixable (and thus must
+	 * never return).
+	 */
 	for (;;)
 		die(str, regs, error_code);
 }
@@ -524,9 +526,11 @@ dotraplinkage void __kprobes do_int3(struct pt_regs *regs, long error_code)
 }
 
 #ifdef CONFIG_X86_64
-/* Help handler running on IST stack to switch back to user stack
-   for scheduling or signal handling. The actual stack switch is done in
-   entry.S */
+/*
+ * Help handler running on IST stack to switch back to user stack
+ * for scheduling or signal handling. The actual stack switch is done in
+ * entry.S
+ */
 asmlinkage __kprobes struct pt_regs *sync_regs(struct pt_regs *eregs)
 {
 	struct pt_regs *regs = eregs;
@@ -536,8 +540,10 @@ asmlinkage __kprobes struct pt_regs *sync_regs(struct pt_regs *eregs)
 	/* Exception from user space */
 	else if (user_mode(eregs))
 		regs = task_pt_regs(current);
-	/* Exception from kernel and interrupts are enabled. Move to
-	   kernel process stack. */
+	/*
+	 * Exception from kernel and interrupts are enabled. Move to
+	 * kernel process stack.
+	 */
 	else if (eregs->flags & X86_EFLAGS_IF)
 		regs = (struct pt_regs *)(eregs->sp -= sizeof(struct pt_regs));
 	if (eregs != regs)
@@ -707,8 +713,10 @@ void math_error(void __user *ip)
 	} else if (err & 0x020) { /* Precision */
 		info.si_code = FPE_FLTRES;
 	} else {
-		/* If we're using IRQ 13, or supposedly even some trap 16
-		   implementations, it's possible we get a spurious trap... */
+		/*
+		 * If we're using IRQ 13, or supposedly even some trap 16
+		 * implementations, it's possible we get a spurious trap...
+		 */
 		return;		/* Spurious trap, no error */
 	}
 	force_sig_info(SIGFPE, &info, task);

commit a73ad3331fdbf4191cf99b83ea9ac7082b6757ba
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Dec 25 10:39:01 2008 -0800

    x86: unify the implementation of FPU traps
    
    On 32 bits, we may suffer IRQ 13, or supposedly we might have a buggy
    implementation which gives spurious trap 16.  We did not check for
    this on 64 bits, but there is no reason we can't make the code the
    same in both cases.  Furthermore, this is presumably rare, so do the
    spurious check last, instead of first.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index f37cee75ab58..f5a640ba04bc 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -689,12 +689,7 @@ void math_error(void __user *ip)
 	cwd = get_fpu_cwd(task);
 	swd = get_fpu_swd(task);
 
-	err = swd & ~cwd & 0x3f;
-
-#ifdef CONFIG_X86_32
-	if (!err)
-		return;
-#endif
+	err = swd & ~cwd;
 
 	if (err & 0x001) {	/* Invalid op */
 		/*
@@ -712,7 +707,9 @@ void math_error(void __user *ip)
 	} else if (err & 0x020) { /* Precision */
 		info.si_code = FPE_FLTRES;
 	} else {
-		info.si_code = __SI_FAULT|SI_KERNEL; /* WTF? */
+		/* If we're using IRQ 13, or supposedly even some trap 16
+		   implementations, it's possible we get a spurious trap... */
+		return;		/* Spurious trap, no error */
 	}
 	force_sig_info(SIGFPE, &info, task);
 }

commit 79a66b96c339626a3e4b226fefc0e45244cfe6ff
Merge: bed4f13065b5 c1c15b65ec30 1fcccb008be1 4a6908a3a050
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 25 11:50:41 2008 +0100

    Merge branches 'x86/pat2' and 'x86/fpu'; commit 'v2.6.28' into x86/core

commit 1fcccb008be12ea823aaa392758e1e41fb82de9a
Author: Jaswinder Singh <jaswinder@infradead.org>
Date:   Tue Dec 23 21:50:11 2008 +0530

    x86: traps.c replace #if CONFIG_X86_32 with #ifdef CONFIG_X86_32
    
    Impact: cleanup, avoid warning on X86_64
    
    Fixes this warning on X86_64:
    
      CC      arch/x86/kernel/traps.o
      arch/x86/kernel/traps.c:695:5: warning: "CONFIG_X86_32" is not defined
    
    Signed-off-by: Jaswinder Singh <jaswinder@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index c320c29255c2..f37cee75ab58 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -691,7 +691,7 @@ void math_error(void __user *ip)
 
 	err = swd & ~cwd & 0x3f;
 
-#if CONFIG_X86_32
+#ifdef CONFIG_X86_32
 	if (!err)
 		return;
 #endif

commit b77b881f21b29aa7efa668fde69ee3dc0372ae3f
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Dec 19 15:23:44 2008 -0800

    x86: fix lguest used_vectors breakage, -v2
    
    Impact: fix lguest, clean up
    
    32-bit lguest used used_vectors to record vectors, but that model of
    allocating vectors changed and got broken, after we changed vector
    allocation to a per_cpu array.
    
    Try enable that for 64bit, and the array is used for all vectors that
    are not managed by vector_irq per_cpu array.
    
    Also kill system_vectors[], that is now a duplication of the
    used_vectors bitmap.
    
    [ merged in cpus4096 due to io_apic.c cpumask changes. ]
    [ -v2, fix build failure ]
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 04d242ab0161..4a6dff39a470 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -72,9 +72,6 @@
 
 #include "cpu/mcheck/mce.h"
 
-DECLARE_BITMAP(used_vectors, NR_VECTORS);
-EXPORT_SYMBOL_GPL(used_vectors);
-
 asmlinkage int system_call(void);
 
 /* Do we ignore FPU interrupts ? */
@@ -89,6 +86,9 @@ gate_desc idt_table[256]
 	__attribute__((__section__(".data.idt"))) = { { { { 0, 0 } } }, };
 #endif
 
+DECLARE_BITMAP(used_vectors, NR_VECTORS);
+EXPORT_SYMBOL_GPL(used_vectors);
+
 static int ignore_nmis;
 
 static inline void conditional_sti(struct pt_regs *regs)
@@ -949,9 +949,7 @@ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 
 void __init trap_init(void)
 {
-#ifdef CONFIG_X86_32
 	int i;
-#endif
 
 #ifdef CONFIG_EISA
 	void __iomem *p = early_ioremap(0x0FFFD9, 4);
@@ -1008,11 +1006,15 @@ void __init trap_init(void)
 	}
 
 	set_system_trap_gate(SYSCALL_VECTOR, &system_call);
+#endif
 
 	/* Reserve all the builtin and the syscall vector: */
 	for (i = 0; i < FIRST_EXTERNAL_VECTOR; i++)
 		set_bit(i, used_vectors);
 
+#ifdef CONFIG_X86_64
+	set_bit(IA32_SYSCALL_VECTOR, used_vectors);
+#else
 	set_bit(SYSCALL_VECTOR, used_vectors);
 #endif
 	/*

commit bed4f13065b520e564adffbfcd1c1a764a9c887e
Merge: 3e5621edb339 bf8bd66d0580
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 23 16:30:31 2008 +0100

    Merge branch 'x86/irq' into x86/core

commit adf77bac052bb5bf0722b2ce2af9fefc5b2d2a71
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Dec 22 17:56:05 2008 -0800

    x86: prioritize the FPU traps for the error code
    
    In the case of multiple FPU errors, prioritize the error codes,
    instead of returning __SI_FAULT, which ends up pushing a 0 as the
    error code to userspace, a POSIX violation.
    
    For i386, we will simply return if there are no errors at all; for
    x86-64 this is probably a "can't happen" (and the code should be
    unified), but for this patch, return __SI_FAULT|SI_KERNEL if this ever
    happens.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 04d242ab0161..c320c29255c2 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -664,7 +664,7 @@ void math_error(void __user *ip)
 {
 	struct task_struct *task;
 	siginfo_t info;
-	unsigned short cwd, swd;
+	unsigned short cwd, swd, err;
 
 	/*
 	 * Save the info for the exception handler and clear the error.
@@ -675,7 +675,6 @@ void math_error(void __user *ip)
 	task->thread.error_code = 0;
 	info.si_signo = SIGFPE;
 	info.si_errno = 0;
-	info.si_code = __SI_FAULT;
 	info.si_addr = ip;
 	/*
 	 * (~cwd & swd) will mask out exceptions that are not set to unmasked
@@ -689,34 +688,31 @@ void math_error(void __user *ip)
 	 */
 	cwd = get_fpu_cwd(task);
 	swd = get_fpu_swd(task);
-	switch (swd & ~cwd & 0x3f) {
-	case 0x000: /* No unmasked exception */
-#ifdef CONFIG_X86_32
+
+	err = swd & ~cwd & 0x3f;
+
+#if CONFIG_X86_32
+	if (!err)
 		return;
 #endif
-	default: /* Multiple exceptions */
-		break;
-	case 0x001: /* Invalid Op */
+
+	if (err & 0x001) {	/* Invalid op */
 		/*
 		 * swd & 0x240 == 0x040: Stack Underflow
 		 * swd & 0x240 == 0x240: Stack Overflow
 		 * User must clear the SF bit (0x40) if set
 		 */
 		info.si_code = FPE_FLTINV;
-		break;
-	case 0x002: /* Denormalize */
-	case 0x010: /* Underflow */
-		info.si_code = FPE_FLTUND;
-		break;
-	case 0x004: /* Zero Divide */
+	} else if (err & 0x004) { /* Divide by Zero */
 		info.si_code = FPE_FLTDIV;
-		break;
-	case 0x008: /* Overflow */
+	} else if (err & 0x008) { /* Overflow */
 		info.si_code = FPE_FLTOVF;
-		break;
-	case 0x020: /* Precision */
+	} else if (err & 0x012) { /* Denormal, Underflow */
+		info.si_code = FPE_FLTUND;
+	} else if (err & 0x020) { /* Precision */
 		info.si_code = FPE_FLTRES;
-		break;
+	} else {
+		info.si_code = __SI_FAULT|SI_KERNEL; /* WTF? */
 	}
 	force_sig_info(SIGFPE, &info, task);
 }

commit 915b0d0104b72fd36af088ba4b11b5690bc96a6c
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Mon Dec 8 19:19:26 2008 -0800

    x86: hardirq: introduce inc_irq_stat()
    
    Impact: cleanup
    
    Introduce inc_irq_stat() macro and unify irq_stat accounting code.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 04d242ab0161..d815293e6d94 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -481,11 +481,7 @@ do_nmi(struct pt_regs *regs, long error_code)
 {
 	nmi_enter();
 
-#ifdef CONFIG_X86_32
-	{ int cpu; cpu = smp_processor_id(); ++nmi_count(cpu); }
-#else
-	add_pda(__nmi_count, 1);
-#endif
+	inc_irq_stat(__nmi_count);
 
 	if (!ignore_nmis)
 		default_do_nmi(regs);

commit d2f6f7aeee890df445be29a60e34925ec15f620c
Author: Andi Kleen <andi@firstfloor.org>
Date:   Tue Oct 21 22:45:22 2008 +0200

    MCE: Don't run 32bit machine checks with interrupts on
    
    Running machine checks with interrupt on is a extremly bad idea. The machine
    check handler only runs when the system is broken and needs to finish
    as quickly as possible.
    
    Remove the respective bogus post 2.6.27 regression and call
    the machine check vector directly again.
    
    This removes only code.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    [Cherry-picked from x86/mce]
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index e062974cce34..04d242ab0161 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -931,14 +931,6 @@ do_device_not_available(struct pt_regs *regs, long error)
 }
 
 #ifdef CONFIG_X86_32
-#ifdef CONFIG_X86_MCE
-dotraplinkage void __kprobes do_machine_check(struct pt_regs *regs, long error)
-{
-	conditional_sti(regs);
-	machine_check_vector(regs, error);
-}
-#endif
-
 dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 {
 	siginfo_t info;

commit dd6e4eba1c03c9562fced21736453396c045f869
Author: Alexander van Heukelum <heukelum@fastmail.fm>
Date:   Sat Oct 4 23:12:40 2008 +0200

    dumpstack: x86: move die_nmi to dumpstack_32.c
    
    For some reason die_nmi is still defined in traps.c for
    i386, but is found in dumpstack_64.c for x86_64. Move it
    to dumpstack_32.c
    
    Signed-off-by: Alexander van Heukelum <heukelum@fastmail.fm>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ffb131f74f78..e062974cce34 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -429,43 +429,6 @@ unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 	printk(KERN_EMERG "Dazed and confused, but trying to continue\n");
 }
 
-#ifdef CONFIG_X86_32
-static DEFINE_SPINLOCK(nmi_print_lock);
-
-void notrace __kprobes die_nmi(char *str, struct pt_regs *regs, int do_panic)
-{
-	if (notify_die(DIE_NMIWATCHDOG, str, regs, 0, 2, SIGINT) == NOTIFY_STOP)
-		return;
-
-	spin_lock(&nmi_print_lock);
-	/*
-	* We are in trouble anyway, lets at least try
-	* to get a message out:
-	*/
-	bust_spinlocks(1);
-	printk(KERN_EMERG "%s", str);
-	printk(" on CPU%d, ip %08lx, registers:\n",
-		smp_processor_id(), regs->ip);
-	show_registers(regs);
-	if (do_panic)
-		panic("Non maskable interrupt");
-	console_silent();
-	spin_unlock(&nmi_print_lock);
-	bust_spinlocks(0);
-
-	/*
-	 * If we are in kernel we are probably nested up pretty bad
-	 * and might aswell get out now while we still can:
-	 */
-	if (!user_mode_vm(regs)) {
-		current->thread.trap_no = 2;
-		crash_kexec(regs);
-	}
-
-	do_exit(SIGSEGV);
-}
-#endif
-
 static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 {
 	unsigned char reason = 0;

commit 8728861b4fead8119a1b7bb856a387320859cd98
Author: Alexander van Heukelum <heukelum@fastmail.fm>
Date:   Fri Oct 3 22:00:40 2008 +0200

    traps: x86: finalize unification of traps.c
    
    traps_32.c and traps_64.c are now equal. Move one to traps.c,
    delete the other one and change the Makefile
    
    Signed-off-by: Alexander van Heukelum <heukelum@fastmail.fm>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
new file mode 100644
index 000000000000..ffb131f74f78
--- /dev/null
+++ b/arch/x86/kernel/traps.c
@@ -0,0 +1,1071 @@
+/*
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ *  Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs
+ *
+ *  Pentium III FXSR, SSE support
+ *	Gareth Hughes <gareth@valinux.com>, May 2000
+ */
+
+/*
+ * Handle hardware traps and faults.
+ */
+#include <linux/interrupt.h>
+#include <linux/kallsyms.h>
+#include <linux/spinlock.h>
+#include <linux/kprobes.h>
+#include <linux/uaccess.h>
+#include <linux/utsname.h>
+#include <linux/kdebug.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/ptrace.h>
+#include <linux/string.h>
+#include <linux/unwind.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/kexec.h>
+#include <linux/sched.h>
+#include <linux/timer.h>
+#include <linux/init.h>
+#include <linux/bug.h>
+#include <linux/nmi.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/io.h>
+
+#ifdef CONFIG_EISA
+#include <linux/ioport.h>
+#include <linux/eisa.h>
+#endif
+
+#ifdef CONFIG_MCA
+#include <linux/mca.h>
+#endif
+
+#if defined(CONFIG_EDAC)
+#include <linux/edac.h>
+#endif
+
+#include <asm/stacktrace.h>
+#include <asm/processor.h>
+#include <asm/debugreg.h>
+#include <asm/atomic.h>
+#include <asm/system.h>
+#include <asm/unwind.h>
+#include <asm/traps.h>
+#include <asm/desc.h>
+#include <asm/i387.h>
+
+#include <mach_traps.h>
+
+#ifdef CONFIG_X86_64
+#include <asm/pgalloc.h>
+#include <asm/proto.h>
+#include <asm/pda.h>
+#else
+#include <asm/processor-flags.h>
+#include <asm/arch_hooks.h>
+#include <asm/nmi.h>
+#include <asm/smp.h>
+#include <asm/io.h>
+#include <asm/traps.h>
+
+#include "cpu/mcheck/mce.h"
+
+DECLARE_BITMAP(used_vectors, NR_VECTORS);
+EXPORT_SYMBOL_GPL(used_vectors);
+
+asmlinkage int system_call(void);
+
+/* Do we ignore FPU interrupts ? */
+char ignore_fpu_irq;
+
+/*
+ * The IDT has to be page-aligned to simplify the Pentium
+ * F0 0F bug workaround.. We have a special link segment
+ * for this.
+ */
+gate_desc idt_table[256]
+	__attribute__((__section__(".data.idt"))) = { { { { 0, 0 } } }, };
+#endif
+
+static int ignore_nmis;
+
+static inline void conditional_sti(struct pt_regs *regs)
+{
+	if (regs->flags & X86_EFLAGS_IF)
+		local_irq_enable();
+}
+
+static inline void preempt_conditional_sti(struct pt_regs *regs)
+{
+	inc_preempt_count();
+	if (regs->flags & X86_EFLAGS_IF)
+		local_irq_enable();
+}
+
+static inline void preempt_conditional_cli(struct pt_regs *regs)
+{
+	if (regs->flags & X86_EFLAGS_IF)
+		local_irq_disable();
+	dec_preempt_count();
+}
+
+#ifdef CONFIG_X86_32
+static inline void
+die_if_kernel(const char *str, struct pt_regs *regs, long err)
+{
+	if (!user_mode_vm(regs))
+		die(str, regs, err);
+}
+
+/*
+ * Perform the lazy TSS's I/O bitmap copy. If the TSS has an
+ * invalid offset set (the LAZY one) and the faulting thread has
+ * a valid I/O bitmap pointer, we copy the I/O bitmap in the TSS,
+ * we set the offset field correctly and return 1.
+ */
+static int lazy_iobitmap_copy(void)
+{
+	struct thread_struct *thread;
+	struct tss_struct *tss;
+	int cpu;
+
+	cpu = get_cpu();
+	tss = &per_cpu(init_tss, cpu);
+	thread = &current->thread;
+
+	if (tss->x86_tss.io_bitmap_base == INVALID_IO_BITMAP_OFFSET_LAZY &&
+	    thread->io_bitmap_ptr) {
+		memcpy(tss->io_bitmap, thread->io_bitmap_ptr,
+		       thread->io_bitmap_max);
+		/*
+		 * If the previously set map was extending to higher ports
+		 * than the current one, pad extra space with 0xff (no access).
+		 */
+		if (thread->io_bitmap_max < tss->io_bitmap_max) {
+			memset((char *) tss->io_bitmap +
+				thread->io_bitmap_max, 0xff,
+				tss->io_bitmap_max - thread->io_bitmap_max);
+		}
+		tss->io_bitmap_max = thread->io_bitmap_max;
+		tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
+		tss->io_bitmap_owner = thread;
+		put_cpu();
+
+		return 1;
+	}
+	put_cpu();
+
+	return 0;
+}
+#endif
+
+static void __kprobes
+do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
+	long error_code, siginfo_t *info)
+{
+	struct task_struct *tsk = current;
+
+#ifdef CONFIG_X86_32
+	if (regs->flags & X86_VM_MASK) {
+		/*
+		 * traps 0, 1, 3, 4, and 5 should be forwarded to vm86.
+		 * On nmi (interrupt 2), do_trap should not be called.
+		 */
+		if (trapnr < 6)
+			goto vm86_trap;
+		goto trap_signal;
+	}
+#endif
+
+	if (!user_mode(regs))
+		goto kernel_trap;
+
+#ifdef CONFIG_X86_32
+trap_signal:
+#endif
+	/*
+	 * We want error_code and trap_no set for userspace faults and
+	 * kernelspace faults which result in die(), but not
+	 * kernelspace faults which are fixed up.  die() gives the
+	 * process no chance to handle the signal and notice the
+	 * kernel fault information, so that won't result in polluting
+	 * the information about previously queued, but not yet
+	 * delivered, faults.  See also do_general_protection below.
+	 */
+	tsk->thread.error_code = error_code;
+	tsk->thread.trap_no = trapnr;
+
+#ifdef CONFIG_X86_64
+	if (show_unhandled_signals && unhandled_signal(tsk, signr) &&
+	    printk_ratelimit()) {
+		printk(KERN_INFO
+		       "%s[%d] trap %s ip:%lx sp:%lx error:%lx",
+		       tsk->comm, tsk->pid, str,
+		       regs->ip, regs->sp, error_code);
+		print_vma_addr(" in ", regs->ip);
+		printk("\n");
+	}
+#endif
+
+	if (info)
+		force_sig_info(signr, info, tsk);
+	else
+		force_sig(signr, tsk);
+	return;
+
+kernel_trap:
+	if (!fixup_exception(regs)) {
+		tsk->thread.error_code = error_code;
+		tsk->thread.trap_no = trapnr;
+		die(str, regs, error_code);
+	}
+	return;
+
+#ifdef CONFIG_X86_32
+vm86_trap:
+	if (handle_vm86_trap((struct kernel_vm86_regs *) regs,
+						error_code, trapnr))
+		goto trap_signal;
+	return;
+#endif
+}
+
+#define DO_ERROR(trapnr, signr, str, name)				\
+dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
+{									\
+	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr)	\
+							== NOTIFY_STOP)	\
+		return;							\
+	conditional_sti(regs);						\
+	do_trap(trapnr, signr, str, regs, error_code, NULL);		\
+}
+
+#define DO_ERROR_INFO(trapnr, signr, str, name, sicode, siaddr)		\
+dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
+{									\
+	siginfo_t info;							\
+	info.si_signo = signr;						\
+	info.si_errno = 0;						\
+	info.si_code = sicode;						\
+	info.si_addr = (void __user *)siaddr;				\
+	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr)	\
+							== NOTIFY_STOP)	\
+		return;							\
+	conditional_sti(regs);						\
+	do_trap(trapnr, signr, str, regs, error_code, &info);		\
+}
+
+DO_ERROR_INFO(0, SIGFPE, "divide error", divide_error, FPE_INTDIV, regs->ip)
+DO_ERROR(4, SIGSEGV, "overflow", overflow)
+DO_ERROR(5, SIGSEGV, "bounds", bounds)
+DO_ERROR_INFO(6, SIGILL, "invalid opcode", invalid_op, ILL_ILLOPN, regs->ip)
+DO_ERROR(9, SIGFPE, "coprocessor segment overrun", coprocessor_segment_overrun)
+DO_ERROR(10, SIGSEGV, "invalid TSS", invalid_TSS)
+DO_ERROR(11, SIGBUS, "segment not present", segment_not_present)
+#ifdef CONFIG_X86_32
+DO_ERROR(12, SIGBUS, "stack segment", stack_segment)
+#endif
+DO_ERROR_INFO(17, SIGBUS, "alignment check", alignment_check, BUS_ADRALN, 0)
+
+#ifdef CONFIG_X86_64
+/* Runs on IST stack */
+dotraplinkage void do_stack_segment(struct pt_regs *regs, long error_code)
+{
+	if (notify_die(DIE_TRAP, "stack segment", regs, error_code,
+			12, SIGBUS) == NOTIFY_STOP)
+		return;
+	preempt_conditional_sti(regs);
+	do_trap(12, SIGBUS, "stack segment", regs, error_code, NULL);
+	preempt_conditional_cli(regs);
+}
+
+dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
+{
+	static const char str[] = "double fault";
+	struct task_struct *tsk = current;
+
+	/* Return not checked because double check cannot be ignored */
+	notify_die(DIE_TRAP, str, regs, error_code, 8, SIGSEGV);
+
+	tsk->thread.error_code = error_code;
+	tsk->thread.trap_no = 8;
+
+	/* This is always a kernel trap and never fixable (and thus must
+	   never return). */
+	for (;;)
+		die(str, regs, error_code);
+}
+#endif
+
+dotraplinkage void __kprobes
+do_general_protection(struct pt_regs *regs, long error_code)
+{
+	struct task_struct *tsk;
+
+	conditional_sti(regs);
+
+#ifdef CONFIG_X86_32
+	if (lazy_iobitmap_copy()) {
+		/* restart the faulting instruction */
+		return;
+	}
+
+	if (regs->flags & X86_VM_MASK)
+		goto gp_in_vm86;
+#endif
+
+	tsk = current;
+	if (!user_mode(regs))
+		goto gp_in_kernel;
+
+	tsk->thread.error_code = error_code;
+	tsk->thread.trap_no = 13;
+
+	if (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&
+			printk_ratelimit()) {
+		printk(KERN_INFO
+			"%s[%d] general protection ip:%lx sp:%lx error:%lx",
+			tsk->comm, task_pid_nr(tsk),
+			regs->ip, regs->sp, error_code);
+		print_vma_addr(" in ", regs->ip);
+		printk("\n");
+	}
+
+	force_sig(SIGSEGV, tsk);
+	return;
+
+#ifdef CONFIG_X86_32
+gp_in_vm86:
+	local_irq_enable();
+	handle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);
+	return;
+#endif
+
+gp_in_kernel:
+	if (fixup_exception(regs))
+		return;
+
+	tsk->thread.error_code = error_code;
+	tsk->thread.trap_no = 13;
+	if (notify_die(DIE_GPF, "general protection fault", regs,
+				error_code, 13, SIGSEGV) == NOTIFY_STOP)
+		return;
+	die("general protection fault", regs, error_code);
+}
+
+static notrace __kprobes void
+mem_parity_error(unsigned char reason, struct pt_regs *regs)
+{
+	printk(KERN_EMERG
+		"Uhhuh. NMI received for unknown reason %02x on CPU %d.\n",
+			reason, smp_processor_id());
+
+	printk(KERN_EMERG
+		"You have some hardware problem, likely on the PCI bus.\n");
+
+#if defined(CONFIG_EDAC)
+	if (edac_handler_set()) {
+		edac_atomic_assert_error();
+		return;
+	}
+#endif
+
+	if (panic_on_unrecovered_nmi)
+		panic("NMI: Not continuing");
+
+	printk(KERN_EMERG "Dazed and confused, but trying to continue\n");
+
+	/* Clear and disable the memory parity error line. */
+	reason = (reason & 0xf) | 4;
+	outb(reason, 0x61);
+}
+
+static notrace __kprobes void
+io_check_error(unsigned char reason, struct pt_regs *regs)
+{
+	unsigned long i;
+
+	printk(KERN_EMERG "NMI: IOCK error (debug interrupt?)\n");
+	show_registers(regs);
+
+	/* Re-enable the IOCK line, wait for a few seconds */
+	reason = (reason & 0xf) | 8;
+	outb(reason, 0x61);
+
+	i = 2000;
+	while (--i)
+		udelay(1000);
+
+	reason &= ~8;
+	outb(reason, 0x61);
+}
+
+static notrace __kprobes void
+unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
+{
+	if (notify_die(DIE_NMIUNKNOWN, "nmi", regs, reason, 2, SIGINT) ==
+			NOTIFY_STOP)
+		return;
+#ifdef CONFIG_MCA
+	/*
+	 * Might actually be able to figure out what the guilty party
+	 * is:
+	 */
+	if (MCA_bus) {
+		mca_handle_nmi();
+		return;
+	}
+#endif
+	printk(KERN_EMERG
+		"Uhhuh. NMI received for unknown reason %02x on CPU %d.\n",
+			reason, smp_processor_id());
+
+	printk(KERN_EMERG "Do you have a strange power saving mode enabled?\n");
+	if (panic_on_unrecovered_nmi)
+		panic("NMI: Not continuing");
+
+	printk(KERN_EMERG "Dazed and confused, but trying to continue\n");
+}
+
+#ifdef CONFIG_X86_32
+static DEFINE_SPINLOCK(nmi_print_lock);
+
+void notrace __kprobes die_nmi(char *str, struct pt_regs *regs, int do_panic)
+{
+	if (notify_die(DIE_NMIWATCHDOG, str, regs, 0, 2, SIGINT) == NOTIFY_STOP)
+		return;
+
+	spin_lock(&nmi_print_lock);
+	/*
+	* We are in trouble anyway, lets at least try
+	* to get a message out:
+	*/
+	bust_spinlocks(1);
+	printk(KERN_EMERG "%s", str);
+	printk(" on CPU%d, ip %08lx, registers:\n",
+		smp_processor_id(), regs->ip);
+	show_registers(regs);
+	if (do_panic)
+		panic("Non maskable interrupt");
+	console_silent();
+	spin_unlock(&nmi_print_lock);
+	bust_spinlocks(0);
+
+	/*
+	 * If we are in kernel we are probably nested up pretty bad
+	 * and might aswell get out now while we still can:
+	 */
+	if (!user_mode_vm(regs)) {
+		current->thread.trap_no = 2;
+		crash_kexec(regs);
+	}
+
+	do_exit(SIGSEGV);
+}
+#endif
+
+static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
+{
+	unsigned char reason = 0;
+	int cpu;
+
+	cpu = smp_processor_id();
+
+	/* Only the BSP gets external NMIs from the system. */
+	if (!cpu)
+		reason = get_nmi_reason();
+
+	if (!(reason & 0xc0)) {
+		if (notify_die(DIE_NMI_IPI, "nmi_ipi", regs, reason, 2, SIGINT)
+								== NOTIFY_STOP)
+			return;
+#ifdef CONFIG_X86_LOCAL_APIC
+		/*
+		 * Ok, so this is none of the documented NMI sources,
+		 * so it must be the NMI watchdog.
+		 */
+		if (nmi_watchdog_tick(regs, reason))
+			return;
+		if (!do_nmi_callback(regs, cpu))
+			unknown_nmi_error(reason, regs);
+#else
+		unknown_nmi_error(reason, regs);
+#endif
+
+		return;
+	}
+	if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT) == NOTIFY_STOP)
+		return;
+
+	/* AK: following checks seem to be broken on modern chipsets. FIXME */
+	if (reason & 0x80)
+		mem_parity_error(reason, regs);
+	if (reason & 0x40)
+		io_check_error(reason, regs);
+#ifdef CONFIG_X86_32
+	/*
+	 * Reassert NMI in case it became active meanwhile
+	 * as it's edge-triggered:
+	 */
+	reassert_nmi();
+#endif
+}
+
+dotraplinkage notrace __kprobes void
+do_nmi(struct pt_regs *regs, long error_code)
+{
+	nmi_enter();
+
+#ifdef CONFIG_X86_32
+	{ int cpu; cpu = smp_processor_id(); ++nmi_count(cpu); }
+#else
+	add_pda(__nmi_count, 1);
+#endif
+
+	if (!ignore_nmis)
+		default_do_nmi(regs);
+
+	nmi_exit();
+}
+
+void stop_nmi(void)
+{
+	acpi_nmi_disable();
+	ignore_nmis++;
+}
+
+void restart_nmi(void)
+{
+	ignore_nmis--;
+	acpi_nmi_enable();
+}
+
+/* May run on IST stack. */
+dotraplinkage void __kprobes do_int3(struct pt_regs *regs, long error_code)
+{
+#ifdef CONFIG_KPROBES
+	if (notify_die(DIE_INT3, "int3", regs, error_code, 3, SIGTRAP)
+			== NOTIFY_STOP)
+		return;
+#else
+	if (notify_die(DIE_TRAP, "int3", regs, error_code, 3, SIGTRAP)
+			== NOTIFY_STOP)
+		return;
+#endif
+
+	preempt_conditional_sti(regs);
+	do_trap(3, SIGTRAP, "int3", regs, error_code, NULL);
+	preempt_conditional_cli(regs);
+}
+
+#ifdef CONFIG_X86_64
+/* Help handler running on IST stack to switch back to user stack
+   for scheduling or signal handling. The actual stack switch is done in
+   entry.S */
+asmlinkage __kprobes struct pt_regs *sync_regs(struct pt_regs *eregs)
+{
+	struct pt_regs *regs = eregs;
+	/* Did already sync */
+	if (eregs == (struct pt_regs *)eregs->sp)
+		;
+	/* Exception from user space */
+	else if (user_mode(eregs))
+		regs = task_pt_regs(current);
+	/* Exception from kernel and interrupts are enabled. Move to
+	   kernel process stack. */
+	else if (eregs->flags & X86_EFLAGS_IF)
+		regs = (struct pt_regs *)(eregs->sp -= sizeof(struct pt_regs));
+	if (eregs != regs)
+		*regs = *eregs;
+	return regs;
+}
+#endif
+
+/*
+ * Our handling of the processor debug registers is non-trivial.
+ * We do not clear them on entry and exit from the kernel. Therefore
+ * it is possible to get a watchpoint trap here from inside the kernel.
+ * However, the code in ./ptrace.c has ensured that the user can
+ * only set watchpoints on userspace addresses. Therefore the in-kernel
+ * watchpoint trap can only occur in code which is reading/writing
+ * from user space. Such code must not hold kernel locks (since it
+ * can equally take a page fault), therefore it is safe to call
+ * force_sig_info even though that claims and releases locks.
+ *
+ * Code in ./signal.c ensures that the debug control register
+ * is restored before we deliver any signal, and therefore that
+ * user code runs with the correct debug control register even though
+ * we clear it here.
+ *
+ * Being careful here means that we don't have to be as careful in a
+ * lot of more complicated places (task switching can be a bit lazy
+ * about restoring all the debug state, and ptrace doesn't have to
+ * find every occurrence of the TF bit that could be saved away even
+ * by user code)
+ *
+ * May run on IST stack.
+ */
+dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
+{
+	struct task_struct *tsk = current;
+	unsigned long condition;
+	int si_code;
+
+	get_debugreg(condition, 6);
+
+	/*
+	 * The processor cleared BTF, so don't mark that we need it set.
+	 */
+	clear_tsk_thread_flag(tsk, TIF_DEBUGCTLMSR);
+	tsk->thread.debugctlmsr = 0;
+
+	if (notify_die(DIE_DEBUG, "debug", regs, condition, error_code,
+						SIGTRAP) == NOTIFY_STOP)
+		return;
+
+	/* It's safe to allow irq's after DR6 has been saved */
+	preempt_conditional_sti(regs);
+
+	/* Mask out spurious debug traps due to lazy DR7 setting */
+	if (condition & (DR_TRAP0|DR_TRAP1|DR_TRAP2|DR_TRAP3)) {
+		if (!tsk->thread.debugreg7)
+			goto clear_dr7;
+	}
+
+#ifdef CONFIG_X86_32
+	if (regs->flags & X86_VM_MASK)
+		goto debug_vm86;
+#endif
+
+	/* Save debug status register where ptrace can see it */
+	tsk->thread.debugreg6 = condition;
+
+	/*
+	 * Single-stepping through TF: make sure we ignore any events in
+	 * kernel space (but re-enable TF when returning to user mode).
+	 */
+	if (condition & DR_STEP) {
+		if (!user_mode(regs))
+			goto clear_TF_reenable;
+	}
+
+	si_code = get_si_code(condition);
+	/* Ok, finally something we can handle */
+	send_sigtrap(tsk, regs, error_code, si_code);
+
+	/*
+	 * Disable additional traps. They'll be re-enabled when
+	 * the signal is delivered.
+	 */
+clear_dr7:
+	set_debugreg(0, 7);
+	preempt_conditional_cli(regs);
+	return;
+
+#ifdef CONFIG_X86_32
+debug_vm86:
+	handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code, 1);
+	preempt_conditional_cli(regs);
+	return;
+#endif
+
+clear_TF_reenable:
+	set_tsk_thread_flag(tsk, TIF_SINGLESTEP);
+	regs->flags &= ~X86_EFLAGS_TF;
+	preempt_conditional_cli(regs);
+	return;
+}
+
+#ifdef CONFIG_X86_64
+static int kernel_math_error(struct pt_regs *regs, const char *str, int trapnr)
+{
+	if (fixup_exception(regs))
+		return 1;
+
+	notify_die(DIE_GPF, str, regs, 0, trapnr, SIGFPE);
+	/* Illegal floating point operation in the kernel */
+	current->thread.trap_no = trapnr;
+	die(str, regs, 0);
+	return 0;
+}
+#endif
+
+/*
+ * Note that we play around with the 'TS' bit in an attempt to get
+ * the correct behaviour even in the presence of the asynchronous
+ * IRQ13 behaviour
+ */
+void math_error(void __user *ip)
+{
+	struct task_struct *task;
+	siginfo_t info;
+	unsigned short cwd, swd;
+
+	/*
+	 * Save the info for the exception handler and clear the error.
+	 */
+	task = current;
+	save_init_fpu(task);
+	task->thread.trap_no = 16;
+	task->thread.error_code = 0;
+	info.si_signo = SIGFPE;
+	info.si_errno = 0;
+	info.si_code = __SI_FAULT;
+	info.si_addr = ip;
+	/*
+	 * (~cwd & swd) will mask out exceptions that are not set to unmasked
+	 * status.  0x3f is the exception bits in these regs, 0x200 is the
+	 * C1 reg you need in case of a stack fault, 0x040 is the stack
+	 * fault bit.  We should only be taking one exception at a time,
+	 * so if this combination doesn't produce any single exception,
+	 * then we have a bad program that isn't synchronizing its FPU usage
+	 * and it will suffer the consequences since we won't be able to
+	 * fully reproduce the context of the exception
+	 */
+	cwd = get_fpu_cwd(task);
+	swd = get_fpu_swd(task);
+	switch (swd & ~cwd & 0x3f) {
+	case 0x000: /* No unmasked exception */
+#ifdef CONFIG_X86_32
+		return;
+#endif
+	default: /* Multiple exceptions */
+		break;
+	case 0x001: /* Invalid Op */
+		/*
+		 * swd & 0x240 == 0x040: Stack Underflow
+		 * swd & 0x240 == 0x240: Stack Overflow
+		 * User must clear the SF bit (0x40) if set
+		 */
+		info.si_code = FPE_FLTINV;
+		break;
+	case 0x002: /* Denormalize */
+	case 0x010: /* Underflow */
+		info.si_code = FPE_FLTUND;
+		break;
+	case 0x004: /* Zero Divide */
+		info.si_code = FPE_FLTDIV;
+		break;
+	case 0x008: /* Overflow */
+		info.si_code = FPE_FLTOVF;
+		break;
+	case 0x020: /* Precision */
+		info.si_code = FPE_FLTRES;
+		break;
+	}
+	force_sig_info(SIGFPE, &info, task);
+}
+
+dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
+{
+	conditional_sti(regs);
+
+#ifdef CONFIG_X86_32
+	ignore_fpu_irq = 1;
+#else
+	if (!user_mode(regs) &&
+	    kernel_math_error(regs, "kernel x87 math error", 16))
+		return;
+#endif
+
+	math_error((void __user *)regs->ip);
+}
+
+static void simd_math_error(void __user *ip)
+{
+	struct task_struct *task;
+	siginfo_t info;
+	unsigned short mxcsr;
+
+	/*
+	 * Save the info for the exception handler and clear the error.
+	 */
+	task = current;
+	save_init_fpu(task);
+	task->thread.trap_no = 19;
+	task->thread.error_code = 0;
+	info.si_signo = SIGFPE;
+	info.si_errno = 0;
+	info.si_code = __SI_FAULT;
+	info.si_addr = ip;
+	/*
+	 * The SIMD FPU exceptions are handled a little differently, as there
+	 * is only a single status/control register.  Thus, to determine which
+	 * unmasked exception was caught we must mask the exception mask bits
+	 * at 0x1f80, and then use these to mask the exception bits at 0x3f.
+	 */
+	mxcsr = get_fpu_mxcsr(task);
+	switch (~((mxcsr & 0x1f80) >> 7) & (mxcsr & 0x3f)) {
+	case 0x000:
+	default:
+		break;
+	case 0x001: /* Invalid Op */
+		info.si_code = FPE_FLTINV;
+		break;
+	case 0x002: /* Denormalize */
+	case 0x010: /* Underflow */
+		info.si_code = FPE_FLTUND;
+		break;
+	case 0x004: /* Zero Divide */
+		info.si_code = FPE_FLTDIV;
+		break;
+	case 0x008: /* Overflow */
+		info.si_code = FPE_FLTOVF;
+		break;
+	case 0x020: /* Precision */
+		info.si_code = FPE_FLTRES;
+		break;
+	}
+	force_sig_info(SIGFPE, &info, task);
+}
+
+dotraplinkage void
+do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
+{
+	conditional_sti(regs);
+
+#ifdef CONFIG_X86_32
+	if (cpu_has_xmm) {
+		/* Handle SIMD FPU exceptions on PIII+ processors. */
+		ignore_fpu_irq = 1;
+		simd_math_error((void __user *)regs->ip);
+		return;
+	}
+	/*
+	 * Handle strange cache flush from user space exception
+	 * in all other cases.  This is undocumented behaviour.
+	 */
+	if (regs->flags & X86_VM_MASK) {
+		handle_vm86_fault((struct kernel_vm86_regs *)regs, error_code);
+		return;
+	}
+	current->thread.trap_no = 19;
+	current->thread.error_code = error_code;
+	die_if_kernel("cache flush denied", regs, error_code);
+	force_sig(SIGSEGV, current);
+#else
+	if (!user_mode(regs) &&
+			kernel_math_error(regs, "kernel simd math error", 19))
+		return;
+	simd_math_error((void __user *)regs->ip);
+#endif
+}
+
+dotraplinkage void
+do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
+{
+	conditional_sti(regs);
+#if 0
+	/* No need to warn about this any longer. */
+	printk(KERN_INFO "Ignoring P6 Local APIC Spurious Interrupt Bug...\n");
+#endif
+}
+
+#ifdef CONFIG_X86_32
+unsigned long patch_espfix_desc(unsigned long uesp, unsigned long kesp)
+{
+	struct desc_struct *gdt = get_cpu_gdt_table(smp_processor_id());
+	unsigned long base = (kesp - uesp) & -THREAD_SIZE;
+	unsigned long new_kesp = kesp - base;
+	unsigned long lim_pages = (new_kesp | (THREAD_SIZE - 1)) >> PAGE_SHIFT;
+	__u64 desc = *(__u64 *)&gdt[GDT_ENTRY_ESPFIX_SS];
+
+	/* Set up base for espfix segment */
+	desc &= 0x00f0ff0000000000ULL;
+	desc |=	((((__u64)base) << 16) & 0x000000ffffff0000ULL) |
+		((((__u64)base) << 32) & 0xff00000000000000ULL) |
+		((((__u64)lim_pages) << 32) & 0x000f000000000000ULL) |
+		(lim_pages & 0xffff);
+	*(__u64 *)&gdt[GDT_ENTRY_ESPFIX_SS] = desc;
+
+	return new_kesp;
+}
+#else
+asmlinkage void __attribute__((weak)) smp_thermal_interrupt(void)
+{
+}
+
+asmlinkage void __attribute__((weak)) mce_threshold_interrupt(void)
+{
+}
+#endif
+
+/*
+ * 'math_state_restore()' saves the current math information in the
+ * old math state array, and gets the new ones from the current task
+ *
+ * Careful.. There are problems with IBM-designed IRQ13 behaviour.
+ * Don't touch unless you *really* know how it works.
+ *
+ * Must be called with kernel preemption disabled (in this case,
+ * local interrupts are disabled at the call-site in entry.S).
+ */
+asmlinkage void math_state_restore(void)
+{
+	struct thread_info *thread = current_thread_info();
+	struct task_struct *tsk = thread->task;
+
+	if (!tsk_used_math(tsk)) {
+		local_irq_enable();
+		/*
+		 * does a slab alloc which can sleep
+		 */
+		if (init_fpu(tsk)) {
+			/*
+			 * ran out of memory!
+			 */
+			do_group_exit(SIGKILL);
+			return;
+		}
+		local_irq_disable();
+	}
+
+	clts();				/* Allow maths ops (or we recurse) */
+#ifdef CONFIG_X86_32
+	restore_fpu(tsk);
+#else
+	/*
+	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
+	 */
+	if (unlikely(restore_fpu_checking(tsk))) {
+		stts();
+		force_sig(SIGSEGV, tsk);
+		return;
+	}
+#endif
+	thread->status |= TS_USEDFPU;	/* So we fnsave on switch_to() */
+	tsk->fpu_counter++;
+}
+EXPORT_SYMBOL_GPL(math_state_restore);
+
+#ifndef CONFIG_MATH_EMULATION
+asmlinkage void math_emulate(long arg)
+{
+	printk(KERN_EMERG
+		"math-emulation not enabled and no coprocessor found.\n");
+	printk(KERN_EMERG "killing %s.\n", current->comm);
+	force_sig(SIGFPE, current);
+	schedule();
+}
+#endif /* CONFIG_MATH_EMULATION */
+
+dotraplinkage void __kprobes
+do_device_not_available(struct pt_regs *regs, long error)
+{
+#ifdef CONFIG_X86_32
+	if (read_cr0() & X86_CR0_EM) {
+		conditional_sti(regs);
+		math_emulate(0);
+	} else {
+		math_state_restore(); /* interrupts still off */
+		conditional_sti(regs);
+	}
+#else
+	math_state_restore();
+#endif
+}
+
+#ifdef CONFIG_X86_32
+#ifdef CONFIG_X86_MCE
+dotraplinkage void __kprobes do_machine_check(struct pt_regs *regs, long error)
+{
+	conditional_sti(regs);
+	machine_check_vector(regs, error);
+}
+#endif
+
+dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
+{
+	siginfo_t info;
+	local_irq_enable();
+
+	info.si_signo = SIGILL;
+	info.si_errno = 0;
+	info.si_code = ILL_BADSTK;
+	info.si_addr = 0;
+	if (notify_die(DIE_TRAP, "iret exception",
+			regs, error_code, 32, SIGILL) == NOTIFY_STOP)
+		return;
+	do_trap(32, SIGILL, "iret exception", regs, error_code, &info);
+}
+#endif
+
+void __init trap_init(void)
+{
+#ifdef CONFIG_X86_32
+	int i;
+#endif
+
+#ifdef CONFIG_EISA
+	void __iomem *p = early_ioremap(0x0FFFD9, 4);
+
+	if (readl(p) == 'E' + ('I'<<8) + ('S'<<16) + ('A'<<24))
+		EISA_bus = 1;
+	early_iounmap(p, 4);
+#endif
+
+	set_intr_gate(0, &divide_error);
+	set_intr_gate_ist(1, &debug, DEBUG_STACK);
+	set_intr_gate_ist(2, &nmi, NMI_STACK);
+	/* int3 can be called from all */
+	set_system_intr_gate_ist(3, &int3, DEBUG_STACK);
+	/* int4 can be called from all */
+	set_system_intr_gate(4, &overflow);
+	set_intr_gate(5, &bounds);
+	set_intr_gate(6, &invalid_op);
+	set_intr_gate(7, &device_not_available);
+#ifdef CONFIG_X86_32
+	set_task_gate(8, GDT_ENTRY_DOUBLEFAULT_TSS);
+#else
+	set_intr_gate_ist(8, &double_fault, DOUBLEFAULT_STACK);
+#endif
+	set_intr_gate(9, &coprocessor_segment_overrun);
+	set_intr_gate(10, &invalid_TSS);
+	set_intr_gate(11, &segment_not_present);
+	set_intr_gate_ist(12, &stack_segment, STACKFAULT_STACK);
+	set_intr_gate(13, &general_protection);
+	set_intr_gate(14, &page_fault);
+	set_intr_gate(15, &spurious_interrupt_bug);
+	set_intr_gate(16, &coprocessor_error);
+	set_intr_gate(17, &alignment_check);
+#ifdef CONFIG_X86_MCE
+	set_intr_gate_ist(18, &machine_check, MCE_STACK);
+#endif
+	set_intr_gate(19, &simd_coprocessor_error);
+
+#ifdef CONFIG_IA32_EMULATION
+	set_system_intr_gate(IA32_SYSCALL_VECTOR, ia32_syscall);
+#endif
+
+#ifdef CONFIG_X86_32
+	if (cpu_has_fxsr) {
+		printk(KERN_INFO "Enabling fast FPU save and restore... ");
+		set_in_cr4(X86_CR4_OSFXSR);
+		printk("done.\n");
+	}
+	if (cpu_has_xmm) {
+		printk(KERN_INFO
+			"Enabling unmasked SIMD FPU exception support... ");
+		set_in_cr4(X86_CR4_OSXMMEXCPT);
+		printk("done.\n");
+	}
+
+	set_system_trap_gate(SYSCALL_VECTOR, &system_call);
+
+	/* Reserve all the builtin and the syscall vector: */
+	for (i = 0; i < FIRST_EXTERNAL_VECTOR; i++)
+		set_bit(i, used_vectors);
+
+	set_bit(SYSCALL_VECTOR, used_vectors);
+#endif
+	/*
+	 * Should be a barrier for any external CPU state:
+	 */
+	cpu_init();
+
+#ifdef CONFIG_X86_32
+	trap_init_hook();
+#endif
+}
