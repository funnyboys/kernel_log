commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index ad5cdd6a5f23..a29a44a98e5b 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -19,7 +19,6 @@
 #include <linux/efi.h>
 
 #include <asm/init.h>
-#include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
 #include <asm/io_apic.h>

commit 8757dc970f550dc399f899be0e7a2c00b7e82e8f
Author: Omar Sandoval <osandov@fb.com>
Date:   Fri Dec 20 08:22:49 2019 -0800

    x86/crash: Define arch_crash_save_vmcoreinfo() if CONFIG_CRASH_CORE=y
    
    On x86 kernels configured with CONFIG_PROC_KCORE=y and
    CONFIG_KEXEC_CORE=n, the vmcoreinfo note in /proc/kcore is incomplete.
    
    Specifically, it is missing arch-specific information like the KASLR
    offset and whether 5-level page tables are enabled. This breaks
    applications like drgn [1] and crash [2], which need this information
    for live debugging via /proc/kcore.
    
    This happens because:
    
    1. CONFIG_PROC_KCORE selects CONFIG_CRASH_CORE.
    2. kernel/crash_core.c (compiled if CONFIG_CRASH_CORE=y) calls
       arch_crash_save_vmcoreinfo() to get the arch-specific parts of
       vmcoreinfo. If it is not defined, then it uses a no-op fallback.
    3. x86 defines arch_crash_save_vmcoreinfo() in
       arch/x86/kernel/machine_kexec_*.c, which is only compiled if
       CONFIG_KEXEC_CORE=y.
    
    Therefore, an x86 kernel with CONFIG_CRASH_CORE=y and
    CONFIG_KEXEC_CORE=n uses the no-op fallback and gets incomplete
    vmcoreinfo data. This isn't relevant to kdump, which requires
    CONFIG_KEXEC_CORE. It only affects applications which read vmcoreinfo at
    runtime, like the ones mentioned above.
    
    Fix it by moving arch_crash_save_vmcoreinfo() into two new
    arch/x86/kernel/crash_core_*.c files, which are gated behind
    CONFIG_CRASH_CORE.
    
    1: https://github.com/osandov/drgn/blob/73dd7def1217e24cc83d8ca95c995decbd9ba24c/libdrgn/program.c#L385
    2: https://github.com/crash-utility/crash/commit/60a42d709280cdf38ab06327a5b4fa9d9208ef86
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Kairui Song <kasong@redhat.com>
    Cc: Lianbo Jiang <lijiang@redhat.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: "Peter Zijlstra (Intel)" <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/0589961254102cca23e3618b96541b89f2b249e2.1576858905.git.osandov@fb.com

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 16e125a50b33..ad5cdd6a5f23 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -398,25 +398,6 @@ void machine_kexec(struct kimage *image)
 	__ftrace_enabled_restore(save_ftrace_enabled);
 }
 
-void arch_crash_save_vmcoreinfo(void)
-{
-	u64 sme_mask = sme_me_mask;
-
-	VMCOREINFO_NUMBER(phys_base);
-	VMCOREINFO_SYMBOL(init_top_pgt);
-	vmcoreinfo_append_str("NUMBER(pgtable_l5_enabled)=%d\n",
-			pgtable_l5_enabled());
-
-#ifdef CONFIG_NUMA
-	VMCOREINFO_SYMBOL(node_data);
-	VMCOREINFO_LENGTH(node_data, MAX_NUMNODES);
-#endif
-	vmcoreinfo_append_str("KERNELOFFSET=%lx\n",
-			      kaslr_offset());
-	VMCOREINFO_NUMBER(KERNEL_IMAGE_SIZE);
-	VMCOREINFO_NUMBER(sme_mask);
-}
-
 /* arch-dependent functionality related to kexec file-based syscall */
 
 #ifdef CONFIG_KEXEC_FILE

commit 7c321eb2b843bf25946100b7e2de4054f71ec068
Author: Lianbo Jiang <lijiang@redhat.com>
Date:   Fri Nov 8 17:00:26 2019 +0800

    x86/kdump: Remove the backup region handling
    
    When the crashkernel kernel command line option is specified, the low
    1M memory will always be reserved now. Therefore, it's not necessary to
    create a backup region anymore and also no need to copy the contents of
    the first 640k to it.
    
    Remove all the code related to handling that backup region.
    
     [ bp: Massage commit message. ]
    
    Signed-off-by: Lianbo Jiang <lijiang@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: bhe@redhat.com
    Cc: Dave Young <dyoung@redhat.com>
    Cc: d.hatayama@fujitsu.com
    Cc: dhowells@redhat.com
    Cc: ebiederm@xmission.com
    Cc: horms@verge.net.au
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: JÃ¼rgen Gross <jgross@suse.com>
    Cc: kexec@lists.infradead.org
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: vgoyal@redhat.com
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20191108090027.11082-3-lijiang@redhat.com

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 5dcd438ad8f2..16e125a50b33 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -298,48 +298,6 @@ static void load_segments(void)
 		);
 }
 
-#ifdef CONFIG_KEXEC_FILE
-/* Update purgatory as needed after various image segments have been prepared */
-static int arch_update_purgatory(struct kimage *image)
-{
-	int ret = 0;
-
-	if (!image->file_mode)
-		return 0;
-
-	/* Setup copying of backup region */
-	if (image->type == KEXEC_TYPE_CRASH) {
-		ret = kexec_purgatory_get_set_symbol(image,
-				"purgatory_backup_dest",
-				&image->arch.backup_load_addr,
-				sizeof(image->arch.backup_load_addr), 0);
-		if (ret)
-			return ret;
-
-		ret = kexec_purgatory_get_set_symbol(image,
-				"purgatory_backup_src",
-				&image->arch.backup_src_start,
-				sizeof(image->arch.backup_src_start), 0);
-		if (ret)
-			return ret;
-
-		ret = kexec_purgatory_get_set_symbol(image,
-				"purgatory_backup_sz",
-				&image->arch.backup_src_sz,
-				sizeof(image->arch.backup_src_sz), 0);
-		if (ret)
-			return ret;
-	}
-
-	return ret;
-}
-#else /* !CONFIG_KEXEC_FILE */
-static inline int arch_update_purgatory(struct kimage *image)
-{
-	return 0;
-}
-#endif /* CONFIG_KEXEC_FILE */
-
 int machine_kexec_prepare(struct kimage *image)
 {
 	unsigned long start_pgtable;
@@ -353,11 +311,6 @@ int machine_kexec_prepare(struct kimage *image)
 	if (result)
 		return result;
 
-	/* update purgatory as needed */
-	result = arch_update_purgatory(image);
-	if (result)
-		return result;
-
 	return 0;
 }
 

commit 565eb5f8c5d379b6a6a3134c76b2fcfecdd007d3
Merge: b7d5c9239855 4eb5fec31e61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 9 11:52:34 2019 -0700

    Merge branch 'x86-kdump-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x865 kdump updates from Thomas Gleixner:
     "Yet more kexec/kdump updates:
    
       - Properly support kexec when AMD's memory encryption (SME) is
         enabled
    
       - Pass reserved e820 ranges to the kexec kernel so both PCI and SME
         can work"
    
    * 'x86-kdump-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      fs/proc/vmcore: Enable dumping of encrypted memory when SEV was active
      x86/kexec: Set the C-bit in the identity map page table when SEV is active
      x86/kexec: Do not map kexec area as decrypted when SEV is active
      x86/crash: Add e820 reserved ranges to kdump kernel's e820 table
      x86/mm: Rework ioremap resource mapping determination
      x86/e820, ioport: Add a new I/O resource descriptor IORES_DESC_RESERVED
      x86/mm: Create a workarea in the kernel for SME early encryption
      x86/mm: Identify the end of the kernel area to be reserved

commit b7d5c9239855f99762e8a547bea03a436e8a12e8
Merge: 608745f12462 8ff80fbe7e98
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 9 11:35:38 2019 -0700

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Thomas Gleixner:
     "Assorted updates to kexec/kdump:
    
       - Proper kexec support for 4/5-level paging and jumping from a
         5-level to a 4-level paging kernel.
    
       - Make the EFI support for kexec/kdump more robust
    
       - Enforce that the GDT is properly aligned instead of getting the
         alignment by chance"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/kdump/64: Restrict kdump kernel reservation to <64TB
      x86/kexec/64: Prevent kexec from 5-level paging to a 4-level only kernel
      x86/boot: Add xloadflags bits to check for 5-level paging support
      x86/boot: Make the GDT 8-byte aligned
      x86/kexec: Add the ACPI NVS region to the ident map
      x86/boot: Call get_rsdp_addr() after console_init()
      Revert "x86/boot: Disable RSDP parsing temporarily"
      x86/boot: Use efi_setup_data for searching RSDP on kexec-ed kernels
      x86/kexec: Add the EFI system tables and ACPI tables to the ident map

commit 85784d16c2cf172cf1ebaf2390d6b7c4045d659c
Author: Lianbo Jiang <lijiang@redhat.com>
Date:   Tue Apr 30 15:44:20 2019 +0800

    x86/kexec: Set the C-bit in the identity map page table when SEV is active
    
    When SEV is active, the second kernel image is loaded into encrypted
    memory. For that, make sure that when kexec builds the identity mapping
    page table, the memory is encrypted (i.e., _PAGE_ENC is set).
    
     [ bp: Sort local args and OR in _PAGE_ENC for more clarity. ]
    
    Co-developed-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Lianbo Jiang <lijiang@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: bhe@redhat.com
    Cc: dyoung@redhat.com
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: kexec@lists.infradead.org
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190430074421.7852-3-lijiang@redhat.com

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 3b38449028e0..16c37fe489bc 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -50,12 +50,13 @@ static void free_transition_pgtable(struct kimage *image)
 
 static int init_transition_pgtable(struct kimage *image, pgd_t *pgd)
 {
+	pgprot_t prot = PAGE_KERNEL_EXEC_NOENC;
+	unsigned long vaddr, paddr;
+	int result = -ENOMEM;
 	p4d_t *p4d;
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
-	unsigned long vaddr, paddr;
-	int result = -ENOMEM;
 
 	vaddr = (unsigned long)relocate_kernel;
 	paddr = __pa(page_address(image->control_code_page)+PAGE_SIZE);
@@ -92,7 +93,11 @@ static int init_transition_pgtable(struct kimage *image, pgd_t *pgd)
 		set_pmd(pmd, __pmd(__pa(pte) | _KERNPG_TABLE));
 	}
 	pte = pte_offset_kernel(pmd, vaddr);
-	set_pte(pte, pfn_pte(paddr >> PAGE_SHIFT, PAGE_KERNEL_EXEC_NOENC));
+
+	if (sev_active())
+		prot = PAGE_KERNEL_EXEC;
+
+	set_pte(pte, pfn_pte(paddr >> PAGE_SHIFT, prot));
 	return 0;
 err:
 	return result;
@@ -129,6 +134,11 @@ static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 	level4p = (pgd_t *)__va(start_pgtable);
 	clear_page(level4p);
 
+	if (sev_active()) {
+		info.page_flag   |= _PAGE_ENC;
+		info.kernpg_flag |= _PAGE_ENC;
+	}
+
 	if (direct_gbpages)
 		info.direct_gbpages = true;
 

commit 1a79c1b8a04153c4c387518967ce851f89e22733
Author: Lianbo Jiang <lijiang@redhat.com>
Date:   Tue Apr 30 15:44:19 2019 +0800

    x86/kexec: Do not map kexec area as decrypted when SEV is active
    
    When a virtual machine panics, its memory needs to be dumped for
    analysis. With memory encryption in the picture, special care must be
    taken when loading a kexec/kdump kernel in a SEV guest.
    
    A SEV guest starts and runs fully encrypted. In order to load a kexec
    kernel and initrd, arch_kexec_post_{alloc,free}_pages() need to not map
    areas as decrypted unconditionally but differentiate whether the kernel
    is running as a SEV guest and if so, leave kexec area encrypted.
    
     [ bp: Reduce commit message to the relevant information pertaining to
       this commit only. ]
    
    Co-developed-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Lianbo Jiang <lijiang@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: bhe@redhat.com
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: dyoung@redhat.com
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: kexec@lists.infradead.org
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190430074421.7852-2-lijiang@redhat.com

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index ceba408ea982..3b38449028e0 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -559,8 +559,20 @@ void arch_kexec_unprotect_crashkres(void)
 	kexec_mark_crashkres(false);
 }
 
+/*
+ * During a traditional boot under SME, SME will encrypt the kernel,
+ * so the SME kexec kernel also needs to be un-encrypted in order to
+ * replicate a normal SME boot.
+ *
+ * During a traditional boot under SEV, the kernel has already been
+ * loaded encrypted, so the SEV kexec kernel needs to be encrypted in
+ * order to replicate a normal SEV boot.
+ */
 int arch_kexec_post_alloc_pages(void *vaddr, unsigned int pages, gfp_t gfp)
 {
+	if (sev_active())
+		return 0;
+
 	/*
 	 * If SME is active we need to be sure that kexec pages are
 	 * not encrypted because when we boot to the new kernel the
@@ -571,6 +583,9 @@ int arch_kexec_post_alloc_pages(void *vaddr, unsigned int pages, gfp_t gfp)
 
 void arch_kexec_pre_free_pages(void *vaddr, unsigned int pages)
 {
+	if (sev_active())
+		return;
+
 	/*
 	 * If SME is active we need to reset the pages back to being
 	 * an encrypted mapping before freeing them.

commit 40b0b3f8fb2d8f55d13ceed41593d46689a6b496
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:46 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 230
    
    Based on 2 normalized pattern(s):
    
      this source code is licensed under the gnu general public license
      version 2 see the file copying for more details
    
      this source code is licensed under general public license version 2
      see
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 52 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.449021192@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index ceba408ea982..d7be2376ac0b 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -1,9 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * handle transition of Linux booting another kernel
  * Copyright (C) 2002-2005 Eric Biederman  <ebiederm@xmission.com>
- *
- * This source code is licensed under the GNU General Public License,
- * Version 2.  See the file COPYING for more details.
  */
 
 #define pr_fmt(fmt)	"kexec: " fmt

commit 5a949b38839e284b1307540c56b03caf57da9736
Author: Kairui Song <kasong@redhat.com>
Date:   Mon Jun 10 15:36:17 2019 +0800

    x86/kexec: Add the ACPI NVS region to the ident map
    
    With the recent addition of RSDP parsing in the decompression stage,
    a kexec-ed kernel now needs ACPI tables to be covered by the identity
    mapping. And in commit
    
      6bbeb276b71f ("x86/kexec: Add the EFI system tables and ACPI tables to the ident map")
    
    the ACPI tables memory region was added to the ident map.
    
    But some machines have only an ACPI NVS memory region and the ACPI
    tables are located in that region. In such case, the kexec-ed kernel
    will still fail when trying to access ACPI tables if they're not mapped.
    
    So add the NVS memory region to the ident map as well.
    
     [ bp: Massage. ]
    
    Fixes: 6bbeb276b71f ("x86/kexec: Add the EFI system tables and ACPI tables to the ident map")
    Suggested-by: Junichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Kairui Song <kasong@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Tested-by: Junichi Nomura <j-nomura@ce.jp.nec.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Chao Fan <fanc.fnst@cn.fujitsu.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: kexec@lists.infradead.org
    Cc: Lianbo Jiang <lijiang@redhat.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190610073617.19767-1-kasong@redhat.com

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 3c77bdf7b32a..b2b88dcaaf88 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -54,14 +54,26 @@ static int mem_region_callback(struct resource *res, void *arg)
 static int
 map_acpi_tables(struct x86_mapping_info *info, pgd_t *level4p)
 {
-	unsigned long flags = IORESOURCE_MEM | IORESOURCE_BUSY;
 	struct init_pgtable_data data;
+	unsigned long flags;
+	int ret;
 
 	data.info = info;
 	data.level4p = level4p;
 	flags = IORESOURCE_MEM | IORESOURCE_BUSY;
-	return walk_iomem_res_desc(IORES_DESC_ACPI_TABLES, flags, 0, -1,
-				   &data, mem_region_callback);
+
+	ret = walk_iomem_res_desc(IORES_DESC_ACPI_TABLES, flags, 0, -1,
+				  &data, mem_region_callback);
+	if (ret && ret != -EINVAL)
+		return ret;
+
+	/* ACPI tables could be located in ACPI Non-volatile Storage region */
+	ret = walk_iomem_res_desc(IORES_DESC_ACPI_NV_STORAGE, flags, 0, -1,
+				  &data, mem_region_callback);
+	if (ret && ret != -EINVAL)
+		return ret;
+
+	return 0;
 }
 #else
 static int map_acpi_tables(struct x86_mapping_info *info, pgd_t *level4p) { return 0; }

commit 6bbeb276b71f06c5267bfd154629b1bec82e7136
Author: Kairui Song <kasong@redhat.com>
Date:   Mon Apr 29 08:23:18 2019 +0800

    x86/kexec: Add the EFI system tables and ACPI tables to the ident map
    
    Currently, only the whole physical memory is identity-mapped for the
    kexec kernel and the regions reserved by firmware are ignored.
    
    However, the recent addition of RSDP parsing in the decompression stage
    and especially:
    
      33f0df8d843d ("x86/boot: Search for RSDP in the EFI tables")
    
    which tries to access EFI system tables and to dig out the RDSP address
    from there, becomes a problem because in certain configurations, they
    might not be mapped in the kexec'ed kernel's address space.
    
    What is more, this problem doesn't appear on all systems because the
    kexec kernel uses gigabyte pages to build the identity mapping. And
    the EFI system tables and ACPI tables can, depending on the system
    configuration, end up being mapped as part of all physical memory, if
    they share the same 1 GB area with the physical memory.
    
    Therefore, make sure they're always mapped.
    
     [ bp: productize half-baked patch:
       - rewrite commit message.
       - correct the map_acpi_tables() function name in the !ACPI case. ]
    
    Signed-off-by: Kairui Song <kasong@redhat.com>
    Signed-off-by: Baoquan He <bhe@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Tested-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Cc: dyoung@redhat.com
    Cc: fanc.fnst@cn.fujitsu.com
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: j-nomura@ce.jp.nec.com
    Cc: kexec@lists.infradead.org
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Lianbo Jiang <lijiang@redhat.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190429002318.GA25400@MiWiFi-R3L-srv

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index ceba408ea982..3c77bdf7b32a 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -18,6 +18,7 @@
 #include <linux/io.h>
 #include <linux/suspend.h>
 #include <linux/vmalloc.h>
+#include <linux/efi.h>
 
 #include <asm/init.h>
 #include <asm/pgtable.h>
@@ -29,6 +30,43 @@
 #include <asm/setup.h>
 #include <asm/set_memory.h>
 
+#ifdef CONFIG_ACPI
+/*
+ * Used while adding mapping for ACPI tables.
+ * Can be reused when other iomem regions need be mapped
+ */
+struct init_pgtable_data {
+	struct x86_mapping_info *info;
+	pgd_t *level4p;
+};
+
+static int mem_region_callback(struct resource *res, void *arg)
+{
+	struct init_pgtable_data *data = arg;
+	unsigned long mstart, mend;
+
+	mstart = res->start;
+	mend = mstart + resource_size(res) - 1;
+
+	return kernel_ident_mapping_init(data->info, data->level4p, mstart, mend);
+}
+
+static int
+map_acpi_tables(struct x86_mapping_info *info, pgd_t *level4p)
+{
+	unsigned long flags = IORESOURCE_MEM | IORESOURCE_BUSY;
+	struct init_pgtable_data data;
+
+	data.info = info;
+	data.level4p = level4p;
+	flags = IORESOURCE_MEM | IORESOURCE_BUSY;
+	return walk_iomem_res_desc(IORES_DESC_ACPI_TABLES, flags, 0, -1,
+				   &data, mem_region_callback);
+}
+#else
+static int map_acpi_tables(struct x86_mapping_info *info, pgd_t *level4p) { return 0; }
+#endif
+
 #ifdef CONFIG_KEXEC_FILE
 const struct kexec_file_ops * const kexec_file_loaders[] = {
 		&kexec_bzImage64_ops,
@@ -36,6 +74,31 @@ const struct kexec_file_ops * const kexec_file_loaders[] = {
 };
 #endif
 
+static int
+map_efi_systab(struct x86_mapping_info *info, pgd_t *level4p)
+{
+#ifdef CONFIG_EFI
+	unsigned long mstart, mend;
+
+	if (!efi_enabled(EFI_BOOT))
+		return 0;
+
+	mstart = (boot_params.efi_info.efi_systab |
+			((u64)boot_params.efi_info.efi_systab_hi<<32));
+
+	if (efi_enabled(EFI_64BIT))
+		mend = mstart + sizeof(efi_system_table_64_t);
+	else
+		mend = mstart + sizeof(efi_system_table_32_t);
+
+	if (!mstart)
+		return 0;
+
+	return kernel_ident_mapping_init(info, level4p, mstart, mend);
+#endif
+	return 0;
+}
+
 static void free_transition_pgtable(struct kimage *image)
 {
 	free_page((unsigned long)image->arch.p4d);
@@ -159,6 +222,18 @@ static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 			return result;
 	}
 
+	/*
+	 * Prepare EFI systab and ACPI tables for kexec kernel since they are
+	 * not covered by pfn_mapped.
+	 */
+	result = map_efi_systab(&info, level4p);
+	if (result)
+		return result;
+
+	result = map_acpi_tables(&info, level4p);
+	if (result)
+		return result;
+
 	return init_transition_pgtable(image, level4p);
 }
 

commit 65f750e5457aef9a8085a99d613fea0430303e93
Author: Lianbo Jiang <lijiang@redhat.com>
Date:   Thu Jan 10 20:19:44 2019 +0800

    x86/kdump: Export the SME mask to vmcoreinfo
    
    On AMD SME machines, makedumpfile tools need to know whether the crashed
    kernel was encrypted.
    
    If SME is enabled in the first kernel, the crashed kernel's page table
    entries (pgd/pud/pmd/pte) contain the memory encryption mask which
    makedumpfile needs to remove in order to obtain the true physical
    address.
    
    Export that mask in a vmcoreinfo variable.
    
     [ bp: Massage commit message and move define at the end of the
       function. ]
    
    Signed-off-by: Lianbo Jiang <lijiang@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: anderson@redhat.com
    Cc: k-hagio@ab.jp.nec.com
    Cc: kexec@lists.infradead.org
    Cc: linux-doc@vger.kernel.org
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190110121944.6050-3-lijiang@redhat.com

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 4c8acdfdc5a7..ceba408ea982 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -352,6 +352,8 @@ void machine_kexec(struct kimage *image)
 
 void arch_crash_save_vmcoreinfo(void)
 {
+	u64 sme_mask = sme_me_mask;
+
 	VMCOREINFO_NUMBER(phys_base);
 	VMCOREINFO_SYMBOL(init_top_pgt);
 	vmcoreinfo_append_str("NUMBER(pgtable_l5_enabled)=%d\n",
@@ -364,6 +366,7 @@ void arch_crash_save_vmcoreinfo(void)
 	vmcoreinfo_append_str("KERNELOFFSET=%lx\n",
 			      kaslr_offset());
 	VMCOREINFO_NUMBER(KERNEL_IMAGE_SIZE);
+	VMCOREINFO_NUMBER(sme_mask);
 }
 
 /* arch-dependent functionality related to kexec file-based syscall */

commit ed7588d5dc6f5e7202fb9bbeb14d94706ba225d7
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri May 18 13:35:24 2018 +0300

    x86/mm: Stop pretending pgtable_l5_enabled is a variable
    
    pgtable_l5_enabled is defined using cpu_feature_enabled() but we refer
    to it as a variable. This is misleading.
    
    Make pgtable_l5_enabled() a function.
    
    We cannot literally define it as a function due to circular dependencies
    between header files. Function-alike macros is close enough.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20180518103528.59260-4-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 6010449ca6d2..4c8acdfdc5a7 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -354,7 +354,8 @@ void arch_crash_save_vmcoreinfo(void)
 {
 	VMCOREINFO_NUMBER(phys_base);
 	VMCOREINFO_SYMBOL(init_top_pgt);
-	VMCOREINFO_NUMBER(pgtable_l5_enabled);
+	vmcoreinfo_append_str("NUMBER(pgtable_l5_enabled)=%d\n",
+			pgtable_l5_enabled());
 
 #ifdef CONFIG_NUMA
 	VMCOREINFO_SYMBOL(node_data);

commit a466ef76b815b86748d9870ef2a430af7b39c710
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Wed May 9 19:42:20 2018 +0900

    x86/kexec: Avoid double free_page() upon do_kexec_load() failure
    
    >From ff82bedd3e12f0d3353282054ae48c3bd8c72012 Mon Sep 17 00:00:00 2001
    From: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Date: Wed, 9 May 2018 12:12:39 +0900
    Subject: [PATCH v3] x86/kexec: avoid double free_page() upon do_kexec_load() failure.
    
    syzbot is reporting crashes after memory allocation failure inside
    do_kexec_load() [1]. This is because free_transition_pgtable() is called
    by both init_transition_pgtable() and machine_kexec_cleanup() when memory
    allocation failed inside init_transition_pgtable().
    
    Regarding 32bit code, machine_kexec_free_page_tables() is called by both
    machine_kexec_alloc_page_tables() and machine_kexec_cleanup() when memory
    allocation failed inside machine_kexec_alloc_page_tables().
    
    Fix this by leaving the error handling to machine_kexec_cleanup()
    (and optionally setting NULL after free_page()).
    
    [1] https://syzkaller.appspot.com/bug?id=91e52396168cf2bdd572fe1e1bc0bc645c1c6b40
    
    Fixes: f5deb79679af6eb4 ("x86: kexec: Use one page table in x86_64 machine_kexec")
    Fixes: 92be3d6bdf2cb349 ("kexec/i386: allocate page table pages dynamically")
    Reported-by: syzbot <syzbot+d96f60296ef613fe1d69@syzkaller.appspotmail.com>
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Baoquan He <bhe@redhat.com>
    Cc: thomas.lendacky@amd.com
    Cc: prudo@linux.vnet.ibm.com
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: syzkaller-bugs@googlegroups.com
    Cc: takahiro.akashi@linaro.org
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: akpm@linux-foundation.org
    Cc: dyoung@redhat.com
    Cc: kirill.shutemov@linux.intel.com
    Link: https://lkml.kernel.org/r/201805091942.DGG12448.tMFVFSJFQOOLHO@I-love.SAKURA.ne.jp

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index a5e55d832d0a..6010449ca6d2 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -39,9 +39,13 @@ const struct kexec_file_ops * const kexec_file_loaders[] = {
 static void free_transition_pgtable(struct kimage *image)
 {
 	free_page((unsigned long)image->arch.p4d);
+	image->arch.p4d = NULL;
 	free_page((unsigned long)image->arch.pud);
+	image->arch.pud = NULL;
 	free_page((unsigned long)image->arch.pmd);
+	image->arch.pmd = NULL;
 	free_page((unsigned long)image->arch.pte);
+	image->arch.pte = NULL;
 }
 
 static int init_transition_pgtable(struct kimage *image, pgd_t *pgd)
@@ -91,7 +95,6 @@ static int init_transition_pgtable(struct kimage *image, pgd_t *pgd)
 	set_pte(pte, pfn_pte(paddr >> PAGE_SHIFT, PAGE_KERNEL_EXEC_NOENC));
 	return 0;
 err:
-	free_transition_pgtable(image);
 	return result;
 }
 

commit 8da0b724959ccd3f8435214ebdaf1aef548967bb
Author: Philipp Rudo <prudo@linux.vnet.ibm.com>
Date:   Fri Apr 13 15:36:39 2018 -0700

    kernel/kexec_file.c: remove mis-use of sh_offset field during purgatory load
    
    The current code uses the sh_offset field in purgatory_info->sechdrs to
    store a pointer to the current load address of the section.  Depending
    whether the section will be loaded or not this is either a pointer into
    purgatory_info->purgatory_buf or kexec_purgatory.  This is not only a
    violation of the ELF standard but also makes the code very hard to
    understand as you cannot tell if the memory you are using is read-only
    or not.
    
    Remove this misuse and store the offset of the section in
    pugaroty_info->purgatory_buf in sh_offset.
    
    Link: http://lkml.kernel.org/r/20180321112751.22196-10-prudo@linux.vnet.ibm.com
    Signed-off-by: Philipp Rudo <prudo@linux.vnet.ibm.com>
    Acked-by: Dave Young <dyoung@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 63dea30c8e02..a5e55d832d0a 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -417,13 +417,15 @@ int arch_kexec_apply_relocations_add(struct purgatory_info *pi,
 		 * rel[i].r_offset contains byte offset from beginning
 		 * of section to the storage unit affected.
 		 *
-		 * This is location to update (->sh_offset). This is temporary
-		 * buffer where section is currently loaded. This will finally
-		 * be loaded to a different address later, pointed to by
+		 * This is location to update. This is temporary buffer
+		 * where section is currently loaded. This will finally be
+		 * loaded to a different address later, pointed to by
 		 * ->sh_addr. kexec takes care of moving it
 		 *  (kexec_load_segment()).
 		 */
-		location = (void *)(section->sh_offset + rel[i].r_offset);
+		location = pi->purgatory_buf;
+		location += section->sh_offset;
+		location += rel[i].r_offset;
 
 		/* Final address of the location */
 		address = section->sh_addr + rel[i].r_offset;

commit 8aec395b8478310521031157ef5d44ef19c2c581
Author: Philipp Rudo <prudo@linux.vnet.ibm.com>
Date:   Fri Apr 13 15:36:24 2018 -0700

    kernel/kexec_file.c: use read-only sections in arch_kexec_apply_relocations*
    
    When the relocations are applied to the purgatory only the section the
    relocations are applied to is writable.  The other sections, i.e.  the
    symtab and .rel/.rela, are in read-only kexec_purgatory.  Highlight this
    by marking the corresponding variables as 'const'.
    
    While at it also change the signatures of arch_kexec_apply_relocations* to
    take section pointers instead of just the index of the relocation section.
    This removes the second lookup and sanity check of the sections in arch
    code.
    
    Link: http://lkml.kernel.org/r/20180321112751.22196-6-prudo@linux.vnet.ibm.com
    Signed-off-by: Philipp Rudo <prudo@linux.vnet.ibm.com>
    Acked-by: Dave Young <dyoung@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index c51d2cf27d93..63dea30c8e02 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -382,52 +382,36 @@ void *arch_kexec_kernel_image_load(struct kimage *image)
 /*
  * Apply purgatory relocations.
  *
- * ehdr: Pointer to elf headers
- * sechdrs: Pointer to section headers.
- * relsec: section index of SHT_RELA section.
+ * @pi:		Purgatory to be relocated.
+ * @section:	Section relocations applying to.
+ * @relsec:	Section containing RELAs.
+ * @symtabsec:	Corresponding symtab.
  *
  * TODO: Some of the code belongs to generic code. Move that in kexec.c.
  */
-int arch_kexec_apply_relocations_add(const Elf64_Ehdr *ehdr,
-				     Elf64_Shdr *sechdrs, unsigned int relsec)
+int arch_kexec_apply_relocations_add(struct purgatory_info *pi,
+				     Elf_Shdr *section, const Elf_Shdr *relsec,
+				     const Elf_Shdr *symtabsec)
 {
 	unsigned int i;
 	Elf64_Rela *rel;
 	Elf64_Sym *sym;
 	void *location;
-	Elf64_Shdr *section, *symtabsec;
 	unsigned long address, sec_base, value;
 	const char *strtab, *name, *shstrtab;
+	const Elf_Shdr *sechdrs;
 
-	/*
-	 * ->sh_offset has been modified to keep the pointer to section
-	 * contents in memory
-	 */
-	rel = (void *)sechdrs[relsec].sh_offset;
-
-	/* Section to which relocations apply */
-	section = &sechdrs[sechdrs[relsec].sh_info];
-
-	pr_debug("Applying relocate section %u to %u\n", relsec,
-		 sechdrs[relsec].sh_info);
-
-	/* Associated symbol table */
-	symtabsec = &sechdrs[sechdrs[relsec].sh_link];
-
-	/* String table */
-	if (symtabsec->sh_link >= ehdr->e_shnum) {
-		/* Invalid strtab section number */
-		pr_err("Invalid string table section index %d\n",
-		       symtabsec->sh_link);
-		return -ENOEXEC;
-	}
+	/* String & section header string table */
+	sechdrs = (void *)pi->ehdr + pi->ehdr->e_shoff;
+	strtab = (char *)pi->ehdr + sechdrs[symtabsec->sh_link].sh_offset;
+	shstrtab = (char *)pi->ehdr + sechdrs[pi->ehdr->e_shstrndx].sh_offset;
 
-	strtab = (char *)sechdrs[symtabsec->sh_link].sh_offset;
+	rel = (void *)pi->ehdr + relsec->sh_offset;
 
-	/* section header string table */
-	shstrtab = (char *)sechdrs[ehdr->e_shstrndx].sh_offset;
+	pr_debug("Applying relocate section %s to %u\n",
+		 shstrtab + relsec->sh_name, relsec->sh_info);
 
-	for (i = 0; i < sechdrs[relsec].sh_size / sizeof(*rel); i++) {
+	for (i = 0; i < relsec->sh_size / sizeof(*rel); i++) {
 
 		/*
 		 * rel[i].r_offset contains byte offset from beginning
@@ -450,8 +434,8 @@ int arch_kexec_apply_relocations_add(const Elf64_Ehdr *ehdr,
 		 * to apply. ELF64_R_SYM() and ELF64_R_TYPE() macros get
 		 * these respectively.
 		 */
-		sym = (Elf64_Sym *)symtabsec->sh_offset +
-				ELF64_R_SYM(rel[i].r_info);
+		sym = (void *)pi->ehdr + symtabsec->sh_offset;
+		sym += ELF64_R_SYM(rel[i].r_info);
 
 		if (sym->st_name)
 			name = strtab + sym->st_name;
@@ -474,12 +458,12 @@ int arch_kexec_apply_relocations_add(const Elf64_Ehdr *ehdr,
 
 		if (sym->st_shndx == SHN_ABS)
 			sec_base = 0;
-		else if (sym->st_shndx >= ehdr->e_shnum) {
+		else if (sym->st_shndx >= pi->ehdr->e_shnum) {
 			pr_err("Invalid section %d for symbol %s\n",
 			       sym->st_shndx, name);
 			return -ENOEXEC;
 		} else
-			sec_base = sechdrs[sym->st_shndx].sh_addr;
+			sec_base = pi->sechdrs[sym->st_shndx].sh_addr;
 
 		value = sym->st_value;
 		value += sec_base;

commit 9ec4ecef0af7790551109283ca039a7c52de343c
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Fri Apr 13 15:35:49 2018 -0700

    kexec_file,x86,powerpc: factor out kexec_file_ops functions
    
    As arch_kexec_kernel_image_{probe,load}(),
    arch_kimage_file_post_load_cleanup() and arch_kexec_kernel_verify_sig()
    are almost duplicated among architectures, they can be commonalized with
    an architecture-defined kexec_file_ops array.  So let's factor them out.
    
    Link: http://lkml.kernel.org/r/20180306102303.9063-3-takahiro.akashi@linaro.org
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Acked-by: Dave Young <dyoung@redhat.com>
    Tested-by: Dave Young <dyoung@redhat.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 93bd4fb603d1..c51d2cf27d93 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -30,8 +30,9 @@
 #include <asm/set_memory.h>
 
 #ifdef CONFIG_KEXEC_FILE
-static struct kexec_file_ops *kexec_file_loaders[] = {
+const struct kexec_file_ops * const kexec_file_loaders[] = {
 		&kexec_bzImage64_ops,
+		NULL
 };
 #endif
 
@@ -364,27 +365,6 @@ void arch_crash_save_vmcoreinfo(void)
 /* arch-dependent functionality related to kexec file-based syscall */
 
 #ifdef CONFIG_KEXEC_FILE
-int arch_kexec_kernel_image_probe(struct kimage *image, void *buf,
-				  unsigned long buf_len)
-{
-	int i, ret = -ENOEXEC;
-	struct kexec_file_ops *fops;
-
-	for (i = 0; i < ARRAY_SIZE(kexec_file_loaders); i++) {
-		fops = kexec_file_loaders[i];
-		if (!fops || !fops->probe)
-			continue;
-
-		ret = fops->probe(buf, buf_len);
-		if (!ret) {
-			image->fops = fops;
-			return ret;
-		}
-	}
-
-	return ret;
-}
-
 void *arch_kexec_kernel_image_load(struct kimage *image)
 {
 	vfree(image->arch.elf_headers);
@@ -399,27 +379,6 @@ void *arch_kexec_kernel_image_load(struct kimage *image)
 				 image->cmdline_buf_len);
 }
 
-int arch_kimage_file_post_load_cleanup(struct kimage *image)
-{
-	if (!image->fops || !image->fops->cleanup)
-		return 0;
-
-	return image->fops->cleanup(image->image_loader_data);
-}
-
-#ifdef CONFIG_KEXEC_VERIFY_SIG
-int arch_kexec_kernel_verify_sig(struct kimage *image, void *kernel,
-				 unsigned long kernel_len)
-{
-	if (!image->fops || !image->fops->verify_sig) {
-		pr_debug("kernel loader does not support signature verification.");
-		return -EKEYREJECTED;
-	}
-
-	return image->fops->verify_sig(kernel, kernel_len);
-}
-#endif
-
 /*
  * Apply purgatory relocations.
  *

commit d22fff81418edc92be534cad8d59da914049bf69
Merge: 986b37c0ae4f eaeb8e76cd57
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 2 15:45:30 2018 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Ingo Molnar:
    
     - Extend the memmap= boot parameter syntax to allow the redeclaration
       and dropping of existing ranges, and to support all e820 range types
       (Jan H. SchÃ¶nherr)
    
     - Improve the W+X boot time security checks to remove false positive
       warnings on Xen (Jan Beulich)
    
     - Support booting as Xen PVH guest (Juergen Gross)
    
     - Improved 5-level paging (LA57) support, in particular it's possible
       now to have a single kernel image for both 4-level and 5-level
       hardware (Kirill A. Shutemov)
    
     - AMD hardware RAM encryption support (SME/SEV) fixes (Tom Lendacky)
    
     - Preparatory commits for hardware-encrypted RAM support on Intel CPUs.
       (Kirill A. Shutemov)
    
     - Improved Intel-MID support (Andy Shevchenko)
    
     - Show EFI page tables in page_tables debug files (Andy Lutomirski)
    
     - ... plus misc fixes and smaller cleanups
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (56 commits)
      x86/cpu/tme: Fix spelling: "configuation" -> "configuration"
      x86/boot: Fix SEV boot failure from change to __PHYSICAL_MASK_SHIFT
      x86/mm: Update comment in detect_tme() regarding x86_phys_bits
      x86/mm/32: Remove unused node_memmap_size_bytes() & CONFIG_NEED_NODE_MEMMAP_SIZE logic
      x86/mm: Remove pointless checks in vmalloc_fault
      x86/platform/intel-mid: Add special handling for ACPI HW reduced platforms
      ACPI, x86/boot: Introduce the ->reduced_hw_early_init() ACPI callback
      ACPI, x86/boot: Split out acpi_generic_reduce_hw_init() and export
      x86/pconfig: Provide defines and helper to run MKTME_KEY_PROG leaf
      x86/pconfig: Detect PCONFIG targets
      x86/tme: Detect if TME and MKTME is activated by BIOS
      x86/boot/compressed/64: Handle 5-level paging boot if kernel is above 4G
      x86/boot/compressed/64: Use page table in trampoline memory
      x86/boot/compressed/64: Use stack from trampoline memory
      x86/boot/compressed/64: Make sure we have a 32-bit code segment
      x86/mm: Do not use paravirtualized calls in native_set_p4d()
      kdump, vmcoreinfo: Export pgtable_l5_enabled value
      x86/boot/compressed/64: Prepare new top-level page table for trampoline
      x86/boot/compressed/64: Set up trampoline memory
      x86/boot/compressed/64: Save and restore trampoline memory
      ...

commit 2451d1e59d5a154a42bcf02e0bfeebb01d8df1e0
Merge: 67dbfc142310 e25283bf83bd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 2 13:38:43 2018 -0700

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 apic updates from Ingo Molnar:
     "The main x86 APIC/IOAPIC changes in this cycle were:
    
       - Robustify kexec support to more carefully restore IRQ hardware
         state before calling into kexec/kdump kernels. (Baoquan He)
    
       - Clean up the local APIC code a bit (Dou Liyang)
    
       - Remove unused callbacks (David Rientjes)"
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/apic: Finish removing unused callbacks
      x86/apic: Drop logical_smp_processor_id() inline
      x86/apic: Modernize the pending interrupt code
      x86/apic: Move pending interrupt check code into it's own function
      x86/apic: Set up through-local-APIC mode on the boot CPU if 'noapic' specified
      x86/apic: Rename variables and functions related to x86_io_apic_ops
      x86/apic: Remove the (now) unused disable_IO_APIC() function
      x86/apic: Fix restoring boot IRQ mode in reboot and kexec/kdump
      x86/apic: Split disable_IO_APIC() into two functions to fix CONFIG_KEXEC_JUMP=y
      x86/apic: Split out restore_boot_irq_mode() from disable_IO_APIC()
      x86/apic: Make setup_local_APIC() static
      x86/apic: Simplify init_bsp_APIC() usage
      x86/x2apic: Mark set_x2apic_phys_mode() as __init

commit c100a583601d357f923c41af5434dc1f8d07890f
Author: Baoquan He <bhe@redhat.com>
Date:   Fri Mar 2 13:18:01 2018 +0800

    kdump, vmcoreinfo: Export pgtable_l5_enabled value
    
    User-space utilities examining crash-kernels need to know if the
    crashed kernel was in 5-level paging mode or not.
    
    So write 'pgtable_l5_enabled' to vmcoreinfo, which covers these
    three cases:
    
      pgtable_l5_enabled == 0 when:
       - Compiled with !CONFIG_X86_5LEVEL
       - Compiled with CONFIG_X86_5LEVEL=y while CPU has no 'la57' flag
    
      pgtable_l5_enabled != 0 when:
       - Compiled with CONFIG_X86_5LEVEL=y and CPU has 'la57' flag
    
    Signed-off-by: Baoquan He <bhe@redhat.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: douly.fnst@cn.fujitsu.com
    Cc: dyoung@redhat.com
    Cc: ebiederm@xmission.com
    Cc: kirill.shutemov@linux.intel.com
    Cc: vgoyal@redhat.com
    Link: http://lkml.kernel.org/r/20180302051801.19594-1-bhe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 3b7427aa7d85..02f913cb27b5 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -350,6 +350,7 @@ void arch_crash_save_vmcoreinfo(void)
 {
 	VMCOREINFO_NUMBER(phys_base);
 	VMCOREINFO_SYMBOL(init_top_pgt);
+	VMCOREINFO_NUMBER(pgtable_l5_enabled);
 
 #ifdef CONFIG_NUMA
 	VMCOREINFO_SYMBOL(node_data);

commit b21ebf2fb4cde1618915a97cc773e287ff49173e
Author: H.J. Lu <hjl.tools@gmail.com>
Date:   Wed Feb 7 14:20:09 2018 -0800

    x86: Treat R_X86_64_PLT32 as R_X86_64_PC32
    
    On i386, there are 2 types of PLTs, PIC and non-PIC.  PIE and shared
    objects must use PIC PLT.  To use PIC PLT, you need to load
    _GLOBAL_OFFSET_TABLE_ into EBX first.  There is no need for that on
    x86-64 since x86-64 uses PC-relative PLT.
    
    On x86-64, for 32-bit PC-relative branches, we can generate PLT32
    relocation, instead of PC32 relocation, which can also be used as
    a marker for 32-bit PC-relative branches.  Linker can always reduce
    PLT32 relocation to PC32 if function is defined locally.   Local
    functions should use PC32 relocation.  As far as Linux kernel is
    concerned, R_X86_64_PLT32 can be treated the same as R_X86_64_PC32
    since Linux kernel doesn't use PLT.
    
    R_X86_64_PLT32 for 32-bit PC-relative branches has been enabled in
    binutils master branch which will become binutils 2.31.
    
    [ hjl is working on having better documentation on this all, but a few
      more notes from him:
    
       "PLT32 relocation is used as marker for PC-relative branches. Because
        of EBX, it looks odd to generate PLT32 relocation on i386 when EBX
        doesn't have GOT.
    
        As for symbol resolution, PLT32 and PC32 relocations are almost
        interchangeable. But when linker sees PLT32 relocation against a
        protected symbol, it can resolved locally at link-time since it is
        used on a branch instruction. Linker can't do that for PC32
        relocation"
    
      but for the kernel use, the two are basically the same, and this
      commit gets things building and working with the current binutils
      master   - Linus ]
    
    Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 1f790cf9d38f..3b7427aa7d85 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -542,6 +542,7 @@ int arch_kexec_apply_relocations_add(const Elf64_Ehdr *ehdr,
 				goto overflow;
 			break;
 		case R_X86_64_PC32:
+		case R_X86_64_PLT32:
 			value -= (u64)address;
 			*(u32 *)location = value;
 			break;

commit 50374b96d2d30c03c8d42b3f8846d8938748d454
Author: Baoquan He <bhe@redhat.com>
Date:   Wed Feb 14 13:46:54 2018 +0800

    x86/apic: Remove the (now) unused disable_IO_APIC() function
    
    No one uses it anymore.
    
    Signed-off-by: Baoquan He <bhe@redhat.com>
    Reviewed-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: douly.fnst@cn.fujitsu.com
    Cc: joro@8bytes.org
    Cc: prarit@redhat.com
    Cc: uobergfe@redhat.com
    Link: http://lkml.kernel.org/r/20180214054656.3780-5-bhe@redhat.com
    [ Rewrote the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 2ab14b9c1a89..5ffbc55ea80f 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -293,9 +293,8 @@ void machine_kexec(struct kimage *image)
 		/*
 		 * We need to put APICs in legacy mode so that we can
 		 * get timer interrupts in second kernel. kexec/kdump
-		 * paths already have calls to disable_IO_APIC() in
-		 * one form or other. kexec jump path also need
-		 * one.
+		 * paths already have calls to restore_boot_irq_mode()
+		 * in one form or other. kexec jump path also need one.
 		 */
 		clear_IO_APIC();
 		restore_boot_irq_mode();

commit 3c9e76dbea004b2c7c3ce872022ceaf5ff0dae79
Author: Baoquan He <bhe@redhat.com>
Date:   Wed Feb 14 13:46:52 2018 +0800

    x86/apic: Split disable_IO_APIC() into two functions to fix CONFIG_KEXEC_JUMP=y
    
    Split  following patches disable_IO_APIC() will be broken up into
    clear_IO_APIC() and restore_boot_irq_mode().
    
    These two functions will be called separately where they are needed
    to fix a regression introduced by:
    
      522e66464467 ("x86/apic: Disable I/O APIC before shutdown of the local APIC").
    
    While the CONFIG_KEXEC_JUMP=y code doesn't call lapic_shutdown() before jump
    like kexec/kdump, so it's not impacted by commit 522e66464467.
    
    Hence here change clear_IO_APIC() as public, and replace disable_IO_APIC()
    with clear_IO_APIC() and restore_boot_irq_mode() to keep CONFIG_KEXEC_JUMP=y
    code unchanged in essence. No functional change.
    
    Signed-off-by: Baoquan He <bhe@redhat.com>
    Reviewed-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: douly.fnst@cn.fujitsu.com
    Cc: joro@8bytes.org
    Cc: prarit@redhat.com
    Cc: uobergfe@redhat.com
    Link: http://lkml.kernel.org/r/20180214054656.3780-3-bhe@redhat.com
    [ Rewrote the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 1f790cf9d38f..2ab14b9c1a89 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -297,7 +297,8 @@ void machine_kexec(struct kimage *image)
 		 * one form or other. kexec jump path also need
 		 * one.
 		 */
-		disable_IO_APIC();
+		clear_IO_APIC();
+		restore_boot_irq_mode();
 #endif
 	}
 

commit 4e237903f95db585b976e7311de2bfdaaf0f6e31
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Fri Jul 28 11:01:16 2017 -0500

    x86/mm, kexec: Fix memory corruption with SME on successive kexecs
    
    After issuing successive kexecs it was found that the SHA hash failed
    verification when booting the kexec'd kernel.  When SME is enabled, the
    change from using pages that were marked encrypted to now being marked as
    not encrypted (through new identify mapped page tables) results in memory
    corruption if there are any cache entries for the previously encrypted
    pages. This is because separate cache entries can exist for the same
    physical location but tagged both with and without the encryption bit.
    
    To prevent this, issue a wbinvd if SME is active before copying the pages
    from the source location to the destination location to clear any possible
    cache entry conflicts.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: <kexec@lists.infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/e7fb8610af3a93e8f8ae6f214cd9249adc0df2b4.1501186516.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 9cf8daacc046..1f790cf9d38f 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -335,7 +335,8 @@ void machine_kexec(struct kimage *image)
 	image->start = relocate_kernel((unsigned long)image->head,
 				       (unsigned long)page_list,
 				       image->start,
-				       image->preserve_context);
+				       image->preserve_context,
+				       sme_active());
 
 #ifdef CONFIG_KEXEC_JUMP
 	if (image->preserve_context)

commit bba4ed011a52d494aa7ef5e08cf226709bbf3f60
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:28 2017 -0500

    x86/mm, kexec: Allow kexec to be used with SME
    
    Provide support so that kexec can be used to boot a kernel when SME is
    enabled.
    
    Support is needed to allocate pages for kexec without encryption.  This
    is needed in order to be able to reboot in the kernel in the same manner
    as originally booted.
    
    Additionally, when shutting down all of the CPUs we need to be sure to
    flush the caches and then halt. This is needed when booting from a state
    where SME was not active into a state where SME is active (or vice-versa).
    Without these steps, it is possible for cache lines to exist for the same
    physical location but tagged both with and without the encryption bit. This
    can cause random memory corruption when caches are flushed depending on
    which cacheline is written last.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: <kexec@lists.infradead.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/b95ff075db3e7cd545313f2fb609a49619a09625.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index cb0a30473c23..9cf8daacc046 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -87,7 +87,7 @@ static int init_transition_pgtable(struct kimage *image, pgd_t *pgd)
 		set_pmd(pmd, __pmd(__pa(pte) | _KERNPG_TABLE));
 	}
 	pte = pte_offset_kernel(pmd, vaddr);
-	set_pte(pte, pfn_pte(paddr >> PAGE_SHIFT, PAGE_KERNEL_EXEC));
+	set_pte(pte, pfn_pte(paddr >> PAGE_SHIFT, PAGE_KERNEL_EXEC_NOENC));
 	return 0;
 err:
 	free_transition_pgtable(image);
@@ -115,6 +115,7 @@ static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 		.alloc_pgt_page	= alloc_pgt_page,
 		.context	= image,
 		.page_flag	= __PAGE_KERNEL_LARGE_EXEC,
+		.kernpg_flag	= _KERNPG_TABLE_NOENC,
 	};
 	unsigned long mstart, mend;
 	pgd_t *level4p;
@@ -602,3 +603,22 @@ void arch_kexec_unprotect_crashkres(void)
 {
 	kexec_mark_crashkres(false);
 }
+
+int arch_kexec_post_alloc_pages(void *vaddr, unsigned int pages, gfp_t gfp)
+{
+	/*
+	 * If SME is active we need to be sure that kexec pages are
+	 * not encrypted because when we boot to the new kernel the
+	 * pages won't be accessed encrypted (initially).
+	 */
+	return set_memory_decrypted((unsigned long)vaddr, pages);
+}
+
+void arch_kexec_pre_free_pages(void *vaddr, unsigned int pages)
+{
+	/*
+	 * If SME is active we need to reset the pages back to being
+	 * an encrypted mapping before freeing them.
+	 */
+	set_memory_encrypted((unsigned long)vaddr, pages);
+}

commit 65ade2f872b474fa8a04c2d397783350326634e6
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Jun 6 14:31:27 2017 +0300

    x86/boot/64: Rename init_level4_pgt and early_level4_pgt
    
    With CONFIG_X86_5LEVEL=y, level 4 is no longer top level of page tables.
    
    Let's give these variable more generic names: init_top_pgt and
    early_top_pgt.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20170606113133.22974-9-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 6f5ca4ebe6e5..cb0a30473c23 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -347,7 +347,7 @@ void machine_kexec(struct kimage *image)
 void arch_crash_save_vmcoreinfo(void)
 {
 	VMCOREINFO_NUMBER(phys_base);
-	VMCOREINFO_SYMBOL(init_level4_pgt);
+	VMCOREINFO_SYMBOL(init_top_pgt);
 
 #ifdef CONFIG_NUMA
 	VMCOREINFO_SYMBOL(node_data);

commit f1e0527d2db416dfdef9c55132fed7fa05910101
Merge: 5836e422e5ca fb8fb46c5628
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 12 10:11:50 2017 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Misc fixes:
    
       - two boot crash fixes
       - unwinder fixes
       - kexec related kernel direct mappings enhancements/fixes
       - more Clang support quirks
       - minor cleanups
       - Documentation fixes"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/intel_rdt: Fix a typo in Documentation
      x86/build: Don't add -maccumulate-outgoing-args w/o compiler support
      x86/boot/32: Fix UP boot on Quark and possibly other platforms
      x86/mm/32: Set the '__vmalloc_start_set' flag in initmem_init()
      x86/kexec/64: Use gbpages for identity mappings if available
      x86/mm: Add support for gbpages to kernel_ident_mapping_init()
      x86/boot: Declare error() as noreturn
      x86/mm/kaslr: Use the _ASM_MUL macro for multiplication to work around Clang incompatibility
      x86/mm: Fix boot crash caused by incorrect loop count calculation in sync_global_pgds()
      x86/asm: Don't use RBP as a temporary register in csum_partial_copy_generic()
      x86/microcode/AMD: Remove redundant NULL check on mc

commit d11636511ed97ceda66a08ecff99f100e1107b76
Author: Laura Abbott <labbott@redhat.com>
Date:   Mon May 8 15:58:11 2017 -0700

    x86: use set_memory.h header
    
    set_memory_* functions have moved to set_memory.h.  Switch to this
    explicitly.
    
    Link: http://lkml.kernel.org/r/1488920133-27229-6-git-send-email-labbott@redhat.com
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 085c3b300d32..ce640428d6fe 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -27,6 +27,7 @@
 #include <asm/debugreg.h>
 #include <asm/kexec-bzimage64.h>
 #include <asm/setup.h>
+#include <asm/set_memory.h>
 
 #ifdef CONFIG_KEXEC_FILE
 static struct kexec_file_ops *kexec_file_loaders[] = {

commit 8638100c52bb7782462b14aad102a4aaf0c7094c
Author: Xunlei Pang <xlpang@redhat.com>
Date:   Thu May 4 09:42:51 2017 +0800

    x86/kexec/64: Use gbpages for identity mappings if available
    
    Kexec sets up all identity mappings before booting into the new
    kernel, and this will cause extra memory consumption for paging
    structures which is quite considerable on modern machines with
    huge memory sizes.
    
    E.g. on a 32TB machine that is kdumping, it could waste around
    128MB (around 4MB/TB) from the reserved memory after kexec sets
    all the identity mappings using the current 2MB page.
    
    Add to that the memory needed for the loaded kdump kernel, initramfs,
    etc., and it causes a kexec syscall -NOMEM failure.
    
    As a result, we had to enlarge reserved memory via "crashkernel=X"
    to work around this problem.
    
    This causes some trouble for distributions that use policies
    to evaluate the proper "crashkernel=X" value for users.
    
    So enable gbpages for kexec mappings.
    
    Signed-off-by: Xunlei Pang <xlpang@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: akpm@linux-foundation.org
    Cc: kexec@lists.infradead.org
    Link: http://lkml.kernel.org/r/1493862171-8799-2-git-send-email-xlpang@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 1d4f2b076545..c25d277d7d7e 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -122,6 +122,10 @@ static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 
 	level4p = (pgd_t *)__va(start_pgtable);
 	clear_page(level4p);
+
+	if (direct_gbpages)
+		info.direct_gbpages = true;
+
 	for (i = 0; i < nr_pfn_mapped; i++) {
 		mstart = pfn_mapped[i].start << PAGE_SHIFT;
 		mend   = pfn_mapped[i].end << PAGE_SHIFT;

commit 66aad4fdf2bf0af29c7decb4433dc5ec6c7c5451
Author: Xunlei Pang <xlpang@redhat.com>
Date:   Thu May 4 09:42:50 2017 +0800

    x86/mm: Add support for gbpages to kernel_ident_mapping_init()
    
    Kernel identity mappings on x86-64 kernels are created in two
    ways: by the early x86 boot code, or by kernel_ident_mapping_init().
    
    Native kernels (which is the dominant usecase) use the former,
    but the kexec and the hibernation code uses kernel_ident_mapping_init().
    
    There's a subtle difference between these two ways of how identity
    mappings are created, the current kernel_ident_mapping_init() code
    creates identity mappings always using 2MB page(PMD level) - while
    the native kernel boot path also utilizes gbpages where available.
    
    This difference is suboptimal both for performance and for memory
    usage: kernel_ident_mapping_init() needs to allocate pages for the
    page tables when creating the new identity mappings.
    
    This patch adds 1GB page(PUD level) support to kernel_ident_mapping_init()
    to address these concerns.
    
    The primary advantage would be better TLB coverage/performance,
    because we'd utilize 1GB TLBs instead of 2MB ones.
    
    It is also useful for machines with large number of memory to
    save paging structure allocations(around 4MB/TB using 2MB page)
    when setting identity mappings for all the memory, after using
    1GB page it will consume only 8KB/TB.
    
    ( Note that this change alone does not activate gbpages in kexec,
      we are doing that in a separate patch. )
    
    Signed-off-by: Xunlei Pang <xlpang@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: akpm@linux-foundation.org
    Cc: kexec@lists.infradead.org
    Link: http://lkml.kernel.org/r/1493862171-8799-1-git-send-email-xlpang@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 085c3b300d32..1d4f2b076545 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -113,7 +113,7 @@ static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 	struct x86_mapping_info info = {
 		.alloc_pgt_page	= alloc_pgt_page,
 		.context	= image,
-		.pmd_flag	= __PAGE_KERNEL_LARGE_EXEC,
+		.page_flag	= __PAGE_KERNEL_LARGE_EXEC,
 	};
 	unsigned long mstart, mend;
 	pgd_t *level4p;

commit 7f68904182e2f346c11b0acd74048181dc6615bb
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Mar 17 21:55:10 2017 +0300

    x86/kexec: Add 5-level paging support
    
    Handle additional page table level in the kexec code.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20170317185515.8636-2-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 857cdbd02867..085c3b300d32 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -36,6 +36,7 @@ static struct kexec_file_ops *kexec_file_loaders[] = {
 
 static void free_transition_pgtable(struct kimage *image)
 {
+	free_page((unsigned long)image->arch.p4d);
 	free_page((unsigned long)image->arch.pud);
 	free_page((unsigned long)image->arch.pmd);
 	free_page((unsigned long)image->arch.pte);
@@ -43,6 +44,7 @@ static void free_transition_pgtable(struct kimage *image)
 
 static int init_transition_pgtable(struct kimage *image, pgd_t *pgd)
 {
+	p4d_t *p4d;
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
@@ -53,13 +55,21 @@ static int init_transition_pgtable(struct kimage *image, pgd_t *pgd)
 	paddr = __pa(page_address(image->control_code_page)+PAGE_SIZE);
 	pgd += pgd_index(vaddr);
 	if (!pgd_present(*pgd)) {
+		p4d = (p4d_t *)get_zeroed_page(GFP_KERNEL);
+		if (!p4d)
+			goto err;
+		image->arch.p4d = p4d;
+		set_pgd(pgd, __pgd(__pa(p4d) | _KERNPG_TABLE));
+	}
+	p4d = p4d_offset(pgd, vaddr);
+	if (!p4d_present(*p4d)) {
 		pud = (pud_t *)get_zeroed_page(GFP_KERNEL);
 		if (!pud)
 			goto err;
 		image->arch.pud = pud;
-		set_pgd(pgd, __pgd(__pa(pud) | _KERNPG_TABLE));
+		set_p4d(p4d, __p4d(__pa(pud) | _KERNPG_TABLE));
 	}
-	pud = pud_offset(pgd, vaddr);
+	pud = pud_offset(p4d, vaddr);
 	if (!pud_present(*pud)) {
 		pmd = (pmd_t *)get_zeroed_page(GFP_KERNEL);
 		if (!pmd)

commit 40c50c1fecdf012a3bf055ec813f0ef2eda2749c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 10 13:17:18 2017 +0100

    kexec, x86/purgatory: Unbreak it and clean it up
    
    The purgatory code defines global variables which are referenced via a
    symbol lookup in the kexec code (core and arch).
    
    A recent commit addressing sparse warnings made these static and thereby
    broke kexec_file.
    
    Why did this happen? Simply because the whole machinery is undocumented and
    lacks any form of forward declarations. The variable names are unspecific
    and lack a prefix, so adding forward declarations creates shadow variables
    in the core code. Aside of that the code relies on magic constants and
    duplicate struct definitions with no way to ensure that these things stay
    in sync. The section placement of the purgatory variables happened by
    chance and not by design.
    
    Unbreak kexec and cleanup the mess:
    
     - Add proper forward declarations and document the usage
     - Use common struct definition
     - Use the proper common defines instead of magic constants
     - Add a purgatory_ prefix to have a proper name space
     - Use ARRAY_SIZE() instead of a homebrewn reimplementation
     - Add proper sections to the purgatory variables [ From Mike ]
    
    Fixes: 72042a8c7b01 ("x86/purgatory: Make functions and variables static")
    Reported-by: Mike Galbraith <<efault@gmx.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Nicholas Mc Guire <der.herr@hofr.at>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: "Tobin C. Harding" <me@tobin.cc>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1703101315140.3681@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 307b1f4543de..857cdbd02867 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -194,19 +194,22 @@ static int arch_update_purgatory(struct kimage *image)
 
 	/* Setup copying of backup region */
 	if (image->type == KEXEC_TYPE_CRASH) {
-		ret = kexec_purgatory_get_set_symbol(image, "backup_dest",
+		ret = kexec_purgatory_get_set_symbol(image,
+				"purgatory_backup_dest",
 				&image->arch.backup_load_addr,
 				sizeof(image->arch.backup_load_addr), 0);
 		if (ret)
 			return ret;
 
-		ret = kexec_purgatory_get_set_symbol(image, "backup_src",
+		ret = kexec_purgatory_get_set_symbol(image,
+				"purgatory_backup_src",
 				&image->arch.backup_src_start,
 				sizeof(image->arch.backup_src_start), 0);
 		if (ret)
 			return ret;
 
-		ret = kexec_purgatory_get_set_symbol(image, "backup_sz",
+		ret = kexec_purgatory_get_set_symbol(image,
+				"purgatory_backup_sz",
 				&image->arch.backup_src_sz,
 				sizeof(image->arch.backup_src_sz), 0);
 		if (ret)

commit 401721ecd1dcb0a428aa5d6832ee05ffbdbffbbe
Author: Baoquan He <bhe@redhat.com>
Date:   Wed Dec 14 15:04:20 2016 -0800

    kexec: export the value of phys_base instead of symbol address
    
    Currently in x86_64, the symbol address of phys_base is exported to
    vmcoreinfo.  Dave Anderson complained this is really useless for his
    Crash implementation.  Because in user-space utility Crash and
    Makedumpfile which exported vmcore information is mainly used for, value
    of phys_base is needed to covert virtual address of exported kernel
    symbol to physical address.  Especially init_level4_pgt, if we want to
    access and go over the page table to look up a PA corresponding to VA,
    firstly we need calculate
    
      page_dir = SYMBOL(init_level4_pgt) - __START_KERNEL_map + phys_base;
    
    Now in Crash and Makedumpfile, we have to analyze the vmcore elf program
    header to get value of phys_base.  As Dave said, it would be preferable
    if it were readily availabl in vmcoreinfo rather than depending upon the
    PT_LOAD semantics.
    
    Hence in this patch change to export the value of phys_base instead of
    its virtual address.
    
    And people also complained that KERNEL_IMAGE_SIZE exporting is x86_64
    only, should be moved into arch dependent function
    arch_crash_save_vmcoreinfo.  Do the moving in this patch.
    
    Link: http://lkml.kernel.org/r/1478568596-30060-2-git-send-email-bhe@redhat.com
    Signed-off-by: Baoquan He <bhe@redhat.com>
    Cc: Thomas Garnier <thgarnie@google.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Xunlei Pang <xlpang@redhat.com>
    Cc: HATAYAMA Daisuke <d.hatayama@jp.fujitsu.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Eugene Surovegin <surovegin@google.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Atsushi Kumagai <ats-kumagai@wm.jp.nec.com>
    Cc: Dave Anderson <anderson@redhat.com>
    Cc: Pratyush Anand <panand@redhat.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 5a294e48b185..307b1f4543de 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -328,7 +328,7 @@ void machine_kexec(struct kimage *image)
 
 void arch_crash_save_vmcoreinfo(void)
 {
-	VMCOREINFO_SYMBOL(phys_base);
+	VMCOREINFO_NUMBER(phys_base);
 	VMCOREINFO_SYMBOL(init_level4_pgt);
 
 #ifdef CONFIG_NUMA
@@ -337,6 +337,7 @@ void arch_crash_save_vmcoreinfo(void)
 #endif
 	vmcoreinfo_append_str("KERNELOFFSET=%lx\n",
 			      kaslr_offset());
+	VMCOREINFO_NUMBER(KERNEL_IMAGE_SIZE);
 }
 
 /* arch-dependent functionality related to kexec file-based syscall */

commit 69f58384791ac6da4165ce8e6defd6f408f4afdf
Author: Baoquan He <bhe@redhat.com>
Date:   Wed Dec 14 15:04:16 2016 -0800

    Revert "kdump, vmcoreinfo: report memory sections virtual addresses"
    
    This reverts commit 0549a3c02efb ("kdump, vmcoreinfo: report memory
    sections virtual addresses").
    
    Commit 0549a3c02efb tells the userspace utility makedumpfile the
    randomized base address of these memmory sections when mm kaslr is
    enabled.  However the following patch "kexec: export the value of
    phys_base instead of symbol address" makes makedumpfile not need these
    addresses any more.
    
    Besides we should use VMCOREINFO_NUMBER to export the value of the
    variable so that we can use the existing number_table mechanism of
    Makedumpfile to fetch it.  So revert it now.  If needed we can add it
    later.
    
    http://lists.infradead.org/pipermail/kexec/2016-October/017540.html
    Link: http://lkml.kernel.org/r/1478568596-30060-1-git-send-email-bhe@redhat.com
    Signed-off-by: Baoquan He <bhe@redhat.com>
    Cc: Thomas Garnier <thgarnie@google.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Xunlei Pang <xlpang@redhat.com>
    Cc: HATAYAMA Daisuke <d.hatayama@jp.fujitsu.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Eugene Surovegin <surovegin@google.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Atsushi Kumagai <ats-kumagai@wm.jp.nec.com>
    Cc: Dave Anderson <anderson@redhat.com>
    Cc: Pratyush Anand <panand@redhat.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 8c1f218926d7..5a294e48b185 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -337,9 +337,6 @@ void arch_crash_save_vmcoreinfo(void)
 #endif
 	vmcoreinfo_append_str("KERNELOFFSET=%lx\n",
 			      kaslr_offset());
-	VMCOREINFO_PAGE_OFFSET(PAGE_OFFSET);
-	VMCOREINFO_VMALLOC_START(VMALLOC_START);
-	VMCOREINFO_VMEMMAP_START(VMEMMAP_START);
 }
 
 /* arch-dependent functionality related to kexec file-based syscall */

commit 0549a3c02efb350776bc869685a361045efd3a29
Author: Thomas Garnier <thgarnie@google.com>
Date:   Tue Oct 11 13:55:08 2016 -0700

    kdump, vmcoreinfo: report memory sections virtual addresses
    
    KASLR memory randomization can randomize the base of the physical memory
    mapping (PAGE_OFFSET), vmalloc (VMALLOC_START) and vmemmap
    (VMEMMAP_START).  Adding these variables on VMCOREINFO so tools can easily
    identify the base of each memory section.
    
    Link: http://lkml.kernel.org/r/1471531632-23003-1-git-send-email-thgarnie@google.com
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Acked-by: Baoquan He <bhe@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Xunlei Pang <xlpang@redhat.com>
    Cc: HATAYAMA Daisuke <d.hatayama@jp.fujitsu.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Eugene Surovegin <surovegin@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 5a294e48b185..8c1f218926d7 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -337,6 +337,9 @@ void arch_crash_save_vmcoreinfo(void)
 #endif
 	vmcoreinfo_append_str("KERNELOFFSET=%lx\n",
 			      kaslr_offset());
+	VMCOREINFO_PAGE_OFFSET(PAGE_OFFSET);
+	VMCOREINFO_VMALLOC_START(VMALLOC_START);
+	VMCOREINFO_VMEMMAP_START(VMEMMAP_START);
 }
 
 /* arch-dependent functionality related to kexec file-based syscall */

commit 1e5768ae7500e7ce6eb73e1b263574d5c19606cf
Author: Xunlei Pang <xlpang@redhat.com>
Date:   Mon May 23 16:24:13 2016 -0700

    kexec: provide arch_kexec_protect(unprotect)_crashkres()
    
    Implement the protection method for the crash kernel memory reservation
    for the 64-bit x86 kdump.
    
    Signed-off-by: Xunlei Pang <xlpang@redhat.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Minfei Huang <mhuang@redhat.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Baoquan He <bhe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index ba7fbba9831b..5a294e48b185 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -538,3 +538,48 @@ int arch_kexec_apply_relocations_add(const Elf64_Ehdr *ehdr,
 	return -ENOEXEC;
 }
 #endif /* CONFIG_KEXEC_FILE */
+
+static int
+kexec_mark_range(unsigned long start, unsigned long end, bool protect)
+{
+	struct page *page;
+	unsigned int nr_pages;
+
+	/*
+	 * For physical range: [start, end]. We must skip the unassigned
+	 * crashk resource with zero-valued "end" member.
+	 */
+	if (!end || start > end)
+		return 0;
+
+	page = pfn_to_page(start >> PAGE_SHIFT);
+	nr_pages = (end >> PAGE_SHIFT) - (start >> PAGE_SHIFT) + 1;
+	if (protect)
+		return set_pages_ro(page, nr_pages);
+	else
+		return set_pages_rw(page, nr_pages);
+}
+
+static void kexec_mark_crashkres(bool protect)
+{
+	unsigned long control;
+
+	kexec_mark_range(crashk_low_res.start, crashk_low_res.end, protect);
+
+	/* Don't touch the control code page used in crash_kexec().*/
+	control = PFN_PHYS(page_to_pfn(kexec_crash_image->control_code_page));
+	/* Control code page is located in the 2nd page. */
+	kexec_mark_range(crashk_res.start, control + PAGE_SIZE - 1, protect);
+	control += KEXEC_CONTROL_PAGE_SIZE;
+	kexec_mark_range(control, crashk_res.end, protect);
+}
+
+void arch_kexec_protect_crashkres(void)
+{
+	kexec_mark_crashkres(true);
+}
+
+void arch_kexec_unprotect_crashkres(void)
+{
+	kexec_mark_crashkres(false);
+}

commit 978e30c9b46161c792ecdad0091fd017b21b8ca5
Author: Xunlei Pang <xlpang@redhat.com>
Date:   Wed Jan 20 15:00:36 2016 -0800

    kexec: move some memembers and definitions within the scope of CONFIG_KEXEC_FILE
    
    Move the stuff currently only used by the kexec file code within
    CONFIG_KEXEC_FILE (and CONFIG_KEXEC_VERIFY_SIG).
    
    Also move internal "struct kexec_sha_region" and "struct kexec_buf" into
    "kexec_internal.h".
    
    Signed-off-by: Xunlei Pang <xlpang@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Young <dyoung@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 819ab3f9c9c7..ba7fbba9831b 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -385,6 +385,7 @@ int arch_kimage_file_post_load_cleanup(struct kimage *image)
 	return image->fops->cleanup(image->image_loader_data);
 }
 
+#ifdef CONFIG_KEXEC_VERIFY_SIG
 int arch_kexec_kernel_verify_sig(struct kimage *image, void *kernel,
 				 unsigned long kernel_len)
 {
@@ -395,6 +396,7 @@ int arch_kexec_kernel_verify_sig(struct kimage *image, void *kernel,
 
 	return image->fops->verify_sig(kernel, kernel_len);
 }
+#endif
 
 /*
  * Apply purgatory relocations.

commit 0faef837e431b4984652f4a14d2075bed108a04d
Merge: 67db8a8086e9 110c14664514
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 23 14:07:26 2015 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching
    
    Pull livepatching fixes from Jiri Kosina:
    
     - symbol lookup locking fix, from Miroslav Benes
    
     - error handling improvements in case of failure of the module coming
       notifier, from Minfei Huang
    
     - we were too pessimistic when kASLR has been enabled on x86 and were
       dropping address hints on the floor unnecessarily in such case.  Fix
       from Jiri Kosina
    
     - a few other small fixes and cleanups
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching:
      livepatch: add module locking around kallsyms calls
      livepatch: annotate klp_init() with __init
      livepatch: introduce patch/func-walking helpers
      livepatch: make kobject in klp_object statically allocated
      livepatch: Prevent patch inconsistencies if the coming module notifier fails
      livepatch: match return value to function signature
      x86: kaslr: fix build due to missing ALIGN definition
      livepatch: x86: make kASLR logic more accurate
      x86: introduce kaslr_offset()

commit d6472302f242559d45dcf4ebace62508dc4d8aeb
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Tue Jun 2 19:01:38 2015 +1000

    x86/mm: Decouple <linux/vmalloc.h> from <asm/io.h>
    
    Nothing in <asm/io.h> uses anything from <linux/vmalloc.h>, so
    remove it from there and fix up the resulting build problems
    triggered on x86 {64|32}-bit {def|allmod|allno}configs.
    
    The breakages were triggering in places where x86 builds relied
    on vmalloc() facilities but did not include <linux/vmalloc.h>
    explicitly and relied on the implicit inclusion via <asm/io.h>.
    
    Also add:
    
      - <linux/init.h> to <linux/io.h>
      - <asm/pgtable_types> to <asm/io.h>
    
    ... which were two other implicit header file dependencies.
    
    Suggested-by: David Miller <davem@davemloft.net>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    [ Tidied up the changelog. ]
    Acked-by: David Miller <davem@davemloft.net>
    Acked-by: Takashi Iwai <tiwai@suse.de>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Vinod Koul <vinod.koul@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Anton Vorontsov <anton@enomsg.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Colin Cross <ccross@android.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: James E.J. Bottomley <JBottomley@odin.com>
    Cc: Jaroslav Kysela <perex@perex.cz>
    Cc: K. Y. Srinivasan <kys@microsoft.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Kristen Carlson Accardi <kristen@linux.intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Suma Ramars <sramars@cisco.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 415480d3ea84..11546b462fa6 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -17,6 +17,7 @@
 #include <linux/ftrace.h>
 #include <linux/io.h>
 #include <linux/suspend.h>
+#include <linux/vmalloc.h>
 
 #include <asm/init.h>
 #include <asm/pgtable.h>

commit 4545c89880138b30a868159bc1b209867b8a5f32
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Apr 27 13:17:19 2015 +0200

    x86: introduce kaslr_offset()
    
    Offset that has been chosen for kaslr during kernel decompression can be
    easily computed as a difference between _text and __START_KERNEL. We are
    already making use of this in dump_kernel_offset() notifier and in
    arch_crash_save_vmcoreinfo().
    
    Introduce kaslr_offset() that makes this computation instead of hard-coding
    it, so that other kernel code (such as live patching) can make use of it.
    Also convert existing users to make use of it.
    
    This patch is equivalent transofrmation without any effects on the resulting
    code:
    
            $ diff -u vmlinux.old.asm vmlinux.new.asm
            --- vmlinux.old.asm     2015-04-28 17:55:19.520983368 +0200
            +++ vmlinux.new.asm     2015-04-28 17:55:24.141206072 +0200
            @@ -1,5 +1,5 @@
    
            -vmlinux.old:     file format elf64-x86-64
            +vmlinux.new:     file format elf64-x86-64
    
            Disassembly of section .text:
            $
    
    Acked-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 415480d3ea84..e1029633f664 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -25,6 +25,7 @@
 #include <asm/io_apic.h>
 #include <asm/debugreg.h>
 #include <asm/kexec-bzimage64.h>
+#include <asm/setup.h>
 
 #ifdef CONFIG_KEXEC_FILE
 static struct kexec_file_ops *kexec_file_loaders[] = {
@@ -334,7 +335,7 @@ void arch_crash_save_vmcoreinfo(void)
 	VMCOREINFO_LENGTH(node_data, MAX_NUMNODES);
 #endif
 	vmcoreinfo_append_str("KERNELOFFSET=%lx\n",
-			      (unsigned long)&_text - __START_KERNEL);
+			      kaslr_offset());
 }
 
 /* arch-dependent functionality related to kexec file-based syscall */

commit 8643e28da27d6d50f772409b8dc80bdab52239fb
Author: Jiang Liu <jiang.liu@linux.intel.com>
Date:   Mon Oct 27 16:12:04 2014 +0800

    x86, irq: Move IOAPIC related declarations from hw_irq.h into io_apic.h
    
    Clean up code by moving IOAPIC related declarations from hw_irq.h into
    io_apic.h.
    
    Signed-off-by: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Grant Likely <grant.likely@linaro.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Matt Fleming <matt.fleming@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Christian Gmeiner <christian.gmeiner@gmail.com>
    Cc: Aubrey <aubrey.li@linux.intel.com>
    Cc: Ryan Desfosses <ryan@desfo.org>
    Cc: Quentin Lambert <lambert.quentin@gmail.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: http://lkml.kernel.org/r/1414397531-28254-14-git-send-email-jiang.liu@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 485981059a40..415480d3ea84 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -22,6 +22,7 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
+#include <asm/io_apic.h>
 #include <asm/debugreg.h>
 #include <asm/kexec-bzimage64.h>
 

commit 74ca317c26a3f8543203b61d262c0ab2e30c384e
Author: Vivek Goyal <vgoyal@redhat.com>
Date:   Fri Aug 29 15:18:46 2014 -0700

    kexec: create a new config option CONFIG_KEXEC_FILE for new syscall
    
    Currently new system call kexec_file_load() and all the associated code
    compiles if CONFIG_KEXEC=y.  But new syscall also compiles purgatory
    code which currently uses gcc option -mcmodel=large.  This option seems
    to be available only gcc 4.4 onwards.
    
    Hiding new functionality behind a new config option will not break
    existing users of old gcc.  Those who wish to enable new functionality
    will require new gcc.  Having said that, I am trying to figure out how
    can I move away from using -mcmodel=large but that can take a while.
    
    I think there are other advantages of introducing this new config
    option.  As this option will be enabled only on x86_64, other arches
    don't have to compile generic kexec code which will never be used.  This
    new code selects CRYPTO=y and CRYPTO_SHA256=y.  And all other arches had
    to do this for CONFIG_KEXEC.  Now with introduction of new config
    option, we can remove crypto dependency from other arches.
    
    Now CONFIG_KEXEC_FILE is available only on x86_64.  So whereever I had
    CONFIG_X86_64 defined, I got rid of that.
    
    For CONFIG_KEXEC_FILE, instead of doing select CRYPTO=y, I changed it to
    "depends on CRYPTO=y".  This should be safer as "select" is not
    recursive.
    
    Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Tested-by: Shaun Ruffell <sruffell@digium.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 8b04018e5d1f..485981059a40 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -25,9 +25,11 @@
 #include <asm/debugreg.h>
 #include <asm/kexec-bzimage64.h>
 
+#ifdef CONFIG_KEXEC_FILE
 static struct kexec_file_ops *kexec_file_loaders[] = {
 		&kexec_bzImage64_ops,
 };
+#endif
 
 static void free_transition_pgtable(struct kimage *image)
 {
@@ -178,6 +180,7 @@ static void load_segments(void)
 		);
 }
 
+#ifdef CONFIG_KEXEC_FILE
 /* Update purgatory as needed after various image segments have been prepared */
 static int arch_update_purgatory(struct kimage *image)
 {
@@ -209,6 +212,12 @@ static int arch_update_purgatory(struct kimage *image)
 
 	return ret;
 }
+#else /* !CONFIG_KEXEC_FILE */
+static inline int arch_update_purgatory(struct kimage *image)
+{
+	return 0;
+}
+#endif /* CONFIG_KEXEC_FILE */
 
 int machine_kexec_prepare(struct kimage *image)
 {
@@ -329,6 +338,7 @@ void arch_crash_save_vmcoreinfo(void)
 
 /* arch-dependent functionality related to kexec file-based syscall */
 
+#ifdef CONFIG_KEXEC_FILE
 int arch_kexec_kernel_image_probe(struct kimage *image, void *buf,
 				  unsigned long buf_len)
 {
@@ -522,3 +532,4 @@ int arch_kexec_apply_relocations_add(const Elf64_Ehdr *ehdr,
 	       (int)ELF64_R_TYPE(rel[i].r_info), value);
 	return -ENOEXEC;
 }
+#endif /* CONFIG_KEXEC_FILE */

commit 8e7d838103feac320baf9e68d73f954840ac1eea
Author: Vivek Goyal <vgoyal@redhat.com>
Date:   Fri Aug 8 14:26:13 2014 -0700

    kexec: verify the signature of signed PE bzImage
    
    This is the final piece of the puzzle of verifying kernel image signature
    during kexec_file_load() syscall.
    
    This patch calls into PE file routines to verify signature of bzImage.  If
    signature are valid, kexec_file_load() succeeds otherwise it fails.
    
    Two new config options have been introduced.  First one is
    CONFIG_KEXEC_VERIFY_SIG.  This option enforces that kernel has to be
    validly signed otherwise kernel load will fail.  If this option is not
    set, no signature verification will be done.  Only exception will be when
    secureboot is enabled.  In that case signature verification should be
    automatically enforced when secureboot is enabled.  But that will happen
    when secureboot patches are merged.
    
    Second config option is CONFIG_KEXEC_BZIMAGE_VERIFY_SIG.  This option
    enables signature verification support on bzImage.  If this option is not
    set and previous one is set, kernel image loading will fail because kernel
    does not have support to verify signature of bzImage.
    
    I tested these patches with both "pesign" and "sbsign" signed bzImages.
    
    I used signing_key.priv key and signing_key.x509 cert for signing as
    generated during kernel build process (if module signing is enabled).
    
    Used following method to sign bzImage.
    
    pesign
    ======
    - Convert DER format cert to PEM format cert
    openssl x509 -in signing_key.x509 -inform DER -out signing_key.x509.PEM -outform
    PEM
    
    - Generate a .p12 file from existing cert and private key file
    openssl pkcs12 -export -out kernel-key.p12 -inkey signing_key.priv -in
    signing_key.x509.PEM
    
    - Import .p12 file into pesign db
    pk12util -i /tmp/kernel-key.p12 -d /etc/pki/pesign
    
    - Sign bzImage
    pesign -i /boot/vmlinuz-3.16.0-rc3+ -o /boot/vmlinuz-3.16.0-rc3+.signed.pesign
    -c "Glacier signing key - Magrathea" -s
    
    sbsign
    ======
    sbsign --key signing_key.priv --cert signing_key.x509.PEM --output
    /boot/vmlinuz-3.16.0-rc3+.signed.sbsign /boot/vmlinuz-3.16.0-rc3+
    
    Patch details:
    
    Well all the hard work is done in previous patches.  Now bzImage loader
    has just call into that code and verify whether bzImage signature are
    valid or not.
    
    Also create two config options.  First one is CONFIG_KEXEC_VERIFY_SIG.
    This option enforces that kernel has to be validly signed otherwise kernel
    load will fail.  If this option is not set, no signature verification will
    be done.  Only exception will be when secureboot is enabled.  In that case
    signature verification should be automatically enforced when secureboot is
    enabled.  But that will happen when secureboot patches are merged.
    
    Second config option is CONFIG_KEXEC_BZIMAGE_VERIFY_SIG.  This option
    enables signature verification support on bzImage.  If this option is not
    set and previous one is set, kernel image loading will fail because kernel
    does not have support to verify signature of bzImage.
    
    Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Matthew Garrett <mjg59@srcf.ucam.org>
    Cc: Greg Kroah-Hartman <greg@kroah.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: WANG Chao <chaowang@redhat.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Matt Fleming <matt@console-pimps.org>
    Cc: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 9330434da777..8b04018e5d1f 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -372,6 +372,17 @@ int arch_kimage_file_post_load_cleanup(struct kimage *image)
 	return image->fops->cleanup(image->image_loader_data);
 }
 
+int arch_kexec_kernel_verify_sig(struct kimage *image, void *kernel,
+				 unsigned long kernel_len)
+{
+	if (!image->fops || !image->fops->verify_sig) {
+		pr_debug("kernel loader does not support signature verification.");
+		return -EKEYREJECTED;
+	}
+
+	return image->fops->verify_sig(kernel, kernel_len);
+}
+
 /*
  * Apply purgatory relocations.
  *

commit dd5f726076cc7639d9713b334c8c133f77c6757a
Author: Vivek Goyal <vgoyal@redhat.com>
Date:   Fri Aug 8 14:26:09 2014 -0700

    kexec: support for kexec on panic using new system call
    
    This patch adds support for loading a kexec on panic (kdump) kernel usning
    new system call.
    
    It prepares ELF headers for memory areas to be dumped and for saved cpu
    registers.  Also prepares the memory map for second kernel and limits its
    boot to reserved areas only.
    
    Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Matthew Garrett <mjg59@srcf.ucam.org>
    Cc: Greg Kroah-Hartman <greg@kroah.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: WANG Chao <chaowang@redhat.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 18d0f9e0b6da..9330434da777 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -178,6 +178,38 @@ static void load_segments(void)
 		);
 }
 
+/* Update purgatory as needed after various image segments have been prepared */
+static int arch_update_purgatory(struct kimage *image)
+{
+	int ret = 0;
+
+	if (!image->file_mode)
+		return 0;
+
+	/* Setup copying of backup region */
+	if (image->type == KEXEC_TYPE_CRASH) {
+		ret = kexec_purgatory_get_set_symbol(image, "backup_dest",
+				&image->arch.backup_load_addr,
+				sizeof(image->arch.backup_load_addr), 0);
+		if (ret)
+			return ret;
+
+		ret = kexec_purgatory_get_set_symbol(image, "backup_src",
+				&image->arch.backup_src_start,
+				sizeof(image->arch.backup_src_start), 0);
+		if (ret)
+			return ret;
+
+		ret = kexec_purgatory_get_set_symbol(image, "backup_sz",
+				&image->arch.backup_src_sz,
+				sizeof(image->arch.backup_src_sz), 0);
+		if (ret)
+			return ret;
+	}
+
+	return ret;
+}
+
 int machine_kexec_prepare(struct kimage *image)
 {
 	unsigned long start_pgtable;
@@ -191,6 +223,11 @@ int machine_kexec_prepare(struct kimage *image)
 	if (result)
 		return result;
 
+	/* update purgatory as needed */
+	result = arch_update_purgatory(image);
+	if (result)
+		return result;
+
 	return 0;
 }
 
@@ -315,6 +352,9 @@ int arch_kexec_kernel_image_probe(struct kimage *image, void *buf,
 
 void *arch_kexec_kernel_image_load(struct kimage *image)
 {
+	vfree(image->arch.elf_headers);
+	image->arch.elf_headers = NULL;
+
 	if (!image->fops || !image->fops->load)
 		return ERR_PTR(-ENOEXEC);
 

commit 27f48d3e633be23656a097baa3be336e04a82d84
Author: Vivek Goyal <vgoyal@redhat.com>
Date:   Fri Aug 8 14:26:06 2014 -0700

    kexec-bzImage64: support for loading bzImage using 64bit entry
    
    This is loader specific code which can load bzImage and set it up for
    64bit entry.  This does not take care of 32bit entry or real mode entry.
    
    32bit mode entry can be implemented if somebody needs it.
    
    Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Matthew Garrett <mjg59@srcf.ucam.org>
    Cc: Greg Kroah-Hartman <greg@kroah.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: WANG Chao <chaowang@redhat.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 88404c440727..18d0f9e0b6da 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -23,9 +23,10 @@
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
 #include <asm/debugreg.h>
+#include <asm/kexec-bzimage64.h>
 
 static struct kexec_file_ops *kexec_file_loaders[] = {
-		NULL,
+		&kexec_bzImage64_ops,
 };
 
 static void free_transition_pgtable(struct kimage *image)
@@ -328,7 +329,7 @@ int arch_kimage_file_post_load_cleanup(struct kimage *image)
 	if (!image->fops || !image->fops->cleanup)
 		return 0;
 
-	return image->fops->cleanup(image);
+	return image->fops->cleanup(image->image_loader_data);
 }
 
 /*

commit 12db5562e0352986a265841638482b84f3a6899b
Author: Vivek Goyal <vgoyal@redhat.com>
Date:   Fri Aug 8 14:26:04 2014 -0700

    kexec: load and relocate purgatory at kernel load time
    
    Load purgatory code in RAM and relocate it based on the location.
    Relocation code has been inspired by module relocation code and purgatory
    relocation code in kexec-tools.
    
    Also compute the checksums of loaded kexec segments and store them in
    purgatory.
    
    Arch independent code provides this functionality so that arch dependent
    bootloaders can make use of it.
    
    Helper functions are provided to get/set symbol values in purgatory which
    are used by bootloaders later to set things like stack and entry point of
    second kernel etc.
    
    Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Matthew Garrett <mjg59@srcf.ucam.org>
    Cc: Greg Kroah-Hartman <greg@kroah.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: WANG Chao <chaowang@redhat.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index c8875b5545e1..88404c440727 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -6,6 +6,8 @@
  * Version 2.  See the file COPYING for more details.
  */
 
+#define pr_fmt(fmt)	"kexec: " fmt
+
 #include <linux/mm.h>
 #include <linux/kexec.h>
 #include <linux/string.h>
@@ -328,3 +330,143 @@ int arch_kimage_file_post_load_cleanup(struct kimage *image)
 
 	return image->fops->cleanup(image);
 }
+
+/*
+ * Apply purgatory relocations.
+ *
+ * ehdr: Pointer to elf headers
+ * sechdrs: Pointer to section headers.
+ * relsec: section index of SHT_RELA section.
+ *
+ * TODO: Some of the code belongs to generic code. Move that in kexec.c.
+ */
+int arch_kexec_apply_relocations_add(const Elf64_Ehdr *ehdr,
+				     Elf64_Shdr *sechdrs, unsigned int relsec)
+{
+	unsigned int i;
+	Elf64_Rela *rel;
+	Elf64_Sym *sym;
+	void *location;
+	Elf64_Shdr *section, *symtabsec;
+	unsigned long address, sec_base, value;
+	const char *strtab, *name, *shstrtab;
+
+	/*
+	 * ->sh_offset has been modified to keep the pointer to section
+	 * contents in memory
+	 */
+	rel = (void *)sechdrs[relsec].sh_offset;
+
+	/* Section to which relocations apply */
+	section = &sechdrs[sechdrs[relsec].sh_info];
+
+	pr_debug("Applying relocate section %u to %u\n", relsec,
+		 sechdrs[relsec].sh_info);
+
+	/* Associated symbol table */
+	symtabsec = &sechdrs[sechdrs[relsec].sh_link];
+
+	/* String table */
+	if (symtabsec->sh_link >= ehdr->e_shnum) {
+		/* Invalid strtab section number */
+		pr_err("Invalid string table section index %d\n",
+		       symtabsec->sh_link);
+		return -ENOEXEC;
+	}
+
+	strtab = (char *)sechdrs[symtabsec->sh_link].sh_offset;
+
+	/* section header string table */
+	shstrtab = (char *)sechdrs[ehdr->e_shstrndx].sh_offset;
+
+	for (i = 0; i < sechdrs[relsec].sh_size / sizeof(*rel); i++) {
+
+		/*
+		 * rel[i].r_offset contains byte offset from beginning
+		 * of section to the storage unit affected.
+		 *
+		 * This is location to update (->sh_offset). This is temporary
+		 * buffer where section is currently loaded. This will finally
+		 * be loaded to a different address later, pointed to by
+		 * ->sh_addr. kexec takes care of moving it
+		 *  (kexec_load_segment()).
+		 */
+		location = (void *)(section->sh_offset + rel[i].r_offset);
+
+		/* Final address of the location */
+		address = section->sh_addr + rel[i].r_offset;
+
+		/*
+		 * rel[i].r_info contains information about symbol table index
+		 * w.r.t which relocation must be made and type of relocation
+		 * to apply. ELF64_R_SYM() and ELF64_R_TYPE() macros get
+		 * these respectively.
+		 */
+		sym = (Elf64_Sym *)symtabsec->sh_offset +
+				ELF64_R_SYM(rel[i].r_info);
+
+		if (sym->st_name)
+			name = strtab + sym->st_name;
+		else
+			name = shstrtab + sechdrs[sym->st_shndx].sh_name;
+
+		pr_debug("Symbol: %s info: %02x shndx: %02x value=%llx size: %llx\n",
+			 name, sym->st_info, sym->st_shndx, sym->st_value,
+			 sym->st_size);
+
+		if (sym->st_shndx == SHN_UNDEF) {
+			pr_err("Undefined symbol: %s\n", name);
+			return -ENOEXEC;
+		}
+
+		if (sym->st_shndx == SHN_COMMON) {
+			pr_err("symbol '%s' in common section\n", name);
+			return -ENOEXEC;
+		}
+
+		if (sym->st_shndx == SHN_ABS)
+			sec_base = 0;
+		else if (sym->st_shndx >= ehdr->e_shnum) {
+			pr_err("Invalid section %d for symbol %s\n",
+			       sym->st_shndx, name);
+			return -ENOEXEC;
+		} else
+			sec_base = sechdrs[sym->st_shndx].sh_addr;
+
+		value = sym->st_value;
+		value += sec_base;
+		value += rel[i].r_addend;
+
+		switch (ELF64_R_TYPE(rel[i].r_info)) {
+		case R_X86_64_NONE:
+			break;
+		case R_X86_64_64:
+			*(u64 *)location = value;
+			break;
+		case R_X86_64_32:
+			*(u32 *)location = value;
+			if (value != *(u32 *)location)
+				goto overflow;
+			break;
+		case R_X86_64_32S:
+			*(s32 *)location = value;
+			if ((s64)value != *(s32 *)location)
+				goto overflow;
+			break;
+		case R_X86_64_PC32:
+			value -= (u64)address;
+			*(u32 *)location = value;
+			break;
+		default:
+			pr_err("Unknown rela relocation: %llu\n",
+			       ELF64_R_TYPE(rel[i].r_info));
+			return -ENOEXEC;
+		}
+	}
+	return 0;
+
+overflow:
+	pr_err("Overflow in relocation type %d value 0x%lx\n",
+	       (int)ELF64_R_TYPE(rel[i].r_info), value);
+	return -ENOEXEC;
+}

commit cb1052581e2bddd6096544f3f944f4e7fdad4c7f
Author: Vivek Goyal <vgoyal@redhat.com>
Date:   Fri Aug 8 14:25:57 2014 -0700

    kexec: implementation of new syscall kexec_file_load
    
    Previous patch provided the interface definition and this patch prvides
    implementation of new syscall.
    
    Previously segment list was prepared in user space.  Now user space just
    passes kernel fd, initrd fd and command line and kernel will create a
    segment list internally.
    
    This patch contains generic part of the code.  Actual segment preparation
    and loading is done by arch and image specific loader.  Which comes in
    next patch.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Matthew Garrett <mjg59@srcf.ucam.org>
    Cc: Greg Kroah-Hartman <greg@kroah.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: WANG Chao <chaowang@redhat.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 679cef0791cd..c8875b5545e1 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -22,6 +22,10 @@
 #include <asm/mmu_context.h>
 #include <asm/debugreg.h>
 
+static struct kexec_file_ops *kexec_file_loaders[] = {
+		NULL,
+};
+
 static void free_transition_pgtable(struct kimage *image)
 {
 	free_page((unsigned long)image->arch.pud);
@@ -283,3 +287,44 @@ void arch_crash_save_vmcoreinfo(void)
 			      (unsigned long)&_text - __START_KERNEL);
 }
 
+/* arch-dependent functionality related to kexec file-based syscall */
+
+int arch_kexec_kernel_image_probe(struct kimage *image, void *buf,
+				  unsigned long buf_len)
+{
+	int i, ret = -ENOEXEC;
+	struct kexec_file_ops *fops;
+
+	for (i = 0; i < ARRAY_SIZE(kexec_file_loaders); i++) {
+		fops = kexec_file_loaders[i];
+		if (!fops || !fops->probe)
+			continue;
+
+		ret = fops->probe(buf, buf_len);
+		if (!ret) {
+			image->fops = fops;
+			return ret;
+		}
+	}
+
+	return ret;
+}
+
+void *arch_kexec_kernel_image_load(struct kimage *image)
+{
+	if (!image->fops || !image->fops->load)
+		return ERR_PTR(-ENOEXEC);
+
+	return image->fops->load(image, image->kernel_buf,
+				 image->kernel_buf_len, image->initrd_buf,
+				 image->initrd_buf_len, image->cmdline_buf,
+				 image->cmdline_buf_len);
+}
+
+int arch_kimage_file_post_load_cleanup(struct kimage *image)
+{
+	if (!image->fops || !image->fops->cleanup)
+		return 0;
+
+	return image->fops->cleanup(image);
+}

commit b6085a865762236bb84934161273cdac6dd11c2d
Author: Eugene Surovegin <surovegin@google.com>
Date:   Thu Jan 23 09:31:20 2014 -0800

    x86, kaslr: export offset in VMCOREINFO ELF notes
    
    Include kASLR offset in VMCOREINFO ELF notes to assist in debugging.
    
    [ hpa: pushing this for v3.14 to avoid having a kernel version with
      kASLR where we can't debug output. ]
    
    Signed-off-by: Eugene Surovegin <surovegin@google.com>
    Link: http://lkml.kernel.org/r/20140123173120.GA25474@www.outflux.net
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 4eabc160696f..679cef0791cd 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -279,5 +279,7 @@ void arch_crash_save_vmcoreinfo(void)
 	VMCOREINFO_SYMBOL(node_data);
 	VMCOREINFO_LENGTH(node_data, MAX_NUMNODES);
 #endif
+	vmcoreinfo_append_str("KERNELOFFSET=%lx\n",
+			      (unsigned long)&_text - __START_KERNEL);
 }
 

commit 0e691cf824f76adefb4498fe39c300aba2c2575a
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Thu Jan 24 12:20:05 2013 -0800

    x86, kexec, 64bit: Only set ident mapping for ram.
    
    We should set mappings only for usable memory ranges under max_pfn
    Otherwise causes same problem that is fixed by
    
            x86, mm: Only direct map addresses that are marked as E820_RAM
    
    This patch exposes pfn_mapped array, and only sets ident mapping for ranges
    in that array.
    
    This patch relies on new kernel_ident_mapping_init that could handle existing
    pgd/pud between different calls.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/1359058816-7615-25-git-send-email-yinghai@kernel.org
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index d2d7e023a8c8..4eabc160696f 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -100,10 +100,15 @@ static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 
 	level4p = (pgd_t *)__va(start_pgtable);
 	clear_page(level4p);
-	result = kernel_ident_mapping_init(&info, level4p,
-						0, max_pfn << PAGE_SHIFT);
-	if (result)
-		return result;
+	for (i = 0; i < nr_pfn_mapped; i++) {
+		mstart = pfn_mapped[i].start << PAGE_SHIFT;
+		mend   = pfn_mapped[i].end << PAGE_SHIFT;
+
+		result = kernel_ident_mapping_init(&info,
+						 level4p, mstart, mend);
+		if (result)
+			return result;
+	}
 
 	/*
 	 * segments's mem ranges could be outside 0 ~ max_pfn,

commit 9ebdc79f7a177d3098b89ba8ef2dd2b235163685
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Thu Jan 24 12:20:04 2013 -0800

    x86, kexec: Replace ident_mapping_init and init_level4_page
    
    Now ident_mapping_init is checking if pgd/pud is present for every 2M,
    so several 2Ms are in same PUD, it will keep checking if pud is there
    with same pud.
    
    init_level4_page just does not check existing pgd/pud.
    
    We could use generic mapping_init with different settings in info to
    replace those two local grown version functions.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/1359058816-7615-24-git-send-email-yinghai@kernel.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index be14ee120c43..d2d7e023a8c8 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -16,144 +16,12 @@
 #include <linux/io.h>
 #include <linux/suspend.h>
 
+#include <asm/init.h>
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
 #include <asm/debugreg.h>
 
-static int init_one_level2_page(struct kimage *image, pgd_t *pgd,
-				unsigned long addr)
-{
-	pud_t *pud;
-	pmd_t *pmd;
-	struct page *page;
-	int result = -ENOMEM;
-
-	addr &= PMD_MASK;
-	pgd += pgd_index(addr);
-	if (!pgd_present(*pgd)) {
-		page = kimage_alloc_control_pages(image, 0);
-		if (!page)
-			goto out;
-		pud = (pud_t *)page_address(page);
-		clear_page(pud);
-		set_pgd(pgd, __pgd(__pa(pud) | _KERNPG_TABLE));
-	}
-	pud = pud_offset(pgd, addr);
-	if (!pud_present(*pud)) {
-		page = kimage_alloc_control_pages(image, 0);
-		if (!page)
-			goto out;
-		pmd = (pmd_t *)page_address(page);
-		clear_page(pmd);
-		set_pud(pud, __pud(__pa(pmd) | _KERNPG_TABLE));
-	}
-	pmd = pmd_offset(pud, addr);
-	if (!pmd_present(*pmd))
-		set_pmd(pmd, __pmd(addr | __PAGE_KERNEL_LARGE_EXEC));
-	result = 0;
-out:
-	return result;
-}
-
-static int ident_mapping_init(struct kimage *image, pgd_t *level4p,
-				unsigned long mstart, unsigned long mend)
-{
-	int result;
-
-	mstart = round_down(mstart, PMD_SIZE);
-	mend   = round_up(mend - 1, PMD_SIZE);
-
-	while (mstart < mend) {
-		result = init_one_level2_page(image, level4p, mstart);
-		if (result)
-			return result;
-
-		mstart += PMD_SIZE;
-	}
-
-	return 0;
-}
-
-static void init_level2_page(pmd_t *level2p, unsigned long addr)
-{
-	unsigned long end_addr;
-
-	addr &= PAGE_MASK;
-	end_addr = addr + PUD_SIZE;
-	while (addr < end_addr) {
-		set_pmd(level2p++, __pmd(addr | __PAGE_KERNEL_LARGE_EXEC));
-		addr += PMD_SIZE;
-	}
-}
-
-static int init_level3_page(struct kimage *image, pud_t *level3p,
-				unsigned long addr, unsigned long last_addr)
-{
-	unsigned long end_addr;
-	int result;
-
-	result = 0;
-	addr &= PAGE_MASK;
-	end_addr = addr + PGDIR_SIZE;
-	while ((addr < last_addr) && (addr < end_addr)) {
-		struct page *page;
-		pmd_t *level2p;
-
-		page = kimage_alloc_control_pages(image, 0);
-		if (!page) {
-			result = -ENOMEM;
-			goto out;
-		}
-		level2p = (pmd_t *)page_address(page);
-		init_level2_page(level2p, addr);
-		set_pud(level3p++, __pud(__pa(level2p) | _KERNPG_TABLE));
-		addr += PUD_SIZE;
-	}
-	/* clear the unused entries */
-	while (addr < end_addr) {
-		pud_clear(level3p++);
-		addr += PUD_SIZE;
-	}
-out:
-	return result;
-}
-
-
-static int init_level4_page(struct kimage *image, pgd_t *level4p,
-				unsigned long addr, unsigned long last_addr)
-{
-	unsigned long end_addr;
-	int result;
-
-	result = 0;
-	addr &= PAGE_MASK;
-	end_addr = addr + (PTRS_PER_PGD * PGDIR_SIZE);
-	while ((addr < last_addr) && (addr < end_addr)) {
-		struct page *page;
-		pud_t *level3p;
-
-		page = kimage_alloc_control_pages(image, 0);
-		if (!page) {
-			result = -ENOMEM;
-			goto out;
-		}
-		level3p = (pud_t *)page_address(page);
-		result = init_level3_page(image, level3p, addr, last_addr);
-		if (result)
-			goto out;
-		set_pgd(level4p++, __pgd(__pa(level3p) | _KERNPG_TABLE));
-		addr += PGDIR_SIZE;
-	}
-	/* clear the unused entries */
-	while (addr < end_addr) {
-		pgd_clear(level4p++);
-		addr += PGDIR_SIZE;
-	}
-out:
-	return result;
-}
-
 static void free_transition_pgtable(struct kimage *image)
 {
 	free_page((unsigned long)image->arch.pud);
@@ -203,15 +71,37 @@ static int init_transition_pgtable(struct kimage *image, pgd_t *pgd)
 	return result;
 }
 
+static void *alloc_pgt_page(void *data)
+{
+	struct kimage *image = (struct kimage *)data;
+	struct page *page;
+	void *p = NULL;
+
+	page = kimage_alloc_control_pages(image, 0);
+	if (page) {
+		p = page_address(page);
+		clear_page(p);
+	}
+
+	return p;
+}
+
 static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 {
+	struct x86_mapping_info info = {
+		.alloc_pgt_page	= alloc_pgt_page,
+		.context	= image,
+		.pmd_flag	= __PAGE_KERNEL_LARGE_EXEC,
+	};
 	unsigned long mstart, mend;
 	pgd_t *level4p;
 	int result;
 	int i;
 
 	level4p = (pgd_t *)__va(start_pgtable);
-	result = init_level4_page(image, level4p, 0, max_pfn << PAGE_SHIFT);
+	clear_page(level4p);
+	result = kernel_ident_mapping_init(&info, level4p,
+						0, max_pfn << PAGE_SHIFT);
 	if (result)
 		return result;
 
@@ -225,7 +115,8 @@ static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 		mstart = image->segment[i].mem;
 		mend   = mstart + image->segment[i].memsz;
 
-		result = ident_mapping_init(image, level4p, mstart, mend);
+		result = kernel_ident_mapping_init(&info,
+						 level4p, mstart, mend);
 
 		if (result)
 			return result;

commit 084d1283986a530828b8898f206adf44d5d3146d
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Thu Jan 24 12:20:03 2013 -0800

    x86, kexec: Set ident mapping for kernel that is above max_pfn
    
    When first kernel is booted with memmap= or mem=  to limit max_pfn.
    kexec can load second kernel above that max_pfn.
    
    We need to set ident mapping for whole image in this case instead of just
    for first 2M.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/1359058816-7615-23-git-send-email-yinghai@kernel.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index b3ea9db39db6..be14ee120c43 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -56,6 +56,25 @@ static int init_one_level2_page(struct kimage *image, pgd_t *pgd,
 	return result;
 }
 
+static int ident_mapping_init(struct kimage *image, pgd_t *level4p,
+				unsigned long mstart, unsigned long mend)
+{
+	int result;
+
+	mstart = round_down(mstart, PMD_SIZE);
+	mend   = round_up(mend - 1, PMD_SIZE);
+
+	while (mstart < mend) {
+		result = init_one_level2_page(image, level4p, mstart);
+		if (result)
+			return result;
+
+		mstart += PMD_SIZE;
+	}
+
+	return 0;
+}
+
 static void init_level2_page(pmd_t *level2p, unsigned long addr)
 {
 	unsigned long end_addr;
@@ -184,22 +203,34 @@ static int init_transition_pgtable(struct kimage *image, pgd_t *pgd)
 	return result;
 }
 
-
 static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 {
+	unsigned long mstart, mend;
 	pgd_t *level4p;
 	int result;
+	int i;
+
 	level4p = (pgd_t *)__va(start_pgtable);
 	result = init_level4_page(image, level4p, 0, max_pfn << PAGE_SHIFT);
 	if (result)
 		return result;
+
 	/*
-	 * image->start may be outside 0 ~ max_pfn, for example when
-	 * jump back to original kernel from kexeced kernel
+	 * segments's mem ranges could be outside 0 ~ max_pfn,
+	 * for example when jump back to original kernel from kexeced kernel.
+	 * or first kernel is booted with user mem map, and second kernel
+	 * could be loaded out of that range.
 	 */
-	result = init_one_level2_page(image, level4p, image->start);
-	if (result)
-		return result;
+	for (i = 0; i < image->nr_segments; i++) {
+		mstart = image->segment[i].mem;
+		mend   = mstart + image->segment[i].memsz;
+
+		result = ident_mapping_init(image, level4p, mstart, mend);
+
+		if (result)
+			return result;
+	}
+
 	return init_transition_pgtable(image, level4p);
 }
 

commit 234bb549eea16ec7d5207ba747fb8dbf489e64c1
Author: Jan Beulich <JBeulich@novell.com>
Date:   Thu Sep 2 13:46:34 2010 +0100

    x86, cleanups: Use clear_page/copy_page rather than memset/memcpy
    
    When operating on whole pages, use clear_page() and copy_page() in
    favor of memset() and memcpy(); after all that's what they are
    intended for.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    LKML-Reference: <4C7FB8CA0200007800013F51@vpn.id2.novell.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 035c8c529181..b3ea9db39db6 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -36,7 +36,7 @@ static int init_one_level2_page(struct kimage *image, pgd_t *pgd,
 		if (!page)
 			goto out;
 		pud = (pud_t *)page_address(page);
-		memset(pud, 0, PAGE_SIZE);
+		clear_page(pud);
 		set_pgd(pgd, __pgd(__pa(pud) | _KERNPG_TABLE));
 	}
 	pud = pud_offset(pgd, addr);
@@ -45,7 +45,7 @@ static int init_one_level2_page(struct kimage *image, pgd_t *pgd,
 		if (!page)
 			goto out;
 		pmd = (pmd_t *)page_address(page);
-		memset(pmd, 0, PAGE_SIZE);
+		clear_page(pmd);
 		set_pud(pud, __pud(__pa(pmd) | _KERNPG_TABLE));
 	}
 	pmd = pmd_offset(pud, addr);

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 4a8bb82248ae..035c8c529181 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -9,6 +9,7 @@
 #include <linux/mm.h>
 #include <linux/kexec.h>
 #include <linux/string.h>
+#include <linux/gfp.h>
 #include <linux/reboot.h>
 #include <linux/numa.h>
 #include <linux/ftrace.h>

commit 17f557e5b5d43a2af66c969f6560ac7105020672
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Mon Jun 1 23:46:03 2009 +0530

    hw-breakpoints: cleanup HW Breakpoint registers before kexec
    
    This patch disables Hardware breakpoints before doing a 'kexec' on the machine
    so that the cpu doesn't keep debug registers values which would be out of
    sync for the new image.
    
    Original-patch-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Reviewed-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 84c3bf209e98..4a8bb82248ae 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -18,6 +18,7 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
+#include <asm/debugreg.h>
 
 static int init_one_level2_page(struct kimage *image, pgd_t *pgd,
 				unsigned long addr)
@@ -282,6 +283,7 @@ void machine_kexec(struct kimage *image)
 
 	/* Interrupts aren't acceptable while we reboot */
 	local_irq_disable();
+	hw_breakpoint_disable();
 
 	if (image->preserve_context) {
 #ifdef CONFIG_X86_IO_APIC

commit 6407df5ca54a511054200a1eb23f78f723ca1de4
Author: Huang Ying <ying.huang@intel.com>
Date:   Fri May 8 10:51:41 2009 +0800

    x86, kexec: fix crashdump panic with CONFIG_KEXEC_JUMP
    
    Tim Starling reported that crashdump will panic with kernel compiled
    with CONFIG_KEXEC_JUMP due to null pointer deference in
    machine_kexec_32.c: machine_kexec(), when deferencing
    kexec_image. Refering to:
    
    http://bugzilla.kernel.org/show_bug.cgi?id=13265
    
    This patch fixes the BUG via replacing global variable reference:
    kexec_image in machine_kexec() with local variable reference: image,
    which is more appropriate, and will not be null.
    
    Same BUG is in machine_kexec_64.c too, so fixed too in the same way.
    
    [ Impact: fix crash on kexec ]
    
    Reported-by: Tim Starling <tstarling@wikimedia.org>
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    LKML-Reference: <1241751101.6259.85.camel@yhuang-dev.sh.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 89cea4d44679..84c3bf209e98 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -274,7 +274,7 @@ void machine_kexec(struct kimage *image)
 	int save_ftrace_enabled;
 
 #ifdef CONFIG_KEXEC_JUMP
-	if (kexec_image->preserve_context)
+	if (image->preserve_context)
 		save_processor_state();
 #endif
 
@@ -333,7 +333,7 @@ void machine_kexec(struct kimage *image)
 				       image->preserve_context);
 
 #ifdef CONFIG_KEXEC_JUMP
-	if (kexec_image->preserve_context)
+	if (image->preserve_context)
 		restore_processor_state();
 #endif
 

commit fee7b0d84cc8c7bc5dc212901c79e93eaf83a5b5
Author: Huang Ying <ying.huang@intel.com>
Date:   Tue Mar 10 10:57:16 2009 +0800

    x86, kexec: x86_64: add kexec jump support for x86_64
    
    Impact: New major feature
    
    This patch add kexec jump support for x86_64. More information about
    kexec jump can be found in corresponding x86_32 support patch.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 7cc5d3d01483..89cea4d44679 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -13,6 +13,7 @@
 #include <linux/numa.h>
 #include <linux/ftrace.h>
 #include <linux/io.h>
+#include <linux/suspend.h>
 
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
@@ -270,19 +271,43 @@ void machine_kexec(struct kimage *image)
 {
 	unsigned long page_list[PAGES_NR];
 	void *control_page;
+	int save_ftrace_enabled;
 
-	tracer_disable();
+#ifdef CONFIG_KEXEC_JUMP
+	if (kexec_image->preserve_context)
+		save_processor_state();
+#endif
+
+	save_ftrace_enabled = __ftrace_enabled_save();
 
 	/* Interrupts aren't acceptable while we reboot */
 	local_irq_disable();
 
+	if (image->preserve_context) {
+#ifdef CONFIG_X86_IO_APIC
+		/*
+		 * We need to put APICs in legacy mode so that we can
+		 * get timer interrupts in second kernel. kexec/kdump
+		 * paths already have calls to disable_IO_APIC() in
+		 * one form or other. kexec jump path also need
+		 * one.
+		 */
+		disable_IO_APIC();
+#endif
+	}
+
 	control_page = page_address(image->control_code_page) + PAGE_SIZE;
-	memcpy(control_page, relocate_kernel, PAGE_SIZE);
+	memcpy(control_page, relocate_kernel, KEXEC_CONTROL_CODE_MAX_SIZE);
 
 	page_list[PA_CONTROL_PAGE] = virt_to_phys(control_page);
+	page_list[VA_CONTROL_PAGE] = (unsigned long)control_page;
 	page_list[PA_TABLE_PAGE] =
 	  (unsigned long)__pa(page_address(image->control_code_page));
 
+	if (image->type == KEXEC_TYPE_DEFAULT)
+		page_list[PA_SWAP_PAGE] = (page_to_pfn(image->swap_page)
+						<< PAGE_SHIFT);
+
 	/*
 	 * The segment registers are funny things, they have both a
 	 * visible and an invisible part.  Whenever the visible part is
@@ -302,8 +327,17 @@ void machine_kexec(struct kimage *image)
 	set_idt(phys_to_virt(0), 0);
 
 	/* now call it */
-	relocate_kernel((unsigned long)image->head, (unsigned long)page_list,
-			image->start);
+	image->start = relocate_kernel((unsigned long)image->head,
+				       (unsigned long)page_list,
+				       image->start,
+				       image->preserve_context);
+
+#ifdef CONFIG_KEXEC_JUMP
+	if (kexec_image->preserve_context)
+		restore_processor_state();
+#endif
+
+	__ftrace_enabled_restore(save_ftrace_enabled);
 }
 
 void arch_crash_save_vmcoreinfo(void)

commit 5359454701ce51a4626b1ef6eb7b16ec35bd458d
Author: Huang Ying <ying.huang@intel.com>
Date:   Tue Mar 10 10:57:04 2009 +0800

    x86, kexec: x86_64: add identity map for pages at image->start
    
    Impact: Fix corner case that cannot yet occur
    
    image->start may be outside of 0 ~ max_pfn, for example when jumping
    back to original kernel from kexeced kenrel. This patch add identity
    map for pages at image->start.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index f8c796fffa0f..7cc5d3d01483 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -18,6 +18,41 @@
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
 
+static int init_one_level2_page(struct kimage *image, pgd_t *pgd,
+				unsigned long addr)
+{
+	pud_t *pud;
+	pmd_t *pmd;
+	struct page *page;
+	int result = -ENOMEM;
+
+	addr &= PMD_MASK;
+	pgd += pgd_index(addr);
+	if (!pgd_present(*pgd)) {
+		page = kimage_alloc_control_pages(image, 0);
+		if (!page)
+			goto out;
+		pud = (pud_t *)page_address(page);
+		memset(pud, 0, PAGE_SIZE);
+		set_pgd(pgd, __pgd(__pa(pud) | _KERNPG_TABLE));
+	}
+	pud = pud_offset(pgd, addr);
+	if (!pud_present(*pud)) {
+		page = kimage_alloc_control_pages(image, 0);
+		if (!page)
+			goto out;
+		pmd = (pmd_t *)page_address(page);
+		memset(pmd, 0, PAGE_SIZE);
+		set_pud(pud, __pud(__pa(pmd) | _KERNPG_TABLE));
+	}
+	pmd = pmd_offset(pud, addr);
+	if (!pmd_present(*pmd))
+		set_pmd(pmd, __pmd(addr | __PAGE_KERNEL_LARGE_EXEC));
+	result = 0;
+out:
+	return result;
+}
+
 static void init_level2_page(pmd_t *level2p, unsigned long addr)
 {
 	unsigned long end_addr;
@@ -153,6 +188,13 @@ static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 	int result;
 	level4p = (pgd_t *)__va(start_pgtable);
 	result = init_level4_page(image, level4p, 0, max_pfn << PAGE_SHIFT);
+	if (result)
+		return result;
+	/*
+	 * image->start may be outside 0 ~ max_pfn, for example when
+	 * jump back to original kernel from kexeced kernel
+	 */
+	result = init_one_level2_page(image, level4p, image->start);
 	if (result)
 		return result;
 	return init_transition_pgtable(image, level4p);

commit fef3a7a17418814733ebde0b40d8e32747677c8f
Author: Huang Ying <ying.huang@intel.com>
Date:   Tue Mar 10 10:56:57 2009 +0800

    x86, kexec: fix kexec x86 coding style
    
    Impact: Cleanup
    
    Fix some coding style issue for kexec x86.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 6993d51b7fd8..f8c796fffa0f 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -12,11 +12,11 @@
 #include <linux/reboot.h>
 #include <linux/numa.h>
 #include <linux/ftrace.h>
+#include <linux/io.h>
 
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
-#include <asm/io.h>
 
 static void init_level2_page(pmd_t *level2p, unsigned long addr)
 {
@@ -83,9 +83,8 @@ static int init_level4_page(struct kimage *image, pgd_t *level4p,
 		}
 		level3p = (pud_t *)page_address(page);
 		result = init_level3_page(image, level3p, addr, last_addr);
-		if (result) {
+		if (result)
 			goto out;
-		}
 		set_pgd(level4p++, __pgd(__pa(level3p) | _KERNPG_TABLE));
 		addr += PGDIR_SIZE;
 	}
@@ -242,7 +241,8 @@ void machine_kexec(struct kimage *image)
 	page_list[PA_TABLE_PAGE] =
 	  (unsigned long)__pa(page_address(image->control_code_page));
 
-	/* The segment registers are funny things, they have both a
+	/*
+	 * The segment registers are funny things, they have both a
 	 * visible and an invisible part.  Whenever the visible part is
 	 * set to a specific selector, the invisible part is loaded
 	 * with from a table in memory.  At no other time is the
@@ -252,11 +252,12 @@ void machine_kexec(struct kimage *image)
 	 * segments, before I zap the gdt with an invalid value.
 	 */
 	load_segments();
-	/* The gdt & idt are now invalid.
+	/*
+	 * The gdt & idt are now invalid.
 	 * If you want to load them you must set up your own idt & gdt.
 	 */
-	set_gdt(phys_to_virt(0),0);
-	set_idt(phys_to_virt(0),0);
+	set_gdt(phys_to_virt(0), 0);
+	set_idt(phys_to_virt(0), 0);
 
 	/* now call it */
 	relocate_kernel((unsigned long)image->head, (unsigned long)page_list,

commit f5deb79679af6eb41b61112fadcda28b2a4cfb0d
Author: Huang Ying <ying.huang@intel.com>
Date:   Tue Feb 3 14:22:48 2009 +0800

    x86: kexec: Use one page table in x86_64 machine_kexec
    
    Impact: reduce kernel BSS size by 7 pages, improve code readability
    
    Two page tables are used in current x86_64 kexec implementation. One
    is used to jump from kernel virtual address to identity map address,
    the other is used to map all physical memory. In fact, on x86_64,
    there is no conflict between kernel virtual address space and physical
    memory space, so just one page table is sufficient. The page table
    pages used to map control page are dynamically allocated to save
    memory if kexec image is not loaded. ASM code used to map control page
    is replaced by C code too.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index c43caa3a91f3..6993d51b7fd8 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -18,15 +18,6 @@
 #include <asm/mmu_context.h>
 #include <asm/io.h>
 
-#define PAGE_ALIGNED __attribute__ ((__aligned__(PAGE_SIZE)))
-static u64 kexec_pgd[512] PAGE_ALIGNED;
-static u64 kexec_pud0[512] PAGE_ALIGNED;
-static u64 kexec_pmd0[512] PAGE_ALIGNED;
-static u64 kexec_pte0[512] PAGE_ALIGNED;
-static u64 kexec_pud1[512] PAGE_ALIGNED;
-static u64 kexec_pmd1[512] PAGE_ALIGNED;
-static u64 kexec_pte1[512] PAGE_ALIGNED;
-
 static void init_level2_page(pmd_t *level2p, unsigned long addr)
 {
 	unsigned long end_addr;
@@ -107,12 +98,65 @@ static int init_level4_page(struct kimage *image, pgd_t *level4p,
 	return result;
 }
 
+static void free_transition_pgtable(struct kimage *image)
+{
+	free_page((unsigned long)image->arch.pud);
+	free_page((unsigned long)image->arch.pmd);
+	free_page((unsigned long)image->arch.pte);
+}
+
+static int init_transition_pgtable(struct kimage *image, pgd_t *pgd)
+{
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+	unsigned long vaddr, paddr;
+	int result = -ENOMEM;
+
+	vaddr = (unsigned long)relocate_kernel;
+	paddr = __pa(page_address(image->control_code_page)+PAGE_SIZE);
+	pgd += pgd_index(vaddr);
+	if (!pgd_present(*pgd)) {
+		pud = (pud_t *)get_zeroed_page(GFP_KERNEL);
+		if (!pud)
+			goto err;
+		image->arch.pud = pud;
+		set_pgd(pgd, __pgd(__pa(pud) | _KERNPG_TABLE));
+	}
+	pud = pud_offset(pgd, vaddr);
+	if (!pud_present(*pud)) {
+		pmd = (pmd_t *)get_zeroed_page(GFP_KERNEL);
+		if (!pmd)
+			goto err;
+		image->arch.pmd = pmd;
+		set_pud(pud, __pud(__pa(pmd) | _KERNPG_TABLE));
+	}
+	pmd = pmd_offset(pud, vaddr);
+	if (!pmd_present(*pmd)) {
+		pte = (pte_t *)get_zeroed_page(GFP_KERNEL);
+		if (!pte)
+			goto err;
+		image->arch.pte = pte;
+		set_pmd(pmd, __pmd(__pa(pte) | _KERNPG_TABLE));
+	}
+	pte = pte_offset_kernel(pmd, vaddr);
+	set_pte(pte, pfn_pte(paddr >> PAGE_SHIFT, PAGE_KERNEL_EXEC));
+	return 0;
+err:
+	free_transition_pgtable(image);
+	return result;
+}
+
 
 static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 {
 	pgd_t *level4p;
+	int result;
 	level4p = (pgd_t *)__va(start_pgtable);
-	return init_level4_page(image, level4p, 0, max_pfn << PAGE_SHIFT);
+	result = init_level4_page(image, level4p, 0, max_pfn << PAGE_SHIFT);
+	if (result)
+		return result;
+	return init_transition_pgtable(image, level4p);
 }
 
 static void set_idt(void *newidt, u16 limit)
@@ -174,7 +218,7 @@ int machine_kexec_prepare(struct kimage *image)
 
 void machine_kexec_cleanup(struct kimage *image)
 {
-	return;
+	free_transition_pgtable(image);
 }
 
 /*
@@ -195,22 +239,6 @@ void machine_kexec(struct kimage *image)
 	memcpy(control_page, relocate_kernel, PAGE_SIZE);
 
 	page_list[PA_CONTROL_PAGE] = virt_to_phys(control_page);
-	page_list[VA_CONTROL_PAGE] = (unsigned long)relocate_kernel;
-	page_list[PA_PGD] = virt_to_phys(&kexec_pgd);
-	page_list[VA_PGD] = (unsigned long)kexec_pgd;
-	page_list[PA_PUD_0] = virt_to_phys(&kexec_pud0);
-	page_list[VA_PUD_0] = (unsigned long)kexec_pud0;
-	page_list[PA_PMD_0] = virt_to_phys(&kexec_pmd0);
-	page_list[VA_PMD_0] = (unsigned long)kexec_pmd0;
-	page_list[PA_PTE_0] = virt_to_phys(&kexec_pte0);
-	page_list[VA_PTE_0] = (unsigned long)kexec_pte0;
-	page_list[PA_PUD_1] = virt_to_phys(&kexec_pud1);
-	page_list[VA_PUD_1] = (unsigned long)kexec_pud1;
-	page_list[PA_PMD_1] = virt_to_phys(&kexec_pmd1);
-	page_list[VA_PMD_1] = (unsigned long)kexec_pmd1;
-	page_list[PA_PTE_1] = virt_to_phys(&kexec_pte1);
-	page_list[VA_PTE_1] = (unsigned long)kexec_pte1;
-
 	page_list[PA_TABLE_PAGE] =
 	  (unsigned long)__pa(page_address(image->control_code_page));
 

commit 3ab83521378268044a448113c6aa9a9e245f4d2f
Author: Huang Ying <ying.huang@intel.com>
Date:   Fri Jul 25 19:45:07 2008 -0700

    kexec jump
    
    This patch provides an enhancement to kexec/kdump.  It implements the
    following features:
    
    - Backup/restore memory used by the original kernel before/after
      kexec.
    
    - Save/restore CPU state before/after kexec.
    
    The features of this patch can be used as a general method to call program in
    physical mode (paging turning off).  This can be used to call BIOS code under
    Linux.
    
    kexec-tools needs to be patched to support kexec jump. The patches and
    the precompiled kexec can be download from the following URL:
    
           source: http://khibernation.sourceforge.net/download/release_v10/kexec-tools/kexec-tools-src_git_kh10.tar.bz2
           patches: http://khibernation.sourceforge.net/download/release_v10/kexec-tools/kexec-tools-patches_git_kh10.tar.bz2
           binary: http://khibernation.sourceforge.net/download/release_v10/kexec-tools/kexec_git_kh10
    
    Usage example of calling some physical mode code and return:
    
    1. Compile and install patched kernel with following options selected:
    
    CONFIG_X86_32=y
    CONFIG_KEXEC=y
    CONFIG_PM=y
    CONFIG_KEXEC_JUMP=y
    
    2. Build patched kexec-tool or download the pre-built one.
    
    3. Build some physical mode executable named such as "phy_mode"
    
    4. Boot kernel compiled in step 1.
    
    5. Load physical mode executable with /sbin/kexec. The shell command
       line can be as follow:
    
       /sbin/kexec --load-preserve-context --args-none phy_mode
    
    6. Call physical mode executable with following shell command line:
    
       /sbin/kexec -e
    
    Implementation point:
    
    To support jumping without reserving memory.  One shadow backup page (source
    page) is allocated for each page used by kexeced code image (destination
    page).  When do kexec_load, the image of kexeced code is loaded into source
    pages, and before executing, the destination pages and the source pages are
    swapped, so the contents of destination pages are backupped.  Before jumping
    to the kexeced code image and after jumping back to the original kernel, the
    destination pages and the source pages are swapped too.
    
    C ABI (calling convention) is used as communication protocol between
    kernel and called code.
    
    A flag named KEXEC_PRESERVE_CONTEXT for sys_kexec_load is added to
    indicate that the loaded kernel image is used for jumping back.
    
    Now, only the i386 architecture is supported.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Acked-by: Vivek Goyal <vgoyal@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Nigel Cunningham <nigel@nigel.suspend2.net>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 9dd9262693a3..c43caa3a91f3 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -181,7 +181,7 @@ void machine_kexec_cleanup(struct kimage *image)
  * Do not allocate memory (or fail in any way) in machine_kexec().
  * We are past the point of no return, committed to rebooting now.
  */
-NORET_TYPE void machine_kexec(struct kimage *image)
+void machine_kexec(struct kimage *image)
 {
 	unsigned long page_list[PAGES_NR];
 	void *control_page;

commit 5806b81ac1c0c52665b91723fd4146a4f86e386b
Merge: d14c8a680ccf 6712e299b7dc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 14 16:11:52 2008 +0200

    Merge branch 'auto-ftrace-next' into tracing/for-linus
    
    Conflicts:
    
            arch/x86/kernel/entry_32.S
            arch/x86/kernel/process_32.c
            arch/x86/kernel/process_64.c
            arch/x86/lib/Makefile
            include/asm-x86/irqflags.h
            kernel/Makefile
            kernel/sched.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit c987d12f8455b19b3b057d63bac3de161bd809fc
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Tue Jun 24 22:14:09 2008 -0700

    x86: remove end_pfn in 64bit
    
    and use max_pfn directly.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 576a03db4511..7830dc4a8380 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -110,7 +110,7 @@ static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
 {
 	pgd_t *level4p;
 	level4p = (pgd_t *)__va(start_pgtable);
- 	return init_level4_page(image, level4p, 0, end_pfn << PAGE_SHIFT);
+	return init_level4_page(image, level4p, 0, max_pfn << PAGE_SHIFT);
 }
 
 static void set_idt(void *newidt, u16 limit)

commit f43fdad8627fec2d21df92799b254dceb66c9c3c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:43 2008 +0200

    ftrace: fix kexec
    
    disable the tracer while kexec pulls the rug from under the old
    kernel.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 576a03db4511..1558fdc174f9 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -11,6 +11,8 @@
 #include <linux/string.h>
 #include <linux/reboot.h>
 #include <linux/numa.h>
+#include <linux/ftrace.h>
+
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
@@ -184,6 +186,8 @@ NORET_TYPE void machine_kexec(struct kimage *image)
 	unsigned long page_list[PAGES_NR];
 	void *control_page;
 
+	tracer_disable();
+
 	/* Interrupts aren't acceptable while we reboot */
 	local_irq_disable();
 

commit 629c8b4cdb354518308663aff2f719e02f69ffbe
Author: Ken'ichi Ohmichi <oomichi@mxs.nes.nec.co.jp>
Date:   Wed Apr 2 13:04:50 2008 -0700

    vmcoreinfo: add the symbol "phys_base"
    
    Fix the problem that makedumpfile sometimes fails on x86_64 machine.
    
    This patch adds the symbol "phys_base" to a vmcoreinfo data.  The
    vmcoreinfo data has the minimum debugging information only for dump
    filtering.  makedumpfile (dump filtering command) gets it to distinguish
    unnecessary pages, and makedumpfile creates a small dumpfile.
    
    On x86_64 kernel which compiled with CONFIG_PHYSICAL_START=0x0 and
    CONFIG_RELOCATABLE=y, makedumpfile fails like the following:
    
     # makedumpfile -d31 /proc/vmcore dumpfile
     The kernel version is not supported.
     The created dumpfile may be incomplete.
     _exclude_free_page: Can't get next online node.
    
     makedumpfile Failed.
     #
    
    The cause is the lack of the symbol "phys_base" in a vmcoreinfo data.
    If the symbol "phys_base" does not exist, makedumpfile considers an
    x86_64 kernel as non relocatable.  As the result, makedumpfile
    misunderstands the physical address where the kernel is loaded, and it
    cannot translate a kernel virtual address to physical address correctly.
    
    To fix this problem, this patch adds the symbol "phys_base" to a
    vmcoreinfo data.
    
    Signed-off-by: Ken'ichi Ohmichi <oomichi@mxs.nes.nec.co.jp>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: <stable@kernel.org>
    Acked-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 236d2f8f7ddc..576a03db4511 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -233,6 +233,7 @@ NORET_TYPE void machine_kexec(struct kimage *image)
 
 void arch_crash_save_vmcoreinfo(void)
 {
+	VMCOREINFO_SYMBOL(phys_base);
 	VMCOREINFO_SYMBOL(init_level4_pgt);
 
 #ifdef CONFIG_NUMA

commit 92df5c3e38c0a0a66a456926039548275dfb3328
Author: Ken'ichi Ohmichi <oomichi@mxs.nes.nec.co.jp>
Date:   Thu Feb 7 00:15:23 2008 -0800

    vmcoreinfo: fix the configuration dependencies
    
    This patch fixes the configuration dependencies in the vmcoreinfo data.
    
    i386's "node_data" is defined in arch/x86/mm/discontig_32.c,
    and x86_64's one is defined in arch/x86/mm/numa_64.c.
    They depend on CONFIG_NUMA:
      arch/x86/mm/Makefile_32:7
        obj-$(CONFIG_NUMA) += discontig_32.o
      arch/x86/mm/Makefile_64:7
        obj-$(CONFIG_NUMA) += numa_64.o
    
    ia64's "pgdat_list" is defined in arch/ia64/mm/discontig.c,
    and it depends on CONFIG_DISCONTIGMEM and CONFIG_SPARSEMEM:
      arch/ia64/mm/Makefile:9-10
        obj-$(CONFIG_DISCONTIGMEM) += discontig.o
        obj-$(CONFIG_SPARSEMEM)    += discontig.o
    
    ia64's "node_memblk" is defined in arch/ia64/mm/numa.c,
    and it depends on CONFIG_NUMA:
      arch/ia64/mm/Makefile:8
        obj-$(CONFIG_NUMA)         += numa.o
    
    Signed-off-by: Ken'ichi Ohmichi <oomichi@mxs.nes.nec.co.jp>
    Acked-by: Simon Horman <horms@verge.net.au>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index a1fef42f8cdb..236d2f8f7ddc 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -234,5 +234,10 @@ NORET_TYPE void machine_kexec(struct kimage *image)
 void arch_crash_save_vmcoreinfo(void)
 {
 	VMCOREINFO_SYMBOL(init_level4_pgt);
+
+#ifdef CONFIG_NUMA
+	VMCOREINFO_SYMBOL(node_data);
+	VMCOREINFO_LENGTH(node_data, MAX_NUMNODES);
+#endif
 }
 

commit b263295dbffd33b0fbff670720fa178c30e3392a
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Jan 30 13:30:47 2008 +0100

    x86: 64-bit, make sparsemem vmemmap the only memory model
    
    Use sparsemem as the only memory model for UP, SMP and NUMA.  Measurements
    indicate that DISCONTIGMEM has a higher overhead than sparsemem.  And
    FLATMEMs benefits are minimal.  So I think its best to simply standardize
    on sparsemem.
    
    Results of page allocator tests (test can be had via git from slab git
    tree branch tests)
    
    Measurements in cycle counts. 1000 allocations were performed and then the
    average cycle count was calculated.
    
    Order   FlatMem Discontig       SparseMem
    0         639     665             641
    1         567     647             593
    2         679     774             692
    3         763     967             781
    4         961    1501             962
    5        1356    2344            1392
    6        2224    3982            2336
    7        4869    7225            5074
    8       12500   14048           12732
    9       27926   28223           28165
    10      58578   58714           58682
    
    (Note that FlatMem is an SMP config and the rest NUMA configurations)
    
    Memory use:
    
    SMP Sparsemem
    -------------
    
    Kernel size:
    
       text    data     bss     dec     hex filename
    3849268  397739 1264856 5511863  541ab7 vmlinux
    
                 total       used       free     shared    buffers     cached
    Mem:       8242252      41164    8201088          0        352      11512
    -/+ buffers/cache:      29300    8212952
    Swap:      9775512          0    9775512
    
    SMP Flatmem
    -----------
    
    Kernel size:
    
       text    data     bss     dec     hex filename
    3844612  397739 1264536 5506887  540747 vmlinux
    
    So 4.5k growth in text size vs. FLATMEM.
    
                 total       used       free     shared    buffers     cached
    Mem:       8244052      40544    8203508          0        352      11484
    -/+ buffers/cache:      28708    8215344
    
    2k growth in overall memory use after boot.
    
    NUMA discontig:
    
       text    data     bss     dec     hex filename
    3888124  470659 1276504 5635287  55fcd7 vmlinux
    
                 total       used       free     shared    buffers     cached
    Mem:       8256256      56908    8199348          0        352      11496
    -/+ buffers/cache:      45060    8211196
    Swap:      9775512          0    9775512
    
    NUMA sparse:
    
       text    data     bss     dec     hex filename
    3896428  470659 1276824 5643911  561e87 vmlinux
    
    8k text growth. Given that we fully inline virt_to_page and friends now
    that is rather good.
    
                 total       used       free     shared    buffers     cached
    Mem:       8264720      57240    8207480          0        352      11516
    -/+ buffers/cache:      45372    8219348
    Swap:      9775512          0    9775512
    
    The total available memory is increased by 8k.
    
    This patch makes sparsemem the default and removes discontig and
    flatmem support from x86.
    
    [ akpm@linux-foundation.org: allnoconfig build fix ]
    
    Acked-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index aa3d2c8f7737..a1fef42f8cdb 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -234,10 +234,5 @@ NORET_TYPE void machine_kexec(struct kimage *image)
 void arch_crash_save_vmcoreinfo(void)
 {
 	VMCOREINFO_SYMBOL(init_level4_pgt);
-
-#ifdef CONFIG_ARCH_DISCONTIGMEM_ENABLE
-	VMCOREINFO_SYMBOL(node_data);
-	VMCOREINFO_LENGTH(node_data, MAX_NUMNODES);
-#endif
 }
 

commit 69243f91257083795065762ce805120b980e256b
Author: Ken'ichi Ohmichi <oomichi@mxs.nes.nec.co.jp>
Date:   Fri Oct 26 14:19:26 2007 +0900

    x86: Dump filtering supports x86_64 sparsemem
    
    This patch adds the symbol "init_level4_pgt" to the vmcoreinfo data so
    that makedumpfile (dump filtering command) supports x86_64 sparsemem
    kernel of linux-2.6.24.
    
    makedumpfile creates a small dumpfile by excluding unnecessary pages for
    the analysis. It checks attributes in page structures and distinguishes
    necessary pages and unnecessary ones. To check them, makedumpfile gets
    the vmcoreinfo data which has the minimum debugging information only for
    dump filtering.
    
    For older x86_64 kernel (linux-2.6.23 or before), makedumpfile translates
    the virtual address of page structure into physical address by subtracting
    PAGE_OFFSET from virtual address, but this translation isn't effective for
    linux-2.6.24 sparsemem kernel, because its page structures are in virtual
    memmap area. makedumpfile should translate their virtual address by 4-levels
    paging and it needs the symbol "init_level4_pgt".
    
    Signed-off-by: Ken'ichi Ohmichi <oomichi@mxs.nes.nec.co.jp>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 0d8577f05422..aa3d2c8f7737 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -233,6 +233,8 @@ NORET_TYPE void machine_kexec(struct kimage *image)
 
 void arch_crash_save_vmcoreinfo(void)
 {
+	VMCOREINFO_SYMBOL(init_level4_pgt);
+
 #ifdef CONFIG_ARCH_DISCONTIGMEM_ENABLE
 	VMCOREINFO_SYMBOL(node_data);
 	VMCOREINFO_LENGTH(node_data, MAX_NUMNODES);

commit 5c3391f9f749023a49c64d607da4fb49263690eb
Author: Bernhard Walle <bwalle@suse.de>
Date:   Thu Oct 18 23:40:59 2007 -0700

    Use extended crashkernel command line on x86_64
    
    This patch removes the crashkernel parsing from
    arch/x86_64/kernel/machine_kexec.c and calls the generic function, introduced
    in the last patch, in setup_bootmem_allocator().
    
    This is necessary because the amount of System RAM must be known in this
    function now because of the new syntax.
    
    Signed-off-by: Bernhard Walle <bwalle@suse.de>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Vivek Goyal <vgoyal@in.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 7450b69710b5..0d8577f05422 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -231,33 +231,6 @@ NORET_TYPE void machine_kexec(struct kimage *image)
 			image->start);
 }
 
-/* crashkernel=size@addr specifies the location to reserve for
- * a crash kernel.  By reserving this memory we guarantee
- * that linux never set's it up as a DMA target.
- * Useful for holding code to do something appropriate
- * after a kernel panic.
- */
-static int __init setup_crashkernel(char *arg)
-{
-	unsigned long size, base;
-	char *p;
-	if (!arg)
-		return -EINVAL;
-	size = memparse(arg, &p);
-	if (arg == p)
-		return -EINVAL;
-	if (*p == '@') {
-		base = memparse(p+1, &p);
-		/* FIXME: Do I want a sanity check to validate the
-		 * memory range?  Yes you do, but it's too early for
-		 * e820 -AK */
-		crashk_res.start = base;
-		crashk_res.end   = base + size - 1;
-	}
-	return 0;
-}
-early_param("crashkernel", setup_crashkernel);
-
 void arch_crash_save_vmcoreinfo(void)
 {
 #ifdef CONFIG_ARCH_DISCONTIGMEM_ENABLE

commit bcbba6c10ef6b14b0542d7ed7380e95168175818
Author: Ken'ichi Ohmichi <oomichi@mxs.nes.nec.co.jp>
Date:   Tue Oct 16 23:27:30 2007 -0700

    add-vmcore: add a prefix "VMCOREINFO_" to the vmcoreinfo macros
    
    Add a prefix "VMCOREINFO_" to the vmcoreinfo macros.  Old vmcoreinfo macros
    were defined as generic names SYMBOL/SIZE/OFFSET /LENGTH/CONFIG, and it is
    impossible to grep for them.  So these names should be changed.  This
    discussion is the following:
    http://www.ussg.iu.edu/hypermail/linux/kernel/0709.1/0415.html
    
    Signed-off-by: Ken'ichi Ohmichi <oomichi@mxs.nes.nec.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index e333ea110a58..7450b69710b5 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -261,8 +261,8 @@ early_param("crashkernel", setup_crashkernel);
 void arch_crash_save_vmcoreinfo(void)
 {
 #ifdef CONFIG_ARCH_DISCONTIGMEM_ENABLE
-	SYMBOL(node_data);
-	LENGTH(node_data, MAX_NUMNODES);
+	VMCOREINFO_SYMBOL(node_data);
+	VMCOREINFO_LENGTH(node_data, MAX_NUMNODES);
 #endif
 }
 

commit fd59d231f81cb02870b9cf15f456a897f3669b4e
Author: Ken'ichi Ohmichi <oomichi@mxs.nes.nec.co.jp>
Date:   Tue Oct 16 23:27:27 2007 -0700

    Add vmcoreinfo
    
    This patch set frees the restriction that makedumpfile users should install a
    vmlinux file (including the debugging information) into each system.
    
    makedumpfile command is the dump filtering feature for kdump.  It creates a
    small dumpfile by filtering unnecessary pages for the analysis.  To
    distinguish unnecessary pages, it needs a vmlinux file including the debugging
    information.  These days, the debugging package becomes a huge file, and it is
    hard to install it into each system.
    
    To solve the problem, kdump developers discussed it at lkml and kexec-ml.  As
    the result, we reached the conclusion that necessary information for dump
    filtering (called "vmcoreinfo") should be embedded into the first kernel file
    and it should be accessed through /proc/vmcore during the second kernel.
    (http://www.uwsg.iu.edu/hypermail/linux/kernel/0707.0/1806.html)
    
    Dan Aloni created the patch set for the above implementation.
    (http://www.uwsg.iu.edu/hypermail/linux/kernel/0707.1/1053.html)
    
    And I updated it for multi architectures and memory models.
    (http://lists.infradead.org/pipermail/kexec/2007-August/000479.html)
    
    Signed-off-by: Dan Aloni <da-x@monatomic.org>
    Signed-off-by: Ken'ichi Ohmichi <oomichi@mxs.nes.nec.co.jp>
    Signed-off-by: Bernhard Walle <bwalle@suse.de>
    Signed-off-by: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index cd1899a2f0c5..e333ea110a58 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -10,6 +10,7 @@
 #include <linux/kexec.h>
 #include <linux/string.h>
 #include <linux/reboot.h>
+#include <linux/numa.h>
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
@@ -257,3 +258,11 @@ static int __init setup_crashkernel(char *arg)
 }
 early_param("crashkernel", setup_crashkernel);
 
+void arch_crash_save_vmcoreinfo(void)
+{
+#ifdef CONFIG_ARCH_DISCONTIGMEM_ENABLE
+	SYMBOL(node_data);
+	LENGTH(node_data, MAX_NUMNODES);
+#endif
+}
+

commit 835c34a1687f524c37d4fb8bad18d642c74bed8d
Author: Dave Jones <davej@redhat.com>
Date:   Fri Oct 12 21:10:53 2007 -0400

    Delete filenames in comments.
    
    Since the x86 merge, lots of files that referenced their own filenames
    are no longer correct.  Rather than keep them up to date, just delete
    them, as they add no real value.
    
    Additionally:
    - fix up comment formatting in scx200_32.c
    - Remove a credit from myself in setup_64.c from a time when we had no SCM
    - remove longwinded history from tsc_32.c which can be figured out from
      git.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index c3a554703672..cd1899a2f0c5 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -1,5 +1,5 @@
 /*
- * machine_kexec.c - handle transition of Linux booting another kernel
+ * handle transition of Linux booting another kernel
  * Copyright (C) 2002-2005 Eric Biederman  <ebiederm@xmission.com>
  *
  * This source code is licensed under the GNU General Public License,

commit 250c22777fe1ccd7ac588579a6c16db4c0161cc5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:17:24 2007 +0200

    x86_64: move kernel
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
new file mode 100644
index 000000000000..c3a554703672
--- /dev/null
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -0,0 +1,259 @@
+/*
+ * machine_kexec.c - handle transition of Linux booting another kernel
+ * Copyright (C) 2002-2005 Eric Biederman  <ebiederm@xmission.com>
+ *
+ * This source code is licensed under the GNU General Public License,
+ * Version 2.  See the file COPYING for more details.
+ */
+
+#include <linux/mm.h>
+#include <linux/kexec.h>
+#include <linux/string.h>
+#include <linux/reboot.h>
+#include <asm/pgtable.h>
+#include <asm/tlbflush.h>
+#include <asm/mmu_context.h>
+#include <asm/io.h>
+
+#define PAGE_ALIGNED __attribute__ ((__aligned__(PAGE_SIZE)))
+static u64 kexec_pgd[512] PAGE_ALIGNED;
+static u64 kexec_pud0[512] PAGE_ALIGNED;
+static u64 kexec_pmd0[512] PAGE_ALIGNED;
+static u64 kexec_pte0[512] PAGE_ALIGNED;
+static u64 kexec_pud1[512] PAGE_ALIGNED;
+static u64 kexec_pmd1[512] PAGE_ALIGNED;
+static u64 kexec_pte1[512] PAGE_ALIGNED;
+
+static void init_level2_page(pmd_t *level2p, unsigned long addr)
+{
+	unsigned long end_addr;
+
+	addr &= PAGE_MASK;
+	end_addr = addr + PUD_SIZE;
+	while (addr < end_addr) {
+		set_pmd(level2p++, __pmd(addr | __PAGE_KERNEL_LARGE_EXEC));
+		addr += PMD_SIZE;
+	}
+}
+
+static int init_level3_page(struct kimage *image, pud_t *level3p,
+				unsigned long addr, unsigned long last_addr)
+{
+	unsigned long end_addr;
+	int result;
+
+	result = 0;
+	addr &= PAGE_MASK;
+	end_addr = addr + PGDIR_SIZE;
+	while ((addr < last_addr) && (addr < end_addr)) {
+		struct page *page;
+		pmd_t *level2p;
+
+		page = kimage_alloc_control_pages(image, 0);
+		if (!page) {
+			result = -ENOMEM;
+			goto out;
+		}
+		level2p = (pmd_t *)page_address(page);
+		init_level2_page(level2p, addr);
+		set_pud(level3p++, __pud(__pa(level2p) | _KERNPG_TABLE));
+		addr += PUD_SIZE;
+	}
+	/* clear the unused entries */
+	while (addr < end_addr) {
+		pud_clear(level3p++);
+		addr += PUD_SIZE;
+	}
+out:
+	return result;
+}
+
+
+static int init_level4_page(struct kimage *image, pgd_t *level4p,
+				unsigned long addr, unsigned long last_addr)
+{
+	unsigned long end_addr;
+	int result;
+
+	result = 0;
+	addr &= PAGE_MASK;
+	end_addr = addr + (PTRS_PER_PGD * PGDIR_SIZE);
+	while ((addr < last_addr) && (addr < end_addr)) {
+		struct page *page;
+		pud_t *level3p;
+
+		page = kimage_alloc_control_pages(image, 0);
+		if (!page) {
+			result = -ENOMEM;
+			goto out;
+		}
+		level3p = (pud_t *)page_address(page);
+		result = init_level3_page(image, level3p, addr, last_addr);
+		if (result) {
+			goto out;
+		}
+		set_pgd(level4p++, __pgd(__pa(level3p) | _KERNPG_TABLE));
+		addr += PGDIR_SIZE;
+	}
+	/* clear the unused entries */
+	while (addr < end_addr) {
+		pgd_clear(level4p++);
+		addr += PGDIR_SIZE;
+	}
+out:
+	return result;
+}
+
+
+static int init_pgtable(struct kimage *image, unsigned long start_pgtable)
+{
+	pgd_t *level4p;
+	level4p = (pgd_t *)__va(start_pgtable);
+ 	return init_level4_page(image, level4p, 0, end_pfn << PAGE_SHIFT);
+}
+
+static void set_idt(void *newidt, u16 limit)
+{
+	struct desc_ptr curidt;
+
+	/* x86-64 supports unaliged loads & stores */
+	curidt.size    = limit;
+	curidt.address = (unsigned long)newidt;
+
+	__asm__ __volatile__ (
+		"lidtq %0\n"
+		: : "m" (curidt)
+		);
+};
+
+
+static void set_gdt(void *newgdt, u16 limit)
+{
+	struct desc_ptr curgdt;
+
+	/* x86-64 supports unaligned loads & stores */
+	curgdt.size    = limit;
+	curgdt.address = (unsigned long)newgdt;
+
+	__asm__ __volatile__ (
+		"lgdtq %0\n"
+		: : "m" (curgdt)
+		);
+};
+
+static void load_segments(void)
+{
+	__asm__ __volatile__ (
+		"\tmovl %0,%%ds\n"
+		"\tmovl %0,%%es\n"
+		"\tmovl %0,%%ss\n"
+		"\tmovl %0,%%fs\n"
+		"\tmovl %0,%%gs\n"
+		: : "a" (__KERNEL_DS) : "memory"
+		);
+}
+
+int machine_kexec_prepare(struct kimage *image)
+{
+	unsigned long start_pgtable;
+	int result;
+
+	/* Calculate the offsets */
+	start_pgtable = page_to_pfn(image->control_code_page) << PAGE_SHIFT;
+
+	/* Setup the identity mapped 64bit page table */
+	result = init_pgtable(image, start_pgtable);
+	if (result)
+		return result;
+
+	return 0;
+}
+
+void machine_kexec_cleanup(struct kimage *image)
+{
+	return;
+}
+
+/*
+ * Do not allocate memory (or fail in any way) in machine_kexec().
+ * We are past the point of no return, committed to rebooting now.
+ */
+NORET_TYPE void machine_kexec(struct kimage *image)
+{
+	unsigned long page_list[PAGES_NR];
+	void *control_page;
+
+	/* Interrupts aren't acceptable while we reboot */
+	local_irq_disable();
+
+	control_page = page_address(image->control_code_page) + PAGE_SIZE;
+	memcpy(control_page, relocate_kernel, PAGE_SIZE);
+
+	page_list[PA_CONTROL_PAGE] = virt_to_phys(control_page);
+	page_list[VA_CONTROL_PAGE] = (unsigned long)relocate_kernel;
+	page_list[PA_PGD] = virt_to_phys(&kexec_pgd);
+	page_list[VA_PGD] = (unsigned long)kexec_pgd;
+	page_list[PA_PUD_0] = virt_to_phys(&kexec_pud0);
+	page_list[VA_PUD_0] = (unsigned long)kexec_pud0;
+	page_list[PA_PMD_0] = virt_to_phys(&kexec_pmd0);
+	page_list[VA_PMD_0] = (unsigned long)kexec_pmd0;
+	page_list[PA_PTE_0] = virt_to_phys(&kexec_pte0);
+	page_list[VA_PTE_0] = (unsigned long)kexec_pte0;
+	page_list[PA_PUD_1] = virt_to_phys(&kexec_pud1);
+	page_list[VA_PUD_1] = (unsigned long)kexec_pud1;
+	page_list[PA_PMD_1] = virt_to_phys(&kexec_pmd1);
+	page_list[VA_PMD_1] = (unsigned long)kexec_pmd1;
+	page_list[PA_PTE_1] = virt_to_phys(&kexec_pte1);
+	page_list[VA_PTE_1] = (unsigned long)kexec_pte1;
+
+	page_list[PA_TABLE_PAGE] =
+	  (unsigned long)__pa(page_address(image->control_code_page));
+
+	/* The segment registers are funny things, they have both a
+	 * visible and an invisible part.  Whenever the visible part is
+	 * set to a specific selector, the invisible part is loaded
+	 * with from a table in memory.  At no other time is the
+	 * descriptor table in memory accessed.
+	 *
+	 * I take advantage of this here by force loading the
+	 * segments, before I zap the gdt with an invalid value.
+	 */
+	load_segments();
+	/* The gdt & idt are now invalid.
+	 * If you want to load them you must set up your own idt & gdt.
+	 */
+	set_gdt(phys_to_virt(0),0);
+	set_idt(phys_to_virt(0),0);
+
+	/* now call it */
+	relocate_kernel((unsigned long)image->head, (unsigned long)page_list,
+			image->start);
+}
+
+/* crashkernel=size@addr specifies the location to reserve for
+ * a crash kernel.  By reserving this memory we guarantee
+ * that linux never set's it up as a DMA target.
+ * Useful for holding code to do something appropriate
+ * after a kernel panic.
+ */
+static int __init setup_crashkernel(char *arg)
+{
+	unsigned long size, base;
+	char *p;
+	if (!arg)
+		return -EINVAL;
+	size = memparse(arg, &p);
+	if (arg == p)
+		return -EINVAL;
+	if (*p == '@') {
+		base = memparse(p+1, &p);
+		/* FIXME: Do I want a sanity check to validate the
+		 * memory range?  Yes you do, but it's too early for
+		 * e820 -AK */
+		crashk_res.start = base;
+		crashk_res.end   = base + size - 1;
+	}
+	return 0;
+}
+early_param("crashkernel", setup_crashkernel);
+
