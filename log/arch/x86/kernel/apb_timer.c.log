commit 003d80535180f74f262c40462b9fccd7f004901a
Author: Johan Hovold <johan@kernel.org>
Date:   Wed May 13 12:09:43 2020 +0200

    x86/apb_timer: Drop unused TSC calibration
    
    Drop the APB-timer TSC calibration, which hasn't been used since the
    removal of Moorestown support by commit
    
      1a8359e411eb ("x86/mid: Remove Intel Moorestown").
    
    Signed-off-by: Johan Hovold <johan@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200513100944.9171-1-johan@kernel.org

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index fe698f96617c..263eeaddb0aa 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -345,56 +345,3 @@ void __init apbt_time_init(void)
 	apb_timer_block_enabled = 0;
 	panic("failed to enable APB timer\n");
 }
-
-/* called before apb_timer_enable, use early map */
-unsigned long apbt_quick_calibrate(void)
-{
-	int i, scale;
-	u64 old, new;
-	u64 t1, t2;
-	unsigned long khz = 0;
-	u32 loop, shift;
-
-	apbt_set_mapping();
-	dw_apb_clocksource_start(clocksource_apbt);
-
-	/* check if the timer can count down, otherwise return */
-	old = dw_apb_clocksource_read(clocksource_apbt);
-	i = 10000;
-	while (--i) {
-		if (old != dw_apb_clocksource_read(clocksource_apbt))
-			break;
-	}
-	if (!i)
-		goto failed;
-
-	/* count 16 ms */
-	loop = (apbt_freq / 1000) << 4;
-
-	/* restart the timer to ensure it won't get to 0 in the calibration */
-	dw_apb_clocksource_start(clocksource_apbt);
-
-	old = dw_apb_clocksource_read(clocksource_apbt);
-	old += loop;
-
-	t1 = rdtsc();
-
-	do {
-		new = dw_apb_clocksource_read(clocksource_apbt);
-	} while (new < old);
-
-	t2 = rdtsc();
-
-	shift = 5;
-	if (unlikely(loop >> shift == 0)) {
-		printk(KERN_INFO
-		       "APBT TSC calibration failed, not enough resolution\n");
-		return 0;
-	}
-	scale = (int)div_u64((t2 - t1), loop >> shift);
-	khz = (scale * (apbt_freq / 1000)) >> shift;
-	printk(KERN_INFO "TSC freq calculated by APB timer is %lu khz\n", khz);
-	return khz;
-failed:
-	return 0;
-}

commit 4bdc0d676a643140bdf17dbf7eafedee3d496a3c
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jan 6 09:43:50 2020 +0100

    remove ioremap_nocache and devm_ioremap_nocache
    
    ioremap has provided non-cached semantics by default since the Linux 2.6
    days, so remove the additional ioremap_nocache interface.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 5da106f84e84..fe698f96617c 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -95,7 +95,7 @@ static inline void apbt_set_mapping(void)
 		printk(KERN_WARNING "No timer base from SFI, use default\n");
 		apbt_address = APBT_DEFAULT_BASE;
 	}
-	apbt_virt_address = ioremap_nocache(apbt_address, APBT_MMAP_SIZE);
+	apbt_virt_address = ioremap(apbt_address, APBT_MMAP_SIZE);
 	if (!apbt_virt_address) {
 		pr_debug("Failed mapping APBT phy address at %lu\n",\
 			 (unsigned long)apbt_address);

commit b886d83c5b621abc84ff9616f14c529be3f6b147
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jun 1 10:08:55 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 441
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation version 2 of the license
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 315 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Armijn Hemel <armijn@tjaldur.nl>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190531190115.503150771@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 65721dc73bd8..5da106f84e84 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -1,14 +1,10 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * apb_timer.c: Driver for Langwell APB timers
  *
  * (C) Copyright 2009 Intel Corporation
  * Author: Jacob Pan (jacob.jun.pan@intel.com)
  *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; version 2
- * of the License.
- *
  * Note:
  * Langwell is the south complex of Intel Moorestown MID platform. There are
  * eight external timers in total that can be used by the operating system.

commit 3ddc76dfc786cc6f87852693227fb0b1f124f807
Merge: b272f732f888 1f3a8e49d8f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 25 14:30:04 2016 -0800

    Merge branch 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer type cleanups from Thomas Gleixner:
     "This series does a tree wide cleanup of types related to
      timers/timekeeping.
    
       - Get rid of cycles_t and use a plain u64. The type is not really
         helpful and caused more confusion than clarity
    
       - Get rid of the ktime union. The union has become useless as we use
         the scalar nanoseconds storage unconditionally now. The 32bit
         timespec alike storage got removed due to the Y2038 limitations
         some time ago.
    
         That leaves the odd union access around for no reason. Clean it up.
    
      Both changes have been done with coccinelle and a small amount of
      manual mopping up"
    
    * 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      ktime: Get rid of ktime_equal()
      ktime: Cleanup ktime_set() usage
      ktime: Get rid of the union
      clocksource: Use a plain u64 instead of cycle_t

commit a5a1d1c2914b5316924c7893eb683a5420ebd3be
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:32:01 2016 +0100

    clocksource: Use a plain u64 instead of cycle_t
    
    There is no point in having an extra type for extra confusion. u64 is
    unambiguous.
    
    Conversion was done with the following coccinelle script:
    
    @rem@
    @@
    -typedef u64 cycle_t;
    
    @fix@
    typedef cycle_t;
    @@
    -cycle_t
    +u64
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 456316f6c868..092ea664d2c6 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -247,7 +247,7 @@ void apbt_setup_secondary_clock(void) {}
 static int apbt_clocksource_register(void)
 {
 	u64 start, now;
-	cycle_t t1;
+	u64 t1;
 
 	/* Start the counter, use timer 2 as source, timer 0/1 for event */
 	dw_apb_clocksource_start(clocksource_apbt);
@@ -355,7 +355,7 @@ unsigned long apbt_quick_calibrate(void)
 {
 	int i, scale;
 	u64 old, new;
-	cycle_t t1, t2;
+	u64 t1, t2;
 	unsigned long khz = 0;
 	u32 loop, shift;
 

commit 73c1b41e63f040e92669e61a02c7893933bfe743
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:19:54 2016 +0100

    cpu/hotplug: Cleanup state names
    
    When the state names got added a script was used to add the extra argument
    to the calls. The script basically converted the state constant to a
    string, but the cleanup to convert these strings into meaningful ones did
    not happen.
    
    Replace all the useless strings with 'subsys/xxx/yyy:state' strings which
    are used in all the other places already.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Link: http://lkml.kernel.org/r/20161221192112.085444152@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 456316f6c868..202a7817beaf 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -234,7 +234,7 @@ static __init int apbt_late_init(void)
 	if (intel_mid_timer_options == INTEL_MID_TIMER_LAPIC_APBT ||
 		!apb_timer_block_enabled)
 		return 0;
-	return cpuhp_setup_state(CPUHP_X86_APB_DEAD, "X86_APB_DEAD", NULL,
+	return cpuhp_setup_state(CPUHP_X86_APB_DEAD, "x86/apb:dead", NULL,
 				 apbt_cpu_dead);
 }
 fs_initcall(apbt_late_init);

commit 148b9e2abea6f50e4620e4ac0683b4913c4364c0
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jul 13 17:16:34 2016 +0000

    x86/apb_timer: Convert to hotplug state machine
    
    Install the callbacks via the state machine. There is no setup just one
    teardown callback. Remove the silly comment about the workqueue up dependency.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160713153335.625342983@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index cefacbad1531..456316f6c868 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -215,26 +215,18 @@ void apbt_setup_secondary_clock(void)
  * cpu timers during the offline process due to the ordering of notification.
  * the extra interrupt is harmless.
  */
-static int apbt_cpuhp_notify(struct notifier_block *n,
-			     unsigned long action, void *hcpu)
+static int apbt_cpu_dead(unsigned int cpu)
 {
-	unsigned long cpu = (unsigned long)hcpu;
 	struct apbt_dev *adev = &per_cpu(cpu_apbt_dev, cpu);
 
-	switch (action & ~CPU_TASKS_FROZEN) {
-	case CPU_DEAD:
-		dw_apb_clockevent_pause(adev->timer);
-		if (system_state == SYSTEM_RUNNING) {
-			pr_debug("skipping APBT CPU %lu offline\n", cpu);
-		} else {
-			pr_debug("APBT clockevent for cpu %lu offline\n", cpu);
-			dw_apb_clockevent_stop(adev->timer);
-		}
-		break;
-	default:
-		pr_debug("APBT notified %lu, no action\n", action);
+	dw_apb_clockevent_pause(adev->timer);
+	if (system_state == SYSTEM_RUNNING) {
+		pr_debug("skipping APBT CPU %u offline\n", cpu);
+	} else {
+		pr_debug("APBT clockevent for cpu %u offline\n", cpu);
+		dw_apb_clockevent_stop(adev->timer);
 	}
-	return NOTIFY_OK;
+	return 0;
 }
 
 static __init int apbt_late_init(void)
@@ -242,9 +234,8 @@ static __init int apbt_late_init(void)
 	if (intel_mid_timer_options == INTEL_MID_TIMER_LAPIC_APBT ||
 		!apb_timer_block_enabled)
 		return 0;
-	/* This notifier should be called after workqueue is ready */
-	hotcpu_notifier(apbt_cpuhp_notify, -20);
-	return 0;
+	return cpuhp_setup_state(CPUHP_X86_APB_DEAD, "X86_APB_DEAD", NULL,
+				 apbt_cpu_dead);
 }
 fs_initcall(apbt_late_init);
 #else

commit a38f98735e168a20573c24dfffa96095b6fe1d23
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 19 11:41:42 2016 +0100

    x86/apb/timer: Use proper mask to modify hotplug action
    
    Magic hex constants are a guarantee for wreckage when the defines change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 222a57076039..cefacbad1531 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -221,7 +221,7 @@ static int apbt_cpuhp_notify(struct notifier_block *n,
 	unsigned long cpu = (unsigned long)hcpu;
 	struct apbt_dev *adev = &per_cpu(cpu_apbt_dev, cpu);
 
-	switch (action & 0xf) {
+	switch (action & ~CPU_TASKS_FROZEN) {
 	case CPU_DEAD:
 		dw_apb_clockevent_pause(adev->timer);
 		if (system_state == SYSTEM_RUNNING) {

commit 4ea1636b04dbd66536fa387bae2eea463efc705b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:44:07 2015 +0200

    x86/asm/tsc: Rename native_read_tsc() to rdtsc()
    
    Now that there is no paravirt TSC, the "native" is
    inappropriate. The function does RDTSC, so give it the obvious
    name: rdtsc().
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/fd43e16281991f096c1e4d21574d9e1402c62d39.1434501121.git.luto@kernel.org
    [ Ported it to v4.2-rc1. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 25efa534c4e4..222a57076039 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -263,7 +263,7 @@ static int apbt_clocksource_register(void)
 
 	/* Verify whether apbt counter works */
 	t1 = dw_apb_clocksource_read(clocksource_apbt);
-	start = native_read_tsc();
+	start = rdtsc();
 
 	/*
 	 * We don't know the TSC frequency yet, but waiting for
@@ -273,7 +273,7 @@ static int apbt_clocksource_register(void)
 	 */
 	do {
 		rep_nop();
-		now = native_read_tsc();
+		now = rdtsc();
 	} while ((now - start) < 200000UL);
 
 	/* APBT is the only always on clocksource, it has to work! */
@@ -390,13 +390,13 @@ unsigned long apbt_quick_calibrate(void)
 	old = dw_apb_clocksource_read(clocksource_apbt);
 	old += loop;
 
-	t1 = native_read_tsc();
+	t1 = rdtsc();
 
 	do {
 		new = dw_apb_clocksource_read(clocksource_apbt);
 	} while (new < old);
 
-	t2 = native_read_tsc();
+	t2 = rdtsc();
 
 	shift = 5;
 	if (unlikely(loop >> shift == 0)) {

commit 87be28aaf1458445d5f648688c2eec0f13b8f3b9
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:43:58 2015 +0200

    x86/asm/tsc: Replace rdtscll() with native_read_tsc()
    
    Now that the ->read_tsc() paravirt hook is gone, rdtscll() is
    just a wrapper around native_read_tsc(). Unwrap it.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/d2449ae62c1b1fb90195bcfb19ef4a35883a04dc.1434501121.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 9fe111cc50f8..25efa534c4e4 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -263,7 +263,7 @@ static int apbt_clocksource_register(void)
 
 	/* Verify whether apbt counter works */
 	t1 = dw_apb_clocksource_read(clocksource_apbt);
-	rdtscll(start);
+	start = native_read_tsc();
 
 	/*
 	 * We don't know the TSC frequency yet, but waiting for
@@ -273,7 +273,7 @@ static int apbt_clocksource_register(void)
 	 */
 	do {
 		rep_nop();
-		rdtscll(now);
+		now = native_read_tsc();
 	} while ((now - start) < 200000UL);
 
 	/* APBT is the only always on clocksource, it has to work! */

commit c6e5ca35c4685cd920b1d5279dbc9f4483d7dfd4
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:43:55 2015 +0200

    x86/asm/tsc: Inline native_read_tsc() and remove __native_read_tsc()
    
    In the following commit:
    
      cdc7957d1954 ("x86: move native_read_tsc() offline")
    
    ... native_read_tsc() was moved out of line, presumably for some
    now-obsolete vDSO-related reason. Undo it.
    
    The entire rdtsc, shl, or sequence is only 11 bytes, and calls
    via rdtscl() and similar helpers were already inlined.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/d05ffe2aaf8468ca475ebc00efad7b2fa174af19.1434501121.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index ede92c3364d3..9fe111cc50f8 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -390,13 +390,13 @@ unsigned long apbt_quick_calibrate(void)
 	old = dw_apb_clocksource_read(clocksource_apbt);
 	old += loop;
 
-	t1 = __native_read_tsc();
+	t1 = native_read_tsc();
 
 	do {
 		new = dw_apb_clocksource_read(clocksource_apbt);
 	} while (new < old);
 
-	t2 = __native_read_tsc();
+	t2 = native_read_tsc();
 
 	shift = 5;
 	if (unlikely(loop >> shift == 0)) {

commit 6648d1b42c349d748839d7bad91cc8a65c73e262
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Apr 13 14:11:51 2015 +0800

    x86/intel-mid: Delay initialization of APB timer
    
    MID has no PIC, but depending on the platform it requires the
    abt_timer, which is connected to irq0. The timer is set up at
    late_time_init().
    
    But, looking at the MID code it seems, that there is no reason to do
    so. The only code which might need the timer working is the TSC
    calibration code, but thats a non issue on MID as that is using its
    own empty calibration function. And check_timer() is not invoked
    either because MID has no PIC and therefor no legacy irqs.
    
    So if you look at intel_mid_time_init() then you'll see that in the
    ARAT case the timer setup is skipped already. So until the point where
    x86_init.timers.setup_percpu_clockev() is called for the boot cpu
    nothing really needs a timer on MID.
    
    According to the MID code the apbt horror is only used for moorestown.
    Medfield and later use the local apic timer without the apbt nonsense.
    
    The best thing we can do is to drop moorestown support and get rid of
    that apbt nonsense alltogether.
    
    I don't think anyone deeply cares about it not being supported from
    3.18 on. The number of devices which sport a moorestown should be
    pretty limited and the only relevant use case of those is to act as a
    pocket heater with short battery life time. Its pretty pointless to
    update kernels on pocket heaters except for bragging reasons.
    
    If someone at Intel really thinks that we need to keep moorestown
    alive for other than documentary and sentimental reasons, then we can
    move the apbt setup to x86_init.timers.setup_percpu_clockev(). At that
    point the IOAPIC is setup already, so it should just work.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Signed-off-by: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: David Cohen <david.a.cohen@linux.intel.com>
    Cc: Sander Eikelenboom <linux@eikelenboom.it>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Kuppuswamy Sathyanarayanan <sathyanarayanan.kuppuswamy@linux.intel.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dimitri Sivanich <sivanich@sgi.com>
    Cc: Rickard Strandqvist <rickard_strandqvist@spectrumdigital.se>
    Link: http://lkml.kernel.org/r/1428905519-23704-30-git-send-email-jiang.liu@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 6a7c23ff21d3..ede92c3364d3 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -171,10 +171,6 @@ static int __init apbt_clockevent_register(void)
 
 static void apbt_setup_irq(struct apbt_dev *adev)
 {
-	/* timer0 irq has been setup early */
-	if (adev->irq == 0)
-		return;
-
 	irq_modify_status(adev->irq, 0, IRQ_MOVE_PCNTXT);
 	irq_set_affinity(adev->irq, cpumask_of(adev->cpu));
 }

commit 2b261f9f7bd9333e62140ca7e02a2f4ffec5e44f
Author: Rickard Strandqvist <rickard_strandqvist@spectrumdigital.se>
Date:   Sun Dec 21 13:58:18 2014 +0100

    x86/platform: Remove unused function from apb_timer.c
    
    Remove the function is_apbt_capable() that is not used anywhere.
    
    This was partially found by using a static code analysis program
    called 'cppcheck'.
    
    Signed-off-by: Rickard Strandqvist <rickard_strandqvist@spectrumdigital.se>
    Reviewed-by: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/1419166698-2470-1-git-send-email-rickard_strandqvist@spectrumdigital.se
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index b708738d016e..6a7c23ff21d3 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -135,14 +135,6 @@ static inline void apbt_clear_mapping(void)
 	apbt_virt_address = NULL;
 }
 
-/*
- * APBT timer interrupt enable / disable
- */
-static inline int is_apbt_capable(void)
-{
-	return apbt_virt_address ? 1 : 0;
-}
-
 static int __init apbt_clockevent_register(void)
 {
 	struct sfi_timer_table_entry *mtmr;

commit f18298595aefa2c836a128ec6e0f75f39965dd81
Author: Jiang Liu <jiang.liu@linux.intel.com>
Date:   Mon Oct 27 13:21:32 2014 +0800

    x86, intel-mid: Create IRQs for APB timers and RTC timers
    
    Intel MID platforms has no legacy interrupts, so no IRQ descriptors
    preallocated. We need to call mp_map_gsi_to_irq() to create IRQ
    descriptors for APB timers and RTC timers, otherwise it may cause
    invalid memory access as:
    [    0.116839] BUG: unable to handle kernel NULL pointer dereference at
    0000003a
    [    0.123803] IP: [<c1071c0e>] setup_irq+0xf/0x4d
    
    Tested-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Signed-off-by: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: David Cohen <david.a.cohen@linux.intel.com>
    Cc: <stable@vger.kernel.org> # 3.17
    Link: http://lkml.kernel.org/r/1414387308-27148-3-git-send-email-jiang.liu@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 5972b108f15a..b708738d016e 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -185,8 +185,6 @@ static void apbt_setup_irq(struct apbt_dev *adev)
 
 	irq_modify_status(adev->irq, 0, IRQ_MOVE_PCNTXT);
 	irq_set_affinity(adev->irq, cpumask_of(adev->cpu));
-	/* APB timer irqs are set up as mp_irqs, timer is edge type */
-	__irq_set_handler(adev->irq, handle_edge_irq, 0, "edge");
 }
 
 /* Should be called with per cpu */

commit 89cbc76768c2fa4ed95545bf961f3a14ddfeed21
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Aug 17 12:30:40 2014 -0500

    x86: Replace __get_cpu_var uses
    
    __get_cpu_var() is used for multiple purposes in the kernel source. One of
    them is address calculation via the form &__get_cpu_var(x).  This calculates
    the address for the instance of the percpu variable of the current processor
    based on an offset.
    
    Other use cases are for storing and retrieving data from the current
    processors percpu area.  __get_cpu_var() can be used as an lvalue when
    writing data or on the right side of an assignment.
    
    __get_cpu_var() is defined as :
    
    #define __get_cpu_var(var) (*this_cpu_ptr(&(var)))
    
    __get_cpu_var() always only does an address determination. However, store
    and retrieve operations could use a segment prefix (or global register on
    other platforms) to avoid the address calculation.
    
    this_cpu_write() and this_cpu_read() can directly take an offset into a
    percpu area and use optimized assembly code to read and write per cpu
    variables.
    
    This patch converts __get_cpu_var into either an explicit address
    calculation using this_cpu_ptr() or into a use of this_cpu operations that
    use the offset.  Thereby address calculations are avoided and less registers
    are used when code is generated.
    
    Transformations done to __get_cpu_var()
    
    1. Determine the address of the percpu instance of the current processor.
    
            DEFINE_PER_CPU(int, y);
            int *x = &__get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(&y);
    
    2. Same as #1 but this time an array structure is involved.
    
            DEFINE_PER_CPU(int, y[20]);
            int *x = __get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(y);
    
    3. Retrieve the content of the current processors instance of a per cpu
    variable.
    
            DEFINE_PER_CPU(int, y);
            int x = __get_cpu_var(y)
    
       Converts to
    
            int x = __this_cpu_read(y);
    
    4. Retrieve the content of a percpu struct
    
            DEFINE_PER_CPU(struct mystruct, y);
            struct mystruct x = __get_cpu_var(y);
    
       Converts to
    
            memcpy(&x, this_cpu_ptr(&y), sizeof(x));
    
    5. Assignment to a per cpu variable
    
            DEFINE_PER_CPU(int, y)
            __get_cpu_var(y) = x;
    
       Converts to
    
            __this_cpu_write(y, x);
    
    6. Increment/Decrement etc of a per cpu variable
    
            DEFINE_PER_CPU(int, y);
            __get_cpu_var(y)++
    
       Converts to
    
            __this_cpu_inc(y)
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86@kernel.org
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index af5b08ab3b71..5972b108f15a 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -146,7 +146,7 @@ static inline int is_apbt_capable(void)
 static int __init apbt_clockevent_register(void)
 {
 	struct sfi_timer_table_entry *mtmr;
-	struct apbt_dev *adev = &__get_cpu_var(cpu_apbt_dev);
+	struct apbt_dev *adev = this_cpu_ptr(&cpu_apbt_dev);
 
 	mtmr = sfi_get_mtmr(APBT_CLOCKEVENT0_NUM);
 	if (mtmr == NULL) {
@@ -200,7 +200,7 @@ void apbt_setup_secondary_clock(void)
 	if (!cpu)
 		return;
 
-	adev = &__get_cpu_var(cpu_apbt_dev);
+	adev = this_cpu_ptr(&cpu_apbt_dev);
 	if (!adev->timer) {
 		adev->timer = dw_apb_clockevent_init(cpu, adev->name,
 			APBT_CLOCKEVENT_RATING, adev_virt_addr(adev),

commit 712b6aa8731a7e148298c58cea66a5209c659e3c
Author: Kuppuswamy Sathyanarayanan <sathyanarayanan.kuppuswamy@linux.intel.com>
Date:   Thu Oct 17 15:35:29 2013 -0700

    intel_mid: Renamed *mrst* to *intel_mid*
    
    mrst is used as common name to represent all intel_mid type
    soc's. But moorsetwon is just one of the intel_mid soc. So
    renamed them to use intel_mid.
    
    This patch mainly renames the variables and related
    functions that uses *mrst* prefix with *intel_mid*.
    
    To ensure that there are no functional changes, I have compared
    the objdump of related files before and after rename and found
    the only difference is symbol and name changes.
    
    Signed-off-by: Kuppuswamy Sathyanarayanan <sathyanarayanan.kuppuswamy@linux.intel.com>
    Link: http://lkml.kernel.org/r/1382049336-21316-6-git-send-email-david.a.cohen@linux.intel.com
    Signed-off-by: David Cohen <david.a.cohen@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 915483604c0c..af5b08ab3b71 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -157,13 +157,13 @@ static int __init apbt_clockevent_register(void)
 
 	adev->num = smp_processor_id();
 	adev->timer = dw_apb_clockevent_init(smp_processor_id(), "apbt0",
-		mrst_timer_options == MRST_TIMER_LAPIC_APBT ?
+		intel_mid_timer_options == INTEL_MID_TIMER_LAPIC_APBT ?
 		APBT_CLOCKEVENT_RATING - 100 : APBT_CLOCKEVENT_RATING,
 		adev_virt_addr(adev), 0, apbt_freq);
 	/* Firmware does EOI handling for us. */
 	adev->timer->eoi = NULL;
 
-	if (mrst_timer_options == MRST_TIMER_LAPIC_APBT) {
+	if (intel_mid_timer_options == INTEL_MID_TIMER_LAPIC_APBT) {
 		global_clock_event = &adev->timer->ced;
 		printk(KERN_DEBUG "%s clockevent registered as global\n",
 		       global_clock_event->name);
@@ -253,7 +253,7 @@ static int apbt_cpuhp_notify(struct notifier_block *n,
 
 static __init int apbt_late_init(void)
 {
-	if (mrst_timer_options == MRST_TIMER_LAPIC_APBT ||
+	if (intel_mid_timer_options == INTEL_MID_TIMER_LAPIC_APBT ||
 		!apb_timer_block_enabled)
 		return 0;
 	/* This notifier should be called after workqueue is ready */
@@ -340,7 +340,7 @@ void __init apbt_time_init(void)
 	}
 #ifdef CONFIG_SMP
 	/* kernel cmdline disable apb timer, so we will use lapic timers */
-	if (mrst_timer_options == MRST_TIMER_LAPIC_APBT) {
+	if (intel_mid_timer_options == INTEL_MID_TIMER_LAPIC_APBT) {
 		printk(KERN_INFO "apbt: disabled per cpu timer\n");
 		return;
 	}

commit 05454c26eb3587b56abc5eb139797ac5afb6d77a
Author: Kuppuswamy Sathyanarayanan <sathyanarayanan.kuppuswamy@linux.intel.com>
Date:   Thu Oct 17 15:35:27 2013 -0700

    intel_mid: Renamed *mrst* to *intel_mid*
    
    Following files contains code that is common to all intel mid
    soc's. So renamed them as below.
    
    mrst/mrst.c              -> intel-mid/intel-mid.c
    mrst/vrtc.c              -> intel-mid/intel_mid_vrtc.c
    mrst/early_printk_mrst.c -> intel-mid/intel_mid_vrtc.c
    pci/mrst.c               -> pci/intel_mid_pci.c
    
    Also, renamed the corresponding header files and made changes
    to the driver files that included these header files.
    
    To ensure that there are no functional changes, I have compared
    the objdump of renamed files before and after rename and found
    that the only difference is file name change.
    
    Signed-off-by: Kuppuswamy Sathyanarayanan <sathyanarayanan.kuppuswamy@linux.intel.com>
    Link: http://lkml.kernel.org/r/1382049336-21316-4-git-send-email-david.a.cohen@linux.intel.com
    Signed-off-by: David Cohen <david.a.cohen@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index c9876efecafb..915483604c0c 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -40,7 +40,7 @@
 
 #include <asm/fixmap.h>
 #include <asm/apb_timer.h>
-#include <asm/mrst.h>
+#include <asm/intel-mid.h>
 #include <asm/time.h>
 
 #define APBT_CLOCKEVENT_RATING		110

commit f98982ce80f9ce6db0fe841c1844cbae0a2700fb
Merge: 29d50523298e 7d0291256ca9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 19 20:11:07 2013 -0800

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 platform changes from Ingo Molnar:
    
     - Support for the Technologic Systems TS-5500 platform, by Vivien
       Didelot
    
     - Improved NUMA support on AMD systems:
    
       Add support for federated systems where multiple memory controllers
       can exist and see each other over multiple PCI domains.  This
       basically means that AMD node ids can be more than 8 now and the code
       handling this is taught to incorporate PCI domain into those IDs.
    
     - Support for the Goldfish virtual Android emulator, by Jun Nakajima,
       Intel, Google, et al.
    
     - Misc fixlets.
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Add TS-5500 platform support
      x86/srat: Simplify memory affinity init error handling
      x86/apb/timer: Remove unnecessary "if"
      goldfish: platform device for x86
      amd64_edac: Fix type usage in NB IDs and memory ranges
      amd64_edac: Fix PCI function lookup
      x86, AMD, NB: Use u16 for northbridge IDs in amd_get_nb_id
      x86, AMD, NB: Add multi-domain support

commit b9975dabe3f0a6e4d1af52c47f66b5558df207a3
Author: Cong Ding <dinggnu@gmail.com>
Date:   Mon Jan 14 22:39:18 2013 +0100

    x86/apb/timer: Remove unnecessary "if"
    
    adev has no chance to be NULL, so we don't need to check it. It
    is also dereferenced just before the check .
    
    Signed-off-by: Cong Ding <dinggnu@gmail.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/r/1358199561-15518-1-git-send-email-dinggnu@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index afdc3f756dea..cf9273571917 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -240,7 +240,7 @@ static int apbt_cpuhp_notify(struct notifier_block *n,
 		dw_apb_clockevent_pause(adev->timer);
 		if (system_state == SYSTEM_RUNNING) {
 			pr_debug("skipping APBT CPU %lu offline\n", cpu);
-		} else if (adev) {
+		} else {
 			pr_debug("APBT clockevent for cpu %lu offline\n", cpu);
 			dw_apb_clockevent_stop(adev->timer);
 		}

commit 8f170faeb458532282dbfa870f456e42c11d1ebb
Author: Sasha Levin <sasha.levin@oracle.com>
Date:   Thu Dec 20 14:11:36 2012 -0500

    x86, apb_timer: remove unused variable percpu_timer
    
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/r/1356030701-16284-28-git-send-email-sasha.levin@oracle.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index afdc3f756dea..cc74fd0c90f2 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -311,7 +311,6 @@ void __init apbt_time_init(void)
 #ifdef CONFIG_SMP
 	int i;
 	struct sfi_timer_table_entry *p_mtmr;
-	unsigned int percpu_timer;
 	struct apbt_dev *adev;
 #endif
 
@@ -346,13 +345,10 @@ void __init apbt_time_init(void)
 		return;
 	}
 	pr_debug("%s: %d CPUs online\n", __func__, num_online_cpus());
-	if (num_possible_cpus() <= sfi_mtimer_num) {
-		percpu_timer = 1;
+	if (num_possible_cpus() <= sfi_mtimer_num)
 		apbt_num_timers_used = num_possible_cpus();
-	} else {
-		percpu_timer = 0;
+	else
 		apbt_num_timers_used = 1;
-	}
 	pr_debug("%s: %d APB timers used\n", __func__, apbt_num_timers_used);
 
 	/* here we set up per CPU timer data structure */

commit 9d0715630ebf7bf70daa5e6d8db0e3061268c61e
Merge: c0c463d34adf 06c3df49521c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 23 10:34:47 2011 -0700

    Merge branch 'timers-clocksource-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-clocksource-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      clocksource: apb: Share APB timer code with other platforms

commit 06c3df49521c1b112b777cc4946e5de057c814ba
Author: Jamie Iles <jamie@jamieiles.com>
Date:   Mon Jun 6 12:43:07 2011 +0100

    clocksource: apb: Share APB timer code with other platforms
    
    The APB timers are an IP block from Synopsys (DesignWare APB timers)
    and are also found in other systems including ARM SoC's.  This patch
    adds functions for creating clock_event_devices and clocksources from
    APB timers but does not do the resource allocation.  This is handled
    in a higher layer to allow the timers to be created from multiple
    methods such as platform_devices.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Signed-off-by: Jamie Iles <jamie@jamieiles.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 289e92862fd9..033d3ecc5744 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -27,15 +27,12 @@
  * timer, but by default APB timer has higher rating than local APIC timers.
  */
 
-#include <linux/clocksource.h>
-#include <linux/clockchips.h>
 #include <linux/delay.h>
+#include <linux/dw_apb_timer.h>
 #include <linux/errno.h>
 #include <linux/init.h>
-#include <linux/sysdev.h>
 #include <linux/slab.h>
 #include <linux/pm.h>
-#include <linux/pci.h>
 #include <linux/sfi.h>
 #include <linux/interrupt.h>
 #include <linux/cpu.h>
@@ -45,75 +42,46 @@
 #include <asm/apb_timer.h>
 #include <asm/mrst.h>
 
-#define APBT_MASK			CLOCKSOURCE_MASK(32)
-#define APBT_SHIFT			22
 #define APBT_CLOCKEVENT_RATING		110
 #define APBT_CLOCKSOURCE_RATING		250
-#define APBT_MIN_DELTA_USEC		200
 
-#define EVT_TO_APBT_DEV(evt) container_of(evt, struct apbt_dev, evt)
 #define APBT_CLOCKEVENT0_NUM   (0)
-#define APBT_CLOCKEVENT1_NUM   (1)
 #define APBT_CLOCKSOURCE_NUM   (2)
 
-static unsigned long apbt_address;
+static phys_addr_t apbt_address;
 static int apb_timer_block_enabled;
 static void __iomem *apbt_virt_address;
-static int phy_cs_timer_id;
 
 /*
  * Common DW APB timer info
  */
-static uint64_t apbt_freq;
-
-static void apbt_set_mode(enum clock_event_mode mode,
-			  struct clock_event_device *evt);
-static int apbt_next_event(unsigned long delta,
-			   struct clock_event_device *evt);
-static cycle_t apbt_read_clocksource(struct clocksource *cs);
-static void apbt_restart_clocksource(struct clocksource *cs);
+static unsigned long apbt_freq;
 
 struct apbt_dev {
-	struct clock_event_device evt;
-	unsigned int num;
-	int cpu;
-	unsigned int irq;
-	unsigned int tick;
-	unsigned int count;
-	unsigned int flags;
-	char name[10];
+	struct dw_apb_clock_event_device	*timer;
+	unsigned int				num;
+	int					cpu;
+	unsigned int				irq;
+	char					name[10];
 };
 
-static DEFINE_PER_CPU(struct apbt_dev, cpu_apbt_dev);
+static struct dw_apb_clocksource *clocksource_apbt;
 
-#ifdef CONFIG_SMP
-static unsigned int apbt_num_timers_used;
-static struct apbt_dev *apbt_devs;
-#endif
-
-static	inline unsigned long apbt_readl_reg(unsigned long a)
+static inline void __iomem *adev_virt_addr(struct apbt_dev *adev)
 {
-	return readl(apbt_virt_address + a);
+	return apbt_virt_address + adev->num * APBTMRS_REG_SIZE;
 }
 
-static inline void apbt_writel_reg(unsigned long d, unsigned long a)
-{
-	writel(d, apbt_virt_address + a);
-}
-
-static inline unsigned long apbt_readl(int n, unsigned long a)
-{
-	return readl(apbt_virt_address + a + n * APBTMRS_REG_SIZE);
-}
+static DEFINE_PER_CPU(struct apbt_dev, cpu_apbt_dev);
 
-static inline void apbt_writel(int n, unsigned long d, unsigned long a)
-{
-	writel(d, apbt_virt_address + a + n * APBTMRS_REG_SIZE);
-}
+#ifdef CONFIG_SMP
+static unsigned int apbt_num_timers_used;
+#endif
 
 static inline void apbt_set_mapping(void)
 {
 	struct sfi_timer_table_entry *mtmr;
+	int phy_cs_timer_id = 0;
 
 	if (apbt_virt_address) {
 		pr_debug("APBT base already mapped\n");
@@ -125,21 +93,18 @@ static inline void apbt_set_mapping(void)
 		       APBT_CLOCKEVENT0_NUM);
 		return;
 	}
-	apbt_address = (unsigned long)mtmr->phys_addr;
+	apbt_address = (phys_addr_t)mtmr->phys_addr;
 	if (!apbt_address) {
 		printk(KERN_WARNING "No timer base from SFI, use default\n");
 		apbt_address = APBT_DEFAULT_BASE;
 	}
 	apbt_virt_address = ioremap_nocache(apbt_address, APBT_MMAP_SIZE);
-	if (apbt_virt_address) {
-		pr_debug("Mapped APBT physical addr %p at virtual addr %p\n",\
-			 (void *)apbt_address, (void *)apbt_virt_address);
-	} else {
-		pr_debug("Failed mapping APBT phy address at %p\n",\
-			 (void *)apbt_address);
+	if (!apbt_virt_address) {
+		pr_debug("Failed mapping APBT phy address at %lu\n",\
+			 (unsigned long)apbt_address);
 		goto panic_noapbt;
 	}
-	apbt_freq = mtmr->freq_hz / USEC_PER_SEC;
+	apbt_freq = mtmr->freq_hz;
 	sfi_free_mtmr(mtmr);
 
 	/* Now figure out the physical timer id for clocksource device */
@@ -148,9 +113,14 @@ static inline void apbt_set_mapping(void)
 		goto panic_noapbt;
 
 	/* Now figure out the physical timer id */
-	phy_cs_timer_id = (unsigned int)(mtmr->phys_addr & 0xff)
-		/ APBTMRS_REG_SIZE;
-	pr_debug("Use timer %d for clocksource\n", phy_cs_timer_id);
+	pr_debug("Use timer %d for clocksource\n",
+		 (int)(mtmr->phys_addr & 0xff) / APBTMRS_REG_SIZE);
+	phy_cs_timer_id = (unsigned int)(mtmr->phys_addr & 0xff) /
+		APBTMRS_REG_SIZE;
+
+	clocksource_apbt = dw_apb_clocksource_init(APBT_CLOCKSOURCE_RATING,
+		"apbt0", apbt_virt_address + phy_cs_timer_id *
+		APBTMRS_REG_SIZE, apbt_freq);
 	return;
 
 panic_noapbt:
@@ -172,82 +142,6 @@ static inline int is_apbt_capable(void)
 	return apbt_virt_address ? 1 : 0;
 }
 
-static struct clocksource clocksource_apbt = {
-	.name		= "apbt",
-	.rating		= APBT_CLOCKSOURCE_RATING,
-	.read		= apbt_read_clocksource,
-	.mask		= APBT_MASK,
-	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
-	.resume		= apbt_restart_clocksource,
-};
-
-/* boot APB clock event device */
-static struct clock_event_device apbt_clockevent = {
-	.name		= "apbt0",
-	.features	= CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT,
-	.set_mode	= apbt_set_mode,
-	.set_next_event = apbt_next_event,
-	.shift		= APBT_SHIFT,
-	.irq		= 0,
-	.rating		= APBT_CLOCKEVENT_RATING,
-};
-
-/*
- * start count down from 0xffff_ffff. this is done by toggling the enable bit
- * then load initial load count to ~0.
- */
-static void apbt_start_counter(int n)
-{
-	unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
-
-	ctrl &= ~APBTMR_CONTROL_ENABLE;
-	apbt_writel(n, ctrl, APBTMR_N_CONTROL);
-	apbt_writel(n, ~0, APBTMR_N_LOAD_COUNT);
-	/* enable, mask interrupt */
-	ctrl &= ~APBTMR_CONTROL_MODE_PERIODIC;
-	ctrl |= (APBTMR_CONTROL_ENABLE | APBTMR_CONTROL_INT);
-	apbt_writel(n, ctrl, APBTMR_N_CONTROL);
-	/* read it once to get cached counter value initialized */
-	apbt_read_clocksource(&clocksource_apbt);
-}
-
-static irqreturn_t apbt_interrupt_handler(int irq, void *data)
-{
-	struct apbt_dev *dev = (struct apbt_dev *)data;
-	struct clock_event_device *aevt = &dev->evt;
-
-	if (!aevt->event_handler) {
-		printk(KERN_INFO "Spurious APBT timer interrupt on %d\n",
-		       dev->num);
-		return IRQ_NONE;
-	}
-	aevt->event_handler(aevt);
-	return IRQ_HANDLED;
-}
-
-static void apbt_restart_clocksource(struct clocksource *cs)
-{
-	apbt_start_counter(phy_cs_timer_id);
-}
-
-static void apbt_enable_int(int n)
-{
-	unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
-	/* clear pending intr */
-	apbt_readl(n, APBTMR_N_EOI);
-	ctrl &= ~APBTMR_CONTROL_INT;
-	apbt_writel(n, ctrl, APBTMR_N_CONTROL);
-}
-
-static void apbt_disable_int(int n)
-{
-	unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
-
-	ctrl |= APBTMR_CONTROL_INT;
-	apbt_writel(n, ctrl, APBTMR_N_CONTROL);
-}
-
-
 static int __init apbt_clockevent_register(void)
 {
 	struct sfi_timer_table_entry *mtmr;
@@ -260,45 +154,21 @@ static int __init apbt_clockevent_register(void)
 		return -ENODEV;
 	}
 
-	/*
-	 * We need to calculate the scaled math multiplication factor for
-	 * nanosecond to apbt tick conversion.
-	 * mult = (nsec/cycle)*2^APBT_SHIFT
-	 */
-	apbt_clockevent.mult = div_sc((unsigned long) mtmr->freq_hz
-				      , NSEC_PER_SEC, APBT_SHIFT);
-
-	/* Calculate the min / max delta */
-	apbt_clockevent.max_delta_ns = clockevent_delta2ns(0x7FFFFFFF,
-							   &apbt_clockevent);
-	apbt_clockevent.min_delta_ns = clockevent_delta2ns(
-		APBT_MIN_DELTA_USEC*apbt_freq,
-		&apbt_clockevent);
-	/*
-	 * Start apbt with the boot cpu mask and make it
-	 * global if not used for per cpu timer.
-	 */
-	apbt_clockevent.cpumask = cpumask_of(smp_processor_id());
 	adev->num = smp_processor_id();
-	memcpy(&adev->evt, &apbt_clockevent, sizeof(struct clock_event_device));
+	adev->timer = dw_apb_clockevent_init(smp_processor_id(), "apbt0",
+		mrst_timer_options == MRST_TIMER_LAPIC_APBT ?
+		APBT_CLOCKEVENT_RATING - 100 : APBT_CLOCKEVENT_RATING,
+		adev_virt_addr(adev), 0, apbt_freq);
+	/* Firmware does EOI handling for us. */
+	adev->timer->eoi = NULL;
 
 	if (mrst_timer_options == MRST_TIMER_LAPIC_APBT) {
-		adev->evt.rating = APBT_CLOCKEVENT_RATING - 100;
-		global_clock_event = &adev->evt;
+		global_clock_event = &adev->timer->ced;
 		printk(KERN_DEBUG "%s clockevent registered as global\n",
 		       global_clock_event->name);
 	}
 
-	if (request_irq(apbt_clockevent.irq, apbt_interrupt_handler,
-			IRQF_TIMER | IRQF_DISABLED | IRQF_NOBALANCING,
-			apbt_clockevent.name, adev)) {
-		printk(KERN_ERR "Failed request IRQ for APBT%d\n",
-		       apbt_clockevent.irq);
-	}
-
-	clockevents_register_device(&adev->evt);
-	/* Start APBT 0 interrupts */
-	apbt_enable_int(APBT_CLOCKEVENT0_NUM);
+	dw_apb_clockevent_register(adev->timer);
 
 	sfi_free_mtmr(mtmr);
 	return 0;
@@ -316,52 +186,34 @@ static void apbt_setup_irq(struct apbt_dev *adev)
 	irq_set_affinity(adev->irq, cpumask_of(adev->cpu));
 	/* APB timer irqs are set up as mp_irqs, timer is edge type */
 	__irq_set_handler(adev->irq, handle_edge_irq, 0, "edge");
-
-	if (system_state == SYSTEM_BOOTING) {
-		if (request_irq(adev->irq, apbt_interrupt_handler,
-					IRQF_TIMER | IRQF_DISABLED |
-					IRQF_NOBALANCING,
-					adev->name, adev)) {
-			printk(KERN_ERR "Failed request IRQ for APBT%d\n",
-			       adev->num);
-		}
-	} else
-		enable_irq(adev->irq);
 }
 
 /* Should be called with per cpu */
 void apbt_setup_secondary_clock(void)
 {
 	struct apbt_dev *adev;
-	struct clock_event_device *aevt;
 	int cpu;
 
 	/* Don't register boot CPU clockevent */
 	cpu = smp_processor_id();
 	if (!cpu)
 		return;
-	/*
-	 * We need to calculate the scaled math multiplication factor for
-	 * nanosecond to apbt tick conversion.
-	 * mult = (nsec/cycle)*2^APBT_SHIFT
-	 */
-	printk(KERN_INFO "Init per CPU clockevent %d\n", cpu);
-	adev = &per_cpu(cpu_apbt_dev, cpu);
-	aevt = &adev->evt;
 
-	memcpy(aevt, &apbt_clockevent, sizeof(*aevt));
-	aevt->cpumask = cpumask_of(cpu);
-	aevt->name = adev->name;
-	aevt->mode = CLOCK_EVT_MODE_UNUSED;
+	adev = &__get_cpu_var(cpu_apbt_dev);
+	if (!adev->timer) {
+		adev->timer = dw_apb_clockevent_init(cpu, adev->name,
+			APBT_CLOCKEVENT_RATING, adev_virt_addr(adev),
+			adev->irq, apbt_freq);
+		adev->timer->eoi = NULL;
+	} else {
+		dw_apb_clockevent_resume(adev->timer);
+	}
 
-	printk(KERN_INFO "Registering CPU %d clockevent device %s, mask %08x\n",
-	       cpu, aevt->name, *(u32 *)aevt->cpumask);
+	printk(KERN_INFO "Registering CPU %d clockevent device %s, cpu %08x\n",
+	       cpu, adev->name, adev->cpu);
 
 	apbt_setup_irq(adev);
-
-	clockevents_register_device(aevt);
-
-	apbt_enable_int(cpu);
+	dw_apb_clockevent_register(adev->timer);
 
 	return;
 }
@@ -384,13 +236,12 @@ static int apbt_cpuhp_notify(struct notifier_block *n,
 
 	switch (action & 0xf) {
 	case CPU_DEAD:
-		disable_irq(adev->irq);
-		apbt_disable_int(cpu);
+		dw_apb_clockevent_pause(adev->timer);
 		if (system_state == SYSTEM_RUNNING) {
 			pr_debug("skipping APBT CPU %lu offline\n", cpu);
 		} else if (adev) {
 			pr_debug("APBT clockevent for cpu %lu offline\n", cpu);
-			free_irq(adev->irq, adev);
+			dw_apb_clockevent_stop(adev->timer);
 		}
 		break;
 	default:
@@ -415,116 +266,16 @@ void apbt_setup_secondary_clock(void) {}
 
 #endif /* CONFIG_SMP */
 
-static void apbt_set_mode(enum clock_event_mode mode,
-			  struct clock_event_device *evt)
-{
-	unsigned long ctrl;
-	uint64_t delta;
-	int timer_num;
-	struct apbt_dev *adev = EVT_TO_APBT_DEV(evt);
-
-	BUG_ON(!apbt_virt_address);
-
-	timer_num = adev->num;
-	pr_debug("%s CPU %d timer %d mode=%d\n",
-		 __func__, first_cpu(*evt->cpumask), timer_num, mode);
-
-	switch (mode) {
-	case CLOCK_EVT_MODE_PERIODIC:
-		delta = ((uint64_t)(NSEC_PER_SEC/HZ)) * apbt_clockevent.mult;
-		delta >>= apbt_clockevent.shift;
-		ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
-		ctrl |= APBTMR_CONTROL_MODE_PERIODIC;
-		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-		/*
-		 * DW APB p. 46, have to disable timer before load counter,
-		 * may cause sync problem.
-		 */
-		ctrl &= ~APBTMR_CONTROL_ENABLE;
-		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-		udelay(1);
-		pr_debug("Setting clock period %d for HZ %d\n", (int)delta, HZ);
-		apbt_writel(timer_num, delta, APBTMR_N_LOAD_COUNT);
-		ctrl |= APBTMR_CONTROL_ENABLE;
-		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-		break;
-		/* APB timer does not have one-shot mode, use free running mode */
-	case CLOCK_EVT_MODE_ONESHOT:
-		ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
-		/*
-		 * set free running mode, this mode will let timer reload max
-		 * timeout which will give time (3min on 25MHz clock) to rearm
-		 * the next event, therefore emulate the one-shot mode.
-		 */
-		ctrl &= ~APBTMR_CONTROL_ENABLE;
-		ctrl &= ~APBTMR_CONTROL_MODE_PERIODIC;
-
-		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-		/* write again to set free running mode */
-		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-
-		/*
-		 * DW APB p. 46, load counter with all 1s before starting free
-		 * running mode.
-		 */
-		apbt_writel(timer_num, ~0, APBTMR_N_LOAD_COUNT);
-		ctrl &= ~APBTMR_CONTROL_INT;
-		ctrl |= APBTMR_CONTROL_ENABLE;
-		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-		break;
-
-	case CLOCK_EVT_MODE_UNUSED:
-	case CLOCK_EVT_MODE_SHUTDOWN:
-		apbt_disable_int(timer_num);
-		ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
-		ctrl &= ~APBTMR_CONTROL_ENABLE;
-		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-		break;
-
-	case CLOCK_EVT_MODE_RESUME:
-		apbt_enable_int(timer_num);
-		break;
-	}
-}
-
-static int apbt_next_event(unsigned long delta,
-			   struct clock_event_device *evt)
-{
-	unsigned long ctrl;
-	int timer_num;
-
-	struct apbt_dev *adev = EVT_TO_APBT_DEV(evt);
-
-	timer_num = adev->num;
-	/* Disable timer */
-	ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
-	ctrl &= ~APBTMR_CONTROL_ENABLE;
-	apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-	/* write new count */
-	apbt_writel(timer_num, delta, APBTMR_N_LOAD_COUNT);
-	ctrl |= APBTMR_CONTROL_ENABLE;
-	apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-	return 0;
-}
-
-static cycle_t apbt_read_clocksource(struct clocksource *cs)
-{
-	unsigned long current_count;
-
-	current_count = apbt_readl(phy_cs_timer_id, APBTMR_N_CURRENT_VALUE);
-	return (cycle_t)~current_count;
-}
-
 static int apbt_clocksource_register(void)
 {
 	u64 start, now;
 	cycle_t t1;
 
 	/* Start the counter, use timer 2 as source, timer 0/1 for event */
-	apbt_start_counter(phy_cs_timer_id);
+	dw_apb_clocksource_start(clocksource_apbt);
 
 	/* Verify whether apbt counter works */
-	t1 = apbt_read_clocksource(&clocksource_apbt);
+	t1 = dw_apb_clocksource_read(clocksource_apbt);
 	rdtscll(start);
 
 	/*
@@ -539,10 +290,10 @@ static int apbt_clocksource_register(void)
 	} while ((now - start) < 200000UL);
 
 	/* APBT is the only always on clocksource, it has to work! */
-	if (t1 == apbt_read_clocksource(&clocksource_apbt))
+	if (t1 == dw_apb_clocksource_read(clocksource_apbt))
 		panic("APBT counter not counting. APBT disabled\n");
 
-	clocksource_register_khz(&clocksource_apbt, (u32)apbt_freq*1000);
+	dw_apb_clocksource_register(clocksource_apbt);
 
 	return 0;
 }
@@ -566,10 +317,7 @@ void __init apbt_time_init(void)
 	if (apb_timer_block_enabled)
 		return;
 	apbt_set_mapping();
-	if (apbt_virt_address) {
-		pr_debug("Found APBT version 0x%lx\n",\
-			 apbt_readl_reg(APBTMRS_COMP_VERSION));
-	} else
+	if (!apbt_virt_address)
 		goto out_noapbt;
 	/*
 	 * Read the frequency and check for a sane value, for ESL model
@@ -577,7 +325,7 @@ void __init apbt_time_init(void)
 	 */
 
 	if (apbt_freq < APBT_MIN_FREQ || apbt_freq > APBT_MAX_FREQ) {
-		pr_debug("APBT has invalid freq 0x%llx\n", apbt_freq);
+		pr_debug("APBT has invalid freq 0x%lx\n", apbt_freq);
 		goto out_noapbt;
 	}
 	if (apbt_clocksource_register()) {
@@ -603,30 +351,20 @@ void __init apbt_time_init(void)
 	} else {
 		percpu_timer = 0;
 		apbt_num_timers_used = 1;
-		adev = &per_cpu(cpu_apbt_dev, 0);
-		adev->flags &= ~APBT_DEV_USED;
 	}
 	pr_debug("%s: %d APB timers used\n", __func__, apbt_num_timers_used);
 
 	/* here we set up per CPU timer data structure */
-	apbt_devs = kzalloc(sizeof(struct apbt_dev) * apbt_num_timers_used,
-			    GFP_KERNEL);
-	if (!apbt_devs) {
-		printk(KERN_ERR "Failed to allocate APB timer devices\n");
-		return;
-	}
 	for (i = 0; i < apbt_num_timers_used; i++) {
 		adev = &per_cpu(cpu_apbt_dev, i);
 		adev->num = i;
 		adev->cpu = i;
 		p_mtmr = sfi_get_mtmr(i);
-		if (p_mtmr) {
-			adev->tick = p_mtmr->freq_hz;
+		if (p_mtmr)
 			adev->irq = p_mtmr->irq;
-		} else
+		else
 			printk(KERN_ERR "Failed to get timer for cpu %d\n", i);
-		adev->count = 0;
-		sprintf(adev->name, "apbt%d", i);
+		snprintf(adev->name, sizeof(adev->name) - 1, "apbt%d", i);
 	}
 #endif
 
@@ -638,17 +376,8 @@ void __init apbt_time_init(void)
 	panic("failed to enable APB timer\n");
 }
 
-static inline void apbt_disable(int n)
-{
-	if (is_apbt_capable()) {
-		unsigned long ctrl =  apbt_readl(n, APBTMR_N_CONTROL);
-		ctrl &= ~APBTMR_CONTROL_ENABLE;
-		apbt_writel(n, ctrl, APBTMR_N_CONTROL);
-	}
-}
-
 /* called before apb_timer_enable, use early map */
-unsigned long apbt_quick_calibrate()
+unsigned long apbt_quick_calibrate(void)
 {
 	int i, scale;
 	u64 old, new;
@@ -657,31 +386,31 @@ unsigned long apbt_quick_calibrate()
 	u32 loop, shift;
 
 	apbt_set_mapping();
-	apbt_start_counter(phy_cs_timer_id);
+	dw_apb_clocksource_start(clocksource_apbt);
 
 	/* check if the timer can count down, otherwise return */
-	old = apbt_read_clocksource(&clocksource_apbt);
+	old = dw_apb_clocksource_read(clocksource_apbt);
 	i = 10000;
 	while (--i) {
-		if (old != apbt_read_clocksource(&clocksource_apbt))
+		if (old != dw_apb_clocksource_read(clocksource_apbt))
 			break;
 	}
 	if (!i)
 		goto failed;
 
 	/* count 16 ms */
-	loop = (apbt_freq * 1000) << 4;
+	loop = (apbt_freq / 1000) << 4;
 
 	/* restart the timer to ensure it won't get to 0 in the calibration */
-	apbt_start_counter(phy_cs_timer_id);
+	dw_apb_clocksource_start(clocksource_apbt);
 
-	old = apbt_read_clocksource(&clocksource_apbt);
+	old = dw_apb_clocksource_read(clocksource_apbt);
 	old += loop;
 
 	t1 = __native_read_tsc();
 
 	do {
-		new = apbt_read_clocksource(&clocksource_apbt);
+		new = dw_apb_clocksource_read(clocksource_apbt);
 	} while (new < old);
 
 	t2 = __native_read_tsc();
@@ -693,7 +422,7 @@ unsigned long apbt_quick_calibrate()
 		return 0;
 	}
 	scale = (int)div_u64((t2 - t1), loop >> shift);
-	khz = (scale * apbt_freq * 1000) >> shift;
+	khz = (scale * (apbt_freq / 1000)) >> shift;
 	printk(KERN_INFO "TSC freq calculated by APB timer is %lu khz\n", khz);
 	return khz;
 failed:

commit 16f871bc30f86560017b9d34520593a28e08f373
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Wed Jun 1 19:05:06 2011 +0100

    x86: i8253: Consolidate definitions of global_clock_event
    
    There are multiple declarations of global_clock_event in header files
    specific to particular clock event implementations.  Consolidate them
    in <asm/time.h> and make sure all users include that header.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Venkatesh Pallipadi (Venki) <venki@google.com>
    Link: http://lkml.kernel.org/r/20110601180610.762763451@duck.linux-mips.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 289e92862fd9..2b6630d75e17 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -44,6 +44,7 @@
 #include <asm/fixmap.h>
 #include <asm/apb_timer.h>
 #include <asm/mrst.h>
+#include <asm/time.h>
 
 #define APBT_MASK			CLOCKSOURCE_MASK(32)
 #define APBT_SHIFT			22

commit a18f22a968de17b29f2310cdb7ba69163e65ec15
Merge: a1c57e0fec53 798778b8653f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 14 12:06:36 2011 +0200

    Merge branch 'consolidate-clksrc-i8253' of master.kernel.org:~rmk/linux-2.6-arm into timers/clocksource
    
    Conflicts:
            arch/ia64/kernel/cyclone.c
            arch/mips/kernel/i8253.c
            arch/x86/kernel/i8253.c
    
    Reason: Resolve conflicts so further cleanups do not conflict further
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 86cc8dfc211695193a060a240ac9c9287606e5d8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 30 00:09:01 2011 +0200

    x86: apb_timer: Fixup genirq fallout
    
    The lonely user of the internal interface was not in the coccinelle
    script.
    
    Reported-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 1293c709ee85..cd1ffed4ee22 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -316,7 +316,7 @@ static void apbt_setup_irq(struct apbt_dev *adev)
 	irq_modify_status(adev->irq, 0, IRQ_MOVE_PCNTXT);
 	irq_set_affinity(adev->irq, cpumask_of(adev->cpu));
 	/* APB timer irqs are set up as mp_irqs, timer is edge type */
-	__set_irq_handler(adev->irq, handle_edge_irq, 0, "edge");
+	__irq_set_handler(adev->irq, handle_edge_irq, 0, "edge");
 
 	if (system_state == SYSTEM_BOOTING) {
 		if (request_irq(adev->irq, apbt_interrupt_handler,

commit d10902812c9cd5583130a4ebb9ad19c60b68149d
Merge: 181f977d134a 25874a299ef8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 20:01:36 2011 -0700

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (27 commits)
      x86: Clean up apic.c and apic.h
      x86: Remove superflous goal definition of tsc_sync
      x86: dt: Correct local apic documentation in device tree bindings
      x86: dt: Cleanup local apic setup
      x86: dt: Fix OLPC=y/INTEL_CE=n build
      rtc: cmos: Add OF bindings
      x86: ce4100: Use OF to setup devices
      x86: ioapic: Add OF bindings for IO_APIC
      x86: dtb: Add generic bus probe
      x86: dtb: Add support for PCI devices backed by dtb nodes
      x86: dtb: Add device tree support for HPET
      x86: dtb: Add early parsing of IO_APIC
      x86: dtb: Add irq domain abstraction
      x86: dtb: Add a device tree for CE4100
      x86: Add device tree support
      x86: e820: Remove conditional early mapping in parse_e820_ext
      x86: OLPC: Make OLPC=n build again
      x86: OLPC: Remove extra OLPC_OPENFIRMWARE_DT indirection
      x86: OLPC: Cleanup config maze completely
      x86: OLPC: Hide OLPC_OPENFIRMWARE config switch
      ...
    
    Fix up conflicts in arch/x86/platform/ce4100/ce4100.c

commit 7b62dbec908a29f448047099bedb5c64c9cb8808
Author: Jacob Pan <jacob.jun.pan@linux.intel.com>
Date:   Wed Feb 23 16:07:26 2011 -0800

    x86/mrst: Fix apb timer rating when lapic timer is used
    
    Need to adjust the clockevent device rating for the structure
    that will be registered with clockevent system instead of the
    temporary structure.
    
    Without this fix, APB timer rating will be higher than LAPIC
    timer such that it can not be released later to be used as the
    broadcast timer.
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    LKML-Reference: <1298506046-439-1-git-send-email-jacob.jun.pan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 51ef31a89be9..51d4e1663066 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -284,7 +284,7 @@ static int __init apbt_clockevent_register(void)
 	memcpy(&adev->evt, &apbt_clockevent, sizeof(struct clock_event_device));
 
 	if (mrst_timer_options == MRST_TIMER_LAPIC_APBT) {
-		apbt_clockevent.rating = APBT_CLOCKEVENT_RATING - 100;
+		adev->evt.rating = APBT_CLOCKEVENT_RATING - 100;
 		global_clock_event = &adev->evt;
 		printk(KERN_DEBUG "%s clockevent registered as global\n",
 		       global_clock_event->name);

commit 695884fb8acd9857e0e7120ccb2150e30f4b8fef
Merge: 5df91509d324 04bea68b2f0e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 22 18:24:26 2011 +0100

    Merge branch 'devicetree/for-x86' of git://git.secretlab.ca/git/linux-2.6 into x86/platform
    
    Reason: x86 devicetree support for ce4100 depends on those device tree
            changes scheduled for .39.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit b01cc1b0eae0dea19257b29347116505fbedf679
Author: John Stultz <johnstul@us.ibm.com>
Date:   Mon Apr 26 19:03:05 2010 -0700

    x86: Convert remaining x86 clocksources to clocksource_register_hz/khz
    
    This converts the remaining x86 clocksources to use
    clocksource_register_hz/khz.
    
    CC: jacob.jun.pan@intel.com
    CC: Glauber Costa <glommer@redhat.com>
    CC: Dimitri Sivanich <sivanich@sgi.com>
    CC: Rusty Russell <rusty@rustcorp.com.au>
    CC: Jeremy Fitzhardinge <jeremy@xensource.com>
    CC: Chris McDermott <lcm@us.ibm.com>
    CC: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com> [xen]
    Signed-off-by: John Stultz <johnstul@us.ibm.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 51ef31a89be9..29ebf5a3b192 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -177,7 +177,6 @@ static struct clocksource clocksource_apbt = {
 	.rating		= APBT_CLOCKSOURCE_RATING,
 	.read		= apbt_read_clocksource,
 	.mask		= APBT_MASK,
-	.shift		= APBT_SHIFT,
 	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
 	.resume		= apbt_restart_clocksource,
 };
@@ -595,14 +594,7 @@ static int apbt_clocksource_register(void)
 	if (t1 == apbt_read_clocksource(&clocksource_apbt))
 		panic("APBT counter not counting. APBT disabled\n");
 
-	/*
-	 * initialize and register APBT clocksource
-	 * convert that to ns/clock cycle
-	 * mult = (ns/c) * 2^APBT_SHIFT
-	 */
-	clocksource_apbt.mult = div_sc(MSEC_PER_SEC,
-				       (unsigned long) apbt_freq, APBT_SHIFT);
-	clocksource_register(&clocksource_apbt);
+	clocksource_register_khz(&clocksource_apbt, (u32)apbt_freq*1000);
 
 	return 0;
 }

commit 5df91509d324d44cfb11e55d9cb02fe18b53b045
Author: jacob.jun.pan@linux.intel.com <jacob.jun.pan@linux.intel.com>
Date:   Fri Feb 18 13:42:54 2011 -0800

    x86: mrst: Remove apb timer read workaround
    
    APB timer current count was unreliable in the earlier silicon, which
    could result in time going backwards. This problem has been fixed in
    the current silicon stepping. This patch removes the workaround which
    was used to check and prevent timer rolling back when APB timer is
    used as clocksource device.
    
    The workaround code was also flawed by potential race condition
    around the cached read value last_read. Though a fix can be done
    by assigning last_read to a local variable at the beginning of
    apbt_read_clocksource(), but this is not necessary anymore.
    
    [ tglx: A sane timer on an Intel chip - I can't believe it ]
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Alan Cox <alan@linux.intel.com>
    LKML-Reference: <1298065374-25532-1-git-send-email-jacob.jun.pan@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 7c9ab59653e8..afc406498c9d 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -506,64 +506,12 @@ static int apbt_next_event(unsigned long delta,
 	return 0;
 }
 
-/*
- * APB timer clock is not in sync with pclk on Langwell, which translates to
- * unreliable read value caused by sampling error. the error does not add up
- * overtime and only happens when sampling a 0 as a 1 by mistake. so the time
- * would go backwards. the following code is trying to prevent time traveling
- * backwards. little bit paranoid.
- */
 static cycle_t apbt_read_clocksource(struct clocksource *cs)
 {
-	unsigned long t0, t1, t2;
-	static unsigned long last_read;
-
-bad_count:
-	t1 = apbt_readl(phy_cs_timer_id,
-			APBTMR_N_CURRENT_VALUE);
-	t2 = apbt_readl(phy_cs_timer_id,
-			APBTMR_N_CURRENT_VALUE);
-	if (unlikely(t1 < t2)) {
-		pr_debug("APBT: read current count error %lx:%lx:%lx\n",
-			 t1, t2, t2 - t1);
-		goto bad_count;
-	}
-	/*
-	 * check against cached last read, makes sure time does not go back.
-	 * it could be a normal rollover but we will do tripple check anyway
-	 */
-	if (unlikely(t2 > last_read)) {
-		/* check if we have a normal rollover */
-		unsigned long raw_intr_status =
-			apbt_readl_reg(APBTMRS_RAW_INT_STATUS);
-		/*
-		 * cs timer interrupt is masked but raw intr bit is set if
-		 * rollover occurs. then we read EOI reg to clear it.
-		 */
-		if (raw_intr_status & (1 << phy_cs_timer_id)) {
-			apbt_readl(phy_cs_timer_id, APBTMR_N_EOI);
-			goto out;
-		}
-		pr_debug("APB CS going back %lx:%lx:%lx ",
-			 t2, last_read, t2 - last_read);
-bad_count_x3:
-		pr_debug("triple check enforced\n");
-		t0 = apbt_readl(phy_cs_timer_id,
-				APBTMR_N_CURRENT_VALUE);
-		udelay(1);
-		t1 = apbt_readl(phy_cs_timer_id,
-				APBTMR_N_CURRENT_VALUE);
-		udelay(1);
-		t2 = apbt_readl(phy_cs_timer_id,
-				APBTMR_N_CURRENT_VALUE);
-		if ((t2 > t1) || (t1 > t0)) {
-			printk(KERN_ERR "Error: APB CS tripple check failed\n");
-			goto bad_count_x3;
-		}
-	}
-out:
-	last_read = t2;
-	return (cycle_t)~t2;
+	unsigned long current_count;
+
+	current_count = apbt_readl(phy_cs_timer_id, APBTMR_N_CURRENT_VALUE);
+	return (cycle_t)~current_count;
 }
 
 static int apbt_clocksource_register(void)

commit 6550904ddbc3c286798a87edf95eeebcc62bc58a
Author: Jacob Pan <jacob.jun.pan@linux.intel.com>
Date:   Thu Jan 13 16:06:44 2011 -0800

    x86, mrst: Set correct APB timer IRQ affinity for secondary cpu
    
    Offlining the secondary CPU causes the timer irq affinity to be set to
    CPU 0. When the secondary CPU is back online again, the wrong irq
    affinity will be used.
    
    This patch ensures secondary per CPU timer always has the correct
    IRQ affinity when enabled.
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    LKML-Reference: <1294963604-18111-1-git-send-email-jacob.jun.pan@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: <stable@kernel.org> 2.6.37

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 7c9ab59653e8..51ef31a89be9 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -313,14 +313,16 @@ static void apbt_setup_irq(struct apbt_dev *adev)
 	if (adev->irq == 0)
 		return;
 
+	irq_modify_status(adev->irq, 0, IRQ_MOVE_PCNTXT);
+	irq_set_affinity(adev->irq, cpumask_of(adev->cpu));
+	/* APB timer irqs are set up as mp_irqs, timer is edge type */
+	__set_irq_handler(adev->irq, handle_edge_irq, 0, "edge");
+
 	if (system_state == SYSTEM_BOOTING) {
-		irq_modify_status(adev->irq, 0, IRQ_MOVE_PCNTXT);
-		irq_set_affinity(adev->irq, cpumask_of(adev->cpu));
-		/* APB timer irqs are set up as mp_irqs, timer is edge type */
-		__set_irq_handler(adev->irq, handle_edge_irq, 0, "edge");
 		if (request_irq(adev->irq, apbt_interrupt_handler,
-				IRQF_TIMER | IRQF_DISABLED | IRQF_NOBALANCING,
-				adev->name, adev)) {
+					IRQF_TIMER | IRQF_DISABLED |
+					IRQF_NOBALANCING,
+					adev->name, adev)) {
 			printk(KERN_ERR "Failed request IRQ for APBT%d\n",
 			       adev->num);
 		}

commit e4d2ebcab11b308b5b59073578efd33eccd55d46
Author: Feng Tang <feng.tang@intel.com>
Date:   Fri Dec 3 11:51:38 2010 +0800

    x86, apbt: Setup affinity for apb timers acting as per-cpu timer
    
    Commit a5ef2e70 "x86: Sanitize apb timer interrupt handling" forgot
    the affinity setup when cleaning up the code, this patch just
    adds the forgotten part
    
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    Cc: Jacob Pan <jacob.jun.pan@intel.com>
    Cc: Alan Cox <alan@linux.intel.com>
    LKML-Reference: <1291348298-21263-2-git-send-email-feng.tang@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 92543c73cf8e..7c9ab59653e8 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -315,6 +315,7 @@ static void apbt_setup_irq(struct apbt_dev *adev)
 
 	if (system_state == SYSTEM_BOOTING) {
 		irq_modify_status(adev->irq, 0, IRQ_MOVE_PCNTXT);
+		irq_set_affinity(adev->irq, cpumask_of(adev->cpu));
 		/* APB timer irqs are set up as mp_irqs, timer is edge type */
 		__set_irq_handler(adev->irq, handle_edge_irq, 0, "edge");
 		if (request_irq(adev->irq, apbt_interrupt_handler,

commit 4a60cfa9457749f7987fd4f3c956dbba5a281129
Merge: 62bea97f54d8 27afdf2008da
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 14:11:46 2010 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (96 commits)
      apic, x86: Use BIOS settings for IBS and MCE threshold interrupt LVT offsets
      apic, x86: Check if EILVT APIC registers are available (AMD only)
      x86: ioapic: Call free_irte only if interrupt remapping enabled
      arm: Use ARCH_IRQ_INIT_FLAGS
      genirq, ARM: Fix boot on ARM platforms
      genirq: Fix CONFIG_GENIRQ_NO_DEPRECATED=y build
      x86: Switch sparse_irq allocations to GFP_KERNEL
      genirq: Switch sparse_irq allocator to GFP_KERNEL
      genirq: Make sparse_lock a mutex
      x86: lguest: Use new irq allocator
      genirq: Remove the now unused sparse irq leftovers
      genirq: Sanitize dynamic irq handling
      genirq: Remove arch_init_chip_data()
      x86: xen: Sanitise sparse_irq handling
      x86: Use sane enumeration
      x86: uv: Clean up the direct access to irq_desc
      x86: Make io_apic.c local functions static
      genirq: Remove irq_2_iommu
      x86: Speed up the irq_remapped check in hot pathes
      intr_remap: Simplify the code further
      ...
    
    Fix up trivial conflicts in arch/x86/Kconfig

commit 214515b5787a1035b2c8807abe8be569de63b2f6
Merge: bf70030dc0b0 d0ed0c32662e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:20:32 2010 -0700

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Remove pr_<level> uses of KERN_<level>
      therm_throt.c: Trivial printk message fix for a unsuitable abbreviation of 'thermal'
      x86: Use {push,pop}{l,q}_cfi in more places
      i386: Add unwind directives to syscall ptregs stubs
      x86-64: Use symbolics instead of raw numbers in entry_64.S
      x86-64: Adjust frame type at paranoid_exit:
      x86-64: Fix unwind annotations in syscall stubs

commit a5ef2e70405c8a9ee380b5ff33a008c75454791f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 28 11:11:10 2010 +0200

    x86: Sanitize apb timer interrupt handling
    
    Disable the interrupt in CPU_DEAD where it belongs. Remove the
    open coded irq_desc manipulation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 08f75fb4f509..42a70a2accc0 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -231,34 +231,6 @@ static void apbt_restart_clocksource(struct clocksource *cs)
 	apbt_start_counter(phy_cs_timer_id);
 }
 
-/* Setup IRQ routing via IOAPIC */
-#ifdef CONFIG_SMP
-static void apbt_setup_irq(struct apbt_dev *adev)
-{
-	struct irq_chip *chip;
-	struct irq_desc *desc;
-
-	/* timer0 irq has been setup early */
-	if (adev->irq == 0)
-		return;
-	desc = irq_to_desc(adev->irq);
-	chip = get_irq_chip(adev->irq);
-	disable_irq(adev->irq);
-	desc->status |= IRQ_MOVE_PCNTXT;
-	irq_set_affinity(adev->irq, cpumask_of(adev->cpu));
-	/* APB timer irqs are set up as mp_irqs, timer is edge triggerred */
-	set_irq_chip_and_handler_name(adev->irq, chip, handle_edge_irq, "edge");
-	enable_irq(adev->irq);
-	if (system_state == SYSTEM_BOOTING)
-		if (request_irq(adev->irq, apbt_interrupt_handler,
-				IRQF_TIMER | IRQF_DISABLED | IRQF_NOBALANCING,
-				adev->name, adev)) {
-			printk(KERN_ERR "Failed request IRQ for APBT%d\n",
-			       adev->num);
-		}
-}
-#endif
-
 static void apbt_enable_int(int n)
 {
 	unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
@@ -334,6 +306,27 @@ static int __init apbt_clockevent_register(void)
 }
 
 #ifdef CONFIG_SMP
+
+static void apbt_setup_irq(struct apbt_dev *adev)
+{
+	/* timer0 irq has been setup early */
+	if (adev->irq == 0)
+		return;
+
+	if (system_state == SYSTEM_BOOTING) {
+		irq_modify_status(adev->irq, 0, IRQ_MOVE_PCNTXT);
+		/* APB timer irqs are set up as mp_irqs, timer is edge type */
+		__set_irq_handler(adev->irq, handle_edge_irq, 0, "edge");
+		if (request_irq(adev->irq, apbt_interrupt_handler,
+				IRQF_TIMER | IRQF_DISABLED | IRQF_NOBALANCING,
+				adev->name, adev)) {
+			printk(KERN_ERR "Failed request IRQ for APBT%d\n",
+			       adev->num);
+		}
+	} else
+		enable_irq(adev->irq);
+}
+
 /* Should be called with per cpu */
 void apbt_setup_secondary_clock(void)
 {
@@ -389,10 +382,11 @@ static int apbt_cpuhp_notify(struct notifier_block *n,
 
 	switch (action & 0xf) {
 	case CPU_DEAD:
+		disable_irq(adev->irq);
 		apbt_disable_int(cpu);
-		if (system_state == SYSTEM_RUNNING)
+		if (system_state == SYSTEM_RUNNING) {
 			pr_debug("skipping APBT CPU %lu offline\n", cpu);
-		else if (adev) {
+		} else if (adev) {
 			pr_debug("APBT clockevent for cpu %lu offline\n", cpu);
 			free_irq(adev->irq, adev);
 		}

commit d0ed0c32662e756e7daf85e70a5a27a9c1111331
Author: Joe Perches <joe@perches.com>
Date:   Sat Sep 11 22:10:54 2010 -0700

    x86: Remove pr_<level> uses of KERN_<level>
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: Jiri Kosina <trivial@kernel.org>
    LKML-Reference: <d40c60f4b036a26db8492848695bdafaa3b42791.1284267142.git.joe@perches.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 8dd77800ff5d..4004417a6b53 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -398,7 +398,7 @@ static int apbt_cpuhp_notify(struct notifier_block *n,
 		}
 		break;
 	default:
-		pr_debug(KERN_INFO "APBT notified %lu, no action\n", action);
+		pr_debug("APBT notified %lu, no action\n", action);
 	}
 	return NOTIFY_OK;
 }
@@ -552,7 +552,7 @@ static cycle_t apbt_read_clocksource(struct clocksource *cs)
 		pr_debug("APB CS going back %lx:%lx:%lx ",
 			 t2, last_read, t2 - last_read);
 bad_count_x3:
-		pr_debug(KERN_INFO "tripple check enforced\n");
+		pr_debug("triple check enforced\n");
 		t0 = apbt_readl(phy_cs_timer_id,
 				APBTMR_N_CURRENT_VALUE);
 		udelay(1);

commit f6e9456c9272bb570df6e217cdbe007e270b1c4e
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jul 21 19:03:58 2010 +0200

    x86, cleanup: Remove obsolete boot_cpu_id variable
    
    boot_cpu_id is there for historical reasons and was renamed to
    boot_cpu_physical_apicid in patch:
    
     c70dcb7 x86: change boot_cpu_id to boot_cpu_physical_apicid
    
    However, there are some remaining occurrences of boot_cpu_id that are
    never touched in the kernel and thus its value is always 0.
    
    This patch removes boot_cpu_id completely.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    LKML-Reference: <1279731838-1522-8-git-send-email-robert.richter@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 8dd77800ff5d..08f75fb4f509 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -343,7 +343,7 @@ void apbt_setup_secondary_clock(void)
 
 	/* Don't register boot CPU clockevent */
 	cpu = smp_processor_id();
-	if (cpu == boot_cpu_id)
+	if (!cpu)
 		return;
 	/*
 	 * We need to calculate the scaled math multiplication factor for

commit a875c01944f0d750eeb1ef3133feceb13f13c4b3
Author: Jacob Pan <jacob.jun.pan@linux.intel.com>
Date:   Wed May 19 12:01:25 2010 -0700

    x86, mrst: add more timer config options
    
    Always-on local APIC timer (ARAT) has been introduced to Medfield, along
    with the platform APB timers we have more timer configuration options
    between Moorestown and Medfield.
    
    This patch adds run-time detection of avaiable timer features so that
    we can treat Medfield as a variant of Moorestown and set up the optimal
    timer options for each platform. i.e.
    
    Medfield: per cpu always-on local APIC timer
    Moorestown: per cpu APB timer
    
    Manual override is possible via cmdline option x86_mrst_timer.
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    LKML-Reference: <1274295685-6774-4-git-send-email-jacob.jun.pan@linux.intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index a35347501d36..8dd77800ff5d 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -43,10 +43,11 @@
 
 #include <asm/fixmap.h>
 #include <asm/apb_timer.h>
+#include <asm/mrst.h>
 
 #define APBT_MASK			CLOCKSOURCE_MASK(32)
 #define APBT_SHIFT			22
-#define APBT_CLOCKEVENT_RATING		150
+#define APBT_CLOCKEVENT_RATING		110
 #define APBT_CLOCKSOURCE_RATING		250
 #define APBT_MIN_DELTA_USEC		200
 
@@ -83,8 +84,6 @@ struct apbt_dev {
 	char name[10];
 };
 
-int disable_apbt_percpu __cpuinitdata;
-
 static DEFINE_PER_CPU(struct apbt_dev, cpu_apbt_dev);
 
 #ifdef CONFIG_SMP
@@ -194,29 +193,6 @@ static struct clock_event_device apbt_clockevent = {
 	.rating		= APBT_CLOCKEVENT_RATING,
 };
 
-/*
- * if user does not want to use per CPU apb timer, just give it a lower rating
- * than local apic timer and skip the late per cpu timer init.
- */
-static inline int __init setup_x86_mrst_timer(char *arg)
-{
-	if (!arg)
-		return -EINVAL;
-
-	if (strcmp("apbt_only", arg) == 0)
-		disable_apbt_percpu = 0;
-	else if (strcmp("lapic_and_apbt", arg) == 0)
-		disable_apbt_percpu = 1;
-	else {
-		pr_warning("X86 MRST timer option %s not recognised"
-			   " use x86_mrst_timer=apbt_only or lapic_and_apbt\n",
-			   arg);
-		return -EINVAL;
-	}
-	return 0;
-}
-__setup("x86_mrst_timer=", setup_x86_mrst_timer);
-
 /*
  * start count down from 0xffff_ffff. this is done by toggling the enable bit
  * then load initial load count to ~0.
@@ -335,7 +311,7 @@ static int __init apbt_clockevent_register(void)
 	adev->num = smp_processor_id();
 	memcpy(&adev->evt, &apbt_clockevent, sizeof(struct clock_event_device));
 
-	if (disable_apbt_percpu) {
+	if (mrst_timer_options == MRST_TIMER_LAPIC_APBT) {
 		apbt_clockevent.rating = APBT_CLOCKEVENT_RATING - 100;
 		global_clock_event = &adev->evt;
 		printk(KERN_DEBUG "%s clockevent registered as global\n",
@@ -429,7 +405,8 @@ static int apbt_cpuhp_notify(struct notifier_block *n,
 
 static __init int apbt_late_init(void)
 {
-	if (disable_apbt_percpu || !apb_timer_block_enabled)
+	if (mrst_timer_options == MRST_TIMER_LAPIC_APBT ||
+		!apb_timer_block_enabled)
 		return 0;
 	/* This notifier should be called after workqueue is ready */
 	hotcpu_notifier(apbt_cpuhp_notify, -20);
@@ -450,6 +427,8 @@ static void apbt_set_mode(enum clock_event_mode mode,
 	int timer_num;
 	struct apbt_dev *adev = EVT_TO_APBT_DEV(evt);
 
+	BUG_ON(!apbt_virt_address);
+
 	timer_num = adev->num;
 	pr_debug("%s CPU %d timer %d mode=%d\n",
 		 __func__, first_cpu(*evt->cpumask), timer_num, mode);
@@ -676,7 +655,7 @@ void __init apbt_time_init(void)
 	}
 #ifdef CONFIG_SMP
 	/* kernel cmdline disable apb timer, so we will use lapic timers */
-	if (disable_apbt_percpu) {
+	if (mrst_timer_options == MRST_TIMER_LAPIC_APBT) {
 		printk(KERN_INFO "apbt: disabled per cpu timer\n");
 		return;
 	}

commit ae7c9b70dcb4313ea3dbcc9a2f240dae6c2b50c0
Author: Jacob Pan <jacob.jun.pan@linux.intel.com>
Date:   Mon Apr 19 11:23:43 2010 -0700

    x86, mrst: Conditionally register cpu hotplug notifier for apbt
    
    APB timer is used on Moorestown platforms but not on a standard PC.
    If APB timer code is compiled in but not initialized at run-time due
    to lack of FW reported SFI table, kernel would panic when the non-boot
    CPUs are offlined and notifier is called.
    
    https://bugzilla.kernel.org/show_bug.cgi?id=15786
    
    This patch ensures CPU hotplug notifier for APB timer is only registered
    when the APBT timer block is initialized.
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    LKML-Reference: <1271701423-1162-1-git-send-email-jacob.jun.pan@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index ff469e470059..a35347501d36 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -429,7 +429,7 @@ static int apbt_cpuhp_notify(struct notifier_block *n,
 
 static __init int apbt_late_init(void)
 {
-	if (disable_apbt_percpu)
+	if (disable_apbt_percpu || !apb_timer_block_enabled)
 		return 0;
 	/* This notifier should be called after workqueue is ready */
 	hotcpu_notifier(apbt_cpuhp_notify, -20);

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 4b7099526d2c..ff469e470059 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -33,6 +33,7 @@
 #include <linux/errno.h>
 #include <linux/init.h>
 #include <linux/sysdev.h>
+#include <linux/slab.h>
 #include <linux/pm.h>
 #include <linux/pci.h>
 #include <linux/sfi.h>

commit 322aafa6645a48c3b7837ca7385f126ab78127fd
Merge: dd04265b028c c7bbf52aa4fa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 7 15:59:39 2010 -0800

    Merge branch 'x86-mrst-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-mrst-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (30 commits)
      x86, mrst: Fix whitespace breakage in apb_timer.c
      x86, mrst: Fix APB timer per cpu clockevent
      x86, mrst: Remove X86_MRST dependency on PCI_IOAPIC
      x86, olpc: Use pci subarch init for OLPC
      x86, pci: Add arch_init to x86_init abstraction
      x86, mrst: Add Kconfig dependencies for Moorestown
      x86, pci: Exclude Moorestown PCI code if CONFIG_X86_MRST=n
      x86, numaq: Make CONFIG_X86_NUMAQ depend on CONFIG_PCI
      x86, pci: Add sanity check for PCI fixed bar probing
      x86, legacy_irq: Remove duplicate vector assigment
      x86, legacy_irq: Remove left over nr_legacy_irqs
      x86, mrst: Platform clock setup code
      x86, apbt: Moorestown APB system timer driver
      x86, mrst: Add vrtc platform data setup code
      x86, mrst: Add platform timer info parsing code
      x86, mrst: Fill in PCI functions in x86_init layer
      x86, mrst: Add dummy legacy pic to platform setup
      x86/PCI: Moorestown PCI support
      x86, ioapic: Add dummy ioapic functions
      x86, ioapic: Early enable ioapic for timer irq
      ...
    
    Fixed up semantic conflict of new clocksources due to commit
    17622339af25 ("clocksource: add argument to resume callback").

commit c7bbf52aa4fa332b84c4f2bb33e69561ee6870b4
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Mar 3 13:38:48 2010 -0800

    x86, mrst: Fix whitespace breakage in apb_timer.c
    
    Checkin bb24c4716185f6e116c440462c65c1f56649183b:
    "Moorestown APB system timer driver" suffered from severe whitespace
    damage in arch/x86/kernel/apb_timer.c due to using Microsoft Lookout
    to send a patch.  Fix the whitespace breakage.
    
    Reported-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 6f27f8b75795..2afa27d01297 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -43,11 +43,11 @@
 #include <asm/fixmap.h>
 #include <asm/apb_timer.h>
 
-#define APBT_MASK      CLOCKSOURCE_MASK(32)
-#define APBT_SHIFT                     22
-#define APBT_CLOCKEVENT_RATING         150
-#define APBT_CLOCKSOURCE_RATING                250
-#define APBT_MIN_DELTA_USEC            200
+#define APBT_MASK			CLOCKSOURCE_MASK(32)
+#define APBT_SHIFT			22
+#define APBT_CLOCKEVENT_RATING		150
+#define APBT_CLOCKSOURCE_RATING		250
+#define APBT_MIN_DELTA_USEC		200
 
 #define EVT_TO_APBT_DEV(evt) container_of(evt, struct apbt_dev, evt)
 #define APBT_CLOCKEVENT0_NUM   (0)
@@ -65,21 +65,21 @@ static int phy_cs_timer_id;
 static uint64_t apbt_freq;
 
 static void apbt_set_mode(enum clock_event_mode mode,
-                         struct clock_event_device *evt);
+			  struct clock_event_device *evt);
 static int apbt_next_event(unsigned long delta,
-                          struct clock_event_device *evt);
+			   struct clock_event_device *evt);
 static cycle_t apbt_read_clocksource(struct clocksource *cs);
 static void apbt_restart_clocksource(void);
 
 struct apbt_dev {
-       struct clock_event_device       evt;
-       unsigned int num;
-       int cpu;
-       unsigned int irq;
-       unsigned int tick;
-       unsigned int count;
-       unsigned int flags;
-       char name[10];
+	struct clock_event_device evt;
+	unsigned int num;
+	int cpu;
+	unsigned int irq;
+	unsigned int tick;
+	unsigned int count;
+	unsigned int flags;
+	char name[10];
 };
 
 int disable_apbt_percpu __cpuinitdata;
@@ -91,77 +91,77 @@ static unsigned int apbt_num_timers_used;
 static struct apbt_dev *apbt_devs;
 #endif
 
-static  inline unsigned long apbt_readl_reg(unsigned long a)
+static	inline unsigned long apbt_readl_reg(unsigned long a)
 {
-       return readl(apbt_virt_address + a);
+	return readl(apbt_virt_address + a);
 }
 
 static inline void apbt_writel_reg(unsigned long d, unsigned long a)
 {
-       writel(d, apbt_virt_address + a);
+	writel(d, apbt_virt_address + a);
 }
 
 static inline unsigned long apbt_readl(int n, unsigned long a)
 {
-       return readl(apbt_virt_address + a + n * APBTMRS_REG_SIZE);
+	return readl(apbt_virt_address + a + n * APBTMRS_REG_SIZE);
 }
 
 static inline void apbt_writel(int n, unsigned long d, unsigned long a)
 {
-       writel(d, apbt_virt_address + a + n * APBTMRS_REG_SIZE);
+	writel(d, apbt_virt_address + a + n * APBTMRS_REG_SIZE);
 }
 
 static inline void apbt_set_mapping(void)
 {
-       struct sfi_timer_table_entry *mtmr;
-
-       if (apbt_virt_address) {
-               pr_debug("APBT base already mapped\n");
-               return;
-       }
-       mtmr = sfi_get_mtmr(APBT_CLOCKEVENT0_NUM);
-       if (mtmr == NULL) {
-               printk(KERN_ERR "Failed to get MTMR %d from SFI\n",
-                       APBT_CLOCKEVENT0_NUM);
-               return;
-       }
-       apbt_address = (unsigned long)mtmr->phys_addr;
-       if (!apbt_address) {
-               printk(KERN_WARNING "No timer base from SFI, use default\n");
-               apbt_address = APBT_DEFAULT_BASE;
-       }
-       apbt_virt_address = ioremap_nocache(apbt_address, APBT_MMAP_SIZE);
-       if (apbt_virt_address) {
-               pr_debug("Mapped APBT physical addr %p at virtual addr %p\n",\
-                       (void *)apbt_address, (void *)apbt_virt_address);
-       } else {
-               pr_debug("Failed mapping APBT phy address at %p\n",\
-                       (void *)apbt_address);
-               goto panic_noapbt;
-       }
-       apbt_freq = mtmr->freq_hz / USEC_PER_SEC;
-       sfi_free_mtmr(mtmr);
-
-       /* Now figure out the physical timer id for clocksource device */
-       mtmr = sfi_get_mtmr(APBT_CLOCKSOURCE_NUM);
-       if (mtmr == NULL)
-               goto panic_noapbt;
-
-       /* Now figure out the physical timer id */
-       phy_cs_timer_id = (unsigned int)(mtmr->phys_addr & 0xff)
-                       / APBTMRS_REG_SIZE;
-       pr_debug("Use timer %d for clocksource\n", phy_cs_timer_id);
-       return;
+	struct sfi_timer_table_entry *mtmr;
+
+	if (apbt_virt_address) {
+		pr_debug("APBT base already mapped\n");
+		return;
+	}
+	mtmr = sfi_get_mtmr(APBT_CLOCKEVENT0_NUM);
+	if (mtmr == NULL) {
+		printk(KERN_ERR "Failed to get MTMR %d from SFI\n",
+		       APBT_CLOCKEVENT0_NUM);
+		return;
+	}
+	apbt_address = (unsigned long)mtmr->phys_addr;
+	if (!apbt_address) {
+		printk(KERN_WARNING "No timer base from SFI, use default\n");
+		apbt_address = APBT_DEFAULT_BASE;
+	}
+	apbt_virt_address = ioremap_nocache(apbt_address, APBT_MMAP_SIZE);
+	if (apbt_virt_address) {
+		pr_debug("Mapped APBT physical addr %p at virtual addr %p\n",\
+			 (void *)apbt_address, (void *)apbt_virt_address);
+	} else {
+		pr_debug("Failed mapping APBT phy address at %p\n",\
+			 (void *)apbt_address);
+		goto panic_noapbt;
+	}
+	apbt_freq = mtmr->freq_hz / USEC_PER_SEC;
+	sfi_free_mtmr(mtmr);
+
+	/* Now figure out the physical timer id for clocksource device */
+	mtmr = sfi_get_mtmr(APBT_CLOCKSOURCE_NUM);
+	if (mtmr == NULL)
+		goto panic_noapbt;
+
+	/* Now figure out the physical timer id */
+	phy_cs_timer_id = (unsigned int)(mtmr->phys_addr & 0xff)
+		/ APBTMRS_REG_SIZE;
+	pr_debug("Use timer %d for clocksource\n", phy_cs_timer_id);
+	return;
 
 panic_noapbt:
-       panic("Failed to setup APB system timer\n");
+	panic("Failed to setup APB system timer\n");
 
 }
 
 static inline void apbt_clear_mapping(void)
 {
-       iounmap(apbt_virt_address);
-       apbt_virt_address = NULL;
+	iounmap(apbt_virt_address);
+	apbt_virt_address = NULL;
 }
 
 /*
@@ -169,28 +169,28 @@ static inline void apbt_clear_mapping(void)
  */
 static inline int is_apbt_capable(void)
 {
-       return apbt_virt_address ? 1 : 0;
+	return apbt_virt_address ? 1 : 0;
 }
 
 static struct clocksource clocksource_apbt = {
-       .name           = "apbt",
-       .rating         = APBT_CLOCKSOURCE_RATING,
-       .read           = apbt_read_clocksource,
-       .mask           = APBT_MASK,
-       .shift          = APBT_SHIFT,
-       .flags          = CLOCK_SOURCE_IS_CONTINUOUS,
-       .resume         = apbt_restart_clocksource,
+	.name		= "apbt",
+	.rating		= APBT_CLOCKSOURCE_RATING,
+	.read		= apbt_read_clocksource,
+	.mask		= APBT_MASK,
+	.shift		= APBT_SHIFT,
+	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
+	.resume		= apbt_restart_clocksource,
 };
 
 /* boot APB clock event device */
 static struct clock_event_device apbt_clockevent = {
-       .name           = "apbt0",
-       .features       = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT,
-       .set_mode       = apbt_set_mode,
-       .set_next_event = apbt_next_event,
-       .shift          = APBT_SHIFT,
-       .irq            = 0,
-       .rating         = APBT_CLOCKEVENT_RATING,
+	.name		= "apbt0",
+	.features	= CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT,
+	.set_mode	= apbt_set_mode,
+	.set_next_event = apbt_next_event,
+	.shift		= APBT_SHIFT,
+	.irq		= 0,
+	.rating		= APBT_CLOCKEVENT_RATING,
 };
 
 /*
@@ -199,20 +199,20 @@ static struct clock_event_device apbt_clockevent = {
  */
 static inline int __init setup_x86_mrst_timer(char *arg)
 {
-       if (!arg)
-               return -EINVAL;
-
-       if (strcmp("apbt_only", arg) == 0)
-               disable_apbt_percpu = 0;
-       else if (strcmp("lapic_and_apbt", arg) == 0)
-               disable_apbt_percpu = 1;
-       else {
-               pr_warning("X86 MRST timer option %s not recognised"
-                       " use x86_mrst_timer=apbt_only or lapic_and_apbt\n",
-                       arg);
-               return -EINVAL;
-       }
-       return 0;
+	if (!arg)
+		return -EINVAL;
+
+	if (strcmp("apbt_only", arg) == 0)
+		disable_apbt_percpu = 0;
+	else if (strcmp("lapic_and_apbt", arg) == 0)
+		disable_apbt_percpu = 1;
+	else {
+		pr_warning("X86 MRST timer option %s not recognised"
+			   " use x86_mrst_timer=apbt_only or lapic_and_apbt\n",
+			   arg);
+		return -EINVAL;
+	}
+	return 0;
 }
 __setup("x86_mrst_timer=", setup_x86_mrst_timer);
 
@@ -222,176 +222,176 @@ __setup("x86_mrst_timer=", setup_x86_mrst_timer);
  */
 static void apbt_start_counter(int n)
 {
-       unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
-
-       ctrl &= ~APBTMR_CONTROL_ENABLE;
-       apbt_writel(n, ctrl, APBTMR_N_CONTROL);
-       apbt_writel(n, ~0, APBTMR_N_LOAD_COUNT);
-       /* enable, mask interrupt */
-       ctrl &= ~APBTMR_CONTROL_MODE_PERIODIC;
-       ctrl |= (APBTMR_CONTROL_ENABLE | APBTMR_CONTROL_INT);
-       apbt_writel(n, ctrl, APBTMR_N_CONTROL);
-       /* read it once to get cached counter value initialized */
-       apbt_read_clocksource(&clocksource_apbt);
+	unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
+
+	ctrl &= ~APBTMR_CONTROL_ENABLE;
+	apbt_writel(n, ctrl, APBTMR_N_CONTROL);
+	apbt_writel(n, ~0, APBTMR_N_LOAD_COUNT);
+	/* enable, mask interrupt */
+	ctrl &= ~APBTMR_CONTROL_MODE_PERIODIC;
+	ctrl |= (APBTMR_CONTROL_ENABLE | APBTMR_CONTROL_INT);
+	apbt_writel(n, ctrl, APBTMR_N_CONTROL);
+	/* read it once to get cached counter value initialized */
+	apbt_read_clocksource(&clocksource_apbt);
 }
 
 static irqreturn_t apbt_interrupt_handler(int irq, void *data)
 {
-       struct apbt_dev *dev = (struct apbt_dev *)data;
-       struct clock_event_device *aevt = &dev->evt;
-
-       if (!aevt->event_handler) {
-               printk(KERN_INFO "Spurious APBT timer interrupt on %d\n",
-                               dev->num);
-               return IRQ_NONE;
-       }
-       aevt->event_handler(aevt);
-       return IRQ_HANDLED;
+	struct apbt_dev *dev = (struct apbt_dev *)data;
+	struct clock_event_device *aevt = &dev->evt;
+
+	if (!aevt->event_handler) {
+		printk(KERN_INFO "Spurious APBT timer interrupt on %d\n",
+		       dev->num);
+		return IRQ_NONE;
+	}
+	aevt->event_handler(aevt);
+	return IRQ_HANDLED;
 }
 
 static void apbt_restart_clocksource(void)
 {
-       apbt_start_counter(phy_cs_timer_id);
+	apbt_start_counter(phy_cs_timer_id);
 }
 
 /* Setup IRQ routing via IOAPIC */
 #ifdef CONFIG_SMP
 static void apbt_setup_irq(struct apbt_dev *adev)
 {
-       struct irq_chip *chip;
-       struct irq_desc *desc;
-
-       /* timer0 irq has been setup early */
-       if (adev->irq == 0)
-               return;
-       desc = irq_to_desc(adev->irq);
-       chip = get_irq_chip(adev->irq);
-       disable_irq(adev->irq);
-       desc->status |= IRQ_MOVE_PCNTXT;
-       irq_set_affinity(adev->irq, cpumask_of(adev->cpu));
-       /* APB timer irqs are set up as mp_irqs, timer is edge triggerred */
-       set_irq_chip_and_handler_name(adev->irq, chip, handle_edge_irq, "edge");
-       enable_irq(adev->irq);
-       if (system_state == SYSTEM_BOOTING)
-               if (request_irq(adev->irq, apbt_interrupt_handler,
-                       IRQF_TIMER | IRQF_DISABLED | IRQF_NOBALANCING,
-                       adev->name, adev)) {
-                       printk(KERN_ERR "Failed request IRQ for APBT%d\n",
-                                       adev->num);
-               }
+	struct irq_chip *chip;
+	struct irq_desc *desc;
+
+	/* timer0 irq has been setup early */
+	if (adev->irq == 0)
+		return;
+	desc = irq_to_desc(adev->irq);
+	chip = get_irq_chip(adev->irq);
+	disable_irq(adev->irq);
+	desc->status |= IRQ_MOVE_PCNTXT;
+	irq_set_affinity(adev->irq, cpumask_of(adev->cpu));
+	/* APB timer irqs are set up as mp_irqs, timer is edge triggerred */
+	set_irq_chip_and_handler_name(adev->irq, chip, handle_edge_irq, "edge");
+	enable_irq(adev->irq);
+	if (system_state == SYSTEM_BOOTING)
+		if (request_irq(adev->irq, apbt_interrupt_handler,
+				IRQF_TIMER | IRQF_DISABLED | IRQF_NOBALANCING,
+				adev->name, adev)) {
+			printk(KERN_ERR "Failed request IRQ for APBT%d\n",
+			       adev->num);
+		}
 }
 #endif
 
 static void apbt_enable_int(int n)
 {
-       unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
-       /* clear pending intr */
-       apbt_readl(n, APBTMR_N_EOI);
-       ctrl &= ~APBTMR_CONTROL_INT;
-       apbt_writel(n, ctrl, APBTMR_N_CONTROL);
+	unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
+	/* clear pending intr */
+	apbt_readl(n, APBTMR_N_EOI);
+	ctrl &= ~APBTMR_CONTROL_INT;
+	apbt_writel(n, ctrl, APBTMR_N_CONTROL);
 }
 
 static void apbt_disable_int(int n)
 {
-       unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
+	unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
 
-       ctrl |= APBTMR_CONTROL_INT;
-       apbt_writel(n, ctrl, APBTMR_N_CONTROL);
+	ctrl |= APBTMR_CONTROL_INT;
+	apbt_writel(n, ctrl, APBTMR_N_CONTROL);
 }
 
 
 static int __init apbt_clockevent_register(void)
 {
-       struct sfi_timer_table_entry *mtmr;
-       struct apbt_dev *adev = &__get_cpu_var(cpu_apbt_dev);
-
-       mtmr = sfi_get_mtmr(APBT_CLOCKEVENT0_NUM);
-       if (mtmr == NULL) {
-               printk(KERN_ERR "Failed to get MTMR %d from SFI\n",
-                       APBT_CLOCKEVENT0_NUM);
-               return -ENODEV;
-       }
-
-       /*
-        * We need to calculate the scaled math multiplication factor for
-        * nanosecond to apbt tick conversion.
-        * mult = (nsec/cycle)*2^APBT_SHIFT
-        */
-       apbt_clockevent.mult = div_sc((unsigned long) mtmr->freq_hz
-                       , NSEC_PER_SEC, APBT_SHIFT);
-
-       /* Calculate the min / max delta */
-       apbt_clockevent.max_delta_ns = clockevent_delta2ns(0x7FFFFFFF,
-                                                          &apbt_clockevent);
-       apbt_clockevent.min_delta_ns = clockevent_delta2ns(
-                                               APBT_MIN_DELTA_USEC*apbt_freq,
-                                               &apbt_clockevent);
-       /*
-        * Start apbt with the boot cpu mask and make it
-        * global if not used for per cpu timer.
-        */
-       apbt_clockevent.cpumask = cpumask_of(smp_processor_id());
-       adev->num = smp_processor_id();
-       memcpy(&adev->evt, &apbt_clockevent, sizeof(struct clock_event_device));
-
-       if (disable_apbt_percpu) {
-               apbt_clockevent.rating = APBT_CLOCKEVENT_RATING - 100;
+	struct sfi_timer_table_entry *mtmr;
+	struct apbt_dev *adev = &__get_cpu_var(cpu_apbt_dev);
+
+	mtmr = sfi_get_mtmr(APBT_CLOCKEVENT0_NUM);
+	if (mtmr == NULL) {
+		printk(KERN_ERR "Failed to get MTMR %d from SFI\n",
+		       APBT_CLOCKEVENT0_NUM);
+		return -ENODEV;
+	}
+
+	/*
+	 * We need to calculate the scaled math multiplication factor for
+	 * nanosecond to apbt tick conversion.
+	 * mult = (nsec/cycle)*2^APBT_SHIFT
+	 */
+	apbt_clockevent.mult = div_sc((unsigned long) mtmr->freq_hz
+				      , NSEC_PER_SEC, APBT_SHIFT);
+
+	/* Calculate the min / max delta */
+	apbt_clockevent.max_delta_ns = clockevent_delta2ns(0x7FFFFFFF,
+							   &apbt_clockevent);
+	apbt_clockevent.min_delta_ns = clockevent_delta2ns(
+		APBT_MIN_DELTA_USEC*apbt_freq,
+		&apbt_clockevent);
+	/*
+	 * Start apbt with the boot cpu mask and make it
+	 * global if not used for per cpu timer.
+	 */
+	apbt_clockevent.cpumask = cpumask_of(smp_processor_id());
+	adev->num = smp_processor_id();
+	memcpy(&adev->evt, &apbt_clockevent, sizeof(struct clock_event_device));
+
+	if (disable_apbt_percpu) {
+		apbt_clockevent.rating = APBT_CLOCKEVENT_RATING - 100;
 		global_clock_event = &adev->evt;
-               printk(KERN_DEBUG "%s clockevent registered as global\n",
-                       global_clock_event->name);
-       }
-
-       if (request_irq(apbt_clockevent.irq, apbt_interrupt_handler,
-               IRQF_TIMER | IRQF_DISABLED | IRQF_NOBALANCING,
-		apbt_clockevent.name, adev)) {
-               printk(KERN_ERR "Failed request IRQ for APBT%d\n",
-                       apbt_clockevent.irq);
-       }
-
-       clockevents_register_device(&adev->evt);
-       /* Start APBT 0 interrupts */
-       apbt_enable_int(APBT_CLOCKEVENT0_NUM);
-
-       sfi_free_mtmr(mtmr);
-       return 0;
+		printk(KERN_DEBUG "%s clockevent registered as global\n",
+		       global_clock_event->name);
+	}
+
+	if (request_irq(apbt_clockevent.irq, apbt_interrupt_handler,
+			IRQF_TIMER | IRQF_DISABLED | IRQF_NOBALANCING,
+			apbt_clockevent.name, adev)) {
+		printk(KERN_ERR "Failed request IRQ for APBT%d\n",
+		       apbt_clockevent.irq);
+	}
+
+	clockevents_register_device(&adev->evt);
+	/* Start APBT 0 interrupts */
+	apbt_enable_int(APBT_CLOCKEVENT0_NUM);
+
+	sfi_free_mtmr(mtmr);
+	return 0;
 }
 
 #ifdef CONFIG_SMP
 /* Should be called with per cpu */
 void apbt_setup_secondary_clock(void)
 {
-       struct apbt_dev *adev;
-       struct clock_event_device *aevt;
-       int cpu;
-
-       /* Don't register boot CPU clockevent */
-       cpu = smp_processor_id();
-       if (cpu == boot_cpu_id)
-               return;
-       /*
-        * We need to calculate the scaled math multiplication factor for
-        * nanosecond to apbt tick conversion.
-        * mult = (nsec/cycle)*2^APBT_SHIFT
-        */
-       printk(KERN_INFO "Init per CPU clockevent %d\n", cpu);
-       adev = &per_cpu(cpu_apbt_dev, cpu);
-       aevt = &adev->evt;
-
-       memcpy(aevt, &apbt_clockevent, sizeof(*aevt));
-       aevt->cpumask = cpumask_of(cpu);
-       aevt->name = adev->name;
-       aevt->mode = CLOCK_EVT_MODE_UNUSED;
-
-       printk(KERN_INFO "Registering CPU %d clockevent device %s, mask %08x\n",
-               cpu, aevt->name, *(u32 *)aevt->cpumask);
-
-       apbt_setup_irq(adev);
-
-       clockevents_register_device(aevt);
-
-       apbt_enable_int(cpu);
-
-       return;
+	struct apbt_dev *adev;
+	struct clock_event_device *aevt;
+	int cpu;
+
+	/* Don't register boot CPU clockevent */
+	cpu = smp_processor_id();
+	if (cpu == boot_cpu_id)
+		return;
+	/*
+	 * We need to calculate the scaled math multiplication factor for
+	 * nanosecond to apbt tick conversion.
+	 * mult = (nsec/cycle)*2^APBT_SHIFT
+	 */
+	printk(KERN_INFO "Init per CPU clockevent %d\n", cpu);
+	adev = &per_cpu(cpu_apbt_dev, cpu);
+	aevt = &adev->evt;
+
+	memcpy(aevt, &apbt_clockevent, sizeof(*aevt));
+	aevt->cpumask = cpumask_of(cpu);
+	aevt->name = adev->name;
+	aevt->mode = CLOCK_EVT_MODE_UNUSED;
+
+	printk(KERN_INFO "Registering CPU %d clockevent device %s, mask %08x\n",
+	       cpu, aevt->name, *(u32 *)aevt->cpumask);
+
+	apbt_setup_irq(adev);
+
+	clockevents_register_device(aevt);
+
+	apbt_enable_int(cpu);
+
+	return;
 }
 
 /*
@@ -405,34 +405,34 @@ void apbt_setup_secondary_clock(void)
  * the extra interrupt is harmless.
  */
 static int apbt_cpuhp_notify(struct notifier_block *n,
-               unsigned long action, void *hcpu)
+			     unsigned long action, void *hcpu)
 {
-       unsigned long cpu = (unsigned long)hcpu;
-       struct apbt_dev *adev = &per_cpu(cpu_apbt_dev, cpu);
-
-       switch (action & 0xf) {
-       case CPU_DEAD:
-               apbt_disable_int(cpu);
-               if (system_state == SYSTEM_RUNNING)
-                       pr_debug("skipping APBT CPU %lu offline\n", cpu);
-               else if (adev) {
-                       pr_debug("APBT clockevent for cpu %lu offline\n", cpu);
-                       free_irq(adev->irq, adev);
-               }
-               break;
-       default:
-               pr_debug(KERN_INFO "APBT notified %lu, no action\n", action);
-       }
-       return NOTIFY_OK;
+	unsigned long cpu = (unsigned long)hcpu;
+	struct apbt_dev *adev = &per_cpu(cpu_apbt_dev, cpu);
+
+	switch (action & 0xf) {
+	case CPU_DEAD:
+		apbt_disable_int(cpu);
+		if (system_state == SYSTEM_RUNNING)
+			pr_debug("skipping APBT CPU %lu offline\n", cpu);
+		else if (adev) {
+			pr_debug("APBT clockevent for cpu %lu offline\n", cpu);
+			free_irq(adev->irq, adev);
+		}
+		break;
+	default:
+		pr_debug(KERN_INFO "APBT notified %lu, no action\n", action);
+	}
+	return NOTIFY_OK;
 }
 
 static __init int apbt_late_init(void)
 {
-       if (disable_apbt_percpu)
-               return 0;
-       /* This notifier should be called after workqueue is ready */
-       hotcpu_notifier(apbt_cpuhp_notify, -20);
-       return 0;
+	if (disable_apbt_percpu)
+		return 0;
+	/* This notifier should be called after workqueue is ready */
+	hotcpu_notifier(apbt_cpuhp_notify, -20);
+	return 0;
 }
 fs_initcall(apbt_late_init);
 #else
@@ -442,93 +442,93 @@ void apbt_setup_secondary_clock(void) {}
 #endif /* CONFIG_SMP */
 
 static void apbt_set_mode(enum clock_event_mode mode,
-                         struct clock_event_device *evt)
+			  struct clock_event_device *evt)
 {
-       unsigned long ctrl;
-       uint64_t delta;
-       int timer_num;
-       struct apbt_dev *adev = EVT_TO_APBT_DEV(evt);
-
-       timer_num = adev->num;
-       pr_debug("%s CPU %d timer %d mode=%d\n",
-               __func__, first_cpu(*evt->cpumask), timer_num, mode);
-
-       switch (mode) {
-       case CLOCK_EVT_MODE_PERIODIC:
-               delta = ((uint64_t)(NSEC_PER_SEC/HZ)) * apbt_clockevent.mult;
-               delta >>= apbt_clockevent.shift;
-               ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
-               ctrl |= APBTMR_CONTROL_MODE_PERIODIC;
-               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-               /*
-                * DW APB p. 46, have to disable timer before load counter,
-                * may cause sync problem.
-                */
-               ctrl &= ~APBTMR_CONTROL_ENABLE;
-               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-               udelay(1);
-               pr_debug("Setting clock period %d for HZ %d\n", (int)delta, HZ);
-               apbt_writel(timer_num, delta, APBTMR_N_LOAD_COUNT);
-               ctrl |= APBTMR_CONTROL_ENABLE;
-               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-               break;
-       /* APB timer does not have one-shot mode, use free running mode */
-       case CLOCK_EVT_MODE_ONESHOT:
-               ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
-               /*
-                * set free running mode, this mode will let timer reload max
-                * timeout which will give time (3min on 25MHz clock) to rearm
-                * the next event, therefore emulate the one-shot mode.
-                */
-               ctrl &= ~APBTMR_CONTROL_ENABLE;
-               ctrl &= ~APBTMR_CONTROL_MODE_PERIODIC;
-
-               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-               /* write again to set free running mode */
-               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-
-               /*
-                * DW APB p. 46, load counter with all 1s before starting free
-                * running mode.
-                */
-               apbt_writel(timer_num, ~0, APBTMR_N_LOAD_COUNT);
-               ctrl &= ~APBTMR_CONTROL_INT;
-               ctrl |= APBTMR_CONTROL_ENABLE;
-               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-               break;
-
-       case CLOCK_EVT_MODE_UNUSED:
-       case CLOCK_EVT_MODE_SHUTDOWN:
-               apbt_disable_int(timer_num);
-               ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
-               ctrl &= ~APBTMR_CONTROL_ENABLE;
-               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-               break;
-
-       case CLOCK_EVT_MODE_RESUME:
-               apbt_enable_int(timer_num);
-               break;
-       }
+	unsigned long ctrl;
+	uint64_t delta;
+	int timer_num;
+	struct apbt_dev *adev = EVT_TO_APBT_DEV(evt);
+
+	timer_num = adev->num;
+	pr_debug("%s CPU %d timer %d mode=%d\n",
+		 __func__, first_cpu(*evt->cpumask), timer_num, mode);
+
+	switch (mode) {
+	case CLOCK_EVT_MODE_PERIODIC:
+		delta = ((uint64_t)(NSEC_PER_SEC/HZ)) * apbt_clockevent.mult;
+		delta >>= apbt_clockevent.shift;
+		ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
+		ctrl |= APBTMR_CONTROL_MODE_PERIODIC;
+		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+		/*
+		 * DW APB p. 46, have to disable timer before load counter,
+		 * may cause sync problem.
+		 */
+		ctrl &= ~APBTMR_CONTROL_ENABLE;
+		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+		udelay(1);
+		pr_debug("Setting clock period %d for HZ %d\n", (int)delta, HZ);
+		apbt_writel(timer_num, delta, APBTMR_N_LOAD_COUNT);
+		ctrl |= APBTMR_CONTROL_ENABLE;
+		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+		break;
+		/* APB timer does not have one-shot mode, use free running mode */
+	case CLOCK_EVT_MODE_ONESHOT:
+		ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
+		/*
+		 * set free running mode, this mode will let timer reload max
+		 * timeout which will give time (3min on 25MHz clock) to rearm
+		 * the next event, therefore emulate the one-shot mode.
+		 */
+		ctrl &= ~APBTMR_CONTROL_ENABLE;
+		ctrl &= ~APBTMR_CONTROL_MODE_PERIODIC;
+
+		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+		/* write again to set free running mode */
+		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+
+		/*
+		 * DW APB p. 46, load counter with all 1s before starting free
+		 * running mode.
+		 */
+		apbt_writel(timer_num, ~0, APBTMR_N_LOAD_COUNT);
+		ctrl &= ~APBTMR_CONTROL_INT;
+		ctrl |= APBTMR_CONTROL_ENABLE;
+		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+		break;
+
+	case CLOCK_EVT_MODE_UNUSED:
+	case CLOCK_EVT_MODE_SHUTDOWN:
+		apbt_disable_int(timer_num);
+		ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
+		ctrl &= ~APBTMR_CONTROL_ENABLE;
+		apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+		break;
+
+	case CLOCK_EVT_MODE_RESUME:
+		apbt_enable_int(timer_num);
+		break;
+	}
 }
 
 static int apbt_next_event(unsigned long delta,
-                          struct clock_event_device *evt)
+			   struct clock_event_device *evt)
 {
-       unsigned long ctrl;
-       int timer_num;
-
-       struct apbt_dev *adev = EVT_TO_APBT_DEV(evt);
-
-       timer_num = adev->num;
-       /* Disable timer */
-       ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
-       ctrl &= ~APBTMR_CONTROL_ENABLE;
-       apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-       /* write new count */
-       apbt_writel(timer_num, delta, APBTMR_N_LOAD_COUNT);
-       ctrl |= APBTMR_CONTROL_ENABLE;
-       apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
-       return  0;
+	unsigned long ctrl;
+	int timer_num;
+
+	struct apbt_dev *adev = EVT_TO_APBT_DEV(evt);
+
+	timer_num = adev->num;
+	/* Disable timer */
+	ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
+	ctrl &= ~APBTMR_CONTROL_ENABLE;
+	apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+	/* write new count */
+	apbt_writel(timer_num, delta, APBTMR_N_LOAD_COUNT);
+	ctrl |= APBTMR_CONTROL_ENABLE;
+	apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+	return 0;
 }
 
 /*
@@ -540,94 +540,94 @@ static int apbt_next_event(unsigned long delta,
  */
 static cycle_t apbt_read_clocksource(struct clocksource *cs)
 {
-       unsigned long t0, t1, t2;
-       static unsigned long last_read;
+	unsigned long t0, t1, t2;
+	static unsigned long last_read;
 
 bad_count:
-       t1 = apbt_readl(phy_cs_timer_id,
-                               APBTMR_N_CURRENT_VALUE);
-       t2 = apbt_readl(phy_cs_timer_id,
-                               APBTMR_N_CURRENT_VALUE);
-       if (unlikely(t1 < t2)) {
-               pr_debug("APBT: read current count error %lx:%lx:%lx\n",
-                               t1, t2, t2 - t1);
-               goto bad_count;
-       }
-       /*
-        * check against cached last read, makes sure time does not go back.
-        * it could be a normal rollover but we will do tripple check anyway
-        */
-       if (unlikely(t2 > last_read)) {
-               /* check if we have a normal rollover */
-               unsigned long raw_intr_status =
-                       apbt_readl_reg(APBTMRS_RAW_INT_STATUS);
-               /*
-                * cs timer interrupt is masked but raw intr bit is set if
-                * rollover occurs. then we read EOI reg to clear it.
-                */
-               if (raw_intr_status & (1 << phy_cs_timer_id)) {
-                       apbt_readl(phy_cs_timer_id, APBTMR_N_EOI);
-                       goto out;
-               }
-               pr_debug("APB CS going back %lx:%lx:%lx ",
-                               t2, last_read, t2 - last_read);
+	t1 = apbt_readl(phy_cs_timer_id,
+			APBTMR_N_CURRENT_VALUE);
+	t2 = apbt_readl(phy_cs_timer_id,
+			APBTMR_N_CURRENT_VALUE);
+	if (unlikely(t1 < t2)) {
+		pr_debug("APBT: read current count error %lx:%lx:%lx\n",
+			 t1, t2, t2 - t1);
+		goto bad_count;
+	}
+	/*
+	 * check against cached last read, makes sure time does not go back.
+	 * it could be a normal rollover but we will do tripple check anyway
+	 */
+	if (unlikely(t2 > last_read)) {
+		/* check if we have a normal rollover */
+		unsigned long raw_intr_status =
+			apbt_readl_reg(APBTMRS_RAW_INT_STATUS);
+		/*
+		 * cs timer interrupt is masked but raw intr bit is set if
+		 * rollover occurs. then we read EOI reg to clear it.
+		 */
+		if (raw_intr_status & (1 << phy_cs_timer_id)) {
+			apbt_readl(phy_cs_timer_id, APBTMR_N_EOI);
+			goto out;
+		}
+		pr_debug("APB CS going back %lx:%lx:%lx ",
+			 t2, last_read, t2 - last_read);
 bad_count_x3:
-               pr_debug(KERN_INFO "tripple check enforced\n");
-               t0 = apbt_readl(phy_cs_timer_id,
-                               APBTMR_N_CURRENT_VALUE);
-               udelay(1);
-               t1 = apbt_readl(phy_cs_timer_id,
-                               APBTMR_N_CURRENT_VALUE);
-               udelay(1);
-               t2 = apbt_readl(phy_cs_timer_id,
-                               APBTMR_N_CURRENT_VALUE);
-               if ((t2 > t1) || (t1 > t0)) {
-                       printk(KERN_ERR "Error: APB CS tripple check failed\n");
-                       goto bad_count_x3;
-               }
-       }
+		pr_debug(KERN_INFO "tripple check enforced\n");
+		t0 = apbt_readl(phy_cs_timer_id,
+				APBTMR_N_CURRENT_VALUE);
+		udelay(1);
+		t1 = apbt_readl(phy_cs_timer_id,
+				APBTMR_N_CURRENT_VALUE);
+		udelay(1);
+		t2 = apbt_readl(phy_cs_timer_id,
+				APBTMR_N_CURRENT_VALUE);
+		if ((t2 > t1) || (t1 > t0)) {
+			printk(KERN_ERR "Error: APB CS tripple check failed\n");
+			goto bad_count_x3;
+		}
+	}
 out:
-       last_read = t2;
-       return (cycle_t)~t2;
+	last_read = t2;
+	return (cycle_t)~t2;
 }
 
 static int apbt_clocksource_register(void)
 {
-       u64 start, now;
-       cycle_t t1;
-
-       /* Start the counter, use timer 2 as source, timer 0/1 for event */
-       apbt_start_counter(phy_cs_timer_id);
-
-       /* Verify whether apbt counter works */
-       t1 = apbt_read_clocksource(&clocksource_apbt);
-       rdtscll(start);
-
-       /*
-        * We don't know the TSC frequency yet, but waiting for
-        * 200000 TSC cycles is safe:
-        * 4 GHz == 50us
-        * 1 GHz == 200us
-        */
-       do {
-               rep_nop();
-               rdtscll(now);
-       } while ((now - start) < 200000UL);
-
-       /* APBT is the only always on clocksource, it has to work! */
-       if (t1 == apbt_read_clocksource(&clocksource_apbt))
-               panic("APBT counter not counting. APBT disabled\n");
-
-       /*
-        * initialize and register APBT clocksource
-        * convert that to ns/clock cycle
-        * mult = (ns/c) * 2^APBT_SHIFT
-        */
-       clocksource_apbt.mult = div_sc(MSEC_PER_SEC,
-               (unsigned long) apbt_freq, APBT_SHIFT);
-       clocksource_register(&clocksource_apbt);
-
-       return 0;
+	u64 start, now;
+	cycle_t t1;
+
+	/* Start the counter, use timer 2 as source, timer 0/1 for event */
+	apbt_start_counter(phy_cs_timer_id);
+
+	/* Verify whether apbt counter works */
+	t1 = apbt_read_clocksource(&clocksource_apbt);
+	rdtscll(start);
+
+	/*
+	 * We don't know the TSC frequency yet, but waiting for
+	 * 200000 TSC cycles is safe:
+	 * 4 GHz == 50us
+	 * 1 GHz == 200us
+	 */
+	do {
+		rep_nop();
+		rdtscll(now);
+	} while ((now - start) < 200000UL);
+
+	/* APBT is the only always on clocksource, it has to work! */
+	if (t1 == apbt_read_clocksource(&clocksource_apbt))
+		panic("APBT counter not counting. APBT disabled\n");
+
+	/*
+	 * initialize and register APBT clocksource
+	 * convert that to ns/clock cycle
+	 * mult = (ns/c) * 2^APBT_SHIFT
+	 */
+	clocksource_apbt.mult = div_sc(MSEC_PER_SEC,
+				       (unsigned long) apbt_freq, APBT_SHIFT);
+	clocksource_register(&clocksource_apbt);
+
+	return 0;
 }
 
 /*
@@ -640,145 +640,145 @@ static int apbt_clocksource_register(void)
 void __init apbt_time_init(void)
 {
 #ifdef CONFIG_SMP
-       int i;
-       struct sfi_timer_table_entry *p_mtmr;
-       unsigned int percpu_timer;
-       struct apbt_dev *adev;
+	int i;
+	struct sfi_timer_table_entry *p_mtmr;
+	unsigned int percpu_timer;
+	struct apbt_dev *adev;
 #endif
 
-       if (apb_timer_block_enabled)
-               return;
-       apbt_set_mapping();
-       if (apbt_virt_address) {
-               pr_debug("Found APBT version 0x%lx\n",\
-                        apbt_readl_reg(APBTMRS_COMP_VERSION));
-       } else
-               goto out_noapbt;
-       /*
-        * Read the frequency and check for a sane value, for ESL model
-        * we extend the possible clock range to allow time scaling.
-        */
-
-       if (apbt_freq < APBT_MIN_FREQ || apbt_freq > APBT_MAX_FREQ) {
-               pr_debug("APBT has invalid freq 0x%llx\n", apbt_freq);
-               goto out_noapbt;
-       }
-       if (apbt_clocksource_register()) {
-               pr_debug("APBT has failed to register clocksource\n");
-               goto out_noapbt;
-       }
-       if (!apbt_clockevent_register())
-               apb_timer_block_enabled = 1;
-       else {
-               pr_debug("APBT has failed to register clockevent\n");
-               goto out_noapbt;
-       }
+	if (apb_timer_block_enabled)
+		return;
+	apbt_set_mapping();
+	if (apbt_virt_address) {
+		pr_debug("Found APBT version 0x%lx\n",\
+			 apbt_readl_reg(APBTMRS_COMP_VERSION));
+	} else
+		goto out_noapbt;
+	/*
+	 * Read the frequency and check for a sane value, for ESL model
+	 * we extend the possible clock range to allow time scaling.
+	 */
+
+	if (apbt_freq < APBT_MIN_FREQ || apbt_freq > APBT_MAX_FREQ) {
+		pr_debug("APBT has invalid freq 0x%llx\n", apbt_freq);
+		goto out_noapbt;
+	}
+	if (apbt_clocksource_register()) {
+		pr_debug("APBT has failed to register clocksource\n");
+		goto out_noapbt;
+	}
+	if (!apbt_clockevent_register())
+		apb_timer_block_enabled = 1;
+	else {
+		pr_debug("APBT has failed to register clockevent\n");
+		goto out_noapbt;
+	}
 #ifdef CONFIG_SMP
-       /* kernel cmdline disable apb timer, so we will use lapic timers */
-       if (disable_apbt_percpu) {
-               printk(KERN_INFO "apbt: disabled per cpu timer\n");
-               return;
-       }
-       pr_debug("%s: %d CPUs online\n", __func__, num_online_cpus());
-       if (num_possible_cpus() <= sfi_mtimer_num) {
-               percpu_timer = 1;
-               apbt_num_timers_used = num_possible_cpus();
-       } else {
-               percpu_timer = 0;
-               apbt_num_timers_used = 1;
-               adev = &per_cpu(cpu_apbt_dev, 0);
-               adev->flags &= ~APBT_DEV_USED;
-       }
-       pr_debug("%s: %d APB timers used\n", __func__, apbt_num_timers_used);
-
-       /* here we set up per CPU timer data structure */
-       apbt_devs = kzalloc(sizeof(struct apbt_dev) * apbt_num_timers_used,
-                               GFP_KERNEL);
-       if (!apbt_devs) {
-               printk(KERN_ERR "Failed to allocate APB timer devices\n");
-               return;
-       }
-       for (i = 0; i < apbt_num_timers_used; i++) {
-               adev = &per_cpu(cpu_apbt_dev, i);
-               adev->num = i;
-               adev->cpu = i;
-               p_mtmr = sfi_get_mtmr(i);
-               if (p_mtmr) {
-                       adev->tick = p_mtmr->freq_hz;
-                       adev->irq = p_mtmr->irq;
-               } else
-                       printk(KERN_ERR "Failed to get timer for cpu %d\n", i);
-               adev->count = 0;
-               sprintf(adev->name, "apbt%d", i);
-       }
+	/* kernel cmdline disable apb timer, so we will use lapic timers */
+	if (disable_apbt_percpu) {
+		printk(KERN_INFO "apbt: disabled per cpu timer\n");
+		return;
+	}
+	pr_debug("%s: %d CPUs online\n", __func__, num_online_cpus());
+	if (num_possible_cpus() <= sfi_mtimer_num) {
+		percpu_timer = 1;
+		apbt_num_timers_used = num_possible_cpus();
+	} else {
+		percpu_timer = 0;
+		apbt_num_timers_used = 1;
+		adev = &per_cpu(cpu_apbt_dev, 0);
+		adev->flags &= ~APBT_DEV_USED;
+	}
+	pr_debug("%s: %d APB timers used\n", __func__, apbt_num_timers_used);
+
+	/* here we set up per CPU timer data structure */
+	apbt_devs = kzalloc(sizeof(struct apbt_dev) * apbt_num_timers_used,
+			    GFP_KERNEL);
+	if (!apbt_devs) {
+		printk(KERN_ERR "Failed to allocate APB timer devices\n");
+		return;
+	}
+	for (i = 0; i < apbt_num_timers_used; i++) {
+		adev = &per_cpu(cpu_apbt_dev, i);
+		adev->num = i;
+		adev->cpu = i;
+		p_mtmr = sfi_get_mtmr(i);
+		if (p_mtmr) {
+			adev->tick = p_mtmr->freq_hz;
+			adev->irq = p_mtmr->irq;
+		} else
+			printk(KERN_ERR "Failed to get timer for cpu %d\n", i);
+		adev->count = 0;
+		sprintf(adev->name, "apbt%d", i);
+	}
 #endif
 
-       return;
+	return;
 
 out_noapbt:
-       apbt_clear_mapping();
-       apb_timer_block_enabled = 0;
-       panic("failed to enable APB timer\n");
+	apbt_clear_mapping();
+	apb_timer_block_enabled = 0;
+	panic("failed to enable APB timer\n");
 }
 
 static inline void apbt_disable(int n)
 {
-       if (is_apbt_capable()) {
-               unsigned long ctrl =  apbt_readl(n, APBTMR_N_CONTROL);
-               ctrl &= ~APBTMR_CONTROL_ENABLE;
-               apbt_writel(n, ctrl, APBTMR_N_CONTROL);
-       }
+	if (is_apbt_capable()) {
+		unsigned long ctrl =  apbt_readl(n, APBTMR_N_CONTROL);
+		ctrl &= ~APBTMR_CONTROL_ENABLE;
+		apbt_writel(n, ctrl, APBTMR_N_CONTROL);
+	}
 }
 
 /* called before apb_timer_enable, use early map */
 unsigned long apbt_quick_calibrate()
 {
-       int i, scale;
-       u64 old, new;
-       cycle_t t1, t2;
-       unsigned long khz = 0;
-       u32 loop, shift;
-
-       apbt_set_mapping();
-       apbt_start_counter(phy_cs_timer_id);
-
-       /* check if the timer can count down, otherwise return */
-       old = apbt_read_clocksource(&clocksource_apbt);
-       i = 10000;
-       while (--i) {
-               if (old != apbt_read_clocksource(&clocksource_apbt))
-                       break;
-       }
-       if (!i)
-               goto failed;
-
-       /* count 16 ms */
-       loop = (apbt_freq * 1000) << 4;
-
-       /* restart the timer to ensure it won't get to 0 in the calibration */
-       apbt_start_counter(phy_cs_timer_id);
-
-       old = apbt_read_clocksource(&clocksource_apbt);
-       old += loop;
-
-       t1 = __native_read_tsc();
-
-       do {
-               new = apbt_read_clocksource(&clocksource_apbt);
-       } while (new < old);
-
-       t2 = __native_read_tsc();
-
-       shift = 5;
-       if (unlikely(loop >> shift == 0)) {
-               printk(KERN_INFO
-               "APBT TSC calibration failed, not enough resolution\n");
-               return 0;
-       }
-       scale = (int)div_u64((t2 - t1), loop >> shift);
-       khz = (scale * apbt_freq * 1000) >> shift;
-       printk(KERN_INFO "TSC freq calculated by APB timer is %lu khz\n", khz);
-       return khz;
+	int i, scale;
+	u64 old, new;
+	cycle_t t1, t2;
+	unsigned long khz = 0;
+	u32 loop, shift;
+
+	apbt_set_mapping();
+	apbt_start_counter(phy_cs_timer_id);
+
+	/* check if the timer can count down, otherwise return */
+	old = apbt_read_clocksource(&clocksource_apbt);
+	i = 10000;
+	while (--i) {
+		if (old != apbt_read_clocksource(&clocksource_apbt))
+			break;
+	}
+	if (!i)
+		goto failed;
+
+	/* count 16 ms */
+	loop = (apbt_freq * 1000) << 4;
+
+	/* restart the timer to ensure it won't get to 0 in the calibration */
+	apbt_start_counter(phy_cs_timer_id);
+
+	old = apbt_read_clocksource(&clocksource_apbt);
+	old += loop;
+
+	t1 = __native_read_tsc();
+
+	do {
+		new = apbt_read_clocksource(&clocksource_apbt);
+	} while (new < old);
+
+	t2 = __native_read_tsc();
+
+	shift = 5;
+	if (unlikely(loop >> shift == 0)) {
+		printk(KERN_INFO
+		       "APBT TSC calibration failed, not enough resolution\n");
+		return 0;
+	}
+	scale = (int)div_u64((t2 - t1), loop >> shift);
+	khz = (scale * apbt_freq * 1000) >> shift;
+	printk(KERN_INFO "TSC freq calculated by APB timer is %lu khz\n", khz);
+	return khz;
 failed:
-       return 0;
+	return 0;
 }

commit 3010673ef5f7bef4b4685566a0713de1b4306c93
Author: Jacob Pan <jacob.jun.pan@linux.intel.com>
Date:   Tue Mar 2 21:01:34 2010 -0800

    x86, mrst: Fix APB timer per cpu clockevent
    
    The current APB timer code incorrectly registers a static copy of the
    clockevent device for the boot CPU. The per cpu clockevent should be
    used instead.
    
    This bug was hidden by zero-initialized data; as such it did not get
    exposed in testing, but was discovered by code review.
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    LKML-Reference: <1267592494-7723-1-git-send-email-jacob.jun.pan@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index 83a345b0256c..6f27f8b75795 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -84,9 +84,10 @@ struct apbt_dev {
 
 int disable_apbt_percpu __cpuinitdata;
 
+static DEFINE_PER_CPU(struct apbt_dev, cpu_apbt_dev);
+
 #ifdef CONFIG_SMP
 static unsigned int apbt_num_timers_used;
-static DEFINE_PER_CPU(struct apbt_dev, cpu_apbt_dev);
 static struct apbt_dev *apbt_devs;
 #endif
 
@@ -302,6 +303,7 @@ static void apbt_disable_int(int n)
 static int __init apbt_clockevent_register(void)
 {
        struct sfi_timer_table_entry *mtmr;
+       struct apbt_dev *adev = &__get_cpu_var(cpu_apbt_dev);
 
        mtmr = sfi_get_mtmr(APBT_CLOCKEVENT0_NUM);
        if (mtmr == NULL) {
@@ -329,22 +331,24 @@ static int __init apbt_clockevent_register(void)
         * global if not used for per cpu timer.
         */
        apbt_clockevent.cpumask = cpumask_of(smp_processor_id());
+       adev->num = smp_processor_id();
+       memcpy(&adev->evt, &apbt_clockevent, sizeof(struct clock_event_device));
 
        if (disable_apbt_percpu) {
                apbt_clockevent.rating = APBT_CLOCKEVENT_RATING - 100;
-               global_clock_event = &apbt_clockevent;
+		global_clock_event = &adev->evt;
                printk(KERN_DEBUG "%s clockevent registered as global\n",
                        global_clock_event->name);
        }
 
        if (request_irq(apbt_clockevent.irq, apbt_interrupt_handler,
                IRQF_TIMER | IRQF_DISABLED | IRQF_NOBALANCING,
-               apbt_clockevent.name, &apbt_clockevent)) {
+		apbt_clockevent.name, adev)) {
                printk(KERN_ERR "Failed request IRQ for APBT%d\n",
                        apbt_clockevent.irq);
        }
 
-       clockevents_register_device(&apbt_clockevent);
+       clockevents_register_device(&adev->evt);
        /* Start APBT 0 interrupts */
        apbt_enable_int(APBT_CLOCKEVENT0_NUM);
 

commit bb24c4716185f6e116c440462c65c1f56649183b
Author: Jacob Pan <jacob.jun.pan@intel.com>
Date:   Wed Sep 2 07:37:17 2009 -0700

    x86, apbt: Moorestown APB system timer driver
    
    Moorestown platform does not have PIT or HPET platform timers.  Instead it
    has a bank of eight APB timers.  The number of available timers to the os
    is exposed via SFI mtmr tables.  All APB timer interrupts are routed via
    ioapic rtes and delivered as MSI.
    Currently, we use timer 0 and 1 for per cpu clockevent devices, timer 2
    for clocksource.
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@intel.com>
    LKML-Reference: <43F901BD926A4E43B106BF17856F0755A318D2D2@orsmsx508.amr.corp.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
new file mode 100644
index 000000000000..83a345b0256c
--- /dev/null
+++ b/arch/x86/kernel/apb_timer.c
@@ -0,0 +1,780 @@
+/*
+ * apb_timer.c: Driver for Langwell APB timers
+ *
+ * (C) Copyright 2009 Intel Corporation
+ * Author: Jacob Pan (jacob.jun.pan@intel.com)
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; version 2
+ * of the License.
+ *
+ * Note:
+ * Langwell is the south complex of Intel Moorestown MID platform. There are
+ * eight external timers in total that can be used by the operating system.
+ * The timer information, such as frequency and addresses, is provided to the
+ * OS via SFI tables.
+ * Timer interrupts are routed via FW/HW emulated IOAPIC independently via
+ * individual redirection table entries (RTE).
+ * Unlike HPET, there is no master counter, therefore one of the timers are
+ * used as clocksource. The overall allocation looks like:
+ *  - timer 0 - NR_CPUs for per cpu timer
+ *  - one timer for clocksource
+ *  - one timer for watchdog driver.
+ * It is also worth notice that APB timer does not support true one-shot mode,
+ * free-running mode will be used here to emulate one-shot mode.
+ * APB timer can also be used as broadcast timer along with per cpu local APIC
+ * timer, but by default APB timer has higher rating than local APIC timers.
+ */
+
+#include <linux/clocksource.h>
+#include <linux/clockchips.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/sysdev.h>
+#include <linux/pm.h>
+#include <linux/pci.h>
+#include <linux/sfi.h>
+#include <linux/interrupt.h>
+#include <linux/cpu.h>
+#include <linux/irq.h>
+
+#include <asm/fixmap.h>
+#include <asm/apb_timer.h>
+
+#define APBT_MASK      CLOCKSOURCE_MASK(32)
+#define APBT_SHIFT                     22
+#define APBT_CLOCKEVENT_RATING         150
+#define APBT_CLOCKSOURCE_RATING                250
+#define APBT_MIN_DELTA_USEC            200
+
+#define EVT_TO_APBT_DEV(evt) container_of(evt, struct apbt_dev, evt)
+#define APBT_CLOCKEVENT0_NUM   (0)
+#define APBT_CLOCKEVENT1_NUM   (1)
+#define APBT_CLOCKSOURCE_NUM   (2)
+
+static unsigned long apbt_address;
+static int apb_timer_block_enabled;
+static void __iomem *apbt_virt_address;
+static int phy_cs_timer_id;
+
+/*
+ * Common DW APB timer info
+ */
+static uint64_t apbt_freq;
+
+static void apbt_set_mode(enum clock_event_mode mode,
+                         struct clock_event_device *evt);
+static int apbt_next_event(unsigned long delta,
+                          struct clock_event_device *evt);
+static cycle_t apbt_read_clocksource(struct clocksource *cs);
+static void apbt_restart_clocksource(void);
+
+struct apbt_dev {
+       struct clock_event_device       evt;
+       unsigned int num;
+       int cpu;
+       unsigned int irq;
+       unsigned int tick;
+       unsigned int count;
+       unsigned int flags;
+       char name[10];
+};
+
+int disable_apbt_percpu __cpuinitdata;
+
+#ifdef CONFIG_SMP
+static unsigned int apbt_num_timers_used;
+static DEFINE_PER_CPU(struct apbt_dev, cpu_apbt_dev);
+static struct apbt_dev *apbt_devs;
+#endif
+
+static  inline unsigned long apbt_readl_reg(unsigned long a)
+{
+       return readl(apbt_virt_address + a);
+}
+
+static inline void apbt_writel_reg(unsigned long d, unsigned long a)
+{
+       writel(d, apbt_virt_address + a);
+}
+
+static inline unsigned long apbt_readl(int n, unsigned long a)
+{
+       return readl(apbt_virt_address + a + n * APBTMRS_REG_SIZE);
+}
+
+static inline void apbt_writel(int n, unsigned long d, unsigned long a)
+{
+       writel(d, apbt_virt_address + a + n * APBTMRS_REG_SIZE);
+}
+
+static inline void apbt_set_mapping(void)
+{
+       struct sfi_timer_table_entry *mtmr;
+
+       if (apbt_virt_address) {
+               pr_debug("APBT base already mapped\n");
+               return;
+       }
+       mtmr = sfi_get_mtmr(APBT_CLOCKEVENT0_NUM);
+       if (mtmr == NULL) {
+               printk(KERN_ERR "Failed to get MTMR %d from SFI\n",
+                       APBT_CLOCKEVENT0_NUM);
+               return;
+       }
+       apbt_address = (unsigned long)mtmr->phys_addr;
+       if (!apbt_address) {
+               printk(KERN_WARNING "No timer base from SFI, use default\n");
+               apbt_address = APBT_DEFAULT_BASE;
+       }
+       apbt_virt_address = ioremap_nocache(apbt_address, APBT_MMAP_SIZE);
+       if (apbt_virt_address) {
+               pr_debug("Mapped APBT physical addr %p at virtual addr %p\n",\
+                       (void *)apbt_address, (void *)apbt_virt_address);
+       } else {
+               pr_debug("Failed mapping APBT phy address at %p\n",\
+                       (void *)apbt_address);
+               goto panic_noapbt;
+       }
+       apbt_freq = mtmr->freq_hz / USEC_PER_SEC;
+       sfi_free_mtmr(mtmr);
+
+       /* Now figure out the physical timer id for clocksource device */
+       mtmr = sfi_get_mtmr(APBT_CLOCKSOURCE_NUM);
+       if (mtmr == NULL)
+               goto panic_noapbt;
+
+       /* Now figure out the physical timer id */
+       phy_cs_timer_id = (unsigned int)(mtmr->phys_addr & 0xff)
+                       / APBTMRS_REG_SIZE;
+       pr_debug("Use timer %d for clocksource\n", phy_cs_timer_id);
+       return;
+
+panic_noapbt:
+       panic("Failed to setup APB system timer\n");
+
+}
+
+static inline void apbt_clear_mapping(void)
+{
+       iounmap(apbt_virt_address);
+       apbt_virt_address = NULL;
+}
+
+/*
+ * APBT timer interrupt enable / disable
+ */
+static inline int is_apbt_capable(void)
+{
+       return apbt_virt_address ? 1 : 0;
+}
+
+static struct clocksource clocksource_apbt = {
+       .name           = "apbt",
+       .rating         = APBT_CLOCKSOURCE_RATING,
+       .read           = apbt_read_clocksource,
+       .mask           = APBT_MASK,
+       .shift          = APBT_SHIFT,
+       .flags          = CLOCK_SOURCE_IS_CONTINUOUS,
+       .resume         = apbt_restart_clocksource,
+};
+
+/* boot APB clock event device */
+static struct clock_event_device apbt_clockevent = {
+       .name           = "apbt0",
+       .features       = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT,
+       .set_mode       = apbt_set_mode,
+       .set_next_event = apbt_next_event,
+       .shift          = APBT_SHIFT,
+       .irq            = 0,
+       .rating         = APBT_CLOCKEVENT_RATING,
+};
+
+/*
+ * if user does not want to use per CPU apb timer, just give it a lower rating
+ * than local apic timer and skip the late per cpu timer init.
+ */
+static inline int __init setup_x86_mrst_timer(char *arg)
+{
+       if (!arg)
+               return -EINVAL;
+
+       if (strcmp("apbt_only", arg) == 0)
+               disable_apbt_percpu = 0;
+       else if (strcmp("lapic_and_apbt", arg) == 0)
+               disable_apbt_percpu = 1;
+       else {
+               pr_warning("X86 MRST timer option %s not recognised"
+                       " use x86_mrst_timer=apbt_only or lapic_and_apbt\n",
+                       arg);
+               return -EINVAL;
+       }
+       return 0;
+}
+__setup("x86_mrst_timer=", setup_x86_mrst_timer);
+
+/*
+ * start count down from 0xffff_ffff. this is done by toggling the enable bit
+ * then load initial load count to ~0.
+ */
+static void apbt_start_counter(int n)
+{
+       unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
+
+       ctrl &= ~APBTMR_CONTROL_ENABLE;
+       apbt_writel(n, ctrl, APBTMR_N_CONTROL);
+       apbt_writel(n, ~0, APBTMR_N_LOAD_COUNT);
+       /* enable, mask interrupt */
+       ctrl &= ~APBTMR_CONTROL_MODE_PERIODIC;
+       ctrl |= (APBTMR_CONTROL_ENABLE | APBTMR_CONTROL_INT);
+       apbt_writel(n, ctrl, APBTMR_N_CONTROL);
+       /* read it once to get cached counter value initialized */
+       apbt_read_clocksource(&clocksource_apbt);
+}
+
+static irqreturn_t apbt_interrupt_handler(int irq, void *data)
+{
+       struct apbt_dev *dev = (struct apbt_dev *)data;
+       struct clock_event_device *aevt = &dev->evt;
+
+       if (!aevt->event_handler) {
+               printk(KERN_INFO "Spurious APBT timer interrupt on %d\n",
+                               dev->num);
+               return IRQ_NONE;
+       }
+       aevt->event_handler(aevt);
+       return IRQ_HANDLED;
+}
+
+static void apbt_restart_clocksource(void)
+{
+       apbt_start_counter(phy_cs_timer_id);
+}
+
+/* Setup IRQ routing via IOAPIC */
+#ifdef CONFIG_SMP
+static void apbt_setup_irq(struct apbt_dev *adev)
+{
+       struct irq_chip *chip;
+       struct irq_desc *desc;
+
+       /* timer0 irq has been setup early */
+       if (adev->irq == 0)
+               return;
+       desc = irq_to_desc(adev->irq);
+       chip = get_irq_chip(adev->irq);
+       disable_irq(adev->irq);
+       desc->status |= IRQ_MOVE_PCNTXT;
+       irq_set_affinity(adev->irq, cpumask_of(adev->cpu));
+       /* APB timer irqs are set up as mp_irqs, timer is edge triggerred */
+       set_irq_chip_and_handler_name(adev->irq, chip, handle_edge_irq, "edge");
+       enable_irq(adev->irq);
+       if (system_state == SYSTEM_BOOTING)
+               if (request_irq(adev->irq, apbt_interrupt_handler,
+                       IRQF_TIMER | IRQF_DISABLED | IRQF_NOBALANCING,
+                       adev->name, adev)) {
+                       printk(KERN_ERR "Failed request IRQ for APBT%d\n",
+                                       adev->num);
+               }
+}
+#endif
+
+static void apbt_enable_int(int n)
+{
+       unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
+       /* clear pending intr */
+       apbt_readl(n, APBTMR_N_EOI);
+       ctrl &= ~APBTMR_CONTROL_INT;
+       apbt_writel(n, ctrl, APBTMR_N_CONTROL);
+}
+
+static void apbt_disable_int(int n)
+{
+       unsigned long ctrl = apbt_readl(n, APBTMR_N_CONTROL);
+
+       ctrl |= APBTMR_CONTROL_INT;
+       apbt_writel(n, ctrl, APBTMR_N_CONTROL);
+}
+
+
+static int __init apbt_clockevent_register(void)
+{
+       struct sfi_timer_table_entry *mtmr;
+
+       mtmr = sfi_get_mtmr(APBT_CLOCKEVENT0_NUM);
+       if (mtmr == NULL) {
+               printk(KERN_ERR "Failed to get MTMR %d from SFI\n",
+                       APBT_CLOCKEVENT0_NUM);
+               return -ENODEV;
+       }
+
+       /*
+        * We need to calculate the scaled math multiplication factor for
+        * nanosecond to apbt tick conversion.
+        * mult = (nsec/cycle)*2^APBT_SHIFT
+        */
+       apbt_clockevent.mult = div_sc((unsigned long) mtmr->freq_hz
+                       , NSEC_PER_SEC, APBT_SHIFT);
+
+       /* Calculate the min / max delta */
+       apbt_clockevent.max_delta_ns = clockevent_delta2ns(0x7FFFFFFF,
+                                                          &apbt_clockevent);
+       apbt_clockevent.min_delta_ns = clockevent_delta2ns(
+                                               APBT_MIN_DELTA_USEC*apbt_freq,
+                                               &apbt_clockevent);
+       /*
+        * Start apbt with the boot cpu mask and make it
+        * global if not used for per cpu timer.
+        */
+       apbt_clockevent.cpumask = cpumask_of(smp_processor_id());
+
+       if (disable_apbt_percpu) {
+               apbt_clockevent.rating = APBT_CLOCKEVENT_RATING - 100;
+               global_clock_event = &apbt_clockevent;
+               printk(KERN_DEBUG "%s clockevent registered as global\n",
+                       global_clock_event->name);
+       }
+
+       if (request_irq(apbt_clockevent.irq, apbt_interrupt_handler,
+               IRQF_TIMER | IRQF_DISABLED | IRQF_NOBALANCING,
+               apbt_clockevent.name, &apbt_clockevent)) {
+               printk(KERN_ERR "Failed request IRQ for APBT%d\n",
+                       apbt_clockevent.irq);
+       }
+
+       clockevents_register_device(&apbt_clockevent);
+       /* Start APBT 0 interrupts */
+       apbt_enable_int(APBT_CLOCKEVENT0_NUM);
+
+       sfi_free_mtmr(mtmr);
+       return 0;
+}
+
+#ifdef CONFIG_SMP
+/* Should be called with per cpu */
+void apbt_setup_secondary_clock(void)
+{
+       struct apbt_dev *adev;
+       struct clock_event_device *aevt;
+       int cpu;
+
+       /* Don't register boot CPU clockevent */
+       cpu = smp_processor_id();
+       if (cpu == boot_cpu_id)
+               return;
+       /*
+        * We need to calculate the scaled math multiplication factor for
+        * nanosecond to apbt tick conversion.
+        * mult = (nsec/cycle)*2^APBT_SHIFT
+        */
+       printk(KERN_INFO "Init per CPU clockevent %d\n", cpu);
+       adev = &per_cpu(cpu_apbt_dev, cpu);
+       aevt = &adev->evt;
+
+       memcpy(aevt, &apbt_clockevent, sizeof(*aevt));
+       aevt->cpumask = cpumask_of(cpu);
+       aevt->name = adev->name;
+       aevt->mode = CLOCK_EVT_MODE_UNUSED;
+
+       printk(KERN_INFO "Registering CPU %d clockevent device %s, mask %08x\n",
+               cpu, aevt->name, *(u32 *)aevt->cpumask);
+
+       apbt_setup_irq(adev);
+
+       clockevents_register_device(aevt);
+
+       apbt_enable_int(cpu);
+
+       return;
+}
+
+/*
+ * this notify handler process CPU hotplug events. in case of S0i3, nonboot
+ * cpus are disabled/enabled frequently, for performance reasons, we keep the
+ * per cpu timer irq registered so that we do need to do free_irq/request_irq.
+ *
+ * TODO: it might be more reliable to directly disable percpu clockevent device
+ * without the notifier chain. currently, cpu 0 may get interrupts from other
+ * cpu timers during the offline process due to the ordering of notification.
+ * the extra interrupt is harmless.
+ */
+static int apbt_cpuhp_notify(struct notifier_block *n,
+               unsigned long action, void *hcpu)
+{
+       unsigned long cpu = (unsigned long)hcpu;
+       struct apbt_dev *adev = &per_cpu(cpu_apbt_dev, cpu);
+
+       switch (action & 0xf) {
+       case CPU_DEAD:
+               apbt_disable_int(cpu);
+               if (system_state == SYSTEM_RUNNING)
+                       pr_debug("skipping APBT CPU %lu offline\n", cpu);
+               else if (adev) {
+                       pr_debug("APBT clockevent for cpu %lu offline\n", cpu);
+                       free_irq(adev->irq, adev);
+               }
+               break;
+       default:
+               pr_debug(KERN_INFO "APBT notified %lu, no action\n", action);
+       }
+       return NOTIFY_OK;
+}
+
+static __init int apbt_late_init(void)
+{
+       if (disable_apbt_percpu)
+               return 0;
+       /* This notifier should be called after workqueue is ready */
+       hotcpu_notifier(apbt_cpuhp_notify, -20);
+       return 0;
+}
+fs_initcall(apbt_late_init);
+#else
+
+void apbt_setup_secondary_clock(void) {}
+
+#endif /* CONFIG_SMP */
+
+static void apbt_set_mode(enum clock_event_mode mode,
+                         struct clock_event_device *evt)
+{
+       unsigned long ctrl;
+       uint64_t delta;
+       int timer_num;
+       struct apbt_dev *adev = EVT_TO_APBT_DEV(evt);
+
+       timer_num = adev->num;
+       pr_debug("%s CPU %d timer %d mode=%d\n",
+               __func__, first_cpu(*evt->cpumask), timer_num, mode);
+
+       switch (mode) {
+       case CLOCK_EVT_MODE_PERIODIC:
+               delta = ((uint64_t)(NSEC_PER_SEC/HZ)) * apbt_clockevent.mult;
+               delta >>= apbt_clockevent.shift;
+               ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
+               ctrl |= APBTMR_CONTROL_MODE_PERIODIC;
+               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+               /*
+                * DW APB p. 46, have to disable timer before load counter,
+                * may cause sync problem.
+                */
+               ctrl &= ~APBTMR_CONTROL_ENABLE;
+               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+               udelay(1);
+               pr_debug("Setting clock period %d for HZ %d\n", (int)delta, HZ);
+               apbt_writel(timer_num, delta, APBTMR_N_LOAD_COUNT);
+               ctrl |= APBTMR_CONTROL_ENABLE;
+               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+               break;
+       /* APB timer does not have one-shot mode, use free running mode */
+       case CLOCK_EVT_MODE_ONESHOT:
+               ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
+               /*
+                * set free running mode, this mode will let timer reload max
+                * timeout which will give time (3min on 25MHz clock) to rearm
+                * the next event, therefore emulate the one-shot mode.
+                */
+               ctrl &= ~APBTMR_CONTROL_ENABLE;
+               ctrl &= ~APBTMR_CONTROL_MODE_PERIODIC;
+
+               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+               /* write again to set free running mode */
+               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+
+               /*
+                * DW APB p. 46, load counter with all 1s before starting free
+                * running mode.
+                */
+               apbt_writel(timer_num, ~0, APBTMR_N_LOAD_COUNT);
+               ctrl &= ~APBTMR_CONTROL_INT;
+               ctrl |= APBTMR_CONTROL_ENABLE;
+               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+               break;
+
+       case CLOCK_EVT_MODE_UNUSED:
+       case CLOCK_EVT_MODE_SHUTDOWN:
+               apbt_disable_int(timer_num);
+               ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
+               ctrl &= ~APBTMR_CONTROL_ENABLE;
+               apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+               break;
+
+       case CLOCK_EVT_MODE_RESUME:
+               apbt_enable_int(timer_num);
+               break;
+       }
+}
+
+static int apbt_next_event(unsigned long delta,
+                          struct clock_event_device *evt)
+{
+       unsigned long ctrl;
+       int timer_num;
+
+       struct apbt_dev *adev = EVT_TO_APBT_DEV(evt);
+
+       timer_num = adev->num;
+       /* Disable timer */
+       ctrl = apbt_readl(timer_num, APBTMR_N_CONTROL);
+       ctrl &= ~APBTMR_CONTROL_ENABLE;
+       apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+       /* write new count */
+       apbt_writel(timer_num, delta, APBTMR_N_LOAD_COUNT);
+       ctrl |= APBTMR_CONTROL_ENABLE;
+       apbt_writel(timer_num, ctrl, APBTMR_N_CONTROL);
+       return  0;
+}
+
+/*
+ * APB timer clock is not in sync with pclk on Langwell, which translates to
+ * unreliable read value caused by sampling error. the error does not add up
+ * overtime and only happens when sampling a 0 as a 1 by mistake. so the time
+ * would go backwards. the following code is trying to prevent time traveling
+ * backwards. little bit paranoid.
+ */
+static cycle_t apbt_read_clocksource(struct clocksource *cs)
+{
+       unsigned long t0, t1, t2;
+       static unsigned long last_read;
+
+bad_count:
+       t1 = apbt_readl(phy_cs_timer_id,
+                               APBTMR_N_CURRENT_VALUE);
+       t2 = apbt_readl(phy_cs_timer_id,
+                               APBTMR_N_CURRENT_VALUE);
+       if (unlikely(t1 < t2)) {
+               pr_debug("APBT: read current count error %lx:%lx:%lx\n",
+                               t1, t2, t2 - t1);
+               goto bad_count;
+       }
+       /*
+        * check against cached last read, makes sure time does not go back.
+        * it could be a normal rollover but we will do tripple check anyway
+        */
+       if (unlikely(t2 > last_read)) {
+               /* check if we have a normal rollover */
+               unsigned long raw_intr_status =
+                       apbt_readl_reg(APBTMRS_RAW_INT_STATUS);
+               /*
+                * cs timer interrupt is masked but raw intr bit is set if
+                * rollover occurs. then we read EOI reg to clear it.
+                */
+               if (raw_intr_status & (1 << phy_cs_timer_id)) {
+                       apbt_readl(phy_cs_timer_id, APBTMR_N_EOI);
+                       goto out;
+               }
+               pr_debug("APB CS going back %lx:%lx:%lx ",
+                               t2, last_read, t2 - last_read);
+bad_count_x3:
+               pr_debug(KERN_INFO "tripple check enforced\n");
+               t0 = apbt_readl(phy_cs_timer_id,
+                               APBTMR_N_CURRENT_VALUE);
+               udelay(1);
+               t1 = apbt_readl(phy_cs_timer_id,
+                               APBTMR_N_CURRENT_VALUE);
+               udelay(1);
+               t2 = apbt_readl(phy_cs_timer_id,
+                               APBTMR_N_CURRENT_VALUE);
+               if ((t2 > t1) || (t1 > t0)) {
+                       printk(KERN_ERR "Error: APB CS tripple check failed\n");
+                       goto bad_count_x3;
+               }
+       }
+out:
+       last_read = t2;
+       return (cycle_t)~t2;
+}
+
+static int apbt_clocksource_register(void)
+{
+       u64 start, now;
+       cycle_t t1;
+
+       /* Start the counter, use timer 2 as source, timer 0/1 for event */
+       apbt_start_counter(phy_cs_timer_id);
+
+       /* Verify whether apbt counter works */
+       t1 = apbt_read_clocksource(&clocksource_apbt);
+       rdtscll(start);
+
+       /*
+        * We don't know the TSC frequency yet, but waiting for
+        * 200000 TSC cycles is safe:
+        * 4 GHz == 50us
+        * 1 GHz == 200us
+        */
+       do {
+               rep_nop();
+               rdtscll(now);
+       } while ((now - start) < 200000UL);
+
+       /* APBT is the only always on clocksource, it has to work! */
+       if (t1 == apbt_read_clocksource(&clocksource_apbt))
+               panic("APBT counter not counting. APBT disabled\n");
+
+       /*
+        * initialize and register APBT clocksource
+        * convert that to ns/clock cycle
+        * mult = (ns/c) * 2^APBT_SHIFT
+        */
+       clocksource_apbt.mult = div_sc(MSEC_PER_SEC,
+               (unsigned long) apbt_freq, APBT_SHIFT);
+       clocksource_register(&clocksource_apbt);
+
+       return 0;
+}
+
+/*
+ * Early setup the APBT timer, only use timer 0 for booting then switch to
+ * per CPU timer if possible.
+ * returns 1 if per cpu apbt is setup
+ * returns 0 if no per cpu apbt is chosen
+ * panic if set up failed, this is the only platform timer on Moorestown.
+ */
+void __init apbt_time_init(void)
+{
+#ifdef CONFIG_SMP
+       int i;
+       struct sfi_timer_table_entry *p_mtmr;
+       unsigned int percpu_timer;
+       struct apbt_dev *adev;
+#endif
+
+       if (apb_timer_block_enabled)
+               return;
+       apbt_set_mapping();
+       if (apbt_virt_address) {
+               pr_debug("Found APBT version 0x%lx\n",\
+                        apbt_readl_reg(APBTMRS_COMP_VERSION));
+       } else
+               goto out_noapbt;
+       /*
+        * Read the frequency and check for a sane value, for ESL model
+        * we extend the possible clock range to allow time scaling.
+        */
+
+       if (apbt_freq < APBT_MIN_FREQ || apbt_freq > APBT_MAX_FREQ) {
+               pr_debug("APBT has invalid freq 0x%llx\n", apbt_freq);
+               goto out_noapbt;
+       }
+       if (apbt_clocksource_register()) {
+               pr_debug("APBT has failed to register clocksource\n");
+               goto out_noapbt;
+       }
+       if (!apbt_clockevent_register())
+               apb_timer_block_enabled = 1;
+       else {
+               pr_debug("APBT has failed to register clockevent\n");
+               goto out_noapbt;
+       }
+#ifdef CONFIG_SMP
+       /* kernel cmdline disable apb timer, so we will use lapic timers */
+       if (disable_apbt_percpu) {
+               printk(KERN_INFO "apbt: disabled per cpu timer\n");
+               return;
+       }
+       pr_debug("%s: %d CPUs online\n", __func__, num_online_cpus());
+       if (num_possible_cpus() <= sfi_mtimer_num) {
+               percpu_timer = 1;
+               apbt_num_timers_used = num_possible_cpus();
+       } else {
+               percpu_timer = 0;
+               apbt_num_timers_used = 1;
+               adev = &per_cpu(cpu_apbt_dev, 0);
+               adev->flags &= ~APBT_DEV_USED;
+       }
+       pr_debug("%s: %d APB timers used\n", __func__, apbt_num_timers_used);
+
+       /* here we set up per CPU timer data structure */
+       apbt_devs = kzalloc(sizeof(struct apbt_dev) * apbt_num_timers_used,
+                               GFP_KERNEL);
+       if (!apbt_devs) {
+               printk(KERN_ERR "Failed to allocate APB timer devices\n");
+               return;
+       }
+       for (i = 0; i < apbt_num_timers_used; i++) {
+               adev = &per_cpu(cpu_apbt_dev, i);
+               adev->num = i;
+               adev->cpu = i;
+               p_mtmr = sfi_get_mtmr(i);
+               if (p_mtmr) {
+                       adev->tick = p_mtmr->freq_hz;
+                       adev->irq = p_mtmr->irq;
+               } else
+                       printk(KERN_ERR "Failed to get timer for cpu %d\n", i);
+               adev->count = 0;
+               sprintf(adev->name, "apbt%d", i);
+       }
+#endif
+
+       return;
+
+out_noapbt:
+       apbt_clear_mapping();
+       apb_timer_block_enabled = 0;
+       panic("failed to enable APB timer\n");
+}
+
+static inline void apbt_disable(int n)
+{
+       if (is_apbt_capable()) {
+               unsigned long ctrl =  apbt_readl(n, APBTMR_N_CONTROL);
+               ctrl &= ~APBTMR_CONTROL_ENABLE;
+               apbt_writel(n, ctrl, APBTMR_N_CONTROL);
+       }
+}
+
+/* called before apb_timer_enable, use early map */
+unsigned long apbt_quick_calibrate()
+{
+       int i, scale;
+       u64 old, new;
+       cycle_t t1, t2;
+       unsigned long khz = 0;
+       u32 loop, shift;
+
+       apbt_set_mapping();
+       apbt_start_counter(phy_cs_timer_id);
+
+       /* check if the timer can count down, otherwise return */
+       old = apbt_read_clocksource(&clocksource_apbt);
+       i = 10000;
+       while (--i) {
+               if (old != apbt_read_clocksource(&clocksource_apbt))
+                       break;
+       }
+       if (!i)
+               goto failed;
+
+       /* count 16 ms */
+       loop = (apbt_freq * 1000) << 4;
+
+       /* restart the timer to ensure it won't get to 0 in the calibration */
+       apbt_start_counter(phy_cs_timer_id);
+
+       old = apbt_read_clocksource(&clocksource_apbt);
+       old += loop;
+
+       t1 = __native_read_tsc();
+
+       do {
+               new = apbt_read_clocksource(&clocksource_apbt);
+       } while (new < old);
+
+       t2 = __native_read_tsc();
+
+       shift = 5;
+       if (unlikely(loop >> shift == 0)) {
+               printk(KERN_INFO
+               "APBT TSC calibration failed, not enough resolution\n");
+               return 0;
+       }
+       scale = (int)div_u64((t2 - t1), loop >> shift);
+       khz = (scale * apbt_freq * 1000) >> shift;
+       printk(KERN_INFO "TSC freq calculated by APB timer is %lu khz\n", khz);
+       return khz;
+failed:
+       return 0;
+}
