commit c8e3dd86600a1a7b165478cc626d69bf07967c15
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Feb 15 11:28:09 2020 -0500

    x86 user stack frame reads: switch to explicit __get_user()
    
    rather than relying upon the magic in raw_copy_from_user()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 2d6898c2cb64..6ad43fc44556 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -96,7 +96,8 @@ struct stack_frame_user {
 };
 
 static int
-copy_stack_frame(const void __user *fp, struct stack_frame_user *frame)
+copy_stack_frame(const struct stack_frame_user __user *fp,
+		 struct stack_frame_user *frame)
 {
 	int ret;
 
@@ -105,7 +106,8 @@ copy_stack_frame(const void __user *fp, struct stack_frame_user *frame)
 
 	ret = 1;
 	pagefault_disable();
-	if (__copy_from_user_inatomic(frame, fp, sizeof(*frame)))
+	if (__get_user(frame->next_fp, &fp->next_fp) ||
+	    __get_user(frame->ret_addr, &fp->ret_addr))
 		ret = 0;
 	pagefault_enable();
 

commit 2af7c85714d8cafadf925d55441458eae312cd6b
Author: Eiichi Tsukata <devel@etsukata.com>
Date:   Mon Jul 22 17:32:16 2019 +0900

    x86/stacktrace: Prevent access_ok() warnings in arch_stack_walk_user()
    
    When arch_stack_walk_user() is called from atomic contexts, access_ok() can
    trigger the following warning if compiled with CONFIG_DEBUG_ATOMIC_SLEEP=y.
    
    Reproducer:
    
      // CONFIG_DEBUG_ATOMIC_SLEEP=y
      # cd /sys/kernel/debug/tracing
      # echo 1 > options/userstacktrace
      # echo 1 > events/irq/irq_handler_entry/enable
    
      WARNING: CPU: 0 PID: 2649 at arch/x86/kernel/stacktrace.c:103 arch_stack_walk_user+0x6e/0xf6
      CPU: 0 PID: 2649 Comm: bash Not tainted 5.3.0-rc1+ #99
      RIP: 0010:arch_stack_walk_user+0x6e/0xf6
      Call Trace:
       <IRQ>
       stack_trace_save_user+0x10a/0x16d
       trace_buffer_unlock_commit_regs+0x185/0x240
       trace_event_buffer_commit+0xec/0x330
       trace_event_raw_event_irq_handler_entry+0x159/0x1e0
       __handle_irq_event_percpu+0x22d/0x440
       handle_irq_event_percpu+0x70/0x100
       handle_irq_event+0x5a/0x8b
       handle_edge_irq+0x12f/0x3f0
       handle_irq+0x34/0x40
       do_IRQ+0xa6/0x1f0
       common_interrupt+0xf/0xf
       </IRQ>
    
    Fix it by calling __range_not_ok() directly instead of access_ok() as
    copy_from_user_nmi() does. This is fine here because the actual copy is
    inside a pagefault disabled region.
    
    Reported-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Eiichi Tsukata <devel@etsukata.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190722083216.16192-2-devel@etsukata.com

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 4f36d3241faf..2d6898c2cb64 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -100,7 +100,7 @@ copy_stack_frame(const void __user *fp, struct stack_frame_user *frame)
 {
 	int ret;
 
-	if (!access_ok(fp, sizeof(*frame)))
+	if (__range_not_ok(fp, sizeof(*frame), TASK_SIZE))
 		return 0;
 
 	ret = 1;

commit cbf5b73d162b22e044fe0b7d51dcaa33be065253
Author: Eiichi Tsukata <devel@etsukata.com>
Date:   Thu Jul 11 11:35:01 2019 +0900

    x86/stacktrace: Prevent infinite loop in arch_stack_walk_user()
    
    arch_stack_walk_user() checks `if (fp == frame.next_fp)` to prevent a
    infinite loop by self reference but it's not enogh for circular reference.
    
    Once a lack of return address is found, there is no point to continue the
    loop, so break out.
    
    Fixes: 02b67518e2b1 ("tracing: add support for userspace stacktraces in tracing/iter_ctrl")
    Signed-off-by: Eiichi Tsukata <devel@etsukata.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Link: https://lkml.kernel.org/r/20190711023501.963-1-devel@etsukata.com

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 2abf27d7df6b..4f36d3241faf 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -129,11 +129,9 @@ void arch_stack_walk_user(stack_trace_consume_fn consume_entry, void *cookie,
 			break;
 		if ((unsigned long)fp < regs->sp)
 			break;
-		if (frame.ret_addr) {
-			if (!consume_entry(cookie, frame.ret_addr, false))
-				return;
-		}
-		if (fp == frame.next_fp)
+		if (!frame.ret_addr)
+			break;
+		if (!consume_entry(cookie, frame.ret_addr, false))
 			break;
 		fp = frame.next_fp;
 	}

commit 3599fe12a125fa7118da2bcc5033d7741fb5f3a1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 11:45:22 2019 +0200

    x86/stacktrace: Use common infrastructure
    
    Replace the stack_trace_save*() functions with the new arch_stack_walk()
    interfaces.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: linux-arch@vger.kernel.org
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: linux-mm@kvack.org
    Cc: David Rientjes <rientjes@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: kasan-dev@googlegroups.com
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: David Sterba <dsterba@suse.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: linux-btrfs@vger.kernel.org
    Cc: dm-devel@redhat.com
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Cc: dri-devel@lists.freedesktop.org
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Link: https://lkml.kernel.org/r/20190425094803.816485461@linutronix.de

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index b2f706f1e0b7..2abf27d7df6b 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -12,75 +12,31 @@
 #include <asm/stacktrace.h>
 #include <asm/unwind.h>
 
-static int save_stack_address(struct stack_trace *trace, unsigned long addr,
-			      bool nosched)
-{
-	if (nosched && in_sched_functions(addr))
-		return 0;
-
-	if (trace->skip > 0) {
-		trace->skip--;
-		return 0;
-	}
-
-	if (trace->nr_entries >= trace->max_entries)
-		return -1;
-
-	trace->entries[trace->nr_entries++] = addr;
-	return 0;
-}
-
-static void noinline __save_stack_trace(struct stack_trace *trace,
-			       struct task_struct *task, struct pt_regs *regs,
-			       bool nosched)
+void arch_stack_walk(stack_trace_consume_fn consume_entry, void *cookie,
+		     struct task_struct *task, struct pt_regs *regs)
 {
 	struct unwind_state state;
 	unsigned long addr;
 
-	if (regs)
-		save_stack_address(trace, regs->ip, nosched);
+	if (regs && !consume_entry(cookie, regs->ip, false))
+		return;
 
 	for (unwind_start(&state, task, regs, NULL); !unwind_done(&state);
 	     unwind_next_frame(&state)) {
 		addr = unwind_get_return_address(&state);
-		if (!addr || save_stack_address(trace, addr, nosched))
+		if (!addr || !consume_entry(cookie, addr, false))
 			break;
 	}
 }
 
 /*
- * Save stack-backtrace addresses into a stack_trace buffer.
+ * This function returns an error if it detects any unreliable features of the
+ * stack.  Otherwise it guarantees that the stack trace is reliable.
+ *
+ * If the task is not 'current', the caller *must* ensure the task is inactive.
  */
-void save_stack_trace(struct stack_trace *trace)
-{
-	trace->skip++;
-	__save_stack_trace(trace, current, NULL, false);
-}
-EXPORT_SYMBOL_GPL(save_stack_trace);
-
-void save_stack_trace_regs(struct pt_regs *regs, struct stack_trace *trace)
-{
-	__save_stack_trace(trace, current, regs, false);
-}
-
-void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
-{
-	if (!try_get_task_stack(tsk))
-		return;
-
-	if (tsk == current)
-		trace->skip++;
-	__save_stack_trace(trace, tsk, NULL, true);
-
-	put_task_stack(tsk);
-}
-EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
-
-#ifdef CONFIG_HAVE_RELIABLE_STACKTRACE
-
-static int __always_inline
-__save_stack_trace_reliable(struct stack_trace *trace,
-			    struct task_struct *task)
+int arch_stack_walk_reliable(stack_trace_consume_fn consume_entry,
+			     void *cookie, struct task_struct *task)
 {
 	struct unwind_state state;
 	struct pt_regs *regs;
@@ -117,7 +73,7 @@ __save_stack_trace_reliable(struct stack_trace *trace,
 		if (!addr)
 			return -EINVAL;
 
-		if (save_stack_address(trace, addr, false))
+		if (!consume_entry(cookie, addr, false))
 			return -EINVAL;
 	}
 
@@ -132,32 +88,6 @@ __save_stack_trace_reliable(struct stack_trace *trace,
 	return 0;
 }
 
-/*
- * This function returns an error if it detects any unreliable features of the
- * stack.  Otherwise it guarantees that the stack trace is reliable.
- *
- * If the task is not 'current', the caller *must* ensure the task is inactive.
- */
-int save_stack_trace_tsk_reliable(struct task_struct *tsk,
-				  struct stack_trace *trace)
-{
-	int ret;
-
-	/*
-	 * If the task doesn't have a stack (e.g., a zombie), the stack is
-	 * "reliably" empty.
-	 */
-	if (!try_get_task_stack(tsk))
-		return 0;
-
-	ret = __save_stack_trace_reliable(trace, tsk);
-
-	put_task_stack(tsk);
-
-	return ret;
-}
-#endif /* CONFIG_HAVE_RELIABLE_STACKTRACE */
-
 /* Userspace stacktrace - based on kernel/trace/trace_sysprof.c */
 
 struct stack_frame_user {
@@ -182,15 +112,15 @@ copy_stack_frame(const void __user *fp, struct stack_frame_user *frame)
 	return ret;
 }
 
-static inline void __save_stack_trace_user(struct stack_trace *trace)
+void arch_stack_walk_user(stack_trace_consume_fn consume_entry, void *cookie,
+			  const struct pt_regs *regs)
 {
-	const struct pt_regs *regs = task_pt_regs(current);
 	const void __user *fp = (const void __user *)regs->bp;
 
-	if (trace->nr_entries < trace->max_entries)
-		trace->entries[trace->nr_entries++] = regs->ip;
+	if (!consume_entry(cookie, regs->ip, false))
+		return;
 
-	while (trace->nr_entries < trace->max_entries) {
+	while (1) {
 		struct stack_frame_user frame;
 
 		frame.next_fp = NULL;
@@ -200,8 +130,8 @@ static inline void __save_stack_trace_user(struct stack_trace *trace)
 		if ((unsigned long)fp < regs->sp)
 			break;
 		if (frame.ret_addr) {
-			trace->entries[trace->nr_entries++] =
-				frame.ret_addr;
+			if (!consume_entry(cookie, frame.ret_addr, false))
+				return;
 		}
 		if (fp == frame.next_fp)
 			break;
@@ -209,11 +139,3 @@ static inline void __save_stack_trace_user(struct stack_trace *trace)
 	}
 }
 
-void save_stack_trace_user(struct stack_trace *trace)
-{
-	/*
-	 * Trace user stack if we are not a kernel thread
-	 */
-	if (current->mm)
-		__save_stack_trace_user(trace);
-}

commit c5c27a0a583844c69a433039e4fd6396ba23551b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 10 12:27:56 2019 +0200

    x86/stacktrace: Remove the pointless ULONG_MAX marker
    
    Terminating the last trace entry with ULONG_MAX is a completely pointless
    exercise and none of the consumers can rely on it because it's
    inconsistently implemented across architectures. In fact quite some of the
    callers remove the entry and adjust stack_trace.nr_entries afterwards.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Alexander Potapenko <glider@google.com>
    Link: https://lkml.kernel.org/r/20190410103643.750954603@linutronix.de

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 5c2d71a1dc06..b2f706f1e0b7 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -46,9 +46,6 @@ static void noinline __save_stack_trace(struct stack_trace *trace,
 		if (!addr || save_stack_address(trace, addr, nosched))
 			break;
 	}
-
-	if (trace->nr_entries < trace->max_entries)
-		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
 
 /*
@@ -97,7 +94,7 @@ __save_stack_trace_reliable(struct stack_trace *trace,
 		if (regs) {
 			/* Success path for user tasks */
 			if (user_mode(regs))
-				goto success;
+				return 0;
 
 			/*
 			 * Kernel mode registers on the stack indicate an
@@ -132,10 +129,6 @@ __save_stack_trace_reliable(struct stack_trace *trace,
 	if (!(task->flags & (PF_KTHREAD | PF_IDLE)))
 		return -EINVAL;
 
-success:
-	if (trace->nr_entries < trace->max_entries)
-		trace->entries[trace->nr_entries++] = ULONG_MAX;
-
 	return 0;
 }
 
@@ -221,9 +214,6 @@ void save_stack_trace_user(struct stack_trace *trace)
 	/*
 	 * Trace user stack if we are not a kernel thread
 	 */
-	if (current->mm) {
+	if (current->mm)
 		__save_stack_trace_user(trace);
-	}
-	if (trace->nr_entries < trace->max_entries)
-		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }

commit 96d4f267e40f9509e8a66e2b39e8b95655617693
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 3 18:57:57 2019 -0800

    Remove 'type' argument from access_ok() function
    
    Nobody has actually used the type (VERIFY_READ vs VERIFY_WRITE) argument
    of the user address range verification function since we got rid of the
    old racy i386-only code to walk page tables by hand.
    
    It existed because the original 80386 would not honor the write protect
    bit when in kernel mode, so you had to do COW by hand before doing any
    user access.  But we haven't supported that in a long time, and these
    days the 'type' argument is a purely historical artifact.
    
    A discussion about extending 'user_access_begin()' to do the range
    checking resulted this patch, because there is no way we're going to
    move the old VERIFY_xyz interface to that model.  And it's best done at
    the end of the merge window when I've done most of my merges, so let's
    just get this done once and for all.
    
    This patch was mostly done with a sed-script, with manual fix-ups for
    the cases that weren't of the trivial 'access_ok(VERIFY_xyz' form.
    
    There were a couple of notable cases:
    
     - csky still had the old "verify_area()" name as an alias.
    
     - the iter_iov code had magical hardcoded knowledge of the actual
       values of VERIFY_{READ,WRITE} (not that they mattered, since nothing
       really used it)
    
     - microblaze used the type argument for a debug printout
    
    but other than those oddities this should be a total no-op patch.
    
    I tried to fix up all architectures, did fairly extensive grepping for
    access_ok() uses, and the changes are trivial, but I may have missed
    something.  Any missed conversion should be trivially fixable, though.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 7627455047c2..5c2d71a1dc06 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -177,7 +177,7 @@ copy_stack_frame(const void __user *fp, struct stack_frame_user *frame)
 {
 	int ret;
 
-	if (!access_ok(VERIFY_READ, fp, sizeof(*frame)))
+	if (!access_ok(fp, sizeof(*frame)))
 		return 0;
 
 	ret = 1;

commit 0c414367c04eeb00c3ebfee0b74c9e7f3b95fd62
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 18 08:47:11 2018 +0200

    x86/stacktrace: Do not fail for ORC with regs on stack
    
    save_stack_trace_reliable now returns "non reliable" when there are
    kernel pt_regs on stack. This means an interrupt or exception happened
    somewhere down the route. It is a problem for the frame pointer
    unwinder, because the frame might not have been set up yet when the irq
    happened, so the unwinder might fail to unwind from the interrupted
    function.
    
    With ORC, this is not a problem, as ORC has out-of-band data. We can
    find ORC data even for the IP in the interrupted function and always
    unwind one level up reliably.
    
    So lift the check to apply only when CONFIG_FRAME_POINTER=y is enabled.
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/lkml/20180518064713.26440-4-jslaby@suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 6acf1d5ca832..7627455047c2 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -106,7 +106,8 @@ __save_stack_trace_reliable(struct stack_trace *trace,
 			 * unreliable.
 			 */
 
-			return -EINVAL;
+			if (IS_ENABLED(CONFIG_FRAME_POINTER))
+				return -EINVAL;
 		}
 
 		addr = unwind_get_return_address(&state);

commit 441ccc3580f45340715fd8f5c4db795b06326404
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 18 08:47:10 2018 +0200

    x86/stacktrace: Clarify the reliable success paths
    
    Make clear which path is for user tasks and for kthreads and idle
    tasks. This will allow easier plug-in of the ORC unwinder in the next
    patches.
    
    Note that we added a check for unwind error to the top of the loop, so
    that an error is returned also for user tasks (the 'goto success' would
    skip the check after the loop otherwise).
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/lkml/20180518064713.26440-3-jslaby@suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index f9dacf6d4667..6acf1d5ca832 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -89,21 +89,24 @@ __save_stack_trace_reliable(struct stack_trace *trace,
 	struct pt_regs *regs;
 	unsigned long addr;
 
-	for (unwind_start(&state, task, NULL, NULL); !unwind_done(&state);
+	for (unwind_start(&state, task, NULL, NULL);
+	     !unwind_done(&state) && !unwind_error(&state);
 	     unwind_next_frame(&state)) {
 
 		regs = unwind_get_entry_regs(&state, NULL);
 		if (regs) {
+			/* Success path for user tasks */
+			if (user_mode(regs))
+				goto success;
+
 			/*
 			 * Kernel mode registers on the stack indicate an
 			 * in-kernel interrupt or exception (e.g., preemption
 			 * or a page fault), which can make frame pointers
 			 * unreliable.
 			 */
-			if (!user_mode(regs))
-				return -EINVAL;
 
-			break;
+			return -EINVAL;
 		}
 
 		addr = unwind_get_return_address(&state);
@@ -124,6 +127,11 @@ __save_stack_trace_reliable(struct stack_trace *trace,
 	if (unwind_error(&state))
 		return -EINVAL;
 
+	/* Success path for non-user tasks, i.e. kthreads and idle tasks */
+	if (!(task->flags & (PF_KTHREAD | PF_IDLE)))
+		return -EINVAL;
+
+success:
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 

commit 17426923b03f098da83b8c1e044934a34959f69b
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 18 08:47:09 2018 +0200

    x86/stacktrace: Remove STACKTRACE_DUMP_ONCE
    
    The stack unwinding can sometimes fail yet. Especially with the
    generated debug info. So do not yell at users -- live patching (the only
    user of this interface) will inform the user about the failure
    gracefully.
    
    And given this was the only user of the macro, remove the macro proper
    too.
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/lkml/20180518064713.26440-2-jslaby@suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 8948b7d9c064..f9dacf6d4667 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -81,16 +81,6 @@ EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
 
 #ifdef CONFIG_HAVE_RELIABLE_STACKTRACE
 
-#define STACKTRACE_DUMP_ONCE(task) ({				\
-	static bool __section(.data.unlikely) __dumped;		\
-								\
-	if (!__dumped) {					\
-		__dumped = true;				\
-		WARN_ON(1);					\
-		show_stack(task, NULL);				\
-	}							\
-})
-
 static int __always_inline
 __save_stack_trace_reliable(struct stack_trace *trace,
 			    struct task_struct *task)
@@ -123,20 +113,16 @@ __save_stack_trace_reliable(struct stack_trace *trace,
 		 * generated code which __kernel_text_address() doesn't know
 		 * about.
 		 */
-		if (!addr) {
-			STACKTRACE_DUMP_ONCE(task);
+		if (!addr)
 			return -EINVAL;
-		}
 
 		if (save_stack_address(trace, addr, false))
 			return -EINVAL;
 	}
 
 	/* Check for stack corruption */
-	if (unwind_error(&state)) {
-		STACKTRACE_DUMP_ONCE(task);
+	if (unwind_error(&state))
 		return -EINVAL;
-	}
 
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;

commit 0797a8d0d79769574550caa5ca5d89c237723250
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 18 08:47:08 2018 +0200

    x86/stacktrace: Do not unwind after user regs
    
    Josh pointed out, that there is no way a frame can be after user regs.
    So remove the last unwind and the check.
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/lkml/20180518064713.26440-1-jslaby@suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 093f2ea5dd56..8948b7d9c064 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -113,15 +113,6 @@ __save_stack_trace_reliable(struct stack_trace *trace,
 			if (!user_mode(regs))
 				return -EINVAL;
 
-			/*
-			 * The last frame contains the user mode syscall
-			 * pt_regs.  Skip it and finish the unwind.
-			 */
-			unwind_next_frame(&state);
-			if (!unwind_done(&state)) {
-				STACKTRACE_DUMP_ONCE(task);
-				return -EINVAL;
-			}
 			break;
 		}
 

commit 00a5ae218d57741088068799b810416ac249a9ce
Merge: d6bbd51587ec 2fd9c41aea47
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 3 16:41:07 2018 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 page table isolation fixes from Thomas Gleixner:
     "A couple of urgent fixes for PTI:
    
       - Fix a PTE mismatch between user and kernel visible mapping of the
         cpu entry area (differs vs. the GLB bit) and causes a TLB mismatch
         MCE on older AMD K8 machines
    
       - Fix the misplaced CR3 switch in the SYSCALL compat entry code which
         causes access to unmapped kernel memory resulting in double faults.
    
       - Fix the section mismatch of the cpu_tss_rw percpu storage caused by
         using a different mechanism for declaration and definition.
    
       - Two fixes for dumpstack which help to decode entry stack issues
         better
    
       - Enable PTI by default in Kconfig. We should have done that earlier,
         but it slipped through the cracks.
    
       - Exclude AMD from the PTI enforcement. Not necessarily a fix, but if
         AMD is so confident that they are not affected, then we should not
         burden users with the overhead"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/process: Define cpu_tss_rw in same section as declaration
      x86/pti: Switch to kernel CR3 at early in entry_SYSCALL_compat()
      x86/dumpstack: Print registers for first stack frame
      x86/dumpstack: Fix partial register dumps
      x86/pti: Make sure the user/kernel PTEs match
      x86/cpu, x86/pti: Do not enable PTI on AMD processors
      x86/pti: Enable PTI by default

commit a9cdbe72c4e8bf3b38781c317a79326e2e1a230d
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Sun Dec 31 10:18:06 2017 -0600

    x86/dumpstack: Fix partial register dumps
    
    The show_regs_safe() logic is wrong.  When there's an iret stack frame,
    it prints the entire pt_regs -- most of which is random stack data --
    instead of just the five registers at the end.
    
    show_regs_safe() is also poorly named: the on_stack() checks aren't for
    safety.  Rename the function to show_regs_if_on_stack() and add a
    comment to explain why the checks are needed.
    
    These issues were introduced with the "partial register dump" feature of
    the following commit:
    
      b02fcf9ba121 ("x86/unwinder: Handle stack overflows more gracefully")
    
    That patch had gone through a few iterations of development, and the
    above issues were artifacts from a previous iteration of the patch where
    'regs' pointed directly to the iret frame rather than to the (partially
    empty) pt_regs.
    
    Tested-by: Alexander Tsoy <alexander@tsoy.me>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Toralf Förster <toralf.foerster@gmx.de>
    Cc: stable@vger.kernel.org
    Fixes: b02fcf9ba121 ("x86/unwinder: Handle stack overflows more gracefully")
    Link: http://lkml.kernel.org/r/5b05b8b344f59db2d3d50dbdeba92d60f2304c54.1514736742.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 8dabd7bf1673..60244bfaf88f 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -98,7 +98,7 @@ static int __save_stack_trace_reliable(struct stack_trace *trace,
 	for (unwind_start(&state, task, NULL, NULL); !unwind_done(&state);
 	     unwind_next_frame(&state)) {
 
-		regs = unwind_get_entry_regs(&state);
+		regs = unwind_get_entry_regs(&state, NULL);
 		if (regs) {
 			/*
 			 * Kernel mode registers on the stack indicate an

commit 6454b3bdd138dfc640deb5e7b9a0668fca2d55dd
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Dec 18 15:13:44 2017 -0600

    x86/stacktrace: Make zombie stack traces reliable
    
    Commit:
    
      1959a60182f4 ("x86/dumpstack: Pin the target stack when dumping it")
    
    changed the behavior of stack traces for zombies.  Before that commit,
    /proc/<pid>/stack reported the last execution path of the zombie before
    it died:
    
      [<ffffffff8105b877>] do_exit+0x6f7/0xa80
      [<ffffffff8105bc79>] do_group_exit+0x39/0xa0
      [<ffffffff8105bcf0>] __wake_up_parent+0x0/0x30
      [<ffffffff8152dd09>] system_call_fastpath+0x16/0x1b
      [<00007fd128f9c4f9>] 0x7fd128f9c4f9
      [<ffffffffffffffff>] 0xffffffffffffffff
    
    After the commit, it just reports an empty stack trace.
    
    The new behavior is actually probably more correct.  If the stack
    refcount has gone down to zero, then the task has already gone through
    do_exit() and isn't going to run anymore.  The stack could be freed at
    any time and is basically gone, so reporting an empty stack makes sense.
    
    However, save_stack_trace_tsk_reliable() treats such a missing stack
    condition as an error.  That can cause livepatch transition stalls if
    there are any unreaped zombies.  Instead, just treat it as a reliable,
    empty stack.
    
    Reported-and-tested-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: live-patching@vger.kernel.org
    Fixes: af085d9084b4 ("stacktrace/x86: add function for detecting reliable stack traces")
    Link: http://lkml.kernel.org/r/e4b09e630e99d0c1080528f0821fc9d9dbaeea82.1513631620.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 77835bc021c7..20161ef53537 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -164,8 +164,12 @@ int save_stack_trace_tsk_reliable(struct task_struct *tsk,
 {
 	int ret;
 
+	/*
+	 * If the task doesn't have a stack (e.g., a zombie), the stack is
+	 * "reliably" empty.
+	 */
 	if (!try_get_task_stack(tsk))
-		return -EINVAL;
+		return 0;
 
 	ret = __save_stack_trace_reliable(trace, tsk);
 

commit 77072f09eab19326dd2424c8dad0a443341a228f
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Fri Sep 29 11:23:35 2017 +0200

    x86/stacktrace: Avoid recording save_stack_trace() wrappers
    
    The save_stack_trace() and save_stack_trace_tsk() wrappers of
    __save_stack_trace() add themselves to the call stack, and thus appear in the
    recorded stacktraces. This is redundant and wasteful when we have limited space
    to record the useful part of the backtrace with e.g. page_owner functionality.
    
    Fix this by making sure __save_stack_trace() is noinline (which matches the
    current gcc decision) and bumping the skip in the wrappers
    (save_stack_trace_tsk() only when called for the current task). This is similar
    to what was done for arm in 3683f44c42e9 ("ARM: stacktrace: avoid listing
    stacktrace functions in stacktrace") and is pending for arm64.
    
    Also make sure that __save_stack_trace_reliable() doesn't get this problem in
    the future by marking it __always_inline (which matches current gcc decision),
    per Josh Poimboeuf.
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170929092335.2744-1-vbabka@suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 8dabd7bf1673..77835bc021c7 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -30,7 +30,7 @@ static int save_stack_address(struct stack_trace *trace, unsigned long addr,
 	return 0;
 }
 
-static void __save_stack_trace(struct stack_trace *trace,
+static void noinline __save_stack_trace(struct stack_trace *trace,
 			       struct task_struct *task, struct pt_regs *regs,
 			       bool nosched)
 {
@@ -56,6 +56,7 @@ static void __save_stack_trace(struct stack_trace *trace,
  */
 void save_stack_trace(struct stack_trace *trace)
 {
+	trace->skip++;
 	__save_stack_trace(trace, current, NULL, false);
 }
 EXPORT_SYMBOL_GPL(save_stack_trace);
@@ -70,6 +71,8 @@ void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 	if (!try_get_task_stack(tsk))
 		return;
 
+	if (tsk == current)
+		trace->skip++;
 	__save_stack_trace(trace, tsk, NULL, true);
 
 	put_task_stack(tsk);
@@ -88,8 +91,9 @@ EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
 	}							\
 })
 
-static int __save_stack_trace_reliable(struct stack_trace *trace,
-				       struct task_struct *task)
+static int __always_inline
+__save_stack_trace_reliable(struct stack_trace *trace,
+			    struct task_struct *task)
 {
 	struct unwind_state state;
 	struct pt_regs *regs;

commit af085d9084b48530153f51e6cad19fd0b1a13ed7
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Feb 13 19:42:28 2017 -0600

    stacktrace/x86: add function for detecting reliable stack traces
    
    For live patching and possibly other use cases, a stack trace is only
    useful if it can be assured that it's completely reliable.  Add a new
    save_stack_trace_tsk_reliable() function to achieve that.
    
    Note that if the target task isn't the current task, and the target task
    is allowed to run, then it could be writing the stack while the unwinder
    is reading it, resulting in possible corruption.  So the caller of
    save_stack_trace_tsk_reliable() must ensure that the task is either
    'current' or inactive.
    
    save_stack_trace_tsk_reliable() relies on the x86 unwinder's detection
    of pt_regs on the stack.  If the pt_regs are not user-mode registers
    from a syscall, then they indicate an in-kernel interrupt or exception
    (e.g. preemption or a page fault), in which case the stack is considered
    unreliable due to the nature of frame pointers.
    
    It also relies on the x86 unwinder's detection of other issues, such as:
    
    - corrupted stack data
    - stack grows the wrong way
    - stack walk doesn't reach the bottom
    - user didn't provide a large enough entries array
    
    Such issues are reported by checking unwind_error() and !unwind_done().
    
    Also add CONFIG_HAVE_RELIABLE_STACKTRACE so arch-independent code can
    determine at build time whether the function is implemented.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Ingo Molnar <mingo@kernel.org>        # for the x86 changes
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 8e2b79b88e51..8dabd7bf1673 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -76,6 +76,101 @@ void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 }
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
 
+#ifdef CONFIG_HAVE_RELIABLE_STACKTRACE
+
+#define STACKTRACE_DUMP_ONCE(task) ({				\
+	static bool __section(.data.unlikely) __dumped;		\
+								\
+	if (!__dumped) {					\
+		__dumped = true;				\
+		WARN_ON(1);					\
+		show_stack(task, NULL);				\
+	}							\
+})
+
+static int __save_stack_trace_reliable(struct stack_trace *trace,
+				       struct task_struct *task)
+{
+	struct unwind_state state;
+	struct pt_regs *regs;
+	unsigned long addr;
+
+	for (unwind_start(&state, task, NULL, NULL); !unwind_done(&state);
+	     unwind_next_frame(&state)) {
+
+		regs = unwind_get_entry_regs(&state);
+		if (regs) {
+			/*
+			 * Kernel mode registers on the stack indicate an
+			 * in-kernel interrupt or exception (e.g., preemption
+			 * or a page fault), which can make frame pointers
+			 * unreliable.
+			 */
+			if (!user_mode(regs))
+				return -EINVAL;
+
+			/*
+			 * The last frame contains the user mode syscall
+			 * pt_regs.  Skip it and finish the unwind.
+			 */
+			unwind_next_frame(&state);
+			if (!unwind_done(&state)) {
+				STACKTRACE_DUMP_ONCE(task);
+				return -EINVAL;
+			}
+			break;
+		}
+
+		addr = unwind_get_return_address(&state);
+
+		/*
+		 * A NULL or invalid return address probably means there's some
+		 * generated code which __kernel_text_address() doesn't know
+		 * about.
+		 */
+		if (!addr) {
+			STACKTRACE_DUMP_ONCE(task);
+			return -EINVAL;
+		}
+
+		if (save_stack_address(trace, addr, false))
+			return -EINVAL;
+	}
+
+	/* Check for stack corruption */
+	if (unwind_error(&state)) {
+		STACKTRACE_DUMP_ONCE(task);
+		return -EINVAL;
+	}
+
+	if (trace->nr_entries < trace->max_entries)
+		trace->entries[trace->nr_entries++] = ULONG_MAX;
+
+	return 0;
+}
+
+/*
+ * This function returns an error if it detects any unreliable features of the
+ * stack.  Otherwise it guarantees that the stack trace is reliable.
+ *
+ * If the task is not 'current', the caller *must* ensure the task is inactive.
+ */
+int save_stack_trace_tsk_reliable(struct task_struct *tsk,
+				  struct stack_trace *trace)
+{
+	int ret;
+
+	if (!try_get_task_stack(tsk))
+		return -EINVAL;
+
+	ret = __save_stack_trace_reliable(trace, tsk);
+
+	put_task_stack(tsk);
+
+	return ret;
+}
+#endif /* CONFIG_HAVE_RELIABLE_STACKTRACE */
+
 /* Userspace stacktrace - based on kernel/trace/trace_sysprof.c */
 
 struct stack_frame_user {
@@ -138,4 +233,3 @@ void save_stack_trace_user(struct stack_trace *trace)
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
-

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 330cae0025d0..8e2b79b88e51 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -5,6 +5,7 @@
  */
 #include <linux/sched.h>
 #include <linux/sched/debug.h>
+#include <linux/sched/task_stack.h>
 #include <linux/stacktrace.h>
 #include <linux/export.h>
 #include <linux/uaccess.h>

commit b17b01533b719e9949e437abf66436a875739b40
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/debug.h>
    
    We are going to split <linux/sched/debug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/debug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 0653788026e2..330cae0025d0 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -4,6 +4,7 @@
  *  Copyright (C) 2006-2009 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
  */
 #include <linux/sched.h>
+#include <linux/sched/debug.h>
 #include <linux/stacktrace.h>
 #include <linux/export.h>
 #include <linux/uaccess.h>

commit 49a612c6b06defbd6e6d334c683fea28006728e3
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Sep 16 14:18:14 2016 -0500

    x86/stacktrace: Convert save_stack_trace_*() to use the new unwinder
    
    Convert save_stack_trace_*() to use the new unwinder.  dump_trace() has
    been deprecated.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nilay Vaish <nilayvaish@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/815494c627d89887db0ce56ceffd58ad16ee6c21.1474045023.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 23fa81e24c8a..0653788026e2 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -8,73 +8,59 @@
 #include <linux/export.h>
 #include <linux/uaccess.h>
 #include <asm/stacktrace.h>
+#include <asm/unwind.h>
 
-static int save_stack_stack(void *data, const char *name)
+static int save_stack_address(struct stack_trace *trace, unsigned long addr,
+			      bool nosched)
 {
-	return 0;
-}
-
-static int
-__save_stack_address(void *data, unsigned long addr, bool reliable, bool nosched)
-{
-	struct stack_trace *trace = data;
-#ifdef CONFIG_FRAME_POINTER
-	if (!reliable)
-		return 0;
-#endif
 	if (nosched && in_sched_functions(addr))
 		return 0;
+
 	if (trace->skip > 0) {
 		trace->skip--;
 		return 0;
 	}
-	if (trace->nr_entries < trace->max_entries) {
-		trace->entries[trace->nr_entries++] = addr;
-		return 0;
-	} else {
-		return -1; /* no more room, stop walking the stack */
-	}
-}
 
-static int save_stack_address(void *data, unsigned long addr, int reliable)
-{
-	return __save_stack_address(data, addr, reliable, false);
+	if (trace->nr_entries >= trace->max_entries)
+		return -1;
+
+	trace->entries[trace->nr_entries++] = addr;
+	return 0;
 }
 
-static int
-save_stack_address_nosched(void *data, unsigned long addr, int reliable)
+static void __save_stack_trace(struct stack_trace *trace,
+			       struct task_struct *task, struct pt_regs *regs,
+			       bool nosched)
 {
-	return __save_stack_address(data, addr, reliable, true);
-}
+	struct unwind_state state;
+	unsigned long addr;
 
-static const struct stacktrace_ops save_stack_ops = {
-	.stack		= save_stack_stack,
-	.address	= save_stack_address,
-	.walk_stack	= print_context_stack,
-};
+	if (regs)
+		save_stack_address(trace, regs->ip, nosched);
 
-static const struct stacktrace_ops save_stack_ops_nosched = {
-	.stack		= save_stack_stack,
-	.address	= save_stack_address_nosched,
-	.walk_stack	= print_context_stack,
-};
+	for (unwind_start(&state, task, regs, NULL); !unwind_done(&state);
+	     unwind_next_frame(&state)) {
+		addr = unwind_get_return_address(&state);
+		if (!addr || save_stack_address(trace, addr, nosched))
+			break;
+	}
+
+	if (trace->nr_entries < trace->max_entries)
+		trace->entries[trace->nr_entries++] = ULONG_MAX;
+}
 
 /*
  * Save stack-backtrace addresses into a stack_trace buffer.
  */
 void save_stack_trace(struct stack_trace *trace)
 {
-	dump_trace(current, NULL, NULL, 0, &save_stack_ops, trace);
-	if (trace->nr_entries < trace->max_entries)
-		trace->entries[trace->nr_entries++] = ULONG_MAX;
+	__save_stack_trace(trace, current, NULL, false);
 }
 EXPORT_SYMBOL_GPL(save_stack_trace);
 
 void save_stack_trace_regs(struct pt_regs *regs, struct stack_trace *trace)
 {
-	dump_trace(current, regs, NULL, 0, &save_stack_ops, trace);
-	if (trace->nr_entries < trace->max_entries)
-		trace->entries[trace->nr_entries++] = ULONG_MAX;
+	__save_stack_trace(trace, current, regs, false);
 }
 
 void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
@@ -82,9 +68,7 @@ void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 	if (!try_get_task_stack(tsk))
 		return;
 
-	dump_trace(tsk, NULL, NULL, 0, &save_stack_ops_nosched, trace);
-	if (trace->nr_entries < trace->max_entries)
-		trace->entries[trace->nr_entries++] = ULONG_MAX;
+	__save_stack_trace(trace, tsk, NULL, true);
 
 	put_task_stack(tsk);
 }

commit 1959a60182f48879635812a03a99c02231ea8677
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Sep 15 22:45:45 2016 -0700

    x86/dumpstack: Pin the target stack when dumping it
    
    Specifically, pin the stack in save_stack_trace_tsk() and
    show_trace_log_lvl().
    
    This will prevent a crash if the target task dies before or while
    dumping its stack once we start freeing task stacks early.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/cf0082cde65d1941a996d026f2b2cdbfaca17bfa.1474003868.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 785aef1c7ef5..23fa81e24c8a 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -79,9 +79,14 @@ void save_stack_trace_regs(struct pt_regs *regs, struct stack_trace *trace)
 
 void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 {
+	if (!try_get_task_stack(tsk))
+		return;
+
 	dump_trace(tsk, NULL, NULL, 0, &save_stack_ops_nosched, trace);
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
+
+	put_task_stack(tsk);
 }
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
 

commit cb76c93982404273d746f3ccd5085b47689099a8
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Wed Sep 14 21:07:42 2016 -0500

    x86/dumpstack: Add get_stack_info() interface
    
    valid_stack_ptr() is buggy: it assumes that all stacks are of size
    THREAD_SIZE, which is not true for exception stacks.  So the
    walk_stack() callbacks will need to know the location of the beginning
    of the stack as well as the end.
    
    Another issue is that in general the various features of a stack (type,
    size, next stack pointer, description string) are scattered around in
    various places throughout the stack dump code.
    
    Encapsulate all that information in a single place with a new stack_info
    struct and a get_stack_info() interface.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nilay Vaish <nilayvaish@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/8164dd0db96b7e6a279fa17ae5e6dc375eecb4a9.1473905218.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 4738f5e0f2ab..785aef1c7ef5 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -9,7 +9,7 @@
 #include <linux/uaccess.h>
 #include <asm/stacktrace.h>
 
-static int save_stack_stack(void *data, char *name)
+static int save_stack_stack(void *data, const char *name)
 {
 	return 0;
 }

commit 186f43608a5c827f8284fe4559225b4dccaa49ef
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:56 2016 -0400

    x86/kernel: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed some implicit header usage that was fixed up accordingly.
    
    Note that some bool/obj-y instances remain since module.h is
    the header for some exception table entry stuff, and for things
    like __init_or_module (code that is tossed when MODULES=n).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-4-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 9ee98eefc44d..4738f5e0f2ab 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -5,7 +5,7 @@
  */
 #include <linux/sched.h>
 #include <linux/stacktrace.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/uaccess.h>
 #include <asm/stacktrace.h>
 

commit 568b329a02f75ed3aaae5eb2cca384cb9e09cb29
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Feb 17 19:58:57 2016 -0800

    perf: generalize perf_callchain
    
    . avoid walking the stack when there is no room left in the buffer
    . generalize get_perf_callchain() to be called from bpf helper
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index fdd0c6430e5a..9ee98eefc44d 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -14,30 +14,34 @@ static int save_stack_stack(void *data, char *name)
 	return 0;
 }
 
-static void
+static int
 __save_stack_address(void *data, unsigned long addr, bool reliable, bool nosched)
 {
 	struct stack_trace *trace = data;
 #ifdef CONFIG_FRAME_POINTER
 	if (!reliable)
-		return;
+		return 0;
 #endif
 	if (nosched && in_sched_functions(addr))
-		return;
+		return 0;
 	if (trace->skip > 0) {
 		trace->skip--;
-		return;
+		return 0;
 	}
-	if (trace->nr_entries < trace->max_entries)
+	if (trace->nr_entries < trace->max_entries) {
 		trace->entries[trace->nr_entries++] = addr;
+		return 0;
+	} else {
+		return -1; /* no more room, stop walking the stack */
+	}
 }
 
-static void save_stack_address(void *data, unsigned long addr, int reliable)
+static int save_stack_address(void *data, unsigned long addr, int reliable)
 {
 	return __save_stack_address(data, addr, reliable, false);
 }
 
-static void
+static int
 save_stack_address_nosched(void *data, unsigned long addr, int reliable)
 {
 	return __save_stack_address(data, addr, reliable, true);

commit 395810627b6a43c8d0ec948884043946fa162308
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Wed Jun 8 16:09:21 2011 +0900

    x86: Swap save_stack_trace_regs parameters
    
    Swap the 1st and 2nd parameters of save_stack_trace_regs()
    as same as the parameters of save_stack_trace_tsk().
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: yrl.pp-manager.tt@hitachi.com
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Link: http://lkml.kernel.org/r/20110608070921.17777.31103.stgit@fedora15
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 55d9bc03f696..fdd0c6430e5a 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -66,7 +66,7 @@ void save_stack_trace(struct stack_trace *trace)
 }
 EXPORT_SYMBOL_GPL(save_stack_trace);
 
-void save_stack_trace_regs(struct stack_trace *trace, struct pt_regs *regs)
+void save_stack_trace_regs(struct pt_regs *regs, struct stack_trace *trace)
 {
 	dump_trace(current, regs, NULL, 0, &save_stack_ops, trace);
 	if (trace->nr_entries < trace->max_entries)

commit 449a66fd1fa75d36dca917704827c40c8f416bca
Author: Richard Weinberger <richard@nod.at>
Date:   Thu May 12 15:11:12 2011 +0200

    x86: Remove warning and warning_symbol from struct stacktrace_ops
    
    Both warning and warning_symbol are nowhere used.
    Let's get rid of them.
    
    Signed-off-by: Richard Weinberger <richard@nod.at>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Soeren Sandmann Pedersen <ssp@redhat.com>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: x86 <x86@kernel.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Link: http://lkml.kernel.org/r/1305205872-10321-2-git-send-email-richard@nod.at
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 6515733a289d..55d9bc03f696 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -9,15 +9,6 @@
 #include <linux/uaccess.h>
 #include <asm/stacktrace.h>
 
-static void save_stack_warning(void *data, char *msg)
-{
-}
-
-static void
-save_stack_warning_symbol(void *data, char *msg, unsigned long symbol)
-{
-}
-
 static int save_stack_stack(void *data, char *name)
 {
 	return 0;
@@ -53,16 +44,12 @@ save_stack_address_nosched(void *data, unsigned long addr, int reliable)
 }
 
 static const struct stacktrace_ops save_stack_ops = {
-	.warning	= save_stack_warning,
-	.warning_symbol	= save_stack_warning_symbol,
 	.stack		= save_stack_stack,
 	.address	= save_stack_address,
 	.walk_stack	= print_context_stack,
 };
 
 static const struct stacktrace_ops save_stack_ops_nosched = {
-	.warning	= save_stack_warning,
-	.warning_symbol	= save_stack_warning_symbol,
 	.stack		= save_stack_stack,
 	.address	= save_stack_address_nosched,
 	.walk_stack	= print_context_stack,

commit e8e999cf3cc733482e390b02ff25a64cecdc0b64
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Fri Mar 18 11:40:06 2011 +0900

    x86, dumpstack: Correct stack dump info when frame pointer is available
    
    Current stack dump code scans entire stack and check each entry
    contains a pointer to kernel code. If CONFIG_FRAME_POINTER=y it
    could mark whether the pointer is valid or not based on value of
    the frame pointer. Invalid entries could be preceded by '?' sign.
    
    However this was not going to happen because scan start point
    was always higher than the frame pointer so that they could not
    meet.
    
    Commit 9c0729dc8062 ("x86: Eliminate bp argument from the stack
    tracing routines") delayed bp acquisition point, so the bp was
    read in lower frame, thus all of the entries were marked
    invalid.
    
    This patch fixes this by reverting above commit while retaining
    stack_frame() helper as suggested by Frederic Weisbecker.
    
    End result looks like below:
    
    before:
    
     [    3.508329] Call Trace:
     [    3.508551]  [<ffffffff814f35c9>] ? panic+0x91/0x199
     [    3.508662]  [<ffffffff814f3739>] ? printk+0x68/0x6a
     [    3.508770]  [<ffffffff81a981b2>] ? mount_block_root+0x257/0x26e
     [    3.508876]  [<ffffffff81a9821f>] ? mount_root+0x56/0x5a
     [    3.508975]  [<ffffffff81a98393>] ? prepare_namespace+0x170/0x1a9
     [    3.509216]  [<ffffffff81a9772b>] ? kernel_init+0x1d2/0x1e2
     [    3.509335]  [<ffffffff81003894>] ? kernel_thread_helper+0x4/0x10
     [    3.509442]  [<ffffffff814f6880>] ? restore_args+0x0/0x30
     [    3.509542]  [<ffffffff81a97559>] ? kernel_init+0x0/0x1e2
     [    3.509641]  [<ffffffff81003890>] ? kernel_thread_helper+0x0/0x10
    
    after:
    
     [    3.522991] Call Trace:
     [    3.523351]  [<ffffffff814f35b9>] panic+0x91/0x199
     [    3.523468]  [<ffffffff814f3729>] ? printk+0x68/0x6a
     [    3.523576]  [<ffffffff81a981b2>] mount_block_root+0x257/0x26e
     [    3.523681]  [<ffffffff81a9821f>] mount_root+0x56/0x5a
     [    3.523780]  [<ffffffff81a98393>] prepare_namespace+0x170/0x1a9
     [    3.523885]  [<ffffffff81a9772b>] kernel_init+0x1d2/0x1e2
     [    3.523987]  [<ffffffff81003894>] kernel_thread_helper+0x4/0x10
     [    3.524228]  [<ffffffff814f6880>] ? restore_args+0x0/0x30
     [    3.524345]  [<ffffffff81a97559>] ? kernel_init+0x0/0x1e2
     [    3.524445]  [<ffffffff81003890>] ? kernel_thread_helper+0x0/0x10
    
     -v5:
       * fix build breakage with oprofile
    
     -v4:
       * use 0 instead of regs->bp
       * separate out printk changes
    
     -v3:
       * apply comment from Frederic
       * add a couple of printk fixes
    
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Soren Sandmann <ssp@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Robert Richter <robert.richter@amd.com>
    LKML-Reference: <1300416006-3163-1-git-send-email-namhyung@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 938c8e10a19a..6515733a289d 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -73,7 +73,7 @@ static const struct stacktrace_ops save_stack_ops_nosched = {
  */
 void save_stack_trace(struct stack_trace *trace)
 {
-	dump_trace(current, NULL, NULL, &save_stack_ops, trace);
+	dump_trace(current, NULL, NULL, 0, &save_stack_ops, trace);
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
@@ -81,14 +81,14 @@ EXPORT_SYMBOL_GPL(save_stack_trace);
 
 void save_stack_trace_regs(struct stack_trace *trace, struct pt_regs *regs)
 {
-	dump_trace(current, regs, NULL, &save_stack_ops, trace);
+	dump_trace(current, regs, NULL, 0, &save_stack_ops, trace);
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
 
 void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 {
-	dump_trace(tsk, NULL, NULL, &save_stack_ops_nosched, trace);
+	dump_trace(tsk, NULL, NULL, 0, &save_stack_ops_nosched, trace);
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }

commit 9c0729dc8062bed96189bd14ac6d4920f3958743
Author: Soeren Sandmann Pedersen <sandmann@redhat.com>
Date:   Fri Nov 5 05:59:39 2010 -0400

    x86: Eliminate bp argument from the stack tracing routines
    
    The various stack tracing routines take a 'bp' argument in which the
    caller is supposed to provide the base pointer to use, or 0 if doesn't
    have one. Since bp is garbage whenever CONFIG_FRAME_POINTER is not
    defined, this means all callers in principle should either always pass
    0, or be conditional on CONFIG_FRAME_POINTER.
    
    However, there are only really three use cases for stack tracing:
    
    (a) Trace the current task, including IRQ stack if any
    (b) Trace the current task, but skip IRQ stack
    (c) Trace some other task
    
    In all cases, if CONFIG_FRAME_POINTER is not defined, bp should just
    be 0.  If it _is_ defined, then
    
    - in case (a) bp should be gotten directly from the CPU's register, so
      the caller should pass NULL for regs,
    
    - in case (b) the caller should should pass the IRQ registers to
      dump_trace(),
    
    - in case (c) bp should be gotten from the top of the task's stack, so
      the caller should pass NULL for regs.
    
    Hence, the bp argument is not necessary because the combination of
    task and regs is sufficient to determine an appropriate value for bp.
    
    This patch introduces a new inline function stack_frame(task, regs)
    that computes the desired bp. This function is then called from the
    two versions of dump_stack().
    
    Signed-off-by: Soren Sandmann <ssp@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arjan van de Ven <arjan@infradead.org>,
    Cc: Frederic Weisbecker <fweisbec@gmail.com>,
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>,
    LKML-Reference: <m3oc9rop28.fsf@dhcp-100-3-82.bos.redhat.com>>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index b53c525368a7..938c8e10a19a 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -73,22 +73,22 @@ static const struct stacktrace_ops save_stack_ops_nosched = {
  */
 void save_stack_trace(struct stack_trace *trace)
 {
-	dump_trace(current, NULL, NULL, 0, &save_stack_ops, trace);
+	dump_trace(current, NULL, NULL, &save_stack_ops, trace);
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
 EXPORT_SYMBOL_GPL(save_stack_trace);
 
-void save_stack_trace_bp(struct stack_trace *trace, unsigned long bp)
+void save_stack_trace_regs(struct stack_trace *trace, struct pt_regs *regs)
 {
-	dump_trace(current, NULL, NULL, bp, &save_stack_ops, trace);
+	dump_trace(current, regs, NULL, &save_stack_ops, trace);
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
 
 void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 {
-	dump_trace(tsk, NULL, NULL, 0, &save_stack_ops_nosched, trace);
+	dump_trace(tsk, NULL, NULL, &save_stack_ops_nosched, trace);
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }

commit 018378c55b03f88ff513aba4e0e93b8d4a9cf241
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jun 3 21:32:43 2010 +0200

    x86: Unify save_stack_address() and save_stack_address_nosched()
    
    Cleanup. Factor the common code in save_stack_address() and
    save_stack_address_nosched().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    LKML-Reference: <20100603193243.GA31534@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index abc321d55870..b53c525368a7 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -23,13 +23,16 @@ static int save_stack_stack(void *data, char *name)
 	return 0;
 }
 
-static void save_stack_address(void *data, unsigned long addr, int reliable)
+static void
+__save_stack_address(void *data, unsigned long addr, bool reliable, bool nosched)
 {
 	struct stack_trace *trace = data;
 #ifdef CONFIG_FRAME_POINTER
 	if (!reliable)
 		return;
 #endif
+	if (nosched && in_sched_functions(addr))
+		return;
 	if (trace->skip > 0) {
 		trace->skip--;
 		return;
@@ -38,22 +41,15 @@ static void save_stack_address(void *data, unsigned long addr, int reliable)
 		trace->entries[trace->nr_entries++] = addr;
 }
 
+static void save_stack_address(void *data, unsigned long addr, int reliable)
+{
+	return __save_stack_address(data, addr, reliable, false);
+}
+
 static void
 save_stack_address_nosched(void *data, unsigned long addr, int reliable)
 {
-	struct stack_trace *trace = data;
-#ifdef CONFIG_FRAME_POINTER
-	if (!reliable)
-		return;
-#endif
-	if (in_sched_functions(addr))
-		return;
-	if (trace->skip > 0) {
-		trace->skip--;
-		return;
-	}
-	if (trace->nr_entries < trace->max_entries)
-		trace->entries[trace->nr_entries++] = addr;
+	return __save_stack_address(data, addr, reliable, true);
 }
 
 static const struct stacktrace_ops save_stack_ops = {

commit 147ec4d2361e355ab32499f739cc24845ceb89da
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jun 3 21:32:39 2010 +0200

    x86: Make save_stack_address() !CONFIG_FRAME_POINTER friendly
    
    If CONFIG_FRAME_POINTER=n, print_context_stack() shouldn't neglect the
    non-reliable addresses on stack, this is all we have if dump_trace(bp)
    is called with the wrong or zero bp.
    
    For example, /proc/pid/stack doesn't work if CONFIG_FRAME_POINTER=n.
    
    This patch obviously has no effect if CONFIG_FRAME_POINTER=y, otherwise
    it reverts 1650743c "x86: don't save unreliable stack trace entries".
    
    Also, remove the unnecessary type-cast.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20100603193239.GA31530@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index ea54d029fe27..abc321d55870 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -26,8 +26,10 @@ static int save_stack_stack(void *data, char *name)
 static void save_stack_address(void *data, unsigned long addr, int reliable)
 {
 	struct stack_trace *trace = data;
+#ifdef CONFIG_FRAME_POINTER
 	if (!reliable)
 		return;
+#endif
 	if (trace->skip > 0) {
 		trace->skip--;
 		return;
@@ -39,9 +41,11 @@ static void save_stack_address(void *data, unsigned long addr, int reliable)
 static void
 save_stack_address_nosched(void *data, unsigned long addr, int reliable)
 {
-	struct stack_trace *trace = (struct stack_trace *)data;
+	struct stack_trace *trace = data;
+#ifdef CONFIG_FRAME_POINTER
 	if (!reliable)
 		return;
+#endif
 	if (in_sched_functions(addr))
 		return;
 	if (trace->skip > 0) {

commit c9cf4dbb4d9ca715d8fedf13301a53296429abc6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed May 19 21:35:17 2010 +0200

    x86: Unify dumpstack.h and stacktrace.h
    
    arch/x86/include/asm/stacktrace.h and arch/x86/kernel/dumpstack.h
    declare headers of objects that deal with the same topic.
    Actually most of the files that include stacktrace.h also include
    dumpstack.h
    
    Although dumpstack.h seems more reserved for internals of stack
    traces, those are quite often needed to define specialized stack
    trace operations. And perf event arch headers are going to need
    access to such low level operations anyway. So don't continue to
    bother with dumpstack.h as it's not anymore about isolated deep
    internals.
    
    v2: fix struct stack_frame definition conflict in sysprof
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Soeren Sandmann <sandmann@daimi.au.dk>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 922eefbb3f6c..ea54d029fe27 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -96,12 +96,13 @@ EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
 
 /* Userspace stacktrace - based on kernel/trace/trace_sysprof.c */
 
-struct stack_frame {
+struct stack_frame_user {
 	const void __user	*next_fp;
 	unsigned long		ret_addr;
 };
 
-static int copy_stack_frame(const void __user *fp, struct stack_frame *frame)
+static int
+copy_stack_frame(const void __user *fp, struct stack_frame_user *frame)
 {
 	int ret;
 
@@ -126,7 +127,7 @@ static inline void __save_stack_trace_user(struct stack_trace *trace)
 		trace->entries[trace->nr_entries++] = regs->ip;
 
 	while (trace->nr_entries < trace->max_entries) {
-		struct stack_frame frame;
+		struct stack_frame_user frame;
 
 		frame.next_fp = NULL;
 		frame.ret_addr = 0;

commit 61c1917f47f73c968e92d04d15370b1dc3ec4592
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Dec 17 05:40:33 2009 +0100

    perf events, x86/stacktrace: Make stack walking optional
    
    The current print_context_stack helper that does the stack
    walking job is good for usual stacktraces as it walks through
    all the stack and reports even addresses that look unreliable,
    which is nice when we don't have frame pointers for example.
    
    But we have users like perf that only require reliable
    stacktraces, and those may want a more adapted stack walker, so
    lets make this function a callback in stacktrace_ops that users
    can tune for their needs.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1261024834-5336-1-git-send-regression-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index c3eb207181fe..922eefbb3f6c 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -53,17 +53,19 @@ save_stack_address_nosched(void *data, unsigned long addr, int reliable)
 }
 
 static const struct stacktrace_ops save_stack_ops = {
-	.warning = save_stack_warning,
-	.warning_symbol = save_stack_warning_symbol,
-	.stack = save_stack_stack,
-	.address = save_stack_address,
+	.warning	= save_stack_warning,
+	.warning_symbol	= save_stack_warning_symbol,
+	.stack		= save_stack_stack,
+	.address	= save_stack_address,
+	.walk_stack	= print_context_stack,
 };
 
 static const struct stacktrace_ops save_stack_ops_nosched = {
-	.warning = save_stack_warning,
-	.warning_symbol = save_stack_warning_symbol,
-	.stack = save_stack_stack,
-	.address = save_stack_address_nosched,
+	.warning	= save_stack_warning,
+	.warning_symbol	= save_stack_warning_symbol,
+	.stack		= save_stack_stack,
+	.address	= save_stack_address_nosched,
+	.walk_stack	= print_context_stack,
 };
 
 /*

commit acc6be5405b90c9f0fb0eb8a74ec4d4b7b5bf48f
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Tue May 20 11:15:43 2008 +0200

    x86: add save_stack_trace_bp() for tracing from a specific stack frame
    
    This will help kmemcheck (and possibly other debugging tools) since we
    can now simply pass regs->bp to the stack tracer instead of specifying
    the number of stack frames to skip, which is unreliable if gcc decides
    to inline functions, etc.
    
    Note that this makes the API incomplete for other architectures, but I
    expect that those can be updated lazily, e.g. when they need it.
    
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 4aaf7e48394f..c3eb207181fe 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -77,6 +77,13 @@ void save_stack_trace(struct stack_trace *trace)
 }
 EXPORT_SYMBOL_GPL(save_stack_trace);
 
+void save_stack_trace_bp(struct stack_trace *trace, unsigned long bp)
+{
+	dump_trace(current, NULL, NULL, bp, &save_stack_ops, trace);
+	if (trace->nr_entries < trace->max_entries)
+		trace->entries[trace->nr_entries++] = ULONG_MAX;
+}
+
 void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 {
 	dump_trace(tsk, NULL, NULL, 0, &save_stack_ops_nosched, trace);

commit 29a679754b1a2581ee456eada6c2de7ce95068bb
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu May 14 23:19:09 2009 -0400

    x86/stacktrace: return 0 instead of -1 for stack ops
    
    If we return -1 in the ops->stack for the stacktrace saving, we end up
    breaking out of the loop if the stack we are tracing is in the exception
    stack. This causes traces like:
    
              <idle>-0     [002] 34263.745825: raise_softirq_irqoff <-__blk_complete_request
              <idle>-0     [002] 34263.745826:
     <= 0
     <= 0
     <= 0
     <= 0
     <= 0
     <= 0
     <= 0
    
    By returning "0" instead, the irq stack is saved as well, and we see:
    
              <idle>-0     [003]   883.280992: raise_softirq_irqoff <-__hrtimer_star
    t_range_ns
              <idle>-0     [003]   883.280992:
     <= hrtimer_start_range_ns
     <= tick_nohz_restart_sched_tick
     <= cpu_idle
     <= start_secondary
     <=
     <= 0
     <= 0
    
    [ Impact: record stacks from interrupts ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index f7bddc2e37d1..4aaf7e48394f 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -20,7 +20,7 @@ save_stack_warning_symbol(void *data, char *msg, unsigned long symbol)
 
 static int save_stack_stack(void *data, char *name)
 {
-	return -1;
+	return 0;
 }
 
 static void save_stack_address(void *data, unsigned long addr, int reliable)

commit 8f47e16348e8e25eedf639092a8a2f10a66aba34
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jan 31 02:03:42 2009 +0100

    x86: update copyrights
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 10786af95545..f7bddc2e37d1 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -1,7 +1,7 @@
 /*
  * Stack trace management functions
  *
- *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
+ *  Copyright (C) 2006-2009 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
  */
 #include <linux/sched.h>
 #include <linux/stacktrace.h>

commit 8d7c6a96164651dbbab449ef0b5c20ae1f76a3a1
Author: Török Edwin <edwintorok@gmail.com>
Date:   Sun Nov 23 12:39:06 2008 +0200

    tracing/stack-tracer: fix style issues
    
    Impact: cleanup
    
    Signed-off-by: Török Edwin <edwintorok@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index b15153060417..10786af95545 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -89,7 +89,7 @@ EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
 
 struct stack_frame {
 	const void __user	*next_fp;
-	unsigned long		return_address;
+	unsigned long		ret_addr;
 };
 
 static int copy_stack_frame(const void __user *fp, struct stack_frame *frame)
@@ -108,33 +108,40 @@ static int copy_stack_frame(const void __user *fp, struct stack_frame *frame)
 	return ret;
 }
 
+static inline void __save_stack_trace_user(struct stack_trace *trace)
+{
+	const struct pt_regs *regs = task_pt_regs(current);
+	const void __user *fp = (const void __user *)regs->bp;
+
+	if (trace->nr_entries < trace->max_entries)
+		trace->entries[trace->nr_entries++] = regs->ip;
+
+	while (trace->nr_entries < trace->max_entries) {
+		struct stack_frame frame;
+
+		frame.next_fp = NULL;
+		frame.ret_addr = 0;
+		if (!copy_stack_frame(fp, &frame))
+			break;
+		if ((unsigned long)fp < regs->sp)
+			break;
+		if (frame.ret_addr) {
+			trace->entries[trace->nr_entries++] =
+				frame.ret_addr;
+		}
+		if (fp == frame.next_fp)
+			break;
+		fp = frame.next_fp;
+	}
+}
+
 void save_stack_trace_user(struct stack_trace *trace)
 {
 	/*
 	 * Trace user stack if we are not a kernel thread
 	 */
 	if (current->mm) {
-		const struct pt_regs *regs = task_pt_regs(current);
-		const void __user *fp = (const void __user *)regs->bp;
-
-		if (trace->nr_entries < trace->max_entries)
-			trace->entries[trace->nr_entries++] = regs->ip;
-
-		while (trace->nr_entries < trace->max_entries) {
-			struct stack_frame frame;
-			frame.next_fp = NULL;
-			frame.return_address = 0;
-			if (!copy_stack_frame(fp, &frame))
-				break;
-			if ((unsigned long)fp < regs->sp)
-				break;
-			if (frame.return_address)
-				trace->entries[trace->nr_entries++] =
-					frame.return_address;
-			if (fp == frame.next_fp)
-				break;
-			fp = frame.next_fp;
-		}
+		__save_stack_trace_user(trace);
 	}
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;

commit 02b67518e2b1c490787dac7f35e1204e74fe21ba
Author: Török Edwin <edwintorok@gmail.com>
Date:   Sat Nov 22 13:28:47 2008 +0200

    tracing: add support for userspace stacktraces in tracing/iter_ctrl
    
    Impact: add new (default-off) tracing visualization feature
    
    Usage example:
    
     mount -t debugfs nodev /sys/kernel/debug
     cd /sys/kernel/debug/tracing
     echo userstacktrace >iter_ctrl
     echo sched_switch >current_tracer
     echo 1 >tracing_enabled
     .... run application ...
     echo 0 >tracing_enabled
    
    Then read one of 'trace','latency_trace','trace_pipe'.
    
    To get the best output you can compile your userspace programs with
    frame pointers (at least glibc + the app you are tracing).
    
    Signed-off-by: Török Edwin <edwintorok@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index a03e7f6d90c3..b15153060417 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -6,6 +6,7 @@
 #include <linux/sched.h>
 #include <linux/stacktrace.h>
 #include <linux/module.h>
+#include <linux/uaccess.h>
 #include <asm/stacktrace.h>
 
 static void save_stack_warning(void *data, char *msg)
@@ -83,3 +84,59 @@ void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
+
+/* Userspace stacktrace - based on kernel/trace/trace_sysprof.c */
+
+struct stack_frame {
+	const void __user	*next_fp;
+	unsigned long		return_address;
+};
+
+static int copy_stack_frame(const void __user *fp, struct stack_frame *frame)
+{
+	int ret;
+
+	if (!access_ok(VERIFY_READ, fp, sizeof(*frame)))
+		return 0;
+
+	ret = 1;
+	pagefault_disable();
+	if (__copy_from_user_inatomic(frame, fp, sizeof(*frame)))
+		ret = 0;
+	pagefault_enable();
+
+	return ret;
+}
+
+void save_stack_trace_user(struct stack_trace *trace)
+{
+	/*
+	 * Trace user stack if we are not a kernel thread
+	 */
+	if (current->mm) {
+		const struct pt_regs *regs = task_pt_regs(current);
+		const void __user *fp = (const void __user *)regs->bp;
+
+		if (trace->nr_entries < trace->max_entries)
+			trace->entries[trace->nr_entries++] = regs->ip;
+
+		while (trace->nr_entries < trace->max_entries) {
+			struct stack_frame frame;
+			frame.next_fp = NULL;
+			frame.return_address = 0;
+			if (!copy_stack_frame(fp, &frame))
+				break;
+			if ((unsigned long)fp < regs->sp)
+				break;
+			if (frame.return_address)
+				trace->entries[trace->nr_entries++] =
+					frame.return_address;
+			if (fp == frame.next_fp)
+				break;
+			fp = frame.next_fp;
+		}
+	}
+	if (trace->nr_entries < trace->max_entries)
+		trace->entries[trace->nr_entries++] = ULONG_MAX;
+}
+

commit 8594698ebddeef5443b7da8258ae33b3eaca61d5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jun 27 21:20:17 2008 +0200

    stacktrace: fix modular build, export print_stack_trace and save_stack_trace
    
    fix:
    
    ERROR: "print_stack_trace" [kernel/backtracetest.ko] undefined!
    ERROR: "save_stack_trace" [kernel/backtracetest.ko] undefined!
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    
    and fix:
    
      Building modules, stage 2.
      MODPOST 376 modules
    ERROR: "print_stack_trace" [kernel/backtracetest.ko] undefined!
    make[1]: *** [__modpost] Error 1
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index c28c342c162f..a03e7f6d90c3 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -74,6 +74,7 @@ void save_stack_trace(struct stack_trace *trace)
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
+EXPORT_SYMBOL_GPL(save_stack_trace);
 
 void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 {
@@ -81,3 +82,4 @@ void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
+EXPORT_SYMBOL_GPL(save_stack_trace_tsk);

commit 1650743cdc0db73478f72c57544ce79ea8f3dda6
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Fri Feb 22 19:23:58 2008 +0100

    x86: don't save unreliable stack trace entries
    
    Currently, there is no way for print_stack_trace() to determine whether
    a given stack trace entry was deemed reliable or not, simply because
    save_stack_trace() does not record this information. (Perhaps needless
    to say, this makes the saved stack traces A LOT harder to read, and
    probably with no other benefits, since debugging features that use
    save_stack_trace() most likely also require frame pointers, etc.)
    
    This patch reverts to the old behaviour of only recording the reliable trace
    entries for saved stack traces.
    
    Signed-off-by: Vegard Nossum <vegardno@ifi.uio.no>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 02f0f61f5b11..c28c342c162f 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -25,6 +25,8 @@ static int save_stack_stack(void *data, char *name)
 static void save_stack_address(void *data, unsigned long addr, int reliable)
 {
 	struct stack_trace *trace = data;
+	if (!reliable)
+		return;
 	if (trace->skip > 0) {
 		trace->skip--;
 		return;
@@ -37,6 +39,8 @@ static void
 save_stack_address_nosched(void *data, unsigned long addr, int reliable)
 {
 	struct stack_trace *trace = (struct stack_trace *)data;
+	if (!reliable)
+		return;
 	if (in_sched_functions(addr))
 		return;
 	if (trace->skip > 0) {

commit ade1af77129dea6e335b525ed3be3b846bc1ec13
Author: Jan Engelhardt <jengelh@computergmbh.de>
Date:   Wed Jan 30 13:33:23 2008 +0100

    x86: remove unneded casts
    
    x86: remove unneeded casts
    
    Signed-off-by: Jan Engelhardt <jengelh@computergmbh.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 4f4021b5bfb5..02f0f61f5b11 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -24,7 +24,7 @@ static int save_stack_stack(void *data, char *name)
 
 static void save_stack_address(void *data, unsigned long addr, int reliable)
 {
-	struct stack_trace *trace = (struct stack_trace *)data;
+	struct stack_trace *trace = data;
 	if (trace->skip > 0) {
 		trace->skip--;
 		return;

commit 5bc27dc2f55fd3043597b5a8de6536183f28a449
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Wed Jan 30 13:33:07 2008 +0100

    x86: pull bp calculation earlier into the backtrace path
    
    Right now, we take the stack pointer early during the backtrace path, but
    only calculate bp several functions deep later, making it hard to reconcile
    the stack and bp backtraces (as well as showing several internal backtrace
    functions on the stack with bp based backtracing).
    
    This patch moves the bp taking to the same place we take the stack pointer;
    sadly this ripples through several layers of the back tracing stack,
    but it's not all that bad in the end I hope.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 8c4e4f5bf040..4f4021b5bfb5 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -33,7 +33,8 @@ static void save_stack_address(void *data, unsigned long addr, int reliable)
 		trace->entries[trace->nr_entries++] = addr;
 }
 
-static void save_stack_address_nosched(void *data, unsigned long addr)
+static void
+save_stack_address_nosched(void *data, unsigned long addr, int reliable)
 {
 	struct stack_trace *trace = (struct stack_trace *)data;
 	if (in_sched_functions(addr))
@@ -65,14 +66,14 @@ static const struct stacktrace_ops save_stack_ops_nosched = {
  */
 void save_stack_trace(struct stack_trace *trace)
 {
-	dump_trace(current, NULL, NULL, &save_stack_ops, trace);
+	dump_trace(current, NULL, NULL, 0, &save_stack_ops, trace);
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
 
 void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 {
-	dump_trace(tsk, NULL, NULL, &save_stack_ops_nosched, trace);
+	dump_trace(tsk, NULL, NULL, 0, &save_stack_ops_nosched, trace);
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }

commit bc850d6b374fffd08336996f4b4d3bbd6bf427f6
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Wed Jan 30 13:33:07 2008 +0100

    x86: add the capability to print fuzzy backtraces
    
    For enhancing the 32 bit EBP based backtracer, I need the capability
    for the backtracer to tell it's customer that an entry is either
    reliable or unreliable, and the backtrace printing code then needs to
    print the unreliable ones slightly different.
    
    This patch adds the basic capability, the next patch will add a user
    of this capability.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index c571edd11878..8c4e4f5bf040 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -22,7 +22,7 @@ static int save_stack_stack(void *data, char *name)
 	return -1;
 }
 
-static void save_stack_address(void *data, unsigned long addr)
+static void save_stack_address(void *data, unsigned long addr, int reliable)
 {
 	struct stack_trace *trace = (struct stack_trace *)data;
 	if (trace->skip > 0) {

commit 3abf024d2abb79614d8c4cb25a70d5596f77d0ad
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 30 13:30:28 2008 +0100

    x86: nuke a ton of unused exports
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 55771fd7e545..c571edd11878 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -69,7 +69,6 @@ void save_stack_trace(struct stack_trace *trace)
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
-EXPORT_SYMBOL(save_stack_trace);
 
 void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 {

commit 9745512ce79de686df354dc70a8d1a74d801892d
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Fri Jan 25 21:08:34 2008 +0100

    sched: latencytop support
    
    LatencyTOP kernel infrastructure; it measures latencies in the
    scheduler and tracks it system wide and per process.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 6fa6cf036c70..55771fd7e545 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -33,6 +33,19 @@ static void save_stack_address(void *data, unsigned long addr)
 		trace->entries[trace->nr_entries++] = addr;
 }
 
+static void save_stack_address_nosched(void *data, unsigned long addr)
+{
+	struct stack_trace *trace = (struct stack_trace *)data;
+	if (in_sched_functions(addr))
+		return;
+	if (trace->skip > 0) {
+		trace->skip--;
+		return;
+	}
+	if (trace->nr_entries < trace->max_entries)
+		trace->entries[trace->nr_entries++] = addr;
+}
+
 static const struct stacktrace_ops save_stack_ops = {
 	.warning = save_stack_warning,
 	.warning_symbol = save_stack_warning_symbol,
@@ -40,6 +53,13 @@ static const struct stacktrace_ops save_stack_ops = {
 	.address = save_stack_address,
 };
 
+static const struct stacktrace_ops save_stack_ops_nosched = {
+	.warning = save_stack_warning,
+	.warning_symbol = save_stack_warning_symbol,
+	.stack = save_stack_stack,
+	.address = save_stack_address_nosched,
+};
+
 /*
  * Save stack-backtrace addresses into a stack_trace buffer.
  */
@@ -50,3 +70,10 @@ void save_stack_trace(struct stack_trace *trace)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
 EXPORT_SYMBOL(save_stack_trace);
+
+void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
+{
+	dump_trace(tsk, NULL, NULL, &save_stack_ops_nosched, trace);
+	if (trace->nr_entries < trace->max_entries)
+		trace->entries[trace->nr_entries++] = ULONG_MAX;
+}

commit 9689ba8ad0dc27c0a2ce40eb4c0f8fb66551119c
Author: Jan Beulich <jbeulich@novell.com>
Date:   Wed Oct 17 18:04:37 2007 +0200

    x86: constify stacktrace_ops
    
    .. as they're never written to.
    
    [ tglx: arch/x86 adaptation ]
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 413e527cdeb9..6fa6cf036c70 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -33,7 +33,7 @@ static void save_stack_address(void *data, unsigned long addr)
 		trace->entries[trace->nr_entries++] = addr;
 }
 
-static struct stacktrace_ops save_stack_ops = {
+static const struct stacktrace_ops save_stack_ops = {
 	.warning = save_stack_warning,
 	.warning_symbol = save_stack_warning_symbol,
 	.stack = save_stack_stack,

commit 835c34a1687f524c37d4fb8bad18d642c74bed8d
Author: Dave Jones <davej@redhat.com>
Date:   Fri Oct 12 21:10:53 2007 -0400

    Delete filenames in comments.
    
    Since the x86 merge, lots of files that referenced their own filenames
    are no longer correct.  Rather than keep them up to date, just delete
    them, as they add no real value.
    
    Additionally:
    - fix up comment formatting in scx200_32.c
    - Remove a credit from myself in setup_64.c from a time when we had no SCM
    - remove longwinded history from tsc_32.c which can be figured out from
      git.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index cb9109113584..413e527cdeb9 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -1,6 +1,4 @@
 /*
- * arch/x86_64/kernel/stacktrace.c
- *
  * Stack trace management functions
  *
  *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

commit 250c22777fe1ccd7ac588579a6c16db4c0161cc5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:17:24 2007 +0200

    x86_64: move kernel
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
new file mode 100644
index 000000000000..cb9109113584
--- /dev/null
+++ b/arch/x86/kernel/stacktrace.c
@@ -0,0 +1,54 @@
+/*
+ * arch/x86_64/kernel/stacktrace.c
+ *
+ * Stack trace management functions
+ *
+ *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
+ */
+#include <linux/sched.h>
+#include <linux/stacktrace.h>
+#include <linux/module.h>
+#include <asm/stacktrace.h>
+
+static void save_stack_warning(void *data, char *msg)
+{
+}
+
+static void
+save_stack_warning_symbol(void *data, char *msg, unsigned long symbol)
+{
+}
+
+static int save_stack_stack(void *data, char *name)
+{
+	return -1;
+}
+
+static void save_stack_address(void *data, unsigned long addr)
+{
+	struct stack_trace *trace = (struct stack_trace *)data;
+	if (trace->skip > 0) {
+		trace->skip--;
+		return;
+	}
+	if (trace->nr_entries < trace->max_entries)
+		trace->entries[trace->nr_entries++] = addr;
+}
+
+static struct stacktrace_ops save_stack_ops = {
+	.warning = save_stack_warning,
+	.warning_symbol = save_stack_warning_symbol,
+	.stack = save_stack_stack,
+	.address = save_stack_address,
+};
+
+/*
+ * Save stack-backtrace addresses into a stack_trace buffer.
+ */
+void save_stack_trace(struct stack_trace *trace)
+{
+	dump_trace(current, NULL, NULL, &save_stack_ops, trace);
+	if (trace->nr_entries < trace->max_entries)
+		trace->entries[trace->nr_entries++] = ULONG_MAX;
+}
+EXPORT_SYMBOL(save_stack_trace);
