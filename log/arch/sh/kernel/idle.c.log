commit 5933f6d220403b55772d2caf48a9a39d777fd630
Author: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
Date:   Fri Dec 28 00:32:24 2018 -0800

    sh: kernel: convert to SPDX identifiers
    
    Update license to use SPDX-License-Identifier instead of verbose license
    text.
    
    Link: http://lkml.kernel.org/r/8736rccswn.wl-kuninori.morimoto.gx@renesas.com
    Signed-off-by: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
    Reviewed-by: Simon Horman <horms+renesas@verge.net.au>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index be616ee0cf87..c20fc5487e05 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -1,11 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * The idle loop for all SuperH platforms.
  *
  *  Copyright (C) 2002 - 2009  Paul Mundt
- *
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
  */
 #include <linux/module.h>
 #include <linux/init.h>

commit f0c5cdb5cf89e56bdef9d71da89d4966dea6ef71
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Jan 29 12:45:11 2014 -0500

    sched/idle, SH: Remove redundant cpuidle_idle_call()
    
    The core idle loop now takes care of it.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-sh@vger.kernel.org
    Cc: linux-pm@vger.kernel.org
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: linaro-kernel@lists.linaro.org
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-v2c3oswyd80xk2vh7fwmu6r8@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 2ea4483fd722..be616ee0cf87 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -16,7 +16,6 @@
 #include <linux/thread_info.h>
 #include <linux/irqflags.h>
 #include <linux/smp.h>
-#include <linux/cpuidle.h>
 #include <linux/atomic.h>
 #include <asm/pgalloc.h>
 #include <asm/smp.h>
@@ -40,8 +39,7 @@ void arch_cpu_idle_dead(void)
 
 void arch_cpu_idle(void)
 {
-	if (cpuidle_idle_call())
-		sh_idle();
+	sh_idle();
 }
 
 void __init select_idle_routine(void)

commit dc775dd886618a1ea6f092bfb3ddc78660aa1a19
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 22:49:59 2013 +0100

    sh: Use generic idle loop
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Link: http://lkml.kernel.org/r/20130321215235.216323644@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 3d5a1b387cc0..2ea4483fd722 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -24,98 +24,24 @@
 
 static void (*sh_idle)(void);
 
-static int hlt_counter;
-
-static int __init nohlt_setup(char *__unused)
-{
-	hlt_counter = 1;
-	return 1;
-}
-__setup("nohlt", nohlt_setup);
-
-static int __init hlt_setup(char *__unused)
-{
-	hlt_counter = 0;
-	return 1;
-}
-__setup("hlt", hlt_setup);
-
-static inline int hlt_works(void)
-{
-	return !hlt_counter;
-}
-
-/*
- * On SMP it's slightly faster (but much more power-consuming!)
- * to poll the ->work.need_resched flag instead of waiting for the
- * cross-CPU IPI to arrive. Use this option with caution.
- */
-static void poll_idle(void)
+void default_idle(void)
 {
+	set_bl_bit();
 	local_irq_enable();
-	while (!need_resched())
-		cpu_relax();
+	/* Isn't this racy ? */
+	cpu_sleep();
+	clear_bl_bit();
 }
 
-void default_idle(void)
+void arch_cpu_idle_dead(void)
 {
-	if (hlt_works()) {
-		clear_thread_flag(TIF_POLLING_NRFLAG);
-		smp_mb__after_clear_bit();
-
-		set_bl_bit();
-		if (!need_resched()) {
-			local_irq_enable();
-			cpu_sleep();
-		} else
-			local_irq_enable();
-
-		set_thread_flag(TIF_POLLING_NRFLAG);
-		clear_bl_bit();
-	} else
-		poll_idle();
+	play_dead();
 }
 
-/*
- * The idle thread. There's no useful work to be done, so just try to conserve
- * power and have a low exit latency (ie sit in a loop waiting for somebody to
- * say that they'd like to reschedule)
- */
-void cpu_idle(void)
+void arch_cpu_idle(void)
 {
-	unsigned int cpu = smp_processor_id();
-
-	set_thread_flag(TIF_POLLING_NRFLAG);
-
-	/* endless idle loop with no priority at all */
-	while (1) {
-		tick_nohz_idle_enter();
-		rcu_idle_enter();
-
-		while (!need_resched()) {
-			check_pgt_cache();
-			rmb();
-
-			if (cpu_is_offline(cpu))
-				play_dead();
-
-			local_irq_disable();
-			/* Don't trace irqs off for idle */
-			stop_critical_timings();
-			if (cpuidle_idle_call())
-				sh_idle();
-			/*
-			 * Sanity check to ensure that sh_idle() returns
-			 * with IRQs enabled
-			 */
-			WARN_ON(irqs_disabled());
-			start_critical_timings();
-		}
-
-		rcu_idle_exit();
-		tick_nohz_idle_exit();
-		schedule_preempt_disabled();
-	}
+	if (cpuidle_idle_call())
+		sh_idle();
 }
 
 void __init select_idle_routine(void)
@@ -123,13 +49,8 @@ void __init select_idle_routine(void)
 	/*
 	 * If a platform has set its own idle routine, leave it alone.
 	 */
-	if (sh_idle)
-		return;
-
-	if (hlt_works())
+	if (!sh_idle)
 		sh_idle = default_idle;
-	else
-		sh_idle = poll_idle;
 }
 
 void stop_this_cpu(void *unused)

commit 3738fa5bbc3acaf6a807cb45f4367cbab412faee
Author: Len Brown <len.brown@intel.com>
Date:   Sat Feb 9 22:52:57 2013 -0500

    sh idle: rename global pm_idle to static sh_idle
    
    SH idle code could use some simplification.
    This patch enables that by guaranteeing
    that "sh_idle" is local, and thus architecture specific.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: linux-sh@vger.kernel.org

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 0c910163caa3..3d5a1b387cc0 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -22,7 +22,7 @@
 #include <asm/smp.h>
 #include <asm/bl_bit.h>
 
-void (*pm_idle)(void);
+static void (*sh_idle)(void);
 
 static int hlt_counter;
 
@@ -103,9 +103,9 @@ void cpu_idle(void)
 			/* Don't trace irqs off for idle */
 			stop_critical_timings();
 			if (cpuidle_idle_call())
-				pm_idle();
+				sh_idle();
 			/*
-			 * Sanity check to ensure that pm_idle() returns
+			 * Sanity check to ensure that sh_idle() returns
 			 * with IRQs enabled
 			 */
 			WARN_ON(irqs_disabled());
@@ -123,13 +123,13 @@ void __init select_idle_routine(void)
 	/*
 	 * If a platform has set its own idle routine, leave it alone.
 	 */
-	if (pm_idle)
+	if (sh_idle)
 		return;
 
 	if (hlt_works())
-		pm_idle = default_idle;
+		sh_idle = default_idle;
 	else
-		pm_idle = poll_idle;
+		sh_idle = poll_idle;
 }
 
 void stop_this_cpu(void *unused)

commit 86627c93b35082f7a0e4d3111546943984b932c7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 7 17:59:51 2012 +0000

    sh: Remove cpu_idle_wait()
    
    cpuidle uses generic kick_all_cpus_sync() now. Remove the unused code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Link: http://lkml.kernel.org/r/20120507175652.461648208@linutronix.de

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index ee226e20c20c..0c910163caa3 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -132,10 +132,6 @@ void __init select_idle_routine(void)
 		pm_idle = poll_idle;
 }
 
-static void do_nothing(void *unused)
-{
-}
-
 void stop_this_cpu(void *unused)
 {
 	local_irq_disable();
@@ -144,19 +140,3 @@ void stop_this_cpu(void *unused)
 	for (;;)
 		cpu_sleep();
 }
-
-/*
- * cpu_idle_wait - Used to ensure that all the CPUs discard old value of
- * pm_idle and update to new pm_idle value. Required while changing pm_idle
- * handler on SMP systems.
- *
- * Caller must have changed pm_idle to the new value before the call. Old
- * pm_idle value will not be used by any CPU after the return of this function.
- */
-void cpu_idle_wait(void)
-{
-	smp_mb();
-	/* kick all the CPUs so that they exit out of pm_idle */
-	smp_call_function(do_nothing, NULL, 1);
-}
-EXPORT_SYMBOL_GPL(cpu_idle_wait);

commit f03c4866d31e913a8dbc84f7d1459abdaf0bd326
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Mar 30 19:29:57 2012 +0900

    sh: fix up fallout from system.h disintegration.
    
    Quite a bit of fallout all over the place, nothing terribly exciting.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 64852ecc6881..ee226e20c20c 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -17,8 +17,8 @@
 #include <linux/irqflags.h>
 #include <linux/smp.h>
 #include <linux/cpuidle.h>
-#include <asm/pgalloc.h>
 #include <linux/atomic.h>
+#include <asm/pgalloc.h>
 #include <asm/smp.h>
 #include <asm/bl_bit.h>
 

commit e839ca528718e68cad32a307dc9aabf01ef3eb05
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:03 2012 +0100

    Disintegrate asm/system.h for SH
    
    Disintegrate asm/system.h for SH.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: linux-sh@vger.kernel.org

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 7e4892826563..64852ecc6881 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -18,9 +18,9 @@
 #include <linux/smp.h>
 #include <linux/cpuidle.h>
 #include <asm/pgalloc.h>
-#include <asm/system.h>
 #include <linux/atomic.h>
 #include <asm/smp.h>
+#include <asm/bl_bit.h>
 
 void (*pm_idle)(void);
 

commit bd2f55361f18347e890d52ff9cfd8895455ec11b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 21 12:33:18 2011 +0100

    sched/rt: Use schedule_preempt_disabled()
    
    Coccinelle based conversion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-24swm5zut3h9c4a6s46x8rws@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 406508d4ce74..7e4892826563 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -114,9 +114,7 @@ void cpu_idle(void)
 
 		rcu_idle_exit();
 		tick_nohz_idle_exit();
-		preempt_enable_no_resched();
-		schedule();
-		preempt_disable();
+		schedule_preempt_disabled();
 	}
 }
 

commit 1268fbc746ea1cd279886a740dcbad4ba5232225
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Nov 17 18:48:14 2011 +0100

    nohz: Remove tick_nohz_idle_enter_norcu() / tick_nohz_idle_exit_norcu()
    
    Those two APIs were provided to optimize the calls of
    tick_nohz_idle_enter() and rcu_idle_enter() into a single
    irq disabled section. This way no interrupt happening in-between would
    needlessly process any RCU job.
    
    Now we are talking about an optimization for which benefits
    have yet to be measured. Let's start simple and completely decouple
    idle rcu and dyntick idle logics to simplify.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index ad58e7535a7c..406508d4ce74 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -89,7 +89,8 @@ void cpu_idle(void)
 
 	/* endless idle loop with no priority at all */
 	while (1) {
-		tick_nohz_idle_enter_norcu();
+		tick_nohz_idle_enter();
+		rcu_idle_enter();
 
 		while (!need_resched()) {
 			check_pgt_cache();
@@ -111,7 +112,8 @@ void cpu_idle(void)
 			start_critical_timings();
 		}
 
-		tick_nohz_idle_exit_norcu();
+		rcu_idle_exit();
+		tick_nohz_idle_exit();
 		preempt_enable_no_resched();
 		schedule();
 		preempt_disable();

commit 2bbb6817c0ac1b5f2a68d720f364f98eeb1ac4fd
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Oct 8 16:01:00 2011 +0200

    nohz: Allow rcu extended quiescent state handling seperately from tick stop
    
    It is assumed that rcu won't be used once we switch to tickless
    mode and until we restart the tick. However this is not always
    true, as in x86-64 where we dereference the idle notifiers after
    the tick is stopped.
    
    To prepare for fixing this, add two new APIs:
    tick_nohz_idle_enter_norcu() and tick_nohz_idle_exit_norcu().
    
    If no use of RCU is made in the idle loop between
    tick_nohz_enter_idle() and tick_nohz_exit_idle() calls, the arch
    must instead call the new *_norcu() version such that the arch doesn't
    need to call rcu_idle_enter() and rcu_idle_exit().
    
    Otherwise the arch must call tick_nohz_enter_idle() and
    tick_nohz_exit_idle() and also call explicitly:
    
    - rcu_idle_enter() after its last use of RCU before the CPU is put
    to sleep.
    - rcu_idle_exit() before the first use of RCU after the CPU is woken
    up.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Hans-Christian Egtvedt <hans-christian.egtvedt@atmel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 6015743020a0..ad58e7535a7c 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -89,7 +89,7 @@ void cpu_idle(void)
 
 	/* endless idle loop with no priority at all */
 	while (1) {
-		tick_nohz_idle_enter();
+		tick_nohz_idle_enter_norcu();
 
 		while (!need_resched()) {
 			check_pgt_cache();
@@ -111,7 +111,7 @@ void cpu_idle(void)
 			start_critical_timings();
 		}
 
-		tick_nohz_idle_exit();
+		tick_nohz_idle_exit_norcu();
 		preempt_enable_no_resched();
 		schedule();
 		preempt_disable();

commit 280f06774afedf849f0b34248ed6aff57d0f6908
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 7 18:22:06 2011 +0200

    nohz: Separate out irq exit and idle loop dyntick logic
    
    The tick_nohz_stop_sched_tick() function, which tries to delay
    the next timer tick as long as possible, can be called from two
    places:
    
    - From the idle loop to start the dytick idle mode
    - From interrupt exit if we have interrupted the dyntick
    idle mode, so that we reprogram the next tick event in
    case the irq changed some internal state that requires this
    action.
    
    There are only few minor differences between both that
    are handled by that function, driven by the ts->inidle
    cpu variable and the inidle parameter. The whole guarantees
    that we only update the dyntick mode on irq exit if we actually
    interrupted the dyntick idle mode, and that we enter in RCU extended
    quiescent state from idle loop entry only.
    
    Split this function into:
    
    - tick_nohz_idle_enter(), which sets ts->inidle to 1, enters
    dynticks idle mode unconditionally if it can, and enters into RCU
    extended quiescent state.
    
    - tick_nohz_irq_exit() which only updates the dynticks idle mode
    when ts->inidle is set (ie: if tick_nohz_idle_enter() has been called).
    
    To maintain symmetry, tick_nohz_restart_sched_tick() has been renamed
    into tick_nohz_idle_exit().
    
    This simplifies the code and micro-optimize the irq exit path (no need
    for local_irq_save there). This also prepares for the split between
    dynticks and rcu extended quiescent state logics. We'll need this split to
    further fix illegal uses of RCU in extended quiescent states in the idle
    loop.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Hans-Christian Egtvedt <hans-christian.egtvedt@atmel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index db4ecd731a00..6015743020a0 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -89,7 +89,7 @@ void cpu_idle(void)
 
 	/* endless idle loop with no priority at all */
 	while (1) {
-		tick_nohz_stop_sched_tick(1);
+		tick_nohz_idle_enter();
 
 		while (!need_resched()) {
 			check_pgt_cache();
@@ -111,7 +111,7 @@ void cpu_idle(void)
 			start_critical_timings();
 		}
 
-		tick_nohz_restart_sched_tick();
+		tick_nohz_idle_exit();
 		preempt_enable_no_resched();
 		schedule();
 		preempt_disable();

commit c66d3fcbf306af3c0c4b6f4e0d81467f89c67702
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Aug 8 16:30:11 2011 +0900

    sh: Fix up fallout from cpuidle changes.
    
    Fixes up the pm_idle redefinition that was introduced with the earlier
    cpuidle changes.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 32114e0941ae..db4ecd731a00 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -22,7 +22,7 @@
 #include <linux/atomic.h>
 #include <asm/smp.h>
 
-static void (*pm_idle)(void);
+void (*pm_idle)(void);
 
 static int hlt_counter;
 

commit cbc158d6bfa1990f7869717bb5270867c66068d1
Author: David Brown <davidb@codeaurora.org>
Date:   Thu Aug 4 09:24:31 2011 -0700

    cpuidle: Consistent spelling of cpuidle_idle_call()
    
    Commit a0bfa1373859e9d11dc92561a8667588803e42d8 mispells
    cpuidle_idle_call() on ARM and SH code.  Fix this to be consistent.
    
    Cc: Kevin Hilman <khilman@deeprootsystems.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: x86@kernel.org
    Cc: Len Brown <len.brown@intel.com>
    Signed-off-by: David Brown <davidb@codeaurora.org>
    [ Also done by Mark Brown - th ebug has been around forever, and was
      noticed in -next, but the idle tree never picked it up. Bad bad bad ]
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 3c45de1db716..32114e0941ae 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -101,7 +101,7 @@ void cpu_idle(void)
 			local_irq_disable();
 			/* Don't trace irqs off for idle */
 			stop_critical_timings();
-			if (cpuidle_call_idle())
+			if (cpuidle_idle_call())
 				pm_idle();
 			/*
 			 * Sanity check to ensure that pm_idle() returns

commit 35e51fe82ddcd8fb7f129d6dd8491c097d388665
Merge: c0c770e610cc a0bfa1373859
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 3 21:54:15 2011 -1000

    Merge branch 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6
    
    * 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6:
      cpuidle: stop depending on pm_idle
      x86 idle: move mwait_idle_with_hints() to where it is used
      cpuidle: replace xen access to x86 pm_idle and default_idle
      cpuidle: create bootparam "cpuidle.off=1"
      mrst_pmu: driver for Intel Moorestown Power Management Unit

commit a0bfa1373859e9d11dc92561a8667588803e42d8
Author: Len Brown <len.brown@intel.com>
Date:   Fri Apr 1 19:34:59 2011 -0400

    cpuidle: stop depending on pm_idle
    
    cpuidle users should call cpuidle_call_idle() directly
    rather than via (pm_idle)() function pointer.
    
    Architecture may choose to continue using (pm_idle)(),
    but cpuidle need not depend on it:
    
      my_arch_cpu_idle()
            ...
            if(cpuidle_call_idle())
                    pm_idle();
    
    cc: Kevin Hilman <khilman@deeprootsystems.com>
    cc: Paul Mundt <lethal@linux-sh.org>
    cc: x86@kernel.org
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 425d604e3a28..9c7099ebfe14 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -16,12 +16,13 @@
 #include <linux/thread_info.h>
 #include <linux/irqflags.h>
 #include <linux/smp.h>
+#include <linux/cpuidle.h>
 #include <asm/pgalloc.h>
 #include <asm/system.h>
 #include <asm/atomic.h>
 #include <asm/smp.h>
 
-void (*pm_idle)(void) = NULL;
+static void (*pm_idle)(void);
 
 static int hlt_counter;
 
@@ -100,7 +101,8 @@ void cpu_idle(void)
 			local_irq_disable();
 			/* Don't trace irqs off for idle */
 			stop_critical_timings();
-			pm_idle();
+			if (cpuidle_call_idle())
+				pm_idle();
 			/*
 			 * Sanity check to ensure that pm_idle() returns
 			 * with IRQs enabled

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 425d604e3a28..84db0d6ccd0d 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -18,7 +18,7 @@
 #include <linux/smp.h>
 #include <asm/pgalloc.h>
 #include <asm/system.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #include <asm/smp.h>
 
 void (*pm_idle)(void) = NULL;

commit 763142d1efb56effe614d71185781796c4b83c78
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Apr 26 19:08:55 2010 +0900

    sh: CPU hotplug support.
    
    This adds preliminary support for CPU hotplug for SH SMP systems.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 204005329fe1..425d604e3a28 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -19,6 +19,7 @@
 #include <asm/pgalloc.h>
 #include <asm/system.h>
 #include <asm/atomic.h>
+#include <asm/smp.h>
 
 void (*pm_idle)(void) = NULL;
 
@@ -89,10 +90,13 @@ void cpu_idle(void)
 	while (1) {
 		tick_nohz_stop_sched_tick(1);
 
-		while (!need_resched() && cpu_online(cpu)) {
+		while (!need_resched()) {
 			check_pgt_cache();
 			rmb();
 
+			if (cpu_is_offline(cpu))
+				play_dead();
+
 			local_irq_disable();
 			/* Don't trace irqs off for idle */
 			stop_critical_timings();

commit f0ccf2770f523bd4fc436886a38e499f9ec95c0e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Apr 26 18:39:50 2010 +0900

    sh: convert online CPU map twiddling to cpumask.
    
    This converts from cpu_set() for the online map to set_cpu_online().
    The two online map modifiers were the last remaining manual map
    manipulation bits, with this in place everything now goes through
    cpumask.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 273f890b17ae..204005329fe1 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -133,7 +133,7 @@ static void do_nothing(void *unused)
 void stop_this_cpu(void *unused)
 {
 	local_irq_disable();
-	cpu_clear(smp_processor_id(), cpu_online_map);
+	set_cpu_online(smp_processor_id(), false);
 
 	for (;;)
 		cpu_sleep();

commit 90851c40769791a6ddeef691c482ecf69bae4a5c
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Mar 23 17:06:47 2010 +0900

    sh: Tidy up a couple of section mismatches.
    
    select_idle_routine() and register_sh_pmu() both needed their annotations
    fixed up to silence section mismatch warnings.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 0fd7b41f0a22..273f890b17ae 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -112,7 +112,7 @@ void cpu_idle(void)
 	}
 }
 
-void __cpuinit select_idle_routine(void)
+void __init select_idle_routine(void)
 {
 	/*
 	 * If a platform has set its own idle routine, leave it alone.

commit fbb82b03653cdb7fd1863b911e7540011259d2ce
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jan 20 16:42:52 2010 +0900

    sh: machine_ops based reboot support.
    
    This provides a machine_ops-based reboot interface loosely cloned from
    x86, and converts the native sh32 and sh64 cases over to it.
    
    Necessary both for tying in SMP support and also enabling platforms like
    SDK7786 to add support for their microcontroller-based power managers.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 6b3d706deac1..0fd7b41f0a22 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -20,10 +20,9 @@
 #include <asm/system.h>
 #include <asm/atomic.h>
 
-static int hlt_counter;
 void (*pm_idle)(void) = NULL;
-void (*pm_power_off)(void);
-EXPORT_SYMBOL(pm_power_off);
+
+static int hlt_counter;
 
 static int __init nohlt_setup(char *__unused)
 {
@@ -131,6 +130,15 @@ static void do_nothing(void *unused)
 {
 }
 
+void stop_this_cpu(void *unused)
+{
+	local_irq_disable();
+	cpu_clear(smp_processor_id(), cpu_online_map);
+
+	for (;;)
+		cpu_sleep();
+}
+
 /*
  * cpu_idle_wait - Used to ensure that all the CPUs discard old value of
  * pm_idle and update to new pm_idle value. Required while changing pm_idle

commit 73a38b839b9295216e8d44dabf54de88270e77b8
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Dec 18 14:40:56 2009 +0900

    sh: Only use bl bit toggling for sleeping idle.
    
    We don't actually require this in the cpu_relax() polling case, so just
    cuddle these around the sleeping version.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 8e47565dcfd1..6b3d706deac1 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -62,6 +62,7 @@ void default_idle(void)
 		clear_thread_flag(TIF_POLLING_NRFLAG);
 		smp_mb__after_clear_bit();
 
+		set_bl_bit();
 		if (!need_resched()) {
 			local_irq_enable();
 			cpu_sleep();
@@ -69,6 +70,7 @@ void default_idle(void)
 			local_irq_enable();
 
 		set_thread_flag(TIF_POLLING_NRFLAG);
+		clear_bl_bit();
 	} else
 		poll_idle();
 }
@@ -92,7 +94,6 @@ void cpu_idle(void)
 			check_pgt_cache();
 			rmb();
 
-			set_bl_bit();
 			local_irq_disable();
 			/* Don't trace irqs off for idle */
 			stop_critical_timings();
@@ -103,7 +104,6 @@ void cpu_idle(void)
 			 */
 			WARN_ON(irqs_disabled());
 			start_critical_timings();
-			clear_bl_bit();
 		}
 
 		tick_nohz_restart_sched_tick();

commit 3147093e1de59081e82fb1d815424c3e952caf3e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Dec 18 14:19:27 2009 +0900

    sh: Restore bl bit toggling in idle loop.
    
    This fixes up some crashes with IRQs racing the need_resched() test under
    QEMU.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index aaff0037fcd7..8e47565dcfd1 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -92,6 +92,7 @@ void cpu_idle(void)
 			check_pgt_cache();
 			rmb();
 
+			set_bl_bit();
 			local_irq_disable();
 			/* Don't trace irqs off for idle */
 			stop_critical_timings();
@@ -102,6 +103,7 @@ void cpu_idle(void)
 			 */
 			WARN_ON(irqs_disabled());
 			start_critical_timings();
+			clear_bl_bit();
 		}
 
 		tick_nohz_restart_sched_tick();

commit 9dbe00a56a60748668d2040cf4e59427060e2252
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Oct 16 17:55:59 2009 +0900

    sh: Fix up IRQ re-enabling for the need_resched() case.
    
    In the case where need_resched() is set in between the cpu_idle() and
    pm_idle() calls we were missing an else case for just re-enabling local
    IRQs and bailing out. This was noticed by the irqs_disabled() warning,
    even though IRQs were being re-enabled elsewhere.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 3243eb23e842..aaff0037fcd7 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -65,7 +65,8 @@ void default_idle(void)
 		if (!need_resched()) {
 			local_irq_enable();
 			cpu_sleep();
-		}
+		} else
+			local_irq_enable();
 
 		set_thread_flag(TIF_POLLING_NRFLAG);
 	} else

commit 0e6d4986e7940125a04ba8c3aa558f3b248cb9b4
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Oct 16 17:27:58 2009 +0900

    sh: Make check_pgt_cache() more aggressive while idling.
    
    This follows the x86 change and moves check_pgt_cache() up under the
    !need_resched() tight loop, rather than simply calling in to it when
    exiting idle.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 8e61241230cb..3243eb23e842 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -88,6 +88,9 @@ void cpu_idle(void)
 		tick_nohz_stop_sched_tick(1);
 
 		while (!need_resched() && cpu_online(cpu)) {
+			check_pgt_cache();
+			rmb();
+
 			local_irq_disable();
 			/* Don't trace irqs off for idle */
 			stop_critical_timings();
@@ -104,7 +107,6 @@ void cpu_idle(void)
 		preempt_enable_no_resched();
 		schedule();
 		preempt_disable();
-		check_pgt_cache();
 	}
 }
 

commit f533c3d340536198a4889a42a68d6c0d79a504e7
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Oct 16 17:20:58 2009 +0900

    sh: Idle loop chainsawing for SMP-based light sleep.
    
    This does a bit of chainsawing of the idle loop code to get light sleep
    working on SMP. Previously this was forcing secondary CPUs in to sleep
    mode with them not coming back if they didn't have their own local
    timers. Given that we use clockevents broadcasting by default, the CPU
    managing the clockevents can't have IRQs disabled before entering its
    sleep state.
    
    This unfortunately leaves us with the age-old need_resched() race in
    between local_irq_enable() and cpu_sleep(), but at present this is
    unavoidable. After some more experimentation it may be possible to layer
    on SR.BL bit manipulation over top of this scheme to inhibit the race
    condition, but given the current potential for missing wakeups, this is
    left as a future exercise.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index 27ff2dc093c7..8e61241230cb 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -21,7 +21,7 @@
 #include <asm/atomic.h>
 
 static int hlt_counter;
-void (*pm_idle)(void);
+void (*pm_idle)(void) = NULL;
 void (*pm_power_off)(void);
 EXPORT_SYMBOL(pm_power_off);
 
@@ -39,41 +39,68 @@ static int __init hlt_setup(char *__unused)
 }
 __setup("hlt", hlt_setup);
 
+static inline int hlt_works(void)
+{
+	return !hlt_counter;
+}
+
+/*
+ * On SMP it's slightly faster (but much more power-consuming!)
+ * to poll the ->work.need_resched flag instead of waiting for the
+ * cross-CPU IPI to arrive. Use this option with caution.
+ */
+static void poll_idle(void)
+{
+	local_irq_enable();
+	while (!need_resched())
+		cpu_relax();
+}
+
 void default_idle(void)
 {
-	if (!hlt_counter) {
+	if (hlt_works()) {
 		clear_thread_flag(TIF_POLLING_NRFLAG);
 		smp_mb__after_clear_bit();
-		set_bl_bit();
-		stop_critical_timings();
 
-		while (!need_resched())
+		if (!need_resched()) {
+			local_irq_enable();
 			cpu_sleep();
+		}
 
-		start_critical_timings();
-		clear_bl_bit();
 		set_thread_flag(TIF_POLLING_NRFLAG);
 	} else
-		while (!need_resched())
-			cpu_relax();
+		poll_idle();
 }
 
+/*
+ * The idle thread. There's no useful work to be done, so just try to conserve
+ * power and have a low exit latency (ie sit in a loop waiting for somebody to
+ * say that they'd like to reschedule)
+ */
 void cpu_idle(void)
 {
+	unsigned int cpu = smp_processor_id();
+
 	set_thread_flag(TIF_POLLING_NRFLAG);
 
 	/* endless idle loop with no priority at all */
 	while (1) {
-		void (*idle)(void) = pm_idle;
+		tick_nohz_stop_sched_tick(1);
 
-		if (!idle)
-			idle = default_idle;
+		while (!need_resched() && cpu_online(cpu)) {
+			local_irq_disable();
+			/* Don't trace irqs off for idle */
+			stop_critical_timings();
+			pm_idle();
+			/*
+			 * Sanity check to ensure that pm_idle() returns
+			 * with IRQs enabled
+			 */
+			WARN_ON(irqs_disabled());
+			start_critical_timings();
+		}
 
-		tick_nohz_stop_sched_tick(1);
-		while (!need_resched())
-			idle();
 		tick_nohz_restart_sched_tick();
-
 		preempt_enable_no_resched();
 		schedule();
 		preempt_disable();
@@ -81,6 +108,20 @@ void cpu_idle(void)
 	}
 }
 
+void __cpuinit select_idle_routine(void)
+{
+	/*
+	 * If a platform has set its own idle routine, leave it alone.
+	 */
+	if (pm_idle)
+		return;
+
+	if (hlt_works())
+		pm_idle = default_idle;
+	else
+		pm_idle = poll_idle;
+}
+
 static void do_nothing(void *unused)
 {
 }

commit 2e046b9487dcc60707cac77fb8f744ec830209cd
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Jun 19 14:40:51 2009 +0900

    sh: Provide cpu_idle_wait() to fix up cpuidle/SMP build.
    
    Crib the x86 cpu_idle_wait() implementation and shove it in with the
    idle code, subsequently enabling ARCH_HAS_CPU_IDLE_WAIT.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index f35ed0348850..27ff2dc093c7 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -1,7 +1,7 @@
 /*
  * The idle loop for all SuperH platforms.
  *
- *  Copyright (C) 2002 - 2008  Paul Mundt
+ *  Copyright (C) 2002 - 2009  Paul Mundt
  *
  * This file is subject to the terms and conditions of the GNU General Public
  * License.  See the file "COPYING" in the main directory of this archive
@@ -15,6 +15,7 @@
 #include <linux/preempt.h>
 #include <linux/thread_info.h>
 #include <linux/irqflags.h>
+#include <linux/smp.h>
 #include <asm/pgalloc.h>
 #include <asm/system.h>
 #include <asm/atomic.h>
@@ -79,3 +80,23 @@ void cpu_idle(void)
 		check_pgt_cache();
 	}
 }
+
+static void do_nothing(void *unused)
+{
+}
+
+/*
+ * cpu_idle_wait - Used to ensure that all the CPUs discard old value of
+ * pm_idle and update to new pm_idle value. Required while changing pm_idle
+ * handler on SMP systems.
+ *
+ * Caller must have changed pm_idle to the new value before the call. Old
+ * pm_idle value will not be used by any CPU after the return of this function.
+ */
+void cpu_idle_wait(void)
+{
+	smp_mb();
+	/* kick all the CPUs so that they exit out of pm_idle */
+	smp_call_function(do_nothing, NULL, 1);
+}
+EXPORT_SYMBOL_GPL(cpu_idle_wait);

commit e869a90ee1235a4f89ecb956e7b7d724d65217c8
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Apr 2 13:08:31 2009 +0900

    sh: Wire up ARCH_HAS_DEFAULT_IDLE for cpuidle.
    
    cpuidle wants ARCH_HAS_DEFAULT_IDLE defined in order to use the
    default idle loop. So, make it accessible and enable it for all
    sh machines.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
index fe59ccfc1152..f35ed0348850 100644
--- a/arch/sh/kernel/idle.c
+++ b/arch/sh/kernel/idle.c
@@ -38,7 +38,7 @@ static int __init hlt_setup(char *__unused)
 }
 __setup("hlt", hlt_setup);
 
-static void default_idle(void)
+void default_idle(void)
 {
 	if (!hlt_counter) {
 		clear_thread_flag(TIF_POLLING_NRFLAG);

commit 1da1180c6e28cf21be356e2701978727558fa198
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Nov 26 15:52:44 2008 +0900

    sh: Split out the idle loop for reuse between _32/_64 variants.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/idle.c b/arch/sh/kernel/idle.c
new file mode 100644
index 000000000000..fe59ccfc1152
--- /dev/null
+++ b/arch/sh/kernel/idle.c
@@ -0,0 +1,81 @@
+/*
+ * The idle loop for all SuperH platforms.
+ *
+ *  Copyright (C) 2002 - 2008  Paul Mundt
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ */
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/mm.h>
+#include <linux/pm.h>
+#include <linux/tick.h>
+#include <linux/preempt.h>
+#include <linux/thread_info.h>
+#include <linux/irqflags.h>
+#include <asm/pgalloc.h>
+#include <asm/system.h>
+#include <asm/atomic.h>
+
+static int hlt_counter;
+void (*pm_idle)(void);
+void (*pm_power_off)(void);
+EXPORT_SYMBOL(pm_power_off);
+
+static int __init nohlt_setup(char *__unused)
+{
+	hlt_counter = 1;
+	return 1;
+}
+__setup("nohlt", nohlt_setup);
+
+static int __init hlt_setup(char *__unused)
+{
+	hlt_counter = 0;
+	return 1;
+}
+__setup("hlt", hlt_setup);
+
+static void default_idle(void)
+{
+	if (!hlt_counter) {
+		clear_thread_flag(TIF_POLLING_NRFLAG);
+		smp_mb__after_clear_bit();
+		set_bl_bit();
+		stop_critical_timings();
+
+		while (!need_resched())
+			cpu_sleep();
+
+		start_critical_timings();
+		clear_bl_bit();
+		set_thread_flag(TIF_POLLING_NRFLAG);
+	} else
+		while (!need_resched())
+			cpu_relax();
+}
+
+void cpu_idle(void)
+{
+	set_thread_flag(TIF_POLLING_NRFLAG);
+
+	/* endless idle loop with no priority at all */
+	while (1) {
+		void (*idle)(void) = pm_idle;
+
+		if (!idle)
+			idle = default_idle;
+
+		tick_nohz_stop_sched_tick(1);
+		while (!need_resched())
+			idle();
+		tick_nohz_restart_sched_tick();
+
+		preempt_enable_no_resched();
+		schedule();
+		preempt_disable();
+		check_pgt_cache();
+	}
+}
