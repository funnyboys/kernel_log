commit 4926627793c0a7e7db2bc674e1d06777e86d8dab
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:50:45 2020 -0700

    mm: remove __get_vm_area
    
    Switch the two remaining callers to use __get_vm_area_caller instead.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200414131348.444715-9-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 934ff84844fa..d432164b23b7 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -103,7 +103,8 @@ static int __sq_remap(struct sq_mapping *map, pgprot_t prot)
 #if defined(CONFIG_MMU)
 	struct vm_struct *vma;
 
-	vma = __get_vm_area(map->size, VM_ALLOC, map->sq_addr, SQ_ADDRMAX);
+	vma = __get_vm_area_caller(map->size, VM_ALLOC, map->sq_addr,
+			SQ_ADDRMAX, __builtin_return_address(0));
 	if (!vma)
 		return -ENOMEM;
 

commit 6ecc0a4dbb076ef658cb5e140a7560d6326c8240
Author: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
Date:   Fri Dec 28 00:32:07 2018 -0800

    sh: sh4: convert to SPDX identifiers
    
    Update license to use SPDX-License-Identifier instead of verbose license
    text, excepting ${LINUX}/arch/sh/kernel/cpu/sh4/softfloat.c which is not
    GPL license
    
    Link: http://lkml.kernel.org/r/87a7lkcsya.wl-kuninori.morimoto.gx@renesas.com
    Signed-off-by: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
    Reviewed-by: Simon Horman <horms+renesas@verge.net.au>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 4ca78ed71ad2..934ff84844fa 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * arch/sh/kernel/cpu/sh4/sq.c
  *
@@ -5,10 +6,6 @@
  *
  * Copyright (C) 2001 - 2006  Paul Mundt
  * Copyright (C) 2001, 2002  M. R. Brown
- *
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
  */
 #include <linux/init.h>
 #include <linux/cpu.h>

commit 71db87ba570038497db1227b7dc61113c4156565
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Thu Jul 30 15:04:01 2015 +0530

    bus: subsys: update return type of ->remove_dev() to void
    
    Its return value is not used by the subsys core and nothing meaningful
    can be done with it, even if we want to use it. The subsys device is
    anyway getting removed.
    
    Update prototype of ->remove_dev() to make its return type as void. Fix
    all usage sites as well.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 0a47bd3e7bee..4ca78ed71ad2 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -355,13 +355,12 @@ static int sq_dev_add(struct device *dev, struct subsys_interface *sif)
 	return error;
 }
 
-static int sq_dev_remove(struct device *dev, struct subsys_interface *sif)
+static void sq_dev_remove(struct device *dev, struct subsys_interface *sif)
 {
 	unsigned int cpu = dev->id;
 	struct kobject *kobj = sq_kobject[cpu];
 
 	kobject_put(kobj);
-	return 0;
 }
 
 static struct subsys_interface sq_interface = {

commit 33dc5c1000c193084a70ffd8f3bd9c67d19f9159
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jan 10 16:24:11 2012 +0900

    sh: Fix up store queue code for subsys_interface changes.
    
    Fixes up a number of build and section mismatch errors introduced by the
    subsys_interface conversion.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index a8140f0bbf6c..0a47bd3e7bee 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -337,7 +337,7 @@ static struct kobj_type ktype_percpu_entry = {
 	.default_attrs	= sq_sysfs_attrs,
 };
 
-static int __devinit sq_dev_add(struct device *dev)
+static int sq_dev_add(struct device *dev, struct subsys_interface *sif)
 {
 	unsigned int cpu = dev->id;
 	struct kobject *kobj;
@@ -355,7 +355,7 @@ static int __devinit sq_dev_add(struct device *dev)
 	return error;
 }
 
-static int __devexit sq_dev_remove(struct device *dev)
+static int sq_dev_remove(struct device *dev, struct subsys_interface *sif)
 {
 	unsigned int cpu = dev->id;
 	struct kobject *kobj = sq_kobject[cpu];
@@ -365,10 +365,10 @@ static int __devexit sq_dev_remove(struct device *dev)
 }
 
 static struct subsys_interface sq_interface = {
-	.name		= "sq"
+	.name		= "sq",
 	.subsys		= &cpu_subsys,
 	.add_dev	= sq_dev_add,
-	.remove_dev	= __devexit_p(sq_dev_remove),
+	.remove_dev	= sq_dev_remove,
 };
 
 static int __init sq_api_init(void)

commit 8a25a2fd126c621f44f3aeaef80d51f00fc11639
Author: Kay Sievers <kay.sievers@vrfy.org>
Date:   Wed Dec 21 14:29:42 2011 -0800

    cpu: convert 'cpu' and 'machinecheck' sysdev_class to a regular subsystem
    
    This moves the 'cpu sysdev_class' over to a regular 'cpu' subsystem
    and converts the devices to regular devices. The sysdev drivers are
    implemented as subsystem interfaces now.
    
    After all sysdev classes are ported to regular driver core entities, the
    sysdev implementation will be entirely removed from the kernel.
    
    Userspace relies on events and generic sysfs subsystem infrastructure
    from sysdev devices, which are made available with this conversion.
    
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Borislav Petkov <bp@amd64.org>
    Cc: Tigran Aivazian <tigran@aivazian.fsnet.co.uk>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Zhang Rui <rui.zhang@intel.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: "Srivatsa S. Bhat" <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index f0907995b4c9..a8140f0bbf6c 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -13,7 +13,7 @@
 #include <linux/init.h>
 #include <linux/cpu.h>
 #include <linux/bitmap.h>
-#include <linux/sysdev.h>
+#include <linux/device.h>
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/slab.h>
@@ -337,9 +337,9 @@ static struct kobj_type ktype_percpu_entry = {
 	.default_attrs	= sq_sysfs_attrs,
 };
 
-static int __devinit sq_sysdev_add(struct sys_device *sysdev)
+static int __devinit sq_dev_add(struct device *dev)
 {
-	unsigned int cpu = sysdev->id;
+	unsigned int cpu = dev->id;
 	struct kobject *kobj;
 	int error;
 
@@ -348,25 +348,27 @@ static int __devinit sq_sysdev_add(struct sys_device *sysdev)
 		return -ENOMEM;
 
 	kobj = sq_kobject[cpu];
-	error = kobject_init_and_add(kobj, &ktype_percpu_entry, &sysdev->kobj,
+	error = kobject_init_and_add(kobj, &ktype_percpu_entry, &dev->kobj,
 				     "%s", "sq");
 	if (!error)
 		kobject_uevent(kobj, KOBJ_ADD);
 	return error;
 }
 
-static int __devexit sq_sysdev_remove(struct sys_device *sysdev)
+static int __devexit sq_dev_remove(struct device *dev)
 {
-	unsigned int cpu = sysdev->id;
+	unsigned int cpu = dev->id;
 	struct kobject *kobj = sq_kobject[cpu];
 
 	kobject_put(kobj);
 	return 0;
 }
 
-static struct sysdev_driver sq_sysdev_driver = {
-	.add		= sq_sysdev_add,
-	.remove		= __devexit_p(sq_sysdev_remove),
+static struct subsys_interface sq_interface = {
+	.name		= "sq"
+	.subsys		= &cpu_subsys,
+	.add_dev	= sq_dev_add,
+	.remove_dev	= __devexit_p(sq_dev_remove),
 };
 
 static int __init sq_api_init(void)
@@ -386,7 +388,7 @@ static int __init sq_api_init(void)
 	if (unlikely(!sq_bitmap))
 		goto out;
 
-	ret = sysdev_driver_register(&cpu_sysdev_class, &sq_sysdev_driver);
+	ret = subsys_interface_register(&sq_interface);
 	if (unlikely(ret != 0))
 		goto out;
 
@@ -401,7 +403,7 @@ static int __init sq_api_init(void)
 
 static void __exit sq_api_exit(void)
 {
-	sysdev_driver_unregister(&cpu_sysdev_class, &sq_sysdev_driver);
+	subsys_interface_unregister(&sq_interface);
 	kfree(sq_bitmap);
 	kmem_cache_destroy(sq_cache);
 }

commit 268bb0ce3e87872cb9290c322b0d35bce230d88f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 20 12:50:29 2011 -0700

    sanitize <linux/prefetch.h> usage
    
    Commit e66eed651fd1 ("list: remove prefetching from regular list
    iterators") removed the include of prefetch.h from list.h, which
    uncovered several cases that had apparently relied on that rather
    obscure header file dependency.
    
    So this fixes things up a bit, using
    
       grep -L linux/prefetch.h $(git grep -l '[^a-z_]prefetchw*(' -- '*.[ch]')
       grep -L 'prefetchw*(' $(git grep -l 'linux/prefetch.h' -- '*.[ch]')
    
    to guide us in finding files that either need <linux/prefetch.h>
    inclusion, or have it despite not needing it.
    
    There are more of them around (mostly network drivers), but this gets
    many core ones.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 14726eef1ce0..f0907995b4c9 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -20,6 +20,7 @@
 #include <linux/vmalloc.h>
 #include <linux/mm.h>
 #include <linux/io.h>
+#include <linux/prefetch.h>
 #include <asm/page.h>
 #include <asm/cacheflush.h>
 #include <cpu/sq.h>

commit 52cf25d0ab7f78eeecc59ac652ed5090f69b619e
Author: Emese Revfy <re.emese@gmail.com>
Date:   Tue Jan 19 02:58:23 2010 +0100

    Driver core: Constify struct sysfs_ops in struct kobj_type
    
    Constify struct sysfs_ops.
    
    This is part of the ops structure constification
    effort started by Arjan van de Ven et al.
    
    Benefits of this constification:
    
     * prevents modification of data that is shared
       (referenced) by many other structure instances
       at runtime
    
     * detects/prevents accidental (but not intentional)
       modification attempts on archs that enforce
       read-only kernel data at runtime
    
     * potentially better optimized code as the compiler
       can assume that the const data cannot be changed
    
     * the compiler/linker move const data into .rodata
       and therefore exclude them from false sharing
    
    Signed-off-by: Emese Revfy <re.emese@gmail.com>
    Acked-by: David Teigland <teigland@redhat.com>
    Acked-by: Matt Domsch <Matt_Domsch@dell.com>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Acked-by: Hans J. Koch <hjk@linutronix.de>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Acked-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index fc065f9da6e5..14726eef1ce0 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -326,7 +326,7 @@ static struct attribute *sq_sysfs_attrs[] = {
 	NULL,
 };
 
-static struct sysfs_ops sq_sysfs_ops = {
+static const struct sysfs_ops sq_sysfs_ops = {
 	.show	= sq_sysfs_show,
 	.store	= sq_sysfs_store,
 };

commit 7bdda6209f224aa784a036df54b22cb338d2e859
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Feb 17 13:23:00 2010 +0900

    sh: Fix up more 64-bit pgprot truncation on SH-X2 TLB.
    
    Both the store queue API and the PMB remapping take unsigned long for
    their pgprot flags, which cuts off the extended protection bits. In the
    case of the PMB this isn't really a problem since the cache attribute
    bits that we care about are all in the lower 32-bits, but we do it just
    to be safe. The store queue remapping on the other hand depends on the
    extended prot bits for enabling userspace access to the mappings.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 97aea9d69b00..fc065f9da6e5 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -100,7 +100,7 @@ static inline void sq_mapping_list_del(struct sq_mapping *map)
 	spin_unlock_irq(&sq_mapping_lock);
 }
 
-static int __sq_remap(struct sq_mapping *map, unsigned long flags)
+static int __sq_remap(struct sq_mapping *map, pgprot_t prot)
 {
 #if defined(CONFIG_MMU)
 	struct vm_struct *vma;
@@ -113,7 +113,7 @@ static int __sq_remap(struct sq_mapping *map, unsigned long flags)
 
 	if (ioremap_page_range((unsigned long)vma->addr,
 			       (unsigned long)vma->addr + map->size,
-			       vma->phys_addr, __pgprot(flags))) {
+			       vma->phys_addr, prot)) {
 		vunmap(vma->addr);
 		return -EAGAIN;
 	}
@@ -135,14 +135,14 @@ static int __sq_remap(struct sq_mapping *map, unsigned long flags)
  * @phys: Physical address of mapping.
  * @size: Length of mapping.
  * @name: User invoking mapping.
- * @flags: Protection flags.
+ * @prot: Protection bits.
  *
  * Remaps the physical address @phys through the next available store queue
  * address of @size length. @name is logged at boot time as well as through
  * the sysfs interface.
  */
 unsigned long sq_remap(unsigned long phys, unsigned int size,
-		       const char *name, unsigned long flags)
+		       const char *name, pgprot_t prot)
 {
 	struct sq_mapping *map;
 	unsigned long end;
@@ -177,7 +177,7 @@ unsigned long sq_remap(unsigned long phys, unsigned int size,
 
 	map->sq_addr = P4SEG_STORE_QUE + (page << PAGE_SHIFT);
 
-	ret = __sq_remap(map, pgprot_val(PAGE_KERNEL_NOCACHE) | flags);
+	ret = __sq_remap(map, prot);
 	if (unlikely(ret != 0))
 		goto out;
 
@@ -309,8 +309,7 @@ static ssize_t mapping_store(const char *buf, size_t count)
 		return -EIO;
 
 	if (likely(len)) {
-		int ret = sq_remap(base, len, "Userspace",
-				   pgprot_val(PAGE_SHARED));
+		int ret = sq_remap(base, len, "Userspace", PAGE_SHARED);
 		if (ret < 0)
 			return ret;
 	} else

commit 9d56dd3b083a3bec56e9da35ce07baca81030b03
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jan 26 12:58:40 2010 +0900

    sh: Mass ctrl_in/outX to __raw_read/writeX conversion.
    
    The old ctrl in/out routines are non-portable and unsuitable for
    cross-platform use. While drivers/sh has already been sanitized, there
    is still quite a lot of code that is not. This converts the arch/sh/ bits
    over, which permits us to flag the routines as deprecated whilst still
    building with -Werror for the architecture code, and to ensure that
    future users are not added.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 8a8a993f55ea..97aea9d69b00 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -43,9 +43,9 @@ static unsigned long *sq_bitmap;
 
 #define store_queue_barrier()			\
 do {						\
-	(void)ctrl_inl(P4SEG_STORE_QUE);	\
-	ctrl_outl(0, P4SEG_STORE_QUE + 0);	\
-	ctrl_outl(0, P4SEG_STORE_QUE + 8);	\
+	(void)__raw_readl(P4SEG_STORE_QUE);	\
+	__raw_writel(0, P4SEG_STORE_QUE + 0);	\
+	__raw_writel(0, P4SEG_STORE_QUE + 8);	\
 } while (0);
 
 /**
@@ -123,8 +123,8 @@ static int __sq_remap(struct sq_mapping *map, unsigned long flags)
 	 * straightforward, as we can just load up each queue's QACR with
 	 * the physical address appropriately masked.
 	 */
-	ctrl_outl(((map->addr >> 26) << 2) & 0x1c, SQ_QACR0);
-	ctrl_outl(((map->addr >> 26) << 2) & 0x1c, SQ_QACR1);
+	__raw_writel(((map->addr >> 26) << 2) & 0x1c, SQ_QACR0);
+	__raw_writel(((map->addr >> 26) << 2) & 0x1c, SQ_QACR1);
 #endif
 
 	return 0;

commit 6a9545bd95e88d61df942b9087cb59b8c7a6dc56
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Aug 4 12:51:06 2008 +0900

    sh: Fix up broken kerneldoc comments.
    
    These were completely unparseable, so fix them up.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index dcdf959a3d44..8a8a993f55ea 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -199,7 +199,7 @@ EXPORT_SYMBOL(sq_remap);
 
 /**
  * sq_unmap - Unmap a Store Queue allocation
- * @map: Pre-allocated Store Queue mapping.
+ * @vaddr: Pre-allocated Store Queue mapping.
  *
  * Unmaps the store queue allocation @map that was previously created by
  * sq_remap(). Also frees up the pte that was previously inserted into

commit f15cbe6f1a4b4d9df59142fc8e4abb973302cf44
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jul 29 08:09:44 2008 +0900

    sh: migrate to arch/sh/include/
    
    This follows the sparc changes a439fe51a1f8eb087c22dd24d69cebae4a3addac.
    
    Most of the moving about was done with Sam's directions at:
    
    http://marc.info/?l=linux-sh&m=121724823706062&w=2
    
    with subsequent hacking and fixups entirely my fault.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 9561b02ade0e..dcdf959a3d44 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -22,7 +22,7 @@
 #include <linux/io.h>
 #include <asm/page.h>
 #include <asm/cacheflush.h>
-#include <asm/cpu/sq.h>
+#include <cpu/sq.h>
 
 struct sq_mapping;
 

commit 866e6b9e5019e210d96ced31fbae531ed756e486
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Tue Mar 4 15:23:47 2008 -0800

    sh: replace remaining __FUNCTION__ occurrences
    
    __FUNCTION__ is gcc-specific, use __func__
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 8250e017bd4e..9561b02ade0e 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -216,7 +216,7 @@ void sq_unmap(unsigned long vaddr)
 
 	if (unlikely(!map)) {
 		printk("%s: bad store queue address 0x%08lx\n",
-		       __FUNCTION__, vaddr);
+		       __func__, vaddr);
 		return;
 	}
 
@@ -233,7 +233,7 @@ void sq_unmap(unsigned long vaddr)
 		vma = remove_vm_area((void *)(map->sq_addr & PAGE_MASK));
 		if (!vma) {
 			printk(KERN_ERR "%s: bad address 0x%08lx\n",
-			       __FUNCTION__, map->sq_addr);
+			       __func__, map->sq_addr);
 			return;
 		}
 	}

commit 25478445c4a39318acbe08ba8df7945766cbb5b5
Author: Alexey Dobriyan <adobriyan@sw.ru>
Date:   Fri Feb 8 04:20:14 2008 -0800

    Fix container_of() usage
    
    Using "attr" twice is not OK, because it effectively prohibits such
    container_of() on variables not named "attr".
    
    Signed-off-by: Alexey Dobriyan <adobriyan@sw.ru>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 3008c00eea6b..8250e017bd4e 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -263,7 +263,7 @@ struct sq_sysfs_attr {
 	ssize_t (*store)(const char *buf, size_t count);
 };
 
-#define to_sq_sysfs_attr(attr)	container_of(attr, struct sq_sysfs_attr, attr)
+#define to_sq_sysfs_attr(a)	container_of(a, struct sq_sysfs_attr, attr)
 
 static ssize_t sq_sysfs_show(struct kobject *kobj, struct attribute *attr,
 			     char *buf)

commit 38a382ae5dd4f4d04e3046816b0a41836094e538
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Thu Dec 20 08:13:05 2007 -0800

    Kobject: convert arch/* from kobject_unregister() to kobject_put()
    
    There is no need for kobject_unregister() anymore, thanks to Kay's
    kobject cleanup changes, so replace all instances of it with
    kobject_put().
    
    
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 97fd9b9a4820..3008c00eea6b 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -360,7 +360,7 @@ static int __devexit sq_sysdev_remove(struct sys_device *sysdev)
 	unsigned int cpu = sysdev->id;
 	struct kobject *kobj = sq_kobject[cpu];
 
-	kobject_unregister(kobj);
+	kobject_put(kobj);
 	return 0;
 }
 

commit d48b335256cd75fcb1a4abb3ce8136d8d9c931cc
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Mon Dec 17 15:54:39 2007 -0400

    Kobject: change arch/sh/kernel/cpu/sh4/sq.c to use kobject_init_and_add
    
    Stop using kobject_register, as this way we can control the sending of
    the uevent properly, after everything is properly initialized.
    
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index b22a78c807e6..97fd9b9a4820 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -341,17 +341,18 @@ static int __devinit sq_sysdev_add(struct sys_device *sysdev)
 {
 	unsigned int cpu = sysdev->id;
 	struct kobject *kobj;
+	int error;
 
 	sq_kobject[cpu] = kzalloc(sizeof(struct kobject), GFP_KERNEL);
 	if (unlikely(!sq_kobject[cpu]))
 		return -ENOMEM;
 
 	kobj = sq_kobject[cpu];
-	kobj->parent = &sysdev->kobj;
-	kobject_set_name(kobj, "%s", "sq");
-	kobj->ktype = &ktype_percpu_entry;
-
-	return kobject_register(kobj);
+	error = kobject_init_and_add(kobj, &ktype_percpu_entry, &sysdev->kobj,
+				     "%s", "sq");
+	if (!error)
+		kobject_uevent(kobj, KOBJ_ADD);
+	return error;
 }
 
 static int __devexit sq_sysdev_remove(struct sys_device *sysdev)

commit b05d1865b46ea72c66ba082598ba370582bb590e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Aug 21 12:25:09 2007 +0900

    sh: Kill off volatile silliness in sq_flush_range().
    
      CC      arch/sh/kernel/cpu/sh4/sq.o
    arch/sh/kernel/cpu/sh4/sq.c: In function 'sq_flush_range':
    arch/sh/kernel/cpu/sh4/sq.c:65: warning: passing argument 1 of 'prefetch' discards qualifiers from pointer target type
    
    This didn't actually need to be volatile in the first place, so just
    kill off the qualifier entirely.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index c21512c6044e..b22a78c807e6 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -58,11 +58,11 @@ do {						\
  */
 void sq_flush_range(unsigned long start, unsigned int len)
 {
-	volatile unsigned long *sq = (unsigned long *)start;
+	unsigned long *sq = (unsigned long *)start;
 
 	/* Flush the queues */
 	for (len >>= 5; len--; sq += 8)
-		prefetchw((void *)sq);
+		prefetchw(sq);
 
 	/* Wait for completion */
 	store_queue_barrier();

commit b067c50a7f58838d8a53670ea3c07e18d7391900
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 25 15:59:47 2007 +0900

    sh: Silence sq compile warning on sh4 nommu.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index b98d6c3e6f36..c21512c6044e 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -208,7 +208,6 @@ EXPORT_SYMBOL(sq_remap);
 void sq_unmap(unsigned long vaddr)
 {
 	struct sq_mapping **p, *map;
-	struct vm_struct *vma;
 	int page;
 
 	for (p = &sq_mapping_list; (map = *p); p = &map->next)
@@ -225,11 +224,18 @@ void sq_unmap(unsigned long vaddr)
 	bitmap_release_region(sq_bitmap, page, get_order(map->size));
 
 #ifdef CONFIG_MMU
-	vma = remove_vm_area((void *)(map->sq_addr & PAGE_MASK));
-	if (!vma) {
-		printk(KERN_ERR "%s: bad address 0x%08lx\n",
-		       __FUNCTION__, map->sq_addr);
-		return;
+	{
+		/*
+		 * Tear down the VMA in the MMU case.
+		 */
+		struct vm_struct *vma;
+
+		vma = remove_vm_area((void *)(map->sq_addr & PAGE_MASK));
+		if (!vma) {
+			printk(KERN_ERR "%s: bad address 0x%08lx\n",
+			       __FUNCTION__, map->sq_addr);
+			return;
+		}
 	}
 #endif
 

commit 20c2df83d25c6a95affe6157a4c9cac4cf5ffaac
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Jul 20 10:11:58 2007 +0900

    mm: Remove slab destructors from kmem_cache_create().
    
    Slab destructors were no longer supported after Christoph's
    c59def9f222d44bb7e2f0a559f2906191a0862d7 change. They've been
    BUGs for both slab and slub, and slob never supported them
    either.
    
    This rips out support for the dtor pointer from kmem_cache_create()
    completely and fixes up every single callsite in the kernel (there were
    about 224, not including the slab allocator definitions themselves,
    or the documentation references).
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index d7fff752e569..b98d6c3e6f36 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -371,8 +371,7 @@ static int __init sq_api_init(void)
 	printk(KERN_NOTICE "sq: Registering store queue API.\n");
 
 	sq_cache = kmem_cache_create("store_queue_cache",
-				sizeof(struct sq_mapping), 0, 0,
-				NULL, NULL);
+				sizeof(struct sq_mapping), 0, 0, NULL);
 	if (unlikely(!sq_cache))
 		return ret;
 

commit 37bda1da4570c2e9c6dd34e77d2120218e384950
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Dec 9 09:16:12 2006 +0900

    sh: Convert remaining remap_area_pages() users to ioremap_page_range().
    
    A couple of these were missed.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 0c9ea38d2caa..d7fff752e569 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -111,8 +111,9 @@ static int __sq_remap(struct sq_mapping *map, unsigned long flags)
 
 	vma->phys_addr = map->addr;
 
-	if (remap_area_pages((unsigned long)vma->addr, vma->phys_addr,
-			     map->size, flags)) {
+	if (ioremap_page_range((unsigned long)vma->addr,
+			       (unsigned long)vma->addr + map->size,
+			       vma->phys_addr, __pgprot(flags))) {
 		vunmap(vma->addr);
 		return -EAGAIN;
 	}
@@ -176,7 +177,7 @@ unsigned long sq_remap(unsigned long phys, unsigned int size,
 
 	map->sq_addr = P4SEG_STORE_QUE + (page << PAGE_SHIFT);
 
-	ret = __sq_remap(map, flags);
+	ret = __sq_remap(map, pgprot_val(PAGE_KERNEL_NOCACHE) | flags);
 	if (unlikely(ret != 0))
 		goto out;
 

commit e18b890bb0881bbab6f4f1a6cd20d9c60d66b003
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:20 2006 -0800

    [PATCH] slab: remove kmem_cache_t
    
    Replace all uses of kmem_cache_t with struct kmem_cache.
    
    The patch was generated using the following script:
    
            #!/bin/sh
            #
            # Replace one string by another in all the kernel sources.
            #
    
            set -e
    
            for file in `find * -name "*.c" -o -name "*.h"|xargs grep -l $1`; do
                    quilt add $file
                    sed -e "1,\$s/$1/$2/g" $file >/tmp/$$
                    mv /tmp/$$ $file
                    quilt refresh
            done
    
    The script was run like this
    
            sh replace kmem_cache_t "struct kmem_cache"
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 55f43506995a..0c9ea38d2caa 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -38,7 +38,7 @@ struct sq_mapping {
 
 static struct sq_mapping *sq_mapping_list;
 static DEFINE_SPINLOCK(sq_mapping_lock);
-static kmem_cache_t *sq_cache;
+static struct kmem_cache *sq_cache;
 static unsigned long *sq_bitmap;
 
 #define store_queue_barrier()			\

commit 9f650cf2b811cfb605f10483eeb1dc86f43cdbcb
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Dec 1 12:01:43 2006 +0900

    sh: Fix store queue bitmap end.
    
    The end of the store queue bitmap is miscalculated when searching
    for a free range in sq_remap(), missing the PAGE_SHIFT shift that's
    done in sq_api_init(). This runs in to workloads where we can scan
    beyond the end of the bitmap.
    
    Spotted by Paul Jackson:
    
            http://marc.theaimsgroup.com/?l=linux-kernel&m=116493191224097&w
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 7bcc73f9b8df..55f43506995a 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -19,7 +19,7 @@
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
 #include <linux/mm.h>
-#include <asm/io.h>
+#include <linux/io.h>
 #include <asm/page.h>
 #include <asm/cacheflush.h>
 #include <asm/cpu/sq.h>
@@ -67,6 +67,7 @@ void sq_flush_range(unsigned long start, unsigned int len)
 	/* Wait for completion */
 	store_queue_barrier();
 }
+EXPORT_SYMBOL(sq_flush_range);
 
 static inline void sq_mapping_list_add(struct sq_mapping *map)
 {
@@ -166,7 +167,7 @@ unsigned long sq_remap(unsigned long phys, unsigned int size,
 	map->size = size;
 	map->name = name;
 
-	page = bitmap_find_free_region(sq_bitmap, 0x04000000,
+	page = bitmap_find_free_region(sq_bitmap, 0x04000000 >> PAGE_SHIFT,
 				       get_order(map->size));
 	if (unlikely(page < 0)) {
 		ret = -ENOSPC;
@@ -193,6 +194,7 @@ unsigned long sq_remap(unsigned long phys, unsigned int size,
 	kmem_cache_free(sq_cache, map);
 	return ret;
 }
+EXPORT_SYMBOL(sq_remap);
 
 /**
  * sq_unmap - Unmap a Store Queue allocation
@@ -234,6 +236,7 @@ void sq_unmap(unsigned long vaddr)
 
 	kmem_cache_free(sq_cache, map);
 }
+EXPORT_SYMBOL(sq_unmap);
 
 /*
  * Needlessly complex sysfs interface. Unfortunately it doesn't seem like
@@ -402,7 +405,3 @@ module_exit(sq_api_exit);
 MODULE_AUTHOR("Paul Mundt <lethal@linux-sh.org>, M. R. Brown <mrbrown@0xd6.org>");
 MODULE_DESCRIPTION("Simple API for SH-4 integrated Store Queues");
 MODULE_LICENSE("GPL");
-
-EXPORT_SYMBOL(sq_remap);
-EXPORT_SYMBOL(sq_unmap);
-EXPORT_SYMBOL(sq_flush_range);

commit d7c30c682a278abe1a52db83f69efec1a9d8f8c2
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 15:49:57 2006 +0900

    sh: Store Queue API rework.
    
    Rewrite the store queue API for a per-cpu interface in the driver
    model. The old miscdevice is dropped, due to TASK_SIZE limitations,
    and no one was using it anyways.
    
    Carve up and allocate store queue space with a bitmap, back sq
    mapping objects with a slab cache, and let userspace worry about
    its own prefetching.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index b148966dd7c7..7bcc73f9b8df 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -1,50 +1,52 @@
 /*
- * arch/sh/kernel/cpu/sq.c
+ * arch/sh/kernel/cpu/sh4/sq.c
  *
  * General management API for SH-4 integrated Store Queues
  *
- * Copyright (C) 2001, 2002, 2003, 2004  Paul Mundt
+ * Copyright (C) 2001 - 2006  Paul Mundt
  * Copyright (C) 2001, 2002  M. R. Brown
  *
- * Some of this code has been adopted directly from the old arch/sh/mm/sq.c
- * hack that was part of the LinuxDC project. For all intents and purposes,
- * this is a completely new interface that really doesn't have much in common
- * with the old zone-based approach at all. In fact, it's only listed here for
- * general completeness.
- *
  * This file is subject to the terms and conditions of the GNU General Public
  * License.  See the file "COPYING" in the main directory of this archive
  * for more details.
  */
 #include <linux/init.h>
+#include <linux/cpu.h>
+#include <linux/bitmap.h>
+#include <linux/sysdev.h>
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/slab.h>
-#include <linux/list.h>
-#include <linux/proc_fs.h>
-#include <linux/miscdevice.h>
 #include <linux/vmalloc.h>
 #include <linux/mm.h>
 #include <asm/io.h>
 #include <asm/page.h>
 #include <asm/cacheflush.h>
-#include <asm/mmu_context.h>
 #include <asm/cpu/sq.h>
 
-static LIST_HEAD(sq_mapping_list);
+struct sq_mapping;
+
+struct sq_mapping {
+	const char *name;
+
+	unsigned long sq_addr;
+	unsigned long addr;
+	unsigned int size;
+
+	struct sq_mapping *next;
+};
+
+static struct sq_mapping *sq_mapping_list;
 static DEFINE_SPINLOCK(sq_mapping_lock);
+static kmem_cache_t *sq_cache;
+static unsigned long *sq_bitmap;
 
-/**
- * sq_flush - Flush (prefetch) the store queue cache
- * @addr: the store queue address to flush
- *
- * Executes a prefetch instruction on the specified store queue cache,
- * so that the cached data is written to physical memory.
- */
-inline void sq_flush(void *addr)
-{
-	__asm__ __volatile__ ("pref @%0" : : "r" (addr) : "memory");
-}
+#define store_queue_barrier()			\
+do {						\
+	(void)ctrl_inl(P4SEG_STORE_QUE);	\
+	ctrl_outl(0, P4SEG_STORE_QUE + 0);	\
+	ctrl_outl(0, P4SEG_STORE_QUE + 8);	\
+} while (0);
 
 /**
  * sq_flush_range - Flush (prefetch) a specific SQ range
@@ -57,154 +59,73 @@ inline void sq_flush(void *addr)
 void sq_flush_range(unsigned long start, unsigned int len)
 {
 	volatile unsigned long *sq = (unsigned long *)start;
-	unsigned long dummy;
 
 	/* Flush the queues */
 	for (len >>= 5; len--; sq += 8)
-		sq_flush((void *)sq);
+		prefetchw((void *)sq);
 
 	/* Wait for completion */
-	dummy = ctrl_inl(P4SEG_STORE_QUE);
-
-	ctrl_outl(0, P4SEG_STORE_QUE + 0);
-	ctrl_outl(0, P4SEG_STORE_QUE + 8);
+	store_queue_barrier();
 }
 
-static struct sq_mapping *__sq_alloc_mapping(unsigned long virt, unsigned long phys, unsigned long size, const char *name)
+static inline void sq_mapping_list_add(struct sq_mapping *map)
 {
-	struct sq_mapping *map;
-
-	if (virt + size > SQ_ADDRMAX)
-		return ERR_PTR(-ENOSPC);
+	struct sq_mapping **p, *tmp;
 
-	map = kmalloc(sizeof(struct sq_mapping), GFP_KERNEL);
-	if (!map)
-		return ERR_PTR(-ENOMEM);
+	spin_lock_irq(&sq_mapping_lock);
 
-	INIT_LIST_HEAD(&map->list);
+	p = &sq_mapping_list;
+	while ((tmp = *p) != NULL)
+		p = &tmp->next;
 
-	map->sq_addr	= virt;
-	map->addr	= phys;
-	map->size	= size + 1;
-	map->name	= name;
+	map->next = tmp;
+	*p = map;
 
-	list_add(&map->list, &sq_mapping_list);
-
-	return map;
+	spin_unlock_irq(&sq_mapping_lock);
 }
 
-static unsigned long __sq_get_next_addr(void)
+static inline void sq_mapping_list_del(struct sq_mapping *map)
 {
-	if (!list_empty(&sq_mapping_list)) {
-		struct list_head *pos, *tmp;
-
-		/*
-		 * Read one off the list head, as it will have the highest
-		 * mapped allocation. Set the next one up right above it.
-		 *
-		 * This is somewhat sub-optimal, as we don't look at
-		 * gaps between allocations or anything lower then the
-		 * highest-level allocation.
-		 *
-		 * However, in the interest of performance and the general
-		 * lack of desire to do constant list rebalancing, we don't
-		 * worry about it.
-		 */
-		list_for_each_safe(pos, tmp, &sq_mapping_list) {
-			struct sq_mapping *entry;
-
-			entry = list_entry(pos, typeof(*entry), list);
-
-			return entry->sq_addr + entry->size;
+	struct sq_mapping **p, *tmp;
+
+	spin_lock_irq(&sq_mapping_lock);
+
+	for (p = &sq_mapping_list; (tmp = *p); p = &tmp->next)
+		if (tmp == map) {
+			*p = tmp->next;
+			break;
 		}
-	}
 
-	return P4SEG_STORE_QUE;
+	spin_unlock_irq(&sq_mapping_lock);
 }
 
-/**
- * __sq_remap - Perform a translation from the SQ to a phys addr
- * @map: sq mapping containing phys and store queue addresses.
- *
- * Maps the store queue address specified in the mapping to the physical
- * address specified in the mapping.
- */
-static struct sq_mapping *__sq_remap(struct sq_mapping *map)
+static int __sq_remap(struct sq_mapping *map, unsigned long flags)
 {
-	unsigned long flags, pteh, ptel;
+#if defined(CONFIG_MMU)
 	struct vm_struct *vma;
-	pgprot_t pgprot;
 
-	/*
-	 * Without an MMU (or with it turned off), this is much more
-	 * straightforward, as we can just load up each queue's QACR with
-	 * the physical address appropriately masked.
-	 */
-
-	ctrl_outl(((map->addr >> 26) << 2) & 0x1c, SQ_QACR0);
-	ctrl_outl(((map->addr >> 26) << 2) & 0x1c, SQ_QACR1);
-
-#ifdef CONFIG_MMU
-	/*
-	 * With an MMU on the other hand, things are slightly more involved.
-	 * Namely, we have to have a direct mapping between the SQ addr and
-	 * the associated physical address in the UTLB by way of setting up
-	 * a virt<->phys translation by hand. We do this by simply specifying
-	 * the SQ addr in UTLB.VPN and the associated physical address in
-	 * UTLB.PPN.
-	 *
-	 * Notably, even though this is a special case translation, and some
-	 * of the configuration bits are meaningless, we're still required
-	 * to have a valid ASID context in PTEH.
-	 *
-	 * We could also probably get by without explicitly setting PTEA, but
-	 * we do it here just for good measure.
-	 */
-	spin_lock_irqsave(&sq_mapping_lock, flags);
-
-	pteh = map->sq_addr;
-	ctrl_outl((pteh & MMU_VPN_MASK) | get_asid(), MMU_PTEH);
-
-	ptel = map->addr & PAGE_MASK;
-
-	if (cpu_data->flags & CPU_HAS_PTEA)
-		ctrl_outl(((ptel >> 28) & 0xe) | (ptel & 0x1), MMU_PTEA);
-
-	pgprot = pgprot_noncached(PAGE_KERNEL);
-
-	ptel &= _PAGE_FLAGS_HARDWARE_MASK;
-	ptel |= pgprot_val(pgprot);
-	ctrl_outl(ptel, MMU_PTEL);
-
-	__asm__ __volatile__ ("ldtlb" : : : "memory");
-
-	spin_unlock_irqrestore(&sq_mapping_lock, flags);
-
-	/*
-	 * Next, we need to map ourselves in the kernel page table, so that
-	 * future accesses after a TLB flush will be handled when we take a
-	 * page fault.
-	 *
-	 * Theoretically we could just do this directly and not worry about
-	 * setting up the translation by hand ahead of time, but for the
-	 * cases where we want a one-shot SQ mapping followed by a quick
-	 * writeout before we hit the TLB flush, we do it anyways. This way
-	 * we at least save ourselves the initial page fault overhead.
-	 */
 	vma = __get_vm_area(map->size, VM_ALLOC, map->sq_addr, SQ_ADDRMAX);
 	if (!vma)
-		return ERR_PTR(-ENOMEM);
+		return -ENOMEM;
 
 	vma->phys_addr = map->addr;
 
 	if (remap_area_pages((unsigned long)vma->addr, vma->phys_addr,
-			     map->size, pgprot_val(pgprot))) {
+			     map->size, flags)) {
 		vunmap(vma->addr);
-		return NULL;
+		return -EAGAIN;
 	}
-#endif /* CONFIG_MMU */
+#else
+	/*
+	 * Without an MMU (or with it turned off), this is much more
+	 * straightforward, as we can just load up each queue's QACR with
+	 * the physical address appropriately masked.
+	 */
+	ctrl_outl(((map->addr >> 26) << 2) & 0x1c, SQ_QACR0);
+	ctrl_outl(((map->addr >> 26) << 2) & 0x1c, SQ_QACR1);
+#endif
 
-	return map;
+	return 0;
 }
 
 /**
@@ -212,42 +133,65 @@ static struct sq_mapping *__sq_remap(struct sq_mapping *map)
  * @phys: Physical address of mapping.
  * @size: Length of mapping.
  * @name: User invoking mapping.
+ * @flags: Protection flags.
  *
  * Remaps the physical address @phys through the next available store queue
  * address of @size length. @name is logged at boot time as well as through
- * the procfs interface.
- *
- * A pre-allocated and filled sq_mapping pointer is returned, and must be
- * cleaned up with a call to sq_unmap() when the user is done with the
- * mapping.
+ * the sysfs interface.
  */
-struct sq_mapping *sq_remap(unsigned long phys, unsigned int size, const char *name)
+unsigned long sq_remap(unsigned long phys, unsigned int size,
+		       const char *name, unsigned long flags)
 {
 	struct sq_mapping *map;
-	unsigned long virt, end;
+	unsigned long end;
 	unsigned int psz;
+	int ret, page;
 
 	/* Don't allow wraparound or zero size */
 	end = phys + size - 1;
-	if (!size || end < phys)
-		return NULL;
+	if (unlikely(!size || end < phys))
+		return -EINVAL;
 	/* Don't allow anyone to remap normal memory.. */
-	if (phys < virt_to_phys(high_memory))
-		return NULL;
+	if (unlikely(phys < virt_to_phys(high_memory)))
+		return -EINVAL;
 
 	phys &= PAGE_MASK;
+	size = PAGE_ALIGN(end + 1) - phys;
+
+	map = kmem_cache_alloc(sq_cache, GFP_KERNEL);
+	if (unlikely(!map))
+		return -ENOMEM;
+
+	map->addr = phys;
+	map->size = size;
+	map->name = name;
+
+	page = bitmap_find_free_region(sq_bitmap, 0x04000000,
+				       get_order(map->size));
+	if (unlikely(page < 0)) {
+		ret = -ENOSPC;
+		goto out;
+	}
+
+	map->sq_addr = P4SEG_STORE_QUE + (page << PAGE_SHIFT);
+
+	ret = __sq_remap(map, flags);
+	if (unlikely(ret != 0))
+		goto out;
+
+	psz = (size + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+	pr_info("sqremap: %15s  [%4d page%s]  va 0x%08lx   pa 0x%08lx\n",
+		likely(map->name) ? map->name : "???",
+		psz, psz == 1 ? " " : "s",
+		map->sq_addr, map->addr);
 
-	size  = PAGE_ALIGN(end + 1) - phys;
-	virt  = __sq_get_next_addr();
-	psz   = (size + (PAGE_SIZE - 1)) / PAGE_SIZE;
-	map   = __sq_alloc_mapping(virt, phys, size, name);
+	sq_mapping_list_add(map);
 
-	printk("sqremap: %15s  [%4d page%s]  va 0x%08lx   pa 0x%08lx\n",
-	       map->name ? map->name : "???",
-	       psz, psz == 1 ? " " : "s",
-	       map->sq_addr, map->addr);
+	return map->sq_addr;
 
-	return __sq_remap(map);
+out:
+	kmem_cache_free(sq_cache, map);
+	return ret;
 }
 
 /**
@@ -258,188 +202,198 @@ struct sq_mapping *sq_remap(unsigned long phys, unsigned int size, const char *n
  * sq_remap(). Also frees up the pte that was previously inserted into
  * the kernel page table and discards the UTLB translation.
  */
-void sq_unmap(struct sq_mapping *map)
+void sq_unmap(unsigned long vaddr)
 {
-	if (map->sq_addr > (unsigned long)high_memory)
-		vfree((void *)(map->sq_addr & PAGE_MASK));
+	struct sq_mapping **p, *map;
+	struct vm_struct *vma;
+	int page;
 
-	list_del(&map->list);
-	kfree(map);
-}
+	for (p = &sq_mapping_list; (map = *p); p = &map->next)
+		if (map->sq_addr == vaddr)
+			break;
 
-/**
- * sq_clear - Clear a store queue range
- * @addr: Address to start clearing from.
- * @len: Length to clear.
- *
- * A quick zero-fill implementation for clearing out memory that has been
- * remapped through the store queues.
- */
-void sq_clear(unsigned long addr, unsigned int len)
-{
-	int i;
+	if (unlikely(!map)) {
+		printk("%s: bad store queue address 0x%08lx\n",
+		       __FUNCTION__, vaddr);
+		return;
+	}
 
-	/* Clear out both queues linearly */
-	for (i = 0; i < 8; i++) {
-		ctrl_outl(0, addr + i + 0);
-		ctrl_outl(0, addr + i + 8);
+	page = (map->sq_addr - P4SEG_STORE_QUE) >> PAGE_SHIFT;
+	bitmap_release_region(sq_bitmap, page, get_order(map->size));
+
+#ifdef CONFIG_MMU
+	vma = remove_vm_area((void *)(map->sq_addr & PAGE_MASK));
+	if (!vma) {
+		printk(KERN_ERR "%s: bad address 0x%08lx\n",
+		       __FUNCTION__, map->sq_addr);
+		return;
 	}
+#endif
+
+	sq_mapping_list_del(map);
 
-	sq_flush_range(addr, len);
+	kmem_cache_free(sq_cache, map);
 }
 
-/**
- * sq_vma_unmap - Unmap a VMA range
- * @area: VMA containing range.
- * @addr: Start of range.
- * @len: Length of range.
+/*
+ * Needlessly complex sysfs interface. Unfortunately it doesn't seem like
+ * there is any other easy way to add things on a per-cpu basis without
+ * putting the directory entries somewhere stupid and having to create
+ * links in sysfs by hand back in to the per-cpu directories.
  *
- * Searches the sq_mapping_list for a mapping matching the sq addr @addr,
- * and subsequently frees up the entry. Further cleanup is done by generic
- * code.
+ * Some day we may want to have an additional abstraction per store
+ * queue, but considering the kobject hell we already have to deal with,
+ * it's simply not worth the trouble.
  */
-static void sq_vma_unmap(struct vm_area_struct *area,
-			 unsigned long addr, size_t len)
-{
-	struct list_head *pos, *tmp;
+static struct kobject *sq_kobject[NR_CPUS];
 
-	list_for_each_safe(pos, tmp, &sq_mapping_list) {
-		struct sq_mapping *entry;
+struct sq_sysfs_attr {
+	struct attribute attr;
+	ssize_t (*show)(char *buf);
+	ssize_t (*store)(const char *buf, size_t count);
+};
 
-		entry = list_entry(pos, typeof(*entry), list);
+#define to_sq_sysfs_attr(attr)	container_of(attr, struct sq_sysfs_attr, attr)
 
-		if (entry->sq_addr == addr) {
-			/*
-			 * We could probably get away without doing the tlb flush
-			 * here, as generic code should take care of most of this
-			 * when unmapping the rest of the VMA range for us. Leave
-			 * it in for added sanity for the time being..
-			 */
-			__flush_tlb_page(get_asid(), entry->sq_addr & PAGE_MASK);
+static ssize_t sq_sysfs_show(struct kobject *kobj, struct attribute *attr,
+			     char *buf)
+{
+	struct sq_sysfs_attr *sattr = to_sq_sysfs_attr(attr);
 
-			list_del(&entry->list);
-			kfree(entry);
+	if (likely(sattr->show))
+		return sattr->show(buf);
 
-			return;
-		}
-	}
+	return -EIO;
 }
 
-/**
- * sq_vma_sync - Sync a VMA range
- * @area: VMA containing range.
- * @start: Start of range.
- * @len: Length of range.
- * @flags: Additional flags.
- *
- * Synchronizes an sq mapped range by flushing the store queue cache for
- * the duration of the mapping.
- *
- * Used internally for user mappings, which must use msync() to prefetch
- * the store queue cache.
- */
-static int sq_vma_sync(struct vm_area_struct *area,
-		       unsigned long start, size_t len, unsigned int flags)
+static ssize_t sq_sysfs_store(struct kobject *kobj, struct attribute *attr,
+			      const char *buf, size_t count)
 {
-	sq_flush_range(start, len);
+	struct sq_sysfs_attr *sattr = to_sq_sysfs_attr(attr);
 
-	return 0;
+	if (likely(sattr->store))
+		return sattr->store(buf, count);
+
+	return -EIO;
 }
 
-static struct vm_operations_struct sq_vma_ops = {
-	.unmap	= sq_vma_unmap,
-	.sync	= sq_vma_sync,
-};
+static ssize_t mapping_show(char *buf)
+{
+	struct sq_mapping **list, *entry;
+	char *p = buf;
 
-/**
- * sq_mmap - mmap() for /dev/cpu/sq
- * @file: unused.
- * @vma: VMA to remap.
- *
- * Remap the specified vma @vma through the store queues, and setup associated
- * information for the new mapping. Also build up the page tables for the new
- * area.
- */
-static int sq_mmap(struct file *file, struct vm_area_struct *vma)
+	for (list = &sq_mapping_list; (entry = *list); list = &entry->next)
+		p += sprintf(p, "%08lx-%08lx [%08lx]: %s\n",
+			     entry->sq_addr, entry->sq_addr + entry->size,
+			     entry->addr, entry->name);
+
+	return p - buf;
+}
+
+static ssize_t mapping_store(const char *buf, size_t count)
 {
-	unsigned long offset = vma->vm_pgoff << PAGE_SHIFT;
-	unsigned long size = vma->vm_end - vma->vm_start;
-	struct sq_mapping *map;
+	unsigned long base = 0, len = 0;
 
-	/*
-	 * We're not interested in any arbitrary virtual address that has
-	 * been stuck in the VMA, as we already know what addresses we
-	 * want. Save off the size, and reposition the VMA to begin at
-	 * the next available sq address.
-	 */
-	vma->vm_start = __sq_get_next_addr();
-	vma->vm_end   = vma->vm_start + size;
+	sscanf(buf, "%lx %lx", &base, &len);
+	if (!base)
+		return -EIO;
 
-	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	if (likely(len)) {
+		int ret = sq_remap(base, len, "Userspace",
+				   pgprot_val(PAGE_SHARED));
+		if (ret < 0)
+			return ret;
+	} else
+		sq_unmap(base);
 
-	vma->vm_flags |= VM_IO | VM_RESERVED;
+	return count;
+}
 
-	map = __sq_alloc_mapping(vma->vm_start, offset, size, "Userspace");
+static struct sq_sysfs_attr mapping_attr =
+	__ATTR(mapping, 0644, mapping_show, mapping_store);
 
-	if (io_remap_pfn_range(vma, map->sq_addr, map->addr >> PAGE_SHIFT,
-				size, vma->vm_page_prot))
-		return -EAGAIN;
+static struct attribute *sq_sysfs_attrs[] = {
+	&mapping_attr.attr,
+	NULL,
+};
 
-	vma->vm_ops = &sq_vma_ops;
+static struct sysfs_ops sq_sysfs_ops = {
+	.show	= sq_sysfs_show,
+	.store	= sq_sysfs_store,
+};
 
-	return 0;
-}
+static struct kobj_type ktype_percpu_entry = {
+	.sysfs_ops	= &sq_sysfs_ops,
+	.default_attrs	= sq_sysfs_attrs,
+};
 
-#ifdef CONFIG_PROC_FS
-static int sq_mapping_read_proc(char *buf, char **start, off_t off,
-				int len, int *eof, void *data)
+static int __devinit sq_sysdev_add(struct sys_device *sysdev)
 {
-	struct list_head *pos;
-	char *p = buf;
+	unsigned int cpu = sysdev->id;
+	struct kobject *kobj;
 
-	list_for_each_prev(pos, &sq_mapping_list) {
-		struct sq_mapping *entry;
+	sq_kobject[cpu] = kzalloc(sizeof(struct kobject), GFP_KERNEL);
+	if (unlikely(!sq_kobject[cpu]))
+		return -ENOMEM;
 
-		entry = list_entry(pos, typeof(*entry), list);
+	kobj = sq_kobject[cpu];
+	kobj->parent = &sysdev->kobj;
+	kobject_set_name(kobj, "%s", "sq");
+	kobj->ktype = &ktype_percpu_entry;
 
-		p += sprintf(p, "%08lx-%08lx [%08lx]: %s\n", entry->sq_addr,
-			     entry->sq_addr + entry->size - 1, entry->addr,
-			     entry->name);
-	}
-
-	return p - buf;
+	return kobject_register(kobj);
 }
-#endif
 
-static struct file_operations sq_fops = {
-	.owner		= THIS_MODULE,
-	.mmap		= sq_mmap,
-};
+static int __devexit sq_sysdev_remove(struct sys_device *sysdev)
+{
+	unsigned int cpu = sysdev->id;
+	struct kobject *kobj = sq_kobject[cpu];
 
-static struct miscdevice sq_dev = {
-	.minor		= STORE_QUEUE_MINOR,
-	.name		= "sq",
-	.fops		= &sq_fops,
+	kobject_unregister(kobj);
+	return 0;
+}
+
+static struct sysdev_driver sq_sysdev_driver = {
+	.add		= sq_sysdev_add,
+	.remove		= __devexit_p(sq_sysdev_remove),
 };
 
 static int __init sq_api_init(void)
 {
-	int ret;
+	unsigned int nr_pages = 0x04000000 >> PAGE_SHIFT;
+	unsigned int size = (nr_pages + (BITS_PER_LONG - 1)) / BITS_PER_LONG;
+	int ret = -ENOMEM;
+
 	printk(KERN_NOTICE "sq: Registering store queue API.\n");
 
-	create_proc_read_entry("sq_mapping", 0, 0, sq_mapping_read_proc, 0);
+	sq_cache = kmem_cache_create("store_queue_cache",
+				sizeof(struct sq_mapping), 0, 0,
+				NULL, NULL);
+	if (unlikely(!sq_cache))
+		return ret;
 
-	ret = misc_register(&sq_dev);
-	if (ret)
-		remove_proc_entry("sq_mapping", NULL);
+	sq_bitmap = kzalloc(size, GFP_KERNEL);
+	if (unlikely(!sq_bitmap))
+		goto out;
+
+	ret = sysdev_driver_register(&cpu_sysdev_class, &sq_sysdev_driver);
+	if (unlikely(ret != 0))
+		goto out;
+
+	return 0;
+
+out:
+	kfree(sq_bitmap);
+	kmem_cache_destroy(sq_cache);
 
 	return ret;
 }
 
 static void __exit sq_api_exit(void)
 {
-	misc_deregister(&sq_dev);
-	remove_proc_entry("sq_mapping", NULL);
+	sysdev_driver_unregister(&cpu_sysdev_class, &sq_sysdev_driver);
+	kfree(sq_bitmap);
+	kmem_cache_destroy(sq_cache);
 }
 
 module_init(sq_api_init);
@@ -448,11 +402,7 @@ module_exit(sq_api_exit);
 MODULE_AUTHOR("Paul Mundt <lethal@linux-sh.org>, M. R. Brown <mrbrown@0xd6.org>");
 MODULE_DESCRIPTION("Simple API for SH-4 integrated Store Queues");
 MODULE_LICENSE("GPL");
-MODULE_ALIAS_MISCDEV(STORE_QUEUE_MINOR);
 
 EXPORT_SYMBOL(sq_remap);
 EXPORT_SYMBOL(sq_unmap);
-EXPORT_SYMBOL(sq_clear);
-EXPORT_SYMBOL(sq_flush);
 EXPORT_SYMBOL(sq_flush_range);
-

commit 749cf486920bf53f16e6a6889d9635a91ffb6c82
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 14:55:41 2006 +0900

    sh: Add flag for MMU PTEA capability.
    
    Add CPU_HAS_PTEA, refactor some of the cpu flag settings.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 32c93b781b51..b148966dd7c7 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -166,9 +166,9 @@ static struct sq_mapping *__sq_remap(struct sq_mapping *map)
 	ctrl_outl((pteh & MMU_VPN_MASK) | get_asid(), MMU_PTEH);
 
 	ptel = map->addr & PAGE_MASK;
-#ifndef CONFIG_CPU_SUBTYPE_SH7780
-	ctrl_outl(((ptel >> 28) & 0xe) | (ptel & 0x1), MMU_PTEA);
-#endif
+
+	if (cpu_data->flags & CPU_HAS_PTEA)
+		ctrl_outl(((ptel >> 28) & 0xe) | (ptel & 0x1), MMU_PTEA);
 
 	pgprot = pgprot_noncached(PAGE_KERNEL);
 

commit 5b19c9081fbd0882c936ec087bf9055a20251dec
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 14:31:40 2006 +0900

    sh: Support for SH7770/SH7780 CPU subtypes.
    
    Merge support for SH7770 and SH7780 SH-4A subtypes.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 00f6a3c8c43b..32c93b781b51 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -166,7 +166,9 @@ static struct sq_mapping *__sq_remap(struct sq_mapping *map)
 	ctrl_outl((pteh & MMU_VPN_MASK) | get_asid(), MMU_PTEH);
 
 	ptel = map->addr & PAGE_MASK;
+#ifndef CONFIG_CPU_SUBTYPE_SH7780
 	ctrl_outl(((ptel >> 28) & 0xe) | (ptel & 0x1), MMU_PTEA);
+#endif
 
 	pgprot = pgprot_noncached(PAGE_KERNEL);
 

commit e4c2cfee5d5cf3e4c16b423be23551aeddf2717b
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 12:31:01 2006 +0900

    sh: Various cosmetic cleanups.
    
    We had quite a bit of whitespace damage, clean most of it up..
    
    Signed-off-by: Stuart Menefy <stuart.menefy@st.com>
    Signed-off-by: Arthur Othieno <a.othieno@bluewin.ch>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index b09805f3ee23..00f6a3c8c43b 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -24,9 +24,10 @@
 #include <linux/proc_fs.h>
 #include <linux/miscdevice.h>
 #include <linux/vmalloc.h>
-
+#include <linux/mm.h>
 #include <asm/io.h>
 #include <asm/page.h>
+#include <asm/cacheflush.h>
 #include <asm/mmu_context.h>
 #include <asm/cpu/sq.h>
 

commit 757be186129b674e3a0146a4bc1861ed0744cd95
Author: Neil Horman <nhorman@tuxdriver.com>
Date:   Sat Aug 5 12:14:45 2006 -0700

    [PATCH] sh: fix proc file removal for superh store queue module
    
    Clean up proc file removal in sq module for superh arch.  currently on a
    failed module load or on module unload a proc file is left registered which
    can cause a random memory execution or oopses if read after unload.  This
    patch cleans up that deregistration.
    
    Signed-off-by: Neil Horman <nhorman@tuxdriver.com>
    Acked-by: Paul Mundt <lethal@linux-sh.org>
    Cc: Kazumoto Kojima <kkojima@rr.iij4u.or.jp>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 781dbb11c038..b09805f3ee23 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -421,18 +421,22 @@ static struct miscdevice sq_dev = {
 
 static int __init sq_api_init(void)
 {
+	int ret;
 	printk(KERN_NOTICE "sq: Registering store queue API.\n");
 
-#ifdef CONFIG_PROC_FS
 	create_proc_read_entry("sq_mapping", 0, 0, sq_mapping_read_proc, 0);
-#endif
 
-	return misc_register(&sq_dev);
+	ret = misc_register(&sq_dev);
+	if (ret)
+		remove_proc_entry("sq_mapping", NULL);
+
+	return ret;
 }
 
 static void __exit sq_api_exit(void)
 {
 	misc_deregister(&sq_dev);
+	remove_proc_entry("sq_mapping", NULL);
 }
 
 module_init(sq_api_init);

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jörn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jörn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 83a4f91bce5a..781dbb11c038 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -19,7 +19,6 @@
 #include <linux/init.h>
 #include <linux/kernel.h>
 #include <linux/module.h>
-#include <linux/config.h>
 #include <linux/slab.h>
 #include <linux/list.h>
 #include <linux/proc_fs.h>

commit 96192ff1a9d0c6ef365d21667080259d83ea2f5b
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Mon Jun 20 21:15:16 2005 -0700

    [PATCH] devfs: Remove the miscdevice devfs_name field as it's no longer needed
    
    Also fixes all drivers that set this field.
    
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
index 8437ea7430fe..83a4f91bce5a 100644
--- a/arch/sh/kernel/cpu/sh4/sq.c
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -417,7 +417,6 @@ static struct file_operations sq_fops = {
 static struct miscdevice sq_dev = {
 	.minor		= STORE_QUEUE_MINOR,
 	.name		= "sq",
-	.devfs_name	= "cpu/sq",
 	.fops		= &sq_fops,
 };
 

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/arch/sh/kernel/cpu/sh4/sq.c b/arch/sh/kernel/cpu/sh4/sq.c
new file mode 100644
index 000000000000..8437ea7430fe
--- /dev/null
+++ b/arch/sh/kernel/cpu/sh4/sq.c
@@ -0,0 +1,453 @@
+/*
+ * arch/sh/kernel/cpu/sq.c
+ *
+ * General management API for SH-4 integrated Store Queues
+ *
+ * Copyright (C) 2001, 2002, 2003, 2004  Paul Mundt
+ * Copyright (C) 2001, 2002  M. R. Brown
+ *
+ * Some of this code has been adopted directly from the old arch/sh/mm/sq.c
+ * hack that was part of the LinuxDC project. For all intents and purposes,
+ * this is a completely new interface that really doesn't have much in common
+ * with the old zone-based approach at all. In fact, it's only listed here for
+ * general completeness.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ */
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/config.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <linux/proc_fs.h>
+#include <linux/miscdevice.h>
+#include <linux/vmalloc.h>
+
+#include <asm/io.h>
+#include <asm/page.h>
+#include <asm/mmu_context.h>
+#include <asm/cpu/sq.h>
+
+static LIST_HEAD(sq_mapping_list);
+static DEFINE_SPINLOCK(sq_mapping_lock);
+
+/**
+ * sq_flush - Flush (prefetch) the store queue cache
+ * @addr: the store queue address to flush
+ *
+ * Executes a prefetch instruction on the specified store queue cache,
+ * so that the cached data is written to physical memory.
+ */
+inline void sq_flush(void *addr)
+{
+	__asm__ __volatile__ ("pref @%0" : : "r" (addr) : "memory");
+}
+
+/**
+ * sq_flush_range - Flush (prefetch) a specific SQ range
+ * @start: the store queue address to start flushing from
+ * @len: the length to flush
+ *
+ * Flushes the store queue cache from @start to @start + @len in a
+ * linear fashion.
+ */
+void sq_flush_range(unsigned long start, unsigned int len)
+{
+	volatile unsigned long *sq = (unsigned long *)start;
+	unsigned long dummy;
+
+	/* Flush the queues */
+	for (len >>= 5; len--; sq += 8)
+		sq_flush((void *)sq);
+
+	/* Wait for completion */
+	dummy = ctrl_inl(P4SEG_STORE_QUE);
+
+	ctrl_outl(0, P4SEG_STORE_QUE + 0);
+	ctrl_outl(0, P4SEG_STORE_QUE + 8);
+}
+
+static struct sq_mapping *__sq_alloc_mapping(unsigned long virt, unsigned long phys, unsigned long size, const char *name)
+{
+	struct sq_mapping *map;
+
+	if (virt + size > SQ_ADDRMAX)
+		return ERR_PTR(-ENOSPC);
+
+	map = kmalloc(sizeof(struct sq_mapping), GFP_KERNEL);
+	if (!map)
+		return ERR_PTR(-ENOMEM);
+
+	INIT_LIST_HEAD(&map->list);
+
+	map->sq_addr	= virt;
+	map->addr	= phys;
+	map->size	= size + 1;
+	map->name	= name;
+
+	list_add(&map->list, &sq_mapping_list);
+
+	return map;
+}
+
+static unsigned long __sq_get_next_addr(void)
+{
+	if (!list_empty(&sq_mapping_list)) {
+		struct list_head *pos, *tmp;
+
+		/*
+		 * Read one off the list head, as it will have the highest
+		 * mapped allocation. Set the next one up right above it.
+		 *
+		 * This is somewhat sub-optimal, as we don't look at
+		 * gaps between allocations or anything lower then the
+		 * highest-level allocation.
+		 *
+		 * However, in the interest of performance and the general
+		 * lack of desire to do constant list rebalancing, we don't
+		 * worry about it.
+		 */
+		list_for_each_safe(pos, tmp, &sq_mapping_list) {
+			struct sq_mapping *entry;
+
+			entry = list_entry(pos, typeof(*entry), list);
+
+			return entry->sq_addr + entry->size;
+		}
+	}
+
+	return P4SEG_STORE_QUE;
+}
+
+/**
+ * __sq_remap - Perform a translation from the SQ to a phys addr
+ * @map: sq mapping containing phys and store queue addresses.
+ *
+ * Maps the store queue address specified in the mapping to the physical
+ * address specified in the mapping.
+ */
+static struct sq_mapping *__sq_remap(struct sq_mapping *map)
+{
+	unsigned long flags, pteh, ptel;
+	struct vm_struct *vma;
+	pgprot_t pgprot;
+
+	/*
+	 * Without an MMU (or with it turned off), this is much more
+	 * straightforward, as we can just load up each queue's QACR with
+	 * the physical address appropriately masked.
+	 */
+
+	ctrl_outl(((map->addr >> 26) << 2) & 0x1c, SQ_QACR0);
+	ctrl_outl(((map->addr >> 26) << 2) & 0x1c, SQ_QACR1);
+
+#ifdef CONFIG_MMU
+	/*
+	 * With an MMU on the other hand, things are slightly more involved.
+	 * Namely, we have to have a direct mapping between the SQ addr and
+	 * the associated physical address in the UTLB by way of setting up
+	 * a virt<->phys translation by hand. We do this by simply specifying
+	 * the SQ addr in UTLB.VPN and the associated physical address in
+	 * UTLB.PPN.
+	 *
+	 * Notably, even though this is a special case translation, and some
+	 * of the configuration bits are meaningless, we're still required
+	 * to have a valid ASID context in PTEH.
+	 *
+	 * We could also probably get by without explicitly setting PTEA, but
+	 * we do it here just for good measure.
+	 */
+	spin_lock_irqsave(&sq_mapping_lock, flags);
+
+	pteh = map->sq_addr;
+	ctrl_outl((pteh & MMU_VPN_MASK) | get_asid(), MMU_PTEH);
+
+	ptel = map->addr & PAGE_MASK;
+	ctrl_outl(((ptel >> 28) & 0xe) | (ptel & 0x1), MMU_PTEA);
+
+	pgprot = pgprot_noncached(PAGE_KERNEL);
+
+	ptel &= _PAGE_FLAGS_HARDWARE_MASK;
+	ptel |= pgprot_val(pgprot);
+	ctrl_outl(ptel, MMU_PTEL);
+
+	__asm__ __volatile__ ("ldtlb" : : : "memory");
+
+	spin_unlock_irqrestore(&sq_mapping_lock, flags);
+
+	/*
+	 * Next, we need to map ourselves in the kernel page table, so that
+	 * future accesses after a TLB flush will be handled when we take a
+	 * page fault.
+	 *
+	 * Theoretically we could just do this directly and not worry about
+	 * setting up the translation by hand ahead of time, but for the
+	 * cases where we want a one-shot SQ mapping followed by a quick
+	 * writeout before we hit the TLB flush, we do it anyways. This way
+	 * we at least save ourselves the initial page fault overhead.
+	 */
+	vma = __get_vm_area(map->size, VM_ALLOC, map->sq_addr, SQ_ADDRMAX);
+	if (!vma)
+		return ERR_PTR(-ENOMEM);
+
+	vma->phys_addr = map->addr;
+
+	if (remap_area_pages((unsigned long)vma->addr, vma->phys_addr,
+			     map->size, pgprot_val(pgprot))) {
+		vunmap(vma->addr);
+		return NULL;
+	}
+#endif /* CONFIG_MMU */
+
+	return map;
+}
+
+/**
+ * sq_remap - Map a physical address through the Store Queues
+ * @phys: Physical address of mapping.
+ * @size: Length of mapping.
+ * @name: User invoking mapping.
+ *
+ * Remaps the physical address @phys through the next available store queue
+ * address of @size length. @name is logged at boot time as well as through
+ * the procfs interface.
+ *
+ * A pre-allocated and filled sq_mapping pointer is returned, and must be
+ * cleaned up with a call to sq_unmap() when the user is done with the
+ * mapping.
+ */
+struct sq_mapping *sq_remap(unsigned long phys, unsigned int size, const char *name)
+{
+	struct sq_mapping *map;
+	unsigned long virt, end;
+	unsigned int psz;
+
+	/* Don't allow wraparound or zero size */
+	end = phys + size - 1;
+	if (!size || end < phys)
+		return NULL;
+	/* Don't allow anyone to remap normal memory.. */
+	if (phys < virt_to_phys(high_memory))
+		return NULL;
+
+	phys &= PAGE_MASK;
+
+	size  = PAGE_ALIGN(end + 1) - phys;
+	virt  = __sq_get_next_addr();
+	psz   = (size + (PAGE_SIZE - 1)) / PAGE_SIZE;
+	map   = __sq_alloc_mapping(virt, phys, size, name);
+
+	printk("sqremap: %15s  [%4d page%s]  va 0x%08lx   pa 0x%08lx\n",
+	       map->name ? map->name : "???",
+	       psz, psz == 1 ? " " : "s",
+	       map->sq_addr, map->addr);
+
+	return __sq_remap(map);
+}
+
+/**
+ * sq_unmap - Unmap a Store Queue allocation
+ * @map: Pre-allocated Store Queue mapping.
+ *
+ * Unmaps the store queue allocation @map that was previously created by
+ * sq_remap(). Also frees up the pte that was previously inserted into
+ * the kernel page table and discards the UTLB translation.
+ */
+void sq_unmap(struct sq_mapping *map)
+{
+	if (map->sq_addr > (unsigned long)high_memory)
+		vfree((void *)(map->sq_addr & PAGE_MASK));
+
+	list_del(&map->list);
+	kfree(map);
+}
+
+/**
+ * sq_clear - Clear a store queue range
+ * @addr: Address to start clearing from.
+ * @len: Length to clear.
+ *
+ * A quick zero-fill implementation for clearing out memory that has been
+ * remapped through the store queues.
+ */
+void sq_clear(unsigned long addr, unsigned int len)
+{
+	int i;
+
+	/* Clear out both queues linearly */
+	for (i = 0; i < 8; i++) {
+		ctrl_outl(0, addr + i + 0);
+		ctrl_outl(0, addr + i + 8);
+	}
+
+	sq_flush_range(addr, len);
+}
+
+/**
+ * sq_vma_unmap - Unmap a VMA range
+ * @area: VMA containing range.
+ * @addr: Start of range.
+ * @len: Length of range.
+ *
+ * Searches the sq_mapping_list for a mapping matching the sq addr @addr,
+ * and subsequently frees up the entry. Further cleanup is done by generic
+ * code.
+ */
+static void sq_vma_unmap(struct vm_area_struct *area,
+			 unsigned long addr, size_t len)
+{
+	struct list_head *pos, *tmp;
+
+	list_for_each_safe(pos, tmp, &sq_mapping_list) {
+		struct sq_mapping *entry;
+
+		entry = list_entry(pos, typeof(*entry), list);
+
+		if (entry->sq_addr == addr) {
+			/*
+			 * We could probably get away without doing the tlb flush
+			 * here, as generic code should take care of most of this
+			 * when unmapping the rest of the VMA range for us. Leave
+			 * it in for added sanity for the time being..
+			 */
+			__flush_tlb_page(get_asid(), entry->sq_addr & PAGE_MASK);
+
+			list_del(&entry->list);
+			kfree(entry);
+
+			return;
+		}
+	}
+}
+
+/**
+ * sq_vma_sync - Sync a VMA range
+ * @area: VMA containing range.
+ * @start: Start of range.
+ * @len: Length of range.
+ * @flags: Additional flags.
+ *
+ * Synchronizes an sq mapped range by flushing the store queue cache for
+ * the duration of the mapping.
+ *
+ * Used internally for user mappings, which must use msync() to prefetch
+ * the store queue cache.
+ */
+static int sq_vma_sync(struct vm_area_struct *area,
+		       unsigned long start, size_t len, unsigned int flags)
+{
+	sq_flush_range(start, len);
+
+	return 0;
+}
+
+static struct vm_operations_struct sq_vma_ops = {
+	.unmap	= sq_vma_unmap,
+	.sync	= sq_vma_sync,
+};
+
+/**
+ * sq_mmap - mmap() for /dev/cpu/sq
+ * @file: unused.
+ * @vma: VMA to remap.
+ *
+ * Remap the specified vma @vma through the store queues, and setup associated
+ * information for the new mapping. Also build up the page tables for the new
+ * area.
+ */
+static int sq_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	unsigned long offset = vma->vm_pgoff << PAGE_SHIFT;
+	unsigned long size = vma->vm_end - vma->vm_start;
+	struct sq_mapping *map;
+
+	/*
+	 * We're not interested in any arbitrary virtual address that has
+	 * been stuck in the VMA, as we already know what addresses we
+	 * want. Save off the size, and reposition the VMA to begin at
+	 * the next available sq address.
+	 */
+	vma->vm_start = __sq_get_next_addr();
+	vma->vm_end   = vma->vm_start + size;
+
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+
+	vma->vm_flags |= VM_IO | VM_RESERVED;
+
+	map = __sq_alloc_mapping(vma->vm_start, offset, size, "Userspace");
+
+	if (io_remap_pfn_range(vma, map->sq_addr, map->addr >> PAGE_SHIFT,
+				size, vma->vm_page_prot))
+		return -EAGAIN;
+
+	vma->vm_ops = &sq_vma_ops;
+
+	return 0;
+}
+
+#ifdef CONFIG_PROC_FS
+static int sq_mapping_read_proc(char *buf, char **start, off_t off,
+				int len, int *eof, void *data)
+{
+	struct list_head *pos;
+	char *p = buf;
+
+	list_for_each_prev(pos, &sq_mapping_list) {
+		struct sq_mapping *entry;
+
+		entry = list_entry(pos, typeof(*entry), list);
+
+		p += sprintf(p, "%08lx-%08lx [%08lx]: %s\n", entry->sq_addr,
+			     entry->sq_addr + entry->size - 1, entry->addr,
+			     entry->name);
+	}
+
+	return p - buf;
+}
+#endif
+
+static struct file_operations sq_fops = {
+	.owner		= THIS_MODULE,
+	.mmap		= sq_mmap,
+};
+
+static struct miscdevice sq_dev = {
+	.minor		= STORE_QUEUE_MINOR,
+	.name		= "sq",
+	.devfs_name	= "cpu/sq",
+	.fops		= &sq_fops,
+};
+
+static int __init sq_api_init(void)
+{
+	printk(KERN_NOTICE "sq: Registering store queue API.\n");
+
+#ifdef CONFIG_PROC_FS
+	create_proc_read_entry("sq_mapping", 0, 0, sq_mapping_read_proc, 0);
+#endif
+
+	return misc_register(&sq_dev);
+}
+
+static void __exit sq_api_exit(void)
+{
+	misc_deregister(&sq_dev);
+}
+
+module_init(sq_api_init);
+module_exit(sq_api_exit);
+
+MODULE_AUTHOR("Paul Mundt <lethal@linux-sh.org>, M. R. Brown <mrbrown@0xd6.org>");
+MODULE_DESCRIPTION("Simple API for SH-4 integrated Store Queues");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS_MISCDEV(STORE_QUEUE_MINOR);
+
+EXPORT_SYMBOL(sq_remap);
+EXPORT_SYMBOL(sq_unmap);
+EXPORT_SYMBOL(sq_clear);
+EXPORT_SYMBOL(sq_flush);
+EXPORT_SYMBOL(sq_flush_range);
+
