commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 02d936406c6e..27751e9470df 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -168,6 +168,4 @@ static inline bool pte_access_permitted(pte_t pte, bool write)
 #define HAVE_ARCH_UNMAPPED_AREA
 #define HAVE_ARCH_UNMAPPED_AREA_TOPDOWN
 
-#include <asm-generic/pgtable.h>
-
 #endif /* __ASM_SH_PGTABLE_H */

commit 37744feebc086908fd89760650f458ab19071750
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Apr 20 11:37:12 2020 +0200

    sh: remove sh5 support
    
    sh5 never became a product and has probably never really worked.
    
    Remove it by recursively deleting all associated Kconfig options
    and all corresponding files.
    
    Reviewed-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Rich Felker <dalias@libc.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index cbd0f3c55a0c..02d936406c6e 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -76,18 +76,10 @@ static inline unsigned long phys_addr_mask(void)
 #define PTE_PHYS_MASK		(phys_addr_mask() & PAGE_MASK)
 #define PTE_FLAGS_MASK		(~(PTE_PHYS_MASK) << PAGE_SHIFT)
 
-#ifdef CONFIG_SUPERH32
 #define VMALLOC_START	(P3SEG)
-#else
-#define VMALLOC_START	(0xf0000000)
-#endif
 #define VMALLOC_END	(FIXADDR_START-2*PAGE_SIZE)
 
-#if defined(CONFIG_SUPERH32)
 #include <asm/pgtable_32.h>
-#else
-#include <asm/pgtable_64.h>
-#endif
 
 /*
  * SH-X and lower (legacy) SuperH parts (SH-3, SH-4, some SH-4A) can't do page
@@ -159,15 +151,6 @@ static inline bool pte_access_permitted(pte_t pte, bool write)
 		prot |= _PAGE_EXT(_PAGE_EXT_KERN_WRITE | _PAGE_EXT_USER_WRITE);
 	return __pte_access_permitted(pte, prot);
 }
-#elif defined(CONFIG_SUPERH64)
-static inline bool pte_access_permitted(pte_t pte, bool write)
-{
-	u64 prot = _PAGE_PRESENT | _PAGE_USER | _PAGE_READ;
-
-	if (write)
-		prot |= _PAGE_WRITE;
-	return __pte_access_permitted(pte, prot);
-}
 #else
 static inline bool pte_access_permitted(pte_t pte, bool write)
 {

commit 782de70c42930baae55234f3df0dc90774924447
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Sep 23 15:35:31 2019 -0700

    mm: consolidate pgtable_cache_init() and pgd_cache_init()
    
    Both pgtable_cache_init() and pgd_cache_init() are used to initialize kmem
    cache for page table allocations on several architectures that do not use
    PAGE_SIZE tables for one or more levels of the page table hierarchy.
    
    Most architectures do not implement these functions and use __weak default
    NOP implementation of pgd_cache_init().  Since there is no such default
    for pgtable_cache_init(), its empty stub is duplicated among most
    architectures.
    
    Rename the definitions of pgd_cache_init() to pgtable_cache_init() and
    drop empty stubs of pgtable_cache_init().
    
    Link: http://lkml.kernel.org/r/1566457046-22637-1-git-send-email-rppt@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Acked-by: Will Deacon <will@kernel.org>         [arm64]
    Acked-by: Thomas Gleixner <tglx@linutronix.de>  [x86]
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Matthew Wilcox <willy@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 9085d1142fa3..cbd0f3c55a0c 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -123,11 +123,6 @@ typedef pte_t *pte_addr_t;
 
 #define pte_pfn(x)		((unsigned long)(((x).pte_low >> PAGE_SHIFT)))
 
-/*
- * Initialise the page table caches
- */
-extern void pgtable_cache_init(void);
-
 struct vm_area_struct;
 struct mm_struct;
 

commit 3c9b9accad9f25a52438c790f76b08d79051d383
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jul 11 20:57:00 2019 -0700

    sh: use the generic get_user_pages_fast code
    
    The sh code is mostly equivalent to the generic one, minus various
    bugfixes and two arch overrides that this patch adds to pgtable.h.
    
    Link: http://lkml.kernel.org/r/20190625143715.1689-7-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Khalid Aziz <khalid.aziz@oracle.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 3587103afe59..9085d1142fa3 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -149,6 +149,43 @@ extern void paging_init(void);
 extern void page_table_range_init(unsigned long start, unsigned long end,
 				  pgd_t *pgd);
 
+static inline bool __pte_access_permitted(pte_t pte, u64 prot)
+{
+	return (pte_val(pte) & (prot | _PAGE_SPECIAL)) == prot;
+}
+
+#ifdef CONFIG_X2TLB
+static inline bool pte_access_permitted(pte_t pte, bool write)
+{
+	u64 prot = _PAGE_PRESENT;
+
+	prot |= _PAGE_EXT(_PAGE_EXT_KERN_READ | _PAGE_EXT_USER_READ);
+	if (write)
+		prot |= _PAGE_EXT(_PAGE_EXT_KERN_WRITE | _PAGE_EXT_USER_WRITE);
+	return __pte_access_permitted(pte, prot);
+}
+#elif defined(CONFIG_SUPERH64)
+static inline bool pte_access_permitted(pte_t pte, bool write)
+{
+	u64 prot = _PAGE_PRESENT | _PAGE_USER | _PAGE_READ;
+
+	if (write)
+		prot |= _PAGE_WRITE;
+	return __pte_access_permitted(pte, prot);
+}
+#else
+static inline bool pte_access_permitted(pte_t pte, bool write)
+{
+	u64 prot = _PAGE_PRESENT | _PAGE_USER;
+
+	if (write)
+		prot |= _PAGE_RW;
+	return __pte_access_permitted(pte, prot);
+}
+#endif
+
+#define pte_access_permitted pte_access_permitted
+
 /* arch/sh/mm/mmap.c */
 #define HAVE_ARCH_UNMAPPED_AREA
 #define HAVE_ARCH_UNMAPPED_AREA_TOPDOWN

commit 6a0abce4c4cce0890e2c930b960b9a05c8c6e5da
Author: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
Date:   Fri Dec 28 00:31:53 2018 -0800

    sh: include: convert to SPDX identifiers
    
    Update license to use SPDX-License-Identifier instead of verbose license
    text.
    
    Link: http://lkml.kernel.org/r/87ftvccszx.wl-kuninori.morimoto.gx@renesas.com
    Signed-off-by: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
    Reviewed-by: Simon Horman <horms+renesas@verge.net.au>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index f6abfe2bca93..3587103afe59 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -1,13 +1,10 @@
-/*
+/* SPDX-License-Identifier: GPL-2.0
+ *
  * This file contains the functions and defines necessary to modify and
  * use the SuperH page table tree.
  *
  * Copyright (C) 1999 Niibe Yutaka
  * Copyright (C) 2002 - 2007 Paul Mundt
- *
- * This file is subject to the terms and conditions of the GNU General
- * Public License.  See the file "COPYING" in the main directory of this
- * archive for more details.
  */
 #ifndef __ASM_SH_PGTABLE_H
 #define __ASM_SH_PGTABLE_H

commit 3010a5ea665a089361e435093bd737399123fcc4
Author: Laurent Dufour <ldufour@linux.vnet.ibm.com>
Date:   Thu Jun 7 17:06:08 2018 -0700

    mm: introduce ARCH_HAS_PTE_SPECIAL
    
    Currently the PTE special supports is turned on in per architecture
    header files.  Most of the time, it is defined in
    arch/*/include/asm/pgtable.h depending or not on some other per
    architecture static definition.
    
    This patch introduce a new configuration variable to manage this
    directly in the Kconfig files.  It would later replace
    __HAVE_ARCH_PTE_SPECIAL.
    
    Here notes for some architecture where the definition of
    __HAVE_ARCH_PTE_SPECIAL is not obvious:
    
    arm
     __HAVE_ARCH_PTE_SPECIAL which is currently defined in
    arch/arm/include/asm/pgtable-3level.h which is included by
    arch/arm/include/asm/pgtable.h when CONFIG_ARM_LPAE is set.
    So select ARCH_HAS_PTE_SPECIAL if ARM_LPAE.
    
    powerpc
    __HAVE_ARCH_PTE_SPECIAL is defined in 2 files:
     - arch/powerpc/include/asm/book3s/64/pgtable.h
     - arch/powerpc/include/asm/pte-common.h
    The first one is included if (PPC_BOOK3S & PPC64) while the second is
    included in all the other cases.
    So select ARCH_HAS_PTE_SPECIAL all the time.
    
    sparc:
    __HAVE_ARCH_PTE_SPECIAL is defined if defined(__sparc__) &&
    defined(__arch64__) which are defined through the compiler in
    sparc/Makefile if !SPARC32 which I assume to be if SPARC64.
    So select ARCH_HAS_PTE_SPECIAL if SPARC64
    
    There is no functional change introduced by this patch.
    
    Link: http://lkml.kernel.org/r/1523433816-14460-2-git-send-email-ldufour@linux.vnet.ibm.com
    Signed-off-by: Laurent Dufour <ldufour@linux.vnet.ibm.com>
    Suggested-by: Jerome Glisse <jglisse@redhat.com>
    Reviewed-by: Jerome Glisse <jglisse@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: "Aneesh Kumar K . V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Albert Ou <albert@sifive.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Christophe LEROY <christophe.leroy@c-s.fr>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 89c513a982fc..f6abfe2bca93 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -156,8 +156,6 @@ extern void page_table_range_init(unsigned long start, unsigned long end,
 #define HAVE_ARCH_UNMAPPED_AREA
 #define HAVE_ARCH_UNMAPPED_AREA_TOPDOWN
 
-#define __HAVE_ARCH_PTE_SPECIAL
-
 #include <asm-generic/pgtable.h>
 
 #endif /* __ASM_SH_PGTABLE_H */

commit d016bf7ece53b2b947bfd769e0842fd2feb7556b
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Feb 11 15:26:41 2015 -0800

    mm: make FIRST_USER_ADDRESS unsigned long on all archs
    
    LKP has triggered a compiler warning after my recent patch "mm: account
    pmd page tables to the process":
    
        mm/mmap.c: In function 'exit_mmap':
     >> mm/mmap.c:2857:2: warning: right shift count >= width of type [enabled by default]
    
    The code:
    
     > 2857                WARN_ON(mm_nr_pmds(mm) >
       2858                                round_up(FIRST_USER_ADDRESS, PUD_SIZE) >> PUD_SHIFT);
    
    In this, on tile, we have FIRST_USER_ADDRESS defined as 0.  round_up() has
    the same type -- int.  PUD_SHIFT.
    
    I think the best way to fix it is to define FIRST_USER_ADDRESS as unsigned
    long.  On every arch for consistency.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reported-by: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index cf434c64408d..89c513a982fc 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -62,7 +62,7 @@ static inline unsigned long long neff_sign_extend(unsigned long val)
 /* Entries per level */
 #define PTRS_PER_PTE	(PAGE_SIZE / (1 << PTE_MAGNITUDE))
 
-#define FIRST_USER_ADDRESS	0
+#define FIRST_USER_ADDRESS	0UL
 
 #define PHYS_ADDR_MASK29		0x1fffffff
 #define PHYS_ADDR_MASK32		0xffffffff

commit 40d158e61840fbbe23be3f37302a3ca237c15491
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat May 11 12:13:10 2013 -0400

    consolidate io_remap_pfn_range definitions
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 9210e93a92c3..cf434c64408d 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -124,9 +124,6 @@ typedef pte_t *pte_addr_t;
 
 #define kern_addr_valid(addr)	(1)
 
-#define io_remap_pfn_range(vma, vaddr, pfn, size, prot)		\
-		remap_pfn_range(vma, vaddr, pfn, size, prot)
-
 #define pte_pfn(x)		((unsigned long)(((x).pte_low >> PAGE_SHIFT)))
 
 /*

commit 65d517eb7224d24ee4206416161390f30d69e622
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue May 31 14:37:44 2011 +0900

    sh64: asm/pgtable.h needs asm/mmu.h
    
    Needed to satisfy the __in_29bit_mode() check.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index db85916b9e95..9210e93a92c3 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -18,6 +18,7 @@
 #include <asm/pgtable-2level.h>
 #endif
 #include <asm/page.h>
+#include <asm/mmu.h>
 
 #ifndef __ASSEMBLY__
 #include <asm/addrspace.h>

commit 8f82f0c7029d39b499389c8e225cf147fb83abe2
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Jan 14 15:46:50 2011 +0900

    sh: Fix up breakage from asm-generic/pgtable.h changes.
    
    We require a forward declaration for mm_struct:
    
    In file included from arch/sh/include/asm/pgtable.h:163,
                     from arch/sh/include/asm/io.h:21,
                     from arch/sh/kernel/machvec.c:20:
    include/asm-generic/pgtable.h:104: error: 'struct mm_struct' declared inside parameter list
    include/asm-generic/pgtable.h: In function 'ptep_get_and_clear_full':
    include/asm-generic/pgtable.h:107: error: passing argument 1 of 'ptep_get_and_clear' from incompatible pointer type
    include/asm-generic/pgtable.h:70: note: expected 'struct mm_struct *' but argument is of type 'struct mm_struct *'
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 083ea068e819..db85916b9e95 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -134,6 +134,7 @@ typedef pte_t *pte_addr_t;
 extern void pgtable_cache_init(void);
 
 struct vm_area_struct;
+struct mm_struct;
 
 extern void __update_cache(struct vm_area_struct *vma,
 			   unsigned long address, pte_t pte);

commit ccedb20c6879ac0237b95b3500d69822f1e5e2ea
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Nov 4 12:51:08 2010 +0900

    sh: Simplify phys_addr_mask()/PTE_PHYS_MASK for 29/32-bit.
    
    Given that __in_29bit_mode() is a constant for the non-PMB case, we can
    simply use the PMB-facing version of phys_addr_mask() and drop the other
    variants.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index a15f1058bbf4..083ea068e819 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -66,7 +66,6 @@ static inline unsigned long long neff_sign_extend(unsigned long val)
 #define PHYS_ADDR_MASK29		0x1fffffff
 #define PHYS_ADDR_MASK32		0xffffffff
 
-#ifdef CONFIG_PMB
 static inline unsigned long phys_addr_mask(void)
 {
 	/* Is the MMU in 29bit mode? */
@@ -75,17 +74,6 @@ static inline unsigned long phys_addr_mask(void)
 
 	return PHYS_ADDR_MASK32;
 }
-#elif defined(CONFIG_32BIT)
-static inline unsigned long phys_addr_mask(void)
-{
-	return PHYS_ADDR_MASK32;
-}
-#else
-static inline unsigned long phys_addr_mask(void)
-{
-	return PHYS_ADDR_MASK29;
-}
-#endif
 
 #define PTE_PHYS_MASK		(phys_addr_mask() & PAGE_MASK)
 #define PTE_FLAGS_MASK		(~(PTE_PHYS_MASK) << PAGE_SHIFT)

commit a16382ce1cbcc3ab12274e20f92c09c439629162
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Oct 27 16:40:19 2010 +0900

    sh64: _PAGE_SPECIAL support.
    
    Now that sh64 has grown extended page flag support we finally have a free
    bit for _PAGE_SPECIAL. Wire it up.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 02f77450cd8f..a15f1058bbf4 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -169,6 +169,8 @@ extern void page_table_range_init(unsigned long start, unsigned long end,
 #define HAVE_ARCH_UNMAPPED_AREA
 #define HAVE_ARCH_UNMAPPED_AREA_TOPDOWN
 
+#define __HAVE_ARCH_PTE_SPECIAL
+
 #include <asm-generic/pgtable.h>
 
 #endif /* __ASM_SH_PGTABLE_H */

commit ac0f6f927db539e03e1f3f61bcd4ed57d5cde7a9
Merge: b1bf9368407a 9f33be2c3a80
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 1 09:15:15 2010 -0800

    Merge branch 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm
    
    * 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm: (100 commits)
      ARM: Eliminate decompressor -Dstatic= PIC hack
      ARM: 5958/1: ARM: U300: fix inverted clk round rate
      ARM: 5956/1: misplaced parentheses
      ARM: 5955/1: ep93xx: move timer defines into core.c and document
      ARM: 5954/1: ep93xx: move gpio interrupt support to gpio.c
      ARM: 5953/1: ep93xx: fix broken build of clock.c
      ARM: 5952/1: ARM: MM: Add ARM_L1_CACHE_SHIFT_6 for handle inside each ARCH Kconfig
      ARM: 5949/1: NUC900 add gpio virtual memory map
      ARM: 5948/1: Enable timer0 to time4 clock support for nuc910
      ARM: 5940/2: ARM: MMCI: remove custom DBG macro and printk
      ARM: make_coherent(): fix problems with highpte, part 2
      MM: Pass a PTE pointer to update_mmu_cache() rather than the PTE itself
      ARM: 5945/1: ep93xx: include correct irq.h in core.c
      ARM: 5933/1: amba-pl011: support hardware flow control
      ARM: 5930/1: Add PKMAP area description to memory.txt.
      ARM: 5929/1: Add checks to detect overlap of memory regions.
      ARM: 5928/1: Change type of VMALLOC_END to unsigned long.
      ARM: 5927/1: Make delimiters of DMA area globally visibly.
      ARM: 5926/1: Add "Virtual kernel memory..." printout.
      ARM: 5920/1: OMAP4: Enable L2 Cache
      ...
    
    Fix up trivial conflict in arch/arm/mach-mx25/clock.c

commit 4b3073e1c53a256275f1079c0fbfbe85883d9275
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Dec 18 16:40:18 2009 +0000

    MM: Pass a PTE pointer to update_mmu_cache() rather than the PTE itself
    
    On VIVT ARM, when we have multiple shared mappings of the same file
    in the same MM, we need to ensure that we have coherency across all
    copies.  We do this via make_coherent() by making the pages
    uncacheable.
    
    This used to work fine, until we allowed highmem with highpte - we
    now have a page table which is mapped as required, and is not available
    for modification via update_mmu_cache().
    
    Ralf Beache suggested getting rid of the PTE value passed to
    update_mmu_cache():
    
      On MIPS update_mmu_cache() calls __update_tlb() which walks pagetables
      to construct a pointer to the pte again.  Passing a pte_t * is much
      more elegant.  Maybe we might even replace the pte argument with the
      pte_t?
    
    Ben Herrenschmidt would also like the pte pointer for PowerPC:
    
      Passing the ptep in there is exactly what I want.  I want that
      -instead- of the PTE value, because I have issue on some ppc cases,
      for I$/D$ coherency, where set_pte_at() may decide to mask out the
      _PAGE_EXEC.
    
    So, pass in the mapped page table pointer into update_mmu_cache(), and
    remove the PTE value, updating all implementations and call sites to
    suit.
    
    Includes a fix from Stephen Rothwell:
    
      sparc: fix fallout from update_mmu_cache API change
    
      Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index ba3046e4f06f..1ff93ac1aa44 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -165,8 +165,9 @@ extern void __update_tlb(struct vm_area_struct *vma,
 			 unsigned long address, pte_t pte);
 
 static inline void
-update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t pte)
+update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *ptep)
 {
+	pte_t pte = *ptep;
 	__update_cache(vma, address, pte);
 	__update_tlb(vma, address, pte);
 }

commit e44d6c4010a4c84369013932eea4721d16cbc08d
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jan 13 19:18:39 2010 +0900

    sh: Rename split-level pgtable headers.
    
    These were originally named _nopmd and _pmd to follow their asm-generic
    counterparts, but we rename them to -2level and -3level for general
    consistency.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 856ece07d31b..aab76528abb9 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -13,9 +13,9 @@
 #define __ASM_SH_PGTABLE_H
 
 #ifdef CONFIG_X2TLB
-#include <asm/pgtable_pmd.h>
+#include <asm/pgtable-3level.h>
 #else
-#include <asm/pgtable_nopmd.h>
+#include <asm/pgtable-2level.h>
 #endif
 #include <asm/page.h>
 

commit 782bb5a532f883540bf403afb19f735a4eefd95b
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jan 13 19:11:14 2010 +0900

    sh: default to extended TLB support.
    
    All SH-X2 and SH-X3 parts support an extended TLB mode, which has been
    left as experimental since support was originally merged. Now that it's
    had some time to stabilize and get some exposure to various platforms,
    we can drop it as an option and default enable it across the board.
    
    This is also good future proofing for newer parts that will drop support
    for the legacy TLB mode completely.
    
    This will also force 3-level page tables for all newer parts, which is
    necessary both for the varying page sizes and larger memories.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 78598ec33d0a..856ece07d31b 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -12,7 +12,7 @@
 #ifndef __ASM_SH_PGTABLE_H
 #define __ASM_SH_PGTABLE_H
 
-#ifdef CONFIG_PGTABLE_LEVELS_3
+#ifdef CONFIG_X2TLB
 #include <asm/pgtable_pmd.h>
 #else
 #include <asm/pgtable_nopmd.h>

commit 2a5eacca85d39d8b6dffae821d7d260f05584dc7
Author: Matt Fleming <matt@console-pimps.org>
Date:   Thu Dec 31 12:19:24 2009 +0000

    sh: Move page table allocation out of line
    
    We also switched away from quicklists and instead moved to slab
    caches. After benchmarking both implementations the difference is
    negligible. The slab caches suit us better though because the size of a
    pgd table is just 4 entries when we're using a 3-level page table layout
    and quicklists always deal with pages.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 9effcc3b0d10..78598ec33d0a 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -141,9 +141,9 @@ typedef pte_t *pte_addr_t;
 #define pte_pfn(x)		((unsigned long)(((x).pte_low >> PAGE_SHIFT)))
 
 /*
- * No page table caches to initialise
+ * Initialise the page table caches
  */
-#define pgtable_cache_init()	do { } while (0)
+extern void pgtable_cache_init(void);
 
 struct vm_area_struct;
 

commit 5d9b4b19f118abfb75e352841f7bf74580d7e427
Author: Matt Fleming <matt@console-pimps.org>
Date:   Sun Dec 13 14:38:50 2009 +0000

    sh: Definitions for 3-level page table layout
    
    If using 64-bit PTEs and 4K pages then each page table has 512 entries
    (as opposed to 1024 entries with 32-bit PTEs). Unlike MIPS, SH follows
    the convention that all structures in the page table (pgd_t, pmd_t,
    pgprot_t, etc) must be the same size. Therefore, 64-bit PTEs require
    64-bit PGD entries, etc. Using 2-levels of page tables and 64-bit PTEs
    it is only possible to map 1GB of virtual address space.
    
    In order to map all 4GB of virtual address space we need to adopt a
    3-level page table layout. This actually works out better for
    CONFIG_SUPERH32 because we only waste 2 PGD entries on the P1 and P2
    areas (which are untranslated) instead of 256.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 9a0f66c1134c..9effcc3b0d10 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -12,7 +12,11 @@
 #ifndef __ASM_SH_PGTABLE_H
 #define __ASM_SH_PGTABLE_H
 
+#ifdef CONFIG_PGTABLE_LEVELS_3
+#include <asm/pgtable_pmd.h>
+#else
 #include <asm/pgtable_nopmd.h>
+#endif
 #include <asm/page.h>
 
 #ifndef __ASSEMBLY__

commit b73c806341cfc7492ede6a2ce713cb579547d0ab
Author: Matt Fleming <matt@console-pimps.org>
Date:   Wed Nov 25 22:00:08 2009 +0000

    sh: Abstract the number of page table levels
    
    Keep the dimensions of the page tables in a separate header file in
    preparation for allowing a three level page table structure.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index ba3046e4f06f..9a0f66c1134c 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -12,7 +12,7 @@
 #ifndef __ASM_SH_PGTABLE_H
 #define __ASM_SH_PGTABLE_H
 
-#include <asm-generic/pgtable-nopmd.h>
+#include <asm/pgtable_nopmd.h>
 #include <asm/page.h>
 
 #ifndef __ASSEMBLY__
@@ -51,28 +51,12 @@ static inline unsigned long long neff_sign_extend(unsigned long val)
 #define	NPHYS_SIGN	(1LL << (NPHYS - 1))
 #define	NPHYS_MASK	(-1LL << NPHYS)
 
-/*
- * traditional two-level paging structure
- */
-/* PTE bits */
-#if defined(CONFIG_X2TLB) || defined(CONFIG_SUPERH64)
-# define PTE_MAGNITUDE	3	/* 64-bit PTEs on extended mode SH-X2 TLB */
-#else
-# define PTE_MAGNITUDE	2	/* 32-bit PTEs */
-#endif
-#define PTE_SHIFT	PAGE_SHIFT
-#define PTE_BITS	(PTE_SHIFT - PTE_MAGNITUDE)
-
-/* PGD bits */
-#define PGDIR_SHIFT	(PTE_SHIFT + PTE_BITS)
 #define PGDIR_SIZE	(1UL << PGDIR_SHIFT)
 #define PGDIR_MASK	(~(PGDIR_SIZE-1))
 
 /* Entries per level */
 #define PTRS_PER_PTE	(PAGE_SIZE / (1 << PTE_MAGNITUDE))
-#define PTRS_PER_PGD	(PAGE_SIZE / sizeof(pgd_t))
 
-#define USER_PTRS_PER_PGD	(TASK_SIZE/PGDIR_SIZE)
 #define FIRST_USER_ADDRESS	0
 
 #define PHYS_ADDR_MASK29		0x1fffffff

commit 2a8bc923455f320da6c460258c21d2235ab2edc8
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Oct 10 22:24:55 2009 +0900

    sh: Shut up CONFIG_32BIT=n compiler warnings.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 5dff5787dfeb..ba3046e4f06f 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -87,7 +87,7 @@ static inline unsigned long phys_addr_mask(void)
 
 	return PHYS_ADDR_MASK32;
 }
-#elif CONFIG_32BIT
+#elif defined(CONFIG_32BIT)
 static inline unsigned long phys_addr_mask(void)
 {
 	return PHYS_ADDR_MASK32;

commit 1f69b6af9171f50135cce8023c84d82fbf42a8f5
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:25 2009 +0000

    sh: Prepare for dynamic PMB support
    
    To allow the MMU to be switched between 29bit and 32bit mode at runtime
    some constants need to swapped for functions that return a runtime
    value.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 4f3efa7d5a64..5dff5787dfeb 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -75,13 +75,31 @@ static inline unsigned long long neff_sign_extend(unsigned long val)
 #define USER_PTRS_PER_PGD	(TASK_SIZE/PGDIR_SIZE)
 #define FIRST_USER_ADDRESS	0
 
-#ifdef CONFIG_32BIT
-#define PHYS_ADDR_MASK		0xffffffff
+#define PHYS_ADDR_MASK29		0x1fffffff
+#define PHYS_ADDR_MASK32		0xffffffff
+
+#ifdef CONFIG_PMB
+static inline unsigned long phys_addr_mask(void)
+{
+	/* Is the MMU in 29bit mode? */
+	if (__in_29bit_mode())
+		return PHYS_ADDR_MASK29;
+
+	return PHYS_ADDR_MASK32;
+}
+#elif CONFIG_32BIT
+static inline unsigned long phys_addr_mask(void)
+{
+	return PHYS_ADDR_MASK32;
+}
 #else
-#define PHYS_ADDR_MASK		0x1fffffff
+static inline unsigned long phys_addr_mask(void)
+{
+	return PHYS_ADDR_MASK29;
+}
 #endif
 
-#define PTE_PHYS_MASK		(PHYS_ADDR_MASK & PAGE_MASK)
+#define PTE_PHYS_MASK		(phys_addr_mask() & PAGE_MASK)
 #define PTE_FLAGS_MASK		(~(PTE_PHYS_MASK) << PAGE_SHIFT)
 
 #ifdef CONFIG_SUPERH32

commit 2739742c24f1a55365e71f0722bfdce8994e9c4e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 09:19:19 2009 +0900

    sh: Provide the kmap_coherent() interface generically.
    
    This plugs in kmap_coherent() for the non-SH4 cases to permit the
    pg-mmu.c bits to be used generically across all CPUs. SH-5 is still in
    the TODO state, but will move over to fixmap and the generic interface
    gradually.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 3cd7127af957..4f3efa7d5a64 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -158,12 +158,6 @@ extern void paging_init(void);
 extern void page_table_range_init(unsigned long start, unsigned long end,
 				  pgd_t *pgd);
 
-#if defined(CONFIG_MMU) && !defined(CONFIG_CPU_SH5)
-extern void kmap_coherent_init(void);
-#else
-#define kmap_coherent_init()	do { } while (0)
-#endif
-
 /* arch/sh/mm/mmap.c */
 #define HAVE_ARCH_UNMAPPED_AREA
 #define HAVE_ARCH_UNMAPPED_AREA_TOPDOWN

commit c7914834ef3b8a396b7e82ea34ac07cdcfe6f868
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Aug 4 17:14:39 2009 +0900

    sh: Tidy up NEFF-based sign extension for SH-5.
    
    This consolidates all of the NEFF-based sign extension for SH-5.
    In the future the other SH code will need to make use of this as well,
    so make it generic in preparation for more 32/64 consolidation.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 43ef3e99fdd1..3cd7127af957 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -36,6 +36,12 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 #define	NEFF_SIGN	(1LL << (NEFF - 1))
 #define	NEFF_MASK	(-1LL << NEFF)
 
+static inline unsigned long long neff_sign_extend(unsigned long val)
+{
+	unsigned long long extended = val;
+	return (extended & NEFF_SIGN) ? (extended | NEFF_MASK) : extended;
+}
+
 #ifdef CONFIG_29BIT
 #define NPHYS		29
 #else

commit 9cef7492696a416663b4edb953a4eade8517ebeb
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 29 00:12:17 2009 +0900

    sh: update_mmu_cache() consolidation.
    
    This splits out a separate __update_cache()/__update_tlb() for
    update_mmu_cache() to wrap in to. This lets us share the common
    __update_cache() bits while keeping special __update_tlb() handling
    broken out.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index ba2333216c5b..43ef3e99fdd1 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -134,8 +134,19 @@ typedef pte_t *pte_addr_t;
 #define pgtable_cache_init()	do { } while (0)
 
 struct vm_area_struct;
-extern void update_mmu_cache(struct vm_area_struct * vma,
-			     unsigned long address, pte_t pte);
+
+extern void __update_cache(struct vm_area_struct *vma,
+			   unsigned long address, pte_t pte);
+extern void __update_tlb(struct vm_area_struct *vma,
+			 unsigned long address, pte_t pte);
+
+static inline void
+update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t pte)
+{
+	__update_cache(vma, address, pte);
+	__update_tlb(vma, address, pte);
+}
+
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
 extern void paging_init(void);
 extern void page_table_range_init(unsigned long start, unsigned long end,

commit 0dfae7d5a21901b28ec0452d71be64adf5ea323e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Jul 27 21:30:17 2009 +0900

    sh: Use the now generic SH-4 clear/copy page ops for all MMU platforms.
    
    Now that the SH-4 page clear/copy ops are generic, they can be used for
    all platforms with CONFIG_MMU=y. SH-5 remains the odd one out, but it too
    will gradually be converted over to using this interface.
    
    SH-3 platforms which do not contain aliases will see no impact from this
    change, while aliasing SH-3 platforms will get the same interface as
    SH-4.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index bef3ab7fc09e..ba2333216c5b 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -141,8 +141,7 @@ extern void paging_init(void);
 extern void page_table_range_init(unsigned long start, unsigned long end,
 				  pgd_t *pgd);
 
-#if !defined(CONFIG_CACHE_OFF) && (defined(CONFIG_CPU_SH4) || \
-    defined(CONFIG_SH7705_CACHE_32KB)) && defined(CONFIG_MMU)
+#if defined(CONFIG_MMU) && !defined(CONFIG_CPU_SH5)
 extern void kmap_coherent_init(void);
 #else
 #define kmap_coherent_init()	do { } while (0)

commit dfff0fa65ab15db45acd64b3189787d37ab163cd
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Jul 27 20:53:22 2009 +0900

    sh: wire up clear_user_highpage() for sh4, convert sh7705.
    
    This wires up clear_user_highpage() on SH-4 and subsequently converts the
    SH7705 32kB cache mode over to using it. Now that the SH-4 implementation
    handles all of the dcache purging directly in the aliasing case, there is
    no need to do this in the default clear_page() implementation.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index d9f68f9c3cb3..bef3ab7fc09e 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -141,7 +141,8 @@ extern void paging_init(void);
 extern void page_table_range_init(unsigned long start, unsigned long end,
 				  pgd_t *pgd);
 
-#if !defined(CONFIG_CACHE_OFF) && defined(CONFIG_CPU_SH4) && defined(CONFIG_MMU)
+#if !defined(CONFIG_CACHE_OFF) && (defined(CONFIG_CPU_SH4) || \
+    defined(CONFIG_SH7705_CACHE_32KB)) && defined(CONFIG_MMU)
 extern void kmap_coherent_init(void);
 #else
 #define kmap_coherent_init()	do { } while (0)

commit 2277ab4a1df50e05bc732fe9488d4e902bb8399a
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 22 19:20:49 2009 +0900

    sh: Migrate from PG_mapped to PG_dcache_dirty.
    
    This inverts the delayed dcache flush a bit to be more in line with other
    platforms. At the same time this also gives us the ability to do some
    more optimizations and cleanup. Now that the update_mmu_cache() callsite
    only tests for the bit, the implementation can gradually be split out and
    made generic, rather than relying on special implementations for each of
    the peculiar CPU types.
    
    SH7705 in 32kB mode and SH-4 still need slightly different handling, but
    this is something that can remain isolated in the varying page copy/clear
    routines. On top of that, SH-X3 is dcache coherent, so there is no need
    to bother with any of these tests in the PTEAEX version of
    update_mmu_cache(), so we kill that off too.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 2a011b18090b..d9f68f9c3cb3 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -133,13 +133,6 @@ typedef pte_t *pte_addr_t;
  */
 #define pgtable_cache_init()	do { } while (0)
 
-#if !defined(CONFIG_CACHE_OFF) && (defined(CONFIG_CPU_SH4) || \
-	defined(CONFIG_SH7705_CACHE_32KB))
-struct mm_struct;
-#define __HAVE_ARCH_PTEP_GET_AND_CLEAR
-pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr, pte_t *ptep);
-#endif
-
 struct vm_area_struct;
 extern void update_mmu_cache(struct vm_area_struct * vma,
 			     unsigned long address, pte_t pte);

commit ee1acbfabd5270b40ce2cfdc202070b7ca91cdff
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu May 7 16:38:16 2009 +0900

    sh: Handle shm_align_mask also for HAVE_ARCH_UNMAPPED_AREA_TOPDOWN.
    
    Presently shm_align_mask is only looked at for the bottom up case, but we
    still want this for proper colouring constraints in the topdown case.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index b517ae08b9c0..2a011b18090b 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -154,6 +154,10 @@ extern void kmap_coherent_init(void);
 #define kmap_coherent_init()	do { } while (0)
 #endif
 
+/* arch/sh/mm/mmap.c */
+#define HAVE_ARCH_UNMAPPED_AREA
+#define HAVE_ARCH_UNMAPPED_AREA_TOPDOWN
+
 #include <asm-generic/pgtable.h>
 
 #endif /* __ASM_SH_PGTABLE_H */

commit acca4f4d9bd657e8bc7e1665ba5077465138f133
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Nov 10 20:00:45 2008 +0900

    sh: Handle fixmap TLB eviction more coherently.
    
    There was a race in the kmap_coherent() implementation. While we
    guarded against preemption, there was nothing preventing eviction of
    the pre-faulted fixmap entry from the UTLB. Under certain workloads
    this would result in the fixmap entries used for cache colouring being
    evicted from the UTLB in the midst of a copy_page().
    
    In addition to pre-faulting, we also make sure to preserve the PTEs
    in the kernel page table and introduce a cached PTE for kmap_coherent()
    usage. This follows a similar change on MIPS ("[MIPS] Fix aliasing bug
    in copy_to_user_page / copy_from_user_page").
    
    Reported-by: Hideo Saito <saito@densan.co.jp>
    Reported-by: CHIKAMA Masaki <masaki.chikama@gmail.com>
    Tested-by: Yoshihiro Shimoda <shimoda.yoshihiro@renesas.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index 52220d70a096..b517ae08b9c0 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -148,6 +148,12 @@ extern void paging_init(void);
 extern void page_table_range_init(unsigned long start, unsigned long end,
 				  pgd_t *pgd);
 
+#if !defined(CONFIG_CACHE_OFF) && defined(CONFIG_CPU_SH4) && defined(CONFIG_MMU)
+extern void kmap_coherent_init(void);
+#else
+#define kmap_coherent_init()	do { } while (0)
+#endif
+
 #include <asm-generic/pgtable.h>
 
 #endif /* __ASM_SH_PGTABLE_H */

commit cb700aa4f13d38726defab3060d3ebeaf67dc189
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Sep 12 20:41:05 2008 +0900

    sh: ioremap_prot support.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
index a4a8f8b93463..52220d70a096 100644
--- a/arch/sh/include/asm/pgtable.h
+++ b/arch/sh/include/asm/pgtable.h
@@ -76,6 +76,7 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 #endif
 
 #define PTE_PHYS_MASK		(PHYS_ADDR_MASK & PAGE_MASK)
+#define PTE_FLAGS_MASK		(~(PTE_PHYS_MASK) << PAGE_SHIFT)
 
 #ifdef CONFIG_SUPERH32
 #define VMALLOC_START	(P3SEG)

commit f15cbe6f1a4b4d9df59142fc8e4abb973302cf44
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jul 29 08:09:44 2008 +0900

    sh: migrate to arch/sh/include/
    
    This follows the sparc changes a439fe51a1f8eb087c22dd24d69cebae4a3addac.
    
    Most of the moving about was done with Sam's directions at:
    
    http://marc.info/?l=linux-sh&m=121724823706062&w=2
    
    with subsequent hacking and fixups entirely my fault.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/pgtable.h b/arch/sh/include/asm/pgtable.h
new file mode 100644
index 000000000000..a4a8f8b93463
--- /dev/null
+++ b/arch/sh/include/asm/pgtable.h
@@ -0,0 +1,152 @@
+/*
+ * This file contains the functions and defines necessary to modify and
+ * use the SuperH page table tree.
+ *
+ * Copyright (C) 1999 Niibe Yutaka
+ * Copyright (C) 2002 - 2007 Paul Mundt
+ *
+ * This file is subject to the terms and conditions of the GNU General
+ * Public License.  See the file "COPYING" in the main directory of this
+ * archive for more details.
+ */
+#ifndef __ASM_SH_PGTABLE_H
+#define __ASM_SH_PGTABLE_H
+
+#include <asm-generic/pgtable-nopmd.h>
+#include <asm/page.h>
+
+#ifndef __ASSEMBLY__
+#include <asm/addrspace.h>
+#include <asm/fixmap.h>
+
+/*
+ * ZERO_PAGE is a global shared page that is always zero: used
+ * for zero-mapped memory areas etc..
+ */
+extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
+#define ZERO_PAGE(vaddr) (virt_to_page(empty_zero_page))
+
+#endif /* !__ASSEMBLY__ */
+
+/*
+ * Effective and physical address definitions, to aid with sign
+ * extension.
+ */
+#define NEFF		32
+#define	NEFF_SIGN	(1LL << (NEFF - 1))
+#define	NEFF_MASK	(-1LL << NEFF)
+
+#ifdef CONFIG_29BIT
+#define NPHYS		29
+#else
+#define NPHYS		32
+#endif
+
+#define	NPHYS_SIGN	(1LL << (NPHYS - 1))
+#define	NPHYS_MASK	(-1LL << NPHYS)
+
+/*
+ * traditional two-level paging structure
+ */
+/* PTE bits */
+#if defined(CONFIG_X2TLB) || defined(CONFIG_SUPERH64)
+# define PTE_MAGNITUDE	3	/* 64-bit PTEs on extended mode SH-X2 TLB */
+#else
+# define PTE_MAGNITUDE	2	/* 32-bit PTEs */
+#endif
+#define PTE_SHIFT	PAGE_SHIFT
+#define PTE_BITS	(PTE_SHIFT - PTE_MAGNITUDE)
+
+/* PGD bits */
+#define PGDIR_SHIFT	(PTE_SHIFT + PTE_BITS)
+#define PGDIR_SIZE	(1UL << PGDIR_SHIFT)
+#define PGDIR_MASK	(~(PGDIR_SIZE-1))
+
+/* Entries per level */
+#define PTRS_PER_PTE	(PAGE_SIZE / (1 << PTE_MAGNITUDE))
+#define PTRS_PER_PGD	(PAGE_SIZE / sizeof(pgd_t))
+
+#define USER_PTRS_PER_PGD	(TASK_SIZE/PGDIR_SIZE)
+#define FIRST_USER_ADDRESS	0
+
+#ifdef CONFIG_32BIT
+#define PHYS_ADDR_MASK		0xffffffff
+#else
+#define PHYS_ADDR_MASK		0x1fffffff
+#endif
+
+#define PTE_PHYS_MASK		(PHYS_ADDR_MASK & PAGE_MASK)
+
+#ifdef CONFIG_SUPERH32
+#define VMALLOC_START	(P3SEG)
+#else
+#define VMALLOC_START	(0xf0000000)
+#endif
+#define VMALLOC_END	(FIXADDR_START-2*PAGE_SIZE)
+
+#if defined(CONFIG_SUPERH32)
+#include <asm/pgtable_32.h>
+#else
+#include <asm/pgtable_64.h>
+#endif
+
+/*
+ * SH-X and lower (legacy) SuperH parts (SH-3, SH-4, some SH-4A) can't do page
+ * protection for execute, and considers it the same as a read. Also, write
+ * permission implies read permission. This is the closest we can get..
+ *
+ * SH-X2 (SH7785) and later parts take this to the opposite end of the extreme,
+ * not only supporting separate execute, read, and write bits, but having
+ * completely separate permission bits for user and kernel space.
+ */
+	 /*xwr*/
+#define __P000	PAGE_NONE
+#define __P001	PAGE_READONLY
+#define __P010	PAGE_COPY
+#define __P011	PAGE_COPY
+#define __P100	PAGE_EXECREAD
+#define __P101	PAGE_EXECREAD
+#define __P110	PAGE_COPY
+#define __P111	PAGE_COPY
+
+#define __S000	PAGE_NONE
+#define __S001	PAGE_READONLY
+#define __S010	PAGE_WRITEONLY
+#define __S011	PAGE_SHARED
+#define __S100	PAGE_EXECREAD
+#define __S101	PAGE_EXECREAD
+#define __S110	PAGE_RWX
+#define __S111	PAGE_RWX
+
+typedef pte_t *pte_addr_t;
+
+#define kern_addr_valid(addr)	(1)
+
+#define io_remap_pfn_range(vma, vaddr, pfn, size, prot)		\
+		remap_pfn_range(vma, vaddr, pfn, size, prot)
+
+#define pte_pfn(x)		((unsigned long)(((x).pte_low >> PAGE_SHIFT)))
+
+/*
+ * No page table caches to initialise
+ */
+#define pgtable_cache_init()	do { } while (0)
+
+#if !defined(CONFIG_CACHE_OFF) && (defined(CONFIG_CPU_SH4) || \
+	defined(CONFIG_SH7705_CACHE_32KB))
+struct mm_struct;
+#define __HAVE_ARCH_PTEP_GET_AND_CLEAR
+pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr, pte_t *ptep);
+#endif
+
+struct vm_area_struct;
+extern void update_mmu_cache(struct vm_area_struct * vma,
+			     unsigned long address, pte_t pte);
+extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
+extern void paging_init(void);
+extern void page_table_range_init(unsigned long start, unsigned long end,
+				  pgd_t *pgd);
+
+#include <asm-generic/pgtable.h>
+
+#endif /* __ASM_SH_PGTABLE_H */
