commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index 56e4418c19b9..172e329fd92d 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef __MMU_H
 #define __MMU_H
 

commit 089b43f9737f2e51c6ce354749f5a9f3f093601c
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Mar 10 16:29:48 2010 +0900

    sh: Fix up NUMA build for 29-bit.
    
    pmb_bolt_mapping() is undefined on 29-bit builds, so provide a stub.
    This fixes up the NUMA build on platforms lacking PMB support.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index 19fe84550b49..56e4418c19b9 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -66,6 +66,13 @@ int pmb_unmap(void __iomem *addr);
 
 #else
 
+static inline int
+pmb_bolt_mapping(unsigned long virt, phys_addr_t phys,
+		 unsigned long size, pgprot_t prot)
+{
+	return -EINVAL;
+}
+
 static inline void __iomem *
 pmb_remap_caller(phys_addr_t phys, unsigned long size,
 		 pgprot_t prot, void *caller)

commit 90e7d649d86f21d478dc134f74c88e19dd472393
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Feb 23 16:20:53 2010 +0900

    sh: reworked dynamic PMB mapping.
    
    This implements a fairly significant overhaul of the dynamic PMB mapping
    code. The primary change here is that the PMB gets its own VMA that
    follows the uncached mapping and we attempt to be a bit more intelligent
    with dynamic sizing, multi-entry mapping, and so forth.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index 15a05b615ba7..19fe84550b49 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -55,19 +55,29 @@ typedef struct {
 
 #ifdef CONFIG_PMB
 /* arch/sh/mm/pmb.c */
-long pmb_remap(unsigned long virt, unsigned long phys,
-	       unsigned long size, pgprot_t prot);
-void pmb_unmap(unsigned long addr);
-void pmb_init(void);
 bool __in_29bit_mode(void);
+
+void pmb_init(void);
+int pmb_bolt_mapping(unsigned long virt, phys_addr_t phys,
+		     unsigned long size, pgprot_t prot);
+void __iomem *pmb_remap_caller(phys_addr_t phys, unsigned long size,
+			       pgprot_t prot, void *caller);
+int pmb_unmap(void __iomem *addr);
+
 #else
-static inline long pmb_remap(unsigned long virt, unsigned long phys,
-			     unsigned long size, pgprot_t prot)
+
+static inline void __iomem *
+pmb_remap_caller(phys_addr_t phys, unsigned long size,
+		 pgprot_t prot, void *caller)
+{
+	return NULL;
+}
+
+static inline int pmb_unmap(void __iomem *addr)
 {
 	return -EINVAL;
 }
 
-#define pmb_unmap(addr)		do { } while (0)
 #define pmb_init(addr)		do { } while (0)
 
 #ifdef CONFIG_29BIT
@@ -77,6 +87,13 @@ static inline long pmb_remap(unsigned long virt, unsigned long phys,
 #endif
 
 #endif /* CONFIG_PMB */
+
+static inline void __iomem *
+pmb_remap(phys_addr_t phys, unsigned long size, pgprot_t prot)
+{
+	return pmb_remap_caller(phys, size, prot, __builtin_return_address(0));
+}
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* __MMU_H */

commit d01447b3197c2c470a14666be2c640407bbbfec7
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Feb 18 18:13:51 2010 +0900

    sh: Merge legacy and dynamic PMB modes.
    
    This implements a bit of rework for the PMB code, which permits us to
    kill off the legacy PMB mode completely. Rather than trusting the boot
    loader to do the right thing, we do a quick verification of the PMB
    contents to determine whether to have the kernel setup the initial
    mappings or whether it needs to mangle them later on instead.
    
    If we're booting from legacy mappings, the kernel will now take control
    of them and make them match the kernel's initial mapping configuration.
    This is accomplished by breaking the initialization phase out in to
    multiple steps: synchronization, merging, and resizing. With the recent
    rework, the synchronization code establishes page links for compound
    mappings already, so we build on top of this for promoting mappings and
    reclaiming unused slots.
    
    At the same time, the changes introduced for the uncached helpers also
    permit us to dynamically resize the uncached mapping without any
    particular headaches. The smallest page size is more than sufficient for
    mapping all of kernel text, and as we're careful not to jump to any far
    off locations in the setup code the mapping can safely be resized
    regardless of whether we are executing from it or not.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index e42c4e2a41df..15a05b615ba7 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -58,7 +58,7 @@ typedef struct {
 long pmb_remap(unsigned long virt, unsigned long phys,
 	       unsigned long size, pgprot_t prot);
 void pmb_unmap(unsigned long addr);
-int pmb_init(void);
+void pmb_init(void);
 bool __in_29bit_mode(void);
 #else
 static inline long pmb_remap(unsigned long virt, unsigned long phys,
@@ -67,14 +67,8 @@ static inline long pmb_remap(unsigned long virt, unsigned long phys,
 	return -EINVAL;
 }
 
-static inline void pmb_unmap(unsigned long addr)
-{
-}
-
-static inline int pmb_init(void)
-{
-	return -ENODEV;
-}
+#define pmb_unmap(addr)		do { } while (0)
+#define pmb_init(addr)		do { } while (0)
 
 #ifdef CONFIG_29BIT
 #define __in_29bit_mode()	(1)

commit d53a0d33bc3a50ea0e8dd1680a2e8435770b162a
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Feb 17 21:17:02 2010 +0900

    sh: PMB locking overhaul.
    
    This implements some locking for the PMB code. A high level rwlock is
    added for dealing with rw accesses on the entry map while a per-entry
    data structure spinlock is added to deal with the PMB entry changing out
    from underneath us.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index 5453169bf052..e42c4e2a41df 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -53,24 +53,6 @@ typedef struct {
 #endif
 } mm_context_t;
 
-struct pmb_entry;
-
-struct pmb_entry {
-	unsigned long vpn;
-	unsigned long ppn;
-	unsigned long flags;
-	unsigned long size;
-
-	/*
-	 * 0 .. NR_PMB_ENTRIES for specific entry selection, or
-	 * PMB_NO_ENTRY to search for a free one
-	 */
-	int entry;
-
-	/* Adjacent entry link for contiguous multi-entry mappings */
-	struct pmb_entry *link;
-};
-
 #ifdef CONFIG_PMB
 /* arch/sh/mm/pmb.c */
 long pmb_remap(unsigned long virt, unsigned long phys,

commit d7813bc9e8e384f5a293b05c095c799d41af3668
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Feb 17 17:56:38 2010 +0900

    sh: Build PMB entry links for existing contiguous multi-page mappings.
    
    This plugs in entry sizing support for existing mappings and then builds
    on top of that for linking together entries that are mapping contiguous
    areas. This will ultimately permit us to coalesce mappings and promote
    head pages while reclaiming PMB slots for dynamic remapping.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index 44c904341414..5453169bf052 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -59,6 +59,7 @@ struct pmb_entry {
 	unsigned long vpn;
 	unsigned long ppn;
 	unsigned long flags;
+	unsigned long size;
 
 	/*
 	 * 0 .. NR_PMB_ENTRIES for specific entry selection, or
@@ -66,7 +67,6 @@ struct pmb_entry {
 	 */
 	int entry;
 
-	struct pmb_entry *next;
 	/* Adjacent entry link for contiguous multi-entry mappings */
 	struct pmb_entry *link;
 };

commit 51becfd96287b3913b13075699433730984e2f4f
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Feb 17 15:33:30 2010 +0900

    sh: PMB tidying.
    
    Some overdue cleanup of the PMB code, killing off unused functionality
    and duplication sprinkled about the tree.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index 151bc922701b..44c904341414 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -11,7 +11,9 @@
 
 #define PMB_ADDR		0xf6100000
 #define PMB_DATA		0xf7100000
-#define PMB_ENTRY_MAX		16
+
+#define NR_PMB_ENTRIES		16
+
 #define PMB_E_MASK		0x0000000f
 #define PMB_E_SHIFT		8
 

commit 7bdda6209f224aa784a036df54b22cb338d2e859
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Feb 17 13:23:00 2010 +0900

    sh: Fix up more 64-bit pgprot truncation on SH-X2 TLB.
    
    Both the store queue API and the PMB remapping take unsigned long for
    their pgprot flags, which cuts off the extended protection bits. In the
    case of the PMB this isn't really a problem since the cache attribute
    bits that we care about are all in the lower 32-bits, but we do it just
    to be safe. The store queue remapping on the other hand depends on the
    extended prot bits for enabling userspace access to the mappings.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index 2fcbedb55002..151bc922701b 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -33,6 +33,7 @@
 #ifndef __ASSEMBLY__
 #include <linux/errno.h>
 #include <linux/threads.h>
+#include <asm/page.h>
 
 /* Default "unsigned long" context */
 typedef unsigned long mm_context_id_t[NR_CPUS];
@@ -71,13 +72,13 @@ struct pmb_entry {
 #ifdef CONFIG_PMB
 /* arch/sh/mm/pmb.c */
 long pmb_remap(unsigned long virt, unsigned long phys,
-	       unsigned long size, unsigned long flags);
+	       unsigned long size, pgprot_t prot);
 void pmb_unmap(unsigned long addr);
 int pmb_init(void);
 bool __in_29bit_mode(void);
 #else
 static inline long pmb_remap(unsigned long virt, unsigned long phys,
-			     unsigned long size, unsigned long flags)
+			     unsigned long size, pgprot_t prot)
 {
 	return -EINVAL;
 }

commit efd54ea315f645ef318708aab5714a5f1f432d03
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Feb 16 18:39:30 2010 +0900

    sh: Merge the legacy PMB mapping and entry synchronization code.
    
    This merges the code for iterating over the legacy PMB mappings and the
    code for synchronizing software state with the hardware mappings. There's
    really no reason to do the same iteration twice, and this also buys us
    the legacy entry logging facility for the dynamic PMB case.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index ca7d91e8aa72..2fcbedb55002 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -25,6 +25,7 @@
 #define PMB_C			0x00000008
 #define PMB_WT			0x00000001
 #define PMB_UB			0x00000200
+#define PMB_CACHE_MASK		(PMB_C | PMB_WT | PMB_UB)
 #define PMB_V			0x00000100
 
 #define PMB_NO_ENTRY		(-1)

commit 2efa53b269ec1e9289a108e1506f53f6f1de440b
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jan 20 16:40:48 2010 +0900

    sh: Make 29/32-bit mode check helper generally available.
    
    Presently __in_29bit_mode() is only defined for the PMB case, but
    it's also easily derived from the CONFIG_29BIT and CONFIG_32BIT &&
    CONFIG_PMB=n cases.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index e5e8f48830ef..ca7d91e8aa72 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -31,6 +31,7 @@
 
 #ifndef __ASSEMBLY__
 #include <linux/errno.h>
+#include <linux/threads.h>
 
 /* Default "unsigned long" context */
 typedef unsigned long mm_context_id_t[NR_CPUS];
@@ -72,6 +73,7 @@ long pmb_remap(unsigned long virt, unsigned long phys,
 	       unsigned long size, unsigned long flags);
 void pmb_unmap(unsigned long addr);
 int pmb_init(void);
+bool __in_29bit_mode(void);
 #else
 static inline long pmb_remap(unsigned long virt, unsigned long phys,
 			     unsigned long size, unsigned long flags)
@@ -87,8 +89,14 @@ static inline int pmb_init(void)
 {
 	return -ENODEV;
 }
-#endif /* CONFIG_PMB */
 
+#ifdef CONFIG_29BIT
+#define __in_29bit_mode()	(1)
+#else
+#define __in_29bit_mode()	(0)
+#endif
+
+#endif /* CONFIG_PMB */
 #endif /* __ASSEMBLY__ */
 
 #endif /* __MMU_H */

commit 46c4e5daea3d5df06e27bf5a49a0c42274db6725
Author: Matt Fleming <matt@console-pimps.org>
Date:   Fri Jan 15 08:00:45 2010 +0900

    sh: Fix CONFIG_PMB=n build.
    
    The last commit introduced the following breakage
    
    arch/sh/include/asm/mmu.h: In function 'pmb_remap':
    arch/sh/include/asm/mmu.h:79: error: expected ';' before '}' token
    
    and...
    
    arch/sh/include/asm/mmu.h:78: error: 'EINVAL' undeclared (first use in this function)
    arch/sh/include/asm/mmu.h:78: error: (Each undeclared identifier is reported only once
    arch/sh/include/asm/mmu.h:78: error: for each function it appears in.)
    arch/sh/include/asm/mmu.h: In function 'pmb_init':
    arch/sh/include/asm/mmu.h:87: error: 'ENODEV' undeclared (first use in this function)
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index 4b0882bf5183..e5e8f48830ef 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -30,6 +30,7 @@
 #define PMB_NO_ENTRY		(-1)
 
 #ifndef __ASSEMBLY__
+#include <linux/errno.h>
 
 /* Default "unsigned long" context */
 typedef unsigned long mm_context_id_t[NR_CPUS];
@@ -75,7 +76,7 @@ int pmb_init(void);
 static inline long pmb_remap(unsigned long virt, unsigned long phys,
 			     unsigned long size, unsigned long flags)
 {
-	return -EINVAL
+	return -EINVAL;
 }
 
 static inline void pmb_unmap(unsigned long addr)

commit a0ab36689a36e583b6e736f1c99ac8c9aebdad59
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jan 13 18:31:48 2010 +0900

    sh: fixed PMB mode refactoring.
    
    This introduces some much overdue chainsawing of the fixed PMB support.
    fixed PMB was introduced initially to work around the fact that dynamic
    PMB mode was relatively broken, though they were never intended to
    converge. The main areas where there are differences are whether the
    system is booted in 29-bit mode or 32-bit mode, and whether legacy
    mappings are to be preserved. Any system booting in true 32-bit mode will
    not care about legacy mappings, so these are roughly decoupled.
    
    Regardless of the entry point, PMB and 32BIT are directly related as far
    as the kernel is concerned, so we also switch back to having one select
    the other.
    
    With legacy mappings iterated through and applied in the initialization
    path it's now possible to finally merge the two implementations and
    permit dynamic remapping overtop of remaining entries regardless of
    whether boot mappings are crafted by hand or inherited from the boot
    loader.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index c7426ad9926e..4b0882bf5183 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -65,11 +65,29 @@ struct pmb_entry {
 	struct pmb_entry *link;
 };
 
+#ifdef CONFIG_PMB
 /* arch/sh/mm/pmb.c */
 long pmb_remap(unsigned long virt, unsigned long phys,
 	       unsigned long size, unsigned long flags);
 void pmb_unmap(unsigned long addr);
 int pmb_init(void);
+#else
+static inline long pmb_remap(unsigned long virt, unsigned long phys,
+			     unsigned long size, unsigned long flags)
+{
+	return -EINVAL
+}
+
+static inline void pmb_unmap(unsigned long addr)
+{
+}
+
+static inline int pmb_init(void)
+{
+	return -ENODEV;
+}
+#endif /* CONFIG_PMB */
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* __MMU_H */

commit 20b5014b3e5fe7b874a3f6a1dc03b0c21cb222cd
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:33 2009 +0000

    sh: Fold fixed-PMB support into dynamic PMB support
    
    The initialisation process differs for CONFIG_PMB and for
    CONFIG_PMB_FIXED. For CONFIG_PMB_FIXED we need to register the PMB
    entries that were allocated by the bootloader.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index 9c84b4546c8d..c7426ad9926e 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -15,6 +15,8 @@
 #define PMB_E_MASK		0x0000000f
 #define PMB_E_SHIFT		8
 
+#define PMB_PFN_MASK		0xff000000
+
 #define PMB_SZ_16M		0x00000000
 #define PMB_SZ_64M		0x00000010
 #define PMB_SZ_128M		0x00000080

commit 8386aebb9e15a94137693ea4f4df84207f71cc75
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:28 2009 +0000

    sh: Make most PMB functions static
    
    There's no need to export the internal PMB functions for allocating,
    freeing and modifying PMB entries, etc. This way we can restrict the
    interface for PMB.
    
    Also remove the static from pmb_init() so that we have more freedom in
    setting up the initial PMB entries and turning on MMU 32bit mode.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index 5025e12b7864..9c84b4546c8d 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -64,16 +64,10 @@ struct pmb_entry {
 };
 
 /* arch/sh/mm/pmb.c */
-int __set_pmb_entry(unsigned long vpn, unsigned long ppn,
-		    unsigned long flags, int *entry);
-int set_pmb_entry(struct pmb_entry *pmbe);
-void clear_pmb_entry(struct pmb_entry *pmbe);
-struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
-			    unsigned long flags);
-void pmb_free(struct pmb_entry *pmbe);
 long pmb_remap(unsigned long virt, unsigned long phys,
 	       unsigned long size, unsigned long flags);
 void pmb_unmap(unsigned long addr);
+int pmb_init(void);
 #endif /* __ASSEMBLY__ */
 
 #endif /* __MMU_H */

commit 1f69b6af9171f50135cce8023c84d82fbf42a8f5
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:25 2009 +0000

    sh: Prepare for dynamic PMB support
    
    To allow the MMU to be switched between 29bit and 32bit mode at runtime
    some constants need to swapped for functions that return a runtime
    value.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index f5963037c9d6..5025e12b7864 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -7,6 +7,8 @@
 #define PMB_PASCR		0xff000070
 #define PMB_IRMCR		0xff000078
 
+#define PASCR_SE		0x80000000
+
 #define PMB_ADDR		0xf6100000
 #define PMB_DATA		0xf7100000
 #define PMB_ENTRY_MAX		16
@@ -75,4 +77,3 @@ void pmb_unmap(unsigned long addr);
 #endif /* __ASSEMBLY__ */
 
 #endif /* __MMU_H */
-

commit 3b4df71b364e230fb7b02356d1f8fce64838ddc7
Author: Francesco VIRLINZI <francesco.virlinzi@st.com>
Date:   Tue Mar 24 13:30:01 2009 +0000

    sh: Sanitize asm/mmu.h for assembly use.
    
    This patch adds the ifndef __ASSEMBLY__ preprocessor to allow the
    defines in the file to be used also in assembly code.
    
    Signed-off-by: Francesco Virlinzi <francesco.virlinzi@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index 6c43625bb1a5..f5963037c9d6 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -1,22 +1,6 @@
 #ifndef __MMU_H
 #define __MMU_H
 
-/* Default "unsigned long" context */
-typedef unsigned long mm_context_id_t[NR_CPUS];
-
-typedef struct {
-#ifdef CONFIG_MMU
-	mm_context_id_t		id;
-	void			*vdso;
-#else
-	unsigned long		end_brk;
-#endif
-#ifdef CONFIG_BINFMT_ELF_FDPIC
-	unsigned long		exec_fdpic_loadmap;
-	unsigned long		interp_fdpic_loadmap;
-#endif
-} mm_context_t;
-
 /*
  * Privileged Space Mapping Buffer (PMB) definitions
  */
@@ -41,6 +25,24 @@ typedef struct {
 
 #define PMB_NO_ENTRY		(-1)
 
+#ifndef __ASSEMBLY__
+
+/* Default "unsigned long" context */
+typedef unsigned long mm_context_id_t[NR_CPUS];
+
+typedef struct {
+#ifdef CONFIG_MMU
+	mm_context_id_t		id;
+	void			*vdso;
+#else
+	unsigned long		end_brk;
+#endif
+#ifdef CONFIG_BINFMT_ELF_FDPIC
+	unsigned long		exec_fdpic_loadmap;
+	unsigned long		interp_fdpic_loadmap;
+#endif
+} mm_context_t;
+
 struct pmb_entry;
 
 struct pmb_entry {
@@ -70,6 +72,7 @@ void pmb_free(struct pmb_entry *pmbe);
 long pmb_remap(unsigned long virt, unsigned long phys,
 	       unsigned long size, unsigned long flags);
 void pmb_unmap(unsigned long addr);
+#endif /* __ASSEMBLY__ */
 
 #endif /* __MMU_H */
 

commit 8feae13110d60cc6287afabc2887366b0eb226c2
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jan 8 12:04:47 2009 +0000

    NOMMU: Make VMAs per MM as for MMU-mode linux
    
    Make VMAs per mm_struct as for MMU-mode linux.  This solves two problems:
    
     (1) In SYSV SHM where nattch for a segment does not reflect the number of
         shmat's (and forks) done.
    
     (2) In mmap() where the VMA's vm_mm is set to point to the parent mm by an
         exec'ing process when VM_EXECUTABLE is specified, regardless of the fact
         that a VMA might be shared and already have its vm_mm assigned to another
         process or a dead process.
    
    A new struct (vm_region) is introduced to track a mapped region and to remember
    the circumstances under which it may be shared and the vm_list_struct structure
    is discarded as it's no longer required.
    
    This patch makes the following additional changes:
    
     (1) Regions are now allocated with alloc_pages() rather than kmalloc() and
         with no recourse to __GFP_COMP, so the pages are not composite.  Instead,
         each page has a reference on it held by the region.  Anything else that is
         interested in such a page will have to get a reference on it to retain it.
         When the pages are released due to unmapping, each page is passed to
         put_page() and will be freed when the page usage count reaches zero.
    
     (2) Excess pages are trimmed after an allocation as the allocation must be
         made as a power-of-2 quantity of pages.
    
     (3) VMAs are added to the parent MM's R/B tree and mmap lists.  As an MM may
         end up with overlapping VMAs within the tree, the VMA struct address is
         appended to the sort key.
    
     (4) Non-anonymous VMAs are now added to the backing inode's prio list.
    
     (5) Holes may be punched in anonymous VMAs with munmap(), releasing parts of
         the backing region.  The VMA and region structs will be split if
         necessary.
    
     (6) sys_shmdt() only releases one attachment to a SYSV IPC shared memory
         segment instead of all the attachments at that addresss.  Multiple
         shmat()'s return the same address under NOMMU-mode instead of different
         virtual addresses as under MMU-mode.
    
     (7) Core dumping for ELF-FDPIC requires fewer exceptions for NOMMU-mode.
    
     (8) /proc/maps is now the global list of mapped regions, and may list bits
         that aren't actually mapped anywhere.
    
     (9) /proc/meminfo gains a line (tagged "MmapCopy") that indicates the amount
         of RAM currently allocated by mmap to hold mappable regions that can't be
         mapped directly.  These are copies of the backing device or file if not
         anonymous.
    
    These changes make NOMMU mode more similar to MMU mode.  The downside is that
    NOMMU mode requires some extra memory to track things over NOMMU without this
    patch (VMAs are no longer shared, and there are now region structs).
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Mike Frysinger <vapier.adi@gmail.com>
    Acked-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
index fdcb93bc6d11..6c43625bb1a5 100644
--- a/arch/sh/include/asm/mmu.h
+++ b/arch/sh/include/asm/mmu.h
@@ -9,7 +9,6 @@ typedef struct {
 	mm_context_id_t		id;
 	void			*vdso;
 #else
-	struct vm_list_struct	*vmlist;
 	unsigned long		end_brk;
 #endif
 #ifdef CONFIG_BINFMT_ELF_FDPIC

commit f15cbe6f1a4b4d9df59142fc8e4abb973302cf44
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jul 29 08:09:44 2008 +0900

    sh: migrate to arch/sh/include/
    
    This follows the sparc changes a439fe51a1f8eb087c22dd24d69cebae4a3addac.
    
    Most of the moving about was done with Sam's directions at:
    
    http://marc.info/?l=linux-sh&m=121724823706062&w=2
    
    with subsequent hacking and fixups entirely my fault.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/mmu.h b/arch/sh/include/asm/mmu.h
new file mode 100644
index 000000000000..fdcb93bc6d11
--- /dev/null
+++ b/arch/sh/include/asm/mmu.h
@@ -0,0 +1,76 @@
+#ifndef __MMU_H
+#define __MMU_H
+
+/* Default "unsigned long" context */
+typedef unsigned long mm_context_id_t[NR_CPUS];
+
+typedef struct {
+#ifdef CONFIG_MMU
+	mm_context_id_t		id;
+	void			*vdso;
+#else
+	struct vm_list_struct	*vmlist;
+	unsigned long		end_brk;
+#endif
+#ifdef CONFIG_BINFMT_ELF_FDPIC
+	unsigned long		exec_fdpic_loadmap;
+	unsigned long		interp_fdpic_loadmap;
+#endif
+} mm_context_t;
+
+/*
+ * Privileged Space Mapping Buffer (PMB) definitions
+ */
+#define PMB_PASCR		0xff000070
+#define PMB_IRMCR		0xff000078
+
+#define PMB_ADDR		0xf6100000
+#define PMB_DATA		0xf7100000
+#define PMB_ENTRY_MAX		16
+#define PMB_E_MASK		0x0000000f
+#define PMB_E_SHIFT		8
+
+#define PMB_SZ_16M		0x00000000
+#define PMB_SZ_64M		0x00000010
+#define PMB_SZ_128M		0x00000080
+#define PMB_SZ_512M		0x00000090
+#define PMB_SZ_MASK		PMB_SZ_512M
+#define PMB_C			0x00000008
+#define PMB_WT			0x00000001
+#define PMB_UB			0x00000200
+#define PMB_V			0x00000100
+
+#define PMB_NO_ENTRY		(-1)
+
+struct pmb_entry;
+
+struct pmb_entry {
+	unsigned long vpn;
+	unsigned long ppn;
+	unsigned long flags;
+
+	/*
+	 * 0 .. NR_PMB_ENTRIES for specific entry selection, or
+	 * PMB_NO_ENTRY to search for a free one
+	 */
+	int entry;
+
+	struct pmb_entry *next;
+	/* Adjacent entry link for contiguous multi-entry mappings */
+	struct pmb_entry *link;
+};
+
+/* arch/sh/mm/pmb.c */
+int __set_pmb_entry(unsigned long vpn, unsigned long ppn,
+		    unsigned long flags, int *entry);
+int set_pmb_entry(struct pmb_entry *pmbe);
+void clear_pmb_entry(struct pmb_entry *pmbe);
+struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
+			    unsigned long flags);
+void pmb_free(struct pmb_entry *pmbe);
+long pmb_remap(unsigned long virt, unsigned long phys,
+	       unsigned long size, unsigned long flags);
+void pmb_unmap(unsigned long addr);
+
+#endif /* __MMU_H */
+
