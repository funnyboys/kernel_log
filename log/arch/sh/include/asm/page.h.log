commit 37744feebc086908fd89760650f458ab19071750
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Apr 20 11:37:12 2020 +0200

    sh: remove sh5 support
    
    sh5 never became a product and has probably never really worked.
    
    Remove it by recursively deleting all associated Kconfig options
    and all corresponding files.
    
    Reviewed-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Rich Felker <dalias@libc.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index ea8d68f58e39..eca5daa43b93 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -35,8 +35,6 @@
 #define HPAGE_SHIFT	22
 #elif defined(CONFIG_HUGETLB_PAGE_SIZE_64MB)
 #define HPAGE_SHIFT	26
-#elif defined(CONFIG_HUGETLB_PAGE_SIZE_512MB)
-#define HPAGE_SHIFT	29
 #endif
 
 #ifdef CONFIG_HUGETLB_PAGE
@@ -82,18 +80,12 @@ typedef struct { unsigned long long pgd; } pgd_t;
 	((x).pte_low | ((unsigned long long)(x).pte_high << 32))
 #define __pte(x) \
 	({ pte_t __pte = {(x), ((unsigned long long)(x)) >> 32}; __pte; })
-#elif defined(CONFIG_SUPERH32)
+#else
 typedef struct { unsigned long pte_low; } pte_t;
 typedef struct { unsigned long pgprot; } pgprot_t;
 typedef struct { unsigned long pgd; } pgd_t;
 #define pte_val(x)	((x).pte_low)
 #define __pte(x)	((pte_t) { (x) } )
-#else
-typedef struct { unsigned long long pte_low; } pte_t;
-typedef struct { unsigned long long pgprot; } pgprot_t;
-typedef struct { unsigned long pgd; } pgd_t;
-#define pte_val(x)	((x).pte_low)
-#define __pte(x)	((pte_t) { (x) } )
 #endif
 
 #define pgd_val(x)	((x).pgd)
@@ -191,15 +183,4 @@ typedef struct page *pgtable_t;
  */
 #define ARCH_DMA_MINALIGN	L1_CACHE_BYTES
 
-#ifdef CONFIG_SUPERH64
-/*
- * While BYTES_PER_WORD == 4 on the current sh64 ABI, GCC will still
- * happily generate {ld/st}.q pairs, requiring us to have 8-byte
- * alignment to avoid traps. The kmalloc alignment is guaranteed by
- * virtue of L1_CACHE_BYTES, requiring this to only be special cased
- * for slab caches.
- */
-#define ARCH_SLAB_MINALIGN	8
-#endif
-
 #endif /* __ASM_SH_PAGE_H */

commit c62da0c35d58518ddb26ff641d2485596567fd96
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Fri Apr 10 14:33:05 2020 -0700

    mm/vma: define a default value for VM_DATA_DEFAULT_FLAGS
    
    There are many platforms with exact same value for VM_DATA_DEFAULT_FLAGS
    This creates a default value for VM_DATA_DEFAULT_FLAGS in line with the
    existing VM_STACK_DEFAULT_FLAGS.  While here, also define some more
    macros with standard VMA access flag combinations that are used
    frequently across many platforms.  Apart from simplification, this
    reduces code duplication as well.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paulburton@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Chris Zankel <chris@zankel.net>
    Link: http://lkml.kernel.org/r/1583391014-8170-2-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 5eef8be3e59f..ea8d68f58e39 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -182,9 +182,6 @@ typedef struct page *pgtable_t;
 #endif
 #define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 
-#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | VM_EXEC | \
-				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
-
 #include <asm-generic/memory_model.h>
 #include <asm-generic/getorder.h>
 

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index ceb5201a30ed..5eef8be3e59f 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef __ASM_SH_PAGE_H
 #define __ASM_SH_PAGE_H
 

commit 934ed25ea505859cec5236dcb1769be5f998dd25
Author: Ross Zwisler <ross.zwisler@linux.intel.com>
Date:   Thu Oct 15 15:28:38 2015 -0700

    sh: add copy_user_page() alias for __copy_user()
    
    copy_user_page() is needed by DAX.  Without this we get a compile error
    for DAX on SH:
    
      fs/dax.c:280:2: error: implicit declaration of function `copy_user_page' [-Werror=implicit-function-declaration]
        copy_user_page(vto, (void __force *)vfrom, vaddr, to);
          ^
    
    This was done with a random config that happened to include DAX support.
    
    This patch has only been compile tested.
    
    Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Matthew Wilcox <willy@linux.intel.com>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index fe20d14ae051..ceb5201a30ed 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -59,6 +59,7 @@ pages_do_alias(unsigned long addr1, unsigned long addr2)
 
 #define clear_page(page)	memset((void *)(page), 0, PAGE_SIZE)
 extern void copy_page(void *to, void *from);
+#define copy_user_page(to, from, vaddr, pg)  __copy_user(to, from, PAGE_SIZE)
 
 struct page;
 struct vm_area_struct;

commit a6c19dfe39941a5d3f4d072121c0a4841e7e26fd
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Aug 8 14:23:40 2014 -0700

    arm64,ia64,ppc,s390,sh,tile,um,x86,mm: remove default gate area
    
    The core mm code will provide a default gate area based on
    FIXADDR_USER_START and FIXADDR_USER_END if
    !defined(__HAVE_ARCH_GATE_AREA) && defined(AT_SYSINFO_EHDR).
    
    This default is only useful for ia64.  arm64, ppc, s390, sh, tile, 64-bit
    UML, and x86_32 have their own code just to disable it.  arm, 32-bit UML,
    and x86_64 have gate areas, but they have their own implementations.
    
    This gets rid of the default and moves the code into ia64.
    
    This should save some code on architectures without a gate area: it's now
    possible to inline the gate_area functions in the default case.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Acked-by: Nathan Lynch <nathan_lynch@mentor.com>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org> [in principle]
    Acked-by: Richard Weinberger <richard@nod.at> [for um]
    Acked-by: Will Deacon <will.deacon@arm.com> [for arm64]
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Nathan Lynch <Nathan_Lynch@mentor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 15d970328f71..fe20d14ae051 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -186,11 +186,6 @@ typedef struct page *pgtable_t;
 #include <asm-generic/memory_model.h>
 #include <asm-generic/getorder.h>
 
-/* vDSO support */
-#ifdef CONFIG_VSYSCALL
-#define __HAVE_ARCH_GATE_AREA
-#endif
-
 /*
  * Some drivers need to perform DMA into kmalloc'ed buffers
  * and so we have to increase the kmalloc minalign for this.

commit dfd3b596fbbfa48b8e7966ef996d587157554b69
Author: Nobuhiro Iwamatsu <nobuhiro.iwamatsu.yj@renesas.com>
Date:   Fri Nov 4 22:13:50 2011 +0900

    sh: Fix cached/uncaced address calculation in 29bit mode
    
    In the case of 29bit mode, CAC/UNCAC_ADDR does not return a right address.
    This revises this problem by using P1SEGADDR and P2SEGADDR in 29bit mode.
    
    Reported-by: Yutaro Ebihara <ebiharaml@si-linux.co.jp>
    Signed-off-by: Nobuhiro Iwamatsu <nobuhiro.iwamatsu.yj@renesas.com>
    Tested-by: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
    Tested-by: Simon Horman <horms@verge.net.au>
    Cc: stable@kernel.org
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 0dca9a5c6be6..15d970328f71 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -151,8 +151,13 @@ typedef struct page *pgtable_t;
 #endif /* !__ASSEMBLY__ */
 
 #ifdef CONFIG_UNCACHED_MAPPING
+#if defined(CONFIG_29BIT)
+#define UNCAC_ADDR(addr)	P2SEGADDR(addr)
+#define CAC_ADDR(addr)		P1SEGADDR(addr)
+#else
 #define UNCAC_ADDR(addr)	((addr) - PAGE_OFFSET + uncached_start)
 #define CAC_ADDR(addr)		((addr) - uncached_start + PAGE_OFFSET)
+#endif
 #else
 #define UNCAC_ADDR(addr)	((addr))
 #define CAC_ADDR(addr)		((addr))

commit e66ac3f26aef131f5ca60350d25fba95f43acd0d
Author: Simon Horman <horms@verge.net.au>
Date:   Thu Sep 15 20:13:00 2011 +0900

    sh: kexec: Add PHYSICAL_START
    
    Add PHYSICAL_START kernel configuration parameter to set the address at
    which the kernel should be loaded.
    
    It has been observed on an sh7757lcr that simply modifying MEMORY_START
    does not achieve this goal for 32bit sh. This is due to MEMORY_OFFSET in
    arch/sh/kernel/vmlinux.lds.S bot being based on MEMORY_START on such
    systems.
    
    Signed-off-by: Simon Horman <horms@verge.net.au>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 822d6084195b..0dca9a5c6be6 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -112,6 +112,16 @@ typedef struct page *pgtable_t;
 #define __MEMORY_START		CONFIG_MEMORY_START
 #define __MEMORY_SIZE		CONFIG_MEMORY_SIZE
 
+/*
+ * PHYSICAL_OFFSET is the offset in physical memory where the base
+ * of the kernel is loaded.
+ */
+#ifdef CONFIG_PHYSICAL_START
+#define PHYSICAL_OFFSET (CONFIG_PHYSICAL_START - __MEMORY_START)
+#else
+#define PHYSICAL_OFFSET 0
+#endif
+
 /*
  * PAGE_OFFSET is the virtual address of the start of kernel address
  * space.

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index c4e0b3d472b9..822d6084195b 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -186,7 +186,7 @@ typedef struct page *pgtable_t;
 /*
  * While BYTES_PER_WORD == 4 on the current sh64 ABI, GCC will still
  * happily generate {ld/st}.q pairs, requiring us to have 8-byte
- * alignment to avoid traps. The kmalloc alignment is gauranteed by
+ * alignment to avoid traps. The kmalloc alignment is guaranteed by
  * virtue of L1_CACHE_BYTES, requiring this to only be special cased
  * for slab caches.
  */

commit a6eb9fe105d5de0053b261148cee56c94b4720ca
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Aug 10 18:03:22 2010 -0700

    dma-mapping: rename ARCH_KMALLOC_MINALIGN to ARCH_DMA_MINALIGN
    
    Now each architecture has the own dma_get_cache_alignment implementation.
    
    dma_get_cache_alignment returns the minimum DMA alignment.  Architectures
    define it as ARCH_KMALLOC_MINALIGN (it's used to make sure that malloc'ed
    buffer is DMA-safe; the buffer doesn't share a cache with the others).  So
    we can unify dma_get_cache_alignment implementations.
    
    This patch:
    
    dma_get_cache_alignment() needs to know if an architecture defines
    ARCH_KMALLOC_MINALIGN or not (needs to know if architecture has DMA
    alignment restriction).  However, slab.h define ARCH_KMALLOC_MINALIGN if
    architectures doesn't define it.
    
    Let's rename ARCH_KMALLOC_MINALIGN to ARCH_DMA_MINALIGN.
    ARCH_KMALLOC_MINALIGN is used only in the internals of slab/slob/slub
    (except for crypto).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index fb703d120d09..c4e0b3d472b9 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -180,7 +180,7 @@ typedef struct page *pgtable_t;
  * Some drivers need to perform DMA into kmalloc'ed buffers
  * and so we have to increase the kmalloc minalign for this.
  */
-#define ARCH_KMALLOC_MINALIGN	L1_CACHE_BYTES
+#define ARCH_DMA_MINALIGN	L1_CACHE_BYTES
 
 #ifdef CONFIG_SUPERH64
 /*

commit 5e2ff328c0668794ff408a4632f5b8a62827571f
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon May 10 20:17:25 2010 +0900

    sh: rework memory limits to work with LMB.
    
    This reworks the memory limit handling to tie in through the available
    LMB infrastructure. This requires a bit of reordering as we need to have
    all of the LMB reservations taken care of prior to establishing the
    limits.
    
    While we're at it, the crash kernel reservation semantics are reworked
    so that we allocate from the bottom up and reduce the risk of having
    to disable the memory limit due to a clash with the crash kernel
    reservation.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 0152c040f6c3..fb703d120d09 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -49,7 +49,7 @@
 
 extern unsigned long shm_align_mask;
 extern unsigned long max_low_pfn, min_low_pfn;
-extern unsigned long memory_start, memory_end;
+extern unsigned long memory_start, memory_end, memory_limit;
 
 static inline unsigned long
 pages_do_alias(unsigned long addr1, unsigned long addr2)

commit 7c4584d39a8a144c60adaf4cf998c3233b6683d9
Author: Matt Fleming <matt@console-pimps.org>
Date:   Sat Apr 24 11:12:12 2010 +0100

    sh: Assembly friendly __pa and __va definitions
    
    This patch defines ___pa and ___va which return the physical and virtual
    address of an address, respectively. These macros are suitable for
    calling from assembly because they don't include the C casts required by
    __pa and __va.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index d71feb359304..0152c040f6c3 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -128,13 +128,18 @@ typedef struct page *pgtable_t;
  * added or subtracted as required.
  */
 #ifdef CONFIG_PMB
-#define __pa(x)	((unsigned long)(x)-PAGE_OFFSET+__MEMORY_START)
-#define __va(x)	((void *)((unsigned long)(x)+PAGE_OFFSET-__MEMORY_START))
+#define ___pa(x)	((x)-PAGE_OFFSET+__MEMORY_START)
+#define ___va(x)	((x)+PAGE_OFFSET-__MEMORY_START)
 #else
-#define __pa(x)	((unsigned long)(x)-PAGE_OFFSET)
-#define __va(x)	((void *)((unsigned long)(x)+PAGE_OFFSET))
+#define ___pa(x)	((x)-PAGE_OFFSET)
+#define ___va(x)	((x)+PAGE_OFFSET)
 #endif
 
+#ifndef __ASSEMBLY__
+#define __pa(x)		___pa((unsigned long)x)
+#define __va(x)		(void *)___va((unsigned long)x)
+#endif /* !__ASSEMBLY__ */
+
 #ifdef CONFIG_UNCACHED_MAPPING
 #define UNCAC_ADDR(addr)	((addr) - PAGE_OFFSET + uncached_start)
 #define CAC_ADDR(addr)		((addr) - uncached_start + PAGE_OFFSET)

commit d01447b3197c2c470a14666be2c640407bbbfec7
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Feb 18 18:13:51 2010 +0900

    sh: Merge legacy and dynamic PMB modes.
    
    This implements a bit of rework for the PMB code, which permits us to
    kill off the legacy PMB mode completely. Rather than trusting the boot
    loader to do the right thing, we do a quick verification of the PMB
    contents to determine whether to have the kernel setup the initial
    mappings or whether it needs to mangle them later on instead.
    
    If we're booting from legacy mappings, the kernel will now take control
    of them and make them match the kernel's initial mapping configuration.
    This is accomplished by breaking the initialization phase out in to
    multiple steps: synchronization, merging, and resizing. With the recent
    rework, the synchronization code establishes page links for compound
    mappings already, so we build on top of this for promoting mappings and
    reclaiming unused slots.
    
    At the same time, the changes introduced for the uncached helpers also
    permit us to dynamically resize the uncached mapping without any
    particular headaches. The smallest page size is more than sufficient for
    mapping all of kernel text, and as we're careful not to jump to any far
    off locations in the setup code the mapping can safely be resized
    regardless of whether we are executing from it or not.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 8237d9f53e56..d71feb359304 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -45,21 +45,12 @@
 #endif
 
 #ifndef __ASSEMBLY__
+#include <asm/uncached.h>
 
 extern unsigned long shm_align_mask;
 extern unsigned long max_low_pfn, min_low_pfn;
 extern unsigned long memory_start, memory_end;
 
-#ifdef CONFIG_UNCACHED_MAPPING
-extern unsigned long uncached_start, uncached_end;
-
-extern int virt_addr_uncached(unsigned long kaddr);
-extern void uncached_init(void);
-#else
-#define virt_addr_uncached(kaddr)	(0)
-#define uncached_init()			do { } while (0)
-#endif
-
 static inline unsigned long
 pages_do_alias(unsigned long addr1, unsigned long addr2)
 {

commit 9edef28653a519bf0a48250f36cce96b1736ec4e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Feb 17 16:28:00 2010 +0900

    sh: uncached mapping helpers.
    
    This adds some helper routines for uncached mapping support. This
    simplifies some of the cases where we need to check the uncached mapping
    boundaries in addition to giving us a centralized location for building
    more complex manipulation on top of.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 3accdc5ab122..8237d9f53e56 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -50,13 +50,22 @@ extern unsigned long shm_align_mask;
 extern unsigned long max_low_pfn, min_low_pfn;
 extern unsigned long memory_start, memory_end;
 
+#ifdef CONFIG_UNCACHED_MAPPING
+extern unsigned long uncached_start, uncached_end;
+
+extern int virt_addr_uncached(unsigned long kaddr);
+extern void uncached_init(void);
+#else
+#define virt_addr_uncached(kaddr)	(0)
+#define uncached_init()			do { } while (0)
+#endif
+
 static inline unsigned long
 pages_do_alias(unsigned long addr1, unsigned long addr2)
 {
 	return (addr1 ^ addr2) & shm_align_mask;
 }
 
-
 #define clear_page(page)	memset((void *)(page), 0, PAGE_SIZE)
 extern void copy_page(void *to, void *from);
 
@@ -135,6 +144,14 @@ typedef struct page *pgtable_t;
 #define __va(x)	((void *)((unsigned long)(x)+PAGE_OFFSET))
 #endif
 
+#ifdef CONFIG_UNCACHED_MAPPING
+#define UNCAC_ADDR(addr)	((addr) - PAGE_OFFSET + uncached_start)
+#define CAC_ADDR(addr)		((addr) - uncached_start + PAGE_OFFSET)
+#else
+#define UNCAC_ADDR(addr)	((addr))
+#define CAC_ADDR(addr)		((addr))
+#endif
+
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 #define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
 

commit 1d5cfcdff793e2f34ec61d902fa5ee0c7e4a2208
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Feb 16 21:43:38 2010 +0900

    sh: Kill off some superfluous legacy PMB special casing.
    
    The __va()/__pa() offsets and the boot memory offsets are consistent for
    all PMB users, so there is no need to special case these for legacy PMB.
    Kill the special casing off and depend on CONFIG_PMB across the board.
    This also fixes up yet another addressing bug for sh64.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 61e58105adc3..3accdc5ab122 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -127,12 +127,7 @@ typedef struct page *pgtable_t;
  * is not visible (it is part of the PMB mapping) and so needs to be
  * added or subtracted as required.
  */
-#if defined(CONFIG_PMB_LEGACY)
-/* phys = virt - PAGE_OFFSET - (__MEMORY_START & 0xe0000000) */
-#define PMB_OFFSET	(PAGE_OFFSET - PXSEG(__MEMORY_START))
-#define __pa(x)	((unsigned long)(x) - PMB_OFFSET)
-#define __va(x)	((void *)((unsigned long)(x) + PMB_OFFSET))
-#elif defined(CONFIG_32BIT)
+#ifdef CONFIG_PMB
 #define __pa(x)	((unsigned long)(x)-PAGE_OFFSET+__MEMORY_START)
 #define __va(x)	((void *)((unsigned long)(x)+PAGE_OFFSET-__MEMORY_START))
 #else

commit 24ef7fc4dcc57afa0c33166c25bfe7676ffd4296
Author: Matt Fleming <matt@console-pimps.org>
Date:   Thu Nov 19 21:11:05 2009 +0000

    sh: Acquire some more page flags for SH-5.
    
    We need some more page flags to hook up _PAGE_WIRED (and eventually
    other things). So use the unused PTE bits above the PPN field as no
    implementations use these for anything currently.
    
    Now that we have _PAGE_WIRED let's provide the SH-5 functions for wiring
    up TLB entries.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index a86c0f1d05d4..61e58105adc3 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -88,7 +88,7 @@ typedef struct { unsigned long pgd; } pgd_t;
 #define __pte(x)	((pte_t) { (x) } )
 #else
 typedef struct { unsigned long long pte_low; } pte_t;
-typedef struct { unsigned long pgprot; } pgprot_t;
+typedef struct { unsigned long long pgprot; } pgprot_t;
 typedef struct { unsigned long pgd; } pgd_t;
 #define pte_val(x)	((x).pte_low)
 #define __pte(x)	((pte_t) { (x) } )

commit a0ab36689a36e583b6e736f1c99ac8c9aebdad59
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jan 13 18:31:48 2010 +0900

    sh: fixed PMB mode refactoring.
    
    This introduces some much overdue chainsawing of the fixed PMB support.
    fixed PMB was introduced initially to work around the fact that dynamic
    PMB mode was relatively broken, though they were never intended to
    converge. The main areas where there are differences are whether the
    system is booted in 29-bit mode or 32-bit mode, and whether legacy
    mappings are to be preserved. Any system booting in true 32-bit mode will
    not care about legacy mappings, so these are roughly decoupled.
    
    Regardless of the entry point, PMB and 32BIT are directly related as far
    as the kernel is concerned, so we also switch back to having one select
    the other.
    
    With legacy mappings iterated through and applied in the initialization
    path it's now possible to finally merge the two implementations and
    permit dynamic remapping overtop of remaining entries regardless of
    whether boot mappings are crafted by hand or inherited from the boot
    loader.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 81bffc0d6860..a86c0f1d05d4 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -127,7 +127,7 @@ typedef struct page *pgtable_t;
  * is not visible (it is part of the PMB mapping) and so needs to be
  * added or subtracted as required.
  */
-#if defined(CONFIG_PMB_FIXED)
+#if defined(CONFIG_PMB_LEGACY)
 /* phys = virt - PAGE_OFFSET - (__MEMORY_START & 0xe0000000) */
 #define PMB_OFFSET	(PAGE_OFFSET - PXSEG(__MEMORY_START))
 #define __pa(x)	((unsigned long)(x) - PMB_OFFSET)

commit 65305ae816ca17a38340aef0ccc92d0c127acccf
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sun Aug 16 00:53:56 2009 +0900

    sh: Convert cache disabled SH-5 over to new cache interface.
    
    The caches enabled case needs more work, but is presently broken
    regardless, so this can be done incrementally.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index a316eeb50b39..81bffc0d6860 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -63,20 +63,12 @@ extern void copy_page(void *to, void *from);
 struct page;
 struct vm_area_struct;
 
-#if defined(CONFIG_CPU_SH5)
-extern void clear_user_page(void *to, unsigned long address, struct page *page);
-extern void copy_user_page(void *to, void *from, unsigned long address,
-			   struct page *page);
-
-#else
 extern void copy_user_highpage(struct page *to, struct page *from,
 			       unsigned long vaddr, struct vm_area_struct *vma);
 #define __HAVE_ARCH_COPY_USER_HIGHPAGE
 extern void clear_user_highpage(struct page *page, unsigned long vaddr);
 #define clear_user_highpage	clear_user_highpage
 
-#endif
-
 /*
  * These are used to make use of C type-checking..
  */

commit dde5e3ffb770ef2854bbc32c51a365e932919e19
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 09:49:32 2009 +0900

    sh: rework nommu for generic cache.c use.
    
    This does a bit of reorganizing for allowing nommu to use the new
    and generic cache.c, no functional changes.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 847eeabb9083..a316eeb50b39 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -68,18 +68,13 @@ extern void clear_user_page(void *to, unsigned long address, struct page *page);
 extern void copy_user_page(void *to, void *from, unsigned long address,
 			   struct page *page);
 
-#elif defined(CONFIG_MMU)
+#else
 extern void copy_user_highpage(struct page *to, struct page *from,
 			       unsigned long vaddr, struct vm_area_struct *vma);
 #define __HAVE_ARCH_COPY_USER_HIGHPAGE
 extern void clear_user_highpage(struct page *page, unsigned long vaddr);
 #define clear_user_highpage	clear_user_highpage
 
-#else
-
-#define clear_user_page(page, vaddr, pg)	clear_page(page)
-#define copy_user_page(to, from, vaddr, pg)	copy_page(to, from)
-
 #endif
 
 /*

commit 0dfae7d5a21901b28ec0452d71be64adf5ea323e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Jul 27 21:30:17 2009 +0900

    sh: Use the now generic SH-4 clear/copy page ops for all MMU platforms.
    
    Now that the SH-4 page clear/copy ops are generic, they can be used for
    all platforms with CONFIG_MMU=y. SH-5 remains the odd one out, but it too
    will gradually be converted over to using this interface.
    
    SH-3 platforms which do not contain aliases will see no impact from this
    change, while aliasing SH-3 platforms will get the same interface as
    SH-4.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 5208b7bfc24e..847eeabb9083 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -63,22 +63,23 @@ extern void copy_page(void *to, void *from);
 struct page;
 struct vm_area_struct;
 
-#if !defined(CONFIG_CACHE_OFF) && defined(CONFIG_MMU) && \
-	(defined(CONFIG_CPU_SH5) || defined(CONFIG_CPU_SH4) || \
-	 defined(CONFIG_SH7705_CACHE_32KB))
+#if defined(CONFIG_CPU_SH5)
 extern void clear_user_page(void *to, unsigned long address, struct page *page);
 extern void copy_user_page(void *to, void *from, unsigned long address,
 			   struct page *page);
-#if defined(CONFIG_CPU_SH4) || defined(CONFIG_SH7705_CACHE_32KB)
+
+#elif defined(CONFIG_MMU)
 extern void copy_user_highpage(struct page *to, struct page *from,
 			       unsigned long vaddr, struct vm_area_struct *vma);
 #define __HAVE_ARCH_COPY_USER_HIGHPAGE
 extern void clear_user_highpage(struct page *page, unsigned long vaddr);
 #define clear_user_highpage	clear_user_highpage
-#endif
+
 #else
+
 #define clear_user_page(page, vaddr, pg)	clear_page(page)
 #define copy_user_page(to, from, vaddr, pg)	copy_page(to, from)
+
 #endif
 
 /*

commit dfff0fa65ab15db45acd64b3189787d37ab163cd
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Jul 27 20:53:22 2009 +0900

    sh: wire up clear_user_highpage() for sh4, convert sh7705.
    
    This wires up clear_user_highpage() on SH-4 and subsequently converts the
    SH7705 32kB cache mode over to using it. Now that the SH-4 implementation
    handles all of the dcache purging directly in the aliasing case, there is
    no need to do this in the default clear_page() implementation.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index a31ab40040f0..5208b7bfc24e 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -56,21 +56,25 @@ pages_do_alias(unsigned long addr1, unsigned long addr2)
 	return (addr1 ^ addr2) & shm_align_mask;
 }
 
-extern void clear_page(void *to);
+
+#define clear_page(page)	memset((void *)(page), 0, PAGE_SIZE)
 extern void copy_page(void *to, void *from);
 
+struct page;
+struct vm_area_struct;
+
 #if !defined(CONFIG_CACHE_OFF) && defined(CONFIG_MMU) && \
 	(defined(CONFIG_CPU_SH5) || defined(CONFIG_CPU_SH4) || \
 	 defined(CONFIG_SH7705_CACHE_32KB))
-struct page;
-struct vm_area_struct;
 extern void clear_user_page(void *to, unsigned long address, struct page *page);
 extern void copy_user_page(void *to, void *from, unsigned long address,
 			   struct page *page);
-#if defined(CONFIG_CPU_SH4)
+#if defined(CONFIG_CPU_SH4) || defined(CONFIG_SH7705_CACHE_32KB)
 extern void copy_user_highpage(struct page *to, struct page *from,
 			       unsigned long vaddr, struct vm_area_struct *vma);
 #define __HAVE_ARCH_COPY_USER_HIGHPAGE
+extern void clear_user_highpage(struct page *page, unsigned long vaddr);
+#define clear_user_highpage	clear_user_highpage
 #endif
 #else
 #define clear_user_page(page, vaddr, pg)	clear_page(page)

commit 2277ab4a1df50e05bc732fe9488d4e902bb8399a
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 22 19:20:49 2009 +0900

    sh: Migrate from PG_mapped to PG_dcache_dirty.
    
    This inverts the delayed dcache flush a bit to be more in line with other
    platforms. At the same time this also gives us the ability to do some
    more optimizations and cleanup. Now that the update_mmu_cache() callsite
    only tests for the bit, the implementation can gradually be split out and
    made generic, rather than relying on special implementations for each of
    the peculiar CPU types.
    
    SH7705 in 32kB mode and SH-4 still need slightly different handling, but
    this is something that can remain isolated in the varying page copy/clear
    routines. On top of that, SH-X3 is dcache coherent, so there is no need
    to bother with any of these tests in the PTEAEX version of
    update_mmu_cache(), so we kill that off too.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 49592c780a6e..a31ab40040f0 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -50,6 +50,12 @@ extern unsigned long shm_align_mask;
 extern unsigned long max_low_pfn, min_low_pfn;
 extern unsigned long memory_start, memory_end;
 
+static inline unsigned long
+pages_do_alias(unsigned long addr1, unsigned long addr2)
+{
+	return (addr1 ^ addr2) & shm_align_mask;
+}
+
 extern void clear_page(void *to);
 extern void copy_page(void *to, void *from);
 

commit 5b17e1cd8928ae65932758ce6478ac6d3e9a86b2
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed May 13 22:56:30 2009 +0000

    asm-generic: rename page.h and uaccess.h
    
    The current asm-generic/page.h only contains the get_order
    function, and asm-generic/uaccess.h only implements
    unaligned accesses. This renames the file to getorder.h
    and uaccess-unaligned.h to make room for new page.h
    and uaccess.h file that will be usable by all simple
    (e.g. nommu) architectures.
    
    Signed-off-by: Remis Lima Baima <remis.developer@googlemail.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 9c6d21ec0240..49592c780a6e 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -163,7 +163,7 @@ typedef struct page *pgtable_t;
 				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
 #include <asm-generic/memory_model.h>
-#include <asm-generic/page.h>
+#include <asm-generic/getorder.h>
 
 /* vDSO support */
 #ifdef CONFIG_VSYSCALL

commit 2f47f44790a9c8fc43e515df3c6be19a35ee5de5
Author: Yoshihiro Shimoda <shimoda.yoshihiro@renesas.com>
Date:   Tue Mar 10 15:49:54 2009 +0900

    sh: Support fixed 32-bit PMB mappings from bootloader.
    
    This provides a method for supporting fixed PMB mappings inherited from
    the bootloader, as an alternative to the dynamic PMB mapping currently
    used by the kernel. In the future these methods will be combined.
    
    P1/P2 area is handled like a regular 29-bit physical address, and local
    bus device are assigned P3 area addresses.
    
    Signed-off-by: Yoshihiro Shimoda <shimoda.yoshihiro@renesas.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 5871d78e47e5..9c6d21ec0240 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -129,7 +129,12 @@ typedef struct page *pgtable_t;
  * is not visible (it is part of the PMB mapping) and so needs to be
  * added or subtracted as required.
  */
-#ifdef CONFIG_32BIT
+#if defined(CONFIG_PMB_FIXED)
+/* phys = virt - PAGE_OFFSET - (__MEMORY_START & 0xe0000000) */
+#define PMB_OFFSET	(PAGE_OFFSET - PXSEG(__MEMORY_START))
+#define __pa(x)	((unsigned long)(x) - PMB_OFFSET)
+#define __va(x)	((void *)((unsigned long)(x) + PMB_OFFSET))
+#elif defined(CONFIG_32BIT)
 #define __pa(x)	((unsigned long)(x)-PAGE_OFFSET+__MEMORY_START)
 #define __va(x)	((void *)((unsigned long)(x)+PAGE_OFFSET-__MEMORY_START))
 #else

commit cb700aa4f13d38726defab3060d3ebeaf67dc189
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Sep 12 20:41:05 2008 +0900

    sh: ioremap_prot support.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
index 77fb8bf02e4e..5871d78e47e5 100644
--- a/arch/sh/include/asm/page.h
+++ b/arch/sh/include/asm/page.h
@@ -104,6 +104,8 @@ typedef struct { unsigned long pgd; } pgd_t;
 
 typedef struct page *pgtable_t;
 
+#define pte_pgprot(x) __pgprot(pte_val(x) & PTE_FLAGS_MASK)
+
 #endif /* !__ASSEMBLY__ */
 
 /*

commit f15cbe6f1a4b4d9df59142fc8e4abb973302cf44
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jul 29 08:09:44 2008 +0900

    sh: migrate to arch/sh/include/
    
    This follows the sparc changes a439fe51a1f8eb087c22dd24d69cebae4a3addac.
    
    Most of the moving about was done with Sam's directions at:
    
    http://marc.info/?l=linux-sh&m=121724823706062&w=2
    
    with subsequent hacking and fixups entirely my fault.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/include/asm/page.h b/arch/sh/include/asm/page.h
new file mode 100644
index 000000000000..77fb8bf02e4e
--- /dev/null
+++ b/arch/sh/include/asm/page.h
@@ -0,0 +1,183 @@
+#ifndef __ASM_SH_PAGE_H
+#define __ASM_SH_PAGE_H
+
+/*
+ * Copyright (C) 1999  Niibe Yutaka
+ */
+
+#include <linux/const.h>
+
+/* PAGE_SHIFT determines the page size */
+#if defined(CONFIG_PAGE_SIZE_4KB)
+# define PAGE_SHIFT	12
+#elif defined(CONFIG_PAGE_SIZE_8KB)
+# define PAGE_SHIFT	13
+#elif defined(CONFIG_PAGE_SIZE_16KB)
+# define PAGE_SHIFT	14
+#elif defined(CONFIG_PAGE_SIZE_64KB)
+# define PAGE_SHIFT	16
+#else
+# error "Bogus kernel page size?"
+#endif
+
+#define PAGE_SIZE	(_AC(1, UL) << PAGE_SHIFT)
+#define PAGE_MASK	(~(PAGE_SIZE-1))
+#define PTE_MASK	PAGE_MASK
+
+#if defined(CONFIG_HUGETLB_PAGE_SIZE_64K)
+#define HPAGE_SHIFT	16
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_256K)
+#define HPAGE_SHIFT	18
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_1MB)
+#define HPAGE_SHIFT	20
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_4MB)
+#define HPAGE_SHIFT	22
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_64MB)
+#define HPAGE_SHIFT	26
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_512MB)
+#define HPAGE_SHIFT	29
+#endif
+
+#ifdef CONFIG_HUGETLB_PAGE
+#define HPAGE_SIZE		(1UL << HPAGE_SHIFT)
+#define HPAGE_MASK		(~(HPAGE_SIZE-1))
+#define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT-PAGE_SHIFT)
+#endif
+
+#ifndef __ASSEMBLY__
+
+extern unsigned long shm_align_mask;
+extern unsigned long max_low_pfn, min_low_pfn;
+extern unsigned long memory_start, memory_end;
+
+extern void clear_page(void *to);
+extern void copy_page(void *to, void *from);
+
+#if !defined(CONFIG_CACHE_OFF) && defined(CONFIG_MMU) && \
+	(defined(CONFIG_CPU_SH5) || defined(CONFIG_CPU_SH4) || \
+	 defined(CONFIG_SH7705_CACHE_32KB))
+struct page;
+struct vm_area_struct;
+extern void clear_user_page(void *to, unsigned long address, struct page *page);
+extern void copy_user_page(void *to, void *from, unsigned long address,
+			   struct page *page);
+#if defined(CONFIG_CPU_SH4)
+extern void copy_user_highpage(struct page *to, struct page *from,
+			       unsigned long vaddr, struct vm_area_struct *vma);
+#define __HAVE_ARCH_COPY_USER_HIGHPAGE
+#endif
+#else
+#define clear_user_page(page, vaddr, pg)	clear_page(page)
+#define copy_user_page(to, from, vaddr, pg)	copy_page(to, from)
+#endif
+
+/*
+ * These are used to make use of C type-checking..
+ */
+#ifdef CONFIG_X2TLB
+typedef struct { unsigned long pte_low, pte_high; } pte_t;
+typedef struct { unsigned long long pgprot; } pgprot_t;
+typedef struct { unsigned long long pgd; } pgd_t;
+#define pte_val(x) \
+	((x).pte_low | ((unsigned long long)(x).pte_high << 32))
+#define __pte(x) \
+	({ pte_t __pte = {(x), ((unsigned long long)(x)) >> 32}; __pte; })
+#elif defined(CONFIG_SUPERH32)
+typedef struct { unsigned long pte_low; } pte_t;
+typedef struct { unsigned long pgprot; } pgprot_t;
+typedef struct { unsigned long pgd; } pgd_t;
+#define pte_val(x)	((x).pte_low)
+#define __pte(x)	((pte_t) { (x) } )
+#else
+typedef struct { unsigned long long pte_low; } pte_t;
+typedef struct { unsigned long pgprot; } pgprot_t;
+typedef struct { unsigned long pgd; } pgd_t;
+#define pte_val(x)	((x).pte_low)
+#define __pte(x)	((pte_t) { (x) } )
+#endif
+
+#define pgd_val(x)	((x).pgd)
+#define pgprot_val(x)	((x).pgprot)
+
+#define __pgd(x) ((pgd_t) { (x) } )
+#define __pgprot(x)	((pgprot_t) { (x) } )
+
+typedef struct page *pgtable_t;
+
+#endif /* !__ASSEMBLY__ */
+
+/*
+ * __MEMORY_START and SIZE are the physical addresses and size of RAM.
+ */
+#define __MEMORY_START		CONFIG_MEMORY_START
+#define __MEMORY_SIZE		CONFIG_MEMORY_SIZE
+
+/*
+ * PAGE_OFFSET is the virtual address of the start of kernel address
+ * space.
+ */
+#define PAGE_OFFSET		CONFIG_PAGE_OFFSET
+
+/*
+ * Virtual to physical RAM address translation.
+ *
+ * In 29 bit mode, the physical offset of RAM from address 0 is visible in
+ * the kernel virtual address space, and thus we don't have to take
+ * this into account when translating. However in 32 bit mode this offset
+ * is not visible (it is part of the PMB mapping) and so needs to be
+ * added or subtracted as required.
+ */
+#ifdef CONFIG_32BIT
+#define __pa(x)	((unsigned long)(x)-PAGE_OFFSET+__MEMORY_START)
+#define __va(x)	((void *)((unsigned long)(x)+PAGE_OFFSET-__MEMORY_START))
+#else
+#define __pa(x)	((unsigned long)(x)-PAGE_OFFSET)
+#define __va(x)	((void *)((unsigned long)(x)+PAGE_OFFSET))
+#endif
+
+#define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
+#define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
+
+/*
+ * PFN = physical frame number (ie PFN 0 == physical address 0)
+ * PFN_START is the PFN of the first page of RAM. By defining this we
+ * don't have struct page entries for the portion of address space
+ * between physical address 0 and the start of RAM.
+ */
+#define PFN_START		(__MEMORY_START >> PAGE_SHIFT)
+#define ARCH_PFN_OFFSET		(PFN_START)
+#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#ifdef CONFIG_FLATMEM
+#define pfn_valid(pfn)		((pfn) >= min_low_pfn && (pfn) < max_low_pfn)
+#endif
+#define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+
+#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | VM_EXEC | \
+				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+
+#include <asm-generic/memory_model.h>
+#include <asm-generic/page.h>
+
+/* vDSO support */
+#ifdef CONFIG_VSYSCALL
+#define __HAVE_ARCH_GATE_AREA
+#endif
+
+/*
+ * Some drivers need to perform DMA into kmalloc'ed buffers
+ * and so we have to increase the kmalloc minalign for this.
+ */
+#define ARCH_KMALLOC_MINALIGN	L1_CACHE_BYTES
+
+#ifdef CONFIG_SUPERH64
+/*
+ * While BYTES_PER_WORD == 4 on the current sh64 ABI, GCC will still
+ * happily generate {ld/st}.q pairs, requiring us to have 8-byte
+ * alignment to avoid traps. The kmalloc alignment is gauranteed by
+ * virtue of L1_CACHE_BYTES, requiring this to only be special cased
+ * for slab caches.
+ */
+#define ARCH_SLAB_MINALIGN	8
+#endif
+
+#endif /* __ASM_SH_PAGE_H */
