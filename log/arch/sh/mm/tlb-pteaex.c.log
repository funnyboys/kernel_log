commit e839ca528718e68cad32a307dc9aabf01ef3eb05
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:03 2012 +0100

    Disintegrate asm/system.h for SH
    
    Disintegrate asm/system.h for SH.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: linux-sh@vger.kernel.org

diff --git a/arch/sh/mm/tlb-pteaex.c b/arch/sh/mm/tlb-pteaex.c
index b71db6af8060..4db21adfe5de 100644
--- a/arch/sh/mm/tlb-pteaex.c
+++ b/arch/sh/mm/tlb-pteaex.c
@@ -12,7 +12,6 @@
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/io.h>
-#include <asm/system.h>
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 

commit be97d758e5728099e95fe229866d5c6c900d3092
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Apr 2 16:13:27 2010 +0900

    sh: Fix up the SH-3 build for recent TLB changes.
    
    While the MMUCR.URB and ITLB/UTLB differentiation works fine for all SH-4
    and later TLBs, these features are absent on SH-3. This splits out
    local_flush_tlb_all() in to SH-4 and PTEAEX copies while restoring the
    old SH-3 one, subsequently fixing up the build.
    
    This will probably want some further reordering and tidying in the
    future, but that's out of scope at present.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-pteaex.c b/arch/sh/mm/tlb-pteaex.c
index bdd0982b56ee..b71db6af8060 100644
--- a/arch/sh/mm/tlb-pteaex.c
+++ b/arch/sh/mm/tlb-pteaex.c
@@ -77,3 +77,31 @@ void local_flush_tlb_one(unsigned long asid, unsigned long page)
 	__raw_writel(asid, MMU_ITLB_ADDRESS_ARRAY2 | MMU_PAGE_ASSOC_BIT);
 	back_to_cached();
 }
+
+void local_flush_tlb_all(void)
+{
+	unsigned long flags, status;
+	int i;
+
+	/*
+	 * Flush all the TLB.
+	 */
+	local_irq_save(flags);
+	jump_to_uncached();
+
+	status = __raw_readl(MMUCR);
+	status = ((status & MMUCR_URB) >> MMUCR_URB_SHIFT);
+
+	if (status == 0)
+		status = MMUCR_URB_NENTRIES;
+
+	for (i = 0; i < status; i++)
+		__raw_writel(0x0, MMU_UTLB_ADDRESS_ARRAY | (i << 8));
+
+	for (i = 0; i < 4; i++)
+		__raw_writel(0x0, MMU_ITLB_ADDRESS_ARRAY | (i << 8));
+
+	back_to_cached();
+	ctrl_barrier();
+	local_irq_restore(flags);
+}

commit a9eb4f6d1a168c830a206306dfbb1f95a7fed6b3
Author: Matt Fleming <matt@console-pimps.org>
Date:   Sun Mar 21 19:51:43 2010 +0000

    sh: Flush ITLB too in PTEAEX's flush_tlb_page()
    
    flush_tlb_page() can be used to flush TLB entries that map executable
    pages. Therefore, we need to ensure that the ITLB is also flushed in
    local_flush_tlb_page().
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-pteaex.c b/arch/sh/mm/tlb-pteaex.c
index 32dc674c550c..bdd0982b56ee 100644
--- a/arch/sh/mm/tlb-pteaex.c
+++ b/arch/sh/mm/tlb-pteaex.c
@@ -73,5 +73,7 @@ void local_flush_tlb_one(unsigned long asid, unsigned long page)
 	jump_to_uncached();
 	__raw_writel(page, MMU_UTLB_ADDRESS_ARRAY | MMU_PAGE_ASSOC_BIT);
 	__raw_writel(asid, MMU_UTLB_ADDRESS_ARRAY2 | MMU_PAGE_ASSOC_BIT);
+	__raw_writel(page, MMU_ITLB_ADDRESS_ARRAY | MMU_PAGE_ASSOC_BIT);
+	__raw_writel(asid, MMU_ITLB_ADDRESS_ARRAY2 | MMU_PAGE_ASSOC_BIT);
 	back_to_cached();
 }

commit 2dc2f8e0c46864e2a3722c84eaa96513d4cf8b2f
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Jan 21 16:05:25 2010 +0900

    sh: Kill off the special uncached section and fixmap.
    
    Now that cached_to_uncached works as advertized in 32-bit mode and we're
    never going to be able to map < 16MB anyways, there's no need for the
    special uncached section. Kill it off.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-pteaex.c b/arch/sh/mm/tlb-pteaex.c
index 409b7c2b4b9d..32dc674c550c 100644
--- a/arch/sh/mm/tlb-pteaex.c
+++ b/arch/sh/mm/tlb-pteaex.c
@@ -68,8 +68,7 @@ void __update_tlb(struct vm_area_struct *vma, unsigned long address, pte_t pte)
  * in extended mode, the legacy 8-bit ASID field in address array 1 has
  * undefined behaviour.
  */
-void __uses_jump_to_uncached local_flush_tlb_one(unsigned long asid,
-						 unsigned long page)
+void local_flush_tlb_one(unsigned long asid, unsigned long page)
 {
 	jump_to_uncached();
 	__raw_writel(page, MMU_UTLB_ADDRESS_ARRAY | MMU_PAGE_ASSOC_BIT);

commit bb29c677b366fdf4f6522cd82228a32567aa98c7
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jan 19 15:20:35 2010 +0900

    sh: Split out MMUCR.URB based entry wiring in to shared helper.
    
    Presently this is duplicated between tlb-sh4 and tlb-pteaex. Split the
    helpers out in to a generic tlb-urb that can be used by any parts
    equipped with MMUCR.URB.
    
    At the same time, move the SH-5 code out-of-line, as we require single
    global state for DTLB entry wiring.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-pteaex.c b/arch/sh/mm/tlb-pteaex.c
index 88c8bb05e16d..409b7c2b4b9d 100644
--- a/arch/sh/mm/tlb-pteaex.c
+++ b/arch/sh/mm/tlb-pteaex.c
@@ -76,69 +76,3 @@ void __uses_jump_to_uncached local_flush_tlb_one(unsigned long asid,
 	__raw_writel(asid, MMU_UTLB_ADDRESS_ARRAY2 | MMU_PAGE_ASSOC_BIT);
 	back_to_cached();
 }
-
-/*
- * Load the entry for 'addr' into the TLB and wire the entry.
- */
-void tlb_wire_entry(struct vm_area_struct *vma, unsigned long addr, pte_t pte)
-{
-	unsigned long status, flags;
-	int urb;
-
-	local_irq_save(flags);
-
-	/* Load the entry into the TLB */
-	__update_tlb(vma, addr, pte);
-
-	/* ... and wire it up. */
-	status = ctrl_inl(MMUCR);
-	urb = (status & MMUCR_URB) >> MMUCR_URB_SHIFT;
-	status &= ~MMUCR_URB;
-
-	/*
-	 * Make sure we're not trying to wire the last TLB entry slot.
-	 */
-	BUG_ON(!--urb);
-
-	urb = urb % MMUCR_URB_NENTRIES;
-
-	status |= (urb << MMUCR_URB_SHIFT);
-	ctrl_outl(status, MMUCR);
-	ctrl_barrier();
-
-	local_irq_restore(flags);
-}
-
-/*
- * Unwire the last wired TLB entry.
- *
- * It should also be noted that it is not possible to wire and unwire
- * TLB entries in an arbitrary order. If you wire TLB entry N, followed
- * by entry N+1, you must unwire entry N+1 first, then entry N. In this
- * respect, it works like a stack or LIFO queue.
- */
-void tlb_unwire_entry(void)
-{
-	unsigned long status, flags;
-	int urb;
-
-	local_irq_save(flags);
-
-	status = ctrl_inl(MMUCR);
-	urb = (status & MMUCR_URB) >> MMUCR_URB_SHIFT;
-	status &= ~MMUCR_URB;
-
-	/*
-	 * Make sure we're not trying to unwire a TLB entry when none
-	 * have been wired.
-	 */
-	BUG_ON(urb++ == MMUCR_URB_NENTRIES);
-
-	urb = urb % MMUCR_URB_NENTRIES;
-
-	status |= (urb << MMUCR_URB_SHIFT);
-	ctrl_outl(status, MMUCR);
-	ctrl_barrier();
-
-	local_irq_restore(flags);
-}

commit 8eda55142080f0373b1f0268fe6d6807f193e713
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Nov 17 21:05:31 2009 +0000

    sh: New extended page flag to wire/unwire TLB entries
    
    Provide a new extended page flag, _PAGE_WIRED and an SH4 implementation
    for wiring TLB entries and use it in the fixmap code path so that we can
    wire the fixmap TLB entry.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>

diff --git a/arch/sh/mm/tlb-pteaex.c b/arch/sh/mm/tlb-pteaex.c
index 409b7c2b4b9d..88c8bb05e16d 100644
--- a/arch/sh/mm/tlb-pteaex.c
+++ b/arch/sh/mm/tlb-pteaex.c
@@ -76,3 +76,69 @@ void __uses_jump_to_uncached local_flush_tlb_one(unsigned long asid,
 	__raw_writel(asid, MMU_UTLB_ADDRESS_ARRAY2 | MMU_PAGE_ASSOC_BIT);
 	back_to_cached();
 }
+
+/*
+ * Load the entry for 'addr' into the TLB and wire the entry.
+ */
+void tlb_wire_entry(struct vm_area_struct *vma, unsigned long addr, pte_t pte)
+{
+	unsigned long status, flags;
+	int urb;
+
+	local_irq_save(flags);
+
+	/* Load the entry into the TLB */
+	__update_tlb(vma, addr, pte);
+
+	/* ... and wire it up. */
+	status = ctrl_inl(MMUCR);
+	urb = (status & MMUCR_URB) >> MMUCR_URB_SHIFT;
+	status &= ~MMUCR_URB;
+
+	/*
+	 * Make sure we're not trying to wire the last TLB entry slot.
+	 */
+	BUG_ON(!--urb);
+
+	urb = urb % MMUCR_URB_NENTRIES;
+
+	status |= (urb << MMUCR_URB_SHIFT);
+	ctrl_outl(status, MMUCR);
+	ctrl_barrier();
+
+	local_irq_restore(flags);
+}
+
+/*
+ * Unwire the last wired TLB entry.
+ *
+ * It should also be noted that it is not possible to wire and unwire
+ * TLB entries in an arbitrary order. If you wire TLB entry N, followed
+ * by entry N+1, you must unwire entry N+1 first, then entry N. In this
+ * respect, it works like a stack or LIFO queue.
+ */
+void tlb_unwire_entry(void)
+{
+	unsigned long status, flags;
+	int urb;
+
+	local_irq_save(flags);
+
+	status = ctrl_inl(MMUCR);
+	urb = (status & MMUCR_URB) >> MMUCR_URB_SHIFT;
+	status &= ~MMUCR_URB;
+
+	/*
+	 * Make sure we're not trying to unwire a TLB entry when none
+	 * have been wired.
+	 */
+	BUG_ON(urb++ == MMUCR_URB_NENTRIES);
+
+	urb = urb % MMUCR_URB_NENTRIES;
+
+	status |= (urb << MMUCR_URB_SHIFT);
+	ctrl_outl(status, MMUCR);
+	ctrl_barrier();
+
+	local_irq_restore(flags);
+}

commit 3ed6e129390fb872c3b7e05a232e5d380fbdfb48
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 29 22:06:58 2009 +0900

    sh: Handle a NULL vma in __update_tlb() for the fast-path.
    
    The TLB miss fast-path presently calls in to update_mmu_cache() to
    set up the entry, and does so with a NULL vma. Check for vma validity
    in the __update_tlb() ptrace checks.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-pteaex.c b/arch/sh/mm/tlb-pteaex.c
index 9aabd313cede..409b7c2b4b9d 100644
--- a/arch/sh/mm/tlb-pteaex.c
+++ b/arch/sh/mm/tlb-pteaex.c
@@ -23,7 +23,7 @@ void __update_tlb(struct vm_area_struct *vma, unsigned long address, pte_t pte)
 	/*
 	 * Handle debugger faulting in for debugee.
 	 */
-	if (current->active_mm != vma->vm_mm)
+	if (vma && current->active_mm != vma->vm_mm)
 		return;
 
 	local_irq_save(flags);

commit 9cef7492696a416663b4edb953a4eade8517ebeb
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 29 00:12:17 2009 +0900

    sh: update_mmu_cache() consolidation.
    
    This splits out a separate __update_cache()/__update_tlb() for
    update_mmu_cache() to wrap in to. This lets us share the common
    __update_cache() bits while keeping special __update_tlb() handling
    broken out.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-pteaex.c b/arch/sh/mm/tlb-pteaex.c
index c39b77363352..9aabd313cede 100644
--- a/arch/sh/mm/tlb-pteaex.c
+++ b/arch/sh/mm/tlb-pteaex.c
@@ -16,15 +16,14 @@
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
-void update_mmu_cache(struct vm_area_struct * vma,
-		      unsigned long address, pte_t pte)
+void __update_tlb(struct vm_area_struct *vma, unsigned long address, pte_t pte)
 {
-	unsigned long flags;
-	unsigned long pteval;
-	unsigned long vpn;
+	unsigned long flags, pteval, vpn;
 
-	/* Ptrace may call this routine. */
-	if (vma && current->active_mm != vma->vm_mm)
+	/*
+	 * Handle debugger faulting in for debugee.
+	 */
+	if (current->active_mm != vma->vm_mm)
 		return;
 
 	local_irq_save(flags);

commit 2277ab4a1df50e05bc732fe9488d4e902bb8399a
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 22 19:20:49 2009 +0900

    sh: Migrate from PG_mapped to PG_dcache_dirty.
    
    This inverts the delayed dcache flush a bit to be more in line with other
    platforms. At the same time this also gives us the ability to do some
    more optimizations and cleanup. Now that the update_mmu_cache() callsite
    only tests for the bit, the implementation can gradually be split out and
    made generic, rather than relying on special implementations for each of
    the peculiar CPU types.
    
    SH7705 in 32kB mode and SH-4 still need slightly different handling, but
    this is something that can remain isolated in the varying page copy/clear
    routines. On top of that, SH-X3 is dcache coherent, so there is no need
    to bother with any of these tests in the PTEAEX version of
    update_mmu_cache(), so we kill that off too.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-pteaex.c b/arch/sh/mm/tlb-pteaex.c
index 2aab3ea934d7..c39b77363352 100644
--- a/arch/sh/mm/tlb-pteaex.c
+++ b/arch/sh/mm/tlb-pteaex.c
@@ -27,23 +27,6 @@ void update_mmu_cache(struct vm_area_struct * vma,
 	if (vma && current->active_mm != vma->vm_mm)
 		return;
 
-#ifndef CONFIG_CACHE_OFF
-	{
-		unsigned long pfn = pte_pfn(pte);
-
-		if (pfn_valid(pfn)) {
-			struct page *page = pfn_to_page(pfn);
-
-			if (!test_bit(PG_mapped, &page->flags)) {
-				unsigned long phys = pte_val(pte) & PTE_PHYS_MASK;
-				__flush_wback_region((void *)P1SEGADDR(phys),
-						     PAGE_SIZE);
-				__set_bit(PG_mapped, &page->flags);
-			}
-		}
-	}
-#endif
-
 	local_irq_save(flags);
 
 	/* Set PTEH register */

commit c54a43e90b80993b2e0772d678563cb2bc6a1b3b
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Mar 17 17:58:33 2009 +0900

    sh: tlb-pteaex: Kill off legacy PTEA updates.
    
    While harmless, PTEA has different semantics on these parts, and is only
    used in extended TLB mode. Kill off the legacy support.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-pteaex.c b/arch/sh/mm/tlb-pteaex.c
index 5c9b2d781e08..2aab3ea934d7 100644
--- a/arch/sh/mm/tlb-pteaex.c
+++ b/arch/sh/mm/tlb-pteaex.c
@@ -64,9 +64,6 @@ void update_mmu_cache(struct vm_area_struct * vma,
 	 * and PR bits, which are cleared) being written out in PTEL.
 	 */
 	__raw_writel(pte.pte_high, MMU_PTEA);
-#else
-	/* TODO: make this look less hacky */
-	__raw_writel(((pteval >> 28) & 0xe) | (pteval & 0x1), MMU_PTEA);
 #endif
 
 	/* Set PTEL register */

commit 8263a67e169fdf0d06d172acbf6c03ae172a69d4
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Mar 17 17:49:49 2009 +0900

    sh: Support for extended ASIDs on PTEAEX-capable SH-X3 cores.
    
    This adds support for extended ASIDs (up to 16-bits) on newer SH-X3 cores
    that implement the PTAEX register and respective functionality. Presently
    only the 65nm SH7786 (90nm only supports legacy 8-bit ASIDs).
    
    The main change is in how the PTE is written out when loading the entry
    in to the TLB, as well as in how the TLB entry is selectively flushed.
    
    While SH-X2 extended mode splits out the memory-mapped U and I-TLB data
    arrays for extra bits, extended ASID mode splits out the address arrays.
    While we don't use the memory-mapped data array access, the address
    array accesses are necessary for selective TLB flushes, so these are
    implemented newly and replace the generic SH-4 implementation.
    
    With this, TLB flushes in switch_mm() are almost non-existent on newer
    parts.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-pteaex.c b/arch/sh/mm/tlb-pteaex.c
new file mode 100644
index 000000000000..5c9b2d781e08
--- /dev/null
+++ b/arch/sh/mm/tlb-pteaex.c
@@ -0,0 +1,99 @@
+/*
+ * arch/sh/mm/tlb-pteaex.c
+ *
+ * TLB operations for SH-X3 CPUs featuring PTE ASID Extensions.
+ *
+ * Copyright (C) 2009 Paul Mundt
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ */
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/io.h>
+#include <asm/system.h>
+#include <asm/mmu_context.h>
+#include <asm/cacheflush.h>
+
+void update_mmu_cache(struct vm_area_struct * vma,
+		      unsigned long address, pte_t pte)
+{
+	unsigned long flags;
+	unsigned long pteval;
+	unsigned long vpn;
+
+	/* Ptrace may call this routine. */
+	if (vma && current->active_mm != vma->vm_mm)
+		return;
+
+#ifndef CONFIG_CACHE_OFF
+	{
+		unsigned long pfn = pte_pfn(pte);
+
+		if (pfn_valid(pfn)) {
+			struct page *page = pfn_to_page(pfn);
+
+			if (!test_bit(PG_mapped, &page->flags)) {
+				unsigned long phys = pte_val(pte) & PTE_PHYS_MASK;
+				__flush_wback_region((void *)P1SEGADDR(phys),
+						     PAGE_SIZE);
+				__set_bit(PG_mapped, &page->flags);
+			}
+		}
+	}
+#endif
+
+	local_irq_save(flags);
+
+	/* Set PTEH register */
+	vpn = address & MMU_VPN_MASK;
+	__raw_writel(vpn, MMU_PTEH);
+
+	/* Set PTEAEX */
+	__raw_writel(get_asid(), MMU_PTEAEX);
+
+	pteval = pte.pte_low;
+
+	/* Set PTEA register */
+#ifdef CONFIG_X2TLB
+	/*
+	 * For the extended mode TLB this is trivial, only the ESZ and
+	 * EPR bits need to be written out to PTEA, with the remainder of
+	 * the protection bits (with the exception of the compat-mode SZ
+	 * and PR bits, which are cleared) being written out in PTEL.
+	 */
+	__raw_writel(pte.pte_high, MMU_PTEA);
+#else
+	/* TODO: make this look less hacky */
+	__raw_writel(((pteval >> 28) & 0xe) | (pteval & 0x1), MMU_PTEA);
+#endif
+
+	/* Set PTEL register */
+	pteval &= _PAGE_FLAGS_HARDWARE_MASK; /* drop software flags */
+#ifdef CONFIG_CACHE_WRITETHROUGH
+	pteval |= _PAGE_WT;
+#endif
+	/* conveniently, we want all the software flags to be 0 anyway */
+	__raw_writel(pteval, MMU_PTEL);
+
+	/* Load the TLB */
+	asm volatile("ldtlb": /* no output */ : /* no input */ : "memory");
+	local_irq_restore(flags);
+}
+
+/*
+ * While SH-X2 extended TLB mode splits out the memory-mapped I/UTLB
+ * data arrays, SH-X3 cores with PTEAEX split out the memory-mapped
+ * address arrays. In compat mode the second array is inaccessible, while
+ * in extended mode, the legacy 8-bit ASID field in address array 1 has
+ * undefined behaviour.
+ */
+void __uses_jump_to_uncached local_flush_tlb_one(unsigned long asid,
+						 unsigned long page)
+{
+	jump_to_uncached();
+	__raw_writel(page, MMU_UTLB_ADDRESS_ARRAY | MMU_PAGE_ASSOC_BIT);
+	__raw_writel(asid, MMU_UTLB_ADDRESS_ARRAY2 | MMU_PAGE_ASSOC_BIT);
+	back_to_cached();
+}
