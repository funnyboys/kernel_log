commit c1e8d7c6a7a682e1405e3e242d32fc377fd196ff
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:54 2020 -0700

    mmap locking API: convert mmap_sem comments
    
    Convert comments that reference mmap_sem to reference mmap_lock instead.
    
    [akpm@linux-foundation.org: fix up linux-next leftovers]
    [akpm@linux-foundation.org: s/lockaphore/lock/, per Vlastimil]
    [akpm@linux-foundation.org: more linux-next fixups, per Michel]
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Laurent Dufour <ldufour@linux.ibm.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-13-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index de9e0a60e119..ddfa9685f1ef 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -182,7 +182,7 @@ static void sh4_flush_cache_all(void *unused)
  * accessed with (hence cache set) is in accord with the physical
  * address (i.e. tag).  It's no different here.
  *
- * Caller takes mm->mmap_sem.
+ * Caller takes mm->mmap_lock.
  */
 static void sh4_flush_cache_mm(void *arg)
 {

commit e05c7b1f2bc4b7b28199b9a7572f73436d97317e
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:33:05 2020 -0700

    mm: pgtable: add shortcuts for accessing kernel PMD and PTE
    
    The powerpc 32-bit implementation of pgtable has nice shortcuts for
    accessing kernel PMD and PTE for a given virtual address.  Make these
    helpers available for all architectures.
    
    [rppt@linux.ibm.com: microblaze: fix page table traversal in setup_rt_frame()]
      Link: http://lkml.kernel.org/r/20200518191511.GD1118872@kernel.org
    [akpm@linux-foundation.org: s/pmd_ptr_k/pmd_off_k/ in various powerpc places]
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-9-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index ed599dadd698..de9e0a60e119 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -207,9 +207,6 @@ static void sh4_flush_cache_page(void *args)
 	struct page *page;
 	unsigned long address, pfn, phys;
 	int map_coherent = 0;
-	pgd_t *pgd;
-	p4d_t *p4d;
-	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
 	void *vaddr;
@@ -223,10 +220,7 @@ static void sh4_flush_cache_page(void *args)
 	if (cpu_context(smp_processor_id(), vma->vm_mm) == NO_CONTEXT)
 		return;
 
-	pgd = pgd_offset(vma->vm_mm, address);
-	p4d = p4d_offset(pgd, address);
-	pud = pud_offset(p4d, address);
-	pmd = pmd_offset(pud, address);
+	pmd = pmd_off(vma->vm_mm, address);
 	pte = pte_offset_kernel(pmd, address);
 
 	/* If the page isn't present, there is nothing to do here. */

commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 45943bcb7042..ed599dadd698 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -16,7 +16,6 @@
 #include <linux/mutex.h>
 #include <linux/fs.h>
 #include <linux/highmem.h>
-#include <asm/pgtable.h>
 #include <asm/mmu_context.h>
 #include <asm/cache_insns.h>
 #include <asm/cacheflush.h>

commit 874e2cc18972d30ecd4d572d1286fe9b594d309c
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Thu Jun 4 16:46:56 2020 -0700

    sh: add support for folded p4d page tables
    
    Implement primitives necessary for the 4th level folding, add walks of p4d
    level where appropriate and remove usage of __ARCH_USE_5LEVEL_HACK.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert+renesas@glider.be>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: James Morse <james.morse@arm.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Stefan Kristiansson <stefan.kristiansson@saunalahti.fi>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200414153455.21744-12-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index eee911422cf9..45943bcb7042 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -209,6 +209,7 @@ static void sh4_flush_cache_page(void *args)
 	unsigned long address, pfn, phys;
 	int map_coherent = 0;
 	pgd_t *pgd;
+	p4d_t *p4d;
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
@@ -224,7 +225,8 @@ static void sh4_flush_cache_page(void *args)
 		return;
 
 	pgd = pgd_offset(vma->vm_mm, address);
-	pud = pud_offset(pgd, address);
+	p4d = p4d_offset(pgd, address);
+	pud = pud_offset(p4d, address);
 	pmd = pmd_offset(pud, address);
 	pte = pte_offset_kernel(pmd, address);
 

commit cb9f753a3731f7fe16447bea45cb6f8e8bb432fb
Author: Huang Ying <ying.huang@intel.com>
Date:   Thu Apr 5 16:24:39 2018 -0700

    mm: fix races between swapoff and flush dcache
    
    Thanks to commit 4b3ef9daa4fc ("mm/swap: split swap cache into 64MB
    trunks"), after swapoff the address_space associated with the swap
    device will be freed.  So page_mapping() users which may touch the
    address_space need some kind of mechanism to prevent the address_space
    from being freed during accessing.
    
    The dcache flushing functions (flush_dcache_page(), etc) in architecture
    specific code may access the address_space of swap device for anonymous
    pages in swap cache via page_mapping() function.  But in some cases
    there are no mechanisms to prevent the swap device from being swapoff,
    for example,
    
      CPU1                                  CPU2
      __get_user_pages()                    swapoff()
        flush_dcache_page()
          mapping = page_mapping()
            ...                               exit_swap_address_space()
            ...                                 kvfree(spaces)
            mapping_mapped(mapping)
    
    The address space may be accessed after being freed.
    
    But from cachetlb.txt and Russell King, flush_dcache_page() only care
    about file cache pages, for anonymous pages, flush_anon_page() should be
    used.  The implementation of flush_dcache_page() in all architectures
    follows this too.  They will check whether page_mapping() is NULL and
    whether mapping_mapped() is true to determine whether to flush the
    dcache immediately.  And they will use interval tree (mapping->i_mmap)
    to find all user space mappings.  While mapping_mapped() and
    mapping->i_mmap isn't used by anonymous pages in swap cache at all.
    
    So, to fix the race between swapoff and flush dcache, __page_mapping()
    is add to return the address_space for file cache pages and NULL
    otherwise.  All page_mapping() invoking in flush dcache functions are
    replaced with page_mapping_file().
    
    [akpm@linux-foundation.org: simplify page_mapping_file(), per Mike]
    Link: http://lkml.kernel.org/r/20180305083634.15174-1-ying.huang@intel.com
    Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 58aaa4f33b81..eee911422cf9 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -112,7 +112,7 @@ static void sh4_flush_dcache_page(void *arg)
 	struct page *page = arg;
 	unsigned long addr = (unsigned long)page_address(page);
 #ifndef CONFIG_SMP
-	struct address_space *mapping = page_mapping(page);
+	struct address_space *mapping = page_mapping_file(page);
 
 	if (mapping && !mapping_mapped(mapping))
 		clear_bit(PG_dcache_clean, &page->flags);

commit e1534ae95004d6a307839a44eed40389d608c935
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Jan 15 16:53:46 2016 -0800

    mm: differentiate page_mapped() from page_mapcount() for compound pages
    
    Let's define page_mapped() to be true for compound pages if any
    sub-pages of the compound page is mapped (with PMD or PTE).
    
    On other hand page_mapcount() return mapcount for this particular small
    page.
    
    This will make cases like page_get_anon_vma() behave correctly once we
    allow huge pages to be mapped with PTE.
    
    Most users outside core-mm should use page_mapcount() instead of
    page_mapped().
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Tested-by: Sasha Levin <sasha.levin@oracle.com>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Jerome Marchand <jmarchan@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 51d8f7f31d1d..58aaa4f33b81 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -241,7 +241,7 @@ static void sh4_flush_cache_page(void *args)
 		 */
 		map_coherent = (current_cpu_data.dcache.n_aliases &&
 			test_bit(PG_dcache_clean, &page->flags) &&
-			page_mapped(page));
+			page_mapcount(page));
 		if (map_coherent)
 			vaddr = kmap_coherent(page, address);
 		else

commit a5f6ea29f9a918403629f8369ae55fac6b09cb53
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Mon Mar 3 15:38:33 2014 -0800

    sh: prefix sh-specific "CCR" and "CCR2" by "SH_"
    
    Commit bcf24e1daa94 ("mmc: omap_hsmmc: use the generic config for
    omap2plus devices"), enabled the build for other platforms for compile
    testing.
    
    sh-allmodconfig now fails with:
    
        include/linux/omap-dma.h:171:8: error: expected identifier before numeric constant
        make[4]: *** [drivers/mmc/host/omap_hsmmc.o] Error 1
    
    This happens because SuperH #defines "CCR", which is one of the enum
    values in include/linux/omap-dma.h.  There's a similar issue with "CCR2"
    on sh2a.
    
    As "CCR" and "CCR2" are too generic names for global #defines, prefix
    them with "SH_" to fix this.
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 0e529285b28d..51d8f7f31d1d 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -133,9 +133,9 @@ static void flush_icache_all(void)
 	jump_to_uncached();
 
 	/* Flush I-cache */
-	ccr = __raw_readl(CCR);
+	ccr = __raw_readl(SH_CCR);
 	ccr |= CCR_CACHE_ICI;
-	__raw_writel(ccr, CCR);
+	__raw_writel(ccr, SH_CCR);
 
 	/*
 	 * back_to_cached() will take care of the barrier for us, don't add

commit f03c4866d31e913a8dbc84f7d1459abdaf0bd326
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Mar 30 19:29:57 2012 +0900

    sh: fix up fallout from system.h disintegration.
    
    Quite a bit of fallout all over the place, nothing terribly exciting.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 112fea12522a..0e529285b28d 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -18,6 +18,7 @@
 #include <linux/highmem.h>
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>
+#include <asm/cache_insns.h>
 #include <asm/cacheflush.h>
 
 /*

commit bc3e11be88010e09692ed1d214407d56caa90075
Author: Cong Wang <amwang@redhat.com>
Date:   Fri Nov 25 23:14:16 2011 +0800

    sh: remove the second argument of k[un]map_atomic()
    
    Signed-off-by: Cong Wang <amwang@redhat.com>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 92eb98633ab0..112fea12522a 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -244,7 +244,7 @@ static void sh4_flush_cache_page(void *args)
 		if (map_coherent)
 			vaddr = kmap_coherent(page, address);
 		else
-			vaddr = kmap_atomic(page, KM_USER0);
+			vaddr = kmap_atomic(page);
 
 		address = (unsigned long)vaddr;
 	}
@@ -259,7 +259,7 @@ static void sh4_flush_cache_page(void *args)
 		if (map_coherent)
 			kunmap_coherent(vaddr);
 		else
-			kunmap_atomic(vaddr, KM_USER0);
+			kunmap_atomic(vaddr);
 	}
 }
 

commit 55661fc1f105ed75852e937bf8ea408270eb0cca
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Dec 1 15:39:51 2010 +0900

    sh: Assume new page cache pages have dirty dcache lines.
    
    This follows the ARM change c01778001a4f5ad9c62d882776235f3f31922fdd
    ("ARM: 6379/1: Assume new page cache pages have dirty D-cache") for the
    same rationale:
    
        There are places in Linux where writes to newly allocated page
        cache pages happen without a subsequent call to flush_dcache_page()
        (several PIO drivers including USB HCD). This patch changes the
        meaning of PG_arch_1 to be PG_dcache_clean and always flush the
        D-cache for a newly mapped page in update_mmu_cache().
    
    This addresses issues seen with executing binaries from MMC, in
    addition to some of the other HCDs that don't explicitly do cache
    management for their pipe-in buffers.
    
    Requested-by: Yoshihiro Shimoda <yoshihiro.shimoda.uh@renesas.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 2cfae81914aa..92eb98633ab0 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -114,7 +114,7 @@ static void sh4_flush_dcache_page(void *arg)
 	struct address_space *mapping = page_mapping(page);
 
 	if (mapping && !mapping_mapped(mapping))
-		set_bit(PG_dcache_dirty, &page->flags);
+		clear_bit(PG_dcache_clean, &page->flags);
 	else
 #endif
 		flush_cache_one(CACHE_OC_ADDRESS_ARRAY |
@@ -239,7 +239,7 @@ static void sh4_flush_cache_page(void *args)
 		 * another ASID than the current one.
 		 */
 		map_coherent = (current_cpu_data.dcache.n_aliases &&
-			!test_bit(PG_dcache_dirty, &page->flags) &&
+			test_bit(PG_dcache_clean, &page->flags) &&
 			page_mapped(page));
 		if (map_coherent)
 			vaddr = kmap_coherent(page, address);

commit 9d56dd3b083a3bec56e9da35ce07baca81030b03
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jan 26 12:58:40 2010 +0900

    sh: Mass ctrl_in/outX to __raw_read/writeX conversion.
    
    The old ctrl in/out routines are non-portable and unsuitable for
    cross-platform use. While drivers/sh has already been sanitized, there
    is still quite a lot of code that is not. This converts the arch/sh/ bits
    over, which permits us to flag the routines as deprecated whilst still
    building with -Werror for the architecture code, and to ensure that
    future users are not added.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 87115b3ee70e..2cfae81914aa 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -132,9 +132,9 @@ static void flush_icache_all(void)
 	jump_to_uncached();
 
 	/* Flush I-cache */
-	ccr = ctrl_inl(CCR);
+	ccr = __raw_readl(CCR);
 	ccr |= CCR_CACHE_ICI;
-	ctrl_outl(ccr, CCR);
+	__raw_writel(ccr, CCR);
 
 	/*
 	 * back_to_cached() will take care of the barrier for us, don't add
@@ -377,9 +377,9 @@ extern void __weak sh4__flush_region_init(void);
 void __init sh4_cache_init(void)
 {
 	printk("PVR=%08x CVR=%08x PRR=%08x\n",
-		ctrl_inl(CCN_PVR),
-		ctrl_inl(CCN_CVR),
-		ctrl_inl(CCN_PRR));
+		__raw_readl(CCN_PVR),
+		__raw_readl(CCN_CVR),
+		__raw_readl(CCN_PRR));
 
 	local_flush_icache_range	= sh4_flush_icache_range;
 	local_flush_dcache_page		= sh4_flush_dcache_page;

commit 2dc2f8e0c46864e2a3722c84eaa96513d4cf8b2f
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Jan 21 16:05:25 2010 +0900

    sh: Kill off the special uncached section and fixmap.
    
    Now that cached_to_uncached works as advertized in 32-bit mode and we're
    never going to be able to map < 16MB anyways, there's no need for the
    special uncached section. Kill it off.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index a2301daeefa3..87115b3ee70e 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -36,7 +36,7 @@ static void __flush_cache_one(unsigned long addr, unsigned long phys,
  * Called from kernel/module.c:sys_init_module and routine for a.out format,
  * signal handler code and kprobes code
  */
-static void __uses_jump_to_uncached sh4_flush_icache_range(void *args)
+static void sh4_flush_icache_range(void *args)
 {
 	struct flusher_data *data = args;
 	unsigned long start, end;
@@ -124,7 +124,7 @@ static void sh4_flush_dcache_page(void *arg)
 }
 
 /* TODO: Selective icache invalidation through IC address array.. */
-static void __uses_jump_to_uncached flush_icache_all(void)
+static void flush_icache_all(void)
 {
 	unsigned long flags, ccr;
 

commit b4c892762373c5e59c7e8db35f5f9a7658602bda
Author: Matt Fleming <matt@console-pimps.org>
Date:   Thu Dec 24 22:17:35 2009 +0000

    sh: Optimise flush_dcache_page() on SH4
    
    If the page is not mapped into any process's address space then aliases
    cannot exist in the cache. So reduce the amount of flushing we perform.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 560ddb6bc8a7..a2301daeefa3 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -109,6 +109,7 @@ static inline void flush_cache_one(unsigned long start, unsigned long phys)
 static void sh4_flush_dcache_page(void *arg)
 {
 	struct page *page = arg;
+	unsigned long addr = (unsigned long)page_address(page);
 #ifndef CONFIG_SMP
 	struct address_space *mapping = page_mapping(page);
 
@@ -116,16 +117,8 @@ static void sh4_flush_dcache_page(void *arg)
 		set_bit(PG_dcache_dirty, &page->flags);
 	else
 #endif
-	{
-		unsigned long phys = page_to_phys(page);
-		unsigned long addr = CACHE_OC_ADDRESS_ARRAY;
-		int i, n;
-
-		/* Loop all the D-cache */
-		n = boot_cpu_data.dcache.n_aliases;
-		for (i = 0; i < n; i++, addr += PAGE_SIZE)
-			flush_cache_one(addr, phys);
-	}
+		flush_cache_one(CACHE_OC_ADDRESS_ARRAY |
+				(addr & shm_align_mask), page_to_phys(page));
 
 	wmb();
 }

commit e717cc6c07f006be36e35189aacb28be4e30ad14
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Dec 8 14:23:11 2009 +0000

    sh: Can't compare physical and virtual addresses for aliases
    
    It does not make sense to compare virtual and physical addresses for
    aliasing, only virtual addresses can be compared for aliases.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index f36a08bf3d5c..560ddb6bc8a7 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -256,8 +256,7 @@ static void sh4_flush_cache_page(void *args)
 		address = (unsigned long)vaddr;
 	}
 
-	if (pages_do_alias(address, phys))
-		flush_cache_one(CACHE_OC_ADDRESS_ARRAY |
+	flush_cache_one(CACHE_OC_ADDRESS_ARRAY |
 			(address & shm_align_mask), phys);
 
 	if (vma->vm_flags & VM_EXEC)

commit a781d1e5ff6277f80ff3c9503775521bc64cf131
Author: Matt Fleming <matt@console-pimps.org>
Date:   Fri Dec 4 16:18:11 2009 +0900

    sh: Drop associative writes for SH-4 cache flushes.
    
    When flushing/invalidating the icache/dcache via the memory-mapped IC/OC
    address arrays, the associative bit should only be used in conjunction with
    virtual addresses. However, we currently flush cache lines based on physical
    address, so stop using the associative bit.
    
    It is a better strategy to use non-associative writes (and physical tags) for
    flushing the caches anyway, because flushing by virtual address (as with the
    A-bit set) requires a valid TLB entry for that virtual address. If one does not
    exist in the TLB no exception is generated and the flush is silently ignored.
    
    This is also future-proofing for SH-4A parts which are gradually phasing out
    associative writes to the cache array due to the aforementioned case of certain
    flushes silently turning in to nops.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 6bfd08d5fb81..f36a08bf3d5c 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -98,7 +98,7 @@ static inline void flush_cache_one(unsigned long start, unsigned long phys)
 		exec_offset = cached_to_uncached;
 
 	local_irq_save(flags);
-	__flush_cache_one(start | SH_CACHE_ASSOC, phys, exec_offset);
+	__flush_cache_one(start, phys, exec_offset);
 	local_irq_restore(flags);
 }
 
@@ -123,7 +123,7 @@ static void sh4_flush_dcache_page(void *arg)
 
 		/* Loop all the D-cache */
 		n = boot_cpu_data.dcache.n_aliases;
-		for (i = 0; i <= n; i++, addr += PAGE_SIZE)
+		for (i = 0; i < n; i++, addr += PAGE_SIZE)
 			flush_cache_one(addr, phys);
 	}
 

commit 76d2318020bf0c0c497af986a25977196715a1b9
Merge: e9c4148fd4f0 969e46a8533a
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Nov 9 10:55:36 2009 +0900

    Merge branch 'sh/stable-updates'

commit a9d244a2ff163247b607c4bb64803230ca8f8acb
Author: Matt Fleming <matt@console-pimps.org>
Date:   Thu Nov 5 23:14:39 2009 +0000

    sh: Account for cache aliases in flush_icache_range()
    
    The icache may also contain aliases so we must account for them just
    like we do when manipulating the dcache. We usually get away with
    aliases in the icache because the instructions that are read from memory
    are read-only, i.e. they never change. However, the place where this
    bites us is when the code has been modified.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 519e2d16cd06..b7f235c74d66 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -72,6 +72,7 @@ static void __uses_jump_to_uncached sh4_flush_icache_range(void *args)
 
 	for (v = start; v < end; v += L1_CACHE_BYTES) {
 		unsigned long icacheaddr;
+		int j, n;
 
 		__ocbwb(v);
 
@@ -79,8 +80,10 @@ static void __uses_jump_to_uncached sh4_flush_icache_range(void *args)
 				cpu_data->icache.entry_mask);
 
 		/* Clear i-cache line valid-bit */
+		n = boot_cpu_data.icache.n_aliases;
 		for (i = 0; i < cpu_data->icache.ways; i++) {
-			__raw_writel(0, icacheaddr);
+			for (j = 0; j < n; j++)
+				__raw_writel(0, icacheaddr + (j * PAGE_SIZE));
 			icacheaddr += cpu_data->icache.way_incr;
 		}
 	}

commit eb3118f652ea7751ecf6a7e467bb637895e3be3b
Author: Matt Fleming <matt@console-pimps.org>
Date:   Thu Oct 29 21:53:30 2009 +0000

    sh: Do not apply virt_to_phys() to a physical address
    
    The variable 'phys' already contains the physical address to flush. It
    is not a virtual address and should not be passed to virt_to_phys().
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 4a2fbf2864de..b5abe949c6ed 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -95,8 +95,7 @@ static inline void flush_cache_one(unsigned long start, unsigned long phys)
 		exec_offset = cached_to_uncached;
 
 	local_irq_save(flags);
-	__flush_cache_one(start | SH_CACHE_ASSOC,
-			  virt_to_phys(phys), exec_offset);
+	__flush_cache_one(start | SH_CACHE_ASSOC, phys, exec_offset);
 	local_irq_restore(flags);
 }
 

commit abeaf33a4101764291ec79cf286e08c0966eb26e
Merge: 731ba3301de4 52a94909f00e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Oct 16 15:14:50 2009 +0900

    Merge branch 'sh/stable-updates'
    
    Conflicts:
            arch/sh/mm/cache-sh4.c

commit a7a7c0e1d12bcfb4a96cae439951232b08c91841
Author: Valentin Sitdikov <valentin.sitdikov@siemens.com>
Date:   Fri Oct 16 14:15:38 2009 +0900

    sh: Fix up single page flushing to use PAGE_SIZE.
    
    Presently The SH-4 cache flushing code uses flush_cache_4096() for most
    of the real flushing work, which breaks down to a fixed 4096 unroll and
    increment. Not only is this sub-optimal for larger page sizes, it's also
    uncovered a bug in sh4_flush_dcache_page() when large page sizes are used
    and we have no cache aliases -- resulting in only a part of the page's
    D-cache lines being written back.
    
    Signed-off-by: Valentin Sitdikov <valentin.sitdikov@siemens.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index a98c7d8984fa..519e2d16cd06 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -26,7 +26,7 @@
 #define MAX_DCACHE_PAGES	64	/* XXX: Tune for ways */
 #define MAX_ICACHE_PAGES	32
 
-static void __flush_cache_4096(unsigned long addr, unsigned long phys,
+static void __flush_cache_one(unsigned long addr, unsigned long phys,
 			       unsigned long exec_offset);
 
 /*
@@ -89,8 +89,7 @@ static void __uses_jump_to_uncached sh4_flush_icache_range(void *args)
 	local_irq_restore(flags);
 }
 
-static inline void flush_cache_4096(unsigned long start,
-				    unsigned long phys)
+static inline void flush_cache_one(unsigned long start, unsigned long phys)
 {
 	unsigned long flags, exec_offset = 0;
 
@@ -103,8 +102,7 @@ static inline void flush_cache_4096(unsigned long start,
 		exec_offset = 0x20000000;
 
 	local_irq_save(flags);
-	__flush_cache_4096(start | SH_CACHE_ASSOC,
-			   P1SEGADDR(phys), exec_offset);
+	__flush_cache_one(start | SH_CACHE_ASSOC, P1SEGADDR(phys), exec_offset);
 	local_irq_restore(flags);
 }
 
@@ -129,8 +127,8 @@ static void sh4_flush_dcache_page(void *arg)
 
 		/* Loop all the D-cache */
 		n = boot_cpu_data.dcache.n_aliases;
-		for (i = 0; i < n; i++, addr += 4096)
-			flush_cache_4096(addr, phys);
+		for (i = 0; i < n; i++, addr += PAGE_SIZE)
+			flush_cache_one(addr, phys);
 	}
 
 	wmb();
@@ -318,11 +316,11 @@ static void sh4_flush_cache_page(void *args)
 	/* We only need to flush D-cache when we have alias */
 	if ((address^phys) & alias_mask) {
 		/* Loop 4K of the D-cache */
-		flush_cache_4096(
+		flush_cache_one(
 			CACHE_OC_ADDRESS_ARRAY | (address & alias_mask),
 			phys);
 		/* Loop another 4K of the D-cache */
-		flush_cache_4096(
+		flush_cache_one(
 			CACHE_OC_ADDRESS_ARRAY | (phys & alias_mask),
 			phys);
 	}
@@ -337,7 +335,7 @@ static void sh4_flush_cache_page(void *args)
 		 * kernel has never executed the code through its identity
 		 * translation.
 		 */
-		flush_cache_4096(
+		flush_cache_one(
 			CACHE_IC_ADDRESS_ARRAY | (address & alias_mask),
 			phys);
 	}
@@ -393,7 +391,7 @@ static void sh4_flush_cache_range(void *args)
 }
 
 /**
- * __flush_cache_4096
+ * __flush_cache_one
  *
  * @addr:  address in memory mapped cache array
  * @phys:  P1 address to flush (has to match tags if addr has 'A' bit
@@ -406,7 +404,7 @@ static void sh4_flush_cache_range(void *args)
  * operation (purge/write-back) is selected by the lower 2 bits of
  * 'phys'.
  */
-static void __flush_cache_4096(unsigned long addr, unsigned long phys,
+static void __flush_cache_one(unsigned long addr, unsigned long phys,
 			       unsigned long exec_offset)
 {
 	int way_count;

commit 1f69b6af9171f50135cce8023c84d82fbf42a8f5
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:25 2009 +0000

    sh: Prepare for dynamic PMB support
    
    To allow the MMU to be switched between 29bit and 32bit mode at runtime
    some constants need to swapped for functions that return a runtime
    value.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 639bb329fc81..56dd55a1b13e 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -88,12 +88,12 @@ static inline void flush_cache_4096(unsigned long start,
 	unsigned long flags, exec_offset = 0;
 
 	/*
-	 * All types of SH-4 require PC to be in P2 to operate on the I-cache.
-	 * Some types of SH-4 require PC to be in P2 to operate on the D-cache.
+	 * All types of SH-4 require PC to be uncached to operate on the I-cache.
+	 * Some types of SH-4 require PC to be uncached to operate on the D-cache.
 	 */
 	if ((boot_cpu_data.flags & CPU_HAS_P2_FLUSH_BUG) ||
 	    (start < CACHE_OC_ADDRESS_ARRAY))
-		exec_offset = 0x20000000;
+		exec_offset = cached_to_uncached;
 
 	local_irq_save(flags);
 	__flush_cache_4096(start | SH_CACHE_ASSOC,

commit 8bd642b17bea31f8361b61c16c8d154638414df4
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:24 2009 +0000

    sh: Obliterate the P1 area macros
    
    Replace the use of PHYSADDR() with __pa(). PHYSADDR() is based on the
    idea that all addresses in P1SEG are untranslated, so we can access an
    address's physical page as an offset from P1SEG. This doesn't work for
    CONFIG_PMB/CONFIG_PMB_FIXED because pages in P1SEG and P2SEG are used
    for PMB mappings and so can be translated to any physical address.
    
    Likewise, replace a P1SEGADDR() use with virt_to_phys().
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 60588c5bf7f9..639bb329fc81 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -97,7 +97,7 @@ static inline void flush_cache_4096(unsigned long start,
 
 	local_irq_save(flags);
 	__flush_cache_4096(start | SH_CACHE_ASSOC,
-			   P1SEGADDR(phys), exec_offset);
+			   virt_to_phys(phys), exec_offset);
 	local_irq_restore(flags);
 }
 

commit 5e3679c594e3a9bf819347bc59f70e03f2c6b272
Merge: a469f627c15d f9e2bdfdbb4c
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Oct 10 21:36:53 2009 +0900

    Merge branch 'sh/cachetlb'

commit a6325247f50628c7e53a483807d0ef2c24a7aa90
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:21 2009 +0000

    sh: Sprinkle __uses_jump_to_uncached
    
    Fix some callers of jump_to_uncached() and back_to_cached() that were
    not annotated with __uses_jump_to_uncached.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index b2453bbef4cd..a98c7d8984fa 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -43,7 +43,7 @@ static void (*__flush_dcache_segment_fn)(unsigned long, unsigned long) =
  * Called from kernel/module.c:sys_init_module and routine for a.out format,
  * signal handler code and kprobes code
  */
-static void sh4_flush_icache_range(void *args)
+static void __uses_jump_to_uncached sh4_flush_icache_range(void *args)
 {
 	struct flusher_data *data = args;
 	unsigned long start, end;

commit c4845a4b2288a9e5d96a0558e474809028c8aff3
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 9 17:13:07 2009 +0900

    sh: Fix up redundant cache flushing for PAGE_SIZE > 4k.
    
    If PAGE_SIZE is presently over 4k we do a lot of extra flushing given
    that we purge the cache 4k at a time. Make it explicitly 4k per
    iteration, rather than iterating for PAGE_SIZE before looping over again.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index e3fbd99b323c..8362d312ad94 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -357,7 +357,7 @@ static void __flush_cache_4096(unsigned long addr, unsigned long phys,
 	 * pointless nead-of-loop check for 0 iterations.
 	 */
 	do {
-		ea = base_addr + PAGE_SIZE;
+		ea = base_addr + 4096;
 		a = base_addr;
 		p = phys;
 

commit deaef20e9789d93c06d2d3b5ffc99939814802ca
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 9 16:06:39 2009 +0900

    sh: Rework sh4_flush_cache_page() for coherent kmap mapping.
    
    This builds on top of the MIPS r4k code that does roughly the same thing.
    This permits the use of kmap_coherent() for mapped pages with dirty
    dcache lines and falls back on kmap_atomic() otherwise.
    
    This also fixes up a problem with the alias check and defers to
    shm_align_mask directly.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 92b7d947db94..e3fbd99b323c 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -2,7 +2,7 @@
  * arch/sh/mm/cache-sh4.c
  *
  * Copyright (C) 1999, 2000, 2002  Niibe Yutaka
- * Copyright (C) 2001 - 2007  Paul Mundt
+ * Copyright (C) 2001 - 2009  Paul Mundt
  * Copyright (C) 2003  Richard Curnow
  * Copyright (c) 2007 STMicroelectronics (R&D) Ltd.
  *
@@ -15,6 +15,8 @@
 #include <linux/io.h>
 #include <linux/mutex.h>
 #include <linux/fs.h>
+#include <linux/highmem.h>
+#include <asm/pgtable.h>
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
@@ -23,7 +25,6 @@
  * flushing. Anything exceeding this will simply flush the dcache in its
  * entirety.
  */
-#define MAX_DCACHE_PAGES	64	/* XXX: Tune for ways */
 #define MAX_ICACHE_PAGES	32
 
 static void __flush_cache_4096(unsigned long addr, unsigned long phys,
@@ -209,44 +210,64 @@ static void sh4_flush_cache_page(void *args)
 {
 	struct flusher_data *data = args;
 	struct vm_area_struct *vma;
+	struct page *page;
 	unsigned long address, pfn, phys;
-	unsigned int alias_mask;
+	int map_coherent = 0;
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+	void *vaddr;
 
 	vma = data->vma;
 	address = data->addr1;
 	pfn = data->addr2;
 	phys = pfn << PAGE_SHIFT;
+	page = pfn_to_page(pfn);
 
 	if (cpu_context(smp_processor_id(), vma->vm_mm) == NO_CONTEXT)
 		return;
 
-	alias_mask = boot_cpu_data.dcache.alias_mask;
-
-	/* We only need to flush D-cache when we have alias */
-	if ((address^phys) & alias_mask) {
-		/* Loop 4K of the D-cache */
-		flush_cache_4096(
-			CACHE_OC_ADDRESS_ARRAY | (address & alias_mask),
-			phys);
-		/* Loop another 4K of the D-cache */
-		flush_cache_4096(
-			CACHE_OC_ADDRESS_ARRAY | (phys & alias_mask),
-			phys);
-	}
+	address &= PAGE_MASK;
+	pgd = pgd_offset(vma->vm_mm, address);
+	pud = pud_offset(pgd, address);
+	pmd = pmd_offset(pud, address);
+	pte = pte_offset_kernel(pmd, address);
+
+	/* If the page isn't present, there is nothing to do here. */
+	if (!(pte_val(*pte) & _PAGE_PRESENT))
+		return;
 
-	alias_mask = boot_cpu_data.icache.alias_mask;
-	if (vma->vm_flags & VM_EXEC) {
+	if ((vma->vm_mm == current->active_mm))
+		vaddr = NULL;
+	else {
 		/*
-		 * Evict entries from the portion of the cache from which code
-		 * may have been executed at this address (virtual).  There's
-		 * no need to evict from the portion corresponding to the
-		 * physical address as for the D-cache, because we know the
-		 * kernel has never executed the code through its identity
-		 * translation.
+		 * Use kmap_coherent or kmap_atomic to do flushes for
+		 * another ASID than the current one.
 		 */
-		flush_cache_4096(
-			CACHE_IC_ADDRESS_ARRAY | (address & alias_mask),
-			phys);
+		map_coherent = (current_cpu_data.dcache.n_aliases &&
+			!test_bit(PG_dcache_dirty, &page->flags) &&
+			page_mapped(page));
+		if (map_coherent)
+			vaddr = kmap_coherent(page, address);
+		else
+			vaddr = kmap_atomic(page, KM_USER0);
+
+		address = (unsigned long)vaddr;
+	}
+
+	if (pages_do_alias(address, phys))
+		flush_cache_4096(CACHE_OC_ADDRESS_ARRAY |
+			(address & shm_align_mask), phys);
+
+	if (vma->vm_flags & VM_EXEC)
+		flush_icache_all();
+
+	if (vaddr) {
+		if (map_coherent)
+			kunmap_coherent(vaddr);
+		else
+			kunmap_atomic(vaddr, KM_USER0);
 	}
 }
 

commit bd6df57481b329dfeeb4889068848ee4f4761561
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 9 14:22:15 2009 +0900

    sh: Kill off segment-based d-cache flushing on SH-4.
    
    This kills off the unrolled segment based flushers on SH-4 and switches
    over to a generic unrolled approach derived from the writethrough segment
    flusher.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index f0999606686f..92b7d947db94 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -29,14 +29,6 @@
 static void __flush_cache_4096(unsigned long addr, unsigned long phys,
 			       unsigned long exec_offset);
 
-/*
- * This is initialised here to ensure that it is not placed in the BSS.  If
- * that were to happen, note that cache_init gets called before the BSS is
- * cleared, so this would get nulled out which would be hopeless.
- */
-static void (*__flush_dcache_segment_fn)(unsigned long, unsigned long) =
-	(void (*)(unsigned long, unsigned long))0xdeadbeef;
-
 /*
  * Write back the range of D-cache, and purge the I-cache.
  *
@@ -158,10 +150,27 @@ static void __uses_jump_to_uncached flush_icache_all(void)
 	local_irq_restore(flags);
 }
 
-static inline void flush_dcache_all(void)
+static void flush_dcache_all(void)
 {
-	(*__flush_dcache_segment_fn)(0UL, boot_cpu_data.dcache.way_size);
-	wmb();
+	unsigned long addr, end_addr, entry_offset;
+
+	end_addr = CACHE_OC_ADDRESS_ARRAY +
+		(current_cpu_data.dcache.sets <<
+		 current_cpu_data.dcache.entry_shift) *
+			current_cpu_data.dcache.ways;
+
+	entry_offset = 1 << current_cpu_data.dcache.entry_shift;
+
+	for (addr = CACHE_OC_ADDRESS_ARRAY; addr < end_addr; ) {
+		__raw_writel(0, addr); addr += entry_offset;
+		__raw_writel(0, addr); addr += entry_offset;
+		__raw_writel(0, addr); addr += entry_offset;
+		__raw_writel(0, addr); addr += entry_offset;
+		__raw_writel(0, addr); addr += entry_offset;
+		__raw_writel(0, addr); addr += entry_offset;
+		__raw_writel(0, addr); addr += entry_offset;
+		__raw_writel(0, addr); addr += entry_offset;
+	}
 }
 
 static void sh4_flush_cache_all(void *unused)
@@ -347,245 +356,6 @@ static void __flush_cache_4096(unsigned long addr, unsigned long phys,
 	} while (--way_count != 0);
 }
 
-/*
- * Break the 1, 2 and 4 way variants of this out into separate functions to
- * avoid nearly all the overhead of having the conditional stuff in the function
- * bodies (+ the 1 and 2 way cases avoid saving any registers too).
- *
- * We want to eliminate unnecessary bus transactions, so this code uses
- * a non-obvious technique.
- *
- * Loop over a cache way sized block of, one cache line at a time. For each
- * line, use movca.a to cause the current cache line contents to be written
- * back, but without reading anything from main memory. However this has the
- * side effect that the cache is now caching that memory location. So follow
- * this with a cache invalidate to mark the cache line invalid. And do all
- * this with interrupts disabled, to avoid the cache line being accidently
- * evicted while it is holding garbage.
- *
- * This also breaks in a number of circumstances:
- * - if there are modifications to the region of memory just above
- *   empty_zero_page (for example because a breakpoint has been placed
- *   there), then these can be lost.
- *
- *   This is because the the memory address which the cache temporarily
- *   caches in the above description is empty_zero_page. So the
- *   movca.l hits the cache (it is assumed that it misses, or at least
- *   isn't dirty), modifies the line and then invalidates it, losing the
- *   required change.
- *
- * - If caches are disabled or configured in write-through mode, then
- *   the movca.l writes garbage directly into memory.
- */
-static void __flush_dcache_segment_writethrough(unsigned long start,
-					        unsigned long extent_per_way)
-{
-	unsigned long addr;
-	int i;
-
-	addr = CACHE_OC_ADDRESS_ARRAY | (start & cpu_data->dcache.entry_mask);
-
-	while (extent_per_way) {
-		for (i = 0; i < cpu_data->dcache.ways; i++)
-			__raw_writel(0, addr + cpu_data->dcache.way_incr * i);
-
-		addr += cpu_data->dcache.linesz;
-		extent_per_way -= cpu_data->dcache.linesz;
-	}
-}
-
-static void __flush_dcache_segment_1way(unsigned long start,
-					unsigned long extent_per_way)
-{
-	unsigned long orig_sr, sr_with_bl;
-	unsigned long base_addr;
-	unsigned long way_incr, linesz, way_size;
-	struct cache_info *dcache;
-	register unsigned long a0, a0e;
-
-	asm volatile("stc sr, %0" : "=r" (orig_sr));
-	sr_with_bl = orig_sr | (1<<28);
-	base_addr = ((unsigned long)&empty_zero_page[0]);
-
-	/*
-	 * The previous code aligned base_addr to 16k, i.e. the way_size of all
-	 * existing SH-4 D-caches.  Whilst I don't see a need to have this
-	 * aligned to any better than the cache line size (which it will be
-	 * anyway by construction), let's align it to at least the way_size of
-	 * any existing or conceivable SH-4 D-cache.  -- RPC
-	 */
-	base_addr = ((base_addr >> 16) << 16);
-	base_addr |= start;
-
-	dcache = &boot_cpu_data.dcache;
-	linesz = dcache->linesz;
-	way_incr = dcache->way_incr;
-	way_size = dcache->way_size;
-
-	a0 = base_addr;
-	a0e = base_addr + extent_per_way;
-	do {
-		asm volatile("ldc %0, sr" : : "r" (sr_with_bl));
-		asm volatile("movca.l r0, @%0\n\t"
-			     "ocbi @%0" : : "r" (a0));
-		a0 += linesz;
-		asm volatile("movca.l r0, @%0\n\t"
-			     "ocbi @%0" : : "r" (a0));
-		a0 += linesz;
-		asm volatile("movca.l r0, @%0\n\t"
-			     "ocbi @%0" : : "r" (a0));
-		a0 += linesz;
-		asm volatile("movca.l r0, @%0\n\t"
-			     "ocbi @%0" : : "r" (a0));
-		asm volatile("ldc %0, sr" : : "r" (orig_sr));
-		a0 += linesz;
-	} while (a0 < a0e);
-}
-
-static void __flush_dcache_segment_2way(unsigned long start,
-					unsigned long extent_per_way)
-{
-	unsigned long orig_sr, sr_with_bl;
-	unsigned long base_addr;
-	unsigned long way_incr, linesz, way_size;
-	struct cache_info *dcache;
-	register unsigned long a0, a1, a0e;
-
-	asm volatile("stc sr, %0" : "=r" (orig_sr));
-	sr_with_bl = orig_sr | (1<<28);
-	base_addr = ((unsigned long)&empty_zero_page[0]);
-
-	/* See comment under 1-way above */
-	base_addr = ((base_addr >> 16) << 16);
-	base_addr |= start;
-
-	dcache = &boot_cpu_data.dcache;
-	linesz = dcache->linesz;
-	way_incr = dcache->way_incr;
-	way_size = dcache->way_size;
-
-	a0 = base_addr;
-	a1 = a0 + way_incr;
-	a0e = base_addr + extent_per_way;
-	do {
-		asm volatile("ldc %0, sr" : : "r" (sr_with_bl));
-		asm volatile("movca.l r0, @%0\n\t"
-			     "movca.l r0, @%1\n\t"
-			     "ocbi @%0\n\t"
-			     "ocbi @%1" : :
-			     "r" (a0), "r" (a1));
-		a0 += linesz;
-		a1 += linesz;
-		asm volatile("movca.l r0, @%0\n\t"
-			     "movca.l r0, @%1\n\t"
-			     "ocbi @%0\n\t"
-			     "ocbi @%1" : :
-			     "r" (a0), "r" (a1));
-		a0 += linesz;
-		a1 += linesz;
-		asm volatile("movca.l r0, @%0\n\t"
-			     "movca.l r0, @%1\n\t"
-			     "ocbi @%0\n\t"
-			     "ocbi @%1" : :
-			     "r" (a0), "r" (a1));
-		a0 += linesz;
-		a1 += linesz;
-		asm volatile("movca.l r0, @%0\n\t"
-			     "movca.l r0, @%1\n\t"
-			     "ocbi @%0\n\t"
-			     "ocbi @%1" : :
-			     "r" (a0), "r" (a1));
-		asm volatile("ldc %0, sr" : : "r" (orig_sr));
-		a0 += linesz;
-		a1 += linesz;
-	} while (a0 < a0e);
-}
-
-static void __flush_dcache_segment_4way(unsigned long start,
-					unsigned long extent_per_way)
-{
-	unsigned long orig_sr, sr_with_bl;
-	unsigned long base_addr;
-	unsigned long way_incr, linesz, way_size;
-	struct cache_info *dcache;
-	register unsigned long a0, a1, a2, a3, a0e;
-
-	asm volatile("stc sr, %0" : "=r" (orig_sr));
-	sr_with_bl = orig_sr | (1<<28);
-	base_addr = ((unsigned long)&empty_zero_page[0]);
-
-	/* See comment under 1-way above */
-	base_addr = ((base_addr >> 16) << 16);
-	base_addr |= start;
-
-	dcache = &boot_cpu_data.dcache;
-	linesz = dcache->linesz;
-	way_incr = dcache->way_incr;
-	way_size = dcache->way_size;
-
-	a0 = base_addr;
-	a1 = a0 + way_incr;
-	a2 = a1 + way_incr;
-	a3 = a2 + way_incr;
-	a0e = base_addr + extent_per_way;
-	do {
-		asm volatile("ldc %0, sr" : : "r" (sr_with_bl));
-		asm volatile("movca.l r0, @%0\n\t"
-			     "movca.l r0, @%1\n\t"
-			     "movca.l r0, @%2\n\t"
-			     "movca.l r0, @%3\n\t"
-			     "ocbi @%0\n\t"
-			     "ocbi @%1\n\t"
-			     "ocbi @%2\n\t"
-			     "ocbi @%3\n\t" : :
-			     "r" (a0), "r" (a1), "r" (a2), "r" (a3));
-		a0 += linesz;
-		a1 += linesz;
-		a2 += linesz;
-		a3 += linesz;
-		asm volatile("movca.l r0, @%0\n\t"
-			     "movca.l r0, @%1\n\t"
-			     "movca.l r0, @%2\n\t"
-			     "movca.l r0, @%3\n\t"
-			     "ocbi @%0\n\t"
-			     "ocbi @%1\n\t"
-			     "ocbi @%2\n\t"
-			     "ocbi @%3\n\t" : :
-			     "r" (a0), "r" (a1), "r" (a2), "r" (a3));
-		a0 += linesz;
-		a1 += linesz;
-		a2 += linesz;
-		a3 += linesz;
-		asm volatile("movca.l r0, @%0\n\t"
-			     "movca.l r0, @%1\n\t"
-			     "movca.l r0, @%2\n\t"
-			     "movca.l r0, @%3\n\t"
-			     "ocbi @%0\n\t"
-			     "ocbi @%1\n\t"
-			     "ocbi @%2\n\t"
-			     "ocbi @%3\n\t" : :
-			     "r" (a0), "r" (a1), "r" (a2), "r" (a3));
-		a0 += linesz;
-		a1 += linesz;
-		a2 += linesz;
-		a3 += linesz;
-		asm volatile("movca.l r0, @%0\n\t"
-			     "movca.l r0, @%1\n\t"
-			     "movca.l r0, @%2\n\t"
-			     "movca.l r0, @%3\n\t"
-			     "ocbi @%0\n\t"
-			     "ocbi @%1\n\t"
-			     "ocbi @%2\n\t"
-			     "ocbi @%3\n\t" : :
-			     "r" (a0), "r" (a1), "r" (a2), "r" (a3));
-		asm volatile("ldc %0, sr" : : "r" (orig_sr));
-		a0 += linesz;
-		a1 += linesz;
-		a2 += linesz;
-		a3 += linesz;
-	} while (a0 < a0e);
-}
-
 extern void __weak sh4__flush_region_init(void);
 
 /*
@@ -593,32 +363,11 @@ extern void __weak sh4__flush_region_init(void);
  */
 void __init sh4_cache_init(void)
 {
-	unsigned int wt_enabled = !!(__raw_readl(CCR) & CCR_CACHE_WT);
-
 	printk("PVR=%08x CVR=%08x PRR=%08x\n",
 		ctrl_inl(CCN_PVR),
 		ctrl_inl(CCN_CVR),
 		ctrl_inl(CCN_PRR));
 
-	if (wt_enabled)
-		__flush_dcache_segment_fn = __flush_dcache_segment_writethrough;
-	else {
-		switch (boot_cpu_data.dcache.ways) {
-		case 1:
-			__flush_dcache_segment_fn = __flush_dcache_segment_1way;
-			break;
-		case 2:
-			__flush_dcache_segment_fn = __flush_dcache_segment_2way;
-			break;
-		case 4:
-			__flush_dcache_segment_fn = __flush_dcache_segment_4way;
-			break;
-		default:
-			panic("unknown number of cache ways\n");
-			break;
-		}
-	}
-
 	local_flush_icache_range	= sh4_flush_icache_range;
 	local_flush_dcache_page		= sh4_flush_dcache_page;
 	local_flush_cache_all		= sh4_flush_cache_all;

commit 31c9efde786252112cc3d04a1ed3513b6ec63a7b
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 9 14:10:28 2009 +0900

    sh: Kill off broken PHYSADDR() usage in sh4_flush_dcache_page().
    
    PHYSADDR() runs in to issues in 32-bit mode when we do not have the
    legacy P1/P2 areas mapped, as such, we need to use page_to_phys()
    directly, which also happens to do the right thing in legacy 29-bit mode.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index a5c339bca8aa..f0999606686f 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -123,12 +123,12 @@ static void sh4_flush_dcache_page(void *arg)
 	else
 #endif
 	{
-		unsigned long phys = PHYSADDR(page_address(page));
+		unsigned long phys = page_to_phys(page);
 		unsigned long addr = CACHE_OC_ADDRESS_ARRAY;
 		int i, n;
 
 		/* Loop all the D-cache */
-		n = boot_cpu_data.dcache.n_aliases;
+		n = boot_cpu_data.dcache.way_incr >> 12;
 		for (i = 0; i < n; i++, addr += 4096)
 			flush_cache_4096(addr, phys);
 	}

commit 654d364e26c797e8a5f9e2a1393607e6ca0106eb
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 9 14:04:06 2009 +0900

    sh: sh4_flush_cache_mm() optimizations.
    
    The i-cache flush in the case of VM_EXEC was added way back when as a
    sanity measure, and in practice we only care about evicting aliases from
    the d-cache. As a result, it's possible to drop the i-cache flush
    completely here.
    
    After careful profiling it's also come up that all of the work associated
    with hunting down aliases and doing ranged flushing ends up generating
    more overhead than simply blasting away the entire dcache, particularly
    if there are many mm's that need to be iterated over. As a result of
    that, just move back to flush_dcache_all() in these cases, which restores
    the old behaviour, and vastly simplifies the path.
    
    Additionally, on platforms without aliases at all, this can simply be
    nopped out. Presently we have the alias check in the SH-4 specific
    version, but this is true for all of the platforms, so move the check up
    to a generic location. This cuts down quite a bit on superfluous cacheop
    IPIs.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index b2453bbef4cd..a5c339bca8aa 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -170,89 +170,13 @@ static void sh4_flush_cache_all(void *unused)
 	flush_icache_all();
 }
 
-static void __flush_cache_mm(struct mm_struct *mm, unsigned long start,
-			     unsigned long end)
-{
-	unsigned long d = 0, p = start & PAGE_MASK;
-	unsigned long alias_mask = boot_cpu_data.dcache.alias_mask;
-	unsigned long n_aliases = boot_cpu_data.dcache.n_aliases;
-	unsigned long select_bit;
-	unsigned long all_aliases_mask;
-	unsigned long addr_offset;
-	pgd_t *dir;
-	pmd_t *pmd;
-	pud_t *pud;
-	pte_t *pte;
-	int i;
-
-	dir = pgd_offset(mm, p);
-	pud = pud_offset(dir, p);
-	pmd = pmd_offset(pud, p);
-	end = PAGE_ALIGN(end);
-
-	all_aliases_mask = (1 << n_aliases) - 1;
-
-	do {
-		if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd))) {
-			p &= PMD_MASK;
-			p += PMD_SIZE;
-			pmd++;
-
-			continue;
-		}
-
-		pte = pte_offset_kernel(pmd, p);
-
-		do {
-			unsigned long phys;
-			pte_t entry = *pte;
-
-			if (!(pte_val(entry) & _PAGE_PRESENT)) {
-				pte++;
-				p += PAGE_SIZE;
-				continue;
-			}
-
-			phys = pte_val(entry) & PTE_PHYS_MASK;
-
-			if ((p ^ phys) & alias_mask) {
-				d |= 1 << ((p & alias_mask) >> PAGE_SHIFT);
-				d |= 1 << ((phys & alias_mask) >> PAGE_SHIFT);
-
-				if (d == all_aliases_mask)
-					goto loop_exit;
-			}
-
-			pte++;
-			p += PAGE_SIZE;
-		} while (p < end && ((unsigned long)pte & ~PAGE_MASK));
-		pmd++;
-	} while (p < end);
-
-loop_exit:
-	addr_offset = 0;
-	select_bit = 1;
-
-	for (i = 0; i < n_aliases; i++) {
-		if (d & select_bit) {
-			(*__flush_dcache_segment_fn)(addr_offset, PAGE_SIZE);
-			wmb();
-		}
-
-		select_bit <<= 1;
-		addr_offset += PAGE_SIZE;
-	}
-}
-
 /*
  * Note : (RPC) since the caches are physically tagged, the only point
  * of flush_cache_mm for SH-4 is to get rid of aliases from the
  * D-cache.  The assumption elsewhere, e.g. flush_cache_range, is that
  * lines can stay resident so long as the virtual address they were
  * accessed with (hence cache set) is in accord with the physical
- * address (i.e. tag).  It's no different here.  So I reckon we don't
- * need to flush the I-cache, since aliases don't matter for that.  We
- * should try that.
+ * address (i.e. tag).  It's no different here.
  *
  * Caller takes mm->mmap_sem.
  */
@@ -263,33 +187,7 @@ static void sh4_flush_cache_mm(void *arg)
 	if (cpu_context(smp_processor_id(), mm) == NO_CONTEXT)
 		return;
 
-	/*
-	 * If cache is only 4k-per-way, there are never any 'aliases'.  Since
-	 * the cache is physically tagged, the data can just be left in there.
-	 */
-	if (boot_cpu_data.dcache.n_aliases == 0)
-		return;
-
-	/*
-	 * Don't bother groveling around the dcache for the VMA ranges
-	 * if there are too many PTEs to make it worthwhile.
-	 */
-	if (mm->nr_ptes >= MAX_DCACHE_PAGES)
-		flush_dcache_all();
-	else {
-		struct vm_area_struct *vma;
-
-		/*
-		 * In this case there are reasonably sized ranges to flush,
-		 * iterate through the VMA list and take care of any aliases.
-		 */
-		for (vma = mm->mmap; vma; vma = vma->vm_next)
-			__flush_cache_mm(mm, vma->vm_start, vma->vm_end);
-	}
-
-	/* Only touch the icache if one of the VMAs has VM_EXEC set. */
-	if (mm->exec_vm)
-		flush_icache_all();
+	flush_dcache_all();
 }
 
 /*
@@ -372,24 +270,10 @@ static void sh4_flush_cache_range(void *args)
 	if (boot_cpu_data.dcache.n_aliases == 0)
 		return;
 
-	/*
-	 * Don't bother with the lookup and alias check if we have a
-	 * wide range to cover, just blow away the dcache in its
-	 * entirety instead. -- PFM.
-	 */
-	if (((end - start) >> PAGE_SHIFT) >= MAX_DCACHE_PAGES)
-		flush_dcache_all();
-	else
-		__flush_cache_mm(vma->vm_mm, start, end);
+	flush_dcache_all();
 
-	if (vma->vm_flags & VM_EXEC) {
-		/*
-		 * TODO: Is this required???  Need to look at how I-cache
-		 * coherency is assured when new programs are loaded to see if
-		 * this matters.
-		 */
+	if (vma->vm_flags & VM_EXEC)
 		flush_icache_all();
-	}
 }
 
 /**

commit 682f88ab74e55dae55ea3bf30b46f56f71b793bd
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 9 13:19:46 2009 +0900

    sh: Cleanup whitespace damage in sh4_flush_icache_range().
    
    There was quite a lot of tab->space damage done here from a former patch,
    clean it up once and for all.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 3ac4945cb493..b2453bbef4cd 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -46,7 +46,6 @@ static void (*__flush_dcache_segment_fn)(unsigned long, unsigned long) =
 static void sh4_flush_icache_range(void *args)
 {
 	struct flusher_data *data = args;
-	int icacheaddr;
 	unsigned long start, end;
 	unsigned long flags, v;
 	int i;
@@ -54,36 +53,40 @@ static void sh4_flush_icache_range(void *args)
 	start = data->addr1;
 	end = data->addr2;
 
-       /* If there are too many pages then just blow the caches */
-        if (((end - start) >> PAGE_SHIFT) >= MAX_ICACHE_PAGES) {
-                local_flush_cache_all(args);
-       } else {
-               /* selectively flush d-cache then invalidate the i-cache */
-               /* this is inefficient, so only use for small ranges */
-               start &= ~(L1_CACHE_BYTES-1);
-               end += L1_CACHE_BYTES-1;
-               end &= ~(L1_CACHE_BYTES-1);
-
-               local_irq_save(flags);
-               jump_to_uncached();
-
-               for (v = start; v < end; v+=L1_CACHE_BYTES) {
-                       asm volatile("ocbwb     %0"
-                                    : /* no output */
-                                    : "m" (__m(v)));
-
-                       icacheaddr = CACHE_IC_ADDRESS_ARRAY | (
-                                       v & cpu_data->icache.entry_mask);
-
-                       for (i = 0; i < cpu_data->icache.ways;
-                               i++, icacheaddr += cpu_data->icache.way_incr)
-                                       /* Clear i-cache line valid-bit */
-                                       ctrl_outl(0, icacheaddr);
-               }
-
-		back_to_cached();
-		local_irq_restore(flags);
+	/* If there are too many pages then just blow away the caches */
+	if (((end - start) >> PAGE_SHIFT) >= MAX_ICACHE_PAGES) {
+		local_flush_cache_all(NULL);
+		return;
+	}
+
+	/*
+	 * Selectively flush d-cache then invalidate the i-cache.
+	 * This is inefficient, so only use this for small ranges.
+	 */
+	start &= ~(L1_CACHE_BYTES-1);
+	end += L1_CACHE_BYTES-1;
+	end &= ~(L1_CACHE_BYTES-1);
+
+	local_irq_save(flags);
+	jump_to_uncached();
+
+	for (v = start; v < end; v += L1_CACHE_BYTES) {
+		unsigned long icacheaddr;
+
+		__ocbwb(v);
+
+		icacheaddr = CACHE_IC_ADDRESS_ARRAY | (v &
+				cpu_data->icache.entry_mask);
+
+		/* Clear i-cache line valid-bit */
+		for (i = 0; i < cpu_data->icache.ways; i++) {
+			__raw_writel(0, icacheaddr);
+			icacheaddr += cpu_data->icache.way_incr;
+		}
 	}
+
+	back_to_cached();
+	local_irq_restore(flags);
 }
 
 static inline void flush_cache_4096(unsigned long start,

commit 983f4c514c4c9ddac1077a2c805fd16cbe3f7487
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Sep 1 21:12:55 2009 +0900

    Revert "sh: Kill off now redundant local irq disabling."
    
    This reverts commit 64a6d72213dd810dd55bd0a503c36150af41c3c3.
    
    Unfortunately we can't use on_each_cpu() for all of the cache ops, as
    some of them only require preempt disabling. This seems to be the same
    issue that impacts the mips r4k caches, where this code was based on.
    This fixes up a deadlock that showed up in some IRQ context cases.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 70fb906419dd..3ac4945cb493 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -48,44 +48,48 @@ static void sh4_flush_icache_range(void *args)
 	struct flusher_data *data = args;
 	int icacheaddr;
 	unsigned long start, end;
-	unsigned long v;
+	unsigned long flags, v;
 	int i;
 
 	start = data->addr1;
 	end = data->addr2;
 
-	/* If there are too many pages then just blow the caches */
-	if (((end - start) >> PAGE_SHIFT) >= MAX_ICACHE_PAGES) {
-		local_flush_cache_all(args);
-	} else {
-		/* selectively flush d-cache then invalidate the i-cache */
-		/* this is inefficient, so only use for small ranges */
-		start &= ~(L1_CACHE_BYTES-1);
-		end += L1_CACHE_BYTES-1;
-		end &= ~(L1_CACHE_BYTES-1);
-
-		jump_to_uncached();
-
-		for (v = start; v < end; v+=L1_CACHE_BYTES) {
-			__ocbwb(v);
-
-			icacheaddr = CACHE_IC_ADDRESS_ARRAY |
-				(v & cpu_data->icache.entry_mask);
-
-			for (i = 0; i < cpu_data->icache.ways;
-				i++, icacheaddr += cpu_data->icache.way_incr)
-				/* Clear i-cache line valid-bit */
-				ctrl_outl(0, icacheaddr);
-		}
+       /* If there are too many pages then just blow the caches */
+        if (((end - start) >> PAGE_SHIFT) >= MAX_ICACHE_PAGES) {
+                local_flush_cache_all(args);
+       } else {
+               /* selectively flush d-cache then invalidate the i-cache */
+               /* this is inefficient, so only use for small ranges */
+               start &= ~(L1_CACHE_BYTES-1);
+               end += L1_CACHE_BYTES-1;
+               end &= ~(L1_CACHE_BYTES-1);
+
+               local_irq_save(flags);
+               jump_to_uncached();
+
+               for (v = start; v < end; v+=L1_CACHE_BYTES) {
+                       asm volatile("ocbwb     %0"
+                                    : /* no output */
+                                    : "m" (__m(v)));
+
+                       icacheaddr = CACHE_IC_ADDRESS_ARRAY | (
+                                       v & cpu_data->icache.entry_mask);
+
+                       for (i = 0; i < cpu_data->icache.ways;
+                               i++, icacheaddr += cpu_data->icache.way_incr)
+                                       /* Clear i-cache line valid-bit */
+                                       ctrl_outl(0, icacheaddr);
+               }
 
 		back_to_cached();
+		local_irq_restore(flags);
 	}
 }
 
 static inline void flush_cache_4096(unsigned long start,
 				    unsigned long phys)
 {
-	unsigned long exec_offset = 0;
+	unsigned long flags, exec_offset = 0;
 
 	/*
 	 * All types of SH-4 require PC to be in P2 to operate on the I-cache.
@@ -95,8 +99,10 @@ static inline void flush_cache_4096(unsigned long start,
 	    (start < CACHE_OC_ADDRESS_ARRAY))
 		exec_offset = 0x20000000;
 
+	local_irq_save(flags);
 	__flush_cache_4096(start | SH_CACHE_ASSOC,
 			   P1SEGADDR(phys), exec_offset);
+	local_irq_restore(flags);
 }
 
 /*
@@ -130,8 +136,9 @@ static void sh4_flush_dcache_page(void *arg)
 /* TODO: Selective icache invalidation through IC address array.. */
 static void __uses_jump_to_uncached flush_icache_all(void)
 {
-	unsigned long ccr;
+	unsigned long flags, ccr;
 
+	local_irq_save(flags);
 	jump_to_uncached();
 
 	/* Flush I-cache */
@@ -143,7 +150,9 @@ static void __uses_jump_to_uncached flush_icache_all(void)
 	 * back_to_cached() will take care of the barrier for us, don't add
 	 * another one!
 	 */
+
 	back_to_cached();
+	local_irq_restore(flags);
 }
 
 static inline void flush_dcache_all(void)

commit ac6a0cf6716bb46813d0161024c66c2af66e53d1
Merge: e76a0136a3cf ce3f7cb96e67
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Sep 1 13:54:14 2009 +0900

    Merge branch 'master' into sh/smp
    
    Conflicts:
            arch/sh/mm/cache-sh4.c

commit ce3f7cb96e67d6518c7fc7b361a76409c3817d64
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Sep 1 13:32:48 2009 +0900

    sh: Fix dcache flushing for N-way write-through caches.
    
    This adopts the special-cased 2-way write-through dcache flusher for
    N-ways and moves it in to the generic path. Assignment is done at runtime
    via the check for the CCR_CACHE_WT bit in the same path as the per-way
    writeback flushers.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 397c1030c7a6..b36a9c986a58 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -25,13 +25,14 @@
 #define MAX_DCACHE_PAGES	64	/* XXX: Tune for ways */
 #define MAX_ICACHE_PAGES	32
 
+static void __flush_dcache_segment_writethrough(unsigned long start,
+						unsigned long extent);
 static void __flush_dcache_segment_1way(unsigned long start,
 					unsigned long extent);
 static void __flush_dcache_segment_2way(unsigned long start,
 					unsigned long extent);
 static void __flush_dcache_segment_4way(unsigned long start,
 					unsigned long extent);
-
 static void __flush_cache_4096(unsigned long addr, unsigned long phys,
 			       unsigned long exec_offset);
 
@@ -95,10 +96,17 @@ static void __init emit_cache_params(void)
  */
 void __init p3_cache_init(void)
 {
+	unsigned int wt_enabled = !!(__raw_readl(CCR) & CCR_CACHE_WT);
+
 	compute_alias(&boot_cpu_data.icache);
 	compute_alias(&boot_cpu_data.dcache);
 	compute_alias(&boot_cpu_data.scache);
 
+	if (wt_enabled) {
+		__flush_dcache_segment_fn = __flush_dcache_segment_writethrough;
+		goto out;
+	}
+
 	switch (boot_cpu_data.dcache.ways) {
 	case 1:
 		__flush_dcache_segment_fn = __flush_dcache_segment_1way;
@@ -114,6 +122,7 @@ void __init p3_cache_init(void)
 		break;
 	}
 
+out:
 	emit_cache_params();
 }
 
@@ -607,6 +616,23 @@ static void __flush_cache_4096(unsigned long addr, unsigned long phys,
  * - If caches are disabled or configured in write-through mode, then
  *   the movca.l writes garbage directly into memory.
  */
+static void __flush_dcache_segment_writethrough(unsigned long start,
+					        unsigned long extent_per_way)
+{
+	unsigned long addr;
+	int i;
+
+	addr = CACHE_OC_ADDRESS_ARRAY | (start & cpu_data->dcache.entry_mask);
+
+	while (extent_per_way) {
+		for (i = 0; i < cpu_data->dcache.ways; i++)
+			__raw_writel(0, addr + cpu_data->dcache.way_incr * i);
+
+		addr += cpu_data->dcache.linesz;
+		extent_per_way -= cpu_data->dcache.linesz;
+	}
+}
+
 static void __flush_dcache_segment_1way(unsigned long start,
 					unsigned long extent_per_way)
 {
@@ -655,25 +681,6 @@ static void __flush_dcache_segment_1way(unsigned long start,
 	} while (a0 < a0e);
 }
 
-#ifdef CONFIG_CACHE_WRITETHROUGH
-/* This method of cache flushing avoids the problems discussed
- * in the comment above if writethrough caches are enabled. */
-static void __flush_dcache_segment_2way(unsigned long start,
-					unsigned long extent_per_way)
-{
-	unsigned long array_addr;
-
-	array_addr = CACHE_OC_ADDRESS_ARRAY |
-		(start & cpu_data->dcache.entry_mask);
-
-	while (extent_per_way) {
-		ctrl_outl(0, array_addr);
-		ctrl_outl(0, array_addr + cpu_data->dcache.way_incr);
-		array_addr += cpu_data->dcache.linesz;
-		extent_per_way -= cpu_data->dcache.linesz;
-	}
-}
-#else
 static void __flush_dcache_segment_2way(unsigned long start,
 					unsigned long extent_per_way)
 {
@@ -732,7 +739,6 @@ static void __flush_dcache_segment_2way(unsigned long start,
 		a1 += linesz;
 	} while (a0 < a0e);
 }
-#endif
 
 static void __flush_dcache_segment_4way(unsigned long start,
 					unsigned long extent_per_way)

commit e76a0136a3cf1859fbc07f122e42293d22229558
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Aug 27 11:31:16 2009 +0900

    sh: Fix up sh4_flush_dcache_page() build on UP.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index e3b77f0fa470..2775f84d9aa3 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -103,8 +103,9 @@ static inline void flush_cache_4096(unsigned long start,
  * Write back & invalidate the D-cache of the page.
  * (To avoid "alias" issues)
  */
-static void sh4_flush_dcache_page(void *page)
+static void sh4_flush_dcache_page(void *arg)
 {
+	struct page *page = arg;
 #ifndef CONFIG_SMP
 	struct address_space *mapping = page_mapping(page);
 

commit ffad9d7a54a5e809007135595c778715aa0fb07a
Author: Stuart Menefy <stuart.menefy@st.com>
Date:   Mon Aug 24 18:39:39 2009 +0900

    sh: Fix problems with cache flushing when cache is in write-through mode
    
    Change the method used to flush the cache in write-through mode to
    avoid corrupted data being written back to memory.
    
    Signed-off-by: Stuart Menefy <stuart.menefy@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 7ce816188313..397c1030c7a6 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -592,6 +592,20 @@ static void __flush_cache_4096(unsigned long addr, unsigned long phys,
  * this with a cache invalidate to mark the cache line invalid. And do all
  * this with interrupts disabled, to avoid the cache line being accidently
  * evicted while it is holding garbage.
+ *
+ * This also breaks in a number of circumstances:
+ * - if there are modifications to the region of memory just above
+ *   empty_zero_page (for example because a breakpoint has been placed
+ *   there), then these can be lost.
+ *
+ *   This is because the the memory address which the cache temporarily
+ *   caches in the above description is empty_zero_page. So the
+ *   movca.l hits the cache (it is assumed that it misses, or at least
+ *   isn't dirty), modifies the line and then invalidates it, losing the
+ *   required change.
+ *
+ * - If caches are disabled or configured in write-through mode, then
+ *   the movca.l writes garbage directly into memory.
  */
 static void __flush_dcache_segment_1way(unsigned long start,
 					unsigned long extent_per_way)
@@ -641,6 +655,25 @@ static void __flush_dcache_segment_1way(unsigned long start,
 	} while (a0 < a0e);
 }
 
+#ifdef CONFIG_CACHE_WRITETHROUGH
+/* This method of cache flushing avoids the problems discussed
+ * in the comment above if writethrough caches are enabled. */
+static void __flush_dcache_segment_2way(unsigned long start,
+					unsigned long extent_per_way)
+{
+	unsigned long array_addr;
+
+	array_addr = CACHE_OC_ADDRESS_ARRAY |
+		(start & cpu_data->dcache.entry_mask);
+
+	while (extent_per_way) {
+		ctrl_outl(0, array_addr);
+		ctrl_outl(0, array_addr + cpu_data->dcache.way_incr);
+		array_addr += cpu_data->dcache.linesz;
+		extent_per_way -= cpu_data->dcache.linesz;
+	}
+}
+#else
 static void __flush_dcache_segment_2way(unsigned long start,
 					unsigned long extent_per_way)
 {
@@ -699,6 +732,7 @@ static void __flush_dcache_segment_2way(unsigned long start,
 		a1 += linesz;
 	} while (a0 < a0e);
 }
+#endif
 
 static void __flush_dcache_segment_4way(unsigned long start,
 					unsigned long extent_per_way)

commit a5cf9e2444ec15de5407696ff21c32dd21ca0a8d
Author: Stuart Menefy <stuart.menefy@st.com>
Date:   Mon Aug 24 17:36:24 2009 +0900

    sh: Improve comments int SH4 cache flushing code
    
    This is a pure documentation, to try to explain why the cache flushing code
    for the SH4 is implemented the way it is.
    
    Signed-off-by: Stuart Menefy <stuart.menefy@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 5cfe08dbb59e..7ce816188313 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -581,6 +581,17 @@ static void __flush_cache_4096(unsigned long addr, unsigned long phys,
  * Break the 1, 2 and 4 way variants of this out into separate functions to
  * avoid nearly all the overhead of having the conditional stuff in the function
  * bodies (+ the 1 and 2 way cases avoid saving any registers too).
+ *
+ * We want to eliminate unnecessary bus transactions, so this code uses
+ * a non-obvious technique.
+ *
+ * Loop over a cache way sized block of, one cache line at a time. For each
+ * line, use movca.a to cause the current cache line contents to be written
+ * back, but without reading anything from main memory. However this has the
+ * side effect that the cache is now caching that memory location. So follow
+ * this with a cache invalidate to mark the cache line invalid. And do all
+ * this with interrupts disabled, to avoid the cache line being accidently
+ * evicted while it is holding garbage.
  */
 static void __flush_dcache_segment_1way(unsigned long start,
 					unsigned long extent_per_way)

commit 64a6d72213dd810dd55bd0a503c36150af41c3c3
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Aug 21 18:21:07 2009 +0900

    sh: Kill off now redundant local irq disabling.
    
    on_each_cpu() takes care of IRQ and preempt handling, the localized
    handling in each of the called functions can be killed off.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 9201b37c7cca..e3b77f0fa470 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -48,48 +48,44 @@ static void sh4_flush_icache_range(void *args)
 	struct flusher_data *data = args;
 	int icacheaddr;
 	unsigned long start, end;
-	unsigned long flags, v;
+	unsigned long v;
 	int i;
 
 	start = data->addr1;
 	end = data->addr2;
 
-       /* If there are too many pages then just blow the caches */
-        if (((end - start) >> PAGE_SHIFT) >= MAX_ICACHE_PAGES) {
-                local_flush_cache_all(args);
-       } else {
-               /* selectively flush d-cache then invalidate the i-cache */
-               /* this is inefficient, so only use for small ranges */
-               start &= ~(L1_CACHE_BYTES-1);
-               end += L1_CACHE_BYTES-1;
-               end &= ~(L1_CACHE_BYTES-1);
-
-               local_irq_save(flags);
-               jump_to_uncached();
-
-               for (v = start; v < end; v+=L1_CACHE_BYTES) {
-                       asm volatile("ocbwb     %0"
-                                    : /* no output */
-                                    : "m" (__m(v)));
-
-                       icacheaddr = CACHE_IC_ADDRESS_ARRAY | (
-                                       v & cpu_data->icache.entry_mask);
-
-                       for (i = 0; i < cpu_data->icache.ways;
-                               i++, icacheaddr += cpu_data->icache.way_incr)
-                                       /* Clear i-cache line valid-bit */
-                                       ctrl_outl(0, icacheaddr);
-               }
+	/* If there are too many pages then just blow the caches */
+	if (((end - start) >> PAGE_SHIFT) >= MAX_ICACHE_PAGES) {
+		local_flush_cache_all(args);
+	} else {
+		/* selectively flush d-cache then invalidate the i-cache */
+		/* this is inefficient, so only use for small ranges */
+		start &= ~(L1_CACHE_BYTES-1);
+		end += L1_CACHE_BYTES-1;
+		end &= ~(L1_CACHE_BYTES-1);
+
+		jump_to_uncached();
+
+		for (v = start; v < end; v+=L1_CACHE_BYTES) {
+			__ocbwb(v);
+
+			icacheaddr = CACHE_IC_ADDRESS_ARRAY |
+				(v & cpu_data->icache.entry_mask);
+
+			for (i = 0; i < cpu_data->icache.ways;
+				i++, icacheaddr += cpu_data->icache.way_incr)
+				/* Clear i-cache line valid-bit */
+				ctrl_outl(0, icacheaddr);
+		}
 
 		back_to_cached();
-		local_irq_restore(flags);
 	}
 }
 
 static inline void flush_cache_4096(unsigned long start,
 				    unsigned long phys)
 {
-	unsigned long flags, exec_offset = 0;
+	unsigned long exec_offset = 0;
 
 	/*
 	 * All types of SH-4 require PC to be in P2 to operate on the I-cache.
@@ -99,10 +95,8 @@ static inline void flush_cache_4096(unsigned long start,
 	    (start < CACHE_OC_ADDRESS_ARRAY))
 		exec_offset = 0x20000000;
 
-	local_irq_save(flags);
 	__flush_cache_4096(start | SH_CACHE_ASSOC,
 			   P1SEGADDR(phys), exec_offset);
-	local_irq_restore(flags);
 }
 
 /*
@@ -135,9 +129,8 @@ static void sh4_flush_dcache_page(void *page)
 /* TODO: Selective icache invalidation through IC address array.. */
 static void __uses_jump_to_uncached flush_icache_all(void)
 {
-	unsigned long flags, ccr;
+	unsigned long ccr;
 
-	local_irq_save(flags);
 	jump_to_uncached();
 
 	/* Flush I-cache */
@@ -149,9 +142,7 @@ static void __uses_jump_to_uncached flush_icache_all(void)
 	 * back_to_cached() will take care of the barrier for us, don't add
 	 * another one!
 	 */
-
 	back_to_cached();
-	local_irq_restore(flags);
 }
 
 static inline void flush_dcache_all(void)

commit f26b2a562b46ab186c8383993ab1332673ac4a47
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Aug 21 17:23:14 2009 +0900

    sh: Make cache flushers SMP-aware.
    
    This does a bit of rework for making the cache flushers SMP-aware. The
    function pointer-based flushers are renamed to local variants with the
    exported interface being commonly implemented and wrapping as necessary.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 6c2db1401080..9201b37c7cca 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -43,15 +43,20 @@ static void (*__flush_dcache_segment_fn)(unsigned long, unsigned long) =
  * Called from kernel/module.c:sys_init_module and routine for a.out format,
  * signal handler code and kprobes code
  */
-static void sh4_flush_icache_range(unsigned long start, unsigned long end)
+static void sh4_flush_icache_range(void *args)
 {
+	struct flusher_data *data = args;
 	int icacheaddr;
+	unsigned long start, end;
 	unsigned long flags, v;
 	int i;
 
+	start = data->addr1;
+	end = data->addr2;
+
        /* If there are too many pages then just blow the caches */
         if (((end - start) >> PAGE_SHIFT) >= MAX_ICACHE_PAGES) {
-                flush_cache_all();
+                local_flush_cache_all(args);
        } else {
                /* selectively flush d-cache then invalidate the i-cache */
                /* this is inefficient, so only use for small ranges */
@@ -104,7 +109,7 @@ static inline void flush_cache_4096(unsigned long start,
  * Write back & invalidate the D-cache of the page.
  * (To avoid "alias" issues)
  */
-static void sh4_flush_dcache_page(struct page *page)
+static void sh4_flush_dcache_page(void *page)
 {
 #ifndef CONFIG_SMP
 	struct address_space *mapping = page_mapping(page);
@@ -155,7 +160,7 @@ static inline void flush_dcache_all(void)
 	wmb();
 }
 
-static void sh4_flush_cache_all(void)
+static void sh4_flush_cache_all(void *unused)
 {
 	flush_dcache_all();
 	flush_icache_all();
@@ -247,8 +252,10 @@ static void __flush_cache_mm(struct mm_struct *mm, unsigned long start,
  *
  * Caller takes mm->mmap_sem.
  */
-static void sh4_flush_cache_mm(struct mm_struct *mm)
+static void sh4_flush_cache_mm(void *arg)
 {
+	struct mm_struct *mm = arg;
+
 	if (cpu_context(smp_processor_id(), mm) == NO_CONTEXT)
 		return;
 
@@ -287,12 +294,18 @@ static void sh4_flush_cache_mm(struct mm_struct *mm)
  * ADDR: Virtual Address (U0 address)
  * PFN: Physical page number
  */
-static void sh4_flush_cache_page(struct vm_area_struct *vma,
-		unsigned long address, unsigned long pfn)
+static void sh4_flush_cache_page(void *args)
 {
-	unsigned long phys = pfn << PAGE_SHIFT;
+	struct flusher_data *data = args;
+	struct vm_area_struct *vma;
+	unsigned long address, pfn, phys;
 	unsigned int alias_mask;
 
+	vma = data->vma;
+	address = data->addr1;
+	pfn = data->addr2;
+	phys = pfn << PAGE_SHIFT;
+
 	if (cpu_context(smp_processor_id(), vma->vm_mm) == NO_CONTEXT)
 		return;
 
@@ -335,9 +348,16 @@ static void sh4_flush_cache_page(struct vm_area_struct *vma,
  * Flushing the cache lines for U0 only isn't enough.
  * We need to flush for P1 too, which may contain aliases.
  */
-static void sh4_flush_cache_range(struct vm_area_struct *vma,
-		unsigned long start, unsigned long end)
+static void sh4_flush_cache_range(void *args)
 {
+	struct flusher_data *data = args;
+	struct vm_area_struct *vma;
+	unsigned long start, end;
+
+	vma = data->vma;
+	start = data->addr1;
+	end = data->addr2;
+
 	if (cpu_context(smp_processor_id(), vma->vm_mm) == NO_CONTEXT)
 		return;
 
@@ -663,13 +683,13 @@ void __init sh4_cache_init(void)
 		break;
 	}
 
-	flush_icache_range	= sh4_flush_icache_range;
-	flush_dcache_page	= sh4_flush_dcache_page;
-	flush_cache_all		= sh4_flush_cache_all;
-	flush_cache_mm		= sh4_flush_cache_mm;
-	flush_cache_dup_mm	= sh4_flush_cache_mm;
-	flush_cache_page	= sh4_flush_cache_page;
-	flush_cache_range	= sh4_flush_cache_range;
+	local_flush_icache_range	= sh4_flush_icache_range;
+	local_flush_dcache_page		= sh4_flush_dcache_page;
+	local_flush_cache_all		= sh4_flush_cache_all;
+	local_flush_cache_mm		= sh4_flush_cache_mm;
+	local_flush_cache_dup_mm	= sh4_flush_cache_mm;
+	local_flush_cache_page		= sh4_flush_cache_page;
+	local_flush_cache_range		= sh4_flush_cache_range;
 
 	sh4__flush_region_init();
 }

commit c139a595878b0e8156476668e3d5c27b6aca7624
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Aug 20 15:24:41 2009 +0900

    sh: Fix up cache-sh4 build on SMP.
    
    mapping is unused on the SMP build, trigger a build error. Move it under
    the ifdef.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 05cb04bc3940..6c2db1401080 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -106,9 +106,9 @@ static inline void flush_cache_4096(unsigned long start,
  */
 static void sh4_flush_dcache_page(struct page *page)
 {
+#ifndef CONFIG_SMP
 	struct address_space *mapping = page_mapping(page);
 
-#ifndef CONFIG_SMP
 	if (mapping && !mapping_mapped(mapping))
 		set_bit(PG_dcache_dirty, &page->flags);
 	else

commit 37443ef3f0406e855e169c87ae3f4ffb4b6ff635
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 12:29:49 2009 +0900

    sh: Migrate SH-4 cacheflush ops to function pointers.
    
    This paves the way for allowing individual CPUs to overload the
    individual flushing routines that they care about without having to
    depend on weak aliases. SH-4 is converted over initially, as it wires
    up pretty much everything. The majority of the other CPUs will simply use
    the default no-op implementation with their own region flushers wired up.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index b5860535e61f..05cb04bc3940 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -26,13 +26,6 @@
 #define MAX_DCACHE_PAGES	64	/* XXX: Tune for ways */
 #define MAX_ICACHE_PAGES	32
 
-static void __flush_dcache_segment_1way(unsigned long start,
-					unsigned long extent);
-static void __flush_dcache_segment_2way(unsigned long start,
-					unsigned long extent);
-static void __flush_dcache_segment_4way(unsigned long start,
-					unsigned long extent);
-
 static void __flush_cache_4096(unsigned long addr, unsigned long phys,
 			       unsigned long exec_offset);
 
@@ -44,39 +37,13 @@ static void __flush_cache_4096(unsigned long addr, unsigned long phys,
 static void (*__flush_dcache_segment_fn)(unsigned long, unsigned long) =
 	(void (*)(unsigned long, unsigned long))0xdeadbeef;
 
-/*
- * SH-4 has virtually indexed and physically tagged cache.
- */
-void __init sh4_cache_init(void)
-{
-	printk("PVR=%08x CVR=%08x PRR=%08x\n",
-		ctrl_inl(CCN_PVR),
-		ctrl_inl(CCN_CVR),
-		ctrl_inl(CCN_PRR));
-
-	switch (boot_cpu_data.dcache.ways) {
-	case 1:
-		__flush_dcache_segment_fn = __flush_dcache_segment_1way;
-		break;
-	case 2:
-		__flush_dcache_segment_fn = __flush_dcache_segment_2way;
-		break;
-	case 4:
-		__flush_dcache_segment_fn = __flush_dcache_segment_4way;
-		break;
-	default:
-		panic("unknown number of cache ways\n");
-		break;
-	}
-}
-
 /*
  * Write back the range of D-cache, and purge the I-cache.
  *
  * Called from kernel/module.c:sys_init_module and routine for a.out format,
  * signal handler code and kprobes code
  */
-void flush_icache_range(unsigned long start, unsigned long end)
+static void sh4_flush_icache_range(unsigned long start, unsigned long end)
 {
 	int icacheaddr;
 	unsigned long flags, v;
@@ -137,7 +104,7 @@ static inline void flush_cache_4096(unsigned long start,
  * Write back & invalidate the D-cache of the page.
  * (To avoid "alias" issues)
  */
-void flush_dcache_page(struct page *page)
+static void sh4_flush_dcache_page(struct page *page)
 {
 	struct address_space *mapping = page_mapping(page);
 
@@ -188,7 +155,7 @@ static inline void flush_dcache_all(void)
 	wmb();
 }
 
-void flush_cache_all(void)
+static void sh4_flush_cache_all(void)
 {
 	flush_dcache_all();
 	flush_icache_all();
@@ -280,7 +247,7 @@ static void __flush_cache_mm(struct mm_struct *mm, unsigned long start,
  *
  * Caller takes mm->mmap_sem.
  */
-void flush_cache_mm(struct mm_struct *mm)
+static void sh4_flush_cache_mm(struct mm_struct *mm)
 {
 	if (cpu_context(smp_processor_id(), mm) == NO_CONTEXT)
 		return;
@@ -320,8 +287,8 @@ void flush_cache_mm(struct mm_struct *mm)
  * ADDR: Virtual Address (U0 address)
  * PFN: Physical page number
  */
-void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
-		      unsigned long pfn)
+static void sh4_flush_cache_page(struct vm_area_struct *vma,
+		unsigned long address, unsigned long pfn)
 {
 	unsigned long phys = pfn << PAGE_SHIFT;
 	unsigned int alias_mask;
@@ -368,8 +335,8 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
  * Flushing the cache lines for U0 only isn't enough.
  * We need to flush for P1 too, which may contain aliases.
  */
-void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
-		       unsigned long end)
+static void sh4_flush_cache_range(struct vm_area_struct *vma,
+		unsigned long start, unsigned long end)
 {
 	if (cpu_context(smp_processor_id(), vma->vm_mm) == NO_CONTEXT)
 		return;
@@ -668,3 +635,41 @@ static void __flush_dcache_segment_4way(unsigned long start,
 		a3 += linesz;
 	} while (a0 < a0e);
 }
+
+extern void __weak sh4__flush_region_init(void);
+
+/*
+ * SH-4 has virtually indexed and physically tagged cache.
+ */
+void __init sh4_cache_init(void)
+{
+	printk("PVR=%08x CVR=%08x PRR=%08x\n",
+		ctrl_inl(CCN_PVR),
+		ctrl_inl(CCN_CVR),
+		ctrl_inl(CCN_PRR));
+
+	switch (boot_cpu_data.dcache.ways) {
+	case 1:
+		__flush_dcache_segment_fn = __flush_dcache_segment_1way;
+		break;
+	case 2:
+		__flush_dcache_segment_fn = __flush_dcache_segment_2way;
+		break;
+	case 4:
+		__flush_dcache_segment_fn = __flush_dcache_segment_4way;
+		break;
+	default:
+		panic("unknown number of cache ways\n");
+		break;
+	}
+
+	flush_icache_range	= sh4_flush_icache_range;
+	flush_dcache_page	= sh4_flush_dcache_page;
+	flush_cache_all		= sh4_flush_cache_all;
+	flush_cache_mm		= sh4_flush_cache_mm;
+	flush_cache_dup_mm	= sh4_flush_cache_mm;
+	flush_cache_page	= sh4_flush_cache_page;
+	flush_cache_range	= sh4_flush_cache_range;
+
+	sh4__flush_region_init();
+}

commit 916e97834e023f89b31f796b53cc9c7956e7fe17
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 11:38:05 2009 +0900

    sh: Kill off unused flush_icache_user_range().
    
    We use flush_cache_page() outright in copy_to_user_page(), and nothing
    else needs it, so just kill it off. SH-5 still defines its own version,
    but that too will go away in the same fashion once it converts over.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 4466787a52aa..b5860535e61f 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -401,20 +401,6 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 	}
 }
 
-/*
- * flush_icache_user_range
- * @vma: VMA of the process
- * @page: page
- * @addr: U0 address
- * @len: length of the range (< page size)
- */
-void flush_icache_user_range(struct vm_area_struct *vma,
-			     struct page *page, unsigned long addr, int len)
-{
-	flush_cache_page(vma, addr, page_to_pfn(page));
-	mb();
-}
-
 /**
  * __flush_cache_4096
  *

commit 0b445dcaf3adda5bec5cc494925bc689fcc59a0e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 11:22:50 2009 +0900

    sh: Don't export flush_dcache_all().
    
    flush_dcache_all() is used internally by the SH-4 cache code, it is not
    part of the exported cache API, so make it static and don't export it.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 4ac844b1432f..4466787a52aa 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -182,7 +182,7 @@ static void __uses_jump_to_uncached flush_icache_all(void)
 	local_irq_restore(flags);
 }
 
-void flush_dcache_all(void)
+static inline void flush_dcache_all(void)
 {
 	(*__flush_dcache_segment_fn)(0UL, boot_cpu_data.dcache.way_size);
 	wmb();

commit 27d59ec1709817a90aa3ab7169f60994a89ad2f5
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 11:11:16 2009 +0900

    sh: Move alias computation to shared cache init.
    
    This migrates the alias computation and printing of probed cache
    parameters from the SH-4 code to the shared cpu_cache_init().
    
    This permits other platforms with aliases to make use of the same
    probe logic without having to roll their own, and also produces
    consistent output regardless of platform.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index df2eb87f1524..4ac844b1432f 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -44,61 +44,15 @@ static void __flush_cache_4096(unsigned long addr, unsigned long phys,
 static void (*__flush_dcache_segment_fn)(unsigned long, unsigned long) =
 	(void (*)(unsigned long, unsigned long))0xdeadbeef;
 
-static void compute_alias(struct cache_info *c)
-{
-	c->alias_mask = ((c->sets - 1) << c->entry_shift) & ~(PAGE_SIZE - 1);
-	c->n_aliases = c->alias_mask ? (c->alias_mask >> PAGE_SHIFT) + 1 : 0;
-}
-
-static void __init emit_cache_params(void)
-{
-	printk("PVR=%08x CVR=%08x PRR=%08x\n",
-		ctrl_inl(CCN_PVR),
-		ctrl_inl(CCN_CVR),
-		ctrl_inl(CCN_PRR));
-	printk("I-cache : n_ways=%d n_sets=%d way_incr=%d\n",
-		boot_cpu_data.icache.ways,
-		boot_cpu_data.icache.sets,
-		boot_cpu_data.icache.way_incr);
-	printk("I-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
-		boot_cpu_data.icache.entry_mask,
-		boot_cpu_data.icache.alias_mask,
-		boot_cpu_data.icache.n_aliases);
-	printk("D-cache : n_ways=%d n_sets=%d way_incr=%d\n",
-		boot_cpu_data.dcache.ways,
-		boot_cpu_data.dcache.sets,
-		boot_cpu_data.dcache.way_incr);
-	printk("D-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
-		boot_cpu_data.dcache.entry_mask,
-		boot_cpu_data.dcache.alias_mask,
-		boot_cpu_data.dcache.n_aliases);
-
-	/*
-	 * Emit Secondary Cache parameters if the CPU has a probed L2.
-	 */
-	if (boot_cpu_data.flags & CPU_HAS_L2_CACHE) {
-		printk("S-cache : n_ways=%d n_sets=%d way_incr=%d\n",
-			boot_cpu_data.scache.ways,
-			boot_cpu_data.scache.sets,
-			boot_cpu_data.scache.way_incr);
-		printk("S-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
-			boot_cpu_data.scache.entry_mask,
-			boot_cpu_data.scache.alias_mask,
-			boot_cpu_data.scache.n_aliases);
-	}
-
-	if (!__flush_dcache_segment_fn)
-		panic("unknown number of cache ways\n");
-}
-
 /*
  * SH-4 has virtually indexed and physically tagged cache.
  */
 void __init sh4_cache_init(void)
 {
-	compute_alias(&boot_cpu_data.icache);
-	compute_alias(&boot_cpu_data.dcache);
-	compute_alias(&boot_cpu_data.scache);
+	printk("PVR=%08x CVR=%08x PRR=%08x\n",
+		ctrl_inl(CCN_PVR),
+		ctrl_inl(CCN_CVR),
+		ctrl_inl(CCN_PRR));
 
 	switch (boot_cpu_data.dcache.ways) {
 	case 1:
@@ -111,11 +65,9 @@ void __init sh4_cache_init(void)
 		__flush_dcache_segment_fn = __flush_dcache_segment_4way;
 		break;
 	default:
-		__flush_dcache_segment_fn = NULL;
+		panic("unknown number of cache ways\n");
 		break;
 	}
-
-	emit_cache_params();
 }
 
 /*

commit ecba1060583635ab55092072441ff903b5e9a659
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 11:05:42 2009 +0900

    sh: Centralize the CPU cache initialization routines.
    
    This provides a central point for CPU cache initialization routines.
    This replaces the antiquated p3_cache_init() method, which the vast
    majority of CPUs never cared about.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 92f87a460a81..df2eb87f1524 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -94,7 +94,7 @@ static void __init emit_cache_params(void)
 /*
  * SH-4 has virtually indexed and physically tagged cache.
  */
-void __init p3_cache_init(void)
+void __init sh4_cache_init(void)
 {
 	compute_alias(&boot_cpu_data.icache);
 	compute_alias(&boot_cpu_data.dcache);

commit e7b8b7f16edc9b363573eadf2ab2683473626071
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 02:21:16 2009 +0900

    sh: NO_CONTEXT ASID optimizations for SH-4 cache flush.
    
    This optimizes for the cases when a CPU does not yet have a valid ASID
    context associated with it, as in this case there is no work for any of
    flush_cache_mm()/flush_cache_page()/flush_cache_range() to do. Based on
    the the MIPS implementation.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index dfc1d0379479..92f87a460a81 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -330,6 +330,9 @@ static void __flush_cache_mm(struct mm_struct *mm, unsigned long start,
  */
 void flush_cache_mm(struct mm_struct *mm)
 {
+	if (cpu_context(smp_processor_id(), mm) == NO_CONTEXT)
+		return;
+
 	/*
 	 * If cache is only 4k-per-way, there are never any 'aliases'.  Since
 	 * the cache is physically tagged, the data can just be left in there.
@@ -371,6 +374,9 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
 	unsigned long phys = pfn << PAGE_SHIFT;
 	unsigned int alias_mask;
 
+	if (cpu_context(smp_processor_id(), vma->vm_mm) == NO_CONTEXT)
+		return;
+
 	alias_mask = boot_cpu_data.dcache.alias_mask;
 
 	/* We only need to flush D-cache when we have alias */
@@ -413,6 +419,9 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
 void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 		       unsigned long end)
 {
+	if (cpu_context(smp_processor_id(), vma->vm_mm) == NO_CONTEXT)
+		return;
+
 	/*
 	 * If cache is only 4k-per-way, there are never any 'aliases'.  Since
 	 * the cache is physically tagged, the data can just be left in there.

commit 817425275271f2514f0dc6952182aa057ce80973
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Aug 4 18:06:01 2009 +0900

    sh: Split out SH-4 __flush_xxx_region() ops.
    
    This splits out the SH-4 __flush_xxx_region() functions and defines them
    as weak symbols. This allows us to provide optimized versions without
    having to ifdef cache-sh4.c to death.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index c3a09b27f8d5..dfc1d0379479 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -118,66 +118,6 @@ void __init p3_cache_init(void)
 	emit_cache_params();
 }
 
-/*
- * Write back the dirty D-caches, but not invalidate them.
- *
- * START: Virtual Address (U0, P1, or P3)
- * SIZE: Size of the region.
- */
-void __flush_wback_region(void *start, int size)
-{
-	unsigned long v;
-	unsigned long begin, end;
-
-	begin = (unsigned long)start & ~(L1_CACHE_BYTES-1);
-	end = ((unsigned long)start + size + L1_CACHE_BYTES-1)
-		& ~(L1_CACHE_BYTES-1);
-	for (v = begin; v < end; v+=L1_CACHE_BYTES) {
-		asm volatile("ocbwb	%0"
-			     : /* no output */
-			     : "m" (__m(v)));
-	}
-}
-
-/*
- * Write back the dirty D-caches and invalidate them.
- *
- * START: Virtual Address (U0, P1, or P3)
- * SIZE: Size of the region.
- */
-void __flush_purge_region(void *start, int size)
-{
-	unsigned long v;
-	unsigned long begin, end;
-
-	begin = (unsigned long)start & ~(L1_CACHE_BYTES-1);
-	end = ((unsigned long)start + size + L1_CACHE_BYTES-1)
-		& ~(L1_CACHE_BYTES-1);
-	for (v = begin; v < end; v+=L1_CACHE_BYTES) {
-		asm volatile("ocbp	%0"
-			     : /* no output */
-			     : "m" (__m(v)));
-	}
-}
-
-/*
- * No write back please
- */
-void __flush_invalidate_region(void *start, int size)
-{
-	unsigned long v;
-	unsigned long begin, end;
-
-	begin = (unsigned long)start & ~(L1_CACHE_BYTES-1);
-	end = ((unsigned long)start + size + L1_CACHE_BYTES-1)
-		& ~(L1_CACHE_BYTES-1);
-	for (v = begin; v < end; v+=L1_CACHE_BYTES) {
-		asm volatile("ocbi	%0"
-			     : /* no output */
-			     : "m" (__m(v)));
-	}
-}
-
 /*
  * Write back the range of D-cache, and purge the I-cache.
  *

commit 2277ab4a1df50e05bc732fe9488d4e902bb8399a
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 22 19:20:49 2009 +0900

    sh: Migrate from PG_mapped to PG_dcache_dirty.
    
    This inverts the delayed dcache flush a bit to be more in line with other
    platforms. At the same time this also gives us the ability to do some
    more optimizations and cleanup. Now that the update_mmu_cache() callsite
    only tests for the bit, the implementation can gradually be split out and
    made generic, rather than relying on special implementations for each of
    the peculiar CPU types.
    
    SH7705 in 32kB mode and SH-4 still need slightly different handling, but
    this is something that can remain isolated in the varying page copy/clear
    routines. On top of that, SH-X3 is dcache coherent, so there is no need
    to bother with any of these tests in the PTEAEX version of
    update_mmu_cache(), so we kill that off too.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 5cfe08dbb59e..c3a09b27f8d5 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -14,6 +14,7 @@
 #include <linux/mm.h>
 #include <linux/io.h>
 #include <linux/mutex.h>
+#include <linux/fs.h>
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
@@ -246,7 +247,14 @@ static inline void flush_cache_4096(unsigned long start,
  */
 void flush_dcache_page(struct page *page)
 {
-	if (test_bit(PG_mapped, &page->flags)) {
+	struct address_space *mapping = page_mapping(page);
+
+#ifndef CONFIG_SMP
+	if (mapping && !mapping_mapped(mapping))
+		set_bit(PG_dcache_dirty, &page->flags);
+	else
+#endif
+	{
 		unsigned long phys = PHYSADDR(page_address(page));
 		unsigned long addr = CACHE_OC_ADDRESS_ARRAY;
 		int i, n;

commit 205a3b4328de1c8ddd99ddd5092bed1344068213
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Sep 5 18:00:29 2008 +0900

    sh: uninline flush_icache_all().
    
    This uses jump_to_uncached() which is now given the noinline attribute
    due to the special section mapping. Kill off the inline attribute to
    fix up compilation failure.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 1fdc8d90254a..5cfe08dbb59e 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -261,7 +261,7 @@ void flush_dcache_page(struct page *page)
 }
 
 /* TODO: Selective icache invalidation through IC address array.. */
-static inline void __uses_jump_to_uncached flush_icache_all(void)
+static void __uses_jump_to_uncached flush_icache_all(void)
 {
 	unsigned long flags, ccr;
 

commit 09b5a10c1944214a6008712bfa92b29f00b84a1a
Author: Chris Smith <chris.smith@st.com>
Date:   Wed Jul 2 15:17:11 2008 +0900

    sh: Optimized flush_icache_range() implementation.
    
    Add implementation of flush_icache_range() suitable for signal handler
    and kprobes. Remove flush_cache_sigtramp() and change signal.c to use
    flush_icache_range().
    
    Signed-off-by: Chris Smith <chris.smith@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 43d7ff6b6ec7..1fdc8d90254a 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -4,6 +4,7 @@
  * Copyright (C) 1999, 2000, 2002  Niibe Yutaka
  * Copyright (C) 2001 - 2007  Paul Mundt
  * Copyright (C) 2003  Richard Curnow
+ * Copyright (c) 2007 STMicroelectronics (R&D) Ltd.
  *
  * This file is subject to the terms and conditions of the GNU General Public
  * License.  See the file "COPYING" in the main directory of this archive
@@ -22,6 +23,7 @@
  * entirety.
  */
 #define MAX_DCACHE_PAGES	64	/* XXX: Tune for ways */
+#define MAX_ICACHE_PAGES	32
 
 static void __flush_dcache_segment_1way(unsigned long start,
 					unsigned long extent);
@@ -178,42 +180,45 @@ void __flush_invalidate_region(void *start, int size)
 /*
  * Write back the range of D-cache, and purge the I-cache.
  *
- * Called from kernel/module.c:sys_init_module and routine for a.out format.
+ * Called from kernel/module.c:sys_init_module and routine for a.out format,
+ * signal handler code and kprobes code
  */
 void flush_icache_range(unsigned long start, unsigned long end)
 {
-	flush_cache_all();
-}
-
-/*
- * Write back the D-cache and purge the I-cache for signal trampoline.
- * .. which happens to be the same behavior as flush_icache_range().
- * So, we simply flush out a line.
- */
-void __uses_jump_to_uncached flush_cache_sigtramp(unsigned long addr)
-{
-	unsigned long v, index;
-	unsigned long flags;
+	int icacheaddr;
+	unsigned long flags, v;
 	int i;
 
-	v = addr & ~(L1_CACHE_BYTES-1);
-	asm volatile("ocbwb	%0"
-		     : /* no output */
-		     : "m" (__m(v)));
-
-	index = CACHE_IC_ADDRESS_ARRAY |
-			(v & boot_cpu_data.icache.entry_mask);
-
-	local_irq_save(flags);
-	jump_to_uncached();
-
-	for (i = 0; i < boot_cpu_data.icache.ways;
-	     i++, index += boot_cpu_data.icache.way_incr)
-		ctrl_outl(0, index);	/* Clear out Valid-bit */
-
-	back_to_cached();
-	wmb();
-	local_irq_restore(flags);
+       /* If there are too many pages then just blow the caches */
+        if (((end - start) >> PAGE_SHIFT) >= MAX_ICACHE_PAGES) {
+                flush_cache_all();
+       } else {
+               /* selectively flush d-cache then invalidate the i-cache */
+               /* this is inefficient, so only use for small ranges */
+               start &= ~(L1_CACHE_BYTES-1);
+               end += L1_CACHE_BYTES-1;
+               end &= ~(L1_CACHE_BYTES-1);
+
+               local_irq_save(flags);
+               jump_to_uncached();
+
+               for (v = start; v < end; v+=L1_CACHE_BYTES) {
+                       asm volatile("ocbwb     %0"
+                                    : /* no output */
+                                    : "m" (__m(v)));
+
+                       icacheaddr = CACHE_IC_ADDRESS_ARRAY | (
+                                       v & cpu_data->icache.entry_mask);
+
+                       for (i = 0; i < cpu_data->icache.ways;
+                               i++, icacheaddr += cpu_data->icache.way_incr)
+                                       /* Clear i-cache line valid-bit */
+                                       ctrl_outl(0, icacheaddr);
+               }
+
+		back_to_cached();
+		local_irq_restore(flags);
+	}
 }
 
 static inline void flush_cache_4096(unsigned long start,

commit cbaa118ecfd99fc5ed7adbd9c34a30e1c05e3c93
Author: Stuart Menefy <stuart.menefy@st.com>
Date:   Fri Nov 30 17:06:36 2007 +0900

    sh: Preparation for uncached jumps through PMB.
    
    Presently most of the 29-bit physical parts do P1/P2 segmentation
    with a 1:1 cached/uncached mapping, jumping between the two to
    control the caching behaviour. This provides the basic infrastructure
    to maintain this behaviour on 32-bit physical parts that don't map
    P1/P2 at all, using a shiny new linker section and corresponding
    fixmap entry.
    
    Signed-off-by: Stuart Menefy <stuart.menefy@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 226b190c5b9c..43d7ff6b6ec7 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -190,7 +190,7 @@ void flush_icache_range(unsigned long start, unsigned long end)
  * .. which happens to be the same behavior as flush_icache_range().
  * So, we simply flush out a line.
  */
-void flush_cache_sigtramp(unsigned long addr)
+void __uses_jump_to_uncached flush_cache_sigtramp(unsigned long addr)
 {
 	unsigned long v, index;
 	unsigned long flags;
@@ -205,13 +205,13 @@ void flush_cache_sigtramp(unsigned long addr)
 			(v & boot_cpu_data.icache.entry_mask);
 
 	local_irq_save(flags);
-	jump_to_P2();
+	jump_to_uncached();
 
 	for (i = 0; i < boot_cpu_data.icache.ways;
 	     i++, index += boot_cpu_data.icache.way_incr)
 		ctrl_outl(0, index);	/* Clear out Valid-bit */
 
-	back_to_P1();
+	back_to_cached();
 	wmb();
 	local_irq_restore(flags);
 }
@@ -256,12 +256,12 @@ void flush_dcache_page(struct page *page)
 }
 
 /* TODO: Selective icache invalidation through IC address array.. */
-static inline void flush_icache_all(void)
+static inline void __uses_jump_to_uncached flush_icache_all(void)
 {
 	unsigned long flags, ccr;
 
 	local_irq_save(flags);
-	jump_to_P2();
+	jump_to_uncached();
 
 	/* Flush I-cache */
 	ccr = ctrl_inl(CCR);
@@ -269,11 +269,11 @@ static inline void flush_icache_all(void)
 	ctrl_outl(ccr, CCR);
 
 	/*
-	 * back_to_P1() will take care of the barrier for us, don't add
+	 * back_to_cached() will take care of the barrier for us, don't add
 	 * another one!
 	 */
 
-	back_to_P1();
+	back_to_cached();
 	local_irq_restore(flags);
 }
 

commit ab27f62002f4dc8f759c1ec069024d8173e5dea0
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Sep 24 17:00:45 2007 +0900

    sh: Calculate cache aliases on L2 caches.
    
    Calculate the number of cache aliases on probed L2 caches, and while
    we're at it, print out the detected statistics at boot time for these
    also.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 6c36c2fb8199..226b190c5b9c 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -70,6 +70,20 @@ static void __init emit_cache_params(void)
 		boot_cpu_data.dcache.alias_mask,
 		boot_cpu_data.dcache.n_aliases);
 
+	/*
+	 * Emit Secondary Cache parameters if the CPU has a probed L2.
+	 */
+	if (boot_cpu_data.flags & CPU_HAS_L2_CACHE) {
+		printk("S-cache : n_ways=%d n_sets=%d way_incr=%d\n",
+			boot_cpu_data.scache.ways,
+			boot_cpu_data.scache.sets,
+			boot_cpu_data.scache.way_incr);
+		printk("S-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
+			boot_cpu_data.scache.entry_mask,
+			boot_cpu_data.scache.alias_mask,
+			boot_cpu_data.scache.n_aliases);
+	}
+
 	if (!__flush_dcache_segment_fn)
 		panic("unknown number of cache ways\n");
 }
@@ -81,6 +95,7 @@ void __init p3_cache_init(void)
 {
 	compute_alias(&boot_cpu_data.icache);
 	compute_alias(&boot_cpu_data.dcache);
+	compute_alias(&boot_cpu_data.scache);
 
 	switch (boot_cpu_data.dcache.ways) {
 	case 1:

commit d10040f7eb808cd984b563d1cf727a1020990a2e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Sep 24 16:38:25 2007 +0900

    sh: Fix alias calculation for non-aliasing cases.
    
    There was an off-by-1 on the cache alias detection logic on SH-4,
    which caused n_aliases to always be 1 even when the page size
    precluded the existence of aliases.
    
    With this corrected, 64KB pages happily reports n_aliases == 0, and
    hits the appropriate fast paths in the flushing routines.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index bbc226469aa1..6c36c2fb8199 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -2,7 +2,7 @@
  * arch/sh/mm/cache-sh4.c
  *
  * Copyright (C) 1999, 2000, 2002  Niibe Yutaka
- * Copyright (C) 2001 - 2006  Paul Mundt
+ * Copyright (C) 2001 - 2007  Paul Mundt
  * Copyright (C) 2003  Richard Curnow
  *
  * This file is subject to the terms and conditions of the GNU General Public
@@ -44,7 +44,7 @@ static void (*__flush_dcache_segment_fn)(unsigned long, unsigned long) =
 static void compute_alias(struct cache_info *c)
 {
 	c->alias_mask = ((c->sets - 1) << c->entry_shift) & ~(PAGE_SIZE - 1);
-	c->n_aliases = (c->alias_mask >> PAGE_SHIFT) + 1;
+	c->n_aliases = c->alias_mask ? (c->alias_mask >> PAGE_SHIFT) + 1 : 0;
 }
 
 static void __init emit_cache_params(void)

commit 7ec9d6f8c0e6932d380da1964021fbebf2311f04
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Sep 21 18:05:20 2007 +0900

    sh: Avoid smp_processor_id() in cache desc paths.
    
    current_cpu_data uses smp_processor_id() in order to find the
    corresponding cpu_data. As the cache descs are all currently
    identical, just have this look at probed results from the boot
    CPU.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 86486326ef1d..bbc226469aa1 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -54,21 +54,21 @@ static void __init emit_cache_params(void)
 		ctrl_inl(CCN_CVR),
 		ctrl_inl(CCN_PRR));
 	printk("I-cache : n_ways=%d n_sets=%d way_incr=%d\n",
-		current_cpu_data.icache.ways,
-		current_cpu_data.icache.sets,
-		current_cpu_data.icache.way_incr);
+		boot_cpu_data.icache.ways,
+		boot_cpu_data.icache.sets,
+		boot_cpu_data.icache.way_incr);
 	printk("I-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
-		current_cpu_data.icache.entry_mask,
-		current_cpu_data.icache.alias_mask,
-		current_cpu_data.icache.n_aliases);
+		boot_cpu_data.icache.entry_mask,
+		boot_cpu_data.icache.alias_mask,
+		boot_cpu_data.icache.n_aliases);
 	printk("D-cache : n_ways=%d n_sets=%d way_incr=%d\n",
-		current_cpu_data.dcache.ways,
-		current_cpu_data.dcache.sets,
-		current_cpu_data.dcache.way_incr);
+		boot_cpu_data.dcache.ways,
+		boot_cpu_data.dcache.sets,
+		boot_cpu_data.dcache.way_incr);
 	printk("D-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
-		current_cpu_data.dcache.entry_mask,
-		current_cpu_data.dcache.alias_mask,
-		current_cpu_data.dcache.n_aliases);
+		boot_cpu_data.dcache.entry_mask,
+		boot_cpu_data.dcache.alias_mask,
+		boot_cpu_data.dcache.n_aliases);
 
 	if (!__flush_dcache_segment_fn)
 		panic("unknown number of cache ways\n");
@@ -79,10 +79,10 @@ static void __init emit_cache_params(void)
  */
 void __init p3_cache_init(void)
 {
-	compute_alias(&current_cpu_data.icache);
-	compute_alias(&current_cpu_data.dcache);
+	compute_alias(&boot_cpu_data.icache);
+	compute_alias(&boot_cpu_data.dcache);
 
-	switch (current_cpu_data.dcache.ways) {
+	switch (boot_cpu_data.dcache.ways) {
 	case 1:
 		__flush_dcache_segment_fn = __flush_dcache_segment_1way;
 		break;
@@ -187,13 +187,13 @@ void flush_cache_sigtramp(unsigned long addr)
 		     : "m" (__m(v)));
 
 	index = CACHE_IC_ADDRESS_ARRAY |
-			(v & current_cpu_data.icache.entry_mask);
+			(v & boot_cpu_data.icache.entry_mask);
 
 	local_irq_save(flags);
 	jump_to_P2();
 
-	for (i = 0; i < current_cpu_data.icache.ways;
-	     i++, index += current_cpu_data.icache.way_incr)
+	for (i = 0; i < boot_cpu_data.icache.ways;
+	     i++, index += boot_cpu_data.icache.way_incr)
 		ctrl_outl(0, index);	/* Clear out Valid-bit */
 
 	back_to_P1();
@@ -210,7 +210,7 @@ static inline void flush_cache_4096(unsigned long start,
 	 * All types of SH-4 require PC to be in P2 to operate on the I-cache.
 	 * Some types of SH-4 require PC to be in P2 to operate on the D-cache.
 	 */
-	if ((current_cpu_data.flags & CPU_HAS_P2_FLUSH_BUG) ||
+	if ((boot_cpu_data.flags & CPU_HAS_P2_FLUSH_BUG) ||
 	    (start < CACHE_OC_ADDRESS_ARRAY))
 		exec_offset = 0x20000000;
 
@@ -232,7 +232,7 @@ void flush_dcache_page(struct page *page)
 		int i, n;
 
 		/* Loop all the D-cache */
-		n = current_cpu_data.dcache.n_aliases;
+		n = boot_cpu_data.dcache.n_aliases;
 		for (i = 0; i < n; i++, addr += 4096)
 			flush_cache_4096(addr, phys);
 	}
@@ -264,7 +264,7 @@ static inline void flush_icache_all(void)
 
 void flush_dcache_all(void)
 {
-	(*__flush_dcache_segment_fn)(0UL, current_cpu_data.dcache.way_size);
+	(*__flush_dcache_segment_fn)(0UL, boot_cpu_data.dcache.way_size);
 	wmb();
 }
 
@@ -278,8 +278,8 @@ static void __flush_cache_mm(struct mm_struct *mm, unsigned long start,
 			     unsigned long end)
 {
 	unsigned long d = 0, p = start & PAGE_MASK;
-	unsigned long alias_mask = current_cpu_data.dcache.alias_mask;
-	unsigned long n_aliases = current_cpu_data.dcache.n_aliases;
+	unsigned long alias_mask = boot_cpu_data.dcache.alias_mask;
+	unsigned long n_aliases = boot_cpu_data.dcache.n_aliases;
 	unsigned long select_bit;
 	unsigned long all_aliases_mask;
 	unsigned long addr_offset;
@@ -366,7 +366,7 @@ void flush_cache_mm(struct mm_struct *mm)
 	 * If cache is only 4k-per-way, there are never any 'aliases'.  Since
 	 * the cache is physically tagged, the data can just be left in there.
 	 */
-	if (current_cpu_data.dcache.n_aliases == 0)
+	if (boot_cpu_data.dcache.n_aliases == 0)
 		return;
 
 	/*
@@ -403,7 +403,7 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
 	unsigned long phys = pfn << PAGE_SHIFT;
 	unsigned int alias_mask;
 
-	alias_mask = current_cpu_data.dcache.alias_mask;
+	alias_mask = boot_cpu_data.dcache.alias_mask;
 
 	/* We only need to flush D-cache when we have alias */
 	if ((address^phys) & alias_mask) {
@@ -417,7 +417,7 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
 			phys);
 	}
 
-	alias_mask = current_cpu_data.icache.alias_mask;
+	alias_mask = boot_cpu_data.icache.alias_mask;
 	if (vma->vm_flags & VM_EXEC) {
 		/*
 		 * Evict entries from the portion of the cache from which code
@@ -449,7 +449,7 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 	 * If cache is only 4k-per-way, there are never any 'aliases'.  Since
 	 * the cache is physically tagged, the data can just be left in there.
 	 */
-	if (current_cpu_data.dcache.n_aliases == 0)
+	if (boot_cpu_data.dcache.n_aliases == 0)
 		return;
 
 	/*
@@ -510,7 +510,7 @@ static void __flush_cache_4096(unsigned long addr, unsigned long phys,
 	unsigned long a, ea, p;
 	unsigned long temp_pc;
 
-	dcache = &current_cpu_data.dcache;
+	dcache = &boot_cpu_data.dcache;
 	/* Write this way for better assembly. */
 	way_count = dcache->ways;
 	way_incr = dcache->way_incr;
@@ -585,7 +585,7 @@ static void __flush_dcache_segment_1way(unsigned long start,
 	base_addr = ((base_addr >> 16) << 16);
 	base_addr |= start;
 
-	dcache = &current_cpu_data.dcache;
+	dcache = &boot_cpu_data.dcache;
 	linesz = dcache->linesz;
 	way_incr = dcache->way_incr;
 	way_size = dcache->way_size;
@@ -627,7 +627,7 @@ static void __flush_dcache_segment_2way(unsigned long start,
 	base_addr = ((base_addr >> 16) << 16);
 	base_addr |= start;
 
-	dcache = &current_cpu_data.dcache;
+	dcache = &boot_cpu_data.dcache;
 	linesz = dcache->linesz;
 	way_incr = dcache->way_incr;
 	way_size = dcache->way_size;
@@ -686,7 +686,7 @@ static void __flush_dcache_segment_4way(unsigned long start,
 	base_addr = ((base_addr >> 16) << 16);
 	base_addr |= start;
 
-	dcache = &current_cpu_data.dcache;
+	dcache = &boot_cpu_data.dcache;
 	linesz = dcache->linesz;
 	way_incr = dcache->way_incr;
 	way_size = dcache->way_size;

commit f0b859e3d63a07995f0db294864c2f3c9228f1e4
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 25 10:43:47 2007 +0900

    sh: Reclaim beginning of P3 space for vmalloc area.
    
    The first 1MB of P3 space was reserved and used for page colouring,
    as we've reworked that to use fixmaps, we can reclaim the space and
    hand it back to VMALLOC_START.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 5d0f73a4fbbb..86486326ef1d 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -98,9 +98,6 @@ void __init p3_cache_init(void)
 	}
 
 	emit_cache_params();
-
-	if (ioremap_page_range(P3SEG, P3SEG + (PAGE_SIZE * 4), 0, PAGE_KERNEL))
-		panic("%s failed.", __FUNCTION__);
 }
 
 /*

commit 8cf1a74305688c85fc8d23ab7432a0c447ee6413
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jul 24 13:28:26 2007 +0900

    sh: Add kmap_coherent()/kunmap_coherent() interface for SH-4.
    
    This wires up kmap_coherent() and kunmap_coherent() on SH-4, and
    moves away from the p3map_mutex and reserved P3 space, opting to
    use fixmaps for colouring instead.
    
    The copy_user_page()/clear_user_page() implementations are moved
    to this, which fixes the nasty blowups with spinlock debugging
    as a result of having some of these calls nested under the page
    table lock.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 981b04089055..5d0f73a4fbbb 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -77,16 +77,8 @@ static void __init emit_cache_params(void)
 /*
  * SH-4 has virtually indexed and physically tagged cache.
  */
-
-/* Worst case assumed to be 64k cache, direct-mapped i.e. 4 synonym bits. */
-#define MAX_P3_MUTEXES 16
-
-struct mutex p3map_mutex[MAX_P3_MUTEXES];
-
 void __init p3_cache_init(void)
 {
-	int i;
-
 	compute_alias(&current_cpu_data.icache);
 	compute_alias(&current_cpu_data.dcache);
 
@@ -109,9 +101,6 @@ void __init p3_cache_init(void)
 
 	if (ioremap_page_range(P3SEG, P3SEG + (PAGE_SIZE * 4), 0, PAGE_KERNEL))
 		panic("%s failed.", __FUNCTION__);
-
-	for (i = 0; i < current_cpu_data.dcache.n_aliases; i++)
-		mutex_init(&p3map_mutex[i]);
 }
 
 /*

commit 39e688a94b94eaba768b1494e19e96f828fc2688
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Mar 5 19:46:47 2007 +0900

    sh: Revert lazy dcache writeback changes.
    
    These ended up causing too many problems on older parts,
    revert for now..
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index e0cd4b7f4aeb..981b04089055 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -237,20 +237,10 @@ static inline void flush_cache_4096(unsigned long start,
 /*
  * Write back & invalidate the D-cache of the page.
  * (To avoid "alias" issues)
- *
- * This uses a lazy write-back on UP, which is explicitly
- * disabled on SMP.
  */
 void flush_dcache_page(struct page *page)
 {
-#ifndef CONFIG_SMP
-	struct address_space *mapping = page_mapping(page);
-
-	if (mapping && !mapping_mapped(mapping))
-		set_bit(PG_dcache_dirty, &page->flags);
-	else
-#endif
-	{
+	if (test_bit(PG_mapped, &page->flags)) {
 		unsigned long phys = PHYSADDR(page_address(page));
 		unsigned long addr = CACHE_OC_ADDRESS_ARRAY;
 		int i, n;

commit 11c1965687b0a472add948d4240dfe65a2fcb298
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Dec 25 10:19:56 2006 +0900

    sh: Fixup cpu_data references for the non-boot CPUs.
    
    There are a lot of bogus cpu_data-> references that only end up working
    for the boot CPU, convert these to current_cpu_data to fixup SMP.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 72bb48773337..e0cd4b7f4aeb 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -54,21 +54,21 @@ static void __init emit_cache_params(void)
 		ctrl_inl(CCN_CVR),
 		ctrl_inl(CCN_PRR));
 	printk("I-cache : n_ways=%d n_sets=%d way_incr=%d\n",
-		cpu_data->icache.ways,
-		cpu_data->icache.sets,
-		cpu_data->icache.way_incr);
+		current_cpu_data.icache.ways,
+		current_cpu_data.icache.sets,
+		current_cpu_data.icache.way_incr);
 	printk("I-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
-		cpu_data->icache.entry_mask,
-		cpu_data->icache.alias_mask,
-		cpu_data->icache.n_aliases);
+		current_cpu_data.icache.entry_mask,
+		current_cpu_data.icache.alias_mask,
+		current_cpu_data.icache.n_aliases);
 	printk("D-cache : n_ways=%d n_sets=%d way_incr=%d\n",
-		cpu_data->dcache.ways,
-		cpu_data->dcache.sets,
-		cpu_data->dcache.way_incr);
+		current_cpu_data.dcache.ways,
+		current_cpu_data.dcache.sets,
+		current_cpu_data.dcache.way_incr);
 	printk("D-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
-		cpu_data->dcache.entry_mask,
-		cpu_data->dcache.alias_mask,
-		cpu_data->dcache.n_aliases);
+		current_cpu_data.dcache.entry_mask,
+		current_cpu_data.dcache.alias_mask,
+		current_cpu_data.dcache.n_aliases);
 
 	if (!__flush_dcache_segment_fn)
 		panic("unknown number of cache ways\n");
@@ -87,10 +87,10 @@ void __init p3_cache_init(void)
 {
 	int i;
 
-	compute_alias(&cpu_data->icache);
-	compute_alias(&cpu_data->dcache);
+	compute_alias(&current_cpu_data.icache);
+	compute_alias(&current_cpu_data.dcache);
 
-	switch (cpu_data->dcache.ways) {
+	switch (current_cpu_data.dcache.ways) {
 	case 1:
 		__flush_dcache_segment_fn = __flush_dcache_segment_1way;
 		break;
@@ -110,7 +110,7 @@ void __init p3_cache_init(void)
 	if (ioremap_page_range(P3SEG, P3SEG + (PAGE_SIZE * 4), 0, PAGE_KERNEL))
 		panic("%s failed.", __FUNCTION__);
 
-	for (i = 0; i < cpu_data->dcache.n_aliases; i++)
+	for (i = 0; i < current_cpu_data.dcache.n_aliases; i++)
 		mutex_init(&p3map_mutex[i]);
 }
 
@@ -200,13 +200,14 @@ void flush_cache_sigtramp(unsigned long addr)
 		     : /* no output */
 		     : "m" (__m(v)));
 
-	index = CACHE_IC_ADDRESS_ARRAY | (v & cpu_data->icache.entry_mask);
+	index = CACHE_IC_ADDRESS_ARRAY |
+			(v & current_cpu_data.icache.entry_mask);
 
 	local_irq_save(flags);
 	jump_to_P2();
 
-	for (i = 0; i < cpu_data->icache.ways;
-	     i++, index += cpu_data->icache.way_incr)
+	for (i = 0; i < current_cpu_data.icache.ways;
+	     i++, index += current_cpu_data.icache.way_incr)
 		ctrl_outl(0, index);	/* Clear out Valid-bit */
 
 	back_to_P1();
@@ -223,7 +224,7 @@ static inline void flush_cache_4096(unsigned long start,
 	 * All types of SH-4 require PC to be in P2 to operate on the I-cache.
 	 * Some types of SH-4 require PC to be in P2 to operate on the D-cache.
 	 */
-	if ((cpu_data->flags & CPU_HAS_P2_FLUSH_BUG) ||
+	if ((current_cpu_data.flags & CPU_HAS_P2_FLUSH_BUG) ||
 	    (start < CACHE_OC_ADDRESS_ARRAY))
 		exec_offset = 0x20000000;
 
@@ -255,7 +256,7 @@ void flush_dcache_page(struct page *page)
 		int i, n;
 
 		/* Loop all the D-cache */
-		n = cpu_data->dcache.n_aliases;
+		n = current_cpu_data.dcache.n_aliases;
 		for (i = 0; i < n; i++, addr += 4096)
 			flush_cache_4096(addr, phys);
 	}
@@ -287,7 +288,7 @@ static inline void flush_icache_all(void)
 
 void flush_dcache_all(void)
 {
-	(*__flush_dcache_segment_fn)(0UL, cpu_data->dcache.way_size);
+	(*__flush_dcache_segment_fn)(0UL, current_cpu_data.dcache.way_size);
 	wmb();
 }
 
@@ -301,8 +302,8 @@ static void __flush_cache_mm(struct mm_struct *mm, unsigned long start,
 			     unsigned long end)
 {
 	unsigned long d = 0, p = start & PAGE_MASK;
-	unsigned long alias_mask = cpu_data->dcache.alias_mask;
-	unsigned long n_aliases = cpu_data->dcache.n_aliases;
+	unsigned long alias_mask = current_cpu_data.dcache.alias_mask;
+	unsigned long n_aliases = current_cpu_data.dcache.n_aliases;
 	unsigned long select_bit;
 	unsigned long all_aliases_mask;
 	unsigned long addr_offset;
@@ -389,7 +390,7 @@ void flush_cache_mm(struct mm_struct *mm)
 	 * If cache is only 4k-per-way, there are never any 'aliases'.  Since
 	 * the cache is physically tagged, the data can just be left in there.
 	 */
-	if (cpu_data->dcache.n_aliases == 0)
+	if (current_cpu_data.dcache.n_aliases == 0)
 		return;
 
 	/*
@@ -426,7 +427,7 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
 	unsigned long phys = pfn << PAGE_SHIFT;
 	unsigned int alias_mask;
 
-	alias_mask = cpu_data->dcache.alias_mask;
+	alias_mask = current_cpu_data.dcache.alias_mask;
 
 	/* We only need to flush D-cache when we have alias */
 	if ((address^phys) & alias_mask) {
@@ -440,7 +441,7 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
 			phys);
 	}
 
-	alias_mask = cpu_data->icache.alias_mask;
+	alias_mask = current_cpu_data.icache.alias_mask;
 	if (vma->vm_flags & VM_EXEC) {
 		/*
 		 * Evict entries from the portion of the cache from which code
@@ -472,7 +473,7 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 	 * If cache is only 4k-per-way, there are never any 'aliases'.  Since
 	 * the cache is physically tagged, the data can just be left in there.
 	 */
-	if (cpu_data->dcache.n_aliases == 0)
+	if (current_cpu_data.dcache.n_aliases == 0)
 		return;
 
 	/*
@@ -533,7 +534,7 @@ static void __flush_cache_4096(unsigned long addr, unsigned long phys,
 	unsigned long a, ea, p;
 	unsigned long temp_pc;
 
-	dcache = &cpu_data->dcache;
+	dcache = &current_cpu_data.dcache;
 	/* Write this way for better assembly. */
 	way_count = dcache->ways;
 	way_incr = dcache->way_incr;
@@ -608,7 +609,7 @@ static void __flush_dcache_segment_1way(unsigned long start,
 	base_addr = ((base_addr >> 16) << 16);
 	base_addr |= start;
 
-	dcache = &cpu_data->dcache;
+	dcache = &current_cpu_data.dcache;
 	linesz = dcache->linesz;
 	way_incr = dcache->way_incr;
 	way_size = dcache->way_size;
@@ -650,7 +651,7 @@ static void __flush_dcache_segment_2way(unsigned long start,
 	base_addr = ((base_addr >> 16) << 16);
 	base_addr |= start;
 
-	dcache = &cpu_data->dcache;
+	dcache = &current_cpu_data.dcache;
 	linesz = dcache->linesz;
 	way_incr = dcache->way_incr;
 	way_size = dcache->way_size;
@@ -709,7 +710,7 @@ static void __flush_dcache_segment_4way(unsigned long start,
 	base_addr = ((base_addr >> 16) << 16);
 	base_addr |= start;
 
-	dcache = &cpu_data->dcache;
+	dcache = &current_cpu_data.dcache;
 	linesz = dcache->linesz;
 	way_incr = dcache->way_incr;
 	way_size = dcache->way_size;

commit 26b7a78c55fbc0e23a7dc19e89fd50f200efc002
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Dec 28 10:31:48 2006 +0900

    sh: Lazy dcache writeback optimizations.
    
    This converts the lazy dcache handling to the model described in
    Documentation/cachetlb.txt and drops the ptep_get_and_clear() hacks
    used for the aliasing dcaches on SH-4 and SH7705 in 32kB mode. As a
    bonus, this slightly cuts down on the cache flushing frequency.
    
    With that and the PTEA handling out of the way, the update_mmu_cache()
    implementations can be consolidated, and we no longer have to worry
    about which configuration the cache is in for the SH7705 case.
    
    And finally, explicitly disable the lazy writeback on SMP (SH-4A).
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index c6955157c989..72bb48773337 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -236,10 +236,20 @@ static inline void flush_cache_4096(unsigned long start,
 /*
  * Write back & invalidate the D-cache of the page.
  * (To avoid "alias" issues)
+ *
+ * This uses a lazy write-back on UP, which is explicitly
+ * disabled on SMP.
  */
 void flush_dcache_page(struct page *page)
 {
-	if (test_bit(PG_mapped, &page->flags)) {
+#ifndef CONFIG_SMP
+	struct address_space *mapping = page_mapping(page);
+
+	if (mapping && !mapping_mapped(mapping))
+		set_bit(PG_dcache_dirty, &page->flags);
+	else
+#endif
+	{
 		unsigned long phys = PHYSADDR(page_address(page));
 		unsigned long addr = CACHE_OC_ADDRESS_ARRAY;
 		int i, n;

commit 37bda1da4570c2e9c6dd34e77d2120218e384950
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Dec 9 09:16:12 2006 +0900

    sh: Convert remaining remap_area_pages() users to ioremap_page_range().
    
    A couple of these were missed.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index ae531affccbd..c6955157c989 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -107,7 +107,7 @@ void __init p3_cache_init(void)
 
 	emit_cache_params();
 
-	if (remap_area_pages(P3SEG, 0, PAGE_SIZE * 4, _PAGE_CACHABLE))
+	if (ioremap_page_range(P3SEG, P3SEG + (PAGE_SIZE * 4), 0, PAGE_KERNEL))
 		panic("%s failed.", __FUNCTION__);
 
 	for (i = 0; i < cpu_data->dcache.n_aliases; i++)

commit 510c72ad2dd4e05e6908755f51ac89482c6eb987
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Nov 27 12:06:26 2006 +0900

    sh: Fixup various PAGE_SIZE == 4096 assumptions.
    
    There were a number of places that made evil PAGE_SIZE == 4k
    assumptions that ended up breaking when trying to play with
    8k and 64k page sizes, this fixes those up.
    
    The most significant change is the way we load THREAD_SIZE,
    previously this was done via:
    
            mov     #(THREAD_SIZE >> 8), reg
            shll8   reg
    
    to avoid a memory access and allow the immediate load. With
    a 64k PAGE_SIZE, we're out of range for the immediate load
    size without resorting to special instructions available in
    later ISAs (movi20s and so on). The "workaround" for this is
    to bump up the shift to 10 and insert a shll2, which gives a
    bit more flexibility while still being much cheaper than a
    memory access.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 7e62ba071d64..ae531affccbd 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -225,7 +225,7 @@ static inline void flush_cache_4096(unsigned long start,
 	 */
 	if ((cpu_data->flags & CPU_HAS_P2_FLUSH_BUG) ||
 	    (start < CACHE_OC_ADDRESS_ARRAY))
-	    	exec_offset = 0x20000000;
+		exec_offset = 0x20000000;
 
 	local_irq_save(flags);
 	__flush_cache_4096(start | SH_CACHE_ASSOC,
@@ -246,7 +246,7 @@ void flush_dcache_page(struct page *page)
 
 		/* Loop all the D-cache */
 		n = cpu_data->dcache.n_aliases;
-		for (i = 0; i < n; i++, addr += PAGE_SIZE)
+		for (i = 0; i < n; i++, addr += 4096)
 			flush_cache_4096(addr, phys);
 	}
 

commit 52e27782e1c4afa1feca0fdf194d279595e0431c
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Nov 21 11:09:41 2006 +0900

    sh: p3map_sem sem2mutex conversion.
    
    Simple sem2mutex conversion for the p3map semaphores.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index e48cc22724d9..7e62ba071d64 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -11,12 +11,8 @@
  */
 #include <linux/init.h>
 #include <linux/mm.h>
-#include <asm/addrspace.h>
-#include <asm/pgtable.h>
-#include <asm/processor.h>
-#include <asm/cache.h>
-#include <asm/io.h>
-#include <asm/pgalloc.h>
+#include <linux/io.h>
+#include <linux/mutex.h>
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
@@ -83,9 +79,9 @@ static void __init emit_cache_params(void)
  */
 
 /* Worst case assumed to be 64k cache, direct-mapped i.e. 4 synonym bits. */
-#define MAX_P3_SEMAPHORES 16
+#define MAX_P3_MUTEXES 16
 
-struct semaphore p3map_sem[MAX_P3_SEMAPHORES];
+struct mutex p3map_mutex[MAX_P3_MUTEXES];
 
 void __init p3_cache_init(void)
 {
@@ -115,7 +111,7 @@ void __init p3_cache_init(void)
 		panic("%s failed.", __FUNCTION__);
 
 	for (i = 0; i < cpu_data->dcache.n_aliases; i++)
-		sema_init(&p3map_sem[i], 1);
+		mutex_init(&p3map_mutex[i]);
 }
 
 /*

commit 33573c0e3243aaa38b6ad96942de85a1b713c2ff
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 18:37:30 2006 +0900

    sh: Fix occasional flush_cache_4096() stack corruption.
    
    IRQs disabling in flush_cache_4096 for cache purge. Under certain
    workloads we would get an IRQ in the middle of a purge operation,
    and the cachelines would remain in an inconsistent state, leading
    to occasional stack corruption.
    
    Signed-off-by: Takeo Takahashi <takahashi.takeo@renesas.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index aa4f62f0e374..e48cc22724d9 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -221,22 +221,20 @@ void flush_cache_sigtramp(unsigned long addr)
 static inline void flush_cache_4096(unsigned long start,
 				    unsigned long phys)
 {
+	unsigned long flags, exec_offset = 0;
+
 	/*
 	 * All types of SH-4 require PC to be in P2 to operate on the I-cache.
 	 * Some types of SH-4 require PC to be in P2 to operate on the D-cache.
 	 */
 	if ((cpu_data->flags & CPU_HAS_P2_FLUSH_BUG) ||
-	    (start < CACHE_OC_ADDRESS_ARRAY)) {
-		unsigned long flags;
-
-		local_irq_save(flags);
-		__flush_cache_4096(start | SH_CACHE_ASSOC,
-				   P1SEGADDR(phys), 0x20000000);
-		local_irq_restore(flags);
-	} else {
-		__flush_cache_4096(start | SH_CACHE_ASSOC,
-				   P1SEGADDR(phys), 0);
-	}
+	    (start < CACHE_OC_ADDRESS_ARRAY))
+	    	exec_offset = 0x20000000;
+
+	local_irq_save(flags);
+	__flush_cache_4096(start | SH_CACHE_ASSOC,
+			   P1SEGADDR(phys), exec_offset);
+	local_irq_restore(flags);
 }
 
 /*

commit 28ccf7f91b1ac42ee1f18480a69d2a7486b625ce
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 18:30:07 2006 +0900

    sh: Selective flush_cache_mm() flushing.
    
    flush_cache_mm() wraps in to flush_cache_all(), which is rather
    excessive given that the number of PTEs within the specified context
    are generally quite low.  Optimize for walking the mm's VMA list and
    selectively flushing the VMA ranges from the dcache. Invalidate the
    icache only if a VMA sets VM_EXEC.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 2203bd6aadb3..aa4f62f0e374 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -2,29 +2,31 @@
  * arch/sh/mm/cache-sh4.c
  *
  * Copyright (C) 1999, 2000, 2002  Niibe Yutaka
- * Copyright (C) 2001, 2002, 2003, 2004, 2005  Paul Mundt
+ * Copyright (C) 2001 - 2006  Paul Mundt
  * Copyright (C) 2003  Richard Curnow
  *
  * This file is subject to the terms and conditions of the GNU General Public
  * License.  See the file "COPYING" in the main directory of this archive
  * for more details.
  */
-
 #include <linux/init.h>
-#include <linux/mman.h>
 #include <linux/mm.h>
-#include <linux/threads.h>
 #include <asm/addrspace.h>
-#include <asm/page.h>
 #include <asm/pgtable.h>
 #include <asm/processor.h>
 #include <asm/cache.h>
 #include <asm/io.h>
-#include <asm/uaccess.h>
 #include <asm/pgalloc.h>
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
+/*
+ * The maximum number of pages we support up to when doing ranged dcache
+ * flushing. Anything exceeding this will simply flush the dcache in its
+ * entirety.
+ */
+#define MAX_DCACHE_PAGES	64	/* XXX: Tune for ways */
+
 static void __flush_dcache_segment_1way(unsigned long start,
 					unsigned long extent);
 static void __flush_dcache_segment_2way(unsigned long start,
@@ -219,14 +221,14 @@ void flush_cache_sigtramp(unsigned long addr)
 static inline void flush_cache_4096(unsigned long start,
 				    unsigned long phys)
 {
-	unsigned long flags;
-
 	/*
 	 * All types of SH-4 require PC to be in P2 to operate on the I-cache.
 	 * Some types of SH-4 require PC to be in P2 to operate on the D-cache.
 	 */
-	if ((cpu_data->flags & CPU_HAS_P2_FLUSH_BUG)
-	   || start < CACHE_OC_ADDRESS_ARRAY) {
+	if ((cpu_data->flags & CPU_HAS_P2_FLUSH_BUG) ||
+	    (start < CACHE_OC_ADDRESS_ARRAY)) {
+		unsigned long flags;
+
 		local_irq_save(flags);
 		__flush_cache_4096(start | SH_CACHE_ASSOC,
 				   P1SEGADDR(phys), 0x20000000);
@@ -257,6 +259,7 @@ void flush_dcache_page(struct page *page)
 	wmb();
 }
 
+/* TODO: Selective icache invalidation through IC address array.. */
 static inline void flush_icache_all(void)
 {
 	unsigned long flags, ccr;
@@ -290,19 +293,121 @@ void flush_cache_all(void)
 	flush_icache_all();
 }
 
+static void __flush_cache_mm(struct mm_struct *mm, unsigned long start,
+			     unsigned long end)
+{
+	unsigned long d = 0, p = start & PAGE_MASK;
+	unsigned long alias_mask = cpu_data->dcache.alias_mask;
+	unsigned long n_aliases = cpu_data->dcache.n_aliases;
+	unsigned long select_bit;
+	unsigned long all_aliases_mask;
+	unsigned long addr_offset;
+	pgd_t *dir;
+	pmd_t *pmd;
+	pud_t *pud;
+	pte_t *pte;
+	int i;
+
+	dir = pgd_offset(mm, p);
+	pud = pud_offset(dir, p);
+	pmd = pmd_offset(pud, p);
+	end = PAGE_ALIGN(end);
+
+	all_aliases_mask = (1 << n_aliases) - 1;
+
+	do {
+		if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd))) {
+			p &= PMD_MASK;
+			p += PMD_SIZE;
+			pmd++;
+
+			continue;
+		}
+
+		pte = pte_offset_kernel(pmd, p);
+
+		do {
+			unsigned long phys;
+			pte_t entry = *pte;
+
+			if (!(pte_val(entry) & _PAGE_PRESENT)) {
+				pte++;
+				p += PAGE_SIZE;
+				continue;
+			}
+
+			phys = pte_val(entry) & PTE_PHYS_MASK;
+
+			if ((p ^ phys) & alias_mask) {
+				d |= 1 << ((p & alias_mask) >> PAGE_SHIFT);
+				d |= 1 << ((phys & alias_mask) >> PAGE_SHIFT);
+
+				if (d == all_aliases_mask)
+					goto loop_exit;
+			}
+
+			pte++;
+			p += PAGE_SIZE;
+		} while (p < end && ((unsigned long)pte & ~PAGE_MASK));
+		pmd++;
+	} while (p < end);
+
+loop_exit:
+	addr_offset = 0;
+	select_bit = 1;
+
+	for (i = 0; i < n_aliases; i++) {
+		if (d & select_bit) {
+			(*__flush_dcache_segment_fn)(addr_offset, PAGE_SIZE);
+			wmb();
+		}
+
+		select_bit <<= 1;
+		addr_offset += PAGE_SIZE;
+	}
+}
+
+/*
+ * Note : (RPC) since the caches are physically tagged, the only point
+ * of flush_cache_mm for SH-4 is to get rid of aliases from the
+ * D-cache.  The assumption elsewhere, e.g. flush_cache_range, is that
+ * lines can stay resident so long as the virtual address they were
+ * accessed with (hence cache set) is in accord with the physical
+ * address (i.e. tag).  It's no different here.  So I reckon we don't
+ * need to flush the I-cache, since aliases don't matter for that.  We
+ * should try that.
+ *
+ * Caller takes mm->mmap_sem.
+ */
 void flush_cache_mm(struct mm_struct *mm)
 {
 	/*
-	 * Note : (RPC) since the caches are physically tagged, the only point
-	 * of flush_cache_mm for SH-4 is to get rid of aliases from the
-	 * D-cache.  The assumption elsewhere, e.g. flush_cache_range, is that
-	 * lines can stay resident so long as the virtual address they were
-	 * accessed with (hence cache set) is in accord with the physical
-	 * address (i.e. tag).  It's no different here.  So I reckon we don't
-	 * need to flush the I-cache, since aliases don't matter for that.  We
-	 * should try that.
+	 * If cache is only 4k-per-way, there are never any 'aliases'.  Since
+	 * the cache is physically tagged, the data can just be left in there.
+	 */
+	if (cpu_data->dcache.n_aliases == 0)
+		return;
+
+	/*
+	 * Don't bother groveling around the dcache for the VMA ranges
+	 * if there are too many PTEs to make it worthwhile.
 	 */
-	flush_cache_all();
+	if (mm->nr_ptes >= MAX_DCACHE_PAGES)
+		flush_dcache_all();
+	else {
+		struct vm_area_struct *vma;
+
+		/*
+		 * In this case there are reasonably sized ranges to flush,
+		 * iterate through the VMA list and take care of any aliases.
+		 */
+		for (vma = mm->mmap; vma; vma = vma->vm_next)
+			__flush_cache_mm(mm, vma->vm_start, vma->vm_end);
+	}
+
+	/* Only touch the icache if one of the VMAs has VM_EXEC set. */
+	if (mm->exec_vm)
+		flush_icache_all();
 }
 
 /*
@@ -311,7 +416,8 @@ void flush_cache_mm(struct mm_struct *mm)
  * ADDR: Virtual Address (U0 address)
  * PFN: Physical page number
  */
-void flush_cache_page(struct vm_area_struct *vma, unsigned long address, unsigned long pfn)
+void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
+		      unsigned long pfn)
 {
 	unsigned long phys = pfn << PAGE_SHIFT;
 	unsigned int alias_mask;
@@ -358,87 +464,22 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long address, unsigne
 void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 		       unsigned long end)
 {
-	unsigned long d = 0, p = start & PAGE_MASK;
-	unsigned long alias_mask = cpu_data->dcache.alias_mask;
-	unsigned long n_aliases = cpu_data->dcache.n_aliases;
-	unsigned long select_bit;
-	unsigned long all_aliases_mask;
-	unsigned long addr_offset;
-	unsigned long phys;
-	pgd_t *dir;
-	pmd_t *pmd;
-	pud_t *pud;
-	pte_t *pte;
-	pte_t entry;
-	int i;
-
 	/*
 	 * If cache is only 4k-per-way, there are never any 'aliases'.  Since
 	 * the cache is physically tagged, the data can just be left in there.
 	 */
-	if (n_aliases == 0)
+	if (cpu_data->dcache.n_aliases == 0)
 		return;
 
-	all_aliases_mask = (1 << n_aliases) - 1;
-
 	/*
 	 * Don't bother with the lookup and alias check if we have a
 	 * wide range to cover, just blow away the dcache in its
 	 * entirety instead. -- PFM.
 	 */
-	if (((end - start) >> PAGE_SHIFT) >= 64) {
+	if (((end - start) >> PAGE_SHIFT) >= MAX_DCACHE_PAGES)
 		flush_dcache_all();
-
-		if (vma->vm_flags & VM_EXEC)
-			flush_icache_all();
-
-		return;
-	}
-
-	dir = pgd_offset(vma->vm_mm, p);
-	pud = pud_offset(dir, p);
-	pmd = pmd_offset(pud, p);
-	end = PAGE_ALIGN(end);
-
-	do {
-		if (pmd_none(*pmd) || pmd_bad(*pmd)) {
-			p &= ~((1 << PMD_SHIFT) - 1);
-			p += (1 << PMD_SHIFT);
-			pmd++;
-
-			continue;
-		}
-
-		pte = pte_offset_kernel(pmd, p);
-
-		do {
-			entry = *pte;
-
-			if ((pte_val(entry) & _PAGE_PRESENT)) {
-				phys = pte_val(entry) & PTE_PHYS_MASK;
-
-				if ((p ^ phys) & alias_mask) {
-					d |= 1 << ((p & alias_mask) >> PAGE_SHIFT);
-					d |= 1 << ((phys & alias_mask) >> PAGE_SHIFT);
-
-					if (d == all_aliases_mask)
-						goto loop_exit;
-				}
-			}
-
-			pte++;
-			p += PAGE_SIZE;
-		} while (p < end && ((unsigned long)pte & ~PAGE_MASK));
-		pmd++;
-	} while (p < end);
-
-loop_exit:
-	for (i = 0, select_bit = 0x1, addr_offset = 0x0; i < n_aliases;
-	     i++, select_bit <<= 1, addr_offset += PAGE_SIZE)
-		if (d & select_bit) {
-			(*__flush_dcache_segment_fn)(addr_offset, PAGE_SIZE);
-			wmb();
-		}
+	else
+		__flush_cache_mm(vma->vm_mm, start, end);
 
 	if (vma->vm_flags & VM_EXEC) {
 		/*
@@ -731,4 +772,3 @@ static void __flush_dcache_segment_4way(unsigned long start,
 		a3 += linesz;
 	} while (a0 < a0e);
 }
-

commit 298476220d1f793ca0ac6c9e5dc817e1ad3e9851
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 14:57:44 2006 +0900

    sh: Add control register barriers.
    
    Currently when making changes to control registers, we
    typically need some time for changes to take effect (8
    nops, generally).  However, for sh4a we simply need to
    do an icbi..
    
    This is a simple patch for implementing a general purpose
    ctrl_barrier() which functions as a control register write
    barrier. There's some additional documentation in the patch
    itself, but it's pretty self explanatory.
    
    There were also some places where we were not doing the
    barrier, which didn't seem to have any adverse effects on
    legacy parts, but certainly did on sh4a. It's safer to have
    the barrier in place for legacy parts as well in these cases,
    though this does make flush_tlb_all() more expensive (by an
    order of 8 nops).  We can ifdef around the flush_tlb_all()
    case for now if it's clear that all legacy parts won't have
    a problem with this.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index c036c2b4ac2b..2203bd6aadb3 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -269,6 +269,11 @@ static inline void flush_icache_all(void)
 	ccr |= CCR_CACHE_ICI;
 	ctrl_outl(ccr, CCR);
 
+	/*
+	 * back_to_P1() will take care of the barrier for us, don't add
+	 * another one!
+	 */
+
 	back_to_P1();
 	local_irq_restore(flags);
 }

commit b638d0b921dc95229af0dfd09cd24850336a2f75
Author: Richard Curnow <richard.curnow@st.com>
Date:   Wed Sep 27 14:09:26 2006 +0900

    sh: Optimized cache handling for SH-4/SH-4A caches.
    
    This reworks some of the SH-4 cache handling code to more easily
    accomodate newer-style caches (particularly for the > direct-mapped
    case), as well as optimizing some of the old code.
    
    Signed-off-by: Richard Curnow <richard.curnow@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 846b63d6f5e8..c036c2b4ac2b 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -25,28 +25,95 @@
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
-extern void __flush_cache_4096(unsigned long addr, unsigned long phys,
+static void __flush_dcache_segment_1way(unsigned long start,
+					unsigned long extent);
+static void __flush_dcache_segment_2way(unsigned long start,
+					unsigned long extent);
+static void __flush_dcache_segment_4way(unsigned long start,
+					unsigned long extent);
+
+static void __flush_cache_4096(unsigned long addr, unsigned long phys,
 			       unsigned long exec_offset);
-extern void __flush_cache_4096_all(unsigned long start);
-static void __flush_cache_4096_all_ex(unsigned long start);
-extern void __flush_dcache_all(void);
-static void __flush_dcache_all_ex(void);
+
+/*
+ * This is initialised here to ensure that it is not placed in the BSS.  If
+ * that were to happen, note that cache_init gets called before the BSS is
+ * cleared, so this would get nulled out which would be hopeless.
+ */
+static void (*__flush_dcache_segment_fn)(unsigned long, unsigned long) =
+	(void (*)(unsigned long, unsigned long))0xdeadbeef;
+
+static void compute_alias(struct cache_info *c)
+{
+	c->alias_mask = ((c->sets - 1) << c->entry_shift) & ~(PAGE_SIZE - 1);
+	c->n_aliases = (c->alias_mask >> PAGE_SHIFT) + 1;
+}
+
+static void __init emit_cache_params(void)
+{
+	printk("PVR=%08x CVR=%08x PRR=%08x\n",
+		ctrl_inl(CCN_PVR),
+		ctrl_inl(CCN_CVR),
+		ctrl_inl(CCN_PRR));
+	printk("I-cache : n_ways=%d n_sets=%d way_incr=%d\n",
+		cpu_data->icache.ways,
+		cpu_data->icache.sets,
+		cpu_data->icache.way_incr);
+	printk("I-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
+		cpu_data->icache.entry_mask,
+		cpu_data->icache.alias_mask,
+		cpu_data->icache.n_aliases);
+	printk("D-cache : n_ways=%d n_sets=%d way_incr=%d\n",
+		cpu_data->dcache.ways,
+		cpu_data->dcache.sets,
+		cpu_data->dcache.way_incr);
+	printk("D-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
+		cpu_data->dcache.entry_mask,
+		cpu_data->dcache.alias_mask,
+		cpu_data->dcache.n_aliases);
+
+	if (!__flush_dcache_segment_fn)
+		panic("unknown number of cache ways\n");
+}
 
 /*
  * SH-4 has virtually indexed and physically tagged cache.
  */
 
-struct semaphore p3map_sem[4];
+/* Worst case assumed to be 64k cache, direct-mapped i.e. 4 synonym bits. */
+#define MAX_P3_SEMAPHORES 16
+
+struct semaphore p3map_sem[MAX_P3_SEMAPHORES];
 
 void __init p3_cache_init(void)
 {
-	if (remap_area_pages(P3SEG, 0, PAGE_SIZE*4, _PAGE_CACHABLE))
+	int i;
+
+	compute_alias(&cpu_data->icache);
+	compute_alias(&cpu_data->dcache);
+
+	switch (cpu_data->dcache.ways) {
+	case 1:
+		__flush_dcache_segment_fn = __flush_dcache_segment_1way;
+		break;
+	case 2:
+		__flush_dcache_segment_fn = __flush_dcache_segment_2way;
+		break;
+	case 4:
+		__flush_dcache_segment_fn = __flush_dcache_segment_4way;
+		break;
+	default:
+		__flush_dcache_segment_fn = NULL;
+		break;
+	}
+
+	emit_cache_params();
+
+	if (remap_area_pages(P3SEG, 0, PAGE_SIZE * 4, _PAGE_CACHABLE))
 		panic("%s failed.", __FUNCTION__);
 
-	sema_init (&p3map_sem[0], 1);
-	sema_init (&p3map_sem[1], 1);
-	sema_init (&p3map_sem[2], 1);
-	sema_init (&p3map_sem[3], 1);
+	for (i = 0; i < cpu_data->dcache.n_aliases; i++)
+		sema_init(&p3map_sem[i], 1);
 }
 
 /*
@@ -91,7 +158,6 @@ void __flush_purge_region(void *start, int size)
 	}
 }
 
-
 /*
  * No write back please
  */
@@ -110,46 +176,6 @@ void __flush_invalidate_region(void *start, int size)
 	}
 }
 
-static void __flush_dcache_all_ex(void)
-{
-	unsigned long addr, end_addr, entry_offset;
-
-	end_addr = CACHE_OC_ADDRESS_ARRAY +
-		(cpu_data->dcache.sets << cpu_data->dcache.entry_shift) *
-		 cpu_data->dcache.ways;
-
-	entry_offset = 1 << cpu_data->dcache.entry_shift;
-	for (addr = CACHE_OC_ADDRESS_ARRAY;
-	     addr < end_addr;
-	     addr += entry_offset) {
-		ctrl_outl(0, addr);
-	}
-}
-
-static void __flush_cache_4096_all_ex(unsigned long start)
-{
-	unsigned long addr, entry_offset;
-	int i;
-
-	entry_offset = 1 << cpu_data->dcache.entry_shift;
-	for (i = 0; i < cpu_data->dcache.ways;
-	     i++, start += cpu_data->dcache.way_incr) {
-		for (addr = CACHE_OC_ADDRESS_ARRAY + start;
-		     addr < CACHE_OC_ADDRESS_ARRAY + 4096 + start;
-		     addr += entry_offset) {
-			ctrl_outl(0, addr);
-		}
-	}
-}
-
-void flush_cache_4096_all(unsigned long start)
-{
-	if (cpu_data->dcache.ways == 1)
-		__flush_cache_4096_all(start);
-	else
-		__flush_cache_4096_all_ex(start);
-}
-
 /*
  * Write back the range of D-cache, and purge the I-cache.
  *
@@ -180,9 +206,11 @@ void flush_cache_sigtramp(unsigned long addr)
 
 	local_irq_save(flags);
 	jump_to_P2();
+
 	for (i = 0; i < cpu_data->icache.ways;
 	     i++, index += cpu_data->icache.way_incr)
 		ctrl_outl(0, index);	/* Clear out Valid-bit */
+
 	back_to_P1();
 	wmb();
 	local_irq_restore(flags);
@@ -194,8 +222,8 @@ static inline void flush_cache_4096(unsigned long start,
 	unsigned long flags;
 
 	/*
-	 * SH7751, SH7751R, and ST40 have no restriction to handle cache.
-	 * (While SH7750 must do that at P2 area.)
+	 * All types of SH-4 require PC to be in P2 to operate on the I-cache.
+	 * Some types of SH-4 require PC to be in P2 to operate on the D-cache.
 	 */
 	if ((cpu_data->flags & CPU_HAS_P2_FLUSH_BUG)
 	   || start < CACHE_OC_ADDRESS_ARRAY) {
@@ -217,12 +245,13 @@ void flush_dcache_page(struct page *page)
 {
 	if (test_bit(PG_mapped, &page->flags)) {
 		unsigned long phys = PHYSADDR(page_address(page));
+		unsigned long addr = CACHE_OC_ADDRESS_ARRAY;
+		int i, n;
 
 		/* Loop all the D-cache */
-		flush_cache_4096(CACHE_OC_ADDRESS_ARRAY,          phys);
-		flush_cache_4096(CACHE_OC_ADDRESS_ARRAY | 0x1000, phys);
-		flush_cache_4096(CACHE_OC_ADDRESS_ARRAY | 0x2000, phys);
-		flush_cache_4096(CACHE_OC_ADDRESS_ARRAY | 0x3000, phys);
+		n = cpu_data->dcache.n_aliases;
+		for (i = 0; i < n; i++, addr += PAGE_SIZE)
+			flush_cache_4096(addr, phys);
 	}
 
 	wmb();
@@ -246,10 +275,7 @@ static inline void flush_icache_all(void)
 
 void flush_dcache_all(void)
 {
-	if (cpu_data->dcache.ways == 1)
-		__flush_dcache_all();
-	else
-		__flush_dcache_all_ex();
+	(*__flush_dcache_segment_fn)(0UL, cpu_data->dcache.way_size);
 	wmb();
 }
 
@@ -261,6 +287,16 @@ void flush_cache_all(void)
 
 void flush_cache_mm(struct mm_struct *mm)
 {
+	/*
+	 * Note : (RPC) since the caches are physically tagged, the only point
+	 * of flush_cache_mm for SH-4 is to get rid of aliases from the
+	 * D-cache.  The assumption elsewhere, e.g. flush_cache_range, is that
+	 * lines can stay resident so long as the virtual address they were
+	 * accessed with (hence cache set) is in accord with the physical
+	 * address (i.e. tag).  It's no different here.  So I reckon we don't
+	 * need to flush the I-cache, since aliases don't matter for that.  We
+	 * should try that.
+	 */
 	flush_cache_all();
 }
 
@@ -273,24 +309,36 @@ void flush_cache_mm(struct mm_struct *mm)
 void flush_cache_page(struct vm_area_struct *vma, unsigned long address, unsigned long pfn)
 {
 	unsigned long phys = pfn << PAGE_SHIFT;
+	unsigned int alias_mask;
+
+	alias_mask = cpu_data->dcache.alias_mask;
 
 	/* We only need to flush D-cache when we have alias */
-	if ((address^phys) & CACHE_ALIAS) {
+	if ((address^phys) & alias_mask) {
 		/* Loop 4K of the D-cache */
 		flush_cache_4096(
-			CACHE_OC_ADDRESS_ARRAY | (address & CACHE_ALIAS),
+			CACHE_OC_ADDRESS_ARRAY | (address & alias_mask),
 			phys);
 		/* Loop another 4K of the D-cache */
 		flush_cache_4096(
-			CACHE_OC_ADDRESS_ARRAY | (phys & CACHE_ALIAS),
+			CACHE_OC_ADDRESS_ARRAY | (phys & alias_mask),
 			phys);
 	}
 
-	if (vma->vm_flags & VM_EXEC)
-		/* Loop 4K (half) of the I-cache */
+	alias_mask = cpu_data->icache.alias_mask;
+	if (vma->vm_flags & VM_EXEC) {
+		/*
+		 * Evict entries from the portion of the cache from which code
+		 * may have been executed at this address (virtual).  There's
+		 * no need to evict from the portion corresponding to the
+		 * physical address as for the D-cache, because we know the
+		 * kernel has never executed the code through its identity
+		 * translation.
+		 */
 		flush_cache_4096(
-			CACHE_IC_ADDRESS_ARRAY | (address & 0x1000),
+			CACHE_IC_ADDRESS_ARRAY | (address & alias_mask),
 			phys);
+	}
 }
 
 /*
@@ -305,14 +353,28 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long address, unsigne
 void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 		       unsigned long end)
 {
-	unsigned long p = start & PAGE_MASK;
+	unsigned long d = 0, p = start & PAGE_MASK;
+	unsigned long alias_mask = cpu_data->dcache.alias_mask;
+	unsigned long n_aliases = cpu_data->dcache.n_aliases;
+	unsigned long select_bit;
+	unsigned long all_aliases_mask;
+	unsigned long addr_offset;
+	unsigned long phys;
 	pgd_t *dir;
 	pmd_t *pmd;
 	pud_t *pud;
 	pte_t *pte;
 	pte_t entry;
-	unsigned long phys;
-	unsigned long d = 0;
+	int i;
+
+	/*
+	 * If cache is only 4k-per-way, there are never any 'aliases'.  Since
+	 * the cache is physically tagged, the data can just be left in there.
+	 */
+	if (n_aliases == 0)
+		return;
+
+	all_aliases_mask = (1 << n_aliases) - 1;
 
 	/*
 	 * Don't bother with the lookup and alias check if we have a
@@ -335,39 +397,52 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 
 	do {
 		if (pmd_none(*pmd) || pmd_bad(*pmd)) {
-			p &= ~((1 << PMD_SHIFT) -1);
+			p &= ~((1 << PMD_SHIFT) - 1);
 			p += (1 << PMD_SHIFT);
 			pmd++;
+
 			continue;
 		}
+
 		pte = pte_offset_kernel(pmd, p);
+
 		do {
 			entry = *pte;
+
 			if ((pte_val(entry) & _PAGE_PRESENT)) {
-				phys = pte_val(entry)&PTE_PHYS_MASK;
-				if ((p^phys) & CACHE_ALIAS) {
-					d |= 1 << ((p & CACHE_ALIAS)>>12);
-					d |= 1 << ((phys & CACHE_ALIAS)>>12);
-					if (d == 0x0f)
+				phys = pte_val(entry) & PTE_PHYS_MASK;
+
+				if ((p ^ phys) & alias_mask) {
+					d |= 1 << ((p & alias_mask) >> PAGE_SHIFT);
+					d |= 1 << ((phys & alias_mask) >> PAGE_SHIFT);
+
+					if (d == all_aliases_mask)
 						goto loop_exit;
 				}
 			}
+
 			pte++;
 			p += PAGE_SIZE;
 		} while (p < end && ((unsigned long)pte & ~PAGE_MASK));
 		pmd++;
 	} while (p < end);
- loop_exit:
-	if (d & 1)
-		flush_cache_4096_all(0);
-	if (d & 2)
-		flush_cache_4096_all(0x1000);
-	if (d & 4)
-		flush_cache_4096_all(0x2000);
-	if (d & 8)
-		flush_cache_4096_all(0x3000);
-	if (vma->vm_flags & VM_EXEC)
+
+loop_exit:
+	for (i = 0, select_bit = 0x1, addr_offset = 0x0; i < n_aliases;
+	     i++, select_bit <<= 1, addr_offset += PAGE_SIZE)
+		if (d & select_bit) {
+			(*__flush_dcache_segment_fn)(addr_offset, PAGE_SIZE);
+			wmb();
+		}
+
+	if (vma->vm_flags & VM_EXEC) {
+		/*
+		 * TODO: Is this required???  Need to look at how I-cache
+		 * coherency is assured when new programs are loaded to see if
+		 * this matters.
+		 */
 		flush_icache_all();
+	}
 }
 
 /*
@@ -384,3 +459,271 @@ void flush_icache_user_range(struct vm_area_struct *vma,
 	mb();
 }
 
+/**
+ * __flush_cache_4096
+ *
+ * @addr:  address in memory mapped cache array
+ * @phys:  P1 address to flush (has to match tags if addr has 'A' bit
+ *         set i.e. associative write)
+ * @exec_offset: set to 0x20000000 if flush has to be executed from P2
+ *               region else 0x0
+ *
+ * The offset into the cache array implied by 'addr' selects the
+ * 'colour' of the virtual address range that will be flushed.  The
+ * operation (purge/write-back) is selected by the lower 2 bits of
+ * 'phys'.
+ */
+static void __flush_cache_4096(unsigned long addr, unsigned long phys,
+			       unsigned long exec_offset)
+{
+	int way_count;
+	unsigned long base_addr = addr;
+	struct cache_info *dcache;
+	unsigned long way_incr;
+	unsigned long a, ea, p;
+	unsigned long temp_pc;
+
+	dcache = &cpu_data->dcache;
+	/* Write this way for better assembly. */
+	way_count = dcache->ways;
+	way_incr = dcache->way_incr;
+
+	/*
+	 * Apply exec_offset (i.e. branch to P2 if required.).
+	 *
+	 * FIXME:
+	 *
+	 *	If I write "=r" for the (temp_pc), it puts this in r6 hence
+	 *	trashing exec_offset before it's been added on - why?  Hence
+	 *	"=&r" as a 'workaround'
+	 */
+	asm volatile("mov.l 1f, %0\n\t"
+		     "add   %1, %0\n\t"
+		     "jmp   @%0\n\t"
+		     "nop\n\t"
+		     ".balign 4\n\t"
+		     "1:  .long 2f\n\t"
+		     "2:\n" : "=&r" (temp_pc) : "r" (exec_offset));
+
+	/*
+	 * We know there will be >=1 iteration, so write as do-while to avoid
+	 * pointless nead-of-loop check for 0 iterations.
+	 */
+	do {
+		ea = base_addr + PAGE_SIZE;
+		a = base_addr;
+		p = phys;
+
+		do {
+			*(volatile unsigned long *)a = p;
+			/*
+			 * Next line: intentionally not p+32, saves an add, p
+			 * will do since only the cache tag bits need to
+			 * match.
+			 */
+			*(volatile unsigned long *)(a+32) = p;
+			a += 64;
+			p += 64;
+		} while (a < ea);
+
+		base_addr += way_incr;
+	} while (--way_count != 0);
+}
+
+/*
+ * Break the 1, 2 and 4 way variants of this out into separate functions to
+ * avoid nearly all the overhead of having the conditional stuff in the function
+ * bodies (+ the 1 and 2 way cases avoid saving any registers too).
+ */
+static void __flush_dcache_segment_1way(unsigned long start,
+					unsigned long extent_per_way)
+{
+	unsigned long orig_sr, sr_with_bl;
+	unsigned long base_addr;
+	unsigned long way_incr, linesz, way_size;
+	struct cache_info *dcache;
+	register unsigned long a0, a0e;
+
+	asm volatile("stc sr, %0" : "=r" (orig_sr));
+	sr_with_bl = orig_sr | (1<<28);
+	base_addr = ((unsigned long)&empty_zero_page[0]);
+
+	/*
+	 * The previous code aligned base_addr to 16k, i.e. the way_size of all
+	 * existing SH-4 D-caches.  Whilst I don't see a need to have this
+	 * aligned to any better than the cache line size (which it will be
+	 * anyway by construction), let's align it to at least the way_size of
+	 * any existing or conceivable SH-4 D-cache.  -- RPC
+	 */
+	base_addr = ((base_addr >> 16) << 16);
+	base_addr |= start;
+
+	dcache = &cpu_data->dcache;
+	linesz = dcache->linesz;
+	way_incr = dcache->way_incr;
+	way_size = dcache->way_size;
+
+	a0 = base_addr;
+	a0e = base_addr + extent_per_way;
+	do {
+		asm volatile("ldc %0, sr" : : "r" (sr_with_bl));
+		asm volatile("movca.l r0, @%0\n\t"
+			     "ocbi @%0" : : "r" (a0));
+		a0 += linesz;
+		asm volatile("movca.l r0, @%0\n\t"
+			     "ocbi @%0" : : "r" (a0));
+		a0 += linesz;
+		asm volatile("movca.l r0, @%0\n\t"
+			     "ocbi @%0" : : "r" (a0));
+		a0 += linesz;
+		asm volatile("movca.l r0, @%0\n\t"
+			     "ocbi @%0" : : "r" (a0));
+		asm volatile("ldc %0, sr" : : "r" (orig_sr));
+		a0 += linesz;
+	} while (a0 < a0e);
+}
+
+static void __flush_dcache_segment_2way(unsigned long start,
+					unsigned long extent_per_way)
+{
+	unsigned long orig_sr, sr_with_bl;
+	unsigned long base_addr;
+	unsigned long way_incr, linesz, way_size;
+	struct cache_info *dcache;
+	register unsigned long a0, a1, a0e;
+
+	asm volatile("stc sr, %0" : "=r" (orig_sr));
+	sr_with_bl = orig_sr | (1<<28);
+	base_addr = ((unsigned long)&empty_zero_page[0]);
+
+	/* See comment under 1-way above */
+	base_addr = ((base_addr >> 16) << 16);
+	base_addr |= start;
+
+	dcache = &cpu_data->dcache;
+	linesz = dcache->linesz;
+	way_incr = dcache->way_incr;
+	way_size = dcache->way_size;
+
+	a0 = base_addr;
+	a1 = a0 + way_incr;
+	a0e = base_addr + extent_per_way;
+	do {
+		asm volatile("ldc %0, sr" : : "r" (sr_with_bl));
+		asm volatile("movca.l r0, @%0\n\t"
+			     "movca.l r0, @%1\n\t"
+			     "ocbi @%0\n\t"
+			     "ocbi @%1" : :
+			     "r" (a0), "r" (a1));
+		a0 += linesz;
+		a1 += linesz;
+		asm volatile("movca.l r0, @%0\n\t"
+			     "movca.l r0, @%1\n\t"
+			     "ocbi @%0\n\t"
+			     "ocbi @%1" : :
+			     "r" (a0), "r" (a1));
+		a0 += linesz;
+		a1 += linesz;
+		asm volatile("movca.l r0, @%0\n\t"
+			     "movca.l r0, @%1\n\t"
+			     "ocbi @%0\n\t"
+			     "ocbi @%1" : :
+			     "r" (a0), "r" (a1));
+		a0 += linesz;
+		a1 += linesz;
+		asm volatile("movca.l r0, @%0\n\t"
+			     "movca.l r0, @%1\n\t"
+			     "ocbi @%0\n\t"
+			     "ocbi @%1" : :
+			     "r" (a0), "r" (a1));
+		asm volatile("ldc %0, sr" : : "r" (orig_sr));
+		a0 += linesz;
+		a1 += linesz;
+	} while (a0 < a0e);
+}
+
+static void __flush_dcache_segment_4way(unsigned long start,
+					unsigned long extent_per_way)
+{
+	unsigned long orig_sr, sr_with_bl;
+	unsigned long base_addr;
+	unsigned long way_incr, linesz, way_size;
+	struct cache_info *dcache;
+	register unsigned long a0, a1, a2, a3, a0e;
+
+	asm volatile("stc sr, %0" : "=r" (orig_sr));
+	sr_with_bl = orig_sr | (1<<28);
+	base_addr = ((unsigned long)&empty_zero_page[0]);
+
+	/* See comment under 1-way above */
+	base_addr = ((base_addr >> 16) << 16);
+	base_addr |= start;
+
+	dcache = &cpu_data->dcache;
+	linesz = dcache->linesz;
+	way_incr = dcache->way_incr;
+	way_size = dcache->way_size;
+
+	a0 = base_addr;
+	a1 = a0 + way_incr;
+	a2 = a1 + way_incr;
+	a3 = a2 + way_incr;
+	a0e = base_addr + extent_per_way;
+	do {
+		asm volatile("ldc %0, sr" : : "r" (sr_with_bl));
+		asm volatile("movca.l r0, @%0\n\t"
+			     "movca.l r0, @%1\n\t"
+			     "movca.l r0, @%2\n\t"
+			     "movca.l r0, @%3\n\t"
+			     "ocbi @%0\n\t"
+			     "ocbi @%1\n\t"
+			     "ocbi @%2\n\t"
+			     "ocbi @%3\n\t" : :
+			     "r" (a0), "r" (a1), "r" (a2), "r" (a3));
+		a0 += linesz;
+		a1 += linesz;
+		a2 += linesz;
+		a3 += linesz;
+		asm volatile("movca.l r0, @%0\n\t"
+			     "movca.l r0, @%1\n\t"
+			     "movca.l r0, @%2\n\t"
+			     "movca.l r0, @%3\n\t"
+			     "ocbi @%0\n\t"
+			     "ocbi @%1\n\t"
+			     "ocbi @%2\n\t"
+			     "ocbi @%3\n\t" : :
+			     "r" (a0), "r" (a1), "r" (a2), "r" (a3));
+		a0 += linesz;
+		a1 += linesz;
+		a2 += linesz;
+		a3 += linesz;
+		asm volatile("movca.l r0, @%0\n\t"
+			     "movca.l r0, @%1\n\t"
+			     "movca.l r0, @%2\n\t"
+			     "movca.l r0, @%3\n\t"
+			     "ocbi @%0\n\t"
+			     "ocbi @%1\n\t"
+			     "ocbi @%2\n\t"
+			     "ocbi @%3\n\t" : :
+			     "r" (a0), "r" (a1), "r" (a2), "r" (a3));
+		a0 += linesz;
+		a1 += linesz;
+		a2 += linesz;
+		a3 += linesz;
+		asm volatile("movca.l r0, @%0\n\t"
+			     "movca.l r0, @%1\n\t"
+			     "movca.l r0, @%2\n\t"
+			     "movca.l r0, @%3\n\t"
+			     "ocbi @%0\n\t"
+			     "ocbi @%1\n\t"
+			     "ocbi @%2\n\t"
+			     "ocbi @%3\n\t" : :
+			     "r" (a0), "r" (a1), "r" (a2), "r" (a3));
+		asm volatile("ldc %0, sr" : : "r" (orig_sr));
+		a0 += linesz;
+		a1 += linesz;
+		a2 += linesz;
+		a3 += linesz;
+	} while (a0 < a0e);
+}
+

commit fdfc74f9fcebdda14609159d5010b758a9409acf
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 14:05:52 2006 +0900

    sh: Support for SH-4A memory barriers.
    
    SH-4A supports 'synco' as a barrier, sprinkle it around
    the cache ops as necessary..
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 94c05d09c3f7..846b63d6f5e8 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -184,6 +184,7 @@ void flush_cache_sigtramp(unsigned long addr)
 	     i++, index += cpu_data->icache.way_incr)
 		ctrl_outl(0, index);	/* Clear out Valid-bit */
 	back_to_P1();
+	wmb();
 	local_irq_restore(flags);
 }
 
@@ -223,6 +224,8 @@ void flush_dcache_page(struct page *page)
 		flush_cache_4096(CACHE_OC_ADDRESS_ARRAY | 0x2000, phys);
 		flush_cache_4096(CACHE_OC_ADDRESS_ARRAY | 0x3000, phys);
 	}
+
+	wmb();
 }
 
 static inline void flush_icache_all(void)
@@ -247,6 +250,7 @@ void flush_dcache_all(void)
 		__flush_dcache_all();
 	else
 		__flush_dcache_all_ex();
+	wmb();
 }
 
 void flush_cache_all(void)
@@ -377,5 +381,6 @@ void flush_icache_user_range(struct vm_area_struct *vma,
 			     struct page *page, unsigned long addr, int len)
 {
 	flush_cache_page(vma, addr, page_to_pfn(page));
+	mb();
 }
 

commit a252710fc5b63b24934905ca47ecf661702d7f00
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 11:29:55 2006 +0900

    sh: flush_cache_range() cleanup and optimizations.
    
    flush_cache_range() wasn't page aligning the end of the range,
    we can't assume that it will always be page aligned, and we
    ended up getting unaligned faults in some rare call paths.
    
    Additionally, we add a small optimization to just purge the
    dcache entirely if the range is large enough that the page
    table walking will take longer. We use an arbitrary value of
    64 pages for the large range size, as per sh64.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index 524cea5b47f9..94c05d09c3f7 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -2,7 +2,7 @@
  * arch/sh/mm/cache-sh4.c
  *
  * Copyright (C) 1999, 2000, 2002  Niibe Yutaka
- * Copyright (C) 2001, 2002, 2003, 2004  Paul Mundt
+ * Copyright (C) 2001, 2002, 2003, 2004, 2005  Paul Mundt
  * Copyright (C) 2003  Richard Curnow
  *
  * This file is subject to the terms and conditions of the GNU General Public
@@ -25,6 +25,8 @@
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
+extern void __flush_cache_4096(unsigned long addr, unsigned long phys,
+			       unsigned long exec_offset);
 extern void __flush_cache_4096_all(unsigned long start);
 static void __flush_cache_4096_all_ex(unsigned long start);
 extern void __flush_dcache_all(void);
@@ -112,9 +114,14 @@ static void __flush_dcache_all_ex(void)
 {
 	unsigned long addr, end_addr, entry_offset;
 
-	end_addr = CACHE_OC_ADDRESS_ARRAY + (cpu_data->dcache.sets << cpu_data->dcache.entry_shift) * cpu_data->dcache.ways;
+	end_addr = CACHE_OC_ADDRESS_ARRAY +
+		(cpu_data->dcache.sets << cpu_data->dcache.entry_shift) *
+		 cpu_data->dcache.ways;
+
 	entry_offset = 1 << cpu_data->dcache.entry_shift;
-	for (addr = CACHE_OC_ADDRESS_ARRAY; addr < end_addr; addr += entry_offset) {
+	for (addr = CACHE_OC_ADDRESS_ARRAY;
+	     addr < end_addr;
+	     addr += entry_offset) {
 		ctrl_outl(0, addr);
 	}
 }
@@ -125,7 +132,8 @@ static void __flush_cache_4096_all_ex(unsigned long start)
 	int i;
 
 	entry_offset = 1 << cpu_data->dcache.entry_shift;
-	for (i = 0; i < cpu_data->dcache.ways; i++, start += cpu_data->dcache.way_incr) {
+	for (i = 0; i < cpu_data->dcache.ways;
+	     i++, start += cpu_data->dcache.way_incr) {
 		for (addr = CACHE_OC_ADDRESS_ARRAY + start;
 		     addr < CACHE_OC_ADDRESS_ARRAY + 4096 + start;
 		     addr += entry_offset) {
@@ -153,14 +161,14 @@ void flush_icache_range(unsigned long start, unsigned long end)
 }
 
 /*
- * Write back the D-cache and purge the I-cache for signal trampoline. 
+ * Write back the D-cache and purge the I-cache for signal trampoline.
  * .. which happens to be the same behavior as flush_icache_range().
  * So, we simply flush out a line.
  */
 void flush_cache_sigtramp(unsigned long addr)
 {
 	unsigned long v, index;
-	unsigned long flags; 
+	unsigned long flags;
 	int i;
 
 	v = addr & ~(L1_CACHE_BYTES-1);
@@ -172,7 +180,8 @@ void flush_cache_sigtramp(unsigned long addr)
 
 	local_irq_save(flags);
 	jump_to_P2();
-	for(i = 0; i < cpu_data->icache.ways; i++, index += cpu_data->icache.way_incr)
+	for (i = 0; i < cpu_data->icache.ways;
+	     i++, index += cpu_data->icache.way_incr)
 		ctrl_outl(0, index);	/* Clear out Valid-bit */
 	back_to_P1();
 	local_irq_restore(flags);
@@ -181,8 +190,7 @@ void flush_cache_sigtramp(unsigned long addr)
 static inline void flush_cache_4096(unsigned long start,
 				    unsigned long phys)
 {
-	unsigned long flags; 
-	extern void __flush_cache_4096(unsigned long addr, unsigned long phys, unsigned long exec_offset);
+	unsigned long flags;
 
 	/*
 	 * SH7751, SH7751R, and ST40 have no restriction to handle cache.
@@ -191,10 +199,12 @@ static inline void flush_cache_4096(unsigned long start,
 	if ((cpu_data->flags & CPU_HAS_P2_FLUSH_BUG)
 	   || start < CACHE_OC_ADDRESS_ARRAY) {
 		local_irq_save(flags);
-		__flush_cache_4096(start | SH_CACHE_ASSOC, P1SEGADDR(phys), 0x20000000);
+		__flush_cache_4096(start | SH_CACHE_ASSOC,
+				   P1SEGADDR(phys), 0x20000000);
 		local_irq_restore(flags);
 	} else {
-		__flush_cache_4096(start | SH_CACHE_ASSOC, P1SEGADDR(phys), 0);
+		__flush_cache_4096(start | SH_CACHE_ASSOC,
+				   P1SEGADDR(phys), 0);
 	}
 }
 
@@ -231,29 +241,22 @@ static inline void flush_icache_all(void)
 	local_irq_restore(flags);
 }
 
-void flush_cache_all(void)
+void flush_dcache_all(void)
 {
 	if (cpu_data->dcache.ways == 1)
 		__flush_dcache_all();
 	else
 		__flush_dcache_all_ex();
+}
+
+void flush_cache_all(void)
+{
+	flush_dcache_all();
 	flush_icache_all();
 }
 
 void flush_cache_mm(struct mm_struct *mm)
 {
-	/* Is there any good way? */
-	/* XXX: possibly call flush_cache_range for each vm area */
-	/* 
-	 * FIXME: Really, the optimal solution here would be able to flush out
-	 * individual lines created by the specified context, but this isn't
-	 * feasible for a number of architectures (such as MIPS, and some
-	 * SPARC) .. is this possible for SuperH?
-	 *
-	 * In the meantime, we'll just flush all of the caches.. this
-	 * seems to be the simplest way to avoid at least a few wasted
-	 * cache flushes. -Lethal
-	 */
 	flush_cache_all();
 }
 
@@ -301,13 +304,30 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 	unsigned long p = start & PAGE_MASK;
 	pgd_t *dir;
 	pmd_t *pmd;
+	pud_t *pud;
 	pte_t *pte;
 	pte_t entry;
 	unsigned long phys;
 	unsigned long d = 0;
 
+	/*
+	 * Don't bother with the lookup and alias check if we have a
+	 * wide range to cover, just blow away the dcache in its
+	 * entirety instead. -- PFM.
+	 */
+	if (((end - start) >> PAGE_SHIFT) >= 64) {
+		flush_dcache_all();
+
+		if (vma->vm_flags & VM_EXEC)
+			flush_icache_all();
+
+		return;
+	}
+
 	dir = pgd_offset(vma->vm_mm, p);
-	pmd = pmd_offset(dir, p);
+	pud = pud_offset(dir, p);
+	pmd = pmd_offset(pud, p);
+	end = PAGE_ALIGN(end);
 
 	do {
 		if (pmd_none(*pmd) || pmd_bad(*pmd)) {
@@ -322,7 +342,7 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 			if ((pte_val(entry) & _PAGE_PRESENT)) {
 				phys = pte_val(entry)&PTE_PHYS_MASK;
 				if ((p^phys) & CACHE_ALIAS) {
-					d |= 1 << ((p & CACHE_ALIAS)>>12); 
+					d |= 1 << ((p & CACHE_ALIAS)>>12);
 					d |= 1 << ((phys & CACHE_ALIAS)>>12);
 					if (d == 0x0f)
 						goto loop_exit;

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jrn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jrn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
index ab833adf28c3..524cea5b47f9 100644
--- a/arch/sh/mm/cache-sh4.c
+++ b/arch/sh/mm/cache-sh4.c
@@ -10,7 +10,6 @@
  * for more details.
  */
 
-#include <linux/config.h>
 #include <linux/init.h>
 #include <linux/mman.h>
 #include <linux/mm.h>

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c
new file mode 100644
index 000000000000..ab833adf28c3
--- /dev/null
+++ b/arch/sh/mm/cache-sh4.c
@@ -0,0 +1,362 @@
+/*
+ * arch/sh/mm/cache-sh4.c
+ *
+ * Copyright (C) 1999, 2000, 2002  Niibe Yutaka
+ * Copyright (C) 2001, 2002, 2003, 2004  Paul Mundt
+ * Copyright (C) 2003  Richard Curnow
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ */
+
+#include <linux/config.h>
+#include <linux/init.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/threads.h>
+#include <asm/addrspace.h>
+#include <asm/page.h>
+#include <asm/pgtable.h>
+#include <asm/processor.h>
+#include <asm/cache.h>
+#include <asm/io.h>
+#include <asm/uaccess.h>
+#include <asm/pgalloc.h>
+#include <asm/mmu_context.h>
+#include <asm/cacheflush.h>
+
+extern void __flush_cache_4096_all(unsigned long start);
+static void __flush_cache_4096_all_ex(unsigned long start);
+extern void __flush_dcache_all(void);
+static void __flush_dcache_all_ex(void);
+
+/*
+ * SH-4 has virtually indexed and physically tagged cache.
+ */
+
+struct semaphore p3map_sem[4];
+
+void __init p3_cache_init(void)
+{
+	if (remap_area_pages(P3SEG, 0, PAGE_SIZE*4, _PAGE_CACHABLE))
+		panic("%s failed.", __FUNCTION__);
+
+	sema_init (&p3map_sem[0], 1);
+	sema_init (&p3map_sem[1], 1);
+	sema_init (&p3map_sem[2], 1);
+	sema_init (&p3map_sem[3], 1);
+}
+
+/*
+ * Write back the dirty D-caches, but not invalidate them.
+ *
+ * START: Virtual Address (U0, P1, or P3)
+ * SIZE: Size of the region.
+ */
+void __flush_wback_region(void *start, int size)
+{
+	unsigned long v;
+	unsigned long begin, end;
+
+	begin = (unsigned long)start & ~(L1_CACHE_BYTES-1);
+	end = ((unsigned long)start + size + L1_CACHE_BYTES-1)
+		& ~(L1_CACHE_BYTES-1);
+	for (v = begin; v < end; v+=L1_CACHE_BYTES) {
+		asm volatile("ocbwb	%0"
+			     : /* no output */
+			     : "m" (__m(v)));
+	}
+}
+
+/*
+ * Write back the dirty D-caches and invalidate them.
+ *
+ * START: Virtual Address (U0, P1, or P3)
+ * SIZE: Size of the region.
+ */
+void __flush_purge_region(void *start, int size)
+{
+	unsigned long v;
+	unsigned long begin, end;
+
+	begin = (unsigned long)start & ~(L1_CACHE_BYTES-1);
+	end = ((unsigned long)start + size + L1_CACHE_BYTES-1)
+		& ~(L1_CACHE_BYTES-1);
+	for (v = begin; v < end; v+=L1_CACHE_BYTES) {
+		asm volatile("ocbp	%0"
+			     : /* no output */
+			     : "m" (__m(v)));
+	}
+}
+
+
+/*
+ * No write back please
+ */
+void __flush_invalidate_region(void *start, int size)
+{
+	unsigned long v;
+	unsigned long begin, end;
+
+	begin = (unsigned long)start & ~(L1_CACHE_BYTES-1);
+	end = ((unsigned long)start + size + L1_CACHE_BYTES-1)
+		& ~(L1_CACHE_BYTES-1);
+	for (v = begin; v < end; v+=L1_CACHE_BYTES) {
+		asm volatile("ocbi	%0"
+			     : /* no output */
+			     : "m" (__m(v)));
+	}
+}
+
+static void __flush_dcache_all_ex(void)
+{
+	unsigned long addr, end_addr, entry_offset;
+
+	end_addr = CACHE_OC_ADDRESS_ARRAY + (cpu_data->dcache.sets << cpu_data->dcache.entry_shift) * cpu_data->dcache.ways;
+	entry_offset = 1 << cpu_data->dcache.entry_shift;
+	for (addr = CACHE_OC_ADDRESS_ARRAY; addr < end_addr; addr += entry_offset) {
+		ctrl_outl(0, addr);
+	}
+}
+
+static void __flush_cache_4096_all_ex(unsigned long start)
+{
+	unsigned long addr, entry_offset;
+	int i;
+
+	entry_offset = 1 << cpu_data->dcache.entry_shift;
+	for (i = 0; i < cpu_data->dcache.ways; i++, start += cpu_data->dcache.way_incr) {
+		for (addr = CACHE_OC_ADDRESS_ARRAY + start;
+		     addr < CACHE_OC_ADDRESS_ARRAY + 4096 + start;
+		     addr += entry_offset) {
+			ctrl_outl(0, addr);
+		}
+	}
+}
+
+void flush_cache_4096_all(unsigned long start)
+{
+	if (cpu_data->dcache.ways == 1)
+		__flush_cache_4096_all(start);
+	else
+		__flush_cache_4096_all_ex(start);
+}
+
+/*
+ * Write back the range of D-cache, and purge the I-cache.
+ *
+ * Called from kernel/module.c:sys_init_module and routine for a.out format.
+ */
+void flush_icache_range(unsigned long start, unsigned long end)
+{
+	flush_cache_all();
+}
+
+/*
+ * Write back the D-cache and purge the I-cache for signal trampoline. 
+ * .. which happens to be the same behavior as flush_icache_range().
+ * So, we simply flush out a line.
+ */
+void flush_cache_sigtramp(unsigned long addr)
+{
+	unsigned long v, index;
+	unsigned long flags; 
+	int i;
+
+	v = addr & ~(L1_CACHE_BYTES-1);
+	asm volatile("ocbwb	%0"
+		     : /* no output */
+		     : "m" (__m(v)));
+
+	index = CACHE_IC_ADDRESS_ARRAY | (v & cpu_data->icache.entry_mask);
+
+	local_irq_save(flags);
+	jump_to_P2();
+	for(i = 0; i < cpu_data->icache.ways; i++, index += cpu_data->icache.way_incr)
+		ctrl_outl(0, index);	/* Clear out Valid-bit */
+	back_to_P1();
+	local_irq_restore(flags);
+}
+
+static inline void flush_cache_4096(unsigned long start,
+				    unsigned long phys)
+{
+	unsigned long flags; 
+	extern void __flush_cache_4096(unsigned long addr, unsigned long phys, unsigned long exec_offset);
+
+	/*
+	 * SH7751, SH7751R, and ST40 have no restriction to handle cache.
+	 * (While SH7750 must do that at P2 area.)
+	 */
+	if ((cpu_data->flags & CPU_HAS_P2_FLUSH_BUG)
+	   || start < CACHE_OC_ADDRESS_ARRAY) {
+		local_irq_save(flags);
+		__flush_cache_4096(start | SH_CACHE_ASSOC, P1SEGADDR(phys), 0x20000000);
+		local_irq_restore(flags);
+	} else {
+		__flush_cache_4096(start | SH_CACHE_ASSOC, P1SEGADDR(phys), 0);
+	}
+}
+
+/*
+ * Write back & invalidate the D-cache of the page.
+ * (To avoid "alias" issues)
+ */
+void flush_dcache_page(struct page *page)
+{
+	if (test_bit(PG_mapped, &page->flags)) {
+		unsigned long phys = PHYSADDR(page_address(page));
+
+		/* Loop all the D-cache */
+		flush_cache_4096(CACHE_OC_ADDRESS_ARRAY,          phys);
+		flush_cache_4096(CACHE_OC_ADDRESS_ARRAY | 0x1000, phys);
+		flush_cache_4096(CACHE_OC_ADDRESS_ARRAY | 0x2000, phys);
+		flush_cache_4096(CACHE_OC_ADDRESS_ARRAY | 0x3000, phys);
+	}
+}
+
+static inline void flush_icache_all(void)
+{
+	unsigned long flags, ccr;
+
+	local_irq_save(flags);
+	jump_to_P2();
+
+	/* Flush I-cache */
+	ccr = ctrl_inl(CCR);
+	ccr |= CCR_CACHE_ICI;
+	ctrl_outl(ccr, CCR);
+
+	back_to_P1();
+	local_irq_restore(flags);
+}
+
+void flush_cache_all(void)
+{
+	if (cpu_data->dcache.ways == 1)
+		__flush_dcache_all();
+	else
+		__flush_dcache_all_ex();
+	flush_icache_all();
+}
+
+void flush_cache_mm(struct mm_struct *mm)
+{
+	/* Is there any good way? */
+	/* XXX: possibly call flush_cache_range for each vm area */
+	/* 
+	 * FIXME: Really, the optimal solution here would be able to flush out
+	 * individual lines created by the specified context, but this isn't
+	 * feasible for a number of architectures (such as MIPS, and some
+	 * SPARC) .. is this possible for SuperH?
+	 *
+	 * In the meantime, we'll just flush all of the caches.. this
+	 * seems to be the simplest way to avoid at least a few wasted
+	 * cache flushes. -Lethal
+	 */
+	flush_cache_all();
+}
+
+/*
+ * Write back and invalidate I/D-caches for the page.
+ *
+ * ADDR: Virtual Address (U0 address)
+ * PFN: Physical page number
+ */
+void flush_cache_page(struct vm_area_struct *vma, unsigned long address, unsigned long pfn)
+{
+	unsigned long phys = pfn << PAGE_SHIFT;
+
+	/* We only need to flush D-cache when we have alias */
+	if ((address^phys) & CACHE_ALIAS) {
+		/* Loop 4K of the D-cache */
+		flush_cache_4096(
+			CACHE_OC_ADDRESS_ARRAY | (address & CACHE_ALIAS),
+			phys);
+		/* Loop another 4K of the D-cache */
+		flush_cache_4096(
+			CACHE_OC_ADDRESS_ARRAY | (phys & CACHE_ALIAS),
+			phys);
+	}
+
+	if (vma->vm_flags & VM_EXEC)
+		/* Loop 4K (half) of the I-cache */
+		flush_cache_4096(
+			CACHE_IC_ADDRESS_ARRAY | (address & 0x1000),
+			phys);
+}
+
+/*
+ * Write back and invalidate D-caches.
+ *
+ * START, END: Virtual Address (U0 address)
+ *
+ * NOTE: We need to flush the _physical_ page entry.
+ * Flushing the cache lines for U0 only isn't enough.
+ * We need to flush for P1 too, which may contain aliases.
+ */
+void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
+		       unsigned long end)
+{
+	unsigned long p = start & PAGE_MASK;
+	pgd_t *dir;
+	pmd_t *pmd;
+	pte_t *pte;
+	pte_t entry;
+	unsigned long phys;
+	unsigned long d = 0;
+
+	dir = pgd_offset(vma->vm_mm, p);
+	pmd = pmd_offset(dir, p);
+
+	do {
+		if (pmd_none(*pmd) || pmd_bad(*pmd)) {
+			p &= ~((1 << PMD_SHIFT) -1);
+			p += (1 << PMD_SHIFT);
+			pmd++;
+			continue;
+		}
+		pte = pte_offset_kernel(pmd, p);
+		do {
+			entry = *pte;
+			if ((pte_val(entry) & _PAGE_PRESENT)) {
+				phys = pte_val(entry)&PTE_PHYS_MASK;
+				if ((p^phys) & CACHE_ALIAS) {
+					d |= 1 << ((p & CACHE_ALIAS)>>12); 
+					d |= 1 << ((phys & CACHE_ALIAS)>>12);
+					if (d == 0x0f)
+						goto loop_exit;
+				}
+			}
+			pte++;
+			p += PAGE_SIZE;
+		} while (p < end && ((unsigned long)pte & ~PAGE_MASK));
+		pmd++;
+	} while (p < end);
+ loop_exit:
+	if (d & 1)
+		flush_cache_4096_all(0);
+	if (d & 2)
+		flush_cache_4096_all(0x1000);
+	if (d & 4)
+		flush_cache_4096_all(0x2000);
+	if (d & 8)
+		flush_cache_4096_all(0x3000);
+	if (vma->vm_flags & VM_EXEC)
+		flush_icache_all();
+}
+
+/*
+ * flush_icache_user_range
+ * @vma: VMA of the process
+ * @page: page
+ * @addr: U0 address
+ * @len: length of the range (< page size)
+ */
+void flush_icache_user_range(struct vm_area_struct *vma,
+			     struct page *page, unsigned long addr, int len)
+{
+	flush_cache_page(vma, addr, page_to_pfn(page));
+}
+
