commit c456cfc2e52bff3540614ac85e0a1da95248f637
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 28 10:10:14 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 211
    
    Based on 1 normalized pattern(s):
    
      released under the terms of the gnu gpl v2 0
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 9 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190528171439.076212120@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index d42dd7e443d5..aa0a9f4680a1 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * arch/sh/mm/tlb-sh4.c
  *
@@ -5,8 +6,6 @@
  *
  * Copyright (C) 1999  Niibe Yutaka
  * Copyright (C) 2002 - 2007 Paul Mundt
- *
- * Released under the terms of the GNU GPL v2.0.
  */
 #include <linux/kernel.h>
 #include <linux/mm.h>

commit e839ca528718e68cad32a307dc9aabf01ef3eb05
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:03 2012 +0100

    Disintegrate asm/system.h for SH
    
    Disintegrate asm/system.h for SH.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: linux-sh@vger.kernel.org

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index cfdf7930d294..d42dd7e443d5 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -11,7 +11,6 @@
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/io.h>
-#include <asm/system.h>
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 

commit be97d758e5728099e95fe229866d5c6c900d3092
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Apr 2 16:13:27 2010 +0900

    sh: Fix up the SH-3 build for recent TLB changes.
    
    While the MMUCR.URB and ITLB/UTLB differentiation works fine for all SH-4
    and later TLBs, these features are absent on SH-3. This splits out
    local_flush_tlb_all() in to SH-4 and PTEAEX copies while restoring the
    old SH-3 one, subsequently fixing up the build.
    
    This will probably want some further reordering and tidying in the
    future, but that's out of scope at present.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index ccac77f504a8..cfdf7930d294 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -80,3 +80,31 @@ void local_flush_tlb_one(unsigned long asid, unsigned long page)
 	__raw_writel(data, addr);
 	back_to_cached();
 }
+
+void local_flush_tlb_all(void)
+{
+	unsigned long flags, status;
+	int i;
+
+	/*
+	 * Flush all the TLB.
+	 */
+	local_irq_save(flags);
+	jump_to_uncached();
+
+	status = __raw_readl(MMUCR);
+	status = ((status & MMUCR_URB) >> MMUCR_URB_SHIFT);
+
+	if (status == 0)
+		status = MMUCR_URB_NENTRIES;
+
+	for (i = 0; i < status; i++)
+		__raw_writel(0x0, MMU_UTLB_ADDRESS_ARRAY | (i << 8));
+
+	for (i = 0; i < 4; i++)
+		__raw_writel(0x0, MMU_ITLB_ADDRESS_ARRAY | (i << 8));
+
+	back_to_cached();
+	ctrl_barrier();
+	local_irq_restore(flags);
+}

commit 9d56dd3b083a3bec56e9da35ce07baca81030b03
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jan 26 12:58:40 2010 +0900

    sh: Mass ctrl_in/outX to __raw_read/writeX conversion.
    
    The old ctrl in/out routines are non-portable and unsuitable for
    cross-platform use. While drivers/sh has already been sanitized, there
    is still quite a lot of code that is not. This converts the arch/sh/ bits
    over, which permits us to flag the routines as deprecated whilst still
    building with -Werror for the architecture code, and to ensure that
    future users are not added.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 624c1daa9f3f..ccac77f504a8 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -29,7 +29,7 @@ void __update_tlb(struct vm_area_struct *vma, unsigned long address, pte_t pte)
 
 	/* Set PTEH register */
 	vpn = (address & MMU_VPN_MASK) | get_asid();
-	ctrl_outl(vpn, MMU_PTEH);
+	__raw_writel(vpn, MMU_PTEH);
 
 	pteval = pte.pte_low;
 
@@ -41,13 +41,13 @@ void __update_tlb(struct vm_area_struct *vma, unsigned long address, pte_t pte)
 	 * the protection bits (with the exception of the compat-mode SZ
 	 * and PR bits, which are cleared) being written out in PTEL.
 	 */
-	ctrl_outl(pte.pte_high, MMU_PTEA);
+	__raw_writel(pte.pte_high, MMU_PTEA);
 #else
 	if (cpu_data->flags & CPU_HAS_PTEA) {
 		/* The last 3 bits and the first one of pteval contains
 		 * the PTEA timing control and space attribute bits
 		 */
-		ctrl_outl(copy_ptea_attributes(pteval), MMU_PTEA);
+		__raw_writel(copy_ptea_attributes(pteval), MMU_PTEA);
 	}
 #endif
 
@@ -57,7 +57,7 @@ void __update_tlb(struct vm_area_struct *vma, unsigned long address, pte_t pte)
 	pteval |= _PAGE_WT;
 #endif
 	/* conveniently, we want all the software flags to be 0 anyway */
-	ctrl_outl(pteval, MMU_PTEL);
+	__raw_writel(pteval, MMU_PTEL);
 
 	/* Load the TLB */
 	asm volatile("ldtlb": /* no output */ : /* no input */ : "memory");
@@ -77,6 +77,6 @@ void local_flush_tlb_one(unsigned long asid, unsigned long page)
 	addr = MMU_UTLB_ADDRESS_ARRAY | MMU_PAGE_ASSOC_BIT;
 	data = page | asid; /* VALID bit is off */
 	jump_to_uncached();
-	ctrl_outl(data, addr);
+	__raw_writel(data, addr);
 	back_to_cached();
 }

commit 2dc2f8e0c46864e2a3722c84eaa96513d4cf8b2f
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Jan 21 16:05:25 2010 +0900

    sh: Kill off the special uncached section and fixmap.
    
    Now that cached_to_uncached works as advertized in 32-bit mode and we're
    never going to be able to map < 16MB anyways, there's no need for the
    special uncached section. Kill it off.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 8cf550e2570f..624c1daa9f3f 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -64,8 +64,7 @@ void __update_tlb(struct vm_area_struct *vma, unsigned long address, pte_t pte)
 	local_irq_restore(flags);
 }
 
-void __uses_jump_to_uncached local_flush_tlb_one(unsigned long asid,
-						 unsigned long page)
+void local_flush_tlb_one(unsigned long asid, unsigned long page)
 {
 	unsigned long addr, data;
 

commit bb29c677b366fdf4f6522cd82228a32567aa98c7
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jan 19 15:20:35 2010 +0900

    sh: Split out MMUCR.URB based entry wiring in to shared helper.
    
    Presently this is duplicated between tlb-sh4 and tlb-pteaex. Split the
    helpers out in to a generic tlb-urb that can be used by any parts
    equipped with MMUCR.URB.
    
    At the same time, move the SH-5 code out-of-line, as we require single
    global state for DTLB entry wiring.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 4c6234743318..8cf550e2570f 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -81,69 +81,3 @@ void __uses_jump_to_uncached local_flush_tlb_one(unsigned long asid,
 	ctrl_outl(data, addr);
 	back_to_cached();
 }
-
-/*
- * Load the entry for 'addr' into the TLB and wire the entry.
- */
-void tlb_wire_entry(struct vm_area_struct *vma, unsigned long addr, pte_t pte)
-{
-	unsigned long status, flags;
-	int urb;
-
-	local_irq_save(flags);
-
-	/* Load the entry into the TLB */
-	__update_tlb(vma, addr, pte);
-
-	/* ... and wire it up. */
-	status = ctrl_inl(MMUCR);
-	urb = (status & MMUCR_URB) >> MMUCR_URB_SHIFT;
-	status &= ~MMUCR_URB;
-
-	/*
-	 * Make sure we're not trying to wire the last TLB entry slot.
-	 */
-	BUG_ON(!--urb);
-
-	urb = urb % MMUCR_URB_NENTRIES;
-
-	status |= (urb << MMUCR_URB_SHIFT);
-	ctrl_outl(status, MMUCR);
-	ctrl_barrier();
-
-	local_irq_restore(flags);
-}
-
-/*
- * Unwire the last wired TLB entry.
- *
- * It should also be noted that it is not possible to wire and unwire
- * TLB entries in an arbitrary order. If you wire TLB entry N, followed
- * by entry N+1, you must unwire entry N+1 first, then entry N. In this
- * respect, it works like a stack or LIFO queue.
- */
-void tlb_unwire_entry(void)
-{
-	unsigned long status, flags;
-	int urb;
-
-	local_irq_save(flags);
-
-	status = ctrl_inl(MMUCR);
-	urb = (status & MMUCR_URB) >> MMUCR_URB_SHIFT;
-	status &= ~MMUCR_URB;
-
-	/*
-	 * Make sure we're not trying to unwire a TLB entry when none
-	 * have been wired.
-	 */
-	BUG_ON(urb++ == MMUCR_URB_NENTRIES);
-
-	urb = urb % MMUCR_URB_NENTRIES;
-
-	status |= (urb << MMUCR_URB_SHIFT);
-	ctrl_outl(status, MMUCR);
-	ctrl_barrier();
-
-	local_irq_restore(flags);
-}

commit 8eda55142080f0373b1f0268fe6d6807f193e713
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Nov 17 21:05:31 2009 +0000

    sh: New extended page flag to wire/unwire TLB entries
    
    Provide a new extended page flag, _PAGE_WIRED and an SH4 implementation
    for wiring TLB entries and use it in the fixmap code path so that we can
    wire the fixmap TLB entry.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 8cf550e2570f..4c6234743318 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -81,3 +81,69 @@ void __uses_jump_to_uncached local_flush_tlb_one(unsigned long asid,
 	ctrl_outl(data, addr);
 	back_to_cached();
 }
+
+/*
+ * Load the entry for 'addr' into the TLB and wire the entry.
+ */
+void tlb_wire_entry(struct vm_area_struct *vma, unsigned long addr, pte_t pte)
+{
+	unsigned long status, flags;
+	int urb;
+
+	local_irq_save(flags);
+
+	/* Load the entry into the TLB */
+	__update_tlb(vma, addr, pte);
+
+	/* ... and wire it up. */
+	status = ctrl_inl(MMUCR);
+	urb = (status & MMUCR_URB) >> MMUCR_URB_SHIFT;
+	status &= ~MMUCR_URB;
+
+	/*
+	 * Make sure we're not trying to wire the last TLB entry slot.
+	 */
+	BUG_ON(!--urb);
+
+	urb = urb % MMUCR_URB_NENTRIES;
+
+	status |= (urb << MMUCR_URB_SHIFT);
+	ctrl_outl(status, MMUCR);
+	ctrl_barrier();
+
+	local_irq_restore(flags);
+}
+
+/*
+ * Unwire the last wired TLB entry.
+ *
+ * It should also be noted that it is not possible to wire and unwire
+ * TLB entries in an arbitrary order. If you wire TLB entry N, followed
+ * by entry N+1, you must unwire entry N+1 first, then entry N. In this
+ * respect, it works like a stack or LIFO queue.
+ */
+void tlb_unwire_entry(void)
+{
+	unsigned long status, flags;
+	int urb;
+
+	local_irq_save(flags);
+
+	status = ctrl_inl(MMUCR);
+	urb = (status & MMUCR_URB) >> MMUCR_URB_SHIFT;
+	status &= ~MMUCR_URB;
+
+	/*
+	 * Make sure we're not trying to unwire a TLB entry when none
+	 * have been wired.
+	 */
+	BUG_ON(urb++ == MMUCR_URB_NENTRIES);
+
+	urb = urb % MMUCR_URB_NENTRIES;
+
+	status |= (urb << MMUCR_URB_SHIFT);
+	ctrl_outl(status, MMUCR);
+	ctrl_barrier();
+
+	local_irq_restore(flags);
+}

commit ac6a0cf6716bb46813d0161024c66c2af66e53d1
Merge: e76a0136a3cf ce3f7cb96e67
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Sep 1 13:54:14 2009 +0900

    Merge branch 'master' into sh/smp
    
    Conflicts:
            arch/sh/mm/cache-sh4.c

commit 6503fe4a6508673c15a509ec4ac3ca5979ae9593
Author: Michael Trimarchi <trimarchimichael@yahoo.it>
Date:   Thu Aug 20 13:27:44 2009 +0900

    sh: Better description of SH-4 PTEA register update.
    
    Signed-off-by: Michael Trimarchi <trimarchimichael@yahoo.it>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index f0c7b7397fa6..fd0d11f1a81c 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -61,9 +61,12 @@ void update_mmu_cache(struct vm_area_struct * vma,
 	 */
 	ctrl_outl(pte.pte_high, MMU_PTEA);
 #else
-	if (cpu_data->flags & CPU_HAS_PTEA)
-		/* TODO: make this look less hacky */
-		ctrl_outl(((pteval >> 28) & 0xe) | (pteval & 0x1), MMU_PTEA);
+	if (cpu_data->flags & CPU_HAS_PTEA) {
+		/* The last 3 bits and the first one of pteval contains
+		 * the PTEA timing control and space attribute bits
+		 */
+		ctrl_outl(copy_ptea_attributes(pteval), MMU_PTEA);
+	}
 #endif
 
 	/* Set PTEL register */

commit 3ed6e129390fb872c3b7e05a232e5d380fbdfb48
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 29 22:06:58 2009 +0900

    sh: Handle a NULL vma in __update_tlb() for the fast-path.
    
    The TLB miss fast-path presently calls in to update_mmu_cache() to
    set up the entry, and does so with a NULL vma. Check for vma validity
    in the __update_tlb() ptrace checks.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 81199f1e5945..7d3c63e707a5 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -22,7 +22,7 @@ void __update_tlb(struct vm_area_struct *vma, unsigned long address, pte_t pte)
 	/*
 	 * Handle debugger faulting in for debugee.
 	 */
-	if (current->active_mm != vma->vm_mm)
+	if (vma && current->active_mm != vma->vm_mm)
 		return;
 
 	local_irq_save(flags);

commit 9cef7492696a416663b4edb953a4eade8517ebeb
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 29 00:12:17 2009 +0900

    sh: update_mmu_cache() consolidation.
    
    This splits out a separate __update_cache()/__update_tlb() for
    update_mmu_cache() to wrap in to. This lets us share the common
    __update_cache() bits while keeping special __update_tlb() handling
    broken out.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index cf50082d2435..81199f1e5945 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -15,33 +15,16 @@
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
-void update_mmu_cache(struct vm_area_struct * vma,
-		      unsigned long address, pte_t pte)
+void __update_tlb(struct vm_area_struct *vma, unsigned long address, pte_t pte)
 {
-	unsigned long flags;
-	unsigned long pteval;
-	unsigned long vpn;
-	unsigned long pfn = pte_pfn(pte);
-	struct page *page;
+	unsigned long flags, pteval, vpn;
 
-	/* Ptrace may call this routine. */
-	if (vma && current->active_mm != vma->vm_mm)
+	/*
+	 * Handle debugger faulting in for debugee.
+	 */
+	if (current->active_mm != vma->vm_mm)
 		return;
 
-	page = pfn_to_page(pfn);
-	if (pfn_valid(pfn) && page_mapping(page)) {
-#ifndef CONFIG_SMP
-		int dirty = test_and_clear_bit(PG_dcache_dirty, &page->flags);
-		if (dirty) {
-
-			unsigned long addr = (unsigned long)page_address(page);
-
-			if (pages_do_alias(addr, address & PAGE_MASK))
-				__flush_wback_region((void *)addr, PAGE_SIZE);
-		}
-#endif
-	}
-
 	local_irq_save(flags);
 
 	/* Set PTEH register */

commit 2277ab4a1df50e05bc732fe9488d4e902bb8399a
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 22 19:20:49 2009 +0900

    sh: Migrate from PG_mapped to PG_dcache_dirty.
    
    This inverts the delayed dcache flush a bit to be more in line with other
    platforms. At the same time this also gives us the ability to do some
    more optimizations and cleanup. Now that the update_mmu_cache() callsite
    only tests for the bit, the implementation can gradually be split out and
    made generic, rather than relying on special implementations for each of
    the peculiar CPU types.
    
    SH7705 in 32kB mode and SH-4 still need slightly different handling, but
    this is something that can remain isolated in the varying page copy/clear
    routines. On top of that, SH-X3 is dcache coherent, so there is no need
    to bother with any of these tests in the PTEAEX version of
    update_mmu_cache(), so we kill that off too.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index f0c7b7397fa6..cf50082d2435 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -21,27 +21,26 @@ void update_mmu_cache(struct vm_area_struct * vma,
 	unsigned long flags;
 	unsigned long pteval;
 	unsigned long vpn;
+	unsigned long pfn = pte_pfn(pte);
+	struct page *page;
 
 	/* Ptrace may call this routine. */
 	if (vma && current->active_mm != vma->vm_mm)
 		return;
 
-#ifndef CONFIG_CACHE_OFF
-	{
-		unsigned long pfn = pte_pfn(pte);
+	page = pfn_to_page(pfn);
+	if (pfn_valid(pfn) && page_mapping(page)) {
+#ifndef CONFIG_SMP
+		int dirty = test_and_clear_bit(PG_dcache_dirty, &page->flags);
+		if (dirty) {
 
-		if (pfn_valid(pfn)) {
-			struct page *page = pfn_to_page(pfn);
+			unsigned long addr = (unsigned long)page_address(page);
 
-			if (!test_bit(PG_mapped, &page->flags)) {
-				unsigned long phys = pte_val(pte) & PTE_PHYS_MASK;
-				__flush_wback_region((void *)P1SEGADDR(phys),
-						     PAGE_SIZE);
-				__set_bit(PG_mapped, &page->flags);
-			}
+			if (pages_do_alias(addr, address & PAGE_MASK))
+				__flush_wback_region((void *)addr, PAGE_SIZE);
 		}
-	}
 #endif
+	}
 
 	local_irq_save(flags);
 

commit cbaa118ecfd99fc5ed7adbd9c34a30e1c05e3c93
Author: Stuart Menefy <stuart.menefy@st.com>
Date:   Fri Nov 30 17:06:36 2007 +0900

    sh: Preparation for uncached jumps through PMB.
    
    Presently most of the 29-bit physical parts do P1/P2 segmentation
    with a 1:1 cached/uncached mapping, jumping between the two to
    control the caching behaviour. This provides the basic infrastructure
    to maintain this behaviour on 32-bit physical parts that don't map
    P1/P2 at all, using a shiny new linker section and corresponding
    fixmap entry.
    
    Signed-off-by: Stuart Menefy <stuart.menefy@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 2d1dd6044307..f0c7b7397fa6 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -79,7 +79,8 @@ void update_mmu_cache(struct vm_area_struct * vma,
 	local_irq_restore(flags);
 }
 
-void local_flush_tlb_one(unsigned long asid, unsigned long page)
+void __uses_jump_to_uncached local_flush_tlb_one(unsigned long asid,
+						 unsigned long page)
 {
 	unsigned long addr, data;
 
@@ -91,7 +92,7 @@ void local_flush_tlb_one(unsigned long asid, unsigned long page)
 	 */
 	addr = MMU_UTLB_ADDRESS_ARRAY | MMU_PAGE_ASSOC_BIT;
 	data = page | asid; /* VALID bit is off */
-	jump_to_P2();
+	jump_to_uncached();
 	ctrl_outl(data, addr);
-	back_to_P1();
+	back_to_cached();
 }

commit d04a0f79f502a87bb17b147afc4b3e39e75275c3
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Sep 21 11:55:03 2007 +0900

    sh: Fix up extended mode TLB for SH-X2+ cores.
    
    The extended mode TLB requires both 64-bit PTEs and a 64-bit pgprot,
    correspondingly, the PGD also has to be 64-bits, so fix that up.
    
    The kernel and user permission bits really are decoupled in early
    cuts of the silicon, which means that we also have to set corresponding
    kernel permissions on user pages or we end up with user pages that the
    kernel simply can't touch (!).
    
    Finally, with those things corrected, really enable MMUCR.ME and
    correct the PTEA value (this simply needs to be the upper 32-bits
    of the PTE, with the size and protection bit encoding).
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 13fde8cc7179..2d1dd6044307 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -4,27 +4,14 @@
  * SH-4 specific TLB operations
  *
  * Copyright (C) 1999  Niibe Yutaka
- * Copyright (C) 2002  Paul Mundt
+ * Copyright (C) 2002 - 2007 Paul Mundt
  *
  * Released under the terms of the GNU GPL v2.0.
  */
-#include <linux/signal.h>
-#include <linux/sched.h>
 #include <linux/kernel.h>
-#include <linux/errno.h>
-#include <linux/string.h>
-#include <linux/types.h>
-#include <linux/ptrace.h>
-#include <linux/mman.h>
 #include <linux/mm.h>
-#include <linux/smp.h>
-#include <linux/smp_lock.h>
-#include <linux/interrupt.h>
-
+#include <linux/io.h>
 #include <asm/system.h>
-#include <asm/io.h>
-#include <asm/uaccess.h>
-#include <asm/pgalloc.h>
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
@@ -62,12 +49,22 @@ void update_mmu_cache(struct vm_area_struct * vma,
 	vpn = (address & MMU_VPN_MASK) | get_asid();
 	ctrl_outl(vpn, MMU_PTEH);
 
-	pteval = pte_val(pte);
+	pteval = pte.pte_low;
 
 	/* Set PTEA register */
+#ifdef CONFIG_X2TLB
+	/*
+	 * For the extended mode TLB this is trivial, only the ESZ and
+	 * EPR bits need to be written out to PTEA, with the remainder of
+	 * the protection bits (with the exception of the compat-mode SZ
+	 * and PR bits, which are cleared) being written out in PTEL.
+	 */
+	ctrl_outl(pte.pte_high, MMU_PTEA);
+#else
 	if (cpu_data->flags & CPU_HAS_PTEA)
 		/* TODO: make this look less hacky */
 		ctrl_outl(((pteval >> 28) & 0xe) | (pteval & 0x1), MMU_PTEA);
+#endif
 
 	/* Set PTEL register */
 	pteval &= _PAGE_FLAGS_HARDWARE_MASK; /* drop software flags */
@@ -98,4 +95,3 @@ void local_flush_tlb_one(unsigned long asid, unsigned long page)
 	ctrl_outl(data, addr);
 	back_to_P1();
 }
-

commit e7bd34a15b85655f24d1b45edbe3bdfebf9d027e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jul 31 17:07:28 2007 +0900

    sh: Support explicit L1 cache disabling.
    
    This reworks the cache mode configuration in Kconfig, and allows for
    explicit selection of write-back/write-through/off configurations.
    All of the cache flushing routines are optimized away for the off
    case.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index f74cf667c8fa..13fde8cc7179 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -34,22 +34,27 @@ void update_mmu_cache(struct vm_area_struct * vma,
 	unsigned long flags;
 	unsigned long pteval;
 	unsigned long vpn;
-	struct page *page;
-	unsigned long pfn;
 
 	/* Ptrace may call this routine. */
 	if (vma && current->active_mm != vma->vm_mm)
 		return;
 
-	pfn = pte_pfn(pte);
-	if (pfn_valid(pfn)) {
-		page = pfn_to_page(pfn);
-		if (!test_bit(PG_mapped, &page->flags)) {
-			unsigned long phys = pte_val(pte) & PTE_PHYS_MASK;
-			__flush_wback_region((void *)P1SEGADDR(phys), PAGE_SIZE);
-			__set_bit(PG_mapped, &page->flags);
+#ifndef CONFIG_CACHE_OFF
+	{
+		unsigned long pfn = pte_pfn(pte);
+
+		if (pfn_valid(pfn)) {
+			struct page *page = pfn_to_page(pfn);
+
+			if (!test_bit(PG_mapped, &page->flags)) {
+				unsigned long phys = pte_val(pte) & PTE_PHYS_MASK;
+				__flush_wback_region((void *)P1SEGADDR(phys),
+						     PAGE_SIZE);
+				__set_bit(PG_mapped, &page->flags);
+			}
 		}
 	}
+#endif
 
 	local_irq_save(flags);
 
@@ -66,7 +71,7 @@ void update_mmu_cache(struct vm_area_struct * vma,
 
 	/* Set PTEL register */
 	pteval &= _PAGE_FLAGS_HARDWARE_MASK; /* drop software flags */
-#ifdef CONFIG_SH_WRITETHROUGH
+#ifdef CONFIG_CACHE_WRITETHROUGH
 	pteval |= _PAGE_WT;
 #endif
 	/* conveniently, we want all the software flags to be 0 anyway */

commit 39e688a94b94eaba768b1494e19e96f828fc2688
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Mar 5 19:46:47 2007 +0900

    sh: Revert lazy dcache writeback changes.
    
    These ended up causing too many problems on older parts,
    revert for now..
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 221e7095473d..f74cf667c8fa 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -8,9 +8,74 @@
  *
  * Released under the terms of the GNU GPL v2.0.
  */
-#include <linux/io.h>
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/smp_lock.h>
+#include <linux/interrupt.h>
+
 #include <asm/system.h>
+#include <asm/io.h>
+#include <asm/uaccess.h>
+#include <asm/pgalloc.h>
 #include <asm/mmu_context.h>
+#include <asm/cacheflush.h>
+
+void update_mmu_cache(struct vm_area_struct * vma,
+		      unsigned long address, pte_t pte)
+{
+	unsigned long flags;
+	unsigned long pteval;
+	unsigned long vpn;
+	struct page *page;
+	unsigned long pfn;
+
+	/* Ptrace may call this routine. */
+	if (vma && current->active_mm != vma->vm_mm)
+		return;
+
+	pfn = pte_pfn(pte);
+	if (pfn_valid(pfn)) {
+		page = pfn_to_page(pfn);
+		if (!test_bit(PG_mapped, &page->flags)) {
+			unsigned long phys = pte_val(pte) & PTE_PHYS_MASK;
+			__flush_wback_region((void *)P1SEGADDR(phys), PAGE_SIZE);
+			__set_bit(PG_mapped, &page->flags);
+		}
+	}
+
+	local_irq_save(flags);
+
+	/* Set PTEH register */
+	vpn = (address & MMU_VPN_MASK) | get_asid();
+	ctrl_outl(vpn, MMU_PTEH);
+
+	pteval = pte_val(pte);
+
+	/* Set PTEA register */
+	if (cpu_data->flags & CPU_HAS_PTEA)
+		/* TODO: make this look less hacky */
+		ctrl_outl(((pteval >> 28) & 0xe) | (pteval & 0x1), MMU_PTEA);
+
+	/* Set PTEL register */
+	pteval &= _PAGE_FLAGS_HARDWARE_MASK; /* drop software flags */
+#ifdef CONFIG_SH_WRITETHROUGH
+	pteval |= _PAGE_WT;
+#endif
+	/* conveniently, we want all the software flags to be 0 anyway */
+	ctrl_outl(pteval, MMU_PTEL);
+
+	/* Load the TLB */
+	asm volatile("ldtlb": /* no output */ : /* no input */ : "memory");
+	local_irq_restore(flags);
+}
 
 void local_flush_tlb_one(unsigned long asid, unsigned long page)
 {
@@ -28,3 +93,4 @@ void local_flush_tlb_one(unsigned long asid, unsigned long page)
 	ctrl_outl(data, addr);
 	back_to_P1();
 }
+

commit ea9af69481730e3d712104dfd549ba6c8ddd29f1
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Dec 25 19:28:54 2006 +0900

    sh: Local TLB flushing variants for SMP prep.
    
    Rename the existing flush routines to local_ variants for use by
    the IPI-backed global flush routines on SMP.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 758d8dec622b..221e7095473d 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -12,7 +12,7 @@
 #include <asm/system.h>
 #include <asm/mmu_context.h>
 
-void __flush_tlb_page(unsigned long asid, unsigned long page)
+void local_flush_tlb_one(unsigned long asid, unsigned long page)
 {
 	unsigned long addr, data;
 

commit 26b7a78c55fbc0e23a7dc19e89fd50f200efc002
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Dec 28 10:31:48 2006 +0900

    sh: Lazy dcache writeback optimizations.
    
    This converts the lazy dcache handling to the model described in
    Documentation/cachetlb.txt and drops the ptep_get_and_clear() hacks
    used for the aliasing dcaches on SH-4 and SH7705 in 32kB mode. As a
    bonus, this slightly cuts down on the cache flushing frequency.
    
    With that and the PTEA handling out of the way, the update_mmu_cache()
    implementations can be consolidated, and we no longer have to worry
    about which configuration the cache is in for the SH7705 case.
    
    And finally, explicitly disable the lazy writeback on SMP (SH-4A).
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 812b2d567de2..758d8dec622b 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -8,74 +8,9 @@
  *
  * Released under the terms of the GNU GPL v2.0.
  */
-#include <linux/signal.h>
-#include <linux/sched.h>
-#include <linux/kernel.h>
-#include <linux/errno.h>
-#include <linux/string.h>
-#include <linux/types.h>
-#include <linux/ptrace.h>
-#include <linux/mman.h>
-#include <linux/mm.h>
-#include <linux/smp.h>
-#include <linux/smp_lock.h>
-#include <linux/interrupt.h>
-
+#include <linux/io.h>
 #include <asm/system.h>
-#include <asm/io.h>
-#include <asm/uaccess.h>
-#include <asm/pgalloc.h>
 #include <asm/mmu_context.h>
-#include <asm/cacheflush.h>
-
-void update_mmu_cache(struct vm_area_struct * vma,
-		      unsigned long address, pte_t pte)
-{
-	unsigned long flags;
-	unsigned long pteval;
-	unsigned long vpn;
-	struct page *page;
-	unsigned long pfn;
-
-	/* Ptrace may call this routine. */
-	if (vma && current->active_mm != vma->vm_mm)
-		return;
-
-	pfn = pte_pfn(pte);
-	if (pfn_valid(pfn)) {
-		page = pfn_to_page(pfn);
-		if (!test_bit(PG_mapped, &page->flags)) {
-			unsigned long phys = pte_val(pte) & PTE_PHYS_MASK;
-			__flush_wback_region((void *)P1SEGADDR(phys), PAGE_SIZE);
-			__set_bit(PG_mapped, &page->flags);
-		}
-	}
-
-	local_irq_save(flags);
-
-	/* Set PTEH register */
-	vpn = (address & MMU_VPN_MASK) | get_asid();
-	ctrl_outl(vpn, MMU_PTEH);
-
-	pteval = pte_val(pte);
-
-	/* Set PTEA register */
-	if (cpu_data->flags & CPU_HAS_PTEA)
-		/* TODO: make this look less hacky */
-		ctrl_outl(((pteval >> 28) & 0xe) | (pteval & 0x1), MMU_PTEA);
-
-	/* Set PTEL register */
-	pteval &= _PAGE_FLAGS_HARDWARE_MASK; /* drop software flags */
-#ifdef CONFIG_SH_WRITETHROUGH
-	pteval |= _PAGE_WT;
-#endif
-	/* conveniently, we want all the software flags to be 0 anyway */
-	ctrl_outl(pteval, MMU_PTEL);
-
-	/* Load the TLB */
-	asm volatile("ldtlb": /* no output */ : /* no input */ : "memory");
-	local_irq_restore(flags);
-}
 
 void __flush_tlb_page(unsigned long asid, unsigned long page)
 {
@@ -93,4 +28,3 @@ void __flush_tlb_page(unsigned long asid, unsigned long page)
 	ctrl_outl(data, addr);
 	back_to_P1();
 }
-

commit 749cf486920bf53f16e6a6889d9635a91ffb6c82
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 14:55:41 2006 +0900

    sh: Add flag for MMU PTEA capability.
    
    Add CPU_HAS_PTEA, refactor some of the cpu flag settings.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 96e5fb0ac4fa..812b2d567de2 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -36,9 +36,6 @@ void update_mmu_cache(struct vm_area_struct * vma,
 	unsigned long vpn;
 	struct page *page;
 	unsigned long pfn;
-#ifndef CONFIG_CPU_SUBTYPE_SH7780
-	unsigned long ptea;
-#endif
 
 	/* Ptrace may call this routine. */
 	if (vma && current->active_mm != vma->vm_mm)
@@ -61,12 +58,11 @@ void update_mmu_cache(struct vm_area_struct * vma,
 	ctrl_outl(vpn, MMU_PTEH);
 
 	pteval = pte_val(pte);
-#ifndef CONFIG_CPU_SUBTYPE_SH7780
+
 	/* Set PTEA register */
-	/* TODO: make this look less hacky */
-	ptea = ((pteval >> 28) & 0xe) | (pteval & 0x1);
-	ctrl_outl(ptea, MMU_PTEA);
-#endif
+	if (cpu_data->flags & CPU_HAS_PTEA)
+		/* TODO: make this look less hacky */
+		ctrl_outl(((pteval >> 28) & 0xe) | (pteval & 0x1), MMU_PTEA);
 
 	/* Set PTEL register */
 	pteval &= _PAGE_FLAGS_HARDWARE_MASK; /* drop software flags */

commit 5b19c9081fbd0882c936ec087bf9055a20251dec
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 14:31:40 2006 +0900

    sh: Support for SH7770/SH7780 CPU subtypes.
    
    Merge support for SH7770 and SH7780 SH-4A subtypes.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
index 115b1b6be40b..96e5fb0ac4fa 100644
--- a/arch/sh/mm/tlb-sh4.c
+++ b/arch/sh/mm/tlb-sh4.c
@@ -36,7 +36,9 @@ void update_mmu_cache(struct vm_area_struct * vma,
 	unsigned long vpn;
 	struct page *page;
 	unsigned long pfn;
+#ifndef CONFIG_CPU_SUBTYPE_SH7780
 	unsigned long ptea;
+#endif
 
 	/* Ptrace may call this routine. */
 	if (vma && current->active_mm != vma->vm_mm)
@@ -59,10 +61,12 @@ void update_mmu_cache(struct vm_area_struct * vma,
 	ctrl_outl(vpn, MMU_PTEH);
 
 	pteval = pte_val(pte);
+#ifndef CONFIG_CPU_SUBTYPE_SH7780
 	/* Set PTEA register */
 	/* TODO: make this look less hacky */
 	ptea = ((pteval >> 28) & 0xe) | (pteval & 0x1);
 	ctrl_outl(ptea, MMU_PTEA);
+#endif
 
 	/* Set PTEL register */
 	pteval &= _PAGE_FLAGS_HARDWARE_MASK; /* drop software flags */

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/arch/sh/mm/tlb-sh4.c b/arch/sh/mm/tlb-sh4.c
new file mode 100644
index 000000000000..115b1b6be40b
--- /dev/null
+++ b/arch/sh/mm/tlb-sh4.c
@@ -0,0 +1,96 @@
+/*
+ * arch/sh/mm/tlb-sh4.c
+ *
+ * SH-4 specific TLB operations
+ *
+ * Copyright (C) 1999  Niibe Yutaka
+ * Copyright (C) 2002  Paul Mundt
+ *
+ * Released under the terms of the GNU GPL v2.0.
+ */
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/smp_lock.h>
+#include <linux/interrupt.h>
+
+#include <asm/system.h>
+#include <asm/io.h>
+#include <asm/uaccess.h>
+#include <asm/pgalloc.h>
+#include <asm/mmu_context.h>
+#include <asm/cacheflush.h>
+
+void update_mmu_cache(struct vm_area_struct * vma,
+		      unsigned long address, pte_t pte)
+{
+	unsigned long flags;
+	unsigned long pteval;
+	unsigned long vpn;
+	struct page *page;
+	unsigned long pfn;
+	unsigned long ptea;
+
+	/* Ptrace may call this routine. */
+	if (vma && current->active_mm != vma->vm_mm)
+		return;
+
+	pfn = pte_pfn(pte);
+	if (pfn_valid(pfn)) {
+		page = pfn_to_page(pfn);
+		if (!test_bit(PG_mapped, &page->flags)) {
+			unsigned long phys = pte_val(pte) & PTE_PHYS_MASK;
+			__flush_wback_region((void *)P1SEGADDR(phys), PAGE_SIZE);
+			__set_bit(PG_mapped, &page->flags);
+		}
+	}
+
+	local_irq_save(flags);
+
+	/* Set PTEH register */
+	vpn = (address & MMU_VPN_MASK) | get_asid();
+	ctrl_outl(vpn, MMU_PTEH);
+
+	pteval = pte_val(pte);
+	/* Set PTEA register */
+	/* TODO: make this look less hacky */
+	ptea = ((pteval >> 28) & 0xe) | (pteval & 0x1);
+	ctrl_outl(ptea, MMU_PTEA);
+
+	/* Set PTEL register */
+	pteval &= _PAGE_FLAGS_HARDWARE_MASK; /* drop software flags */
+#ifdef CONFIG_SH_WRITETHROUGH
+	pteval |= _PAGE_WT;
+#endif
+	/* conveniently, we want all the software flags to be 0 anyway */
+	ctrl_outl(pteval, MMU_PTEL);
+
+	/* Load the TLB */
+	asm volatile("ldtlb": /* no output */ : /* no input */ : "memory");
+	local_irq_restore(flags);
+}
+
+void __flush_tlb_page(unsigned long asid, unsigned long page)
+{
+	unsigned long addr, data;
+
+	/*
+	 * NOTE: PTEH.ASID should be set to this MM
+	 *       _AND_ we need to write ASID to the array.
+	 *
+	 * It would be simple if we didn't need to set PTEH.ASID...
+	 */
+	addr = MMU_UTLB_ADDRESS_ARRAY | MMU_PAGE_ASSOC_BIT;
+	data = page | asid; /* VALID bit is off */
+	jump_to_P2();
+	ctrl_outl(data, addr);
+	back_to_P1();
+}
+
