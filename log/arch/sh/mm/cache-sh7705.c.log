commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index ed25eba80667..48978293226c 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -16,7 +16,6 @@
 #include <linux/threads.h>
 #include <asm/addrspace.h>
 #include <asm/page.h>
-#include <asm/pgtable.h>
 #include <asm/processor.h>
 #include <asm/cache.h>
 #include <asm/io.h>

commit cb9f753a3731f7fe16447bea45cb6f8e8bb432fb
Author: Huang Ying <ying.huang@intel.com>
Date:   Thu Apr 5 16:24:39 2018 -0700

    mm: fix races between swapoff and flush dcache
    
    Thanks to commit 4b3ef9daa4fc ("mm/swap: split swap cache into 64MB
    trunks"), after swapoff the address_space associated with the swap
    device will be freed.  So page_mapping() users which may touch the
    address_space need some kind of mechanism to prevent the address_space
    from being freed during accessing.
    
    The dcache flushing functions (flush_dcache_page(), etc) in architecture
    specific code may access the address_space of swap device for anonymous
    pages in swap cache via page_mapping() function.  But in some cases
    there are no mechanisms to prevent the swap device from being swapoff,
    for example,
    
      CPU1                                  CPU2
      __get_user_pages()                    swapoff()
        flush_dcache_page()
          mapping = page_mapping()
            ...                               exit_swap_address_space()
            ...                                 kvfree(spaces)
            mapping_mapped(mapping)
    
    The address space may be accessed after being freed.
    
    But from cachetlb.txt and Russell King, flush_dcache_page() only care
    about file cache pages, for anonymous pages, flush_anon_page() should be
    used.  The implementation of flush_dcache_page() in all architectures
    follows this too.  They will check whether page_mapping() is NULL and
    whether mapping_mapped() is true to determine whether to flush the
    dcache immediately.  And they will use interval tree (mapping->i_mmap)
    to find all user space mappings.  While mapping_mapped() and
    mapping->i_mmap isn't used by anonymous pages in swap cache at all.
    
    So, to fix the race between swapoff and flush dcache, __page_mapping()
    is add to return the address_space for file cache pages and NULL
    otherwise.  All page_mapping() invoking in flush dcache functions are
    replaced with page_mapping_file().
    
    [akpm@linux-foundation.org: simplify page_mapping_file(), per Mike]
    Link: http://lkml.kernel.org/r/20180305083634.15174-1-ying.huang@intel.com
    Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 6cd2aa395817..ed25eba80667 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -136,7 +136,7 @@ static void __flush_dcache_page(unsigned long phys)
 static void sh7705_flush_dcache_page(void *arg)
 {
 	struct page *page = arg;
-	struct address_space *mapping = page_mapping(page);
+	struct address_space *mapping = page_mapping_file(page);
 
 	if (mapping && !mapping_mapped(mapping))
 		clear_bit(PG_dcache_clean, &page->flags);

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 7729cca727eb..6cd2aa395817 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -20,7 +20,7 @@
 #include <asm/processor.h>
 #include <asm/cache.h>
 #include <asm/io.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/pgalloc.h>
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>

commit 55661fc1f105ed75852e937bf8ea408270eb0cca
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Dec 1 15:39:51 2010 +0900

    sh: Assume new page cache pages have dirty dcache lines.
    
    This follows the ARM change c01778001a4f5ad9c62d882776235f3f31922fdd
    ("ARM: 6379/1: Assume new page cache pages have dirty D-cache") for the
    same rationale:
    
        There are places in Linux where writes to newly allocated page
        cache pages happen without a subsequent call to flush_dcache_page()
        (several PIO drivers including USB HCD). This patch changes the
        meaning of PG_arch_1 to be PG_dcache_clean and always flush the
        D-cache for a newly mapped page in update_mmu_cache().
    
    This addresses issues seen with executing binaries from MMC, in
    addition to some of the other HCDs that don't explicitly do cache
    management for their pipe-in buffers.
    
    Requested-by: Yoshihiro Shimoda <yoshihiro.shimoda.uh@renesas.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index f498da1cce7a..7729cca727eb 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -139,7 +139,7 @@ static void sh7705_flush_dcache_page(void *arg)
 	struct address_space *mapping = page_mapping(page);
 
 	if (mapping && !mapping_mapped(mapping))
-		set_bit(PG_dcache_dirty, &page->flags);
+		clear_bit(PG_dcache_clean, &page->flags);
 	else
 		__flush_dcache_page(__pa(page_address(page)));
 }

commit 9d56dd3b083a3bec56e9da35ce07baca81030b03
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jan 26 12:58:40 2010 +0900

    sh: Mass ctrl_in/outX to __raw_read/writeX conversion.
    
    The old ctrl in/out routines are non-portable and unsuitable for
    cross-platform use. While drivers/sh has already been sanitized, there
    is still quite a lot of code that is not. This converts the arch/sh/ bits
    over, which permits us to flag the routines as deprecated whilst still
    building with -Werror for the architecture code, and to ensure that
    future users are not added.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 870293ee539e..f498da1cce7a 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -48,10 +48,10 @@ static inline void cache_wback_all(void)
 			unsigned long data;
 			int v = SH_CACHE_UPDATED | SH_CACHE_VALID;
 
-			data = ctrl_inl(addr);
+			data = __raw_readl(addr);
 
 			if ((data & v) == v)
-				ctrl_outl(data & ~v, addr);
+				__raw_writel(data & ~v, addr);
 
 		}
 
@@ -115,10 +115,10 @@ static void __flush_dcache_page(unsigned long phys)
 		     addr += current_cpu_data.dcache.linesz) {
 			unsigned long data;
 
-			data = ctrl_inl(addr) & (0x1ffffC00 | SH_CACHE_VALID);
+			data = __raw_readl(addr) & (0x1ffffC00 | SH_CACHE_VALID);
 		        if (data == phys) {
 				data &= ~(SH_CACHE_VALID | SH_CACHE_UPDATED);
-				ctrl_outl(data, addr);
+				__raw_writel(data, addr);
 			}
 		}
 

commit 2dc2f8e0c46864e2a3722c84eaa96513d4cf8b2f
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Jan 21 16:05:25 2010 +0900

    sh: Kill off the special uncached section and fixmap.
    
    Now that cached_to_uncached works as advertized in 32-bit mode and we're
    never going to be able to map < 16MB anyways, there's no need for the
    special uncached section. Kill it off.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index f527fb70fce6..870293ee539e 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -78,7 +78,7 @@ static void sh7705_flush_icache_range(void *args)
 /*
  * Writeback&Invalidate the D-cache of the page
  */
-static void __uses_jump_to_uncached __flush_dcache_page(unsigned long phys)
+static void __flush_dcache_page(unsigned long phys)
 {
 	unsigned long ways, waysize, addrstart;
 	unsigned long flags;
@@ -144,7 +144,7 @@ static void sh7705_flush_dcache_page(void *arg)
 		__flush_dcache_page(__pa(page_address(page)));
 }
 
-static void __uses_jump_to_uncached sh7705_flush_cache_all(void *args)
+static void sh7705_flush_cache_all(void *args)
 {
 	unsigned long flags;
 

commit 8bd642b17bea31f8361b61c16c8d154638414df4
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:24 2009 +0000

    sh: Obliterate the P1 area macros
    
    Replace the use of PHYSADDR() with __pa(). PHYSADDR() is based on the
    idea that all addresses in P1SEG are untranslated, so we can access an
    address's physical page as an offset from P1SEG. This doesn't work for
    CONFIG_PMB/CONFIG_PMB_FIXED because pages in P1SEG and P2SEG are used
    for PMB mappings and so can be translated to any physical address.
    
    Likewise, replace a P1SEGADDR() use with virt_to_phys().
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 2601935eb589..f527fb70fce6 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -141,7 +141,7 @@ static void sh7705_flush_dcache_page(void *arg)
 	if (mapping && !mapping_mapped(mapping))
 		set_bit(PG_dcache_dirty, &page->flags);
 	else
-		__flush_dcache_page(PHYSADDR(page_address(page)));
+		__flush_dcache_page(__pa(page_address(page)));
 }
 
 static void __uses_jump_to_uncached sh7705_flush_cache_all(void *args)

commit a6325247f50628c7e53a483807d0ef2c24a7aa90
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:21 2009 +0000

    sh: Sprinkle __uses_jump_to_uncached
    
    Fix some callers of jump_to_uncached() and back_to_cached() that were
    not annotated with __uses_jump_to_uncached.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 2cadee2037ac..2601935eb589 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -78,7 +78,7 @@ static void sh7705_flush_icache_range(void *args)
 /*
  * Writeback&Invalidate the D-cache of the page
  */
-static void __flush_dcache_page(unsigned long phys)
+static void __uses_jump_to_uncached __flush_dcache_page(unsigned long phys)
 {
 	unsigned long ways, waysize, addrstart;
 	unsigned long flags;
@@ -144,7 +144,7 @@ static void sh7705_flush_dcache_page(void *arg)
 		__flush_dcache_page(PHYSADDR(page_address(page)));
 }
 
-static void sh7705_flush_cache_all(void *args)
+static void __uses_jump_to_uncached sh7705_flush_cache_all(void *args)
 {
 	unsigned long flags;
 

commit c8c2df9055074197ba12902c6d7e840667fb56d6
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Sep 15 09:47:35 2009 +0900

    sh: Fix up sh7705 flush_dcache_page() build.
    
    Type mismatch caused the page deref to blow up, fix it up as per the sh4
    change.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 6293f57fa888..2cadee2037ac 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -133,8 +133,9 @@ static void __flush_dcache_page(unsigned long phys)
  * Write back & invalidate the D-cache of the page.
  * (To avoid "alias" issues)
  */
-static void sh7705_flush_dcache_page(void *page)
+static void sh7705_flush_dcache_page(void *arg)
 {
+	struct page *page = arg;
 	struct address_space *mapping = page_mapping(page);
 
 	if (mapping && !mapping_mapped(mapping))

commit 983f4c514c4c9ddac1077a2c805fd16cbe3f7487
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Sep 1 21:12:55 2009 +0900

    Revert "sh: Kill off now redundant local irq disabling."
    
    This reverts commit 64a6d72213dd810dd55bd0a503c36150af41c3c3.
    
    Unfortunately we can't use on_each_cpu() for all of the cache ops, as
    some of them only require preempt disabling. This seems to be the same
    issue that impacts the mips r4k caches, where this code was based on.
    This fixes up a deadlock that showed up in some IRQ context cases.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 9dc38660e3de..6293f57fa888 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -81,6 +81,7 @@ static void sh7705_flush_icache_range(void *args)
 static void __flush_dcache_page(unsigned long phys)
 {
 	unsigned long ways, waysize, addrstart;
+	unsigned long flags;
 
 	phys |= SH_CACHE_VALID;
 
@@ -97,6 +98,7 @@ static void __flush_dcache_page(unsigned long phys)
 	 * potential cache aliasing, therefore the optimisation is probably not
 	 * possible.
 	 */
+	local_irq_save(flags);
 	jump_to_uncached();
 
 	ways = current_cpu_data.dcache.ways;
@@ -124,6 +126,7 @@ static void __flush_dcache_page(unsigned long phys)
 	} while (--ways);
 
 	back_to_cached();
+	local_irq_restore(flags);
 }
 
 /*
@@ -142,9 +145,14 @@ static void sh7705_flush_dcache_page(void *page)
 
 static void sh7705_flush_cache_all(void *args)
 {
+	unsigned long flags;
+
+	local_irq_save(flags);
 	jump_to_uncached();
+
 	cache_wback_all();
 	back_to_cached();
+	local_irq_restore(flags);
 }
 
 /*

commit 64a6d72213dd810dd55bd0a503c36150af41c3c3
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Aug 21 18:21:07 2009 +0900

    sh: Kill off now redundant local irq disabling.
    
    on_each_cpu() takes care of IRQ and preempt handling, the localized
    handling in each of the called functions can be killed off.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 6293f57fa888..9dc38660e3de 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -81,7 +81,6 @@ static void sh7705_flush_icache_range(void *args)
 static void __flush_dcache_page(unsigned long phys)
 {
 	unsigned long ways, waysize, addrstart;
-	unsigned long flags;
 
 	phys |= SH_CACHE_VALID;
 
@@ -98,7 +97,6 @@ static void __flush_dcache_page(unsigned long phys)
 	 * potential cache aliasing, therefore the optimisation is probably not
 	 * possible.
 	 */
-	local_irq_save(flags);
 	jump_to_uncached();
 
 	ways = current_cpu_data.dcache.ways;
@@ -126,7 +124,6 @@ static void __flush_dcache_page(unsigned long phys)
 	} while (--ways);
 
 	back_to_cached();
-	local_irq_restore(flags);
 }
 
 /*
@@ -145,14 +142,9 @@ static void sh7705_flush_dcache_page(void *page)
 
 static void sh7705_flush_cache_all(void *args)
 {
-	unsigned long flags;
-
-	local_irq_save(flags);
 	jump_to_uncached();
-
 	cache_wback_all();
 	back_to_cached();
-	local_irq_restore(flags);
 }
 
 /*

commit f26b2a562b46ab186c8383993ab1332673ac4a47
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Aug 21 17:23:14 2009 +0900

    sh: Make cache flushers SMP-aware.
    
    This does a bit of rework for making the cache flushers SMP-aware. The
    function pointer-based flushers are renamed to local variants with the
    exported interface being commonly implemented and wrapping as necessary.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index f1d5c803c04b..6293f57fa888 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -64,8 +64,14 @@ static inline void cache_wback_all(void)
  *
  * Called from kernel/module.c:sys_init_module and routine for a.out format.
  */
-static void sh7705_flush_icache_range(unsigned long start, unsigned long end)
+static void sh7705_flush_icache_range(void *args)
 {
+	struct flusher_data *data = args;
+	unsigned long start, end;
+
+	start = data->addr1;
+	end = data->addr2;
+
 	__flush_wback_region((void *)start, end - start);
 }
 
@@ -127,7 +133,7 @@ static void __flush_dcache_page(unsigned long phys)
  * Write back & invalidate the D-cache of the page.
  * (To avoid "alias" issues)
  */
-static void sh7705_flush_dcache_page(struct page *page)
+static void sh7705_flush_dcache_page(void *page)
 {
 	struct address_space *mapping = page_mapping(page);
 
@@ -137,7 +143,7 @@ static void sh7705_flush_dcache_page(struct page *page)
 		__flush_dcache_page(PHYSADDR(page_address(page)));
 }
 
-static void sh7705_flush_cache_all(void)
+static void sh7705_flush_cache_all(void *args)
 {
 	unsigned long flags;
 
@@ -149,44 +155,16 @@ static void sh7705_flush_cache_all(void)
 	local_irq_restore(flags);
 }
 
-static void sh7705_flush_cache_mm(struct mm_struct *mm)
-{
-	/* Is there any good way? */
-	/* XXX: possibly call flush_cache_range for each vm area */
-	flush_cache_all();
-}
-
-/*
- * Write back and invalidate D-caches.
- *
- * START, END: Virtual Address (U0 address)
- *
- * NOTE: We need to flush the _physical_ page entry.
- * Flushing the cache lines for U0 only isn't enough.
- * We need to flush for P1 too, which may contain aliases.
- */
-static void sh7705_flush_cache_range(struct vm_area_struct *vma,
-			unsigned long start, unsigned long end)
-{
-
-	/*
-	 * We could call flush_cache_page for the pages of these range,
-	 * but it's not efficient (scan the caches all the time...).
-	 *
-	 * We can't use A-bit magic, as there's the case we don't have
-	 * valid entry on TLB.
-	 */
-	flush_cache_all();
-}
-
 /*
  * Write back and invalidate I/D-caches for the page.
  *
  * ADDRESS: Virtual Address (U0 address)
  */
-static void sh7705_flush_cache_page(struct vm_area_struct *vma,
-		unsigned long address, unsigned long pfn)
+static void sh7705_flush_cache_page(void *args)
 {
+	struct flusher_data *data = args;
+	unsigned long pfn = data->addr2;
+
 	__flush_dcache_page(pfn << PAGE_SHIFT);
 }
 
@@ -198,20 +176,19 @@ static void sh7705_flush_cache_page(struct vm_area_struct *vma,
  * Not entirely sure why this is necessary on SH3 with 32K cache but
  * without it we get occasional "Memory fault" when loading a program.
  */
-static void sh7705_flush_icache_page(struct vm_area_struct *vma,
-				     struct page *page)
+static void sh7705_flush_icache_page(void *page)
 {
 	__flush_purge_region(page_address(page), PAGE_SIZE);
 }
 
 void __init sh7705_cache_init(void)
 {
-	flush_icache_range	= sh7705_flush_icache_range;
-	flush_dcache_page	= sh7705_flush_dcache_page;
-	flush_cache_all		= sh7705_flush_cache_all;
-	flush_cache_mm		= sh7705_flush_cache_mm;
-	flush_cache_dup_mm	= sh7705_flush_cache_mm;
-	flush_cache_range	= sh7705_flush_cache_range;
-	flush_cache_page	= sh7705_flush_cache_page;
-	flush_icache_page	= sh7705_flush_icache_page;
+	local_flush_icache_range	= sh7705_flush_icache_range;
+	local_flush_dcache_page		= sh7705_flush_dcache_page;
+	local_flush_cache_all		= sh7705_flush_cache_all;
+	local_flush_cache_mm		= sh7705_flush_cache_all;
+	local_flush_cache_dup_mm	= sh7705_flush_cache_all;
+	local_flush_cache_range		= sh7705_flush_cache_all;
+	local_flush_cache_page		= sh7705_flush_cache_page;
+	local_flush_icache_page		= sh7705_flush_icache_page;
 }

commit 0d051d90bb08b516b9d6c30d25f83d3c6b5b1c1d
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 12:53:39 2009 +0900

    sh: Convert SH7705 extended mode to new cacheflush interface.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index fa37bff306b9..f1d5c803c04b 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -64,7 +64,7 @@ static inline void cache_wback_all(void)
  *
  * Called from kernel/module.c:sys_init_module and routine for a.out format.
  */
-void flush_icache_range(unsigned long start, unsigned long end)
+static void sh7705_flush_icache_range(unsigned long start, unsigned long end)
 {
 	__flush_wback_region((void *)start, end - start);
 }
@@ -72,7 +72,7 @@ void flush_icache_range(unsigned long start, unsigned long end)
 /*
  * Writeback&Invalidate the D-cache of the page
  */
-static void __uses_jump_to_uncached __flush_dcache_page(unsigned long phys)
+static void __flush_dcache_page(unsigned long phys)
 {
 	unsigned long ways, waysize, addrstart;
 	unsigned long flags;
@@ -127,7 +127,7 @@ static void __uses_jump_to_uncached __flush_dcache_page(unsigned long phys)
  * Write back & invalidate the D-cache of the page.
  * (To avoid "alias" issues)
  */
-void flush_dcache_page(struct page *page)
+static void sh7705_flush_dcache_page(struct page *page)
 {
 	struct address_space *mapping = page_mapping(page);
 
@@ -137,7 +137,7 @@ void flush_dcache_page(struct page *page)
 		__flush_dcache_page(PHYSADDR(page_address(page)));
 }
 
-void __uses_jump_to_uncached flush_cache_all(void)
+static void sh7705_flush_cache_all(void)
 {
 	unsigned long flags;
 
@@ -149,7 +149,7 @@ void __uses_jump_to_uncached flush_cache_all(void)
 	local_irq_restore(flags);
 }
 
-void flush_cache_mm(struct mm_struct *mm)
+static void sh7705_flush_cache_mm(struct mm_struct *mm)
 {
 	/* Is there any good way? */
 	/* XXX: possibly call flush_cache_range for each vm area */
@@ -165,8 +165,8 @@ void flush_cache_mm(struct mm_struct *mm)
  * Flushing the cache lines for U0 only isn't enough.
  * We need to flush for P1 too, which may contain aliases.
  */
-void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
-		       unsigned long end)
+static void sh7705_flush_cache_range(struct vm_area_struct *vma,
+			unsigned long start, unsigned long end)
 {
 
 	/*
@@ -184,8 +184,8 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
  *
  * ADDRESS: Virtual Address (U0 address)
  */
-void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
-		      unsigned long pfn)
+static void sh7705_flush_cache_page(struct vm_area_struct *vma,
+		unsigned long address, unsigned long pfn)
 {
 	__flush_dcache_page(pfn << PAGE_SHIFT);
 }
@@ -198,7 +198,20 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
  * Not entirely sure why this is necessary on SH3 with 32K cache but
  * without it we get occasional "Memory fault" when loading a program.
  */
-void flush_icache_page(struct vm_area_struct *vma, struct page *page)
+static void sh7705_flush_icache_page(struct vm_area_struct *vma,
+				     struct page *page)
 {
 	__flush_purge_region(page_address(page), PAGE_SIZE);
 }
+
+void __init sh7705_cache_init(void)
+{
+	flush_icache_range	= sh7705_flush_icache_range;
+	flush_dcache_page	= sh7705_flush_dcache_page;
+	flush_cache_all		= sh7705_flush_cache_all;
+	flush_cache_mm		= sh7705_flush_cache_mm;
+	flush_cache_dup_mm	= sh7705_flush_cache_mm;
+	flush_cache_range	= sh7705_flush_cache_range;
+	flush_cache_page	= sh7705_flush_cache_page;
+	flush_icache_page	= sh7705_flush_icache_page;
+}

commit 2277ab4a1df50e05bc732fe9488d4e902bb8399a
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jul 22 19:20:49 2009 +0900

    sh: Migrate from PG_mapped to PG_dcache_dirty.
    
    This inverts the delayed dcache flush a bit to be more in line with other
    platforms. At the same time this also gives us the ability to do some
    more optimizations and cleanup. Now that the update_mmu_cache() callsite
    only tests for the bit, the implementation can gradually be split out and
    made generic, rather than relying on special implementations for each of
    the peculiar CPU types.
    
    SH7705 in 32kB mode and SH-4 still need slightly different handling, but
    this is something that can remain isolated in the varying page copy/clear
    routines. On top of that, SH-X3 is dcache coherent, so there is no need
    to bother with any of these tests in the PTEAEX version of
    update_mmu_cache(), so we kill that off too.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 22dacc778823..fa37bff306b9 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -12,6 +12,7 @@
 #include <linux/init.h>
 #include <linux/mman.h>
 #include <linux/mm.h>
+#include <linux/fs.h>
 #include <linux/threads.h>
 #include <asm/addrspace.h>
 #include <asm/page.h>
@@ -128,7 +129,11 @@ static void __uses_jump_to_uncached __flush_dcache_page(unsigned long phys)
  */
 void flush_dcache_page(struct page *page)
 {
-	if (test_bit(PG_mapped, &page->flags))
+	struct address_space *mapping = page_mapping(page);
+
+	if (mapping && !mapping_mapped(mapping))
+		set_bit(PG_dcache_dirty, &page->flags);
+	else
 		__flush_dcache_page(PHYSADDR(page_address(page)));
 }
 

commit cbaa118ecfd99fc5ed7adbd9c34a30e1c05e3c93
Author: Stuart Menefy <stuart.menefy@st.com>
Date:   Fri Nov 30 17:06:36 2007 +0900

    sh: Preparation for uncached jumps through PMB.
    
    Presently most of the 29-bit physical parts do P1/P2 segmentation
    with a 1:1 cached/uncached mapping, jumping between the two to
    control the caching behaviour. This provides the basic infrastructure
    to maintain this behaviour on 32-bit physical parts that don't map
    P1/P2 at all, using a shiny new linker section and corresponding
    fixmap entry.
    
    Signed-off-by: Stuart Menefy <stuart.menefy@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 4896d7376926..22dacc778823 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -71,7 +71,7 @@ void flush_icache_range(unsigned long start, unsigned long end)
 /*
  * Writeback&Invalidate the D-cache of the page
  */
-static void __flush_dcache_page(unsigned long phys)
+static void __uses_jump_to_uncached __flush_dcache_page(unsigned long phys)
 {
 	unsigned long ways, waysize, addrstart;
 	unsigned long flags;
@@ -92,7 +92,7 @@ static void __flush_dcache_page(unsigned long phys)
 	 * possible.
 	 */
 	local_irq_save(flags);
-	jump_to_P2();
+	jump_to_uncached();
 
 	ways = current_cpu_data.dcache.ways;
 	waysize = current_cpu_data.dcache.sets;
@@ -118,7 +118,7 @@ static void __flush_dcache_page(unsigned long phys)
 		addrstart += current_cpu_data.dcache.way_incr;
 	} while (--ways);
 
-	back_to_P1();
+	back_to_cached();
 	local_irq_restore(flags);
 }
 
@@ -132,15 +132,15 @@ void flush_dcache_page(struct page *page)
 		__flush_dcache_page(PHYSADDR(page_address(page)));
 }
 
-void flush_cache_all(void)
+void __uses_jump_to_uncached flush_cache_all(void)
 {
 	unsigned long flags;
 
 	local_irq_save(flags);
-	jump_to_P2();
+	jump_to_uncached();
 
 	cache_wback_all();
-	back_to_P1();
+	back_to_cached();
 	local_irq_restore(flags);
 }
 

commit 39e688a94b94eaba768b1494e19e96f828fc2688
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Mar 5 19:46:47 2007 +0900

    sh: Revert lazy dcache writeback changes.
    
    These ended up causing too many problems on older parts,
    revert for now..
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 31f8deb7a158..4896d7376926 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -3,11 +3,11 @@
  *
  * Copyright (C) 1999, 2000  Niibe Yutaka
  * Copyright (C) 2004  Alex Song
- * Copyright (C) 2006  Paul Mundt
  *
  * This file is subject to the terms and conditions of the GNU General Public
  * License.  See the file "COPYING" in the main directory of this archive
  * for more details.
+ *
  */
 #include <linux/init.h>
 #include <linux/mman.h>
@@ -51,6 +51,7 @@ static inline void cache_wback_all(void)
 
 			if ((data & v) == v)
 				ctrl_outl(data & ~v, addr);
+
 		}
 
 		addrstart += current_cpu_data.dcache.way_incr;
@@ -127,11 +128,7 @@ static void __flush_dcache_page(unsigned long phys)
  */
 void flush_dcache_page(struct page *page)
 {
-	struct address_space *mapping = page_mapping(page);
-
-	if (mapping && !mapping_mapped(mapping))
-		set_bit(PG_dcache_dirty, &page->flags);
-	else
+	if (test_bit(PG_mapped, &page->flags))
 		__flush_dcache_page(PHYSADDR(page_address(page)));
 }
 

commit 11c1965687b0a472add948d4240dfe65a2fcb298
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Dec 25 10:19:56 2006 +0900

    sh: Fixup cpu_data references for the non-boot CPUs.
    
    There are a lot of bogus cpu_data-> references that only end up working
    for the boot CPU, convert these to current_cpu_data to fixup SMP.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 2808b580d984..31f8deb7a158 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -32,9 +32,9 @@ static inline void cache_wback_all(void)
 {
 	unsigned long ways, waysize, addrstart;
 
-	ways = cpu_data->dcache.ways;
-	waysize = cpu_data->dcache.sets;
-	waysize <<= cpu_data->dcache.entry_shift;
+	ways = current_cpu_data.dcache.ways;
+	waysize = current_cpu_data.dcache.sets;
+	waysize <<= current_cpu_data.dcache.entry_shift;
 
 	addrstart = CACHE_OC_ADDRESS_ARRAY;
 
@@ -43,7 +43,7 @@ static inline void cache_wback_all(void)
 
 		for (addr = addrstart;
 		     addr < addrstart + waysize;
-		     addr += cpu_data->dcache.linesz) {
+		     addr += current_cpu_data.dcache.linesz) {
 			unsigned long data;
 			int v = SH_CACHE_UPDATED | SH_CACHE_VALID;
 
@@ -53,7 +53,7 @@ static inline void cache_wback_all(void)
 				ctrl_outl(data & ~v, addr);
 		}
 
-		addrstart += cpu_data->dcache.way_incr;
+		addrstart += current_cpu_data.dcache.way_incr;
 	} while (--ways);
 }
 
@@ -93,9 +93,9 @@ static void __flush_dcache_page(unsigned long phys)
 	local_irq_save(flags);
 	jump_to_P2();
 
-	ways = cpu_data->dcache.ways;
-	waysize = cpu_data->dcache.sets;
-	waysize <<= cpu_data->dcache.entry_shift;
+	ways = current_cpu_data.dcache.ways;
+	waysize = current_cpu_data.dcache.sets;
+	waysize <<= current_cpu_data.dcache.entry_shift;
 
 	addrstart = CACHE_OC_ADDRESS_ARRAY;
 
@@ -104,7 +104,7 @@ static void __flush_dcache_page(unsigned long phys)
 
 		for (addr = addrstart;
 		     addr < addrstart + waysize;
-		     addr += cpu_data->dcache.linesz) {
+		     addr += current_cpu_data.dcache.linesz) {
 			unsigned long data;
 
 			data = ctrl_inl(addr) & (0x1ffffC00 | SH_CACHE_VALID);
@@ -114,7 +114,7 @@ static void __flush_dcache_page(unsigned long phys)
 			}
 		}
 
-		addrstart += cpu_data->dcache.way_incr;
+		addrstart += current_cpu_data.dcache.way_incr;
 	} while (--ways);
 
 	back_to_P1();

commit 26b7a78c55fbc0e23a7dc19e89fd50f200efc002
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Dec 28 10:31:48 2006 +0900

    sh: Lazy dcache writeback optimizations.
    
    This converts the lazy dcache handling to the model described in
    Documentation/cachetlb.txt and drops the ptep_get_and_clear() hacks
    used for the aliasing dcaches on SH-4 and SH7705 in 32kB mode. As a
    bonus, this slightly cuts down on the cache flushing frequency.
    
    With that and the PTEA handling out of the way, the update_mmu_cache()
    implementations can be consolidated, and we no longer have to worry
    about which configuration the cache is in for the SH7705 case.
    
    And finally, explicitly disable the lazy writeback on SMP (SH-4A).
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index 045abdf078f5..2808b580d984 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -3,11 +3,11 @@
  *
  * Copyright (C) 1999, 2000  Niibe Yutaka
  * Copyright (C) 2004  Alex Song
+ * Copyright (C) 2006  Paul Mundt
  *
  * This file is subject to the terms and conditions of the GNU General Public
  * License.  See the file "COPYING" in the main directory of this archive
  * for more details.
- *
  */
 #include <linux/init.h>
 #include <linux/mman.h>
@@ -51,7 +51,6 @@ static inline void cache_wback_all(void)
 
 			if ((data & v) == v)
 				ctrl_outl(data & ~v, addr);
-
 		}
 
 		addrstart += cpu_data->dcache.way_incr;
@@ -128,7 +127,11 @@ static void __flush_dcache_page(unsigned long phys)
  */
 void flush_dcache_page(struct page *page)
 {
-	if (test_bit(PG_mapped, &page->flags))
+	struct address_space *mapping = page_mapping(page);
+
+	if (mapping && !mapping_mapped(mapping))
+		set_bit(PG_dcache_dirty, &page->flags);
+	else
 		__flush_dcache_page(PHYSADDR(page_address(page)));
 }
 

commit 0f08f338083cc1d68788ccbccc44bd0502fc57ae
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 17:03:56 2006 +0900

    sh: More cosmetic cleanups and trivial fixes.
    
    Nothing exciting here, just trivial fixes..
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index bf94eedb0a8e..045abdf078f5 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -9,7 +9,6 @@
  * for more details.
  *
  */
-
 #include <linux/init.h>
 #include <linux/mman.h>
 #include <linux/mm.h>
@@ -25,14 +24,10 @@
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
-/* The 32KB cache on the SH7705 suffers from the same synonym problem
- * as SH4 CPUs */
-
-#define __pte_offset(address) \
-		((address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
-#define pte_offset(dir, address) ((pte_t *) pmd_page_vaddr(*(dir)) + \
-		__pte_offset(address))
-
+/*
+ * The 32KB cache on the SH7705 suffers from the same synonym problem
+ * as SH4 CPUs
+ */
 static inline void cache_wback_all(void)
 {
 	unsigned long ways, waysize, addrstart;
@@ -73,7 +68,6 @@ void flush_icache_range(unsigned long start, unsigned long end)
 	__flush_wback_region((void *)start, end - start);
 }
 
-
 /*
  * Writeback&Invalidate the D-cache of the page
  */
@@ -128,7 +122,6 @@ static void __flush_dcache_page(unsigned long phys)
 	local_irq_restore(flags);
 }
 
-
 /*
  * Write back & invalidate the D-cache of the page.
  * (To avoid "alias" issues)
@@ -186,7 +179,8 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
  *
  * ADDRESS: Virtual Address (U0 address)
  */
-void flush_cache_page(struct vm_area_struct *vma, unsigned long address, unsigned long pfn)
+void flush_cache_page(struct vm_area_struct *vma, unsigned long address,
+		      unsigned long pfn)
 {
 	__flush_dcache_page(pfn << PAGE_SHIFT);
 }
@@ -203,4 +197,3 @@ void flush_icache_page(struct vm_area_struct *vma, struct page *page)
 {
 	__flush_purge_region(page_address(page), PAGE_SIZE);
 }
-

commit 46a82b2d5591335277ed2930611f6acb4ce654ed
Author: Dave McCracken <dmccr@us.ibm.com>
Date:   Mon Sep 25 23:31:48 2006 -0700

    [PATCH] Standardize pxx_page macros
    
    One of the changes necessary for shared page tables is to standardize the
    pxx_page macros.  pte_page and pmd_page have always returned the struct
    page associated with their entry, while pte_page_kernel and pmd_page_kernel
    have returned the kernel virtual address.  pud_page and pgd_page, on the
    other hand, return the kernel virtual address.
    
    Shared page tables needs pud_page and pgd_page to return the actual page
    structures.  There are very few actual users of these functions, so it is
    simple to standardize their usage.
    
    Since this is basic cleanup, I am submitting these changes as a standalone
    patch.  Per Hugh Dickins' comments about it, I am also changing the
    pxx_page_kernel macros to pxx_page_vaddr to clarify their meaning.
    
    Signed-off-by: Dave McCracken <dmccr@us.ibm.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
index ad8ed7d41e16..bf94eedb0a8e 100644
--- a/arch/sh/mm/cache-sh7705.c
+++ b/arch/sh/mm/cache-sh7705.c
@@ -30,7 +30,7 @@
 
 #define __pte_offset(address) \
 		((address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
-#define pte_offset(dir, address) ((pte_t *) pmd_page_kernel(*(dir)) + \
+#define pte_offset(dir, address) ((pte_t *) pmd_page_vaddr(*(dir)) + \
 		__pte_offset(address))
 
 static inline void cache_wback_all(void)

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c
new file mode 100644
index 000000000000..ad8ed7d41e16
--- /dev/null
+++ b/arch/sh/mm/cache-sh7705.c
@@ -0,0 +1,206 @@
+/*
+ * arch/sh/mm/cache-sh7705.c
+ *
+ * Copyright (C) 1999, 2000  Niibe Yutaka
+ * Copyright (C) 2004  Alex Song
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/threads.h>
+#include <asm/addrspace.h>
+#include <asm/page.h>
+#include <asm/pgtable.h>
+#include <asm/processor.h>
+#include <asm/cache.h>
+#include <asm/io.h>
+#include <asm/uaccess.h>
+#include <asm/pgalloc.h>
+#include <asm/mmu_context.h>
+#include <asm/cacheflush.h>
+
+/* The 32KB cache on the SH7705 suffers from the same synonym problem
+ * as SH4 CPUs */
+
+#define __pte_offset(address) \
+		((address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
+#define pte_offset(dir, address) ((pte_t *) pmd_page_kernel(*(dir)) + \
+		__pte_offset(address))
+
+static inline void cache_wback_all(void)
+{
+	unsigned long ways, waysize, addrstart;
+
+	ways = cpu_data->dcache.ways;
+	waysize = cpu_data->dcache.sets;
+	waysize <<= cpu_data->dcache.entry_shift;
+
+	addrstart = CACHE_OC_ADDRESS_ARRAY;
+
+	do {
+		unsigned long addr;
+
+		for (addr = addrstart;
+		     addr < addrstart + waysize;
+		     addr += cpu_data->dcache.linesz) {
+			unsigned long data;
+			int v = SH_CACHE_UPDATED | SH_CACHE_VALID;
+
+			data = ctrl_inl(addr);
+
+			if ((data & v) == v)
+				ctrl_outl(data & ~v, addr);
+
+		}
+
+		addrstart += cpu_data->dcache.way_incr;
+	} while (--ways);
+}
+
+/*
+ * Write back the range of D-cache, and purge the I-cache.
+ *
+ * Called from kernel/module.c:sys_init_module and routine for a.out format.
+ */
+void flush_icache_range(unsigned long start, unsigned long end)
+{
+	__flush_wback_region((void *)start, end - start);
+}
+
+
+/*
+ * Writeback&Invalidate the D-cache of the page
+ */
+static void __flush_dcache_page(unsigned long phys)
+{
+	unsigned long ways, waysize, addrstart;
+	unsigned long flags;
+
+	phys |= SH_CACHE_VALID;
+
+	/*
+	 * Here, phys is the physical address of the page. We check all the
+	 * tags in the cache for those with the same page number as this page
+	 * (by masking off the lowest 2 bits of the 19-bit tag; these bits are
+	 * derived from the offset within in the 4k page). Matching valid
+	 * entries are invalidated.
+	 *
+	 * Since 2 bits of the cache index are derived from the virtual page
+	 * number, knowing this would reduce the number of cache entries to be
+	 * searched by a factor of 4. However this function exists to deal with
+	 * potential cache aliasing, therefore the optimisation is probably not
+	 * possible.
+	 */
+	local_irq_save(flags);
+	jump_to_P2();
+
+	ways = cpu_data->dcache.ways;
+	waysize = cpu_data->dcache.sets;
+	waysize <<= cpu_data->dcache.entry_shift;
+
+	addrstart = CACHE_OC_ADDRESS_ARRAY;
+
+	do {
+		unsigned long addr;
+
+		for (addr = addrstart;
+		     addr < addrstart + waysize;
+		     addr += cpu_data->dcache.linesz) {
+			unsigned long data;
+
+			data = ctrl_inl(addr) & (0x1ffffC00 | SH_CACHE_VALID);
+		        if (data == phys) {
+				data &= ~(SH_CACHE_VALID | SH_CACHE_UPDATED);
+				ctrl_outl(data, addr);
+			}
+		}
+
+		addrstart += cpu_data->dcache.way_incr;
+	} while (--ways);
+
+	back_to_P1();
+	local_irq_restore(flags);
+}
+
+
+/*
+ * Write back & invalidate the D-cache of the page.
+ * (To avoid "alias" issues)
+ */
+void flush_dcache_page(struct page *page)
+{
+	if (test_bit(PG_mapped, &page->flags))
+		__flush_dcache_page(PHYSADDR(page_address(page)));
+}
+
+void flush_cache_all(void)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	jump_to_P2();
+
+	cache_wback_all();
+	back_to_P1();
+	local_irq_restore(flags);
+}
+
+void flush_cache_mm(struct mm_struct *mm)
+{
+	/* Is there any good way? */
+	/* XXX: possibly call flush_cache_range for each vm area */
+	flush_cache_all();
+}
+
+/*
+ * Write back and invalidate D-caches.
+ *
+ * START, END: Virtual Address (U0 address)
+ *
+ * NOTE: We need to flush the _physical_ page entry.
+ * Flushing the cache lines for U0 only isn't enough.
+ * We need to flush for P1 too, which may contain aliases.
+ */
+void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
+		       unsigned long end)
+{
+
+	/*
+	 * We could call flush_cache_page for the pages of these range,
+	 * but it's not efficient (scan the caches all the time...).
+	 *
+	 * We can't use A-bit magic, as there's the case we don't have
+	 * valid entry on TLB.
+	 */
+	flush_cache_all();
+}
+
+/*
+ * Write back and invalidate I/D-caches for the page.
+ *
+ * ADDRESS: Virtual Address (U0 address)
+ */
+void flush_cache_page(struct vm_area_struct *vma, unsigned long address, unsigned long pfn)
+{
+	__flush_dcache_page(pfn << PAGE_SHIFT);
+}
+
+/*
+ * This is called when a page-cache page is about to be mapped into a
+ * user process' address space.  It offers an opportunity for a
+ * port to ensure d-cache/i-cache coherency if necessary.
+ *
+ * Not entirely sure why this is necessary on SH3 with 32K cache but
+ * without it we get occasional "Memory fault" when loading a program.
+ */
+void flush_icache_page(struct vm_area_struct *vma, struct page *page)
+{
+	__flush_purge_region(page_address(page), PAGE_SIZE);
+}
+
