commit 37744feebc086908fd89760650f458ab19071750
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Apr 20 11:37:12 2020 +0200

    sh: remove sh5 support
    
    sh5 never became a product and has probably never really worked.
    
    Remove it by recursively deleting all associated Kconfig options
    and all corresponding files.
    
    Reviewed-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Rich Felker <dalias@libc.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 464f160a9576..3aef78ceb820 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -355,12 +355,6 @@ void __init cpu_cache_init(void)
 		}
 	}
 
-	if (boot_cpu_data.family == CPU_FAMILY_SH5) {
-		extern void __weak sh5_cache_init(void);
-
-		sh5_cache_init();
-	}
-
 skip:
 	emit_cache_params();
 }

commit c456cfc2e52bff3540614ac85e0a1da95248f637
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 28 10:10:14 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 211
    
    Based on 1 normalized pattern(s):
    
      released under the terms of the gnu gpl v2 0
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 9 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190528171439.076212120@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 36554a9ea99b..464f160a9576 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -1,10 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * arch/sh/mm/cache.c
  *
  * Copyright (C) 1999, 2000, 2002  Niibe Yutaka
  * Copyright (C) 2002 - 2010  Paul Mundt
- *
- * Released under the terms of the GNU GPL v2.0.
  */
 #include <linux/mm.h>
 #include <linux/init.h>

commit a1e262f6f126466f51d6955fb5bd6aaf0aacf68f
Author: Rich Felker <dalias@libc.org>
Date:   Mon Feb 15 18:36:33 2016 +0000

    sh: do not perform IPI-based cache flush except on boards that need it
    
    Signed-off-by: Rich Felker <dalias@libc.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 70cc52f2fab8..36554a9ea99b 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -42,6 +42,8 @@ static inline void cacheop_on_each_cpu(void (*func) (void *info), void *info,
 {
 	preempt_disable();
 
+	/* Needing IPI for cross-core flush is SHX3-specific. */
+#ifdef CONFIG_CPU_SHX3
 	/*
 	 * It's possible that this gets called early on when IRQs are
 	 * still disabled due to ioremapping by the boot CPU, so don't
@@ -49,6 +51,7 @@ static inline void cacheop_on_each_cpu(void (*func) (void *info), void *info,
 	 */
 	if (num_online_cpus() > 1)
 		smp_call_function(func, info, wait);
+#endif
 
 	func(info);
 

commit 5a846abad07f6f30adfa3e46c5c7a47d2e7b1e63
Author: Rich Felker <dalias@libc.org>
Date:   Thu Mar 17 23:09:37 2016 +0000

    sh: add support for J-Core J2 processor
    
    At the CPU/ISA level, the J2 is compatible with SH-2, and thus the
    changes to add J2 support build on existing SH-2 support. However, J2
    does not duplicate the memory-mapped SH-2 features like the cache
    interface. Instead, the cache interfaces is described in the device
    tree, and new code is added to be able to access the flat device tree
    at early boot before it is unflattened.
    
    Support is also added for receiving interrupts on trap numbers in the
    range 16 to 31, since the J-Core aic1 interrupt controller generates
    these traps. This range was unused but nominally for hardware
    exceptions on SH-2, and a few values in this range were used for
    exceptions on SH-2A, but SH-2A has its own version of the relevant
    code.
    
    No individual cpu subtypes are added for J2 since the intent moving
    forward is to represent SoCs with device tree rather than as
    hard-coded subtypes in the kernel. The CPU_SUBTYPE_J2 Kconfig item
    exists only to fit into the existing cpu selection mechanism until it
    is overhauled.
    
    Signed-off-by: Rich Felker <dalias@libc.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 776d664a40c5..70cc52f2fab8 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -309,7 +309,11 @@ void __init cpu_cache_init(void)
 	if (unlikely(cache_disabled))
 		goto skip;
 
-	if (boot_cpu_data.family == CPU_FAMILY_SH2) {
+	if (boot_cpu_data.type == CPU_J2) {
+		extern void __weak j2_cache_init(void);
+
+		j2_cache_init();
+	} else if (boot_cpu_data.family == CPU_FAMILY_SH2) {
 		extern void __weak sh2_cache_init(void);
 
 		sh2_cache_init();

commit 57155c6523074dd937b8feafcfaa98c82218faa6
Author: Rich Felker <dalias@libc.org>
Date:   Tue Mar 22 22:02:23 2016 +0000

    sh: disable aliased page logic on NOMMU models
    
    SH3/4 (with MMU) have a virtually indexed cache, requiring explicit
    work to avoid consistency problems arising from having the same
    physical address range cached in multiple cache lines. This is
    unneeded for the NOMMU case, and some of the resulting code paths
    (kmap_coherent) don't work. SH2 only avoided this problem by having a
    4-way associative cache with way size equal to the page size (4k),
    yielding no cache index bits outside of the page offset and thus no
    aliases.
    
    Signed-off-by: Rich Felker <dalias@libc.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index e58cfbf45150..776d664a40c5 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -244,7 +244,11 @@ void flush_cache_sigtramp(unsigned long address)
 
 static void compute_alias(struct cache_info *c)
 {
+#ifdef CONFIG_MMU
 	c->alias_mask = ((c->sets - 1) << c->entry_shift) & ~(PAGE_SIZE - 1);
+#else
+	c->alias_mask = 0;
+#endif
 	c->n_aliases = c->alias_mask ? (c->alias_mask >> PAGE_SHIFT) + 1 : 0;
 }
 

commit e1534ae95004d6a307839a44eed40389d608c935
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Jan 15 16:53:46 2016 -0800

    mm: differentiate page_mapped() from page_mapcount() for compound pages
    
    Let's define page_mapped() to be true for compound pages if any
    sub-pages of the compound page is mapped (with PMD or PTE).
    
    On other hand page_mapcount() return mapcount for this particular small
    page.
    
    This will make cases like page_get_anon_vma() behave correctly once we
    allow huge pages to be mapped with PTE.
    
    Most users outside core-mm should use page_mapcount() instead of
    page_mapped().
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Tested-by: Sasha Levin <sasha.levin@oracle.com>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Jerome Marchand <jmarchan@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index f770e3992620..e58cfbf45150 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -59,7 +59,7 @@ void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
 		       unsigned long vaddr, void *dst, const void *src,
 		       unsigned long len)
 {
-	if (boot_cpu_data.dcache.n_aliases && page_mapped(page) &&
+	if (boot_cpu_data.dcache.n_aliases && page_mapcount(page) &&
 	    test_bit(PG_dcache_clean, &page->flags)) {
 		void *vto = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(vto, src, len);
@@ -78,7 +78,7 @@ void copy_from_user_page(struct vm_area_struct *vma, struct page *page,
 			 unsigned long vaddr, void *dst, const void *src,
 			 unsigned long len)
 {
-	if (boot_cpu_data.dcache.n_aliases && page_mapped(page) &&
+	if (boot_cpu_data.dcache.n_aliases && page_mapcount(page) &&
 	    test_bit(PG_dcache_clean, &page->flags)) {
 		void *vfrom = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(dst, vfrom, len);
@@ -97,7 +97,7 @@ void copy_user_highpage(struct page *to, struct page *from,
 
 	vto = kmap_atomic(to);
 
-	if (boot_cpu_data.dcache.n_aliases && page_mapped(from) &&
+	if (boot_cpu_data.dcache.n_aliases && page_mapcount(from) &&
 	    test_bit(PG_dcache_clean, &from->flags)) {
 		vfrom = kmap_coherent(from, vaddr);
 		copy_page(vto, vfrom);
@@ -153,7 +153,7 @@ void __flush_anon_page(struct page *page, unsigned long vmaddr)
 	unsigned long addr = (unsigned long) page_address(page);
 
 	if (pages_do_alias(addr, vmaddr)) {
-		if (boot_cpu_data.dcache.n_aliases && page_mapped(page) &&
+		if (boot_cpu_data.dcache.n_aliases && page_mapcount(page) &&
 		    test_bit(PG_dcache_clean, &page->flags)) {
 			void *kaddr;
 

commit e3560305192cd51b3c07206c85eb4231594dd58b
Author: Pranith Kumar <bobby.prani@gmail.com>
Date:   Fri Aug 29 15:19:09 2014 -0700

    flush_icache_range: export symbol to fix build errors
    
    Fix building errors occuring due to a missing export of
    flush_icache_range() in
    
    kisskb.ellerman.id.au/kisskb/buildresult/11677809/
    
    ERROR: "flush_icache_range" [drivers/misc/lkdtm.ko] undefined!
    
    Signed-off-by: Pranith Kumar <bobby.prani@gmail.com>
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>    [arc]
    Acked-by: Richard Kuo <rkuo@codeaurora.org>     [hexagon]
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Chris Zankel <chris@zankel.net>
    Acked-by: Max Filippov <jcmvbkbc@gmail.com>     [xtensa]
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Acked-by: Zhigang Lu <zlu@tilera.com>           [tile]
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 097c2cdd117f..f770e3992620 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -229,6 +229,7 @@ void flush_icache_range(unsigned long start, unsigned long end)
 
 	cacheop_on_each_cpu(local_flush_icache_range, (void *)&data, 1);
 }
+EXPORT_SYMBOL(flush_icache_range);
 
 void flush_icache_page(struct vm_area_struct *vma, struct page *page)
 {

commit a5f6ea29f9a918403629f8369ae55fac6b09cb53
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Mon Mar 3 15:38:33 2014 -0800

    sh: prefix sh-specific "CCR" and "CCR2" by "SH_"
    
    Commit bcf24e1daa94 ("mmc: omap_hsmmc: use the generic config for
    omap2plus devices"), enabled the build for other platforms for compile
    testing.
    
    sh-allmodconfig now fails with:
    
        include/linux/omap-dma.h:171:8: error: expected identifier before numeric constant
        make[4]: *** [drivers/mmc/host/omap_hsmmc.o] Error 1
    
    This happens because SuperH #defines "CCR", which is one of the enum
    values in include/linux/omap-dma.h.  There's a similar issue with "CCR2"
    on sh2a.
    
    As "CCR" and "CCR2" are too generic names for global #defines, prefix
    them with "SH_" to fix this.
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 616966a96cba..097c2cdd117f 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -285,8 +285,8 @@ void __init cpu_cache_init(void)
 {
 	unsigned int cache_disabled = 0;
 
-#ifdef CCR
-	cache_disabled = !(__raw_readl(CCR) & CCR_CACHE_ENABLE);
+#ifdef SH_CCR
+	cache_disabled = !(__raw_readl(SH_CCR) & CCR_CACHE_ENABLE);
 #endif
 
 	compute_alias(&boot_cpu_data.icache);

commit bc3e11be88010e09692ed1d214407d56caa90075
Author: Cong Wang <amwang@redhat.com>
Date:   Fri Nov 25 23:14:16 2011 +0800

    sh: remove the second argument of k[un]map_atomic()
    
    Signed-off-by: Cong Wang <amwang@redhat.com>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 5a580ea04429..616966a96cba 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -95,7 +95,7 @@ void copy_user_highpage(struct page *to, struct page *from,
 {
 	void *vfrom, *vto;
 
-	vto = kmap_atomic(to, KM_USER1);
+	vto = kmap_atomic(to);
 
 	if (boot_cpu_data.dcache.n_aliases && page_mapped(from) &&
 	    test_bit(PG_dcache_clean, &from->flags)) {
@@ -103,16 +103,16 @@ void copy_user_highpage(struct page *to, struct page *from,
 		copy_page(vto, vfrom);
 		kunmap_coherent(vfrom);
 	} else {
-		vfrom = kmap_atomic(from, KM_USER0);
+		vfrom = kmap_atomic(from);
 		copy_page(vto, vfrom);
-		kunmap_atomic(vfrom, KM_USER0);
+		kunmap_atomic(vfrom);
 	}
 
 	if (pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK) ||
 	    (vma->vm_flags & VM_EXEC))
 		__flush_purge_region(vto, PAGE_SIZE);
 
-	kunmap_atomic(vto, KM_USER1);
+	kunmap_atomic(vto);
 	/* Make sure this page is cleared on other CPU's too before using it */
 	smp_wmb();
 }
@@ -120,14 +120,14 @@ EXPORT_SYMBOL(copy_user_highpage);
 
 void clear_user_highpage(struct page *page, unsigned long vaddr)
 {
-	void *kaddr = kmap_atomic(page, KM_USER0);
+	void *kaddr = kmap_atomic(page);
 
 	clear_page(kaddr);
 
 	if (pages_do_alias((unsigned long)kaddr, vaddr & PAGE_MASK))
 		__flush_purge_region(kaddr, PAGE_SIZE);
 
-	kunmap_atomic(kaddr, KM_USER0);
+	kunmap_atomic(kaddr);
 }
 EXPORT_SYMBOL(clear_user_highpage);
 

commit a25bbe12224e649fe12cba7a2fa920180a35c8a9
Author: Stuart Menefy <stuart.menefy@st.com>
Date:   Mon Jan 31 17:50:29 2011 +0000

    sh: Flush executable pages in copy_user_highpage
    
    This resolves a problem seen when using the Android dynamic linker.
    Sometimes the dynamic linker would seg-fault at start up and this
    was eventually traced to the handling of a COW fault for a page which
    was being modified by the linker. If there was no cache aliasing between
    the kernel and the user page, the page was not flushed, leaving the
    newly copied data in the D-cache. However when executing instructions
    from that page, the I-cache is filled directly from external memory,
    rather than the D-cache, and causing garbage to be executed.
    
    Signed-off-by: Stuart Menefy <stuart.menefy@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 88d3dc3d30d5..5a580ea04429 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -108,7 +108,8 @@ void copy_user_highpage(struct page *to, struct page *from,
 		kunmap_atomic(vfrom, KM_USER0);
 	}
 
-	if (pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK))
+	if (pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK) ||
+	    (vma->vm_flags & VM_EXEC))
 		__flush_purge_region(vto, PAGE_SIZE);
 
 	kunmap_atomic(vto, KM_USER1);

commit 55661fc1f105ed75852e937bf8ea408270eb0cca
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Dec 1 15:39:51 2010 +0900

    sh: Assume new page cache pages have dirty dcache lines.
    
    This follows the ARM change c01778001a4f5ad9c62d882776235f3f31922fdd
    ("ARM: 6379/1: Assume new page cache pages have dirty D-cache") for the
    same rationale:
    
        There are places in Linux where writes to newly allocated page
        cache pages happen without a subsequent call to flush_dcache_page()
        (several PIO drivers including USB HCD). This patch changes the
        meaning of PG_arch_1 to be PG_dcache_clean and always flush the
        D-cache for a newly mapped page in update_mmu_cache().
    
    This addresses issues seen with executing binaries from MMC, in
    addition to some of the other HCDs that don't explicitly do cache
    management for their pipe-in buffers.
    
    Requested-by: Yoshihiro Shimoda <yoshihiro.shimoda.uh@renesas.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index ba401d137bb9..88d3dc3d30d5 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -60,14 +60,14 @@ void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
 		       unsigned long len)
 {
 	if (boot_cpu_data.dcache.n_aliases && page_mapped(page) &&
-	    !test_bit(PG_dcache_dirty, &page->flags)) {
+	    test_bit(PG_dcache_clean, &page->flags)) {
 		void *vto = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(vto, src, len);
 		kunmap_coherent(vto);
 	} else {
 		memcpy(dst, src, len);
 		if (boot_cpu_data.dcache.n_aliases)
-			set_bit(PG_dcache_dirty, &page->flags);
+			clear_bit(PG_dcache_clean, &page->flags);
 	}
 
 	if (vma->vm_flags & VM_EXEC)
@@ -79,14 +79,14 @@ void copy_from_user_page(struct vm_area_struct *vma, struct page *page,
 			 unsigned long len)
 {
 	if (boot_cpu_data.dcache.n_aliases && page_mapped(page) &&
-	    !test_bit(PG_dcache_dirty, &page->flags)) {
+	    test_bit(PG_dcache_clean, &page->flags)) {
 		void *vfrom = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(dst, vfrom, len);
 		kunmap_coherent(vfrom);
 	} else {
 		memcpy(dst, src, len);
 		if (boot_cpu_data.dcache.n_aliases)
-			set_bit(PG_dcache_dirty, &page->flags);
+			clear_bit(PG_dcache_clean, &page->flags);
 	}
 }
 
@@ -98,7 +98,7 @@ void copy_user_highpage(struct page *to, struct page *from,
 	vto = kmap_atomic(to, KM_USER1);
 
 	if (boot_cpu_data.dcache.n_aliases && page_mapped(from) &&
-	    !test_bit(PG_dcache_dirty, &from->flags)) {
+	    test_bit(PG_dcache_clean, &from->flags)) {
 		vfrom = kmap_coherent(from, vaddr);
 		copy_page(vto, vfrom);
 		kunmap_coherent(vfrom);
@@ -141,7 +141,7 @@ void __update_cache(struct vm_area_struct *vma,
 
 	page = pfn_to_page(pfn);
 	if (pfn_valid(pfn)) {
-		int dirty = test_and_clear_bit(PG_dcache_dirty, &page->flags);
+		int dirty = !test_and_set_bit(PG_dcache_clean, &page->flags);
 		if (dirty)
 			__flush_purge_region(page_address(page), PAGE_SIZE);
 	}
@@ -153,7 +153,7 @@ void __flush_anon_page(struct page *page, unsigned long vmaddr)
 
 	if (pages_do_alias(addr, vmaddr)) {
 		if (boot_cpu_data.dcache.n_aliases && page_mapped(page) &&
-		    !test_bit(PG_dcache_dirty, &page->flags)) {
+		    test_bit(PG_dcache_clean, &page->flags)) {
 			void *kaddr;
 
 			kaddr = kmap_coherent(page, vmaddr);

commit 3cf6fa1e334a3a4af702f92229690195018b747f
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Apr 19 17:27:17 2010 +0900

    sh: Enable SH-X3 hardware synonym avoidance handling.
    
    This enables support for the hardware synonym avoidance handling on SH-X3
    CPUs for the case where dcache aliases are possible. icache handling is
    retained, but we flip on broadcasting of the block invalidations due to
    the lack of coherency otherwise on SMP.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 0f4095d7ac8b..ba401d137bb9 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -334,6 +334,13 @@ void __init cpu_cache_init(void)
 		extern void __weak sh4_cache_init(void);
 
 		sh4_cache_init();
+
+		if ((boot_cpu_data.type == CPU_SH7786) ||
+		    (boot_cpu_data.type == CPU_SHX3)) {
+			extern void __weak shx3_cache_init(void);
+
+			shx3_cache_init();
+		}
 	}
 
 	if (boot_cpu_data.family == CPU_FAMILY_SH5) {

commit a6198a238baceae9d4e0ce3915f6d239c89b5c08
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Jan 15 14:21:37 2010 +0900

    sh: Guard against early IPIs in flush_cache_all().
    
    flush_cache_all() gets called in to when we do some early ioremapping.
    Unfortunately on SDK7786 the interrupt controller itself requires
    ioremapping, leading to a bit of a chicken and egg scenario. For now,
    don't bother with IPI crosscalls if there aren't any other CPUs online.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index b8607fa7ae12..0f4095d7ac8b 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -2,7 +2,7 @@
  * arch/sh/mm/cache.c
  *
  * Copyright (C) 1999, 2000, 2002  Niibe Yutaka
- * Copyright (C) 2002 - 2009  Paul Mundt
+ * Copyright (C) 2002 - 2010  Paul Mundt
  *
  * Released under the terms of the GNU GPL v2.0.
  */
@@ -41,8 +41,17 @@ static inline void cacheop_on_each_cpu(void (*func) (void *info), void *info,
                                    int wait)
 {
 	preempt_disable();
-	smp_call_function(func, info, wait);
+
+	/*
+	 * It's possible that this gets called early on when IRQs are
+	 * still disabled due to ioremapping by the boot CPU, so don't
+	 * even attempt IPIs unless there are other CPUs online.
+	 */
+	if (num_online_cpus() > 1)
+		smp_call_function(func, info, wait);
+
 	func(info);
+
 	preempt_enable();
 }
 

commit 76382b5bdb77c29ab430e1b82ef1c604c8dd113b
Author: Markus Pietrek <Markus.Pietrek@emtrion.de>
Date:   Thu Dec 24 15:12:02 2009 +0900

    sh: Ensure all PG_dcache_dirty pages are written back.
    
    With some of the cache rework an address aliasing optimization was added,
    but this managed to fail on certain mappings resulting in pages with
    PG_dcache_dirty set never writing back their dcache lines. This patch
    reverts to the earlier behaviour of simply always writing back when the
    dirty bit is set.
    
    Signed-off-by: Markus Pietrek <Markus.Pietrek@emtrion.de>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index e9415d3ea94a..b8607fa7ae12 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -133,12 +133,8 @@ void __update_cache(struct vm_area_struct *vma,
 	page = pfn_to_page(pfn);
 	if (pfn_valid(pfn)) {
 		int dirty = test_and_clear_bit(PG_dcache_dirty, &page->flags);
-		if (dirty) {
-			unsigned long addr = (unsigned long)page_address(page);
-
-			if (pages_do_alias(addr, address & PAGE_MASK))
-				__flush_purge_region((void *)addr, PAGE_SIZE);
-		}
+		if (dirty)
+			__flush_purge_region(page_address(page), PAGE_SIZE);
 	}
 }
 

commit 7e01c949989b984c074469e04ab99c47367c7187
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Dec 4 15:14:52 2009 +0900

    sh: Partial revert of copy/clear_user_highpage() optimizations.
    
    These still require more testing, so revert them for now. We keep the
    off-by-1 in the fixmap colouring and drop the rest.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 997c7e42b1e1..e9415d3ea94a 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -46,18 +46,6 @@ static inline void cacheop_on_each_cpu(void (*func) (void *info), void *info,
 	preempt_enable();
 }
 
-/*
- * copy_to_user_page
- * @vma: vm_area_struct holding the pages
- * @page: struct page
- * @vaddr: user space address
- * @dst: address of page in kernel space (possibly from kmap)
- * @src: source address in kernel logical memory
- * @len: length of data in bytes (may be less than PAGE_SIZE)
- *
- * Copy data into the address space of a process other than the current
- * process (eg for ptrace).
- */
 void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
 		       unsigned long vaddr, void *dst, const void *src,
 		       unsigned long len)
@@ -93,49 +81,28 @@ void copy_from_user_page(struct vm_area_struct *vma, struct page *page,
 	}
 }
 
-/*
- * copy_user_highpage
- * @to: destination page
- * @from: source page
- * @vaddr: address of pages in user address space
- * @vma: vm_area_struct holding the pages
- *
- * This is used in COW implementation to copy data from page @from to
- * page @to. @from was previousl mapped at @vaddr, and @to will be.
- * As this is used only in the COW implementation, this means that the
- * source is unmodified, and so we don't have to worry about cache
- * aliasing on that side.
- */
-#ifdef CONFIG_HIGHMEM
-/*
- * If we ever have a real highmem system, this code will need fixing
- * (as will clear_user/clear_user_highmem), because the kmap potentitally
- * creates another alias risk.
- */
-#error This code is broken with real HIGHMEM
-#endif
 void copy_user_highpage(struct page *to, struct page *from,
 			unsigned long vaddr, struct vm_area_struct *vma)
 {
 	void *vfrom, *vto;
 
 	vto = kmap_atomic(to, KM_USER1);
-	vfrom = kmap_atomic(from, KM_USER0);
-
-	if (pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK))
-		__flush_invalidate_region(vto, PAGE_SIZE);
 
 	if (boot_cpu_data.dcache.n_aliases && page_mapped(from) &&
 	    !test_bit(PG_dcache_dirty, &from->flags)) {
-		void *vto_coloured = kmap_coherent(to, vaddr);
-		copy_page(vto_coloured, vfrom);
-		kunmap_coherent(vto_coloured);
-	} else
+		vfrom = kmap_coherent(from, vaddr);
 		copy_page(vto, vfrom);
+		kunmap_coherent(vfrom);
+	} else {
+		vfrom = kmap_atomic(from, KM_USER0);
+		copy_page(vto, vfrom);
+		kunmap_atomic(vfrom, KM_USER0);
+	}
 
-	kunmap_atomic(vfrom, KM_USER0);
-	kunmap_atomic(vto, KM_USER1);
+	if (pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK))
+		__flush_purge_region(vto, PAGE_SIZE);
 
+	kunmap_atomic(vto, KM_USER1);
 	/* Make sure this page is cleared on other CPU's too before using it */
 	smp_wmb();
 }
@@ -145,17 +112,10 @@ void clear_user_highpage(struct page *page, unsigned long vaddr)
 {
 	void *kaddr = kmap_atomic(page, KM_USER0);
 
-	if (pages_do_alias((unsigned long)kaddr, vaddr & PAGE_MASK)) {
-		void *vto;
+	clear_page(kaddr);
 
-		/* Kernel alias may have modified data in the cache. */
-		__flush_invalidate_region(kaddr, PAGE_SIZE);
-
-		vto = kmap_coherent(page, vaddr);
-		clear_page(vto);
-		kunmap_coherent(vto);
-	} else
-		clear_page(kaddr);
+	if (pages_do_alias((unsigned long)kaddr, vaddr & PAGE_MASK))
+		__flush_purge_region(kaddr, PAGE_SIZE);
 
 	kunmap_atomic(kaddr, KM_USER0);
 }

commit 39ac11c1607f1d566e7cf885acd403fa4f07f8a2
Author: Stuart Menefy <stuart.menefy@st.com>
Date:   Tue Oct 27 15:14:06 2009 +0000

    sh: Improve performance of SH4 versions of copy/clear_user_highpage
    
    The previous implementation of clear_user_highpage and copy_user_highpage
    checked to see if there was a D-cache aliasing issue between the user
    and kernel mappings of a page, but if there was they always did a
    flush with writeback on the dirtied kernel alias.
    
    However as we now have the ability to map a page into kernel space
    with the same cache colour as the user mapping, there is no need to
    write back this data.
    
    Currently we also invalidate the kernel alias as a precaution, however
    I'm not sure if this is actually required.
    
    Also correct the definition of FIX_CMAP_END so that the mappings created
    by kmap_coherent() are actually at the correct colour.
    
    Signed-off-by: Stuart Menefy <stuart.menefy@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index e9415d3ea94a..997c7e42b1e1 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -46,6 +46,18 @@ static inline void cacheop_on_each_cpu(void (*func) (void *info), void *info,
 	preempt_enable();
 }
 
+/*
+ * copy_to_user_page
+ * @vma: vm_area_struct holding the pages
+ * @page: struct page
+ * @vaddr: user space address
+ * @dst: address of page in kernel space (possibly from kmap)
+ * @src: source address in kernel logical memory
+ * @len: length of data in bytes (may be less than PAGE_SIZE)
+ *
+ * Copy data into the address space of a process other than the current
+ * process (eg for ptrace).
+ */
 void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
 		       unsigned long vaddr, void *dst, const void *src,
 		       unsigned long len)
@@ -81,28 +93,49 @@ void copy_from_user_page(struct vm_area_struct *vma, struct page *page,
 	}
 }
 
+/*
+ * copy_user_highpage
+ * @to: destination page
+ * @from: source page
+ * @vaddr: address of pages in user address space
+ * @vma: vm_area_struct holding the pages
+ *
+ * This is used in COW implementation to copy data from page @from to
+ * page @to. @from was previousl mapped at @vaddr, and @to will be.
+ * As this is used only in the COW implementation, this means that the
+ * source is unmodified, and so we don't have to worry about cache
+ * aliasing on that side.
+ */
+#ifdef CONFIG_HIGHMEM
+/*
+ * If we ever have a real highmem system, this code will need fixing
+ * (as will clear_user/clear_user_highmem), because the kmap potentitally
+ * creates another alias risk.
+ */
+#error This code is broken with real HIGHMEM
+#endif
 void copy_user_highpage(struct page *to, struct page *from,
 			unsigned long vaddr, struct vm_area_struct *vma)
 {
 	void *vfrom, *vto;
 
 	vto = kmap_atomic(to, KM_USER1);
+	vfrom = kmap_atomic(from, KM_USER0);
+
+	if (pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK))
+		__flush_invalidate_region(vto, PAGE_SIZE);
 
 	if (boot_cpu_data.dcache.n_aliases && page_mapped(from) &&
 	    !test_bit(PG_dcache_dirty, &from->flags)) {
-		vfrom = kmap_coherent(from, vaddr);
+		void *vto_coloured = kmap_coherent(to, vaddr);
+		copy_page(vto_coloured, vfrom);
+		kunmap_coherent(vto_coloured);
+	} else
 		copy_page(vto, vfrom);
-		kunmap_coherent(vfrom);
-	} else {
-		vfrom = kmap_atomic(from, KM_USER0);
-		copy_page(vto, vfrom);
-		kunmap_atomic(vfrom, KM_USER0);
-	}
-
-	if (pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK))
-		__flush_purge_region(vto, PAGE_SIZE);
 
+	kunmap_atomic(vfrom, KM_USER0);
 	kunmap_atomic(vto, KM_USER1);
+
 	/* Make sure this page is cleared on other CPU's too before using it */
 	smp_wmb();
 }
@@ -112,10 +145,17 @@ void clear_user_highpage(struct page *page, unsigned long vaddr)
 {
 	void *kaddr = kmap_atomic(page, KM_USER0);
 
-	clear_page(kaddr);
+	if (pages_do_alias((unsigned long)kaddr, vaddr & PAGE_MASK)) {
+		void *vto;
 
-	if (pages_do_alias((unsigned long)kaddr, vaddr & PAGE_MASK))
-		__flush_purge_region(kaddr, PAGE_SIZE);
+		/* Kernel alias may have modified data in the cache. */
+		__flush_invalidate_region(kaddr, PAGE_SIZE);
+
+		vto = kmap_coherent(page, vaddr);
+		clear_page(vto);
+		kunmap_coherent(vto);
+	} else
+		clear_page(kaddr);
 
 	kunmap_atomic(kaddr, KM_USER0);
 }

commit 3af539e59cf3213cbe31ce7008f1db51c52665ca
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Nov 12 17:03:28 2009 +0900

    sh64: Fix up reworked cache op build.
    
    This gets the build fixed up for the sh64 cache enabled case.
    Disabling still needs further abstraction for independent I/D-cache
    disabling.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 63c132998f24..e9415d3ea94a 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -277,7 +277,11 @@ static void __init emit_cache_params(void)
 
 void __init cpu_cache_init(void)
 {
-	unsigned int cache_disabled = !(__raw_readl(CCR) & CCR_CACHE_ENABLE);
+	unsigned int cache_disabled = 0;
+
+#ifdef CCR
+	cache_disabled = !(__raw_readl(CCR) & CCR_CACHE_ENABLE);
+#endif
 
 	compute_alias(&boot_cpu_data.icache);
 	compute_alias(&boot_cpu_data.dcache);

commit 0a993b0a290a2672500000b0ce811efc093f8467
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Oct 27 10:51:35 2009 +0900

    sh64: cache flush symbol exports.
    
    These were previously hidden in sh_ksyms_32, despite also being needed
    for sh64 now that the cache.c code is shared.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index fc372a1d3132..63c132998f24 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -27,8 +27,11 @@ void (*local_flush_icache_page)(void *args) = cache_noop;
 void (*local_flush_cache_sigtramp)(void *args) = cache_noop;
 
 void (*__flush_wback_region)(void *start, int size);
+EXPORT_SYMBOL(__flush_wback_region);
 void (*__flush_purge_region)(void *start, int size);
+EXPORT_SYMBOL(__flush_purge_region);
 void (*__flush_invalidate_region)(void *start, int size);
+EXPORT_SYMBOL(__flush_invalidate_region);
 
 static inline void noop__flush_region(void *start, int size)
 {
@@ -161,6 +164,7 @@ void flush_cache_all(void)
 {
 	cacheop_on_each_cpu(local_flush_cache_all, NULL, 1);
 }
+EXPORT_SYMBOL(flush_cache_all);
 
 void flush_cache_mm(struct mm_struct *mm)
 {
@@ -201,11 +205,13 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 
 	cacheop_on_each_cpu(local_flush_cache_range, (void *)&data, 1);
 }
+EXPORT_SYMBOL(flush_cache_range);
 
 void flush_dcache_page(struct page *page)
 {
 	cacheop_on_each_cpu(local_flush_dcache_page, page, 1);
 }
+EXPORT_SYMBOL(flush_dcache_page);
 
 void flush_icache_range(unsigned long start, unsigned long end)
 {

commit abeaf33a4101764291ec79cf286e08c0966eb26e
Merge: 731ba3301de4 52a94909f00e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Oct 16 15:14:50 2009 +0900

    Merge branch 'sh/stable-updates'
    
    Conflicts:
            arch/sh/mm/cache-sh4.c

commit 5fb80ae8bd7549034845ebfba694d483070b768b
Author: Magnus Damm <damm@opensource.se>
Date:   Fri Oct 16 14:38:48 2009 +0900

    sh: disabled cache handling fix.
    
    Add code to handle the cache disabled case. Fixes breakage introduced by
    37443ef3f0406e855e169c87ae3f4ffb4b6ff635 ("sh: Migrate SH-4 cacheflush
    ops to function pointers."). Without this patch configuring caches off
    with CONFIG_CACHE_OFF=y makes kfr2r09 and migo-r lock up in fbdev
    deferred io or early user space.
    
    Signed-off-by: Magnus Damm <damm@opensource.se>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 5e1091be9dc4..a2dc7f9ecc51 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -265,6 +265,8 @@ static void __init emit_cache_params(void)
 
 void __init cpu_cache_init(void)
 {
+	unsigned int cache_disabled = !(__raw_readl(CCR) & CCR_CACHE_ENABLE);
+
 	compute_alias(&boot_cpu_data.icache);
 	compute_alias(&boot_cpu_data.dcache);
 	compute_alias(&boot_cpu_data.scache);
@@ -273,6 +275,13 @@ void __init cpu_cache_init(void)
 	__flush_purge_region		= noop__flush_region;
 	__flush_invalidate_region	= noop__flush_region;
 
+	/*
+	 * No flushing is necessary in the disabled cache case so we can
+	 * just keep the noop functions in local_flush_..() and __flush_..()
+	 */
+	if (unlikely(cache_disabled))
+		goto skip;
+
 	if (boot_cpu_data.family == CPU_FAMILY_SH2) {
 		extern void __weak sh2_cache_init(void);
 
@@ -312,5 +321,6 @@ void __init cpu_cache_init(void)
 		sh5_cache_init();
 	}
 
+skip:
 	emit_cache_params();
 }

commit 95019b48addc014c7de6653517663527a1747fb7
Merge: 7a0064d67215 964f7e5a5681
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Oct 13 11:27:08 2009 +0900

    Merge branch 'sh/stable-updates'

commit 964f7e5a56814b32c727821de77d22bd7ef782bc
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Oct 13 11:18:34 2009 +0900

    sh: force dcache flush if dcache_dirty bit set.
    
    This too follows the ARM change, given that the issue at hand applies to
    all platforms that implement lazy D-cache writeback.
    
    This fixes up the case when a page mapping disappears between the
    flush_dcache_page() call (when PG_dcache_dirty is set for the page) and
    the update_mmu_cache() call -- such as in the case of swap cache being
    freed early. This kills off the mapping test in update_mmu_cache() and
    switches to simply testing for PG_dcache_dirty.
    
    Reported-by: Nitin Gupta <ngupta@vflare.org>
    Reported-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 35c37b7f717a..5e1091be9dc4 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -128,7 +128,7 @@ void __update_cache(struct vm_area_struct *vma,
 		return;
 
 	page = pfn_to_page(pfn);
-	if (pfn_valid(pfn) && page_mapping(page)) {
+	if (pfn_valid(pfn)) {
 		int dirty = test_and_clear_bit(PG_dcache_dirty, &page->flags);
 		if (dirty) {
 			unsigned long addr = (unsigned long)page_address(page);

commit 654d364e26c797e8a5f9e2a1393607e6ca0106eb
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 9 14:04:06 2009 +0900

    sh: sh4_flush_cache_mm() optimizations.
    
    The i-cache flush in the case of VM_EXEC was added way back when as a
    sanity measure, and in practice we only care about evicting aliases from
    the d-cache. As a result, it's possible to drop the i-cache flush
    completely here.
    
    After careful profiling it's also come up that all of the work associated
    with hunting down aliases and doing ranged flushing ends up generating
    more overhead than simply blasting away the entire dcache, particularly
    if there are many mm's that need to be iterated over. As a result of
    that, just move back to flush_dcache_all() in these cases, which restores
    the old behaviour, and vastly simplifies the path.
    
    Additionally, on platforms without aliases at all, this can simply be
    nopped out. Presently we have the alias check in the SH-4 specific
    version, but this is true for all of the platforms, so move the check up
    to a generic location. This cuts down quite a bit on superfluous cacheop
    IPIs.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 35c37b7f717a..4aa926054531 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -164,11 +164,17 @@ void flush_cache_all(void)
 
 void flush_cache_mm(struct mm_struct *mm)
 {
+	if (boot_cpu_data.dcache.n_aliases == 0)
+		return;
+
 	cacheop_on_each_cpu(local_flush_cache_mm, mm, 1);
 }
 
 void flush_cache_dup_mm(struct mm_struct *mm)
 {
+	if (boot_cpu_data.dcache.n_aliases == 0)
+		return;
+
 	cacheop_on_each_cpu(local_flush_cache_dup_mm, mm, 1);
 }
 

commit 6e4154d4c2dd3d7e61d19ddd2527322ce34c2f5a
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Sep 8 16:21:00 2009 +0900

    sh: Use more aggressive dcache purging in kmap teardown.
    
    This fixes up a number of outstanding issues observed with old mappings
    on the same colour hanging around. This requires some more optimal
    handling, but is a safe fallback until all of the corner cases have been
    handled.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 8e4a8d1ac4a9..35c37b7f717a 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -97,7 +97,7 @@ void copy_user_highpage(struct page *to, struct page *from,
 	}
 
 	if (pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK))
-		__flush_wback_region(vto, PAGE_SIZE);
+		__flush_purge_region(vto, PAGE_SIZE);
 
 	kunmap_atomic(vto, KM_USER1);
 	/* Make sure this page is cleared on other CPU's too before using it */
@@ -112,7 +112,7 @@ void clear_user_highpage(struct page *page, unsigned long vaddr)
 	clear_page(kaddr);
 
 	if (pages_do_alias((unsigned long)kaddr, vaddr & PAGE_MASK))
-		__flush_wback_region(kaddr, PAGE_SIZE);
+		__flush_purge_region(kaddr, PAGE_SIZE);
 
 	kunmap_atomic(kaddr, KM_USER0);
 }
@@ -134,7 +134,7 @@ void __update_cache(struct vm_area_struct *vma,
 			unsigned long addr = (unsigned long)page_address(page);
 
 			if (pages_do_alias(addr, address & PAGE_MASK))
-				__flush_wback_region((void *)addr, PAGE_SIZE);
+				__flush_purge_region((void *)addr, PAGE_SIZE);
 		}
 	}
 }
@@ -149,10 +149,11 @@ void __flush_anon_page(struct page *page, unsigned long vmaddr)
 			void *kaddr;
 
 			kaddr = kmap_coherent(page, vmaddr);
-			__flush_wback_region((void *)kaddr, PAGE_SIZE);
+			/* XXX.. For now kunmap_coherent() does a purge */
+			/* __flush_purge_region((void *)kaddr, PAGE_SIZE); */
 			kunmap_coherent(kaddr);
 		} else
-			__flush_wback_region((void *)addr, PAGE_SIZE);
+			__flush_purge_region((void *)addr, PAGE_SIZE);
 	}
 }
 

commit 0906a3ad33a254094fb74828e3ddb9af8771a6da
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Sep 3 17:21:10 2009 +0900

    sh: Fix up and optimize the kmap_coherent() interface.
    
    This fixes up the kmap_coherent/kunmap_coherent() interface for recent
    changes both in the page fault path and the shared cache flushers, as
    well as adding in some optimizations.
    
    One of the key things to note here is that the TLB flush itself is
    deferred until the unmap, and the call in to update_mmu_cache() itself
    goes away, relying on the regular page fault path to handle the lazy
    dcache writeback if necessary.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index db2b1c5beffd..8e4a8d1ac4a9 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -51,7 +51,7 @@ void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
 	    !test_bit(PG_dcache_dirty, &page->flags)) {
 		void *vto = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(vto, src, len);
-		kunmap_coherent();
+		kunmap_coherent(vto);
 	} else {
 		memcpy(dst, src, len);
 		if (boot_cpu_data.dcache.n_aliases)
@@ -70,7 +70,7 @@ void copy_from_user_page(struct vm_area_struct *vma, struct page *page,
 	    !test_bit(PG_dcache_dirty, &page->flags)) {
 		void *vfrom = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(dst, vfrom, len);
-		kunmap_coherent();
+		kunmap_coherent(vfrom);
 	} else {
 		memcpy(dst, src, len);
 		if (boot_cpu_data.dcache.n_aliases)
@@ -89,7 +89,7 @@ void copy_user_highpage(struct page *to, struct page *from,
 	    !test_bit(PG_dcache_dirty, &from->flags)) {
 		vfrom = kmap_coherent(from, vaddr);
 		copy_page(vto, vfrom);
-		kunmap_coherent();
+		kunmap_coherent(vfrom);
 	} else {
 		vfrom = kmap_atomic(from, KM_USER0);
 		copy_page(vto, vfrom);
@@ -150,7 +150,7 @@ void __flush_anon_page(struct page *page, unsigned long vmaddr)
 
 			kaddr = kmap_coherent(page, vmaddr);
 			__flush_wback_region((void *)kaddr, PAGE_SIZE);
-			kunmap_coherent();
+			kunmap_coherent(kaddr);
 		} else
 			__flush_wback_region((void *)addr, PAGE_SIZE);
 	}

commit 6f3795788b030c3c190fa063adfe519e016cc6fd
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Sep 1 21:21:36 2009 +0900

    sh: Fix up UP deadlock with SMP-aware cache ops.
    
    This builds on top of the previous reversion and implements a special
    on_each_cpu() variant that simple disables preemption across the call
    while leaving the interrupt state to the function itself. There were some
    unintended consequences with IRQ disabling in some of these paths on UP
    that ran in to a deadlock scenario with IRQs being missed.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 411fe6058429..db2b1c5beffd 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -34,6 +34,15 @@ static inline void noop__flush_region(void *start, int size)
 {
 }
 
+static inline void cacheop_on_each_cpu(void (*func) (void *info), void *info,
+                                   int wait)
+{
+	preempt_disable();
+	smp_call_function(func, info, wait);
+	func(info);
+	preempt_enable();
+}
+
 void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
 		       unsigned long vaddr, void *dst, const void *src,
 		       unsigned long len)
@@ -149,17 +158,17 @@ void __flush_anon_page(struct page *page, unsigned long vmaddr)
 
 void flush_cache_all(void)
 {
-	on_each_cpu(local_flush_cache_all, NULL, 1);
+	cacheop_on_each_cpu(local_flush_cache_all, NULL, 1);
 }
 
 void flush_cache_mm(struct mm_struct *mm)
 {
-	on_each_cpu(local_flush_cache_mm, mm, 1);
+	cacheop_on_each_cpu(local_flush_cache_mm, mm, 1);
 }
 
 void flush_cache_dup_mm(struct mm_struct *mm)
 {
-	on_each_cpu(local_flush_cache_dup_mm, mm, 1);
+	cacheop_on_each_cpu(local_flush_cache_dup_mm, mm, 1);
 }
 
 void flush_cache_page(struct vm_area_struct *vma, unsigned long addr,
@@ -171,7 +180,7 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long addr,
 	data.addr1 = addr;
 	data.addr2 = pfn;
 
-	on_each_cpu(local_flush_cache_page, (void *)&data, 1);
+	cacheop_on_each_cpu(local_flush_cache_page, (void *)&data, 1);
 }
 
 void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
@@ -183,12 +192,12 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 	data.addr1 = start;
 	data.addr2 = end;
 
-	on_each_cpu(local_flush_cache_range, (void *)&data, 1);
+	cacheop_on_each_cpu(local_flush_cache_range, (void *)&data, 1);
 }
 
 void flush_dcache_page(struct page *page)
 {
-	on_each_cpu(local_flush_dcache_page, page, 1);
+	cacheop_on_each_cpu(local_flush_dcache_page, page, 1);
 }
 
 void flush_icache_range(unsigned long start, unsigned long end)
@@ -199,18 +208,18 @@ void flush_icache_range(unsigned long start, unsigned long end)
 	data.addr1 = start;
 	data.addr2 = end;
 
-	on_each_cpu(local_flush_icache_range, (void *)&data, 1);
+	cacheop_on_each_cpu(local_flush_icache_range, (void *)&data, 1);
 }
 
 void flush_icache_page(struct vm_area_struct *vma, struct page *page)
 {
 	/* Nothing uses the VMA, so just pass the struct page along */
-	on_each_cpu(local_flush_icache_page, page, 1);
+	cacheop_on_each_cpu(local_flush_icache_page, page, 1);
 }
 
 void flush_cache_sigtramp(unsigned long address)
 {
-	on_each_cpu(local_flush_cache_sigtramp, (void *)address, 1);
+	cacheop_on_each_cpu(local_flush_cache_sigtramp, (void *)address, 1);
 }
 
 static void compute_alias(struct cache_info *c)

commit f26b2a562b46ab186c8383993ab1332673ac4a47
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Aug 21 17:23:14 2009 +0900

    sh: Make cache flushers SMP-aware.
    
    This does a bit of rework for making the cache flushers SMP-aware. The
    function pointer-based flushers are renamed to local variants with the
    exported interface being commonly implemented and wrapping as necessary.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index d60239460436..411fe6058429 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -1,5 +1,5 @@
 /*
- * arch/sh/mm/pg-mmu.c
+ * arch/sh/mm/cache.c
  *
  * Copyright (C) 1999, 2000, 2002  Niibe Yutaka
  * Copyright (C) 2002 - 2009  Paul Mundt
@@ -10,63 +10,26 @@
 #include <linux/init.h>
 #include <linux/mutex.h>
 #include <linux/fs.h>
+#include <linux/smp.h>
 #include <linux/highmem.h>
 #include <linux/module.h>
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
-void (*flush_cache_all)(void);
-void (*flush_cache_mm)(struct mm_struct *mm);
-void (*flush_cache_dup_mm)(struct mm_struct *mm);
-void (*flush_cache_page)(struct vm_area_struct *vma,
-				unsigned long addr, unsigned long pfn);
-void (*flush_cache_range)(struct vm_area_struct *vma,
-				 unsigned long start, unsigned long end);
-void (*flush_dcache_page)(struct page *page);
-void (*flush_icache_range)(unsigned long start, unsigned long end);
-void (*flush_icache_page)(struct vm_area_struct *vma,
-				 struct page *page);
-void (*flush_cache_sigtramp)(unsigned long address);
+void (*local_flush_cache_all)(void *args) = cache_noop;
+void (*local_flush_cache_mm)(void *args) = cache_noop;
+void (*local_flush_cache_dup_mm)(void *args) = cache_noop;
+void (*local_flush_cache_page)(void *args) = cache_noop;
+void (*local_flush_cache_range)(void *args) = cache_noop;
+void (*local_flush_dcache_page)(void *args) = cache_noop;
+void (*local_flush_icache_range)(void *args) = cache_noop;
+void (*local_flush_icache_page)(void *args) = cache_noop;
+void (*local_flush_cache_sigtramp)(void *args) = cache_noop;
+
 void (*__flush_wback_region)(void *start, int size);
 void (*__flush_purge_region)(void *start, int size);
 void (*__flush_invalidate_region)(void *start, int size);
 
-static inline void noop_flush_cache_all(void)
-{
-}
-
-static inline void noop_flush_cache_mm(struct mm_struct *mm)
-{
-}
-
-static inline void noop_flush_cache_page(struct vm_area_struct *vma,
-				unsigned long addr, unsigned long pfn)
-{
-}
-
-static inline void noop_flush_cache_range(struct vm_area_struct *vma,
-				 unsigned long start, unsigned long end)
-{
-}
-
-static inline void noop_flush_dcache_page(struct page *page)
-{
-}
-
-static inline void noop_flush_icache_range(unsigned long start,
-					   unsigned long end)
-{
-}
-
-static inline void noop_flush_icache_page(struct vm_area_struct *vma,
-					  struct page *page)
-{
-}
-
-static inline void noop_flush_cache_sigtramp(unsigned long address)
-{
-}
-
 static inline void noop__flush_region(void *start, int size)
 {
 }
@@ -184,6 +147,72 @@ void __flush_anon_page(struct page *page, unsigned long vmaddr)
 	}
 }
 
+void flush_cache_all(void)
+{
+	on_each_cpu(local_flush_cache_all, NULL, 1);
+}
+
+void flush_cache_mm(struct mm_struct *mm)
+{
+	on_each_cpu(local_flush_cache_mm, mm, 1);
+}
+
+void flush_cache_dup_mm(struct mm_struct *mm)
+{
+	on_each_cpu(local_flush_cache_dup_mm, mm, 1);
+}
+
+void flush_cache_page(struct vm_area_struct *vma, unsigned long addr,
+		      unsigned long pfn)
+{
+	struct flusher_data data;
+
+	data.vma = vma;
+	data.addr1 = addr;
+	data.addr2 = pfn;
+
+	on_each_cpu(local_flush_cache_page, (void *)&data, 1);
+}
+
+void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
+		       unsigned long end)
+{
+	struct flusher_data data;
+
+	data.vma = vma;
+	data.addr1 = start;
+	data.addr2 = end;
+
+	on_each_cpu(local_flush_cache_range, (void *)&data, 1);
+}
+
+void flush_dcache_page(struct page *page)
+{
+	on_each_cpu(local_flush_dcache_page, page, 1);
+}
+
+void flush_icache_range(unsigned long start, unsigned long end)
+{
+	struct flusher_data data;
+
+	data.vma = NULL;
+	data.addr1 = start;
+	data.addr2 = end;
+
+	on_each_cpu(local_flush_icache_range, (void *)&data, 1);
+}
+
+void flush_icache_page(struct vm_area_struct *vma, struct page *page)
+{
+	/* Nothing uses the VMA, so just pass the struct page along */
+	on_each_cpu(local_flush_icache_page, page, 1);
+}
+
+void flush_cache_sigtramp(unsigned long address)
+{
+	on_each_cpu(local_flush_cache_sigtramp, (void *)address, 1);
+}
+
 static void compute_alias(struct cache_info *c)
 {
 	c->alias_mask = ((c->sets - 1) << c->entry_shift) & ~(PAGE_SIZE - 1);
@@ -230,16 +259,6 @@ void __init cpu_cache_init(void)
 	compute_alias(&boot_cpu_data.dcache);
 	compute_alias(&boot_cpu_data.scache);
 
-	flush_cache_all		= noop_flush_cache_all;
-	flush_cache_mm		= noop_flush_cache_mm;
-	flush_cache_dup_mm	= noop_flush_cache_mm;
-	flush_cache_page	= noop_flush_cache_page;
-	flush_cache_range	= noop_flush_cache_range;
-	flush_dcache_page	= noop_flush_dcache_page;
-	flush_icache_range	= noop_flush_icache_range;
-	flush_icache_page	= noop_flush_icache_page;
-	flush_cache_sigtramp	= noop_flush_cache_sigtramp;
-
 	__flush_wback_region		= noop__flush_region;
 	__flush_purge_region		= noop__flush_region;
 	__flush_invalidate_region	= noop__flush_region;

commit 2b4315185a06414c4ab40fb0db50dce1b534a1d9
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sun Aug 16 02:16:44 2009 +0900

    sh: Wire up sh5_cache_init().
    
    Now that the SH-5 code is more or less behaving with the new cacheflush
    interface, wire up the initialization code.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 8618ccdc1ca5..d60239460436 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -277,5 +277,11 @@ void __init cpu_cache_init(void)
 		sh4_cache_init();
 	}
 
+	if (boot_cpu_data.family == CPU_FAMILY_SH5) {
+		extern void __weak sh5_cache_init(void);
+
+		sh5_cache_init();
+	}
+
 	emit_cache_params();
 }

commit 0d051d90bb08b516b9d6c30d25f83d3c6b5b1c1d
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 12:53:39 2009 +0900

    sh: Convert SH7705 extended mode to new cacheflush interface.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index c9480b48c746..8618ccdc1ca5 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -260,6 +260,13 @@ void __init cpu_cache_init(void)
 		extern void __weak sh3_cache_init(void);
 
 		sh3_cache_init();
+
+		if ((boot_cpu_data.type == CPU_SH7705) &&
+		    (boot_cpu_data.dcache.sets == 512)) {
+			extern void __weak sh7705_cache_init(void);
+
+			sh7705_cache_init();
+		}
 	}
 
 	if ((boot_cpu_data.family == CPU_FAMILY_SH4) ||

commit 79f1c9da5e5fc5f4705836d8c1cee2213fc80640
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 12:42:55 2009 +0900

    sh: Convert SH-3 to new cacheflush interface.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index b56cce408912..c9480b48c746 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -256,6 +256,12 @@ void __init cpu_cache_init(void)
 		sh2a_cache_init();
 	}
 
+	if (boot_cpu_data.family == CPU_FAMILY_SH3) {
+		extern void __weak sh3_cache_init(void);
+
+		sh3_cache_init();
+	}
+
 	if ((boot_cpu_data.family == CPU_FAMILY_SH4) ||
 	    (boot_cpu_data.family == CPU_FAMILY_SH4A) ||
 	    (boot_cpu_data.family == CPU_FAMILY_SH4AL_DSP)) {

commit a58e1a2ab4f6334c50dfbda83d3a5c6e0b2b4bee
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 12:38:29 2009 +0900

    sh: Convert SH-2A to new cacheflush interface.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 5ac299d6604e..b56cce408912 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -250,6 +250,12 @@ void __init cpu_cache_init(void)
 		sh2_cache_init();
 	}
 
+	if (boot_cpu_data.family == CPU_FAMILY_SH2A) {
+		extern void __weak sh2a_cache_init(void);
+
+		sh2a_cache_init();
+	}
+
 	if ((boot_cpu_data.family == CPU_FAMILY_SH4) ||
 	    (boot_cpu_data.family == CPU_FAMILY_SH4A) ||
 	    (boot_cpu_data.family == CPU_FAMILY_SH4AL_DSP)) {

commit 109b44a82a7a8ae32d7fb257480f92f2d96f0daf
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 12:35:15 2009 +0900

    sh: Convert SH-2 to new cacheflush interface.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index da5bc6ac1b28..5ac299d6604e 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -244,6 +244,12 @@ void __init cpu_cache_init(void)
 	__flush_purge_region		= noop__flush_region;
 	__flush_invalidate_region	= noop__flush_region;
 
+	if (boot_cpu_data.family == CPU_FAMILY_SH2) {
+		extern void __weak sh2_cache_init(void);
+
+		sh2_cache_init();
+	}
+
 	if ((boot_cpu_data.family == CPU_FAMILY_SH4) ||
 	    (boot_cpu_data.family == CPU_FAMILY_SH4A) ||
 	    (boot_cpu_data.family == CPU_FAMILY_SH4AL_DSP)) {

commit 37443ef3f0406e855e169c87ae3f4ffb4b6ff635
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 12:29:49 2009 +0900

    sh: Migrate SH-4 cacheflush ops to function pointers.
    
    This paves the way for allowing individual CPUs to overload the
    individual flushing routines that they care about without having to
    depend on weak aliases. SH-4 is converted over initially, as it wires
    up pretty much everything. The majority of the other CPUs will simply use
    the default no-op implementation with their own region flushers wired up.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index a31e5c46e7a6..da5bc6ac1b28 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -15,6 +15,62 @@
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
+void (*flush_cache_all)(void);
+void (*flush_cache_mm)(struct mm_struct *mm);
+void (*flush_cache_dup_mm)(struct mm_struct *mm);
+void (*flush_cache_page)(struct vm_area_struct *vma,
+				unsigned long addr, unsigned long pfn);
+void (*flush_cache_range)(struct vm_area_struct *vma,
+				 unsigned long start, unsigned long end);
+void (*flush_dcache_page)(struct page *page);
+void (*flush_icache_range)(unsigned long start, unsigned long end);
+void (*flush_icache_page)(struct vm_area_struct *vma,
+				 struct page *page);
+void (*flush_cache_sigtramp)(unsigned long address);
+void (*__flush_wback_region)(void *start, int size);
+void (*__flush_purge_region)(void *start, int size);
+void (*__flush_invalidate_region)(void *start, int size);
+
+static inline void noop_flush_cache_all(void)
+{
+}
+
+static inline void noop_flush_cache_mm(struct mm_struct *mm)
+{
+}
+
+static inline void noop_flush_cache_page(struct vm_area_struct *vma,
+				unsigned long addr, unsigned long pfn)
+{
+}
+
+static inline void noop_flush_cache_range(struct vm_area_struct *vma,
+				 unsigned long start, unsigned long end)
+{
+}
+
+static inline void noop_flush_dcache_page(struct page *page)
+{
+}
+
+static inline void noop_flush_icache_range(unsigned long start,
+					   unsigned long end)
+{
+}
+
+static inline void noop_flush_icache_page(struct vm_area_struct *vma,
+					  struct page *page)
+{
+}
+
+static inline void noop_flush_cache_sigtramp(unsigned long address)
+{
+}
+
+static inline void noop__flush_region(void *start, int size)
+{
+}
+
 void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
 		       unsigned long vaddr, void *dst, const void *src,
 		       unsigned long len)
@@ -174,6 +230,20 @@ void __init cpu_cache_init(void)
 	compute_alias(&boot_cpu_data.dcache);
 	compute_alias(&boot_cpu_data.scache);
 
+	flush_cache_all		= noop_flush_cache_all;
+	flush_cache_mm		= noop_flush_cache_mm;
+	flush_cache_dup_mm	= noop_flush_cache_mm;
+	flush_cache_page	= noop_flush_cache_page;
+	flush_cache_range	= noop_flush_cache_range;
+	flush_dcache_page	= noop_flush_dcache_page;
+	flush_icache_range	= noop_flush_icache_range;
+	flush_icache_page	= noop_flush_icache_page;
+	flush_cache_sigtramp	= noop_flush_cache_sigtramp;
+
+	__flush_wback_region		= noop__flush_region;
+	__flush_purge_region		= noop__flush_region;
+	__flush_invalidate_region	= noop__flush_region;
+
 	if ((boot_cpu_data.family == CPU_FAMILY_SH4) ||
 	    (boot_cpu_data.family == CPU_FAMILY_SH4A) ||
 	    (boot_cpu_data.family == CPU_FAMILY_SH4AL_DSP)) {

commit 27d59ec1709817a90aa3ab7169f60994a89ad2f5
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 11:11:16 2009 +0900

    sh: Move alias computation to shared cache init.
    
    This migrates the alias computation and printing of probed cache
    parameters from the SH-4 code to the shared cpu_cache_init().
    
    This permits other platforms with aliases to make use of the same
    probe logic without having to roll their own, and also produces
    consistent output regardless of platform.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index 659981ffae24..a31e5c46e7a6 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -128,8 +128,52 @@ void __flush_anon_page(struct page *page, unsigned long vmaddr)
 	}
 }
 
+static void compute_alias(struct cache_info *c)
+{
+	c->alias_mask = ((c->sets - 1) << c->entry_shift) & ~(PAGE_SIZE - 1);
+	c->n_aliases = c->alias_mask ? (c->alias_mask >> PAGE_SHIFT) + 1 : 0;
+}
+
+static void __init emit_cache_params(void)
+{
+	printk(KERN_NOTICE "I-cache : n_ways=%d n_sets=%d way_incr=%d\n",
+		boot_cpu_data.icache.ways,
+		boot_cpu_data.icache.sets,
+		boot_cpu_data.icache.way_incr);
+	printk(KERN_NOTICE "I-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
+		boot_cpu_data.icache.entry_mask,
+		boot_cpu_data.icache.alias_mask,
+		boot_cpu_data.icache.n_aliases);
+	printk(KERN_NOTICE "D-cache : n_ways=%d n_sets=%d way_incr=%d\n",
+		boot_cpu_data.dcache.ways,
+		boot_cpu_data.dcache.sets,
+		boot_cpu_data.dcache.way_incr);
+	printk(KERN_NOTICE "D-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
+		boot_cpu_data.dcache.entry_mask,
+		boot_cpu_data.dcache.alias_mask,
+		boot_cpu_data.dcache.n_aliases);
+
+	/*
+	 * Emit Secondary Cache parameters if the CPU has a probed L2.
+	 */
+	if (boot_cpu_data.flags & CPU_HAS_L2_CACHE) {
+		printk(KERN_NOTICE "S-cache : n_ways=%d n_sets=%d way_incr=%d\n",
+			boot_cpu_data.scache.ways,
+			boot_cpu_data.scache.sets,
+			boot_cpu_data.scache.way_incr);
+		printk(KERN_NOTICE "S-cache : entry_mask=0x%08x alias_mask=0x%08x n_aliases=%d\n",
+			boot_cpu_data.scache.entry_mask,
+			boot_cpu_data.scache.alias_mask,
+			boot_cpu_data.scache.n_aliases);
+	}
+}
+
 void __init cpu_cache_init(void)
 {
+	compute_alias(&boot_cpu_data.icache);
+	compute_alias(&boot_cpu_data.dcache);
+	compute_alias(&boot_cpu_data.scache);
+
 	if ((boot_cpu_data.family == CPU_FAMILY_SH4) ||
 	    (boot_cpu_data.family == CPU_FAMILY_SH4A) ||
 	    (boot_cpu_data.family == CPU_FAMILY_SH4AL_DSP)) {
@@ -137,4 +181,6 @@ void __init cpu_cache_init(void)
 
 		sh4_cache_init();
 	}
+
+	emit_cache_params();
 }

commit ecba1060583635ab55092072441ff903b5e9a659
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 11:05:42 2009 +0900

    sh: Centralize the CPU cache initialization routines.
    
    This provides a central point for CPU cache initialization routines.
    This replaces the antiquated p3_cache_init() method, which the vast
    majority of CPUs never cared about.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
index f51d0a4eb3ba..659981ffae24 100644
--- a/arch/sh/mm/cache.c
+++ b/arch/sh/mm/cache.c
@@ -127,3 +127,14 @@ void __flush_anon_page(struct page *page, unsigned long vmaddr)
 			__flush_wback_region((void *)addr, PAGE_SIZE);
 	}
 }
+
+void __init cpu_cache_init(void)
+{
+	if ((boot_cpu_data.family == CPU_FAMILY_SH4) ||
+	    (boot_cpu_data.family == CPU_FAMILY_SH4A) ||
+	    (boot_cpu_data.family == CPU_FAMILY_SH4AL_DSP)) {
+		extern void __weak sh4_cache_init(void);
+
+		sh4_cache_init();
+	}
+}

commit cbbe2f68f678a90bebeb30b8a7fcd8aed0614879
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sat Aug 15 09:30:39 2009 +0900

    sh: rename pg-mmu.c -> cache.c, enable generically.
    
    This builds in the newly created cache.c (renamed from pg-mmu.c) for both
    MMU and NOMMU configurations. The kmap_coherent() stubs and alias
    information recorded by each CPU family takes care of doing the right
    thing while enabling the code to be commonly shared.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/cache.c b/arch/sh/mm/cache.c
new file mode 100644
index 000000000000..f51d0a4eb3ba
--- /dev/null
+++ b/arch/sh/mm/cache.c
@@ -0,0 +1,129 @@
+/*
+ * arch/sh/mm/pg-mmu.c
+ *
+ * Copyright (C) 1999, 2000, 2002  Niibe Yutaka
+ * Copyright (C) 2002 - 2009  Paul Mundt
+ *
+ * Released under the terms of the GNU GPL v2.0.
+ */
+#include <linux/mm.h>
+#include <linux/init.h>
+#include <linux/mutex.h>
+#include <linux/fs.h>
+#include <linux/highmem.h>
+#include <linux/module.h>
+#include <asm/mmu_context.h>
+#include <asm/cacheflush.h>
+
+void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
+		       unsigned long vaddr, void *dst, const void *src,
+		       unsigned long len)
+{
+	if (boot_cpu_data.dcache.n_aliases && page_mapped(page) &&
+	    !test_bit(PG_dcache_dirty, &page->flags)) {
+		void *vto = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
+		memcpy(vto, src, len);
+		kunmap_coherent();
+	} else {
+		memcpy(dst, src, len);
+		if (boot_cpu_data.dcache.n_aliases)
+			set_bit(PG_dcache_dirty, &page->flags);
+	}
+
+	if (vma->vm_flags & VM_EXEC)
+		flush_cache_page(vma, vaddr, page_to_pfn(page));
+}
+
+void copy_from_user_page(struct vm_area_struct *vma, struct page *page,
+			 unsigned long vaddr, void *dst, const void *src,
+			 unsigned long len)
+{
+	if (boot_cpu_data.dcache.n_aliases && page_mapped(page) &&
+	    !test_bit(PG_dcache_dirty, &page->flags)) {
+		void *vfrom = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
+		memcpy(dst, vfrom, len);
+		kunmap_coherent();
+	} else {
+		memcpy(dst, src, len);
+		if (boot_cpu_data.dcache.n_aliases)
+			set_bit(PG_dcache_dirty, &page->flags);
+	}
+}
+
+void copy_user_highpage(struct page *to, struct page *from,
+			unsigned long vaddr, struct vm_area_struct *vma)
+{
+	void *vfrom, *vto;
+
+	vto = kmap_atomic(to, KM_USER1);
+
+	if (boot_cpu_data.dcache.n_aliases && page_mapped(from) &&
+	    !test_bit(PG_dcache_dirty, &from->flags)) {
+		vfrom = kmap_coherent(from, vaddr);
+		copy_page(vto, vfrom);
+		kunmap_coherent();
+	} else {
+		vfrom = kmap_atomic(from, KM_USER0);
+		copy_page(vto, vfrom);
+		kunmap_atomic(vfrom, KM_USER0);
+	}
+
+	if (pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK))
+		__flush_wback_region(vto, PAGE_SIZE);
+
+	kunmap_atomic(vto, KM_USER1);
+	/* Make sure this page is cleared on other CPU's too before using it */
+	smp_wmb();
+}
+EXPORT_SYMBOL(copy_user_highpage);
+
+void clear_user_highpage(struct page *page, unsigned long vaddr)
+{
+	void *kaddr = kmap_atomic(page, KM_USER0);
+
+	clear_page(kaddr);
+
+	if (pages_do_alias((unsigned long)kaddr, vaddr & PAGE_MASK))
+		__flush_wback_region(kaddr, PAGE_SIZE);
+
+	kunmap_atomic(kaddr, KM_USER0);
+}
+EXPORT_SYMBOL(clear_user_highpage);
+
+void __update_cache(struct vm_area_struct *vma,
+		    unsigned long address, pte_t pte)
+{
+	struct page *page;
+	unsigned long pfn = pte_pfn(pte);
+
+	if (!boot_cpu_data.dcache.n_aliases)
+		return;
+
+	page = pfn_to_page(pfn);
+	if (pfn_valid(pfn) && page_mapping(page)) {
+		int dirty = test_and_clear_bit(PG_dcache_dirty, &page->flags);
+		if (dirty) {
+			unsigned long addr = (unsigned long)page_address(page);
+
+			if (pages_do_alias(addr, address & PAGE_MASK))
+				__flush_wback_region((void *)addr, PAGE_SIZE);
+		}
+	}
+}
+
+void __flush_anon_page(struct page *page, unsigned long vmaddr)
+{
+	unsigned long addr = (unsigned long) page_address(page);
+
+	if (pages_do_alias(addr, vmaddr)) {
+		if (boot_cpu_data.dcache.n_aliases && page_mapped(page) &&
+		    !test_bit(PG_dcache_dirty, &page->flags)) {
+			void *kaddr;
+
+			kaddr = kmap_coherent(page, vmaddr);
+			__flush_wback_region((void *)kaddr, PAGE_SIZE);
+			kunmap_coherent();
+		} else
+			__flush_wback_region((void *)addr, PAGE_SIZE);
+	}
+}
