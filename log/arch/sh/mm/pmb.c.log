commit 65fddcfca8ad14778f71a57672fd01e8112d30fa
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:42 2020 -0700

    mm: reorder includes after introduction of linux/pgtable.h
    
    The replacement of <asm/pgrable.h> with <linux/pgtable.h> made the include
    of the latter in the middle of asm includes.  Fix this up with the aid of
    the below script and manual adjustments here and there.
    
            import sys
            import re
    
            if len(sys.argv) is not 3:
                print "USAGE: %s <file> <header>" % (sys.argv[0])
                sys.exit(1)
    
            hdr_to_move="#include <linux/%s>" % sys.argv[2]
            moved = False
            in_hdrs = False
    
            with open(sys.argv[1], "r") as f:
                lines = f.readlines()
                for _line in lines:
                    line = _line.rstrip('
    ')
                    if line == hdr_to_move:
                        continue
                    if line.startswith("#include <linux/"):
                        in_hdrs = True
                    elif not moved and in_hdrs:
                        moved = True
                        print hdr_to_move
                    print line
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 76bb1b55fab8..b20aba6e1b37 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -23,10 +23,10 @@
 #include <linux/io.h>
 #include <linux/spinlock.h>
 #include <linux/vmalloc.h>
+#include <linux/pgtable.h>
 #include <asm/cacheflush.h>
 #include <linux/sizes.h>
 #include <linux/uaccess.h>
-#include <linux/pgtable.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
 #include <asm/mmu_context.h>

commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index b59bad86b31e..76bb1b55fab8 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -26,7 +26,7 @@
 #include <asm/cacheflush.h>
 #include <linux/sizes.h>
 #include <linux/uaccess.h>
-#include <asm/pgtable.h>
+#include <linux/pgtable.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
 #include <asm/mmu_context.h>

commit 03eb2a08fccc49f93587666e4e1a14ce00df955a
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Tue Jan 22 15:50:30 2019 +0100

    sh: no need to check return value of debugfs_create functions
    
    When calling debugfs functions, there is no need to ever check the
    return value.  The function can work or not, but the code logic should
    never do something different based on this.
    
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: <linux-sh@vger.kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index a53a040d0054..b59bad86b31e 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -861,13 +861,8 @@ static const struct file_operations pmb_debugfs_fops = {
 
 static int __init pmb_debugfs_init(void)
 {
-	struct dentry *dentry;
-
-	dentry = debugfs_create_file("pmb", S_IFREG | S_IRUGO,
-				     arch_debugfs_dir, NULL, &pmb_debugfs_fops);
-	if (!dentry)
-		return -ENOMEM;
-
+	debugfs_create_file("pmb", S_IFREG | S_IRUGO, arch_debugfs_dir, NULL,
+			    &pmb_debugfs_fops);
 	return 0;
 }
 subsys_initcall(pmb_debugfs_init);

commit 87dfb311b707cd4c4b666c9af0fa15acbe6eee99
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Tue May 14 15:46:51 2019 -0700

    treewide: replace #include <asm/sizes.h> with #include <linux/sizes.h>
    
    Since commit dccd2304cc90 ("ARM: 7430/1: sizes.h: move from asm-generic
    to <linux/sizes.h>"), <asm/sizes.h> and <asm-generic/sizes.h> are just
    wrappers of <linux/sizes.h>.
    
    This commit replaces all <asm/sizes.h> and <asm-generic/sizes.h> to
    prepare for the removal.
    
    Link: http://lkml.kernel.org/r/1553267665-27228-1-git-send-email-yamada.masahiro@socionext.com
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 7b2cc490ebb7..a53a040d0054 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -24,7 +24,7 @@
 #include <linux/spinlock.h>
 #include <linux/vmalloc.h>
 #include <asm/cacheflush.h>
-#include <asm/sizes.h>
+#include <linux/sizes.h>
 #include <linux/uaccess.h>
 #include <asm/pgtable.h>
 #include <asm/page.h>

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 7160c9fd6fe3..7b2cc490ebb7 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -25,7 +25,7 @@
 #include <linux/vmalloc.h>
 #include <asm/cacheflush.h>
 #include <asm/sizes.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/pgtable.h>
 #include <asm/page.h>
 #include <asm/mmu.h>

commit e839ca528718e68cad32a307dc9aabf01ef3eb05
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:03 2012 +0100

    Disintegrate asm/system.h for SH
    
    Disintegrate asm/system.h for SH.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: linux-sh@vger.kernel.org

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index fad52f1f6812..7160c9fd6fe3 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -25,7 +25,6 @@
 #include <linux/vmalloc.h>
 #include <asm/cacheflush.h>
 #include <asm/sizes.h>
-#include <asm/system.h>
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
 #include <asm/page.h>

commit d4cc183f7b9f639a048291e9cd95f0c255664b98
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Mar 23 19:05:18 2011 +0900

    sh: pmb: Use struct syscore_ops instead of sysdevs
    
    This converts the PMB code over to use the new syscore_ops and kills off
    the old sysdev utilization, as per Rafael's example.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index b20b1b3eee4b..fad52f1f6812 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -3,7 +3,7 @@
  *
  * Privileged Space Mapping Buffer (PMB) Support.
  *
- * Copyright (C) 2005 - 2010  Paul Mundt
+ * Copyright (C) 2005 - 2011  Paul Mundt
  * Copyright (C) 2010  Matt Fleming
  *
  * This file is subject to the terms and conditions of the GNU General Public
@@ -12,7 +12,7 @@
  */
 #include <linux/init.h>
 #include <linux/kernel.h>
-#include <linux/sysdev.h>
+#include <linux/syscore_ops.h>
 #include <linux/cpu.h>
 #include <linux/module.h>
 #include <linux/bitops.h>
@@ -874,46 +874,31 @@ static int __init pmb_debugfs_init(void)
 subsys_initcall(pmb_debugfs_init);
 
 #ifdef CONFIG_PM
-static int pmb_sysdev_suspend(struct sys_device *dev, pm_message_t state)
+static void pmb_syscore_resume(void)
 {
-	static pm_message_t prev_state;
+	struct pmb_entry *pmbe;
 	int i;
 
-	/* Restore the PMB after a resume from hibernation */
-	if (state.event == PM_EVENT_ON &&
-	    prev_state.event == PM_EVENT_FREEZE) {
-		struct pmb_entry *pmbe;
-
-		read_lock(&pmb_rwlock);
+	read_lock(&pmb_rwlock);
 
-		for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
-			if (test_bit(i, pmb_map)) {
-				pmbe = &pmb_entry_list[i];
-				set_pmb_entry(pmbe);
-			}
+	for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
+		if (test_bit(i, pmb_map)) {
+			pmbe = &pmb_entry_list[i];
+			set_pmb_entry(pmbe);
 		}
-
-		read_unlock(&pmb_rwlock);
 	}
 
-	prev_state = state;
-
-	return 0;
-}
-
-static int pmb_sysdev_resume(struct sys_device *dev)
-{
-	return pmb_sysdev_suspend(dev, PMSG_ON);
+	read_unlock(&pmb_rwlock);
 }
 
-static struct sysdev_driver pmb_sysdev_driver = {
-	.suspend = pmb_sysdev_suspend,
-	.resume = pmb_sysdev_resume,
+static struct syscore_ops pmb_syscore_ops = {
+	.resume = pmb_syscore_resume,
 };
 
 static int __init pmb_sysdev_init(void)
 {
-	return sysdev_driver_register(&cpu_sysdev_class, &pmb_sysdev_driver);
+	register_syscore_ops(&pmb_syscore_ops);
+	return 0;
 }
 subsys_initcall(pmb_sysdev_init);
 #endif

commit f7fcec93b619337feb9da829b8a9ab6ba86393bc
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Oct 14 03:49:15 2010 +0900

    sh: Fix up PMB locking.
    
    This first converts the PMB locking over to raw spinlocks, and secondly
    fixes up a nested locking issue that was triggering lockdep early on:
    
     swapper/0 is trying to acquire lock:
      (&pmbe->lock){......}, at: [<806be9bc>] pmb_init+0xf4/0x4dc
    
     but task is already holding lock:
      (&pmbe->lock){......}, at: [<806be98e>] pmb_init+0xc6/0x4dc
    
     other info that might help us debug this:
     1 lock held by swapper/0:
      #0:  (&pmbe->lock){......}, at: [<806be98e>] pmb_init+0xc6/0x4dc
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 233c011c4d22..b20b1b3eee4b 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -40,7 +40,7 @@ struct pmb_entry {
 	unsigned long flags;
 	unsigned long size;
 
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	/*
 	 * 0 .. NR_PMB_ENTRIES for specific entry selection, or
@@ -265,7 +265,7 @@ static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 
 	memset(pmbe, 0, sizeof(struct pmb_entry));
 
-	spin_lock_init(&pmbe->lock);
+	raw_spin_lock_init(&pmbe->lock);
 
 	pmbe->vpn	= vpn;
 	pmbe->ppn	= ppn;
@@ -327,9 +327,9 @@ static void set_pmb_entry(struct pmb_entry *pmbe)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&pmbe->lock, flags);
+	raw_spin_lock_irqsave(&pmbe->lock, flags);
 	__set_pmb_entry(pmbe);
-	spin_unlock_irqrestore(&pmbe->lock, flags);
+	raw_spin_unlock_irqrestore(&pmbe->lock, flags);
 }
 #endif /* CONFIG_PM */
 
@@ -368,7 +368,7 @@ int pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,
 				return PTR_ERR(pmbe);
 			}
 
-			spin_lock_irqsave(&pmbe->lock, flags);
+			raw_spin_lock_irqsave(&pmbe->lock, flags);
 
 			pmbe->size = pmb_sizes[i].size;
 
@@ -383,9 +383,10 @@ int pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,
 			 * entries for easier tear-down.
 			 */
 			if (likely(pmbp)) {
-				spin_lock(&pmbp->lock);
+				raw_spin_lock_nested(&pmbp->lock,
+						     SINGLE_DEPTH_NESTING);
 				pmbp->link = pmbe;
-				spin_unlock(&pmbp->lock);
+				raw_spin_unlock(&pmbp->lock);
 			}
 
 			pmbp = pmbe;
@@ -398,7 +399,7 @@ int pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,
 			i--;
 			mapped++;
 
-			spin_unlock_irqrestore(&pmbe->lock, flags);
+			raw_spin_unlock_irqrestore(&pmbe->lock, flags);
 		}
 	} while (size >= SZ_16M);
 
@@ -627,15 +628,14 @@ static void __init pmb_synchronize(void)
 			continue;
 		}
 
-		spin_lock_irqsave(&pmbe->lock, irqflags);
+		raw_spin_lock_irqsave(&pmbe->lock, irqflags);
 
 		for (j = 0; j < ARRAY_SIZE(pmb_sizes); j++)
 			if (pmb_sizes[j].flag == size)
 				pmbe->size = pmb_sizes[j].size;
 
 		if (pmbp) {
-			spin_lock(&pmbp->lock);
-
+			raw_spin_lock_nested(&pmbp->lock, SINGLE_DEPTH_NESTING);
 			/*
 			 * Compare the previous entry against the current one to
 			 * see if the entries span a contiguous mapping. If so,
@@ -644,13 +644,12 @@ static void __init pmb_synchronize(void)
 			 */
 			if (pmb_can_merge(pmbp, pmbe))
 				pmbp->link = pmbe;
-
-			spin_unlock(&pmbp->lock);
+			raw_spin_unlock(&pmbp->lock);
 		}
 
 		pmbp = pmbe;
 
-		spin_unlock_irqrestore(&pmbe->lock, irqflags);
+		raw_spin_unlock_irqrestore(&pmbe->lock, irqflags);
 	}
 }
 
@@ -757,7 +756,7 @@ static void __init pmb_resize(void)
 		/*
 		 * Found it, now resize it.
 		 */
-		spin_lock_irqsave(&pmbe->lock, flags);
+		raw_spin_lock_irqsave(&pmbe->lock, flags);
 
 		pmbe->size = SZ_16M;
 		pmbe->flags &= ~PMB_SZ_MASK;
@@ -767,7 +766,7 @@ static void __init pmb_resize(void)
 
 		__set_pmb_entry(pmbe);
 
-		spin_unlock_irqrestore(&pmbe->lock, flags);
+		raw_spin_unlock_irqrestore(&pmbe->lock, flags);
 	}
 
 	read_unlock(&pmb_rwlock);

commit 3f224f4e057ce67713f3e7a8890f2fbe12d047a5
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Sep 24 04:04:26 2010 +0900

    sh: provide generic arch_debugfs_dir.
    
    While sh previously had its own debugfs root, there now exists a
    common arch_debugfs_dir prototype, so we switch everything over to
    that.  Presumably once more architectures start making use of this
    we'll be able to just kill off the stub kdebugfs wrapper.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 6379091a1647..233c011c4d22 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -866,11 +866,9 @@ static int __init pmb_debugfs_init(void)
 	struct dentry *dentry;
 
 	dentry = debugfs_create_file("pmb", S_IFREG | S_IRUGO,
-				     sh_debugfs_root, NULL, &pmb_debugfs_fops);
+				     arch_debugfs_dir, NULL, &pmb_debugfs_fops);
 	if (!dentry)
 		return -ENOMEM;
-	if (IS_ERR(dentry))
-		return PTR_ERR(dentry);
 
 	return 0;
 }

commit 0e6f989ba83e6fa64e979d3488f01670b8be7959
Author: Julia Lawall <julia@diku.dk>
Date:   Sun Jun 20 11:24:54 2010 +0000

    arch/sh/mm: Eliminate a double lock
    
    The function begins and ends with a read_lock.  The latter is changed to a
    read_unlock.
    
    A simplified version of the semantic match that finds this problem is as
    follows: (http://coccinelle.lip6.fr/)
    
    // <smpl>
    @locked@
    expression E1;
    position p;
    @@
    
    read_lock(E1@p,...);
    
    @r exists@
    expression x <= locked.E1;
    expression locked.E1;
    expression E2;
    identifier lock;
    position locked.p,p1,p2;
    @@
    
    *lock@p1 (E1@p,...);
    ... when != E1
        when != \(x = E2\|&x\)
    *lock@p2 (E1,...);
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Acked-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 18623ba751b3..6379091a1647 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -770,7 +770,7 @@ static void __init pmb_resize(void)
 		spin_unlock_irqrestore(&pmbe->lock, flags);
 	}
 
-	read_lock(&pmb_rwlock);
+	read_unlock(&pmb_rwlock);
 }
 #endif
 

commit dfbca89987b74c34d9b1a2414b0e5ccee65347e0
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue May 11 13:50:29 2010 +0900

    sh: Reject small mappings for PMB bolting.
    
    The minimum section size for the PMB is 16M, so just always error
    out early if the specified size is too small. This permits us to
    unconditionally call in to pmb_bolt_mapping() with variable sizes
    without wasting a TLB and cache flush for the range.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index e9f5384f3f1c..18623ba751b3 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -341,6 +341,8 @@ int pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,
 	unsigned long flags, pmb_flags;
 	int i, mapped;
 
+	if (size < SZ_16M)
+		return -EINVAL;
 	if (!pmb_addr_valid(vaddr, size))
 		return -EFAULT;
 	if (pmb_mapping_exists(vaddr, phys, size))

commit e19553427c2e8fdb04fdd98e407164bb59a840ba
Merge: 35f6cd4a0643 83515bc7df81
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Apr 26 16:08:27 2010 +0900

    Merge branch 'sh/stable-updates'
    
    Conflicts:
            arch/sh/kernel/dwarf.c
            drivers/dma/shdma.c
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

commit c7b03fa0bdc04e00bfbdc4cc69da144b11108f37
Author: Matt Fleming <matt@console-pimps.org>
Date:   Sun Apr 25 17:29:07 2010 +0100

    sh: Do not try merging two 128MB PMB mappings
    
    There is a logic error in pmb_merge() that means we will incorrectly try
    to merge two 128MB PMB mappings into one mapping. However, 256MB isn't a
    valid PMB map size and pmb_merge() will actually drop the second 128MB
    mapping.
    
    This patch allows my SDK7786 board to boot when configured with
    CONFIG_MEMORY_SIZE=0x10000000.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 3cc21933063b..c0fdc217ece5 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -681,7 +681,7 @@ static void __init pmb_merge(struct pmb_entry *head)
 	/*
 	 * The merged page size must be valid.
 	 */
-	if (!pmb_size_valid(newsize))
+	if (!depth || !pmb_size_valid(newsize))
 		return;
 
 	head->flags &= ~PMB_SZ_MASK;

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 3cc21933063b..e43ec600afcf 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -15,7 +15,6 @@
 #include <linux/sysdev.h>
 #include <linux/cpu.h>
 #include <linux/module.h>
-#include <linux/slab.h>
 #include <linux/bitops.h>
 #include <linux/debugfs.h>
 #include <linux/fs.h>

commit 3fe0f36c7edcd20af0a3cafc68bdd62534c0a7f0
Author: Matt Fleming <matt@console-pimps.org>
Date:   Mon Mar 22 22:09:58 2010 +0000

    sh: Fix build after dynamic PMB rework
    
    set_pmb_entry() is now only used by a function that is wrapped in #ifdef
    CONFIG_PM, so wrap set_pmb_entry() in CONFIG_PM too.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 0b14dcf05da7..3cc21933063b 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -323,6 +323,7 @@ static void __clear_pmb_entry(struct pmb_entry *pmbe)
 	writel_uncached(data_val & ~PMB_V, data);
 }
 
+#ifdef CONFIG_PM
 static void set_pmb_entry(struct pmb_entry *pmbe)
 {
 	unsigned long flags;
@@ -331,6 +332,7 @@ static void set_pmb_entry(struct pmb_entry *pmbe)
 	__set_pmb_entry(pmbe);
 	spin_unlock_irqrestore(&pmbe->lock, flags);
 }
+#endif /* CONFIG_PM */
 
 int pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,
 		     unsigned long size, pgprot_t prot)

commit b5b6c7eea1124de5b110a48ac62650a690ed2419
Author: Matt Fleming <matt@console-pimps.org>
Date:   Sun Mar 21 19:51:52 2010 +0000

    sh: Replace unsafe manipulation of MMUCR
    
    Setting the TI in MMUCR causes all the TLB bits in MMUCR to be
    cleared. Unfortunately, the TLB wired bits are also cleared when setting
    the TI bit, causing any wired TLB entries to become unwired.
    
    Use local_flush_tlb_all() which implements TLB flushing in a safer
    manner by using the memory-mapped TLB registers. As each CPU has its own
    PMB the modifications in pmb_init() only affect the local CPU, so only
    flush the local CPU's TLB.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index a4662e2782c3..0b14dcf05da7 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -802,7 +802,7 @@ void __init pmb_init(void)
 	writel_uncached(0, PMB_IRMCR);
 
 	/* Flush out the TLB */
-	__raw_writel(__raw_readl(MMUCR) | MMUCR_TI, MMUCR);
+	local_flush_tlb_all();
 	ctrl_barrier();
 }
 

commit 62c8cbbfc2367e706317f56ac21959120ae72773
Author: Pawel Moll <pawel.moll@st.com>
Date:   Fri Feb 19 10:26:31 2010 +0000

    sh: Move PMB debugfs entry initialization to later stage
    
    ... so the "sh_debugfs_root" is already available. Previously it
    wasn't and in result its path was "/sys/kernel/debug/pmb" instead of
    "/sys/kernel/debug/sh/pmb".
    
    Signed-off-by: Pawel Moll <pawel.moll@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 30035caeb73a..a4662e2782c3 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -871,7 +871,7 @@ static int __init pmb_debugfs_init(void)
 
 	return 0;
 }
-postcore_initcall(pmb_debugfs_init);
+subsys_initcall(pmb_debugfs_init);
 
 #ifdef CONFIG_PM
 static int pmb_sysdev_suspend(struct sys_device *dev, pm_message_t state)

commit 281983d6ff2674ca2e4868de628c65809d84fa4c
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Mar 4 16:44:20 2010 +0900

    sh: fix up MMU reset with variable PMB mapping sizes.
    
    Presently we run in to issues with the MMU resetting the CPU when
    variable sized mappings are employed. This takes a slightly more
    aggressive approach to keeping the TLB and cache state sane before
    establishing the mappings in order to cut down on races observed on
    SMP configurations.
    
    At the same time, we bump the VMA range up to the 0xb000...0xc000 range,
    as there still seems to be some undocumented behaviour in setting up
    variable mappings in the 0xa000...0xb000 range, resulting in reset by the
    TLB.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 75b8861ec624..30035caeb73a 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -24,6 +24,7 @@
 #include <linux/io.h>
 #include <linux/spinlock.h>
 #include <linux/vmalloc.h>
+#include <asm/cacheflush.h>
 #include <asm/sizes.h>
 #include <asm/system.h>
 #include <asm/uaccess.h>
@@ -292,9 +293,18 @@ static void pmb_free(struct pmb_entry *pmbe)
  */
 static void __set_pmb_entry(struct pmb_entry *pmbe)
 {
+	unsigned long addr, data;
+
+	addr = mk_pmb_addr(pmbe->entry);
+	data = mk_pmb_data(pmbe->entry);
+
+	jump_to_uncached();
+
 	/* Set V-bit */
-	__raw_writel(pmbe->ppn | pmbe->flags | PMB_V, mk_pmb_data(pmbe->entry));
-	__raw_writel(pmbe->vpn | PMB_V, mk_pmb_addr(pmbe->entry));
+	__raw_writel(pmbe->vpn | PMB_V, addr);
+	__raw_writel(pmbe->ppn | pmbe->flags | PMB_V, data);
+
+	back_to_cached();
 }
 
 static void __clear_pmb_entry(struct pmb_entry *pmbe)
@@ -326,6 +336,7 @@ int pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,
 		     unsigned long size, pgprot_t prot)
 {
 	struct pmb_entry *pmbp, *pmbe;
+	unsigned long orig_addr, orig_size;
 	unsigned long flags, pmb_flags;
 	int i, mapped;
 
@@ -334,6 +345,11 @@ int pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,
 	if (pmb_mapping_exists(vaddr, phys, size))
 		return 0;
 
+	orig_addr = vaddr;
+	orig_size = size;
+
+	flush_tlb_kernel_range(vaddr, vaddr + size);
+
 	pmb_flags = pgprot_to_pmb_flags(prot);
 	pmbp = NULL;
 
@@ -383,13 +399,15 @@ int pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,
 		}
 	} while (size >= SZ_16M);
 
+	flush_cache_vmap(orig_addr, orig_addr + orig_size);
+
 	return 0;
 }
 
 void __iomem *pmb_remap_caller(phys_addr_t phys, unsigned long size,
 			       pgprot_t prot, void *caller)
 {
-	unsigned long orig_addr, vaddr;
+	unsigned long vaddr;
 	phys_addr_t offset, last_addr;
 	phys_addr_t align_mask;
 	unsigned long aligned;
@@ -417,19 +435,24 @@ void __iomem *pmb_remap_caller(phys_addr_t phys, unsigned long size,
 	phys &= align_mask;
 	aligned = ALIGN(last_addr, pmb_sizes[i].size) - phys;
 
-	area = __get_vm_area_caller(aligned, VM_IOREMAP, uncached_end,
+	/*
+	 * XXX: This should really start from uncached_end, but this
+	 * causes the MMU to reset, so for now we restrict it to the
+	 * 0xb000...0xc000 range.
+	 */
+	area = __get_vm_area_caller(aligned, VM_IOREMAP, 0xb0000000,
 				    P3SEG, caller);
 	if (!area)
 		return NULL;
 
 	area->phys_addr = phys;
-	orig_addr = vaddr = (unsigned long)area->addr;
+	vaddr = (unsigned long)area->addr;
 
 	ret = pmb_bolt_mapping(vaddr, phys, size, prot);
 	if (unlikely(ret != 0))
 		return ERR_PTR(ret);
 
-	return (void __iomem *)(offset + (char *)orig_addr);
+	return (void __iomem *)(offset + (char *)vaddr);
 }
 
 int pmb_unmap(void __iomem *addr)
@@ -477,6 +500,8 @@ static void __pmb_unmap_entry(struct pmb_entry *pmbe, int depth)
 		 */
 		__clear_pmb_entry(pmbe);
 
+		flush_cache_vunmap(pmbe->vpn, pmbe->vpn + pmbe->size);
+
 		pmbe = pmblink->link;
 
 		pmb_free(pmblink);

commit a1042aa248e4ea7f39d5ce13f080cbf3b6c42618
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Mar 3 13:13:25 2010 +0900

    sh: check for existing mappings for bolted PMB entries.
    
    When entries are being bolted unconditionally it's possible that the boot
    loader has established mappings that are within range that we don't want
    to clobber. Perform some basic validation to ensure that the new mapping
    is out of range before allowing the entry setup to take place.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 55d21902d707..75b8861ec624 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -128,13 +128,67 @@ static inline unsigned long pgprot_to_pmb_flags(pgprot_t prot)
 	return pmb_flags;
 }
 
-static bool pmb_can_merge(struct pmb_entry *a, struct pmb_entry *b)
+static inline bool pmb_can_merge(struct pmb_entry *a, struct pmb_entry *b)
 {
 	return (b->vpn == (a->vpn + a->size)) &&
 	       (b->ppn == (a->ppn + a->size)) &&
 	       (b->flags == a->flags);
 }
 
+static bool pmb_mapping_exists(unsigned long vaddr, phys_addr_t phys,
+			       unsigned long size)
+{
+	int i;
+
+	read_lock(&pmb_rwlock);
+
+	for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
+		struct pmb_entry *pmbe, *iter;
+		unsigned long span;
+
+		if (!test_bit(i, pmb_map))
+			continue;
+
+		pmbe = &pmb_entry_list[i];
+
+		/*
+		 * See if VPN and PPN are bounded by an existing mapping.
+		 */
+		if ((vaddr < pmbe->vpn) || (vaddr >= (pmbe->vpn + pmbe->size)))
+			continue;
+		if ((phys < pmbe->ppn) || (phys >= (pmbe->ppn + pmbe->size)))
+			continue;
+
+		/*
+		 * Now see if we're in range of a simple mapping.
+		 */
+		if (size <= pmbe->size) {
+			read_unlock(&pmb_rwlock);
+			return true;
+		}
+
+		span = pmbe->size;
+
+		/*
+		 * Finally for sizes that involve compound mappings, walk
+		 * the chain.
+		 */
+		for (iter = pmbe->link; iter; iter = iter->link)
+			span += iter->size;
+
+		/*
+		 * Nothing else to do if the range requirements are met.
+		 */
+		if (size <= span) {
+			read_unlock(&pmb_rwlock);
+			return true;
+		}
+	}
+
+	read_unlock(&pmb_rwlock);
+	return false;
+}
+
 static bool pmb_size_valid(unsigned long size)
 {
 	int i;
@@ -272,64 +326,62 @@ int pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,
 		     unsigned long size, pgprot_t prot)
 {
 	struct pmb_entry *pmbp, *pmbe;
-	unsigned long pmb_flags;
+	unsigned long flags, pmb_flags;
 	int i, mapped;
 
 	if (!pmb_addr_valid(vaddr, size))
 		return -EFAULT;
+	if (pmb_mapping_exists(vaddr, phys, size))
+		return 0;
 
 	pmb_flags = pgprot_to_pmb_flags(prot);
 	pmbp = NULL;
 
-again:
-	for (i = mapped = 0; i < ARRAY_SIZE(pmb_sizes); i++) {
-		unsigned long flags;
-
-		if (size < pmb_sizes[i].size)
-			continue;
-
-		pmbe = pmb_alloc(vaddr, phys, pmb_flags | pmb_sizes[i].flag,
-				 PMB_NO_ENTRY);
-		if (IS_ERR(pmbe)) {
-			pmb_unmap_entry(pmbp, mapped);
-			return PTR_ERR(pmbe);
-		}
-
-		spin_lock_irqsave(&pmbe->lock, flags);
+	do {
+		for (i = mapped = 0; i < ARRAY_SIZE(pmb_sizes); i++) {
+			if (size < pmb_sizes[i].size)
+				continue;
+
+			pmbe = pmb_alloc(vaddr, phys, pmb_flags |
+					 pmb_sizes[i].flag, PMB_NO_ENTRY);
+			if (IS_ERR(pmbe)) {
+				pmb_unmap_entry(pmbp, mapped);
+				return PTR_ERR(pmbe);
+			}
 
-		pmbe->size = pmb_sizes[i].size;
+			spin_lock_irqsave(&pmbe->lock, flags);
 
-		__set_pmb_entry(pmbe);
+			pmbe->size = pmb_sizes[i].size;
 
-		phys	+= pmbe->size;
-		vaddr	+= pmbe->size;
-		size	-= pmbe->size;
+			__set_pmb_entry(pmbe);
 
-		/*
-		 * Link adjacent entries that span multiple PMB entries
-		 * for easier tear-down.
-		 */
-		if (likely(pmbp)) {
-			spin_lock(&pmbp->lock);
-			pmbp->link = pmbe;
-			spin_unlock(&pmbp->lock);
-		}
+			phys	+= pmbe->size;
+			vaddr	+= pmbe->size;
+			size	-= pmbe->size;
 
-		pmbp = pmbe;
+			/*
+			 * Link adjacent entries that span multiple PMB
+			 * entries for easier tear-down.
+			 */
+			if (likely(pmbp)) {
+				spin_lock(&pmbp->lock);
+				pmbp->link = pmbe;
+				spin_unlock(&pmbp->lock);
+			}
 
-		/*
-		 * Instead of trying smaller sizes on every iteration
-		 * (even if we succeed in allocating space), try using
-		 * pmb_sizes[i].size again.
-		 */
-		i--;
-		mapped++;
+			pmbp = pmbe;
 
-		spin_unlock_irqrestore(&pmbe->lock, flags);
-	}
+			/*
+			 * Instead of trying smaller sizes on every
+			 * iteration (even if we succeed in allocating
+			 * space), try using pmb_sizes[i].size again.
+			 */
+			i--;
+			mapped++;
 
-	if (size >= SZ_16M)
-		goto again;
+			spin_unlock_irqrestore(&pmbe->lock, flags);
+		}
+	} while (size >= SZ_16M);
 
 	return 0;
 }
@@ -374,7 +426,7 @@ void __iomem *pmb_remap_caller(phys_addr_t phys, unsigned long size,
 	orig_addr = vaddr = (unsigned long)area->addr;
 
 	ret = pmb_bolt_mapping(vaddr, phys, size, prot);
-	if (ret != 0)
+	if (unlikely(ret != 0))
 		return ERR_PTR(ret);
 
 	return (void __iomem *)(offset + (char *)orig_addr);

commit 6eb3c735d29e799810ce82118f9260d0044327b7
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Mar 2 17:22:29 2010 +0900

    sh: fixed virt/phys mapping helpers for PMB.
    
    This moves the pmb_remap_caller() mapping logic out in to
    pmb_bolt_mapping(), which enables us to establish fixed mappings in
    places such as the NUMA code.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 9429355c18ca..55d21902d707 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -270,60 +270,19 @@ static void set_pmb_entry(struct pmb_entry *pmbe)
 
 int pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,
 		     unsigned long size, pgprot_t prot)
-{
-	return 0;
-}
-
-void __iomem *pmb_remap_caller(phys_addr_t phys, unsigned long size,
-			       pgprot_t prot, void *caller)
 {
 	struct pmb_entry *pmbp, *pmbe;
 	unsigned long pmb_flags;
 	int i, mapped;
-	unsigned long orig_addr, vaddr;
-	phys_addr_t offset, last_addr;
-	phys_addr_t align_mask;
-	unsigned long aligned;
-	struct vm_struct *area;
 
-	if (!pmb_iomapping_enabled)
-		return NULL;
-
-	/*
-	 * Small mappings need to go through the TLB.
-	 */
-	if (size < SZ_16M)
-		return ERR_PTR(-EINVAL);
-	if (!pmb_prot_valid(prot))
-		return ERR_PTR(-EINVAL);
+	if (!pmb_addr_valid(vaddr, size))
+		return -EFAULT;
 
-	pmbp = NULL;
 	pmb_flags = pgprot_to_pmb_flags(prot);
-	mapped = 0;
-
-	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)
-		if (size >= pmb_sizes[i].size)
-			break;
-
-	last_addr = phys + size;
-	align_mask = ~(pmb_sizes[i].size - 1);
-	offset = phys & ~align_mask;
-	phys &= align_mask;
-	aligned = ALIGN(last_addr, pmb_sizes[i].size) - phys;
-
-	area = __get_vm_area_caller(aligned, VM_IOREMAP, uncached_end,
-				    P3SEG, caller);
-	if (!area)
-		return NULL;
-
-	area->phys_addr = phys;
-	orig_addr = vaddr = (unsigned long)area->addr;
-
-	if (!pmb_addr_valid(vaddr, aligned))
-		return ERR_PTR(-EFAULT);
+	pmbp = NULL;
 
 again:
-	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++) {
+	for (i = mapped = 0; i < ARRAY_SIZE(pmb_sizes); i++) {
 		unsigned long flags;
 
 		if (size < pmb_sizes[i].size)
@@ -333,7 +292,7 @@ void __iomem *pmb_remap_caller(phys_addr_t phys, unsigned long size,
 				 PMB_NO_ENTRY);
 		if (IS_ERR(pmbe)) {
 			pmb_unmap_entry(pmbp, mapped);
-			return pmbe;
+			return PTR_ERR(pmbe);
 		}
 
 		spin_lock_irqsave(&pmbe->lock, flags);
@@ -372,6 +331,52 @@ void __iomem *pmb_remap_caller(phys_addr_t phys, unsigned long size,
 	if (size >= SZ_16M)
 		goto again;
 
+	return 0;
+}
+
+void __iomem *pmb_remap_caller(phys_addr_t phys, unsigned long size,
+			       pgprot_t prot, void *caller)
+{
+	unsigned long orig_addr, vaddr;
+	phys_addr_t offset, last_addr;
+	phys_addr_t align_mask;
+	unsigned long aligned;
+	struct vm_struct *area;
+	int i, ret;
+
+	if (!pmb_iomapping_enabled)
+		return NULL;
+
+	/*
+	 * Small mappings need to go through the TLB.
+	 */
+	if (size < SZ_16M)
+		return ERR_PTR(-EINVAL);
+	if (!pmb_prot_valid(prot))
+		return ERR_PTR(-EINVAL);
+
+	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)
+		if (size >= pmb_sizes[i].size)
+			break;
+
+	last_addr = phys + size;
+	align_mask = ~(pmb_sizes[i].size - 1);
+	offset = phys & ~align_mask;
+	phys &= align_mask;
+	aligned = ALIGN(last_addr, pmb_sizes[i].size) - phys;
+
+	area = __get_vm_area_caller(aligned, VM_IOREMAP, uncached_end,
+				    P3SEG, caller);
+	if (!area)
+		return NULL;
+
+	area->phys_addr = phys;
+	orig_addr = vaddr = (unsigned long)area->addr;
+
+	ret = pmb_bolt_mapping(vaddr, phys, size, prot);
+	if (ret != 0)
+		return ERR_PTR(ret);
+
 	return (void __iomem *)(offset + (char *)orig_addr);
 }
 

commit 4cfa8e75d6854699597e21fd570721d63f899934
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Mar 2 16:49:50 2010 +0900

    sh: make pmb iomapping configurable.
    
    This plugs in an early_param for permitting transparent PMB-backed
    ioremapping to be enabled/disabled. For the time being, we use a
    default-disabled policy.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 9a516b89839a..9429355c18ca 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -68,6 +68,8 @@ static DEFINE_RWLOCK(pmb_rwlock);
 static struct pmb_entry pmb_entry_list[NR_PMB_ENTRIES];
 static DECLARE_BITMAP(pmb_map, NR_PMB_ENTRIES);
 
+static unsigned int pmb_iomapping_enabled;
+
 static __always_inline unsigned long mk_pmb_entry(unsigned int entry)
 {
 	return (entry & PMB_E_MASK) << PMB_E_SHIFT;
@@ -284,6 +286,9 @@ void __iomem *pmb_remap_caller(phys_addr_t phys, unsigned long size,
 	unsigned long aligned;
 	struct vm_struct *area;
 
+	if (!pmb_iomapping_enabled)
+		return NULL;
+
 	/*
 	 * Small mappings need to go through the TLB.
 	 */
@@ -684,6 +689,18 @@ static void __init pmb_resize(void)
 }
 #endif
 
+static int __init early_pmb(char *p)
+{
+	if (!p)
+		return 0;
+
+	if (strstr(p, "iomap"))
+		pmb_iomapping_enabled = 1;
+
+	return 0;
+}
+early_param("pmb", early_pmb);
+
 void __init pmb_init(void)
 {
 	/* Synchronize software state */

commit 90e7d649d86f21d478dc134f74c88e19dd472393
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Feb 23 16:20:53 2010 +0900

    sh: reworked dynamic PMB mapping.
    
    This implements a fairly significant overhaul of the dynamic PMB mapping
    code. The primary change here is that the PMB gets its own VMA that
    follows the uncached mapping and we attempt to be a bit more intelligent
    with dynamic sizing, multi-entry mapping, and so forth.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 35b364f931ea..9a516b89839a 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -23,6 +23,7 @@
 #include <linux/err.h>
 #include <linux/io.h>
 #include <linux/spinlock.h>
+#include <linux/vmalloc.h>
 #include <asm/sizes.h>
 #include <asm/system.h>
 #include <asm/uaccess.h>
@@ -51,6 +52,16 @@ struct pmb_entry {
 	struct pmb_entry *link;
 };
 
+static struct {
+	unsigned long size;
+	int flag;
+} pmb_sizes[] = {
+	{ .size	= SZ_512M, .flag = PMB_SZ_512M, },
+	{ .size = SZ_128M, .flag = PMB_SZ_128M, },
+	{ .size = SZ_64M,  .flag = PMB_SZ_64M,  },
+	{ .size = SZ_16M,  .flag = PMB_SZ_16M,  },
+};
+
 static void pmb_unmap_entry(struct pmb_entry *, int depth);
 
 static DEFINE_RWLOCK(pmb_rwlock);
@@ -72,6 +83,88 @@ static __always_inline unsigned long mk_pmb_data(unsigned int entry)
 	return mk_pmb_entry(entry) | PMB_DATA;
 }
 
+static __always_inline unsigned int pmb_ppn_in_range(unsigned long ppn)
+{
+	return ppn >= __pa(memory_start) && ppn < __pa(memory_end);
+}
+
+/*
+ * Ensure that the PMB entries match our cache configuration.
+ *
+ * When we are in 32-bit address extended mode, CCR.CB becomes
+ * invalid, so care must be taken to manually adjust cacheable
+ * translations.
+ */
+static __always_inline unsigned long pmb_cache_flags(void)
+{
+	unsigned long flags = 0;
+
+#if defined(CONFIG_CACHE_OFF)
+	flags |= PMB_WT | PMB_UB;
+#elif defined(CONFIG_CACHE_WRITETHROUGH)
+	flags |= PMB_C | PMB_WT | PMB_UB;
+#elif defined(CONFIG_CACHE_WRITEBACK)
+	flags |= PMB_C;
+#endif
+
+	return flags;
+}
+
+/*
+ * Convert typical pgprot value to the PMB equivalent
+ */
+static inline unsigned long pgprot_to_pmb_flags(pgprot_t prot)
+{
+	unsigned long pmb_flags = 0;
+	u64 flags = pgprot_val(prot);
+
+	if (flags & _PAGE_CACHABLE)
+		pmb_flags |= PMB_C;
+	if (flags & _PAGE_WT)
+		pmb_flags |= PMB_WT | PMB_UB;
+
+	return pmb_flags;
+}
+
+static bool pmb_can_merge(struct pmb_entry *a, struct pmb_entry *b)
+{
+	return (b->vpn == (a->vpn + a->size)) &&
+	       (b->ppn == (a->ppn + a->size)) &&
+	       (b->flags == a->flags);
+}
+
+static bool pmb_size_valid(unsigned long size)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)
+		if (pmb_sizes[i].size == size)
+			return true;
+
+	return false;
+}
+
+static inline bool pmb_addr_valid(unsigned long addr, unsigned long size)
+{
+	return (addr >= P1SEG && (addr + size - 1) < P3SEG);
+}
+
+static inline bool pmb_prot_valid(pgprot_t prot)
+{
+	return (pgprot_val(prot) & _PAGE_USER) == 0;
+}
+
+static int pmb_size_to_flags(unsigned long size)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)
+		if (pmb_sizes[i].size == size)
+			return pmb_sizes[i].flag;
+
+	return 0;
+}
+
 static int pmb_alloc_entry(void)
 {
 	int pos;
@@ -138,34 +231,14 @@ static void pmb_free(struct pmb_entry *pmbe)
 	pmbe->link	= NULL;
 }
 
-/*
- * Ensure that the PMB entries match our cache configuration.
- *
- * When we are in 32-bit address extended mode, CCR.CB becomes
- * invalid, so care must be taken to manually adjust cacheable
- * translations.
- */
-static __always_inline unsigned long pmb_cache_flags(void)
-{
-	unsigned long flags = 0;
-
-#if defined(CONFIG_CACHE_WRITETHROUGH)
-	flags |= PMB_C | PMB_WT | PMB_UB;
-#elif defined(CONFIG_CACHE_WRITEBACK)
-	flags |= PMB_C;
-#endif
-
-	return flags;
-}
-
 /*
  * Must be run uncached.
  */
 static void __set_pmb_entry(struct pmb_entry *pmbe)
 {
-	writel_uncached(pmbe->vpn | PMB_V, mk_pmb_addr(pmbe->entry));
-	writel_uncached(pmbe->ppn | pmbe->flags | PMB_V,
-			mk_pmb_data(pmbe->entry));
+	/* Set V-bit */
+	__raw_writel(pmbe->ppn | pmbe->flags | PMB_V, mk_pmb_data(pmbe->entry));
+	__raw_writel(pmbe->vpn | PMB_V, mk_pmb_addr(pmbe->entry));
 }
 
 static void __clear_pmb_entry(struct pmb_entry *pmbe)
@@ -193,39 +266,56 @@ static void set_pmb_entry(struct pmb_entry *pmbe)
 	spin_unlock_irqrestore(&pmbe->lock, flags);
 }
 
-static struct {
-	unsigned long size;
-	int flag;
-} pmb_sizes[] = {
-	{ .size	= SZ_512M, .flag = PMB_SZ_512M, },
-	{ .size = SZ_128M, .flag = PMB_SZ_128M, },
-	{ .size = SZ_64M,  .flag = PMB_SZ_64M,  },
-	{ .size = SZ_16M,  .flag = PMB_SZ_16M,  },
-};
+int pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,
+		     unsigned long size, pgprot_t prot)
+{
+	return 0;
+}
 
-long pmb_remap(unsigned long vaddr, unsigned long phys,
-	       unsigned long size, pgprot_t prot)
+void __iomem *pmb_remap_caller(phys_addr_t phys, unsigned long size,
+			       pgprot_t prot, void *caller)
 {
 	struct pmb_entry *pmbp, *pmbe;
-	unsigned long wanted;
-	int pmb_flags, i;
-	long err;
-	u64 flags;
+	unsigned long pmb_flags;
+	int i, mapped;
+	unsigned long orig_addr, vaddr;
+	phys_addr_t offset, last_addr;
+	phys_addr_t align_mask;
+	unsigned long aligned;
+	struct vm_struct *area;
 
-	flags = pgprot_val(prot);
+	/*
+	 * Small mappings need to go through the TLB.
+	 */
+	if (size < SZ_16M)
+		return ERR_PTR(-EINVAL);
+	if (!pmb_prot_valid(prot))
+		return ERR_PTR(-EINVAL);
 
-	pmb_flags = PMB_WT | PMB_UB;
+	pmbp = NULL;
+	pmb_flags = pgprot_to_pmb_flags(prot);
+	mapped = 0;
 
-	/* Convert typical pgprot value to the PMB equivalent */
-	if (flags & _PAGE_CACHABLE) {
-		pmb_flags |= PMB_C;
+	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)
+		if (size >= pmb_sizes[i].size)
+			break;
 
-		if ((flags & _PAGE_WT) == 0)
-			pmb_flags &= ~(PMB_WT | PMB_UB);
-	}
+	last_addr = phys + size;
+	align_mask = ~(pmb_sizes[i].size - 1);
+	offset = phys & ~align_mask;
+	phys &= align_mask;
+	aligned = ALIGN(last_addr, pmb_sizes[i].size) - phys;
 
-	pmbp = NULL;
-	wanted = size;
+	area = __get_vm_area_caller(aligned, VM_IOREMAP, uncached_end,
+				    P3SEG, caller);
+	if (!area)
+		return NULL;
+
+	area->phys_addr = phys;
+	orig_addr = vaddr = (unsigned long)area->addr;
+
+	if (!pmb_addr_valid(vaddr, aligned))
+		return ERR_PTR(-EFAULT);
 
 again:
 	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++) {
@@ -237,19 +327,19 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 		pmbe = pmb_alloc(vaddr, phys, pmb_flags | pmb_sizes[i].flag,
 				 PMB_NO_ENTRY);
 		if (IS_ERR(pmbe)) {
-			err = PTR_ERR(pmbe);
-			goto out;
+			pmb_unmap_entry(pmbp, mapped);
+			return pmbe;
 		}
 
 		spin_lock_irqsave(&pmbe->lock, flags);
 
+		pmbe->size = pmb_sizes[i].size;
+
 		__set_pmb_entry(pmbe);
 
-		phys	+= pmb_sizes[i].size;
-		vaddr	+= pmb_sizes[i].size;
-		size	-= pmb_sizes[i].size;
-
-		pmbe->size = pmb_sizes[i].size;
+		phys	+= pmbe->size;
+		vaddr	+= pmbe->size;
+		size	-= pmbe->size;
 
 		/*
 		 * Link adjacent entries that span multiple PMB entries
@@ -269,6 +359,7 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 		 * pmb_sizes[i].size again.
 		 */
 		i--;
+		mapped++;
 
 		spin_unlock_irqrestore(&pmbe->lock, flags);
 	}
@@ -276,61 +367,35 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 	if (size >= SZ_16M)
 		goto again;
 
-	return wanted - size;
-
-out:
-	pmb_unmap_entry(pmbp, NR_PMB_ENTRIES);
-
-	return err;
+	return (void __iomem *)(offset + (char *)orig_addr);
 }
 
-void pmb_unmap(unsigned long addr)
+int pmb_unmap(void __iomem *addr)
 {
 	struct pmb_entry *pmbe = NULL;
-	int i;
+	unsigned long vaddr = (unsigned long __force)addr;
+	int i, found = 0;
 
 	read_lock(&pmb_rwlock);
 
 	for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
 		if (test_bit(i, pmb_map)) {
 			pmbe = &pmb_entry_list[i];
-			if (pmbe->vpn == addr)
+			if (pmbe->vpn == vaddr) {
+				found = 1;
 				break;
+			}
 		}
 	}
 
 	read_unlock(&pmb_rwlock);
 
-	pmb_unmap_entry(pmbe, NR_PMB_ENTRIES);
-}
-
-static bool pmb_can_merge(struct pmb_entry *a, struct pmb_entry *b)
-{
-	return (b->vpn == (a->vpn + a->size)) &&
-	       (b->ppn == (a->ppn + a->size)) &&
-	       (b->flags == a->flags);
-}
-
-static bool pmb_size_valid(unsigned long size)
-{
-	int i;
-
-	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)
-		if (pmb_sizes[i].size == size)
-			return true;
-
-	return false;
-}
-
-static int pmb_size_to_flags(unsigned long size)
-{
-	int i;
-
-	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)
-		if (pmb_sizes[i].size == size)
-			return pmb_sizes[i].flag;
+	if (found) {
+		pmb_unmap_entry(pmbe, NR_PMB_ENTRIES);
+		return 0;
+	}
 
-	return 0;
+	return -EINVAL;
 }
 
 static void __pmb_unmap_entry(struct pmb_entry *pmbe, int depth)
@@ -368,11 +433,6 @@ static void pmb_unmap_entry(struct pmb_entry *pmbe, int depth)
 	write_unlock_irqrestore(&pmb_rwlock, flags);
 }
 
-static __always_inline unsigned int pmb_ppn_in_range(unsigned long ppn)
-{
-	return ppn >= __pa(memory_start) && ppn < __pa(memory_end);
-}
-
 static void __init pmb_notify(void)
 {
 	int i;

commit 4b62c0f1e76fe3327b695c49195af8b58e4da057
Author: Robert P. J. Day <rpjday@crashcourse.ca>
Date:   Sat Feb 27 18:35:08 2010 +0000

    sh: No need to explicitly include <linux/rwlock.h>.
    
    Since <linux/spinlock.h> already includes <linux/rwlock.h>, and the
    latter file will warn about not having included the former file
    anyway, there is no value in including rwlock.h explicitly.
    
    Signed-off-by: Robert P. J. Day <rpjday@crashcourse.ca>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 198bcff5e96f..35b364f931ea 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -23,7 +23,6 @@
 #include <linux/err.h>
 #include <linux/io.h>
 #include <linux/spinlock.h>
-#include <linux/rwlock.h>
 #include <asm/sizes.h>
 #include <asm/system.h>
 #include <asm/uaccess.h>

commit d01447b3197c2c470a14666be2c640407bbbfec7
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Feb 18 18:13:51 2010 +0900

    sh: Merge legacy and dynamic PMB modes.
    
    This implements a bit of rework for the PMB code, which permits us to
    kill off the legacy PMB mode completely. Rather than trusting the boot
    loader to do the right thing, we do a quick verification of the PMB
    contents to determine whether to have the kernel setup the initial
    mappings or whether it needs to mangle them later on instead.
    
    If we're booting from legacy mappings, the kernel will now take control
    of them and make them match the kernel's initial mapping configuration.
    This is accomplished by breaking the initialization phase out in to
    multiple steps: synchronization, merging, and resizing. With the recent
    rework, the synchronization code establishes page links for compound
    mappings already, so we build on top of this for promoting mappings and
    reclaiming unused slots.
    
    At the same time, the changes introduced for the uncached helpers also
    permit us to dynamically resize the uncached mapping without any
    particular headaches. The smallest page size is more than sufficient for
    mapping all of kernel text, and as we're careful not to jump to any far
    off locations in the setup code the mapping can safely be resized
    regardless of whether we are executing from it or not.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index b9d5476e1284..198bcff5e96f 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -52,7 +52,7 @@ struct pmb_entry {
 	struct pmb_entry *link;
 };
 
-static void pmb_unmap_entry(struct pmb_entry *);
+static void pmb_unmap_entry(struct pmb_entry *, int depth);
 
 static DEFINE_RWLOCK(pmb_rwlock);
 static struct pmb_entry pmb_entry_list[NR_PMB_ENTRIES];
@@ -115,13 +115,14 @@ static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 
 	pmbe = &pmb_entry_list[pos];
 
+	memset(pmbe, 0, sizeof(struct pmb_entry));
+
 	spin_lock_init(&pmbe->lock);
 
 	pmbe->vpn	= vpn;
 	pmbe->ppn	= ppn;
 	pmbe->flags	= flags;
 	pmbe->entry	= pos;
-	pmbe->size	= 0;
 
 	return pmbe;
 
@@ -133,7 +134,9 @@ static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 static void pmb_free(struct pmb_entry *pmbe)
 {
 	__clear_bit(pmbe->entry, pmb_map);
-	pmbe->entry = PMB_NO_ENTRY;
+
+	pmbe->entry	= PMB_NO_ENTRY;
+	pmbe->link	= NULL;
 }
 
 /*
@@ -161,9 +164,6 @@ static __always_inline unsigned long pmb_cache_flags(void)
  */
 static void __set_pmb_entry(struct pmb_entry *pmbe)
 {
-	pmbe->flags &= ~PMB_CACHE_MASK;
-	pmbe->flags |= pmb_cache_flags();
-
 	writel_uncached(pmbe->vpn | PMB_V, mk_pmb_addr(pmbe->entry));
 	writel_uncached(pmbe->ppn | pmbe->flags | PMB_V,
 			mk_pmb_data(pmbe->entry));
@@ -280,7 +280,7 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 	return wanted - size;
 
 out:
-	pmb_unmap_entry(pmbp);
+	pmb_unmap_entry(pmbp, NR_PMB_ENTRIES);
 
 	return err;
 }
@@ -302,18 +302,40 @@ void pmb_unmap(unsigned long addr)
 
 	read_unlock(&pmb_rwlock);
 
-	pmb_unmap_entry(pmbe);
+	pmb_unmap_entry(pmbe, NR_PMB_ENTRIES);
 }
 
-static void pmb_unmap_entry(struct pmb_entry *pmbe)
+static bool pmb_can_merge(struct pmb_entry *a, struct pmb_entry *b)
 {
-	unsigned long flags;
+	return (b->vpn == (a->vpn + a->size)) &&
+	       (b->ppn == (a->ppn + a->size)) &&
+	       (b->flags == a->flags);
+}
 
-	if (unlikely(!pmbe))
-		return;
+static bool pmb_size_valid(unsigned long size)
+{
+	int i;
 
-	write_lock_irqsave(&pmb_rwlock, flags);
+	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)
+		if (pmb_sizes[i].size == size)
+			return true;
+
+	return false;
+}
+
+static int pmb_size_to_flags(unsigned long size)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)
+		if (pmb_sizes[i].size == size)
+			return pmb_sizes[i].flag;
 
+	return 0;
+}
+
+static void __pmb_unmap_entry(struct pmb_entry *pmbe, int depth)
+{
 	do {
 		struct pmb_entry *pmblink = pmbe;
 
@@ -332,8 +354,18 @@ static void pmb_unmap_entry(struct pmb_entry *pmbe)
 		pmbe = pmblink->link;
 
 		pmb_free(pmblink);
-	} while (pmbe);
+	} while (pmbe && --depth);
+}
+
+static void pmb_unmap_entry(struct pmb_entry *pmbe, int depth)
+{
+	unsigned long flags;
 
+	if (unlikely(!pmbe))
+		return;
+
+	write_lock_irqsave(&pmb_rwlock, flags);
+	__pmb_unmap_entry(pmbe, depth);
 	write_unlock_irqrestore(&pmb_rwlock, flags);
 }
 
@@ -342,14 +374,40 @@ static __always_inline unsigned int pmb_ppn_in_range(unsigned long ppn)
 	return ppn >= __pa(memory_start) && ppn < __pa(memory_end);
 }
 
-static int pmb_synchronize_mappings(void)
+static void __init pmb_notify(void)
 {
-	unsigned int applied = 0;
-	struct pmb_entry *pmbp = NULL;
-	int i, j;
+	int i;
 
 	pr_info("PMB: boot mappings:\n");
 
+	read_lock(&pmb_rwlock);
+
+	for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
+		struct pmb_entry *pmbe;
+
+		if (!test_bit(i, pmb_map))
+			continue;
+
+		pmbe = &pmb_entry_list[i];
+
+		pr_info("       0x%08lx -> 0x%08lx [ %4ldMB %2scached ]\n",
+			pmbe->vpn >> PAGE_SHIFT, pmbe->ppn >> PAGE_SHIFT,
+			pmbe->size >> 20, (pmbe->flags & PMB_C) ? "" : "un");
+	}
+
+	read_unlock(&pmb_rwlock);
+}
+
+/*
+ * Sync our software copy of the PMB mappings with those in hardware. The
+ * mappings in the hardware PMB were either set up by the bootloader or
+ * very early on by the kernel.
+ */
+static void __init pmb_synchronize(void)
+{
+	struct pmb_entry *pmbp = NULL;
+	int i, j;
+
 	/*
 	 * Run through the initial boot mappings, log the established
 	 * ones, and blow away anything that falls outside of the valid
@@ -432,10 +490,10 @@ static int pmb_synchronize_mappings(void)
 			/*
 			 * Compare the previous entry against the current one to
 			 * see if the entries span a contiguous mapping. If so,
-			 * setup the entry links accordingly.
+			 * setup the entry links accordingly. Compound mappings
+			 * are later coalesced.
 			 */
-			if ((pmbe->vpn == (pmbp->vpn + pmbp->size)) &&
-			    (pmbe->ppn == (pmbp->ppn + pmbp->size)))
+			if (pmb_can_merge(pmbp, pmbe))
 				pmbp->link = pmbe;
 
 			spin_unlock(&pmbp->lock);
@@ -444,37 +502,150 @@ static int pmb_synchronize_mappings(void)
 		pmbp = pmbe;
 
 		spin_unlock_irqrestore(&pmbe->lock, irqflags);
+	}
+}
 
-		pr_info("\t0x%08lx -> 0x%08lx [ %ldMB %scached ]\n",
-			vpn >> PAGE_SHIFT, ppn >> PAGE_SHIFT, pmbe->size >> 20,
-			(data_val & PMB_C) ? "" : "un");
+static void __init pmb_merge(struct pmb_entry *head)
+{
+	unsigned long span, newsize;
+	struct pmb_entry *tail;
+	int i = 1, depth = 0;
+
+	span = newsize = head->size;
 
-		applied++;
+	tail = head->link;
+	while (tail) {
+		span += tail->size;
+
+		if (pmb_size_valid(span)) {
+			newsize = span;
+			depth = i;
+		}
+
+		/* This is the end of the line.. */
+		if (!tail->link)
+			break;
+
+		tail = tail->link;
+		i++;
 	}
 
-	return (applied == 0);
+	/*
+	 * The merged page size must be valid.
+	 */
+	if (!pmb_size_valid(newsize))
+		return;
+
+	head->flags &= ~PMB_SZ_MASK;
+	head->flags |= pmb_size_to_flags(newsize);
+
+	head->size = newsize;
+
+	__pmb_unmap_entry(head->link, depth);
+	__set_pmb_entry(head);
 }
 
-int pmb_init(void)
+static void __init pmb_coalesce(void)
 {
-	int ret;
+	unsigned long flags;
+	int i;
+
+	write_lock_irqsave(&pmb_rwlock, flags);
+
+	for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
+		struct pmb_entry *pmbe;
+
+		if (!test_bit(i, pmb_map))
+			continue;
+
+		pmbe = &pmb_entry_list[i];
+
+		/*
+		 * We're only interested in compound mappings
+		 */
+		if (!pmbe->link)
+			continue;
+
+		/*
+		 * Nothing to do if it already uses the largest possible
+		 * page size.
+		 */
+		if (pmbe->size == SZ_512M)
+			continue;
+
+		pmb_merge(pmbe);
+	}
+
+	write_unlock_irqrestore(&pmb_rwlock, flags);
+}
+
+#ifdef CONFIG_UNCACHED_MAPPING
+static void __init pmb_resize(void)
+{
+	int i;
 
 	/*
-	 * Sync our software copy of the PMB mappings with those in
-	 * hardware. The mappings in the hardware PMB were either set up
-	 * by the bootloader or very early on by the kernel.
+	 * If the uncached mapping was constructed by the kernel, it will
+	 * already be a reasonable size.
 	 */
-	ret = pmb_synchronize_mappings();
-	if (unlikely(ret == 0))
-		return 0;
+	if (uncached_size == SZ_16M)
+		return;
+
+	read_lock(&pmb_rwlock);
+
+	for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
+		struct pmb_entry *pmbe;
+		unsigned long flags;
+
+		if (!test_bit(i, pmb_map))
+			continue;
+
+		pmbe = &pmb_entry_list[i];
+
+		if (pmbe->vpn != uncached_start)
+			continue;
+
+		/*
+		 * Found it, now resize it.
+		 */
+		spin_lock_irqsave(&pmbe->lock, flags);
+
+		pmbe->size = SZ_16M;
+		pmbe->flags &= ~PMB_SZ_MASK;
+		pmbe->flags |= pmb_size_to_flags(pmbe->size);
+
+		uncached_resize(pmbe->size);
+
+		__set_pmb_entry(pmbe);
+
+		spin_unlock_irqrestore(&pmbe->lock, flags);
+	}
+
+	read_lock(&pmb_rwlock);
+}
+#endif
+
+void __init pmb_init(void)
+{
+	/* Synchronize software state */
+	pmb_synchronize();
+
+	/* Attempt to combine compound mappings */
+	pmb_coalesce();
+
+#ifdef CONFIG_UNCACHED_MAPPING
+	/* Resize initial mappings, if necessary */
+	pmb_resize();
+#endif
+
+	/* Log them */
+	pmb_notify();
 
 	writel_uncached(0, PMB_IRMCR);
 
 	/* Flush out the TLB */
 	__raw_writel(__raw_readl(MMUCR) | MMUCR_TI, MMUCR);
 	ctrl_barrier();
-
-	return 0;
 }
 
 bool __in_29bit_mode(void)

commit 2e450643d70b62e0192577681b227d7d5d2efa45
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Feb 18 13:26:05 2010 +0900

    sh: Use uncached I/O helpers in PMB setup.
    
    The PMB code is an example of something that spends an absurd amount of
    time running uncached when only a couple of operations really need to be.
    This switches over to the shiny new uncached helpers, permitting us to
    spend far more time running cached.
    
    Additionally, MMUCR twiddling is perfectly safe from cached space given
    that it's paired with a control register barrier, so fix that up, too.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index e65e8b8e2a5e..b9d5476e1284 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -161,32 +161,28 @@ static __always_inline unsigned long pmb_cache_flags(void)
  */
 static void __set_pmb_entry(struct pmb_entry *pmbe)
 {
-	jump_to_uncached();
-
 	pmbe->flags &= ~PMB_CACHE_MASK;
 	pmbe->flags |= pmb_cache_flags();
 
-	__raw_writel(pmbe->vpn | PMB_V, mk_pmb_addr(pmbe->entry));
-	__raw_writel(pmbe->ppn | pmbe->flags | PMB_V, mk_pmb_data(pmbe->entry));
-
-	back_to_cached();
+	writel_uncached(pmbe->vpn | PMB_V, mk_pmb_addr(pmbe->entry));
+	writel_uncached(pmbe->ppn | pmbe->flags | PMB_V,
+			mk_pmb_data(pmbe->entry));
 }
 
 static void __clear_pmb_entry(struct pmb_entry *pmbe)
 {
-	unsigned int entry = pmbe->entry;
-	unsigned long addr;
+	unsigned long addr, data;
+	unsigned long addr_val, data_val;
 
-	jump_to_uncached();
+	addr = mk_pmb_addr(pmbe->entry);
+	data = mk_pmb_data(pmbe->entry);
 
-	/* Clear V-bit */
-	addr = mk_pmb_addr(entry);
-	__raw_writel(__raw_readl(addr) & ~PMB_V, addr);
+	addr_val = __raw_readl(addr);
+	data_val = __raw_readl(data);
 
-	addr = mk_pmb_data(entry);
-	__raw_writel(__raw_readl(addr) & ~PMB_V, addr);
-
-	back_to_cached();
+	/* Clear V-bit */
+	writel_uncached(addr_val & ~PMB_V, addr);
+	writel_uncached(data_val & ~PMB_V, data);
 }
 
 static void set_pmb_entry(struct pmb_entry *pmbe)
@@ -400,8 +396,8 @@ static int pmb_synchronize_mappings(void)
 			/*
 			 * Invalidate anything out of bounds.
 			 */
-			__raw_writel(addr_val & ~PMB_V, addr);
-			__raw_writel(data_val & ~PMB_V, data);
+			writel_uncached(addr_val & ~PMB_V, addr);
+			writel_uncached(data_val & ~PMB_V, data);
 			continue;
 		}
 
@@ -411,7 +407,8 @@ static int pmb_synchronize_mappings(void)
 		if (data_val & PMB_C) {
 			data_val &= ~PMB_CACHE_MASK;
 			data_val |= pmb_cache_flags();
-			__raw_writel(data_val, data);
+
+			writel_uncached(data_val, data);
 		}
 
 		size = data_val & PMB_SZ_MASK;
@@ -462,25 +459,20 @@ int pmb_init(void)
 {
 	int ret;
 
-	jump_to_uncached();
-
 	/*
 	 * Sync our software copy of the PMB mappings with those in
 	 * hardware. The mappings in the hardware PMB were either set up
 	 * by the bootloader or very early on by the kernel.
 	 */
 	ret = pmb_synchronize_mappings();
-	if (unlikely(ret == 0)) {
-		back_to_cached();
+	if (unlikely(ret == 0))
 		return 0;
-	}
 
-	__raw_writel(0, PMB_IRMCR);
+	writel_uncached(0, PMB_IRMCR);
 
 	/* Flush out the TLB */
 	__raw_writel(__raw_readl(MMUCR) | MMUCR_TI, MMUCR);
-
-	back_to_cached();
+	ctrl_barrier();
 
 	return 0;
 }

commit d53a0d33bc3a50ea0e8dd1680a2e8435770b162a
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Feb 17 21:17:02 2010 +0900

    sh: PMB locking overhaul.
    
    This implements some locking for the PMB code. A high level rwlock is
    added for dealing with rw accesses on the entry map while a per-entry
    data structure spinlock is added to deal with the PMB entry changing out
    from underneath us.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index cb808a8aaffc..e65e8b8e2a5e 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -22,6 +22,8 @@
 #include <linux/seq_file.h>
 #include <linux/err.h>
 #include <linux/io.h>
+#include <linux/spinlock.h>
+#include <linux/rwlock.h>
 #include <asm/sizes.h>
 #include <asm/system.h>
 #include <asm/uaccess.h>
@@ -30,8 +32,29 @@
 #include <asm/mmu.h>
 #include <asm/mmu_context.h>
 
+struct pmb_entry;
+
+struct pmb_entry {
+	unsigned long vpn;
+	unsigned long ppn;
+	unsigned long flags;
+	unsigned long size;
+
+	spinlock_t lock;
+
+	/*
+	 * 0 .. NR_PMB_ENTRIES for specific entry selection, or
+	 * PMB_NO_ENTRY to search for a free one
+	 */
+	int entry;
+
+	/* Adjacent entry link for contiguous multi-entry mappings */
+	struct pmb_entry *link;
+};
+
 static void pmb_unmap_entry(struct pmb_entry *);
 
+static DEFINE_RWLOCK(pmb_rwlock);
 static struct pmb_entry pmb_entry_list[NR_PMB_ENTRIES];
 static DECLARE_BITMAP(pmb_map, NR_PMB_ENTRIES);
 
@@ -52,16 +75,13 @@ static __always_inline unsigned long mk_pmb_data(unsigned int entry)
 
 static int pmb_alloc_entry(void)
 {
-	unsigned int pos;
+	int pos;
 
-repeat:
 	pos = find_first_zero_bit(pmb_map, NR_PMB_ENTRIES);
-
-	if (unlikely(pos > NR_PMB_ENTRIES))
-		return -ENOSPC;
-
-	if (test_and_set_bit(pos, pmb_map))
-		goto repeat;
+	if (pos >= 0 && pos < NR_PMB_ENTRIES)
+		__set_bit(pos, pmb_map);
+	else
+		pos = -ENOSPC;
 
 	return pos;
 }
@@ -70,21 +90,32 @@ static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 				   unsigned long flags, int entry)
 {
 	struct pmb_entry *pmbe;
+	unsigned long irqflags;
+	void *ret = NULL;
 	int pos;
 
+	write_lock_irqsave(&pmb_rwlock, irqflags);
+
 	if (entry == PMB_NO_ENTRY) {
 		pos = pmb_alloc_entry();
-		if (pos < 0)
-			return ERR_PTR(pos);
+		if (unlikely(pos < 0)) {
+			ret = ERR_PTR(pos);
+			goto out;
+		}
 	} else {
-		if (test_and_set_bit(entry, pmb_map))
-			return ERR_PTR(-ENOSPC);
+		if (__test_and_set_bit(entry, pmb_map)) {
+			ret = ERR_PTR(-ENOSPC);
+			goto out;
+		}
+
 		pos = entry;
 	}
 
+	write_unlock_irqrestore(&pmb_rwlock, irqflags);
+
 	pmbe = &pmb_entry_list[pos];
-	if (!pmbe)
-		return ERR_PTR(-ENOMEM);
+
+	spin_lock_init(&pmbe->lock);
 
 	pmbe->vpn	= vpn;
 	pmbe->ppn	= ppn;
@@ -93,11 +124,15 @@ static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 	pmbe->size	= 0;
 
 	return pmbe;
+
+out:
+	write_unlock_irqrestore(&pmb_rwlock, irqflags);
+	return ret;
 }
 
 static void pmb_free(struct pmb_entry *pmbe)
 {
-	clear_bit(pmbe->entry, pmb_map);
+	__clear_bit(pmbe->entry, pmb_map);
 	pmbe->entry = PMB_NO_ENTRY;
 }
 
@@ -124,7 +159,7 @@ static __always_inline unsigned long pmb_cache_flags(void)
 /*
  * Must be run uncached.
  */
-static void set_pmb_entry(struct pmb_entry *pmbe)
+static void __set_pmb_entry(struct pmb_entry *pmbe)
 {
 	jump_to_uncached();
 
@@ -137,7 +172,7 @@ static void set_pmb_entry(struct pmb_entry *pmbe)
 	back_to_cached();
 }
 
-static void clear_pmb_entry(struct pmb_entry *pmbe)
+static void __clear_pmb_entry(struct pmb_entry *pmbe)
 {
 	unsigned int entry = pmbe->entry;
 	unsigned long addr;
@@ -154,6 +189,15 @@ static void clear_pmb_entry(struct pmb_entry *pmbe)
 	back_to_cached();
 }
 
+static void set_pmb_entry(struct pmb_entry *pmbe)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&pmbe->lock, flags);
+	__set_pmb_entry(pmbe);
+	spin_unlock_irqrestore(&pmbe->lock, flags);
+}
+
 static struct {
 	unsigned long size;
 	int flag;
@@ -190,6 +234,8 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 
 again:
 	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++) {
+		unsigned long flags;
+
 		if (size < pmb_sizes[i].size)
 			continue;
 
@@ -200,7 +246,9 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 			goto out;
 		}
 
-		set_pmb_entry(pmbe);
+		spin_lock_irqsave(&pmbe->lock, flags);
+
+		__set_pmb_entry(pmbe);
 
 		phys	+= pmb_sizes[i].size;
 		vaddr	+= pmb_sizes[i].size;
@@ -212,8 +260,11 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 		 * Link adjacent entries that span multiple PMB entries
 		 * for easier tear-down.
 		 */
-		if (likely(pmbp))
+		if (likely(pmbp)) {
+			spin_lock(&pmbp->lock);
 			pmbp->link = pmbe;
+			spin_unlock(&pmbp->lock);
+		}
 
 		pmbp = pmbe;
 
@@ -223,9 +274,11 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 		 * pmb_sizes[i].size again.
 		 */
 		i--;
+
+		spin_unlock_irqrestore(&pmbe->lock, flags);
 	}
 
-	if (size >= 0x1000000)
+	if (size >= SZ_16M)
 		goto again;
 
 	return wanted - size;
@@ -238,29 +291,32 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 
 void pmb_unmap(unsigned long addr)
 {
-	struct pmb_entry *pmbe;
+	struct pmb_entry *pmbe = NULL;
 	int i;
 
+	read_lock(&pmb_rwlock);
+
 	for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
 		if (test_bit(i, pmb_map)) {
 			pmbe = &pmb_entry_list[i];
-			if (pmbe->vpn == addr) {
-				pmb_unmap_entry(pmbe);
+			if (pmbe->vpn == addr)
 				break;
-			}
 		}
 	}
+
+	read_unlock(&pmb_rwlock);
+
+	pmb_unmap_entry(pmbe);
 }
 
 static void pmb_unmap_entry(struct pmb_entry *pmbe)
 {
+	unsigned long flags;
+
 	if (unlikely(!pmbe))
 		return;
 
-	if (!test_bit(pmbe->entry, pmb_map)) {
-		WARN_ON(1);
-		return;
-	}
+	write_lock_irqsave(&pmb_rwlock, flags);
 
 	do {
 		struct pmb_entry *pmblink = pmbe;
@@ -272,15 +328,17 @@ static void pmb_unmap_entry(struct pmb_entry *pmbe)
 		 * this entry in pmb_alloc() (even if we haven't filled
 		 * it yet).
 		 *
-		 * Therefore, calling clear_pmb_entry() is safe as no
+		 * Therefore, calling __clear_pmb_entry() is safe as no
 		 * other mapping can be using that slot.
 		 */
-		clear_pmb_entry(pmbe);
+		__clear_pmb_entry(pmbe);
 
 		pmbe = pmblink->link;
 
 		pmb_free(pmblink);
 	} while (pmbe);
+
+	write_unlock_irqrestore(&pmb_rwlock, flags);
 }
 
 static __always_inline unsigned int pmb_ppn_in_range(unsigned long ppn)
@@ -316,6 +374,7 @@ static int pmb_synchronize_mappings(void)
 		unsigned long addr, data;
 		unsigned long addr_val, data_val;
 		unsigned long ppn, vpn, flags;
+		unsigned long irqflags;
 		unsigned int size;
 		struct pmb_entry *pmbe;
 
@@ -364,21 +423,31 @@ static int pmb_synchronize_mappings(void)
 			continue;
 		}
 
+		spin_lock_irqsave(&pmbe->lock, irqflags);
+
 		for (j = 0; j < ARRAY_SIZE(pmb_sizes); j++)
 			if (pmb_sizes[j].flag == size)
 				pmbe->size = pmb_sizes[j].size;
 
-		/*
-		 * Compare the previous entry against the current one to
-		 * see if the entries span a contiguous mapping. If so,
-		 * setup the entry links accordingly.
-		 */
-		if (pmbp && ((pmbe->vpn == (pmbp->vpn + pmbp->size)) &&
-			     (pmbe->ppn == (pmbp->ppn + pmbp->size))))
-			pmbp->link = pmbe;
+		if (pmbp) {
+			spin_lock(&pmbp->lock);
+
+			/*
+			 * Compare the previous entry against the current one to
+			 * see if the entries span a contiguous mapping. If so,
+			 * setup the entry links accordingly.
+			 */
+			if ((pmbe->vpn == (pmbp->vpn + pmbp->size)) &&
+			    (pmbe->ppn == (pmbp->ppn + pmbp->size)))
+				pmbp->link = pmbe;
+
+			spin_unlock(&pmbp->lock);
+		}
 
 		pmbp = pmbe;
 
+		spin_unlock_irqrestore(&pmbe->lock, irqflags);
+
 		pr_info("\t0x%08lx -> 0x%08lx [ %ldMB %scached ]\n",
 			vpn >> PAGE_SHIFT, ppn >> PAGE_SHIFT, pmbe->size >> 20,
 			(data_val & PMB_C) ? "" : "un");
@@ -493,14 +562,21 @@ static int pmb_sysdev_suspend(struct sys_device *dev, pm_message_t state)
 	if (state.event == PM_EVENT_ON &&
 	    prev_state.event == PM_EVENT_FREEZE) {
 		struct pmb_entry *pmbe;
+
+		read_lock(&pmb_rwlock);
+
 		for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
 			if (test_bit(i, pmb_map)) {
 				pmbe = &pmb_entry_list[i];
 				set_pmb_entry(pmbe);
 			}
 		}
+
+		read_unlock(&pmb_rwlock);
 	}
+
 	prev_state = state;
+
 	return 0;
 }
 

commit 0065b96775f1eff167a2c3343a41582e8fab4c6c
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Feb 17 18:05:23 2010 +0900

    sh: Fix up dynamically created write-through PMB mappings.
    
    Write-through PMB mappings still require the cache bit to be set, even if
    they're to be flagged with a different cache policy and bufferability
    bit. To reduce some of the confusion surrounding the flag encoding we
    centralize the cache mask based on the system cache policy while we're at
    it.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index f2ad6e374b64..cb808a8aaffc 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -101,6 +101,26 @@ static void pmb_free(struct pmb_entry *pmbe)
 	pmbe->entry = PMB_NO_ENTRY;
 }
 
+/*
+ * Ensure that the PMB entries match our cache configuration.
+ *
+ * When we are in 32-bit address extended mode, CCR.CB becomes
+ * invalid, so care must be taken to manually adjust cacheable
+ * translations.
+ */
+static __always_inline unsigned long pmb_cache_flags(void)
+{
+	unsigned long flags = 0;
+
+#if defined(CONFIG_CACHE_WRITETHROUGH)
+	flags |= PMB_C | PMB_WT | PMB_UB;
+#elif defined(CONFIG_CACHE_WRITEBACK)
+	flags |= PMB_C;
+#endif
+
+	return flags;
+}
+
 /*
  * Must be run uncached.
  */
@@ -108,18 +128,10 @@ static void set_pmb_entry(struct pmb_entry *pmbe)
 {
 	jump_to_uncached();
 
-	__raw_writel(pmbe->vpn | PMB_V, mk_pmb_addr(pmbe->entry));
-
-#ifdef CONFIG_CACHE_WRITETHROUGH
-	/*
-	 * When we are in 32-bit address extended mode, CCR.CB becomes
-	 * invalid, so care must be taken to manually adjust cacheable
-	 * translations.
-	 */
-	if (likely(pmbe->flags & PMB_C))
-		pmbe->flags |= PMB_WT;
-#endif
+	pmbe->flags &= ~PMB_CACHE_MASK;
+	pmbe->flags |= pmb_cache_flags();
 
+	__raw_writel(pmbe->vpn | PMB_V, mk_pmb_addr(pmbe->entry));
 	__raw_writel(pmbe->ppn | pmbe->flags | PMB_V, mk_pmb_data(pmbe->entry));
 
 	back_to_cached();
@@ -163,14 +175,15 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 
 	flags = pgprot_val(prot);
 
+	pmb_flags = PMB_WT | PMB_UB;
+
 	/* Convert typical pgprot value to the PMB equivalent */
 	if (flags & _PAGE_CACHABLE) {
-		if (flags & _PAGE_WT)
-			pmb_flags = PMB_WT;
-		else
-			pmb_flags = PMB_C;
-	} else
-		pmb_flags = PMB_WT | PMB_UB;
+		pmb_flags |= PMB_C;
+
+		if ((flags & _PAGE_WT) == 0)
+			pmb_flags &= ~(PMB_WT | PMB_UB);
+	}
 
 	pmbp = NULL;
 	wanted = size;
@@ -337,13 +350,8 @@ static int pmb_synchronize_mappings(void)
 		 * Update the caching attributes if necessary
 		 */
 		if (data_val & PMB_C) {
-#if defined(CONFIG_CACHE_WRITETHROUGH)
-			data_val |= PMB_WT;
-#elif defined(CONFIG_CACHE_WRITEBACK)
-			data_val &= ~PMB_WT;
-#else
-			data_val &= ~(PMB_C | PMB_WT);
-#endif
+			data_val &= ~PMB_CACHE_MASK;
+			data_val |= pmb_cache_flags();
 			__raw_writel(data_val, data);
 		}
 

commit d7813bc9e8e384f5a293b05c095c799d41af3668
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Feb 17 17:56:38 2010 +0900

    sh: Build PMB entry links for existing contiguous multi-page mappings.
    
    This plugs in entry sizing support for existing mappings and then builds
    on top of that for linking together entries that are mapping contiguous
    areas. This will ultimately permit us to coalesce mappings and promote
    head pages while reclaiming PMB slots for dynamic remapping.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 924f3e4b3a82..f2ad6e374b64 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -90,20 +90,15 @@ static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 	pmbe->ppn	= ppn;
 	pmbe->flags	= flags;
 	pmbe->entry	= pos;
+	pmbe->size	= 0;
 
 	return pmbe;
 }
 
 static void pmb_free(struct pmb_entry *pmbe)
 {
-	int pos = pmbe->entry;
-
-	pmbe->vpn	= 0;
-	pmbe->ppn	= 0;
-	pmbe->flags	= 0;
-	pmbe->entry	= 0;
-
-	clear_bit(pos, pmb_map);
+	clear_bit(pmbe->entry, pmb_map);
+	pmbe->entry = PMB_NO_ENTRY;
 }
 
 /*
@@ -198,6 +193,8 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 		vaddr	+= pmb_sizes[i].size;
 		size	-= pmb_sizes[i].size;
 
+		pmbe->size = pmb_sizes[i].size;
+
 		/*
 		 * Link adjacent entries that span multiple PMB entries
 		 * for easier tear-down.
@@ -273,25 +270,7 @@ static void pmb_unmap_entry(struct pmb_entry *pmbe)
 	} while (pmbe);
 }
 
-static inline void
-pmb_log_mapping(unsigned long data_val, unsigned long vpn, unsigned long ppn)
-{
-	unsigned int size;
-	const char *sz_str;
-
-	size = data_val & PMB_SZ_MASK;
-
-	sz_str = (size == PMB_SZ_16M)  ? " 16MB":
-		 (size == PMB_SZ_64M)  ? " 64MB":
-		 (size == PMB_SZ_128M) ? "128MB":
-					 "512MB";
-
-	pr_info("\t0x%08lx -> 0x%08lx [ %s %scached ]\n",
-		vpn >> PAGE_SHIFT, ppn >> PAGE_SHIFT, sz_str,
-		(data_val & PMB_C) ? "" : "un");
-}
-
-static inline unsigned int pmb_ppn_in_range(unsigned long ppn)
+static __always_inline unsigned int pmb_ppn_in_range(unsigned long ppn)
 {
 	return ppn >= __pa(memory_start) && ppn < __pa(memory_end);
 }
@@ -299,7 +278,8 @@ static inline unsigned int pmb_ppn_in_range(unsigned long ppn)
 static int pmb_synchronize_mappings(void)
 {
 	unsigned int applied = 0;
-	int i;
+	struct pmb_entry *pmbp = NULL;
+	int i, j;
 
 	pr_info("PMB: boot mappings:\n");
 
@@ -323,6 +303,7 @@ static int pmb_synchronize_mappings(void)
 		unsigned long addr, data;
 		unsigned long addr_val, data_val;
 		unsigned long ppn, vpn, flags;
+		unsigned int size;
 		struct pmb_entry *pmbe;
 
 		addr = mk_pmb_addr(i);
@@ -366,7 +347,8 @@ static int pmb_synchronize_mappings(void)
 			__raw_writel(data_val, data);
 		}
 
-		flags = data_val & (PMB_SZ_MASK | PMB_CACHE_MASK);
+		size = data_val & PMB_SZ_MASK;
+		flags = size | (data_val & PMB_CACHE_MASK);
 
 		pmbe = pmb_alloc(vpn, ppn, flags, i);
 		if (IS_ERR(pmbe)) {
@@ -374,7 +356,24 @@ static int pmb_synchronize_mappings(void)
 			continue;
 		}
 
-		pmb_log_mapping(data_val, vpn, ppn);
+		for (j = 0; j < ARRAY_SIZE(pmb_sizes); j++)
+			if (pmb_sizes[j].flag == size)
+				pmbe->size = pmb_sizes[j].size;
+
+		/*
+		 * Compare the previous entry against the current one to
+		 * see if the entries span a contiguous mapping. If so,
+		 * setup the entry links accordingly.
+		 */
+		if (pmbp && ((pmbe->vpn == (pmbp->vpn + pmbp->size)) &&
+			     (pmbe->ppn == (pmbp->ppn + pmbp->size))))
+			pmbp->link = pmbe;
+
+		pmbp = pmbe;
+
+		pr_info("\t0x%08lx -> 0x%08lx [ %ldMB %scached ]\n",
+			vpn >> PAGE_SHIFT, ppn >> PAGE_SHIFT, pmbe->size >> 20,
+			(data_val & PMB_C) ? "" : "un");
 
 		applied++;
 	}

commit 51becfd96287b3913b13075699433730984e2f4f
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Feb 17 15:33:30 2010 +0900

    sh: PMB tidying.
    
    Some overdue cleanup of the PMB code, killing off unused functionality
    and duplication sprinkled about the tree.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 509a444a30ab..924f3e4b3a82 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -21,32 +21,31 @@
 #include <linux/fs.h>
 #include <linux/seq_file.h>
 #include <linux/err.h>
+#include <linux/io.h>
+#include <asm/sizes.h>
 #include <asm/system.h>
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
-#include <asm/io.h>
 #include <asm/mmu_context.h>
 
-#define NR_PMB_ENTRIES	16
-
-static void __pmb_unmap(struct pmb_entry *);
+static void pmb_unmap_entry(struct pmb_entry *);
 
 static struct pmb_entry pmb_entry_list[NR_PMB_ENTRIES];
-static unsigned long pmb_map;
+static DECLARE_BITMAP(pmb_map, NR_PMB_ENTRIES);
 
-static inline unsigned long mk_pmb_entry(unsigned int entry)
+static __always_inline unsigned long mk_pmb_entry(unsigned int entry)
 {
 	return (entry & PMB_E_MASK) << PMB_E_SHIFT;
 }
 
-static inline unsigned long mk_pmb_addr(unsigned int entry)
+static __always_inline unsigned long mk_pmb_addr(unsigned int entry)
 {
 	return mk_pmb_entry(entry) | PMB_ADDR;
 }
 
-static inline unsigned long mk_pmb_data(unsigned int entry)
+static __always_inline unsigned long mk_pmb_data(unsigned int entry)
 {
 	return mk_pmb_entry(entry) | PMB_DATA;
 }
@@ -56,12 +55,12 @@ static int pmb_alloc_entry(void)
 	unsigned int pos;
 
 repeat:
-	pos = find_first_zero_bit(&pmb_map, NR_PMB_ENTRIES);
+	pos = find_first_zero_bit(pmb_map, NR_PMB_ENTRIES);
 
 	if (unlikely(pos > NR_PMB_ENTRIES))
 		return -ENOSPC;
 
-	if (test_and_set_bit(pos, &pmb_map))
+	if (test_and_set_bit(pos, pmb_map))
 		goto repeat;
 
 	return pos;
@@ -78,7 +77,7 @@ static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 		if (pos < 0)
 			return ERR_PTR(pos);
 	} else {
-		if (test_and_set_bit(entry, &pmb_map))
+		if (test_and_set_bit(entry, pmb_map))
 			return ERR_PTR(-ENOSPC);
 		pos = entry;
 	}
@@ -104,16 +103,17 @@ static void pmb_free(struct pmb_entry *pmbe)
 	pmbe->flags	= 0;
 	pmbe->entry	= 0;
 
-	clear_bit(pos, &pmb_map);
+	clear_bit(pos, pmb_map);
 }
 
 /*
- * Must be in P2 for __set_pmb_entry()
+ * Must be run uncached.
  */
-static void __set_pmb_entry(unsigned long vpn, unsigned long ppn,
-			    unsigned long flags, int pos)
+static void set_pmb_entry(struct pmb_entry *pmbe)
 {
-	__raw_writel(vpn | PMB_V, mk_pmb_addr(pos));
+	jump_to_uncached();
+
+	__raw_writel(pmbe->vpn | PMB_V, mk_pmb_addr(pmbe->entry));
 
 #ifdef CONFIG_CACHE_WRITETHROUGH
 	/*
@@ -121,17 +121,12 @@ static void __set_pmb_entry(unsigned long vpn, unsigned long ppn,
 	 * invalid, so care must be taken to manually adjust cacheable
 	 * translations.
 	 */
-	if (likely(flags & PMB_C))
-		flags |= PMB_WT;
+	if (likely(pmbe->flags & PMB_C))
+		pmbe->flags |= PMB_WT;
 #endif
 
-	__raw_writel(ppn | flags | PMB_V, mk_pmb_data(pos));
-}
+	__raw_writel(pmbe->ppn | pmbe->flags | PMB_V, mk_pmb_data(pmbe->entry));
 
-static void set_pmb_entry(struct pmb_entry *pmbe)
-{
-	jump_to_uncached();
-	__set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, pmbe->entry);
 	back_to_cached();
 }
 
@@ -140,9 +135,6 @@ static void clear_pmb_entry(struct pmb_entry *pmbe)
 	unsigned int entry = pmbe->entry;
 	unsigned long addr;
 
-	if (unlikely(entry >= NR_PMB_ENTRIES))
-		return;
-
 	jump_to_uncached();
 
 	/* Clear V-bit */
@@ -155,15 +147,14 @@ static void clear_pmb_entry(struct pmb_entry *pmbe)
 	back_to_cached();
 }
 
-
 static struct {
 	unsigned long size;
 	int flag;
 } pmb_sizes[] = {
-	{ .size	= 0x20000000, .flag = PMB_SZ_512M, },
-	{ .size = 0x08000000, .flag = PMB_SZ_128M, },
-	{ .size = 0x04000000, .flag = PMB_SZ_64M,  },
-	{ .size = 0x01000000, .flag = PMB_SZ_16M,  },
+	{ .size	= SZ_512M, .flag = PMB_SZ_512M, },
+	{ .size = SZ_128M, .flag = PMB_SZ_128M, },
+	{ .size = SZ_64M,  .flag = PMB_SZ_64M,  },
+	{ .size = SZ_16M,  .flag = PMB_SZ_16M,  },
 };
 
 long pmb_remap(unsigned long vaddr, unsigned long phys,
@@ -230,34 +221,36 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 	return wanted - size;
 
 out:
-	if (pmbp)
-		__pmb_unmap(pmbp);
+	pmb_unmap_entry(pmbp);
 
 	return err;
 }
 
 void pmb_unmap(unsigned long addr)
 {
-	struct pmb_entry *pmbe = NULL;
+	struct pmb_entry *pmbe;
 	int i;
 
 	for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
-		if (test_bit(i, &pmb_map)) {
+		if (test_bit(i, pmb_map)) {
 			pmbe = &pmb_entry_list[i];
-			if (pmbe->vpn == addr)
+			if (pmbe->vpn == addr) {
+				pmb_unmap_entry(pmbe);
 				break;
+			}
 		}
 	}
+}
 
+static void pmb_unmap_entry(struct pmb_entry *pmbe)
+{
 	if (unlikely(!pmbe))
 		return;
 
-	__pmb_unmap(pmbe);
-}
-
-static void __pmb_unmap(struct pmb_entry *pmbe)
-{
-	BUG_ON(!test_bit(pmbe->entry, &pmb_map));
+	if (!test_bit(pmbe->entry, pmb_map)) {
+		WARN_ON(1);
+		return;
+	}
 
 	do {
 		struct pmb_entry *pmblink = pmbe;
@@ -326,7 +319,7 @@ static int pmb_synchronize_mappings(void)
 	 * jumping between the cached and uncached mappings and tearing
 	 * down alternating mappings while executing from the other.
 	 */
-	for (i = 0; i < PMB_ENTRY_MAX; i++) {
+	for (i = 0; i < NR_PMB_ENTRIES; i++) {
 		unsigned long addr, data;
 		unsigned long addr_val, data_val;
 		unsigned long ppn, vpn, flags;
@@ -494,7 +487,7 @@ static int pmb_sysdev_suspend(struct sys_device *dev, pm_message_t state)
 	    prev_state.event == PM_EVENT_FREEZE) {
 		struct pmb_entry *pmbe;
 		for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
-			if (test_bit(i, &pmb_map)) {
+			if (test_bit(i, pmb_map)) {
 				pmbe = &pmb_entry_list[i];
 				set_pmb_entry(pmbe);
 			}

commit 7bdda6209f224aa784a036df54b22cb338d2e859
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Feb 17 13:23:00 2010 +0900

    sh: Fix up more 64-bit pgprot truncation on SH-X2 TLB.
    
    Both the store queue API and the PMB remapping take unsigned long for
    their pgprot flags, which cuts off the extended protection bits. In the
    case of the PMB this isn't really a problem since the cache attribute
    bits that we care about are all in the lower 32-bits, but we do it just
    to be safe. The store queue remapping on the other hand depends on the
    extended prot bits for enabling userspace access to the mappings.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index f822f83418e4..509a444a30ab 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -24,6 +24,7 @@
 #include <asm/system.h>
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
+#include <asm/page.h>
 #include <asm/mmu.h>
 #include <asm/io.h>
 #include <asm/mmu_context.h>
@@ -166,12 +167,15 @@ static struct {
 };
 
 long pmb_remap(unsigned long vaddr, unsigned long phys,
-	       unsigned long size, unsigned long flags)
+	       unsigned long size, pgprot_t prot)
 {
 	struct pmb_entry *pmbp, *pmbe;
 	unsigned long wanted;
 	int pmb_flags, i;
 	long err;
+	u64 flags;
+
+	flags = pgprot_val(prot);
 
 	/* Convert typical pgprot value to the PMB equivalent */
 	if (flags & _PAGE_CACHABLE) {

commit efd54ea315f645ef318708aab5714a5f1f432d03
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Feb 16 18:39:30 2010 +0900

    sh: Merge the legacy PMB mapping and entry synchronization code.
    
    This merges the code for iterating over the legacy PMB mappings and the
    code for synchronizing software state with the hardware mappings. There's
    really no reason to do the same iteration twice, and this also buys us
    the legacy entry logging facility for the dynamic PMB case.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index a06483076a41..f822f83418e4 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -276,41 +276,57 @@ static void __pmb_unmap(struct pmb_entry *pmbe)
 	} while (pmbe);
 }
 
-#ifdef CONFIG_PMB_LEGACY
+static inline void
+pmb_log_mapping(unsigned long data_val, unsigned long vpn, unsigned long ppn)
+{
+	unsigned int size;
+	const char *sz_str;
+
+	size = data_val & PMB_SZ_MASK;
+
+	sz_str = (size == PMB_SZ_16M)  ? " 16MB":
+		 (size == PMB_SZ_64M)  ? " 64MB":
+		 (size == PMB_SZ_128M) ? "128MB":
+					 "512MB";
+
+	pr_info("\t0x%08lx -> 0x%08lx [ %s %scached ]\n",
+		vpn >> PAGE_SHIFT, ppn >> PAGE_SHIFT, sz_str,
+		(data_val & PMB_C) ? "" : "un");
+}
+
 static inline unsigned int pmb_ppn_in_range(unsigned long ppn)
 {
-	return ppn >= __MEMORY_START && ppn < __MEMORY_START + __MEMORY_SIZE;
+	return ppn >= __pa(memory_start) && ppn < __pa(memory_end);
 }
 
-static int pmb_apply_legacy_mappings(void)
+static int pmb_synchronize_mappings(void)
 {
 	unsigned int applied = 0;
 	int i;
 
-	pr_info("PMB: Preserving legacy mappings:\n");
+	pr_info("PMB: boot mappings:\n");
 
 	/*
-	 * The following entries are setup by the bootloader.
+	 * Run through the initial boot mappings, log the established
+	 * ones, and blow away anything that falls outside of the valid
+	 * PPN range. Specifically, we only care about existing mappings
+	 * that impact the cached/uncached sections.
 	 *
-	 * Entry       VPN	   PPN	    V	SZ	C	UB
-	 * --------------------------------------------------------
-	 *   0      0xA0000000 0x00000000   1   64MB    0       0
-	 *   1      0xA4000000 0x04000000   1   16MB    0       0
-	 *   2      0xA6000000 0x08000000   1   16MB    0       0
-	 *   9      0x88000000 0x48000000   1  128MB    1       1
-	 *  10      0x90000000 0x50000000   1  128MB    1       1
-	 *  11      0x98000000 0x58000000   1  128MB    1       1
-	 *  13      0xA8000000 0x48000000   1  128MB    0       0
-	 *  14      0xB0000000 0x50000000   1  128MB    0       0
-	 *  15      0xB8000000 0x58000000   1  128MB    0       0
+	 * Note that touching these can be a bit of a minefield; the boot
+	 * loader can establish multi-page mappings with the same caching
+	 * attributes, so we need to ensure that we aren't modifying a
+	 * mapping that we're presently executing from, or may execute
+	 * from in the case of straddling page boundaries.
 	 *
-	 * The only entries the we need are the ones that map the kernel
-	 * at the cached and uncached addresses.
+	 * In the future we will have to tidy up after the boot loader by
+	 * jumping between the cached and uncached mappings and tearing
+	 * down alternating mappings while executing from the other.
 	 */
 	for (i = 0; i < PMB_ENTRY_MAX; i++) {
 		unsigned long addr, data;
 		unsigned long addr_val, data_val;
-		unsigned long ppn, vpn;
+		unsigned long ppn, vpn, flags;
+		struct pmb_entry *pmbe;
 
 		addr = mk_pmb_addr(i);
 		data = mk_pmb_data(i);
@@ -330,106 +346,66 @@ static int pmb_apply_legacy_mappings(void)
 		/*
 		 * Only preserve in-range mappings.
 		 */
-		if (pmb_ppn_in_range(ppn)) {
-			unsigned int size;
-			char *sz_str = NULL;
-
-			size = data_val & PMB_SZ_MASK;
-
-			sz_str = (size == PMB_SZ_16M)  ? " 16MB":
-				 (size == PMB_SZ_64M)  ? " 64MB":
-				 (size == PMB_SZ_128M) ? "128MB":
-							 "512MB";
-
-			pr_info("\t0x%08lx -> 0x%08lx [ %s %scached ]\n",
-				vpn >> PAGE_SHIFT, ppn >> PAGE_SHIFT, sz_str,
-				(data_val & PMB_C) ? "" : "un");
-
-			applied++;
-		} else {
+		if (!pmb_ppn_in_range(ppn)) {
 			/*
 			 * Invalidate anything out of bounds.
 			 */
 			__raw_writel(addr_val & ~PMB_V, addr);
 			__raw_writel(data_val & ~PMB_V, data);
+			continue;
 		}
+
+		/*
+		 * Update the caching attributes if necessary
+		 */
+		if (data_val & PMB_C) {
+#if defined(CONFIG_CACHE_WRITETHROUGH)
+			data_val |= PMB_WT;
+#elif defined(CONFIG_CACHE_WRITEBACK)
+			data_val &= ~PMB_WT;
+#else
+			data_val &= ~(PMB_C | PMB_WT);
+#endif
+			__raw_writel(data_val, data);
+		}
+
+		flags = data_val & (PMB_SZ_MASK | PMB_CACHE_MASK);
+
+		pmbe = pmb_alloc(vpn, ppn, flags, i);
+		if (IS_ERR(pmbe)) {
+			WARN_ON_ONCE(1);
+			continue;
+		}
+
+		pmb_log_mapping(data_val, vpn, ppn);
+
+		applied++;
 	}
 
 	return (applied == 0);
 }
-#else
-static inline int pmb_apply_legacy_mappings(void)
-{
-	return 1;
-}
-#endif
 
 int pmb_init(void)
 {
-	int i;
-	unsigned long addr, data;
-	unsigned long ret;
+	int ret;
 
 	jump_to_uncached();
 
-	/*
-	 * Attempt to apply the legacy boot mappings if configured. If
-	 * this is successful then we simply carry on with those and
-	 * don't bother establishing additional memory mappings. Dynamic
-	 * device mappings through pmb_remap() can still be bolted on
-	 * after this.
-	 */
-	ret = pmb_apply_legacy_mappings();
-	if (ret == 0) {
-		back_to_cached();
-		return 0;
-	}
-
 	/*
 	 * Sync our software copy of the PMB mappings with those in
 	 * hardware. The mappings in the hardware PMB were either set up
 	 * by the bootloader or very early on by the kernel.
 	 */
-	for (i = 0; i < PMB_ENTRY_MAX; i++) {
-		struct pmb_entry *pmbe;
-		unsigned long vpn, ppn, flags;
-
-		addr = PMB_DATA + (i << PMB_E_SHIFT);
-		data = __raw_readl(addr);
-		if (!(data & PMB_V))
-			continue;
-
-		if (data & PMB_C) {
-#if defined(CONFIG_CACHE_WRITETHROUGH)
-			data |= PMB_WT;
-#elif defined(CONFIG_CACHE_WRITEBACK)
-			data &= ~PMB_WT;
-#else
-			data &= ~(PMB_C | PMB_WT);
-#endif
-		}
-		__raw_writel(data, addr);
-
-		ppn = data & PMB_PFN_MASK;
-
-		flags = data & (PMB_C | PMB_WT | PMB_UB);
-		flags |= data & PMB_SZ_MASK;
-
-		addr = PMB_ADDR + (i << PMB_E_SHIFT);
-		data = __raw_readl(addr);
-
-		vpn = data & PMB_PFN_MASK;
-
-		pmbe = pmb_alloc(vpn, ppn, flags, i);
-		WARN_ON(IS_ERR(pmbe));
+	ret = pmb_synchronize_mappings();
+	if (unlikely(ret == 0)) {
+		back_to_cached();
+		return 0;
 	}
 
 	__raw_writel(0, PMB_IRMCR);
 
 	/* Flush out the TLB */
-	i =  __raw_readl(MMUCR);
-	i |= MMUCR_TI;
-	__raw_writel(i, MMUCR);
+	__raw_writel(__raw_readl(MMUCR) | MMUCR_TI, MMUCR);
 
 	back_to_cached();
 

commit 55cef91a5d553265f03fe159f9fcdfac36902248
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Feb 16 17:14:04 2010 +0900

    sh: Prevent fixed slot PMB remapping from clobbering boot entries.
    
    The PMB initialization code walks the entries and synchronizes the
    software PMB state with the hardware mappings, preserving the slot index.
    Unfortunately pmb_alloc() only tested the bit position in the entry map
    and failed to set it, resulting in subsequent remaps being able to be
    dynamically assigned a slot that trampled an existing boot mapping with
    general badness ensuing.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 3c9bf5b5c36f..a06483076a41 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -77,7 +77,7 @@ static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 		if (pos < 0)
 			return ERR_PTR(pos);
 	} else {
-		if (test_bit(entry, &pmb_map))
+		if (test_and_set_bit(entry, &pmb_map))
 			return ERR_PTR(-ENOSPC);
 		pos = entry;
 	}

commit 9d56dd3b083a3bec56e9da35ce07baca81030b03
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jan 26 12:58:40 2010 +0900

    sh: Mass ctrl_in/outX to __raw_read/writeX conversion.
    
    The old ctrl in/out routines are non-portable and unsuitable for
    cross-platform use. While drivers/sh has already been sanitized, there
    is still quite a lot of code that is not. This converts the arch/sh/ bits
    over, which permits us to flag the routines as deprecated whilst still
    building with -Werror for the architecture code, and to ensure that
    future users are not added.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 3d5eece7e6d0..3c9bf5b5c36f 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -112,7 +112,7 @@ static void pmb_free(struct pmb_entry *pmbe)
 static void __set_pmb_entry(unsigned long vpn, unsigned long ppn,
 			    unsigned long flags, int pos)
 {
-	ctrl_outl(vpn | PMB_V, mk_pmb_addr(pos));
+	__raw_writel(vpn | PMB_V, mk_pmb_addr(pos));
 
 #ifdef CONFIG_CACHE_WRITETHROUGH
 	/*
@@ -124,7 +124,7 @@ static void __set_pmb_entry(unsigned long vpn, unsigned long ppn,
 		flags |= PMB_WT;
 #endif
 
-	ctrl_outl(ppn | flags | PMB_V, mk_pmb_data(pos));
+	__raw_writel(ppn | flags | PMB_V, mk_pmb_data(pos));
 }
 
 static void set_pmb_entry(struct pmb_entry *pmbe)
@@ -146,10 +146,10 @@ static void clear_pmb_entry(struct pmb_entry *pmbe)
 
 	/* Clear V-bit */
 	addr = mk_pmb_addr(entry);
-	ctrl_outl(ctrl_inl(addr) & ~PMB_V, addr);
+	__raw_writel(__raw_readl(addr) & ~PMB_V, addr);
 
 	addr = mk_pmb_data(entry);
-	ctrl_outl(ctrl_inl(addr) & ~PMB_V, addr);
+	__raw_writel(__raw_readl(addr) & ~PMB_V, addr);
 
 	back_to_cached();
 }
@@ -395,7 +395,7 @@ int pmb_init(void)
 		unsigned long vpn, ppn, flags;
 
 		addr = PMB_DATA + (i << PMB_E_SHIFT);
-		data = ctrl_inl(addr);
+		data = __raw_readl(addr);
 		if (!(data & PMB_V))
 			continue;
 
@@ -408,7 +408,7 @@ int pmb_init(void)
 			data &= ~(PMB_C | PMB_WT);
 #endif
 		}
-		ctrl_outl(data, addr);
+		__raw_writel(data, addr);
 
 		ppn = data & PMB_PFN_MASK;
 
@@ -416,7 +416,7 @@ int pmb_init(void)
 		flags |= data & PMB_SZ_MASK;
 
 		addr = PMB_ADDR + (i << PMB_E_SHIFT);
-		data = ctrl_inl(addr);
+		data = __raw_readl(addr);
 
 		vpn = data & PMB_PFN_MASK;
 
@@ -424,12 +424,12 @@ int pmb_init(void)
 		WARN_ON(IS_ERR(pmbe));
 	}
 
-	ctrl_outl(0, PMB_IRMCR);
+	__raw_writel(0, PMB_IRMCR);
 
 	/* Flush out the TLB */
-	i =  ctrl_inl(MMUCR);
+	i =  __raw_readl(MMUCR);
 	i |= MMUCR_TI;
-	ctrl_outl(i, MMUCR);
+	__raw_writel(i, MMUCR);
 
 	back_to_cached();
 
@@ -454,8 +454,8 @@ static int pmb_seq_show(struct seq_file *file, void *iter)
 		unsigned int size;
 		char *sz_str = NULL;
 
-		addr = ctrl_inl(mk_pmb_addr(i));
-		data = ctrl_inl(mk_pmb_data(i));
+		addr = __raw_readl(mk_pmb_addr(i));
+		data = __raw_readl(mk_pmb_data(i));
 
 		size = data & PMB_SZ_MASK;
 		sz_str = (size == PMB_SZ_16M)  ? " 16MB":

commit 2dc2f8e0c46864e2a3722c84eaa96513d4cf8b2f
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Jan 21 16:05:25 2010 +0900

    sh: Kill off the special uncached section and fixmap.
    
    Now that cached_to_uncached works as advertized in 32-bit mode and we're
    never going to be able to map < 16MB anyways, there's no need for the
    special uncached section. Kill it off.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index d318fa6caffe..3d5eece7e6d0 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -127,14 +127,14 @@ static void __set_pmb_entry(unsigned long vpn, unsigned long ppn,
 	ctrl_outl(ppn | flags | PMB_V, mk_pmb_data(pos));
 }
 
-static void __uses_jump_to_uncached set_pmb_entry(struct pmb_entry *pmbe)
+static void set_pmb_entry(struct pmb_entry *pmbe)
 {
 	jump_to_uncached();
 	__set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, pmbe->entry);
 	back_to_cached();
 }
 
-static void __uses_jump_to_uncached clear_pmb_entry(struct pmb_entry *pmbe)
+static void clear_pmb_entry(struct pmb_entry *pmbe)
 {
 	unsigned int entry = pmbe->entry;
 	unsigned long addr;
@@ -364,7 +364,7 @@ static inline int pmb_apply_legacy_mappings(void)
 }
 #endif
 
-int __uses_jump_to_uncached pmb_init(void)
+int pmb_init(void)
 {
 	int i;
 	unsigned long addr, data;

commit 2efa53b269ec1e9289a108e1506f53f6f1de440b
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jan 20 16:40:48 2010 +0900

    sh: Make 29/32-bit mode check helper generally available.
    
    Presently __in_29bit_mode() is only defined for the PMB case, but
    it's also easily derived from the CONFIG_29BIT and CONFIG_32BIT &&
    CONFIG_PMB=n cases.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index b796b6c021b4..d318fa6caffe 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -436,6 +436,11 @@ int __uses_jump_to_uncached pmb_init(void)
 	return 0;
 }
 
+bool __in_29bit_mode(void)
+{
+        return (__raw_readl(PMB_PASCR) & PASCR_SE) == 0;
+}
+
 static int pmb_seq_show(struct seq_file *file, void *iter)
 {
 	int i;

commit 3d467676abf5f01f5ee99056273a58486968e252
Author: Matt Fleming <matt@console-pimps.org>
Date:   Mon Jan 18 19:33:10 2010 +0900

    sh: Setup early PMB mappings.
    
    More and more boards are going to start shipping that boot with the MMU
    in 32BIT mode by default. Previously we relied on the bootloader to
    setup PMB mappings for use by the kernel but we also need to cater for
    boards whose bootloaders don't set them up.
    
    If CONFIG_PMB_LEGACY is not enabled we have full control over our PMB
    mappings and can compress our address space. Usually, the distance
    between the the cached and uncached mappings of RAM is always 512MB,
    however we can compress the distance to be the amount of RAM on the
    board.
    
    pmb_init() now becomes much simpler. It no longer has to calculate any
    mappings, it just has to synchronise the software PMB table with the
    hardware.
    
    Tested on SDK7786 and SH7785LCR.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 8f7dbf183fb0..b796b6c021b4 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -3,11 +3,8 @@
  *
  * Privileged Space Mapping Buffer (PMB) Support.
  *
- * Copyright (C) 2005 - 2010 Paul Mundt
- *
- * P1/P2 Section mapping definitions from map32.h, which was:
- *
- *	Copyright 2003 (c) Lineo Solutions,Inc.
+ * Copyright (C) 2005 - 2010  Paul Mundt
+ * Copyright (C) 2010  Matt Fleming
  *
  * This file is subject to the terms and conditions of the GNU General Public
  * License.  See the file "COPYING" in the main directory of this archive
@@ -280,46 +277,82 @@ static void __pmb_unmap(struct pmb_entry *pmbe)
 }
 
 #ifdef CONFIG_PMB_LEGACY
+static inline unsigned int pmb_ppn_in_range(unsigned long ppn)
+{
+	return ppn >= __MEMORY_START && ppn < __MEMORY_START + __MEMORY_SIZE;
+}
+
 static int pmb_apply_legacy_mappings(void)
 {
-	int i;
-	unsigned long addr, data;
 	unsigned int applied = 0;
+	int i;
 
-	for (i = 0; i < PMB_ENTRY_MAX; i++) {
-		struct pmb_entry *pmbe;
-		unsigned long vpn, ppn, flags;
-
-		addr = PMB_DATA + (i << PMB_E_SHIFT);
-		data = ctrl_inl(addr);
-		if (!(data & PMB_V))
-			continue;
+	pr_info("PMB: Preserving legacy mappings:\n");
 
-		if (data & PMB_C) {
-#if defined(CONFIG_CACHE_WRITETHROUGH)
-			data |= PMB_WT;
-#elif defined(CONFIG_CACHE_WRITEBACK)
-			data &= ~PMB_WT;
-#else
-			data &= ~(PMB_C | PMB_WT);
-#endif
-		}
-		ctrl_outl(data, addr);
-
-		ppn = data & PMB_PFN_MASK;
+	/*
+	 * The following entries are setup by the bootloader.
+	 *
+	 * Entry       VPN	   PPN	    V	SZ	C	UB
+	 * --------------------------------------------------------
+	 *   0      0xA0000000 0x00000000   1   64MB    0       0
+	 *   1      0xA4000000 0x04000000   1   16MB    0       0
+	 *   2      0xA6000000 0x08000000   1   16MB    0       0
+	 *   9      0x88000000 0x48000000   1  128MB    1       1
+	 *  10      0x90000000 0x50000000   1  128MB    1       1
+	 *  11      0x98000000 0x58000000   1  128MB    1       1
+	 *  13      0xA8000000 0x48000000   1  128MB    0       0
+	 *  14      0xB0000000 0x50000000   1  128MB    0       0
+	 *  15      0xB8000000 0x58000000   1  128MB    0       0
+	 *
+	 * The only entries the we need are the ones that map the kernel
+	 * at the cached and uncached addresses.
+	 */
+	for (i = 0; i < PMB_ENTRY_MAX; i++) {
+		unsigned long addr, data;
+		unsigned long addr_val, data_val;
+		unsigned long ppn, vpn;
 
-		flags = data & (PMB_C | PMB_WT | PMB_UB);
-		flags |= data & PMB_SZ_MASK;
+		addr = mk_pmb_addr(i);
+		data = mk_pmb_data(i);
 
-		addr = PMB_ADDR + (i << PMB_E_SHIFT);
-		data = ctrl_inl(addr);
+		addr_val = __raw_readl(addr);
+		data_val = __raw_readl(data);
 
-		vpn = data & PMB_PFN_MASK;
+		/*
+		 * Skip over any bogus entries
+		 */
+		if (!(data_val & PMB_V) || !(addr_val & PMB_V))
+			continue;
 
-		pmbe = pmb_alloc(vpn, ppn, flags, i);
-		WARN_ON(IS_ERR(pmbe));
+		ppn = data_val & PMB_PFN_MASK;
+		vpn = addr_val & PMB_PFN_MASK;
 
-		applied++;
+		/*
+		 * Only preserve in-range mappings.
+		 */
+		if (pmb_ppn_in_range(ppn)) {
+			unsigned int size;
+			char *sz_str = NULL;
+
+			size = data_val & PMB_SZ_MASK;
+
+			sz_str = (size == PMB_SZ_16M)  ? " 16MB":
+				 (size == PMB_SZ_64M)  ? " 64MB":
+				 (size == PMB_SZ_128M) ? "128MB":
+							 "512MB";
+
+			pr_info("\t0x%08lx -> 0x%08lx [ %s %scached ]\n",
+				vpn >> PAGE_SHIFT, ppn >> PAGE_SHIFT, sz_str,
+				(data_val & PMB_C) ? "" : "un");
+
+			applied++;
+		} else {
+			/*
+			 * Invalidate anything out of bounds.
+			 */
+			__raw_writel(addr_val & ~PMB_V, addr);
+			__raw_writel(data_val & ~PMB_V, data);
+		}
 	}
 
 	return (applied == 0);
@@ -333,8 +366,9 @@ static inline int pmb_apply_legacy_mappings(void)
 
 int __uses_jump_to_uncached pmb_init(void)
 {
-	unsigned int i;
-	unsigned long size, ret;
+	int i;
+	unsigned long addr, data;
+	unsigned long ret;
 
 	jump_to_uncached();
 
@@ -352,25 +386,45 @@ int __uses_jump_to_uncached pmb_init(void)
 	}
 
 	/*
-	 * Insert PMB entries for the P1 and P2 areas so that, after
-	 * we've switched the MMU to 32-bit mode, the semantics of P1
-	 * and P2 are the same as in 29-bit mode, e.g.
-	 *
-	 *	P1 - provides a cached window onto physical memory
-	 *	P2 - provides an uncached window onto physical memory
+	 * Sync our software copy of the PMB mappings with those in
+	 * hardware. The mappings in the hardware PMB were either set up
+	 * by the bootloader or very early on by the kernel.
 	 */
-	size = (unsigned long)__MEMORY_START + __MEMORY_SIZE;
+	for (i = 0; i < PMB_ENTRY_MAX; i++) {
+		struct pmb_entry *pmbe;
+		unsigned long vpn, ppn, flags;
 
-	ret = pmb_remap(P1SEG, 0x00000000, size, PMB_C);
-	BUG_ON(ret != size);
+		addr = PMB_DATA + (i << PMB_E_SHIFT);
+		data = ctrl_inl(addr);
+		if (!(data & PMB_V))
+			continue;
 
-	ret = pmb_remap(P2SEG, 0x00000000, size, PMB_WT | PMB_UB);
-	BUG_ON(ret != size);
+		if (data & PMB_C) {
+#if defined(CONFIG_CACHE_WRITETHROUGH)
+			data |= PMB_WT;
+#elif defined(CONFIG_CACHE_WRITEBACK)
+			data &= ~PMB_WT;
+#else
+			data &= ~(PMB_C | PMB_WT);
+#endif
+		}
+		ctrl_outl(data, addr);
 
-	ctrl_outl(0, PMB_IRMCR);
+		ppn = data & PMB_PFN_MASK;
+
+		flags = data & (PMB_C | PMB_WT | PMB_UB);
+		flags |= data & PMB_SZ_MASK;
 
-	/* PMB.SE and UB[7] */
-	ctrl_outl(PASCR_SE | (1 << 7), PMB_PASCR);
+		addr = PMB_ADDR + (i << PMB_E_SHIFT);
+		data = ctrl_inl(addr);
+
+		vpn = data & PMB_PFN_MASK;
+
+		pmbe = pmb_alloc(vpn, ppn, flags, i);
+		WARN_ON(IS_ERR(pmbe));
+	}
+
+	ctrl_outl(0, PMB_IRMCR);
 
 	/* Flush out the TLB */
 	i =  ctrl_inl(MMUCR);

commit a0ab36689a36e583b6e736f1c99ac8c9aebdad59
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jan 13 18:31:48 2010 +0900

    sh: fixed PMB mode refactoring.
    
    This introduces some much overdue chainsawing of the fixed PMB support.
    fixed PMB was introduced initially to work around the fact that dynamic
    PMB mode was relatively broken, though they were never intended to
    converge. The main areas where there are differences are whether the
    system is booted in 29-bit mode or 32-bit mode, and whether legacy
    mappings are to be preserved. Any system booting in true 32-bit mode will
    not care about legacy mappings, so these are roughly decoupled.
    
    Regardless of the entry point, PMB and 32BIT are directly related as far
    as the kernel is concerned, so we also switch back to having one select
    the other.
    
    With legacy mappings iterated through and applied in the initialization
    path it's now possible to finally merge the two implementations and
    permit dynamic remapping overtop of remaining entries regardless of
    whether boot mappings are crafted by hand or inherited from the boot
    loader.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 280f6a166035..8f7dbf183fb0 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -3,7 +3,7 @@
  *
  * Privileged Space Mapping Buffer (PMB) Support.
  *
- * Copyright (C) 2005, 2006, 2007 Paul Mundt
+ * Copyright (C) 2005 - 2010 Paul Mundt
  *
  * P1/P2 Section mapping definitions from map32.h, which was:
  *
@@ -279,51 +279,12 @@ static void __pmb_unmap(struct pmb_entry *pmbe)
 	} while (pmbe);
 }
 
-#ifdef CONFIG_PMB
-int __uses_jump_to_uncached pmb_init(void)
-{
-	unsigned int i;
-	long size, ret;
-
-	jump_to_uncached();
-
-	/*
-	 * Insert PMB entries for the P1 and P2 areas so that, after
-	 * we've switched the MMU to 32-bit mode, the semantics of P1
-	 * and P2 are the same as in 29-bit mode, e.g.
-	 *
-	 *	P1 - provides a cached window onto physical memory
-	 *	P2 - provides an uncached window onto physical memory
-	 */
-	size = __MEMORY_START + __MEMORY_SIZE;
-
-	ret = pmb_remap(P1SEG, 0x00000000, size, PMB_C);
-	BUG_ON(ret != size);
-
-	ret = pmb_remap(P2SEG, 0x00000000, size, PMB_WT | PMB_UB);
-	BUG_ON(ret != size);
-
-	ctrl_outl(0, PMB_IRMCR);
-
-	/* PMB.SE and UB[7] */
-	ctrl_outl(PASCR_SE | (1 << 7), PMB_PASCR);
-
-	/* Flush out the TLB */
-	i =  ctrl_inl(MMUCR);
-	i |= MMUCR_TI;
-	ctrl_outl(i, MMUCR);
-
-	back_to_cached();
-
-	return 0;
-}
-#else
-int __uses_jump_to_uncached pmb_init(void)
+#ifdef CONFIG_PMB_LEGACY
+static int pmb_apply_legacy_mappings(void)
 {
 	int i;
 	unsigned long addr, data;
-
-	jump_to_uncached();
+	unsigned int applied = 0;
 
 	for (i = 0; i < PMB_ENTRY_MAX; i++) {
 		struct pmb_entry *pmbe;
@@ -357,13 +318,69 @@ int __uses_jump_to_uncached pmb_init(void)
 
 		pmbe = pmb_alloc(vpn, ppn, flags, i);
 		WARN_ON(IS_ERR(pmbe));
+
+		applied++;
+	}
+
+	return (applied == 0);
+}
+#else
+static inline int pmb_apply_legacy_mappings(void)
+{
+	return 1;
+}
+#endif
+
+int __uses_jump_to_uncached pmb_init(void)
+{
+	unsigned int i;
+	unsigned long size, ret;
+
+	jump_to_uncached();
+
+	/*
+	 * Attempt to apply the legacy boot mappings if configured. If
+	 * this is successful then we simply carry on with those and
+	 * don't bother establishing additional memory mappings. Dynamic
+	 * device mappings through pmb_remap() can still be bolted on
+	 * after this.
+	 */
+	ret = pmb_apply_legacy_mappings();
+	if (ret == 0) {
+		back_to_cached();
+		return 0;
 	}
 
+	/*
+	 * Insert PMB entries for the P1 and P2 areas so that, after
+	 * we've switched the MMU to 32-bit mode, the semantics of P1
+	 * and P2 are the same as in 29-bit mode, e.g.
+	 *
+	 *	P1 - provides a cached window onto physical memory
+	 *	P2 - provides an uncached window onto physical memory
+	 */
+	size = (unsigned long)__MEMORY_START + __MEMORY_SIZE;
+
+	ret = pmb_remap(P1SEG, 0x00000000, size, PMB_C);
+	BUG_ON(ret != size);
+
+	ret = pmb_remap(P2SEG, 0x00000000, size, PMB_WT | PMB_UB);
+	BUG_ON(ret != size);
+
+	ctrl_outl(0, PMB_IRMCR);
+
+	/* PMB.SE and UB[7] */
+	ctrl_outl(PASCR_SE | (1 << 7), PMB_PASCR);
+
+	/* Flush out the TLB */
+	i =  ctrl_inl(MMUCR);
+	i |= MMUCR_TI;
+	ctrl_outl(i, MMUCR);
+
 	back_to_cached();
 
 	return 0;
 }
-#endif /* CONFIG_PMB */
 
 static int pmb_seq_show(struct seq_file *file, void *iter)
 {
@@ -462,6 +479,5 @@ static int __init pmb_sysdev_init(void)
 {
 	return sysdev_driver_register(&cpu_sysdev_class, &pmb_sysdev_driver);
 }
-
 subsys_initcall(pmb_sysdev_init);
 #endif

commit 20b5014b3e5fe7b874a3f6a1dc03b0c21cb222cd
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:33 2009 +0000

    sh: Fold fixed-PMB support into dynamic PMB support
    
    The initialisation process differs for CONFIG_PMB and for
    CONFIG_PMB_FIXED. For CONFIG_PMB_FIXED we need to register the PMB
    entries that were allocated by the bootloader.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 7e64f6d960c5..280f6a166035 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -70,14 +70,20 @@ static int pmb_alloc_entry(void)
 }
 
 static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
-				   unsigned long flags)
+				   unsigned long flags, int entry)
 {
 	struct pmb_entry *pmbe;
 	int pos;
 
-	pos = pmb_alloc_entry();
-	if (pos < 0)
-		return ERR_PTR(pos);
+	if (entry == PMB_NO_ENTRY) {
+		pos = pmb_alloc_entry();
+		if (pos < 0)
+			return ERR_PTR(pos);
+	} else {
+		if (test_bit(entry, &pmb_map))
+			return ERR_PTR(-ENOSPC);
+		pos = entry;
+	}
 
 	pmbe = &pmb_entry_list[pos];
 	if (!pmbe)
@@ -187,7 +193,8 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 		if (size < pmb_sizes[i].size)
 			continue;
 
-		pmbe = pmb_alloc(vaddr, phys, pmb_flags | pmb_sizes[i].flag);
+		pmbe = pmb_alloc(vaddr, phys, pmb_flags | pmb_sizes[i].flag,
+				 PMB_NO_ENTRY);
 		if (IS_ERR(pmbe)) {
 			err = PTR_ERR(pmbe);
 			goto out;
@@ -272,6 +279,7 @@ static void __pmb_unmap(struct pmb_entry *pmbe)
 	} while (pmbe);
 }
 
+#ifdef CONFIG_PMB
 int __uses_jump_to_uncached pmb_init(void)
 {
 	unsigned int i;
@@ -309,6 +317,53 @@ int __uses_jump_to_uncached pmb_init(void)
 
 	return 0;
 }
+#else
+int __uses_jump_to_uncached pmb_init(void)
+{
+	int i;
+	unsigned long addr, data;
+
+	jump_to_uncached();
+
+	for (i = 0; i < PMB_ENTRY_MAX; i++) {
+		struct pmb_entry *pmbe;
+		unsigned long vpn, ppn, flags;
+
+		addr = PMB_DATA + (i << PMB_E_SHIFT);
+		data = ctrl_inl(addr);
+		if (!(data & PMB_V))
+			continue;
+
+		if (data & PMB_C) {
+#if defined(CONFIG_CACHE_WRITETHROUGH)
+			data |= PMB_WT;
+#elif defined(CONFIG_CACHE_WRITEBACK)
+			data &= ~PMB_WT;
+#else
+			data &= ~(PMB_C | PMB_WT);
+#endif
+		}
+		ctrl_outl(data, addr);
+
+		ppn = data & PMB_PFN_MASK;
+
+		flags = data & (PMB_C | PMB_WT | PMB_UB);
+		flags |= data & PMB_SZ_MASK;
+
+		addr = PMB_ADDR + (i << PMB_E_SHIFT);
+		data = ctrl_inl(addr);
+
+		vpn = data & PMB_PFN_MASK;
+
+		pmbe = pmb_alloc(vpn, ppn, flags, i);
+		WARN_ON(IS_ERR(pmbe));
+	}
+
+	back_to_cached();
+
+	return 0;
+}
+#endif /* CONFIG_PMB */
 
 static int pmb_seq_show(struct seq_file *file, void *iter)
 {

commit ef269b32763b22100eda9c0bf99d462c6cd65377
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:32 2009 +0000

    sh: Fix the offset from P1SEG/P2SEG where we map RAM
    
    We need to map the gap between 0x00000000 and __MEMORY_START in the PMB,
    as well as RAM.
    
    With this change my 7785LCR board can switch to 32bit MMU mode at
    runtime.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 2d009bdcf901..7e64f6d960c5 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -275,7 +275,7 @@ static void __pmb_unmap(struct pmb_entry *pmbe)
 int __uses_jump_to_uncached pmb_init(void)
 {
 	unsigned int i;
-	long size;
+	long size, ret;
 
 	jump_to_uncached();
 
@@ -287,12 +287,13 @@ int __uses_jump_to_uncached pmb_init(void)
 	 *	P1 - provides a cached window onto physical memory
 	 *	P2 - provides an uncached window onto physical memory
 	 */
-	size = pmb_remap(P2SEG, __MEMORY_START, __MEMORY_SIZE,
-			 PMB_WT | PMB_UB);
-	BUG_ON(size != __MEMORY_SIZE);
+	size = __MEMORY_START + __MEMORY_SIZE;
 
-	size = pmb_remap(P1SEG, __MEMORY_START, __MEMORY_SIZE, PMB_C);
-	BUG_ON(size != __MEMORY_SIZE);
+	ret = pmb_remap(P1SEG, 0x00000000, size, PMB_C);
+	BUG_ON(ret != size);
+
+	ret = pmb_remap(P2SEG, 0x00000000, size, PMB_WT | PMB_UB);
+	BUG_ON(ret != size);
 
 	ctrl_outl(0, PMB_IRMCR);
 

commit 3105121949b609964f370d42d1b90fe7fc01d6b1
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:30 2009 +0000

    sh: Remap physical memory into P1 and P2 in pmb_init()
    
    Eventually we'll have complete control over what physical memory gets
    mapped where and we can probably do other interesting things. For now
    though, when the MMU is in 32-bit mode, we map physical memory into the
    P1 and P2 virtual address ranges with the same semantics as they have in
    29-bit mode.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index baf365fcdb4a..2d009bdcf901 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -38,26 +38,6 @@ static void __pmb_unmap(struct pmb_entry *);
 static struct pmb_entry pmb_entry_list[NR_PMB_ENTRIES];
 static unsigned long pmb_map;
 
-static struct pmb_entry pmb_init_map[] = {
-	/* vpn         ppn         flags (ub/sz/c/wt) */
-
-	/* P1 Section Mappings */
-	{ 0x80000000, 0x00000000, PMB_SZ_64M  | PMB_C, },
-	{ 0x84000000, 0x04000000, PMB_SZ_64M  | PMB_C, },
-	{ 0x88000000, 0x08000000, PMB_SZ_128M | PMB_C, },
-	{ 0x90000000, 0x10000000, PMB_SZ_64M  | PMB_C, },
-	{ 0x94000000, 0x14000000, PMB_SZ_64M  | PMB_C, },
-	{ 0x98000000, 0x18000000, PMB_SZ_64M  | PMB_C, },
-
-	/* P2 Section Mappings */
-	{ 0xa0000000, 0x00000000, PMB_UB | PMB_SZ_64M  | PMB_WT, },
-	{ 0xa4000000, 0x04000000, PMB_UB | PMB_SZ_64M  | PMB_WT, },
-	{ 0xa8000000, 0x08000000, PMB_UB | PMB_SZ_128M | PMB_WT, },
-	{ 0xb0000000, 0x10000000, PMB_UB | PMB_SZ_64M  | PMB_WT, },
-	{ 0xb4000000, 0x14000000, PMB_UB | PMB_SZ_64M  | PMB_WT, },
-	{ 0xb8000000, 0x18000000, PMB_UB | PMB_SZ_64M  | PMB_WT, },
-};
-
 static inline unsigned long mk_pmb_entry(unsigned int entry)
 {
 	return (entry & PMB_E_MASK) << PMB_E_SHIFT;
@@ -156,13 +136,7 @@ static void __uses_jump_to_uncached clear_pmb_entry(struct pmb_entry *pmbe)
 	unsigned int entry = pmbe->entry;
 	unsigned long addr;
 
-	/*
-	 * Don't allow clearing of wired init entries, P1 or P2 access
-	 * without a corresponding mapping in the PMB will lead to reset
-	 * by the TLB.
-	 */
-	if (unlikely(entry < ARRAY_SIZE(pmb_init_map) ||
-		     entry >= NR_PMB_ENTRIES))
+	if (unlikely(entry >= NR_PMB_ENTRIES))
 		return;
 
 	jump_to_uncached();
@@ -300,28 +274,30 @@ static void __pmb_unmap(struct pmb_entry *pmbe)
 
 int __uses_jump_to_uncached pmb_init(void)
 {
-	unsigned int nr_entries = ARRAY_SIZE(pmb_init_map);
-	unsigned int entry, i;
-
-	BUG_ON(unlikely(nr_entries >= NR_PMB_ENTRIES));
+	unsigned int i;
+	long size;
 
 	jump_to_uncached();
 
 	/*
-	 * Ordering is important, P2 must be mapped in the PMB before we
-	 * can set PMB.SE, and P1 must be mapped before we jump back to
-	 * P1 space.
+	 * Insert PMB entries for the P1 and P2 areas so that, after
+	 * we've switched the MMU to 32-bit mode, the semantics of P1
+	 * and P2 are the same as in 29-bit mode, e.g.
+	 *
+	 *	P1 - provides a cached window onto physical memory
+	 *	P2 - provides an uncached window onto physical memory
 	 */
-	for (entry = 0; entry < nr_entries; entry++) {
-		struct pmb_entry *pmbe = pmb_init_map + entry;
+	size = pmb_remap(P2SEG, __MEMORY_START, __MEMORY_SIZE,
+			 PMB_WT | PMB_UB);
+	BUG_ON(size != __MEMORY_SIZE);
 
-		__set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, entry);
-	}
+	size = pmb_remap(P1SEG, __MEMORY_START, __MEMORY_SIZE, PMB_C);
+	BUG_ON(size != __MEMORY_SIZE);
 
 	ctrl_outl(0, PMB_IRMCR);
 
 	/* PMB.SE and UB[7] */
-	ctrl_outl((1 << 31) | (1 << 7), PMB_PASCR);
+	ctrl_outl(PASCR_SE | (1 << 7), PMB_PASCR);
 
 	/* Flush out the TLB */
 	i =  ctrl_inl(MMUCR);

commit edd7de803c79c7df117bf3f0e22ffdba1b1ef256
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:29 2009 +0000

    sh: Get rid of the kmem cache code
    
    Unfortunately, at the time during in boot when we want to be setting up
    the PMB entries, the kmem subsystem hasn't been initialised.
    
    We now match pmb_map slots with pmb_entry_list slots. When we find an
    empty slot in pmb_map, we set the bit, thereby acquiring the
    corresponding pmb_entry_list entry. There is a benefit in using this
    static array of struct pmb_entry's; we don't need to acquire any locks
    in order to traverse the list of struct pmb_entry's.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index f01c8191144c..baf365fcdb4a 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -35,7 +35,7 @@
 
 static void __pmb_unmap(struct pmb_entry *);
 
-static struct kmem_cache *pmb_cache;
+static struct pmb_entry pmb_entry_list[NR_PMB_ENTRIES];
 static unsigned long pmb_map;
 
 static struct pmb_entry pmb_init_map[] = {
@@ -73,32 +73,6 @@ static inline unsigned long mk_pmb_data(unsigned int entry)
 	return mk_pmb_entry(entry) | PMB_DATA;
 }
 
-static DEFINE_SPINLOCK(pmb_list_lock);
-static struct pmb_entry *pmb_list;
-
-static inline void pmb_list_add(struct pmb_entry *pmbe)
-{
-	struct pmb_entry **p, *tmp;
-
-	p = &pmb_list;
-	while ((tmp = *p) != NULL)
-		p = &tmp->next;
-
-	pmbe->next = tmp;
-	*p = pmbe;
-}
-
-static inline void pmb_list_del(struct pmb_entry *pmbe)
-{
-	struct pmb_entry **p, *tmp;
-
-	for (p = &pmb_list; (tmp = *p); p = &tmp->next)
-		if (tmp == pmbe) {
-			*p = tmp->next;
-			return;
-		}
-}
-
 static int pmb_alloc_entry(void)
 {
 	unsigned int pos;
@@ -125,7 +99,7 @@ static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 	if (pos < 0)
 		return ERR_PTR(pos);
 
-	pmbe = kmem_cache_alloc(pmb_cache, GFP_KERNEL);
+	pmbe = &pmb_entry_list[pos];
 	if (!pmbe)
 		return ERR_PTR(-ENOMEM);
 
@@ -134,20 +108,19 @@ static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 	pmbe->flags	= flags;
 	pmbe->entry	= pos;
 
-	spin_lock_irq(&pmb_list_lock);
-	pmb_list_add(pmbe);
-	spin_unlock_irq(&pmb_list_lock);
-
 	return pmbe;
 }
 
 static void pmb_free(struct pmb_entry *pmbe)
 {
-	spin_lock_irq(&pmb_list_lock);
-	pmb_list_del(pmbe);
-	spin_unlock_irq(&pmb_list_lock);
+	int pos = pmbe->entry;
 
-	kmem_cache_free(pmb_cache, pmbe);
+	pmbe->vpn	= 0;
+	pmbe->ppn	= 0;
+	pmbe->flags	= 0;
+	pmbe->entry	= 0;
+
+	clear_bit(pos, &pmb_map);
 }
 
 /*
@@ -202,8 +175,6 @@ static void __uses_jump_to_uncached clear_pmb_entry(struct pmb_entry *pmbe)
 	ctrl_outl(ctrl_inl(addr) & ~PMB_V, addr);
 
 	back_to_cached();
-
-	clear_bit(entry, &pmb_map);
 }
 
 
@@ -285,11 +256,16 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 
 void pmb_unmap(unsigned long addr)
 {
-	struct pmb_entry **p, *pmbe;
+	struct pmb_entry *pmbe = NULL;
+	int i;
 
-	for (p = &pmb_list; (pmbe = *p); p = &pmbe->next)
-		if (pmbe->vpn == addr)
-			break;
+	for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
+		if (test_bit(i, &pmb_map)) {
+			pmbe = &pmb_entry_list[i];
+			if (pmbe->vpn == addr)
+				break;
+		}
+	}
 
 	if (unlikely(!pmbe))
 		return;
@@ -299,7 +275,7 @@ void pmb_unmap(unsigned long addr)
 
 static void __pmb_unmap(struct pmb_entry *pmbe)
 {
-	WARN_ON(!test_bit(pmbe->entry, &pmb_map));
+	BUG_ON(!test_bit(pmbe->entry, &pmb_map));
 
 	do {
 		struct pmb_entry *pmblink = pmbe;
@@ -322,11 +298,6 @@ static void __pmb_unmap(struct pmb_entry *pmbe)
 	} while (pmbe);
 }
 
-static void pmb_cache_ctor(void *pmb)
-{
-	memset(pmb, 0, sizeof(struct pmb_entry));
-}
-
 int __uses_jump_to_uncached pmb_init(void)
 {
 	unsigned int nr_entries = ARRAY_SIZE(pmb_init_map);
@@ -334,9 +305,6 @@ int __uses_jump_to_uncached pmb_init(void)
 
 	BUG_ON(unlikely(nr_entries >= NR_PMB_ENTRIES));
 
-	pmb_cache = kmem_cache_create("pmb", sizeof(struct pmb_entry), 0,
-				      SLAB_PANIC, pmb_cache_ctor);
-
 	jump_to_uncached();
 
 	/*
@@ -431,15 +399,18 @@ postcore_initcall(pmb_debugfs_init);
 static int pmb_sysdev_suspend(struct sys_device *dev, pm_message_t state)
 {
 	static pm_message_t prev_state;
+	int i;
 
 	/* Restore the PMB after a resume from hibernation */
 	if (state.event == PM_EVENT_ON &&
 	    prev_state.event == PM_EVENT_FREEZE) {
 		struct pmb_entry *pmbe;
-		spin_lock_irq(&pmb_list_lock);
-		for (pmbe = pmb_list; pmbe; pmbe = pmbe->next)
-			set_pmb_entry(pmbe);
-		spin_unlock_irq(&pmb_list_lock);
+		for (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {
+			if (test_bit(i, &pmb_map)) {
+				pmbe = &pmb_entry_list[i];
+				set_pmb_entry(pmbe);
+			}
+		}
 	}
 	prev_state = state;
 	return 0;

commit 8386aebb9e15a94137693ea4f4df84207f71cc75
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:28 2009 +0000

    sh: Make most PMB functions static
    
    There's no need to export the internal PMB functions for allocating,
    freeing and modifying PMB entries, etc. This way we can restrict the
    interface for PMB.
    
    Also remove the static from pmb_init() so that we have more freedom in
    setting up the initial PMB entries and turning on MMU 32bit mode.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index b8a33949296a..f01c8191144c 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -115,8 +115,8 @@ static int pmb_alloc_entry(void)
 	return pos;
 }
 
-struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
-			    unsigned long flags)
+static struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
+				   unsigned long flags)
 {
 	struct pmb_entry *pmbe;
 	int pos;
@@ -141,7 +141,7 @@ struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 	return pmbe;
 }
 
-void pmb_free(struct pmb_entry *pmbe)
+static void pmb_free(struct pmb_entry *pmbe)
 {
 	spin_lock_irq(&pmb_list_lock);
 	pmb_list_del(pmbe);
@@ -153,8 +153,8 @@ void pmb_free(struct pmb_entry *pmbe)
 /*
  * Must be in P2 for __set_pmb_entry()
  */
-void __set_pmb_entry(unsigned long vpn, unsigned long ppn,
-		    unsigned long flags, int pos)
+static void __set_pmb_entry(unsigned long vpn, unsigned long ppn,
+			    unsigned long flags, int pos)
 {
 	ctrl_outl(vpn | PMB_V, mk_pmb_addr(pos));
 
@@ -171,14 +171,14 @@ void __set_pmb_entry(unsigned long vpn, unsigned long ppn,
 	ctrl_outl(ppn | flags | PMB_V, mk_pmb_data(pos));
 }
 
-void __uses_jump_to_uncached set_pmb_entry(struct pmb_entry *pmbe)
+static void __uses_jump_to_uncached set_pmb_entry(struct pmb_entry *pmbe)
 {
 	jump_to_uncached();
 	__set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, pmbe->entry);
 	back_to_cached();
 }
 
-void __uses_jump_to_uncached clear_pmb_entry(struct pmb_entry *pmbe)
+static void __uses_jump_to_uncached clear_pmb_entry(struct pmb_entry *pmbe)
 {
 	unsigned int entry = pmbe->entry;
 	unsigned long addr;
@@ -327,7 +327,7 @@ static void pmb_cache_ctor(void *pmb)
 	memset(pmb, 0, sizeof(struct pmb_entry));
 }
 
-static int __uses_jump_to_uncached pmb_init(void)
+int __uses_jump_to_uncached pmb_init(void)
 {
 	unsigned int nr_entries = ARRAY_SIZE(pmb_init_map);
 	unsigned int entry, i;
@@ -364,7 +364,6 @@ static int __uses_jump_to_uncached pmb_init(void)
 
 	return 0;
 }
-arch_initcall(pmb_init);
 
 static int pmb_seq_show(struct seq_file *file, void *iter)
 {

commit 067784f6239e08a084b4d8d597e14435331eae51
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:23 2009 +0000

    sh: Allocate PMB entry slot earlier
    
    Simplify set_pmb_entry() by removing the possibility of not finding a
    free slot in the PMB. Instead we now allocate a slot in pmb_alloc() so
    that if there are no free slots we fail at allocation time, rather than
    in set_pmb_entry().
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index aade31102112..b8a33949296a 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -99,10 +99,31 @@ static inline void pmb_list_del(struct pmb_entry *pmbe)
 		}
 }
 
+static int pmb_alloc_entry(void)
+{
+	unsigned int pos;
+
+repeat:
+	pos = find_first_zero_bit(&pmb_map, NR_PMB_ENTRIES);
+
+	if (unlikely(pos > NR_PMB_ENTRIES))
+		return -ENOSPC;
+
+	if (test_and_set_bit(pos, &pmb_map))
+		goto repeat;
+
+	return pos;
+}
+
 struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 			    unsigned long flags)
 {
 	struct pmb_entry *pmbe;
+	int pos;
+
+	pos = pmb_alloc_entry();
+	if (pos < 0)
+		return ERR_PTR(pos);
 
 	pmbe = kmem_cache_alloc(pmb_cache, GFP_KERNEL);
 	if (!pmbe)
@@ -111,6 +132,7 @@ struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 	pmbe->vpn	= vpn;
 	pmbe->ppn	= ppn;
 	pmbe->flags	= flags;
+	pmbe->entry	= pos;
 
 	spin_lock_irq(&pmb_list_lock);
 	pmb_list_add(pmbe);
@@ -131,23 +153,9 @@ void pmb_free(struct pmb_entry *pmbe)
 /*
  * Must be in P2 for __set_pmb_entry()
  */
-int __set_pmb_entry(unsigned long vpn, unsigned long ppn,
-		    unsigned long flags, int *entry)
+void __set_pmb_entry(unsigned long vpn, unsigned long ppn,
+		    unsigned long flags, int pos)
 {
-	unsigned int pos = *entry;
-
-	if (unlikely(pos == PMB_NO_ENTRY))
-		pos = find_first_zero_bit(&pmb_map, NR_PMB_ENTRIES);
-
-repeat:
-	if (unlikely(pos > NR_PMB_ENTRIES))
-		return -ENOSPC;
-
-	if (test_and_set_bit(pos, &pmb_map)) {
-		pos = find_first_zero_bit(&pmb_map, NR_PMB_ENTRIES);
-		goto repeat;
-	}
-
 	ctrl_outl(vpn | PMB_V, mk_pmb_addr(pos));
 
 #ifdef CONFIG_CACHE_WRITETHROUGH
@@ -161,21 +169,13 @@ int __set_pmb_entry(unsigned long vpn, unsigned long ppn,
 #endif
 
 	ctrl_outl(ppn | flags | PMB_V, mk_pmb_data(pos));
-
-	*entry = pos;
-
-	return 0;
 }
 
-int __uses_jump_to_uncached set_pmb_entry(struct pmb_entry *pmbe)
+void __uses_jump_to_uncached set_pmb_entry(struct pmb_entry *pmbe)
 {
-	int ret;
-
 	jump_to_uncached();
-	ret = __set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, &pmbe->entry);
+	__set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, pmbe->entry);
 	back_to_cached();
-
-	return ret;
 }
 
 void __uses_jump_to_uncached clear_pmb_entry(struct pmb_entry *pmbe)
@@ -239,8 +239,6 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 
 again:
 	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++) {
-		int ret;
-
 		if (size < pmb_sizes[i].size)
 			continue;
 
@@ -250,12 +248,7 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 			goto out;
 		}
 
-		ret = set_pmb_entry(pmbe);
-		if (ret != 0) {
-			pmb_free(pmbe);
-			err = -EBUSY;
-			goto out;
-		}
+		set_pmb_entry(pmbe);
 
 		phys	+= pmb_sizes[i].size;
 		vaddr	+= pmb_sizes[i].size;
@@ -311,8 +304,17 @@ static void __pmb_unmap(struct pmb_entry *pmbe)
 	do {
 		struct pmb_entry *pmblink = pmbe;
 
-		if (pmbe->entry != PMB_NO_ENTRY)
-			clear_pmb_entry(pmbe);
+		/*
+		 * We may be called before this pmb_entry has been
+		 * entered into the PMB table via set_pmb_entry(), but
+		 * that's OK because we've allocated a unique slot for
+		 * this entry in pmb_alloc() (even if we haven't filled
+		 * it yet).
+		 *
+		 * Therefore, calling clear_pmb_entry() is safe as no
+		 * other mapping can be using that slot.
+		 */
+		clear_pmb_entry(pmbe);
 
 		pmbe = pmblink->link;
 
@@ -322,11 +324,7 @@ static void __pmb_unmap(struct pmb_entry *pmbe)
 
 static void pmb_cache_ctor(void *pmb)
 {
-	struct pmb_entry *pmbe = pmb;
-
 	memset(pmb, 0, sizeof(struct pmb_entry));
-
-	pmbe->entry = PMB_NO_ENTRY;
 }
 
 static int __uses_jump_to_uncached pmb_init(void)
@@ -349,7 +347,7 @@ static int __uses_jump_to_uncached pmb_init(void)
 	for (entry = 0; entry < nr_entries; entry++) {
 		struct pmb_entry *pmbe = pmb_init_map + entry;
 
-		__set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, &entry);
+		__set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, entry);
 	}
 
 	ctrl_outl(0, PMB_IRMCR);

commit a2767cfb1d9d97c3f861743f1ad595a80b75ec99
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:34 2009 +0000

    sh: Don't allocate smaller sized mappings on every iteration
    
    Currently, we've got the less than ideal situation where if we need to
    allocate a 256MB mapping we'll allocate four entries like so,
    
             entry 1: 128MB
             entry 2:  64MB
             entry 3:  16MB
             entry 4:  16MB
    
    This is because as we execute the loop in pmb_remap() we will
    progressively try mapping the remaining address space with smaller and
    smaller sizes. This isn't good because the size we use on one iteration
    may be the perfect size to use on the next iteration, for instance when
    the initial size is divisible by one of the PMB mapping sizes.
    
    With this patch, we now only need two entries in the PMB to map 256MB of
    address space,
    
              entry 1: 128MB
              entry 2: 128MB
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 58f935896b44..aade31102112 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -269,6 +269,13 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 			pmbp->link = pmbe;
 
 		pmbp = pmbe;
+
+		/*
+		 * Instead of trying smaller sizes on every iteration
+		 * (even if we succeed in allocating space), try using
+		 * pmb_sizes[i].size again.
+		 */
+		i--;
 	}
 
 	if (size >= 0x1000000)

commit fc2bdefdde89b54d8fcde7bbf7d0adc0ce5cb044
Author: Matt Fleming <matt@console-pimps.org>
Date:   Tue Oct 6 21:22:22 2009 +0000

    sh: Plug PMB alloc memory leak
    
    If we fail to allocate a PMB entry in pmb_remap() we must remember to
    clear and free any PMB entries that we may have previously allocated,
    e.g. if we were allocating a multiple entry mapping.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index b1a714a92b14..58f935896b44 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -33,6 +33,8 @@
 
 #define NR_PMB_ENTRIES	16
 
+static void __pmb_unmap(struct pmb_entry *);
+
 static struct kmem_cache *pmb_cache;
 static unsigned long pmb_map;
 
@@ -218,9 +220,10 @@ static struct {
 long pmb_remap(unsigned long vaddr, unsigned long phys,
 	       unsigned long size, unsigned long flags)
 {
-	struct pmb_entry *pmbp;
+	struct pmb_entry *pmbp, *pmbe;
 	unsigned long wanted;
 	int pmb_flags, i;
+	long err;
 
 	/* Convert typical pgprot value to the PMB equivalent */
 	if (flags & _PAGE_CACHABLE) {
@@ -236,20 +239,22 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 
 again:
 	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++) {
-		struct pmb_entry *pmbe;
 		int ret;
 
 		if (size < pmb_sizes[i].size)
 			continue;
 
 		pmbe = pmb_alloc(vaddr, phys, pmb_flags | pmb_sizes[i].flag);
-		if (IS_ERR(pmbe))
-			return PTR_ERR(pmbe);
+		if (IS_ERR(pmbe)) {
+			err = PTR_ERR(pmbe);
+			goto out;
+		}
 
 		ret = set_pmb_entry(pmbe);
 		if (ret != 0) {
 			pmb_free(pmbe);
-			return -EBUSY;
+			err = -EBUSY;
+			goto out;
 		}
 
 		phys	+= pmb_sizes[i].size;
@@ -270,6 +275,12 @@ long pmb_remap(unsigned long vaddr, unsigned long phys,
 		goto again;
 
 	return wanted - size;
+
+out:
+	if (pmbp)
+		__pmb_unmap(pmbp);
+
+	return err;
 }
 
 void pmb_unmap(unsigned long addr)
@@ -283,12 +294,19 @@ void pmb_unmap(unsigned long addr)
 	if (unlikely(!pmbe))
 		return;
 
+	__pmb_unmap(pmbe);
+}
+
+static void __pmb_unmap(struct pmb_entry *pmbe)
+{
 	WARN_ON(!test_bit(pmbe->entry, &pmb_map));
 
 	do {
 		struct pmb_entry *pmblink = pmbe;
 
-		clear_pmb_entry(pmbe);
+		if (pmbe->entry != PMB_NO_ENTRY)
+			clear_pmb_entry(pmbe);
+
 		pmbe = pmblink->link;
 
 		pmb_free(pmblink);

commit a83c0b739f3ad1887704cfa9f1ee5ee208cf1532
Author: Francesco VIRLINZI <francesco.virlinzi@st.com>
Date:   Wed Mar 11 10:39:02 2009 +0000

    sh: PMB hibernation support
    
    This implements preliminary suspend/resume support for the PMB.
    
    Signed-off-by: Francesco Virlinzi <francesco.virlinzi@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 84241676265e..b1a714a92b14 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -15,6 +15,8 @@
  */
 #include <linux/init.h>
 #include <linux/kernel.h>
+#include <linux/sysdev.h>
+#include <linux/cpu.h>
 #include <linux/module.h>
 #include <linux/slab.h>
 #include <linux/bitops.h>
@@ -402,3 +404,39 @@ static int __init pmb_debugfs_init(void)
 	return 0;
 }
 postcore_initcall(pmb_debugfs_init);
+
+#ifdef CONFIG_PM
+static int pmb_sysdev_suspend(struct sys_device *dev, pm_message_t state)
+{
+	static pm_message_t prev_state;
+
+	/* Restore the PMB after a resume from hibernation */
+	if (state.event == PM_EVENT_ON &&
+	    prev_state.event == PM_EVENT_FREEZE) {
+		struct pmb_entry *pmbe;
+		spin_lock_irq(&pmb_list_lock);
+		for (pmbe = pmb_list; pmbe; pmbe = pmbe->next)
+			set_pmb_entry(pmbe);
+		spin_unlock_irq(&pmb_list_lock);
+	}
+	prev_state = state;
+	return 0;
+}
+
+static int pmb_sysdev_resume(struct sys_device *dev)
+{
+	return pmb_sysdev_suspend(dev, PMSG_ON);
+}
+
+static struct sysdev_driver pmb_sysdev_driver = {
+	.suspend = pmb_sysdev_suspend,
+	.resume = pmb_sysdev_resume,
+};
+
+static int __init pmb_sysdev_init(void)
+{
+	return sysdev_driver_register(&cpu_sysdev_class, &pmb_sysdev_driver);
+}
+
+subsys_initcall(pmb_sysdev_init);
+#endif

commit 25627c7fd71269e2658b6872eef65719ee80b9aa
Author: Zhaolei <zhaolei@cn.fujitsu.com>
Date:   Fri Oct 17 19:25:09 2008 +0800

    Fix debugfs_create_file's error checking method for arch/sh/mm/
    
    debugfs_create_file() returns NULL if an error occurs, returns -ENODEV
    when debugfs is not enabled in the kernel.
    
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index cef727669c87..84241676265e 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -394,6 +394,8 @@ static int __init pmb_debugfs_init(void)
 
 	dentry = debugfs_create_file("pmb", S_IFREG | S_IRUGO,
 				     sh_debugfs_root, NULL, &pmb_debugfs_fops);
+	if (!dentry)
+		return -ENOMEM;
 	if (IS_ERR(dentry))
 		return PTR_ERR(dentry);
 

commit 45dabf1427a0a876f733b07239ade1bdb0e06010
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Tue Jun 24 13:30:23 2008 +0800

    sh: fix seq_file memory leak
    
    When using single_open(), single_release() should be used instead
    of seq_release(), otherwise there is a memory leak.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 46911bcbf17b..cef727669c87 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -385,7 +385,7 @@ static const struct file_operations pmb_debugfs_fops = {
 	.open		= pmb_debugfs_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
-	.release	= seq_release,
+	.release	= single_release,
 };
 
 static int __init pmb_debugfs_init(void)

commit 51cc50685a4275c6a02653670af9f108a64e01cf
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Jul 25 19:45:34 2008 -0700

    SL*B: drop kmem cache argument from constructor
    
    Kmem cache passed to constructor is only needed for constructors that are
    themselves multiplexeres.  Nobody uses this "feature", nor does anybody uses
    passed kmem cache in non-trivial way, so pass only pointer to object.
    
    Non-trivial places are:
            arch/powerpc/mm/init_64.c
            arch/powerpc/mm/hugetlbpage.c
    
    This is flag day, yes.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Jon Tollefson <kniht@linux.vnet.ibm.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Matt Mackall <mpm@selenic.com>
    [akpm@linux-foundation.org: fix arch/powerpc/mm/hugetlbpage.c]
    [akpm@linux-foundation.org: fix mm/slab.c]
    [akpm@linux-foundation.org: fix ubifs]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 0b0ec6e04753..46911bcbf17b 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -293,7 +293,7 @@ void pmb_unmap(unsigned long addr)
 	} while (pmbe);
 }
 
-static void pmb_cache_ctor(struct kmem_cache *cachep, void *pmb)
+static void pmb_cache_ctor(void *pmb)
 {
 	struct pmb_entry *pmbe = pmb;
 

commit b9e393c2babb8b6956de52fc580b7c23f3629232
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Mar 7 17:19:58 2008 +0900

    sh: Create an sh debugfs root.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index ab81c602295f..0b0ec6e04753 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -393,7 +393,7 @@ static int __init pmb_debugfs_init(void)
 	struct dentry *dentry;
 
 	dentry = debugfs_create_file("pmb", S_IFREG | S_IRUGO,
-				     NULL, NULL, &pmb_debugfs_fops);
+				     sh_debugfs_root, NULL, &pmb_debugfs_fops);
 	if (IS_ERR(dentry))
 		return PTR_ERR(dentry);
 

commit cbaa118ecfd99fc5ed7adbd9c34a30e1c05e3c93
Author: Stuart Menefy <stuart.menefy@st.com>
Date:   Fri Nov 30 17:06:36 2007 +0900

    sh: Preparation for uncached jumps through PMB.
    
    Presently most of the 29-bit physical parts do P1/P2 segmentation
    with a 1:1 cached/uncached mapping, jumping between the two to
    control the caching behaviour. This provides the basic infrastructure
    to maintain this behaviour on 32-bit physical parts that don't map
    P1/P2 at all, using a shiny new linker section and corresponding
    fixmap entry.
    
    Signed-off-by: Stuart Menefy <stuart.menefy@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index ef6ab39eaf65..ab81c602295f 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -163,18 +163,18 @@ int __set_pmb_entry(unsigned long vpn, unsigned long ppn,
 	return 0;
 }
 
-int set_pmb_entry(struct pmb_entry *pmbe)
+int __uses_jump_to_uncached set_pmb_entry(struct pmb_entry *pmbe)
 {
 	int ret;
 
-	jump_to_P2();
+	jump_to_uncached();
 	ret = __set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, &pmbe->entry);
-	back_to_P1();
+	back_to_cached();
 
 	return ret;
 }
 
-void clear_pmb_entry(struct pmb_entry *pmbe)
+void __uses_jump_to_uncached clear_pmb_entry(struct pmb_entry *pmbe)
 {
 	unsigned int entry = pmbe->entry;
 	unsigned long addr;
@@ -188,7 +188,7 @@ void clear_pmb_entry(struct pmb_entry *pmbe)
 		     entry >= NR_PMB_ENTRIES))
 		return;
 
-	jump_to_P2();
+	jump_to_uncached();
 
 	/* Clear V-bit */
 	addr = mk_pmb_addr(entry);
@@ -197,7 +197,7 @@ void clear_pmb_entry(struct pmb_entry *pmbe)
 	addr = mk_pmb_data(entry);
 	ctrl_outl(ctrl_inl(addr) & ~PMB_V, addr);
 
-	back_to_P1();
+	back_to_cached();
 
 	clear_bit(entry, &pmb_map);
 }
@@ -302,7 +302,7 @@ static void pmb_cache_ctor(struct kmem_cache *cachep, void *pmb)
 	pmbe->entry = PMB_NO_ENTRY;
 }
 
-static int __init pmb_init(void)
+static int __uses_jump_to_uncached pmb_init(void)
 {
 	unsigned int nr_entries = ARRAY_SIZE(pmb_init_map);
 	unsigned int entry, i;
@@ -312,7 +312,7 @@ static int __init pmb_init(void)
 	pmb_cache = kmem_cache_create("pmb", sizeof(struct pmb_entry), 0,
 				      SLAB_PANIC, pmb_cache_ctor);
 
-	jump_to_P2();
+	jump_to_uncached();
 
 	/*
 	 * Ordering is important, P2 must be mapped in the PMB before we
@@ -335,7 +335,7 @@ static int __init pmb_init(void)
 	i |= MMUCR_TI;
 	ctrl_outl(i, MMUCR);
 
-	back_to_P1();
+	back_to_cached();
 
 	return 0;
 }

commit 53ff09422e5e7a6d6198b767c8f494e43ec8e3ae
Author: Nobuhiro Iwamatsu <iwamatsu@nigauri.org>
Date:   Fri Nov 30 12:33:17 2007 +0900

    sh: Fix compile error of arch/sh/mm/pmb.c
    
    Signed-off-by: Nobuhiro Iwamatsu <iwamatsu@nigauri.org>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index b632051d6ce5..ef6ab39eaf65 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -305,7 +305,7 @@ static void pmb_cache_ctor(struct kmem_cache *cachep, void *pmb)
 static int __init pmb_init(void)
 {
 	unsigned int nr_entries = ARRAY_SIZE(pmb_init_map);
-	unsigned int entry;
+	unsigned int entry, i;
 
 	BUG_ON(unlikely(nr_entries >= NR_PMB_ENTRIES));
 

commit eddeeb32fe303910c58c4e3c27fde4b6f1503350
Author: Stuart Menefy <stuart.menefy@st.com>
Date:   Mon Nov 26 21:32:40 2007 +0900

    sh: Invalidate the TLB after applying PMB mappings.
    
    Signed-off-by: Stuart Menefy <stuart.menefy@st.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 1d45b82f0a63..b632051d6ce5 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -27,6 +27,7 @@
 #include <asm/pgtable.h>
 #include <asm/mmu.h>
 #include <asm/io.h>
+#include <asm/mmu_context.h>
 
 #define NR_PMB_ENTRIES	16
 
@@ -329,6 +330,11 @@ static int __init pmb_init(void)
 	/* PMB.SE and UB[7] */
 	ctrl_outl((1 << 31) | (1 << 7), PMB_PASCR);
 
+	/* Flush out the TLB */
+	i =  ctrl_inl(MMUCR);
+	i |= MMUCR_TI;
+	ctrl_outl(i, MMUCR);
+
 	back_to_P1();
 
 	return 0;

commit 4ba9b9d0ba0a49d91fa6417c7510ee36f48cf957
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 23:25:51 2007 -0700

    Slab API: remove useless ctor parameter and reorder parameters
    
    Slab constructors currently have a flags parameter that is never used.  And
    the order of the arguments is opposite to other slab functions.  The object
    pointer is placed before the kmem_cache pointer.
    
    Convert
    
            ctor(void *object, struct kmem_cache *s, unsigned long flags)
    
    to
    
            ctor(struct kmem_cache *s, void *object)
    
    throughout the kernel
    
    [akpm@linux-foundation.org: coupla fixes]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 7d43758dc244..1d45b82f0a63 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -292,8 +292,7 @@ void pmb_unmap(unsigned long addr)
 	} while (pmbe);
 }
 
-static void pmb_cache_ctor(void *pmb, struct kmem_cache *cachep,
-			   unsigned long flags)
+static void pmb_cache_ctor(struct kmem_cache *cachep, void *pmb)
 {
 	struct pmb_entry *pmbe = pmb;
 

commit e7bd34a15b85655f24d1b45edbe3bdfebf9d027e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Jul 31 17:07:28 2007 +0900

    sh: Support explicit L1 cache disabling.
    
    This reworks the cache mode configuration in Kconfig, and allows for
    explicit selection of write-back/write-through/off configurations.
    All of the cache flushing routines are optimized away for the off
    case.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index a08a4a958add..7d43758dc244 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -145,7 +145,7 @@ int __set_pmb_entry(unsigned long vpn, unsigned long ppn,
 
 	ctrl_outl(vpn | PMB_V, mk_pmb_addr(pos));
 
-#ifdef CONFIG_SH_WRITETHROUGH
+#ifdef CONFIG_CACHE_WRITETHROUGH
 	/*
 	 * When we are in 32-bit address extended mode, CCR.CB becomes
 	 * invalid, so care must be taken to manually adjust cacheable

commit 20c2df83d25c6a95affe6157a4c9cac4cf5ffaac
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Jul 20 10:11:58 2007 +0900

    mm: Remove slab destructors from kmem_cache_create().
    
    Slab destructors were no longer supported after Christoph's
    c59def9f222d44bb7e2f0a559f2906191a0862d7 change. They've been
    BUGs for both slab and slub, and slob never supported them
    either.
    
    This rips out support for the dtor pointer from kmem_cache_create()
    completely and fixes up every single callsite in the kernel (there were
    about 224, not including the slab allocator definitions themselves,
    or the documentation references).
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index b6a5a338145b..a08a4a958add 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -310,7 +310,7 @@ static int __init pmb_init(void)
 	BUG_ON(unlikely(nr_entries >= NR_PMB_ENTRIES));
 
 	pmb_cache = kmem_cache_create("pmb", sizeof(struct pmb_entry), 0,
-				      SLAB_PANIC, pmb_cache_ctor, NULL);
+				      SLAB_PANIC, pmb_cache_ctor);
 
 	jump_to_P2();
 

commit 38c425f69c8d949620384f917e00652eaf390ec9
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri May 11 11:26:10 2007 +0900

    sh: Kill off pmb slab cache destructor.
    
    This is the last remaining slab destructor in the kernel, which
    we kill off and move the resultant list tracking logic up to
    the pmb_alloc()/pmb_free() paths.
    
    As Christoph Lameter pointed out, it's potentially unsafe to be
    taking the list lock in the destructor anyways, so this is also
    more fundamentally correct.
    
    With this in place, we're all set for killing off slab destructors
    from the kernel entirely.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 02aae06527dc..b6a5a338145b 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -3,7 +3,7 @@
  *
  * Privileged Space Mapping Buffer (PMB) Support.
  *
- * Copyright (C) 2005, 2006 Paul Mundt
+ * Copyright (C) 2005, 2006, 2007 Paul Mundt
  *
  * P1/P2 Section mapping definitions from map32.h, which was:
  *
@@ -68,6 +68,32 @@ static inline unsigned long mk_pmb_data(unsigned int entry)
 	return mk_pmb_entry(entry) | PMB_DATA;
 }
 
+static DEFINE_SPINLOCK(pmb_list_lock);
+static struct pmb_entry *pmb_list;
+
+static inline void pmb_list_add(struct pmb_entry *pmbe)
+{
+	struct pmb_entry **p, *tmp;
+
+	p = &pmb_list;
+	while ((tmp = *p) != NULL)
+		p = &tmp->next;
+
+	pmbe->next = tmp;
+	*p = pmbe;
+}
+
+static inline void pmb_list_del(struct pmb_entry *pmbe)
+{
+	struct pmb_entry **p, *tmp;
+
+	for (p = &pmb_list; (tmp = *p); p = &tmp->next)
+		if (tmp == pmbe) {
+			*p = tmp->next;
+			return;
+		}
+}
+
 struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 			    unsigned long flags)
 {
@@ -81,11 +107,19 @@ struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
 	pmbe->ppn	= ppn;
 	pmbe->flags	= flags;
 
+	spin_lock_irq(&pmb_list_lock);
+	pmb_list_add(pmbe);
+	spin_unlock_irq(&pmb_list_lock);
+
 	return pmbe;
 }
 
 void pmb_free(struct pmb_entry *pmbe)
 {
+	spin_lock_irq(&pmb_list_lock);
+	pmb_list_del(pmbe);
+	spin_unlock_irq(&pmb_list_lock);
+
 	kmem_cache_free(pmb_cache, pmbe);
 }
 
@@ -167,31 +201,6 @@ void clear_pmb_entry(struct pmb_entry *pmbe)
 	clear_bit(entry, &pmb_map);
 }
 
-static DEFINE_SPINLOCK(pmb_list_lock);
-static struct pmb_entry *pmb_list;
-
-static inline void pmb_list_add(struct pmb_entry *pmbe)
-{
-	struct pmb_entry **p, *tmp;
-
-	p = &pmb_list;
-	while ((tmp = *p) != NULL)
-		p = &tmp->next;
-
-	pmbe->next = tmp;
-	*p = pmbe;
-}
-
-static inline void pmb_list_del(struct pmb_entry *pmbe)
-{
-	struct pmb_entry **p, *tmp;
-
-	for (p = &pmb_list; (tmp = *p); p = &tmp->next)
-		if (tmp == pmbe) {
-			*p = tmp->next;
-			return;
-		}
-}
 
 static struct {
 	unsigned long size;
@@ -283,25 +292,14 @@ void pmb_unmap(unsigned long addr)
 	} while (pmbe);
 }
 
-static void pmb_cache_ctor(void *pmb, struct kmem_cache *cachep, unsigned long flags)
+static void pmb_cache_ctor(void *pmb, struct kmem_cache *cachep,
+			   unsigned long flags)
 {
 	struct pmb_entry *pmbe = pmb;
 
 	memset(pmb, 0, sizeof(struct pmb_entry));
 
-	spin_lock_irq(&pmb_list_lock);
-
 	pmbe->entry = PMB_NO_ENTRY;
-	pmb_list_add(pmbe);
-
-	spin_unlock_irq(&pmb_list_lock);
-}
-
-static void pmb_cache_dtor(void *pmb, struct kmem_cache *cachep, unsigned long flags)
-{
-	spin_lock_irq(&pmb_list_lock);
-	pmb_list_del(pmb);
-	spin_unlock_irq(&pmb_list_lock);
 }
 
 static int __init pmb_init(void)
@@ -312,8 +310,7 @@ static int __init pmb_init(void)
 	BUG_ON(unlikely(nr_entries >= NR_PMB_ENTRIES));
 
 	pmb_cache = kmem_cache_create("pmb", sizeof(struct pmb_entry), 0,
-				      SLAB_PANIC, pmb_cache_ctor,
-				      pmb_cache_dtor);
+				      SLAB_PANIC, pmb_cache_ctor, NULL);
 
 	jump_to_P2();
 

commit 0e6b9c98be1b517bf99a21d8a7036a8a21e47dd1
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Tue May 8 00:23:13 2007 -0700

    use SLAB_PANIC flag cleanup
    
    Use SLAB_PANIC and delete duplicated panic().
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Ian Molton <spyro@f2s.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index d0d45e2e0ab3..02aae06527dc 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -311,9 +311,9 @@ static int __init pmb_init(void)
 
 	BUG_ON(unlikely(nr_entries >= NR_PMB_ENTRIES));
 
-	pmb_cache = kmem_cache_create("pmb", sizeof(struct pmb_entry),
-				      0, 0, pmb_cache_ctor, pmb_cache_dtor);
-	BUG_ON(!pmb_cache);
+	pmb_cache = kmem_cache_create("pmb", sizeof(struct pmb_entry), 0,
+				      SLAB_PANIC, pmb_cache_ctor,
+				      pmb_cache_dtor);
 
 	jump_to_P2();
 

commit 5dfe4c964a0dd7bb3a1d64a4166835a153146207
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Mon Feb 12 00:55:31 2007 -0800

    [PATCH] mark struct file_operations const 2
    
    Many struct file_operations in the kernel can be "const".  Marking them const
    moves these to the .rodata section, which avoids false sharing with potential
    dirty data.  In addition it'll catch accidental writes at compile time to
    these shared resources.
    
    [akpm@osdl.org: sparc64 fix]
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index b60ad83a7635..d0d45e2e0ab3 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -378,7 +378,7 @@ static int pmb_debugfs_open(struct inode *inode, struct file *file)
 	return single_open(file, pmb_seq_show, NULL);
 }
 
-static struct file_operations pmb_debugfs_fops = {
+static const struct file_operations pmb_debugfs_fops = {
 	.owner		= THIS_MODULE,
 	.open		= pmb_debugfs_open,
 	.read		= seq_read,

commit e18b890bb0881bbab6f4f1a6cd20d9c60d66b003
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:20 2006 -0800

    [PATCH] slab: remove kmem_cache_t
    
    Replace all uses of kmem_cache_t with struct kmem_cache.
    
    The patch was generated using the following script:
    
            #!/bin/sh
            #
            # Replace one string by another in all the kernel sources.
            #
    
            set -e
    
            for file in `find * -name "*.c" -o -name "*.h"|xargs grep -l $1`; do
                    quilt add $file
                    sed -e "1,\$s/$1/$2/g" $file >/tmp/$$
                    mv /tmp/$$ $file
                    quilt refresh
            done
    
    The script was run like this
    
            sh replace kmem_cache_t "struct kmem_cache"
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 92e745341e4d..b60ad83a7635 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -30,7 +30,7 @@
 
 #define NR_PMB_ENTRIES	16
 
-static kmem_cache_t *pmb_cache;
+static struct kmem_cache *pmb_cache;
 static unsigned long pmb_map;
 
 static struct pmb_entry pmb_init_map[] = {
@@ -283,7 +283,7 @@ void pmb_unmap(unsigned long addr)
 	} while (pmbe);
 }
 
-static void pmb_cache_ctor(void *pmb, kmem_cache_t *cachep, unsigned long flags)
+static void pmb_cache_ctor(void *pmb, struct kmem_cache *cachep, unsigned long flags)
 {
 	struct pmb_entry *pmbe = pmb;
 
@@ -297,7 +297,7 @@ static void pmb_cache_ctor(void *pmb, kmem_cache_t *cachep, unsigned long flags)
 	spin_unlock_irq(&pmb_list_lock);
 }
 
-static void pmb_cache_dtor(void *pmb, kmem_cache_t *cachep, unsigned long flags)
+static void pmb_cache_dtor(void *pmb, struct kmem_cache *cachep, unsigned long flags)
 {
 	spin_lock_irq(&pmb_list_lock);
 	pmb_list_del(pmb);

commit 0f08f338083cc1d68788ccbccc44bd0502fc57ae
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 17:03:56 2006 +0900

    sh: More cosmetic cleanups and trivial fixes.
    
    Nothing exciting here, just trivial fixes..
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index 819fd0faf022..92e745341e4d 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -337,10 +337,8 @@ static int __init pmb_init(void)
 
 	return 0;
 }
-
 arch_initcall(pmb_init);
 
-#ifdef CONFIG_DEBUG_FS
 static int pmb_seq_show(struct seq_file *file, void *iter)
 {
 	int i;
@@ -399,6 +397,4 @@ static int __init pmb_debugfs_init(void)
 
 	return 0;
 }
-
 postcore_initcall(pmb_debugfs_init);
-#endif

commit d7cdc9e8ac82c43fdcd4fde6b5b53d2dcba7f707
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 15:16:42 2006 +0900

    sh: ioremap() overhaul.
    
    ioremap() overhaul. Add support for transparent PMB mapping, get rid of
    p3_ioremap(), etc. Also drop ioremap() and iounmap() routines from the
    machvec, as everyone can use the generic ioremap() API instead. For PCI
    memory apertures and other special cases, use the pci_iomap() API, as
    boards are already required to get the mapping right there.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
index ff5bde745647..819fd0faf022 100644
--- a/arch/sh/mm/pmb.c
+++ b/arch/sh/mm/pmb.c
@@ -3,7 +3,7 @@
  *
  * Privileged Space Mapping Buffer (PMB) Support.
  *
- * Copyright (C) 2005 Paul Mundt
+ * Copyright (C) 2005, 2006 Paul Mundt
  *
  * P1/P2 Section mapping definitions from map32.h, which was:
  *
@@ -24,6 +24,7 @@
 #include <linux/err.h>
 #include <asm/system.h>
 #include <asm/uaccess.h>
+#include <asm/pgtable.h>
 #include <asm/mmu.h>
 #include <asm/io.h>
 
@@ -127,11 +128,15 @@ int __set_pmb_entry(unsigned long vpn, unsigned long ppn,
 	return 0;
 }
 
-void set_pmb_entry(struct pmb_entry *pmbe)
+int set_pmb_entry(struct pmb_entry *pmbe)
 {
+	int ret;
+
 	jump_to_P2();
-	__set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, &pmbe->entry);
+	ret = __set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, &pmbe->entry);
 	back_to_P1();
+
+	return ret;
 }
 
 void clear_pmb_entry(struct pmb_entry *pmbe)
@@ -162,11 +167,141 @@ void clear_pmb_entry(struct pmb_entry *pmbe)
 	clear_bit(entry, &pmb_map);
 }
 
+static DEFINE_SPINLOCK(pmb_list_lock);
+static struct pmb_entry *pmb_list;
+
+static inline void pmb_list_add(struct pmb_entry *pmbe)
+{
+	struct pmb_entry **p, *tmp;
+
+	p = &pmb_list;
+	while ((tmp = *p) != NULL)
+		p = &tmp->next;
+
+	pmbe->next = tmp;
+	*p = pmbe;
+}
+
+static inline void pmb_list_del(struct pmb_entry *pmbe)
+{
+	struct pmb_entry **p, *tmp;
+
+	for (p = &pmb_list; (tmp = *p); p = &tmp->next)
+		if (tmp == pmbe) {
+			*p = tmp->next;
+			return;
+		}
+}
+
+static struct {
+	unsigned long size;
+	int flag;
+} pmb_sizes[] = {
+	{ .size	= 0x20000000, .flag = PMB_SZ_512M, },
+	{ .size = 0x08000000, .flag = PMB_SZ_128M, },
+	{ .size = 0x04000000, .flag = PMB_SZ_64M,  },
+	{ .size = 0x01000000, .flag = PMB_SZ_16M,  },
+};
+
+long pmb_remap(unsigned long vaddr, unsigned long phys,
+	       unsigned long size, unsigned long flags)
+{
+	struct pmb_entry *pmbp;
+	unsigned long wanted;
+	int pmb_flags, i;
+
+	/* Convert typical pgprot value to the PMB equivalent */
+	if (flags & _PAGE_CACHABLE) {
+		if (flags & _PAGE_WT)
+			pmb_flags = PMB_WT;
+		else
+			pmb_flags = PMB_C;
+	} else
+		pmb_flags = PMB_WT | PMB_UB;
+
+	pmbp = NULL;
+	wanted = size;
+
+again:
+	for (i = 0; i < ARRAY_SIZE(pmb_sizes); i++) {
+		struct pmb_entry *pmbe;
+		int ret;
+
+		if (size < pmb_sizes[i].size)
+			continue;
+
+		pmbe = pmb_alloc(vaddr, phys, pmb_flags | pmb_sizes[i].flag);
+		if (IS_ERR(pmbe))
+			return PTR_ERR(pmbe);
+
+		ret = set_pmb_entry(pmbe);
+		if (ret != 0) {
+			pmb_free(pmbe);
+			return -EBUSY;
+		}
+
+		phys	+= pmb_sizes[i].size;
+		vaddr	+= pmb_sizes[i].size;
+		size	-= pmb_sizes[i].size;
+
+		/*
+		 * Link adjacent entries that span multiple PMB entries
+		 * for easier tear-down.
+		 */
+		if (likely(pmbp))
+			pmbp->link = pmbe;
+
+		pmbp = pmbe;
+	}
+
+	if (size >= 0x1000000)
+		goto again;
+
+	return wanted - size;
+}
+
+void pmb_unmap(unsigned long addr)
+{
+	struct pmb_entry **p, *pmbe;
+
+	for (p = &pmb_list; (pmbe = *p); p = &pmbe->next)
+		if (pmbe->vpn == addr)
+			break;
+
+	if (unlikely(!pmbe))
+		return;
+
+	WARN_ON(!test_bit(pmbe->entry, &pmb_map));
+
+	do {
+		struct pmb_entry *pmblink = pmbe;
+
+		clear_pmb_entry(pmbe);
+		pmbe = pmblink->link;
+
+		pmb_free(pmblink);
+	} while (pmbe);
+}
+
 static void pmb_cache_ctor(void *pmb, kmem_cache_t *cachep, unsigned long flags)
 {
+	struct pmb_entry *pmbe = pmb;
+
 	memset(pmb, 0, sizeof(struct pmb_entry));
 
-	((struct pmb_entry *)pmb)->entry = PMB_NO_ENTRY;
+	spin_lock_irq(&pmb_list_lock);
+
+	pmbe->entry = PMB_NO_ENTRY;
+	pmb_list_add(pmbe);
+
+	spin_unlock_irq(&pmb_list_lock);
+}
+
+static void pmb_cache_dtor(void *pmb, kmem_cache_t *cachep, unsigned long flags)
+{
+	spin_lock_irq(&pmb_list_lock);
+	pmb_list_del(pmb);
+	spin_unlock_irq(&pmb_list_lock);
 }
 
 static int __init pmb_init(void)
@@ -177,7 +312,7 @@ static int __init pmb_init(void)
 	BUG_ON(unlikely(nr_entries >= NR_PMB_ENTRIES));
 
 	pmb_cache = kmem_cache_create("pmb", sizeof(struct pmb_entry),
-				      0, 0, pmb_cache_ctor, NULL);
+				      0, 0, pmb_cache_ctor, pmb_cache_dtor);
 	BUG_ON(!pmb_cache);
 
 	jump_to_P2();

commit 0c7b1df69c62209db19d1279dd882b37c04c5c2f
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Sep 27 15:08:07 2006 +0900

    sh: SH-4A Privileged Space Mapping Buffer (PMB) support.
    
    Add support for 32-bit physical addressing through the SH-4A
    Privileged Space Mapping Buffer (PMB).
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/sh/mm/pmb.c b/arch/sh/mm/pmb.c
new file mode 100644
index 000000000000..ff5bde745647
--- /dev/null
+++ b/arch/sh/mm/pmb.c
@@ -0,0 +1,269 @@
+/*
+ * arch/sh/mm/pmb.c
+ *
+ * Privileged Space Mapping Buffer (PMB) Support.
+ *
+ * Copyright (C) 2005 Paul Mundt
+ *
+ * P1/P2 Section mapping definitions from map32.h, which was:
+ *
+ *	Copyright 2003 (c) Lineo Solutions,Inc.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ */
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/bitops.h>
+#include <linux/debugfs.h>
+#include <linux/fs.h>
+#include <linux/seq_file.h>
+#include <linux/err.h>
+#include <asm/system.h>
+#include <asm/uaccess.h>
+#include <asm/mmu.h>
+#include <asm/io.h>
+
+#define NR_PMB_ENTRIES	16
+
+static kmem_cache_t *pmb_cache;
+static unsigned long pmb_map;
+
+static struct pmb_entry pmb_init_map[] = {
+	/* vpn         ppn         flags (ub/sz/c/wt) */
+
+	/* P1 Section Mappings */
+	{ 0x80000000, 0x00000000, PMB_SZ_64M  | PMB_C, },
+	{ 0x84000000, 0x04000000, PMB_SZ_64M  | PMB_C, },
+	{ 0x88000000, 0x08000000, PMB_SZ_128M | PMB_C, },
+	{ 0x90000000, 0x10000000, PMB_SZ_64M  | PMB_C, },
+	{ 0x94000000, 0x14000000, PMB_SZ_64M  | PMB_C, },
+	{ 0x98000000, 0x18000000, PMB_SZ_64M  | PMB_C, },
+
+	/* P2 Section Mappings */
+	{ 0xa0000000, 0x00000000, PMB_UB | PMB_SZ_64M  | PMB_WT, },
+	{ 0xa4000000, 0x04000000, PMB_UB | PMB_SZ_64M  | PMB_WT, },
+	{ 0xa8000000, 0x08000000, PMB_UB | PMB_SZ_128M | PMB_WT, },
+	{ 0xb0000000, 0x10000000, PMB_UB | PMB_SZ_64M  | PMB_WT, },
+	{ 0xb4000000, 0x14000000, PMB_UB | PMB_SZ_64M  | PMB_WT, },
+	{ 0xb8000000, 0x18000000, PMB_UB | PMB_SZ_64M  | PMB_WT, },
+};
+
+static inline unsigned long mk_pmb_entry(unsigned int entry)
+{
+	return (entry & PMB_E_MASK) << PMB_E_SHIFT;
+}
+
+static inline unsigned long mk_pmb_addr(unsigned int entry)
+{
+	return mk_pmb_entry(entry) | PMB_ADDR;
+}
+
+static inline unsigned long mk_pmb_data(unsigned int entry)
+{
+	return mk_pmb_entry(entry) | PMB_DATA;
+}
+
+struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,
+			    unsigned long flags)
+{
+	struct pmb_entry *pmbe;
+
+	pmbe = kmem_cache_alloc(pmb_cache, GFP_KERNEL);
+	if (!pmbe)
+		return ERR_PTR(-ENOMEM);
+
+	pmbe->vpn	= vpn;
+	pmbe->ppn	= ppn;
+	pmbe->flags	= flags;
+
+	return pmbe;
+}
+
+void pmb_free(struct pmb_entry *pmbe)
+{
+	kmem_cache_free(pmb_cache, pmbe);
+}
+
+/*
+ * Must be in P2 for __set_pmb_entry()
+ */
+int __set_pmb_entry(unsigned long vpn, unsigned long ppn,
+		    unsigned long flags, int *entry)
+{
+	unsigned int pos = *entry;
+
+	if (unlikely(pos == PMB_NO_ENTRY))
+		pos = find_first_zero_bit(&pmb_map, NR_PMB_ENTRIES);
+
+repeat:
+	if (unlikely(pos > NR_PMB_ENTRIES))
+		return -ENOSPC;
+
+	if (test_and_set_bit(pos, &pmb_map)) {
+		pos = find_first_zero_bit(&pmb_map, NR_PMB_ENTRIES);
+		goto repeat;
+	}
+
+	ctrl_outl(vpn | PMB_V, mk_pmb_addr(pos));
+
+#ifdef CONFIG_SH_WRITETHROUGH
+	/*
+	 * When we are in 32-bit address extended mode, CCR.CB becomes
+	 * invalid, so care must be taken to manually adjust cacheable
+	 * translations.
+	 */
+	if (likely(flags & PMB_C))
+		flags |= PMB_WT;
+#endif
+
+	ctrl_outl(ppn | flags | PMB_V, mk_pmb_data(pos));
+
+	*entry = pos;
+
+	return 0;
+}
+
+void set_pmb_entry(struct pmb_entry *pmbe)
+{
+	jump_to_P2();
+	__set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, &pmbe->entry);
+	back_to_P1();
+}
+
+void clear_pmb_entry(struct pmb_entry *pmbe)
+{
+	unsigned int entry = pmbe->entry;
+	unsigned long addr;
+
+	/*
+	 * Don't allow clearing of wired init entries, P1 or P2 access
+	 * without a corresponding mapping in the PMB will lead to reset
+	 * by the TLB.
+	 */
+	if (unlikely(entry < ARRAY_SIZE(pmb_init_map) ||
+		     entry >= NR_PMB_ENTRIES))
+		return;
+
+	jump_to_P2();
+
+	/* Clear V-bit */
+	addr = mk_pmb_addr(entry);
+	ctrl_outl(ctrl_inl(addr) & ~PMB_V, addr);
+
+	addr = mk_pmb_data(entry);
+	ctrl_outl(ctrl_inl(addr) & ~PMB_V, addr);
+
+	back_to_P1();
+
+	clear_bit(entry, &pmb_map);
+}
+
+static void pmb_cache_ctor(void *pmb, kmem_cache_t *cachep, unsigned long flags)
+{
+	memset(pmb, 0, sizeof(struct pmb_entry));
+
+	((struct pmb_entry *)pmb)->entry = PMB_NO_ENTRY;
+}
+
+static int __init pmb_init(void)
+{
+	unsigned int nr_entries = ARRAY_SIZE(pmb_init_map);
+	unsigned int entry;
+
+	BUG_ON(unlikely(nr_entries >= NR_PMB_ENTRIES));
+
+	pmb_cache = kmem_cache_create("pmb", sizeof(struct pmb_entry),
+				      0, 0, pmb_cache_ctor, NULL);
+	BUG_ON(!pmb_cache);
+
+	jump_to_P2();
+
+	/*
+	 * Ordering is important, P2 must be mapped in the PMB before we
+	 * can set PMB.SE, and P1 must be mapped before we jump back to
+	 * P1 space.
+	 */
+	for (entry = 0; entry < nr_entries; entry++) {
+		struct pmb_entry *pmbe = pmb_init_map + entry;
+
+		__set_pmb_entry(pmbe->vpn, pmbe->ppn, pmbe->flags, &entry);
+	}
+
+	ctrl_outl(0, PMB_IRMCR);
+
+	/* PMB.SE and UB[7] */
+	ctrl_outl((1 << 31) | (1 << 7), PMB_PASCR);
+
+	back_to_P1();
+
+	return 0;
+}
+
+arch_initcall(pmb_init);
+
+#ifdef CONFIG_DEBUG_FS
+static int pmb_seq_show(struct seq_file *file, void *iter)
+{
+	int i;
+
+	seq_printf(file, "V: Valid, C: Cacheable, WT: Write-Through\n"
+			 "CB: Copy-Back, B: Buffered, UB: Unbuffered\n");
+	seq_printf(file, "ety   vpn  ppn  size   flags\n");
+
+	for (i = 0; i < NR_PMB_ENTRIES; i++) {
+		unsigned long addr, data;
+		unsigned int size;
+		char *sz_str = NULL;
+
+		addr = ctrl_inl(mk_pmb_addr(i));
+		data = ctrl_inl(mk_pmb_data(i));
+
+		size = data & PMB_SZ_MASK;
+		sz_str = (size == PMB_SZ_16M)  ? " 16MB":
+			 (size == PMB_SZ_64M)  ? " 64MB":
+			 (size == PMB_SZ_128M) ? "128MB":
+					         "512MB";
+
+		/* 02: V 0x88 0x08 128MB C CB  B */
+		seq_printf(file, "%02d: %c 0x%02lx 0x%02lx %s %c %s %s\n",
+			   i, ((addr & PMB_V) && (data & PMB_V)) ? 'V' : ' ',
+			   (addr >> 24) & 0xff, (data >> 24) & 0xff,
+			   sz_str, (data & PMB_C) ? 'C' : ' ',
+			   (data & PMB_WT) ? "WT" : "CB",
+			   (data & PMB_UB) ? "UB" : " B");
+	}
+
+	return 0;
+}
+
+static int pmb_debugfs_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, pmb_seq_show, NULL);
+}
+
+static struct file_operations pmb_debugfs_fops = {
+	.owner		= THIS_MODULE,
+	.open		= pmb_debugfs_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= seq_release,
+};
+
+static int __init pmb_debugfs_init(void)
+{
+	struct dentry *dentry;
+
+	dentry = debugfs_create_file("pmb", S_IFREG | S_IRUGO,
+				     NULL, NULL, &pmb_debugfs_fops);
+	if (IS_ERR(dentry))
+		return PTR_ERR(dentry);
+
+	return 0;
+}
+
+postcore_initcall(pmb_debugfs_init);
+#endif
