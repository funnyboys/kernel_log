commit 1b5c0967ab8aa9424cdd5108de4e055d8aeaa9d0
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu May 21 16:55:55 2020 +0000

    powerpc/40x: Remove support for IBM 403GCX
    
    CONFIG_403GCX is not user selectable and is not
    selected by any platform.
    
    Remove it.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/635f8f5ce9d1f761b3bd8dc3e8ddad500cea26c4.1590079968.git.christophe.leroy@csgroup.eu

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 609cab1d58f2..2124b7090db9 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -6,7 +6,7 @@
 
 
 /* bytes per L1 cache line */
-#if defined(CONFIG_PPC_8xx) || defined(CONFIG_403GCX)
+#if defined(CONFIG_PPC_8xx)
 #define L1_CACHE_SHIFT		4
 #define MAX_COPY_PREFETCH	1
 #define IFETCH_ALIGN_SHIFT	2

commit a7032637b54186e5649917679727d7feaec932b1
Author: Nick Desaulniers <ndesaulniers@google.com>
Date:   Mon Aug 12 14:50:43 2019 -0700

    powerpc: Prefer __section and __printf from compiler_attributes.h
    
    Reported-by: Sedat Dilek <sedat.dilek@gmail.com>
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Nick Desaulniers <ndesaulniers@google.com>
    [mpe: Drop changes to a/p/boot which doesn't use linux headers]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190812215052.71840-10-ndesaulniers@google.com

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 72b81015cebe..609cab1d58f2 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -97,7 +97,7 @@ static inline u32 l1_icache_bytes(void)
 
 #endif
 
-#define __read_mostly __attribute__((__section__(".data..read_mostly")))
+#define __read_mostly __section(.data..read_mostly)
 
 #ifdef CONFIG_PPC_BOOK3S_32
 extern long _get_L2CR(void);

commit 23eb7f560a2a6a1b0dbaaaae8685da75314347e4
Author: Alastair D'Silva <alastair@d-silva.org>
Date:   Mon Nov 4 13:32:56 2019 +1100

    powerpc: Convert flush_icache_range & friends to C
    
    Similar to commit 22e9c88d486a
    ("powerpc/64: reuse PPC32 static inline flush_dcache_range()")
    this patch converts the following ASM symbols to C:
        flush_icache_range()
        __flush_dcache_icache()
        __flush_dcache_icache_phys()
    
    This was done as we discovered a long-standing bug where the length of the
    range was truncated due to using a 32 bit shift instead of a 64 bit one.
    
    By converting these functions to C, it becomes easier to maintain.
    
    flush_dcache_icache_phys() retains a critical assembler section as we must
    ensure there are no memory accesses while the data MMU is disabled
    (authored by Christophe Leroy). Since this has no external callers, it has
    also been made static, allowing the compiler to inline it within
    flush_dcache_icache_page().
    
    Signed-off-by: Alastair D'Silva <alastair@d-silva.org>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    [mpe: Minor fixups, don't export __flush_dcache_icache()]
    Link: https://lore.kernel.org/r/20191104023305.9581-5-alastair@au1.ibm.com

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index afb88754e0e0..72b81015cebe 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -96,22 +96,7 @@ static inline u32 l1_icache_bytes(void)
 }
 
 #endif
-#endif /* ! __ASSEMBLY__ */
-
-#if defined(__ASSEMBLY__)
-/*
- * For a snooping icache, we still need a dummy icbi to purge all the
- * prefetched instructions from the ifetch buffers. We also need a sync
- * before the icbi to order the the actual stores to memory that might
- * have modified instructions with the icbi.
- */
-#define PURGE_PREFETCHED_INS	\
-	sync;			\
-	icbi	0,r3;		\
-	sync;			\
-	isync
 
-#else
 #define __read_mostly __attribute__((__section__(".data..read_mostly")))
 
 #ifdef CONFIG_PPC_BOOK3S_32
@@ -145,6 +130,17 @@ static inline void dcbst(void *addr)
 {
 	__asm__ __volatile__ ("dcbst 0, %0" : : "r"(addr) : "memory");
 }
+
+static inline void icbi(void *addr)
+{
+	asm volatile ("icbi 0, %0" : : "r"(addr) : "memory");
+}
+
+static inline void iccci(void *addr)
+{
+	asm volatile ("iccci 0, %0" : : "r"(addr) : "memory");
+}
+
 #endif /* !__ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* _ASM_POWERPC_CACHE_H */

commit 7a0745c5e03ff1129864bc6d80f5c4417e8d7893
Author: Alastair D'Silva <alastair@d-silva.org>
Date:   Mon Nov 4 13:32:55 2019 +1100

    powerpc: define helpers to get L1 icache sizes
    
    This patch adds helpers to retrieve icache sizes, and renames the existing
    helpers to make it clear that they are for dcache.
    
    Signed-off-by: Alastair D'Silva <alastair@d-silva.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20191104023305.9581-4-alastair@au1.ibm.com

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 45e3137ccd71..afb88754e0e0 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -55,25 +55,46 @@ struct ppc64_caches {
 
 extern struct ppc64_caches ppc64_caches;
 
-static inline u32 l1_cache_shift(void)
+static inline u32 l1_dcache_shift(void)
 {
 	return ppc64_caches.l1d.log_block_size;
 }
 
-static inline u32 l1_cache_bytes(void)
+static inline u32 l1_dcache_bytes(void)
 {
 	return ppc64_caches.l1d.block_size;
 }
+
+static inline u32 l1_icache_shift(void)
+{
+	return ppc64_caches.l1i.log_block_size;
+}
+
+static inline u32 l1_icache_bytes(void)
+{
+	return ppc64_caches.l1i.block_size;
+}
 #else
-static inline u32 l1_cache_shift(void)
+static inline u32 l1_dcache_shift(void)
 {
 	return L1_CACHE_SHIFT;
 }
 
-static inline u32 l1_cache_bytes(void)
+static inline u32 l1_dcache_bytes(void)
 {
 	return L1_CACHE_BYTES;
 }
+
+static inline u32 l1_icache_shift(void)
+{
+	return L1_CACHE_SHIFT;
+}
+
+static inline u32 l1_icache_bytes(void)
+{
+	return L1_CACHE_BYTES;
+}
+
 #endif
 #endif /* ! __ASSEMBLY__ */
 

commit ed4289e8b48845888ee46377bd2b55884a55e60b
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Jul 29 22:28:54 2019 +1000

    Revert "powerpc: slightly improve cache helpers"
    
    This reverts commit 6c5875843b87c3adea2beade9d1b8b3d4523900a.
    
    It triggers a probable compiler bug on clang which leads to crashes.
    
    With GCC it allows the compiler to use a more efficient register
    allocation but current GCC versions never do that at any of the current
    call sites, so there's no benefit.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index b3388d95f451..45e3137ccd71 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -107,22 +107,22 @@ extern void _set_L3CR(unsigned long);
 
 static inline void dcbz(void *addr)
 {
-	__asm__ __volatile__ ("dcbz %y0" : : "Z"(*(u8 *)addr) : "memory");
+	__asm__ __volatile__ ("dcbz 0, %0" : : "r"(addr) : "memory");
 }
 
 static inline void dcbi(void *addr)
 {
-	__asm__ __volatile__ ("dcbi %y0" : : "Z"(*(u8 *)addr) : "memory");
+	__asm__ __volatile__ ("dcbi 0, %0" : : "r"(addr) : "memory");
 }
 
 static inline void dcbf(void *addr)
 {
-	__asm__ __volatile__ ("dcbf %y0" : : "Z"(*(u8 *)addr) : "memory");
+	__asm__ __volatile__ ("dcbf 0, %0" : : "r"(addr) : "memory");
 }
 
 static inline void dcbst(void *addr)
 {
-	__asm__ __volatile__ ("dcbst %y0" : : "Z"(*(u8 *)addr) : "memory");
+	__asm__ __volatile__ ("dcbst 0, %0" : : "r"(addr) : "memory");
 }
 #endif /* !__ASSEMBLY__ */
 #endif /* __KERNEL__ */

commit 22e9c88d486a0536d337d6e0973968be0a4cd4b2
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue May 14 09:05:16 2019 +0000

    powerpc/64: reuse PPC32 static inline flush_dcache_range()
    
    This patch drops the assembly PPC64 version of flush_dcache_range()
    and re-uses the PPC32 static inline version.
    
    With GCC 8.1, the following code is generated:
    
    void flush_test(unsigned long start, unsigned long stop)
    {
            flush_dcache_range(start, stop);
    }
    
    0000000000000130 <.flush_test>:
     130:   3d 22 00 00     addis   r9,r2,0
                            132: R_PPC64_TOC16_HA   .data+0x8
     134:   81 09 00 00     lwz     r8,0(r9)
                            136: R_PPC64_TOC16_LO   .data+0x8
     138:   3d 22 00 00     addis   r9,r2,0
                            13a: R_PPC64_TOC16_HA   .data+0xc
     13c:   80 e9 00 00     lwz     r7,0(r9)
                            13e: R_PPC64_TOC16_LO   .data+0xc
     140:   7d 48 00 d0     neg     r10,r8
     144:   7d 43 18 38     and     r3,r10,r3
     148:   7c 00 04 ac     hwsync
     14c:   4c 00 01 2c     isync
     150:   39 28 ff ff     addi    r9,r8,-1
     154:   7c 89 22 14     add     r4,r9,r4
     158:   7c 83 20 50     subf    r4,r3,r4
     15c:   7c 89 3c 37     srd.    r9,r4,r7
     160:   41 82 00 1c     beq     17c <.flush_test+0x4c>
     164:   7d 29 03 a6     mtctr   r9
     168:   60 00 00 00     nop
     16c:   60 00 00 00     nop
     170:   7c 00 18 ac     dcbf    0,r3
     174:   7c 63 42 14     add     r3,r3,r8
     178:   42 00 ff f8     bdnz    170 <.flush_test+0x40>
     17c:   7c 00 04 ac     hwsync
     180:   4c 00 01 2c     isync
     184:   4e 80 00 20     blr
     188:   60 00 00 00     nop
     18c:   60 00 00 00     nop
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index e84d1622eeb6..b3388d95f451 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -54,6 +54,16 @@ struct ppc64_caches {
 };
 
 extern struct ppc64_caches ppc64_caches;
+
+static inline u32 l1_cache_shift(void)
+{
+	return ppc64_caches.l1d.log_block_size;
+}
+
+static inline u32 l1_cache_bytes(void)
+{
+	return ppc64_caches.l1d.block_size;
+}
 #else
 static inline u32 l1_cache_shift(void)
 {

commit d98fc70fc139b72ae098d24fde42ad70c8ff2f81
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue May 14 09:05:15 2019 +0000

    powerpc/32: define helpers to get L1 cache sizes.
    
    This patch defines C helpers to retrieve the size of
    cache blocks and uses them in the cacheflush functions.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index df8e4c407366..e84d1622eeb6 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -33,7 +33,8 @@
 
 #define IFETCH_ALIGN_BYTES	(1 << IFETCH_ALIGN_SHIFT)
 
-#if defined(__powerpc64__) && !defined(__ASSEMBLY__)
+#if !defined(__ASSEMBLY__)
+#ifdef CONFIG_PPC64
 
 struct ppc_cache_info {
 	u32 size;
@@ -53,7 +54,18 @@ struct ppc64_caches {
 };
 
 extern struct ppc64_caches ppc64_caches;
-#endif /* __powerpc64__ && ! __ASSEMBLY__ */
+#else
+static inline u32 l1_cache_shift(void)
+{
+	return L1_CACHE_SHIFT;
+}
+
+static inline u32 l1_cache_bytes(void)
+{
+	return L1_CACHE_BYTES;
+}
+#endif
+#endif /* ! __ASSEMBLY__ */
 
 #if defined(__ASSEMBLY__)
 /*

commit 6c5875843b87c3adea2beade9d1b8b3d4523900a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri May 10 09:24:48 2019 +0000

    powerpc: slightly improve cache helpers
    
    Cache instructions (dcbz, dcbi, dcbf and dcbst) take two registers
    that are summed to obtain the target address. Using 'Z' constraint
    and '%y0' argument gives GCC the opportunity to use both registers
    instead of only one with the second being forced to 0.
    
    Suggested-by: Segher Boessenkool <segher@kernel.crashing.org>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 40ea5b3781c6..df8e4c407366 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -85,22 +85,22 @@ extern void _set_L3CR(unsigned long);
 
 static inline void dcbz(void *addr)
 {
-	__asm__ __volatile__ ("dcbz 0, %0" : : "r"(addr) : "memory");
+	__asm__ __volatile__ ("dcbz %y0" : : "Z"(*(u8 *)addr) : "memory");
 }
 
 static inline void dcbi(void *addr)
 {
-	__asm__ __volatile__ ("dcbi 0, %0" : : "r"(addr) : "memory");
+	__asm__ __volatile__ ("dcbi %y0" : : "Z"(*(u8 *)addr) : "memory");
 }
 
 static inline void dcbf(void *addr)
 {
-	__asm__ __volatile__ ("dcbf 0, %0" : : "r"(addr) : "memory");
+	__asm__ __volatile__ ("dcbf %y0" : : "Z"(*(u8 *)addr) : "memory");
 }
 
 static inline void dcbst(void *addr)
 {
-	__asm__ __volatile__ ("dcbst 0, %0" : : "r"(addr) : "memory");
+	__asm__ __volatile__ ("dcbst %y0" : : "Z"(*(u8 *)addr) : "memory");
 }
 #endif /* !__ASSEMBLY__ */
 #endif /* __KERNEL__ */

commit d7cceda96badc1bd444cff27ab9c375a1277c1e3
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Nov 17 10:24:56 2018 +0000

    powerpc: change CONFIG_6xx to CONFIG_PPC_BOOK3S_32
    
    Today we have:
    
    config PPC_BOOK3S_32
            bool "512x/52xx/6xx/7xx/74xx/82xx/83xx/86xx"
            [depends on PPC32 within a choice]
    
    config PPC_BOOK3S
            def_bool y
            depends on PPC_BOOK3S_32 || PPC_BOOK3S_64
    
    config 6xx
            def_bool y
            depends on PPC32 && PPC_BOOK3S
    
    6xx is therefore redundant with PPC_BOOK3S_32.
    
    In order to make the code clearer, lets use preferably PPC_BOOK3S_32.
    This will allow to remove CONFIG_6xx in a later patch.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 66298461b640..40ea5b3781c6 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -71,7 +71,7 @@ extern struct ppc64_caches ppc64_caches;
 #else
 #define __read_mostly __attribute__((__section__(".data..read_mostly")))
 
-#ifdef CONFIG_6xx
+#ifdef CONFIG_PPC_BOOK3S_32
 extern long _get_L2CR(void);
 extern long _get_L3CR(void);
 extern void _set_L2CR(unsigned long);

commit 1128bb7813a896bd608fb622eee3c26aaf33b473
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri May 18 15:01:16 2018 +0200

    powerpc/lib: Adjust .balign inside string functions for PPC32
    
    commit 87a156fb18fe1 ("Align hot loops of some string functions")
    degraded the performance of string functions by adding useless
    nops
    
    A simple benchmark on an 8xx calling 100000x a memchr() that
    matches the first byte runs in 41668 TB ticks before this patch
    and in 35986 TB ticks after this patch. So this gives an
    improvement of approx 10%
    
    Another benchmark doing the same with a memchr() matching the 128th
    byte runs in 1011365 TB ticks before this patch and 1005682 TB ticks
    after this patch, so regardless on the number of loops, removing
    those useless nops improves the test by 5683 TB ticks.
    
    Fixes: 87a156fb18fe1 ("Align hot loops of some string functions")
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index c1d257aa4c2d..66298461b640 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -9,11 +9,14 @@
 #if defined(CONFIG_PPC_8xx) || defined(CONFIG_403GCX)
 #define L1_CACHE_SHIFT		4
 #define MAX_COPY_PREFETCH	1
+#define IFETCH_ALIGN_SHIFT	2
 #elif defined(CONFIG_PPC_E500MC)
 #define L1_CACHE_SHIFT		6
 #define MAX_COPY_PREFETCH	4
+#define IFETCH_ALIGN_SHIFT	3
 #elif defined(CONFIG_PPC32)
 #define MAX_COPY_PREFETCH	4
+#define IFETCH_ALIGN_SHIFT	3	/* 603 fetches 2 insn at a time */
 #if defined(CONFIG_PPC_47x)
 #define L1_CACHE_SHIFT		7
 #else

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index d122f7f957ce..c1d257aa4c2d 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_POWERPC_CACHE_H
 #define _ASM_POWERPC_CACHE_H
 

commit 968159c0031ac1e07ab4426397e786c9c483f068
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Aug 8 13:58:54 2017 +0200

    powerpc/8xx: Getting rid of remaining use of CONFIG_8xx
    
    Two config options exist to define powerpc MPC8xx:
    * CONFIG_PPC_8xx
    * CONFIG_8xx
    
    arch/powerpc/platforms/Kconfig.cputype has contained the following
    comment about CONFIG_8xx item for some years:
    "# this is temp to handle compat with arch=ppc"
    
    arch/powerpc is now the only place with remaining use of
    CONFIG_8xx: get rid of them.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 5a90292afbad..d122f7f957ce 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -5,7 +5,7 @@
 
 
 /* bytes per L1 cache line */
-#if defined(CONFIG_8xx) || defined(CONFIG_403GCX)
+#if defined(CONFIG_PPC_8xx) || defined(CONFIG_403GCX)
 #define L1_CACHE_SHIFT		4
 #define MAX_COPY_PREFETCH	1
 #elif defined(CONFIG_PPC_E500MC)

commit 98a5f361b8625c6f4841d6ba013bbf0e80d08147
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Feb 3 17:20:07 2017 +1100

    powerpc: Add new cache geometry aux vectors
    
    This adds AUX vectors for the L1I,D, L2 and L3 cache levels
    providing for each cache level the size of the cache in bytes
    and the geometry (line size and number of ways).
    
    We chose to not use the existing alpha/sh definition which
    packs all the information in a single entry per cache level as
    it is too restricted to represent some of the geometries used
    on POWER.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index d7cf60f87604..5a90292afbad 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -38,6 +38,7 @@ struct ppc_cache_info {
 	u32 log_block_size;
 	u32 blocks_per_page;
 	u32 sets;
+	u32 assoc;
 };
 
 struct ppc64_caches {

commit 65e01f386fcddb3460be78fc886856889f80ecc7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:48 2017 -0600

    powerpc/64: Add L2 and L3 cache shape info
    
    Retrieved from device-tree when available
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 823750fa6e66..d7cf60f87604 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -43,6 +43,8 @@ struct ppc_cache_info {
 struct ppc64_caches {
 	struct ppc_cache_info l1d;
 	struct ppc_cache_info l1i;
+	struct ppc_cache_info l2;
+	struct ppc_cache_info l3;
 };
 
 extern struct ppc64_caches ppc64_caches;

commit e2827fe5c1566f66a922dd7493cbe4522c50580a
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:47 2017 -0600

    powerpc/64: Clean up ppc64_caches using a struct per cache
    
    We have two set of identical struct members for the I and D sides
    and mostly identical bunches of code to parse the device-tree to
    populate them. Instead make a ppc_cache_info structure with one
    copy for I and one for D
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 1fa364340146..823750fa6e66 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -30,19 +30,19 @@
 #define IFETCH_ALIGN_BYTES	(1 << IFETCH_ALIGN_SHIFT)
 
 #if defined(__powerpc64__) && !defined(__ASSEMBLY__)
+
+struct ppc_cache_info {
+	u32 size;
+	u32 line_size;
+	u32 block_size;	/* L1 only */
+	u32 log_block_size;
+	u32 blocks_per_page;
+	u32 sets;
+};
+
 struct ppc64_caches {
-	u32	dsize;			/* L1 d-cache size */
-	u32	dline_size;		/* L1 d-cache line size	*/
-	u32	dblock_size;		/* L1 d-cache block size */
-	u32	log_dblock_size;
-	u32	dblocks_per_page;
-	u32	dsets;
-	u32	isize;			/* L1 i-cache size */
-	u32	iline_size;		/* L1 d-cache line size	*/
-	u32	iblock_size;		/* L1 i-cache block size */
-	u32	log_iblock_size;
-	u32	iblocks_per_page;
-	u32	isets;
+	struct ppc_cache_info l1d;
+	struct ppc_cache_info l1i;
 };
 
 extern struct ppc64_caches ppc64_caches;

commit 5d451a87e5ebbde18c2b48284778f29d308816c2
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:46 2017 -0600

    powerpc/64: Retrieve number of L1 cache sets from device-tree
    
    It will be used to calculate the associativity
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 25ee433a8261..1fa364340146 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -36,11 +36,13 @@ struct ppc64_caches {
 	u32	dblock_size;		/* L1 d-cache block size */
 	u32	log_dblock_size;
 	u32	dblocks_per_page;
+	u32	dsets;
 	u32	isize;			/* L1 i-cache size */
 	u32	iline_size;		/* L1 d-cache line size	*/
 	u32	iblock_size;		/* L1 i-cache block size */
 	u32	log_iblock_size;
 	u32	iblocks_per_page;
+	u32	isets;
 };
 
 extern struct ppc64_caches ppc64_caches;

commit bd067f83b0840e798328d14133ce4542d3bf9e71
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:45 2017 -0600

    powerpc/64: Fix naming of cache block vs. cache line
    
    In a number of places we called "cache line size" what is actually
    the cache block size, which in the powerpc architecture, means the
    effective size to use with cache management instructions (it can
    be different from the actual cache line size).
    
    We fix the naming across the board and properly retrieve both
    pieces of information when available in the device-tree.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 7657aa897a38..25ee433a8261 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -33,12 +33,14 @@
 struct ppc64_caches {
 	u32	dsize;			/* L1 d-cache size */
 	u32	dline_size;		/* L1 d-cache line size	*/
-	u32	log_dline_size;
-	u32	dlines_per_page;
+	u32	dblock_size;		/* L1 d-cache block size */
+	u32	log_dblock_size;
+	u32	dblocks_per_page;
 	u32	isize;			/* L1 i-cache size */
-	u32	iline_size;		/* L1 i-cache line size	*/
-	u32	log_iline_size;
-	u32	ilines_per_page;
+	u32	iline_size;		/* L1 d-cache line size	*/
+	u32	iblock_size;		/* L1 i-cache block size */
+	u32	log_iblock_size;
+	u32	iblocks_per_page;
 };
 
 extern struct ppc64_caches ppc64_caches;

commit f4329f2ecb149282fdfdd8830a936a56b1497a05
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Oct 13 14:43:52 2016 +1100

    powerpc/64s: Reduce exception alignment
    
    Exception handlers are aligned to 128 bytes (L1 cache) on 64s, which is
    overkill. It can reduce the icache footprint of any individual exception
    path. However taken as a whole, the expansion in icache footprint seems
    likely to be counter-productive and cause more total misses.
    
    Create IFETCH_ALIGN_SHIFT/BYTES, which should give optimal ifetch
    alignment with much more reasonable alignment. This saves 1792 bytes
    from head_64.o text with an allmodconfig build.
    
    Other subarchitectures should define appropriate IFETCH_ALIGN_SHIFT
    values if this becomes more widely used.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index ffbafbf76b19..7657aa897a38 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -20,12 +20,15 @@
 #endif
 #else /* CONFIG_PPC64 */
 #define L1_CACHE_SHIFT		7
+#define IFETCH_ALIGN_SHIFT	4 /* POWER8,9 */
 #endif
 
 #define	L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
 
 #define	SMP_CACHE_BYTES		L1_CACHE_BYTES
 
+#define IFETCH_ALIGN_BYTES	(1 << IFETCH_ALIGN_SHIFT)
+
 #if defined(__powerpc64__) && !defined(__ASSEMBLY__)
 struct ppc64_caches {
 	u32	dsize;			/* L1 d-cache size */

commit d6bfa02fccf58b957f457c1bd0bb71f6ab169a0b
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Feb 9 17:08:23 2016 +0100

    powerpc: add inline functions for cache related instructions
    
    This patch adds inline functions to use dcbz, dcbi, dcbf, dcbst
    from C functions
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 5f8229e24fe6..ffbafbf76b19 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -69,6 +69,25 @@ extern void _set_L3CR(unsigned long);
 #define _set_L3CR(val)	do { } while(0)
 #endif
 
+static inline void dcbz(void *addr)
+{
+	__asm__ __volatile__ ("dcbz 0, %0" : : "r"(addr) : "memory");
+}
+
+static inline void dcbi(void *addr)
+{
+	__asm__ __volatile__ ("dcbi 0, %0" : : "r"(addr) : "memory");
+}
+
+static inline void dcbf(void *addr)
+{
+	__asm__ __volatile__ ("dcbf 0, %0" : : "r"(addr) : "memory");
+}
+
+static inline void dcbst(void *addr)
+{
+	__asm__ __volatile__ ("dcbst 0, %0" : : "r"(addr) : "memory");
+}
 #endif /* !__ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* _ASM_POWERPC_CACHE_H */

commit 23316316c1af0677a041c81f3ad6efb9dc470b33
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Oct 21 16:03:14 2015 +1100

    powerpc: Revert "Use the POWER8 Micro Partition Prefetch Engine in KVM HV on POWER8"
    
    This reverts commit 9678cdaae939 ("Use the POWER8 Micro Partition
    Prefetch Engine in KVM HV on POWER8") because the original commit had
    multiple, partly self-cancelling bugs, that could cause occasional
    memory corruption.
    
    In fact the logmpp instruction was incorrectly using register r0 as the
    source of the buffer address and operation code, and depending on what
    was in r0, it would either do nothing or corrupt the 64k page pointed to
    by r0.
    
    The logmpp instruction encoding and the operation code definitions could
    be corrected, but then there is the problem that there is no clearly
    defined way to know when the hardware has finished writing to the
    buffer.
    
    The original commit attempted to work around this by aborting the
    write-out before starting the prefetch, but this is ineffective in the
    case where the virtual core is now executing on a different physical
    core from the one where the write-out was initiated.
    
    These problems plus advice from the hardware designers not to use the
    function (since the measured performance improvement from using the
    feature was actually mostly negative), mean that reverting the code is
    the best option.
    
    Fixes: 9678cdaae939 ("Use the POWER8 Micro Partition Prefetch Engine in KVM HV on POWER8")
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 0dc42c5082b7..5f8229e24fe6 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -3,7 +3,6 @@
 
 #ifdef __KERNEL__
 
-#include <asm/reg.h>
 
 /* bytes per L1 cache line */
 #if defined(CONFIG_8xx) || defined(CONFIG_403GCX)
@@ -40,12 +39,6 @@ struct ppc64_caches {
 };
 
 extern struct ppc64_caches ppc64_caches;
-
-static inline void logmpp(u64 x)
-{
-	asm volatile(PPC_LOGMPP(R1) : : "r" (x));
-}
-
 #endif /* __powerpc64__ && ! __ASSEMBLY__ */
 
 #if defined(__ASSEMBLY__)

commit b05ae4ee602b7dc90771408ccf0972e1b3801a35
Author: Kyle Moffett <Kyle.D.Moffett@boeing.com>
Date:   Mon Nov 14 21:32:10 2011 -0500

    powerpc: Remove duplicate cacheable_memcpy/memzero functions
    
    These functions are only used from one place each.  If the cacheable_*
    versions really are more efficient, then those changes should be
    migrated into the common code instead.
    
    NOTE: The old routines are just flat buggy on kernels that support
          hardware with different cacheline sizes.
    
    Signed-off-by: Kyle Moffett <Kyle.D.Moffett@boeing.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 34a05a1a990b..0dc42c5082b7 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -76,9 +76,6 @@ extern void _set_L3CR(unsigned long);
 #define _set_L3CR(val)	do { } while(0)
 #endif
 
-extern void cacheable_memzero(void *p, unsigned int nb);
-extern void *cacheable_memcpy(void *, const void *, unsigned int);
-
 #endif /* !__ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* _ASM_POWERPC_CACHE_H */

commit 9678cdaae93932473f696fdea5debf3eee1e1260
Author: Stewart Smith <stewart@linux.vnet.ibm.com>
Date:   Fri Jul 18 14:18:43 2014 +1000

    Use the POWER8 Micro Partition Prefetch Engine in KVM HV on POWER8
    
    The POWER8 processor has a Micro Partition Prefetch Engine, which is
    a fancy way of saying "has way to store and load contents of L2 or
    L2+MRU way of L3 cache". We initiate the storing of the log (list of
    addresses) using the logmpp instruction and start restore by writing
    to a SPR.
    
    The logmpp instruction takes parameters in a single 64bit register:
    - starting address of the table to store log of L2/L2+L3 cache contents
      - 32kb for L2
      - 128kb for L2+L3
      - Aligned relative to maximum size of the table (32kb or 128kb)
    - Log control (no-op, L2 only, L2 and L3, abort logout)
    
    We should abort any ongoing logging before initiating one.
    
    To initiate restore, we write to the MPPR SPR. The format of what to write
    to the SPR is similar to the logmpp instruction parameter:
    - starting address of the table to read from (same alignment requirements)
    - table size (no data, until end of table)
    - prefetch rate (from fastest possible to slower. about every 8, 16, 24 or
      32 cycles)
    
    The idea behind loading and storing the contents of L2/L3 cache is to
    reduce memory latency in a system that is frequently swapping vcores on
    a physical CPU.
    
    The best case scenario for doing this is when some vcores are doing very
    cache heavy workloads. The worst case is when they have about 0 cache hits,
    so we just generate needless memory operations.
    
    This implementation just does L2 store/load. In my benchmarks this proves
    to be useful.
    
    Benchmark 1:
     - 16 core POWER8
     - 3x Ubuntu 14.04LTS guests (LE) with 8 VCPUs each
     - No split core/SMT
     - two guests running sysbench memory test.
       sysbench --test=memory --num-threads=8 run
     - one guest running apache bench (of default HTML page)
       ab -n 490000 -c 400 http://localhost/
    
    This benchmark aims to measure performance of real world application (apache)
    where other guests are cache hot with their own workloads. The sysbench memory
    benchmark does pointer sized writes to a (small) memory buffer in a loop.
    
    In this benchmark with this patch I can see an improvement both in requests
    per second (~5%) and in mean and median response times (again, about 5%).
    The spread of minimum and maximum response times were largely unchanged.
    
    benchmark 2:
     - Same VM config as benchmark 1
     - all three guests running sysbench memory benchmark
    
    This benchmark aims to see if there is a positive or negative affect to this
    cache heavy benchmark. Although due to the nature of the benchmark (stores) we
    may not see a difference in performance, but rather hopefully an improvement
    in consistency of performance (when vcore switched in, don't have to wait
    many times for cachelines to be pulled in)
    
    The results of this benchmark are improvements in consistency of performance
    rather than performance itself. With this patch, the few outliers in duration
    go away and we get more consistent performance in each guest.
    
    benchmark 3:
     - same 3 guests and CPU configuration as benchmark 1 and 2.
     - two idle guests
     - 1 guest running STREAM benchmark
    
    This scenario also saw performance improvement with this patch. On Copy and
    Scale workloads from STREAM, I got 5-6% improvement with this patch. For
    Add and triad, it was around 10% (or more).
    
    benchmark 4:
     - same 3 guests as previous benchmarks
     - two guests running sysbench --memory, distinctly different cache heavy
       workload
     - one guest running STREAM benchmark.
    
    Similar improvements to benchmark 3.
    
    benchmark 5:
     - 1 guest, 8 VCPUs, Ubuntu 14.04
     - Host configured with split core (SMT8, subcores-per-core=4)
     - STREAM benchmark
    
    In this benchmark, we see a 10-20% performance improvement across the board
    of STREAM benchmark results with this patch.
    
    Based on preliminary investigation and microbenchmarks
    by Prerna Saxena <prerna@linux.vnet.ibm.com>
    
    Signed-off-by: Stewart Smith <stewart@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index ed0afc1e44a4..34a05a1a990b 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -3,6 +3,7 @@
 
 #ifdef __KERNEL__
 
+#include <asm/reg.h>
 
 /* bytes per L1 cache line */
 #if defined(CONFIG_8xx) || defined(CONFIG_403GCX)
@@ -39,6 +40,12 @@ struct ppc64_caches {
 };
 
 extern struct ppc64_caches ppc64_caches;
+
+static inline void logmpp(u64 x)
+{
+	asm volatile(PPC_LOGMPP(R1) : : "r" (x));
+}
+
 #endif /* __powerpc64__ && ! __ASSEMBLY__ */
 
 #if defined(__ASSEMBLY__)

commit 0ce636700c5bad54eda0e62903a1803f6d67b31d
Author: Kevin Hao <haokexin@gmail.com>
Date:   Thu Aug 22 09:30:35 2013 +0800

    powerpc: purge all the prefetched instructions for the coherent icache flush
    
    As Benjamin Herrenschmidt has indicated, we still need a dummy icbi to
    purge all the prefetched instructions from the ifetch buffers for the
    snooping icache. We also need a sync before the icbi to order the
    actual stores to memory that might have modified instructions with
    the icbi.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 9e495c9a6a88..ed0afc1e44a4 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -41,8 +41,20 @@ struct ppc64_caches {
 extern struct ppc64_caches ppc64_caches;
 #endif /* __powerpc64__ && ! __ASSEMBLY__ */
 
-#if !defined(__ASSEMBLY__)
+#if defined(__ASSEMBLY__)
+/*
+ * For a snooping icache, we still need a dummy icbi to purge all the
+ * prefetched instructions from the ifetch buffers. We also need a sync
+ * before the icbi to order the the actual stores to memory that might
+ * have modified instructions with the icbi.
+ */
+#define PURGE_PREFETCHED_INS	\
+	sync;			\
+	icbi	0,r3;		\
+	sync;			\
+	isync
 
+#else
 #define __read_mostly __attribute__((__section__(".data..read_mostly")))
 
 #ifdef CONFIG_6xx

commit ae3a197e3d0bfe3f4bf1693723e82dc018c096f3
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:02 2012 +0100

    Disintegrate asm/system.h for PowerPC
    
    Disintegrate asm/system.h for PowerPC.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    cc: linuxppc-dev@lists.ozlabs.org

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 4b509411ad8a..9e495c9a6a88 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -42,8 +42,24 @@ extern struct ppc64_caches ppc64_caches;
 #endif /* __powerpc64__ && ! __ASSEMBLY__ */
 
 #if !defined(__ASSEMBLY__)
+
 #define __read_mostly __attribute__((__section__(".data..read_mostly")))
+
+#ifdef CONFIG_6xx
+extern long _get_L2CR(void);
+extern long _get_L3CR(void);
+extern void _set_L2CR(unsigned long);
+extern void _set_L3CR(unsigned long);
+#else
+#define _get_L2CR()	0L
+#define _get_L3CR()	0L
+#define _set_L2CR(val)	do { } while(0)
+#define _set_L3CR(val)	do { } while(0)
 #endif
 
+extern void cacheable_memzero(void *p, unsigned int nb);
+extern void *cacheable_memcpy(void *, const void *, unsigned int);
+
+#endif /* !__ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* _ASM_POWERPC_CACHE_H */

commit 1f73897861b8ef0be64ff4b801f8d6f830f683b5
Merge: b904d7131d11 64ffc9ff424c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 1 08:55:52 2010 -0700

    Merge branch 'for-35' of git://repo.or.cz/linux-kbuild
    
    * 'for-35' of git://repo.or.cz/linux-kbuild: (81 commits)
      kbuild: Revert part of e8d400a to resolve a conflict
      kbuild: Fix checking of scm-identifier variable
      gconfig: add support to show hidden options that have prompts
      menuconfig: add support to show hidden options which have prompts
      gconfig: remove show_debug option
      gconfig: remove dbg_print_ptype() and dbg_print_stype()
      kconfig: fix zconfdump()
      kconfig: some small fixes
      add random binaries to .gitignore
      kbuild: Include gen_initramfs_list.sh and the file list in the .d file
      kconfig: recalc symbol value before showing search results
      .gitignore: ignore *.lzo files
      headerdep: perlcritic warning
      scripts/Makefile.lib: Align the output of LZO
      kbuild: Generate modules.builtin in make modules_install
      Revert "kbuild: specify absolute paths for cscope"
      kbuild: Do not unnecessarily regenerate modules.builtin
      headers_install: use local file handles
      headers_check: fix perl warnings
      export_report: fix perl warnings
      ...

commit e7f75ad01d590243904c2d95ab47e6b2e9ef6dad
Author: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
Date:   Fri Mar 5 10:43:12 2010 +0000

    powerpc/47x: Base ppc476 support
    
    This patch adds the base support for the 476 processor.  The code was
    primarily written by Ben Herrenschmidt and Torez Smith, but I've been
    maintaining it for a while.
    
    The goal is to have a single binary that will run on 44x and 47x, but
    we still have some details to work out.  The biggest is that the L1 cache
    line size differs on the two platforms, but it's currently a compile-time
    option.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Torez Smith  <lnxtorez@linux.vnet.ibm.com>
    Signed-off-by: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 81de6eb3455d..725634fc18c6 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -12,8 +12,12 @@
 #define L1_CACHE_SHIFT		6
 #define MAX_COPY_PREFETCH	4
 #elif defined(CONFIG_PPC32)
-#define L1_CACHE_SHIFT		5
 #define MAX_COPY_PREFETCH	4
+#if defined(CONFIG_PPC_47x)
+#define L1_CACHE_SHIFT		7
+#else
+#define L1_CACHE_SHIFT		5
+#endif
 #else /* CONFIG_PPC64 */
 #define L1_CACHE_SHIFT		7
 #endif

commit 54cb27a71f51d304342c79e62fd7667f2171062b
Author: Denys Vlasenko <vda.linux@googlemail.com>
Date:   Sat Feb 20 01:03:44 2010 +0100

    Rename .data.read_mostly to .data..read_mostly.
    
    Signed-off-by: Denys Vlasenko <vda.linux@googlemail.com>
    Signed-off-by: Michal Marek <mmarek@suse.cz>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
index 81de6eb3455d..3f41ab9c1e4e 100644
--- a/arch/powerpc/include/asm/cache.h
+++ b/arch/powerpc/include/asm/cache.h
@@ -38,7 +38,7 @@ extern struct ppc64_caches ppc64_caches;
 #endif /* __powerpc64__ && ! __ASSEMBLY__ */
 
 #if !defined(__ASSEMBLY__)
-#define __read_mostly __attribute__((__section__(".data.read_mostly")))
+#define __read_mostly __attribute__((__section__(".data..read_mostly")))
 #endif
 
 #endif /* __KERNEL__ */

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/cache.h b/arch/powerpc/include/asm/cache.h
new file mode 100644
index 000000000000..81de6eb3455d
--- /dev/null
+++ b/arch/powerpc/include/asm/cache.h
@@ -0,0 +1,45 @@
+#ifndef _ASM_POWERPC_CACHE_H
+#define _ASM_POWERPC_CACHE_H
+
+#ifdef __KERNEL__
+
+
+/* bytes per L1 cache line */
+#if defined(CONFIG_8xx) || defined(CONFIG_403GCX)
+#define L1_CACHE_SHIFT		4
+#define MAX_COPY_PREFETCH	1
+#elif defined(CONFIG_PPC_E500MC)
+#define L1_CACHE_SHIFT		6
+#define MAX_COPY_PREFETCH	4
+#elif defined(CONFIG_PPC32)
+#define L1_CACHE_SHIFT		5
+#define MAX_COPY_PREFETCH	4
+#else /* CONFIG_PPC64 */
+#define L1_CACHE_SHIFT		7
+#endif
+
+#define	L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
+
+#define	SMP_CACHE_BYTES		L1_CACHE_BYTES
+
+#if defined(__powerpc64__) && !defined(__ASSEMBLY__)
+struct ppc64_caches {
+	u32	dsize;			/* L1 d-cache size */
+	u32	dline_size;		/* L1 d-cache line size	*/
+	u32	log_dline_size;
+	u32	dlines_per_page;
+	u32	isize;			/* L1 i-cache size */
+	u32	iline_size;		/* L1 i-cache line size	*/
+	u32	log_iline_size;
+	u32	ilines_per_page;
+};
+
+extern struct ppc64_caches ppc64_caches;
+#endif /* __powerpc64__ && ! __ASSEMBLY__ */
+
+#if !defined(__ASSEMBLY__)
+#define __read_mostly __attribute__((__section__(".data.read_mostly")))
+#endif
+
+#endif /* __KERNEL__ */
+#endif /* _ASM_POWERPC_CACHE_H */
