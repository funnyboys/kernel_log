commit 7974c4732642f710b5111165ae1f7f7fed822282
Author: Christophe Leroy <christophe.leroy@csgroup.eu>
Date:   Tue May 19 05:49:29 2020 +0000

    powerpc/32s: Implement dedicated kasan_init_region()
    
    Implement a kasan_init_region() dedicated to book3s/32 that
    allocates KASAN regions using BATs.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/709e821602b48a1d7c211a9b156da26db98c3e9d.1589866984.git.christophe.leroy@csgroup.eu

diff --git a/arch/powerpc/include/asm/kasan.h b/arch/powerpc/include/asm/kasan.h
index 107a24c3f7b3..be85c7005fb1 100644
--- a/arch/powerpc/include/asm/kasan.h
+++ b/arch/powerpc/include/asm/kasan.h
@@ -34,6 +34,7 @@ static inline void kasan_init(void) { }
 static inline void kasan_late_init(void) { }
 #endif
 
+void kasan_update_early_region(unsigned long k_start, unsigned long k_end, pte_t pte);
 int kasan_init_shadow_page_tables(unsigned long k_start, unsigned long k_end);
 int kasan_init_region(void *start, size_t size);
 

commit ec97d022f621c6c850aec46d8818b49c6aae95ad
Author: Christophe Leroy <christophe.leroy@csgroup.eu>
Date:   Tue May 19 05:48:48 2020 +0000

    powerpc/kasan: Declare kasan_init_region() weak
    
    In order to alloc sub-arches to alloc KASAN regions using optimised
    methods (Huge pages on 8xx, BATs on BOOK3S, ...), declare
    kasan_init_region() weak.
    
    Also make kasan_init_shadow_page_tables() accessible from outside,
    so that it can be called from the specific kasan_init_region()
    functions if needed.
    
    And populate remaining KASAN address space only once performed
    the region mapping, to allow 8xx to allocate hugepd instead of
    standard page tables for mapping via 8M hugepages.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/3c1ce419fa1b5a4171b92d7fb16455ca17e1b96d.1589866984.git.christophe.leroy@csgroup.eu

diff --git a/arch/powerpc/include/asm/kasan.h b/arch/powerpc/include/asm/kasan.h
index 4769bbf7173a..107a24c3f7b3 100644
--- a/arch/powerpc/include/asm/kasan.h
+++ b/arch/powerpc/include/asm/kasan.h
@@ -34,5 +34,8 @@ static inline void kasan_init(void) { }
 static inline void kasan_late_init(void) { }
 #endif
 
+int kasan_init_shadow_page_tables(unsigned long k_start, unsigned long k_end);
+int kasan_init_region(void *start, size_t size);
+
 #endif /* __ASSEMBLY */
 #endif

commit d2a91cef9bbdeb87b7449fdab1a6be6000930210
Author: Christophe Leroy <christophe.leroy@csgroup.eu>
Date:   Tue May 19 05:48:45 2020 +0000

    powerpc/kasan: Fix shadow pages allocation failure
    
    Doing kasan pages allocation in MMU_init is too early, kernel doesn't
    have access yet to the entire memory space and memblock_alloc() fails
    when the kernel is a bit big.
    
    Do it from kasan_init() instead.
    
    Fixes: 2edb16efc899 ("powerpc/32: Add KASAN support")
    Cc: stable@vger.kernel.org
    Signed-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/c24163ee5d5f8cdf52fefa45055ceb35435b8f15.1589866984.git.christophe.leroy@csgroup.eu

diff --git a/arch/powerpc/include/asm/kasan.h b/arch/powerpc/include/asm/kasan.h
index fc900937f653..4769bbf7173a 100644
--- a/arch/powerpc/include/asm/kasan.h
+++ b/arch/powerpc/include/asm/kasan.h
@@ -27,12 +27,10 @@
 
 #ifdef CONFIG_KASAN
 void kasan_early_init(void);
-void kasan_mmu_init(void);
 void kasan_init(void);
 void kasan_late_init(void);
 #else
 static inline void kasan_init(void) { }
-static inline void kasan_mmu_init(void) { }
 static inline void kasan_late_init(void) { }
 #endif
 

commit 3a66a24f6060e6775f8c02ac52329ea0152d7e58
Author: Christophe Leroy <christophe.leroy@csgroup.eu>
Date:   Tue May 19 05:48:44 2020 +0000

    powerpc/kasan: Fix issues by lowering KASAN_SHADOW_END
    
    At the time being, KASAN_SHADOW_END is 0x100000000, which
    is 0 in 32 bits representation.
    
    This leads to a couple of issues:
    - kasan_remap_early_shadow_ro() does nothing because the comparison
    k_cur < k_end is always false.
    - In ptdump, address comparison for markers display fails and the
    marker's name is printed at the start of the KASAN area instead of
    being printed at the end.
    
    However, there is no need to shadow the KASAN shadow area itself,
    so the KASAN shadow area can stop shadowing memory at the start
    of itself.
    
    With a PAGE_OFFSET set to 0xc0000000, KASAN shadow area is then going
    from 0xf8000000 to 0xff000000.
    
    Fixes: cbd18991e24f ("powerpc/mm: Fix an Oops in kasan_mmu_init()")
    Cc: stable@vger.kernel.org
    Signed-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/ae1a3c0d19a37410c209c3fc453634cfcc0ee318.1589866984.git.christophe.leroy@csgroup.eu

diff --git a/arch/powerpc/include/asm/kasan.h b/arch/powerpc/include/asm/kasan.h
index fbff9ff9032e..fc900937f653 100644
--- a/arch/powerpc/include/asm/kasan.h
+++ b/arch/powerpc/include/asm/kasan.h
@@ -23,9 +23,7 @@
 
 #define KASAN_SHADOW_OFFSET	ASM_CONST(CONFIG_KASAN_SHADOW_OFFSET)
 
-#define KASAN_SHADOW_END	0UL
-
-#define KASAN_SHADOW_SIZE	(KASAN_SHADOW_END - KASAN_SHADOW_START)
+#define KASAN_SHADOW_END	(-(-KASAN_SHADOW_START >> KASAN_SHADOW_SCALE_SHIFT))
 
 #ifdef CONFIG_KASAN
 void kasan_early_init(void);

commit 3d4247fcc938d0ab5cf6fdb752dae07fdeab9736
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Jan 14 17:54:00 2020 +0000

    powerpc/32: Add support of KASAN_VMALLOC
    
    Add support of KASAN_VMALLOC on PPC32.
    
    To allow this, the early shadow covering the VMALLOC space
    need to be removed once high_memory var is set and before
    freeing memblock.
    
    And the VMALLOC area need to be aligned such that boundaries
    are covered by a full shadow page.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/031dec5487bde9b2181c8b3c9800e1879cf98c1a.1579024426.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/kasan.h b/arch/powerpc/include/asm/kasan.h
index 296e51c2f066..fbff9ff9032e 100644
--- a/arch/powerpc/include/asm/kasan.h
+++ b/arch/powerpc/include/asm/kasan.h
@@ -31,9 +31,11 @@
 void kasan_early_init(void);
 void kasan_mmu_init(void);
 void kasan_init(void);
+void kasan_late_init(void);
 #else
 static inline void kasan_init(void) { }
 static inline void kasan_mmu_init(void) { }
+static inline void kasan_late_init(void) { }
 #endif
 
 #endif /* __ASSEMBLY */

commit 2edb16efc899f9c232e2d880930b855e4cf55df4
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 16:23:34 2019 +0000

    powerpc/32: Add KASAN support
    
    This patch adds KASAN support for PPC32. The following patch
    will add an early activation of hash table for book3s. Until
    then, a warning will be raised if trying to use KASAN on an
    hash 6xx.
    
    To support KASAN, this patch initialises that MMU mapings for
    accessing to the KASAN shadow area defined in a previous patch.
    
    An early mapping is set as soon as the kernel code has been
    relocated at its definitive place.
    
    Then the definitive mapping is set once paging is initialised.
    
    For modules, the shadow area is allocated at module_alloc().
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kasan.h b/arch/powerpc/include/asm/kasan.h
index 05274dea3109..296e51c2f066 100644
--- a/arch/powerpc/include/asm/kasan.h
+++ b/arch/powerpc/include/asm/kasan.h
@@ -27,5 +27,14 @@
 
 #define KASAN_SHADOW_SIZE	(KASAN_SHADOW_END - KASAN_SHADOW_START)
 
+#ifdef CONFIG_KASAN
+void kasan_early_init(void);
+void kasan_mmu_init(void);
+void kasan_init(void);
+#else
+static inline void kasan_init(void) { }
+static inline void kasan_mmu_init(void) { }
+#endif
+
 #endif /* __ASSEMBLY */
 #endif

commit b4abe38fd698ace6942edeeb79a5b8a60a7af4fa
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 16:23:32 2019 +0000

    powerpc/32: prepare shadow area for KASAN
    
    This patch prepares a shadow area for KASAN.
    
    The shadow area will be at the top of the kernel virtual
    memory space above the fixmap area and will occupy one
    eighth of the total kernel virtual memory space.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kasan.h b/arch/powerpc/include/asm/kasan.h
index 2c179a39d4ba..05274dea3109 100644
--- a/arch/powerpc/include/asm/kasan.h
+++ b/arch/powerpc/include/asm/kasan.h
@@ -12,4 +12,20 @@
 #define EXPORT_SYMBOL_KASAN(fn)
 #endif
 
+#ifndef __ASSEMBLY__
+
+#include <asm/page.h>
+
+#define KASAN_SHADOW_SCALE_SHIFT	3
+
+#define KASAN_SHADOW_START	(KASAN_SHADOW_OFFSET + \
+				 (PAGE_OFFSET >> KASAN_SHADOW_SCALE_SHIFT))
+
+#define KASAN_SHADOW_OFFSET	ASM_CONST(CONFIG_KASAN_SHADOW_OFFSET)
+
+#define KASAN_SHADOW_END	0UL
+
+#define KASAN_SHADOW_SIZE	(KASAN_SHADOW_END - KASAN_SHADOW_START)
+
+#endif /* __ASSEMBLY */
 #endif

commit 26deb04342e343ac58ab05bc7d2345ff0be9b667
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 16:23:26 2019 +0000

    powerpc: prepare string/mem functions for KASAN
    
    CONFIG_KASAN implements wrappers for memcpy() memmove() and memset()
    Those wrappers are doing the verification then call respectively
    __memcpy() __memmove() and __memset(). The arches are therefore
    expected to rename their optimised functions that way.
    
    For files on which KASAN is inhibited, #defines are used to allow
    them to directly call optimised versions of the functions without
    going through the KASAN wrappers.
    
    See commit 393f203f5fd5 ("x86_64: kasan: add interceptors for
    memset/memmove/memcpy functions") for details.
    
    Other string / mem functions do not (yet) have kasan wrappers,
    we therefore have to fallback to the generic versions when
    KASAN is active, otherwise KASAN checks will be skipped.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Fixups to keep selftests working]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kasan.h b/arch/powerpc/include/asm/kasan.h
new file mode 100644
index 000000000000..2c179a39d4ba
--- /dev/null
+++ b/arch/powerpc/include/asm/kasan.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __ASM_KASAN_H
+#define __ASM_KASAN_H
+
+#ifdef CONFIG_KASAN
+#define _GLOBAL_KASAN(fn)	_GLOBAL(__##fn)
+#define _GLOBAL_TOC_KASAN(fn)	_GLOBAL_TOC(__##fn)
+#define EXPORT_SYMBOL_KASAN(fn)	EXPORT_SYMBOL(__##fn)
+#else
+#define _GLOBAL_KASAN(fn)	_GLOBAL(fn)
+#define _GLOBAL_TOC_KASAN(fn)	_GLOBAL_TOC(fn)
+#define EXPORT_SYMBOL_KASAN(fn)
+#endif
+
+#endif
