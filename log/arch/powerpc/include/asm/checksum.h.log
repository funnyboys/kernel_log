commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 72a65d744a28..9cce06194dcc 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -1,12 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 #ifndef _ASM_POWERPC_CHECKSUM_H
 #define _ASM_POWERPC_CHECKSUM_H
 #ifdef __KERNEL__
 
 /*
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #include <linux/bitops.h>

commit d065ee93aab6ef4c2a5af5c455b5044bd5136547
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Feb 15 10:32:02 2019 +0000

    powerpc: drop unused GENERIC_CSUM Kconfig item
    
    Commit d4fde568a34a ("powerpc/64: Use optimized checksum routines on
    little-endian") converted last powerpc user of GENERIC_CSUM.
    
    This patch does a final cleanup dropping the Kconfig GENERIC_CSUM
    option which is always 'n', and associated piece of code in
    asm/checksum.h
    
    Fixes: d4fde568a34a ("powerpc/64: Use optimized checksum routines on little-endian")
    Reported-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index a78a57e5058d..72a65d744a28 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -9,9 +9,6 @@
  * 2 of the License, or (at your option) any later version.
  */
 
-#ifdef CONFIG_GENERIC_CSUM
-#include <asm-generic/checksum.h>
-#else
 #include <linux/bitops.h>
 #include <linux/in6.h>
 /*
@@ -217,6 +214,5 @@ __sum16 csum_ipv6_magic(const struct in6_addr *saddr,
 			const struct in6_addr *daddr,
 			__u32 len, __u8 proto, __wsum sum);
 
-#endif
 #endif /* __KERNEL__ */
 #endif

commit e9c4943a107b56696e4872cdffdba6b0c7277c77
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu May 24 11:33:18 2018 +0000

    powerpc: Implement csum_ipv6_magic in assembly
    
    The generic csum_ipv6_magic() generates a pretty bad result
    
    00000000 <csum_ipv6_magic>: (PPC32)
       0:   81 23 00 00     lwz     r9,0(r3)
       4:   81 03 00 04     lwz     r8,4(r3)
       8:   7c e7 4a 14     add     r7,r7,r9
       c:   7d 29 38 10     subfc   r9,r9,r7
      10:   7d 4a 51 10     subfe   r10,r10,r10
      14:   7d 27 42 14     add     r9,r7,r8
      18:   7d 2a 48 50     subf    r9,r10,r9
      1c:   80 e3 00 08     lwz     r7,8(r3)
      20:   7d 08 48 10     subfc   r8,r8,r9
      24:   7d 4a 51 10     subfe   r10,r10,r10
      28:   7d 29 3a 14     add     r9,r9,r7
      2c:   81 03 00 0c     lwz     r8,12(r3)
      30:   7d 2a 48 50     subf    r9,r10,r9
      34:   7c e7 48 10     subfc   r7,r7,r9
      38:   7d 4a 51 10     subfe   r10,r10,r10
      3c:   7d 29 42 14     add     r9,r9,r8
      40:   7d 2a 48 50     subf    r9,r10,r9
      44:   80 e4 00 00     lwz     r7,0(r4)
      48:   7d 08 48 10     subfc   r8,r8,r9
      4c:   7d 4a 51 10     subfe   r10,r10,r10
      50:   7d 29 3a 14     add     r9,r9,r7
      54:   7d 2a 48 50     subf    r9,r10,r9
      58:   81 04 00 04     lwz     r8,4(r4)
      5c:   7c e7 48 10     subfc   r7,r7,r9
      60:   7d 4a 51 10     subfe   r10,r10,r10
      64:   7d 29 42 14     add     r9,r9,r8
      68:   7d 2a 48 50     subf    r9,r10,r9
      6c:   80 e4 00 08     lwz     r7,8(r4)
      70:   7d 08 48 10     subfc   r8,r8,r9
      74:   7d 4a 51 10     subfe   r10,r10,r10
      78:   7d 29 3a 14     add     r9,r9,r7
      7c:   7d 2a 48 50     subf    r9,r10,r9
      80:   81 04 00 0c     lwz     r8,12(r4)
      84:   7c e7 48 10     subfc   r7,r7,r9
      88:   7d 4a 51 10     subfe   r10,r10,r10
      8c:   7d 29 42 14     add     r9,r9,r8
      90:   7d 2a 48 50     subf    r9,r10,r9
      94:   7d 08 48 10     subfc   r8,r8,r9
      98:   7d 4a 51 10     subfe   r10,r10,r10
      9c:   7d 29 2a 14     add     r9,r9,r5
      a0:   7d 2a 48 50     subf    r9,r10,r9
      a4:   7c a5 48 10     subfc   r5,r5,r9
      a8:   7c 63 19 10     subfe   r3,r3,r3
      ac:   7d 29 32 14     add     r9,r9,r6
      b0:   7d 23 48 50     subf    r9,r3,r9
      b4:   7c c6 48 10     subfc   r6,r6,r9
      b8:   7c 63 19 10     subfe   r3,r3,r3
      bc:   7c 63 48 50     subf    r3,r3,r9
      c0:   54 6a 80 3e     rotlwi  r10,r3,16
      c4:   7c 63 52 14     add     r3,r3,r10
      c8:   7c 63 18 f8     not     r3,r3
      cc:   54 63 84 3e     rlwinm  r3,r3,16,16,31
      d0:   4e 80 00 20     blr
    
    0000000000000000 <.csum_ipv6_magic>: (PPC64)
       0:   81 23 00 00     lwz     r9,0(r3)
       4:   80 03 00 04     lwz     r0,4(r3)
       8:   81 63 00 08     lwz     r11,8(r3)
       c:   7c e7 4a 14     add     r7,r7,r9
      10:   7f 89 38 40     cmplw   cr7,r9,r7
      14:   7d 47 02 14     add     r10,r7,r0
      18:   7d 30 10 26     mfocrf  r9,1
      1c:   55 29 f7 fe     rlwinm  r9,r9,30,31,31
      20:   7d 4a 4a 14     add     r10,r10,r9
      24:   7f 80 50 40     cmplw   cr7,r0,r10
      28:   7d 2a 5a 14     add     r9,r10,r11
      2c:   80 03 00 0c     lwz     r0,12(r3)
      30:   81 44 00 00     lwz     r10,0(r4)
      34:   7d 10 10 26     mfocrf  r8,1
      38:   55 08 f7 fe     rlwinm  r8,r8,30,31,31
      3c:   7d 29 42 14     add     r9,r9,r8
      40:   81 04 00 04     lwz     r8,4(r4)
      44:   7f 8b 48 40     cmplw   cr7,r11,r9
      48:   7d 29 02 14     add     r9,r9,r0
      4c:   7d 70 10 26     mfocrf  r11,1
      50:   55 6b f7 fe     rlwinm  r11,r11,30,31,31
      54:   7d 29 5a 14     add     r9,r9,r11
      58:   7f 80 48 40     cmplw   cr7,r0,r9
      5c:   7d 29 52 14     add     r9,r9,r10
      60:   7c 10 10 26     mfocrf  r0,1
      64:   54 00 f7 fe     rlwinm  r0,r0,30,31,31
      68:   7d 69 02 14     add     r11,r9,r0
      6c:   7f 8a 58 40     cmplw   cr7,r10,r11
      70:   7c 0b 42 14     add     r0,r11,r8
      74:   81 44 00 08     lwz     r10,8(r4)
      78:   7c f0 10 26     mfocrf  r7,1
      7c:   54 e7 f7 fe     rlwinm  r7,r7,30,31,31
      80:   7c 00 3a 14     add     r0,r0,r7
      84:   7f 88 00 40     cmplw   cr7,r8,r0
      88:   7d 20 52 14     add     r9,r0,r10
      8c:   80 04 00 0c     lwz     r0,12(r4)
      90:   7d 70 10 26     mfocrf  r11,1
      94:   55 6b f7 fe     rlwinm  r11,r11,30,31,31
      98:   7d 29 5a 14     add     r9,r9,r11
      9c:   7f 8a 48 40     cmplw   cr7,r10,r9
      a0:   7d 29 02 14     add     r9,r9,r0
      a4:   7d 70 10 26     mfocrf  r11,1
      a8:   55 6b f7 fe     rlwinm  r11,r11,30,31,31
      ac:   7d 29 5a 14     add     r9,r9,r11
      b0:   7f 80 48 40     cmplw   cr7,r0,r9
      b4:   7d 29 2a 14     add     r9,r9,r5
      b8:   7c 10 10 26     mfocrf  r0,1
      bc:   54 00 f7 fe     rlwinm  r0,r0,30,31,31
      c0:   7d 29 02 14     add     r9,r9,r0
      c4:   7f 85 48 40     cmplw   cr7,r5,r9
      c8:   7c 09 32 14     add     r0,r9,r6
      cc:   7d 50 10 26     mfocrf  r10,1
      d0:   55 4a f7 fe     rlwinm  r10,r10,30,31,31
      d4:   7c 00 52 14     add     r0,r0,r10
      d8:   7f 80 30 40     cmplw   cr7,r0,r6
      dc:   7d 30 10 26     mfocrf  r9,1
      e0:   55 29 ef fe     rlwinm  r9,r9,29,31,31
      e4:   7c 09 02 14     add     r0,r9,r0
      e8:   54 03 80 3e     rotlwi  r3,r0,16
      ec:   7c 03 02 14     add     r0,r3,r0
      f0:   7c 03 00 f8     not     r3,r0
      f4:   78 63 84 22     rldicl  r3,r3,48,48
      f8:   4e 80 00 20     blr
    
    This patch implements it in assembly for both PPC32 and PPC64
    
    Link: https://github.com/linuxppc/linux/issues/9
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Segher Boessenkool <segher@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 54065caa40b3..a78a57e5058d 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -13,6 +13,7 @@
 #include <asm-generic/checksum.h>
 #else
 #include <linux/bitops.h>
+#include <linux/in6.h>
 /*
  * Computes the checksum of a memory block at src, length len,
  * and adds in "sum" (32-bit), while copying the block to dst.
@@ -211,6 +212,11 @@ static inline __sum16 ip_compute_csum(const void *buff, int len)
 	return csum_fold(csum_partial(buff, len, 0));
 }
 
+#define _HAVE_ARCH_IPV6_CSUM
+__sum16 csum_ipv6_magic(const struct in6_addr *saddr,
+			const struct in6_addr *daddr,
+			__u32 len, __u8 proto, __wsum sum);
+
 #endif
 #endif /* __KERNEL__ */
 #endif

commit 55a0edf083022e402042255a0afb03d0b3a63a9b
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Apr 10 08:34:35 2018 +0200

    powerpc/64: optimises from64to32()
    
    The current implementation of from64to32() gives a poor result:
    
    0000000000000270 <.from64to32>:
     270:   38 00 ff ff     li      r0,-1
     274:   78 69 00 22     rldicl  r9,r3,32,32
     278:   78 00 00 20     clrldi  r0,r0,32
     27c:   7c 60 00 38     and     r0,r3,r0
     280:   7c 09 02 14     add     r0,r9,r0
     284:   78 09 00 22     rldicl  r9,r0,32,32
     288:   7c 00 4a 14     add     r0,r0,r9
     28c:   78 03 00 20     clrldi  r3,r0,32
     290:   4e 80 00 20     blr
    
    This patch modifies from64to32() to operate in the same
    spirit as csum_fold()
    
    It swaps the two 32-bit halves of sum then it adds it with the
    unswapped sum. If there is a carry from adding the two 32-bit halves,
    it will carry from the lower half into the upper half, giving us the
    correct sum in the upper half.
    
    The resulting code is:
    
    0000000000000260 <.from64to32>:
     260:   78 60 00 02     rotldi  r0,r3,32
     264:   7c 60 1a 14     add     r3,r0,r3
     268:   78 63 00 22     rldicl  r3,r3,32,32
     26c:   4e 80 00 20     blr
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 4e63787dc3be..54065caa40b3 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -12,6 +12,7 @@
 #ifdef CONFIG_GENERIC_CSUM
 #include <asm-generic/checksum.h>
 #else
+#include <linux/bitops.h>
 /*
  * Computes the checksum of a memory block at src, length len,
  * and adds in "sum" (32-bit), while copying the block to dst.
@@ -55,11 +56,7 @@ static inline __sum16 csum_fold(__wsum sum)
 
 static inline u32 from64to32(u64 x)
 {
-	/* add up 32-bit and 32-bit for 32+c bit */
-	x = (x & 0xffffffff) + (x >> 32);
-	/* add up carry.. */
-	x = (x & 0xffffffff) + (x >> 32);
-	return (u32)x;
+	return (x + ror64(x, 32)) >> 32;
 }
 
 static inline __wsum csum_tcpudp_nofold(__be32 saddr, __be32 daddr, __u32 len,

commit 96f391cf40ee5c9201cc7b55abe3903761e6a2e2
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Apr 10 08:34:37 2018 +0200

    Revert "powerpc/64: Fix checksum folding in csum_add()"
    
    This reverts commit 6ad966d7303b70165228dba1ee8da1a05c10eefe.
    
    That commit was pointless, because csum_add() sums two 32 bits
    values, so the sum is 0x1fffffffe at the maximum.
    And then when adding upper part (1) and lower part (0xfffffffe),
    the result is 0xffffffff which doesn't carry.
    Any lower value will not carry either.
    
    And behind the fact that this commit is useless, it also kills the
    whole purpose of having an arch specific inline csum_add()
    because the resulting code gets even worse than what is obtained
    with the generic implementation of csum_add()
    
    0000000000000240 <.csum_add>:
     240:   38 00 ff ff     li      r0,-1
     244:   7c 84 1a 14     add     r4,r4,r3
     248:   78 00 00 20     clrldi  r0,r0,32
     24c:   78 89 00 22     rldicl  r9,r4,32,32
     250:   7c 80 00 38     and     r0,r4,r0
     254:   7c 09 02 14     add     r0,r9,r0
     258:   78 09 00 22     rldicl  r9,r0,32,32
     25c:   7c 00 4a 14     add     r0,r0,r9
     260:   78 03 00 20     clrldi  r3,r0,32
     264:   4e 80 00 20     blr
    
    In comparison, the generic implementation of csum_add() gives:
    
    0000000000000290 <.csum_add>:
     290:   7c 63 22 14     add     r3,r3,r4
     294:   7f 83 20 40     cmplw   cr7,r3,r4
     298:   7c 10 10 26     mfocrf  r0,1
     29c:   54 00 ef fe     rlwinm  r0,r0,29,31,31
     2a0:   7c 60 1a 14     add     r3,r0,r3
     2a4:   78 63 00 20     clrldi  r3,r3,32
     2a8:   4e 80 00 20     blr
    
    And the reverted implementation for PPC64 gives:
    
    0000000000000240 <.csum_add>:
     240:   7c 84 1a 14     add     r4,r4,r3
     244:   78 80 00 22     rldicl  r0,r4,32,32
     248:   7c 80 22 14     add     r4,r0,r4
     24c:   78 83 00 20     clrldi  r3,r4,32
     250:   4e 80 00 20     blr
    
    Fixes: 6ad966d7303b7 ("powerpc/64: Fix checksum folding in csum_add()")
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 842124b199b5..4e63787dc3be 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -112,7 +112,7 @@ static inline __wsum csum_add(__wsum csum, __wsum addend)
 
 #ifdef __powerpc64__
 	res += (__force u64)addend;
-	return (__force __wsum) from64to32(res);
+	return (__force __wsum)((u32)res + (res >> 32));
 #else
 	asm("addc %0,%0,%1;"
 	    "addze %0,%0;"

commit 6ad966d7303b70165228dba1ee8da1a05c10eefe
Author: Shile Zhang <shile.zhang@nokia.com>
Date:   Sat Feb 4 17:03:40 2017 +0800

    powerpc/64: Fix checksum folding in csum_add()
    
    Paul's patch to fix checksum folding, commit b492f7e4e07a ("powerpc/64:
    Fix checksum folding in csum_tcpudp_nofold and ip_fast_csum_nofold")
    missed a case in csum_add(). Fix it.
    
    Signed-off-by: Shile Zhang <shile.zhang@nokia.com>
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 4e63787dc3be..842124b199b5 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -112,7 +112,7 @@ static inline __wsum csum_add(__wsum csum, __wsum addend)
 
 #ifdef __powerpc64__
 	res += (__force u64)addend;
-	return (__force __wsum)((u32)res + (res >> 32));
+	return (__force __wsum) from64to32(res);
 #else
 	asm("addc %0,%0,%1;"
 	    "addze %0,%0;"

commit d4fde568a34a93897dfb9ae64cfe9dda9d5c908c
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Nov 3 16:15:42 2016 +1100

    powerpc/64: Use optimized checksum routines on little-endian
    
    Currently we have optimized hand-coded assembly checksum routines for
    big-endian 64-bit systems, but for little-endian we use the generic C
    routines. This modifies the optimized routines to work for
    little-endian. With this, we no longer need to enable
    CONFIG_GENERIC_CSUM. This also fixes a couple of comments in
    checksum_64.S so they accurately reflect what the associated instruction
    does.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    [mpe: Use the more common __BIG_ENDIAN__]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 5b1a6e39afa7..4e63787dc3be 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -70,7 +70,11 @@ static inline __wsum csum_tcpudp_nofold(__be32 saddr, __be32 daddr, __u32 len,
 
 	s += (__force u32)saddr;
 	s += (__force u32)daddr;
+#ifdef __BIG_ENDIAN__
 	s += proto + len;
+#else
+	s += (proto + len) << 8;
+#endif
 	return (__force __wsum) from64to32(s);
 #else
     __asm__("\n\

commit b492f7e4e07a28e706db26cf4943bb0911435426
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Nov 3 16:10:55 2016 +1100

    powerpc/64: Fix checksum folding in csum_tcpudp_nofold and ip_fast_csum_nofold
    
    These functions compute an IP checksum by computing a 64-bit sum and
    folding it to 32 bits (the "nofold" in their names refers to folding
    down to 16 bits).  However, doing (u32) (s + (s >> 32)) is not
    sufficient to fold a 64-bit sum to 32 bits correctly.  The addition
    can produce a carry out from bit 31, which needs to be added in to
    the sum to produce the correct result.
    
    To fix this, we copy the from64to32() function from lib/checksum.c
    and use that.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 1e8fceb308a5..5b1a6e39afa7 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -53,17 +53,25 @@ static inline __sum16 csum_fold(__wsum sum)
 	return (__force __sum16)(~((__force u32)sum + tmp) >> 16);
 }
 
+static inline u32 from64to32(u64 x)
+{
+	/* add up 32-bit and 32-bit for 32+c bit */
+	x = (x & 0xffffffff) + (x >> 32);
+	/* add up carry.. */
+	x = (x & 0xffffffff) + (x >> 32);
+	return (u32)x;
+}
+
 static inline __wsum csum_tcpudp_nofold(__be32 saddr, __be32 daddr, __u32 len,
 					__u8 proto, __wsum sum)
 {
 #ifdef __powerpc64__
-	unsigned long s = (__force u32)sum;
+	u64 s = (__force u32)sum;
 
 	s += (__force u32)saddr;
 	s += (__force u32)daddr;
 	s += proto + len;
-	s += (s >> 32);
-	return (__force __wsum) s;
+	return (__force __wsum) from64to32(s);
 #else
     __asm__("\n\
 	addc %0,%0,%1 \n\
@@ -123,8 +131,7 @@ static inline __wsum ip_fast_csum_nofold(const void *iph, unsigned int ihl)
 
 	for (i = 0; i < ihl - 1; i++, ptr++)
 		s += *ptr;
-	s += (s >> 32);
-	return (__force __wsum)s;
+	return (__force __wsum)from64to32(s);
 #else
 	__wsum sum, tmp;
 

commit f9d4286b9516b02e795214412d36885f572b57ad
Author: Ivan Vecera <ivecera@redhat.com>
Date:   Thu Oct 27 16:30:06 2016 +0200

    arch/powerpc: Update parameters for csum_tcpudp_magic & csum_tcpudp_nofold
    
    Commit 01cfbad "ipv4: Update parameters for csum_tcpudp_magic to their
    original types" changed parameters for csum_tcpudp_magic and
    csum_tcpudp_nofold for many platforms but not for PowerPC.
    
    Fixes: 01cfbad "ipv4: Update parameters for csum_tcpudp_magic to their original types"
    Cc: Alexander Duyck <aduyck@mirantis.com>
    Signed-off-by: Ivan Vecera <ivecera@redhat.com>
    Acked-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index ee655ed1ff1b..1e8fceb308a5 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -53,10 +53,8 @@ static inline __sum16 csum_fold(__wsum sum)
 	return (__force __sum16)(~((__force u32)sum + tmp) >> 16);
 }
 
-static inline __wsum csum_tcpudp_nofold(__be32 saddr, __be32 daddr,
-                                     unsigned short len,
-                                     unsigned short proto,
-                                     __wsum sum)
+static inline __wsum csum_tcpudp_nofold(__be32 saddr, __be32 daddr, __u32 len,
+					__u8 proto, __wsum sum)
 {
 #ifdef __powerpc64__
 	unsigned long s = (__force u32)sum;
@@ -83,10 +81,8 @@ static inline __wsum csum_tcpudp_nofold(__be32 saddr, __be32 daddr,
  * computes the checksum of the TCP/UDP pseudo-header
  * returns a 16-bit checksum, already complemented
  */
-static inline __sum16 csum_tcpudp_magic(__be32 saddr, __be32 daddr,
-					unsigned short len,
-					unsigned short proto,
-					__wsum sum)
+static inline __sum16 csum_tcpudp_magic(__be32 saddr, __be32 daddr, __u32 len,
+					__u8 proto, __wsum sum)
 {
 	return csum_fold(csum_tcpudp_nofold(saddr, daddr, len, proto, sum));
 }

commit 7e393220b6e1ecfa5520d1b2ca31150b7588f458
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Mar 7 18:44:37 2016 +0100

    powerpc: optimise csum_partial() call when len is constant
    
    csum_partial is often called for small fixed length packets
    for which it is suboptimal to use the generic csum_partial()
    function.
    
    For instance, in my configuration, I got:
    * One place calling it with constant len 4
    * Seven places calling it with constant len 8
    * Three places calling it with constant len 14
    * One place calling it with constant len 20
    * One place calling it with constant len 24
    * One place calling it with constant len 32
    
    This patch renames csum_partial() to __csum_partial() and
    implements csum_partial() as a wrapper inline function which
    * uses csum_add() for small 16bits multiple constant length
    * uses ip_fast_csum() for other 32bits multiple constant
    * uses __csum_partial() in all other cases
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 74cd8d82628a..ee655ed1ff1b 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -12,20 +12,6 @@
 #ifdef CONFIG_GENERIC_CSUM
 #include <asm-generic/checksum.h>
 #else
-/*
- * computes the checksum of a memory block at buff, length len,
- * and adds in "sum" (32-bit)
- *
- * returns a 32-bit number suitable for feeding into itself
- * or csum_tcpudp_magic
- *
- * this function must be called with even lengths, except
- * for the last fragment, which may be odd
- *
- * it's best to have buff aligned on a 32-bit boundary
- */
-extern __wsum csum_partial(const void *buff, int len, __wsum sum);
-
 /*
  * Computes the checksum of a memory block at src, length len,
  * and adds in "sum" (32-bit), while copying the block to dst.
@@ -67,15 +53,6 @@ static inline __sum16 csum_fold(__wsum sum)
 	return (__force __sum16)(~((__force u32)sum + tmp) >> 16);
 }
 
-/*
- * this routine is used for miscellaneous IP-like checksums, mainly
- * in icmp.c
- */
-static inline __sum16 ip_compute_csum(const void *buff, int len)
-{
-	return csum_fold(csum_partial(buff, len, 0));
-}
-
 static inline __wsum csum_tcpudp_nofold(__be32 saddr, __be32 daddr,
                                      unsigned short len,
                                      unsigned short proto,
@@ -174,6 +151,62 @@ static inline __sum16 ip_fast_csum(const void *iph, unsigned int ihl)
 	return csum_fold(ip_fast_csum_nofold(iph, ihl));
 }
 
+/*
+ * computes the checksum of a memory block at buff, length len,
+ * and adds in "sum" (32-bit)
+ *
+ * returns a 32-bit number suitable for feeding into itself
+ * or csum_tcpudp_magic
+ *
+ * this function must be called with even lengths, except
+ * for the last fragment, which may be odd
+ *
+ * it's best to have buff aligned on a 32-bit boundary
+ */
+__wsum __csum_partial(const void *buff, int len, __wsum sum);
+
+static inline __wsum csum_partial(const void *buff, int len, __wsum sum)
+{
+	if (__builtin_constant_p(len) && len <= 16 && (len & 1) == 0) {
+		if (len == 2)
+			sum = csum_add(sum, (__force __wsum)*(const u16 *)buff);
+		if (len >= 4)
+			sum = csum_add(sum, (__force __wsum)*(const u32 *)buff);
+		if (len == 6)
+			sum = csum_add(sum, (__force __wsum)
+					    *(const u16 *)(buff + 4));
+		if (len >= 8)
+			sum = csum_add(sum, (__force __wsum)
+					    *(const u32 *)(buff + 4));
+		if (len == 10)
+			sum = csum_add(sum, (__force __wsum)
+					    *(const u16 *)(buff + 8));
+		if (len >= 12)
+			sum = csum_add(sum, (__force __wsum)
+					    *(const u32 *)(buff + 8));
+		if (len == 14)
+			sum = csum_add(sum, (__force __wsum)
+					    *(const u16 *)(buff + 12));
+		if (len >= 16)
+			sum = csum_add(sum, (__force __wsum)
+					    *(const u32 *)(buff + 12));
+	} else if (__builtin_constant_p(len) && (len & 3) == 0) {
+		sum = csum_add(sum, ip_fast_csum_nofold(buff, len >> 2));
+	} else {
+		sum = __csum_partial(buff, len, sum);
+	}
+	return sum;
+}
+
+/*
+ * this routine is used for miscellaneous IP-like checksums, mainly
+ * in icmp.c
+ */
+static inline __sum16 ip_compute_csum(const void *buff, int len)
+{
+	return csum_fold(csum_partial(buff, len, 0));
+}
+
 #endif
 #endif /* __KERNEL__ */
 #endif

commit 5a8847c83ce6072d6fdf0d15d9aa060c0b83537f
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Sep 22 16:34:34 2015 +0200

    powerpc: simplify csum_add(a, b) in case a or b is constant 0
    
    Simplify csum_add(a, b) in case a or b is constant 0
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 1778f75bf769..74cd8d82628a 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -119,7 +119,13 @@ static inline __wsum csum_add(__wsum csum, __wsum addend)
 {
 #ifdef __powerpc64__
 	u64 res = (__force u64)csum;
+#endif
+	if (__builtin_constant_p(csum) && csum == 0)
+		return addend;
+	if (__builtin_constant_p(addend) && addend == 0)
+		return csum;
 
+#ifdef __powerpc64__
 	res += (__force u64)addend;
 	return (__force __wsum)((u32)res + (res >> 32));
 #else

commit 37e08cad8f177f7bf6226a9b3724234ac3d3c81d
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Sep 22 16:34:25 2015 +0200

    powerpc: inline ip_fast_csum()
    
    In several architectures, ip_fast_csum() is inlined
    There are functions like ip_send_check() which do nothing
    much more than calling ip_fast_csum().
    Inlining ip_fast_csum() allows the compiler to optimise better
    
    Suggested-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [scottwood: whitespace and cast fixes]
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index afa6722cb7d2..1778f75bf769 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -9,16 +9,9 @@
  * 2 of the License, or (at your option) any later version.
  */
 
-/*
- * This is a version of ip_compute_csum() optimized for IP headers,
- * which always checksum on 4 octet boundaries.  ihl is the number
- * of 32-bit words and is always >= 5.
- */
 #ifdef CONFIG_GENERIC_CSUM
 #include <asm-generic/checksum.h>
 #else
-extern __sum16 ip_fast_csum(const void *iph, unsigned int ihl);
-
 /*
  * computes the checksum of a memory block at buff, length len,
  * and adds in "sum" (32-bit)
@@ -137,6 +130,44 @@ static inline __wsum csum_add(__wsum csum, __wsum addend)
 #endif
 }
 
+/*
+ * This is a version of ip_compute_csum() optimized for IP headers,
+ * which always checksum on 4 octet boundaries.  ihl is the number
+ * of 32-bit words and is always >= 5.
+ */
+static inline __wsum ip_fast_csum_nofold(const void *iph, unsigned int ihl)
+{
+	const u32 *ptr = (const u32 *)iph + 1;
+#ifdef __powerpc64__
+	unsigned int i;
+	u64 s = *(const u32 *)iph;
+
+	for (i = 0; i < ihl - 1; i++, ptr++)
+		s += *ptr;
+	s += (s >> 32);
+	return (__force __wsum)s;
+#else
+	__wsum sum, tmp;
+
+	asm("mtctr %3;"
+	    "addc %0,%4,%5;"
+	    "1: lwzu %1, 4(%2);"
+	    "adde %0,%0,%1;"
+	    "bdnz 1b;"
+	    "addze %0,%0;"
+	    : "=r" (sum), "=r" (tmp), "+b" (ptr)
+	    : "r" (ihl - 2), "r" (*(const u32 *)iph), "r" (*ptr)
+	    : "ctr", "xer", "memory");
+
+	return sum;
+#endif
+}
+
+static inline __sum16 ip_fast_csum(const void *iph, unsigned int ihl)
+{
+	return csum_fold(ip_fast_csum_nofold(iph, ihl));
+}
+
 #endif
 #endif /* __KERNEL__ */
 #endif

commit 03bc8b0fc87616c75c6dd060a2191e7fc8faacb6
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Sep 22 16:34:23 2015 +0200

    powerpc32: checksum_wrappers_64 becomes checksum_wrappers
    
    The powerpc64 checksum wrapper functions adds csum_and_copy_to_user()
    which otherwise is implemented in include/net/checksum.h by using
    csum_partial() then copy_to_user()
    
    Those two wrapper fonctions are also applicable to powerpc32 as it is
    based on the use of csum_partial_copy_generic() which also
    exists on powerpc32
    
    This patch renames arch/powerpc/lib/checksum_wrappers_64.c to
    arch/powerpc/lib/checksum_wrappers.c and
    makes it non-conditional to CONFIG_WORD_SIZE
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index d2ca07bb8837..afa6722cb7d2 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -47,21 +47,12 @@ extern __wsum csum_partial_copy_generic(const void *src, void *dst,
 					      int len, __wsum sum,
 					      int *src_err, int *dst_err);
 
-#ifdef __powerpc64__
 #define _HAVE_ARCH_COPY_AND_CSUM_FROM_USER
 extern __wsum csum_and_copy_from_user(const void __user *src, void *dst,
 				      int len, __wsum sum, int *err_ptr);
 #define HAVE_CSUM_COPY_USER
 extern __wsum csum_and_copy_to_user(const void *src, void __user *dst,
 				    int len, __wsum sum, int *err_ptr);
-#else
-/*
- * the same as csum_partial, but copies from src to dst while it
- * checksums.
- */
-#define csum_partial_copy_from_user(src, dst, len, sum, errp)   \
-        csum_partial_copy_generic((__force const void *)(src), (dst), (len), (sum), (errp), NULL)
-#endif
 
 #define csum_partial_copy_nocheck(src, dst, len, sum)   \
         csum_partial_copy_generic((src), (dst), (len), (sum), NULL, NULL)

commit 11dfbf588ae697bc5362c24294c2605f09c2d3d4
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Sep 22 16:34:21 2015 +0200

    powerpc: mark xer clobbered in csum_add()
    
    addc uses carry so xer is clobbered in csum_add()
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index e8d9ef4755a4..d2ca07bb8837 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -141,7 +141,7 @@ static inline __wsum csum_add(__wsum csum, __wsum addend)
 #else
 	asm("addc %0,%0,%1;"
 	    "addze %0,%0;"
-	    : "+r" (csum) : "r" (addend));
+	    : "+r" (csum) : "r" (addend) : "xer");
 	return csum;
 #endif
 }

commit 501c8de7b061d2dc0c21a0a79fee3eddf30af8dd
Author: LEROY Christophe <christophe.leroy@c-s.fr>
Date:   Tue May 19 17:18:57 2015 +0200

    powerpc: add support for csum_add()
    
    The C version of csum_add() as defined in include/net/checksum.h gives
    the following assembly in ppc32:
           0:       7c 04 1a 14     add     r0,r4,r3
           4:       7c 64 00 10     subfc   r3,r4,r0
           8:       7c 63 19 10     subfe   r3,r3,r3
           c:       7c 63 00 50     subf    r3,r3,r0
    and the following in ppc64:
       0xc000000000001af8 <+0>:     add     r3,r3,r4
       0xc000000000001afc <+4>:     cmplw   cr7,r3,r4
       0xc000000000001b00 <+8>:     mfcr    r4
       0xc000000000001b04 <+12>:    rlwinm  r4,r4,29,31,31
       0xc000000000001b08 <+16>:    add     r3,r4,r3
       0xc000000000001b0c <+20>:    clrldi  r3,r3,32
       0xc000000000001b10 <+24>:    blr
    
    include/net/checksum.h also offers the possibility to define an arch
    specific function.  This patch provides a specific csum_add() inline
    function.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 5e43d2d40084..e8d9ef4755a4 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -130,6 +130,22 @@ static inline __sum16 csum_tcpudp_magic(__be32 saddr, __be32 daddr,
 	return csum_fold(csum_tcpudp_nofold(saddr, daddr, len, proto, sum));
 }
 
+#define HAVE_ARCH_CSUM_ADD
+static inline __wsum csum_add(__wsum csum, __wsum addend)
+{
+#ifdef __powerpc64__
+	u64 res = (__force u64)csum;
+
+	res += (__force u64)addend;
+	return (__force __wsum)((u32)res + (res >> 32));
+#else
+	asm("addc %0,%0,%1;"
+	    "addze %0,%0;"
+	    : "+r" (csum) : "r" (addend));
+	return csum;
+#endif
+}
+
 #endif
 #endif /* __KERNEL__ */
 #endif

commit 92c985f1d76c228abe202cb3004f03c9b45860d1
Author: LEROY Christophe <christophe.leroy@c-s.fr>
Date:   Tue May 19 17:18:55 2015 +0200

    powerpc: put csum_tcpudp_magic inline
    
    csum_tcpudp_magic() is only a few instructions, and does modify
    really few registers. So it is not worth having it as a separate
    function and suffer function branching and saving of volatile
    registers.
    
    This patch makes it inline by use of the already existing
    csum_tcpudp_nofold() function.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 8251a3ba870f..5e43d2d40084 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -19,15 +19,6 @@
 #else
 extern __sum16 ip_fast_csum(const void *iph, unsigned int ihl);
 
-/*
- * computes the checksum of the TCP/UDP pseudo-header
- * returns a 16-bit checksum, already complemented
- */
-extern __sum16 csum_tcpudp_magic(__be32 saddr, __be32 daddr,
-					unsigned short len,
-					unsigned short proto,
-					__wsum sum);
-
 /*
  * computes the checksum of a memory block at buff, length len,
  * and adds in "sum" (32-bit)
@@ -127,6 +118,18 @@ static inline __wsum csum_tcpudp_nofold(__be32 saddr, __be32 daddr,
 #endif
 }
 
+/*
+ * computes the checksum of the TCP/UDP pseudo-header
+ * returns a 16-bit checksum, already complemented
+ */
+static inline __sum16 csum_tcpudp_magic(__be32 saddr, __be32 daddr,
+					unsigned short len,
+					unsigned short proto,
+					__wsum sum)
+{
+	return csum_fold(csum_tcpudp_nofold(saddr, daddr, len, proto, sum));
+}
+
 #endif
 #endif /* __KERNEL__ */
 #endif

commit 7a332b0c9a59e0b0777dec55eefdda0f9a24ac52
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Sep 23 12:04:51 2013 +1000

    powerpc: Use generic checksum code in little endian
    
    We need to fix some endian issues in our checksum code. For now
    just enable the generic checksum routines for little endian builds.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index ce0c28495f9a..8251a3ba870f 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -14,6 +14,9 @@
  * which always checksum on 4 octet boundaries.  ihl is the number
  * of 32-bit words and is always >= 5.
  */
+#ifdef CONFIG_GENERIC_CSUM
+#include <asm-generic/checksum.h>
+#else
 extern __sum16 ip_fast_csum(const void *iph, unsigned int ihl);
 
 /*
@@ -123,5 +126,7 @@ static inline __wsum csum_tcpudp_nofold(__be32 saddr, __be32 daddr,
 	return sum;
 #endif
 }
+
+#endif
 #endif /* __KERNEL__ */
 #endif

commit 8c77391475bc3284a380fc46aaf0bcf26bde3ae6
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Aug 2 20:11:36 2010 +0000

    powerpc: Add 64bit csum_and_copy_to_user
    
    This adds the equivalent of csum_and_copy_from_user for the receive side so we
    can copy and checksum in one pass. It is modelled on the generic checksum
    routine.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 9ea58c0e7cfb..ce0c28495f9a 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -57,6 +57,9 @@ extern __wsum csum_partial_copy_generic(const void *src, void *dst,
 #define _HAVE_ARCH_COPY_AND_CSUM_FROM_USER
 extern __wsum csum_and_copy_from_user(const void __user *src, void *dst,
 				      int len, __wsum sum, int *err_ptr);
+#define HAVE_CSUM_COPY_USER
+extern __wsum csum_and_copy_to_user(const void *src, void __user *dst,
+				    int len, __wsum sum, int *err_ptr);
 #else
 /*
  * the same as csum_partial, but copies from src to dst while it

commit fdd374b62ca4df144c0138359dcffa83df7a0ea8
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Aug 2 20:09:52 2010 +0000

    powerpc: Optimise 64bit csum_partial_copy_generic and add csum_and_copy_from_user
    
    We use the same core loop as the new csum_partial, adding in the
    stores and exception handling code. To keep things simple we do all the
    exception fixup in csum_and_copy_from_user. This wrapper function is
    modelled on the generic checksum code and is careful to always calculate
    a complete checksum even if we only copied part of the data to userspace.
    
    To test this I forced checksumming on over loopback and ran socklib (a
    simple TCP benchmark). On a POWER6 575 throughput improved by 19% with
    this patch. If I forced both the sender and receiver onto the same cpu
    (with the hope of shifting the benchmark from being cache bandwidth limited
    to cpu limited), adding this patch improved performance by 55%
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
index 7cdf358337cf..9ea58c0e7cfb 100644
--- a/arch/powerpc/include/asm/checksum.h
+++ b/arch/powerpc/include/asm/checksum.h
@@ -52,12 +52,19 @@ extern __wsum csum_partial(const void *buff, int len, __wsum sum);
 extern __wsum csum_partial_copy_generic(const void *src, void *dst,
 					      int len, __wsum sum,
 					      int *src_err, int *dst_err);
+
+#ifdef __powerpc64__
+#define _HAVE_ARCH_COPY_AND_CSUM_FROM_USER
+extern __wsum csum_and_copy_from_user(const void __user *src, void *dst,
+				      int len, __wsum sum, int *err_ptr);
+#else
 /*
  * the same as csum_partial, but copies from src to dst while it
  * checksums.
  */
 #define csum_partial_copy_from_user(src, dst, len, sum, errp)   \
         csum_partial_copy_generic((__force const void *)(src), (dst), (len), (sum), (errp), NULL)
+#endif
 
 #define csum_partial_copy_nocheck(src, dst, len, sum)   \
         csum_partial_copy_generic((src), (dst), (len), (sum), NULL, NULL)

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/checksum.h b/arch/powerpc/include/asm/checksum.h
new file mode 100644
index 000000000000..7cdf358337cf
--- /dev/null
+++ b/arch/powerpc/include/asm/checksum.h
@@ -0,0 +1,117 @@
+#ifndef _ASM_POWERPC_CHECKSUM_H
+#define _ASM_POWERPC_CHECKSUM_H
+#ifdef __KERNEL__
+
+/*
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+/*
+ * This is a version of ip_compute_csum() optimized for IP headers,
+ * which always checksum on 4 octet boundaries.  ihl is the number
+ * of 32-bit words and is always >= 5.
+ */
+extern __sum16 ip_fast_csum(const void *iph, unsigned int ihl);
+
+/*
+ * computes the checksum of the TCP/UDP pseudo-header
+ * returns a 16-bit checksum, already complemented
+ */
+extern __sum16 csum_tcpudp_magic(__be32 saddr, __be32 daddr,
+					unsigned short len,
+					unsigned short proto,
+					__wsum sum);
+
+/*
+ * computes the checksum of a memory block at buff, length len,
+ * and adds in "sum" (32-bit)
+ *
+ * returns a 32-bit number suitable for feeding into itself
+ * or csum_tcpudp_magic
+ *
+ * this function must be called with even lengths, except
+ * for the last fragment, which may be odd
+ *
+ * it's best to have buff aligned on a 32-bit boundary
+ */
+extern __wsum csum_partial(const void *buff, int len, __wsum sum);
+
+/*
+ * Computes the checksum of a memory block at src, length len,
+ * and adds in "sum" (32-bit), while copying the block to dst.
+ * If an access exception occurs on src or dst, it stores -EFAULT
+ * to *src_err or *dst_err respectively (if that pointer is not
+ * NULL), and, for an error on src, zeroes the rest of dst.
+ *
+ * Like csum_partial, this must be called with even lengths,
+ * except for the last fragment.
+ */
+extern __wsum csum_partial_copy_generic(const void *src, void *dst,
+					      int len, __wsum sum,
+					      int *src_err, int *dst_err);
+/*
+ * the same as csum_partial, but copies from src to dst while it
+ * checksums.
+ */
+#define csum_partial_copy_from_user(src, dst, len, sum, errp)   \
+        csum_partial_copy_generic((__force const void *)(src), (dst), (len), (sum), (errp), NULL)
+
+#define csum_partial_copy_nocheck(src, dst, len, sum)   \
+        csum_partial_copy_generic((src), (dst), (len), (sum), NULL, NULL)
+
+
+/*
+ * turns a 32-bit partial checksum (e.g. from csum_partial) into a
+ * 1's complement 16-bit checksum.
+ */
+static inline __sum16 csum_fold(__wsum sum)
+{
+	unsigned int tmp;
+
+	/* swap the two 16-bit halves of sum */
+	__asm__("rlwinm %0,%1,16,0,31" : "=r" (tmp) : "r" (sum));
+	/* if there is a carry from adding the two 16-bit halves,
+	   it will carry from the lower half into the upper half,
+	   giving us the correct sum in the upper half. */
+	return (__force __sum16)(~((__force u32)sum + tmp) >> 16);
+}
+
+/*
+ * this routine is used for miscellaneous IP-like checksums, mainly
+ * in icmp.c
+ */
+static inline __sum16 ip_compute_csum(const void *buff, int len)
+{
+	return csum_fold(csum_partial(buff, len, 0));
+}
+
+static inline __wsum csum_tcpudp_nofold(__be32 saddr, __be32 daddr,
+                                     unsigned short len,
+                                     unsigned short proto,
+                                     __wsum sum)
+{
+#ifdef __powerpc64__
+	unsigned long s = (__force u32)sum;
+
+	s += (__force u32)saddr;
+	s += (__force u32)daddr;
+	s += proto + len;
+	s += (s >> 32);
+	return (__force __wsum) s;
+#else
+    __asm__("\n\
+	addc %0,%0,%1 \n\
+	adde %0,%0,%2 \n\
+	adde %0,%0,%3 \n\
+	addze %0,%0 \n\
+	"
+	: "=r" (sum)
+	: "r" (daddr), "r"(saddr), "r"(proto + len), "0"(sum));
+	return sum;
+#endif
+}
+#endif /* __KERNEL__ */
+#endif
