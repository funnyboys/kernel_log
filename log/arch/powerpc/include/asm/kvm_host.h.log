commit 2610a57f64d55b5d09340c2b716cf20922b88605
Author: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
Date:   Mon Apr 27 12:35:10 2020 +0800

    KVM: PPC: Remove redundant kvm_run from vcpu_arch
    
    The 'kvm_run' field already exists in the 'vcpu' structure, which
    is the same structure as the 'kvm_run' in the 'vcpu_arch' and
    should be deleted.
    
    Signed-off-by: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 337047ba4a56..7e2d061d0445 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -795,7 +795,6 @@ struct kvm_vcpu_arch {
 	struct mmio_hpte_cache_entry *pgfault_cache;
 
 	struct task_struct *run_task;
-	struct kvm_run *kvm_run;
 
 	spinlock_t vpa_update_lock;
 	struct kvmppc_vpa vpa;

commit da4ad88cab5867ee240dfd0585e9d115a8cc47db
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Thu Apr 23 22:48:37 2020 -0700

    kvm: Replace vcpu->swait with rcuwait
    
    The use of any sort of waitqueue (simple or regular) for
    wait/waking vcpus has always been an overkill and semantically
    wrong. Because this is per-vcpu (which is blocked) there is
    only ever a single waiting vcpu, thus no need for any sort of
    queue.
    
    As such, make use of the rcuwait primitive, with the following
    considerations:
    
      - rcuwait already provides the proper barriers that serialize
      concurrent waiter and waker.
    
      - Task wakeup is done in rcu read critical region, with a
      stable task pointer.
    
      - Because there is no concurrency among waiters, we need
      not worry about rcuwait_wait_event() calls corrupting
      the wait->task. As a consequence, this saves the locking
      done in swait when modifying the queue. This also applies
      to per-vcore wait for powerpc kvm-hv.
    
    The x86 tscdeadline_latency test mentioned in 8577370fb0cb
    ("KVM: Use simple waitqueue for vcpu->wq") shows that, on avg,
    latency is reduced by around 15-20% with this change.
    
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: kvmarm@lists.cs.columbia.edu
    Cc: linux-mips@vger.kernel.org
    Reviewed-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Message-Id: <20200424054837.5138-6-dave@stgolabs.net>
    [Avoid extra logic changes. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1dc63101ffe1..337047ba4a56 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -751,7 +751,7 @@ struct kvm_vcpu_arch {
 	u8 irq_pending; /* Used by XIVE to signal pending guest irqs */
 	u32 last_inst;
 
-	struct swait_queue_head *wqp;
+	struct rcuwait *waitp;
 	struct kvmppc_vcore *vcore;
 	int ret;
 	int trap;

commit d38c07afc356ddebaa3ed8ecb3f553340e05c969
Merge: 31c0aa87ec8a c17eb4dca5a3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 5 11:12:59 2020 -0700

    Merge tag 'powerpc-5.7-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Slightly late as I had to rebase mid-week to insert a bug fix:
    
       - A large series from Nick for 64-bit to further rework our exception
         vectors, and rewrite portions of the syscall entry/exit and
         interrupt return in C. The result is much easier to follow code
         that is also faster in general.
    
       - Cleanup of our ptrace code to split various parts out that had
         become badly intertwined with #ifdefs over the years.
    
       - Changes to our NUMA setup under the PowerVM hypervisor which should
         hopefully avoid non-sensical topologies which can lead to warnings
         from the workqueue code and other problems.
    
       - MAINTAINERS updates to remove some of our old orphan entries and
         update the status of others.
    
       - Quite a few other small changes and fixes all over the map.
    
      Thanks to: Abdul Haleem, afzal mohammed, Alexey Kardashevskiy, Andrew
      Donnellan, Aneesh Kumar K.V, Balamuruhan S, Cédric Le Goater, Chen
      Zhou, Christophe JAILLET, Christophe Leroy, Christoph Hellwig, Clement
      Courbet, Daniel Axtens, David Gibson, Douglas Miller, Fabiano Rosas,
      Fangrui Song, Ganesh Goudar, Gautham R. Shenoy, Greg Kroah-Hartman,
      Greg Kurz, Gustavo Luiz Duarte, Hari Bathini, Ilie Halip, Jan Kara,
      Joe Lawrence, Joe Perches, Kajol Jain, Larry Finger, Laurentiu Tudor,
      Leonardo Bras, Libor Pechacek, Madhavan Srinivasan, Mahesh Salgaonkar,
      Masahiro Yamada, Masami Hiramatsu, Mauricio Faria de Oliveira, Michael
      Neuling, Michal Suchanek, Mike Rapoport, Nageswara R Sastry, Nathan
      Chancellor, Nathan Lynch, Naveen N. Rao, Nicholas Piggin, Nick
      Desaulniers, Oliver O'Halloran, Po-Hsu Lin, Pratik Rajesh Sampat,
      Rasmus Villemoes, Ravi Bangoria, Roman Bolshakov, Sam Bobroff,
      Sandipan Das, Santosh S, Sedat Dilek, Segher Boessenkool, Shilpasri G
      Bhat, Sourabh Jain, Srikar Dronamraju, Stephen Rothwell, Tyrel
      Datwyler, Vaibhav Jain, YueHaibing"
    
    * tag 'powerpc-5.7-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (158 commits)
      powerpc: Make setjmp/longjmp signature standard
      powerpc/cputable: Remove unnecessary copy of cpu_spec->oprofile_type
      powerpc: Suppress .eh_frame generation
      powerpc: Drop -fno-dwarf2-cfi-asm
      powerpc/32: drop unused ISA_DMA_THRESHOLD
      powerpc/powernv: Add documentation for the opal sensor_groups sysfs interfaces
      selftests/powerpc: Fix try-run when source tree is not writable
      powerpc/vmlinux.lds: Explicitly retain .gnu.hash
      powerpc/ptrace: move ptrace_triggered() into hw_breakpoint.c
      powerpc/ptrace: create ppc_gethwdinfo()
      powerpc/ptrace: create ptrace_get_debugreg()
      powerpc/ptrace: split out ADV_DEBUG_REGS related functions.
      powerpc/ptrace: move register viewing functions out of ptrace.c
      powerpc/ptrace: split out TRANSACTIONAL_MEM related functions.
      powerpc/ptrace: split out SPE related functions.
      powerpc/ptrace: split out ALTIVEC related functions.
      powerpc/ptrace: split out VSX related functions.
      powerpc/ptrace: drop PARAMETER_SAVE_AREA_OFFSET
      powerpc/ptrace: drop unnecessary #ifdefs CONFIG_PPC64
      powerpc/ptrace: remove unused header includes
      ...

commit 9a5788c615f52f6d7bf0b61986a632d4ec86791d
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Mar 19 15:29:55 2020 +1100

    KVM: PPC: Book3S HV: Add a capability for enabling secure guests
    
    At present, on Power systems with Protected Execution Facility
    hardware and an ultravisor, a KVM guest can transition to being a
    secure guest at will.  Userspace (QEMU) has no way of knowing
    whether a host system is capable of running secure guests.  This
    will present a problem in future when the ultravisor is capable of
    migrating secure guests from one host to another, because
    virtualization management software will have no way to ensure that
    secure guests only run in domains where all of the hosts can
    support secure guests.
    
    This adds a VM capability which has two functions: (a) userspace
    can query it to find out whether the host can support secure guests,
    and (b) userspace can enable it for a guest, which allows that
    guest to become a secure guest.  If userspace does not enable it,
    KVM will return an error when the ultravisor does the hypercall
    that indicates that the guest is starting to transition to a
    secure guest.  The ultravisor will then abort the transition and
    the guest will terminate.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Ram Pai <linuxram@us.ibm.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 6e8b8ffd06ad..f99b4333dfba 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -303,6 +303,7 @@ struct kvm_arch {
 	u8 radix;
 	u8 fwnmi_enabled;
 	u8 secure_guest;
+	u8 svm_enabled;
 	bool threads_indep;
 	bool nested_enable;
 	pgd_t *pgtable;

commit c4fd527f52ecb135018655c7f56f87800872c5bc
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Sun Feb 9 11:58:57 2020 +0100

    powerpc/kvm: no need to check return value of debugfs_create functions
    
    When calling debugfs functions, there is no need to ever check the
    return value.  The function can work or not, but the code logic should
    never do something different based on this.
    
    Because of this cleanup, we get to remove a few fields in struct
    kvm_arch that are now unused.
    
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    [mpe: Fix build error in kvm/timing.c, adapt kvmppc_remove_cpu_debugfs()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200209105901.1620958-2-gregkh@linuxfoundation.org

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 6e8b8ffd06ad..877f8aa2bc1e 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -308,8 +308,6 @@ struct kvm_arch {
 	pgd_t *pgtable;
 	u64 process_table;
 	struct dentry *debugfs_dir;
-	struct dentry *htab_dentry;
-	struct dentry *radix_dentry;
 	struct kvm_resize_hpt *resize_hpt; /* protected by kvm->lock */
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 #ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
@@ -830,7 +828,6 @@ struct kvm_vcpu_arch {
 	struct kvmhv_tb_accumulator cede_time;	/* time napping inside guest */
 
 	struct dentry *debugfs_dir;
-	struct dentry *debugfs_timings;
 #endif /* CONFIG_KVM_BOOK3S_HV_EXIT_TIMING */
 };
 

commit 3a43970d55e9fd5475d3c4e5fe398ab831ec6c3a
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Mon Jan 6 18:02:37 2020 -0800

    KVM: PPC: Book3S HV: Implement H_SVM_INIT_ABORT hcall
    
    Implement the H_SVM_INIT_ABORT hcall which the Ultravisor can use to
    abort an SVM after it has issued the H_SVM_INIT_START and before the
    H_SVM_INIT_DONE hcalls. This hcall could be used when Ultravisor
    encounters security violations or other errors when starting an SVM.
    
    Note that this hcall is different from UV_SVM_TERMINATE ucall which
    is used by HV to terminate/cleanup an VM that has becore secure.
    
    The H_SVM_INIT_ABORT basically undoes operations that were done
    since the H_SVM_INIT_START hcall - i.e page-out all the VM pages back
    to normal memory, and terminate the SVM.
    
    (If we do not bring the pages back to normal memory, the text/data
    of the VM would be stuck in secure memory and since the SVM did not
    go secure, its MSR_S bit will be clear and the VM wont be able to
    access its pages even to do a clean exit).
    
    Based on patches and discussion with Paul Mackerras, Ram Pai and
    Bharata Rao.
    
    Signed-off-by: Ram Pai <linuxram@linux.ibm.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.ibm.com>
    Signed-off-by: Bharata B Rao <bharata@linux.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0a398f2321c2..6e8b8ffd06ad 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -278,6 +278,7 @@ struct kvm_resize_hpt;
 /* Flag values for kvm_arch.secure_guest */
 #define KVMPPC_SECURE_INIT_START 0x1 /* H_SVM_INIT_START has been called */
 #define KVMPPC_SECURE_INIT_DONE  0x2 /* H_SVM_INIT_DONE completed */
+#define KVMPPC_SECURE_INIT_ABORT 0x4 /* H_SVM_INIT_ABORT issued */
 
 struct kvm_arch {
 	unsigned int lpid;

commit ca9f4942670c37407bb109090eaf776ce2ccc54c
Author: Bharata B Rao <bharata@linux.ibm.com>
Date:   Mon Nov 25 08:36:26 2019 +0530

    KVM: PPC: Book3S HV: Support for running secure guests
    
    A pseries guest can be run as secure guest on Ultravisor-enabled
    POWER platforms. On such platforms, this driver will be used to manage
    the movement of guest pages between the normal memory managed by
    hypervisor (HV) and secure memory managed by Ultravisor (UV).
    
    HV is informed about the guest's transition to secure mode via hcalls:
    
    H_SVM_INIT_START: Initiate securing a VM
    H_SVM_INIT_DONE: Conclude securing a VM
    
    As part of H_SVM_INIT_START, register all existing memslots with
    the UV. H_SVM_INIT_DONE call by UV informs HV that transition of
    the guest to secure mode is complete.
    
    These two states (transition to secure mode STARTED and transition
    to secure mode COMPLETED) are recorded in kvm->arch.secure_guest.
    Setting these states will cause the assembly code that enters the
    guest to call the UV_RETURN ucall instead of trying to enter the
    guest directly.
    
    Migration of pages betwen normal and secure memory of secure
    guest is implemented in H_SVM_PAGE_IN and H_SVM_PAGE_OUT hcalls.
    
    H_SVM_PAGE_IN: Move the content of a normal page to secure page
    H_SVM_PAGE_OUT: Move the content of a secure page to normal page
    
    Private ZONE_DEVICE memory equal to the amount of secure memory
    available in the platform for running secure guests is created.
    Whenever a page belonging to the guest becomes secure, a page from
    this private device memory is used to represent and track that secure
    page on the HV side. The movement of pages between normal and secure
    memory is done via migrate_vma_pages() using UV_PAGE_IN and
    UV_PAGE_OUT ucalls.
    
    In order to prevent the device private pages (that correspond to pages
    of secure guest) from participating in KSM merging, H_SVM_PAGE_IN
    calls ksm_madvise() under read version of mmap_sem. However
    ksm_madvise() needs to be under write lock.  Hence we call
    kvmppc_svm_page_in with mmap_sem held for writing, and it then
    downgrades to a read lock after calling ksm_madvise.
    
    [paulus@ozlabs.org - roll in patch "KVM: PPC: Book3S HV: Take write
     mmap_sem when calling ksm_madvise"]
    
    Signed-off-by: Bharata B Rao <bharata@linux.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 4273e799203d..0a398f2321c2 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -275,6 +275,10 @@ struct kvm_hpt_info {
 
 struct kvm_resize_hpt;
 
+/* Flag values for kvm_arch.secure_guest */
+#define KVMPPC_SECURE_INIT_START 0x1 /* H_SVM_INIT_START has been called */
+#define KVMPPC_SECURE_INIT_DONE  0x2 /* H_SVM_INIT_DONE completed */
+
 struct kvm_arch {
 	unsigned int lpid;
 	unsigned int smt_mode;		/* # vcpus per virtual core */
@@ -330,6 +334,8 @@ struct kvm_arch {
 #endif
 	struct kvmppc_ops *kvm_ops;
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	struct mutex uvmem_lock;
+	struct list_head uvmem_pfns;
 	struct mutex mmu_setup_lock;	/* nests inside vcpu mutexes */
 	u64 l1_ptcr;
 	int max_nested_lpid;

commit 87a45e07a5abfec4d6b0e8356718f8919d0a3c20
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Oct 2 16:00:22 2019 +1000

    KVM: PPC: Book3S: Replace reset_msr mmu op with inject_interrupt arch op
    
    reset_msr sets the MSR for interrupt injection, but it's cleaner and
    more flexible to provide a single op to set both MSR and PC for the
    interrupt.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 6fe6ad64cba5..4273e799203d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -401,7 +401,6 @@ struct kvmppc_mmu {
 	u32  (*mfsrin)(struct kvm_vcpu *vcpu, u32 srnum);
 	int  (*xlate)(struct kvm_vcpu *vcpu, gva_t eaddr,
 		      struct kvmppc_pte *pte, bool data, bool iswrite);
-	void (*reset_msr)(struct kvm_vcpu *vcpu);
 	void (*tlbie)(struct kvm_vcpu *vcpu, ulong addr, bool large);
 	int  (*esid_to_vsid)(struct kvm_vcpu *vcpu, ulong esid, u64 *vsid);
 	u64  (*ea_to_vp)(struct kvm_vcpu *vcpu, gva_t eaddr, bool data);

commit 45824fc0da6e46cc5d563105e1eaaf3098a686f9
Merge: 8c2b418c3f95 d9101bfa6adc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 20 11:48:06 2019 -0700

    Merge tag 'powerpc-5.4-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "This is a bit late, partly due to me travelling, and partly due to a
      power outage knocking out some of my test systems *while* I was
      travelling.
    
       - Initial support for running on a system with an Ultravisor, which
         is software that runs below the hypervisor and protects guests
         against some attacks by the hypervisor.
    
       - Support for building the kernel to run as a "Secure Virtual
         Machine", ie. as a guest capable of running on a system with an
         Ultravisor.
    
       - Some changes to our DMA code on bare metal, to allow devices with
         medium sized DMA masks (> 32 && < 59 bits) to use more than 2GB of
         DMA space.
    
       - Support for firmware assisted crash dumps on bare metal (powernv).
    
       - Two series fixing bugs in and refactoring our PCI EEH code.
    
       - A large series refactoring our exception entry code to use gas
         macros, both to make it more readable and also enable some future
         optimisations.
    
      As well as many cleanups and other minor features & fixups.
    
      Thanks to: Adam Zerella, Alexey Kardashevskiy, Alistair Popple, Andrew
      Donnellan, Aneesh Kumar K.V, Anju T Sudhakar, Anshuman Khandual,
      Balbir Singh, Benjamin Herrenschmidt, Cédric Le Goater, Christophe
      JAILLET, Christophe Leroy, Christopher M. Riedl, Christoph Hellwig,
      Claudio Carvalho, Daniel Axtens, David Gibson, David Hildenbrand,
      Desnes A. Nunes do Rosario, Ganesh Goudar, Gautham R. Shenoy, Greg
      Kurz, Guerney Hunt, Gustavo Romero, Halil Pasic, Hari Bathini, Joakim
      Tjernlund, Jonathan Neuschafer, Jordan Niethe, Leonardo Bras, Lianbo
      Jiang, Madhavan Srinivasan, Mahesh Salgaonkar, Mahesh Salgaonkar,
      Masahiro Yamada, Maxiwell S. Garcia, Michael Anderson, Nathan
      Chancellor, Nathan Lynch, Naveen N. Rao, Nicholas Piggin, Oliver
      O'Halloran, Qian Cai, Ram Pai, Ravi Bangoria, Reza Arbab, Ryan Grimm,
      Sam Bobroff, Santosh Sivaraj, Segher Boessenkool, Sukadev Bhattiprolu,
      Thiago Bauermann, Thiago Jung Bauermann, Thomas Gleixner, Tom
      Lendacky, Vasant Hegde"
    
    * tag 'powerpc-5.4-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (264 commits)
      powerpc/mm/mce: Keep irqs disabled during lockless page table walk
      powerpc: Use ftrace_graph_ret_addr() when unwinding
      powerpc/ftrace: Enable HAVE_FUNCTION_GRAPH_RET_ADDR_PTR
      ftrace: Look up the address of return_to_handler() using helpers
      powerpc: dump kernel log before carrying out fadump or kdump
      docs: powerpc: Add missing documentation reference
      powerpc/xmon: Fix output of XIVE IPI
      powerpc/xmon: Improve output of XIVE interrupts
      powerpc/mm/radix: remove useless kernel messages
      powerpc/fadump: support holes in kernel boot memory area
      powerpc/fadump: remove RMA_START and RMA_END macros
      powerpc/fadump: update documentation about option to release opalcore
      powerpc/fadump: consider f/w load area
      powerpc/opalcore: provide an option to invalidate /sys/firmware/opal/core file
      powerpc/opalcore: export /sys/firmware/opal/core for analysing opal crashes
      powerpc/fadump: update documentation about CONFIG_PRESERVE_FA_DUMP
      powerpc/fadump: add support to preserve crash data on FADUMP disabled kernel
      powerpc/fadump: improve how crashed kernel's memory is reserved
      powerpc/fadump: consider reserved ranges while releasing memory
      powerpc/fadump: make crash memory ranges array allocation generic
      ...

commit 6c85b7bc637b64e681760f62c0eafba2f56745c6
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Thu Aug 22 00:48:38 2019 -0300

    powerpc/kvm: Use UV_RETURN ucall to return to ultravisor
    
    When an SVM makes an hypercall or incurs some other exception, the
    Ultravisor usually forwards (a.k.a. reflects) the exceptions to the
    Hypervisor. After processing the exception, Hypervisor uses the
    UV_RETURN ultracall to return control back to the SVM.
    
    The expected register state on entry to this ultracall is:
    
    * Non-volatile registers are restored to their original values.
    * If returning from an hypercall, register R0 contains the return value
      (unlike other ultracalls) and, registers R4 through R12 contain any
      output values of the hypercall.
    * R3 contains the ultracall number, i.e UV_RETURN.
    * If returning with a synthesized interrupt, R2 contains the
      synthesized interrupt number.
    
    Thanks to input from Paul Mackerras, Ram Pai and Mike Anderson.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Claudio Carvalho <cclaudio@linux.ibm.com>
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190822034838.27876-8-cclaudio@linux.ibm.com

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e6e5f59aaa97..4bb552d639b8 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -283,6 +283,7 @@ struct kvm_arch {
 	cpumask_t cpu_in_guest;
 	u8 radix;
 	u8 fwnmi_enabled;
+	u8 secure_guest;
 	bool threads_indep;
 	bool nested_enable;
 	pgd_t *pgtable;

commit d22deab6960a6cb015a36e74a2dcbab6ca9f5544
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Tue Aug 20 16:13:49 2019 +1000

    KVM: PPC: Book3S HV: Define usage types for rmap array in guest memslot
    
    The rmap array in the guest memslot is an array of size number of guest
    pages, allocated at memslot creation time. Each rmap entry in this array
    is used to store information about the guest page to which it
    corresponds. For example for a hpt guest it is used to store a lock bit,
    rc bits, a present bit and the index of a hpt entry in the guest hpt
    which maps this page. For a radix guest which is running nested guests
    it is used to store a pointer to a linked list of nested rmap entries
    which store the nested guest physical address which maps this guest
    address and for which there is a pte in the shadow page table.
    
    As there are currently two uses for the rmap array, and the potential
    for this to expand to more in the future, define a type field (being the
    top 8 bits of the rmap entry) to be used to define the type of the rmap
    entry which is currently present and define two values for this field
    for the two current uses of the rmap array.
    
    Since the nested case uses the rmap entry to store a pointer, define
    this type as having the two high bits set as is expected for a pointer.
    Define the hpt entry type as having bit 56 set (bit 7 IBM bit ordering).
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e6e5f59aaa97..6fb5fb4779e0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -232,11 +232,25 @@ struct revmap_entry {
 };
 
 /*
- * We use the top bit of each memslot->arch.rmap entry as a lock bit,
- * and bit 32 as a present flag.  The bottom 32 bits are the
- * index in the guest HPT of a HPTE that points to the page.
+ * The rmap array of size number of guest pages is allocated for each memslot.
+ * This array is used to store usage specific information about the guest page.
+ * Below are the encodings of the various possible usage types.
  */
-#define KVMPPC_RMAP_LOCK_BIT	63
+/* Free bits which can be used to define a new usage */
+#define KVMPPC_RMAP_TYPE_MASK	0xff00000000000000
+#define KVMPPC_RMAP_NESTED	0xc000000000000000	/* Nested rmap array */
+#define KVMPPC_RMAP_HPT		0x0100000000000000	/* HPT guest */
+
+/*
+ * rmap usage definition for a hash page table (hpt) guest:
+ * 0x0000080000000000	Lock bit
+ * 0x0000018000000000	RC bits
+ * 0x0000000100000000	Present bit
+ * 0x00000000ffffffff	HPT index bits
+ * The bottom 32 bits are the index in the guest HPT of a HPTE that points to
+ * the page.
+ */
+#define KVMPPC_RMAP_LOCK_BIT	43
 #define KVMPPC_RMAP_RC_SHIFT	32
 #define KVMPPC_RMAP_REFERENCED	(HPTE_R_R << KVMPPC_RMAP_RC_SHIFT)
 #define KVMPPC_RMAP_PRESENT	0x100000000ul

commit d94d71cb45fda694a7189839f1c6aacb4f615f95
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 29 07:12:40 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 266
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not write to the free
      software foundation 51 franklin street fifth floor boston ma 02110
      1301 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 67 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190529141333.953658117@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d10df677c452..e6e5f59aaa97 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -1,16 +1,5 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, version 2, as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
  *
  * Copyright IBM Corp. 2007
  *

commit 1659e27d2bc1ef47b6d031abe01b467f18cb72d9
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed May 29 11:54:00 2019 +1000

    KVM: PPC: Book3S: Use new mutex to synchronize access to rtas token list
    
    Currently the Book 3S KVM code uses kvm->lock to synchronize access
    to the kvm->arch.rtas_tokens list.  Because this list is scanned
    inside kvmppc_rtas_hcall(), which is called with the vcpu mutex held,
    taking kvm->lock cause a lock inversion problem, which could lead to
    a deadlock.
    
    To fix this, we add a new mutex, kvm->arch.rtas_token_lock, which nests
    inside the vcpu mutexes, and use that instead of kvm->lock when
    accessing the rtas token list.
    
    This removes the lockdep_assert_held() in kvmppc_rtas_tokens_free().
    At this point we don't hold the new mutex, but that is OK because
    kvmppc_rtas_tokens_free() is only called when the whole VM is being
    destroyed, and at that point nothing can be looking up a token in
    the list.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 26b3ce487ddc..d10df677c452 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -309,6 +309,7 @@ struct kvm_arch {
 #ifdef CONFIG_PPC_BOOK3S_64
 	struct list_head spapr_tce_tables;
 	struct list_head rtas_tokens;
+	struct mutex rtas_token_lock;
 	DECLARE_BITMAP(enabled_hcalls, MAX_HCALL_OPCODE/4 + 1);
 #endif
 #ifdef CONFIG_KVM_MPIC

commit 0d4ee88d92884c661fcafd5576da243aa943dc24
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu May 23 16:35:34 2019 +1000

    KVM: PPC: Book3S HV: Use new mutex to synchronize MMU setup
    
    Currently the HV KVM code uses kvm->lock in conjunction with a flag,
    kvm->arch.mmu_ready, to synchronize MMU setup and hold off vcpu
    execution until the MMU-related data structures are ready.  However,
    this means that kvm->lock is being taken inside vcpu->mutex, which
    is contrary to Documentation/virtual/kvm/locking.txt and results in
    lockdep warnings.
    
    To fix this, we add a new mutex, kvm->arch.mmu_setup_lock, which nests
    inside the vcpu mutexes, and is taken in the places where kvm->lock
    was taken that are related to MMU setup.
    
    Additionally we take the new mutex in the vcpu creation code at the
    point where we are creating a new vcore, in order to provide mutual
    exclusion with kvmppc_update_lpcr() and ensure that an update to
    kvm->arch.lpcr doesn't get missed, which could otherwise lead to a
    stale vcore->lpcr value.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 013c76a0a03e..26b3ce487ddc 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -325,6 +325,7 @@ struct kvm_arch {
 #endif
 	struct kvmppc_ops *kvm_ops;
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	struct mutex mmu_setup_lock;	/* nests inside vcpu mutexes */
 	u64 l1_ptcr;
 	int max_nested_lpid;
 	struct kvm_nested_guest *nested_guests[KVM_MAX_NESTED_GUESTS];

commit 5422e95103cf9663bc86cf1056a3ea44c2e2f09e
Author: Cédric Le Goater <clg@kaod.org>
Date:   Thu Apr 18 12:39:42 2019 +0200

    KVM: PPC: Book3S HV: XIVE: Replace the 'destroy' method by a 'release' method
    
    When a P9 sPAPR VM boots, the CAS negotiation process determines which
    interrupt mode to use (XICS legacy or XIVE native) and invokes a
    machine reset to activate the chosen mode.
    
    We introduce 'release' methods for the XICS-on-XIVE and the XIVE
    native KVM devices which are called when the file descriptor of the
    device is closed after the TIMA and ESB pages have been unmapped.
    They perform the necessary cleanups : clear the vCPU interrupt
    presenters that could be attached and then destroy the device. The
    'release' methods replace the 'destroy' methods as 'destroy' is not
    called anymore once 'release' is. Compatibility with older QEMU is
    nevertheless maintained.
    
    This is not considered as a safe operation as the vCPUs are still
    running and could be referencing the KVM device through their
    presenters. To protect the system from any breakage, the kvmppc_xive
    objects representing both KVM devices are now stored in an array under
    the VM. Allocation is performed on first usage and memory is freed
    only when the VM exits.
    
    [paulus@ozlabs.org - Moved freeing of xive structures to book3s.c,
     put it under #ifdef CONFIG_KVM_XICS.]
    
    Signed-off-by: Cédric Le Goater <clg@kaod.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index eac25fd7e631..013c76a0a03e 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -316,7 +316,11 @@ struct kvm_arch {
 #endif
 #ifdef CONFIG_KVM_XICS
 	struct kvmppc_xics *xics;
-	struct kvmppc_xive *xive;
+	struct kvmppc_xive *xive;    /* Current XIVE device in use */
+	struct {
+		struct kvmppc_xive *native;
+		struct kvmppc_xive *xics_on_xive;
+	} xive_devices;
 	struct kvmppc_passthru_irqmap *pimap;
 #endif
 	struct kvmppc_ops *kvm_ops;

commit eacc56bb9de3e6830ddc169553772cd6de59ee4c
Author: Cédric Le Goater <clg@kaod.org>
Date:   Thu Apr 18 12:39:28 2019 +0200

    KVM: PPC: Book3S HV: XIVE: Introduce a new capability KVM_CAP_PPC_IRQ_XIVE
    
    The user interface exposes a new capability KVM_CAP_PPC_IRQ_XIVE to
    let QEMU connect the vCPU presenters to the XIVE KVM device if
    required. The capability is not advertised for now as the full support
    for the XIVE native exploitation mode is not yet available. When this
    is case, the capability will be advertised on PowerNV Hypervisors
    only. Nested guests (pseries KVM Hypervisor) are not supported.
    
    Internally, the interface to the new KVM device is protected with a
    new interrupt mode: KVMPPC_IRQ_XIVE.
    
    Signed-off-by: Cédric Le Goater <clg@kaod.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 9074f1d7613c..eac25fd7e631 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -453,6 +453,7 @@ struct kvmppc_passthru_irqmap {
 #define KVMPPC_IRQ_DEFAULT	0
 #define KVMPPC_IRQ_MPIC		1
 #define KVMPPC_IRQ_XICS		2 /* Includes a XIVE option */
+#define KVMPPC_IRQ_XIVE		3 /* XIVE native exploitation mode */
 
 #define MMIO_HPTE_CACHE_SIZE	4
 

commit 90c73795afa24890bd2ae4f3b359de04b4147d37
Author: Cédric Le Goater <clg@kaod.org>
Date:   Thu Apr 18 12:39:27 2019 +0200

    KVM: PPC: Book3S HV: Add a new KVM device for the XIVE native exploitation mode
    
    This is the basic framework for the new KVM device supporting the XIVE
    native exploitation mode. The user interface exposes a new KVM device
    to be created by QEMU, only available when running on a L0 hypervisor.
    Support for nested guests is not available yet.
    
    The XIVE device reuses the device structure of the XICS-on-XIVE device
    as they have a lot in common. That could possibly change in the future
    if the need arise.
    
    Signed-off-by: Cédric Le Goater <clg@kaod.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 07cefa53222e..9074f1d7613c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -225,6 +225,7 @@ extern struct kvm_device_ops kvm_xics_ops;
 struct kvmppc_xive;
 struct kvmppc_xive_vcpu;
 extern struct kvm_device_ops kvm_xive_ops;
+extern struct kvm_device_ops kvm_xive_native_ops;
 
 struct kvmppc_passthru_irqmap;
 

commit e1a1ef84cd07f72ce12f139eb9a37d3f9028e7a7
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Fri Mar 29 16:43:26 2019 +1100

    KVM: PPC: Book3S: Allocate guest TCEs on demand too
    
    We already allocate hardware TCE tables in multiple levels and skip
    intermediate levels when we can, now it is a turn of the KVM TCE tables.
    Thankfully these are allocated already in 2 levels.
    
    This moves the table's last level allocation from the creating helper to
    kvmppc_tce_put() and kvm_spapr_tce_fault(). Since such allocation cannot
    be done in real mode, this creates a virtual mode version of
    kvmppc_tce_put() which handles allocations.
    
    This adds kvmppc_rm_ioba_validate() to do an additional test if
    the consequent kvmppc_tce_put() needs a page which has not been allocated;
    if this is the case, we bail out to virtual mode handlers.
    
    The allocations are protected by a new mutex as kvm->lock is not suitable
    for the task because the fault handler is called with the mmap_sem held
    but kvmhv_setup_mmu() locks kvm->lock and mmap_sem in the reverse order.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e6b5bb012ccb..07cefa53222e 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -201,6 +201,8 @@ struct kvmppc_spapr_tce_iommu_table {
 	struct kref kref;
 };
 
+#define TCES_PER_PAGE	(PAGE_SIZE / sizeof(u64))
+
 struct kvmppc_spapr_tce_table {
 	struct list_head list;
 	struct kvm *kvm;
@@ -210,6 +212,7 @@ struct kvmppc_spapr_tce_table {
 	u64 offset;		/* in pages */
 	u64 size;		/* window size in pages */
 	struct list_head iommu_tables;
+	struct mutex alloc_lock;
 	struct page *pages[0];
 };
 

commit 54a1f393ce1b6645462d7f42e36171f86f54d030
Merge: a67794cafbc4 0a0c50f771f5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Feb 22 17:43:05 2019 +0100

    Merge tag 'kvm-ppc-next-5.1-1' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into kvm-next
    
    PPC KVM update for 5.1
    
    There are no major new features this time, just a collection of bug
    fixes and improvements in various areas, including machine check
    handling and context switching of protection-key-related registers.

commit 152482580a1b0accb60676063a1ac57b2d12daf6
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 5 12:54:17 2019 -0800

    KVM: Call kvm_arch_memslots_updated() before updating memslots
    
    kvm_arch_memslots_updated() is at this point in time an x86-specific
    hook for handling MMIO generation wraparound.  x86 stashes 19 bits of
    the memslots generation number in its MMIO sptes in order to avoid
    full page fault walks for repeat faults on emulated MMIO addresses.
    Because only 19 bits are used, wrapping the MMIO generation number is
    possible, if unlikely.  kvm_arch_memslots_updated() alerts x86 that
    the generation has changed so that it can invalidate all MMIO sptes in
    case the effective MMIO generation has wrapped so as to avoid using a
    stale spte, e.g. a (very) old spte that was created with generation==0.
    
    Given that the purpose of kvm_arch_memslots_updated() is to prevent
    consuming stale entries, it needs to be called before the new generation
    is propagated to memslots.  Invalidating the MMIO sptes after updating
    memslots means that there is a window where a vCPU could dereference
    the new memslots generation, e.g. 0, and incorrectly reuse an old MMIO
    spte that was created with (pre-wrap) generation==0.
    
    Fixes: e59dbe09f8e6 ("KVM: Introduce kvm_arch_memslots_updated()")
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0f98f00da2ea..19693b8add93 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -837,7 +837,7 @@ struct kvm_vcpu_arch {
 static inline void kvm_arch_hardware_disable(void) {}
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
-static inline void kvm_arch_memslots_updated(struct kvm *kvm, struct kvm_memslots *slots) {}
+static inline void kvm_arch_memslots_updated(struct kvm *kvm, u64 gen) {}
 static inline void kvm_arch_flush_shadow_all(struct kvm *kvm) {}
 static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}
 static inline void kvm_arch_exit(void) {}

commit 8f1f7b9bedbce8d84e0b6b8beac671a6bc8f02c9
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Tue Feb 19 14:53:45 2019 +1100

    KVM: PPC: Book3S HV: Add KVM stat largepages_[2M/1G]
    
    This adds an entry to the kvm_stats_debugfs directory which provides the
    number of large (2M or 1G) pages which have been used to setup the guest
    mappings, for radix guests.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 091430339db1..4af498a53905 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -99,6 +99,8 @@ struct kvm_nested_guest;
 
 struct kvm_vm_stat {
 	ulong remote_tlb_flush;
+	ulong num_2M_pages;
+	ulong num_1G_pages;
 };
 
 struct kvm_vcpu_stat {

commit 41a8645ab1c3c37f96955fec3360e123dc06abcd
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Feb 4 19:06:18 2019 +1100

    KVM: PPC: Book3S PR: Add emulation for slbfee. instruction
    
    Recent kernels, since commit e15a4fea4dee ("powerpc/64s/hash: Add
    some SLB debugging tests", 2018-10-03) use the slbfee. instruction,
    which PR KVM currently does not have code to emulate.  Consequently
    recent kernels fail to boot under PR KVM.  This adds emulation of
    slbfee., enabling these kernels to boot successfully.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0f98f00da2ea..091430339db1 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -377,6 +377,7 @@ struct kvmppc_mmu {
 	void (*slbmte)(struct kvm_vcpu *vcpu, u64 rb, u64 rs);
 	u64  (*slbmfee)(struct kvm_vcpu *vcpu, u64 slb_nr);
 	u64  (*slbmfev)(struct kvm_vcpu *vcpu, u64 slb_nr);
+	int  (*slbfee)(struct kvm_vcpu *vcpu, gva_t eaddr, ulong *ret_slb);
 	void (*slbie)(struct kvm_vcpu *vcpu, u64 slb_nr);
 	void (*slbia)(struct kvm_vcpu *vcpu);
 	/* book3s */

commit 748c0e312fce983bd7854b369b192e24dce90878
Author: Lan Tianyu <Tianyu.Lan@microsoft.com>
Date:   Thu Dec 6 21:21:10 2018 +0800

    KVM: Make kvm_set_spte_hva() return int
    
    The patch is to make kvm_set_spte_hva() return int and caller can
    check return value to determine flush tlb or not.
    
    Signed-off-by: Lan Tianyu <Tianyu.Lan@microsoft.com>
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 7a2483a139cf..0f98f00da2ea 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -72,7 +72,7 @@ extern int kvm_unmap_hva_range(struct kvm *kvm,
 			       unsigned long start, unsigned long end);
 extern int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
 extern int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
-extern void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
+extern int kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 
 #define HPTEG_CACHE_NUM			(1 << 15)
 #define HPTEG_HASH_BITS_PTE		13

commit 873db2cd9a6d7f017d8f4c637cf4166c038c27d6
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Fri Dec 14 16:29:08 2018 +1100

    KVM: PPC: Book3S HV: Allow passthrough of an emulated device to an L2 guest
    
    Allow for a device which is being emulated at L0 (the host) for an L1
    guest to be passed through to a nested (L2) guest.
    
    The existing kvmppc_hv_emulate_mmio function can be used here. The main
    challenge is that for a load the result must be stored into the L2 gpr,
    not an L1 gpr as would normally be the case after going out to qemu to
    complete the operation. This presents a challenge as at this point the
    L2 gpr state has been written back into L1 memory.
    
    To work around this we store the address in L1 memory of the L2 gpr
    where the result of the load is to be stored and use the new io_gpr
    value KVM_MMIO_REG_NESTED_GPR to indicate that this is a nested load for
    which completion must be done when returning back into the kernel. Then
    in kvmppc_complete_mmio_load() the resultant value is written into L1
    memory at the location of the indicated L2 gpr.
    
    Note that we don't currently let an L1 guest emulate a device for an L2
    guest which is then passed through to an L3 guest.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index fac6f631ed29..7a2483a139cf 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -793,6 +793,7 @@ struct kvm_vcpu_arch {
 	/* For support of nested guests */
 	struct kvm_nested_guest *nested;
 	u32 nested_vcpu_id;
+	gpa_t nested_io_gpr;
 #endif
 
 #ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING
@@ -827,6 +828,8 @@ struct kvm_vcpu_arch {
 #define KVM_MMIO_REG_FQPR	0x00c0
 #define KVM_MMIO_REG_VSX	0x0100
 #define KVM_MMIO_REG_VMX	0x0180
+#define KVM_MMIO_REG_NESTED_GPR	0xffc0
+
 
 #define __KVM_HAVE_ARCH_WQP
 #define __KVM_HAVE_CREATE_DEVICE

commit fd10be257312b5d883f89d62d691443e95678fdd
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Mon Oct 8 16:31:07 2018 +1100

    KVM: PPC: Book3S HV: Handle page fault for a nested guest
    
    Consider a normal (L1) guest running under the main hypervisor (L0),
    and then a nested guest (L2) running under the L1 guest which is acting
    as a nested hypervisor. L0 has page tables to map the address space for
    L1 providing the translation from L1 real address -> L0 real address;
    
            L1
            |
            | (L1 -> L0)
            |
            ----> L0
    
    There are also page tables in L1 used to map the address space for L2
    providing the translation from L2 real address -> L1 read address. Since
    the hardware can only walk a single level of page table, we need to
    maintain in L0 a "shadow_pgtable" for L2 which provides the translation
    from L2 real address -> L0 real address. Which looks like;
    
            L2                              L2
            |                               |
            | (L2 -> L1)                    |
            |                               |
            ----> L1                        | (L2 -> L0)
                  |                         |
                  | (L1 -> L0)              |
                  |                         |
                  ----> L0                  --------> L0
    
    When a page fault occurs while running a nested (L2) guest we need to
    insert a pte into this "shadow_pgtable" for the L2 -> L0 mapping. To
    do this we need to:
    
    1. Walk the pgtable in L1 memory to find the L2 -> L1 mapping, and
       provide a page fault to L1 if this mapping doesn't exist.
    2. Use our L1 -> L0 pgtable to convert this L1 address to an L0 address,
       or try to insert a pte for that mapping if it doesn't exist.
    3. Now we have a L2 -> L0 mapping, insert this into our shadow_pgtable
    
    Once this mapping exists we can take rc faults when hardware is unable
    to automatically set the reference and change bits in the pte. On these
    we need to:
    
    1. Check the rc bits on the L2 -> L1 pte match, and otherwise reflect
       the fault down to L1.
    2. Set the rc bits in the L1 -> L0 pte which corresponds to the same
       host page.
    3. Set the rc bits in the L2 -> L0 pte.
    
    As we reuse a large number of functions in book3s_64_mmu_radix.c for
    this we also needed to refactor a number of these functions to take
    an lpid parameter so that the correct lpid is used for tlb invalidations.
    The functionality however has remained the same.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index ceb9f20a0b24..fac6f631ed29 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -367,7 +367,9 @@ struct kvmppc_pte {
 	bool may_write		: 1;
 	bool may_execute	: 1;
 	unsigned long wimg;
+	unsigned long rc;
 	u8 page_size;		/* MMU_PAGE_xxx */
+	u8 page_shift;
 };
 
 struct kvmppc_mmu {

commit 360cae313702cdd0b90f82c261a8302fecef030a
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:31:04 2018 +1100

    KVM: PPC: Book3S HV: Nested guest entry via hypercall
    
    This adds a new hypercall, H_ENTER_NESTED, which is used by a nested
    hypervisor to enter one of its nested guests.  The hypercall supplies
    register values in two structs.  Those values are copied by the level 0
    (L0) hypervisor (the one which is running in hypervisor mode) into the
    vcpu struct of the L1 guest, and then the guest is run until an
    interrupt or error occurs which needs to be reported to L1 via the
    hypercall return value.
    
    Currently this assumes that the L0 and L1 hypervisors are the same
    endianness, and the structs passed as arguments are in native
    endianness.  If they are of different endianness, the version number
    check will fail and the hcall will be rejected.
    
    Nested hypervisors do not support indep_threads_mode=N, so this adds
    code to print a warning message if the administrator has set
    indep_threads_mode=N, and treat it as Y.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index c35d4f2c4d90..ceb9f20a0b24 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -95,6 +95,7 @@ struct dtl_entry;
 
 struct kvmppc_vcpu_book3s;
 struct kvmppc_book3s_shadow_vcpu;
+struct kvm_nested_guest;
 
 struct kvm_vm_stat {
 	ulong remote_tlb_flush;
@@ -786,6 +787,10 @@ struct kvm_vcpu_arch {
 	u32 emul_inst;
 
 	u32 online;
+
+	/* For support of nested guests */
+	struct kvm_nested_guest *nested;
+	u32 nested_vcpu_id;
 #endif
 
 #ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING

commit 8e3f5fc1045dc49fd175b978c5457f5f51e7a2ce
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:31:03 2018 +1100

    KVM: PPC: Book3S HV: Framework and hcall stubs for nested virtualization
    
    This starts the process of adding the code to support nested HV-style
    virtualization.  It defines a new H_SET_PARTITION_TABLE hypercall which
    a nested hypervisor can use to set the base address and size of a
    partition table in its memory (analogous to the PTCR register).
    On the host (level 0 hypervisor) side, the H_SET_PARTITION_TABLE
    hypercall from the guest is handled by code that saves the virtual
    PTCR value for the guest.
    
    This also adds code for creating and destroying nested guests and for
    reading the partition table entry for a nested guest from L1 memory.
    Each nested guest has its own shadow LPID value, different in general
    from the LPID value used by the nested hypervisor to refer to it.  The
    shadow LPID value is allocated at nested guest creation time.
    
    Nested hypervisor functionality is only available for a radix guest,
    which therefore means a radix host on a POWER9 (or later) processor.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index c9cc42f73b3c..c35d4f2c4d90 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -46,6 +46,7 @@
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 #include <asm/kvm_book3s_asm.h>		/* for MAX_SMT_THREADS */
 #define KVM_MAX_VCPU_ID		(MAX_SMT_THREADS * KVM_MAX_VCORES)
+#define KVM_MAX_NESTED_GUESTS	KVMPPC_NR_LPIDS
 
 #else
 #define KVM_MAX_VCPU_ID		KVM_MAX_VCPUS
@@ -287,6 +288,7 @@ struct kvm_arch {
 	u8 radix;
 	u8 fwnmi_enabled;
 	bool threads_indep;
+	bool nested_enable;
 	pgd_t *pgtable;
 	u64 process_table;
 	struct dentry *debugfs_dir;
@@ -312,6 +314,9 @@ struct kvm_arch {
 #endif
 	struct kvmppc_ops *kvm_ops;
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	u64 l1_ptcr;
+	int max_nested_lpid;
+	struct kvm_nested_guest *nested_guests[KVM_MAX_NESTED_GUESTS];
 	/* This array can grow quite large, keep it at the end */
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
 #endif

commit fd0944baad806dfb4c777124ec712c55b714ff51
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:30:58 2018 +1100

    KVM: PPC: Use ccr field in pt_regs struct embedded in vcpu struct
    
    When the 'regs' field was added to struct kvm_vcpu_arch, the code
    was changed to use several of the fields inside regs (e.g., gpr, lr,
    etc.) but not the ccr field, because the ccr field in struct pt_regs
    is 64 bits on 64-bit platforms, but the cr field in kvm_vcpu_arch is
    only 32 bits.  This changes the code to use the regs.ccr field
    instead of cr, and changes the assembly code on 64-bit platforms to
    use 64-bit loads and stores instead of 32-bit ones.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index a3d4f61a409d..c9cc42f73b3c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -538,8 +538,6 @@ struct kvm_vcpu_arch {
 	ulong tar;
 #endif
 
-	u32 cr;
-
 #ifdef CONFIG_PPC_BOOK3S
 	ulong hflags;
 	ulong guest_owned_ext;

commit 9a94d3ee2d159927c0f8e5078228eadbce8dda43
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:30:57 2018 +1100

    KVM: PPC: Book3S HV: Add a debugfs file to dump radix mappings
    
    This adds a file called 'radix' in the debugfs directory for the
    guest, which when read gives all of the valid leaf PTEs in the
    partition-scoped radix tree for a radix guest, in human-readable
    format.  It is analogous to the existing 'htab' file which dumps
    the HPT entries for a HPT guest.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 3cd0b9f45c2a..a3d4f61a409d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -291,6 +291,7 @@ struct kvm_arch {
 	u64 process_table;
 	struct dentry *debugfs_dir;
 	struct dentry *htab_dentry;
+	struct dentry *radix_dentry;
 	struct kvm_resize_hpt *resize_hpt; /* protected by kvm->lock */
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 #ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE

commit d24ea8a7336a2c392728e2cf909d607a680feb7b
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:30:48 2018 +1100

    KVM: PPC: Book3S: Simplify external interrupt handling
    
    Currently we use two bits in the vcpu pending_exceptions bitmap to
    indicate that an external interrupt is pending for the guest, one
    for "one-shot" interrupts that are cleared when delivered, and one
    for interrupts that persist until cleared by an explicit action of
    the OS (e.g. an acknowledge to an interrupt controller).  The
    BOOK3S_IRQPRIO_EXTERNAL bit is used for one-shot interrupt requests
    and BOOK3S_IRQPRIO_EXTERNAL_LEVEL is used for persisting interrupts.
    
    In practice BOOK3S_IRQPRIO_EXTERNAL never gets used, because our
    Book3S platforms generally, and pseries in particular, expect
    external interrupt requests to persist until they are acknowledged
    at the interrupt controller.  That combined with the confusion
    introduced by having two bits for what is essentially the same thing
    makes it attractive to simplify things by only using one bit.  This
    patch does that.
    
    With this patch there is only BOOK3S_IRQPRIO_EXTERNAL, and by default
    it has the semantics of a persisting interrupt.  In order to avoid
    breaking the ABI, we introduce a new "external_oneshot" flag which
    preserves the behaviour of the KVM_INTERRUPT ioctl with the
    KVM_INTERRUPT_SET argument.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 906bcbdfd2a1..3cd0b9f45c2a 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -707,6 +707,7 @@ struct kvm_vcpu_arch {
 	u8 hcall_needed;
 	u8 epr_flags; /* KVMPPC_EPR_xxx */
 	u8 epr_needed;
+	u8 external_oneshot;	/* clear external irq after delivery */
 
 	u32 cpr0_cfgaddr; /* holds the last set cpr0_cfgaddr */
 

commit 1ebe6b81ebdba8faf377d1d7d84ad9368e7a0bae
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Jul 26 14:53:54 2018 +1000

    KVM: PPC: Book3S HV: Allow creating max number of VCPUs on POWER9
    
    Commit 1e175d2 ("KVM: PPC: Book3S HV: Pack VCORE IDs to access full
    VCPU ID space", 2018-07-25) allowed use of VCPU IDs up to
    KVM_MAX_VCPU_ID on POWER9 in all guest SMT modes and guest emulated
    hardware SMT modes.  However, with the current definition of
    KVM_MAX_VCPU_ID, a guest SMT mode of 1 and an emulated SMT mode of 8,
    it is only possible to create KVM_MAX_VCPUS / 2 VCPUS, because
    threads_per_subcore is 4 on POWER9 CPUs.  (Using an emulated SMT mode
    of 8 is useful when migrating VMs to or from POWER8 hosts.)
    
    This increases KVM_MAX_VCPU_ID to 8 * KVM_MAX_VCPUS when HV KVM is
    configured in, so that a full complement of KVM_MAX_VCPUS VCPUs can
    be created on POWER9 in all guest SMT modes and emulated hardware
    SMT modes.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 5b9e6608c5bf..906bcbdfd2a1 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -42,7 +42,14 @@
 #define KVM_USER_MEM_SLOTS	512
 
 #include <asm/cputhreads.h>
-#define KVM_MAX_VCPU_ID                (threads_per_subcore * KVM_MAX_VCORES)
+
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+#include <asm/kvm_book3s_asm.h>		/* for MAX_SMT_THREADS */
+#define KVM_MAX_VCPU_ID		(MAX_SMT_THREADS * KVM_MAX_VCORES)
+
+#else
+#define KVM_MAX_VCPU_ID		KVM_MAX_VCPUS
+#endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
 

commit 4eeb85568e5653c71d901f7593d3f3e7e2a5414f
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 28 09:48:26 2018 +0800

    KVM: PPC: Remove mmio_vsx_tx_sx_enabled in KVM MMIO emulation
    
    Originally PPC KVM MMIO emulation uses only 0~31#(5 bits) for VSR
    reg number, and use mmio_vsx_tx_sx_enabled field together for
    0~63# VSR regs.
    
    Currently PPC KVM MMIO emulation is reimplemented with analyse_instr()
    assistance.  analyse_instr() returns 0~63 for VSR register number, so
    it is not necessary to use additional mmio_vsx_tx_sx_enabled field
    any more.
    
    This patch extends related reg bits (expand io_gpr to u16 from u8
    and use 6 bits for VSR reg#), so that mmio_vsx_tx_sx_enabled can
    be removed.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index fa4efa7e88f7..5b9e6608c5bf 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -672,7 +672,7 @@ struct kvm_vcpu_arch {
 	gva_t vaddr_accessed;
 	pgd_t *pgdir;
 
-	u8 io_gpr; /* GPR used as IO source/target */
+	u16 io_gpr; /* GPR used as IO source/target */
 	u8 mmio_host_swabbed;
 	u8 mmio_sign_extend;
 	/* conversion between single and double precision */
@@ -688,7 +688,6 @@ struct kvm_vcpu_arch {
 	 */
 	u8 mmio_vsx_copy_nums;
 	u8 mmio_vsx_offset;
-	u8 mmio_vsx_tx_sx_enabled;
 	u8 mmio_vmx_copy_nums;
 	u8 mmio_vmx_offset;
 	u8 mmio_copy_type;
@@ -801,14 +800,14 @@ struct kvm_vcpu_arch {
 #define KVMPPC_VCPU_BUSY_IN_HOST	2
 
 /* Values for vcpu->arch.io_gpr */
-#define KVM_MMIO_REG_MASK	0x001f
-#define KVM_MMIO_REG_EXT_MASK	0xffe0
+#define KVM_MMIO_REG_MASK	0x003f
+#define KVM_MMIO_REG_EXT_MASK	0xffc0
 #define KVM_MMIO_REG_GPR	0x0000
-#define KVM_MMIO_REG_FPR	0x0020
-#define KVM_MMIO_REG_QPR	0x0040
-#define KVM_MMIO_REG_FQPR	0x0060
-#define KVM_MMIO_REG_VSX	0x0080
-#define KVM_MMIO_REG_VMX	0x00c0
+#define KVM_MMIO_REG_FPR	0x0040
+#define KVM_MMIO_REG_QPR	0x0080
+#define KVM_MMIO_REG_FQPR	0x00c0
+#define KVM_MMIO_REG_VSX	0x0100
+#define KVM_MMIO_REG_VMX	0x0180
 
 #define __KVM_HAVE_ARCH_WQP
 #define __KVM_HAVE_CREATE_DEVICE

commit 8d2e2fc5e082a7b3f858cefb6e65700f28d2955e
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Wed May 23 15:01:58 2018 +0800

    KVM: PPC: Book3S PR: Add transaction memory save/restore skeleton
    
    The transaction memory checkpoint area save/restore behavior is
    triggered when VCPU qemu process is switching out/into CPU, i.e.
    at kvmppc_core_vcpu_put_pr() and kvmppc_core_vcpu_load_pr().
    
    MSR TM active state is determined by TS bits:
        active: 10(transactional) or 01 (suspended)
        inactive: 00 (non-transactional)
    We don't "fake" TM functionality for guest. We "sync" guest virtual
    MSR TM active state(10 or 01) with shadow MSR. That is to say,
    we don't emulate a transactional guest with a TM inactive MSR.
    
    TM SPR support(TFIAR/TFAR/TEXASR) has already been supported by
    commit 9916d57e64a4 ("KVM: PPC: Book3S PR: Expose TM registers").
    Math register support (FPR/VMX/VSX) will be done at subsequent
    patch.
    
    Whether TM context need to be saved/restored can be determined
    by kvmppc_get_msr() TM active state:
            * TM active - save/restore TM context
            * TM inactive - no need to do so and only save/restore
    TM SPRs.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Suggested-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 8dc5e439b387..fa4efa7e88f7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -627,7 +627,6 @@ struct kvm_vcpu_arch {
 
 	struct thread_vr_state vr_tm;
 	u32 vrsave_tm; /* also USPRG0 */
-
 #endif
 
 #ifdef CONFIG_KVM_EXIT_TIMING

commit acc9eb9305fecd958e2877c4e6cd3284d01c2e82
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 21 13:24:26 2018 +0800

    KVM: PPC: Reimplement LOAD_VMX/STORE_VMX instruction mmio emulation with analyse_instr() input
    
    This patch reimplements LOAD_VMX/STORE_VMX MMIO emulation with
    analyse_instr() input. When emulating the store, the VMX reg will need to
    be flushed so that the right reg val can be retrieved before writing to
    IO MEM.
    
    This patch also adds support for lvebx/lvehx/lvewx/stvebx/stvehx/stvewx
    MMIO emulation. To meet the requirement of handling different element
    sizes, kvmppc_handle_load128_by2x64()/kvmppc_handle_store128_by2x64()
    were replaced with kvmppc_handle_vmx_load()/kvmppc_handle_vmx_store().
    
    The framework used is similar to VSX instruction MMIO emulation.
    
    Suggested-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index fe506c86404a..8dc5e439b387 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -691,6 +691,7 @@ struct kvm_vcpu_arch {
 	u8 mmio_vsx_offset;
 	u8 mmio_vsx_tx_sx_enabled;
 	u8 mmio_vmx_copy_nums;
+	u8 mmio_vmx_offset;
 	u8 mmio_copy_type;
 	u8 osi_needed;
 	u8 osi_enabled;

commit da2a32b876e979d74f84746ae8d066e1d54b568f
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 21 13:24:25 2018 +0800

    KVM: PPC: Expand mmio_vsx_copy_type to cover VMX load/store element types
    
    VSX MMIO emulation uses mmio_vsx_copy_type to represent VSX emulated
    element size/type, such as KVMPPC_VSX_COPY_DWORD_LOAD, etc. This
    patch expands mmio_vsx_copy_type to cover VMX copy type, such as
    KVMPPC_VMX_COPY_BYTE(stvebx/lvebx), etc. As a result,
    mmio_vsx_copy_type is also renamed to mmio_copy_type.
    
    It is a preparation for reimplementing VMX MMIO emulation.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 4bade292892f..fe506c86404a 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -455,6 +455,11 @@ struct mmio_hpte_cache {
 #define KVMPPC_VSX_COPY_DWORD_LOAD_DUMP	3
 #define KVMPPC_VSX_COPY_WORD_LOAD_DUMP	4
 
+#define KVMPPC_VMX_COPY_BYTE		8
+#define KVMPPC_VMX_COPY_HWORD		9
+#define KVMPPC_VMX_COPY_WORD		10
+#define KVMPPC_VMX_COPY_DWORD		11
+
 struct openpic;
 
 /* W0 and W1 of a XIVE thread management context */
@@ -677,16 +682,16 @@ struct kvm_vcpu_arch {
 	 * Number of simulations for vsx.
 	 * If we use 2*8bytes to simulate 1*16bytes,
 	 * then the number should be 2 and
-	 * mmio_vsx_copy_type=KVMPPC_VSX_COPY_DWORD.
+	 * mmio_copy_type=KVMPPC_VSX_COPY_DWORD.
 	 * If we use 4*4bytes to simulate 1*16bytes,
 	 * the number should be 4 and
 	 * mmio_vsx_copy_type=KVMPPC_VSX_COPY_WORD.
 	 */
 	u8 mmio_vsx_copy_nums;
 	u8 mmio_vsx_offset;
-	u8 mmio_vsx_copy_type;
 	u8 mmio_vsx_tx_sx_enabled;
 	u8 mmio_vmx_copy_nums;
+	u8 mmio_copy_type;
 	u8 osi_needed;
 	u8 osi_enabled;
 	u8 papr_enabled;

commit 94dd7fa1c0b75e909fa54d86ce2d1aaf2c54ceb7
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 21 13:24:20 2018 +0800

    KVM: PPC: Add KVMPPC_VSX_COPY_WORD_LOAD_DUMP type support for mmio emulation
    
    Some VSX instructions like lxvwsx will splat word into VSR. This patch
    adds a new VSX copy type KVMPPC_VSX_COPY_WORD_LOAD_DUMP to support this.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Reviewed-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 89f44ecc4dbd..4bade292892f 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -453,6 +453,7 @@ struct mmio_hpte_cache {
 #define KVMPPC_VSX_COPY_WORD		1
 #define KVMPPC_VSX_COPY_DWORD		2
 #define KVMPPC_VSX_COPY_DWORD_LOAD_DUMP	3
+#define KVMPPC_VSX_COPY_WORD_LOAD_DUMP	4
 
 struct openpic;
 

commit b7557451475d747740bc1598045bd273ece80ab0
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu May 17 16:59:10 2018 +1000

    KVM: PPC: Book3S HV: Lockless tlbie for HPT hcalls
    
    tlbies to an LPAR do not have to be serialised since POWER4/PPC970,
    after which the MMU_FTR_LOCKLESS_TLBIE feature was introduced to
    avoid tlbie locking.
    
    Since commit c17b98cf6028 ("KVM: PPC: Book3S HV: Remove code for
    PPC970 processors"), KVM no longer supports processors that do not
    have this feature, so the tlbie locking can be removed completely.
    A sanity check for the feature is put in kvmppc_mmu_hv_init.
    
    Testing was done on a POWER9 system in HPT mode, with a -smp 32 guest
    in HPT mode. 32 instances of the powerpc fork benchmark from selftests
    were run with --fork, and the results measured.
    
    Without this patch, total throughput was about 13.5K/sec, and this is
    the top of the host profile:
    
       74.52%  [k] do_tlbies
        2.95%  [k] kvmppc_book3s_hv_page_fault
        1.80%  [k] calc_checksum
        1.80%  [k] kvmppc_vcpu_run_hv
        1.49%  [k] kvmppc_run_core
    
    After this patch, throughput was about 51K/sec, with this profile:
    
       21.28%  [k] do_tlbies
        5.26%  [k] kvmppc_run_core
        4.88%  [k] kvmppc_book3s_hv_page_fault
        3.30%  [k] _raw_spin_lock_irqsave
        3.25%  [k] gup_pgd_range
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 8b0ee5e09ea3..89f44ecc4dbd 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -269,7 +269,6 @@ struct kvm_arch {
 	unsigned long host_lpcr;
 	unsigned long sdr1;
 	unsigned long host_sdr1;
-	int tlbie_lock;
 	unsigned long lpcr;
 	unsigned long vrma_slb_v;
 	int mmu_ready;

commit 173c520a049f57e2af498a3f0557d07797ce1c1b
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 7 14:20:08 2018 +0800

    KVM: PPC: Move nip/ctr/lr/xer registers to pt_regs in kvm_vcpu_arch
    
    This patch moves nip/ctr/lr/xer registers from scattered places in
    kvm_vcpu_arch to pt_regs structure.
    
    cr register is "unsigned long" in pt_regs and u32 in vcpu->arch.
    It will need more consideration and may move in later patches.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index a75443a372bb..8b0ee5e09ea3 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -521,14 +521,10 @@ struct kvm_vcpu_arch {
 	u32 qpr[32];
 #endif
 
-	ulong pc;
-	ulong ctr;
-	ulong lr;
 #ifdef CONFIG_PPC_BOOK3S
 	ulong tar;
 #endif
 
-	ulong xer;
 	u32 cr;
 
 #ifdef CONFIG_PPC_BOOK3S

commit 1143a70665c2175a33a40d8f2dc277978fbf7640
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 7 14:20:07 2018 +0800

    KVM: PPC: Add pt_regs into kvm_vcpu_arch and move vcpu->arch.gpr[] into it
    
    Current regs are scattered at kvm_vcpu_arch structure and it will
    be more neat to organize them into pt_regs structure.
    
    Also it will enable reimplementation of MMIO emulation code with
    analyse_instr() later.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 9703f8f229c9..a75443a372bb 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -486,7 +486,7 @@ struct kvm_vcpu_arch {
 	struct kvmppc_book3s_shadow_vcpu *shadow_vcpu;
 #endif
 
-	ulong gpr[32];
+	struct pt_regs regs;
 
 	struct thread_fp_state fp;
 

commit a1f158262a3e00fe396f2d21ef1cffdfc29226dc
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Apr 20 15:33:21 2018 +1000

    KVM: PPC: Book3S HV: Add 'online' register to ONE_REG interface
    
    This adds a new KVM_REG_PPC_ONLINE register which userspace can set
    to 0 or 1 via the GET/SET_ONE_REG interface to indicate whether it
    considers the VCPU to be offline (0), that is, not currently running,
    or online (1).  This will be used in a later patch to configure the
    register which controls PURR and SPURR accumulation.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 17498e9a26e4..9703f8f229c9 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -772,6 +772,8 @@ struct kvm_vcpu_arch {
 	u64 busy_preempt;
 
 	u32 emul_inst;
+
+	u32 online;
 #endif
 
 #ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING

commit d8312a3f61024352f1c7cb967571fd53631b0d6c
Merge: e9092d0d9796 e01bca2fc698
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 9 11:42:31 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - VHE optimizations
    
       - EL2 address space randomization
    
       - speculative execution mitigations ("variant 3a", aka execution past
         invalid privilege register access)
    
       - bugfixes and cleanups
    
      PPC:
       - improvements for the radix page fault handler for HV KVM on POWER9
    
      s390:
       - more kvm stat counters
    
       - virtio gpu plumbing
    
       - documentation
    
       - facilities improvements
    
      x86:
       - support for VMware magic I/O port and pseudo-PMCs
    
       - AMD pause loop exiting
    
       - support for AMD core performance extensions
    
       - support for synchronous register access
    
       - expose nVMX capabilities to userspace
    
       - support for Hyper-V signaling via eventfd
    
       - use Enlightened VMCS when running on Hyper-V
    
       - allow userspace to disable MWAIT/HLT/PAUSE vmexits
    
       - usual roundup of optimizations and nested virtualization bugfixes
    
      Generic:
       - API selftest infrastructure (though the only tests are for x86 as
         of now)"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (174 commits)
      kvm: x86: fix a prototype warning
      kvm: selftests: add sync_regs_test
      kvm: selftests: add API testing infrastructure
      kvm: x86: fix a compile warning
      KVM: X86: Add Force Emulation Prefix for "emulate the next instruction"
      KVM: X86: Introduce handle_ud()
      KVM: vmx: unify adjacent #ifdefs
      x86: kvm: hide the unused 'cpu' variable
      KVM: VMX: remove bogus WARN_ON in handle_ept_misconfig
      Revert "KVM: X86: Fix SMRAM accessing even if VM is shutdown"
      kvm: Add emulation for movups/movupd
      KVM: VMX: raise internal error for exception during invalid protected mode state
      KVM: nVMX: Optimization: Dont set KVM_REQ_EVENT when VMExit with nested_run_pending
      KVM: nVMX: Require immediate-exit when event reinjected to L2 and L1 event pending
      KVM: x86: Fix misleading comments on handling pending exceptions
      KVM: x86: Rename interrupt.pending to interrupt.injected
      KVM: VMX: No need to clear pending NMI/interrupt on inject realmode interrupt
      x86/kvm: use Enlightened VMCS when running on Hyper-V
      x86/hyper-v: detect nested features
      x86/hyper-v: define struct hv_enlightened_vmcs and clean field bits
      ...

commit 4bb3c7a0208fc13ca70598efd109901a7cd45ae7
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Mar 21 21:32:01 2018 +1100

    KVM: PPC: Book3S HV: Work around transactional memory bugs in POWER9
    
    POWER9 has hardware bugs relating to transactional memory and thread
    reconfiguration (changes to hardware SMT mode).  Specifically, the core
    does not have enough storage to store a complete checkpoint of all the
    architected state for all four threads.  The DD2.2 version of POWER9
    includes hardware modifications designed to allow hypervisor software
    to implement workarounds for these problems.  This patch implements
    those workarounds in KVM code so that KVM guests see a full, working
    transactional memory implementation.
    
    The problems center around the use of TM suspended state, where the
    CPU has a checkpointed state but execution is not transactional.  The
    workaround is to implement a "fake suspend" state, which looks to the
    guest like suspended state but the CPU does not store a checkpoint.
    In this state, any instruction that would cause a transition to
    transactional state (rfid, rfebb, mtmsrd, tresume) or would use the
    checkpointed state (treclaim) causes a "soft patch" interrupt (vector
    0x1500) to the hypervisor so that it can be emulated.  The trechkpt
    instruction also causes a soft patch interrupt.
    
    On POWER9 DD2.2, we avoid returning to the guest in any state which
    would require a checkpoint to be present.  The trechkpt in the guest
    entry path which would normally create that checkpoint is replaced by
    either a transition to fake suspend state, if the guest is in suspend
    state, or a rollback to the pre-transactional state if the guest is in
    transactional state.  Fake suspend state is indicated by a flag in the
    PACA plus a new bit in the PSSCR.  The new PSSCR bit is write-only and
    reads back as 0.
    
    On exit from the guest, if the guest is in fake suspend state, we still
    do the treclaim instruction as we would in real suspend state, in order
    to get into non-transactional state, but we do not save the resulting
    register state since there was no checkpoint.
    
    Emulation of the instructions that cause a softpatch interrupt is
    handled in two paths.  If the guest is in real suspend mode, we call
    kvmhv_p9_tm_emulation_early() to handle the cases where the guest is
    transitioning to transactional state.  This is called before we do the
    treclaim in the guest exit path; because we haven't done treclaim, we
    can get back to the guest with the transaction still active.  If the
    instruction is a case that kvmhv_p9_tm_emulation_early() doesn't
    handle, or if the guest is in fake suspend state, then we proceed to
    do the complete guest exit path and subsequently call
    kvmhv_p9_tm_emulation() in host context with the MMU on.  This handles
    all the cases including the cases that generate program interrupts
    (illegal instruction or TM Bad Thing) and facility unavailable
    interrupts.
    
    The emulation is reasonably straightforward and is mostly concerned
    with checking for exception conditions and updating the state of
    registers such as MSR and CR0.  The treclaim emulation takes care to
    ensure that the TEXASR register gets updated as if it were the guest
    treclaim instruction that had done failure recording, not the treclaim
    done in hypervisor state in the guest exit path.
    
    With this, the KVM_CAP_PPC_HTM capability returns true (1) even if
    transactional memory is not available to host userspace.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1f53b562726f..deb54293398c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -610,6 +610,7 @@ struct kvm_vcpu_arch {
 	u64 tfhar;
 	u64 texasr;
 	u64 tfiar;
+	u64 orig_texasr;
 
 	u32 cr_tm;
 	u64 xer_tm;

commit 39c983ea0f96a270d4876c4148e3bb2d9cd3294f
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Feb 22 15:16:54 2018 +1100

    KVM: PPC: Remove unused kvm_unmap_hva callback
    
    Since commit fb1522e099f0 ("KVM: update to new mmu_notifier semantic
    v2", 2017-08-31), the MMU notifier code in KVM no longer calls the
    kvm_unmap_hva callback.  This removes the PPC implementations of
    kvm_unmap_hva().
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1f53b562726f..6b69d7999381 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -60,7 +60,6 @@
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 
-extern int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 extern int kvm_unmap_hva_range(struct kvm *kvm,
 			       unsigned long start, unsigned long end);
 extern int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);

commit 09f984961c137c4b252c368adab7e1c9f035fa59
Author: Jose Ricardo Ziviani <joserz@linux.vnet.ibm.com>
Date:   Sat Feb 3 18:24:26 2018 -0200

    KVM: PPC: Book3S: Add MMIO emulation for VMX instructions
    
    This patch provides the MMIO load/store vector indexed
    X-Form emulation.
    
    Instructions implemented:
    lvx: the quadword in storage addressed by the result of EA &
    0xffff_ffff_ffff_fff0 is loaded into VRT.
    
    stvx: the contents of VRS are stored into the quadword in storage
    addressed by the result of EA & 0xffff_ffff_ffff_fff0.
    
    Reported-by: Gopesh Kumar Chaudhary <gopchaud@in.ibm.com>
    Reported-by: Balamuruhan S <bala24@linux.vnet.ibm.com>
    Signed-off-by: Jose Ricardo Ziviani <joserz@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index fef8133becc8..1f53b562726f 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -690,6 +690,7 @@ struct kvm_vcpu_arch {
 	u8 mmio_vsx_offset;
 	u8 mmio_vsx_copy_type;
 	u8 mmio_vsx_tx_sx_enabled;
+	u8 mmio_vmx_copy_nums;
 	u8 osi_needed;
 	u8 osi_enabled;
 	u8 papr_enabled;
@@ -804,6 +805,7 @@ struct kvm_vcpu_arch {
 #define KVM_MMIO_REG_QPR	0x0040
 #define KVM_MMIO_REG_FQPR	0x0060
 #define KVM_MMIO_REG_VSX	0x0080
+#define KVM_MMIO_REG_VMX	0x00c0
 
 #define __KVM_HAVE_ARCH_WQP
 #define __KVM_HAVE_CREATE_DEVICE

commit 9b9b13a6d1537ddc4caccd6f1c41b78edbc08437
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Jan 12 13:37:16 2018 +1100

    KVM: PPC: Book3S HV: Keep XIVE escalation interrupt masked unless ceded
    
    This works on top of the single escalation support. When in single
    escalation, with this change, we will keep the escalation interrupt
    disabled unless the VCPU is in H_CEDE (idle). In any other case, we
    know the VCPU will be rescheduled and thus there is no need to take
    escalation interrupts in the host whenever a guest interrupt fires.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0c44fa67608d..fef8133becc8 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -740,7 +740,10 @@ struct kvm_vcpu_arch {
 	struct kvmppc_xive_vcpu *xive_vcpu; /* XIVE virtual CPU data */
 	__be32 xive_cam_word;    /* Cooked W2 in proper endian with valid bit */
 	u8 xive_pushed;		 /* Is the VP pushed on the physical CPU ? */
+	u8 xive_esc_on;		 /* Is the escalation irq enabled ? */
 	union xive_tma_w01 xive_saved_state; /* W0..1 of XIVE thread state */
+	u64 xive_esc_raddr;	 /* Escalation interrupt ESB real addr */
+	u64 xive_esc_vaddr;	 /* Escalation interrupt ESB virt addr */
 #endif
 
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE

commit 35c2405efc0142860c4b698f4c6331567c4ca1ef
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Jan 12 13:37:15 2018 +1100

    KVM: PPC: Book3S HV: Make xive_pushed a byte, not a word
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index bfe51356af5e..0c44fa67608d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -739,7 +739,7 @@ struct kvm_vcpu_arch {
 	struct kvmppc_icp *icp; /* XICS presentation controller */
 	struct kvmppc_xive_vcpu *xive_vcpu; /* XIVE virtual CPU data */
 	__be32 xive_cam_word;    /* Cooked W2 in proper endian with valid bit */
-	u32 xive_pushed;	 /* Is the VP pushed on the physical CPU ? */
+	u8 xive_pushed;		 /* Is the VP pushed on the physical CPU ? */
 	union xive_tma_w01 xive_saved_state; /* W0..1 of XIVE thread state */
 #endif
 

commit 2267ea7661798a42f0da648a2970e2a03f4bc370
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Jan 12 13:37:13 2018 +1100

    KVM: PPC: Book3S HV: Don't use existing "prodded" flag for XIVE escalations
    
    The prodded flag is only cleared at the beginning of H_CEDE,
    so every time we have an escalation, we will cause the *next*
    H_CEDE to return immediately.
    
    Instead use a dedicated "irq_pending" flag to indicate that
    a guest interrupt is pending for the VCPU. We don't reuse the
    existing exception bitmap so as to avoid expensive atomic ops.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 3aa5b577cd60..bfe51356af5e 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -709,6 +709,7 @@ struct kvm_vcpu_arch {
 	u8 ceded;
 	u8 prodded;
 	u8 doorbell_request;
+	u8 irq_pending; /* Used by XIVE to signal pending guest irqs */
 	u32 last_inst;
 
 	struct swait_queue_head *wqp;

commit 516f7898ae20d9dd902a85522676055a4de9dc9b
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 16 16:11:57 2017 +1100

    KVM: PPC: Book3S HV: Allow for running POWER9 host in single-threaded mode
    
    This patch allows for a mode on POWER9 hosts where we control all the
    threads of a core, much as we do on POWER8.  The mode is controlled by
    a module parameter on the kvm_hv module, called "indep_threads_mode".
    The normal mode on POWER9 is the "independent threads" mode, with
    indep_threads_mode=Y, where the host is in SMT4 mode (or in fact any
    desired SMT mode) and each thread independently enters and exits from
    KVM guests without reference to what other threads in the core are
    doing.
    
    If indep_threads_mode is set to N at the point when a VM is started,
    KVM will expect every core that the guest runs on to be in single
    threaded mode (that is, threads 1, 2 and 3 offline), and will set the
    flag that prevents secondary threads from coming online.  We can still
    use all four threads; the code that implements dynamic micro-threading
    on POWER8 will become active in over-commit situations and will allow
    up to three other VCPUs to be run on the secondary threads of the core
    whenever a VCPU is run.
    
    The reason for wanting this mode is that this will allow us to run HPT
    guests on a radix host on a POWER9 machine that does not support
    "mixed mode", that is, having some threads in a core be in HPT mode
    while other threads are in radix mode.  It will also make it possible
    to implement a "strict threads" mode in future, if desired.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d831a3883175..3aa5b577cd60 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -281,6 +281,7 @@ struct kvm_arch {
 	cpumask_t cpu_in_guest;
 	u8 radix;
 	u8 fwnmi_enabled;
+	bool threads_indep;
 	pgd_t *pgtable;
 	u64 process_table;
 	struct dentry *debugfs_dir;

commit e641a317830b6bd26e6dc2ef5fe2c1c181dd5cc2
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Oct 26 16:39:19 2017 +1100

    KVM: PPC: Book3S HV: Unify dirty page map between HPT and radix
    
    Currently, the HPT code in HV KVM maintains a dirty bit per guest page
    in the rmap array, whether or not dirty page tracking has been enabled
    for the memory slot.  In contrast, the radix code maintains a dirty
    bit per guest page in memslot->dirty_bitmap, and only does so when
    dirty page tracking has been enabled.
    
    This changes the HPT code to maintain the dirty bits in the memslot
    dirty_bitmap like radix does.  This results in slightly less code
    overall, and will mean that we do not lose the dirty bits when
    transitioning between HPT and radix mode in future.
    
    There is one minor change to behaviour as a result.  With HPT, when
    dirty tracking was enabled for a memslot, we would previously clear
    all the dirty bits at that point (both in the HPT entries and in the
    rmap arrays), meaning that a KVM_GET_DIRTY_LOG ioctl immediately
    following would show no pages as dirty (assuming no vcpus have run
    in the meantime).  With this change, the dirty bits on HPT entries
    are not cleared at the point where dirty tracking is enabled, so
    KVM_GET_DIRTY_LOG would show as dirty any guest pages that are
    resident in the HPT and dirty.  This is consistent with what happens
    on radix.
    
    This also fixes a bug in the mark_pages_dirty() function for radix
    (in the sense that the function no longer exists).  In the case where
    a large page of 64 normal pages or more is marked dirty, the
    addressing of the dirty bitmap was incorrect and could write past
    the end of the bitmap.  Fortunately this case was never hit in
    practice because a 2MB large page is only 32 x 64kB pages, and we
    don't support backing the guest with 1GB huge pages at this point.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 49493ea5520b..d831a3883175 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -235,10 +235,7 @@ struct revmap_entry {
  */
 #define KVMPPC_RMAP_LOCK_BIT	63
 #define KVMPPC_RMAP_RC_SHIFT	32
-#define KVMPPC_RMAP_CHG_SHIFT	48
 #define KVMPPC_RMAP_REFERENCED	(HPTE_R_R << KVMPPC_RMAP_RC_SHIFT)
-#define KVMPPC_RMAP_CHANGED	(HPTE_R_C << KVMPPC_RMAP_RC_SHIFT)
-#define KVMPPC_RMAP_CHG_ORDER	(0x3ful << KVMPPC_RMAP_CHG_SHIFT)
 #define KVMPPC_RMAP_PRESENT	0x100000000ul
 #define KVMPPC_RMAP_INDEX	0xfffffffful
 

commit 1b151ce466175746b1b1a87d42ba5f5a050a5aba
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Sep 13 15:53:48 2017 +1000

    KVM: PPC: Book3S HV: Rename hpte_setup_done to mmu_ready
    
    This renames the kvm->arch.hpte_setup_done field to mmu_ready because
    we will want to use it for radix guests too -- both for setting things
    up before vcpu execution, and for excluding vcpus from executing while
    MMU-related things get changed, such as in future switching the MMU
    from radix to HPT mode or vice-versa.
    
    This also moves the call to kvmppc_setup_partition_table() that was
    done in kvmppc_hv_setup_htab_rma() for HPT guests, and the setting
    of mmu_ready, into the caller in kvmppc_vcpu_run_hv().
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e372ed871c51..49493ea5520b 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -276,7 +276,7 @@ struct kvm_arch {
 	int tlbie_lock;
 	unsigned long lpcr;
 	unsigned long vrma_slb_v;
-	int hpte_setup_done;
+	int mmu_ready;
 	atomic_t vcpus_running;
 	u32 online_vcores;
 	atomic_t hpte_mod_interest;

commit fb1522e099f0c69f36655af233a64e3f55941f5b
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Thu Aug 31 17:17:37 2017 -0400

    KVM: update to new mmu_notifier semantic v2
    
    Calls to mmu_notifier_invalidate_page() were replaced by calls to
    mmu_notifier_invalidate_range() and are now bracketed by calls to
    mmu_notifier_invalidate_range_start()/end()
    
    Remove now useless invalidate_page callback.
    
    Changed since v1 (Linus Torvalds)
        - remove now useless kvm_arch_mmu_notifier_invalidate_page()
    
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Tested-by: Mike Galbraith <efault@gmx.de>
    Tested-by: Adam Borowski <kilobyte@angband.pl>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: kvm@vger.kernel.org
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 8b3f1238d07f..e372ed871c51 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -67,11 +67,6 @@ extern int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
 extern int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 extern void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 
-static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
-							 unsigned long address)
-{
-}
-
 #define HPTEG_CACHE_NUM			(1 << 15)
 #define HPTEG_HASH_BITS_PTE		13
 #define HPTEG_HASH_BITS_PTE_LONG	12

commit 8a53e7e572252e551fd4b172dc207f8beca1ae20
Merge: 00c14757f6ab 8b24e69fc47e
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jul 3 10:41:59 2017 +0200

    Merge branch 'kvm-ppc-next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD
    
    - Better machine check handling for HV KVM
    - Ability to support guests with threads=2, 4 or 8 on POWER9
    - Fix for a race that could cause delayed recognition of signals
    - Fix for a bug where POWER9 guests could sleep with interrupts
      pending.

commit e20bbd3d8d5c4432db8fd6f091b096963236064f
Author: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
Date:   Thu May 11 16:33:37 2017 +0530

    KVM: PPC: Book3S HV: Exit guest upon MCE when FWNMI capability is enabled
    
    Enhance KVM to cause a guest exit with KVM_EXIT_NMI
    exit reason upon a machine check exception (MCE) in
    the guest address space if the KVM_CAP_PPC_FWNMI
    capability is enabled (instead of delivering a 0x200
    interrupt to guest). This enables QEMU to build error
    log and deliver machine check exception to guest via
    guest registered machine check handler.
    
    This approach simplifies the delivery of machine
    check exception to guest OS compared to the earlier
    approach of KVM directly invoking 0x200 guest interrupt
    vector.
    
    This design/approach is based on the feedback for the
    QEMU patches to handle machine check exception. Details
    of earlier approach of handling machine check exception
    in QEMU and related discussions can be found at:
    
    https://lists.nongnu.org/archive/html/qemu-devel/2014-11/msg00813.html
    
    Note:
    
    This patch now directly invokes machine_check_print_event_info()
    from kvmppc_handle_exit_hv() to print the event to host console
    at the time of guest exit before the exception is passed on to the
    guest. Hence, the host-side handling which was performed earlier
    via machine_check_fwnmi is removed.
    
    The reasons for this approach is (i) it is not possible
    to distinguish whether the exception occurred in the
    guest or the host from the pt_regs passed on the
    machine_check_exception(). Hence machine_check_exception()
    calls panic, instead of passing on the exception to
    the guest, if the machine check exception is not
    recoverable. (ii) the approach introduced in this
    patch gives opportunity to the host kernel to perform
    actions in virtual mode before passing on the exception
    to the guest. This approach does not require complex
    tweaks to machine_check_fwnmi and friends.
    
    Signed-off-by: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 05866391f406..7d64f99ea3b8 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -35,6 +35,7 @@
 #include <asm/page.h>
 #include <asm/cacheflush.h>
 #include <asm/hvcall.h>
+#include <asm/mce.h>
 
 #define KVM_MAX_VCPUS		NR_CPUS
 #define KVM_MAX_VCORES		NR_CPUS
@@ -727,6 +728,7 @@ struct kvm_vcpu_arch {
 	int prev_cpu;
 	bool timer_running;
 	wait_queue_head_t cpu_run;
+	struct machine_check_event mce_evt; /* Valid if trap == 0x200 */
 
 	struct kvm_vcpu_arch_shared *shared;
 #if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_KVM_BOOK3S_PR_POSSIBLE)

commit 134764ed6e12d9f99b3de68b8aaeae1ba842d91c
Author: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
Date:   Thu May 11 16:32:48 2017 +0530

    KVM: PPC: Book3S HV: Add new capability to control MCE behaviour
    
    This introduces a new KVM capability to control how KVM behaves
    on machine check exception (MCE) in HV KVM guests.
    
    If this capability has not been enabled, KVM redirects machine check
    exceptions to guest's 0x200 vector, if the address in error belongs to
    the guest. With this capability enabled, KVM will cause a guest exit
    with the exit reason indicating an NMI.
    
    The new capability is required to avoid problems if a new kernel/KVM
    is used with an old QEMU, running a guest that doesn't issue
    "ibm,nmi-register".  As old QEMU does not understand the NMI exit
    type, it treats it as a fatal error.  However, the guest could have
    handled the machine check error if the exception was delivered to
    guest's 0x200 interrupt vector instead of NMI exit in case of old
    QEMU.
    
    [paulus@ozlabs.org - Reworded the commit message to be clearer,
     enable only on HV KVM.]
    
    Signed-off-by: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 683c3c82ce9c..05866391f406 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -287,6 +287,7 @@ struct kvm_arch {
 	cpumask_t need_tlb_flush;
 	cpumask_t cpu_in_guest;
 	u8 radix;
+	u8 fwnmi_enabled;
 	pgd_t *pgtable;
 	u64 process_table;
 	struct dentry *debugfs_dir;

commit 579006944e0d675361e987c646b83d2d1725966c
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Tue May 16 16:41:20 2017 +1000

    KVM: PPC: Book3S HV: Virtualize doorbell facility on POWER9
    
    On POWER9, we no longer have the restriction that we had on POWER8
    where all threads in a core have to be in the same partition, so
    the CPU threads are now independent.  However, we still want to be
    able to run guests with a virtual SMT topology, if only to allow
    migration of guests from POWER8 systems to POWER9.
    
    A guest that has a virtual SMT mode greater than 1 will expect to
    be able to use the doorbell facility; it will expect the msgsndp
    and msgclrp instructions to work appropriately and to be able to read
    sensible values from the TIR (thread identification register) and
    DPDES (directed privileged doorbell exception status) special-purpose
    registers.  However, since each CPU thread is a separate sub-processor
    in POWER9, these instructions and registers can only be used within
    a single CPU thread.
    
    In order for these instructions to appear to act correctly according
    to the guest's virtual SMT mode, we have to trap and emulate them.
    We cause them to trap by clearing the HFSCR_MSGP bit in the HFSCR
    register.  The emulation is triggered by the hypervisor facility
    unavailable interrupt that occurs when the guest uses them.
    
    To cause a doorbell interrupt to occur within the guest, we set the
    DPDES register to 1.  If the guest has interrupts enabled, the CPU
    will generate a doorbell interrupt and clear the DPDES register in
    hardware.  The DPDES hardware register for the guest is saved in the
    vcpu->arch.vcore->dpdes field.  Since this gets written by the guest
    exit code, other VCPUs wishing to cause a doorbell interrupt don't
    write that field directly, but instead set a vcpu->arch.doorbell_request
    flag.  This is consumed and set to 0 by the guest entry code, which
    then sets DPDES to 1.
    
    Emulating reads of the DPDES register is somewhat involved, because
    it requires reading the doorbell pending interrupt status of all of the
    VCPU threads in the virtual core, and if any of those VCPUs are
    running, their doorbell status is only up-to-date in the hardware
    DPDES registers of the CPUs where they are running.  In order to get
    a reasonable approximation of the current doorbell status, we send
    those CPUs an IPI, causing an exit from the guest which will update
    the vcpu->arch.vcore->dpdes field.  We then use that value in
    constructing the emulated DPDES register value.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e8991808ea9c..683c3c82ce9c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -268,6 +268,7 @@ struct kvm_resize_hpt;
 struct kvm_arch {
 	unsigned int lpid;
 	unsigned int smt_mode;		/* # vcpus per virtual core */
+	unsigned int emul_smt_mode;	/* emualted SMT mode, on P9 */
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	unsigned int tlb_sets;
 	struct kvm_hpt_info hpt;
@@ -712,6 +713,7 @@ struct kvm_vcpu_arch {
 	unsigned long pending_exceptions;
 	u8 ceded;
 	u8 prodded;
+	u8 doorbell_request;
 	u32 last_inst;
 
 	struct swait_queue_head *wqp;

commit 3c313524605a6afd8207448a8e9967f5e8cba734
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Feb 6 13:24:41 2017 +1100

    KVM: PPC: Book3S HV: Allow userspace to set the desired SMT mode
    
    This allows userspace to set the desired virtual SMT (simultaneous
    multithreading) mode for a VM, that is, the number of VCPUs that
    get assigned to each virtual core.  Previously, the virtual SMT mode
    was fixed to the number of threads per subcore, and if userspace
    wanted to have fewer vcpus per vcore, then it would achieve that by
    using a sparse CPU numbering.  This had the disadvantage that the
    vcpu numbers can get quite large, particularly for SMT1 guests on
    a POWER8 with 8 threads per core.  With this patch, userspace can
    set its desired virtual SMT mode and then use contiguous vcpu
    numbering.
    
    On POWER8, where the threading mode is "strict", the virtual SMT mode
    must be less than or equal to the number of threads per subcore.  On
    POWER9, which implements a "loose" threading mode, the virtual SMT
    mode can be any power of 2 between 1 and 8, even though there is
    effectively one thread per subcore, since the threads are independent
    and can all be in different partitions.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d3aae32acb54..e8991808ea9c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -267,6 +267,7 @@ struct kvm_resize_hpt;
 
 struct kvm_arch {
 	unsigned int lpid;
+	unsigned int smt_mode;		/* # vcpus per virtual core */
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	unsigned int tlb_sets;
 	struct kvm_hpt_info hpt;

commit 769377f77ca2087baeaf97ce0b7026abeba3b581
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Feb 15 14:30:17 2017 +1100

    KVM: PPC: Book3S HV: Context-switch HFSCR between host and guest on POWER9
    
    This adds code to allow us to use a different value for the HFSCR
    (Hypervisor Facilities Status and Control Register) when running the
    guest from that which applies in the host.  The reason for doing this
    is to allow us to trap the msgsndp instruction and related operations
    in future so that they can be virtualized.  We also save the value of
    HFSCR when a hypervisor facility unavailable interrupt occurs, because
    the high byte of HFSCR indicates which facility the guest attempted to
    access.
    
    We save and restore the host value on guest entry/exit because some
    bits of it affect host userspace execution.
    
    We only do all this on POWER9, not on POWER8, because we are not
    intending to virtualize any of the facilities controlled by HFSCR on
    POWER8.  In particular, the HFSCR bit that controls execution of
    msgsndp and related operations does not exist on POWER8.  The HFSCR
    doesn't exist at all on POWER7.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 3f879c802feb..d3aae32acb54 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -566,6 +566,7 @@ struct kvm_vcpu_arch {
 	ulong wort;
 	ulong tid;
 	ulong psscr;
+	ulong hfscr;
 	ulong shadow_srr1;
 #endif
 	u32 vrsave; /* also USPRG0 */

commit 1bc3fe818c9e823248f6ec299b1c518aa2df347c
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon May 22 16:55:16 2017 +1000

    KVM: PPC: Book3S HV: Enable guests to use large decrementer mode on POWER9
    
    This allows userspace (e.g. QEMU) to enable large decrementer mode for
    the guest when running on a POWER9 host, by setting the LPCR_LD bit in
    the guest LPCR value.  With this, the guest exit code saves 64 bits of
    the guest DEC value on exit.  Other places that use the guest DEC
    value check the LPCR_LD bit in the guest LPCR value, and if it is set,
    omit the 32-bit sign extension that would otherwise be done.
    
    This doesn't change the DEC emulation used by PR KVM because PR KVM
    is not supported on POWER9 yet.
    
    This is partly based on an earlier patch by Oliver O'Halloran.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 9c51ac4b8f36..3f879c802feb 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -579,7 +579,7 @@ struct kvm_vcpu_arch {
 	ulong mcsrr0;
 	ulong mcsrr1;
 	ulong mcsr;
-	u32 dec;
+	ulong dec;
 #ifdef CONFIG_BOOKE
 	u32 decar;
 #endif

commit 2387149eade25f32dcf1398811b3d0293181d005
Author: Andrew Jones <drjones@redhat.com>
Date:   Sun Jun 4 14:43:51 2017 +0200

    KVM: improve arch vcpu request defining
    
    Marc Zyngier suggested that we define the arch specific VCPU request
    base, rather than requiring each arch to remember to start from 8.
    That suggestion, along with Radim Krcmar's recent VCPU request flag
    addition, snowballed into defining something of an arch VCPU request
    defining API.
    
    No functional change.
    
    (Looks like x86 is running out of arch VCPU request bits.  Maybe
     someday we'll need to extend to 64.)
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 9c51ac4b8f36..50e0bc9723cc 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -52,8 +52,8 @@
 #define KVM_IRQCHIP_NUM_PINS     256
 
 /* PPC-specific vcpu->requests bit members */
-#define KVM_REQ_WATCHDOG           8
-#define KVM_REQ_EPR_EXIT           9
+#define KVM_REQ_WATCHDOG	KVM_ARCH_REQ(0)
+#define KVM_REQ_EPR_EXIT	KVM_ARCH_REQ(1)
 
 #include <linux/mmu_notifier.h>
 

commit fb7dcf723dd2cb1d5d8f2f49c3023130938848e3
Merge: db4b0dfab7b0 5af50993850a
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Apr 28 08:23:16 2017 +1000

    Merge remote-tracking branch 'remotes/powerpc/topic/xive' into kvm-ppc-next
    
    This merges in the powerpc topic/xive branch to bring in the code for
    the in-kernel XICS interrupt controller emulation to use the new XIVE
    (eXternal Interrupt Virtualization Engine) hardware in the POWER9 chip
    directly, rather than via a XICS emulation in firmware.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

commit 5af50993850a48ba749b122173d789ea90976c72
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:56 2017 +1000

    KVM: PPC: Book3S HV: Native usage of the XIVE interrupt controller
    
    This patch makes KVM capable of using the XIVE interrupt controller
    to provide the standard PAPR "XICS" style hypercalls. It is necessary
    for proper operations when the host uses XIVE natively.
    
    This has been lightly tested on an actual system, including PCI
    pass-through with a TG3 device.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [mpe: Cleanup pr_xxx(), unsplit pr_xxx() strings, etc., fix build
     failures by adding KVM_XIVE which depends on KVM_XICS and XIVE, and
     adding empty stubs for the kvm_xive_xxx() routines, fixup subject,
     integrate fixes from Paul for building PR=y HV=n]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 7bba8f415627..5a8ab4a758f1 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -205,6 +205,12 @@ struct kvmppc_spapr_tce_table {
 /* XICS components, defined in book3s_xics.c */
 struct kvmppc_xics;
 struct kvmppc_icp;
+extern struct kvm_device_ops kvm_xics_ops;
+
+/* XIVE components, defined in book3s_xive.c */
+struct kvmppc_xive;
+struct kvmppc_xive_vcpu;
+extern struct kvm_device_ops kvm_xive_ops;
 
 struct kvmppc_passthru_irqmap;
 
@@ -293,6 +299,7 @@ struct kvm_arch {
 #endif
 #ifdef CONFIG_KVM_XICS
 	struct kvmppc_xics *xics;
+	struct kvmppc_xive *xive;
 	struct kvmppc_passthru_irqmap *pimap;
 #endif
 	struct kvmppc_ops *kvm_ops;
@@ -421,7 +428,7 @@ struct kvmppc_passthru_irqmap {
 
 #define KVMPPC_IRQ_DEFAULT	0
 #define KVMPPC_IRQ_MPIC		1
-#define KVMPPC_IRQ_XICS		2
+#define KVMPPC_IRQ_XICS		2 /* Includes a XIVE option */
 
 #define MMIO_HPTE_CACHE_SIZE	4
 
@@ -443,6 +450,21 @@ struct mmio_hpte_cache {
 
 struct openpic;
 
+/* W0 and W1 of a XIVE thread management context */
+union xive_tma_w01 {
+	struct {
+		u8	nsr;
+		u8	cppr;
+		u8	ipb;
+		u8	lsmfb;
+		u8	ack;
+		u8	inc;
+		u8	age;
+		u8	pipr;
+	};
+	__be64 w01;
+};
+
 struct kvm_vcpu_arch {
 	ulong host_stack;
 	u32 host_pid;
@@ -688,6 +710,10 @@ struct kvm_vcpu_arch {
 	struct openpic *mpic;	/* KVM_IRQ_MPIC */
 #ifdef CONFIG_KVM_XICS
 	struct kvmppc_icp *icp; /* XICS presentation controller */
+	struct kvmppc_xive_vcpu *xive_vcpu; /* XIVE virtual CPU data */
+	__be32 xive_cam_word;    /* Cooked W2 in proper endian with valid bit */
+	u32 xive_pushed;	 /* Is the VP pushed on the physical CPU ? */
+	union xive_tma_w01 xive_saved_state; /* W0..1 of XIVE thread state */
 #endif
 
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE

commit 121f80ba68f1a5779a36d7b3247206e60e0a7418
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Wed Mar 22 15:21:56 2017 +1100

    KVM: PPC: VFIO: Add in-kernel acceleration for VFIO
    
    This allows the host kernel to handle H_PUT_TCE, H_PUT_TCE_INDIRECT
    and H_STUFF_TCE requests targeted an IOMMU TCE table used for VFIO
    without passing them to user space which saves time on switching
    to user space and back.
    
    This adds H_PUT_TCE/H_PUT_TCE_INDIRECT/H_STUFF_TCE handlers to KVM.
    KVM tries to handle a TCE request in the real mode, if failed
    it passes the request to the virtual mode to complete the operation.
    If it a virtual mode handler fails, the request is passed to
    the user space; this is not expected to happen though.
    
    To avoid dealing with page use counters (which is tricky in real mode),
    this only accelerates SPAPR TCE IOMMU v2 clients which are required
    to pre-register the userspace memory. The very first TCE request will
    be handled in the VFIO SPAPR TCE driver anyway as the userspace view
    of the TCE table (iommu_table::it_userspace) is not allocated till
    the very first mapping happens and we cannot call vmalloc in real mode.
    
    If we fail to update a hardware IOMMU table unexpected reason, we just
    clear it and move on as there is nothing really we can do about it -
    for example, if we hot plug a VFIO device to a guest, existing TCE tables
    will be mirrored automatically to the hardware and there is no interface
    to report to the guest about possible failures.
    
    This adds new attribute - KVM_DEV_VFIO_GROUP_SET_SPAPR_TCE - to
    the VFIO KVM device. It takes a VFIO group fd and SPAPR TCE table fd
    and associates a physical IOMMU table with the SPAPR TCE table (which
    is a guest view of the hardware IOMMU table). The iommu_table object
    is cached and referenced so we do not have to look up for it in real mode.
    
    This does not implement the UNSET counterpart as there is no use for it -
    once the acceleration is enabled, the existing userspace won't
    disable it unless a VFIO container is destroyed; this adds necessary
    cleanup to the KVM_DEV_VFIO_GROUP_DEL handler.
    
    This advertises the new KVM_CAP_SPAPR_TCE_VFIO capability to the user
    space.
    
    This adds real mode version of WARN_ON_ONCE() as the generic version
    causes problems with rcu_sched. Since we testing what vmalloc_to_phys()
    returns in the code, this also adds a check for already existing
    vmalloc_to_phys() call in kvmppc_rm_h_put_tce_indirect().
    
    This finally makes use of vfio_external_user_iommu_id() which was
    introduced quite some time ago and was considered for removal.
    
    Tests show that this patch increases transmission speed from 220MB/s
    to 750..1020MB/s on 10Gb network (Chelsea CXGB3 10Gb ethernet card).
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Acked-by: Alex Williamson <alex.williamson@redhat.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0f3ac09cbfe0..77c60826d145 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -188,6 +188,13 @@ struct kvmppc_pginfo {
 	atomic_t refcnt;
 };
 
+struct kvmppc_spapr_tce_iommu_table {
+	struct rcu_head rcu;
+	struct list_head next;
+	struct iommu_table *tbl;
+	struct kref kref;
+};
+
 struct kvmppc_spapr_tce_table {
 	struct list_head list;
 	struct kvm *kvm;
@@ -196,6 +203,7 @@ struct kvmppc_spapr_tce_table {
 	u32 page_shift;
 	u64 offset;		/* in pages */
 	u64 size;		/* window size in pages */
+	struct list_head iommu_tables;
 	struct page *pages[0];
 };
 

commit 96df2267695199b9377bd641c7eb68c393b81b0b
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Fri Mar 24 17:49:22 2017 +1100

    KVM: PPC: Book3S PR: Preserve storage control bits
    
    PR KVM page fault handler performs eaddr to pte translation for a guest,
    however kvmppc_mmu_book3s_64_xlate() does not preserve WIMG bits
    (storage control) in the kvmppc_pte struct. If PR KVM is running as
    a second level guest under HV KVM, and PR KVM tries inserting HPT entry,
    this fails in HV KVM if it already has this mapping.
    
    This preserves WIMG bits between kvmppc_mmu_book3s_64_xlate() and
    kvmppc_mmu_map_page().
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 2acc691ed9d0..0f3ac09cbfe0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -342,6 +342,7 @@ struct kvmppc_pte {
 	bool may_read		: 1;
 	bool may_write		: 1;
 	bool may_execute	: 1;
+	unsigned long wimg;
 	u8 page_size;		/* MMU_PAGE_xxx */
 };
 

commit 6f63e81bda98cbb549b01faf978884692ded438d
Author: Bin Lu <lblulb@linux.vnet.ibm.com>
Date:   Tue Feb 21 21:12:36 2017 +0800

    KVM: PPC: Book3S: Add MMIO emulation for FP and VSX instructions
    
    This patch provides the MMIO load/store emulation for instructions
    of 'double & vector unsigned char & vector signed char & vector
    unsigned short & vector signed short & vector unsigned int & vector
    signed int & vector double '.
    
    The instructions that this adds emulation for are:
    
    - ldx, ldux, lwax,
    - lfs, lfsx, lfsu, lfsux, lfd, lfdx, lfdu, lfdux,
    - stfs, stfsx, stfsu, stfsux, stfd, stfdx, stfdu, stfdux, stfiwx,
    - lxsdx, lxsspx, lxsiwax, lxsiwzx, lxvd2x, lxvw4x, lxvdsx,
    - stxsdx, stxsspx, stxsiwx, stxvd2x, stxvw4x
    
    [paulus@ozlabs.org - some cleanups, fixes and rework, make it
     compile for Book E, fix build when PR KVM is built in]
    
    Signed-off-by: Bin Lu <lblulb@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 01d05c76f1c7..2acc691ed9d0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -438,6 +438,11 @@ struct mmio_hpte_cache {
 	unsigned int index;
 };
 
+#define KVMPPC_VSX_COPY_NONE		0
+#define KVMPPC_VSX_COPY_WORD		1
+#define KVMPPC_VSX_COPY_DWORD		2
+#define KVMPPC_VSX_COPY_DWORD_LOAD_DUMP	3
+
 struct openpic;
 
 struct kvm_vcpu_arch {
@@ -641,6 +646,21 @@ struct kvm_vcpu_arch {
 	u8 io_gpr; /* GPR used as IO source/target */
 	u8 mmio_host_swabbed;
 	u8 mmio_sign_extend;
+	/* conversion between single and double precision */
+	u8 mmio_sp64_extend;
+	/*
+	 * Number of simulations for vsx.
+	 * If we use 2*8bytes to simulate 1*16bytes,
+	 * then the number should be 2 and
+	 * mmio_vsx_copy_type=KVMPPC_VSX_COPY_DWORD.
+	 * If we use 4*4bytes to simulate 1*16bytes,
+	 * the number should be 4 and
+	 * mmio_vsx_copy_type=KVMPPC_VSX_COPY_WORD.
+	 */
+	u8 mmio_vsx_copy_nums;
+	u8 mmio_vsx_offset;
+	u8 mmio_vsx_copy_type;
+	u8 mmio_vsx_tx_sx_enabled;
 	u8 osi_needed;
 	u8 osi_enabled;
 	u8 papr_enabled;
@@ -729,6 +749,8 @@ struct kvm_vcpu_arch {
 };
 
 #define VCPU_FPR(vcpu, i)	(vcpu)->arch.fp.fpr[i][TS_FPROFFSET]
+#define VCPU_VSX_FPR(vcpu, i, j)	((vcpu)->arch.fp.fpr[i][j])
+#define VCPU_VSX_VR(vcpu, i)		((vcpu)->arch.vr.vr[i])
 
 /* Values for vcpu->arch.state */
 #define KVMPPC_VCPU_NOTREADY		0
@@ -742,6 +764,7 @@ struct kvm_vcpu_arch {
 #define KVM_MMIO_REG_FPR	0x0020
 #define KVM_MMIO_REG_QPR	0x0040
 #define KVM_MMIO_REG_FQPR	0x0060
+#define KVM_MMIO_REG_VSX	0x0080
 
 #define __KVM_HAVE_ARCH_WQP
 #define __KVM_HAVE_CREATE_DEVICE

commit 4b4357e02523ec63ad853f927f5d93a25101a1d2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Mar 31 13:53:23 2017 +0200

    kvm: make KVM_COALESCED_MMIO_PAGE_OFFSET public
    
    Its value has never changed; we might as well make it part of the ABI instead
    of using the return value of KVM_CHECK_EXTENSION(KVM_CAP_COALESCED_MMIO).
    
    Because PPC does not always make MMIO available, the code has to be made
    dependent on CONFIG_KVM_MMIO rather than KVM_COALESCED_MMIO_PAGE_OFFSET.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 7bba8f415627..01d05c76f1c7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -45,9 +45,6 @@
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
 
-#ifdef CONFIG_KVM_MMIO
-#define KVM_COALESCED_MMIO_PAGE_OFFSET 1
-#endif
 #define KVM_HALT_POLL_NS_DEFAULT 10000	/* 10 us */
 
 /* These values are internal and can be increased later */

commit 5e9859699aba74c0e297645e7d1734cd4b964de7
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Dec 20 16:49:05 2016 +1100

    KVM: PPC: Book3S HV: Outline of KVM-HV HPT resizing implementation
    
    This adds a not yet working outline of the HPT resizing PAPR
    extension.  Specifically it adds the necessary ioctl() functions,
    their basic steps, the work function which will handle preparation for
    the resize, and synchronization between these, the guest page fault
    path and guest HPT update path.
    
    The actual guts of the implementation isn't here yet, so for now the
    calls will always fail.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0aa0f22d775a..7bba8f415627 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -252,6 +252,8 @@ struct kvm_hpt_info {
 	int cma;
 };
 
+struct kvm_resize_hpt;
+
 struct kvm_arch {
 	unsigned int lpid;
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
@@ -276,6 +278,7 @@ struct kvm_arch {
 	u64 process_table;
 	struct dentry *debugfs_dir;
 	struct dentry *htab_dentry;
+	struct kvm_resize_hpt *resize_hpt; /* protected by kvm->lock */
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 #ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
 	struct mutex hpt_mutex;

commit 3d089f84c6f9b7b0eda993142d73961a44b553d2
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Dec 20 16:49:01 2016 +1100

    KVM: PPC: Book3S HV: Don't store values derivable from HPT order
    
    Currently the kvm_hpt_info structure stores the hashed page table's order,
    and also the number of HPTEs it contains and a mask for its size.  The
    last two can be easily derived from the order, so remove them and just
    calculate them as necessary with a couple of helper inlines.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index ea6f0c659936..0aa0f22d775a 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -246,8 +246,6 @@ struct kvm_hpt_info {
 	unsigned long virt;
 	/* Array of reverse mapping entries for each guest HPTE */
 	struct revmap_entry *rev;
-	unsigned long npte;
-	unsigned long mask;
 	/* Guest HPT size is 2**(order) bytes */
 	u32 order;
 	/* 1 if HPT allocated with CMA, 0 otherwise */

commit 3f9d4f5a5f35e402e91bedf0c15e29cef187a29d
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Dec 20 16:49:00 2016 +1100

    KVM: PPC: Book3S HV: Gather HPT related variables into sub-structure
    
    Currently, the powerpc kvm_arch structure contains a number of variables
    tracking the state of the guest's hashed page table (HPT) in KVM HV.  This
    patch gathers them all together into a single kvm_hpt_info substructure.
    This makes life more convenient for the upcoming HPT resizing
    implementation.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index b2dbeac3f450..ea6f0c659936 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -241,12 +241,24 @@ struct kvm_arch_memory_slot {
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 };
 
+struct kvm_hpt_info {
+	/* Host virtual (linear mapping) address of guest HPT */
+	unsigned long virt;
+	/* Array of reverse mapping entries for each guest HPTE */
+	struct revmap_entry *rev;
+	unsigned long npte;
+	unsigned long mask;
+	/* Guest HPT size is 2**(order) bytes */
+	u32 order;
+	/* 1 if HPT allocated with CMA, 0 otherwise */
+	int cma;
+};
+
 struct kvm_arch {
 	unsigned int lpid;
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	unsigned int tlb_sets;
-	unsigned long hpt_virt;
-	struct revmap_entry *revmap;
+	struct kvm_hpt_info hpt;
 	atomic64_t mmio_update;
 	unsigned int host_lpid;
 	unsigned long host_lpcr;
@@ -256,15 +268,11 @@ struct kvm_arch {
 	unsigned long lpcr;
 	unsigned long vrma_slb_v;
 	int hpte_setup_done;
-	u32 hpt_order;
 	atomic_t vcpus_running;
 	u32 online_vcores;
-	unsigned long hpt_npte;
-	unsigned long hpt_mask;
 	atomic_t hpte_mod_interest;
 	cpumask_t need_tlb_flush;
 	cpumask_t cpu_in_guest;
-	int hpt_cma_alloc;
 	u8 radix;
 	pgd_t *pgtable;
 	u64 process_table;

commit a29ebeaf5575d03eef178bb87c425a1e46cae1ca
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 30 21:21:50 2017 +1100

    KVM: PPC: Book3S HV: Invalidate TLB on radix guest vcpu movement
    
    With radix, the guest can do TLB invalidations itself using the tlbie
    (global) and tlbiel (local) TLB invalidation instructions.  Linux guests
    use local TLB invalidations for translations that have only ever been
    accessed on one vcpu.  However, that doesn't mean that the translations
    have only been accessed on one physical cpu (pcpu) since vcpus can move
    around from one pcpu to another.  Thus a tlbiel might leave behind stale
    TLB entries on a pcpu where the vcpu previously ran, and if that task
    then moves back to that previous pcpu, it could see those stale TLB
    entries and thus access memory incorrectly.  The usual symptom of this
    is random segfaults in userspace programs in the guest.
    
    To cope with this, we detect when a vcpu is about to start executing on
    a thread in a core that is a different core from the last time it
    executed.  If that is the case, then we mark the core as needing a
    TLB flush and then send an interrupt to any thread in the core that is
    currently running a vcpu from the same guest.  This will get those vcpus
    out of the guest, and the first one to re-enter the guest will do the
    TLB flush.  The reason for interrupting the vcpus executing on the old
    core is to cope with the following scenario:
    
            CPU 0                   CPU 1                   CPU 4
            (core 0)                        (core 0)                        (core 1)
    
            VCPU 0 runs task X      VCPU 1 runs
            core 0 TLB gets
            entries from task X
            VCPU 0 moves to CPU 4
                                                            VCPU 0 runs task X
                                                            Unmap pages of task X
                                                            tlbiel
    
                                    (still VCPU 1)                  task X moves to VCPU 1
                                    task X runs
                                    task X sees stale TLB
                                    entries
    
    That is, as soon as the VCPU starts executing on the new core, it
    could unmap and tlbiel some page table entries, and then the task
    could migrate to one of the VCPUs running on the old core and
    potentially see stale TLB entries.
    
    Since the TLB is shared between all the threads in a core, we only
    use the bit of kvm->arch.need_tlb_flush corresponding to the first
    thread in the core.  To ensure that we don't have a window where we
    can miss a flush, this moves the clearing of the bit from before the
    actual flush to after it.  This way, two threads might both do the
    flush, but we prevent the situation where one thread can enter the
    guest before the flush is finished.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index da1421a4d6f2..b2dbeac3f450 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -263,6 +263,7 @@ struct kvm_arch {
 	unsigned long hpt_mask;
 	atomic_t hpte_mod_interest;
 	cpumask_t need_tlb_flush;
+	cpumask_t cpu_in_guest;
 	int hpt_cma_alloc;
 	u8 radix;
 	pgd_t *pgtable;
@@ -661,6 +662,7 @@ struct kvm_vcpu_arch {
 	int state;
 	int ptid;
 	int thread_cpu;
+	int prev_cpu;
 	bool timer_running;
 	wait_queue_head_t cpu_run;
 

commit f4c51f841d2ac7d36cacb84efbc383190861f87c
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 30 21:21:45 2017 +1100

    KVM: PPC: Book3S HV: Modify guest entry/exit paths to handle radix guests
    
    This adds code to  branch around the parts that radix guests don't
    need - clearing and loading the SLB with the guest SLB contents,
    saving the guest SLB contents on exit, and restoring the host SLB
    contents.
    
    Since the host is now using radix, we need to save and restore the
    host value for the PID register.
    
    On hypervisor data/instruction storage interrupts, we don't do the
    guest HPT lookup on radix, but just save the guest physical address
    for the fault (from the ASDR register) in the vcpu struct.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index fb73518bd03b..da1421a4d6f2 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -606,6 +606,7 @@ struct kvm_vcpu_arch {
 	ulong fault_dar;
 	u32 fault_dsisr;
 	unsigned long intr_msr;
+	ulong fault_gpa;	/* guest real address of page fault (POWER9) */
 #endif
 
 #ifdef CONFIG_BOOKE

commit 9e04ba69beec372ddf857c700ff922e95f50b0d0
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 30 21:21:44 2017 +1100

    KVM: PPC: Book3S HV: Add basic infrastructure for radix guests
    
    This adds a field in struct kvm_arch and an inline helper to
    indicate whether a guest is a radix guest or not, plus a new file
    to contain the radix MMU code, which currently contains just a
    translate function which knows how to traverse the guest page
    tables to translate an address.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 944532dc4a57..fb73518bd03b 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -264,6 +264,8 @@ struct kvm_arch {
 	atomic_t hpte_mod_interest;
 	cpumask_t need_tlb_flush;
 	int hpt_cma_alloc;
+	u8 radix;
+	pgd_t *pgtable;
 	u64 process_table;
 	struct dentry *debugfs_dir;
 	struct dentry *htab_dentry;

commit 468808bd35c4aa3cf7d9fde0ebb010270038734b
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 30 21:21:42 2017 +1100

    KVM: PPC: Book3S HV: Set process table for HPT guests on POWER9
    
    This adds the implementation of the KVM_PPC_CONFIGURE_V3_MMU ioctl
    for HPT guests on POWER9.  With this, we can return 1 for the
    KVM_CAP_PPC_MMU_HASH_V3 capability.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e59b172666cd..944532dc4a57 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -264,6 +264,7 @@ struct kvm_arch {
 	atomic_t hpte_mod_interest;
 	cpumask_t need_tlb_flush;
 	int hpt_cma_alloc;
+	u64 process_table;
 	struct dentry *debugfs_dir;
 	struct dentry *htab_dentry;
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */

commit f4944613ad1ab6760589d5791488be1236c07fcc
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Fri Oct 14 11:53:22 2016 +1100

    KVM: PPC: Decrease the powerpc default halt poll max value
    
    KVM_HALT_POLL_NS_DEFAULT is an arch specific constant which sets the
    default value of the halt_poll_ns kvm module parameter which determines
    the global maximum halt polling interval.
    
    The current value for powerpc is 500000 (500us) which means that any
    repetitive workload with a period of less than that can drive the cpu
    usage to 100% where it may have been mostly idle without halt polling.
    This presents the possibility of a large increase in power usage with
    a comparatively small performance benefit.
    
    Reduce the default to 10000 (10us) and a user can tune this themselves
    to set their affinity for halt polling based on the trade off between power
    and performance which they are willing to make.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index f15713ae3f34..e59b172666cd 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -48,7 +48,7 @@
 #ifdef CONFIG_KVM_MMIO
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 #endif
-#define KVM_HALT_POLL_NS_DEFAULT 500000
+#define KVM_HALT_POLL_NS_DEFAULT 10000	/* 10 us */
 
 /* These values are internal and can be increased later */
 #define KVM_NR_IRQCHIPS          1

commit 7c5b06cadf274f2867523c1130c11387545f808e
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Nov 18 08:28:51 2016 +1100

    KVM: PPC: Book3S HV: Adapt TLB invalidations to work on POWER9
    
    POWER9 adds new capabilities to the tlbie (TLB invalidate entry)
    and tlbiel (local tlbie) instructions.  Both instructions get a
    set of new parameters (RIC, PRS and R) which appear as bits in the
    instruction word.  The tlbiel instruction now has a second register
    operand, which contains a PID and/or LPID value if needed, and
    should otherwise contain 0.
    
    This adapts KVM-HV's usage of tlbie and tlbiel to work on POWER9
    as well as older processors.  Since we only handle HPT guests so
    far, we need RIC=0 PRS=0 R=0, which ends up with the same instruction
    word as on previous processors, so we don't need to conditionally
    execute different instructions depending on the processor.
    
    The local flush on first entry to a guest in book3s_hv_rmhandlers.S
    is a loop which depends on the number of TLB sets.  Rather than
    using feature sections to set the number of iterations based on
    which CPU we're on, we now work out this number at VM creation time
    and store it in the kvm_arch struct.  That will make it possible to
    get the number from the device tree in future, which will help with
    compatibility with future processors.
    
    Since mmu_partition_table_set_entry() does a global flush of the
    whole LPID, we don't need to do the TLB flush on first entry to the
    guest on each processor.  Therefore we don't set all bits in the
    tlb_need_flush bitmap on VM startup on POWER9.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 9556de61b1bb..f15713ae3f34 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -244,6 +244,7 @@ struct kvm_arch_memory_slot {
 struct kvm_arch {
 	unsigned int lpid;
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	unsigned int tlb_sets;
 	unsigned long hpt_virt;
 	struct revmap_entry *revmap;
 	atomic64_t mmio_update;

commit e9cf1e085647b433ccd98582681b17121ecfdc21
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Nov 18 13:11:42 2016 +1100

    KVM: PPC: Book3S HV: Add new POWER9 guest-accessible SPRs
    
    This adds code to handle two new guest-accessible special-purpose
    registers on POWER9: TIDR (thread ID register) and PSSCR (processor
    stop status and control register).  They are context-switched
    between host and guest, and the guest values can be read and set
    via the one_reg interface.
    
    The PSSCR contains some fields which are guest-accessible and some
    which are only accessible in hypervisor mode.  We only allow the
    guest-accessible fields to be read or set by userspace.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0e584ee57730..9556de61b1bb 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -517,6 +517,8 @@ struct kvm_vcpu_arch {
 	ulong tcscr;
 	ulong acop;
 	ulong wort;
+	ulong tid;
+	ulong psscr;
 	ulong shadow_srr1;
 #endif
 	u32 vrsave; /* also USPRG0 */

commit 0d808df06a44200f52262b6eb72bcb6042f5a7c5
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Nov 7 15:09:58 2016 +1100

    KVM: PPC: Book3S HV: Save/restore XER in checkpointed register state
    
    When switching from/to a guest that has a transaction in progress,
    we need to save/restore the checkpointed register state.  Although
    XER is part of the CPU state that gets checkpointed, the code that
    does this saving and restoring doesn't save/restore XER.
    
    This fixes it by saving and restoring the XER.  To allow userspace
    to read/write the checkpointed XER value, we also add a new ONE_REG
    specifier.
    
    The visible effect of this bug is that the guest may see its XER
    value being corrupted when it uses transactions.
    
    Fixes: e4e38121507a ("KVM: PPC: Book3S HV: Add transactional memory support")
    Fixes: 0a8eccefcb34 ("KVM: PPC: Book3S HV: Add missing code for transaction reclaim on guest exit")
    Cc: stable@vger.kernel.org # v3.15+
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 20ef27d09d05..0e584ee57730 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -565,6 +565,7 @@ struct kvm_vcpu_arch {
 	u64 tfiar;
 
 	u32 cr_tm;
+	u64 xer_tm;
 	u64 lr_tm;
 	u64 ctr_tm;
 	u64 amr_tm;

commit a56ee9f8f01c5a11ced541f00c67646336f402b6
Author: Yongji Xie <xyjxie@linux.vnet.ibm.com>
Date:   Fri Nov 4 13:55:12 2016 +0800

    KVM: PPC: Book3S HV: Add a per vcpu cache for recently page faulted MMIO entries
    
    This keeps a per vcpu cache for recently page faulted MMIO entries.
    On a page fault, if the entry exists in the cache, we can avoid some
    time-consuming paths, for example, looking up HPT, locking HPTE twice
    and searching mmio gfn from memslots, then directly call
    kvmppc_hv_emulate_mmio().
    
    In current implenment, we limit the size of cache to four. We think
    it's enough to cover the high-frequency MMIO HPTEs in most case.
    For example, considering the case of using virtio device, for virtio
    legacy devices, one HPTE could handle notifications from up to
    1024 (64K page / 64 byte Port IO register) devices, so one cache entry
    is enough; for virtio modern devices, we always need one HPTE to handle
    notification for each device because modern device would use a 8M MMIO
    register to notify host instead of Port IO register, typically the
    system's configuration should not exceed four virtio devices per
    vcpu, four cache entry is also enough in this case. Of course, if needed,
    we could also modify the macro to a module parameter in the future.
    
    Signed-off-by: Yongji Xie <xyjxie@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 28350a294b1e..20ef27d09d05 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -246,6 +246,7 @@ struct kvm_arch {
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	unsigned long hpt_virt;
 	struct revmap_entry *revmap;
+	atomic64_t mmio_update;
 	unsigned int host_lpid;
 	unsigned long host_lpcr;
 	unsigned long sdr1;
@@ -408,6 +409,24 @@ struct kvmppc_passthru_irqmap {
 #define KVMPPC_IRQ_MPIC		1
 #define KVMPPC_IRQ_XICS		2
 
+#define MMIO_HPTE_CACHE_SIZE	4
+
+struct mmio_hpte_cache_entry {
+	unsigned long hpte_v;
+	unsigned long hpte_r;
+	unsigned long rpte;
+	unsigned long pte_index;
+	unsigned long eaddr;
+	unsigned long slb_v;
+	long mmio_update;
+	unsigned int slb_base_pshift;
+};
+
+struct mmio_hpte_cache {
+	struct mmio_hpte_cache_entry entry[MMIO_HPTE_CACHE_SIZE];
+	unsigned int index;
+};
+
 struct openpic;
 
 struct kvm_vcpu_arch {
@@ -655,9 +674,11 @@ struct kvm_vcpu_arch {
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	struct kvm_vcpu_arch_shared shregs;
 
+	struct mmio_hpte_cache mmio_cache;
 	unsigned long pgfault_addr;
 	long pgfault_index;
 	unsigned long pgfault_hpte[2];
+	struct mmio_hpte_cache_entry *pgfault_cache;
 
 	struct task_struct *run_task;
 	struct kvm_run *kvm_run;

commit 88b02cf97bb7e742db3e31671d54177e3e19fd89
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Sep 15 13:42:52 2016 +1000

    KVM: PPC: Book3S: Treat VTB as a per-subcore register, not per-thread
    
    POWER8 has one virtual timebase (VTB) register per subcore, not one
    per CPU thread.  The HV KVM code currently treats VTB as a per-thread
    register, which can lead to spurious soft lockup messages from guests
    which use the VTB as the time source for the soft lockup detector.
    (CPUs before POWER8 did not have the VTB register.)
    
    For HV KVM, this fixes the problem by making only the primary thread
    in each virtual core save and restore the VTB value.  With this,
    the VTB state becomes part of the kvmppc_vcore structure.  This
    also means that "piggybacking" of multiple virtual cores onto one
    subcore is not possible on POWER8, because then the virtual cores
    would share a single VTB register.
    
    PR KVM emulates a VTB register, which is per-vcpu because PR KVM
    has no notion of CPU threads or SMT.  For PR KVM we move the VTB
    state into the kvmppc_vcpu_book3s struct.
    
    Cc: stable@vger.kernel.org # v3.14+
    Reported-by: Thomas Huth <thuth@redhat.com>
    Tested-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index ed30d2ea21b7..28350a294b1e 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -475,7 +475,6 @@ struct kvm_vcpu_arch {
 	ulong purr;
 	ulong spurr;
 	ulong ic;
-	ulong vtb;
 	ulong dscr;
 	ulong amr;
 	ulong uamor;

commit 65e7026a6c90484fbaa076d2c51e61baf7241960
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Fri Aug 19 15:35:57 2016 +1000

    KVM: PPC: Book3S HV: Counters for passthrough IRQ stats
    
    Add VCPU stat counters to track affinity for passthrough
    interrupts.
    
    pthru_all: Counts all passthrough interrupts whose IRQ mappings are
               in the kvmppc_passthru_irq_map structure.
    pthru_host: Counts all cached passthrough interrupts that were injected
                from the host through kvm_set_irq (i.e. not handled in
                real mode).
    pthru_bad_aff: Counts how many cached passthrough interrupts have
                   bad affinity (receiving CPU is not running VCPU that is
                   the target of the virtual interrupt in the guest).
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 89ac1f6c2cb2..ed30d2ea21b7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -137,6 +137,9 @@ struct kvm_vcpu_stat {
 	u64 ld_slow;
 	u64 st_slow;
 #endif
+	u64 pthru_all;
+	u64 pthru_host;
+	u64 pthru_bad_aff;
 };
 
 enum kvm_exit_types {

commit 8daaafc88b46fb3af952e92d7c2816a8950e1363
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Fri Aug 19 15:35:48 2016 +1000

    KVM: PPC: Book3S HV: Introduce kvmppc_passthru_irqmap
    
    This patch introduces an IRQ mapping structure, the
    kvmppc_passthru_irqmap structure that is to be used
    to map the real hardware IRQ in the host with the virtual
    hardware IRQ (gsi) that is injected into a guest by KVM for
    passthrough adapters.
    
    Currently, we assume a separate IRQ mapping structure for
    each guest. Each kvmppc_passthru_irqmap has a mapping arrays,
    containing all defined real<->virtual IRQs.
    
    [paulus@ozlabs.org - removed irq_chip field from struct
     kvmppc_passthru_irqmap; changed parameter for
     kvmppc_get_passthru_irqmap from struct kvm_vcpu * to struct
     kvm *, removed small cached array.]
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 373003f4551d..89ac1f6c2cb2 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -203,6 +203,8 @@ struct kvmppc_spapr_tce_table {
 struct kvmppc_xics;
 struct kvmppc_icp;
 
+struct kvmppc_passthru_irqmap;
+
 /*
  * The reverse mapping array has one entry for each HPTE,
  * which stores the guest's view of the second word of the HPTE
@@ -273,6 +275,7 @@ struct kvm_arch {
 #endif
 #ifdef CONFIG_KVM_XICS
 	struct kvmppc_xics *xics;
+	struct kvmppc_passthru_irqmap *pimap;
 #endif
 	struct kvmppc_ops *kvm_ops;
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
@@ -369,6 +372,20 @@ struct kvmhv_tb_accumulator {
 	u64	tb_max;		/* max time */
 };
 
+#ifdef CONFIG_PPC_BOOK3S_64
+struct kvmppc_irq_map {
+	u32	r_hwirq;
+	u32	v_hwirq;
+	struct irq_desc *desc;
+};
+
+#define	KVMPPC_PIRQ_MAPPED	1024
+struct kvmppc_passthru_irqmap {
+	int n_mapped;
+	struct kvmppc_irq_map mapped[KVMPPC_PIRQ_MAPPED];
+};
+#endif
+
 # ifdef CONFIG_PPC_FSL_BOOK3E
 #define KVMPPC_BOOKE_IAC_NUM	2
 #define KVMPPC_BOOKE_DAC_NUM	2

commit 2a27f514a47d39c50aaa5c07831ab35178955d47
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Tue Aug 2 14:03:23 2016 +1000

    KVM: PPC: Implement existing and add new halt polling vcpu stats
    
    vcpu stats are used to collect information about a vcpu which can be viewed
    in the debugfs. For example halt_attempted_poll and halt_successful_poll
    are used to keep track of the number of times the vcpu attempts to and
    successfully polls. These stats are currently not used on powerpc.
    
    Implement incrementation of the halt_attempted_poll and
    halt_successful_poll vcpu stats for powerpc. Since these stats are summed
    over all the vcpus for all running guests it doesn't matter which vcpu
    they are attributed to, thus we choose the current runner vcpu of the
    vcore.
    
    Also add new vcpu stats: halt_poll_success_ns, halt_poll_fail_ns and
    halt_wait_ns to be used to accumulate the total time spend polling
    successfully, polling unsuccessfully and waiting respectively, and
    halt_successful_wait to accumulate the number of times the vcpu waits.
    Given that halt_poll_success_ns, halt_poll_fail_ns and halt_wait_ns are
    expressed in nanoseconds it is necessary to represent these as 64-bit
    quantities, otherwise they would overflow after only about 4 seconds.
    
    Given that the total time spend either polling or waiting will be known and
    the number of times that each was done, it will be possible to determine
    the average poll and wait times. This will give the ability to tune the kvm
    module parameters based on the calculated average wait and poll times.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Reviewed-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 5ec1dcf300f6..373003f4551d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -116,8 +116,12 @@ struct kvm_vcpu_stat {
 	u64 emulated_inst_exits;
 	u64 dec_exits;
 	u64 ext_intr_exits;
+	u64 halt_poll_success_ns;
+	u64 halt_poll_fail_ns;
+	u64 halt_wait_ns;
 	u64 halt_successful_poll;
 	u64 halt_attempted_poll;
+	u64 halt_successful_wait;
 	u64 halt_poll_invalid;
 	u64 halt_wakeup;
 	u64 dbell_exits;

commit 8a7e75d47b68193339f8727cf4503271d0a0b1d0
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Tue Aug 2 14:03:22 2016 +1000

    KVM: Add provisioning for ulong vm stats and u64 vcpu stats
    
    vms and vcpus have statistics associated with them which can be viewed
    within the debugfs. Currently it is assumed within the vcpu_stat_get() and
    vm_stat_get() functions that all of these statistics are represented as
    u32s, however the next patch adds some u64 vcpu statistics.
    
    Change all vcpu statistics to u64 and modify vcpu_stat_get() accordingly.
    Since vcpu statistics are per vcpu, they will only be updated by a single
    vcpu at a time so this shouldn't present a problem on 32-bit machines
    which can't atomically increment 64-bit numbers. However vm statistics
    could potentially be updated by multiple vcpus from that vm at a time.
    To avoid the overhead of atomics make all vm statistics ulong such that
    they are 64-bit on 64-bit systems where they can be atomically incremented
    and are 32-bit on 32-bit systems which may not be able to atomically
    increment 64-bit numbers. Modify vm_stat_get() to expect ulongs.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Reviewed-by: David Matlack <dmatlack@google.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 6ece4a854a59..5ec1dcf300f6 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -97,41 +97,41 @@ struct kvmppc_vcpu_book3s;
 struct kvmppc_book3s_shadow_vcpu;
 
 struct kvm_vm_stat {
-	u32 remote_tlb_flush;
+	ulong remote_tlb_flush;
 };
 
 struct kvm_vcpu_stat {
-	u32 sum_exits;
-	u32 mmio_exits;
-	u32 signal_exits;
-	u32 light_exits;
+	u64 sum_exits;
+	u64 mmio_exits;
+	u64 signal_exits;
+	u64 light_exits;
 	/* Account for special types of light exits: */
-	u32 itlb_real_miss_exits;
-	u32 itlb_virt_miss_exits;
-	u32 dtlb_real_miss_exits;
-	u32 dtlb_virt_miss_exits;
-	u32 syscall_exits;
-	u32 isi_exits;
-	u32 dsi_exits;
-	u32 emulated_inst_exits;
-	u32 dec_exits;
-	u32 ext_intr_exits;
-	u32 halt_successful_poll;
-	u32 halt_attempted_poll;
-	u32 halt_poll_invalid;
-	u32 halt_wakeup;
-	u32 dbell_exits;
-	u32 gdbell_exits;
-	u32 ld;
-	u32 st;
+	u64 itlb_real_miss_exits;
+	u64 itlb_virt_miss_exits;
+	u64 dtlb_real_miss_exits;
+	u64 dtlb_virt_miss_exits;
+	u64 syscall_exits;
+	u64 isi_exits;
+	u64 dsi_exits;
+	u64 emulated_inst_exits;
+	u64 dec_exits;
+	u64 ext_intr_exits;
+	u64 halt_successful_poll;
+	u64 halt_attempted_poll;
+	u64 halt_poll_invalid;
+	u64 halt_wakeup;
+	u64 dbell_exits;
+	u64 gdbell_exits;
+	u64 ld;
+	u64 st;
 #ifdef CONFIG_PPC_BOOK3S
-	u32 pf_storage;
-	u32 pf_instruc;
-	u32 sp_storage;
-	u32 sp_instruc;
-	u32 queue_intr;
-	u32 ld_slow;
-	u32 st_slow;
+	u64 pf_storage;
+	u64 pf_instruc;
+	u64 sp_storage;
+	u64 sp_instruc;
+	u64 queue_intr;
+	u64 ld_slow;
+	u64 st_slow;
 #endif
 };
 

commit 0cda69dd7cd64fdd54bdf584b5d6ba53767ba422
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Tue Aug 2 14:03:21 2016 +1000

    KVM: PPC: Book3S HV: Implement halt polling
    
    This patch introduces new halt polling functionality into the kvm_hv kernel
    module. When a vcore is idle it will poll for some period of time before
    scheduling itself out.
    
    When all of the runnable vcpus on a vcore have ceded (and thus the vcore is
    idle) we schedule ourselves out to allow something else to run. In the
    event that we need to wake up very quickly (for example an interrupt
    arrives), we are required to wait until we get scheduled again.
    
    Implement halt polling so that when a vcore is idle, and before scheduling
    ourselves, we poll for vcpus in the runnable_threads list which have
    pending exceptions or which leave the ceded state. If we poll successfully
    then we can get back into the guest very quickly without ever scheduling
    ourselves, otherwise we schedule ourselves out as before.
    
    There exists generic halt_polling code in virt/kvm_main.c, however on
    powerpc the polling conditions are different to the generic case. It would
    be nice if we could just implement an arch specific kvm_check_block()
    function, but there is still other arch specific things which need to be
    done for kvm_hv (for example manipulating vcore states) which means that a
    separate implementation is the best option.
    
    Testing of this patch with a TCP round robin test between two guests with
    virtio network interfaces has found a decrease in round trip time of ~15us
    on average. A performance gain is only seen when going out of and
    back into the guest often and quickly, otherwise there is no net benefit
    from the polling. The polling interval is adjusted such that when we are
    often scheduled out for long periods of time it is reduced, and when we
    often poll successfully it is increased. The rate at which the polling
    interval increases or decreases, and the maximum polling interval, can
    be set through module parameters.
    
    Based on the implementation in the generic kvm module by Wanpeng Li and
    Paolo Bonzini, and on direction from Paul Mackerras.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 851575a0c328..6ece4a854a59 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -296,6 +296,7 @@ struct kvm_arch {
 #define VCORE_SLEEPING	3
 #define VCORE_RUNNING	4
 #define VCORE_EXITING	5
+#define VCORE_POLLING	6
 
 /*
  * Struct used to manage memory for a virtual processor area

commit 7b5f8272c792d49da73d98e9ca32f4cbb6d53808
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Tue Aug 2 14:03:20 2016 +1000

    KVM: PPC: Book3S HV: Change vcore element runnable_threads from linked-list to array
    
    The struct kvmppc_vcore is a structure used to store various information
    about a virtual core for a kvm guest. The runnable_threads element of the
    struct provides a list of all of the currently runnable vcpus on the core
    (those in the KVMPPC_VCPU_RUNNABLE state). The previous implementation of
    this list was a linked_list. The next patch requires that the list be able
    to be iterated over without holding the vcore lock.
    
    Reimplement the runnable_threads list in the kvmppc_vcore struct as an
    array. Implement function to iterate over valid entries in the array and
    update access sites accordingly.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 7ff9919916c3..851575a0c328 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -635,7 +635,6 @@ struct kvm_vcpu_arch {
 	long pgfault_index;
 	unsigned long pgfault_hpte[2];
 
-	struct list_head run_list;
 	struct task_struct *run_task;
 	struct kvm_run *kvm_run;
 

commit e64fb7e272885c1ea3cd2f68f267ae12fa04c8b1
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Tue Aug 2 14:03:19 2016 +1000

    KVM: PPC: Book3S HV: Move struct kvmppc_vcore from kvm_host.h to kvm_book3s.h
    
    The next commit will introduce a member to the kvmppc_vcore struct which
    references MAX_SMT_THREADS which is defined in kvm_book3s_asm.h, however
    this file isn't included in kvm_host.h directly. Thus compiling for
    certain platforms such as pmac32_defconfig and ppc64e_defconfig with KVM
    fails due to MAX_SMT_THREADS not being defined.
    
    Move the struct kvmppc_vcore definition to kvm_book3s.h which explicitly
    includes kvm_book3s_asm.h.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e36ce0cff766..7ff9919916c3 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -277,41 +277,6 @@ struct kvm_arch {
 #endif
 };
 
-/*
- * Struct for a virtual core.
- * Note: entry_exit_map combines a bitmap of threads that have entered
- * in the bottom 8 bits and a bitmap of threads that have exited in the
- * next 8 bits.  This is so that we can atomically set the entry bit
- * iff the exit map is 0 without taking a lock.
- */
-struct kvmppc_vcore {
-	int n_runnable;
-	int num_threads;
-	int entry_exit_map;
-	int napping_threads;
-	int first_vcpuid;
-	u16 pcpu;
-	u16 last_cpu;
-	u8 vcore_state;
-	u8 in_guest;
-	struct kvmppc_vcore *master_vcore;
-	struct list_head runnable_threads;
-	struct list_head preempt_list;
-	spinlock_t lock;
-	struct swait_queue_head wq;
-	spinlock_t stoltb_lock;	/* protects stolen_tb and preempt_tb */
-	u64 stolen_tb;
-	u64 preempt_tb;
-	struct kvm_vcpu *runner;
-	struct kvm *kvm;
-	u64 tb_offset;		/* guest timebase - host timebase */
-	ulong lpcr;
-	u32 arch_compat;
-	ulong pcr;
-	ulong dpdes;		/* doorbell state (POWER8) */
-	ulong conferring_threads;
-};
-
 #define VCORE_ENTRY_MAP(vc)	((vc)->entry_exit_map & 0xff)
 #define VCORE_EXIT_MAP(vc)	((vc)->entry_exit_map >> 8)
 #define VCORE_IS_EXITING(vc)	(VCORE_EXIT_MAP(vc) != 0)

commit 34a75b0f63356097ae9f706d64a793934891002f
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Aug 10 11:27:27 2016 +1000

    KVM: PPC: Implement kvm_arch_intc_initialized() for PPC
    
    It doesn't make sense to create irqfds for a VM that doesn't have
    in-kernel interrupt controller emulation.  There is an existing
    interface for architecture code to tell the irqfd code whether or
    not any interrupt controller has been initialized, called
    kvm_arch_intc_initialized(), so let's implement that for powerpc.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index ec35af34a3fb..e36ce0cff766 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -43,6 +43,8 @@
 #include <asm/cputhreads.h>
 #define KVM_MAX_VCPU_ID                (threads_per_subcore * KVM_MAX_VCORES)
 
+#define __KVM_HAVE_ARCH_INTC_INITIALIZED
+
 #ifdef CONFIG_KVM_MMIO
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 #endif

commit 3491caf2755e9f312666712510d80b00c81ff247
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri May 13 12:16:35 2016 +0200

    KVM: halt_polling: provide a way to qualify wakeups during poll
    
    Some wakeups should not be considered a sucessful poll. For example on
    s390 I/O interrupts are usually floating, which means that _ALL_ CPUs
    would be considered runnable - letting all vCPUs poll all the time for
    transactional like workload, even if one vCPU would be enough.
    This can result in huge CPU usage for large guests.
    This patch lets architectures provide a way to qualify wakeups if they
    should be considered a good/bad wakeups in regard to polls.
    
    For s390 the implementation will fence of halt polling for anything but
    known good, single vCPU events. The s390 implementation for floating
    interrupts does a wakeup for one vCPU, but the interrupt will be delivered
    by whatever CPU checks first for a pending interrupt. We prefer the
    woken up CPU by marking the poll of this CPU as "good" poll.
    This code will also mark several other wakeup reasons like IPI or
    expired timers as "good". This will of course also mark some events as
    not sucessful. As  KVM on z runs always as a 2nd level hypervisor,
    we prefer to not poll, unless we are really sure, though.
    
    This patch successfully limits the CPU usage for cases like uperf 1byte
    transactional ping pong workload or wakeup heavy workload like OLTP
    while still providing a proper speedup.
    
    This also introduced a new vcpu stat "halt_poll_no_tuning" that marks
    wakeups that are considered not good for polling.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Radim Krčmář <rkrcmar@redhat.com> (for an earlier version)
    Cc: David Matlack <dmatlack@google.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    [Rename config symbol. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index a07645c17818..ec35af34a3fb 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -116,6 +116,7 @@ struct kvm_vcpu_stat {
 	u32 ext_intr_exits;
 	u32 halt_successful_poll;
 	u32 halt_attempted_poll;
+	u32 halt_poll_invalid;
 	u32 halt_wakeup;
 	u32 dbell_exits;
 	u32 gdbell_exits;
@@ -727,5 +728,6 @@ static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}
 static inline void kvm_arch_exit(void) {}
 static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu) {}
 static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu) {}
+static inline void kvm_arch_vcpu_block_finish(struct kvm_vcpu *vcpu) {}
 
 #endif /* __POWERPC_KVM_HOST_H__ */

commit 0b1b1dfd52a67f4f09a18cb82337199bc90ad7fb
Author: Greg Kurz <gkurz@linux.vnet.ibm.com>
Date:   Mon May 9 18:13:37 2016 +0200

    kvm: introduce KVM_MAX_VCPU_ID
    
    The KVM_MAX_VCPUS define provides the maximum number of vCPUs per guest, and
    also the upper limit for vCPU ids. This is okay for all archs except PowerPC
    which can have higher ids, depending on the cpu/core/thread topology. In the
    worst case (single threaded guest, host with 8 threads per core), it limits
    the maximum number of vCPUS to KVM_MAX_VCPUS / 8.
    
    This patch separates the vCPU numbering from the total number of vCPUs, with
    the introduction of KVM_MAX_VCPU_ID, as the maximal valid value for vCPU ids
    plus one.
    
    The corresponding KVM_CAP_MAX_VCPU_ID allows userspace to validate vCPU ids
    before passing them to KVM_CREATE_VCPU.
    
    This patch only implements KVM_MAX_VCPU_ID with a specific value for PowerPC.
    Other archs continue to return KVM_MAX_VCPUS instead.
    
    Suggested-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Greg Kurz <gkurz@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d7b343170453..a07645c17818 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -40,6 +40,9 @@
 #define KVM_MAX_VCORES		NR_CPUS
 #define KVM_USER_MEM_SLOTS	512
 
+#include <asm/cputhreads.h>
+#define KVM_MAX_VCPU_ID                (threads_per_subcore * KVM_MAX_VCORES)
+
 #ifdef CONFIG_KVM_MMIO
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 #endif

commit 10dc3747661bea9215417b659449bb7b8ed3df2c
Merge: 047486d8e7c2 f958ee745f70
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 16 09:55:35 2016 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "One of the largest releases for KVM...  Hardly any generic
      changes, but lots of architecture-specific updates.
    
      ARM:
       - VHE support so that we can run the kernel at EL2 on ARMv8.1 systems
       - PMU support for guests
       - 32bit world switch rewritten in C
       - various optimizations to the vgic save/restore code.
    
      PPC:
       - enabled KVM-VFIO integration ("VFIO device")
       - optimizations to speed up IPIs between vcpus
       - in-kernel handling of IOMMU hypercalls
       - support for dynamic DMA windows (DDW).
    
      s390:
       - provide the floating point registers via sync regs;
       - separated instruction vs.  data accesses
       - dirty log improvements for huge guests
       - bugfixes and documentation improvements.
    
      x86:
       - Hyper-V VMBus hypercall userspace exit
       - alternative implementation of lowest-priority interrupts using
         vector hashing (for better VT-d posted interrupt support)
       - fixed guest debugging with nested virtualizations
       - improved interrupt tracking in the in-kernel IOAPIC
       - generic infrastructure for tracking writes to guest
         memory - currently its only use is to speedup the legacy shadow
         paging (pre-EPT) case, but in the future it will be used for
         virtual GPUs as well
       - much cleanup (LAPIC, kvmclock, MMU, PIT), including ubsan fixes"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (217 commits)
      KVM: x86: remove eager_fpu field of struct kvm_vcpu_arch
      KVM: x86: disable MPX if host did not enable MPX XSAVE features
      arm64: KVM: vgic-v3: Only wipe LRs on vcpu exit
      arm64: KVM: vgic-v3: Reset LRs at boot time
      arm64: KVM: vgic-v3: Do not save an LR known to be empty
      arm64: KVM: vgic-v3: Save maintenance interrupt state only if required
      arm64: KVM: vgic-v3: Avoid accessing ICH registers
      KVM: arm/arm64: vgic-v2: Make GICD_SGIR quicker to hit
      KVM: arm/arm64: vgic-v2: Only wipe LRs on vcpu exit
      KVM: arm/arm64: vgic-v2: Reset LRs at boot time
      KVM: arm/arm64: vgic-v2: Do not save an LR known to be empty
      KVM: arm/arm64: vgic-v2: Move GICH_ELRSR saving to its own function
      KVM: arm/arm64: vgic-v2: Save maintenance interrupt state only if required
      KVM: arm/arm64: vgic-v2: Avoid accessing GICH registers
      KVM: s390: allocate only one DMA page per VM
      KVM: s390: enable STFLE interpretation only if enabled for the guest
      KVM: s390: wake up when the VCPU cpu timer expires
      KVM: s390: step the VCPU timer while in enabled wait
      KVM: s390: protect VCPU cpu timer with a seqcount
      KVM: s390: step VCPU cpu timer during kvm_run ioctl
      ...

commit 14f853f1b257b69cf0213ad8c49c01038ccf7ef9
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Tue Mar 1 17:54:39 2016 +1100

    KVM: PPC: Add @offset to kvmppc_spapr_tce_table
    
    This enables userspace view of TCE tables to start from non-zero offset
    on a bus. This will be used for huge DMA windows.
    
    This only changes the internal structure, the user interface needs to
    change in order to use an offset.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index edf66f770498..2e7c79101652 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -184,6 +184,7 @@ struct kvmppc_spapr_tce_table {
 	u64 liobn;
 	struct rcu_head rcu;
 	u32 page_shift;
+	u64 offset;		/* in pages */
 	u64 size;		/* window size in pages */
 	struct page *pages[0];
 };

commit fe26e52712ccab6648df17ecc029a68a69a01a85
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Tue Mar 1 17:54:38 2016 +1100

    KVM: PPC: Add @page_shift to kvmppc_spapr_tce_table
    
    At the moment the kvmppc_spapr_tce_table struct can only describe
    4GB windows and handle fixed size (4K) pages. Dynamic DMA windows
    support more so these limits need to be extended.
    
    This replaces window_size (in bytes, 4GB max) with page_shift (32bit)
    and size (64bit, in pages).
    
    This should cause no behavioural change as this is changing
    the internal structures only - the user interface still only
    allows one to create a 32-bit table with 4KiB pages at this stage.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index ffdbc2dc18f9..edf66f770498 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -182,8 +182,9 @@ struct kvmppc_spapr_tce_table {
 	struct list_head list;
 	struct kvm *kvm;
 	u64 liobn;
-	u32 window_size;
 	struct rcu_head rcu;
+	u32 page_shift;
+	u64 size;		/* window size in pages */
 	struct page *pages[0];
 };
 

commit 8577370fb0cbe88266b7583d8d3b9f43ced077a0
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Feb 19 09:46:39 2016 +0100

    KVM: Use simple waitqueue for vcpu->wq
    
    The problem:
    
    On -rt, an emulated LAPIC timer instances has the following path:
    
    1) hard interrupt
    2) ksoftirqd is scheduled
    3) ksoftirqd wakes up vcpu thread
    4) vcpu thread is scheduled
    
    This extra context switch introduces unnecessary latency in the
    LAPIC path for a KVM guest.
    
    The solution:
    
    Allow waking up vcpu thread from hardirq context,
    thus avoiding the need for ksoftirqd to be scheduled.
    
    Normal waitqueues make use of spinlocks, which on -RT
    are sleepable locks. Therefore, waking up a waitqueue
    waiter involves locking a sleeping lock, which
    is not allowed from hard interrupt context.
    
    cyclictest command line:
    
    This patch reduces the average latency in my tests from 14us to 11us.
    
    Daniel writes:
    Paolo asked for numbers from kvm-unit-tests/tscdeadline_latency
    benchmark on mainline. The test was run 1000 times on
    tip/sched/core 4.4.0-rc8-01134-g0905f04:
    
      ./x86-run x86/tscdeadline_latency.flat -cpu host
    
    with idle=poll.
    
    The test seems not to deliver really stable numbers though most of
    them are smaller. Paolo write:
    
    "Anything above ~10000 cycles means that the host went to C1 or
    lower---the number means more or less nothing in that case.
    
    The mean shows an improvement indeed."
    
    Before:
    
                   min             max         mean           std
    count  1000.000000     1000.000000  1000.000000   1000.000000
    mean   5162.596000  2019270.084000  5824.491541  20681.645558
    std      75.431231   622607.723969    89.575700   6492.272062
    min    4466.000000    23928.000000  5537.926500    585.864966
    25%    5163.000000  1613252.750000  5790.132275  16683.745433
    50%    5175.000000  2281919.000000  5834.654000  23151.990026
    75%    5190.000000  2382865.750000  5861.412950  24148.206168
    max    5228.000000  4175158.000000  6254.827300  46481.048691
    
    After
                   min            max         mean           std
    count  1000.000000     1000.00000  1000.000000   1000.000000
    mean   5143.511000  2076886.10300  5813.312474  21207.357565
    std      77.668322   610413.09583    86.541500   6331.915127
    min    4427.000000    25103.00000  5529.756600    559.187707
    25%    5148.000000  1691272.75000  5784.889825  17473.518244
    50%    5160.000000  2308328.50000  5832.025000  23464.837068
    75%    5172.000000  2393037.75000  5853.177675  24223.969976
    max    5222.000000  3922458.00000  6186.720500  42520.379830
    
    [Patch was originaly based on the swait implementation found in the -rt
     tree. Daniel ported it to mainline's version and gathered the
     benchmark numbers for tscdeadline_latency test.]
    
    Signed-off-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-rt-users@vger.kernel.org
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1455871601-27484-4-git-send-email-wagi@monom.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 9d08d8cbed1a..c98afa538b3a 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -289,7 +289,7 @@ struct kvmppc_vcore {
 	struct list_head runnable_threads;
 	struct list_head preempt_list;
 	spinlock_t lock;
-	wait_queue_head_t wq;
+	struct swait_queue_head wq;
 	spinlock_t stoltb_lock;	/* protects stolen_tb and preempt_tb */
 	u64 stolen_tb;
 	u64 preempt_tb;
@@ -629,7 +629,7 @@ struct kvm_vcpu_arch {
 	u8 prodded;
 	u32 last_inst;
 
-	wait_queue_head_t *wqp;
+	struct swait_queue_head *wqp;
 	struct kvmppc_vcore *vcore;
 	int ret;
 	int trap;

commit 366baf28ee3fc22dea504a0bddf8edd1e9bcee70
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Mon Feb 15 12:55:05 2016 +1100

    KVM: PPC: Use RCU for arch.spapr_tce_tables
    
    At the moment only spapr_tce_tables updates are protected against races
    but not lookups. This fixes missing protection by using RCU for the list.
    As lookups also happen in real mode, this uses
    list_for_each_entry_lockless() (which is expected not to access any
    vmalloc'd memory).
    
    This converts release_spapr_tce_table() to a RCU scheduled handler.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 9d08d8cbed1a..ffdbc2dc18f9 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -183,6 +183,7 @@ struct kvmppc_spapr_tce_table {
 	struct kvm *kvm;
 	u64 liobn;
 	u32 window_size;
+	struct rcu_head rcu;
 	struct page *pages[0];
 };
 

commit 171b5682aa8597174e80ec4128c87538103f2213
Merge: 45bdbcfdf241 b4d7f161feb3
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jan 15 17:49:39 2016 +0100

    Merge branch 'kvm-ppc-next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD

commit 2860c4b1678646c99f5f1d77d026cd12ffd8a3a9
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 7 15:05:10 2016 +0100

    KVM: move architecture-dependent requests to arch/
    
    Since the numbers now overlap, it makes sense to enumerate
    them in asm/kvm_host.h rather than linux/kvm_host.h.  Functions
    that refer to architecture-specific requests are also moved
    to arch/.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index cfa758c6b4f6..271fefbbe521 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -50,6 +50,10 @@
 #define KVM_NR_IRQCHIPS          1
 #define KVM_IRQCHIP_NUM_PINS     256
 
+/* PPC-specific vcpu->requests bit members */
+#define KVM_REQ_WATCHDOG           8
+#define KVM_REQ_EPR_EXIT           9
+
 #include <linux/mmu_notifier.h>
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER

commit 696066f875bc86d0a9cdcf9cbc4846d89b1f38db
Author: Thomas Huth <thuth@redhat.com>
Date:   Wed Dec 9 11:34:07 2015 +0100

    KVM: PPC: Increase memslots to 512
    
    Only using 32 memslots for KVM on powerpc is way too low, you can
    nowadays hit this limit quite fast by adding a couple of PCI devices
    and/or pluggable memory DIMMs to the guest.
    
    x86 already increased the KVM_USER_MEM_SLOTS to 509, to satisfy 256
    pluggable DIMM slots, 3 private slots and 253 slots for other things
    like PCI devices (i.e. resulting in 256 + 3 + 253 = 512 slots in
    total). We should do something similar for powerpc, and since we do
    not use private slots here, we can set the value to 512 directly.
    
    While we're at it, also remove the KVM_MEM_SLOTS_NUM definition
    from the powerpc-specific header since this gets defined in the
    generic kvm_host.h header anyway.
    
    Signed-off-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index cfa758c6b4f6..8c8f2435a13d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -38,8 +38,7 @@
 
 #define KVM_MAX_VCPUS		NR_CPUS
 #define KVM_MAX_VCORES		NR_CPUS
-#define KVM_USER_MEM_SLOTS 32
-#define KVM_MEM_SLOTS_NUM KVM_USER_MEM_SLOTS
+#define KVM_USER_MEM_SLOTS	512
 
 #ifdef CONFIG_KVM_MMIO
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1

commit 933425fb0010bd02bd459b41e63082756818ffce
Merge: a3e7531535a0 a3eaa8649e4c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 5 16:26:26 2015 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "First batch of KVM changes for 4.4.
    
      s390:
         A bunch of fixes and optimizations for interrupt and time handling.
    
      PPC:
         Mostly bug fixes.
    
      ARM:
         No big features, but many small fixes and prerequisites including:
    
          - a number of fixes for the arch-timer
    
          - introducing proper level-triggered semantics for the arch-timers
    
          - a series of patches to synchronously halt a guest (prerequisite
            for IRQ forwarding)
    
          - some tracepoint improvements
    
          - a tweak for the EL2 panic handlers
    
          - some more VGIC cleanups getting rid of redundant state
    
      x86:
         Quite a few changes:
    
          - support for VT-d posted interrupts (i.e. PCI devices can inject
            interrupts directly into vCPUs).  This introduces a new
            component (in virt/lib/) that connects VFIO and KVM together.
            The same infrastructure will be used for ARM interrupt
            forwarding as well.
    
          - more Hyper-V features, though the main one Hyper-V synthetic
            interrupt controller will have to wait for 4.5.  These will let
            KVM expose Hyper-V devices.
    
          - nested virtualization now supports VPID (same as PCID but for
            vCPUs) which makes it quite a bit faster
    
          - for future hardware that supports NVDIMM, there is support for
            clflushopt, clwb, pcommit
    
          - support for "split irqchip", i.e.  LAPIC in kernel +
            IOAPIC/PIC/PIT in userspace, which reduces the attack surface of
            the hypervisor
    
          - obligatory smattering of SMM fixes
    
          - on the guest side, stable scheduler clock support was rewritten
            to not require help from the hypervisor"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (123 commits)
      KVM: VMX: Fix commit which broke PML
      KVM: x86: obey KVM_X86_QUIRK_CD_NW_CLEARED in kvm_set_cr0()
      KVM: x86: allow RSM from 64-bit mode
      KVM: VMX: fix SMEP and SMAP without EPT
      KVM: x86: move kvm_set_irq_inatomic to legacy device assignment
      KVM: device assignment: remove pointless #ifdefs
      KVM: x86: merge kvm_arch_set_irq with kvm_set_msi_inatomic
      KVM: x86: zero apic_arb_prio on reset
      drivers/hv: share Hyper-V SynIC constants with userspace
      KVM: x86: handle SMBASE as physical address in RSM
      KVM: x86: add read_phys to x86_emulate_ops
      KVM: x86: removing unused variable
      KVM: don't pointlessly leave KVM_COMPAT=y in non-KVM configs
      KVM: arm/arm64: Merge vgic_set_lr() and vgic_sync_lr_elrsr()
      KVM: arm/arm64: Clean up vgic_retire_lr() and surroundings
      KVM: arm/arm64: Optimize away redundant LR tracking
      KVM: s390: use simple switch statement as multiplexer
      KVM: s390: drop useless newline in debugging data
      KVM: s390: SCA must not cross page boundaries
      KVM: arm: Do not indent the arguments of DECLARE_BITMAP
      ...

commit 3217f7c25bca66eed9b07f0b8bfd1937169b0736
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Aug 27 16:41:15 2015 +0200

    KVM: Add kvm_arch_vcpu_{un}blocking callbacks
    
    Some times it is useful for architecture implementations of KVM to know
    when the VCPU thread is about to block or when it comes back from
    blocking (arm/arm64 needs to know this to properly implement timers, for
    example).
    
    Therefore provide a generic architecture callback function in line with
    what we do elsewhere for KVM generic-arch interactions.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 827a38d7a9db..c9f122d00920 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -718,5 +718,7 @@ static inline void kvm_arch_memslots_updated(struct kvm *kvm, struct kvm_memslot
 static inline void kvm_arch_flush_shadow_all(struct kvm *kvm) {}
 static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}
 static inline void kvm_arch_exit(void) {}
+static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu) {}
+static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu) {}
 
 #endif /* __POWERPC_KVM_HOST_H__ */

commit 23316316c1af0677a041c81f3ad6efb9dc470b33
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Oct 21 16:03:14 2015 +1100

    powerpc: Revert "Use the POWER8 Micro Partition Prefetch Engine in KVM HV on POWER8"
    
    This reverts commit 9678cdaae939 ("Use the POWER8 Micro Partition
    Prefetch Engine in KVM HV on POWER8") because the original commit had
    multiple, partly self-cancelling bugs, that could cause occasional
    memory corruption.
    
    In fact the logmpp instruction was incorrectly using register r0 as the
    source of the buffer address and operation code, and depending on what
    was in r0, it would either do nothing or corrupt the 64k page pointed to
    by r0.
    
    The logmpp instruction encoding and the operation code definitions could
    be corrected, but then there is the problem that there is no clearly
    defined way to know when the hardware has finished writing to the
    buffer.
    
    The original commit attempted to work around this by aborting the
    write-out before starting the prefetch, but this is ineffective in the
    case where the virtual core is now executing on a different physical
    core from the one where the write-out was initiated.
    
    These problems plus advice from the hardware designers not to use the
    function (since the measured performance improvement from using the
    feature was actually mostly negative), mean that reverting the code is
    the best option.
    
    Fixes: 9678cdaae939 ("Use the POWER8 Micro Partition Prefetch Engine in KVM HV on POWER8")
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 827a38d7a9db..887c259556df 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -297,8 +297,6 @@ struct kvmppc_vcore {
 	u32 arch_compat;
 	ulong pcr;
 	ulong dpdes;		/* doorbell state (POWER8) */
-	void *mpp_buffer; /* Micro Partition Prefetch buffer */
-	bool mpp_buffer_is_valid;
 	ulong conferring_threads;
 };
 

commit 920552b213e3dc832a874b4e7ba29ecddbab31bc
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Sep 18 12:34:53 2015 +0200

    KVM: disable halt_poll_ns as default for s390x
    
    We observed some performance degradation on s390x with dynamic
    halt polling. Until we can provide a proper fix, let's enable
    halt_poll_ns as default only for supported architectures.
    
    Architectures are now free to set their own halt_poll_ns
    default value.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 195886a583ba..827a38d7a9db 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -44,6 +44,7 @@
 #ifdef CONFIG_KVM_MMIO
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 #endif
+#define KVM_HALT_POLL_NS_DEFAULT 500000
 
 /* These values are internal and can be increased later */
 #define KVM_NR_IRQCHIPS          1

commit 62bea5bff486644ecf363fe8a1a2f6f32c614a49
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Sep 15 18:27:57 2015 +0200

    KVM: add halt_attempted_poll to VCPU stats
    
    This new statistic can help diagnosing VCPUs that, for any reason,
    trigger bad behavior of halt_poll_ns autotuning.
    
    For example, say halt_poll_ns = 480000, and wakeups are spaced exactly
    like 479us, 481us, 479us, 481us. Then KVM always fails polling and wastes
    10+20+40+80+160+320+480 = 1110 microseconds out of every
    479+481+479+481+479+481+479 = 3359 microseconds. The VCPU then
    is consuming about 30% more CPU than it would use without
    polling.  This would show as an abnormally high number of
    attempted polling compared to the successful polls.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com<
    Reviewed-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 98eebbf66340..195886a583ba 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -108,6 +108,7 @@ struct kvm_vcpu_stat {
 	u32 dec_exits;
 	u32 ext_intr_exits;
 	u32 halt_successful_poll;
+	u32 halt_attempted_poll;
 	u32 halt_wakeup;
 	u32 dbell_exits;
 	u32 gdbell_exits;

commit f35f3a48d6ee810557b58e6b7d933438999401b6
Author: Thomas Huth <thuth@redhat.com>
Date:   Wed Sep 2 11:14:48 2015 +0200

    KVM: PPC: Book3S: Fix size of the PSPB register
    
    The size of the Problem State Priority Boost Register is only
    32 bits, but the kvm_vcpu_arch->pspb variable is declared as
    "ulong", ie. 64-bit. However, the assembler code accesses this
    variable with 32-bit accesses, and the KVM_REG_PPC_PSPB macro
    is defined with SIZE_U32, too, so that the current code is
    broken on big endian hosts: kvmppc_get_one_reg_hv() will only
    return zero for this register since it is using the wrong half
    of the pspb variable. Let's fix this problem by adjusting the
    size of the pspb field in the kvm_vcpu_arch structure.
    
    Signed-off-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e187b6a56e7e..98eebbf66340 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -486,7 +486,7 @@ struct kvm_vcpu_arch {
 	ulong ciabr;
 	ulong cfar;
 	ulong ppr;
-	ulong pspb;
+	u32 pspb;
 	ulong fscr;
 	ulong shadow_fscr;
 	ulong ebbhr;

commit 08fe1e7bd216339501c4eb0d0df0f413d715327a
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 24 21:18:06 2015 +1000

    KVM: PPC: Book3S HV: Fix bug in dirty page tracking
    
    This fixes a bug in the tracking of pages that get modified by the
    guest.  If the guest creates a large-page HPTE, writes to memory
    somewhere within the large page, and then removes the HPTE, we only
    record the modified state for the first normal page within the large
    page, when in fact the guest might have modified some other normal
    page within the large page.
    
    To fix this we use some unused bits in the rmap entry to record the
    order (log base 2) of the size of the page that was modified, when
    removing an HPTE.  Then in kvm_test_clear_dirty_npages() we use that
    order to return the correct number of modified pages.
    
    The same thing could in principle happen when removing a HPTE at the
    host's request, i.e. when paging out a page, except that we never
    page out large pages, and the guest can only create large-page HPTEs
    if the guest RAM is backed by large pages.  However, we also fix
    this case for the sake of future-proofing.
    
    The reference bit is also subject to the same loss of information.  We
    don't make the same fix here for the reference bit because there isn't
    an interface for userspace to find out which pages the guest has
    referenced, whereas there is one for userspace to find out which pages
    the guest has modified.  Because of this loss of information, the
    kvm_age_hva_hv() and kvm_test_age_hva_hv() functions might incorrectly
    say that a page has not been referenced when it has, but that doesn't
    matter greatly because we never page or swap out large pages.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 80eb29ab262a..e187b6a56e7e 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -205,8 +205,10 @@ struct revmap_entry {
  */
 #define KVMPPC_RMAP_LOCK_BIT	63
 #define KVMPPC_RMAP_RC_SHIFT	32
+#define KVMPPC_RMAP_CHG_SHIFT	48
 #define KVMPPC_RMAP_REFERENCED	(HPTE_R_R << KVMPPC_RMAP_RC_SHIFT)
 #define KVMPPC_RMAP_CHANGED	(HPTE_R_C << KVMPPC_RMAP_RC_SHIFT)
+#define KVMPPC_RMAP_CHG_ORDER	(0x3ful << KVMPPC_RMAP_CHG_SHIFT)
 #define KVMPPC_RMAP_PRESENT	0x100000000ul
 #define KVMPPC_RMAP_INDEX	0xfffffffful
 

commit b4deba5c41e9f6d3239606c9e060853d9decfee1
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jul 2 20:38:16 2015 +1000

    KVM: PPC: Book3S HV: Implement dynamic micro-threading on POWER8
    
    This builds on the ability to run more than one vcore on a physical
    core by using the micro-threading (split-core) modes of the POWER8
    chip.  Previously, only vcores from the same VM could be run together,
    and (on POWER8) only if they had just one thread per core.  With the
    ability to split the core on guest entry and unsplit it on guest exit,
    we can run up to 8 vcpu threads from up to 4 different VMs, and we can
    run multiple vcores with 2 or 4 vcpus per vcore.
    
    Dynamic micro-threading is only available if the static configuration
    of the cores is whole-core mode (unsplit), and only on POWER8.
    
    To manage this, we introduce a new kvm_split_mode struct which is
    shared across all of the subcores in the core, with a pointer in the
    paca on each thread.  In addition we extend the core_info struct to
    have information on each subcore.  When deciding whether to add a
    vcore to the set already on the core, we now have two possibilities:
    (a) piggyback the vcore onto an existing subcore, or (b) start a new
    subcore.
    
    Currently, when any vcpu needs to exit the guest and switch to host
    virtual mode, we interrupt all the threads in all subcores and switch
    the core back to whole-core mode.  It may be possible in future to
    allow some of the subcores to keep executing in the guest while
    subcore 0 switches to the host, but that is not implemented in this
    patch.
    
    This adds a module parameter called dynamic_mt_modes which controls
    which micro-threading (split-core) modes the code will consider, as a
    bitmap.  In other words, if it is 0, no micro-threading mode is
    considered; if it is 2, only 2-way micro-threading is considered; if
    it is 4, only 4-way, and if it is 6, both 2-way and 4-way
    micro-threading mode will be considered.  The default is 6.
    
    With this, we now have secondary threads which are the primary thread
    for their subcore and therefore need to do the MMU switch.  These
    threads will need to be started even if they have no vcpu to run, so
    we use the vcore pointer in the PACA rather than the vcpu pointer to
    trigger them.
    
    It is now possible for thread 0 to find that an exit has been
    requested before it gets to switch the subcore state to the guest.  In
    that case we haven't added the guest's timebase offset to the
    timebase, so we need to be careful not to subtract the offset in the
    guest exit path.  In fact we just skip the whole path that switches
    back to host context, since we haven't switched to the guest context.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 2b7449017ae8..80eb29ab262a 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -302,6 +302,9 @@ struct kvmppc_vcore {
 #define VCORE_EXIT_MAP(vc)	((vc)->entry_exit_map >> 8)
 #define VCORE_IS_EXITING(vc)	(VCORE_EXIT_MAP(vc) != 0)
 
+/* This bit is used when a vcore exit is triggered from outside the vcore */
+#define VCORE_EXIT_REQ		0x10000
+
 /*
  * Values for vcore_state.
  * Note that these are arranged such that lower values

commit ec257165082616841a354dd915801ed43e3553be
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 24 21:18:03 2015 +1000

    KVM: PPC: Book3S HV: Make use of unused threads when running guests
    
    When running a virtual core of a guest that is configured with fewer
    threads per core than the physical cores have, the extra physical
    threads are currently unused.  This makes it possible to use them to
    run one or more other virtual cores from the same guest when certain
    conditions are met.  This applies on POWER7, and on POWER8 to guests
    with one thread per virtual core.  (It doesn't apply to POWER8 guests
    with multiple threads per vcore because they require a 1-1 virtual to
    physical thread mapping in order to be able to use msgsndp and the
    TIR.)
    
    The idea is that we maintain a list of preempted vcores for each
    physical cpu (i.e. each core, since the host runs single-threaded).
    Then, when a vcore is about to run, it checks to see if there are
    any vcores on the list for its physical cpu that could be
    piggybacked onto this vcore's execution.  If so, those additional
    vcores are put into state VCORE_PIGGYBACK and their runnable VCPU
    threads are started as well as the original vcore, which is called
    the master vcore.
    
    After the vcores have exited the guest, the extra ones are put back
    onto the preempted list if any of their VCPUs are still runnable and
    not idle.
    
    This means that vcpu->arch.ptid is no longer necessarily the same as
    the physical thread that the vcpu runs on.  In order to make it easier
    for code that wants to send an IPI to know which CPU to target, we
    now store that in a new field in struct vcpu_arch, called thread_cpu.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Tested-by: Laurent Vivier <lvivier@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d91f65b28e32..2b7449017ae8 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -278,7 +278,9 @@ struct kvmppc_vcore {
 	u16 last_cpu;
 	u8 vcore_state;
 	u8 in_guest;
+	struct kvmppc_vcore *master_vcore;
 	struct list_head runnable_threads;
+	struct list_head preempt_list;
 	spinlock_t lock;
 	wait_queue_head_t wq;
 	spinlock_t stoltb_lock;	/* protects stolen_tb and preempt_tb */
@@ -300,12 +302,18 @@ struct kvmppc_vcore {
 #define VCORE_EXIT_MAP(vc)	((vc)->entry_exit_map >> 8)
 #define VCORE_IS_EXITING(vc)	(VCORE_EXIT_MAP(vc) != 0)
 
-/* Values for vcore_state */
+/*
+ * Values for vcore_state.
+ * Note that these are arranged such that lower values
+ * (< VCORE_SLEEPING) don't require stolen time accounting
+ * on load/unload, and higher values do.
+ */
 #define VCORE_INACTIVE	0
-#define VCORE_SLEEPING	1
-#define VCORE_PREEMPT	2
-#define VCORE_RUNNING	3
-#define VCORE_EXITING	4
+#define VCORE_PREEMPT	1
+#define VCORE_PIGGYBACK	2
+#define VCORE_SLEEPING	3
+#define VCORE_RUNNING	4
+#define VCORE_EXITING	5
 
 /*
  * Struct used to manage memory for a virtual processor area
@@ -619,6 +627,7 @@ struct kvm_vcpu_arch {
 	int trap;
 	int state;
 	int ptid;
+	int thread_cpu;
 	bool timer_running;
 	wait_queue_head_t cpu_run;
 

commit 15f46015ee17681b542432df21747f5c51857156
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sun May 17 21:26:08 2015 +0200

    KVM: add memslots argument to kvm_arch_memslots_updated
    
    Prepare for the case of multiple address spaces.
    
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index a193a13cf08b..d91f65b28e32 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -698,7 +698,7 @@ struct kvm_vcpu_arch {
 static inline void kvm_arch_hardware_disable(void) {}
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
-static inline void kvm_arch_memslots_updated(struct kvm *kvm) {}
+static inline void kvm_arch_memslots_updated(struct kvm *kvm, struct kvm_memslots *slots) {}
 static inline void kvm_arch_flush_shadow_all(struct kvm *kvm) {}
 static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}
 static inline void kvm_arch_exit(void) {}

commit eadf16a912b6bdf8bd476bde2f19fb41d06e0c3b
Merge: 4a6554665c62 2fa462f82621
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 26 13:06:22 2015 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull second batch of KVM changes from Paolo Bonzini:
     "This mostly includes the PPC changes for 4.1, which this time cover
      Book3S HV only (debugging aids, minor performance improvements and
      some cleanups).  But there are also bug fixes and small cleanups for
      ARM, x86 and s390.
    
      The task_migration_notifier revert and real fix is still pending
      review, but I'll send it as soon as possible after -rc1"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (29 commits)
      KVM: arm/arm64: check IRQ number on userland injection
      KVM: arm: irqfd: fix value returned by kvm_irq_map_gsi
      KVM: VMX: Preserve host CR4.MCE value while in guest mode.
      KVM: PPC: Book3S HV: Use msgsnd for signalling threads on POWER8
      KVM: PPC: Book3S HV: Translate kvmhv_commence_exit to C
      KVM: PPC: Book3S HV: Streamline guest entry and exit
      KVM: PPC: Book3S HV: Use bitmap of active threads rather than count
      KVM: PPC: Book3S HV: Use decrementer to wake napping threads
      KVM: PPC: Book3S HV: Don't wake thread with no vcpu on guest IPI
      KVM: PPC: Book3S HV: Get rid of vcore nap_count and n_woken
      KVM: PPC: Book3S HV: Move vcore preemption point up into kvmppc_run_vcpu
      KVM: PPC: Book3S HV: Minor cleanups
      KVM: PPC: Book3S HV: Simplify handling of VCPUs that need a VPA update
      KVM: PPC: Book3S HV: Accumulate timing information for real-mode code
      KVM: PPC: Book3S HV: Create debugfs file for each guest's HPT
      KVM: PPC: Book3S HV: Add ICP real mode counters
      KVM: PPC: Book3S HV: Move virtual mode ICP functions to real-mode
      KVM: PPC: Book3S HV: Convert ICS mutex lock to spin lock
      KVM: PPC: Book3S HV: Add guest->host real mode completion counters
      KVM: PPC: Book3S HV: Add helpers for lock/unlock hpte
      ...

commit 7d6c40da198ac18bd5dd2cd18628d5b4c615d842
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:09 2015 +1100

    KVM: PPC: Book3S HV: Use bitmap of active threads rather than count
    
    Currently, the entry_exit_count field in the kvmppc_vcore struct
    contains two 8-bit counts, one of the threads that have started entering
    the guest, and one of the threads that have started exiting the guest.
    This changes it to an entry_exit_map field which contains two bitmaps
    of 8 bits each.  The advantage of doing this is that it gives us a
    bitmap of which threads need to be signalled when exiting the guest.
    That means that we no longer need to use the trick of setting the
    HDEC to 0 to pull the other threads out of the guest, which led in
    some cases to a spurious HDEC interrupt on the next guest entry.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1517faac9b98..d67a83830bd1 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -263,15 +263,15 @@ struct kvm_arch {
 
 /*
  * Struct for a virtual core.
- * Note: entry_exit_count combines an entry count in the bottom 8 bits
- * and an exit count in the next 8 bits.  This is so that we can
- * atomically increment the entry count iff the exit count is 0
- * without taking the lock.
+ * Note: entry_exit_map combines a bitmap of threads that have entered
+ * in the bottom 8 bits and a bitmap of threads that have exited in the
+ * next 8 bits.  This is so that we can atomically set the entry bit
+ * iff the exit map is 0 without taking a lock.
  */
 struct kvmppc_vcore {
 	int n_runnable;
 	int num_threads;
-	int entry_exit_count;
+	int entry_exit_map;
 	int napping_threads;
 	int first_vcpuid;
 	u16 pcpu;
@@ -296,8 +296,9 @@ struct kvmppc_vcore {
 	ulong conferring_threads;
 };
 
-#define VCORE_ENTRY_COUNT(vc)	((vc)->entry_exit_count & 0xff)
-#define VCORE_EXIT_COUNT(vc)	((vc)->entry_exit_count >> 8)
+#define VCORE_ENTRY_MAP(vc)	((vc)->entry_exit_map & 0xff)
+#define VCORE_EXIT_MAP(vc)	((vc)->entry_exit_map >> 8)
+#define VCORE_IS_EXITING(vc)	(VCORE_EXIT_MAP(vc) != 0)
 
 /* Values for vcore_state */
 #define VCORE_INACTIVE	0

commit 5d5b99cd6818bdbea287d23ef055bba1a8a9e648
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:06 2015 +1100

    KVM: PPC: Book3S HV: Get rid of vcore nap_count and n_woken
    
    We can tell when a secondary thread has finished running a guest by
    the fact that it clears its kvm_hstate.kvm_vcpu pointer, so there
    is no real need for the nap_count field in the kvmppc_vcore struct.
    This changes kvmppc_wait_for_nap to poll the kvm_hstate.kvm_vcpu
    pointers of the secondary threads rather than polling vc->nap_count.
    Besides reducing the size of the kvmppc_vcore struct by 8 bytes,
    this also means that we can tell which secondary threads have got
    stuck and thus print a more informative error message.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 83c44257b005..1517faac9b98 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -272,8 +272,6 @@ struct kvmppc_vcore {
 	int n_runnable;
 	int num_threads;
 	int entry_exit_count;
-	int n_woken;
-	int nap_count;
 	int napping_threads;
 	int first_vcpuid;
 	u16 pcpu;

commit 25fedfca94cfbf2461314c6c34ef58e74a31b025
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:05 2015 +1100

    KVM: PPC: Book3S HV: Move vcore preemption point up into kvmppc_run_vcpu
    
    Rather than calling cond_resched() in kvmppc_run_core() before doing
    the post-processing for the vcpus that we have just run (that is,
    calling kvmppc_handle_exit_hv(), kvmppc_set_timer(), etc.), we now do
    that post-processing before calling cond_resched(), and that post-
    processing is moved out into its own function, post_guest_process().
    
    The reschedule point is now in kvmppc_run_vcpu() and we define a new
    vcore state, VCORE_PREEMPT, to indicate that that the vcore's runner
    task is runnable but not running.  (Doing the reschedule with the
    vcore in VCORE_INACTIVE state would be bad because there are potentially
    other vcpus waiting for the runner in kvmppc_wait_for_exec() which
    then wouldn't get woken up.)
    
    Also, we make use of the handy cond_resched_lock() function, which
    unlocks and relocks vc->lock for us around the reschedule.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 3eecd8868b01..83c44257b005 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -304,8 +304,9 @@ struct kvmppc_vcore {
 /* Values for vcore_state */
 #define VCORE_INACTIVE	0
 #define VCORE_SLEEPING	1
-#define VCORE_RUNNING	2
-#define VCORE_EXITING	3
+#define VCORE_PREEMPT	2
+#define VCORE_RUNNING	3
+#define VCORE_EXITING	4
 
 /*
  * Struct used to manage memory for a virtual processor area

commit 1f09c3ed86287d40fef90611cbbee055313f52cf
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:04 2015 +1100

    KVM: PPC: Book3S HV: Minor cleanups
    
    * Remove unused kvmppc_vcore::n_busy field.
    * Remove setting of RMOR, since it was only used on PPC970 and the
      PPC970 KVM support has been removed.
    * Don't use r1 or r2 in setting the runlatch since they are
      conventionally reserved for other things; use r0 instead.
    * Streamline the code a little and remove the ext_interrupt_to_host
      label.
    * Add some comments about register usage.
    * hcall_try_real_mode doesn't need to be global, and can't be
      called from C code anyway.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 2f339ff9b851..3eecd8868b01 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -227,7 +227,6 @@ struct kvm_arch {
 	unsigned long host_sdr1;
 	int tlbie_lock;
 	unsigned long lpcr;
-	unsigned long rmor;
 	unsigned long vrma_slb_v;
 	int hpte_setup_done;
 	u32 hpt_order;
@@ -271,7 +270,6 @@ struct kvm_arch {
  */
 struct kvmppc_vcore {
 	int n_runnable;
-	int n_busy;
 	int num_threads;
 	int entry_exit_count;
 	int n_woken;

commit d911f0beddc2a9248dbf375fc50a4bbf30947822
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:03 2015 +1100

    KVM: PPC: Book3S HV: Simplify handling of VCPUs that need a VPA update
    
    Previously, if kvmppc_run_core() was running a VCPU that needed a VPA
    update (i.e. one of its 3 virtual processor areas needed to be pinned
    in memory so the host real mode code can update it on guest entry and
    exit), we would drop the vcore lock and do the update there and then.
    Future changes will make it inconvenient to drop the lock, so instead
    we now remove it from the list of runnable VCPUs and wake up its
    VCPU task.  This will have the effect that the VCPU task will exit
    kvmppc_run_vcpu(), go around the do loop in kvmppc_vcpu_run_hv(), and
    re-enter kvmppc_run_vcpu(), whereupon it will do the necessary call
    to kvmppc_update_vpas() and then rejoin the vcore.
    
    The one complication is that the runner VCPU (whose VCPU task is the
    current task) might be one of the ones that gets removed from the
    runnable list.  In that case we just return from kvmppc_run_core()
    and let the code in kvmppc_run_vcpu() wake up another VCPU task to be
    the runner if necessary.
    
    This all means that the VCORE_STARTING state is no longer used, so we
    remove it.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d2068bba9059..2f339ff9b851 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -306,9 +306,8 @@ struct kvmppc_vcore {
 /* Values for vcore_state */
 #define VCORE_INACTIVE	0
 #define VCORE_SLEEPING	1
-#define VCORE_STARTING	2
-#define VCORE_RUNNING	3
-#define VCORE_EXITING	4
+#define VCORE_RUNNING	2
+#define VCORE_EXITING	3
 
 /*
  * Struct used to manage memory for a virtual processor area

commit b6c295df3131c6fa25f8f29625ee0609506150ad
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:02 2015 +1100

    KVM: PPC: Book3S HV: Accumulate timing information for real-mode code
    
    This reads the timebase at various points in the real-mode guest
    entry/exit code and uses that to accumulate total, minimum and
    maximum time spent in those parts of the code.  Currently these
    times are accumulated per vcpu in 5 parts of the code:
    
    * rm_entry - time taken from the start of kvmppc_hv_entry() until
      just before entering the guest.
    * rm_intr - time from when we take a hypervisor interrupt in the
      guest until we either re-enter the guest or decide to exit to the
      host.  This includes time spent handling hcalls in real mode.
    * rm_exit - time from when we decide to exit the guest until the
      return from kvmppc_hv_entry().
    * guest - time spend in the guest
    * cede - time spent napping in real mode due to an H_CEDE hcall
      while other threads in the same vcore are active.
    
    These times are exposed in debugfs in a directory per vcpu that
    contains a file called "timings".  This file contains one line for
    each of the 5 timings above, with the name followed by a colon and
    4 numbers, which are the count (number of times the code has been
    executed), the total time, the minimum time, and the maximum time,
    all in nanoseconds.
    
    The overhead of the extra code amounts to about 30ns for an hcall that
    is handled in real mode (e.g. H_SET_DABR), which is about 25%.  Since
    production environments may not wish to incur this overhead, the new
    code is conditional on a new config symbol,
    CONFIG_KVM_BOOK3S_HV_EXIT_TIMING.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index f1d0bbc0f079..d2068bba9059 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -369,6 +369,14 @@ struct kvmppc_slb {
 	u8 base_page_size;	/* MMU_PAGE_xxx */
 };
 
+/* Struct used to accumulate timing information in HV real mode code */
+struct kvmhv_tb_accumulator {
+	u64	seqcount;	/* used to synchronize access, also count * 2 */
+	u64	tb_total;	/* total time in timebase ticks */
+	u64	tb_min;		/* min time */
+	u64	tb_max;		/* max time */
+};
+
 # ifdef CONFIG_PPC_FSL_BOOK3E
 #define KVMPPC_BOOKE_IAC_NUM	2
 #define KVMPPC_BOOKE_DAC_NUM	2
@@ -657,6 +665,19 @@ struct kvm_vcpu_arch {
 
 	u32 emul_inst;
 #endif
+
+#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING
+	struct kvmhv_tb_accumulator *cur_activity;	/* What we're timing */
+	u64	cur_tb_start;			/* when it started */
+	struct kvmhv_tb_accumulator rm_entry;	/* real-mode entry code */
+	struct kvmhv_tb_accumulator rm_intr;	/* real-mode intr handling */
+	struct kvmhv_tb_accumulator rm_exit;	/* real-mode exit code */
+	struct kvmhv_tb_accumulator guest_time;	/* guest execution */
+	struct kvmhv_tb_accumulator cede_time;	/* time napping inside guest */
+
+	struct dentry *debugfs_dir;
+	struct dentry *debugfs_timings;
+#endif /* CONFIG_KVM_BOOK3S_HV_EXIT_TIMING */
 };
 
 #define VCPU_FPR(vcpu, i)	(vcpu)->arch.fp.fpr[i][TS_FPROFFSET]

commit e23a808b1681d398a983ebc51179efc51c4a1eaf
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:01 2015 +1100

    KVM: PPC: Book3S HV: Create debugfs file for each guest's HPT
    
    This creates a debugfs directory for each HV guest (assuming debugfs
    is enabled in the kernel config), and within that directory, a file
    by which the contents of the guest's HPT (hashed page table) can be
    read.  The directory is named vmnnnn, where nnnn is the PID of the
    process that created the guest.  The file is named "htab".  This is
    intended to help in debugging problems in the host's management
    of guest memory.
    
    The contents of the file consist of a series of lines like this:
    
      3f48 4000d032bf003505 0000000bd7ff1196 00000003b5c71196
    
    The first field is the index of the entry in the HPT, the second and
    third are the HPT entry, so the third entry contains the real page
    number that is mapped by the entry if the entry's valid bit is set.
    The fourth field is the guest's view of the second doubleword of the
    entry, so it contains the guest physical address.  (The format of the
    second through fourth fields are described in the Power ISA and also
    in arch/powerpc/include/asm/mmu-hash64.h.)
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 015773f5bb33..f1d0bbc0f079 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -238,6 +238,8 @@ struct kvm_arch {
 	atomic_t hpte_mod_interest;
 	cpumask_t need_tlb_flush;
 	int hpt_cma_alloc;
+	struct dentry *debugfs_dir;
+	struct dentry *htab_dentry;
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 #ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
 	struct mutex hpt_mutex;

commit 31037ecad275e9ad9bc671c34f72b495cf708ca3
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Mar 20 20:39:42 2015 +1100

    KVM: PPC: Book3S HV: Remove RMA-related variables from code
    
    We don't support real-mode areas now that 970 support is removed.
    Remove the remaining details of rma from the code.  Also rename
    rma_setup_done to hpte_setup_done to better reflect the changes.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 8ef05121d3cd..015773f5bb33 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -228,9 +228,8 @@ struct kvm_arch {
 	int tlbie_lock;
 	unsigned long lpcr;
 	unsigned long rmor;
-	struct kvm_rma_info *rma;
 	unsigned long vrma_slb_v;
-	int rma_setup_done;
+	int hpte_setup_done;
 	u32 hpt_order;
 	atomic_t vcpus_running;
 	u32 online_vcores;

commit d078eed35de3866cb4af654db87765f53edbacce
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Feb 3 16:36:24 2015 +1100

    powerpc: Cleanup KVM emulated load/store endian handling
    
    Sometimes the KVM code on powerpc needs to emulate load or store
    instructions from the guest, which can include both normal and byte
    reversed forms.
    
    We currently (AFAICT) handle this correctly, but some variable names are
    very misleading.  In particular we use "is_bigendian" in several places to
    actually mean "is the IO the same endian as the host", but we now support
    little-endian powerpc hosts.  This also ties into the misleadingly named
    ld_le*() and st_le*() functions, which in fact always byteswap, even on
    an LE host.
    
    This patch cleans this up by renaming to more accurate "host_swabbed", and
    uses the generic swab*() functions instead of the powerpc specific and
    misleadingly named ld_le*() and st_le*() functions.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 8ef05121d3cd..c610961720c7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -585,7 +585,7 @@ struct kvm_vcpu_arch {
 	pgd_t *pgdir;
 
 	u8 io_gpr; /* GPR used as IO source/target */
-	u8 mmio_is_bigendian;
+	u8 mmio_host_swabbed;
 	u8 mmio_sign_extend;
 	u8 osi_needed;
 	u8 osi_enabled;

commit f7819512996361280b86259222456fcf15aad926
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Feb 4 18:20:58 2015 +0100

    kvm: add halt_poll_ns module parameter
    
    This patch introduces a new module parameter for the KVM module; when it
    is present, KVM attempts a bit of polling on every HLT before scheduling
    itself out via kvm_vcpu_block.
    
    This parameter helps a lot for latency-bound workloads---in particular
    I tested it with O_DSYNC writes with a battery-backed disk in the host.
    In this case, writes are fast (because the data doesn't have to go all
    the way to the platters) but they cannot be merged by either the host or
    the guest.  KVM's performance here is usually around 30% of bare metal,
    or 50% if you use cache=directsync or cache=writethrough (these
    parameters avoid that the guest sends pointless flush requests, and
    at the same time they are not slow because of the battery-backed cache).
    The bad performance happens because on every halt the host CPU decides
    to halt itself too.  When the interrupt comes, the vCPU thread is then
    migrated to a new physical CPU, and in general the latency is horrible
    because the vCPU thread has to be scheduled back in.
    
    With this patch performance reaches 60-65% of bare metal and, more
    important, 99% of what you get if you use idle=poll in the guest.  This
    means that the tunable gets rid of this particular bottleneck, and more
    work can be done to improve performance in the kernel or QEMU.
    
    Of course there is some price to pay; every time an otherwise idle vCPUs
    is interrupted by an interrupt, it will poll unnecessarily and thus
    impose a little load on the host.  The above results were obtained with
    a mostly random value of the parameter (500000), and the load was around
    1.5-2.5% CPU usage on one of the host's core for each idle guest vCPU.
    
    The patch also adds a new stat, /sys/kernel/debug/kvm/halt_successful_poll,
    that can be used to tune the parameter.  It counts how many HLT
    instructions received an interrupt during the polling period; each
    successful poll avoids that Linux schedules the VCPU thread out and back
    in, and may also avoid a likely trip to C1 and back for the physical CPU.
    
    While the VM is idle, a Linux 4 VCPU VM halts around 10 times per second.
    Of these halts, almost all are failed polls.  During the benchmark,
    instead, basically all halts end within the polling period, except a more
    or less constant stream of 50 per second coming from vCPUs that are not
    running the benchmark.  The wasted time is thus very low.  Things may
    be slightly different for Windows VMs, which have a ~10 ms timer tick.
    
    The effect is also visible on Marcelo's recently-introduced latency
    test for the TSC deadline timer.  Though of course a non-RT kernel has
    awful latency bounds, the latency of the timer is around 8000-10000 clock
    cycles compared to 20000-120000 without setting halt_poll_ns.  For the TSC
    deadline timer, thus, the effect is both a smaller average latency and
    a smaller variance.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 7efd666a3fa7..8ef05121d3cd 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -107,6 +107,7 @@ struct kvm_vcpu_stat {
 	u32 emulated_inst_exits;
 	u32 dec_exits;
 	u32 ext_intr_exits;
+	u32 halt_successful_poll;
 	u32 halt_wakeup;
 	u32 dbell_exits;
 	u32 gdbell_exits;

commit 90fd09f804213bcb9e092314c25b49d95153ad28
Author: Sam Bobroff <sam.bobroff@au1.ibm.com>
Date:   Wed Dec 3 13:30:40 2014 +1100

    KVM: PPC: Book3S HV: Improve H_CONFER implementation
    
    Currently the H_CONFER hcall is implemented in kernel virtual mode,
    meaning that whenever a guest thread does an H_CONFER, all the threads
    in that virtual core have to exit the guest.  This is bad for
    performance because it interrupts the other threads even if they
    are doing useful work.
    
    The H_CONFER hcall is called by a guest VCPU when it is spinning on a
    spinlock and it detects that the spinlock is held by a guest VCPU that
    is currently not running on a physical CPU.  The idea is to give this
    VCPU's time slice to the holder VCPU so that it can make progress
    towards releasing the lock.
    
    To avoid having the other threads exit the guest unnecessarily,
    we add a real-mode implementation of H_CONFER that checks whether
    the other threads are doing anything.  If all the other threads
    are idle (i.e. in H_CEDE) or trying to confer (i.e. in H_CONFER),
    it returns H_TOO_HARD which causes a guest exit and allows the
    H_CONFER to be handled in virtual mode.
    
    Otherwise it spins for a short time (up to 10 microseconds) to give
    other threads the chance to observe that this thread is trying to
    confer.  The spin loop also terminates when any thread exits the guest
    or when all other threads are idle or trying to confer.  If the
    timeout is reached, the H_CONFER returns H_SUCCESS.  In this case the
    guest VCPU will recheck the spinlock word and most likely call
    H_CONFER again.
    
    This also improves the implementation of the H_CONFER virtual mode
    handler.  If the VCPU is part of a virtual core (vcore) which is
    runnable, there will be a 'runner' VCPU which has taken responsibility
    for running the vcore.  In this case we yield to the runner VCPU
    rather than the target VCPU.
    
    We also introduce a check on the target VCPU's yield count: if it
    differs from the yield count passed to H_CONFER, the target VCPU
    has run since H_CONFER was called and may have already released
    the lock.  This check is required by PAPR.
    
    Signed-off-by: Sam Bobroff <sam.bobroff@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 65441875b025..7efd666a3fa7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -295,6 +295,7 @@ struct kvmppc_vcore {
 	ulong dpdes;		/* doorbell state (POWER8) */
 	void *mpp_buffer; /* Micro Partition Prefetch buffer */
 	bool mpp_buffer_is_valid;
+	ulong conferring_threads;
 };
 
 #define VCORE_ENTRY_COUNT(vc)	((vc)->entry_exit_count & 0xff)

commit 4a157d61b48c7cdb8d751001442a14ebac80229f
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Dec 3 13:30:39 2014 +1100

    KVM: PPC: Book3S HV: Fix endianness of instruction obtained from HEIR register
    
    There are two ways in which a guest instruction can be obtained from
    the guest in the guest exit code in book3s_hv_rmhandlers.S.  If the
    exit was caused by a Hypervisor Emulation interrupt (i.e. an illegal
    instruction), the offending instruction is in the HEIR register
    (Hypervisor Emulation Instruction Register).  If the exit was caused
    by a load or store to an emulated MMIO device, we load the instruction
    from the guest by turning data relocation on and loading the instruction
    with an lwz instruction.
    
    Unfortunately, in the case where the guest has opposite endianness to
    the host, these two methods give results of different endianness, but
    both get put into vcpu->arch.last_inst.  The HEIR value has been loaded
    using guest endianness, whereas the lwz will load the instruction using
    host endianness.  The rest of the code that uses vcpu->arch.last_inst
    assumes it was loaded using host endianness.
    
    To fix this, we define a new vcpu field to store the HEIR value.  Then,
    in kvmppc_handle_exit_hv(), we transfer the value from this new field to
    vcpu->arch.last_inst, doing a byte-swap if the guest and host endianness
    differ.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 5686a429d4b7..65441875b025 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -651,6 +651,8 @@ struct kvm_vcpu_arch {
 	spinlock_t tbacct_lock;
 	u64 busy_stolen;
 	u64 busy_preempt;
+
+	u32 emul_inst;
 #endif
 };
 

commit c17b98cf6028704e1f953d6a25ed6140425ccfd0
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Dec 3 13:30:38 2014 +1100

    KVM: PPC: Book3S HV: Remove code for PPC970 processors
    
    This removes the code that was added to enable HV KVM to work
    on PPC970 processors.  The PPC970 is an old CPU that doesn't
    support virtualizing guest memory.  Removing PPC970 support also
    lets us remove the code for allocating and managing contiguous
    real-mode areas, the code for the !kvm->arch.using_mmu_notifiers
    case, the code for pinning pages of guest memory when first
    accessed and keeping track of which pages have been pinned, and
    the code for handling H_ENTER hypercalls in virtual mode.
    
    Book3S HV KVM is now supported only on POWER7 and POWER8 processors.
    The KVM_CAP_PPC_RMA capability now always returns 0.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 7cf94a5e8411..5686a429d4b7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -180,11 +180,6 @@ struct kvmppc_spapr_tce_table {
 	struct page *pages[0];
 };
 
-struct kvm_rma_info {
-	atomic_t use_count;
-	unsigned long base_pfn;
-};
-
 /* XICS components, defined in book3s_xics.c */
 struct kvmppc_xics;
 struct kvmppc_icp;
@@ -214,16 +209,9 @@ struct revmap_entry {
 #define KVMPPC_RMAP_PRESENT	0x100000000ul
 #define KVMPPC_RMAP_INDEX	0xfffffffful
 
-/* Low-order bits in memslot->arch.slot_phys[] */
-#define KVMPPC_PAGE_ORDER_MASK	0x1f
-#define KVMPPC_PAGE_NO_CACHE	HPTE_R_I	/* 0x20 */
-#define KVMPPC_PAGE_WRITETHRU	HPTE_R_W	/* 0x40 */
-#define KVMPPC_GOT_PAGE		0x80
-
 struct kvm_arch_memory_slot {
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	unsigned long *rmap;
-	unsigned long *slot_phys;
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 };
 
@@ -242,14 +230,12 @@ struct kvm_arch {
 	struct kvm_rma_info *rma;
 	unsigned long vrma_slb_v;
 	int rma_setup_done;
-	int using_mmu_notifiers;
 	u32 hpt_order;
 	atomic_t vcpus_running;
 	u32 online_vcores;
 	unsigned long hpt_npte;
 	unsigned long hpt_mask;
 	atomic_t hpte_mod_interest;
-	spinlock_t slot_phys_lock;
 	cpumask_t need_tlb_flush;
 	int hpt_cma_alloc;
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */

commit 2711e248a352d7ecc8767b1dfa1f0c2356cb7f4b
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Dec 4 16:43:28 2014 +1100

    KVM: PPC: Book3S HV: Simplify locking around stolen time calculations
    
    Currently the calculations of stolen time for PPC Book3S HV guests
    uses fields in both the vcpu struct and the kvmppc_vcore struct.  The
    fields in the kvmppc_vcore struct are protected by the
    vcpu->arch.tbacct_lock of the vcpu that has taken responsibility for
    running the virtual core.  This works correctly but confuses lockdep,
    because it sees that the code takes the tbacct_lock for a vcpu in
    kvmppc_remove_runnable() and then takes another vcpu's tbacct_lock in
    vcore_stolen_time(), and it thinks there is a possibility of deadlock,
    causing it to print reports like this:
    
    =============================================
    [ INFO: possible recursive locking detected ]
    3.18.0-rc7-kvm-00016-g8db4bc6 #89 Not tainted
    ---------------------------------------------
    qemu-system-ppc/6188 is trying to acquire lock:
     (&(&vcpu->arch.tbacct_lock)->rlock){......}, at: [<d00000000ecb1fe8>] .vcore_stolen_time+0x48/0xd0 [kvm_hv]
    
    but task is already holding lock:
     (&(&vcpu->arch.tbacct_lock)->rlock){......}, at: [<d00000000ecb25a0>] .kvmppc_remove_runnable.part.3+0x30/0xd0 [kvm_hv]
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(&(&vcpu->arch.tbacct_lock)->rlock);
      lock(&(&vcpu->arch.tbacct_lock)->rlock);
    
     *** DEADLOCK ***
    
     May be due to missing lock nesting notation
    
    3 locks held by qemu-system-ppc/6188:
     #0:  (&vcpu->mutex){+.+.+.}, at: [<d00000000eb93f98>] .vcpu_load+0x28/0xe0 [kvm]
     #1:  (&(&vcore->lock)->rlock){+.+...}, at: [<d00000000ecb41b0>] .kvmppc_vcpu_run_hv+0x530/0x1530 [kvm_hv]
     #2:  (&(&vcpu->arch.tbacct_lock)->rlock){......}, at: [<d00000000ecb25a0>] .kvmppc_remove_runnable.part.3+0x30/0xd0 [kvm_hv]
    
    stack backtrace:
    CPU: 40 PID: 6188 Comm: qemu-system-ppc Not tainted 3.18.0-rc7-kvm-00016-g8db4bc6 #89
    Call Trace:
    [c000000b2754f3f0] [c000000000b31b6c] .dump_stack+0x88/0xb4 (unreliable)
    [c000000b2754f470] [c0000000000faeb8] .__lock_acquire+0x1878/0x2190
    [c000000b2754f600] [c0000000000fbf0c] .lock_acquire+0xcc/0x1a0
    [c000000b2754f6d0] [c000000000b2954c] ._raw_spin_lock_irq+0x4c/0x70
    [c000000b2754f760] [d00000000ecb1fe8] .vcore_stolen_time+0x48/0xd0 [kvm_hv]
    [c000000b2754f7f0] [d00000000ecb25b4] .kvmppc_remove_runnable.part.3+0x44/0xd0 [kvm_hv]
    [c000000b2754f880] [d00000000ecb43ec] .kvmppc_vcpu_run_hv+0x76c/0x1530 [kvm_hv]
    [c000000b2754f9f0] [d00000000eb9f46c] .kvmppc_vcpu_run+0x2c/0x40 [kvm]
    [c000000b2754fa60] [d00000000eb9c9a4] .kvm_arch_vcpu_ioctl_run+0x54/0x160 [kvm]
    [c000000b2754faf0] [d00000000eb94538] .kvm_vcpu_ioctl+0x498/0x760 [kvm]
    [c000000b2754fcb0] [c000000000267eb4] .do_vfs_ioctl+0x444/0x770
    [c000000b2754fd90] [c0000000002682a4] .SyS_ioctl+0xc4/0xe0
    [c000000b2754fe30] [c0000000000092e4] syscall_exit+0x0/0x98
    
    In order to make the locking easier to analyse, we change the code to
    use a spinlock in the kvmppc_vcore struct to protect the stolen_tb and
    preempt_tb fields.  This lock needs to be an irq-safe lock since it is
    used in the kvmppc_core_vcpu_load_hv() and kvmppc_core_vcpu_put_hv()
    functions, which are called with the scheduler rq lock held, which is
    an irq-safe lock.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 047855619cc4..7cf94a5e8411 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -297,6 +297,7 @@ struct kvmppc_vcore {
 	struct list_head runnable_threads;
 	spinlock_t lock;
 	wait_queue_head_t wq;
+	spinlock_t stoltb_lock;	/* protects stolen_tb and preempt_tb */
 	u64 stolen_tb;
 	u64 preempt_tb;
 	struct kvm_vcpu *runner;

commit 00c027db0cc4b7387b258330482c6e5f5e836b18
Merge: c24ae0dcd3e8 8d0eff638564
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Sep 24 23:19:45 2014 +0200

    Merge tag 'signed-kvm-ppc-next' of git://github.com/agraf/linux-2.6 into kvm-next
    
    Patch queue for ppc - 2014-09-24
    
    New awesome things in this release:
    
      - E500: e6500 core support
      - E500: guest and remote debug support
      - Book3S: remote sw breakpoint support
      - Book3S: HV: Minor bugfixes
    
    Alexander Graf (1):
          KVM: PPC: Pass enum to kvmppc_get_last_inst
    
    Bharat Bhushan (8):
          KVM: PPC: BOOKE: allow debug interrupt at "debug level"
          KVM: PPC: BOOKE : Emulate rfdi instruction
          KVM: PPC: BOOKE: Allow guest to change MSR_DE
          KVM: PPC: BOOKE: Clear guest dbsr in userspace exit KVM_EXIT_DEBUG
          KVM: PPC: BOOKE: Guest and hardware visible debug registers are same
          KVM: PPC: BOOKE: Add one reg interface for DBSR
          KVM: PPC: BOOKE: Add one_reg documentation of SPRG9 and DBSR
          KVM: PPC: BOOKE: Emulate debug registers and exception
    
    Madhavan Srinivasan (2):
          powerpc/kvm: support to handle sw breakpoint
          powerpc/kvm: common sw breakpoint instr across ppc
    
    Michael Neuling (1):
          KVM: PPC: Book3S HV: Add register name when loading toc
    
    Mihai Caraman (10):
          powerpc/booke: Restrict SPE exception handlers to e200/e500 cores
          powerpc/booke: Revert SPE/AltiVec common defines for interrupt numbers
          KVM: PPC: Book3E: Increase FPU laziness
          KVM: PPC: Book3e: Add AltiVec support
          KVM: PPC: Make ONE_REG powerpc generic
          KVM: PPC: Move ONE_REG AltiVec support to powerpc
          KVM: PPC: Remove the tasklet used by the hrtimer
          KVM: PPC: Remove shared defines for SPE and AltiVec interrupts
          KVM: PPC: e500mc: Add support for single threaded vcpus on e6500 core
          KVM: PPC: Book3E: Enable e6500 core
    
    Paul Mackerras (2):
          KVM: PPC: Book3S HV: Increase timeout for grabbing secondary threads
          KVM: PPC: Book3S HV: Only accept host PVR value for guest PVR

commit fe71557afbec641fee73711e40602bed37f6f33b
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Wed Sep 24 15:57:57 2014 +0800

    kvm: Add arch specific mmu notifier for page invalidation
    
    This will be used to let the guest run while the APIC access page is
    not pinned.  Because subsequent patches will fill in the function
    for x86, place the (still empty) x86 implementation in the x86.c file
    instead of adding an inline function in kvm_host.h.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d329bc5543a2..2cf6c1587d43 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -60,6 +60,11 @@ extern int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
 extern int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 extern void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 
+static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
+							 unsigned long address)
+{
+}
+
 #define HPTEG_CACHE_NUM			(1 << 15)
 #define HPTEG_HASH_BITS_PTE		13
 #define HPTEG_HASH_BITS_PTE_LONG	12

commit 57128468080a8b6ea452223036d3e417f748af55
Author: Andres Lagar-Cavilla <andreslc@google.com>
Date:   Mon Sep 22 14:54:42 2014 -0700

    kvm: Fix page ageing bugs
    
    1. We were calling clear_flush_young_notify in unmap_one, but we are
    within an mmu notifier invalidate range scope. The spte exists no more
    (due to range_start) and the accessed bit info has already been
    propagated (due to kvm_pfn_set_accessed). Simply call
    clear_flush_young.
    
    2. We clear_flush_young on a primary MMU PMD, but this may be mapped
    as a collection of PTEs by the secondary MMU (e.g. during log-dirty).
    This required expanding the interface of the clear_flush_young mmu
    notifier, so a lot of code has been trivially touched.
    
    3. In the absence of shadow_accessed_mask (e.g. EPT A bit), we emulate
    the access bit by blowing the spte. This requires proper synchronizing
    with MMU notifier consumers, like every other removal of spte's does.
    
    Signed-off-by: Andres Lagar-Cavilla <andreslc@google.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 604000882352..d329bc5543a2 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -56,7 +56,7 @@
 extern int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 extern int kvm_unmap_hva_range(struct kvm *kvm,
 			       unsigned long start, unsigned long end);
-extern int kvm_age_hva(struct kvm *kvm, unsigned long hva);
+extern int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
 extern int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 extern void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 

commit d02d4d156e72baf9a6628c76eb53019124d3c82f
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Mon Sep 1 17:19:56 2014 +0300

    KVM: PPC: Remove the tasklet used by the hrtimer
    
    Powerpc timer implementation is a copycat version of s390. Now that they removed
    the tasklet with commit ea74c0ea1b24a6978a6ebc80ba4dbc7b7848b32d follow this
    optimization.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Bogdan Purcareata <bogdan.purcareata@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index bad3491cc32c..d2432401d301 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -606,7 +606,6 @@ struct kvm_vcpu_arch {
 	u32 cpr0_cfgaddr; /* holds the last set cpr0_cfgaddr */
 
 	struct hrtimer dec_timer;
-	struct tasklet_struct tasklet;
 	u64 dec_jiffies;
 	u64 dec_expires;
 	unsigned long pending_exceptions;

commit 348ba71081cd8444178d24d3ed13d34fc1b61dae
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Wed Aug 6 12:08:55 2014 +0530

    KVM: PPC: BOOKE: Guest and hardware visible debug registers are same
    
    Guest visible debug register and hardware visible debug registers are
    same, so ther is no need to have arch->shadow_dbg_reg, instead use
    arch->dbg_reg.
    
    Signed-off-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 8a8da0acfd28..bad3491cc32c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -585,8 +585,6 @@ struct kvm_vcpu_arch {
 	u32 crit_save;
 	/* guest debug registers*/
 	struct debug_reg dbg_reg;
-	/* hardware visible debug registers when in guest state */
-	struct debug_reg shadow_dbg_reg;
 #endif
 	gpa_t paddr_accessed;
 	gva_t vaddr_accessed;

commit c8ca97ca9b87c0a9c9e67feda656b8dbca65cf08
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Wed Aug 6 12:08:52 2014 +0530

    KVM: PPC: BOOKE : Emulate rfdi instruction
    
    This patch adds "rfdi" instruction emulation which is required for
    guest debug hander on BOOKE-HV
    
    Signed-off-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 604000882352..8a8da0acfd28 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -139,6 +139,7 @@ enum kvm_exit_types {
 	EMULATED_TLBWE_EXITS,
 	EMULATED_RFI_EXITS,
 	EMULATED_RFCI_EXITS,
+	EMULATED_RFDI_EXITS,
 	DEC_EXITS,
 	EXT_INTR_EXITS,
 	HALT_WAKEUP,

commit 13a34e067eab24fec882e1834fbf2cc31911d474
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Aug 28 15:13:03 2014 +0200

    KVM: remove garbage arg to *hardware_{en,dis}able
    
    In the beggining was on_each_cpu(), which required an unused argument to
    kvm_arch_ops.hardware_{en,dis}able, but this was soon forgotten.
    
    Remove unnecessary arguments that stem from this.
    
    Signed-off-by: Radim KrÄmÃ¡Å™ <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 237cc0cc80a2..604000882352 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -682,7 +682,7 @@ struct kvm_vcpu_arch {
 #define __KVM_HAVE_ARCH_WQP
 #define __KVM_HAVE_CREATE_DEVICE
 
-static inline void kvm_arch_hardware_disable(void *garbage) {}
+static inline void kvm_arch_hardware_disable(void) {}
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
 static inline void kvm_arch_memslots_updated(struct kvm *kvm) {}

commit 0865e636aef751966e6e0f8950a26bc7391e923c
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Aug 28 15:13:02 2014 +0200

    KVM: static inline empty kvm_arch functions
    
    Using static inline is going to save few bytes and cycles.
    For example on powerpc, the difference is 700 B after stripping.
    (5 kB before)
    
    This patch also deals with two overlooked empty functions:
    kvm_arch_flush_shadow was not removed from arch/mips/kvm/mips.c
      2df72e9bc KVM: split kvm_arch_flush_shadow
    and kvm_arch_sched_in never made it into arch/ia64/kvm/kvm-ia64.c.
      e790d9ef6 KVM: add kvm_arch_sched_in
    
    Signed-off-by: Radim KrÄmÃ¡Å™ <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0e597283c5c6..237cc0cc80a2 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -682,4 +682,12 @@ struct kvm_vcpu_arch {
 #define __KVM_HAVE_ARCH_WQP
 #define __KVM_HAVE_CREATE_DEVICE
 
+static inline void kvm_arch_hardware_disable(void *garbage) {}
+static inline void kvm_arch_hardware_unsetup(void) {}
+static inline void kvm_arch_sync_events(struct kvm *kvm) {}
+static inline void kvm_arch_memslots_updated(struct kvm *kvm) {}
+static inline void kvm_arch_flush_shadow_all(struct kvm *kvm) {}
+static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}
+static inline void kvm_arch_exit(void) {}
+
 #endif /* __POWERPC_KVM_HOST_H__ */

commit 656473003bc7e056c3bbd4a4d9832dad01e86f76
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Aug 29 14:01:17 2014 +0200

    KVM: forward declare structs in kvm_types.h
    
    Opaque KVM structs are useful for prototypes in asm/kvm_host.h, to avoid
    "'struct foo' declared inside parameter list" warnings (and consequent
    breakage due to conflicting types).
    
    Move them from individual files to a generic place in linux/kvm_types.h.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 98d9dd50d063..0e597283c5c6 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -53,7 +53,6 @@
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 
-struct kvm;
 extern int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 extern int kvm_unmap_hva_range(struct kvm *kvm,
 			       unsigned long start, unsigned long end);
@@ -76,10 +75,6 @@ extern void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 /* Physical Address Mask - allowed range of real mode RAM access */
 #define KVM_PAM			0x0fffffffffffffffULL
 
-struct kvm;
-struct kvm_run;
-struct kvm_vcpu;
-
 struct lppaca;
 struct slb_shadow;
 struct dtl_entry;

commit ce91ddc471b77ec75e5b2a43c803efac605f37b3
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jul 28 19:29:13 2014 +0200

    KVM: PPC: Remove DCR handling
    
    DCR handling was only needed for 440 KVM. Since we removed it, we can also
    remove handling of DCR accesses.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 66f5b5914e6c..98d9dd50d063 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -94,7 +94,6 @@ struct kvm_vm_stat {
 struct kvm_vcpu_stat {
 	u32 sum_exits;
 	u32 mmio_exits;
-	u32 dcr_exits;
 	u32 signal_exits;
 	u32 light_exits;
 	/* Account for special types of light exits: */
@@ -126,7 +125,6 @@ struct kvm_vcpu_stat {
 
 enum kvm_exit_types {
 	MMIO_EXITS,
-	DCR_EXITS,
 	SIGNAL_EXITS,
 	ITLB_REAL_MISS_EXITS,
 	ITLB_VIRT_MISS_EXITS,
@@ -601,8 +599,6 @@ struct kvm_vcpu_arch {
 	u8 io_gpr; /* GPR used as IO source/target */
 	u8 mmio_is_bigendian;
 	u8 mmio_sign_extend;
-	u8 dcr_needed;
-	u8 dcr_is_write;
 	u8 osi_needed;
 	u8 osi_enabled;
 	u8 papr_enabled;

commit 35c4a7330dbe1ae6f590a5645b185e35ddb3f6d9
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jun 20 13:58:16 2014 +0200

    KVM: PPC: Move kvmppc_ld/st to common code
    
    We have enough common infrastructure now to resolve GVA->GPA mappings at
    runtime. With this we can move our book3s specific helpers to load / store
    in guest virtual address space to common code as well.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 11385bb46527..66f5b5914e6c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -111,15 +111,15 @@ struct kvm_vcpu_stat {
 	u32 halt_wakeup;
 	u32 dbell_exits;
 	u32 gdbell_exits;
+	u32 ld;
+	u32 st;
 #ifdef CONFIG_PPC_BOOK3S
 	u32 pf_storage;
 	u32 pf_instruc;
 	u32 sp_storage;
 	u32 sp_instruc;
 	u32 queue_intr;
-	u32 ld;
 	u32 ld_slow;
-	u32 st;
 	u32 st_slow;
 #endif
 };

commit 9678cdaae93932473f696fdea5debf3eee1e1260
Author: Stewart Smith <stewart@linux.vnet.ibm.com>
Date:   Fri Jul 18 14:18:43 2014 +1000

    Use the POWER8 Micro Partition Prefetch Engine in KVM HV on POWER8
    
    The POWER8 processor has a Micro Partition Prefetch Engine, which is
    a fancy way of saying "has way to store and load contents of L2 or
    L2+MRU way of L3 cache". We initiate the storing of the log (list of
    addresses) using the logmpp instruction and start restore by writing
    to a SPR.
    
    The logmpp instruction takes parameters in a single 64bit register:
    - starting address of the table to store log of L2/L2+L3 cache contents
      - 32kb for L2
      - 128kb for L2+L3
      - Aligned relative to maximum size of the table (32kb or 128kb)
    - Log control (no-op, L2 only, L2 and L3, abort logout)
    
    We should abort any ongoing logging before initiating one.
    
    To initiate restore, we write to the MPPR SPR. The format of what to write
    to the SPR is similar to the logmpp instruction parameter:
    - starting address of the table to read from (same alignment requirements)
    - table size (no data, until end of table)
    - prefetch rate (from fastest possible to slower. about every 8, 16, 24 or
      32 cycles)
    
    The idea behind loading and storing the contents of L2/L3 cache is to
    reduce memory latency in a system that is frequently swapping vcores on
    a physical CPU.
    
    The best case scenario for doing this is when some vcores are doing very
    cache heavy workloads. The worst case is when they have about 0 cache hits,
    so we just generate needless memory operations.
    
    This implementation just does L2 store/load. In my benchmarks this proves
    to be useful.
    
    Benchmark 1:
     - 16 core POWER8
     - 3x Ubuntu 14.04LTS guests (LE) with 8 VCPUs each
     - No split core/SMT
     - two guests running sysbench memory test.
       sysbench --test=memory --num-threads=8 run
     - one guest running apache bench (of default HTML page)
       ab -n 490000 -c 400 http://localhost/
    
    This benchmark aims to measure performance of real world application (apache)
    where other guests are cache hot with their own workloads. The sysbench memory
    benchmark does pointer sized writes to a (small) memory buffer in a loop.
    
    In this benchmark with this patch I can see an improvement both in requests
    per second (~5%) and in mean and median response times (again, about 5%).
    The spread of minimum and maximum response times were largely unchanged.
    
    benchmark 2:
     - Same VM config as benchmark 1
     - all three guests running sysbench memory benchmark
    
    This benchmark aims to see if there is a positive or negative affect to this
    cache heavy benchmark. Although due to the nature of the benchmark (stores) we
    may not see a difference in performance, but rather hopefully an improvement
    in consistency of performance (when vcore switched in, don't have to wait
    many times for cachelines to be pulled in)
    
    The results of this benchmark are improvements in consistency of performance
    rather than performance itself. With this patch, the few outliers in duration
    go away and we get more consistent performance in each guest.
    
    benchmark 3:
     - same 3 guests and CPU configuration as benchmark 1 and 2.
     - two idle guests
     - 1 guest running STREAM benchmark
    
    This scenario also saw performance improvement with this patch. On Copy and
    Scale workloads from STREAM, I got 5-6% improvement with this patch. For
    Add and triad, it was around 10% (or more).
    
    benchmark 4:
     - same 3 guests as previous benchmarks
     - two guests running sysbench --memory, distinctly different cache heavy
       workload
     - one guest running STREAM benchmark.
    
    Similar improvements to benchmark 3.
    
    benchmark 5:
     - 1 guest, 8 VCPUs, Ubuntu 14.04
     - Host configured with split core (SMT8, subcores-per-core=4)
     - STREAM benchmark
    
    In this benchmark, we see a 10-20% performance improvement across the board
    of STREAM benchmark results with this patch.
    
    Based on preliminary investigation and microbenchmarks
    by Prerna Saxena <prerna@linux.vnet.ibm.com>
    
    Signed-off-by: Stewart Smith <stewart@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 5fe2b5d17bc0..11385bb46527 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -307,6 +307,8 @@ struct kvmppc_vcore {
 	u32 arch_compat;
 	ulong pcr;
 	ulong dpdes;		/* doorbell state (POWER8) */
+	void *mpp_buffer; /* Micro Partition Prefetch buffer */
+	bool mpp_buffer_is_valid;
 };
 
 #define VCORE_ENTRY_COUNT(vc)	((vc)->entry_exit_count & 0xff)

commit b2677b8dd8de0dc1496ede4da09b9dfd59f15cea
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jul 25 10:38:59 2014 +0200

    KVM: PPC: Remove 440 support
    
    The 440 target hasn't been properly functioning for a few releases and
    before I was the only one who fixes a very serious bug that indicates to
    me that nobody used it before either.
    
    Furthermore KVM on 440 is slow to the extent of unusable.
    
    We don't have to carry along completely unused code. Remove 440 and give
    us one less thing to worry about.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 562f685d1678..5fe2b5d17bc0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -49,7 +49,6 @@
 #define KVM_NR_IRQCHIPS          1
 #define KVM_IRQCHIP_NUM_PINS     256
 
-#if !defined(CONFIG_KVM_440)
 #include <linux/mmu_notifier.h>
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER
@@ -62,8 +61,6 @@ extern int kvm_age_hva(struct kvm *kvm, unsigned long hva);
 extern int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 extern void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 
-#endif
-
 #define HPTEG_CACHE_NUM			(1 << 15)
 #define HPTEG_HASH_BITS_PTE		13
 #define HPTEG_HASH_BITS_PTE_LONG	12

commit 99e99d19a86dc596703ed79dcecf9ca6b32a6a8a
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Mon Jul 21 11:23:26 2014 +0530

    kvm: ppc: bookehv: Save restore SPRN_SPRG9 on guest entry exit
    
    SPRN_SPRG is used by debug interrupt handler, so this is required for
    debug support.
    
    Signed-off-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 855ba4d9539d..562f685d1678 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -587,6 +587,7 @@ struct kvm_vcpu_arch {
 	u32 mmucfg;
 	u32 eptcfg;
 	u32 epr;
+	u64 sprg9;
 	u32 pwrmgtcr0;
 	u32 crit_save;
 	/* guest debug registers*/

commit 1287cb3fa85cd4a0d18402f6a23e1d4c6a9d7b8b
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jul 4 12:52:51 2014 +0200

    KVM: PPC: Book3S: Move vcore definition to end of kvm_arch struct
    
    When building KVM with a lot of vcores (NR_CPUS is big), we can potentially
    get out of the ld immediate range for dereferences inside that struct.
    
    Move the array to the end of our kvm_arch struct. This fixes compilation
    issues with NR_CPUS=2048 for me.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index faf2f0e56bb8..855ba4d9539d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -255,7 +255,6 @@ struct kvm_arch {
 	atomic_t hpte_mod_interest;
 	spinlock_t slot_phys_lock;
 	cpumask_t need_tlb_flush;
-	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
 	int hpt_cma_alloc;
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 #ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
@@ -273,6 +272,10 @@ struct kvm_arch {
 	struct kvmppc_xics *xics;
 #endif
 	struct kvmppc_ops *kvm_ops;
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	/* This array can grow quite large, keep it at the end */
+	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
+#endif
 };
 
 /*

commit debf27d6b92d7a98e0153ca8e3a990ea7a45b4da
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Fri Jul 4 11:17:28 2014 +0300

    KVM: PPC: e500: Emulate power management control SPR
    
    For FSL e6500 core the kernel uses power management SPR register (PWRMGTCR0)
    to enable idle power down for cores and devices by setting up the idle count
    period at boot time. With the host already controlling the power management
    configuration the guest could simply benefit from it, so emulate guest request
    as a general store.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 62b2cee450a5..faf2f0e56bb8 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -584,6 +584,7 @@ struct kvm_vcpu_arch {
 	u32 mmucfg;
 	u32 eptcfg;
 	u32 epr;
+	u32 pwrmgtcr0;
 	u32 crit_save;
 	/* guest debug registers*/
 	struct debug_reg dbg_reg;

commit 699a0ea0823d32030b0666b28ff8633960f7ffa7
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jun 2 11:02:59 2014 +1000

    KVM: PPC: Book3S: Controls for in-kernel sPAPR hypercall handling
    
    This provides a way for userspace controls which sPAPR hcalls get
    handled in the kernel.  Each hcall can be individually enabled or
    disabled for in-kernel handling, except for H_RTAS.  The exception
    for H_RTAS is because userspace can already control whether
    individual RTAS functions are handled in-kernel or not via the
    KVM_PPC_RTAS_DEFINE_TOKEN ioctl, and because the numeric value for
    H_RTAS is out of the normal sequence of hcall numbers.
    
    Hcalls are enabled or disabled using the KVM_ENABLE_CAP ioctl for the
    KVM_CAP_PPC_ENABLE_HCALL capability on the file descriptor for the VM.
    The args field of the struct kvm_enable_cap specifies the hcall number
    in args[0] and the enable/disable flag in args[1]; 0 means disable
    in-kernel handling (so that the hcall will always cause an exit to
    userspace) and 1 means enable.  Enabling or disabling in-kernel
    handling of an hcall is effective across the whole VM.
    
    The ability for KVM_ENABLE_CAP to be used on a VM file descriptor
    on PowerPC is new, added by this commit.  The KVM_CAP_ENABLE_CAP_VM
    capability advertises that this ability exists.
    
    When a VM is created, an initial set of hcalls are enabled for
    in-kernel handling.  The set that is enabled is the set that have
    an in-kernel implementation at this point.  Any new hcall
    implementations from this point onwards should not be added to the
    default set without a good reason.
    
    No distinction is made between real-mode and virtual-mode hcall
    implementations; the one setting controls them both.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index f9ae69682ce1..62b2cee450a5 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -34,6 +34,7 @@
 #include <asm/processor.h>
 #include <asm/page.h>
 #include <asm/cacheflush.h>
+#include <asm/hvcall.h>
 
 #define KVM_MAX_VCPUS		NR_CPUS
 #define KVM_MAX_VCORES		NR_CPUS
@@ -263,6 +264,7 @@ struct kvm_arch {
 #ifdef CONFIG_PPC_BOOK3S_64
 	struct list_head spapr_tce_tables;
 	struct list_head rtas_tokens;
+	DECLARE_BITMAP(enabled_hcalls, MAX_HCALL_OPCODE/4 + 1);
 #endif
 #ifdef CONFIG_KVM_MPIC
 	struct openpic *mpic;

commit 06da28e76b87331ebccdb6d486cfd94835b8be5e
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Jun 5 17:38:05 2014 +0530

    KVM: PPC: BOOK3S: PR: Emulate instruction counter
    
    Writing to IC is not allowed in the privileged mode.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index bd3caeaeebe1..f9ae69682ce1 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -506,6 +506,7 @@ struct kvm_vcpu_arch {
 	/* Time base value when we entered the guest */
 	u64 entry_tb;
 	u64 entry_vtb;
+	u64 entry_ic;
 	u32 tcr;
 	ulong tsr; /* we need to perform set/clr_bits() which requires ulong */
 	u32 ivor[64];

commit 8f42ab2749d00ea15157ab896cfbed73a247b3e1
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Jun 5 17:38:02 2014 +0530

    KVM: PPC: BOOK3S: PR: Emulate virtual timebase register
    
    virtual time base register is a per VM, per cpu register that needs
    to be saved and restored on vm exit and entry. Writing to VTB is not
    allowed in the privileged mode.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [agraf: fix compile error]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 4a58731a0a72..bd3caeaeebe1 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -505,6 +505,7 @@ struct kvm_vcpu_arch {
 #endif
 	/* Time base value when we entered the guest */
 	u64 entry_tb;
+	u64 entry_vtb;
 	u32 tcr;
 	ulong tsr; /* we need to perform set/clr_bits() which requires ulong */
 	u32 ivor[64];

commit 3cd60e31185343d4132ca7cf3c9becb903b3ec1b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Jun 4 16:47:55 2014 +0530

    KVM: PPC: BOOK3S: PR: Fix PURR and SPURR emulation
    
    We use time base for PURR and SPURR emulation with PR KVM since we
    are emulating a single threaded core. When using time base
    we need to make sure that we don't accumulate time spent in the host
    in PURR and SPURR value.
    
    Also we don't need to emulate mtspr because both the registers are
    hypervisor resource.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index bb66d8b8efdf..4a58731a0a72 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -503,8 +503,8 @@ struct kvm_vcpu_arch {
 #ifdef CONFIG_BOOKE
 	u32 decar;
 #endif
-	u32 tbl;
-	u32 tbu;
+	/* Time base value when we entered the guest */
+	u64 entry_tb;
 	u32 tcr;
 	ulong tsr; /* we need to perform set/clr_bits() which requires ulong */
 	u32 ivor[64];

commit f3383cf80e417e86fcc84a2eb4c96bc52842d8d9
Author: Alexander Graf <agraf@suse.de>
Date:   Mon May 12 01:08:32 2014 +0200

    KVM: PPC: Disable NX for old magic page using guests
    
    Old guests try to use the magic page, but map their trampoline code inside
    of an NX region.
    
    Since we can't fix those old kernels, try to detect whether the guest is sane
    or not. If not, just disable NX functionality in KVM so that old guests at
    least work at all. For newer guests, add a bit that we can set to keep NX
    functionality available.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 29fbb554af5c..bb66d8b8efdf 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -631,6 +631,7 @@ struct kvm_vcpu_arch {
 #endif
 	unsigned long magic_page_pa; /* phys addr to map the magic page to */
 	unsigned long magic_page_ea; /* effect. addr to map the magic page to */
+	bool disable_kernel_nx;
 
 	int irq_type;		/* one of KVM_IRQ_* */
 	int irq_cpu_id;

commit e14e7a1e537d6e18f9c511f25c25c5efb7799fb5
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Apr 22 12:26:58 2014 +0200

    KVM: PPC: Book3S PR: Expose TAR facility to guest
    
    POWER8 implements a new register called TAR. This register has to be
    enabled in FSCR and then from KVM's point of view is mere storage.
    
    This patch enables the guest to use TAR.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 232ec5f0b886..29fbb554af5c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -449,7 +449,9 @@ struct kvm_vcpu_arch {
 	ulong pc;
 	ulong ctr;
 	ulong lr;
+#ifdef CONFIG_PPC_BOOK3S
 	ulong tar;
+#endif
 
 	ulong xer;
 	u32 cr;

commit 616dff86028298dbc91174fb3d12b8ed8cd74955
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Apr 29 16:48:44 2014 +0200

    KVM: PPC: Book3S PR: Handle Facility interrupt and FSCR
    
    POWER8 introduced a new interrupt type called "Facility unavailable interrupt"
    which contains its status message in a new register called FSCR.
    
    Handle these exits and try to emulate instructions for unhandled facilities.
    Follow-on patches enable KVM to expose specific facilities into the guest.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 15f19d3cc584..232ec5f0b886 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -475,6 +475,7 @@ struct kvm_vcpu_arch {
 	ulong ppr;
 	ulong pspb;
 	ulong fscr;
+	ulong shadow_fscr;
 	ulong ebbhr;
 	ulong ebbrr;
 	ulong bescr;

commit 5deb8e7ad8ac7e3fcdfa042acff617f461b361c2
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Apr 24 13:46:24 2014 +0200

    KVM: PPC: Make shared struct aka magic page guest endian
    
    The shared (magic) page is a data structure that contains often used
    supervisor privileged SPRs accessible via memory to the user to reduce
    the number of exits we have to take to read/write them.
    
    When we actually share this structure with the guest we have to maintain
    it in guest endianness, because some of the patch tricks only work with
    native endian load/store operations.
    
    Since we only share the structure with either host or guest in little
    endian on book3s_64 pr mode, we don't have to worry about booke or book3s hv.
    
    For booke, the shared struct stays big endian. For book3s_64 hv we maintain
    the struct in host native endian, since it never gets shared with the guest.
    
    For book3s_64 pr we introduce a variable that tells us which endianness the
    shared struct is in and route every access to it through helper inline
    functions that evaluate this variable.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d342f8efc843..15f19d3cc584 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -623,6 +623,9 @@ struct kvm_vcpu_arch {
 	wait_queue_head_t cpu_run;
 
 	struct kvm_vcpu_arch_shared *shared;
+#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_KVM_BOOK3S_PR_POSSIBLE)
+	bool shared_big_endian;
+#endif
 	unsigned long magic_page_pa; /* phys addr to map the magic page to */
 	unsigned long magic_page_ea; /* effect. addr to map the magic page to */
 

commit e5ee5422f8867d8b8108f8e1f0f47dc59b043f5b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon May 5 08:39:44 2014 +0530

    KVM: PPC: BOOK3S: PR: Enable Little Endian PR guest
    
    This patch make sure we inherit the LE bit correctly in different case
    so that we can run Little Endian distro in PR mode
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1eaea2dea174..d342f8efc843 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -562,6 +562,7 @@ struct kvm_vcpu_arch {
 #ifdef CONFIG_PPC_BOOK3S
 	ulong fault_dar;
 	u32 fault_dsisr;
+	unsigned long intr_msr;
 #endif
 
 #ifdef CONFIG_BOOKE
@@ -654,7 +655,6 @@ struct kvm_vcpu_arch {
 	spinlock_t tbacct_lock;
 	u64 busy_stolen;
 	u64 busy_preempt;
-	unsigned long intr_msr;
 #endif
 };
 

commit 7b490411c37f7ab7965cbdfe5e3ec28eadb6db5b
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Jan 8 21:25:32 2014 +1100

    KVM: PPC: Book3S HV: Add new state for transactional memory
    
    Add new state for transactional memory (TM) to kvm_vcpu_arch.  Also add
    asm-offset bits that are going to be required.
    
    This also moves the existing TFHAR, TFIAR and TEXASR SPRs into a
    CONFIG_PPC_TRANSACTIONAL_MEM section.  This requires some code changes to
    ensure we still compile with CONFIG_PPC_TRANSACTIONAL_MEM=N.  Much of the added
    the added #ifdefs are removed in a later patch when the bulk of the TM code is
    added.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix merge conflict]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 7726a3bc8ff0..1eaea2dea174 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -475,9 +475,6 @@ struct kvm_vcpu_arch {
 	ulong ppr;
 	ulong pspb;
 	ulong fscr;
-	ulong tfhar;
-	ulong tfiar;
-	ulong texasr;
 	ulong ebbhr;
 	ulong ebbrr;
 	ulong bescr;
@@ -526,6 +523,27 @@ struct kvm_vcpu_arch {
 	u64 siar;
 	u64 sdar;
 	u64 sier;
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+	u64 tfhar;
+	u64 texasr;
+	u64 tfiar;
+
+	u32 cr_tm;
+	u64 lr_tm;
+	u64 ctr_tm;
+	u64 amr_tm;
+	u64 ppr_tm;
+	u64 dscr_tm;
+	u64 tar_tm;
+
+	ulong gpr_tm[32];
+
+	struct thread_fp_state fp_tm;
+
+	struct thread_vr_state vr_tm;
+	u32 vrsave_tm; /* also USPRG0 */
+
+#endif
 
 #ifdef CONFIG_KVM_EXIT_TIMING
 	struct mutex exit_timing_lock;

commit d682916a381ac7c8eb965c10ab64bc7cc2f18647
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Jan 8 21:25:30 2014 +1100

    KVM: PPC: Book3S HV: Basic little-endian guest support
    
    We create a guest MSR from scratch when delivering exceptions in
    a few places.  Instead of extracting LPCR[ILE] and inserting it
    into MSR_LE each time, we simply create a new variable intr_msr which
    contains the entire MSR to use.  For a little-endian guest, userspace
    needs to set the ILE (interrupt little-endian) bit in the LPCR for
    each vcpu (or at least one vcpu in each virtual core).
    
    [paulus@samba.org - removed H_SET_MODE implementation from original
    version of the patch, and made kvmppc_set_lpcr update vcpu->arch.intr_msr.]
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d161bc09153b..7726a3bc8ff0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -636,6 +636,7 @@ struct kvm_vcpu_arch {
 	spinlock_t tbacct_lock;
 	u64 busy_stolen;
 	u64 busy_preempt;
+	unsigned long intr_msr;
 #endif
 };
 

commit 8563bf52d509213e746295341ab52896b562ca5e
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jan 8 21:25:29 2014 +1100

    KVM: PPC: Book3S HV: Add support for DABRX register on POWER7
    
    The DABRX (DABR extension) register on POWER7 processors provides finer
    control over which accesses cause a data breakpoint interrupt.  It
    contains 3 bits which indicate whether to enable accesses in user,
    kernel and hypervisor modes respectively to cause data breakpoint
    interrupts, plus one bit that enables both real mode and virtual mode
    accesses to cause interrupts.  Currently, KVM sets DABRX to allow
    both kernel and user accesses to cause interrupts while in the guest.
    
    This adds support for the guest to specify other values for DABRX.
    PAPR defines a H_SET_XDABR hcall to allow the guest to set both DABR
    and DABRX with one call.  This adds a real-mode implementation of
    H_SET_XDABR, which shares most of its code with the existing H_SET_DABR
    implementation.  To support this, we add a per-vcpu field to store the
    DABRX value plus code to get and set it via the ONE_REG interface.
    
    For Linux guests to use this new hcall, userspace needs to add
    "hcall-xdabr" to the set of strings in the /chosen/hypertas-functions
    property in the device tree.  If userspace does this and then migrates
    the guest to a host where the kernel doesn't include this patch, then
    userspace will need to implement H_SET_XDABR by writing the specified
    DABR value to the DABR using the ONE_REG interface.  In that case, the
    old kernel will set DABRX to DABRX_USER | DABRX_KERNEL.  That should
    still work correctly, at least for Linux guests, since Linux guests
    cope with getting data breakpoint interrupts in modes that weren't
    requested by just ignoring the interrupt, and Linux guests never set
    DABRX_BTI.
    
    The other thing this does is to make H_SET_DABR and H_SET_XDABR work
    on POWER8, which has the DAWR and DAWRX instead of DABR/X.  Guests that
    know about POWER8 should use H_SET_MODE rather than H_SET_[X]DABR, but
    guests running in POWER7 compatibility mode will still use H_SET_[X]DABR.
    For them, this adds the logic to convert DABR/X values into DAWR/X values
    on POWER8.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 81c92d1d7978..d161bc09153b 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -466,6 +466,7 @@ struct kvm_vcpu_arch {
 	ulong uamor;
 	ulong iamr;
 	u32 ctrl;
+	u32 dabrx;
 	ulong dabr;
 	ulong dawr;
 	ulong dawrx;

commit b005255e12a311d2c87ea70a7c7b192b2187c22c
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Jan 8 21:25:21 2014 +1100

    KVM: PPC: Book3S HV: Context-switch new POWER8 SPRs
    
    This adds fields to the struct kvm_vcpu_arch to store the new
    guest-accessible SPRs on POWER8, adds code to the get/set_one_reg
    functions to allow userspace to access this state, and adds code to
    the guest entry and exit to context-switch these SPRs between host
    and guest.
    
    Note that DPDES (Directed Privileged Doorbell Exception State) is
    shared between threads on a core; hence we store it in struct
    kvmppc_vcore and have the master thread save and restore it.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index b850544dbc3f..81c92d1d7978 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -304,6 +304,7 @@ struct kvmppc_vcore {
 	ulong lpcr;
 	u32 arch_compat;
 	ulong pcr;
+	ulong dpdes;		/* doorbell state (POWER8) */
 };
 
 #define VCORE_ENTRY_COUNT(vc)	((vc)->entry_exit_count & 0xff)
@@ -448,6 +449,7 @@ struct kvm_vcpu_arch {
 	ulong pc;
 	ulong ctr;
 	ulong lr;
+	ulong tar;
 
 	ulong xer;
 	u32 cr;
@@ -457,13 +459,32 @@ struct kvm_vcpu_arch {
 	ulong guest_owned_ext;
 	ulong purr;
 	ulong spurr;
+	ulong ic;
+	ulong vtb;
 	ulong dscr;
 	ulong amr;
 	ulong uamor;
+	ulong iamr;
 	u32 ctrl;
 	ulong dabr;
+	ulong dawr;
+	ulong dawrx;
+	ulong ciabr;
 	ulong cfar;
 	ulong ppr;
+	ulong pspb;
+	ulong fscr;
+	ulong tfhar;
+	ulong tfiar;
+	ulong texasr;
+	ulong ebbhr;
+	ulong ebbrr;
+	ulong bescr;
+	ulong csigr;
+	ulong tacr;
+	ulong tcscr;
+	ulong acop;
+	ulong wort;
 	ulong shadow_srr1;
 #endif
 	u32 vrsave; /* also USPRG0 */
@@ -498,10 +519,12 @@ struct kvm_vcpu_arch {
 	u32 ccr1;
 	u32 dbsr;
 
-	u64 mmcr[3];
+	u64 mmcr[5];
 	u32 pmc[8];
+	u32 spmc[2];
 	u64 siar;
 	u64 sdar;
+	u64 sier;
 
 #ifdef CONFIG_KVM_EXIT_TIMING
 	struct mutex exit_timing_lock;

commit e0b7ec058c0eb7ba8d5d937d81de2bd16db6970e
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jan 8 21:25:20 2014 +1100

    KVM: PPC: Book3S HV: Align physical and virtual CPU thread numbers
    
    On a threaded processor such as POWER7, we group VCPUs into virtual
    cores and arrange that the VCPUs in a virtual core run on the same
    physical core.  Currently we don't enforce any correspondence between
    virtual thread numbers within a virtual core and physical thread
    numbers.  Physical threads are allocated starting at 0 on a first-come
    first-served basis to runnable virtual threads (VCPUs).
    
    POWER8 implements a new "msgsndp" instruction which guest kernels can
    use to interrupt other threads in the same core or sub-core.  Since
    the instruction takes the destination physical thread ID as a parameter,
    it becomes necessary to align the physical thread IDs with the virtual
    thread IDs, that is, to make sure virtual thread N within a virtual
    core always runs on physical thread N.
    
    This means that it's possible that thread 0, which is where we call
    __kvmppc_vcore_entry, may end up running some other vcpu than the
    one whose task called kvmppc_run_core(), or it may end up running
    no vcpu at all, if for example thread 0 of the virtual core is
    currently executing in userspace.  However, we do need thread 0
    to be responsible for switching the MMU -- a previous version of
    this patch that had other threads switching the MMU was found to
    be responsible for occasional memory corruption and machine check
    interrupts in the guest on POWER7 machines.
    
    To accommodate this, we no longer pass the vcpu pointer to
    __kvmppc_vcore_entry, but instead let the assembly code load it from
    the PACA.  Since the assembly code will need to know the kvm pointer
    and the thread ID for threads which don't have a vcpu, we move the
    thread ID into the PACA and we add a kvm pointer to the virtual core
    structure.
    
    In the case where thread 0 has no vcpu to run, it still calls into
    kvmppc_hv_entry in order to do the MMU switch, and then naps until
    either its vcpu is ready to run in the guest, or some other thread
    needs to exit the guest.  In the latter case, thread 0 jumps to the
    code that switches the MMU back to the host.  This control flow means
    that now we switch the MMU before loading any guest vcpu state.
    Similarly, on guest exit we now save all the guest vcpu state before
    switching the MMU back to the host.  This has required substantial
    code movement, making the diff rather large.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 2c2ca5faf7f2..b850544dbc3f 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -288,6 +288,7 @@ struct kvmppc_vcore {
 	int n_woken;
 	int nap_count;
 	int napping_threads;
+	int first_vcpuid;
 	u16 pcpu;
 	u16 last_cpu;
 	u8 vcore_state;
@@ -298,6 +299,7 @@ struct kvmppc_vcore {
 	u64 stolen_tb;
 	u64 preempt_tb;
 	struct kvm_vcpu *runner;
+	struct kvm *kvm;
 	u64 tb_offset;		/* guest timebase - host timebase */
 	ulong lpcr;
 	u32 arch_compat;

commit 08c9a188d0d0fc0f0c5e17d89a06bb59c493110f
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Mon Nov 18 11:18:54 2013 +0530

    kvm: powerpc: use caching attributes as per linux pte
    
    KVM uses same WIM tlb attributes as the corresponding qemu pte.
    For this we now search the linux pte for the requested page and
    get these cache caching/coherency attributes from pte.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Reviewed-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 3ca0b430eaee..2c2ca5faf7f2 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -540,6 +540,7 @@ struct kvm_vcpu_arch {
 #endif
 	gpa_t paddr_accessed;
 	gva_t vaddr_accessed;
+	pgd_t *pgdir;
 
 	u8 io_gpr; /* GPR used as IO source/target */
 	u8 mmio_is_bigendian;
@@ -597,7 +598,6 @@ struct kvm_vcpu_arch {
 	struct list_head run_list;
 	struct task_struct *run_task;
 	struct kvm_run *kvm_run;
-	pgd_t *pgdir;
 
 	spinlock_t vpa_update_lock;
 	struct kvmppc_vpa vpa;

commit efff19122315f1431f6b02cd2983b15f5d3957bd
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Oct 15 20:43:02 2013 +1100

    KVM: PPC: Store FP/VSX/VMX state in thread_fp/vr_state structures
    
    This uses struct thread_fp_state and struct thread_vr_state to store
    the floating-point, VMX/Altivec and VSX state, rather than flat arrays.
    This makes transferring the state to/from the thread_struct simpler
    and allows us to unify the get/set_one_reg implementations for the
    VSX registers.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 237d1d25b448..3ca0b430eaee 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -410,8 +410,7 @@ struct kvm_vcpu_arch {
 
 	ulong gpr[32];
 
-	u64 fpr[32];
-	u64 fpscr;
+	struct thread_fp_state fp;
 
 #ifdef CONFIG_SPE
 	ulong evr[32];
@@ -420,12 +419,7 @@ struct kvm_vcpu_arch {
 	u64 acc;
 #endif
 #ifdef CONFIG_ALTIVEC
-	vector128 vr[32];
-	vector128 vscr;
-#endif
-
-#ifdef CONFIG_VSX
-	u64 vsr[64];
+	struct thread_vr_state vr;
 #endif
 
 #ifdef CONFIG_KVM_BOOKE_HV
@@ -619,6 +613,8 @@ struct kvm_vcpu_arch {
 #endif
 };
 
+#define VCPU_FPR(vcpu, i)	(vcpu)->arch.fp.fpr[i][TS_FPROFFSET]
+
 /* Values for vcpu->arch.state */
 #define KVMPPC_VCPU_NOTREADY		0
 #define KVMPPC_VCPU_RUNNABLE		1

commit cbbc58d4fdfab1a39a6ac1b41fcb17885952157a
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:18:01 2013 +0530

    kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine
    
    This moves the kvmppc_ops callbacks to be a per VM entity. This
    enables us to select HV and PR mode when creating a VM. We also
    allow both kvm-hv and kvm-pr kernel module to be loaded. To
    achieve this we move /dev/kvm ownership to kvm.ko module. Depending on
    which KVM mode we select during VM creation we take a reference
    count on respective module
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [agraf: fix coding style]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 61ce4dca45d3..237d1d25b448 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -270,6 +270,7 @@ struct kvm_arch {
 #ifdef CONFIG_KVM_XICS
 	struct kvmppc_xics *xics;
 #endif
+	struct kvmppc_ops *kvm_ops;
 };
 
 /*

commit 9975f5e3692d320b4259a4d2edd8a979adb1e535
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:17:52 2013 +0530

    kvm: powerpc: book3s: Add a new config variable CONFIG_KVM_BOOK3S_HV_POSSIBLE
    
    This help ups to select the relevant code in the kernel code
    when we later move HV and PR bits as seperate modules. The patch
    also makes the config options for PR KVM selectable
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 9e9f689106e2..61ce4dca45d3 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -224,15 +224,15 @@ struct revmap_entry {
 #define KVMPPC_GOT_PAGE		0x80
 
 struct kvm_arch_memory_slot {
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	unsigned long *rmap;
 	unsigned long *slot_phys;
-#endif /* CONFIG_KVM_BOOK3S_64_HV */
+#endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 };
 
 struct kvm_arch {
 	unsigned int lpid;
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	unsigned long hpt_virt;
 	struct revmap_entry *revmap;
 	unsigned int host_lpid;
@@ -256,7 +256,7 @@ struct kvm_arch {
 	cpumask_t need_tlb_flush;
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
 	int hpt_cma_alloc;
-#endif /* CONFIG_KVM_BOOK3S_64_HV */
+#endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 #ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
 	struct mutex hpt_mutex;
 #endif
@@ -592,7 +592,7 @@ struct kvm_vcpu_arch {
 	struct kvmppc_icp *icp; /* XICS presentation controller */
 #endif
 
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	struct kvm_vcpu_arch_shared shregs;
 
 	unsigned long pgfault_addr;

commit 7aa79938f7d76f5865d0b2a2d9bbe2337560261f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:17:51 2013 +0530

    kvm: powerpc: book3s: pr: Rename KVM_BOOK3S_PR to KVM_BOOK3S_PR_POSSIBLE
    
    With later patches supporting PR kvm as a kernel module, the changes
    that has to be built into the main kernel binary to enable PR KVM module
    is now selected via KVM_BOOK3S_PR_POSSIBLE
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 4959ff1b5376..9e9f689106e2 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -257,7 +257,7 @@ struct kvm_arch {
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
 	int hpt_cma_alloc;
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
-#ifdef CONFIG_KVM_BOOK3S_PR
+#ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
 	struct mutex hpt_mutex;
 #endif
 #ifdef CONFIG_PPC_BOOK3S_64

commit ce11e48b7fdd256ec68b932a89b397a790566031
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Thu Jul 4 12:27:47 2013 +0530

    KVM: PPC: E500: Add userspace debug stub support
    
    This patch adds the debug stub support on booke/bookehv.
    Now QEMU debug stub can use hw breakpoint, watchpoint and
    software breakpoint to debug guest.
    
    This is how we save/restore debug register context when switching
    between guest, userspace and kernel user-process:
    
    When QEMU is running
     -> thread->debug_reg == QEMU debug register context.
     -> Kernel will handle switching the debug register on context switch.
     -> no vcpu_load() called
    
    QEMU makes ioctls (except RUN)
     -> This will call vcpu_load()
     -> should not change context.
     -> Some ioctls can change vcpu debug register, context saved in vcpu->debug_regs
    
    QEMU Makes RUN ioctl
     -> Save thread->debug_reg on STACK
     -> Store thread->debug_reg == vcpu->debug_reg
     -> load thread->debug_reg
     -> RUN VCPU ( So thread points to vcpu context )
    
    Context switch happens When VCPU running
     -> makes vcpu_load() should not load any context
     -> kernel loads the vcpu context as thread->debug_regs points to vcpu context.
    
    On heavyweight_exit
     -> Load the context saved on stack in thread->debug_reg
    
    Currently we do not support debug resource emulation to guest,
    On debug exception, always exit to user space irrespective of
    user space is expecting the debug exception or not. If this is
    unexpected exception (breakpoint/watchpoint event not set by
    userspace) then let us leave the action on user space. This
    is similar to what it was before, only thing is that now we
    have proper exit state available to user space.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 7e83c5ff830a..4959ff1b5376 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -538,7 +538,10 @@ struct kvm_vcpu_arch {
 	u32 eptcfg;
 	u32 epr;
 	u32 crit_save;
+	/* guest debug registers*/
 	struct debug_reg dbg_reg;
+	/* hardware visible debug registers when in guest state */
+	struct debug_reg shadow_dbg_reg;
 #endif
 	gpa_t paddr_accessed;
 	gva_t vaddr_accessed;

commit 547465ef8bcad77a3a73dad5151d9d28a0c1b88d
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Thu Jul 4 12:27:46 2013 +0530

    KVM: PPC: E500: Using "struct debug_reg"
    
    For KVM also use the "struct debug_reg" defined in asm/processor.h
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index b6881917cd84..7e83c5ff830a 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -387,17 +387,6 @@ struct kvmppc_slb {
 #define KVMPPC_EPR_USER		1 /* exit to userspace to fill EPR */
 #define KVMPPC_EPR_KERNEL	2 /* in-kernel irqchip */
 
-struct kvmppc_booke_debug_reg {
-	u32 dbcr0;
-	u32 dbcr1;
-	u32 dbcr2;
-#ifdef CONFIG_KVM_E500MC
-	u32 dbcr4;
-#endif
-	u64 iac[KVMPPC_BOOKE_MAX_IAC];
-	u64 dac[KVMPPC_BOOKE_MAX_DAC];
-};
-
 #define KVMPPC_IRQ_DEFAULT	0
 #define KVMPPC_IRQ_MPIC		1
 #define KVMPPC_IRQ_XICS		2
@@ -549,7 +538,7 @@ struct kvm_vcpu_arch {
 	u32 eptcfg;
 	u32 epr;
 	u32 crit_save;
-	struct kvmppc_booke_debug_reg dbg_reg;
+	struct debug_reg dbg_reg;
 #endif
 	gpa_t paddr_accessed;
 	gva_t vaddr_accessed;

commit 93b159b466bdc9753bba5c3c51b40d7ddbbcc07c
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:51 2013 +1000

    KVM: PPC: Book3S PR: Better handling of host-side read-only pages
    
    Currently we request write access to all pages that get mapped into the
    guest, even if the guest is only loading from the page.  This reduces
    the effectiveness of KSM because it means that we unshare every page we
    access.  Also, we always set the changed (C) bit in the guest HPTE if
    it allows writing, even for a guest load.
    
    This fixes both these problems.  We pass an 'iswrite' flag to the
    mmu.xlate() functions and to kvmppc_mmu_map_page() to indicate whether
    the access is a load or a store.  The mmu.xlate() functions now only
    set C for stores.  kvmppc_gfn_to_pfn() now calls gfn_to_pfn_prot()
    instead of gfn_to_pfn() so that it can indicate whether we need write
    access to the page, and get back a 'writable' flag to indicate whether
    the page is writable or not.  If that 'writable' flag is clear, we then
    make the host HPTE read-only even if the guest HPTE allowed writing.
    
    This means that we can get a protection fault when the guest writes to a
    page that it has mapped read-write but which is read-only on the host
    side (perhaps due to KSM having merged the page).  Thus we now call
    kvmppc_handle_pagefault() for protection faults as well as HPTE not found
    faults.  In kvmppc_handle_pagefault(), if the access was allowed by the
    guest HPTE and we thus need to install a new host HPTE, we then need to
    remove the old host HPTE if there is one.  This is done with a new
    function, kvmppc_mmu_unmap_page(), which uses kvmppc_mmu_pte_vflush() to
    find and remove the old host HPTE.
    
    Since the memslot-related functions require the KVM SRCU read lock to
    be held, this adds srcu_read_lock/unlock pairs around the calls to
    kvmppc_handle_pagefault().
    
    Finally, this changes kvmppc_mmu_book3s_32_xlate_pte() to not ignore
    guest HPTEs that don't permit access, and to return -EPERM for accesses
    that are not permitted by the page protections.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 404dbc81434d..b6881917cd84 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -348,7 +348,8 @@ struct kvmppc_mmu {
 	/* book3s */
 	void (*mtsrin)(struct kvm_vcpu *vcpu, u32 srnum, ulong value);
 	u32  (*mfsrin)(struct kvm_vcpu *vcpu, u32 srnum);
-	int  (*xlate)(struct kvm_vcpu *vcpu, gva_t eaddr, struct kvmppc_pte *pte, bool data);
+	int  (*xlate)(struct kvm_vcpu *vcpu, gva_t eaddr,
+		      struct kvmppc_pte *pte, bool data, bool iswrite);
 	void (*reset_msr)(struct kvm_vcpu *vcpu);
 	void (*tlbie)(struct kvm_vcpu *vcpu, ulong addr, bool large);
 	int  (*esid_to_vsid)(struct kvm_vcpu *vcpu, ulong esid, u64 *vsid);

commit 3ff955024d186c512ee91263df9c850d6ae34a12
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:49 2013 +1000

    KVM: PPC: Book3S PR: Allocate kvm_vcpu structs from kvm_vcpu_cache
    
    This makes PR KVM allocate its kvm_vcpu structs from the kvm_vcpu_cache
    rather than having them embedded in the kvmppc_vcpu_book3s struct,
    which is allocated with vzalloc.  The reason is to reduce the
    differences between PR and HV KVM in order to make is easier to have
    them coexist in one kernel binary.
    
    With this, the kvm_vcpu struct has a pointer to the kvmppc_vcpu_book3s
    struct.  The pointer to the kvmppc_book3s_shadow_vcpu struct has moved
    from the kvmppc_vcpu_book3s struct to the kvm_vcpu struct, and is only
    present for 32-bit, since it is only used for 32-bit.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: squash in compile fix from Aneesh]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0fe48729d07d..404dbc81434d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -86,6 +86,9 @@ struct lppaca;
 struct slb_shadow;
 struct dtl_entry;
 
+struct kvmppc_vcpu_book3s;
+struct kvmppc_book3s_shadow_vcpu;
+
 struct kvm_vm_stat {
 	u32 remote_tlb_flush;
 };
@@ -408,6 +411,10 @@ struct kvm_vcpu_arch {
 	int slb_max;		/* 1 + index of last valid entry in slb[] */
 	int slb_nr;		/* total number of entries in SLB */
 	struct kvmppc_mmu mmu;
+	struct kvmppc_vcpu_book3s *book3s;
+#endif
+#ifdef CONFIG_PPC_BOOK3S_32
+	struct kvmppc_book3s_shadow_vcpu *shadow_vcpu;
 #endif
 
 	ulong gpr[32];

commit 9308ab8e2da933d895ebbb903bf459e33ed94dec
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:48 2013 +1000

    KVM: PPC: Book3S PR: Make HPT accesses and updates SMP-safe
    
    This adds a per-VM mutex to provide mutual exclusion between vcpus
    for accesses to and updates of the guest hashed page table (HPT).
    This also makes the code use single-byte writes to the HPT entry
    when updating of the reference (R) and change (C) bits.  The reason
    for doing this, rather than writing back the whole HPTE, is that on
    non-PAPR virtual machines, the guest OS might be writing to the HPTE
    concurrently, and writing back the whole HPTE might conflict with
    that.  Also, real hardware does single-byte writes to update R and C.
    
    The new mutex is taken in kvmppc_mmu_book3s_64_xlate() when reading
    the HPT and updating R and/or C, and in the PAPR HPT update hcalls
    (H_ENTER, H_REMOVE, etc.).  Having the mutex means that we don't need
    to use a hypervisor lock bit in the HPT update hcalls, and we don't
    need to be careful about the order in which the bytes of the HPTE are
    updated by those hcalls.
    
    The other change here is to make emulated TLB invalidations (tlbie)
    effective across all vcpus.  To do this we call kvmppc_mmu_pte_vflush
    for all vcpus in kvmppc_ppc_book3s_64_tlbie().
    
    For 32-bit, this makes the setting of the accessed and dirty bits use
    single-byte writes, and makes tlbie invalidate shadow HPTEs for all
    vcpus.
    
    With this, PR KVM can successfully run SMP guests.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 3d8b8a8921f0..0fe48729d07d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -254,6 +254,9 @@ struct kvm_arch {
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
 	int hpt_cma_alloc;
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
+#ifdef CONFIG_KVM_BOOK3S_PR
+	struct mutex hpt_mutex;
+#endif
 #ifdef CONFIG_PPC_BOOK3S_64
 	struct list_head spapr_tce_tables;
 	struct list_head rtas_tokens;

commit a4a0f2524acc2c602cadd8e743be19d86f3a746b
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:44 2013 +1000

    KVM: PPC: Book3S PR: Allow guest to use 64k pages
    
    This adds the code to interpret 64k HPTEs in the guest hashed page
    table (HPT), 64k SLB entries, and to tell the guest about 64k pages
    in kvm_vm_ioctl_get_smmu_info().  Guest 64k pages are still shadowed
    by 4k pages.
    
    This also adds another hash table to the four we have already in
    book3s_mmu_hpte.c to allow us to find all the PTEs that we have
    instantiated that match a given 64k guest page.
    
    The tlbie instruction changed starting with POWER6 to use a bit in
    the RB operand to indicate large page invalidations, and to use other
    RB bits to indicate the base and actual page sizes and the segment
    size.  64k pages came in slightly earlier, with POWER5++.
    We use one bit in vcpu->arch.hflags to indicate that the emulated
    cpu supports 64k pages, and another to indicate that it has the new
    tlbie definition.
    
    The KVM_PPC_GET_SMMU_INFO ioctl presents a bit of a problem, because
    the MMU capabilities depend on which CPU model we're emulating, but it
    is a VM ioctl not a VCPU ioctl and therefore doesn't get passed a VCPU
    fd.  In addition, commonly-used userspace (QEMU) calls it before
    setting the PVR for any VCPU.  Therefore, as a best effort we look at
    the first vcpu in the VM and return 64k pages or not depending on its
    capabilities.  We also make the PVR default to the host PVR on recent
    CPUs that support 1TB segments (and therefore multiple page sizes as
    well) so that KVM_PPC_GET_SMMU_INFO will include 64k page and 1TB
    segment support on those CPUs.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index f48f3f09177f..3d8b8a8921f0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -68,10 +68,12 @@ extern void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 #define HPTEG_HASH_BITS_PTE_LONG	12
 #define HPTEG_HASH_BITS_VPTE		13
 #define HPTEG_HASH_BITS_VPTE_LONG	5
+#define HPTEG_HASH_BITS_VPTE_64K	11
 #define HPTEG_HASH_NUM_PTE		(1 << HPTEG_HASH_BITS_PTE)
 #define HPTEG_HASH_NUM_PTE_LONG		(1 << HPTEG_HASH_BITS_PTE_LONG)
 #define HPTEG_HASH_NUM_VPTE		(1 << HPTEG_HASH_BITS_VPTE)
 #define HPTEG_HASH_NUM_VPTE_LONG	(1 << HPTEG_HASH_BITS_VPTE_LONG)
+#define HPTEG_HASH_NUM_VPTE_64K		(1 << HPTEG_HASH_BITS_VPTE_64K)
 
 /* Physical Address Mask - allowed range of real mode RAM access */
 #define KVM_PAM			0x0fffffffffffffffULL
@@ -327,6 +329,7 @@ struct kvmppc_pte {
 	bool may_read		: 1;
 	bool may_write		: 1;
 	bool may_execute	: 1;
+	u8 page_size;		/* MMU_PAGE_xxx */
 };
 
 struct kvmppc_mmu {
@@ -359,6 +362,7 @@ struct kvmppc_slb {
 	bool large	: 1;	/* PTEs are 16MB */
 	bool tb		: 1;	/* 1TB segment */
 	bool class	: 1;
+	u8 base_page_size;	/* MMU_PAGE_xxx */
 };
 
 # ifdef CONFIG_PPC_FSL_BOOK3E

commit a2d56020d1d91934e7bb3e7c8a5a3b5921ce121b
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:43 2013 +1000

    KVM: PPC: Book3S PR: Keep volatile reg values in vcpu rather than shadow_vcpu
    
    Currently PR-style KVM keeps the volatile guest register values
    (R0 - R13, CR, LR, CTR, XER, PC) in a shadow_vcpu struct rather than
    the main kvm_vcpu struct.  For 64-bit, the shadow_vcpu exists in two
    places, a kmalloc'd struct and in the PACA, and it gets copied back
    and forth in kvmppc_core_vcpu_load/put(), because the real-mode code
    can't rely on being able to access the kmalloc'd struct.
    
    This changes the code to copy the volatile values into the shadow_vcpu
    as one of the last things done before entering the guest.  Similarly
    the values are copied back out of the shadow_vcpu to the kvm_vcpu
    immediately after exiting the guest.  We arrange for interrupts to be
    still disabled at this point so that we can't get preempted on 64-bit
    and end up copying values from the wrong PACA.
    
    This means that the accessor functions in kvm_book3s.h for these
    registers are greatly simplified, and are same between PR and HV KVM.
    In places where accesses to shadow_vcpu fields are now replaced by
    accesses to the kvm_vcpu, we can also remove the svcpu_get/put pairs.
    Finally, on 64-bit, we don't need the kmalloc'd struct at all any more.
    
    With this, the time to read the PVR one million times in a loop went
    from 567.7ms to 575.5ms (averages of 6 values), an increase of about
    1.4% for this worse-case test for guest entries and exits.  The
    standard deviation of the measurements is about 11ms, so the
    difference is only marginally significant statistically.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index b1e8f2ba2a9d..f48f3f09177f 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -463,6 +463,7 @@ struct kvm_vcpu_arch {
 	ulong dabr;
 	ulong cfar;
 	ulong ppr;
+	ulong shadow_srr1;
 #endif
 	u32 vrsave; /* also USPRG0 */
 	u32 mmucr;

commit 388cc6e133132e6c9b64e7d5361114a3a7d57663
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Sep 21 14:35:02 2013 +1000

    KVM: PPC: Book3S HV: Support POWER6 compatibility mode on POWER7
    
    This enables us to use the Processor Compatibility Register (PCR) on
    POWER7 to put the processor into architecture 2.05 compatibility mode
    when running a guest.  In this mode the new instructions and registers
    that were introduced on POWER7 are disabled in user mode.  This
    includes all the VSX facilities plus several other instructions such
    as ldbrx, stdbrx, popcntw, popcntd, etc.
    
    To select this mode, we have a new register accessible through the
    set/get_one_reg interface, called KVM_REG_PPC_ARCH_COMPAT.  Setting
    this to zero gives the full set of capabilities of the processor.
    Setting it to one of the "logical" PVR values defined in PAPR puts
    the vcpu into the compatibility mode for the corresponding
    architecture level.  The supported values are:
    
    0x0f000002      Architecture 2.05 (POWER6)
    0x0f000003      Architecture 2.06 (POWER7)
    0x0f100003      Architecture 2.06+ (POWER7+)
    
    Since the PCR is per-core, the architecture compatibility level and
    the corresponding PCR value are stored in the struct kvmppc_vcore, and
    are therefore shared between all vcpus in a virtual core.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: squash in fix to add missing break statements and documentation]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 4934e13fb23c..b1e8f2ba2a9d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -291,6 +291,8 @@ struct kvmppc_vcore {
 	struct kvm_vcpu *runner;
 	u64 tb_offset;		/* guest timebase - host timebase */
 	ulong lpcr;
+	u32 arch_compat;
+	ulong pcr;
 };
 
 #define VCORE_ENTRY_COUNT(vc)	((vc)->entry_exit_count & 0xff)

commit 4b8473c9c19dff1b0c672f182cc50b9952cf42e7
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:39 2013 +1000

    KVM: PPC: Book3S HV: Add support for guest Program Priority Register
    
    POWER7 and later IBM server processors have a register called the
    Program Priority Register (PPR), which controls the priority of
    each hardware CPU SMT thread, and affects how fast it runs compared
    to other SMT threads.  This priority can be controlled by writing to
    the PPR or by use of a set of instructions of the form or rN,rN,rN
    which are otherwise no-ops but have been defined to set the priority
    to particular levels.
    
    This adds code to context switch the PPR when entering and exiting
    guests and to make the PPR value accessible through the SET/GET_ONE_REG
    interface.  When entering the guest, we set the PPR as late as
    possible, because if we are setting a low thread priority it will
    make the code run slowly from that point on.  Similarly, the
    first-level interrupt handlers save the PPR value in the PACA very
    early on, and set the thread priority to the medium level, so that
    the interrupt handling code runs at a reasonable speed.
    
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 6eabffcb1c3c..4934e13fb23c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -460,6 +460,7 @@ struct kvm_vcpu_arch {
 	u32 ctrl;
 	ulong dabr;
 	ulong cfar;
+	ulong ppr;
 #endif
 	u32 vrsave; /* also USPRG0 */
 	u32 mmucr;

commit a0144e2a6b0b4a137a32f0102354782547bf0935
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:38 2013 +1000

    KVM: PPC: Book3S HV: Store LPCR value for each virtual core
    
    This adds the ability to have a separate LPCR (Logical Partitioning
    Control Register) value relating to a guest for each virtual core,
    rather than only having a single value for the whole VM.  This
    corresponds to what real POWER hardware does, where there is a LPCR
    per CPU thread but most of the fields are required to have the same
    value on all active threads in a core.
    
    The per-virtual-core LPCR can be read and written using the
    GET/SET_ONE_REG interface.  Userspace can can only modify the
    following fields of the LPCR value:
    
    DPFD    Default prefetch depth
    ILE     Interrupt little-endian
    TC      Translation control (secondary HPT hash group search disable)
    
    We still maintain a per-VM default LPCR value in kvm->arch.lpcr, which
    contains bits relating to memory management, i.e. the Virtualized
    Partition Memory (VPM) bits and the bits relating to guest real mode.
    When this default value is updated, the update needs to be propagated
    to the per-vcore values, so we add a kvmppc_update_lpcr() helper to do
    that.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix whitespace]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e4d67a606e43..6eabffcb1c3c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -290,6 +290,7 @@ struct kvmppc_vcore {
 	u64 preempt_tb;
 	struct kvm_vcpu *runner;
 	u64 tb_offset;		/* guest timebase - host timebase */
+	ulong lpcr;
 };
 
 #define VCORE_ENTRY_COUNT(vc)	((vc)->entry_exit_count & 0xff)

commit 93b0f4dc29c5f077a1c97bd1d484147230c3779a
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 6 13:17:46 2013 +1000

    KVM: PPC: Book3S HV: Implement timebase offset for guests
    
    This allows guests to have a different timebase origin from the host.
    This is needed for migration, where a guest can migrate from one host
    to another and the two hosts might have a different timebase origin.
    However, the timebase seen by the guest must not go backwards, and
    should go forwards only by a small amount corresponding to the time
    taken for the migration.
    
    Therefore this provides a new per-vcpu value accessed via the one_reg
    interface using the new KVM_REG_PPC_TB_OFFSET identifier.  This value
    defaults to 0 and is not modified by KVM.  On entering the guest, this
    value is added onto the timebase, and on exiting the guest, it is
    subtracted from the timebase.
    
    This is only supported for recent POWER hardware which has the TBU40
    (timebase upper 40 bits) register.  Writing to the TBU40 register only
    alters the upper 40 bits of the timebase, leaving the lower 24 bits
    unchanged.  This provides a way to modify the timebase for guest
    migration without disturbing the synchronization of the timebase
    registers across CPU cores.  The kernel rounds up the value given
    to a multiple of 2^24.
    
    Timebase values stored in KVM structures (struct kvm_vcpu, struct
    kvmppc_vcore, etc.) are stored as host timebase values.  The timebase
    values in the dispatch trace log need to be guest timebase values,
    however, since that is read directly by the guest.  This moves the
    setting of vcpu->arch.dec_expires on guest exit to a point after we
    have restored the host timebase so that vcpu->arch.dec_expires is a
    host timebase value.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d9b21af62610..e4d67a606e43 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -289,6 +289,7 @@ struct kvmppc_vcore {
 	u64 stolen_tb;
 	u64 preempt_tb;
 	struct kvm_vcpu *runner;
+	u64 tb_offset;		/* guest timebase - host timebase */
 };
 
 #define VCORE_ENTRY_COUNT(vc)	((vc)->entry_exit_count & 0xff)

commit 14941789f2a13cd89e2dd567c4f708e571ab714e
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 6 13:11:18 2013 +1000

    KVM: PPC: Book3S HV: Save/restore SIAR and SDAR along with other PMU registers
    
    Currently we are not saving and restoring the SIAR and SDAR registers in
    the PMU (performance monitor unit) on guest entry and exit.  The result
    is that performance monitoring tools in the guest could get false
    information about where a program was executing and what data it was
    accessing at the time of a performance monitor interrupt.  This fixes
    it by saving and restoring these registers along with the other PMU
    registers on guest entry/exit.
    
    This also provides a way for userspace to access these values for a
    vcpu via the one_reg interface.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0866230b7c2d..d9b21af62610 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -493,6 +493,8 @@ struct kvm_vcpu_arch {
 
 	u64 mmcr[3];
 	u32 pmc[8];
+	u64 siar;
+	u64 sdar;
 
 #ifdef CONFIG_KVM_EXIT_TIMING
 	struct mutex exit_timing_lock;

commit 2c5350e934501f1af8010c608d8dbf72ad25fdc6
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Wed Oct 2 14:22:33 2013 -0700

    KVM: PPC: Get rid of KVM_HPAGE defines
    
    Now when the main kvm code relying on these defines has been moved to
    the x86 specific part of the world, we can get rid of these.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 33283532e9d8..0866230b7c2d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -63,11 +63,6 @@ extern void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 
 #endif
 
-/* We don't currently support large pages. */
-#define KVM_HPAGE_GFN_SHIFT(x)	0
-#define KVM_NR_PAGE_SIZES	1
-#define KVM_PAGES_PER_HPAGE(x)	(1UL<<31)
-
 #define HPTEG_CACHE_NUM			(1 << 15)
 #define HPTEG_HASH_BITS_PTE		13
 #define HPTEG_HASH_BITS_PTE_LONG	12

commit 6c45b810989d1c04194499d666f695d3f811965f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 2 11:15:17 2013 +0530

    powerpc/kvm: Contiguous memory allocator based RMA allocation
    
    Older version of power architecture use Real Mode Offset register and Real Mode Limit
    Selector for mapping guest Real Mode Area. The guest RMA should be physically
    contigous since we use the range when address translation is not enabled.
    
    This patch switch RMA allocation code to use contigous memory allocator. The patch
    also remove the the linear allocator which not used any more
    
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0097dab3d601..33283532e9d8 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -183,13 +183,9 @@ struct kvmppc_spapr_tce_table {
 	struct page *pages[0];
 };
 
-struct kvmppc_linear_info {
-	void		*base_virt;
-	unsigned long	 base_pfn;
-	unsigned long	 npages;
-	struct list_head list;
-	atomic_t	 use_count;
-	int		 type;
+struct kvm_rma_info {
+	atomic_t use_count;
+	unsigned long base_pfn;
 };
 
 /* XICS components, defined in book3s_xics.c */
@@ -246,7 +242,7 @@ struct kvm_arch {
 	int tlbie_lock;
 	unsigned long lpcr;
 	unsigned long rmor;
-	struct kvmppc_linear_info *rma;
+	struct kvm_rma_info *rma;
 	unsigned long vrma_slb_v;
 	int rma_setup_done;
 	int using_mmu_notifiers;

commit fa61a4e376d2129690c82dfb05b31705a67d6e0b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 2 11:15:16 2013 +0530

    powerpc/kvm: Contiguous memory allocator based hash page table allocation
    
    Powerpc architecture uses a hash based page table mechanism for mapping virtual
    addresses to physical address. The architecture require this hash page table to
    be physically contiguous. With KVM on Powerpc currently we use early reservation
    mechanism for allocating guest hash page table. This implies that we need to
    reserve a big memory region to ensure we can create large number of guest
    simultaneously with KVM on Power. Another disadvantage is that the reserved memory
    is not available to rest of the subsystems and and that implies we limit the total
    available memory in the host.
    
    This patch series switch the guest hash page table allocation to use
    contiguous memory allocator.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index af326cde7cb6..0097dab3d601 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -259,7 +259,7 @@ struct kvm_arch {
 	spinlock_t slot_phys_lock;
 	cpumask_t need_tlb_flush;
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
-	struct kvmppc_linear_info *hpt_li;
+	int hpt_cma_alloc;
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
 #ifdef CONFIG_PPC_BOOK3S_64
 	struct list_head spapr_tce_tables;

commit bc5ad3f3701116e7db57268e6f89010ec714697e
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 17 20:30:26 2013 +0000

    KVM: PPC: Book3S: Add kernel emulation for the XICS interrupt controller
    
    This adds in-kernel emulation of the XICS (eXternal Interrupt
    Controller Specification) interrupt controller specified by PAPR, for
    both HV and PR KVM guests.
    
    The XICS emulation supports up to 1048560 interrupt sources.
    Interrupt source numbers below 16 are reserved; 0 is used to mean no
    interrupt and 2 is used for IPIs.  Internally these are represented in
    blocks of 1024, called ICS (interrupt controller source) entities, but
    that is not visible to userspace.
    
    Each vcpu gets one ICP (interrupt controller presentation) entity,
    used to store the per-vcpu state such as vcpu priority, pending
    interrupt state, IPI request, etc.
    
    This does not include any API or any way to connect vcpus to their
    ICP state; that will be added in later patches.
    
    This is based on an initial implementation by Michael Ellerman
    <michael@ellerman.id.au> reworked by Benjamin Herrenschmidt and
    Paul Mackerras.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix typo, add dependency on !KVM_MPIC]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 311f7e6f09e9..af326cde7cb6 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -192,6 +192,10 @@ struct kvmppc_linear_info {
 	int		 type;
 };
 
+/* XICS components, defined in book3s_xics.c */
+struct kvmppc_xics;
+struct kvmppc_icp;
+
 /*
  * The reverse mapping array has one entry for each HPTE,
  * which stores the guest's view of the second word of the HPTE
@@ -264,6 +268,9 @@ struct kvm_arch {
 #ifdef CONFIG_KVM_MPIC
 	struct openpic *mpic;
 #endif
+#ifdef CONFIG_KVM_XICS
+	struct kvmppc_xics *xics;
+#endif
 };
 
 /*
@@ -387,6 +394,7 @@ struct kvmppc_booke_debug_reg {
 
 #define KVMPPC_IRQ_DEFAULT	0
 #define KVMPPC_IRQ_MPIC		1
+#define KVMPPC_IRQ_XICS		2
 
 struct openpic;
 
@@ -574,6 +582,9 @@ struct kvm_vcpu_arch {
 	int irq_type;		/* one of KVM_IRQ_* */
 	int irq_cpu_id;
 	struct openpic *mpic;	/* KVM_IRQ_MPIC */
+#ifdef CONFIG_KVM_XICS
+	struct kvmppc_icp *icp; /* XICS presentation controller */
+#endif
 
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	struct kvm_vcpu_arch_shared shregs;

commit 8e591cb7204739efa8e15967ea334eb367039dde
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed Apr 17 20:30:00 2013 +0000

    KVM: PPC: Book3S: Add infrastructure to implement kernel-side RTAS calls
    
    For pseries machine emulation, in order to move the interrupt
    controller code to the kernel, we need to intercept some RTAS
    calls in the kernel itself.  This adds an infrastructure to allow
    in-kernel handlers to be registered for RTAS services by name.
    A new ioctl, KVM_PPC_RTAS_DEFINE_TOKEN, then allows userspace to
    associate token values with those service names.  Then, when the
    guest requests an RTAS service with one of those token values, it
    will be handled by the relevant in-kernel handler rather than being
    passed up to userspace as at present.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix warning]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 13740a645a6d..311f7e6f09e9 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -259,6 +259,7 @@ struct kvm_arch {
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
 #ifdef CONFIG_PPC_BOOK3S_64
 	struct list_head spapr_tce_tables;
+	struct list_head rtas_tokens;
 #endif
 #ifdef CONFIG_KVM_MPIC
 	struct openpic *mpic;

commit de9ba2f36368d21314860ee24893a6ffef01e548
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Apr 16 17:42:19 2013 +0200

    KVM: PPC: Support irq routing and irqfd for in-kernel MPIC
    
    Now that all the irq routing and irqfd pieces are generic, we can expose
    real irqchip support to all of KVM's internal helpers.
    
    This allows us to use irqfd with the in-kernel MPIC.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index c3f8ceffa412..13740a645a6d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -44,6 +44,10 @@
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 #endif
 
+/* These values are internal and can be increased later */
+#define KVM_NR_IRQCHIPS          1
+#define KVM_IRQCHIP_NUM_PINS     256
+
 #if !defined(CONFIG_KVM_440)
 #include <linux/mmu_notifier.h>
 
@@ -256,6 +260,9 @@ struct kvm_arch {
 #ifdef CONFIG_PPC_BOOK3S_64
 	struct list_head spapr_tce_tables;
 #endif
+#ifdef CONFIG_KVM_MPIC
+	struct openpic *mpic;
+#endif
 };
 
 /*

commit eb1e4f43e0f47f2655372c7d32c43db9711c278e
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Apr 12 14:08:47 2013 +0000

    kvm/ppc/mpic: add KVM_CAP_IRQ_MPIC
    
    Enabling this capability connects the vcpu to the designated in-kernel
    MPIC.  Using explicit connections between vcpus and irqchips allows
    for flexibility, but the main benefit at the moment is that it
    simplifies the code -- KVM doesn't need vm-global state to remember
    which MPIC object is associated with this vm, and it doesn't need to
    care about ordering between irqchip creation and vcpu creation.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: add stub functions for kvmppc_mpic_{dis,}connect_vcpu]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 153c8c2b0f88..c3f8ceffa412 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -377,6 +377,11 @@ struct kvmppc_booke_debug_reg {
 	u64 dac[KVMPPC_BOOKE_MAX_DAC];
 };
 
+#define KVMPPC_IRQ_DEFAULT	0
+#define KVMPPC_IRQ_MPIC		1
+
+struct openpic;
+
 struct kvm_vcpu_arch {
 	ulong host_stack;
 	u32 host_pid;
@@ -558,6 +563,10 @@ struct kvm_vcpu_arch {
 	unsigned long magic_page_pa; /* phys addr to map the magic page to */
 	unsigned long magic_page_ea; /* effect. addr to map the magic page to */
 
+	int irq_type;		/* one of KVM_IRQ_* */
+	int irq_cpu_id;
+	struct openpic *mpic;	/* KVM_IRQ_MPIC */
+
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	struct kvm_vcpu_arch_shared shregs;
 

commit 5df554ad5b7522ea62b0ff9d5be35183494efc21
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Apr 12 14:08:46 2013 +0000

    kvm/ppc/mpic: in-kernel MPIC emulation
    
    Hook the MPIC code up to the KVM interfaces, add locking, etc.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: add stub function for kvmppc_mpic_set_epr, non-booke, 64bit]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1443768a6588..153c8c2b0f88 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -361,6 +361,11 @@ struct kvmppc_slb {
 #define KVMPPC_BOOKE_MAX_IAC	4
 #define KVMPPC_BOOKE_MAX_DAC	2
 
+/* KVMPPC_EPR_USER takes precedence over KVMPPC_EPR_KERNEL */
+#define KVMPPC_EPR_NONE		0 /* EPR not supported */
+#define KVMPPC_EPR_USER		1 /* exit to userspace to fill EPR */
+#define KVMPPC_EPR_KERNEL	2 /* in-kernel irqchip */
+
 struct kvmppc_booke_debug_reg {
 	u32 dbcr0;
 	u32 dbcr1;
@@ -526,7 +531,7 @@ struct kvm_vcpu_arch {
 	u8 sane;
 	u8 cpu_type;
 	u8 hcall_needed;
-	u8 epr_enabled;
+	u8 epr_flags; /* KVMPPC_EPR_xxx */
 	u8 epr_needed;
 
 	u32 cpr0_cfgaddr; /* holds the last set cpr0_cfgaddr */
@@ -593,5 +598,6 @@ struct kvm_vcpu_arch {
 #define KVM_MMIO_REG_FQPR	0x0060
 
 #define __KVM_HAVE_ARCH_WQP
+#define __KVM_HAVE_CREATE_DEVICE
 
 #endif /* __POWERPC_KVM_HOST_H__ */

commit c35635efdc0312e013ebda1c8f3b5dd038c0d0e7
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Apr 18 19:51:04 2013 +0000

    KVM: PPC: Book3S HV: Report VPA and DTL modifications in dirty map
    
    At present, the KVM_GET_DIRTY_LOG ioctl doesn't report modifications
    done by the host to the virtual processor areas (VPAs) and dispatch
    trace logs (DTLs) registered by the guest.  This is because those
    modifications are done either in real mode or in the host kernel
    context, and in neither case does the access go through the guest's
    HPT, and thus no change (C) bit gets set in the guest's HPT.
    
    However, the changes done by the host do need to be tracked so that
    the modified pages get transferred when doing live migration.  In
    order to track these modifications, this adds a dirty flag to the
    struct representing the VPA/DTL areas, and arranges to set the flag
    when the VPA/DTL gets modified by the host.  Then, when we are
    collecting the dirty log, we also check the dirty flags for the
    VPA and DTL for each vcpu and set the relevant bit in the dirty log
    if necessary.  Doing this also means we now need to keep track of
    the guest physical address of the VPA/DTL areas.
    
    So as not to lose track of modifications to a VPA/DTL area when it gets
    unregistered, or when a new area gets registered in its place, we need
    to transfer the dirty state to the rmap chain.  This adds code to
    kvmppc_unpin_guest_page() to do that if the area was dirty.  To simplify
    that code, we now require that all VPA, DTL and SLB shadow buffer areas
    fit within a single host page.  Guests already comply with this
    requirement because pHyp requires that these areas not cross a 4k
    boundary.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 8a48e686a755..1443768a6588 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -301,11 +301,13 @@ struct kvmppc_vcore {
  * that a guest can register.
  */
 struct kvmppc_vpa {
+	unsigned long gpa;	/* Current guest phys addr */
 	void *pinned_addr;	/* Address in kernel linear mapping */
 	void *pinned_end;	/* End of region */
 	unsigned long next_gpa;	/* Guest phys addr for update */
 	unsigned long len;	/* Number of bytes required */
 	u8 update_pending;	/* 1 => update pinned_addr from next_gpa */
+	bool dirty;		/* true => area has been modified by kernel */
 };
 
 struct kvmppc_pte {

commit 9a6061d7fdedbf025549adf5c9d920d90bbf4a69
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Thu Apr 11 00:03:11 2013 +0000

    KVM: PPC: e500: Add support for EPTCFG register
    
    EPTCFG register defined by E.PT is accessed unconditionally by Linux guests
    in the presence of MAV 2.0. Emulate it now.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 3b6cee3e33a8..8a48e686a755 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -504,6 +504,7 @@ struct kvm_vcpu_arch {
 	u32 tlbcfg[4];
 	u32 tlbps[4];
 	u32 mmucfg;
+	u32 eptcfg;
 	u32 epr;
 	u32 crit_save;
 	struct kvmppc_booke_debug_reg dbg_reg;

commit 307d9008ed4f28920e0e78719e10d0f407341e00
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Thu Apr 11 00:03:10 2013 +0000

    KVM: PPC: e500: Add support for TLBnPS registers
    
    Add support for TLBnPS registers available in MMU Architecture Version
    (MAV) 2.0.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e34f8fee9080..3b6cee3e33a8 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -502,6 +502,7 @@ struct kvm_vcpu_arch {
 	spinlock_t wdt_lock;
 	struct timer_list wdt_timer;
 	u32 tlbcfg[4];
+	u32 tlbps[4];
 	u32 mmucfg;
 	u32 epr;
 	u32 crit_save;

commit 15b708beee6841e0a59ded702c8bfe3042a5b5a4
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed Feb 27 18:13:10 2013 +0000

    KVM: PPC: booke: Added debug handler
    
    Installed debug handler will be used for guest debug support
    and debug facility emulation features (patches for these
    features will follow this patch).
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    [bharat.bhushan@freescale.com: Substantial changes]
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d1bb86074721..e34f8fee9080 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -504,6 +504,7 @@ struct kvm_vcpu_arch {
 	u32 tlbcfg[4];
 	u32 mmucfg;
 	u32 epr;
+	u32 crit_save;
 	struct kvmppc_booke_debug_reg dbg_reg;
 #endif
 	gpa_t paddr_accessed;

commit 89f883372fa60f604d136924baf3e89ff1870e9e
Merge: 9e2d59ad580d 6b73a96065e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 24 13:07:18 2013 -0800

    Merge tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Marcelo Tosatti:
     "KVM updates for the 3.9 merge window, including x86 real mode
      emulation fixes, stronger memory slot interface restrictions, mmu_lock
      spinlock hold time reduction, improved handling of large page faults
      on shadow, initial APICv HW acceleration support, s390 channel IO
      based virtio, amongst others"
    
    * tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (143 commits)
      Revert "KVM: MMU: lazily drop large spte"
      x86: pvclock kvm: align allocation size to page size
      KVM: nVMX: Remove redundant get_vmcs12 from nested_vmx_exit_handled_msr
      x86 emulator: fix parity calculation for AAD instruction
      KVM: PPC: BookE: Handle alignment interrupts
      booke: Added DBCR4 SPR number
      KVM: PPC: booke: Allow multiple exception types
      KVM: PPC: booke: use vcpu reference from thread_struct
      KVM: Remove user_alloc from struct kvm_memory_slot
      KVM: VMX: disable apicv by default
      KVM: s390: Fix handling of iscs.
      KVM: MMU: cleanup __direct_map
      KVM: MMU: remove pt_access in mmu_set_spte
      KVM: MMU: cleanup mapping-level
      KVM: MMU: lazily drop large spte
      KVM: VMX: cleanup vmx_set_cr0().
      KVM: VMX: add missing exit names to VMX_EXIT_REASONS array
      KVM: VMX: disable SMEP feature when guest is in non-paging mode
      KVM: Remove duplicate text in api.txt
      Revert "KVM: MMU: split kvm_mmu_free_page"
      ...

commit 0acb91112a148fbb31678e66839ef757f3be3aa4
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Feb 4 18:10:51 2013 +0000

    powerpc/kvm/book3s_hv: Preserve guest CFAR register value
    
    The CFAR (Come-From Address Register) is a useful debugging aid that
    exists on POWER7 processors.  Currently HV KVM doesn't save or restore
    the CFAR register for guest vcpus, making the CFAR of limited use in
    guests.
    
    This adds the necessary code to capture the CFAR value saved in the
    early exception entry code (it has to be saved before any branch is
    executed), save it in the vcpu.arch struct, and restore it on entry
    to the guest.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index ca9bf459db6a..03d7beae89a0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -440,6 +440,7 @@ struct kvm_vcpu_arch {
 	ulong uamor;
 	u32 ctrl;
 	ulong dabr;
+	ulong cfar;
 #endif
 	u32 vrsave; /* also USPRG0 */
 	u32 mmucr;

commit 1c810636556c8d53a37406b34a64d9b9b0161aa6
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 4 18:12:48 2013 +0100

    KVM: PPC: BookE: Implement EPR exit
    
    The External Proxy Facility in FSL BookE chips allows the interrupt
    controller to automatically acknowledge an interrupt as soon as a
    core gets its pending external interrupt delivered.
    
    Today, user space implements the interrupt controller, so we need to
    check on it during such a cycle.
    
    This patch implements logic for user space to enable EPR exiting,
    disable EPR exiting and EPR exiting itself, so that user space can
    acknowledge an interrupt when an external interrupt has successfully
    been delivered into the guest vcpu.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index ab49c6cf891c..8a72d59467eb 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -520,6 +520,8 @@ struct kvm_vcpu_arch {
 	u8 sane;
 	u8 cpu_type;
 	u8 hcall_needed;
+	u8 epr_enabled;
+	u8 epr_needed;
 
 	u32 cpr0_cfgaddr; /* holds the last set cpr0_cfgaddr */
 

commit 0743247fbf0c4a27185b2aa1fdda91d0745dfed1
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:15 2012 -0700

    KVM: Make KVM_PRIVATE_MEM_SLOTS optional
    
    Seems like everyone copied x86 and defined 4 private memory slots
    that never actually get used.  Even x86 only uses 3 of the 4.  These
    aren't exposed so there's no need to add padding.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index ac19ad60ae8c..ab49c6cf891c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -38,9 +38,7 @@
 #define KVM_MAX_VCPUS		NR_CPUS
 #define KVM_MAX_VCORES		NR_CPUS
 #define KVM_USER_MEM_SLOTS 32
-/* memory slots that does not exposed to userspace */
-#define KVM_PRIVATE_MEM_SLOTS 4
-#define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)
+#define KVM_MEM_SLOTS_NUM KVM_USER_MEM_SLOTS
 
 #ifdef CONFIG_KVM_MMIO
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1

commit bbacc0c111c3c5d1f3192b8cc1642b9c3954f80d
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:09 2012 -0700

    KVM: Rename KVM_MEMORY_SLOTS -> KVM_USER_MEM_SLOTS
    
    It's easy to confuse KVM_MEMORY_SLOTS and KVM_MEM_SLOTS_NUM.  One is
    the user accessible slots and the other is user + private.  Make this
    more obvious.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index ca9bf459db6a..ac19ad60ae8c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -37,10 +37,10 @@
 
 #define KVM_MAX_VCPUS		NR_CPUS
 #define KVM_MAX_VCORES		NR_CPUS
-#define KVM_MEMORY_SLOTS 32
+#define KVM_USER_MEM_SLOTS 32
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4
-#define KVM_MEM_SLOTS_NUM (KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)
+#define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 
 #ifdef CONFIG_KVM_MMIO
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1

commit 62b4db0042aa753810e0d4f184481cc107c925ba
Author: Alexander Graf <agraf@suse.de>
Date:   Sat Dec 1 14:50:26 2012 +0100

    KVM: PPC: Make EPCR a valid field for booke64 and bookehv
    
    In BookE, EPCR is defined and valid when either the HV or the 64bit
    category are implemented. Reflect this in the field definition.
    
    Today the only KVM target on 64bit is HV enabled, so there is no
    change in actual source code, but this keeps the code closer to the
    spec and doesn't build up artificial road blocks for a PR KVM
    on 64bit.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 62fbd38b15fa..ca9bf459db6a 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -406,13 +406,18 @@ struct kvm_vcpu_arch {
 	u32 host_mas4;
 	u32 host_mas6;
 	u32 shadow_epcr;
-	u32 epcr;
 	u32 shadow_msrp;
 	u32 eplc;
 	u32 epsc;
 	u32 oldpir;
 #endif
 
+#if defined(CONFIG_BOOKE)
+#if defined(CONFIG_KVM_BOOKE_HV) || defined(CONFIG_64BIT)
+	u32 epcr;
+#endif
+#endif
+
 #ifdef CONFIG_PPC_BOOK3S
 	/* For Gekko paired singles */
 	u32 qpr[32];

commit 1b400ba0cd24a5994d792c7cfa0ee24cac266d3c
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Nov 21 23:28:08 2012 +0000

    KVM: PPC: Book3S HV: Improve handling of local vs. global TLB invalidations
    
    When we change or remove a HPT (hashed page table) entry, we can do
    either a global TLB invalidation (tlbie) that works across the whole
    machine, or a local invalidation (tlbiel) that only affects this core.
    Currently we do local invalidations if the VM has only one vcpu or if
    the guest requests it with the H_LOCAL flag, though the guest Linux
    kernel currently doesn't ever use H_LOCAL.  Then, to cope with the
    possibility that vcpus moving around to different physical cores might
    expose stale TLB entries, there is some code in kvmppc_hv_entry to
    flush the whole TLB of entries for this VM if either this vcpu is now
    running on a different physical core from where it last ran, or if this
    physical core last ran a different vcpu.
    
    There are a number of problems on POWER7 with this as it stands:
    
    - The TLB invalidation is done per thread, whereas it only needs to be
      done per core, since the TLB is shared between the threads.
    - With the possibility of the host paging out guest pages, the use of
      H_LOCAL by an SMP guest is dangerous since the guest could possibly
      retain and use a stale TLB entry pointing to a page that had been
      removed from the guest.
    - The TLB invalidations that we do when a vcpu moves from one physical
      core to another are unnecessary in the case of an SMP guest that isn't
      using H_LOCAL.
    - The optimization of using local invalidations rather than global should
      apply to guests with one virtual core, not just one vcpu.
    
    (None of this applies on PPC970, since there we always have to
    invalidate the whole TLB when entering and leaving the guest, and we
    can't support paging out guest memory.)
    
    To fix these problems and simplify the code, we now maintain a simple
    cpumask of which cpus need to flush the TLB on entry to the guest.
    (This is indexed by cpu, though we only ever use the bits for thread
    0 of each core.)  Whenever we do a local TLB invalidation, we set the
    bits for every cpu except the bit for thread 0 of the core that we're
    currently running on.  Whenever we enter a guest, we test and clear the
    bit for our core, and flush the TLB if it was set.
    
    On initial startup of the VM, and when resetting the HPT, we set all the
    bits in the need_tlb_flush cpumask, since any core could potentially have
    stale TLB entries from the previous VM to use the same LPID, or the
    previous contents of the HPT.
    
    Then, we maintain a count of the number of online virtual cores, and use
    that when deciding whether to use a local invalidation rather than the
    number of online vcpus.  The code to make that decision is extracted out
    into a new function, global_invalidates().  For multi-core guests on
    POWER7 (i.e. when we are using mmu notifiers), we now never do local
    invalidations regardless of the H_LOCAL flag.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 58c72646c445..62fbd38b15fa 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -246,11 +246,12 @@ struct kvm_arch {
 	int using_mmu_notifiers;
 	u32 hpt_order;
 	atomic_t vcpus_running;
+	u32 online_vcores;
 	unsigned long hpt_npte;
 	unsigned long hpt_mask;
 	atomic_t hpte_mod_interest;
 	spinlock_t slot_phys_lock;
-	unsigned short last_vcpu[NR_CPUS];
+	cpumask_t need_tlb_flush;
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
 	struct kvmppc_linear_info *hpt_li;
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
@@ -275,6 +276,7 @@ struct kvmppc_vcore {
 	int nap_count;
 	int napping_threads;
 	u16 pcpu;
+	u16 last_cpu;
 	u8 vcore_state;
 	u8 in_guest;
 	struct list_head runnable_threads;
@@ -523,7 +525,6 @@ struct kvm_vcpu_arch {
 	u64 dec_jiffies;
 	u64 dec_expires;
 	unsigned long pending_exceptions;
-	u16 last_cpu;
 	u8 ceded;
 	u8 prodded;
 	u32 last_inst;

commit 44e5f6be62741bd44968f40f3afa1cff1df983f2
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Nov 19 22:52:49 2012 +0000

    KVM: PPC: Book3S HV: Add a mechanism for recording modified HPTEs
    
    This uses a bit in our record of the guest view of the HPTE to record
    when the HPTE gets modified.  We use a reserved bit for this, and ensure
    that this bit is always cleared in HPTE values returned to the guest.
    
    The recording of modified HPTEs is only done if other code indicates
    its interest by setting kvm->arch.hpte_mod_interest to a non-zero value.
    The reason for this is that when later commits add facilities for
    userspace to read the HPT, the first pass of reading the HPT will be
    quicker if there are no (or very few) HPTEs marked as modified,
    rather than having most HPTEs marked as modified.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 3093896015f0..58c72646c445 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -248,6 +248,7 @@ struct kvm_arch {
 	atomic_t vcpus_running;
 	unsigned long hpt_npte;
 	unsigned long hpt_mask;
+	atomic_t hpte_mod_interest;
 	spinlock_t slot_phys_lock;
 	unsigned short last_vcpu[NR_CPUS];
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];

commit c7b676709c163e12ec161c0593c2c76809c25ff4
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 15 01:18:07 2012 +0000

    KVM: PPC: Book3S HV: Fix accounting of stolen time
    
    Currently the code that accounts stolen time tends to overestimate the
    stolen time, and will sometimes report more stolen time in a DTL
    (dispatch trace log) entry than has elapsed since the last DTL entry.
    This can cause guests to underflow the user or system time measured
    for some tasks, leading to ridiculous CPU percentages and total runtimes
    being reported by top and other utilities.
    
    In addition, the current code was designed for the previous policy where
    a vcore would only run when all the vcpus in it were runnable, and so
    only counted stolen time on a per-vcore basis.  Now that a vcore can
    run while some of the vcpus in it are doing other things in the kernel
    (e.g. handling a page fault), we need to count the time when a vcpu task
    is preempted while it is not running as part of a vcore as stolen also.
    
    To do this, we bring back the BUSY_IN_HOST vcpu state and extend the
    vcpu_load/put functions to count preemption time while the vcpu is
    in that state.  Handling the transitions between the RUNNING and
    BUSY_IN_HOST states requires checking and updating two variables
    (accumulated time stolen and time last preempted), so we add a new
    spinlock, vcpu->arch.tbacct_lock.  This protects both the per-vcpu
    stolen/preempt-time variables, and the per-vcore variables while this
    vcpu is running the vcore.
    
    Finally, we now don't count time spent in userspace as stolen time.
    The task could be executing in userspace on behalf of the vcpu, or
    it could be preempted, or the vcpu could be genuinely stopped.  Since
    we have no way of dividing up the time between these cases, we don't
    count any of it as stolen.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1e8cbd1299fc..3093896015f0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -559,12 +559,17 @@ struct kvm_vcpu_arch {
 	unsigned long dtl_index;
 	u64 stolen_logged;
 	struct kvmppc_vpa slb_shadow;
+
+	spinlock_t tbacct_lock;
+	u64 busy_stolen;
+	u64 busy_preempt;
 #endif
 };
 
 /* Values for vcpu->arch.state */
 #define KVMPPC_VCPU_NOTREADY		0
 #define KVMPPC_VCPU_RUNNABLE		1
+#define KVMPPC_VCPU_BUSY_IN_HOST	2
 
 /* Values for vcpu->arch.io_gpr */
 #define KVM_MMIO_REG_MASK	0x001f

commit 8455d79e2163997e479931b8d5b7e60a92cd2b86
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 15 01:17:42 2012 +0000

    KVM: PPC: Book3S HV: Run virtual core whenever any vcpus in it can run
    
    Currently the Book3S HV code implements a policy on multi-threaded
    processors (i.e. POWER7) that requires all of the active vcpus in a
    virtual core to be ready to run before we run the virtual core.
    However, that causes problems on reset, because reset stops all vcpus
    except vcpu 0, and can also reduce throughput since all four threads
    in a virtual core have to wait whenever any one of them hits a
    hypervisor page fault.
    
    This relaxes the policy, allowing the virtual core to run as soon as
    any vcpu in it is runnable.  With this, the KVMPPC_VCPU_STOPPED state
    and the KVMPPC_VCPU_BUSY_IN_HOST state have been combined into a single
    KVMPPC_VCPU_NOTREADY state, since we no longer need to distinguish
    between them.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 218534d46ae9..1e8cbd1299fc 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -563,9 +563,8 @@ struct kvm_vcpu_arch {
 };
 
 /* Values for vcpu->arch.state */
-#define KVMPPC_VCPU_STOPPED		0
-#define KVMPPC_VCPU_BUSY_IN_HOST	1
-#define KVMPPC_VCPU_RUNNABLE		2
+#define KVMPPC_VCPU_NOTREADY		0
+#define KVMPPC_VCPU_RUNNABLE		1
 
 /* Values for vcpu->arch.io_gpr */
 #define KVM_MMIO_REG_MASK	0x001f

commit 2f12f03436847e063cda8cc4c339ad84961cbf39
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 15 01:17:17 2012 +0000

    KVM: PPC: Book3S HV: Fixes for late-joining threads
    
    If a thread in a virtual core becomes runnable while other threads
    in the same virtual core are already running in the guest, it is
    possible for the latecomer to join the others on the core without
    first pulling them all out of the guest.  Currently this only happens
    rarely, when a vcpu is first started.  This fixes some bugs and
    omissions in the code in this case.
    
    First, we need to check for VPA updates for the latecomer and make
    a DTL entry for it.  Secondly, if it comes along while the master
    vcpu is doing a VPA update, we don't need to do anything since the
    master will pick it up in kvmppc_run_core.  To handle this correctly
    we introduce a new vcore state, VCORE_STARTING.  Thirdly, there is
    a race because we currently clear the hardware thread's hwthread_req
    before waiting to see it get to nap.  A latecomer thread could have
    its hwthread_req cleared before it gets to test it, and therefore
    never increment the nap_count, leading to messages about wait_for_nap
    timeouts.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 68f5a308737a..218534d46ae9 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -289,9 +289,10 @@ struct kvmppc_vcore {
 
 /* Values for vcore_state */
 #define VCORE_INACTIVE	0
-#define VCORE_RUNNING	1
-#define VCORE_EXITING	2
-#define VCORE_SLEEPING	3
+#define VCORE_SLEEPING	1
+#define VCORE_STARTING	2
+#define VCORE_RUNNING	3
+#define VCORE_EXITING	4
 
 /*
  * Struct used to manage memory for a virtual processor area

commit a66b48c3a39fa1c4223d4f847fdc7a04ed1618de
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 11 13:27:46 2012 +0000

    KVM: PPC: Move kvm->arch.slot_phys into memslot.arch
    
    Now that we have an architecture-specific field in the kvm_memory_slot
    structure, we can use it to store the array of page physical addresses
    that we need for Book3S HV KVM on PPC970 processors.  This reduces the
    size of struct kvm_arch for Book3S HV, and also reduces the size of
    struct kvm_arch_memory_slot for other PPC KVM variants since the fields
    in it are now only compiled in for Book3S HV.
    
    This necessitates making the kvm_arch_create_memslot and
    kvm_arch_free_memslot operations specific to each PPC KVM variant.
    That in turn means that we now don't allocate the rmap arrays on
    Book3S PR and Book E.
    
    Since we now unpin pages and free the slot_phys array in
    kvmppc_core_free_memslot, we no longer need to do it in
    kvmppc_core_destroy_vm, since the generic code takes care to free
    all the memslots when destroying a VM.
    
    We now need the new memslot to be passed in to
    kvmppc_core_prepare_memory_region, since we need to initialize its
    arch.slot_phys member on Book3S HV.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index f20a5ef1c7e8..68f5a308737a 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -204,7 +204,7 @@ struct revmap_entry {
 };
 
 /*
- * We use the top bit of each memslot->rmap entry as a lock bit,
+ * We use the top bit of each memslot->arch.rmap entry as a lock bit,
  * and bit 32 as a present flag.  The bottom 32 bits are the
  * index in the guest HPT of a HPTE that points to the page.
  */
@@ -215,14 +215,17 @@ struct revmap_entry {
 #define KVMPPC_RMAP_PRESENT	0x100000000ul
 #define KVMPPC_RMAP_INDEX	0xfffffffful
 
-/* Low-order bits in kvm->arch.slot_phys[][] */
+/* Low-order bits in memslot->arch.slot_phys[] */
 #define KVMPPC_PAGE_ORDER_MASK	0x1f
 #define KVMPPC_PAGE_NO_CACHE	HPTE_R_I	/* 0x20 */
 #define KVMPPC_PAGE_WRITETHRU	HPTE_R_W	/* 0x40 */
 #define KVMPPC_GOT_PAGE		0x80
 
 struct kvm_arch_memory_slot {
+#ifdef CONFIG_KVM_BOOK3S_64_HV
 	unsigned long *rmap;
+	unsigned long *slot_phys;
+#endif /* CONFIG_KVM_BOOK3S_64_HV */
 };
 
 struct kvm_arch {
@@ -246,8 +249,6 @@ struct kvm_arch {
 	unsigned long hpt_npte;
 	unsigned long hpt_mask;
 	spinlock_t slot_phys_lock;
-	unsigned long *slot_phys[KVM_MEM_SLOTS_NUM];
-	int slot_npages[KVM_MEM_SLOTS_NUM];
 	unsigned short last_vcpu[NR_CPUS];
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
 	struct kvmppc_linear_info *hpt_li;

commit 6df8d3fc58dde84fc82a9ec2581440e54dfd3d14
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed Aug 8 21:17:55 2012 +0000

    booke: Added ONE_REG interface for IAC/DAC debug registers
    
    IAC/DAC are defined as 32 bit while they are 64 bit wide. So ONE_REG
    interface is added to set/get them.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 51b0ccd56769..f20a5ef1c7e8 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -346,6 +346,27 @@ struct kvmppc_slb {
 	bool class	: 1;
 };
 
+# ifdef CONFIG_PPC_FSL_BOOK3E
+#define KVMPPC_BOOKE_IAC_NUM	2
+#define KVMPPC_BOOKE_DAC_NUM	2
+# else
+#define KVMPPC_BOOKE_IAC_NUM	4
+#define KVMPPC_BOOKE_DAC_NUM	2
+# endif
+#define KVMPPC_BOOKE_MAX_IAC	4
+#define KVMPPC_BOOKE_MAX_DAC	2
+
+struct kvmppc_booke_debug_reg {
+	u32 dbcr0;
+	u32 dbcr1;
+	u32 dbcr2;
+#ifdef CONFIG_KVM_E500MC
+	u32 dbcr4;
+#endif
+	u64 iac[KVMPPC_BOOKE_MAX_IAC];
+	u64 dac[KVMPPC_BOOKE_MAX_DAC];
+};
+
 struct kvm_vcpu_arch {
 	ulong host_stack;
 	u32 host_pid;
@@ -440,8 +461,6 @@ struct kvm_vcpu_arch {
 
 	u32 ccr0;
 	u32 ccr1;
-	u32 dbcr0;
-	u32 dbcr1;
 	u32 dbsr;
 
 	u64 mmcr[3];
@@ -476,6 +495,7 @@ struct kvm_vcpu_arch {
 	u32 tlbcfg[4];
 	u32 mmucfg;
 	u32 epr;
+	struct kvmppc_booke_debug_reg dbg_reg;
 #endif
 	gpa_t paddr_accessed;
 	gva_t vaddr_accessed;

commit f61c94bb99ca4253ac5dd57750e1af209a4beb7a
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed Aug 8 20:38:19 2012 +0000

    KVM: PPC: booke: Add watchdog emulation
    
    This patch adds the watchdog emulation in KVM. The watchdog
    emulation is enabled by KVM_ENABLE_CAP(KVM_CAP_PPC_BOOKE_WATCHDOG) ioctl.
    The kernel timer are used for watchdog emulation and emulates
    h/w watchdog state machine. On watchdog timer expiry, it exit to QEMU
    if TCR.WRC is non ZERO. QEMU can reset/shutdown etc depending upon how
    it is configured.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [bharat.bhushan@freescale.com: reworked patch]
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    [agraf: adjust to new request framework]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 4a5ec8f573c7..51b0ccd56769 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -471,6 +471,8 @@ struct kvm_vcpu_arch {
 	ulong fault_esr;
 	ulong queued_dear;
 	ulong queued_esr;
+	spinlock_t wdt_lock;
+	struct timer_list wdt_timer;
 	u32 tlbcfg[4];
 	u32 mmucfg;
 	u32 epr;
@@ -486,6 +488,7 @@ struct kvm_vcpu_arch {
 	u8 osi_needed;
 	u8 osi_enabled;
 	u8 papr_enabled;
+	u8 watchdog_enabled;
 	u8 sane;
 	u8 cpu_type;
 	u8 hcall_needed;

commit 9b0cb3c808fef0d75d6f79ab9684246e6879f9c1
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Aug 10 13:23:55 2012 +0200

    KVM: PPC: Book3s: PR: Add (dumb) MMU Notifier support
    
    Now that we have very simple MMU Notifier support for e500 in place,
    also add the same simple support to book3s. It gets us one step closer
    to actual fast support.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index cea9d3aab71c..4a5ec8f573c7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -46,8 +46,7 @@
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 #endif
 
-#if defined(CONFIG_KVM_BOOK3S_64_HV) || defined(CONFIG_KVM_E500V2) || \
-    defined(CONFIG_KVM_E500MC)
+#if !defined(CONFIG_KVM_440)
 #include <linux/mmu_notifier.h>
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER

commit 862d31f788f9a249f7656d02d8d4006e306108ce
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Jul 31 00:19:50 2012 +0200

    KVM: PPC: E500: Implement MMU notifiers
    
    The e500 target has lived without mmu notifiers ever since it got
    introduced, but fails for the user space check on them with hugetlbfs.
    
    So in order to get that one working, implement mmu notifiers in a
    reasonably dumb fashion and be happy. On embedded hardware, we almost
    never end up with mmu notifier calls, since most people don't overcommit.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 28e8f5e5c63e..cea9d3aab71c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -46,7 +46,8 @@
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 #endif
 
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#if defined(CONFIG_KVM_BOOK3S_64_HV) || defined(CONFIG_KVM_E500V2) || \
+    defined(CONFIG_KVM_E500MC)
 #include <linux/mmu_notifier.h>
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER

commit c78aa4c4b94b5b148be576a9f1570e31fe282b46
Merge: 90993cdd1800 9acb172543ae
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Sun Aug 26 13:58:41 2012 -0300

    Merge remote-tracking branch 'upstream/master' into queue
    
    Merging critical fixes from upstream required for development.
    
    * upstream/master: (809 commits)
      libata: Add a space to " 2GB ATA Flash Disk" DMA blacklist entry
      Revert "powerpc: Update g5_defconfig"
      powerpc/perf: Use pmc_overflow() to detect rolled back events
      powerpc: Fix VMX in interrupt check in POWER7 copy loops
      powerpc: POWER7 copy_to_user/copy_from_user patch applied twice
      powerpc: Fix personality handling in ppc64_personality()
      powerpc/dma-iommu: Fix IOMMU window check
      powerpc: Remove unnecessary ifdefs
      powerpc/kgdb: Restore current_thread_info properly
      powerpc/kgdb: Bail out of KGDB when we've been triggered
      powerpc/kgdb: Do not set kgdb_single_step on ppc
      powerpc/mpic_msgr: Add missing includes
      powerpc: Fix null pointer deref in perf hardware breakpoints
      powerpc: Fixup whitespace in xmon
      powerpc: Fix xmon dl command for new printk implementation
      xfs: check for possible overflow in xfs_ioc_trim
      xfs: unlock the AGI buffer when looping in xfs_dialloc
      xfs: fix uninitialised variable in xfs_rtbuf_get()
      powerpc/fsl: fix "Failed to mount /dev: No such device" errors
      powerpc/fsl: update defconfigs
      ...
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

commit 249ba1ee0f8fcb4e40caa5fbea11dafde201cc46
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Aug 3 13:56:33 2012 +0200

    KVM: PPC: Add cache flush on page map
    
    When we map a page that wasn't icache cleared before, do so when first
    mapping it in KVM using the same information bits as the Linux mapping
    logic. That way we are 100% sure that any page we map does not have stale
    entries in the icache.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 50ea12fd7bf5..a8bf5c673a3c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -33,6 +33,7 @@
 #include <asm/kvm_asm.h>
 #include <asm/processor.h>
 #include <asm/page.h>
+#include <asm/cacheflush.h>
 
 #define KVM_MAX_VCPUS		NR_CPUS
 #define KVM_MAX_VCORES		NR_CPUS

commit d89cc617b954aff4030fce178f7d86f59aaf713d
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Wed Aug 1 18:03:28 2012 +0900

    KVM: Push rmap into kvm_arch_memory_slot
    
    Two reasons:
     - x86 can integrate rmap and rmap_pde and remove heuristics in
       __gfn_to_rmap().
     - Some architectures do not need rmap.
    
    Since rmap is one of the most memory consuming stuff in KVM, ppc'd
    better restrict the allocation to Book3S HV.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 572ad0141268..a29e0918172a 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -221,6 +221,7 @@ struct revmap_entry {
 #define KVMPPC_GOT_PAGE		0x80
 
 struct kvm_arch_memory_slot {
+	unsigned long *rmap;
 };
 
 struct kvm_arch {

commit b3ae2096974b12c3af2ad1a4e7716b084949867f
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Mon Jul 2 17:56:33 2012 +0900

    KVM: Introduce kvm_unmap_hva_range() for kvm_mmu_notifier_invalidate_range_start()
    
    When we tested KVM under memory pressure, with THP enabled on the host,
    we noticed that MMU notifier took a long time to invalidate huge pages.
    
    Since the invalidation was done with mmu_lock held, it not only wasted
    the CPU but also made the host harder to respond.
    
    This patch mitigates this by using kvm_handle_hva_range().
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Cc: Alexander Graf <agraf@suse.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 50ea12fd7bf5..572ad0141268 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -52,6 +52,8 @@
 
 struct kvm;
 extern int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
+extern int kvm_unmap_hva_range(struct kvm *kvm,
+			       unsigned long start, unsigned long end);
 extern int kvm_age_hva(struct kvm *kvm, unsigned long hva);
 extern int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 extern void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);

commit 21bd000abff7d587229dbbee6f8c17f3aad9f9d8
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Sun May 20 23:21:23 2012 +0000

    KVM: PPC: booke: Added DECAR support
    
    Added the decrementer auto-reload support. DECAR is readable
    on e500v2/e500mc and later cpus.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index dd783beb88b3..50ea12fd7bf5 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -418,7 +418,9 @@ struct kvm_vcpu_arch {
 	ulong mcsrr1;
 	ulong mcsr;
 	u32 dec;
+#ifdef CONFIG_BOOKE
 	u32 decar;
+#endif
 	u32 tbl;
 	u32 tbu;
 	u32 tcr;

commit 32fad281c0680ed0ccade7dda85a2121cf9b1d06
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri May 4 02:32:53 2012 +0000

    KVM: PPC: Book3S HV: Make the guest hash table size configurable
    
    This adds a new ioctl to enable userspace to control the size of the guest
    hashed page table (HPT) and to clear it out when resetting the guest.
    The KVM_PPC_ALLOCATE_HTAB ioctl is a VM ioctl and takes as its parameter
    a pointer to a u32 containing the desired order of the HPT (log base 2
    of the size in bytes), which is updated on successful return to the
    actual order of the HPT which was allocated.
    
    There must be no vcpus running at the time of this ioctl.  To enforce
    this, we now keep a count of the number of vcpus running in
    kvm->arch.vcpus_running.
    
    If the ioctl is called when a HPT has already been allocated, we don't
    reallocate the HPT but just clear it out.  We first clear the
    kvm->arch.rma_setup_done flag, which has two effects: (a) since we hold
    the kvm->lock mutex, it will prevent any vcpus from starting to run until
    we're done, and (b) it means that the first vcpu to run after we're done
    will re-establish the VRMA if necessary.
    
    If userspace doesn't call this ioctl before running the first vcpu, the
    kernel will allocate a default-sized HPT at that point.  We do it then
    rather than when creating the VM, as the code did previously, so that
    userspace has a chance to do the ioctl if it wants.
    
    When allocating the HPT, we can allocate either from the kernel page
    allocator, or from the preallocated pool.  If userspace is asking for
    a different size from the preallocated HPTs, we first try to allocate
    using the kernel page allocator.  Then we try to allocate from the
    preallocated pool, and then if that fails, we try allocating decreasing
    sizes from the kernel page allocator, down to the minimum size allowed
    (256kB).  Note that the kernel page allocator limits allocations to
    1 << CONFIG_FORCE_MAX_ZONEORDER pages, which by default corresponds to
    16MB (on 64-bit powerpc, at least).
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix module compilation]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d848cdc49715..dd783beb88b3 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -237,6 +237,10 @@ struct kvm_arch {
 	unsigned long vrma_slb_v;
 	int rma_setup_done;
 	int using_mmu_notifiers;
+	u32 hpt_order;
+	atomic_t vcpus_running;
+	unsigned long hpt_npte;
+	unsigned long hpt_mask;
 	spinlock_t slot_phys_lock;
 	unsigned long *slot_phys[KVM_MEM_SLOTS_NUM];
 	int slot_npages[KVM_MEM_SLOTS_NUM];

commit f31e65e1170edba4a86bd8cba0318e251d3746d0
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Mar 15 21:58:34 2012 +0000

    kvm/book3s: Make kernel emulated H_PUT_TCE available for "PR" KVM
    
    There is nothing in the code for emulating TCE tables in the kernel
    that prevents it from working on "PR" KVM... other than ifdef's and
    location of the code.
    
    This and moves the bulk of the code there to a new file called
    book3s_64_vio.c.
    
    This speeds things up a bit on my G5.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [agraf: fix for hv kvm, 32bit, whitespace]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 42a527e70490..d848cdc49715 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -237,7 +237,6 @@ struct kvm_arch {
 	unsigned long vrma_slb_v;
 	int rma_setup_done;
 	int using_mmu_notifiers;
-	struct list_head spapr_tce_tables;
 	spinlock_t slot_phys_lock;
 	unsigned long *slot_phys[KVM_MEM_SLOTS_NUM];
 	int slot_npages[KVM_MEM_SLOTS_NUM];
@@ -245,6 +244,9 @@ struct kvm_arch {
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
 	struct kvmppc_linear_info *hpt_li;
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
+#ifdef CONFIG_PPC_BOOK3S_64
+	struct list_head spapr_tce_tables;
+#endif
 };
 
 /*

commit 6020c0f6e78888b6023559e9bf633ad0092a1709
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Mar 12 02:26:30 2012 +0100

    KVM: PPC: Pass EA to updating emulation ops
    
    When emulating updating load/store instructions (lwzu, stwu, ...) we need to
    write the effective address of the load/store into a register.
    
    Currently, we write the physical address in there, which is very wrong. So
    instead let's save off where the virtual fault was on MMIO and use that
    information as value to put into the register.
    
    While at it, also move the XOP variants of the above instructions to the new
    scheme of using the already known vaddr instead of calculating it themselves.
    
    Reported-by: Jörg Sommer <joerg@alea.gnuu.de>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 014eaf27a239..42a527e70490 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -464,6 +464,7 @@ struct kvm_vcpu_arch {
 	u32 epr;
 #endif
 	gpa_t paddr_accessed;
+	gva_t vaddr_accessed;
 
 	u8 io_gpr; /* GPR used as IO source/target */
 	u8 mmio_is_bigendian;

commit 0456ec4ff2b832ab9ff476ed687fea704500f1cd
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Feb 3 00:56:21 2012 +0000

    KVM: PPC: Book3S HV: Report stolen time to guest through dispatch trace log
    
    This adds code to measure "stolen" time per virtual core in units of
    timebase ticks, and to report the stolen time to the guest using the
    dispatch trace log (DTL).  The guest can register an area of memory
    for the DTL for a given vcpu.  The DTL is a ring buffer where KVM
    fills in one entry every time it enters the guest for that vcpu.
    
    Stolen time is measured as time when the virtual core is not running,
    either because the vcore is not runnable (e.g. some of its vcpus are
    executing elsewhere in the kernel or in userspace), or when the vcpu
    thread that is running the vcore is preempted.  This includes time
    when all the vcpus are idle (i.e. have executed the H_CEDE hypercall),
    which is OK because the guest accounts stolen time while idle as idle
    time.
    
    Each vcpu keeps a record of how much stolen time has been reported to
    the guest for that vcpu so far.  When we are about to enter the guest,
    we create a new DTL entry (if the guest vcpu has a DTL) and report the
    difference between total stolen time for the vcore and stolen time
    reported so far for the vcpu as the "enqueue to dispatch" time in the
    DTL entry.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 93ffd8dd8554..014eaf27a239 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -268,6 +268,9 @@ struct kvmppc_vcore {
 	struct list_head runnable_threads;
 	spinlock_t lock;
 	wait_queue_head_t wq;
+	u64 stolen_tb;
+	u64 preempt_tb;
+	struct kvm_vcpu *runner;
 };
 
 #define VCORE_ENTRY_COUNT(vc)	((vc)->entry_exit_count & 0xff)
@@ -516,6 +519,7 @@ struct kvm_vcpu_arch {
 	struct kvmppc_vpa dtl;
 	struct dtl_entry *dtl_ptr;
 	unsigned long dtl_index;
+	u64 stolen_logged;
 	struct kvmppc_vpa slb_shadow;
 #endif
 };

commit 2e25aa5f64b18a97f35266e51c71ff4dc644db0c
Author: Paul Mackerras <paulus@samba.org>
Date:   Sun Feb 19 17:46:32 2012 +0000

    KVM: PPC: Book3S HV: Make virtual processor area registration more robust
    
    The PAPR API allows three sorts of per-virtual-processor areas to be
    registered (VPA, SLB shadow buffer, and dispatch trace log), and
    furthermore, these can be registered and unregistered for another
    virtual CPU.  Currently we just update the vcpu fields pointing to
    these areas at the time of registration or unregistration.  If this
    is done on another vcpu, there is the possibility that the target vcpu
    is using those fields at the time and could end up using a bogus
    pointer and corrupting memory.
    
    This fixes the race by making the target cpu itself do the update, so
    we can be sure that the update happens at a time when the fields
    aren't being used.  Each area now has a struct kvmppc_vpa which is
    used to manage these updates.  There is also a spinlock which protects
    access to all of the kvmppc_vpa structs, other than to the pinned_addr
    fields.  (We could have just taken the spinlock when using the vpa,
    slb_shadow or dtl fields, but that would mean taking the spinlock on
    every guest entry and exit.)
    
    This also changes 'struct dtl' (which was undefined) to 'struct dtl_entry',
    which is what the rest of the kernel uses.
    
    Thanks to Michael Ellerman <michael@ellerman.id.au> for pointing out
    the need to initialize vcpu->arch.vpa_update_lock.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 97ecdaf82956..93ffd8dd8554 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -82,7 +82,7 @@ struct kvm_vcpu;
 
 struct lppaca;
 struct slb_shadow;
-struct dtl;
+struct dtl_entry;
 
 struct kvm_vm_stat {
 	u32 remote_tlb_flush;
@@ -279,6 +279,19 @@ struct kvmppc_vcore {
 #define VCORE_EXITING	2
 #define VCORE_SLEEPING	3
 
+/*
+ * Struct used to manage memory for a virtual processor area
+ * registered by a PAPR guest.  There are three types of area
+ * that a guest can register.
+ */
+struct kvmppc_vpa {
+	void *pinned_addr;	/* Address in kernel linear mapping */
+	void *pinned_end;	/* End of region */
+	unsigned long next_gpa;	/* Guest phys addr for update */
+	unsigned long len;	/* Number of bytes required */
+	u8 update_pending;	/* 1 => update pinned_addr from next_gpa */
+};
+
 struct kvmppc_pte {
 	ulong eaddr;
 	u64 vpage;
@@ -473,11 +486,6 @@ struct kvm_vcpu_arch {
 	u8 prodded;
 	u32 last_inst;
 
-	struct lppaca *vpa;
-	struct slb_shadow *slb_shadow;
-	struct dtl *dtl;
-	struct dtl *dtl_end;
-
 	wait_queue_head_t *wqp;
 	struct kvmppc_vcore *vcore;
 	int ret;
@@ -502,6 +510,13 @@ struct kvm_vcpu_arch {
 	struct task_struct *run_task;
 	struct kvm_run *kvm_run;
 	pgd_t *pgdir;
+
+	spinlock_t vpa_update_lock;
+	struct kvmppc_vpa vpa;
+	struct kvmppc_vpa dtl;
+	struct dtl_entry *dtl_ptr;
+	unsigned long dtl_index;
+	struct kvmppc_vpa slb_shadow;
 #endif
 };
 

commit 5fd8505ea4b7456d57eacefbf00b669f15f5f0c0
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Feb 16 15:04:54 2012 +0000

    KVM: PPC: bookehv: add comment about shadow_msr
    
    For BookE HV the guest visible MSR is shared->msr and is identical to
    the MSR that is in use while the guest is running, because we can't trap
    reads from/to MSR.
    
    So shadow_msr is unused there. Indicate that with a comment.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e645623728fc..97ecdaf82956 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -386,6 +386,7 @@ struct kvm_vcpu_arch {
 #endif
 	u32 vrsave; /* also USPRG0 */
 	u32 mmucr;
+	/* shadow_msr is unused for BookE HV */
 	ulong shadow_msr;
 	ulong csrr0;
 	ulong csrr1;

commit d30f6e480055e5be12e7a03fd11ea912a451daa5
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Dec 20 15:34:43 2011 +0000

    KVM: PPC: booke: category E.HV (GS-mode) support
    
    Chips such as e500mc that implement category E.HV in Power ISA 2.06
    provide hardware virtualization features, including a new MSR mode for
    guest state.  The guest OS can perform many operations without trapping
    into the hypervisor, including transitions to and from guest userspace.
    
    Since we can use SRR1[GS] to reliably tell whether an exception came from
    guest state, instead of messing around with IVPR, we use DO_KVM similarly
    to book3s.
    
    Current issues include:
     - Machine checks from guest state are not routed to the host handler.
     - The guest can cause a host oops by executing an emulated instruction
       in a page that lacks read permission.  Existing e500/4xx support has
       the same problem.
    
    Includes work by Ashish Kalra <Ashish.Kalra@freescale.com>,
    Varun Sethi <Varun.Sethi@freescale.com>, and
    Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: remove pt_regs usage]
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 5b81cbc43a42..e645623728fc 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -106,6 +106,8 @@ struct kvm_vcpu_stat {
 	u32 dec_exits;
 	u32 ext_intr_exits;
 	u32 halt_wakeup;
+	u32 dbell_exits;
+	u32 gdbell_exits;
 #ifdef CONFIG_PPC_BOOK3S
 	u32 pf_storage;
 	u32 pf_instruc;
@@ -140,6 +142,7 @@ enum kvm_exit_types {
 	EMULATED_TLBSX_EXITS,
 	EMULATED_TLBWE_EXITS,
 	EMULATED_RFI_EXITS,
+	EMULATED_RFCI_EXITS,
 	DEC_EXITS,
 	EXT_INTR_EXITS,
 	HALT_WAKEUP,
@@ -147,6 +150,8 @@ enum kvm_exit_types {
 	FP_UNAVAIL,
 	DEBUG_EXITS,
 	TIMEINGUEST,
+	DBELL_EXITS,
+	GDBELL_EXITS,
 	__NUMBER_OF_KVM_EXIT_TYPES
 };
 
@@ -217,10 +222,10 @@ struct kvm_arch_memory_slot {
 };
 
 struct kvm_arch {
+	unsigned int lpid;
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	unsigned long hpt_virt;
 	struct revmap_entry *revmap;
-	unsigned int lpid;
 	unsigned int host_lpid;
 	unsigned long host_lpcr;
 	unsigned long sdr1;
@@ -345,6 +350,17 @@ struct kvm_vcpu_arch {
 	u64 vsr[64];
 #endif
 
+#ifdef CONFIG_KVM_BOOKE_HV
+	u32 host_mas4;
+	u32 host_mas6;
+	u32 shadow_epcr;
+	u32 epcr;
+	u32 shadow_msrp;
+	u32 eplc;
+	u32 epsc;
+	u32 oldpir;
+#endif
+
 #ifdef CONFIG_PPC_BOOK3S
 	/* For Gekko paired singles */
 	u32 qpr[32];
@@ -428,6 +444,7 @@ struct kvm_vcpu_arch {
 	ulong queued_esr;
 	u32 tlbcfg[4];
 	u32 mmucfg;
+	u32 epr;
 #endif
 	gpa_t paddr_accessed;
 

commit 8fdd21a26876ea6c486c38bfa75fdd18ba299351
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Dec 20 15:34:34 2011 +0000

    KVM: PPC: e500: refactor core-specific TLB code
    
    The PID handling is e500v1/v2-specific, and is moved to e500.c.
    
    The MMU sregs code and kvmppc_core_vcpu_translate will be shared with
    e500mc, and is moved from e500.c to e500_tlb.c.
    
    Partially based on patches from Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: fix bisectability]
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 20ab5b2dbd0f..5b81cbc43a42 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -426,6 +426,8 @@ struct kvm_vcpu_arch {
 	ulong fault_esr;
 	ulong queued_dear;
 	ulong queued_esr;
+	u32 tlbcfg[4];
+	u32 mmucfg;
 #endif
 	gpa_t paddr_accessed;
 

commit 2246f8b56315befa30f3d3d2800e0734c774f70e
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Mar 13 22:35:01 2012 +0100

    KVM: PPC: Rework wqp conditional code
    
    On PowerPC, we sometimes use a waitqueue per core, not per thread,
    so we can't always use the vcpu internal waitqueue.
    
    This code has been generalized by Christoffer Dall recently, but
    unfortunately broke compilation for PowerPC. At the time the helper
    function is defined, struct kvm_vcpu is not declared yet, so we can't
    dereference it.
    
    This patch moves all logic into the generic inline function, at which
    time we have all information necessary.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 889383735e73..20ab5b2dbd0f 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -498,10 +498,6 @@ struct kvm_vcpu_arch {
 #define KVM_MMIO_REG_QPR	0x0040
 #define KVM_MMIO_REG_FQPR	0x0060
 
-#define __KVM_HAVE_ARCH_VCPU_GET_WQ 1
-static inline wait_queue_head *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
-{
-	return vcpu->arch.wqp;
-}
+#define __KVM_HAVE_ARCH_WQP
 
 #endif /* __POWERPC_KVM_HOST_H__ */

commit b6d33834bd4e8bdf4a199812e31b3e36da53c794
Author: Christoffer Dall <c.dall@virtualopensystems.com>
Date:   Thu Mar 8 16:44:24 2012 -0500

    KVM: Factor out kvm_vcpu_kick to arch-generic code
    
    The kvm_vcpu_kick function performs roughly the same funcitonality on
    most all architectures, so we shouldn't have separate copies.
    
    PowerPC keeps a pointer to interchanging waitqueues on the vcpu_arch
    structure and to accomodate this special need a
    __KVM_HAVE_ARCH_VCPU_GET_WQ define and accompanying function
    kvm_arch_vcpu_wq have been defined. For all other architectures this
    is a generic inline that just returns &vcpu->wq;
    
    Acked-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Christoffer Dall <c.dall@virtualopensystems.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 52eb9c1f4fe0..889383735e73 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -498,4 +498,10 @@ struct kvm_vcpu_arch {
 #define KVM_MMIO_REG_QPR	0x0040
 #define KVM_MMIO_REG_FQPR	0x0060
 
+#define __KVM_HAVE_ARCH_VCPU_GET_WQ 1
+static inline wait_queue_head *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
+{
+	return vcpu->arch.wqp;
+}
+
 #endif /* __POWERPC_KVM_HOST_H__ */

commit db3fe4eb45f3555d91a7124e18cf3a2f2a30eb90
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Wed Feb 8 13:02:18 2012 +0900

    KVM: Introduce kvm_memory_slot::arch and move lpage_info into it
    
    Some members of kvm_memory_slot are not used by every architecture.
    
    This patch is the first step to make this difference clear by
    introducing kvm_memory_slot::arch;  lpage_info is moved into it.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1843d5d2a3be..52eb9c1f4fe0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -213,6 +213,9 @@ struct revmap_entry {
 #define KVMPPC_PAGE_WRITETHRU	HPTE_R_W	/* 0x40 */
 #define KVMPPC_GOT_PAGE		0x80
 
+struct kvm_arch_memory_slot {
+};
+
 struct kvm_arch {
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	unsigned long hpt_virt;

commit d2a1b483a4a3f4bbb5fec1877f716c15ac7fa405
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jan 16 19:12:11 2012 +0100

    KVM: PPC: Add HPT preallocator
    
    We're currently allocating 16MB of linear memory on demand when creating
    a guest. That does work some times, but finding 16MB of linear memory
    available in the system at runtime is definitely not a given.
    
    So let's add another command line option similar to the RMA preallocator,
    that we can use to keep a pool of page tables around. Now, when a guest
    gets created it has a pretty low chance of receiving an OOM.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 8221e717bbce..1843d5d2a3be 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -235,6 +235,7 @@ struct kvm_arch {
 	int slot_npages[KVM_MEM_SLOTS_NUM];
 	unsigned short last_vcpu[NR_CPUS];
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
+	struct kvmppc_linear_info *hpt_li;
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
 };
 

commit b4e706111d501991c59d2af23a299ab52a06b03d
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jan 16 16:50:10 2012 +0100

    KVM: PPC: Convert RMA allocation into generic code
    
    We have code to allocate big chunks of linear memory on bootup for later use.
    This code is currently used for RMA allocation, but can be useful beyond that
    extent.
    
    Make it generic so we can reuse it for other stuff later.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index af438b1e8a3c..8221e717bbce 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -173,12 +173,13 @@ struct kvmppc_spapr_tce_table {
 	struct page *pages[0];
 };
 
-struct kvmppc_rma_info {
+struct kvmppc_linear_info {
 	void		*base_virt;
 	unsigned long	 base_pfn;
 	unsigned long	 npages;
 	struct list_head list;
-	atomic_t 	 use_count;
+	atomic_t	 use_count;
+	int		 type;
 };
 
 /*
@@ -224,7 +225,7 @@ struct kvm_arch {
 	int tlbie_lock;
 	unsigned long lpcr;
 	unsigned long rmor;
-	struct kvmppc_rma_info *rma;
+	struct kvmppc_linear_info *rma;
 	unsigned long vrma_slb_v;
 	int rma_setup_done;
 	int using_mmu_notifiers;

commit b3c5d3c2a49602c370de6d02fdb923bc48cd1abc
Author: Alexander Graf <agraf@suse.de>
Date:   Sat Jan 7 02:07:38 2012 +0100

    KVM: PPC: Rename MMIO register identifiers
    
    We need the KVM_REG namespace for generic register settings now, so
    let's rename the existing users to something different, enabling
    us to reuse the namespace for more visible interfaces.
    
    While at it, also move these private constants to a private header.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1cb6e522485b..af438b1e8a3c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -485,4 +485,12 @@ struct kvm_vcpu_arch {
 #define KVMPPC_VCPU_BUSY_IN_HOST	1
 #define KVMPPC_VCPU_RUNNABLE		2
 
+/* Values for vcpu->arch.io_gpr */
+#define KVM_MMIO_REG_MASK	0x001f
+#define KVM_MMIO_REG_EXT_MASK	0xffe0
+#define KVM_MMIO_REG_GPR	0x0000
+#define KVM_MMIO_REG_FPR	0x0020
+#define KVM_MMIO_REG_QPR	0x0040
+#define KVM_MMIO_REG_FQPR	0x0060
+
 #endif /* __POWERPC_KVM_HOST_H__ */

commit bad3b5075eeb18cb1641b4171618add638bc0fa7
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Dec 15 02:02:02 2011 +0000

    KVM: PPC: Book3s HV: Maintain separate guest and host views of R and C bits
    
    This allows both the guest and the host to use the referenced (R) and
    changed (C) bits in the guest hashed page table.  The guest has a view
    of R and C that is maintained in the guest_rpte field of the revmap
    entry for the HPTE, and the host has a view that is maintained in the
    rmap entry for the associated gfn.
    
    Both view are updated from the guest HPT.  If a bit (R or C) is zero
    in either view, it will be initially set to zero in the HPTE (or HPTEs),
    until set to 1 by hardware.  When an HPTE is removed for any reason,
    the R and C bits from the HPTE are ORed into both views.  We have to
    be careful to read the R and C bits from the HPTE after invalidating
    it, but before unlocking it, in case of any late updates by the hardware.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 968f3aa61cd1..1cb6e522485b 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -200,8 +200,9 @@ struct revmap_entry {
  * index in the guest HPT of a HPTE that points to the page.
  */
 #define KVMPPC_RMAP_LOCK_BIT	63
-#define KVMPPC_RMAP_REF_BIT	33
-#define KVMPPC_RMAP_REFERENCED	(1ul << KVMPPC_RMAP_REF_BIT)
+#define KVMPPC_RMAP_RC_SHIFT	32
+#define KVMPPC_RMAP_REFERENCED	(HPTE_R_R << KVMPPC_RMAP_RC_SHIFT)
+#define KVMPPC_RMAP_CHANGED	(HPTE_R_C << KVMPPC_RMAP_RC_SHIFT)
 #define KVMPPC_RMAP_PRESENT	0x100000000ul
 #define KVMPPC_RMAP_INDEX	0xfffffffful
 

commit 342d3db763f2621ed4546ebf8f6c61cb29d7fbdb
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:38:05 2011 +0000

    KVM: PPC: Implement MMU notifiers for Book3S HV guests
    
    This adds the infrastructure to enable us to page out pages underneath
    a Book3S HV guest, on processors that support virtualized partition
    memory, that is, POWER7.  Instead of pinning all the guest's pages,
    we now look in the host userspace Linux page tables to find the
    mapping for a given guest page.  Then, if the userspace Linux PTE
    gets invalidated, kvm_unmap_hva() gets called for that address, and
    we replace all the guest HPTEs that refer to that page with absent
    HPTEs, i.e. ones with the valid bit clear and the HPTE_V_ABSENT bit
    set, which will cause an HDSI when the guest tries to access them.
    Finally, the page fault handler is extended to reinstantiate the
    guest HPTE when the guest tries to access a page which has been paged
    out.
    
    Since we can't intercept the guest DSI and ISI interrupts on PPC970,
    we still have to pin all the guest pages on PPC970.  We have a new flag,
    kvm->arch.using_mmu_notifiers, that indicates whether we can page
    guest pages out.  If it is not set, the MMU notifier callbacks do
    nothing and everything operates as before.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 937cacaaf236..968f3aa61cd1 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -32,6 +32,7 @@
 #include <linux/atomic.h>
 #include <asm/kvm_asm.h>
 #include <asm/processor.h>
+#include <asm/page.h>
 
 #define KVM_MAX_VCPUS		NR_CPUS
 #define KVM_MAX_VCORES		NR_CPUS
@@ -44,6 +45,19 @@
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 #endif
 
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+#include <linux/mmu_notifier.h>
+
+#define KVM_ARCH_WANT_MMU_NOTIFIER
+
+struct kvm;
+extern int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
+extern int kvm_age_hva(struct kvm *kvm, unsigned long hva);
+extern int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
+extern void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
+
+#endif
+
 /* We don't currently support large pages. */
 #define KVM_HPAGE_GFN_SHIFT(x)	0
 #define KVM_NR_PAGE_SIZES	1
@@ -212,6 +226,7 @@ struct kvm_arch {
 	struct kvmppc_rma_info *rma;
 	unsigned long vrma_slb_v;
 	int rma_setup_done;
+	int using_mmu_notifiers;
 	struct list_head spapr_tce_tables;
 	spinlock_t slot_phys_lock;
 	unsigned long *slot_phys[KVM_MEM_SLOTS_NUM];
@@ -460,6 +475,7 @@ struct kvm_vcpu_arch {
 	struct list_head run_list;
 	struct task_struct *run_task;
 	struct kvm_run *kvm_run;
+	pgd_t *pgdir;
 #endif
 };
 

commit 697d3899dcb4bcd918d060a92db57b794e56b077
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:36:37 2011 +0000

    KVM: PPC: Implement MMIO emulation support for Book3S HV guests
    
    This provides the low-level support for MMIO emulation in Book3S HV
    guests.  When the guest tries to map a page which is not covered by
    any memslot, that page is taken to be an MMIO emulation page.  Instead
    of inserting a valid HPTE, we insert an HPTE that has the valid bit
    clear but another hypervisor software-use bit set, which we call
    HPTE_V_ABSENT, to indicate that this is an absent page.  An
    absent page is treated much like a valid page as far as guest hcalls
    (H_ENTER, H_REMOVE, H_READ etc.) are concerned, except of course that
    an absent HPTE doesn't need to be invalidated with tlbie since it
    was never valid as far as the hardware is concerned.
    
    When the guest accesses a page for which there is an absent HPTE, it
    will take a hypervisor data storage interrupt (HDSI) since we now set
    the VPM1 bit in the LPCR.  Our HDSI handler for HPTE-not-present faults
    looks up the hash table and if it finds an absent HPTE mapping the
    requested virtual address, will switch to kernel mode and handle the
    fault in kvmppc_book3s_hv_page_fault(), which at present just calls
    kvmppc_hv_emulate_mmio() to set up the MMIO emulation.
    
    This is based on an earlier patch by Benjamin Herrenschmidt, but since
    heavily reworked.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 97cb2d7865f3..937cacaaf236 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -210,6 +210,7 @@ struct kvm_arch {
 	unsigned long lpcr;
 	unsigned long rmor;
 	struct kvmppc_rma_info *rma;
+	unsigned long vrma_slb_v;
 	int rma_setup_done;
 	struct list_head spapr_tce_tables;
 	spinlock_t slot_phys_lock;
@@ -452,6 +453,10 @@ struct kvm_vcpu_arch {
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	struct kvm_vcpu_arch_shared shregs;
 
+	unsigned long pgfault_addr;
+	long pgfault_index;
+	unsigned long pgfault_hpte[2];
+
 	struct list_head run_list;
 	struct task_struct *run_task;
 	struct kvm_run *kvm_run;

commit 06ce2c63d933e347f8a199f123a8a293619ab3d2
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:33:07 2011 +0000

    KVM: PPC: Maintain a doubly-linked list of guest HPTEs for each gfn
    
    This expands the reverse mapping array to contain two links for each
    HPTE which are used to link together HPTEs that correspond to the
    same guest logical page.  Each circular list of HPTEs is pointed to
    by the rmap array entry for the guest logical page, pointed to by
    the relevant memslot.  Links are 32-bit HPT entry indexes rather than
    full 64-bit pointers, to save space.  We use 3 of the remaining 32
    bits in the rmap array entries as a lock bit, a referenced bit and
    a present bit (the present bit is needed since HPTE index 0 is valid).
    The bit lock for the rmap chain nests inside the HPTE lock bit.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 243bc8038572..97cb2d7865f3 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -170,12 +170,27 @@ struct kvmppc_rma_info {
 /*
  * The reverse mapping array has one entry for each HPTE,
  * which stores the guest's view of the second word of the HPTE
- * (including the guest physical address of the mapping).
+ * (including the guest physical address of the mapping),
+ * plus forward and backward pointers in a doubly-linked ring
+ * of HPTEs that map the same host page.  The pointers in this
+ * ring are 32-bit HPTE indexes, to save space.
  */
 struct revmap_entry {
 	unsigned long guest_rpte;
+	unsigned int forw, back;
 };
 
+/*
+ * We use the top bit of each memslot->rmap entry as a lock bit,
+ * and bit 32 as a present flag.  The bottom 32 bits are the
+ * index in the guest HPT of a HPTE that points to the page.
+ */
+#define KVMPPC_RMAP_LOCK_BIT	63
+#define KVMPPC_RMAP_REF_BIT	33
+#define KVMPPC_RMAP_REFERENCED	(1ul << KVMPPC_RMAP_REF_BIT)
+#define KVMPPC_RMAP_PRESENT	0x100000000ul
+#define KVMPPC_RMAP_INDEX	0xfffffffful
+
 /* Low-order bits in kvm->arch.slot_phys[][] */
 #define KVMPPC_PAGE_ORDER_MASK	0x1f
 #define KVMPPC_PAGE_NO_CACHE	HPTE_R_I	/* 0x20 */

commit 9d0ef5ea043d1242897d15c71bd1a15da79b4a5d
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:32:27 2011 +0000

    KVM: PPC: Allow I/O mappings in memory slots
    
    This provides for the case where userspace maps an I/O device into the
    address range of a memory slot using a VM_PFNMAP mapping.  In that
    case, we work out the pfn from vma->vm_pgoff, and record the cache
    enable bits from vma->vm_page_prot in two low-order bits in the
    slot_phys array entries.  Then, in kvmppc_h_enter() we check that the
    cache bits in the HPTE that the guest wants to insert match the cache
    bits in the slot_phys array entry.  However, we do allow the guest to
    create what it thinks is a non-cacheable or write-through mapping to
    memory that is actually cacheable, so that we can use normal system
    memory as part of an emulated device later on.  In that case the actual
    HPTE we insert is a cacheable HPTE.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 9252d5e3758d..243bc8038572 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -178,6 +178,8 @@ struct revmap_entry {
 
 /* Low-order bits in kvm->arch.slot_phys[][] */
 #define KVMPPC_PAGE_ORDER_MASK	0x1f
+#define KVMPPC_PAGE_NO_CACHE	HPTE_R_I	/* 0x20 */
+#define KVMPPC_PAGE_WRITETHRU	HPTE_R_W	/* 0x40 */
 #define KVMPPC_GOT_PAGE		0x80
 
 struct kvm_arch {

commit da9d1d7f2875cc8c1ffbce8f3501d0b33f4e7a4d
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:31:41 2011 +0000

    KVM: PPC: Allow use of small pages to back Book3S HV guests
    
    This relaxes the requirement that the guest memory be provided as
    16MB huge pages, allowing it to be provided as normal memory, i.e.
    in pages of PAGE_SIZE bytes (4k or 64k).  To allow this, we index
    the kvm->arch.slot_phys[] arrays with a small page index, even if
    huge pages are being used, and use the low-order 5 bits of each
    entry to store the order of the enclosing page with respect to
    normal pages, i.e. log_2(enclosing_page_size / PAGE_SIZE).
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index beb22ba71e26..9252d5e3758d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -177,14 +177,13 @@ struct revmap_entry {
 };
 
 /* Low-order bits in kvm->arch.slot_phys[][] */
+#define KVMPPC_PAGE_ORDER_MASK	0x1f
 #define KVMPPC_GOT_PAGE		0x80
 
 struct kvm_arch {
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	unsigned long hpt_virt;
 	struct revmap_entry *revmap;
-	unsigned long ram_psize;
-	unsigned long ram_porder;
 	unsigned int lpid;
 	unsigned int host_lpid;
 	unsigned long host_lpcr;

commit c77162dee7aff6ab5f075da9b60f649cbbeb86cc
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:31:00 2011 +0000

    KVM: PPC: Only get pages when actually needed, not in prepare_memory_region()
    
    This removes the code from kvmppc_core_prepare_memory_region() that
    looked up the VMA for the region being added and called hva_to_page
    to get the pfns for the memory.  We have no guarantee that there will
    be anything mapped there at the time of the KVM_SET_USER_MEMORY_REGION
    ioctl call; userspace can do that ioctl and then map memory into the
    region later.
    
    Instead we defer looking up the pfn for each memory page until it is
    needed, which generally means when the guest does an H_ENTER hcall on
    the page.  Since we can't call get_user_pages in real mode, if we don't
    already have the pfn for the page, kvmppc_h_enter() will return
    H_TOO_HARD and we then call kvmppc_virtmode_h_enter() once we get back
    to kernel context.  That calls kvmppc_get_guest_page() to get the pfn
    for the page, and then calls back to kvmppc_h_enter() to redo the HPTE
    insertion.
    
    When the first vcpu starts executing, we need to have the RMO or VRMA
    region mapped so that the guest's real mode accesses will work.  Thus
    we now have a check in kvmppc_vcpu_run() to see if the RMO/VRMA is set
    up and if not, call kvmppc_hv_setup_rma().  It checks if the memslot
    starting at guest physical 0 now has RMO memory mapped there; if so it
    sets it up for the guest, otherwise on POWER7 it sets up the VRMA.
    The function that does that, kvmppc_map_vrma, is now a bit simpler,
    as it calls kvmppc_virtmode_h_enter instead of creating the HPTE itself.
    
    Since we are now potentially updating entries in the slot_phys[]
    arrays from multiple vcpu threads, we now have a spinlock protecting
    those updates to ensure that we don't lose track of any references
    to pages.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 7a17ab5b9058..beb22ba71e26 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -194,7 +194,9 @@ struct kvm_arch {
 	unsigned long lpcr;
 	unsigned long rmor;
 	struct kvmppc_rma_info *rma;
+	int rma_setup_done;
 	struct list_head spapr_tce_tables;
+	spinlock_t slot_phys_lock;
 	unsigned long *slot_phys[KVM_MEM_SLOTS_NUM];
 	int slot_npages[KVM_MEM_SLOTS_NUM];
 	unsigned short last_vcpu[NR_CPUS];

commit b2b2f16508de10bb1863bdd4ec1fa212111df5b4
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:28:21 2011 +0000

    KVM: PPC: Keep page physical addresses in per-slot arrays
    
    This allocates an array for each memory slot that is added to store
    the physical addresses of the pages in the slot.  This array is
    vmalloc'd and accessed in kvmppc_h_enter using real_vmalloc_addr().
    This allows us to remove the ram_pginfo field from the kvm_arch
    struct, and removes the 64GB guest RAM limit that we had.
    
    We use the low-order bits of the array entries to store a flag
    indicating that we have done get_page on the corresponding page,
    and therefore need to call put_page when we are finished with the
    page.  Currently this is set for all pages except those in our
    special RMO regions.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 629df2ed22f7..7a17ab5b9058 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -38,6 +38,7 @@
 #define KVM_MEMORY_SLOTS 32
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4
+#define KVM_MEM_SLOTS_NUM (KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 
 #ifdef CONFIG_KVM_MMIO
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
@@ -175,25 +176,27 @@ struct revmap_entry {
 	unsigned long guest_rpte;
 };
 
+/* Low-order bits in kvm->arch.slot_phys[][] */
+#define KVMPPC_GOT_PAGE		0x80
+
 struct kvm_arch {
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	unsigned long hpt_virt;
 	struct revmap_entry *revmap;
-	unsigned long ram_npages;
 	unsigned long ram_psize;
 	unsigned long ram_porder;
-	struct kvmppc_pginfo *ram_pginfo;
 	unsigned int lpid;
 	unsigned int host_lpid;
 	unsigned long host_lpcr;
 	unsigned long sdr1;
 	unsigned long host_sdr1;
 	int tlbie_lock;
-	int n_rma_pages;
 	unsigned long lpcr;
 	unsigned long rmor;
 	struct kvmppc_rma_info *rma;
 	struct list_head spapr_tce_tables;
+	unsigned long *slot_phys[KVM_MEM_SLOTS_NUM];
+	int slot_npages[KVM_MEM_SLOTS_NUM];
 	unsigned short last_vcpu[NR_CPUS];
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
 #endif /* CONFIG_KVM_BOOK3S_64_HV */

commit 8936dda4c2ed070ecebd786baf35b08584accf4a
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:27:39 2011 +0000

    KVM: PPC: Keep a record of HV guest view of hashed page table entries
    
    This adds an array that parallels the guest hashed page table (HPT),
    that is, it has one entry per HPTE, used to store the guest's view
    of the second doubleword of the corresponding HPTE.  The first
    doubleword in the HPTE is the same as the guest's idea of it, so we
    don't need to store a copy, but the second doubleword in the HPTE has
    the real page number rather than the guest's logical page number.
    This allows us to remove the back_translate() and reverse_xlate()
    functions.
    
    This "reverse mapping" array is vmalloc'd, meaning that to access it
    in real mode we have to walk the kernel's page tables explicitly.
    That is done by the new real_vmalloc_addr() function.  (In fact this
    returns an address in the linear mapping, so the result is usable
    both in real mode and in virtual mode.)
    
    There are also some minor cleanups here: moving the definitions of
    HPT_ORDER etc. to a header file and defining HPT_NPTE for HPT_NPTEG << 3.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 66c75cddaec6..629df2ed22f7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -166,9 +166,19 @@ struct kvmppc_rma_info {
 	atomic_t 	 use_count;
 };
 
+/*
+ * The reverse mapping array has one entry for each HPTE,
+ * which stores the guest's view of the second word of the HPTE
+ * (including the guest physical address of the mapping).
+ */
+struct revmap_entry {
+	unsigned long guest_rpte;
+};
+
 struct kvm_arch {
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	unsigned long hpt_virt;
+	struct revmap_entry *revmap;
 	unsigned long ram_npages;
 	unsigned long ram_psize;
 	unsigned long ram_porder;

commit dfd4d47e9a71c5a35eb67a44cd311efbe1846b7e
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Nov 17 12:39:59 2011 +0000

    KVM: PPC: booke: Improve timer register emulation
    
    Decrementers are now properly driven by TCR/TSR, and the guest
    has full read/write access to these registers.
    
    The decrementer keeps ticking (and setting the TSR bit) regardless of
    whether the interrupts are enabled with TCR.
    
    The decrementer stops at zero, rather than going negative.
    
    Decrementers (and FITs, once implemented) are delivered as
    level-triggered interrupts -- dequeued when the TSR bit is cleared, not
    on delivery.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    [scottwood@freescale.com: significant changes]
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index bfd0c9912da5..66c75cddaec6 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -330,7 +330,7 @@ struct kvm_vcpu_arch {
 	u32 tbl;
 	u32 tbu;
 	u32 tcr;
-	u32 tsr;
+	ulong tsr; /* we need to perform set/clr_bits() which requires ulong */
 	u32 ivor[64];
 	ulong ivpr;
 	u32 pvr;

commit b59049720dd95021dfe0d9f4e1fa9458a67cfe29
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Nov 8 18:23:30 2011 -0600

    KVM: PPC: Paravirtualize SPRG4-7, ESR, PIR, MASn
    
    This allows additional registers to be accessed by the guest
    in PR-mode KVM without trapping.
    
    SPRG4-7 are readable from userspace.  On booke, KVM will sync
    these registers when it enters the guest, so that accesses from
    guest userspace will work.  The guest kernel, OTOH, must consistently
    use either the real registers or the shared area between exits.  This
    also applies to the already-paravirted SPRG3.
    
    On non-booke, it's not clear to what extent SPRG4-7 are supported
    (they're not architected for book3s, but exist on at least some classic
    chips).  They are copied in the get/set regs ioctls, but I do not see any
    non-booke emulation.  I also do not see any syncing with real registers
    (in PR-mode) including the user-readable SPRG3.  This patch should not
    make that situation any worse.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index bf8af5d5d5dc..bfd0c9912da5 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -318,10 +318,6 @@ struct kvm_vcpu_arch {
 	u32 vrsave; /* also USPRG0 */
 	u32 mmucr;
 	ulong shadow_msr;
-	ulong sprg4;
-	ulong sprg5;
-	ulong sprg6;
-	ulong sprg7;
 	ulong csrr0;
 	ulong csrr1;
 	ulong dsrr0;
@@ -329,7 +325,6 @@ struct kvm_vcpu_arch {
 	ulong mcsrr0;
 	ulong mcsrr1;
 	ulong mcsr;
-	ulong esr;
 	u32 dec;
 	u32 decar;
 	u32 tbl;
@@ -338,7 +333,6 @@ struct kvm_vcpu_arch {
 	u32 tsr;
 	u32 ivor[64];
 	ulong ivpr;
-	u32 pir;
 	u32 pvr;
 
 	u32 shadow_pid;

commit 19ccb76a1938ab364a412253daec64613acbf3df
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Jul 23 17:42:46 2011 +1000

    KVM: PPC: Implement H_CEDE hcall for book3s_hv in real-mode code
    
    With a KVM guest operating in SMT4 mode (i.e. 4 hardware threads per
    core), whenever a CPU goes idle, we have to pull all the other
    hardware threads in the core out of the guest, because the H_CEDE
    hcall is handled in the kernel.  This is inefficient.
    
    This adds code to book3s_hv_rmhandlers.S to handle the H_CEDE hcall
    in real mode.  When a guest vcpu does an H_CEDE hcall, we now only
    exit to the kernel if all the other vcpus in the same core are also
    idle.  Otherwise we mark this vcpu as napping, save state that could
    be lost in nap mode (mainly GPRs and FPRs), and execute the nap
    instruction.  When the thread wakes up, because of a decrementer or
    external interrupt, we come back in at kvm_start_guest (from the
    system reset interrupt vector), find the `napping' flag set in the
    paca, and go to the resume path.
    
    This has some other ramifications.  First, when starting a core, we
    now start all the threads, both those that are immediately runnable and
    those that are idle.  This is so that we don't have to pull all the
    threads out of the guest when an idle thread gets a decrementer interrupt
    and wants to start running.  In fact the idle threads will all start
    with the H_CEDE hcall returning; being idle they will just do another
    H_CEDE immediately and go to nap mode.
    
    This required some changes to kvmppc_run_core() and kvmppc_run_vcpu().
    These functions have been restructured to make them simpler and clearer.
    We introduce a level of indirection in the wait queue that gets woken
    when external and decrementer interrupts get generated for a vcpu, so
    that we can have the 4 vcpus in a vcore using the same wait queue.
    We need this because the 4 vcpus are being handled by one thread.
    
    Secondly, when we need to exit from the guest to the kernel, we now
    have to generate an IPI for any napping threads, because an HDEC
    interrupt doesn't wake up a napping thread.
    
    Thirdly, we now need to be able to handle virtual external interrupts
    and decrementer interrupts becoming pending while a thread is napping,
    and deliver those interrupts to the guest when the thread wakes.
    This is done in kvmppc_cede_reentry, just before fast_guest_return.
    
    Finally, since we are not using the generic kvm_vcpu_block for book3s_hv,
    and hence not calling kvm_arch_vcpu_runnable, we can remove the #ifdef
    from kvm_arch_vcpu_runnable.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index dec3054f6ad4..bf8af5d5d5dc 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -198,21 +198,29 @@ struct kvm_arch {
  */
 struct kvmppc_vcore {
 	int n_runnable;
-	int n_blocked;
+	int n_busy;
 	int num_threads;
 	int entry_exit_count;
 	int n_woken;
 	int nap_count;
+	int napping_threads;
 	u16 pcpu;
-	u8 vcore_running;
+	u8 vcore_state;
 	u8 in_guest;
 	struct list_head runnable_threads;
 	spinlock_t lock;
+	wait_queue_head_t wq;
 };
 
 #define VCORE_ENTRY_COUNT(vc)	((vc)->entry_exit_count & 0xff)
 #define VCORE_EXIT_COUNT(vc)	((vc)->entry_exit_count >> 8)
 
+/* Values for vcore_state */
+#define VCORE_INACTIVE	0
+#define VCORE_RUNNING	1
+#define VCORE_EXITING	2
+#define VCORE_SLEEPING	3
+
 struct kvmppc_pte {
 	ulong eaddr;
 	u64 vpage;
@@ -403,11 +411,13 @@ struct kvm_vcpu_arch {
 	struct dtl *dtl;
 	struct dtl *dtl_end;
 
+	wait_queue_head_t *wqp;
 	struct kvmppc_vcore *vcore;
 	int ret;
 	int trap;
 	int state;
 	int ptid;
+	bool timer_running;
 	wait_queue_head_t cpu_run;
 
 	struct kvm_vcpu_arch_shared *shared;
@@ -423,8 +433,9 @@ struct kvm_vcpu_arch {
 #endif
 };
 
-#define KVMPPC_VCPU_BUSY_IN_HOST	0
-#define KVMPPC_VCPU_BLOCKED		1
+/* Values for vcpu->arch.state */
+#define KVMPPC_VCPU_STOPPED		0
+#define KVMPPC_VCPU_BUSY_IN_HOST	1
 #define KVMPPC_VCPU_RUNNABLE		2
 
 #endif /* __POWERPC_KVM_HOST_H__ */

commit 02143947603fe90237a0423d34dd8943de229f78
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Jul 23 17:41:44 2011 +1000

    KVM: PPC: book3s_pr: Simplify transitions between virtual and real mode
    
    This simplifies the way that the book3s_pr makes the transition to
    real mode when entering the guest.  We now call kvmppc_entry_trampoline
    (renamed from kvmppc_rmcall) in the base kernel using a normal function
    call instead of doing an indirect call through a pointer in the vcpu.
    If kvm is a module, the module loader takes care of generating a
    trampoline as it does for other calls to functions outside the module.
    
    kvmppc_entry_trampoline then disables interrupts and jumps to
    kvmppc_handler_trampoline_enter in real mode using an rfi[d].
    That then uses the link register as the address to return to
    (potentially in module space) when the guest exits.
    
    This also simplifies the way that we call the Linux interrupt handler
    when we exit the guest due to an external, decrementer or performance
    monitor interrupt.  Instead of turning on the MMU, then deciding that
    we need to call the Linux handler and turning the MMU back off again,
    we now go straight to the handler at the point where we would turn the
    MMU on.  The handler will then return to the virtual-mode code
    (potentially in the module).
    
    Along the way, this moves the setting and clearing of the HID5 DCBZ32
    bit into real-mode interrupts-off code, and also makes sure that
    we clear the MSR[RI] bit before loading values into SRR0/1.
    
    The net result is that we no longer need any code addresses to be
    stored in vcpu->arch.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 2b8284f4b4b7..dec3054f6ad4 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -258,14 +258,6 @@ struct kvm_vcpu_arch {
 	ulong host_stack;
 	u32 host_pid;
 #ifdef CONFIG_PPC_BOOK3S
-	ulong host_msr;
-	ulong host_r2;
-	void *host_retip;
-	ulong trampoline_lowmem;
-	ulong trampoline_enter;
-	ulong highmem_handler;
-	ulong rmcall;
-	ulong host_paca_phys;
 	struct kvmppc_slb slb[64];
 	int slb_max;		/* 1 + index of last valid entry in slb[] */
 	int slb_nr;		/* total number of entries in SLB */

commit af8f38b3499f0d4a3c354df2435f0fb2dded250a
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Aug 10 13:57:08 2011 +0200

    KVM: PPC: Add sanity checking to vcpu_run
    
    There are multiple features in PowerPC KVM that can now be enabled
    depending on the user's wishes. Some of the combinations don't make
    sense or don't work though.
    
    So this patch adds a way to check if the executing environment would
    actually be able to run the guest properly. It also adds sanity
    checks if PVR is set (should always be true given the current code
    flow), if PAPR is only used with book3s_64 where it works and that
    HV KVM is only used in PAPR mode.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e6813021126d..2b8284f4b4b7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -390,6 +390,8 @@ struct kvm_vcpu_arch {
 	u8 osi_needed;
 	u8 osi_enabled;
 	u8 papr_enabled;
+	u8 sane;
+	u8 cpu_type;
 	u8 hcall_needed;
 
 	u32 cpr0_cfgaddr; /* holds the last set cpr0_cfgaddr */

commit 9432ba6015371f186926cd62e2395718217a17a1
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 8 16:08:55 2011 +0200

    KVM: PPC: Add papr_enabled flag
    
    When running a PAPR guest, some things change. The privilege level drops
    from hypervisor to supervisor, SDR1 gets treated differently and we interpret
    hypercalls. For bisectability sake, add the flag now, but only enable it when
    all the support code is there.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index cc22b282d755..e6813021126d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -389,6 +389,7 @@ struct kvm_vcpu_arch {
 	u8 dcr_is_write;
 	u8 osi_needed;
 	u8 osi_enabled;
+	u8 papr_enabled;
 	u8 hcall_needed;
 
 	u32 cpr0_cfgaddr; /* holds the last set cpr0_cfgaddr */

commit 9e368f2915601cd5bc7f5fd638b58435b018bbd7
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:40:08 2011 +0000

    KVM: PPC: book3s_hv: Add support for PPC970-family processors
    
    This adds support for running KVM guests in supervisor mode on those
    PPC970 processors that have a usable hypervisor mode.  Unfortunately,
    Apple G5 machines have supervisor mode disabled (MSR[HV] is forced to
    1), but the YDL PowerStation does have a usable hypervisor mode.
    
    There are several differences between the PPC970 and POWER7 in how
    guests are managed.  These differences are accommodated using the
    CPU_FTR_ARCH_201 (PPC970) and CPU_FTR_ARCH_206 (POWER7) CPU feature
    bits.  Notably, on PPC970:
    
    * The LPCR, LPID or RMOR registers don't exist, and the functions of
      those registers are provided by bits in HID4 and one bit in HID0.
    
    * External interrupts can be directed to the hypervisor, but unlike
      POWER7 they are masked by MSR[EE] in non-hypervisor modes and use
      SRR0/1 not HSRR0/1.
    
    * There is no virtual RMA (VRMA) mode; the guest must use an RMO
      (real mode offset) area.
    
    * The TLB entries are not tagged with the LPID, so it is necessary to
      flush the whole TLB on partition switch.  Furthermore, when switching
      partitions we have to ensure that no other CPU is executing the tlbie
      or tlbsync instructions in either the old or the new partition,
      otherwise undefined behaviour can occur.
    
    * The PMU has 8 counters (PMC registers) rather than 6.
    
    * The DSCR, PURR, SPURR, AMR, AMOR, UAMOR registers don't exist.
    
    * The SLB has 64 entries rather than 32.
    
    * There is no mediated external interrupt facility, so if we switch to
      a guest that has a virtual external interrupt pending but the guest
      has MSR[EE] = 0, we have to arrange to have an interrupt pending for
      it so that we can get control back once it re-enables interrupts.  We
      do that by sending ourselves an IPI with smp_send_reschedule after
      hard-disabling interrupts.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index f572d9cc31bd..cc22b282d755 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -353,7 +353,7 @@ struct kvm_vcpu_arch {
 	u32 dbsr;
 
 	u64 mmcr[3];
-	u32 pmc[6];
+	u32 pmc[8];
 
 #ifdef CONFIG_KVM_EXIT_TIMING
 	struct mutex exit_timing_lock;

commit aa04b4cc5be64b4fb9ef4e0fdf2418e2f4737fb2
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:25:44 2011 +0000

    KVM: PPC: Allocate RMAs (Real Mode Areas) at boot for use by guests
    
    This adds infrastructure which will be needed to allow book3s_hv KVM to
    run on older POWER processors, including PPC970, which don't support
    the Virtual Real Mode Area (VRMA) facility, but only the Real Mode
    Offset (RMO) facility.  These processors require a physically
    contiguous, aligned area of memory for each guest.  When the guest does
    an access in real mode (MMU off), the address is compared against a
    limit value, and if it is lower, the address is ORed with an offset
    value (from the Real Mode Offset Register (RMOR)) and the result becomes
    the real address for the access.  The size of the RMA has to be one of
    a set of supported values, which usually includes 64MB, 128MB, 256MB
    and some larger powers of 2.
    
    Since we are unlikely to be able to allocate 64MB or more of physically
    contiguous memory after the kernel has been running for a while, we
    allocate a pool of RMAs at boot time using the bootmem allocator.  The
    size and number of the RMAs can be set using the kvm_rma_size=xx and
    kvm_rma_count=xx kernel command line options.
    
    KVM exports a new capability, KVM_CAP_PPC_RMA, to signal the availability
    of the pool of preallocated RMAs.  The capability value is 1 if the
    processor can use an RMA but doesn't require one (because it supports
    the VRMA facility), or 2 if the processor requires an RMA for each guest.
    
    This adds a new ioctl, KVM_ALLOCATE_RMA, which allocates an RMA from the
    pool and returns a file descriptor which can be used to map the RMA.  It
    also returns the size of the RMA in the argument structure.
    
    Having an RMA means we will get multiple KMV_SET_USER_MEMORY_REGION
    ioctl calls from userspace.  To cope with this, we now preallocate the
    kvm->arch.ram_pginfo array when the VM is created with a size sufficient
    for up to 64GB of guest memory.  Subsequently we will get rid of this
    array and use memory associated with each memslot instead.
    
    This moves most of the code that translates the user addresses into
    host pfns (page frame numbers) out of kvmppc_prepare_vrma up one level
    to kvmppc_core_prepare_memory_region.  Also, instead of having to look
    up the VMA for each page in order to check the page size, we now check
    that the pages we get are compound pages of 16MB.  However, if we are
    adding memory that is mapped to an RMA, we don't bother with calling
    get_user_pages_fast and instead just offset from the base pfn for the
    RMA.
    
    Typically the RMA gets added after vcpus are created, which makes it
    inconvenient to have the LPCR (logical partition control register) value
    in the vcpu->arch struct, since the LPCR controls whether the processor
    uses RMA or VRMA for the guest.  This moves the LPCR value into the
    kvm->arch struct and arranges for the MER (mediated external request)
    bit, which is the only bit that varies between vcpus, to be set in
    assembly code when going into the guest if there is a pending external
    interrupt request.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0d6d569e19c7..f572d9cc31bd 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -28,6 +28,8 @@
 #include <linux/threads.h>
 #include <linux/spinlock.h>
 #include <linux/kvm_para.h>
+#include <linux/list.h>
+#include <linux/atomic.h>
 #include <asm/kvm_asm.h>
 #include <asm/processor.h>
 
@@ -156,6 +158,14 @@ struct kvmppc_spapr_tce_table {
 	struct page *pages[0];
 };
 
+struct kvmppc_rma_info {
+	void		*base_virt;
+	unsigned long	 base_pfn;
+	unsigned long	 npages;
+	struct list_head list;
+	atomic_t 	 use_count;
+};
+
 struct kvm_arch {
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	unsigned long hpt_virt;
@@ -169,6 +179,10 @@ struct kvm_arch {
 	unsigned long sdr1;
 	unsigned long host_sdr1;
 	int tlbie_lock;
+	int n_rma_pages;
+	unsigned long lpcr;
+	unsigned long rmor;
+	struct kvmppc_rma_info *rma;
 	struct list_head spapr_tce_tables;
 	unsigned short last_vcpu[NR_CPUS];
 	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
@@ -295,7 +309,6 @@ struct kvm_vcpu_arch {
 	ulong guest_owned_ext;
 	ulong purr;
 	ulong spurr;
-	ulong lpcr;
 	ulong dscr;
 	ulong amr;
 	ulong uamor;

commit 371fefd6f2dc46668e00871930dde613b88d4bde
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:23:08 2011 +0000

    KVM: PPC: Allow book3s_hv guests to use SMT processor modes
    
    This lifts the restriction that book3s_hv guests can only run one
    hardware thread per core, and allows them to use up to 4 threads
    per core on POWER7.  The host still has to run single-threaded.
    
    This capability is advertised to qemu through a new KVM_CAP_PPC_SMT
    capability.  The return value of the ioctl querying this capability
    is the number of vcpus per virtual CPU core (vcore), currently 4.
    
    To use this, the host kernel should be booted with all threads
    active, and then all the secondary threads should be offlined.
    This will put the secondary threads into nap mode.  KVM will then
    wake them from nap mode and use them for running guest code (while
    they are still offline).  To wake the secondary threads, we send
    them an IPI using a new xics_wake_cpu() function, implemented in
    arch/powerpc/sysdev/xics/icp-native.c.  In other words, at this stage
    we assume that the platform has a XICS interrupt controller and
    we are using icp-native.c to drive it.  Since the woken thread will
    need to acknowledge and clear the IPI, we also export the base
    physical address of the XICS registers using kvmppc_set_xics_phys()
    for use in the low-level KVM book3s code.
    
    When a vcpu is created, it is assigned to a virtual CPU core.
    The vcore number is obtained by dividing the vcpu number by the
    number of threads per core in the host.  This number is exported
    to userspace via the KVM_CAP_PPC_SMT capability.  If qemu wishes
    to run the guest in single-threaded mode, it should make all vcpu
    numbers be multiples of the number of threads per core.
    
    We distinguish three states of a vcpu: runnable (i.e., ready to execute
    the guest), blocked (that is, idle), and busy in host.  We currently
    implement a policy that the vcore can run only when all its threads
    are runnable or blocked.  This way, if a vcpu needs to execute elsewhere
    in the kernel or in qemu, it can do so without being starved of CPU
    by the other vcpus.
    
    When a vcore starts to run, it executes in the context of one of the
    vcpu threads.  The other vcpu threads all go to sleep and stay asleep
    until something happens requiring the vcpu thread to return to qemu,
    or to wake up to run the vcore (this can happen when another vcpu
    thread goes from busy in host state to blocked).
    
    It can happen that a vcpu goes from blocked to runnable state (e.g.
    because of an interrupt), and the vcore it belongs to is already
    running.  In that case it can start to run immediately as long as
    the none of the vcpus in the vcore have started to exit the guest.
    We send the next free thread in the vcore an IPI to get it to start
    to execute the guest.  It synchronizes with the other threads via
    the vcore->entry_exit_count field to make sure that it doesn't go
    into the guest if the other vcpus are exiting by the time that it
    is ready to actually enter the guest.
    
    Note that there is no fixed relationship between the hardware thread
    number and the vcpu number.  Hardware threads are assigned to vcpus
    as they become runnable, so we will always use the lower-numbered
    hardware threads in preference to higher-numbered threads if not all
    the vcpus in the vcore are runnable, regardless of which vcpus are
    runnable.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 5616e39a7fa4..0d6d569e19c7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -25,10 +25,14 @@
 #include <linux/interrupt.h>
 #include <linux/types.h>
 #include <linux/kvm_types.h>
+#include <linux/threads.h>
+#include <linux/spinlock.h>
 #include <linux/kvm_para.h>
 #include <asm/kvm_asm.h>
+#include <asm/processor.h>
 
-#define KVM_MAX_VCPUS 1
+#define KVM_MAX_VCPUS		NR_CPUS
+#define KVM_MAX_VCORES		NR_CPUS
 #define KVM_MEMORY_SLOTS 32
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4
@@ -167,9 +171,34 @@ struct kvm_arch {
 	int tlbie_lock;
 	struct list_head spapr_tce_tables;
 	unsigned short last_vcpu[NR_CPUS];
+	struct kvmppc_vcore *vcores[KVM_MAX_VCORES];
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
 };
 
+/*
+ * Struct for a virtual core.
+ * Note: entry_exit_count combines an entry count in the bottom 8 bits
+ * and an exit count in the next 8 bits.  This is so that we can
+ * atomically increment the entry count iff the exit count is 0
+ * without taking the lock.
+ */
+struct kvmppc_vcore {
+	int n_runnable;
+	int n_blocked;
+	int num_threads;
+	int entry_exit_count;
+	int n_woken;
+	int nap_count;
+	u16 pcpu;
+	u8 vcore_running;
+	u8 in_guest;
+	struct list_head runnable_threads;
+	spinlock_t lock;
+};
+
+#define VCORE_ENTRY_COUNT(vc)	((vc)->entry_exit_count & 0xff)
+#define VCORE_EXIT_COUNT(vc)	((vc)->entry_exit_count >> 8)
+
 struct kvmppc_pte {
 	ulong eaddr;
 	u64 vpage;
@@ -365,14 +394,29 @@ struct kvm_vcpu_arch {
 	struct slb_shadow *slb_shadow;
 	struct dtl *dtl;
 	struct dtl *dtl_end;
+
+	struct kvmppc_vcore *vcore;
+	int ret;
 	int trap;
+	int state;
+	int ptid;
+	wait_queue_head_t cpu_run;
+
 	struct kvm_vcpu_arch_shared *shared;
 	unsigned long magic_page_pa; /* phys addr to map the magic page to */
 	unsigned long magic_page_ea; /* effect. addr to map the magic page to */
 
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	struct kvm_vcpu_arch_shared shregs;
+
+	struct list_head run_list;
+	struct task_struct *run_task;
+	struct kvm_run *kvm_run;
 #endif
 };
 
+#define KVMPPC_VCPU_BUSY_IN_HOST	0
+#define KVMPPC_VCPU_BLOCKED		1
+#define KVMPPC_VCPU_RUNNABLE		2
+
 #endif /* __POWERPC_KVM_HOST_H__ */

commit 54738c097163c3f01e67ccc85462b78d4d4f495f
Author: David Gibson <dwg@au1.ibm.com>
Date:   Wed Jun 29 00:22:41 2011 +0000

    KVM: PPC: Accelerate H_PUT_TCE by implementing it in real mode
    
    This improves I/O performance for guests using the PAPR
    paravirtualization interface by making the H_PUT_TCE hcall faster, by
    implementing it in real mode.  H_PUT_TCE is used for updating virtual
    IOMMU tables, and is used both for virtual I/O and for real I/O in the
    PAPR interface.
    
    Since this moves the IOMMU tables into the kernel, we define a new
    KVM_CREATE_SPAPR_TCE ioctl to allow qemu to create the tables.  The
    ioctl returns a file descriptor which can be used to mmap the newly
    created table.  The qemu driver models use them in the same way as
    userspace managed tables, but they can be updated directly by the
    guest with a real-mode H_PUT_TCE implementation, reducing the number
    of host/guest context switches during guest IO.
    
    There are certain circumstances where it is useful for userland qemu
    to write to the TCE table even if the kernel H_PUT_TCE path is used
    most of the time.  Specifically, allowing this will avoid awkwardness
    when we need to reset the table.  More importantly, we will in the
    future need to write the table in order to restore its state after a
    checkpoint resume or migration.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 6ebf1721680a..5616e39a7fa4 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -144,6 +144,14 @@ struct kvmppc_pginfo {
 	atomic_t refcnt;
 };
 
+struct kvmppc_spapr_tce_table {
+	struct list_head list;
+	struct kvm *kvm;
+	u64 liobn;
+	u32 window_size;
+	struct page *pages[0];
+};
+
 struct kvm_arch {
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	unsigned long hpt_virt;
@@ -157,6 +165,7 @@ struct kvm_arch {
 	unsigned long sdr1;
 	unsigned long host_sdr1;
 	int tlbie_lock;
+	struct list_head spapr_tce_tables;
 	unsigned short last_vcpu[NR_CPUS];
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
 };

commit a8606e20e41a8149456bafdf76ad29d47672027c
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:22:05 2011 +0000

    KVM: PPC: Handle some PAPR hcalls in the kernel
    
    This adds the infrastructure for handling PAPR hcalls in the kernel,
    either early in the guest exit path while we are still in real mode,
    or later once the MMU has been turned back on and we are in the full
    kernel context.  The advantage of handling hcalls in real mode if
    possible is that we avoid two partition switches -- and this will
    become more important when we support SMT4 guests, since a partition
    switch means we have to pull all of the threads in the core out of
    the guest.  The disadvantage is that we can only access the kernel
    linear mapping, not anything vmalloced or ioremapped, since the MMU
    is off.
    
    This also adds code to handle the following hcalls in real mode:
    
    H_ENTER       Add an HPTE to the hashed page table
    H_REMOVE      Remove an HPTE from the hashed page table
    H_READ        Read HPTEs from the hashed page table
    H_PROTECT     Change the protection bits in an HPTE
    H_BULK_REMOVE Remove up to 4 HPTEs from the hashed page table
    H_SET_DABR    Set the data address breakpoint register
    
    Plus code to handle the following hcalls in the kernel:
    
    H_CEDE        Idle the vcpu until an interrupt or H_PROD hcall arrives
    H_PROD        Wake up a ceded vcpu
    H_REGISTER_VPA Register a virtual processor area (VPA)
    
    The code that runs in real mode has to be in the base kernel, not in
    the module, if KVM is compiled as a module.  The real-mode code can
    only access the kernel linear mapping, not vmalloc or ioremap space.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 4a3f790d5fc4..6ebf1721680a 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -59,6 +59,10 @@ struct kvm;
 struct kvm_run;
 struct kvm_vcpu;
 
+struct lppaca;
+struct slb_shadow;
+struct dtl;
+
 struct kvm_vm_stat {
 	u32 remote_tlb_flush;
 };
@@ -344,7 +348,14 @@ struct kvm_vcpu_arch {
 	u64 dec_expires;
 	unsigned long pending_exceptions;
 	u16 last_cpu;
+	u8 ceded;
+	u8 prodded;
 	u32 last_inst;
+
+	struct lppaca *vpa;
+	struct slb_shadow *slb_shadow;
+	struct dtl *dtl;
+	struct dtl *dtl_end;
 	int trap;
 	struct kvm_vcpu_arch_shared *shared;
 	unsigned long magic_page_pa; /* phys addr to map the magic page to */

commit de56a948b9182fbcf92cb8212f114de096c2d574
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:21:34 2011 +0000

    KVM: PPC: Add support for Book3S processors in hypervisor mode
    
    This adds support for KVM running on 64-bit Book 3S processors,
    specifically POWER7, in hypervisor mode.  Using hypervisor mode means
    that the guest can use the processor's supervisor mode.  That means
    that the guest can execute privileged instructions and access privileged
    registers itself without trapping to the host.  This gives excellent
    performance, but does mean that KVM cannot emulate a processor
    architecture other than the one that the hardware implements.
    
    This code assumes that the guest is running paravirtualized using the
    PAPR (Power Architecture Platform Requirements) interface, which is the
    interface that IBM's PowerVM hypervisor uses.  That means that existing
    Linux distributions that run on IBM pSeries machines will also run
    under KVM without modification.  In order to communicate the PAPR
    hypercalls to qemu, this adds a new KVM_EXIT_PAPR_HCALL exit code
    to include/linux/kvm.h.
    
    Currently the choice between book3s_hv support and book3s_pr support
    (i.e. the existing code, which runs the guest in user mode) has to be
    made at kernel configuration time, so a given kernel binary can only
    do one or the other.
    
    This new book3s_hv code doesn't support MMIO emulation at present.
    Since we are running paravirtualized guests, this isn't a serious
    restriction.
    
    With the guest running in supervisor mode, most exceptions go straight
    to the guest.  We will never get data or instruction storage or segment
    interrupts, alignment interrupts, decrementer interrupts, program
    interrupts, single-step interrupts, etc., coming to the hypervisor from
    the guest.  Therefore this introduces a new KVMTEST_NONHV macro for the
    exception entry path so that we don't have to do the KVM test on entry
    to those exception handlers.
    
    We do however get hypervisor decrementer, hypervisor data storage,
    hypervisor instruction storage, and hypervisor emulation assist
    interrupts, so we have to handle those.
    
    In hypervisor mode, real-mode accesses can access all of RAM, not just
    a limited amount.  Therefore we put all the guest state in the vcpu.arch
    and use the shadow_vcpu in the PACA only for temporary scratch space.
    We allocate the vcpu with kzalloc rather than vzalloc, and we don't use
    anything in the kvmppc_vcpu_book3s struct, so we don't allocate it.
    We don't have a shared page with the guest, but we still need a
    kvm_vcpu_arch_shared struct to store the values of various registers,
    so we include one in the vcpu_arch struct.
    
    The POWER7 processor has a restriction that all threads in a core have
    to be in the same partition.  MMU-on kernel code counts as a partition
    (partition 0), so we have to do a partition switch on every entry to and
    exit from the guest.  At present we require the host and guest to run
    in single-thread mode because of this hardware restriction.
    
    This code allocates a hashed page table for the guest and initializes
    it with HPTEs for the guest's Virtual Real Memory Area (VRMA).  We
    require that the guest memory is allocated using 16MB huge pages, in
    order to simplify the low-level memory management.  This also means that
    we can get away without tracking paging activity in the host for now,
    since huge pages can't be paged or swapped.
    
    This also adds a few new exports needed by the book3s_hv code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 069eb9fc6c41..4a3f790d5fc4 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -33,7 +33,9 @@
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4
 
+#ifdef CONFIG_KVM_MMIO
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
+#endif
 
 /* We don't currently support large pages. */
 #define KVM_HPAGE_GFN_SHIFT(x)	0
@@ -133,7 +135,26 @@ struct kvmppc_exit_timing {
 	};
 };
 
+struct kvmppc_pginfo {
+	unsigned long pfn;
+	atomic_t refcnt;
+};
+
 struct kvm_arch {
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+	unsigned long hpt_virt;
+	unsigned long ram_npages;
+	unsigned long ram_psize;
+	unsigned long ram_porder;
+	struct kvmppc_pginfo *ram_pginfo;
+	unsigned int lpid;
+	unsigned int host_lpid;
+	unsigned long host_lpcr;
+	unsigned long sdr1;
+	unsigned long host_sdr1;
+	int tlbie_lock;
+	unsigned short last_vcpu[NR_CPUS];
+#endif /* CONFIG_KVM_BOOK3S_64_HV */
 };
 
 struct kvmppc_pte {
@@ -190,7 +211,7 @@ struct kvm_vcpu_arch {
 	ulong rmcall;
 	ulong host_paca_phys;
 	struct kvmppc_slb slb[64];
-	int slb_max;		/* # valid entries in slb[] */
+	int slb_max;		/* 1 + index of last valid entry in slb[] */
 	int slb_nr;		/* total number of entries in SLB */
 	struct kvmppc_mmu mmu;
 #endif
@@ -212,7 +233,7 @@ struct kvm_vcpu_arch {
 #endif
 
 #ifdef CONFIG_VSX
-	u64 vsr[32];
+	u64 vsr[64];
 #endif
 
 #ifdef CONFIG_PPC_BOOK3S
@@ -220,18 +241,24 @@ struct kvm_vcpu_arch {
 	u32 qpr[32];
 #endif
 
-#ifdef CONFIG_BOOKE
 	ulong pc;
 	ulong ctr;
 	ulong lr;
 
 	ulong xer;
 	u32 cr;
-#endif
 
 #ifdef CONFIG_PPC_BOOK3S
 	ulong hflags;
 	ulong guest_owned_ext;
+	ulong purr;
+	ulong spurr;
+	ulong lpcr;
+	ulong dscr;
+	ulong amr;
+	ulong uamor;
+	u32 ctrl;
+	ulong dabr;
 #endif
 	u32 vrsave; /* also USPRG0 */
 	u32 mmucr;
@@ -270,6 +297,9 @@ struct kvm_vcpu_arch {
 	u32 dbcr1;
 	u32 dbsr;
 
+	u64 mmcr[3];
+	u32 pmc[6];
+
 #ifdef CONFIG_KVM_EXIT_TIMING
 	struct mutex exit_timing_lock;
 	struct kvmppc_exit_timing timing_exit;
@@ -284,8 +314,12 @@ struct kvm_vcpu_arch {
 	struct dentry *debugfs_exit_timing;
 #endif
 
+#ifdef CONFIG_PPC_BOOK3S
+	ulong fault_dar;
+	u32 fault_dsisr;
+#endif
+
 #ifdef CONFIG_BOOKE
-	u32 last_inst;
 	ulong fault_dear;
 	ulong fault_esr;
 	ulong queued_dear;
@@ -300,16 +334,25 @@ struct kvm_vcpu_arch {
 	u8 dcr_is_write;
 	u8 osi_needed;
 	u8 osi_enabled;
+	u8 hcall_needed;
 
 	u32 cpr0_cfgaddr; /* holds the last set cpr0_cfgaddr */
 
 	struct hrtimer dec_timer;
 	struct tasklet_struct tasklet;
 	u64 dec_jiffies;
+	u64 dec_expires;
 	unsigned long pending_exceptions;
+	u16 last_cpu;
+	u32 last_inst;
+	int trap;
 	struct kvm_vcpu_arch_shared *shared;
 	unsigned long magic_page_pa; /* phys addr to map the magic page to */
 	unsigned long magic_page_ea; /* effect. addr to map the magic page to */
+
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+	struct kvm_vcpu_arch_shared shregs;
+#endif
 };
 
 #endif /* __POWERPC_KVM_HOST_H__ */

commit c4befc58a0cc5a8cc5b4a7234d67b6b16dec4e70
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:17:33 2011 +0000

    KVM: PPC: Move fields between struct kvm_vcpu_arch and kvmppc_vcpu_book3s
    
    This moves the slb field, which represents the state of the emulated
    SLB, from the kvmppc_vcpu_book3s struct to the kvm_vcpu_arch, and the
    hpte_hash_[v]pte[_long] fields from kvm_vcpu_arch to kvmppc_vcpu_book3s.
    This is in accord with the principle that the kvm_vcpu_arch struct
    represents the state of the emulated CPU, and the kvmppc_vcpu_book3s
    struct holds the auxiliary data structures used in the emulation.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 6e05b2d13683..069eb9fc6c41 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -163,16 +163,18 @@ struct kvmppc_mmu {
 	bool (*is_dcbz32)(struct kvm_vcpu *vcpu);
 };
 
-struct hpte_cache {
-	struct hlist_node list_pte;
-	struct hlist_node list_pte_long;
-	struct hlist_node list_vpte;
-	struct hlist_node list_vpte_long;
-	struct rcu_head rcu_head;
-	u64 host_va;
-	u64 pfn;
-	ulong slot;
-	struct kvmppc_pte pte;
+struct kvmppc_slb {
+	u64 esid;
+	u64 vsid;
+	u64 orige;
+	u64 origv;
+	bool valid	: 1;
+	bool Ks		: 1;
+	bool Kp		: 1;
+	bool nx		: 1;
+	bool large	: 1;	/* PTEs are 16MB */
+	bool tb		: 1;	/* 1TB segment */
+	bool class	: 1;
 };
 
 struct kvm_vcpu_arch {
@@ -187,6 +189,9 @@ struct kvm_vcpu_arch {
 	ulong highmem_handler;
 	ulong rmcall;
 	ulong host_paca_phys;
+	struct kvmppc_slb slb[64];
+	int slb_max;		/* # valid entries in slb[] */
+	int slb_nr;		/* total number of entries in SLB */
 	struct kvmppc_mmu mmu;
 #endif
 
@@ -305,15 +310,6 @@ struct kvm_vcpu_arch {
 	struct kvm_vcpu_arch_shared *shared;
 	unsigned long magic_page_pa; /* phys addr to map the magic page to */
 	unsigned long magic_page_ea; /* effect. addr to map the magic page to */
-
-#ifdef CONFIG_PPC_BOOK3S
-	struct hlist_head hpte_hash_pte[HPTEG_HASH_NUM_PTE];
-	struct hlist_head hpte_hash_pte_long[HPTEG_HASH_NUM_PTE_LONG];
-	struct hlist_head hpte_hash_vpte[HPTEG_HASH_NUM_VPTE];
-	struct hlist_head hpte_hash_vpte_long[HPTEG_HASH_NUM_VPTE_LONG];
-	int hpte_cache_count;
-	spinlock_t mmu_lock;
-#endif
 };
 
 #endif /* __POWERPC_KVM_HOST_H__ */

commit dd9ebf1f94354b010f2ac7a98bf69168636cb08e
Author: Liu Yu <yu.liu@freescale.com>
Date:   Tue Jun 14 18:35:14 2011 -0500

    KVM: PPC: e500: Add shadow PID support
    
    Dynamically assign host PIDs to guest PIDs, splitting each guest PID into
    multiple host (shadow) PIDs based on kernel/user and MSR[IS/DS].  Use
    both PID0 and PID1 so that the shadow PIDs for the right mode can be
    selected, that correspond both to guest TID = zero and guest TID = guest
    PID.
    
    This allows us to significantly reduce the frequency of needing to
    invalidate the entire TLB.  When the guest mode or PID changes, we just
    update the host PID0/PID1.  And since the allocation of shadow PIDs is
    global, multiple guests can share the TLB without conflict.
    
    Note that KVM does not yet support the guest setting PID1 or PID2 to
    a value other than zero.  This will need to be fixed for nested KVM
    to work.  Until then, we enforce the requirement for guest PID1/PID2
    to stay zero by failing the emulation if the guest tries to set them
    to something else.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index c4ce1054b866..6e05b2d13683 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -255,6 +255,7 @@ struct kvm_vcpu_arch {
 	u32 pvr;
 
 	u32 shadow_pid;
+	u32 shadow_pid1;
 	u32 pid;
 	u32 swap_pid;
 

commit 4cd35f675ba41a99a477e28a6add4a66833325f2
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Jun 14 18:34:31 2011 -0500

    KVM: PPC: e500: Save/restore SPE state
    
    This is done lazily.  The SPE save will be done only if the guest has
    used SPE since the last preemption or heavyweight exit.  Restore will be
    done only on demand, when enabling MSR_SPE in the shadow MSR, in response
    to an SPE fault or mtmsr emulation.
    
    For SPEFSCR, Linux already switches it on context switch (non-lazily), so
    the only remaining bit is to save it between qemu and the guest.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 12cb1807e8d7..c4ce1054b866 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -195,6 +195,12 @@ struct kvm_vcpu_arch {
 	u64 fpr[32];
 	u64 fpscr;
 
+#ifdef CONFIG_SPE
+	ulong evr[32];
+	ulong spefscr;
+	ulong host_spefscr;
+	u64 acc;
+#endif
 #ifdef CONFIG_ALTIVEC
 	vector128 vr[32];
 	vector128 vscr;

commit ecee273fc48f7f48f0c2f074335c43aaa790c308
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Jun 14 18:34:29 2011 -0500

    KVM: PPC: booke: use shadow_msr
    
    Keep the guest MSR and the guest-mode true MSR separate, rather than
    modifying the guest MSR on each guest entry to produce a true MSR.
    
    Any bits which should be modified based on guest MSR must be explicitly
    propagated from vcpu->arch.shared->msr to vcpu->arch.shadow_msr in
    kvmppc_set_msr().
    
    While we're modifying the guest entry code, reorder a few instructions
    to bury some load latencies.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 186f150b9b89..12cb1807e8d7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -219,12 +219,12 @@ struct kvm_vcpu_arch {
 #endif
 
 #ifdef CONFIG_PPC_BOOK3S
-	ulong shadow_msr;
 	ulong hflags;
 	ulong guest_owned_ext;
 #endif
 	u32 vrsave; /* also USPRG0 */
 	u32 mmucr;
+	ulong shadow_msr;
 	ulong sprg4;
 	ulong sprg5;
 	ulong sprg6;

commit 5ce941ee4258b836cf818d2ac159d8cf3ebad648
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Apr 27 17:24:21 2011 -0500

    KVM: PPC: booke: add sregs support
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index a16804399165..186f150b9b89 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -233,6 +233,9 @@ struct kvm_vcpu_arch {
 	ulong csrr1;
 	ulong dsrr0;
 	ulong dsrr1;
+	ulong mcsrr0;
+	ulong mcsrr1;
+	ulong mcsr;
 	ulong esr;
 	u32 dec;
 	u32 decar;

commit eab176722f4628b2d9cf76221a43dd3a0e37e632
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Apr 27 17:24:10 2011 -0500

    KVM: PPC: booke: save/restore VRSAVE (a.k.a. USPRG0)
    
    Linux doesn't use USPRG0 (now renamed VRSAVE in the architecture, even
    when Altivec isn't involved), but a guest might.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 890897cee050..a16804399165 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -223,6 +223,7 @@ struct kvm_vcpu_arch {
 	ulong hflags;
 	ulong guest_owned_ext;
 #endif
+	u32 vrsave; /* also USPRG0 */
 	u32 mmucr;
 	ulong sprg4;
 	ulong sprg5;

commit 09000adb86550d2895b64faa52e64eaec3cae7b2
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Fri Mar 25 10:32:13 2011 +0530

    KVM: PPC: Fix issue clearing exit timing counters
    
    Following dump is observed on host when clearing the exit timing counters
    
    [root@p1021mds kvm]# echo -n 'c' > vm1200_vcpu0_timing
    INFO: task echo:1276 blocked for more than 120 seconds.
    "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    echo          D 0ff5bf94     0  1276   1190 0x00000000
    Call Trace:
    [c2157e40] [c0007908] __switch_to+0x9c/0xc4
    [c2157e50] [c040293c] schedule+0x1b4/0x3bc
    [c2157e90] [c04032dc] __mutex_lock_slowpath+0x74/0xc0
    [c2157ec0] [c00369e4] kvmppc_init_timing_stats+0x20/0xb8
    [c2157ed0] [c0036b00] kvmppc_exit_timing_write+0x84/0x98
    [c2157ef0] [c00b9f90] vfs_write+0xc0/0x16c
    [c2157f10] [c00ba284] sys_write+0x4c/0x90
    [c2157f40] [c000e320] ret_from_syscall+0x0/0x3c
    
            The vcpu->mutex is used by kvm_ioctl_* (KVM_RUN etc) and same was
    used when clearing the stats (in kvmppc_init_timing_stats()). What happens
    is that when the guest is idle then it held the vcpu->mutx. While the
    exiting timing process waits for guest to release the vcpu->mutex and
    a hang state is reached.
    
            Now using seprate lock for exit timing stats.
    
    Signed-off-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Acked-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index bba3b9b72a39..890897cee050 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -255,6 +255,7 @@ struct kvm_vcpu_arch {
 	u32 dbsr;
 
 #ifdef CONFIG_KVM_EXIT_TIMING
+	struct mutex exit_timing_lock;
 	struct kvmppc_exit_timing timing_exit;
 	struct kvmppc_exit_timing timing_last_enter;
 	u32 last_exit_type;

commit 2d27fc5eac0205588cb59ae138062e5e96695276
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 15:04:19 2010 +0200

    KVM: PPC: Add book3s_32 tlbie flush acceleration
    
    On Book3s_32 the tlbie instruction flushed effective addresses by the mask
    0x0ffff000. This is pretty hard to reflect with a hash that hashes ~0xfff, so
    to speed up that target we should also keep a special hash around for it.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index fafc71aa3343..bba3b9b72a39 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -42,9 +42,11 @@
 
 #define HPTEG_CACHE_NUM			(1 << 15)
 #define HPTEG_HASH_BITS_PTE		13
+#define HPTEG_HASH_BITS_PTE_LONG	12
 #define HPTEG_HASH_BITS_VPTE		13
 #define HPTEG_HASH_BITS_VPTE_LONG	5
 #define HPTEG_HASH_NUM_PTE		(1 << HPTEG_HASH_BITS_PTE)
+#define HPTEG_HASH_NUM_PTE_LONG		(1 << HPTEG_HASH_BITS_PTE_LONG)
 #define HPTEG_HASH_NUM_VPTE		(1 << HPTEG_HASH_BITS_VPTE)
 #define HPTEG_HASH_NUM_VPTE_LONG	(1 << HPTEG_HASH_BITS_VPTE_LONG)
 
@@ -163,6 +165,7 @@ struct kvmppc_mmu {
 
 struct hpte_cache {
 	struct hlist_node list_pte;
+	struct hlist_node list_pte_long;
 	struct hlist_node list_vpte;
 	struct hlist_node list_vpte_long;
 	struct rcu_head rcu_head;
@@ -293,6 +296,7 @@ struct kvm_vcpu_arch {
 
 #ifdef CONFIG_PPC_BOOK3S
 	struct hlist_head hpte_hash_pte[HPTEG_HASH_NUM_PTE];
+	struct hlist_head hpte_hash_pte_long[HPTEG_HASH_NUM_PTE_LONG];
 	struct hlist_head hpte_hash_vpte[HPTEG_HASH_NUM_VPTE];
 	struct hlist_head hpte_hash_vpte_long[HPTEG_HASH_NUM_VPTE_LONG];
 	int hpte_cache_count;

commit 2e0908afaf03675d22e40ce45a66b8d2070214ac
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 15:04:17 2010 +0200

    KVM: PPC: RCU'ify the Book3s MMU
    
    So far we've been running all code without locking of any sort. This wasn't
    really an issue because I didn't see any parallel access to the shadow MMU
    code coming.
    
    But then I started to implement dirty bitmapping to MOL which has the video
    code in its own thread, so suddenly we had the dirty bitmap code run in
    parallel to the shadow mmu code. And with that came trouble.
    
    So I went ahead and made the MMU modifying functions as parallelizable as
    I could think of. I hope I didn't screw up too much RCU logic :-). If you
    know your way around RCU and locking and what needs to be done when, please
    take a look at this patch.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e1da77579e65..fafc71aa3343 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -165,6 +165,7 @@ struct hpte_cache {
 	struct hlist_node list_pte;
 	struct hlist_node list_vpte;
 	struct hlist_node list_vpte_long;
+	struct rcu_head rcu_head;
 	u64 host_va;
 	u64 pfn;
 	ulong slot;
@@ -295,6 +296,7 @@ struct kvm_vcpu_arch {
 	struct hlist_head hpte_hash_vpte[HPTEG_HASH_NUM_VPTE];
 	struct hlist_head hpte_hash_vpte_long[HPTEG_HASH_NUM_VPTE_LONG];
 	int hpte_cache_count;
+	spinlock_t mmu_lock;
 #endif
 };
 

commit beb03f14da9ceff76ff08cbb8af064b52dc21f7e
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:53 2010 +0200

    KVM: PPC: First magic page steps
    
    We will be introducing a method to project the shared page in guest context.
    As soon as we're talking about this coupling, the shared page is colled magic
    page.
    
    This patch introduces simple defines, so the follow-up patches are easier to
    read.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1674da8134cb..e1da77579e65 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -287,6 +287,8 @@ struct kvm_vcpu_arch {
 	u64 dec_jiffies;
 	unsigned long pending_exceptions;
 	struct kvm_vcpu_arch_shared *shared;
+	unsigned long magic_page_pa; /* phys addr to map the magic page to */
+	unsigned long magic_page_ea; /* effect. addr to map the magic page to */
 
 #ifdef CONFIG_PPC_BOOK3S
 	struct hlist_head hpte_hash_pte[HPTEG_HASH_NUM_PTE];

commit 28e83b4fa7f8bd114940fa933ac8cbe80969eba2
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:52 2010 +0200

    KVM: PPC: Make PAM a define
    
    On PowerPC it's very normal to not support all of the physical RAM in real mode.
    To check if we're matching on the shared page or not, we need to know the limits
    so we can restrain ourselves to that range.
    
    So let's make it a define instead of open-coding it. And while at it, let's also
    increase it.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    
    v2 -> v3:
    
      - RMO -> PAM (non-magic page)
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 221cf85e9a6e..1674da8134cb 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -48,6 +48,9 @@
 #define HPTEG_HASH_NUM_VPTE		(1 << HPTEG_HASH_BITS_VPTE)
 #define HPTEG_HASH_NUM_VPTE_LONG	(1 << HPTEG_HASH_BITS_VPTE_LONG)
 
+/* Physical Address Mask - allowed range of real mode RAM access */
+#define KVM_PAM			0x0fffffffffffffffULL
+
 struct kvm;
 struct kvm_run;
 struct kvm_vcpu;

commit a73a9599e03eef1324d5aeecaebc1b339d2e1664
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:47 2010 +0200

    KVM: PPC: Convert SPRG[0-4] to shared page
    
    When in kernel mode there are 4 additional registers available that are
    simple data storage. Instead of exiting to the hypervisor to read and
    write those, we can just share them with the guest using the page.
    
    This patch converts all users of the current field to the shared page.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 5255d754f9a9..221cf85e9a6e 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -217,10 +217,6 @@ struct kvm_vcpu_arch {
 	ulong guest_owned_ext;
 #endif
 	u32 mmucr;
-	ulong sprg0;
-	ulong sprg1;
-	ulong sprg2;
-	ulong sprg3;
 	ulong sprg4;
 	ulong sprg5;
 	ulong sprg6;

commit de7906c36ca1e22a3e3600e95c6a4e2c1e4e2e9c
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:46 2010 +0200

    KVM: PPC: Convert SRR0 and SRR1 to shared page
    
    The SRR0 and SRR1 registers contain cached values of the PC and MSR
    respectively. They get written to by the hypervisor when an interrupt
    occurs or directly by the kernel. They are also used to tell the rfi(d)
    instruction where to jump to.
    
    Because it only gets touched on defined events that, it's very simple to
    share with the guest. Hypervisor and guest both have full r/w access.
    
    This patch converts all users of the current field to the shared page.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index c852408eac38..5255d754f9a9 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -225,8 +225,6 @@ struct kvm_vcpu_arch {
 	ulong sprg5;
 	ulong sprg6;
 	ulong sprg7;
-	ulong srr0;
-	ulong srr1;
 	ulong csrr0;
 	ulong csrr1;
 	ulong dsrr0;

commit 5e030186dfc4e4e47c84d2557b17e4aa06c76f96
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:45 2010 +0200

    KVM: PPC: Convert DAR to shared page.
    
    The DAR register contains the address a data page fault occured at. This
    register behaves pretty much like a simple data storage register that gets
    written to on data faults. There is no hypervisor interaction required on
    read or write.
    
    This patch converts all users of the current field to the shared page.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index ba20f90655f3..c852408eac38 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -231,7 +231,6 @@ struct kvm_vcpu_arch {
 	ulong csrr1;
 	ulong dsrr0;
 	ulong dsrr1;
-	ulong dear;
 	ulong esr;
 	u32 dec;
 	u32 decar;

commit 666e7252a15b7fc4a116e65deaf6da5e4ce660e3
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:43 2010 +0200

    KVM: PPC: Convert MSR to shared page
    
    One of the most obvious registers to share with the guest directly is the
    MSR. The MSR contains the "interrupts enabled" flag which the guest has to
    toggle in critical sections.
    
    So in order to bring the overhead of interrupt en- and disabling down, let's
    put msr into the shared page. Keep in mind that even though you can fully read
    its contents, writing to it doesn't always update all state. There are a few
    safe fields that don't require hypervisor interaction. See the documentation
    for a list of MSR bits that are safe to be set from inside the guest.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 53edacdf6940..ba20f90655f3 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -211,7 +211,6 @@ struct kvm_vcpu_arch {
 	u32 cr;
 #endif
 
-	ulong msr;
 #ifdef CONFIG_PPC_BOOK3S
 	ulong shadow_msr;
 	ulong hflags;

commit 96bc451a153297bf1f99ef2d633d512ea349ae7a
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:42 2010 +0200

    KVM: PPC: Introduce shared page
    
    For transparent variable sharing between the hypervisor and guest, I introduce
    a shared page. This shared page will contain all the registers the guest can
    read and write safely without exiting guest context.
    
    This patch only implements the stubs required for the basic structure of the
    shared page. The actual register moving follows.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index b0b23c007d6e..53edacdf6940 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -25,6 +25,7 @@
 #include <linux/interrupt.h>
 #include <linux/types.h>
 #include <linux/kvm_types.h>
+#include <linux/kvm_para.h>
 #include <asm/kvm_asm.h>
 
 #define KVM_MAX_VCPUS 1
@@ -290,6 +291,7 @@ struct kvm_vcpu_arch {
 	struct tasklet_struct tasklet;
 	u64 dec_jiffies;
 	unsigned long pending_exceptions;
+	struct kvm_vcpu_arch_shared *shared;
 
 #ifdef CONFIG_PPC_BOOK3S
 	struct hlist_head hpte_hash_pte[HPTEG_HASH_NUM_PTE];

commit 828554136bbacae6e39fc31b9cd7e7c660ad7530
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Thu Jul 1 16:00:11 2010 +0200

    KVM: Remove unnecessary divide operations
    
    This patch converts unnecessary divide and modulo operations
    in the KVM large page related code into logical operations.
    This allows to convert gfn_t to u64 while not breaking 32
    bit builds.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index e004eafcd3f0..b0b23c007d6e 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -35,6 +35,7 @@
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 
 /* We don't currently support large pages. */
+#define KVM_HPAGE_GFN_SHIFT(x)	0
 #define KVM_NR_PAGE_SIZES	1
 #define KVM_PAGES_PER_HPAGE(x)	(1UL<<31)
 

commit fef093bec0364ff5e6fd488cd81637f6bb3a2d0d
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Jun 30 15:18:46 2010 +0200

    KVM: PPC: Make use of hash based Shadow MMU
    
    We just introduced generic functions to handle shadow pages on PPC.
    This patch makes the respective backends make use of them, getting
    rid of a lot of duplicate code along the way.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0c9ad869decd..e004eafcd3f0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -38,7 +38,13 @@
 #define KVM_NR_PAGE_SIZES	1
 #define KVM_PAGES_PER_HPAGE(x)	(1UL<<31)
 
-#define HPTEG_CACHE_NUM 1024
+#define HPTEG_CACHE_NUM			(1 << 15)
+#define HPTEG_HASH_BITS_PTE		13
+#define HPTEG_HASH_BITS_VPTE		13
+#define HPTEG_HASH_BITS_VPTE_LONG	5
+#define HPTEG_HASH_NUM_PTE		(1 << HPTEG_HASH_BITS_PTE)
+#define HPTEG_HASH_NUM_VPTE		(1 << HPTEG_HASH_BITS_VPTE)
+#define HPTEG_HASH_NUM_VPTE_LONG	(1 << HPTEG_HASH_BITS_VPTE_LONG)
 
 struct kvm;
 struct kvm_run;
@@ -151,6 +157,9 @@ struct kvmppc_mmu {
 };
 
 struct hpte_cache {
+	struct hlist_node list_pte;
+	struct hlist_node list_vpte;
+	struct hlist_node list_vpte_long;
 	u64 host_va;
 	u64 pfn;
 	ulong slot;
@@ -282,8 +291,10 @@ struct kvm_vcpu_arch {
 	unsigned long pending_exceptions;
 
 #ifdef CONFIG_PPC_BOOK3S
-	struct hpte_cache hpte_cache[HPTEG_CACHE_NUM];
-	int hpte_cache_offset;
+	struct hlist_head hpte_hash_pte[HPTEG_HASH_NUM_PTE];
+	struct hlist_head hpte_hash_vpte[HPTEG_HASH_NUM_VPTE];
+	struct hlist_head hpte_hash_vpte_long[HPTEG_HASH_NUM_VPTE_LONG];
+	int hpte_cache_count;
 #endif
 };
 

commit af7b4d104b36e782a5a97dd55958c3c63964e088
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Apr 20 02:49:46 2010 +0200

    KVM: PPC: Convert u64 -> ulong
    
    There are some pieces in the code that I overlooked that still use
    u64s instead of longs. This slows down 32 bit hosts unnecessarily, so
    let's just move them to ulong.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 5a83995105f8..0c9ad869decd 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -124,9 +124,9 @@ struct kvm_arch {
 };
 
 struct kvmppc_pte {
-	u64 eaddr;
+	ulong eaddr;
 	u64 vpage;
-	u64 raddr;
+	ulong raddr;
 	bool may_read		: 1;
 	bool may_write		: 1;
 	bool may_execute	: 1;
@@ -145,7 +145,7 @@ struct kvmppc_mmu {
 	int  (*xlate)(struct kvm_vcpu *vcpu, gva_t eaddr, struct kvmppc_pte *pte, bool data);
 	void (*reset_msr)(struct kvm_vcpu *vcpu);
 	void (*tlbie)(struct kvm_vcpu *vcpu, ulong addr, bool large);
-	int  (*esid_to_vsid)(struct kvm_vcpu *vcpu, u64 esid, u64 *vsid);
+	int  (*esid_to_vsid)(struct kvm_vcpu *vcpu, ulong esid, u64 *vsid);
 	u64  (*ea_to_vp)(struct kvm_vcpu *vcpu, gva_t eaddr, bool data);
 	bool (*is_dcbz32)(struct kvm_vcpu *vcpu);
 };

commit 0604675fe17f68741730cebe74422605bb79d972
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:44 2010 +0200

    KVM: PPC: Use now shadowed vcpu fields
    
    The shadow vcpu now contains some fields we don't use from the vcpu anymore.
    Access to them happens using inline functions that happily use the shadow
    vcpu fields.
    
    So let's now ifdef them out to booke only and add asm-offsets.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 22801f802312..5a83995105f8 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -191,11 +191,11 @@ struct kvm_vcpu_arch {
 	u32 qpr[32];
 #endif
 
+#ifdef CONFIG_BOOKE
 	ulong pc;
 	ulong ctr;
 	ulong lr;
 
-#ifdef CONFIG_BOOKE
 	ulong xer;
 	u32 cr;
 #endif
@@ -203,7 +203,6 @@ struct kvm_vcpu_arch {
 	ulong msr;
 #ifdef CONFIG_PPC_BOOK3S
 	ulong shadow_msr;
-	ulong shadow_srr1;
 	ulong hflags;
 	ulong guest_owned_ext;
 #endif
@@ -258,14 +257,13 @@ struct kvm_vcpu_arch {
 	struct dentry *debugfs_exit_timing;
 #endif
 
+#ifdef CONFIG_BOOKE
 	u32 last_inst;
-#ifdef CONFIG_PPC64
-	u32 fault_dsisr;
-#endif
 	ulong fault_dear;
 	ulong fault_esr;
 	ulong queued_dear;
 	ulong queued_esr;
+#endif
 	gpa_t paddr_accessed;
 
 	u8 io_gpr; /* GPR used as IO source/target */

commit 00c3a37ca332f54f2187720e51f7c0e18e91d7c9
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:42 2010 +0200

    KVM: PPC: Use CONFIG_PPC_BOOK3S define
    
    Upstream recently added a new name for PPC64: Book3S_64.
    
    So instead of using CONFIG_PPC64 we should use CONFIG_PPC_BOOK3S consotently.
    That makes understanding the code easier (I hope).
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 5869a487e2e0..22801f802312 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -66,7 +66,7 @@ struct kvm_vcpu_stat {
 	u32 dec_exits;
 	u32 ext_intr_exits;
 	u32 halt_wakeup;
-#ifdef CONFIG_PPC64
+#ifdef CONFIG_PPC_BOOK3S
 	u32 pf_storage;
 	u32 pf_instruc;
 	u32 sp_storage;
@@ -160,7 +160,7 @@ struct hpte_cache {
 struct kvm_vcpu_arch {
 	ulong host_stack;
 	u32 host_pid;
-#ifdef CONFIG_PPC64
+#ifdef CONFIG_PPC_BOOK3S
 	ulong host_msr;
 	ulong host_r2;
 	void *host_retip;
@@ -201,7 +201,7 @@ struct kvm_vcpu_arch {
 #endif
 
 	ulong msr;
-#ifdef CONFIG_PPC64
+#ifdef CONFIG_PPC_BOOK3S
 	ulong shadow_msr;
 	ulong shadow_srr1;
 	ulong hflags;
@@ -283,7 +283,7 @@ struct kvm_vcpu_arch {
 	u64 dec_jiffies;
 	unsigned long pending_exceptions;
 
-#ifdef CONFIG_PPC64
+#ifdef CONFIG_PPC_BOOK3S
 	struct hpte_cache hpte_cache[HPTEG_CACHE_NUM];
 	int hpte_cache_offset;
 #endif

commit 3ed9c6d2b5aa0ac365c52a2a3a370ac499f21e45
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Mar 24 21:48:36 2010 +0100

    KVM: PPC: Make bools bitfields
    
    Bool defaults to at least byte width. We usually only want to waste a single
    bit on this. So let's move all the bool values to bitfields, potentially
    saving memory.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 486f1cafd5f7..5869a487e2e0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -127,9 +127,9 @@ struct kvmppc_pte {
 	u64 eaddr;
 	u64 vpage;
 	u64 raddr;
-	bool may_read;
-	bool may_write;
-	bool may_execute;
+	bool may_read		: 1;
+	bool may_write		: 1;
+	bool may_execute	: 1;
 };
 
 struct kvmppc_mmu {

commit ad0a048b096ac819f28667602285453468a8d8f9
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Mar 24 21:48:30 2010 +0100

    KVM: PPC: Add OSI hypercall interface
    
    MOL uses its own hypercall interface to call back into userspace when
    the guest wants to do something.
    
    So let's implement that as an exit reason, specify it with a CAP and
    only really use it when userspace wants us to.
    
    The only user of it so far is MOL.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 0ebda67ad6a8..486f1cafd5f7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -273,6 +273,8 @@ struct kvm_vcpu_arch {
 	u8 mmio_sign_extend;
 	u8 dcr_needed;
 	u8 dcr_is_write;
+	u8 osi_needed;
+	u8 osi_enabled;
 
 	u32 cpr0_cfgaddr; /* holds the last set cpr0_cfgaddr */
 

commit c8027f165228b4c62bad31609d5c9e98ddfb8ef6
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Mar 24 21:48:19 2010 +0100

    KVM: PPC: Make DSISR 32 bits wide
    
    DSISR is only defined as 32 bits wide. So let's reflect that in the
    structs too.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 119deb4750d9..0ebda67ad6a8 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -260,7 +260,7 @@ struct kvm_vcpu_arch {
 
 	u32 last_inst;
 #ifdef CONFIG_PPC64
-	ulong fault_dsisr;
+	u32 fault_dsisr;
 #endif
 	ulong fault_dear;
 	ulong fault_esr;

commit 3587d5348ced089666c51411bd9d771fb0b072cf
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Feb 19 11:00:30 2010 +0100

    KVM: PPC: Teach MMIO Signedness
    
    The guest I was trying to get to run uses the LHA and LHAU instructions.
    Those instructions basically do a load, but also sign extend the result.
    
    Since we need to fill our registers by hand when doing MMIO, we also need
    to sign extend manually.
    
    This patch implements sign extended MMIO and the LHA(U) instructions.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index fb87dcf418bf..119deb4750d9 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -270,6 +270,7 @@ struct kvm_vcpu_arch {
 
 	u8 io_gpr; /* GPR used as IO source/target */
 	u8 mmio_is_bigendian;
+	u8 mmio_sign_extend;
 	u8 dcr_needed;
 	u8 dcr_is_write;
 

commit c62e096dec032c82bae60545623c24743116f5dd
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Feb 19 11:00:28 2010 +0100

    KVM: PPC: Make fpscr 64-bit
    
    Modern PowerPCs have a 64 bit wide FPSCR register. Let's accomodate for that
    and make it 64 bits in our vcpu struct too.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 4b14dcd4874b..fb87dcf418bf 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -175,7 +175,7 @@ struct kvm_vcpu_arch {
 	ulong gpr[32];
 
 	u64 fpr[32];
-	u32 fpscr;
+	u64 fpscr;
 
 #ifdef CONFIG_ALTIVEC
 	vector128 vr[32];

commit 5aa9e2f43aedff25e735b4c44e0f0f6e5d1a40ae
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Feb 19 11:00:27 2010 +0100

    KVM: PPC: Add QPR registers
    
    The Gekko has GPRs, SPRs and FPRs like normal PowerPC codes, but
    it also has QPRs which are basically single precision only FPU registers
    that get used when in paired single mode.
    
    The following patches depend on them being around, so let's add the
    definitions early.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 5e5bae7e152f..4b14dcd4874b 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -186,6 +186,11 @@ struct kvm_vcpu_arch {
 	u64 vsr[32];
 #endif
 
+#ifdef CONFIG_PPC_BOOK3S
+	/* For Gekko paired singles */
+	u32 qpr[32];
+#endif
+
 	ulong pc;
 	ulong ctr;
 	ulong lr;

commit daf5e27109c8c16c987e955cc6abbbc0af050edd
Author: Liu Yu <yu.liu@freescale.com>
Date:   Tue Feb 2 19:44:35 2010 +0800

    KVM: ppc/booke: Set ESR and DEAR when inject interrupt to guest
    
    Old method prematurely sets ESR and DEAR.
    Move this part after we decide to inject interrupt,
    which is more like hardware behave.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Acked-by: Hollis Blanchard <hollis@penguinppc.org>
    Acked-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 715aa6baf6f0..5e5bae7e152f 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -259,6 +259,8 @@ struct kvm_vcpu_arch {
 #endif
 	ulong fault_dear;
 	ulong fault_esr;
+	ulong queued_dear;
+	ulong queued_esr;
 	gpa_t paddr_accessed;
 
 	u8 io_gpr; /* GPR used as IO source/target */

commit f7adbba1e5d464b0d449adac1eb2519be6be9728
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 15 14:49:13 2010 +0100

    KVM: PPC: Keep SRR1 flags around in shadow_msr
    
    SRR1 stores more information that just the MSR value. It also stores
    valuable information about the type of interrupt we received, for
    example whether the storage interrupt we just got was because of a
    missing htab entry or not.
    
    We use that information to speed up the exit path.
    
    Now if we get preempted before we can interpret the shadow_msr values,
    we get into vcpu_put which then calls the MSR handler, which then sets
    all the SRR1 information bits in shadow_msr to 0. Great.
    
    So let's preserve the SRR1 specific bits in shadow_msr whenever we set
    the MSR. They don't hurt.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index c30a70c6b931..715aa6baf6f0 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -198,6 +198,7 @@ struct kvm_vcpu_arch {
 	ulong msr;
 #ifdef CONFIG_PPC64
 	ulong shadow_msr;
+	ulong shadow_srr1;
 	ulong hflags;
 	ulong guest_owned_ext;
 #endif

commit 180a34d2d3fda0151154f9cead4aab9dddd3d0c1
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 15 14:49:11 2010 +0100

    KVM: PPC: Add support for FPU/Altivec/VSX
    
    When our guest starts using either the FPU, Altivec or VSX we need to make
    sure Linux knows about it and sneak into its process switching code
    accordingly.
    
    This patch makes accesses to the above parts of the system work inside the
    VM.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index f7215e622dfd..c30a70c6b931 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -172,9 +172,20 @@ struct kvm_vcpu_arch {
 	struct kvmppc_mmu mmu;
 #endif
 
-	u64 fpr[32];
 	ulong gpr[32];
 
+	u64 fpr[32];
+	u32 fpscr;
+
+#ifdef CONFIG_ALTIVEC
+	vector128 vr[32];
+	vector128 vscr;
+#endif
+
+#ifdef CONFIG_VSX
+	u64 vsr[32];
+#endif
+
 	ulong pc;
 	ulong ctr;
 	ulong lr;
@@ -188,6 +199,7 @@ struct kvm_vcpu_arch {
 #ifdef CONFIG_PPC64
 	ulong shadow_msr;
 	ulong hflags;
+	ulong guest_owned_ext;
 #endif
 	u32 mmucr;
 	ulong sprg0;

commit 021ec9c69f8b7b20f46296cc76cc4cb341b25191
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 8 02:58:06 2010 +0100

    KVM: PPC: Call SLB patching code in interrupt safe manner
    
    Currently we're racy when doing the transition from IR=1 to IR=0, from
    the module memory entry code to the real mode SLB switching code.
    
    To work around that I took a look at the RTAS entry code which is faced
    with a similar problem and did the same thing:
    
      A small helper in linear mapped memory that does mtmsr with IR=0 and
      then RFIs info the actual handler.
    
    Thanks to that trick we can safely take page faults in the entry code
    and only need to be really wary of what to do as of the SLB switching
    part.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d615fa8a1412..f7215e622dfd 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -167,6 +167,7 @@ struct kvm_vcpu_arch {
 	ulong trampoline_lowmem;
 	ulong trampoline_enter;
 	ulong highmem_handler;
+	ulong rmcall;
 	ulong host_paca_phys;
 	struct kvmppc_mmu mmu;
 #endif

commit 7e57cba06074da84d7c24d8c3f44040d2d8c88ac
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 8 02:58:03 2010 +0100

    KVM: PPC: Use PACA backed shadow vcpu
    
    We're being horribly racy right now. All the entry and exit code hijacks
    random fields from the PACA that could easily be used by different code in
    case we get interrupted, for example by a #MC or even page fault.
    
    After discussing this with Ben, we figured it's best to reserve some more
    space in the PACA and just shove off some vcpu state to there.
    
    That way we can drastically improve the readability of the code, make it
    less racy and less complex.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1201f62d0d73..d615fa8a1412 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -175,10 +175,13 @@ struct kvm_vcpu_arch {
 	ulong gpr[32];
 
 	ulong pc;
-	u32 cr;
 	ulong ctr;
 	ulong lr;
+
+#ifdef CONFIG_BOOKE
 	ulong xer;
+	u32 cr;
+#endif
 
 	ulong msr;
 #ifdef CONFIG_PPC64

commit 544c6761bb05a1dd19a39cb9bed096273f9bdb36
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Nov 2 12:02:31 2009 +0000

    Use hrtimers for the decrementer
    
    Following S390's good example we should use hrtimers for the decrementer too!
    This patch converts the timer from the old mechanism to hrtimers.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 2cff5fe0cbe6..1201f62d0d73 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -21,7 +21,8 @@
 #define __POWERPC_KVM_HOST_H__
 
 #include <linux/mutex.h>
-#include <linux/timer.h>
+#include <linux/hrtimer.h>
+#include <linux/interrupt.h>
 #include <linux/types.h>
 #include <linux/kvm_types.h>
 #include <asm/kvm_asm.h>
@@ -250,7 +251,8 @@ struct kvm_vcpu_arch {
 
 	u32 cpr0_cfgaddr; /* holds the last set cpr0_cfgaddr */
 
-	struct timer_list dec_timer;
+	struct hrtimer dec_timer;
+	struct tasklet_struct tasklet;
 	u64 dec_jiffies;
 	unsigned long pending_exceptions;
 

commit ca95150b3a9f3f3146a686296f2156a7ec6e98e9
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Oct 30 05:47:04 2009 +0000

    Add Book3s fields to vcpu structs
    
    We need to store more information than we currently have for vcpus
    when running on Book3s.
    
    So let's extend the internal struct definitions.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index c9c930ed11d7..2cff5fe0cbe6 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -37,6 +37,8 @@
 #define KVM_NR_PAGE_SIZES	1
 #define KVM_PAGES_PER_HPAGE(x)	(1UL<<31)
 
+#define HPTEG_CACHE_NUM 1024
+
 struct kvm;
 struct kvm_run;
 struct kvm_vcpu;
@@ -63,6 +65,17 @@ struct kvm_vcpu_stat {
 	u32 dec_exits;
 	u32 ext_intr_exits;
 	u32 halt_wakeup;
+#ifdef CONFIG_PPC64
+	u32 pf_storage;
+	u32 pf_instruc;
+	u32 sp_storage;
+	u32 sp_instruc;
+	u32 queue_intr;
+	u32 ld;
+	u32 ld_slow;
+	u32 st;
+	u32 st_slow;
+#endif
 };
 
 enum kvm_exit_types {
@@ -109,9 +122,53 @@ struct kvmppc_exit_timing {
 struct kvm_arch {
 };
 
+struct kvmppc_pte {
+	u64 eaddr;
+	u64 vpage;
+	u64 raddr;
+	bool may_read;
+	bool may_write;
+	bool may_execute;
+};
+
+struct kvmppc_mmu {
+	/* book3s_64 only */
+	void (*slbmte)(struct kvm_vcpu *vcpu, u64 rb, u64 rs);
+	u64  (*slbmfee)(struct kvm_vcpu *vcpu, u64 slb_nr);
+	u64  (*slbmfev)(struct kvm_vcpu *vcpu, u64 slb_nr);
+	void (*slbie)(struct kvm_vcpu *vcpu, u64 slb_nr);
+	void (*slbia)(struct kvm_vcpu *vcpu);
+	/* book3s */
+	void (*mtsrin)(struct kvm_vcpu *vcpu, u32 srnum, ulong value);
+	u32  (*mfsrin)(struct kvm_vcpu *vcpu, u32 srnum);
+	int  (*xlate)(struct kvm_vcpu *vcpu, gva_t eaddr, struct kvmppc_pte *pte, bool data);
+	void (*reset_msr)(struct kvm_vcpu *vcpu);
+	void (*tlbie)(struct kvm_vcpu *vcpu, ulong addr, bool large);
+	int  (*esid_to_vsid)(struct kvm_vcpu *vcpu, u64 esid, u64 *vsid);
+	u64  (*ea_to_vp)(struct kvm_vcpu *vcpu, gva_t eaddr, bool data);
+	bool (*is_dcbz32)(struct kvm_vcpu *vcpu);
+};
+
+struct hpte_cache {
+	u64 host_va;
+	u64 pfn;
+	ulong slot;
+	struct kvmppc_pte pte;
+};
+
 struct kvm_vcpu_arch {
-	u32 host_stack;
+	ulong host_stack;
 	u32 host_pid;
+#ifdef CONFIG_PPC64
+	ulong host_msr;
+	ulong host_r2;
+	void *host_retip;
+	ulong trampoline_lowmem;
+	ulong trampoline_enter;
+	ulong highmem_handler;
+	ulong host_paca_phys;
+	struct kvmppc_mmu mmu;
+#endif
 
 	u64 fpr[32];
 	ulong gpr[32];
@@ -123,6 +180,10 @@ struct kvm_vcpu_arch {
 	ulong xer;
 
 	ulong msr;
+#ifdef CONFIG_PPC64
+	ulong shadow_msr;
+	ulong hflags;
+#endif
 	u32 mmucr;
 	ulong sprg0;
 	ulong sprg1;
@@ -149,6 +210,7 @@ struct kvm_vcpu_arch {
 	u32 ivor[64];
 	ulong ivpr;
 	u32 pir;
+	u32 pvr;
 
 	u32 shadow_pid;
 	u32 pid;
@@ -174,6 +236,9 @@ struct kvm_vcpu_arch {
 #endif
 
 	u32 last_inst;
+#ifdef CONFIG_PPC64
+	ulong fault_dsisr;
+#endif
 	ulong fault_dear;
 	ulong fault_esr;
 	gpa_t paddr_accessed;
@@ -186,7 +251,13 @@ struct kvm_vcpu_arch {
 	u32 cpr0_cfgaddr; /* holds the last set cpr0_cfgaddr */
 
 	struct timer_list dec_timer;
+	u64 dec_jiffies;
 	unsigned long pending_exceptions;
+
+#ifdef CONFIG_PPC64
+	struct hpte_cache hpte_cache[HPTEG_CACHE_NUM];
+	int hpte_cache_offset;
+#endif
 };
 
 #endif /* __POWERPC_KVM_HOST_H__ */

commit ec04b2604c3707a46db1d26d98f82b11d0844669
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Jun 19 15:16:23 2009 +0200

    KVM: Prepare memslot data structures for multiple hugepage sizes
    
    [avi: fix build on non-x86]
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index d4caa6127f55..c9c930ed11d7 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -34,7 +34,8 @@
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 
 /* We don't currently support large pages. */
-#define KVM_PAGES_PER_HPAGE (1UL << 31)
+#define KVM_NR_PAGE_SIZES	1
+#define KVM_PAGES_PER_HPAGE(x)	(1UL<<31)
 
 struct kvm;
 struct kvm_run;

commit 5b7c1a2c17e77cd5416755bb9ac63278996f6c51
Author: Liu Yu <yu.liu@freescale.com>
Date:   Fri Jun 5 14:54:30 2009 +0800

    KVM: ppc: e500: Directly pass pvr to guest
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index fddc3ed715fa..d4caa6127f55 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -153,7 +153,6 @@ struct kvm_vcpu_arch {
 	u32 pid;
 	u32 swap_pid;
 
-	u32 pvr;
 	u32 ccr0;
 	u32 ccr1;
 	u32 dbcr0;

commit c428dcc9b9f967945992a2f8529e8c50a31d7913
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Wed Jun 17 15:04:19 2009 +1000

    KVM: Make KVM_HPAGES_PER_HPAGE unsigned long to avoid build error on powerpc
    
    Eliminates this compiler warning:
    
    arch/powerpc/kvm/../../../virt/kvm/kvm_main.c:1178: error: integer overflow in expression
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index dfdf13c9fefd..fddc3ed715fa 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -34,7 +34,7 @@
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 
 /* We don't currently support large pages. */
-#define KVM_PAGES_PER_HPAGE (1<<31)
+#define KVM_PAGES_PER_HPAGE (1UL << 31)
 
 struct kvm;
 struct kvm_run;

commit f5d0906b5bafd7faea553ed1cc92bd06755b66b9
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sun Jan 4 13:51:09 2009 -0600

    KVM: ppc: remove debug support broken by KVM debug rewrite
    
    After the rewrite of KVM's debug support, this code doesn't even build any
    more.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 1c6187697520..dfdf13c9fefd 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -111,11 +111,6 @@ struct kvm_arch {
 struct kvm_vcpu_arch {
 	u32 host_stack;
 	u32 host_pid;
-	u32 host_dbcr0;
-	u32 host_dbcr1;
-	u32 host_dbcr2;
-	u32 host_iac[4];
-	u32 host_msr;
 
 	u64 fpr[32];
 	ulong gpr[32];

commit bb3a8a178dec1e46df3138a30f76acf67fe12397
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sat Jan 3 16:23:13 2009 -0600

    KVM: ppc: Add extra E500 exceptions
    
    e500 has additional interrupt vectors (and corresponding IVORs) for SPE and
    performance monitoring interrupts.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 50e5ce1b400a..1c6187697520 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -150,7 +150,7 @@ struct kvm_vcpu_arch {
 	u32 tbu;
 	u32 tcr;
 	u32 tsr;
-	u32 ivor[16];
+	u32 ivor[64];
 	ulong ivpr;
 	u32 pir;
 

commit f7b200af8f1da748cc67993caeff3923d6014070
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sat Jan 3 16:23:07 2009 -0600

    KVM: ppc: Add dbsr in kvm_vcpu_arch
    
    Kernel for E500 need clear dbsr when startup.
    So add dbsr register in kvm_vcpu_arch for BOOKE.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 65c4d492d722..50e5ce1b400a 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -163,6 +163,7 @@ struct kvm_vcpu_arch {
 	u32 ccr1;
 	u32 dbcr0;
 	u32 dbcr1;
+	u32 dbsr;
 
 #ifdef CONFIG_KVM_EXIT_TIMING
 	struct kvmppc_exit_timing timing_exit;

commit c46fb0211f8b93de45400bff814c0f29335af5fc
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sat Jan 3 16:22:58 2009 -0600

    KVM: ppc: move struct kvmppc_44x_tlbe into 44x-specific header
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index a22600537a8c..65c4d492d722 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -64,13 +64,6 @@ struct kvm_vcpu_stat {
 	u32 halt_wakeup;
 };
 
-struct kvmppc_44x_tlbe {
-	u32 tid; /* Only the low 8 bits are used. */
-	u32 word0;
-	u32 word1;
-	u32 word2;
-};
-
 enum kvm_exit_types {
 	MMIO_EXITS,
 	DCR_EXITS,

commit 989c0f0ed56468a4aa48711cef5acf122a40d1dd
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Thu Dec 18 12:33:18 2008 +0100

    KVM: Remove old kvm_guest_debug structs
    
    Remove the remaining arch fragments of the old guest debug interface
    that now break non-x86 builds.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index c1e436fe7738..a22600537a8c 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -200,10 +200,4 @@ struct kvm_vcpu_arch {
 	unsigned long pending_exceptions;
 };
 
-struct kvm_guest_debug {
-	int enabled;
-	unsigned long bp[4];
-	int singlestep;
-};
-
 #endif /* __POWERPC_KVM_HOST_H__ */

commit 7b7015914b30ad8d9136d41412c5129b9bc9af70
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Tue Dec 2 15:51:58 2008 -0600

    KVM: ppc: mostly cosmetic updates to the exit timing accounting code
    
    The only significant changes were to kvmppc_exit_timing_write() and
    kvmppc_exit_timing_show(), both of which were dramatically simplified.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 2f5b49f2a98e..c1e436fe7738 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -102,9 +102,8 @@ enum kvm_exit_types {
 	__NUMBER_OF_KVM_EXIT_TYPES
 };
 
-#ifdef CONFIG_KVM_EXIT_TIMING
 /* allow access to big endian 32bit upper/lower parts and 64bit var */
-struct exit_timing {
+struct kvmppc_exit_timing {
 	union {
 		u64 tv64;
 		struct {
@@ -112,7 +111,6 @@ struct exit_timing {
 		} tv32;
 	};
 };
-#endif
 
 struct kvm_arch {
 };
@@ -174,8 +172,8 @@ struct kvm_vcpu_arch {
 	u32 dbcr1;
 
 #ifdef CONFIG_KVM_EXIT_TIMING
-	struct exit_timing timing_exit;
-	struct exit_timing timing_last_enter;
+	struct kvmppc_exit_timing timing_exit;
+	struct kvmppc_exit_timing timing_last_enter;
 	u32 last_exit_type;
 	u32 timing_count_type[__NUMBER_OF_KVM_EXIT_TYPES];
 	u64 timing_sum_duration[__NUMBER_OF_KVM_EXIT_TYPES];

commit 73e75b416ffcfa3a84952d8e389a0eca080f00e1
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Tue Dec 2 15:51:57 2008 -0600

    KVM: ppc: Implement in-kernel exit timing statistics
    
    Existing KVM statistics are either just counters (kvm_stat) reported for
    KVM generally or trace based aproaches like kvm_trace.
    For KVM on powerpc we had the need to track the timings of the different exit
    types. While this could be achieved parsing data created with a kvm_trace
    extension this adds too much overhead (at least on embedded PowerPC) slowing
    down the workloads we wanted to measure.
    
    Therefore this patch adds a in-kernel exit timing statistic to the powerpc kvm
    code. These statistic is available per vm&vcpu under the kvm debugfs directory.
    As this statistic is low, but still some overhead it can be enabled via a
    .config entry and should be off by default.
    
    Since this patch touched all powerpc kvm_stat code anyway this code is now
    merged and simplified together with the exit timing statistic code (still
    working with exit timing disabled in .config).
    
    Signed-off-by: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index a4a7d5ef6cf8..2f5b49f2a98e 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -71,6 +71,49 @@ struct kvmppc_44x_tlbe {
 	u32 word2;
 };
 
+enum kvm_exit_types {
+	MMIO_EXITS,
+	DCR_EXITS,
+	SIGNAL_EXITS,
+	ITLB_REAL_MISS_EXITS,
+	ITLB_VIRT_MISS_EXITS,
+	DTLB_REAL_MISS_EXITS,
+	DTLB_VIRT_MISS_EXITS,
+	SYSCALL_EXITS,
+	ISI_EXITS,
+	DSI_EXITS,
+	EMULATED_INST_EXITS,
+	EMULATED_MTMSRWE_EXITS,
+	EMULATED_WRTEE_EXITS,
+	EMULATED_MTSPR_EXITS,
+	EMULATED_MFSPR_EXITS,
+	EMULATED_MTMSR_EXITS,
+	EMULATED_MFMSR_EXITS,
+	EMULATED_TLBSX_EXITS,
+	EMULATED_TLBWE_EXITS,
+	EMULATED_RFI_EXITS,
+	DEC_EXITS,
+	EXT_INTR_EXITS,
+	HALT_WAKEUP,
+	USR_PR_INST,
+	FP_UNAVAIL,
+	DEBUG_EXITS,
+	TIMEINGUEST,
+	__NUMBER_OF_KVM_EXIT_TYPES
+};
+
+#ifdef CONFIG_KVM_EXIT_TIMING
+/* allow access to big endian 32bit upper/lower parts and 64bit var */
+struct exit_timing {
+	union {
+		u64 tv64;
+		struct {
+			u32 tbu, tbl;
+		} tv32;
+	};
+};
+#endif
+
 struct kvm_arch {
 };
 
@@ -130,6 +173,19 @@ struct kvm_vcpu_arch {
 	u32 dbcr0;
 	u32 dbcr1;
 
+#ifdef CONFIG_KVM_EXIT_TIMING
+	struct exit_timing timing_exit;
+	struct exit_timing timing_last_enter;
+	u32 last_exit_type;
+	u32 timing_count_type[__NUMBER_OF_KVM_EXIT_TYPES];
+	u64 timing_sum_duration[__NUMBER_OF_KVM_EXIT_TYPES];
+	u64 timing_sum_quad_duration[__NUMBER_OF_KVM_EXIT_TYPES];
+	u64 timing_min_duration[__NUMBER_OF_KVM_EXIT_TYPES];
+	u64 timing_max_duration[__NUMBER_OF_KVM_EXIT_TYPES];
+	u64 timing_last_exit;
+	struct dentry *debugfs_exit_timing;
+#endif
+
 	u32 last_inst;
 	ulong fault_dear;
 	ulong fault_esr;

commit 5cf8ca22146fa106f3bb865631ec04f5b499508f
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:19 2008 -0600

    KVM: ppc: adjust vcpu types to support 64-bit cores
    
    However, some of these fields could be split into separate per-core structures
    in the future.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 765d8ec8b7d6..a4a7d5ef6cf8 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -84,32 +84,32 @@ struct kvm_vcpu_arch {
 	u32 host_msr;
 
 	u64 fpr[32];
-	u32 gpr[32];
+	ulong gpr[32];
 
-	u32 pc;
+	ulong pc;
 	u32 cr;
-	u32 ctr;
-	u32 lr;
-	u32 xer;
+	ulong ctr;
+	ulong lr;
+	ulong xer;
 
-	u32 msr;
+	ulong msr;
 	u32 mmucr;
-	u32 sprg0;
-	u32 sprg1;
-	u32 sprg2;
-	u32 sprg3;
-	u32 sprg4;
-	u32 sprg5;
-	u32 sprg6;
-	u32 sprg7;
-	u32 srr0;
-	u32 srr1;
-	u32 csrr0;
-	u32 csrr1;
-	u32 dsrr0;
-	u32 dsrr1;
-	u32 dear;
-	u32 esr;
+	ulong sprg0;
+	ulong sprg1;
+	ulong sprg2;
+	ulong sprg3;
+	ulong sprg4;
+	ulong sprg5;
+	ulong sprg6;
+	ulong sprg7;
+	ulong srr0;
+	ulong srr1;
+	ulong csrr0;
+	ulong csrr1;
+	ulong dsrr0;
+	ulong dsrr1;
+	ulong dear;
+	ulong esr;
 	u32 dec;
 	u32 decar;
 	u32 tbl;
@@ -117,7 +117,7 @@ struct kvm_vcpu_arch {
 	u32 tcr;
 	u32 tsr;
 	u32 ivor[16];
-	u32 ivpr;
+	ulong ivpr;
 	u32 pir;
 
 	u32 shadow_pid;
@@ -131,8 +131,8 @@ struct kvm_vcpu_arch {
 	u32 dbcr1;
 
 	u32 last_inst;
-	u32 fault_dear;
-	u32 fault_esr;
+	ulong fault_dear;
+	ulong fault_esr;
 	gpa_t paddr_accessed;
 
 	u8 io_gpr; /* GPR used as IO source/target */

commit db93f5745d836f81cef0b4101a7c2685eeb55efb
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:18 2008 -0600

    KVM: ppc: create struct kvm_vcpu_44x and introduce container_of() accessor
    
    This patch doesn't yet move all 44x-specific data into the new structure, but
    is the first step down that path. In the future we may also want to create a
    struct kvm_vcpu_booke.
    
    Based on patch from Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index f5850d7d57a5..765d8ec8b7d6 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -74,20 +74,7 @@ struct kvmppc_44x_tlbe {
 struct kvm_arch {
 };
 
-/* XXX Can't include mmu-44x.h because it redefines struct mm_context. */
-#define PPC44x_TLB_SIZE 64
-
 struct kvm_vcpu_arch {
-	/* Unmodified copy of the guest's TLB. */
-	struct kvmppc_44x_tlbe guest_tlb[PPC44x_TLB_SIZE];
-	/* TLB that's actually used when the guest is running. */
-	struct kvmppc_44x_tlbe shadow_tlb[PPC44x_TLB_SIZE];
-	/* Pages which are referenced in the shadow TLB. */
-	struct page *shadow_pages[PPC44x_TLB_SIZE];
-
-	/* Track which TLB entries we've modified in the current exit. */
-	u8 shadow_tlb_mod[PPC44x_TLB_SIZE];
-
 	u32 host_stack;
 	u32 host_pid;
 	u32 host_dbcr0;

commit 9dd921cfea734409a931ccc6eafd7f09850311e9
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:14 2008 -0600

    KVM: ppc: Refactor powerpc.c to relocate 440-specific code
    
    This introduces a set of core-provided hooks. For 440, some of these are
    implemented by booke.c, with the rest in (the new) 44x.c.
    
    Note that these hooks are link-time, not run-time. Since it is not possible to
    build a single kernel for both e500 and 440 (for example), using function
    pointers would only add overhead.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index df733511d671..f5850d7d57a5 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -74,6 +74,9 @@ struct kvmppc_44x_tlbe {
 struct kvm_arch {
 };
 
+/* XXX Can't include mmu-44x.h because it redefines struct mm_context. */
+#define PPC44x_TLB_SIZE 64
+
 struct kvm_vcpu_arch {
 	/* Unmodified copy of the guest's TLB. */
 	struct kvmppc_44x_tlbe guest_tlb[PPC44x_TLB_SIZE];

commit 0f55dc481ea5c4f87fc0161cb1b8c6e2cafae8fc
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:12 2008 -0600

    KVM: ppc: Rename "struct tlbe" to "struct kvmppc_44x_tlbe"
    
    This will ease ports to other cores.
    
    Also remove unused "struct kvm_tlb" while we're at it.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 34b52b7180cd..df733511d671 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -64,7 +64,7 @@ struct kvm_vcpu_stat {
 	u32 halt_wakeup;
 };
 
-struct tlbe {
+struct kvmppc_44x_tlbe {
 	u32 tid; /* Only the low 8 bits are used. */
 	u32 word0;
 	u32 word1;
@@ -76,9 +76,9 @@ struct kvm_arch {
 
 struct kvm_vcpu_arch {
 	/* Unmodified copy of the guest's TLB. */
-	struct tlbe guest_tlb[PPC44x_TLB_SIZE];
+	struct kvmppc_44x_tlbe guest_tlb[PPC44x_TLB_SIZE];
 	/* TLB that's actually used when the guest is running. */
-	struct tlbe shadow_tlb[PPC44x_TLB_SIZE];
+	struct kvmppc_44x_tlbe shadow_tlb[PPC44x_TLB_SIZE];
 	/* Pages which are referenced in the shadow TLB. */
 	struct page *shadow_pages[PPC44x_TLB_SIZE];
 

commit 49dd2c492895828a90ecdf889e7fe9cfb40a82a7
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Jul 25 13:54:53 2008 -0500

    KVM: powerpc: Map guest userspace with TID=0 mappings
    
    When we use TID=N userspace mappings, we must ensure that kernel mappings have
    been destroyed when entering userspace. Using TID=1/TID=0 for kernel/user
    mappings and running userspace with PID=0 means that userspace can't access the
    kernel mappings, but the kernel can directly access userspace.
    
    The net is that we don't need to flush the TLB on privilege switches, but we do
    on guest context switches (which are far more infrequent). Guest boot time
    performance improvement: about 30%.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 4338b03da8f9..34b52b7180cd 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -129,7 +129,11 @@ struct kvm_vcpu_arch {
 	u32 ivor[16];
 	u32 ivpr;
 	u32 pir;
+
+	u32 shadow_pid;
 	u32 pid;
+	u32 swap_pid;
+
 	u32 pvr;
 	u32 ccr0;
 	u32 ccr1;

commit 83aae4a8098eb8a40a2e9dab3714354182143b4f
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Jul 25 13:54:52 2008 -0500

    KVM: ppc: Write only modified shadow entries into the TLB on exit
    
    Track which TLB entries need to be written, instead of overwriting everything
    below the high water mark. Typically only a single guest TLB entry will be
    modified in a single exit.
    
    Guest boot time performance improvement: about 15%.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index dc3a7562bae4..4338b03da8f9 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -82,6 +82,9 @@ struct kvm_vcpu_arch {
 	/* Pages which are referenced in the shadow TLB. */
 	struct page *shadow_pages[PPC44x_TLB_SIZE];
 
+	/* Track which TLB entries we've modified in the current exit. */
+	u8 shadow_tlb_mod[PPC44x_TLB_SIZE];
+
 	u32 host_stack;
 	u32 host_pid;
 	u32 host_dbcr0;

commit 20754c2495a791b5b429c0da63394c86ade978e7
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Jul 25 13:54:51 2008 -0500

    KVM: ppc: Stop saving host TLB state
    
    We're saving the host TLB state to memory on every exit, but never using it.
    Originally I had thought that we'd want to restore host TLB for heavyweight
    exits, but that could actually hurt when context switching to an unrelated host
    process (i.e. not qemu).
    
    Since this decreases the performance penalty of all exits, this patch improves
    guest boot time by about 15%.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 23bad40b0ea6..dc3a7562bae4 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -81,8 +81,6 @@ struct kvm_vcpu_arch {
 	struct tlbe shadow_tlb[PPC44x_TLB_SIZE];
 	/* Pages which are referenced in the shadow TLB. */
 	struct page *shadow_pages[PPC44x_TLB_SIZE];
-	/* Copy of the host's TLB. */
-	struct tlbe host_tlb[PPC44x_TLB_SIZE];
 
 	u32 host_stack;
 	u32 host_pid;

commit 6a0ab738ef42d87951b3980f61b1f4cbb14d4171
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Jul 25 13:54:49 2008 -0500

    KVM: ppc: guest breakpoint support
    
    Allow host userspace to program hardware debug registers to set breakpoints
    inside guests.
    
    Signed-off-by: Jerone Young <jyoung5@us.ibm.com>
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 2655e2a4831e..23bad40b0ea6 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -86,6 +86,11 @@ struct kvm_vcpu_arch {
 
 	u32 host_stack;
 	u32 host_pid;
+	u32 host_dbcr0;
+	u32 host_dbcr1;
+	u32 host_dbcr2;
+	u32 host_iac[4];
+	u32 host_msr;
 
 	u64 fpr[32];
 	u32 gpr[32];

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
new file mode 100644
index 000000000000..2655e2a4831e
--- /dev/null
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -0,0 +1,155 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+ *
+ * Copyright IBM Corp. 2007
+ *
+ * Authors: Hollis Blanchard <hollisb@us.ibm.com>
+ */
+
+#ifndef __POWERPC_KVM_HOST_H__
+#define __POWERPC_KVM_HOST_H__
+
+#include <linux/mutex.h>
+#include <linux/timer.h>
+#include <linux/types.h>
+#include <linux/kvm_types.h>
+#include <asm/kvm_asm.h>
+
+#define KVM_MAX_VCPUS 1
+#define KVM_MEMORY_SLOTS 32
+/* memory slots that does not exposed to userspace */
+#define KVM_PRIVATE_MEM_SLOTS 4
+
+#define KVM_COALESCED_MMIO_PAGE_OFFSET 1
+
+/* We don't currently support large pages. */
+#define KVM_PAGES_PER_HPAGE (1<<31)
+
+struct kvm;
+struct kvm_run;
+struct kvm_vcpu;
+
+struct kvm_vm_stat {
+	u32 remote_tlb_flush;
+};
+
+struct kvm_vcpu_stat {
+	u32 sum_exits;
+	u32 mmio_exits;
+	u32 dcr_exits;
+	u32 signal_exits;
+	u32 light_exits;
+	/* Account for special types of light exits: */
+	u32 itlb_real_miss_exits;
+	u32 itlb_virt_miss_exits;
+	u32 dtlb_real_miss_exits;
+	u32 dtlb_virt_miss_exits;
+	u32 syscall_exits;
+	u32 isi_exits;
+	u32 dsi_exits;
+	u32 emulated_inst_exits;
+	u32 dec_exits;
+	u32 ext_intr_exits;
+	u32 halt_wakeup;
+};
+
+struct tlbe {
+	u32 tid; /* Only the low 8 bits are used. */
+	u32 word0;
+	u32 word1;
+	u32 word2;
+};
+
+struct kvm_arch {
+};
+
+struct kvm_vcpu_arch {
+	/* Unmodified copy of the guest's TLB. */
+	struct tlbe guest_tlb[PPC44x_TLB_SIZE];
+	/* TLB that's actually used when the guest is running. */
+	struct tlbe shadow_tlb[PPC44x_TLB_SIZE];
+	/* Pages which are referenced in the shadow TLB. */
+	struct page *shadow_pages[PPC44x_TLB_SIZE];
+	/* Copy of the host's TLB. */
+	struct tlbe host_tlb[PPC44x_TLB_SIZE];
+
+	u32 host_stack;
+	u32 host_pid;
+
+	u64 fpr[32];
+	u32 gpr[32];
+
+	u32 pc;
+	u32 cr;
+	u32 ctr;
+	u32 lr;
+	u32 xer;
+
+	u32 msr;
+	u32 mmucr;
+	u32 sprg0;
+	u32 sprg1;
+	u32 sprg2;
+	u32 sprg3;
+	u32 sprg4;
+	u32 sprg5;
+	u32 sprg6;
+	u32 sprg7;
+	u32 srr0;
+	u32 srr1;
+	u32 csrr0;
+	u32 csrr1;
+	u32 dsrr0;
+	u32 dsrr1;
+	u32 dear;
+	u32 esr;
+	u32 dec;
+	u32 decar;
+	u32 tbl;
+	u32 tbu;
+	u32 tcr;
+	u32 tsr;
+	u32 ivor[16];
+	u32 ivpr;
+	u32 pir;
+	u32 pid;
+	u32 pvr;
+	u32 ccr0;
+	u32 ccr1;
+	u32 dbcr0;
+	u32 dbcr1;
+
+	u32 last_inst;
+	u32 fault_dear;
+	u32 fault_esr;
+	gpa_t paddr_accessed;
+
+	u8 io_gpr; /* GPR used as IO source/target */
+	u8 mmio_is_bigendian;
+	u8 dcr_needed;
+	u8 dcr_is_write;
+
+	u32 cpr0_cfgaddr; /* holds the last set cpr0_cfgaddr */
+
+	struct timer_list dec_timer;
+	unsigned long pending_exceptions;
+};
+
+struct kvm_guest_debug {
+	int enabled;
+	unsigned long bp[4];
+	int singlestep;
+};
+
+#endif /* __POWERPC_KVM_HOST_H__ */
