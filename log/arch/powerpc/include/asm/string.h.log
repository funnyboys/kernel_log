commit 4d4a273854c9eac1a1b163830658dc825e183ad5
Author: Balbir Singh <bsingharora@gmail.com>
Date:   Tue Aug 20 13:43:51 2019 +0530

    powerpc/memcpy: Add memcpy_mcsafe for pmem
    
    The pmem infrastructure uses memcpy_mcsafe in the pmem layer so as to
    convert machine check exceptions into a return value on failure in case
    a machine check exception is encountered during the memcpy. The return
    value is the number of bytes remaining to be copied.
    
    This patch largely borrows from the copyuser_power7 logic and does not add
    the VMX optimizations, largely to keep the patch simple. If needed those
    optimizations can be folded in.
    
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    [arbab@linux.ibm.com: Added symbol export]
    Co-developed-by: Santosh Sivaraj <santosh@fossix.org>
    Signed-off-by: Santosh Sivaraj <santosh@fossix.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190820081352.8641-7-santosh@fossix.org

diff --git a/arch/powerpc/include/asm/string.h b/arch/powerpc/include/asm/string.h
index 9bf6dffb4090..b72692702f35 100644
--- a/arch/powerpc/include/asm/string.h
+++ b/arch/powerpc/include/asm/string.h
@@ -53,7 +53,9 @@ void *__memmove(void *to, const void *from, __kernel_size_t n);
 #ifndef CONFIG_KASAN
 #define __HAVE_ARCH_MEMSET32
 #define __HAVE_ARCH_MEMSET64
+#define __HAVE_ARCH_MEMCPY_MCSAFE
 
+extern int memcpy_mcsafe(void *dst, const void *src, __kernel_size_t sz);
 extern void *__memset16(uint16_t *, uint16_t v, __kernel_size_t);
 extern void *__memset32(uint32_t *, uint32_t v, __kernel_size_t);
 extern void *__memset64(uint64_t *, uint64_t v, __kernel_size_t);

commit 26deb04342e343ac58ab05bc7d2345ff0be9b667
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 16:23:26 2019 +0000

    powerpc: prepare string/mem functions for KASAN
    
    CONFIG_KASAN implements wrappers for memcpy() memmove() and memset()
    Those wrappers are doing the verification then call respectively
    __memcpy() __memmove() and __memset(). The arches are therefore
    expected to rename their optimised functions that way.
    
    For files on which KASAN is inhibited, #defines are used to allow
    them to directly call optimised versions of the functions without
    going through the KASAN wrappers.
    
    See commit 393f203f5fd5 ("x86_64: kasan: add interceptors for
    memset/memmove/memcpy functions") for details.
    
    Other string / mem functions do not (yet) have kasan wrappers,
    we therefore have to fallback to the generic versions when
    KASAN is active, otherwise KASAN checks will be skipped.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Fixups to keep selftests working]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/string.h b/arch/powerpc/include/asm/string.h
index 1647de15a31e..9bf6dffb4090 100644
--- a/arch/powerpc/include/asm/string.h
+++ b/arch/powerpc/include/asm/string.h
@@ -4,14 +4,17 @@
 
 #ifdef __KERNEL__
 
+#ifndef CONFIG_KASAN
 #define __HAVE_ARCH_STRNCPY
 #define __HAVE_ARCH_STRNCMP
+#define __HAVE_ARCH_MEMCHR
+#define __HAVE_ARCH_MEMCMP
+#define __HAVE_ARCH_MEMSET16
+#endif
+
 #define __HAVE_ARCH_MEMSET
 #define __HAVE_ARCH_MEMCPY
 #define __HAVE_ARCH_MEMMOVE
-#define __HAVE_ARCH_MEMCMP
-#define __HAVE_ARCH_MEMCHR
-#define __HAVE_ARCH_MEMSET16
 #define __HAVE_ARCH_MEMCPY_FLUSHCACHE
 
 extern char * strcpy(char *,const char *);
@@ -27,7 +30,27 @@ extern int memcmp(const void *,const void *,__kernel_size_t);
 extern void * memchr(const void *,int,__kernel_size_t);
 extern void * memcpy_flushcache(void *,const void *,__kernel_size_t);
 
+void *__memset(void *s, int c, __kernel_size_t count);
+void *__memcpy(void *to, const void *from, __kernel_size_t n);
+void *__memmove(void *to, const void *from, __kernel_size_t n);
+
+#if defined(CONFIG_KASAN) && !defined(__SANITIZE_ADDRESS__)
+/*
+ * For files that are not instrumented (e.g. mm/slub.c) we
+ * should use not instrumented version of mem* functions.
+ */
+#define memcpy(dst, src, len) __memcpy(dst, src, len)
+#define memmove(dst, src, len) __memmove(dst, src, len)
+#define memset(s, c, n) __memset(s, c, n)
+
+#ifndef __NO_FORTIFY
+#define __NO_FORTIFY /* FORTIFY_SOURCE uses __builtin_memcpy, etc. */
+#endif
+
+#endif
+
 #ifdef CONFIG_PPC64
+#ifndef CONFIG_KASAN
 #define __HAVE_ARCH_MEMSET32
 #define __HAVE_ARCH_MEMSET64
 
@@ -49,8 +72,11 @@ static inline void *memset64(uint64_t *p, uint64_t v, __kernel_size_t n)
 {
 	return __memset64(p, v, n * 8);
 }
+#endif
 #else
+#ifndef CONFIG_KASAN
 #define __HAVE_ARCH_STRLEN
+#endif
 
 extern void *memset16(uint16_t *, uint16_t, __kernel_size_t);
 #endif

commit 9412b234501eef1d273cc759387a232f0027c574
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Wed Aug 1 09:01:14 2018 +0000

    powerpc/lib: Implement strlen() in assembly for PPC32
    
    The generic implementation of strlen() reads strings byte per byte.
    
    This patch implements strlen() in assembly based on a read of entire
    words, in the same spirit as what some other arches and glibc do.
    
    On a 8xx the time spent in strlen is reduced by 3/4 for long strings.
    
    strlen() selftest on an 8xx provides the following values:
    
    Before the patch (ie with the generic strlen() in lib/string.c):
    
      len 256 : time = 1.195055
      len 016 : time = 0.083745
      len 008 : time = 0.046828
      len 004 : time = 0.028390
    
    After the patch:
    
      len 256 : time = 0.272185 ==> 78% improvment
      len 016 : time = 0.040632 ==> 51% improvment
      len 008 : time = 0.033060 ==> 29% improvment
      len 004 : time = 0.029149 ==> 2% degradation
    
    On a 832x:
    
    Before the patch:
    
      len 256 : time = 0.236125
      len 016 : time = 0.018136
      len 008 : time = 0.011000
      len 004 : time = 0.007229
    
    After the patch:
    
      len 256 : time = 0.094950 ==> 60% improvment
      len 016 : time = 0.013357 ==> 26% improvment
      len 008 : time = 0.010586 ==> 4% improvment
      len 004 : time = 0.008784
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/string.h b/arch/powerpc/include/asm/string.h
index 9b8cedf618f4..1647de15a31e 100644
--- a/arch/powerpc/include/asm/string.h
+++ b/arch/powerpc/include/asm/string.h
@@ -50,6 +50,8 @@ static inline void *memset64(uint64_t *p, uint64_t v, __kernel_size_t n)
 	return __memset64(p, v, n * 8);
 }
 #else
+#define __HAVE_ARCH_STRLEN
+
 extern void *memset16(uint16_t *, uint16_t, __kernel_size_t);
 #endif
 #endif /* __KERNEL__ */

commit 5b0e2cb020085efe202123162502e0b551e49a0e
Merge: 758f875848d7 3ffa9d9e2a7c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 12:47:46 2017 -0800

    Merge tag 'powerpc-4.15-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "A bit of a small release, I suspect in part due to me travelling for
      KS. But my backlog of patches to review is smaller than usual, so I
      think in part folks just didn't send as much this cycle.
    
      Non-highlights:
    
       - Five fixes for the >128T address space handling, both to fix bugs
         in our implementation and to bring the semantics exactly into line
         with x86.
    
      Highlights:
    
       - Support for a new OPAL call on bare metal machines which gives us a
         true NMI (ie. is not masked by MSR[EE]=0) for debugging etc.
    
       - Support for Power9 DD2 in the CXL driver.
    
       - Improvements to machine check handling so that uncorrectable errors
         can be reported into the generic memory_failure() machinery.
    
       - Some fixes and improvements for VPHN, which is used under PowerVM
         to notify the Linux partition of topology changes.
    
       - Plumbing to enable TM (transactional memory) without suspend on
         some Power9 processors (PPC_FEATURE2_HTM_NO_SUSPEND).
    
       - Support for emulating vector loads form cache-inhibited memory, on
         some Power9 revisions.
    
       - Disable the fast-endian switch "syscall" by default (behind a
         CONFIG), we believe it has never had any users.
    
       - A major rework of the API drivers use when initiating and waiting
         for long running operations performed by OPAL firmware, and changes
         to the powernv_flash driver to use the new API.
    
       - Several fixes for the handling of FP/VMX/VSX while processes are
         using transactional memory.
    
       - Optimisations of TLB range flushes when using the radix MMU on
         Power9.
    
       - Improvements to the VAS facility used to access coprocessors on
         Power9, and related improvements to the way the NX crypto driver
         handles requests.
    
       - Implementation of PMEM_API and UACCESS_FLUSHCACHE for 64-bit.
    
      Thanks to: Alexey Kardashevskiy, Alistair Popple, Allen Pais, Andrew
      Donnellan, Aneesh Kumar K.V, Arnd Bergmann, Balbir Singh, Benjamin
      Herrenschmidt, Breno Leitao, Christophe Leroy, Christophe Lombard,
      Cyril Bur, Frederic Barrat, Gautham R. Shenoy, Geert Uytterhoeven,
      Guilherme G. Piccoli, Gustavo Romero, Haren Myneni, Joel Stanley,
      Kamalesh Babulal, Kautuk Consul, Markus Elfring, Masami Hiramatsu,
      Michael Bringmann, Michael Neuling, Michal Suchanek, Naveen N. Rao,
      Nicholas Piggin, Oliver O'Halloran, Paul Mackerras, Pedro Miraglia
      Franco de Carvalho, Philippe Bergheaud, Sandipan Das, Seth Forshee,
      Shriya, Stephen Rothwell, Stewart Smith, Sukadev Bhattiprolu, Tyrel
      Datwyler, Vaibhav Jain, Vaidyanathan Srinivasan, and William A.
      Kennington III"
    
    * tag 'powerpc-4.15-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (151 commits)
      powerpc/64s: Fix Power9 DD2.0 workarounds by adding DD2.1 feature
      powerpc/64s: Fix masking of SRR1 bits on instruction fault
      powerpc/64s: mm_context.addr_limit is only used on hash
      powerpc/64s/radix: Fix 128TB-512TB virtual address boundary case allocation
      powerpc/64s/hash: Allow MAP_FIXED allocations to cross 128TB boundary
      powerpc/64s/hash: Fix fork() with 512TB process address space
      powerpc/64s/hash: Fix 128TB-512TB virtual address boundary case allocation
      powerpc/64s/hash: Fix 512T hint detection to use >= 128T
      powerpc: Fix DABR match on hash based systems
      powerpc/signal: Properly handle return value from uprobe_deny_signal()
      powerpc/fadump: use kstrtoint to handle sysfs store
      powerpc/lib: Implement UACCESS_FLUSHCACHE API
      powerpc/lib: Implement PMEM API
      powerpc/powernv/npu: Don't explicitly flush nmmu tlb
      powerpc/powernv/npu: Use flush_all_mm() instead of flush_tlb_mm()
      powerpc/powernv/idle: Round up latency and residency values
      powerpc/kprobes: refactor kprobe_lookup_name for safer string operations
      powerpc/kprobes: Blacklist emulate_update_regs() from kprobes
      powerpc/kprobes: Do not disable interrupts for optprobes and kprobes_on_ftrace
      powerpc/kprobes: Disable preemption before invoking probe handler for optprobes
      ...

commit 6c44741d75a27985a0496cd58b123a58c7177108
Author: Oliver O'Halloran <oohall@gmail.com>
Date:   Thu Oct 19 18:13:55 2017 +1100

    powerpc/lib: Implement UACCESS_FLUSHCACHE API
    
    Implement the architecture specific portitions of the UACCESS_FLUSHCACHE
    API. This provides functions for the copy_user_flushcache iterator that
    ensure that when the copy is finished the destination buffer contains
    a copy of the original and that the destination buffer is clean in the
    processor caches.
    
    Signed-off-by: Oliver O'Halloran <oohall@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/string.h b/arch/powerpc/include/asm/string.h
index cc9addefb51c..2796eda17d92 100644
--- a/arch/powerpc/include/asm/string.h
+++ b/arch/powerpc/include/asm/string.h
@@ -11,6 +11,7 @@
 #define __HAVE_ARCH_MEMCMP
 #define __HAVE_ARCH_MEMCHR
 #define __HAVE_ARCH_MEMSET16
+#define __HAVE_ARCH_MEMCPY_FLUSHCACHE
 
 extern char * strcpy(char *,const char *);
 extern char * strncpy(char *,const char *, __kernel_size_t);
@@ -23,6 +24,7 @@ extern void * memcpy(void *,const void *,__kernel_size_t);
 extern void * memmove(void *,const void *,__kernel_size_t);
 extern int memcmp(const void *,const void *,__kernel_size_t);
 extern void * memchr(const void *,int,__kernel_size_t);
+extern void * memcpy_flushcache(void *,const void *,__kernel_size_t);
 
 #ifdef CONFIG_PPC64
 #define __HAVE_ARCH_MEMSET32

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/string.h b/arch/powerpc/include/asm/string.h
index cc9addefb51c..d98ac188cedb 100644
--- a/arch/powerpc/include/asm/string.h
+++ b/arch/powerpc/include/asm/string.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_POWERPC_STRING_H
 #define _ASM_POWERPC_STRING_H
 

commit da74f659205ea08cb0fd0b3050637b0e0eb31520
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Wed Aug 23 16:54:32 2017 +0200

    powerpc/32: add memset16()
    
    Commit 694fc88ce271f ("powerpc/string: Implement optimized
    memset variants") added memset16(), memset32() and memset64()
    for the 64 bits PPC.
    
    On 32 bits, memset64() is not relevant, and as shown below,
    the generic version of memset32() gives a good code, so only
    memset16() is candidate for an optimised version.
    
    000009c0 <memset32>:
     9c0:   2c 05 00 00     cmpwi   r5,0
     9c4:   39 23 ff fc     addi    r9,r3,-4
     9c8:   4d 82 00 20     beqlr
     9cc:   7c a9 03 a6     mtctr   r5
     9d0:   94 89 00 04     stwu    r4,4(r9)
     9d4:   42 00 ff fc     bdnz    9d0 <memset32+0x10>
     9d8:   4e 80 00 20     blr
    
    The last part of memset() handling the not 4-bytes multiples
    operates on bytes, making it unsuitable for handling word without
    modification. As it would increase memset() complexity, it is
    better to implement memset16() from scratch. In addition it
    has the advantage of allowing a more optimised memset16() than what
    we would have by using the memset() function.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/string.h b/arch/powerpc/include/asm/string.h
index b0e82466d4e8..cc9addefb51c 100644
--- a/arch/powerpc/include/asm/string.h
+++ b/arch/powerpc/include/asm/string.h
@@ -10,6 +10,7 @@
 #define __HAVE_ARCH_MEMMOVE
 #define __HAVE_ARCH_MEMCMP
 #define __HAVE_ARCH_MEMCHR
+#define __HAVE_ARCH_MEMSET16
 
 extern char * strcpy(char *,const char *);
 extern char * strncpy(char *,const char *, __kernel_size_t);
@@ -24,7 +25,6 @@ extern int memcmp(const void *,const void *,__kernel_size_t);
 extern void * memchr(const void *,int,__kernel_size_t);
 
 #ifdef CONFIG_PPC64
-#define __HAVE_ARCH_MEMSET16
 #define __HAVE_ARCH_MEMSET32
 #define __HAVE_ARCH_MEMSET64
 
@@ -46,6 +46,8 @@ static inline void *memset64(uint64_t *p, uint64_t v, __kernel_size_t n)
 {
 	return __memset64(p, v, n * 8);
 }
+#else
+extern void *memset16(uint16_t *, uint16_t, __kernel_size_t);
 #endif
 #endif /* __KERNEL__ */
 

commit 694fc88ce271fd48f7939c032c1247fef81db57f
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Tue Mar 28 01:07:40 2017 +0530

    powerpc/string: Implement optimized memset variants
    
    Based on Matthew Wilcox's patches for other architectures.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/string.h b/arch/powerpc/include/asm/string.h
index da3cdffca440..b0e82466d4e8 100644
--- a/arch/powerpc/include/asm/string.h
+++ b/arch/powerpc/include/asm/string.h
@@ -23,6 +23,30 @@ extern void * memmove(void *,const void *,__kernel_size_t);
 extern int memcmp(const void *,const void *,__kernel_size_t);
 extern void * memchr(const void *,int,__kernel_size_t);
 
+#ifdef CONFIG_PPC64
+#define __HAVE_ARCH_MEMSET16
+#define __HAVE_ARCH_MEMSET32
+#define __HAVE_ARCH_MEMSET64
+
+extern void *__memset16(uint16_t *, uint16_t v, __kernel_size_t);
+extern void *__memset32(uint32_t *, uint32_t v, __kernel_size_t);
+extern void *__memset64(uint64_t *, uint64_t v, __kernel_size_t);
+
+static inline void *memset16(uint16_t *p, uint16_t v, __kernel_size_t n)
+{
+	return __memset16(p, v, n * 2);
+}
+
+static inline void *memset32(uint32_t *p, uint32_t v, __kernel_size_t n)
+{
+	return __memset32(p, v, n * 4);
+}
+
+static inline void *memset64(uint64_t *p, uint64_t v, __kernel_size_t n)
+{
+	return __memset64(p, v, n * 8);
+}
+#endif
 #endif /* __KERNEL__ */
 
 #endif	/* _ASM_POWERPC_STRING_H */

commit 3ece16632b64120df2ef566ce32afbdb4aa8af1e
Author: Anton Blanchard <anton@samba.org>
Date:   Thu May 26 08:38:13 2016 +1000

    powerpc: Remove assembly versions of strcpy, strcat, strlen and strcmp
    
    A number of our assembly implementations of string functions do not
    align their hot loops. I was going to align them manually, but I
    realised that they are are almost instruction for instruction
    identical to what gcc produces, with the advantage that gcc does
    align them.
    
    In light of that, let's just remove the assembly versions.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/string.h b/arch/powerpc/include/asm/string.h
index e40010abcaf1..da3cdffca440 100644
--- a/arch/powerpc/include/asm/string.h
+++ b/arch/powerpc/include/asm/string.h
@@ -3,12 +3,8 @@
 
 #ifdef __KERNEL__
 
-#define __HAVE_ARCH_STRCPY
 #define __HAVE_ARCH_STRNCPY
-#define __HAVE_ARCH_STRLEN
-#define __HAVE_ARCH_STRCMP
 #define __HAVE_ARCH_STRNCMP
-#define __HAVE_ARCH_STRCAT
 #define __HAVE_ARCH_MEMSET
 #define __HAVE_ARCH_MEMCPY
 #define __HAVE_ARCH_MEMMOVE

commit 00f554fadebb96877ad449758dc90303a9826afe
Author: Philippe Bergheaud <felix@linux.vnet.ibm.com>
Date:   Wed Apr 30 09:12:01 2014 +1000

    powerpc: memcpy optimization for 64bit LE
    
    Unaligned stores take alignment exceptions on POWER7 running in little-endian.
    This is a dumb little-endian base memcpy that prevents unaligned stores.
    Once booted the feature fixup code switches over to the VMX copy loops
    (which are already endian safe).
    
    The question is what we do before that switch over. The base 64bit
    memcpy takes alignment exceptions on POWER7 so we can't use it as is.
    Fixing the causes of alignment exception would slow it down, because
    we'd need to ensure all loads and stores are aligned either through
    rotate tricks or bytewise loads and stores. Either would be bad for
    all other 64bit platforms.
    
    [ I simplified the loop a bit - Anton ]
    
    Signed-off-by: Philippe Bergheaud <felix@linux.vnet.ibm.com>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/string.h b/arch/powerpc/include/asm/string.h
index 0dffad6bcc84..e40010abcaf1 100644
--- a/arch/powerpc/include/asm/string.h
+++ b/arch/powerpc/include/asm/string.h
@@ -10,9 +10,7 @@
 #define __HAVE_ARCH_STRNCMP
 #define __HAVE_ARCH_STRCAT
 #define __HAVE_ARCH_MEMSET
-#ifdef __BIG_ENDIAN__
 #define __HAVE_ARCH_MEMCPY
-#endif
 #define __HAVE_ARCH_MEMMOVE
 #define __HAVE_ARCH_MEMCMP
 #define __HAVE_ARCH_MEMCHR
@@ -24,9 +22,7 @@ extern int strcmp(const char *,const char *);
 extern int strncmp(const char *, const char *, __kernel_size_t);
 extern char * strcat(char *, const char *);
 extern void * memset(void *,int,__kernel_size_t);
-#ifdef __BIG_ENDIAN__
 extern void * memcpy(void *,const void *,__kernel_size_t);
-#endif
 extern void * memmove(void *,const void *,__kernel_size_t);
 extern int memcmp(const void *,const void *,__kernel_size_t);
 extern void * memchr(const void *,int,__kernel_size_t);

commit de577a356848a629b2c7f252ca3d1bc87375b52b
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Sep 23 12:04:52 2013 +1000

    powerpc: Use generic memcpy code in little endian
    
    We need to fix some endian issues in our memcpy code. For now
    just enable the generic memcpy routine for little endian builds.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/string.h b/arch/powerpc/include/asm/string.h
index e40010abcaf1..0dffad6bcc84 100644
--- a/arch/powerpc/include/asm/string.h
+++ b/arch/powerpc/include/asm/string.h
@@ -10,7 +10,9 @@
 #define __HAVE_ARCH_STRNCMP
 #define __HAVE_ARCH_STRCAT
 #define __HAVE_ARCH_MEMSET
+#ifdef __BIG_ENDIAN__
 #define __HAVE_ARCH_MEMCPY
+#endif
 #define __HAVE_ARCH_MEMMOVE
 #define __HAVE_ARCH_MEMCMP
 #define __HAVE_ARCH_MEMCHR
@@ -22,7 +24,9 @@ extern int strcmp(const char *,const char *);
 extern int strncmp(const char *, const char *, __kernel_size_t);
 extern char * strcat(char *, const char *);
 extern void * memset(void *,int,__kernel_size_t);
+#ifdef __BIG_ENDIAN__
 extern void * memcpy(void *,const void *,__kernel_size_t);
+#endif
 extern void * memmove(void *,const void *,__kernel_size_t);
 extern int memcmp(const void *,const void *,__kernel_size_t);
 extern void * memchr(const void *,int,__kernel_size_t);

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/string.h b/arch/powerpc/include/asm/string.h
new file mode 100644
index 000000000000..e40010abcaf1
--- /dev/null
+++ b/arch/powerpc/include/asm/string.h
@@ -0,0 +1,32 @@
+#ifndef _ASM_POWERPC_STRING_H
+#define _ASM_POWERPC_STRING_H
+
+#ifdef __KERNEL__
+
+#define __HAVE_ARCH_STRCPY
+#define __HAVE_ARCH_STRNCPY
+#define __HAVE_ARCH_STRLEN
+#define __HAVE_ARCH_STRCMP
+#define __HAVE_ARCH_STRNCMP
+#define __HAVE_ARCH_STRCAT
+#define __HAVE_ARCH_MEMSET
+#define __HAVE_ARCH_MEMCPY
+#define __HAVE_ARCH_MEMMOVE
+#define __HAVE_ARCH_MEMCMP
+#define __HAVE_ARCH_MEMCHR
+
+extern char * strcpy(char *,const char *);
+extern char * strncpy(char *,const char *, __kernel_size_t);
+extern __kernel_size_t strlen(const char *);
+extern int strcmp(const char *,const char *);
+extern int strncmp(const char *, const char *, __kernel_size_t);
+extern char * strcat(char *, const char *);
+extern void * memset(void *,int,__kernel_size_t);
+extern void * memcpy(void *,const void *,__kernel_size_t);
+extern void * memmove(void *,const void *,__kernel_size_t);
+extern int memcmp(const void *,const void *,__kernel_size_t);
+extern void * memchr(const void *,int,__kernel_size_t);
+
+#endif /* __KERNEL__ */
+
+#endif	/* _ASM_POWERPC_STRING_H */
