commit 6cc0c16d82f889f0083f3608237189afb55b67be
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 26 03:35:37 2020 +1000

    powerpc/64s: Implement interrupt exit logic in C
    
    Implement the bulk of interrupt return logic in C. The asm return code
    must handle a few cases: restoring full GPRs, and emulating stack
    store.
    
    The stack store emulation is significantly simplfied, rather than
    creating a new return frame and switching to that before performing
    the store, it uses the PACA to keep a scratch register around to
    perform the store.
    
    The asm return code is moved into 64e for now. The new logic has made
    allowance for 64e, but I don't have a full environment that works well
    to test it, and even booting in emulated qemu is not great for stress
    testing. 64e shouldn't be too far off working with this, given a bit
    more testing and auditing of the logic.
    
    This is slightly faster on a POWER9 (page fault speed increases about
    1.1%), probably due to reduced mtmsrd.
    
    mpe: Includes fixes from Nick for _TIF_EMULATE_STACK_STORE
    handling (including the fast_interrupt_return path), to remove
    trace_hardirqs_on(), and fixes the interrupt-return part of the
    MSR_VSX restore bug caught by tm-unavailable selftest.
    
    mpe: Incorporate fix from Nick:
    
    The return-to-kernel path has to replay any soft-pending interrupts if
    it is returning to a context that had interrupts soft-enabled. It has
    to do this carefully and avoid plain enabling interrupts if this is an
    irq context, which can cause multiple nesting of interrupts on the
    stack, and other unexpected issues.
    
    The code which avoided this case got the soft-mask state wrong, and
    marked interrupts as enabled before going around again to retry. This
    seems to be mostly harmless except when PREEMPT=y, this calls
    preempt_schedule_irq with irqs apparently enabled and runs into a BUG
    in kernel/sched/core.c
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Suchanek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200225173541.1549955-29-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index ab59a4904254..7d81e86a1e5d 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -99,6 +99,8 @@ void __init machine_init(u64 dt_ptr);
 #endif
 long system_call_exception(long r3, long r4, long r5, long r6, long r7, long r8, unsigned long r0, struct pt_regs *regs);
 notrace unsigned long syscall_exit_prepare(unsigned long r3, struct pt_regs *regs);
+notrace unsigned long interrupt_exit_user_prepare(struct pt_regs *regs, unsigned long msr);
+notrace unsigned long interrupt_exit_kernel_prepare(struct pt_regs *regs, unsigned long msr);
 
 long ppc_fadvise64_64(int fd, int advice, u32 offset_high, u32 offset_low,
 		      u32 len_high, u32 len_low);

commit 68b34588e2027f699a3c034235f21cd19356b2e6
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 26 03:35:34 2020 +1000

    powerpc/64/sycall: Implement syscall entry/exit logic in C
    
    System call entry and particularly exit code is beyond the limit of
    what is reasonable to implement in asm.
    
    This conversion moves all conditional branches out of the asm code,
    except for the case that all GPRs should be restored at exit.
    
    Null syscall test is about 5% faster after this patch, because the
    exit work is handled under local_irq_disable, and the hard mask and
    pending interrupt replay is handled after that, which avoids games
    with MSR.
    
    mpe: Includes subsequent fixes from Nick:
    
    This fixes 4 issues caught by TM selftests. First was a tm-syscall bug
    that hit due to tabort_syscall being called after interrupts were
    reconciled (in a subsequent patch), which led to interrupts being
    enabled before tabort_syscall was called. Rather than going through an
    un-reconciling interrupts for the return, I just go back to putting
    the test early in asm, the C-ification of that wasn't a big win
    anyway.
    
    Second is the syscall return _TIF_USER_WORK_MASK check would go into
    an infinite loop if _TIF_RESTORE_TM became set. The asm code uses
    _TIF_USER_WORK_MASK to brach to slowpath which includes
    restore_tm_state.
    
    Third is system call return was not calling restore_tm_state, I missed
    this completely (alhtough it's in the return from interrupt C
    conversion because when the asm syscall code encountered problems it
    would branch to the interrupt return code.
    
    Fourth is MSR_VEC missing from restore_math, which was caught by
    tm-unavailable selftest taking an unexpected facility unavailable
    interrupt when testing VSX unavailble exception with MSR.FP=1
    MSR.VEC=1. Fourth case also has a fixup in a subsequent patch.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Suchanek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200225173541.1549955-26-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 983c0084fb3f..ab59a4904254 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -97,6 +97,8 @@ ppc_select(int n, fd_set __user *inp, fd_set __user *outp, fd_set __user *exp,
 unsigned long __init early_init(unsigned long dt_ptr);
 void __init machine_init(u64 dt_ptr);
 #endif
+long system_call_exception(long r3, long r4, long r5, long r6, long r7, long r8, unsigned long r0, struct pt_regs *regs);
+notrace unsigned long syscall_exit_prepare(unsigned long r3, struct pt_regs *regs);
 
 long ppc_fadvise64_64(int fd, int advice, u32 offset_high, u32 offset_low,
 		      u32 len_high, u32 len_low);
@@ -104,14 +106,6 @@ long sys_switch_endian(void);
 notrace unsigned int __check_irq_replay(void);
 void notrace restore_interrupts(void);
 
-/* ptrace */
-long do_syscall_trace_enter(struct pt_regs *regs);
-void do_syscall_trace_leave(struct pt_regs *regs);
-
-/* process */
-void restore_math(struct pt_regs *regs);
-void restore_tm_state(struct pt_regs *regs);
-
 /* prom_init (OpenFirmware) */
 unsigned long __init prom_init(unsigned long r3, unsigned long r4,
 			       unsigned long pp,
@@ -122,9 +116,6 @@ unsigned long __init prom_init(unsigned long r3, unsigned long r4,
 void __init early_setup(unsigned long dt_ptr);
 void early_setup_secondary(void);
 
-/* time */
-void accumulate_stolen_time(void);
-
 /* misc runtime */
 extern u64 __bswapdi2(u64);
 extern s64 __lshrdi3(s64, int);

commit ceb307474506f888e8f16dab183405ff01dffa08
Merge: 0da522107e5d b111df8447ac
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 1 14:00:59 2019 -0800

    Merge tag 'y2038-cleanups-5.5' of git://git.kernel.org:/pub/scm/linux/kernel/git/arnd/playground
    
    Pull y2038 cleanups from Arnd Bergmann:
     "y2038 syscall implementation cleanups
    
      This is a series of cleanups for the y2038 work, mostly intended for
      namespace cleaning: the kernel defines the traditional time_t, timeval
      and timespec types that often lead to y2038-unsafe code. Even though
      the unsafe usage is mostly gone from the kernel, having the types and
      associated functions around means that we can still grow new users,
      and that we may be missing conversions to safe types that actually
      matter.
    
      There are still a number of driver specific patches needed to get the
      last users of these types removed, those have been submitted to the
      respective maintainers"
    
    Link: https://lore.kernel.org/lkml/20191108210236.1296047-1-arnd@arndb.de/
    
    * tag 'y2038-cleanups-5.5' of git://git.kernel.org:/pub/scm/linux/kernel/git/arnd/playground: (26 commits)
      y2038: alarm: fix half-second cut-off
      y2038: ipc: fix x32 ABI breakage
      y2038: fix typo in powerpc vdso "LOPART"
      y2038: allow disabling time32 system calls
      y2038: itimer: change implementation to timespec64
      y2038: move itimer reset into itimer.c
      y2038: use compat_{get,set}_itimer on alpha
      y2038: itimer: compat handling to itimer.c
      y2038: time: avoid timespec usage in settimeofday()
      y2038: timerfd: Use timespec64 internally
      y2038: elfcore: Use __kernel_old_timeval for process times
      y2038: make ns_to_compat_timeval use __kernel_old_timeval
      y2038: socket: use __kernel_old_timespec instead of timespec
      y2038: socket: remove timespec reference in timestamping
      y2038: syscalls: change remaining timeval to __kernel_old_timeval
      y2038: rusage: use __kernel_old_timeval
      y2038: uapi: change __kernel_time_t to __kernel_old_time_t
      y2038: stat: avoid 'time_t' in 'struct stat'
      y2038: ipc: remove __kernel_time_t reference from headers
      y2038: vdso: powerpc: avoid timespec references
      ...

commit 75d319c06e6a76f67549c0ae1007dc3167804f4e
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Oct 25 22:56:17 2019 +0200

    y2038: syscalls: change remaining timeval to __kernel_old_timeval
    
    All of the remaining syscalls that pass a timeval (gettimeofday, utime,
    futimesat) can trivially be changed to pass a __kernel_old_timeval
    instead, which has a compatible layout, but avoids ambiguity with
    the timeval type in user space.
    
    Acked-by: Christian Brauner <christian.brauner@ubuntu.com>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 8561498e653c..2c25dc079cb9 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -92,7 +92,8 @@ long sys_swapcontext(struct ucontext __user *old_ctx,
 long sys_debug_setcontext(struct ucontext __user *ctx,
 			  int ndbg, struct sig_dbg_op __user *dbg);
 int
-ppc_select(int n, fd_set __user *inp, fd_set __user *outp, fd_set __user *exp, struct timeval __user *tvp);
+ppc_select(int n, fd_set __user *inp, fd_set __user *outp, fd_set __user *exp,
+	   struct __kernel_old_timeval __user *tvp);
 unsigned long __init early_init(unsigned long dt_ptr);
 void __init machine_init(u64 dt_ptr);
 #endif

commit af2e8c68b9c5403f77096969c516f742f5bb29e0
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Nov 13 21:05:44 2019 +1100

    KVM: PPC: Book3S HV: Flush link stack on guest exit to host kernel
    
    On some systems that are vulnerable to Spectre v2, it is up to
    software to flush the link stack (return address stack), in order to
    protect against Spectre-RSB.
    
    When exiting from a guest we do some house keeping and then
    potentially exit to C code which is several stack frames deep in the
    host kernel. We will then execute a series of returns without
    preceeding calls, opening up the possiblity that the guest could have
    poisoned the link stack, and direct speculative execution of the host
    to a gadget of some sort.
    
    To prevent this we add a flush of the link stack on exit from a guest.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 3ee92f692e64..d84d1417ddb6 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -153,9 +153,11 @@ void _kvmppc_save_tm_pr(struct kvm_vcpu *vcpu, u64 guest_msr);
 extern s32 patch__call_flush_count_cache;
 extern s32 patch__flush_count_cache_return;
 extern s32 patch__flush_link_stack_return;
+extern s32 patch__call_kvm_flush_link_stack;
 extern s32 patch__memset_nocache, patch__memcpy_nocache;
 
 extern long flush_count_cache;
+extern long kvm_flush_link_stack;
 
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 void kvmppc_save_tm_hv(struct kvm_vcpu *vcpu, u64 msr, bool preserve_nv);

commit 39e72bf96f5847ba87cc5bd7a3ce0fed813dc9ad
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Nov 13 21:05:41 2019 +1100

    powerpc/book3s64: Fix link stack flush on context switch
    
    In commit ee13cb249fab ("powerpc/64s: Add support for software count
    cache flush"), I added support for software to flush the count
    cache (indirect branch cache) on context switch if firmware told us
    that was the required mitigation for Spectre v2.
    
    As part of that code we also added a software flush of the link
    stack (return address stack), which protects against Spectre-RSB
    between user processes.
    
    That is all correct for CPUs that activate that mitigation, which is
    currently Power9 Nimbus DD2.3.
    
    What I got wrong is that on older CPUs, where firmware has disabled
    the count cache, we also need to flush the link stack on context
    switch.
    
    To fix it we create a new feature bit which is not set by firmware,
    which tells us we need to flush the link stack. We set that when
    firmware tells us that either of the existing Spectre v2 mitigations
    are enabled.
    
    Then we adjust the patching code so that if we see that feature bit we
    enable the link stack flush. If we're also told to flush the count
    cache in software then we fall through and do that also.
    
    On the older CPUs we don't need to do do the software count cache
    flush, firmware has disabled it, so in that case we patch in an early
    return after the link stack flush.
    
    The naming of some of the functions is awkward after this patch,
    because they're called "count cache" but they also do link stack. But
    we'll fix that up in a later commit to ease backporting.
    
    This is the fix for CVE-2019-18660.
    
    Reported-by: Anthony Steinhauser <asteinhauser@google.com>
    Fixes: ee13cb249fab ("powerpc/64s: Add support for software count cache flush")
    Cc: stable@vger.kernel.org # v4.4+
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 8561498e653c..3ee92f692e64 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -152,6 +152,7 @@ void _kvmppc_save_tm_pr(struct kvm_vcpu *vcpu, u64 guest_msr);
 /* Patch sites */
 extern s32 patch__call_flush_count_cache;
 extern s32 patch__flush_count_cache_return;
+extern s32 patch__flush_link_stack_return;
 extern s32 patch__memset_nocache, patch__memcpy_nocache;
 
 extern long flush_count_cache;

commit 370011a27028d6f05e598ed6211a0ca2dc0213f7
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Thu Sep 5 23:50:29 2019 +0530

    powerpc/ftrace: Enable HAVE_FUNCTION_GRAPH_RET_ADDR_PTR
    
    This associates entries in the ftrace_ret_stack with corresponding stack
    frames, enabling more robust stack unwinding. Also update the only user
    of ftrace_graph_ret_addr() to pass the stack pointer.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/0224f2d0971b069c678e2ff678cfc2cd1e114cfe.1567707399.git.naveen.n.rao@linux.vnet.ibm.com

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 49196d35e3bb..8561498e653c 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -134,7 +134,8 @@ extern int __ucmpdi2(u64, u64);
 
 /* tracing */
 void _mcount(void);
-unsigned long prepare_ftrace_return(unsigned long parent, unsigned long ip);
+unsigned long prepare_ftrace_return(unsigned long parent, unsigned long ip,
+						unsigned long sp);
 
 void pnv_power9_force_smt4_catch(void);
 void pnv_power9_force_smt4_release(void);

commit 136bc0397ae21dbf63ca02e5775ad353a479cd2f
Author: Thiago Jung Bauermann <bauerman@linux.ibm.com>
Date:   Mon Aug 19 23:13:12 2019 -0300

    powerpc/pseries: Introduce option to build secure virtual machines
    
    Introduce CONFIG_PPC_SVM to control support for secure guests and include
    Ultravisor-related helpers when it is selected
    
    Signed-off-by: Thiago Jung Bauermann <bauerman@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190820021326.6884-3-bauerman@linux.ibm.com

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index e698f48cbc6d..49196d35e3bb 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -36,7 +36,7 @@ void __trace_hcall_entry(unsigned long opcode, unsigned long *args);
 void __trace_hcall_exit(long opcode, long retval, unsigned long *retbuf);
 
 /* Ultravisor */
-#ifdef CONFIG_PPC_POWERNV
+#if defined(CONFIG_PPC_POWERNV) || defined(CONFIG_PPC_SVM)
 long ucall_norets(unsigned long opcode, ...);
 #else
 static inline long ucall_norets(unsigned long opcode, ...)

commit a49dddbdb0cca1d00fc9251e543a0aac09a6a65b
Author: Claudio Carvalho <cclaudio@linux.ibm.com>
Date:   Thu Aug 22 00:48:33 2019 -0300

    powerpc/kernel: Add ucall_norets() ultravisor call handler
    
    The ultracalls (ucalls for short) allow the Secure Virtual Machines
    (SVM)s and hypervisor to request services from the ultravisor such as
    accessing a register or memory region that can only be accessed when
    running in ultravisor-privileged mode.
    
    This patch adds the ucall_norets() ultravisor call handler.
    
    The specific service needed from an ucall is specified in register
    R3 (the first parameter to the ucall). Other parameters to the
    ucall, if any, are specified in registers R4 through R12.
    
    Return value of all ucalls is in register R3. Other output values
    from the ucall, if any, are returned in registers R4 through R12.
    
    Each ucall returns specific error codes, applicable in the context
    of the ucall. However, like with the PowerPC Architecture Platform
    Reference (PAPR), if no specific error code is defined for a particular
    situation, then the ucall will fallback to an erroneous
    parameter-position based code. i.e U_PARAMETER, U_P2, U_P3 etc depending
    on the ucall parameter that may have caused the error.
    
    Every host kernel (powernv) needs to be able to do ucalls in case it
    ends up being run in a machine with ultravisor enabled. Otherwise, the
    kernel may crash early in boot trying to access ultravisor resources,
    for instance, trying to set the partition table entry 0. Secure guests
    also need to be able to do ucalls and its kernel may not have
    CONFIG_PPC_POWERNV=y. For that reason, the ucall.S file is placed under
    arch/powerpc/kernel.
    
    If ultravisor is not enabled, the ucalls will be redirected to the
    hypervisor which must handle/fail the call.
    
    Thanks to inputs from Ram Pai and Michael Anderson.
    
    Signed-off-by: Claudio Carvalho <cclaudio@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190822034838.27876-3-cclaudio@linux.ibm.com

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index ec1c97a8e8cb..e698f48cbc6d 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -15,6 +15,7 @@
 #include <asm/epapr_hcalls.h>
 #include <asm/dcr.h>
 #include <asm/mmu_context.h>
+#include <asm/ultravisor-api.h>
 
 #include <uapi/asm/ucontext.h>
 
@@ -34,6 +35,16 @@ extern struct static_key hcall_tracepoint_key;
 void __trace_hcall_entry(unsigned long opcode, unsigned long *args);
 void __trace_hcall_exit(long opcode, long retval, unsigned long *retbuf);
 
+/* Ultravisor */
+#ifdef CONFIG_PPC_POWERNV
+long ucall_norets(unsigned long opcode, ...);
+#else
+static inline long ucall_norets(unsigned long opcode, ...)
+{
+	return U_NOT_AVAILABLE;
+}
+#endif
+
 /* OPAL */
 int64_t __opal_call(int64_t a0, int64_t a1, int64_t a2, int64_t a3,
 		    int64_t a4, int64_t a5, int64_t a6, int64_t a7,

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 296584e6dd55..ec1c97a8e8cb 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 #ifndef _ASM_POWERPC_ASM_PROTOTYPES_H
 #define _ASM_POWERPC_ASM_PROTOTYPES_H
 /*
@@ -5,11 +6,6 @@
  * from asm, and any associated variables.
  *
  * Copyright 2016, Daniel Axtens, IBM Corporation.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version 2
- * of the License, or (at your option) any later version.
  */
 
 #include <linux/threads.h>

commit bd3524feac214f0ab9693c6d4c0cb5be8e1318b9
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Mar 1 22:56:36 2019 +1000

    powerpc/64s: Fix unrelocated interrupt trampoline address test
    
    The recent commit got this test wrong, it declared the assembler
    symbols the wrong way, and also used the wrong symbol name
    (xxx_start rather than start_xxx, see asm/head-64.h).
    
    Fixes: ccd477028a ("powerpc/64s: Fix HV NMI vs HV interrupt recoverability test")
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index effdd096fa4c..296584e6dd55 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -49,14 +49,6 @@ int exit_vmx_usercopy(void);
 int enter_vmx_ops(void);
 void *exit_vmx_ops(void *dest);
 
-/* Exceptions */
-#ifdef CONFIG_PPC_POWERNV
-extern unsigned long real_trampolines_start;
-extern unsigned long real_trampolines_end;
-extern unsigned long virt_trampolines_start;
-extern unsigned long virt_trampolines_end;
-#endif
-
 /* Traps */
 long machine_check_early(struct pt_regs *regs);
 long hmi_exception_realmode(struct pt_regs *regs);

commit 75d9fc7fd94eb43cdf0bec04499a27ced780af19
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Feb 26 19:30:35 2019 +1000

    powerpc/powernv: move OPAL call wrapper tracing and interrupt handling to C
    
    The OPAL call wrapper gets interrupt disabling wrong. It disables
    interrupts just by clearing MSR[EE], which has two problems:
    
    - It doesn't call into the IRQ tracing subsystem, which means tracing
      across OPAL calls does not always notice IRQs have been disabled.
    
    - It doesn't go through the IRQ soft-mask code, which causes a minor
      bug. MSR[EE] can not be restored by saving the MSR then clearing
      MSR[EE], because a racing interrupt while soft-masked could clear
      MSR[EE] between the two steps. This can cause MSR[EE] to be
      incorrectly enabled when the OPAL call returns. Fortunately that
      should only result in another masked interrupt being taken to
      disable MSR[EE] again, but it's a bit sloppy.
    
    The existing code also saves MSR to PACA, which is not re-entrant if
    there is a nested OPAL call from different MSR contexts, which can
    happen these days with SRESET interrupts on bare metal.
    
    To fix these issues, move the tracing and IRQ handling code to C, and
    call into asm just for the low level call when everything is ready to
    go. Save the MSR on stack rather than PACA.
    
    Performance cost is kept to a minimum with a few optimisations:
    
    - The endian switch upon return is combined with the MSR restore,
      which avoids an expensive context synchronizing operation for LE
      kernels. This makes up for the additional mtmsrd to enable
      interrupts with local_irq_enable().
    
    - blr is now used to return from the opal_* functions that are called
      as C functions, to avoid link stack corruption. This requires a
      skiboot fix as well to keep the call stack balanced.
    
    A NULL call is more costly after this, (410ns->430ns on POWER9), but
    OPAL calls are generally not performance critical at this scale.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index e01f31fb0865..effdd096fa4c 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -37,13 +37,11 @@ void kexec_copy_flush(struct kimage *image);
 extern struct static_key hcall_tracepoint_key;
 void __trace_hcall_entry(unsigned long opcode, unsigned long *args);
 void __trace_hcall_exit(long opcode, long retval, unsigned long *retbuf);
-/* OPAL tracing */
-#ifdef CONFIG_JUMP_LABEL
-extern struct static_key opal_tracepoint_key;
-#endif
 
-void __trace_opal_entry(unsigned long opcode, unsigned long *args);
-void __trace_opal_exit(long opcode, unsigned long retval);
+/* OPAL */
+int64_t __opal_call(int64_t a0, int64_t a1, int64_t a2, int64_t a3,
+		    int64_t a4, int64_t a5, int64_t a6, int64_t a7,
+		    int64_t opcode, uint64_t msr);
 
 /* VMX copying */
 int enter_vmx_usercopy(void);

commit ccd477028a202993b9ddca5d2404fdaca3b7a55c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Feb 26 18:51:07 2019 +1000

    powerpc/64s: Fix HV NMI vs HV interrupt recoverability test
    
    HV interrupts that use HSRR registers do not enter with MSR[RI] clear,
    but their entry code is not recoverable vs NMI, due to shared use of
    HSPRG1 as a scratch register to save r13.
    
    This means that a system reset or machine check that hits in HSRR
    interrupt entry can cause r13 to be silently corrupted.
    
    Fix this by marking NMIs non-recoverable if they land in HV interrupt
    ranges.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 1484df6779ab..e01f31fb0865 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -51,6 +51,14 @@ int exit_vmx_usercopy(void);
 int enter_vmx_ops(void);
 void *exit_vmx_ops(void *dest);
 
+/* Exceptions */
+#ifdef CONFIG_PPC_POWERNV
+extern unsigned long real_trampolines_start;
+extern unsigned long real_trampolines_end;
+extern unsigned long virt_trampolines_start;
+extern unsigned long virt_trampolines_end;
+#endif
+
 /* Traps */
 long machine_check_early(struct pt_regs *regs);
 long hmi_exception_realmode(struct pt_regs *regs);

commit 7c19c2e5f9c18e364a306253065474e5f6ad960c
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:09:02 2019 +0000

    powerpc: 'current_set' is now a table of task_struct pointers
    
    The table of pointers 'current_set' has been used for retrieving
    the stack and current. They used to be thread_info pointers as
    they were pointing to the stack and current was taken from the
    'task' field of the thread_info.
    
    Now, the pointers of 'current_set' table are now both pointers
    to task_struct and pointers to thread_info.
    
    As they are used to get current, and the stack pointer is
    retrieved from current's stack field, this patch changes
    their type to task_struct, and renames secondary_ti to
    secondary_current.
    
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 1d911f68a23b..1484df6779ab 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -23,8 +23,8 @@
 #include <uapi/asm/ucontext.h>
 
 /* SMP */
-extern struct thread_info *current_set[NR_CPUS];
-extern struct thread_info *secondary_ti;
+extern struct task_struct *current_set[NR_CPUS];
+extern struct task_struct *secondary_current;
 void start_secondary(void *unused);
 
 /* kexec */

commit e9666d10a5677a494260d60d1fa0b73cc7646eb3
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Mon Dec 31 00:14:15 2018 +0900

    jump_label: move 'asm goto' support test to Kconfig
    
    Currently, CONFIG_JUMP_LABEL just means "I _want_ to use jump label".
    
    The jump label is controlled by HAVE_JUMP_LABEL, which is defined
    like this:
    
      #if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
      # define HAVE_JUMP_LABEL
      #endif
    
    We can improve this by testing 'asm goto' support in Kconfig, then
    make JUMP_LABEL depend on CC_HAS_ASM_GOTO.
    
    Ugly #ifdef HAVE_JUMP_LABEL will go away, and CONFIG_JUMP_LABEL will
    match to the real kernel capability.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 6f201b199c02..1d911f68a23b 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -38,7 +38,7 @@ extern struct static_key hcall_tracepoint_key;
 void __trace_hcall_entry(unsigned long opcode, unsigned long *args);
 void __trace_hcall_exit(long opcode, long retval, unsigned long *retbuf);
 /* OPAL tracing */
-#ifdef HAVE_JUMP_LABEL
+#ifdef CONFIG_JUMP_LABEL
 extern struct static_key opal_tracepoint_key;
 #endif
 

commit f91203e71c64202b4b20fd0c2d5e9471b4c5cd6c
Author: Breno Leitao <leitao@debian.org>
Date:   Thu Nov 1 18:54:01 2018 -0300

    powerpc/mm: remove unused function prototype
    
    Commit f384796c40dc ("powerpc/mm: Add support for handling > 512TB address
    in SLB miss") removed function slb_miss_bad_addr(struct pt_regs *regs), but
    kept its declaration in the prototype file. This patch simply removes the
    function definition.
    
    Signed-off-by: Breno Leitao <leitao@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index ec691d489656..6f201b199c02 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -61,7 +61,6 @@ void RunModeException(struct pt_regs *regs);
 void single_step_exception(struct pt_regs *regs);
 void program_check_exception(struct pt_regs *regs);
 void alignment_exception(struct pt_regs *regs);
-void slb_miss_bad_addr(struct pt_regs *regs);
 void StackOverflow(struct pt_regs *regs);
 void kernel_fp_unavailable_exception(struct pt_regs *regs);
 void altivec_unavailable_exception(struct pt_regs *regs);

commit 685f7e4f161425b137056abe35ba8ef7b669d83d
Merge: c7a2c49ea6c9 58cfbac25b1f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 26 14:36:21 2018 -0700

    Merge tag 'powerpc-4.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - A large series to rewrite our SLB miss handling, replacing a lot of
         fairly complicated asm with much fewer lines of C.
    
       - Following on from that, we now maintain a cache of SLB entries for
         each process and preload them on context switch. Leading to a 27%
         speedup for our context switch benchmark on Power9.
    
       - Improvements to our handling of SLB multi-hit errors. We now print
         more debug information when they occur, and try to continue running
         by flushing the SLB and reloading, rather than treating them as
         fatal.
    
       - Enable THP migration on 64-bit Book3S machines (eg. Power7/8/9).
    
       - Add support for physical memory up to 2PB in the linear mapping on
         64-bit Book3S. We only support up to 512TB as regular system
         memory, otherwise the percpu allocator runs out of vmalloc space.
    
       - Add stack protector support for 32 and 64-bit, with a per-task
         canary.
    
       - Add support for PTRACE_SYSEMU and PTRACE_SYSEMU_SINGLESTEP.
    
       - Support recognising "big cores" on Power9, where two SMT4 cores are
         presented to us as a single SMT8 core.
    
       - A large series to cleanup some of our ioremap handling and PTE
         flags.
    
       - Add a driver for the PAPR SCM (storage class memory) interface,
         allowing guests to operate on SCM devices (acked by Dan).
    
       - Changes to our ftrace code to handle very large kernels, where we
         need to use a trampoline to get to ftrace_caller().
    
      And many other smaller enhancements and cleanups.
    
      Thanks to: Alan Modra, Alistair Popple, Aneesh Kumar K.V, Anton
      Blanchard, Aravinda Prasad, Bartlomiej Zolnierkiewicz, Benjamin
      Herrenschmidt, Breno Leitao, Cédric Le Goater, Christophe Leroy,
      Christophe Lombard, Dan Carpenter, Daniel Axtens, Finn Thain, Gautham
      R. Shenoy, Gustavo Romero, Haren Myneni, Hari Bathini, Jia Hongtao,
      Joel Stanley, John Allen, Laurent Dufour, Madhavan Srinivasan, Mahesh
      Salgaonkar, Mark Hairgrove, Masahiro Yamada, Michael Bringmann,
      Michael Neuling, Michal Suchanek, Murilo Opsfelder Araujo, Nathan
      Fontenot, Naveen N. Rao, Nicholas Piggin, Nick Desaulniers, Oliver
      O'Halloran, Paul Mackerras, Petr Vorel, Rashmica Gupta, Reza Arbab,
      Rob Herring, Sam Bobroff, Samuel Mendoza-Jonas, Scott Wood, Stan
      Johnson, Stephen Rothwell, Stewart Smith, Suraj Jitindar Singh, Tyrel
      Datwyler, Vaibhav Jain, Vasant Hegde, YueHaibing, zhong jiang"
    
    * tag 'powerpc-4.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (221 commits)
      Revert "selftests/powerpc: Fix out-of-tree build errors"
      powerpc/msi: Fix compile error on mpc83xx
      powerpc: Fix stack protector crashes on CPU hotplug
      powerpc/traps: restore recoverability of machine_check interrupts
      powerpc/64/module: REL32 relocation range check
      powerpc/64s/radix: Fix radix__flush_tlb_collapsed_pmd double flushing pmd
      selftests/powerpc: Add a test of wild bctr
      powerpc/mm: Fix page table dump to work on Radix
      powerpc/mm/radix: Display if mappings are exec or not
      powerpc/mm/radix: Simplify split mapping logic
      powerpc/mm/radix: Remove the retry in the split mapping logic
      powerpc/mm/radix: Fix small page at boundary when splitting
      powerpc/mm/radix: Fix overuse of small pages in splitting logic
      powerpc/mm/radix: Fix off-by-one in split mapping logic
      powerpc/ftrace: Handle large kernel configs
      powerpc/mm: Fix WARN_ON with THP NUMA migration
      selftests/powerpc: Fix out-of-tree build errors
      powerpc/time: no steal_time when CONFIG_PPC_SPLPAR is not selected
      powerpc/time: Only set CONFIG_ARCH_HAS_SCALED_CPUTIME on PPC64
      powerpc/time: isolate scaled cputime accounting in dedicated functions.
      ...

commit 48e7b76957457f9a6f086ca2bbe49ec1ffd75f84
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:51 2018 +1000

    powerpc/64s/hash: Convert SLB miss handlers to C
    
    This patch moves SLB miss handlers completely to C, using the standard
    exception handler macros to set up the stack and branch to C.
    
    This can be done because the segment containing the kernel stack is
    always bolted, so accessing it with relocation on will not cause an
    SLB exception.
    
    Arbitrary kernel memory must not be accessed when handling kernel
    space SLB misses, so care should be taken there. However user SLB
    misses can access any kernel memory, which can be used to move some
    fields out of the paca (in later patches).
    
    User SLB misses could quite easily reconcile IRQs and set up a first
    class kernel environment and exit via ret_from_except, however that
    doesn't seem to be necessary at the moment, so we only do that if a
    bad fault is encountered.
    
    [ Credit to Aneesh for bug fixes, error checks, and improvements to
      bad address handling, etc ]
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Disallow tracing for all of slb.c for now.]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 9bc98c239305..2741831482f4 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -77,6 +77,8 @@ void kernel_bad_stack(struct pt_regs *regs);
 void system_reset_exception(struct pt_regs *regs);
 void machine_check_exception(struct pt_regs *regs);
 void emulation_assist_interrupt(struct pt_regs *regs);
+long do_slb_fault(struct pt_regs *regs, unsigned long ea);
+void do_bad_slb_fault(struct pt_regs *regs, unsigned long ea, long err);
 
 /* signals, syscalls and interrupts */
 long sys_swapcontext(struct ucontext __user *old_ctx,

commit 4bad77799fede9d95abaf499fff385608ee14877
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:31:06 2018 +1100

    KVM: PPC: Book3S HV: Handle hypercalls correctly when nested
    
    When we are running as a nested hypervisor, we use a hypercall to
    enter the guest rather than code in book3s_hv_rmhandlers.S.  This means
    that the hypercall handlers listed in hcall_real_table never get called.
    There are some hypercalls that are handled there and not in
    kvmppc_pseries_do_hcall(), which therefore won't get processed for
    a nested guest.
    
    To fix this, we add cases to kvmppc_pseries_do_hcall() to handle those
    hypercalls, with the following exceptions:
    
    - The HPT hypercalls (H_ENTER, H_REMOVE, etc.) are not handled because
      we only support radix mode for nested guests.
    
    - H_CEDE has to be handled specially because the cede logic in
      kvmhv_run_single_vcpu assumes that it has been processed by the time
      that kvmhv_p9_guest_entry() returns.  Therefore we put a special
      case for H_CEDE in kvmhv_p9_guest_entry().
    
    For the XICS hypercalls, if real-mode processing is enabled, then the
    virtual-mode handlers assume that they are being called only to finish
    up the operation.  Therefore we turn off the real-mode flag in the XICS
    code when running as a nested hypervisor.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 5c9b00cec4f2..c55ba3b4873b 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -167,4 +167,8 @@ void kvmhv_load_guest_pmu(struct kvm_vcpu *vcpu);
 
 int __kvmhv_vcpu_entry_p9(struct kvm_vcpu *vcpu);
 
+long kvmppc_h_set_dabr(struct kvm_vcpu *vcpu, unsigned long dabr);
+long kvmppc_h_set_xdabr(struct kvm_vcpu *vcpu, unsigned long dabr,
+			unsigned long dabrx);
+
 #endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */

commit 95a6432ce903858a2f285d611275340aa574c6ac
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:30:55 2018 +1100

    KVM: PPC: Book3S HV: Streamlined guest entry/exit path on P9 for radix guests
    
    This creates an alternative guest entry/exit path which is used for
    radix guests on POWER9 systems when we have indep_threads_mode=Y.  In
    these circumstances there is exactly one vcpu per vcore and there is
    no coordination required between vcpus or vcores; the vcpu can enter
    the guest without needing to synchronize with anything else.
    
    The new fast path is implemented almost entirely in C in book3s_hv.c
    and runs with the MMU on until the guest is entered.  On guest exit
    we use the existing path until the point where we are committed to
    exiting the guest (as distinct from handling an interrupt in the
    low-level code and returning to the guest) and we have pulled the
    guest context from the XIVE.  At that point we check a flag in the
    stack frame to see whether we came in via the old path and the new
    path; if we came in via the new path then we go back to C code to do
    the rest of the process of saving the guest context and restoring the
    host context.
    
    The C code is split into separate functions for handling the
    OS-accessible state and the hypervisor state, with the idea that the
    latter can be replaced by a hypercall when we implement nested
    virtualization.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    [mpe: Fix CONFIG_ALTIVEC=n build]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 0c1a2b01512a..5c9b00cec4f2 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -165,4 +165,6 @@ void kvmhv_load_host_pmu(void);
 void kvmhv_save_guest_pmu(struct kvm_vcpu *vcpu, bool pmu_in_use);
 void kvmhv_load_guest_pmu(struct kvm_vcpu *vcpu);
 
+int __kvmhv_vcpu_entry_p9(struct kvm_vcpu *vcpu);
+
 #endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */

commit 7854f7545bff69567fc3fcd8831752138b892483
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:30:53 2018 +1100

    KVM: PPC: Book3S: Rework TM save/restore code and make it C-callable
    
    This adds a parameter to __kvmppc_save_tm and __kvmppc_restore_tm
    which allows the caller to indicate whether it wants the nonvolatile
    register state to be preserved across the call, as required by the C
    calling conventions.  This parameter being non-zero also causes the
    MSR bits that enable TM, FP, VMX and VSX to be preserved.  The
    condition register and DSCR are now always preserved.
    
    With this, kvmppc_save_tm_hv and kvmppc_restore_tm_hv can be called
    from C code provided the 3rd parameter is non-zero.  So that these
    functions can be called from modules, they now include code to set
    the TOC pointer (r2) on entry, as they can call other built-in C
    functions which will assume the TOC to have been set.
    
    Also, the fake suspend code in kvmppc_save_tm_hv is modified here to
    assume that treclaim in fake-suspend state does not modify any registers,
    which is the case on POWER9.  This enables the code to be simplified
    quite a bit.
    
    _kvmppc_save_tm_pr and _kvmppc_restore_tm_pr become much simpler with
    this change, since they now only need to save and restore TAR and pass
    1 for the 3rd argument to __kvmppc_{save,restore}_tm.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 024e8fc905d7..0c1a2b01512a 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -150,6 +150,16 @@ extern s32 patch__memset_nocache, patch__memcpy_nocache;
 
 extern long flush_count_cache;
 
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+void kvmppc_save_tm_hv(struct kvm_vcpu *vcpu, u64 msr, bool preserve_nv);
+void kvmppc_restore_tm_hv(struct kvm_vcpu *vcpu, u64 msr, bool preserve_nv);
+#else
+static inline void kvmppc_save_tm_hv(struct kvm_vcpu *vcpu, u64 msr,
+				     bool preserve_nv) { }
+static inline void kvmppc_restore_tm_hv(struct kvm_vcpu *vcpu, u64 msr,
+					bool preserve_nv) { }
+#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
+
 void kvmhv_save_host_pmu(void);
 void kvmhv_load_host_pmu(void);
 void kvmhv_save_guest_pmu(struct kvm_vcpu *vcpu, bool pmu_in_use);

commit 41f4e631daf80d16d818ac17c4144bd5b9a11f33
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:30:51 2018 +1100

    KVM: PPC: Book3S HV: Extract PMU save/restore operations as C-callable functions
    
    This pulls out the assembler code that is responsible for saving and
    restoring the PMU state for the host and guest into separate functions
    so they can be used from an alternate entry path.  The calling
    convention is made compatible with C.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 1f4691ce4126..024e8fc905d7 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -150,4 +150,9 @@ extern s32 patch__memset_nocache, patch__memcpy_nocache;
 
 extern long flush_count_cache;
 
+void kvmhv_save_host_pmu(void);
+void kvmhv_load_host_pmu(void);
+void kvmhv_save_guest_pmu(struct kvm_vcpu *vcpu, bool pmu_in_use);
+void kvmhv_load_guest_pmu(struct kvm_vcpu *vcpu);
+
 #endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */

commit 51423a9c9b09352bea1c53b8324db78bf3b170d1
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Sep 25 14:10:04 2018 +0000

    powerpc/traps: merge unrecoverable_exception() and nonrecoverable_exception()
    
    PPC32 uses nonrecoverable_exception() while PPC64 uses
    unrecoverable_exception().
    
    Both functions are doing almost the same thing.
    
    This patch removes nonrecoverable_exception()
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 1f4691ce4126..9bc98c239305 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -63,7 +63,6 @@ void program_check_exception(struct pt_regs *regs);
 void alignment_exception(struct pt_regs *regs);
 void slb_miss_bad_addr(struct pt_regs *regs);
 void StackOverflow(struct pt_regs *regs);
-void nonrecoverable_exception(struct pt_regs *regs);
 void kernel_fp_unavailable_exception(struct pt_regs *regs);
 void altivec_unavailable_exception(struct pt_regs *regs);
 void vsx_unavailable_exception(struct pt_regs *regs);

commit 54be0b9c7c9888ebe63b89a31a17ee3df6a68d61
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Oct 2 23:56:39 2018 +1000

    Revert "convert SLB miss handlers to C" and subsequent commits
    
    This reverts commits:
      5e46e29e6a97 ("powerpc/64s/hash: convert SLB miss handlers to C")
      8fed04d0f6ae ("powerpc/64s/hash: remove user SLB data from the paca")
      655deecf67b2 ("powerpc/64s/hash: SLB allocation status bitmaps")
      2e1626744e8d ("powerpc/64s/hash: provide arch_setup_exec hooks for hash slice setup")
      89ca4e126a3f ("powerpc/64s/hash: Add a SLB preload cache")
    
    This series had a few bugs, and the fixes are not all trivial. So
    revert most of it for now.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 78ed3c3f879a..1f4691ce4126 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -78,8 +78,6 @@ void kernel_bad_stack(struct pt_regs *regs);
 void system_reset_exception(struct pt_regs *regs);
 void machine_check_exception(struct pt_regs *regs);
 void emulation_assist_interrupt(struct pt_regs *regs);
-long do_slb_fault(struct pt_regs *regs, unsigned long ea);
-void do_bad_slb_fault(struct pt_regs *regs, unsigned long ea, long err);
 
 /* signals, syscalls and interrupts */
 long sys_swapcontext(struct ucontext __user *old_ctx,

commit 5e46e29e6a977a71f6b5bed414b7bcdbff5a6a43
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:51 2018 +1000

    powerpc/64s/hash: convert SLB miss handlers to C
    
    This patch moves SLB miss handlers completely to C, using the standard
    exception handler macros to set up the stack and branch to C.
    
    This can be done because the segment containing the kernel stack is
    always bolted, so accessing it with relocation on will not cause an
    SLB exception.
    
    Arbitrary kernel memory may not be accessed when handling kernel space
    SLB misses, so care should be taken there. However user SLB misses can
    access any kernel memory, which can be used to move some fields out of
    the paca (in later patches).
    
    User SLB misses could quite easily reconcile IRQs and set up a first
    class kernel environment and exit via ret_from_except, however that
    doesn't seem to be necessary at the moment, so we only do that if a
    bad fault is encountered.
    
    [ Credit to Aneesh for bug fixes, error checks, and improvements to bad
      address handling, etc ]
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    
    Since RFC:
    - Added MSR[RI] handling
    - Fixed up a register loss bug exposed by irq tracing (Aneesh)
    - Reject misses outside the defined kernel regions (Aneesh)
    - Added several more sanity checks and error handling (Aneesh), we may
      look at consolidating these tests and tightenig up the code but for
      a first pass we decided it's better to check carefully.
    
    Since v1:
    - Fixed SLB cache corruption (Aneesh)
    - Fixed untidy SLBE allocation "leak" in get_vsid error case
    - Now survives some stress testing on real hardware
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 1f4691ce4126..78ed3c3f879a 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -78,6 +78,8 @@ void kernel_bad_stack(struct pt_regs *regs);
 void system_reset_exception(struct pt_regs *regs);
 void machine_check_exception(struct pt_regs *regs);
 void emulation_assist_interrupt(struct pt_regs *regs);
+long do_slb_fault(struct pt_regs *regs, unsigned long ea);
+void do_bad_slb_fault(struct pt_regs *regs, unsigned long ea, long err);
 
 /* signals, syscalls and interrupts */
 long sys_swapcontext(struct ucontext __user *old_ctx,

commit fa54a981ea7a852c145b05c95abba11e81fc1157
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Aug 9 08:14:41 2018 +0000

    powerpc/lib: Use patch_site to patch copy_32 functions once cache is enabled
    
    The symbol memcpy_nocache_branch defined in order to allow patching
    of memset function once cache is enabled leads to confusing reports
    by perf tool.
    
    Using the new patch_site functionality solves this issue.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 70fdc5b9b9fb..1f4691ce4126 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -146,6 +146,7 @@ void _kvmppc_save_tm_pr(struct kvm_vcpu *vcpu, u64 guest_msr);
 /* Patch sites */
 extern s32 patch__call_flush_count_cache;
 extern s32 patch__flush_count_cache_return;
+extern s32 patch__memset_nocache, patch__memcpy_nocache;
 
 extern long flush_count_cache;
 

commit ee13cb249fabdff8b90aaff61add347749280087
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jul 24 01:07:54 2018 +1000

    powerpc/64s: Add support for software count cache flush
    
    Some CPU revisions support a mode where the count cache needs to be
    flushed by software on context switch. Additionally some revisions may
    have a hardware accelerated flush, in which case the software flush
    sequence can be shortened.
    
    If we detect the appropriate flag from firmware we patch a branch
    into _switch() which takes us to a count cache flush sequence.
    
    That sequence in turn may be patched to return early if we detect that
    the CPU supports accelerating the flush sequence in hardware.
    
    Add debugfs support for reporting the state of the flush, as well as
    runtime disabling it.
    
    And modify the spectre_v2 sysfs file to report the state of the
    software flush.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 769567b66c0c..70fdc5b9b9fb 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -143,4 +143,10 @@ struct kvm_vcpu;
 void _kvmppc_restore_tm_pr(struct kvm_vcpu *vcpu, u64 guest_msr);
 void _kvmppc_save_tm_pr(struct kvm_vcpu *vcpu, u64 guest_msr);
 
+/* Patch sites */
+extern s32 patch__call_flush_count_cache;
+extern s32 patch__flush_count_cache_return;
+
+extern long flush_count_cache;
+
 #endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */

commit d58badfb7cf1792ab4f1d0cd7896d733b85d650f
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Thu Jun 7 09:57:53 2018 +0800

    powerpc/64: enhance memcmp() with VMX instruction for long bytes comparision
    
    This patch add VMX primitives to do memcmp() in case the compare size
    is equal or greater than 4K bytes. KSM feature can benefit from this.
    
    Test result with following test program(replace the "^>" with ""):
    ------
    ># cat tools/testing/selftests/powerpc/stringloops/memcmp.c
    >#include <malloc.h>
    >#include <stdlib.h>
    >#include <string.h>
    >#include <time.h>
    >#include "utils.h"
    >#define SIZE (1024 * 1024 * 900)
    >#define ITERATIONS 40
    
    int test_memcmp(const void *s1, const void *s2, size_t n);
    
    static int testcase(void)
    {
            char *s1;
            char *s2;
            unsigned long i;
    
            s1 = memalign(128, SIZE);
            if (!s1) {
                    perror("memalign");
                    exit(1);
            }
    
            s2 = memalign(128, SIZE);
            if (!s2) {
                    perror("memalign");
                    exit(1);
            }
    
            for (i = 0; i < SIZE; i++)  {
                    s1[i] = i & 0xff;
                    s2[i] = i & 0xff;
            }
            for (i = 0; i < ITERATIONS; i++) {
                    int ret = test_memcmp(s1, s2, SIZE);
    
                    if (ret) {
                            printf("return %d at[%ld]! should have returned zero\n", ret, i);
                            abort();
                    }
            }
    
            return 0;
    }
    
    int main(void)
    {
            return test_harness(testcase, "memcmp");
    }
    ------
    Without this patch (but with the first patch "powerpc/64: Align bytes
    before fall back to .Lshort in powerpc64 memcmp()." in the series):
            4.726728762 seconds time elapsed                                          ( +-  3.54%)
    With VMX patch:
            4.234335473 seconds time elapsed                                          ( +-  2.63%)
                    There is ~+10% improvement.
    
    Testing with unaligned and different offset version (make s1 and s2 shift
    random offset within 16 bytes) can archieve higher improvement than 10%..
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 7841b8a60657..769567b66c0c 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -48,8 +48,8 @@ void __trace_opal_exit(long opcode, unsigned long retval);
 /* VMX copying */
 int enter_vmx_usercopy(void);
 int exit_vmx_usercopy(void);
-int enter_vmx_copy(void);
-void * exit_vmx_copy(void *dest);
+int enter_vmx_ops(void);
+void *exit_vmx_ops(void *dest);
 
 /* Traps */
 long machine_check_early(struct pt_regs *regs);

commit 09027ab73b82ec773f6ab8ae9468d7e15b748dfa
Merge: 273ba45796c1 f61e0d3cc4ae
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jun 14 17:42:54 2018 +0200

    Merge tag 'kvm-ppc-next-4.18-2' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD

commit 481c63acba2559139d0b46db0d83caca05c3cf63
Merge: 667416f38554 eacbb218fbba
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sun Jun 3 20:23:54 2018 +1000

    Merge branch 'topic/ppc-kvm' into next
    
    Merge in some commits we're sharing with the kvm-ppc tree.

commit caa3be92bebc5b87a221900ac408aa99b0badf3d
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Wed May 23 15:01:50 2018 +0800

    KVM: PPC: Book3S PR: Add C function wrapper for _kvmppc_save/restore_tm()
    
    Currently __kvmppc_save/restore_tm() APIs can only be invoked from
    assembly function. This patch adds C function wrappers for them so
    that they can be safely called from C function.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index dfdcb2374c28..5da683bebc7f 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -141,7 +141,13 @@ unsigned long prepare_ftrace_return(unsigned long parent, unsigned long ip);
 void pnv_power9_force_smt4_catch(void);
 void pnv_power9_force_smt4_release(void);
 
+/* Transaction memory related */
 void tm_enable(void);
 void tm_disable(void);
 void tm_abort(uint8_t cause);
+
+struct kvm_vcpu;
+void _kvmppc_restore_tm_pr(struct kvm_vcpu *vcpu, u64 guest_msr);
+void _kvmppc_save_tm_pr(struct kvm_vcpu *vcpu, u64 guest_msr);
+
 #endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */

commit eacbb218fbbab5923775059f7232a9622dc47b2a
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Wed May 23 15:01:46 2018 +0800

    powerpc: Export tm_enable()/tm_disable/tm_abort() APIs
    
    This patch exports tm_enable()/tm_disable/tm_abort() APIs, which
    will be used for PR KVM transactional memory logic.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Reviewed-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index d9713ad62e3c..dfdcb2374c28 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -141,4 +141,7 @@ unsigned long prepare_ftrace_return(unsigned long parent, unsigned long ip);
 void pnv_power9_force_smt4_catch(void);
 void pnv_power9_force_smt4_release(void);
 
+void tm_enable(void);
+void tm_disable(void);
+void tm_abort(uint8_t cause);
 #endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */

commit f3675644e172301e88354dc7bfca96c124301145
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed May 2 23:20:47 2018 +1000

    powerpc/syscalls: signal_{32, 64} - switch to SYSCALL_DEFINE
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    [mpe: Fix sys_debug_setcontext() prototype to return long]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 068760d61e7e..6f661e3757c8 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -80,18 +80,12 @@ void machine_check_exception(struct pt_regs *regs);
 void emulation_assist_interrupt(struct pt_regs *regs);
 
 /* signals, syscalls and interrupts */
-#ifdef CONFIG_PPC64
-int sys_swapcontext(struct ucontext __user *old_ctx,
-		    struct ucontext __user *new_ctx,
-		    long ctx_size, long r6, long r7, long r8, struct pt_regs *regs);
-#else
 long sys_swapcontext(struct ucontext __user *old_ctx,
 		    struct ucontext __user *new_ctx,
-		    int ctx_size, int r6, int r7, int r8, struct pt_regs *regs);
-int sys_debug_setcontext(struct ucontext __user *ctx,
-			 int ndbg, struct sig_dbg_op __user *dbg,
-			 int r6, int r7, int r8,
-			 struct pt_regs *regs);
+		    long ctx_size);
+#ifdef CONFIG_PPC32
+long sys_debug_setcontext(struct ucontext __user *ctx,
+			  int ndbg, struct sig_dbg_op __user *dbg);
 int
 ppc_select(int n, fd_set __user *inp, fd_set __user *outp, fd_set __user *exp, struct timeval __user *tvp);
 unsigned long __init early_init(unsigned long dt_ptr);

commit 8f2133cc0e1f9718f3e2d39b7587f4d6c9649a54
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon May 7 23:03:55 2018 +1000

    powerpc/pseries: hcall_exit tracepoint retval should be signed
    
    The hcall_exit() tracepoint has retval defined as unsigned long. That
    leads to humours results like:
    
      bash-3686  [009] d..2   854.134094: hcall_entry: opcode=24
      bash-3686  [009] d..2   854.134095: hcall_exit: opcode=24 retval=18446744073709551609
    
    It's normal for some hcalls to return negative values, displaying them
    as unsigned isn't very helpful. So change it to signed.
    
      bash-3711  [001] d..2   471.691008: hcall_entry: opcode=24
      bash-3711  [001] d..2   471.691008: hcall_exit: opcode=24 retval=-7
    
    Which can be more easily compared to H_NOT_FOUND in hvcall.h
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Tested-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index d9713ad62e3c..068760d61e7e 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -36,8 +36,7 @@ void kexec_copy_flush(struct kimage *image);
 /* pseries hcall tracing */
 extern struct static_key hcall_tracepoint_key;
 void __trace_hcall_entry(unsigned long opcode, unsigned long *args);
-void __trace_hcall_exit(long opcode, unsigned long retval,
-			unsigned long *retbuf);
+void __trace_hcall_exit(long opcode, long retval, unsigned long *retbuf);
 /* OPAL tracing */
 #ifdef HAVE_JUMP_LABEL
 extern struct static_key opal_tracepoint_key;

commit a26cf1c9fe3c2e3b671b490aeb708ea72fb5ac0a
Merge: 78e5dfea84dc 681c617b7c42
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Mar 24 08:43:18 2018 +1100

    Merge branch 'topic/ppc-kvm' into next
    
    This brings in two series from Paul, one of which touches KVM code and
    may need to be merged into the kvm-ppc tree to resolve conflicts.

commit 7672691a08c886e53ccbf8cdca406f8c92ec7a20
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Mar 21 21:32:00 2018 +1100

    powerpc/powernv: Provide a way to force a core into SMT4 mode
    
    POWER9 processors up to and including "Nimbus" v2.2 have hardware
    bugs relating to transactional memory and thread reconfiguration.
    One of these bugs has a workaround which is to get the core into
    SMT4 state temporarily.  This workaround is only needed when
    running bare-metal.
    
    This patch provides a function which gets the core into SMT4 mode
    by preventing threads from going to a stop state, and waking up
    those which are already in a stop state.  Once at least 3 threads
    are not in a stop state, the core will be in SMT4 and we can
    continue.
    
    To do this, we add a "dont_stop" flag to the paca to tell the
    thread not to go into a stop state.  If this flag is set,
    power9_idle_stop() just returns immediately with a return value
    of 0.  The pnv_power9_force_smt4_catch() function does the following:
    
    1. Set the dont_stop flag for each thread in the core, except
       ourselves (in fact we use an atomic_inc() in case more than
       one thread is calling this function concurrently).
    2. See how many threads are awake, indicated by their
       requested_psscr field in the paca being 0.  If this is at
       least 3, skip to step 5.
    3. Send a doorbell interrupt to each thread that was seen as
       being in a stop state in step 2.
    4. Until at least 3 threads are awake, scan the threads to which
       we sent a doorbell interrupt and check if they are awake now.
    
    This relies on the following properties:
    
    - Once dont_stop is non-zero, requested_psccr can't go from zero to
      non-zero, except transiently (and without the thread doing stop).
    - requested_psscr being zero guarantees that the thread isn't in
      a state-losing stop state where thread reconfiguration could occur.
    - Doing stop with a PSSCR value of 0 won't be a state-losing stop
      and thus won't allow thread reconfiguration.
    - Once threads_per_core/2 + 1 (i.e. 3) threads are awake, the core
      must be in SMT4 mode, since SMT modes are powers of 2.
    
    This does add a sync to power9_idle_stop(), which is necessary to
    provide the correct ordering between setting requested_psscr and
    checking dont_stop.  The overhead of the sync should be unnoticeable
    compared to the latency of going into and out of a stop state.
    
    Because some objected to incurring this extra latency on systems where
    the XER[SO] bug is not relevant, I have put the test in
    power9_idle_stop inside a feature section.  This means that
    pnv_power9_force_smt4_catch() WILL NOT WORK correctly on systems
    without the CPU_FTR_P9_TM_XER_SO_BUG feature bit set, and will
    probably hang the system.
    
    In order to cater for uses where the caller has an operation that
    has to be done while the core is in SMT4, the core continues to be
    kept in SMT4 after pnv_power9_force_smt4_catch() function returns,
    until the pnv_power9_force_smt4_release() function is called.
    It undoes the effect of step 1 above and allows the other threads
    to go into a stop state.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 7330150bfe34..4e14d2304d5f 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -126,4 +126,7 @@ extern int __ucmpdi2(u64, u64);
 void _mcount(void);
 unsigned long prepare_ftrace_return(unsigned long parent, unsigned long ip);
 
+void pnv_power9_force_smt4_catch(void);
+void pnv_power9_force_smt4_release(void);
+
 #endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */

commit e82d70cf965072a3872ea97b7d8df4b6f29fc09f
Author: Mathieu Malaterre <malat@debian.org>
Date:   Thu Mar 8 22:31:59 2018 +1100

    powerpc/32: Add missing prototypes for (early|machine)_init()
    
    early_init() and machine_init() have no prototype, add one in
    asm-prototypes.h.
    
    Fixes the following warnings (treated as error in W=1):
      arch/powerpc/kernel/setup_32.c:68:30: error: no previous prototype for ‘early_init’
      arch/powerpc/kernel/setup_32.c:99:21: error: no previous prototype for ‘machine_init’
    
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    [mpe: Move them to asm-prototypes.h, drop other functions]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 4d8b89b46018..0bdeff415a72 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -95,7 +95,10 @@ int sys_debug_setcontext(struct ucontext __user *ctx,
 			 struct pt_regs *regs);
 int
 ppc_select(int n, fd_set __user *inp, fd_set __user *outp, fd_set __user *exp, struct timeval __user *tvp);
+unsigned long __init early_init(unsigned long dt_ptr);
+void __init machine_init(u64 dt_ptr);
 #endif
+
 long ppc_fadvise64_64(int fd, int advice, u32 offset_high, u32 offset_low,
 		      u32 len_high, u32 len_low);
 long sys_switch_endian(void);

commit bf7fb32dd5fc8a6bd25aaab05d3acddd1223ba03
Author: Mathieu Malaterre <malat@debian.org>
Date:   Sun Feb 25 18:22:35 2018 +0100

    powerpc: Add missing prototypes for ppc_select() & ppc_fadvise64_64()
    
    Add missing prototypes for ppc_select() & ppc_fadvise64_64() to header
    asm-prototypes.h. Fix the following warnings (treated as errors in W=1)
    
      arch/powerpc/kernel/syscalls.c:87:1: error: no previous prototype for ‘ppc_select’
      arch/powerpc/kernel/syscalls.c:119:6: error: no previous prototype for ‘ppc_fadvise64_64’
    
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 7c23d9ead694..4d8b89b46018 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -93,7 +93,11 @@ int sys_debug_setcontext(struct ucontext __user *ctx,
 			 int ndbg, struct sig_dbg_op __user *dbg,
 			 int r6, int r7, int r8,
 			 struct pt_regs *regs);
+int
+ppc_select(int n, fd_set __user *inp, fd_set __user *outp, fd_set __user *exp, struct timeval __user *tvp);
 #endif
+long ppc_fadvise64_64(int fd, int advice, u32 offset_high, u32 offset_low,
+		      u32 len_high, u32 len_low);
 long sys_switch_endian(void);
 notrace unsigned int __check_irq_replay(void);
 void notrace restore_interrupts(void);

commit 0d60619e1c0ca20eb26103610349923451827688
Author: Mathieu Malaterre <malat@debian.org>
Date:   Sun Feb 25 18:22:32 2018 +0100

    powerpc: Add missing prototype for sys_debug_setcontext()
    
    In commit 81e7009ea46c ("powerpc: merge ppc signal.c and ppc64
    signal32.c") the function sys_debug_setcontext was added without a
    prototype.
    
    Fix compilation warning (treated as error in W=1):
      arch/powerpc/kernel/signal_32.c:1227:5: error: no previous prototype for ‘sys_debug_setcontext’
    
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 0af1925e30db..7c23d9ead694 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -89,6 +89,10 @@ int sys_swapcontext(struct ucontext __user *old_ctx,
 long sys_swapcontext(struct ucontext __user *old_ctx,
 		    struct ucontext __user *new_ctx,
 		    int ctx_size, int r6, int r7, int r8, struct pt_regs *regs);
+int sys_debug_setcontext(struct ucontext __user *ctx,
+			 int ndbg, struct sig_dbg_op __user *dbg,
+			 int r6, int r7, int r8,
+			 struct pt_regs *regs);
 #endif
 long sys_switch_endian(void);
 notrace unsigned int __check_irq_replay(void);

commit 45b4d27a3897d6094bcf84bc87743954e038620a
Author: Mathieu Malaterre <malat@debian.org>
Date:   Sun Feb 25 18:22:25 2018 +0100

    powerpc: Add missing prototype for slb_miss_bad_addr()
    
    In commit f0f558b131db ("powerpc/mm: Preserve CFAR value on SLB miss caused
    by access to bogus address"), the function slb_miss_bad_addr() was added
    without a prototype. This commit adds it.
    
    Fix a warning (treated as error in W=1):
      arch/powerpc/kernel/traps.c:1498:6: error: no previous prototype for ‘slb_miss_bad_addr’
    
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 7330150bfe34..0af1925e30db 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -62,6 +62,7 @@ void RunModeException(struct pt_regs *regs);
 void single_step_exception(struct pt_regs *regs);
 void program_check_exception(struct pt_regs *regs);
 void alignment_exception(struct pt_regs *regs);
+void slb_miss_bad_addr(struct pt_regs *regs);
 void StackOverflow(struct pt_regs *regs);
 void nonrecoverable_exception(struct pt_regs *regs);
 void kernel_fp_unavailable_exception(struct pt_regs *regs);

commit 43a8888f0a70db62488fc3ad1505f6c6a2267803
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Fri Dec 2 02:38:38 2016 +0000

    powerpc: Fix missing CRCs, add more asm-prototypes.h declarations
    
    Add declarations for:
      - __mfdcr, __mtdcr (if CONFIG_PPC_DCR_NATIVE=y; through <asm/dcr.h>)
      - switch_mmu_context (if CONFIG_PPC_BOOK3S_64=n; through <asm/mmu_context.h>)
    
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index e02db66a77e3..7330150bfe34 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -17,6 +17,8 @@
 #include <asm/checksum.h>
 #include <linux/uaccess.h>
 #include <asm/epapr_hcalls.h>
+#include <asm/dcr.h>
+#include <asm/mmu_context.h>
 
 #include <uapi/asm/ucontext.h>
 

commit b3a7864c6feb0fb30bc2cd3726570746bec41697
Author: Tobin C. Harding <me@tobin.cc>
Date:   Mon Mar 6 19:49:46 2017 +1100

    powerpc/ftrace: Add prototype for prepare_ftrace_return()
    
    Sparse emits a warning: symbol 'prepare_ftrace_return' was not
    declared. Should it be static? prepare_ftrace_return() is called from
    assembler and should not be static.
    
    Add a prototype for it to asm-prototypes.h and include that in ftrace.c.
    
    Signed-off-by: Tobin C. Harding <me@tobin.cc>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index f6c5264287e5..e02db66a77e3 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -120,6 +120,8 @@ extern s64 __ashrdi3(s64, int);
 extern int __cmpdi2(s64, s64);
 extern int __ucmpdi2(u64, u64);
 
+/* tracing */
 void _mcount(void);
+unsigned long prepare_ftrace_return(unsigned long parent, unsigned long ip);
 
 #endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */

commit 99ad503287daf78e19e64e0e51f1d60a2a592217
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Feb 9 23:55:43 2017 +1100

    powerpc: Add a prototype for mcount() so it can be versioned
    
    Currently we get a warning that _mcount() can't be versioned:
    
      WARNING: EXPORT symbol "_mcount" [vmlinux] version generation failed, symbol will not be versioned.
    
    Add a prototype to asm-prototypes.h to fix it.
    
    The prototype is not really correct, mcount() is not a normal function,
    it has a special ABI. But for the purpose of versioning it doesn't
    matter.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index ba47c70712f9..f6c5264287e5 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -120,4 +120,6 @@ extern s64 __ashrdi3(s64, int);
 extern int __cmpdi2(s64, s64);
 extern int __ucmpdi2(u64, u64);
 
+void _mcount(void);
+
 #endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index 81592562e0f8..ba47c70712f9 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -15,7 +15,7 @@
 #include <linux/threads.h>
 #include <asm/cacheflush.h>
 #include <asm/checksum.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/epapr_hcalls.h>
 
 #include <uapi/asm/ucontext.h>

commit de399813b521ea7e38bbfb5e5b620b5e202e5783
Merge: 57ca04ab4401 c6f6634721c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 16 09:26:42 2016 -0800

    Merge tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for the kexec_file_load() syscall, which is a prereq for
         secure and trusted boot.
    
       - Prevent kernel execution of userspace on P9 Radix (similar to
         SMEP/PXN).
    
       - Sort the exception tables at build time, to save time at boot, and
         store them as relative offsets to save space in the kernel image &
         memory.
    
       - Allow building the kernel with thin archives, which should allow us
         to build an allyesconfig once some other fixes land.
    
       - Build fixes to allow us to correctly rebuild when changing the
         kernel endian from big to little or vice versa.
    
       - Plumbing so that we can avoid doing a full mm TLB flush on P9
         Radix.
    
       - Initial stack protector support (-fstack-protector).
    
       - Support for dumping the radix (aka. Linux) and hash page tables via
         debugfs.
    
       - Fix an oops in cxl coredump generation when cxl_get_fd() is used.
    
       - Freescale updates from Scott: "Highlights include 8xx hugepage
         support, qbman fixes/cleanup, device tree updates, and some misc
         cleanup."
    
       - Many and varied fixes and minor enhancements as always.
    
      Thanks to:
        Alexey Kardashevskiy, Andrew Donnellan, Aneesh Kumar K.V, Anshuman
        Khandual, Anton Blanchard, Balbir Singh, Bartlomiej Zolnierkiewicz,
        Christophe Jaillet, Christophe Leroy, Denis Kirjanov, Elimar
        Riesebieter, Frederic Barrat, Gautham R. Shenoy, Geliang Tang, Geoff
        Levand, Jack Miller, Johan Hovold, Lars-Peter Clausen, Libin,
        Madhavan Srinivasan, Michael Neuling, Nathan Fontenot, Naveen N.
        Rao, Nicholas Piggin, Pan Xinhui, Peter Senna Tschudin, Rashmica
        Gupta, Rui Teng, Russell Currey, Scott Wood, Simon Guo, Suraj
        Jitindar Singh, Thiago Jung Bauermann, Tobias Klauser, Vaibhav Jain"
    
    [ And thanks to Michael, who took time off from a new baby to get this
      pull request done.   - Linus ]
    
    * tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (174 commits)
      powerpc/fsl/dts: add FMan node for t1042d4rdb
      powerpc/fsl/dts: add sg_2500_aqr105_phy4 alias on t1024rdb
      powerpc/fsl/dts: add QMan and BMan nodes on t1024
      powerpc/fsl/dts: add QMan and BMan nodes on t1023
      soc/fsl/qman: test: use DEFINE_SPINLOCK()
      powerpc/fsl-lbc: use DEFINE_SPINLOCK()
      powerpc/8xx: Implement support of hugepages
      powerpc: get hugetlbpage handling more generic
      powerpc: port 64 bits pgtable_cache to 32 bits
      powerpc/boot: Request no dynamic linker for boot wrapper
      soc/fsl/bman: Use resource_size instead of computation
      soc/fsl/qe: use builtin_platform_driver
      powerpc/fsl_pmc: use builtin_platform_driver
      powerpc/83xx/suspend: use builtin_platform_driver
      powerpc/ftrace: Fix the comments for ftrace_modify_code
      powerpc/perf: macros for power9 format encoding
      powerpc/perf: power9 raw event format encoding
      powerpc/perf: update attribute_group data structure
      powerpc/perf: factor out the event format field
      powerpc/mm/iommu, vfio/spapr: Put pages on VFIO container shutdown
      ...

commit 82de5797a26087c651a06de327e6aa9ea69d5d7f
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Mon Nov 21 22:36:40 2016 +0530

    powerpc: Remove extraneous header from asm-prototypes.h
    
    Commit 03465f899bda ("powerpc: Use kprobe blacklist for exception
    handlers") removed __kprobes annotation from some of the prototypes,
    but left the kprobes header include directive unchanged. Remove it as it
    is no longer needed.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index d1492736d852..dfef1174663e 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -13,8 +13,6 @@
  */
 
 #include <linux/threads.h>
-#include <linux/kprobes.h>
-
 #include <uapi/asm/ucontext.h>
 
 /* SMP */

commit 9e5f68842276672a05737c23e407250f776cbf35
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Nov 22 14:52:22 2016 +1100

    powerpc: Fix missing CRCs, add more asm-prototypes.h declarations
    
    After patch 4efca4ed0 ("kbuild: modversions for EXPORT_SYMBOL() for asm"),
    asm exports can get modversions CRCs generated if they have C definitions
    in asm-prototypes.h. This patch adds missing definitions for 32 and 64 bit
    allmodconfig builds.
    
    Fixes: 9445aa1a3062 ("ppc: move exports to definitions")
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index d1492736d852..e0baba1535e6 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -14,6 +14,10 @@
 
 #include <linux/threads.h>
 #include <linux/kprobes.h>
+#include <asm/cacheflush.h>
+#include <asm/checksum.h>
+#include <asm/uaccess.h>
+#include <asm/epapr_hcalls.h>
 
 #include <uapi/asm/ucontext.h>
 
@@ -109,4 +113,12 @@ void early_setup_secondary(void);
 /* time */
 void accumulate_stolen_time(void);
 
+/* misc runtime */
+extern u64 __bswapdi2(u64);
+extern s64 __lshrdi3(s64, int);
+extern s64 __ashldi3(s64, int);
+extern s64 __ashrdi3(s64, int);
+extern int __cmpdi2(s64, s64);
+extern int __ucmpdi2(u64, u64);
+
 #endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */

commit 03465f899bdac70d34f6ca447a74d8ae9e284ce5
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Sep 16 20:48:08 2016 +1000

    powerpc: Use kprobe blacklist for exception handlers
    
    Currently we mark the C implementations of some exception handlers as
    __kprobes. This has the effect of putting them in the ".kprobes.text"
    section, which separates them from the rest of the text.
    
    Instead we can use the blacklist macros to add the symbols to a
    blacklist which kprobes will check. This allows the linker to move
    exception handler functions close to callers and avoids trampolines in
    larger kernels.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Reword change log a bit]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index a5b239948bfb..d1492736d852 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -54,8 +54,8 @@ void SMIException(struct pt_regs *regs);
 void handle_hmi_exception(struct pt_regs *regs);
 void instruction_breakpoint_exception(struct pt_regs *regs);
 void RunModeException(struct pt_regs *regs);
-void __kprobes single_step_exception(struct pt_regs *regs);
-void __kprobes program_check_exception(struct pt_regs *regs);
+void single_step_exception(struct pt_regs *regs);
+void program_check_exception(struct pt_regs *regs);
 void alignment_exception(struct pt_regs *regs);
 void StackOverflow(struct pt_regs *regs);
 void nonrecoverable_exception(struct pt_regs *regs);
@@ -72,7 +72,7 @@ void unrecoverable_exception(struct pt_regs *regs);
 void kernel_bad_stack(struct pt_regs *regs);
 void system_reset_exception(struct pt_regs *regs);
 void machine_check_exception(struct pt_regs *regs);
-void __kprobes emulation_assist_interrupt(struct pt_regs *regs);
+void emulation_assist_interrupt(struct pt_regs *regs);
 
 /* signals, syscalls and interrupts */
 #ifdef CONFIG_PPC64

commit 0545d5436aefddff7ca417adc1a431c108403a35
Author: Daniel Axtens <dja@axtens.net>
Date:   Tue Sep 6 15:32:43 2016 +1000

    powerpc/sparse: Add more assembler prototypes
    
    Another set of things that are only called from assembler and so need
    prototypes to keep sparse happy.
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
index e71b9097594c..a5b239948bfb 100644
--- a/arch/powerpc/include/asm/asm-prototypes.h
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -15,6 +15,8 @@
 #include <linux/threads.h>
 #include <linux/kprobes.h>
 
+#include <uapi/asm/ucontext.h>
+
 /* SMP */
 extern struct thread_info *current_set[NR_CPUS];
 extern struct thread_info *secondary_ti;
@@ -72,4 +74,39 @@ void system_reset_exception(struct pt_regs *regs);
 void machine_check_exception(struct pt_regs *regs);
 void __kprobes emulation_assist_interrupt(struct pt_regs *regs);
 
+/* signals, syscalls and interrupts */
+#ifdef CONFIG_PPC64
+int sys_swapcontext(struct ucontext __user *old_ctx,
+		    struct ucontext __user *new_ctx,
+		    long ctx_size, long r6, long r7, long r8, struct pt_regs *regs);
+#else
+long sys_swapcontext(struct ucontext __user *old_ctx,
+		    struct ucontext __user *new_ctx,
+		    int ctx_size, int r6, int r7, int r8, struct pt_regs *regs);
+#endif
+long sys_switch_endian(void);
+notrace unsigned int __check_irq_replay(void);
+void notrace restore_interrupts(void);
+
+/* ptrace */
+long do_syscall_trace_enter(struct pt_regs *regs);
+void do_syscall_trace_leave(struct pt_regs *regs);
+
+/* process */
+void restore_math(struct pt_regs *regs);
+void restore_tm_state(struct pt_regs *regs);
+
+/* prom_init (OpenFirmware) */
+unsigned long __init prom_init(unsigned long r3, unsigned long r4,
+			       unsigned long pp,
+			       unsigned long r6, unsigned long r7,
+			       unsigned long kbase);
+
+/* setup */
+void __init early_setup(unsigned long dt_ptr);
+void early_setup_secondary(void);
+
+/* time */
+void accumulate_stolen_time(void);
+
 #endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */

commit 42f5b4cacd783faf05e3ff8bf85e8be31f3dfa9d
Author: Daniel Axtens <dja@axtens.net>
Date:   Wed May 18 11:16:50 2016 +1000

    powerpc: Introduce asm-prototypes.h
    
    Sparse picked up a number of functions that are implemented in C and
    then only referred to in asm code.
    
    This introduces asm-prototypes.h, which provides a place for
    prototypes of these functions.
    
    This silences some sparse warnings.
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    [mpe: Add include guards, clean up copyright & GPL text]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/asm-prototypes.h b/arch/powerpc/include/asm/asm-prototypes.h
new file mode 100644
index 000000000000..e71b9097594c
--- /dev/null
+++ b/arch/powerpc/include/asm/asm-prototypes.h
@@ -0,0 +1,75 @@
+#ifndef _ASM_POWERPC_ASM_PROTOTYPES_H
+#define _ASM_POWERPC_ASM_PROTOTYPES_H
+/*
+ * This file is for prototypes of C functions that are only called
+ * from asm, and any associated variables.
+ *
+ * Copyright 2016, Daniel Axtens, IBM Corporation.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2
+ * of the License, or (at your option) any later version.
+ */
+
+#include <linux/threads.h>
+#include <linux/kprobes.h>
+
+/* SMP */
+extern struct thread_info *current_set[NR_CPUS];
+extern struct thread_info *secondary_ti;
+void start_secondary(void *unused);
+
+/* kexec */
+struct paca_struct;
+struct kimage;
+extern struct paca_struct kexec_paca;
+void kexec_copy_flush(struct kimage *image);
+
+/* pseries hcall tracing */
+extern struct static_key hcall_tracepoint_key;
+void __trace_hcall_entry(unsigned long opcode, unsigned long *args);
+void __trace_hcall_exit(long opcode, unsigned long retval,
+			unsigned long *retbuf);
+/* OPAL tracing */
+#ifdef HAVE_JUMP_LABEL
+extern struct static_key opal_tracepoint_key;
+#endif
+
+void __trace_opal_entry(unsigned long opcode, unsigned long *args);
+void __trace_opal_exit(long opcode, unsigned long retval);
+
+/* VMX copying */
+int enter_vmx_usercopy(void);
+int exit_vmx_usercopy(void);
+int enter_vmx_copy(void);
+void * exit_vmx_copy(void *dest);
+
+/* Traps */
+long machine_check_early(struct pt_regs *regs);
+long hmi_exception_realmode(struct pt_regs *regs);
+void SMIException(struct pt_regs *regs);
+void handle_hmi_exception(struct pt_regs *regs);
+void instruction_breakpoint_exception(struct pt_regs *regs);
+void RunModeException(struct pt_regs *regs);
+void __kprobes single_step_exception(struct pt_regs *regs);
+void __kprobes program_check_exception(struct pt_regs *regs);
+void alignment_exception(struct pt_regs *regs);
+void StackOverflow(struct pt_regs *regs);
+void nonrecoverable_exception(struct pt_regs *regs);
+void kernel_fp_unavailable_exception(struct pt_regs *regs);
+void altivec_unavailable_exception(struct pt_regs *regs);
+void vsx_unavailable_exception(struct pt_regs *regs);
+void fp_unavailable_tm(struct pt_regs *regs);
+void altivec_unavailable_tm(struct pt_regs *regs);
+void vsx_unavailable_tm(struct pt_regs *regs);
+void facility_unavailable_exception(struct pt_regs *regs);
+void TAUException(struct pt_regs *regs);
+void altivec_assist_exception(struct pt_regs *regs);
+void unrecoverable_exception(struct pt_regs *regs);
+void kernel_bad_stack(struct pt_regs *regs);
+void system_reset_exception(struct pt_regs *regs);
+void machine_check_exception(struct pt_regs *regs);
+void __kprobes emulation_assist_interrupt(struct pt_regs *regs);
+
+#endif /* _ASM_POWERPC_ASM_PROTOTYPES_H */
