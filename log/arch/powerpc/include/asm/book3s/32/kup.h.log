commit 74016701fe5f873ae23bf02835407227138d874d
Author: Christophe Leroy <christophe.leroy@csgroup.eu>
Date:   Sat May 30 17:16:33 2020 +0000

    powerpc/32s: Fix another build failure with CONFIG_PPC_KUAP_DEBUG
    
    'thread' doesn't exist in kuap_check() macro.
    
    Use 'current' instead.
    
    Fixes: a68c31fc01ef ("powerpc/32s: Implement Kernel Userspace Access Protection")
    Cc: stable@vger.kernel.org
    Reported-by: kbuild test robot <lkp@intel.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/b459e1600b969047a74e34251a84a3d6fdf1f312.1590858925.git.christophe.leroy@csgroup.eu

diff --git a/arch/powerpc/include/asm/book3s/32/kup.h b/arch/powerpc/include/asm/book3s/32/kup.h
index 5a267b7e4971..32fd4452e960 100644
--- a/arch/powerpc/include/asm/book3s/32/kup.h
+++ b/arch/powerpc/include/asm/book3s/32/kup.h
@@ -2,6 +2,7 @@
 #ifndef _ASM_POWERPC_BOOK3S_32_KUP_H
 #define _ASM_POWERPC_BOOK3S_32_KUP_H
 
+#include <asm/bug.h>
 #include <asm/book3s/32/mmu-hash.h>
 
 #ifdef __ASSEMBLY__
@@ -75,7 +76,7 @@
 
 .macro kuap_check	current, gpr
 #ifdef CONFIG_PPC_KUAP_DEBUG
-	lwz	\gpr, KUAP(thread)
+	lwz	\gpr, THREAD + KUAP(\current)
 999:	twnei	\gpr, 0
 	EMIT_BUG_ENTRY 999b, __FILE__, __LINE__, (BUGFLAG_WARNING | BUGFLAG_ONCE)
 #endif

commit baddc87d6824cda18037881352fe97382fdb0867
Merge: bb5f33c06940 595d153dd102
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue May 26 22:56:03 2020 +1000

    Merge branch 'fixes' into next
    
    Merge our fixes branch from this cycle. It contains several important
    fixes we need in next for testing purposes, and also some that will
    conflict with upcoming changes.

commit 4833ce06e6855d526234618b746ffb71d6612c9a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Apr 20 07:47:05 2020 +0000

    powerpc/32s: Fix build failure with CONFIG_PPC_KUAP_DEBUG
    
    gpr2 is not a parametre of kuap_check(), it doesn't exist.
    
    Use gpr instead.
    
    Fixes: a68c31fc01ef ("powerpc/32s: Implement Kernel Userspace Access Protection")
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: stable@vger.kernel.org
    Link: https://lore.kernel.org/r/ea599546f2a7771bde551393889e44e6b2632332.1587368807.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/32/kup.h b/arch/powerpc/include/asm/book3s/32/kup.h
index 3c0ba22dc360..db0a1c281587 100644
--- a/arch/powerpc/include/asm/book3s/32/kup.h
+++ b/arch/powerpc/include/asm/book3s/32/kup.h
@@ -75,7 +75,7 @@
 
 .macro kuap_check	current, gpr
 #ifdef CONFIG_PPC_KUAP_DEBUG
-	lwz	\gpr2, KUAP(thread)
+	lwz	\gpr, KUAP(thread)
 999:	twnei	\gpr, 0
 	EMIT_BUG_ENTRY 999b, __FILE__, __LINE__, (BUGFLAG_WARNING | BUGFLAG_ONCE)
 #endif

commit 4fe5cda9f89d0aea8e915b7c96ae34bda4e12e51
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 3 07:20:53 2020 +0000

    powerpc/uaccess: Implement user_read_access_begin and user_write_access_begin
    
    Add support for selective read or write user access with
    user_read_access_begin/end and user_write_access_begin/end.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/6c83af0f0809ef2a955c39ac622767f6cbede035.1585898438.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/32/kup.h b/arch/powerpc/include/asm/book3s/32/kup.h
index 3c0ba22dc360..1617e73bee30 100644
--- a/arch/powerpc/include/asm/book3s/32/kup.h
+++ b/arch/powerpc/include/asm/book3s/32/kup.h
@@ -108,7 +108,7 @@ static __always_inline void allow_user_access(void __user *to, const void __user
 	u32 addr, end;
 
 	BUILD_BUG_ON(!__builtin_constant_p(dir));
-	BUILD_BUG_ON(dir == KUAP_CURRENT);
+	BUILD_BUG_ON(dir & ~KUAP_READ_WRITE);
 
 	if (!(dir & KUAP_WRITE))
 		return;
@@ -131,7 +131,7 @@ static __always_inline void prevent_user_access(void __user *to, const void __us
 
 	BUILD_BUG_ON(!__builtin_constant_p(dir));
 
-	if (dir == KUAP_CURRENT) {
+	if (dir & KUAP_CURRENT_WRITE) {
 		u32 kuap = current->thread.kuap;
 
 		if (unlikely(!kuap))

commit 3d7dfd632f9b60cfce069b4da517e6b1a1c3f613
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Jan 24 11:54:45 2020 +0000

    powerpc: Implement user_access_save() and user_access_restore()
    
    Implement user_access_save() and user_access_restore()
    
    On 8xx and radix:
      - On save, get the value of the associated special register then
        prevent user access.
      - On restore, set back the saved value to the associated special
        register.
    
    On book3s/32:
      - On save, get the value stored in current->thread.kuap and prevent
        user access.
      - On restore, regenerate address range from the stored value and
        reopen read/write access for that range.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/54f2f74938006b33c55a416674807b42ef222068.1579866752.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/32/kup.h b/arch/powerpc/include/asm/book3s/32/kup.h
index 17e069291c72..3c0ba22dc360 100644
--- a/arch/powerpc/include/asm/book3s/32/kup.h
+++ b/arch/powerpc/include/asm/book3s/32/kup.h
@@ -153,6 +153,29 @@ static __always_inline void prevent_user_access(void __user *to, const void __us
 	kuap_update_sr(mfsrin(addr) | SR_KS, addr, end);	/* set Ks */
 }
 
+static inline unsigned long prevent_user_access_return(void)
+{
+	unsigned long flags = current->thread.kuap;
+	unsigned long addr = flags & 0xf0000000;
+	unsigned long end = flags << 28;
+	void __user *to = (__force void __user *)addr;
+
+	if (flags)
+		prevent_user_access(to, to, end - addr, KUAP_READ_WRITE);
+
+	return flags;
+}
+
+static inline void restore_user_access(unsigned long flags)
+{
+	unsigned long addr = flags & 0xf0000000;
+	unsigned long end = flags << 28;
+	void __user *to = (__force void __user *)addr;
+
+	if (flags)
+		allow_user_access(to, to, end - addr, KUAP_READ_WRITE);
+}
+
 static inline bool
 bad_kuap_fault(struct pt_regs *regs, unsigned long address, bool is_write)
 {

commit bedb4dbe443c11ff551b4ae4e48c8676fdc96467
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Jan 24 11:54:43 2020 +0000

    powerpc/32s: Prepare prevent_user_access() for user_access_end()
    
    In preparation of implementing user_access_begin and friends
    on powerpc, the book3s/32 version of prevent_user_access() need
    to be prepared for user_access_end().
    
    user_access_end() doesn't provide the address and size which
    were passed to user_access_begin(), required by prevent_user_access()
    to know which segment to modify.
    
    The list of segments which where unprotected by allow_user_access()
    are available in current->kuap. But we don't want prevent_user_access()
    to read this all the time, especially everytime it is 0 (for instance
    because the access was not a write access).
    
    Implement a special direction named KUAP_CURRENT. In this case only,
    the addr and end are retrieved from current->kuap.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/55bcc1f25d8200892a31f67a0b024ff3b816c3cc.1579866752.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/32/kup.h b/arch/powerpc/include/asm/book3s/32/kup.h
index de29fb752cca..17e069291c72 100644
--- a/arch/powerpc/include/asm/book3s/32/kup.h
+++ b/arch/powerpc/include/asm/book3s/32/kup.h
@@ -108,6 +108,8 @@ static __always_inline void allow_user_access(void __user *to, const void __user
 	u32 addr, end;
 
 	BUILD_BUG_ON(!__builtin_constant_p(dir));
+	BUILD_BUG_ON(dir == KUAP_CURRENT);
+
 	if (!(dir & KUAP_WRITE))
 		return;
 
@@ -117,6 +119,7 @@ static __always_inline void allow_user_access(void __user *to, const void __user
 		return;
 
 	end = min(addr + size, TASK_SIZE);
+
 	current->thread.kuap = (addr & 0xf0000000) | ((((end - 1) >> 28) + 1) & 0xf);
 	kuap_update_sr(mfsrin(addr) & ~SR_KS, addr, end);	/* Clear Ks */
 }
@@ -127,15 +130,25 @@ static __always_inline void prevent_user_access(void __user *to, const void __us
 	u32 addr, end;
 
 	BUILD_BUG_ON(!__builtin_constant_p(dir));
-	if (!(dir & KUAP_WRITE))
-		return;
 
-	addr = (__force u32)to;
+	if (dir == KUAP_CURRENT) {
+		u32 kuap = current->thread.kuap;
 
-	if (unlikely(addr >= TASK_SIZE || !size))
+		if (unlikely(!kuap))
+			return;
+
+		addr = kuap & 0xf0000000;
+		end = kuap << 28;
+	} else if (dir & KUAP_WRITE) {
+		addr = (__force u32)to;
+		end = min(addr + size, TASK_SIZE);
+
+		if (unlikely(addr >= TASK_SIZE || !size))
+			return;
+	} else {
 		return;
+	}
 
-	end = min(addr + size, TASK_SIZE);
 	current->thread.kuap = 0;
 	kuap_update_sr(mfsrin(addr) | SR_KS, addr, end);	/* set Ks */
 }

commit 88f8c080d47f307871840a0a825ef556673eb592
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Jan 24 11:54:42 2020 +0000

    powerpc/32s: Drop NULL addr verification
    
    NULL addr is a user address. Don't waste time checking it. If
    someone tries to access it, it will SIGFAULT the same way as for
    address 1, so no need to make it special.
    
    The special case is when not doing a write, in that case we want
    to drop the entire function. This is now handled by 'dir' param
    and not by the nulity of 'to' anymore.
    
    Also make beginning of prevent_user_access() similar
    to beginning of allow_user_access(), and tell the compiler
    that writing in kernel space or with a 0 length is unlikely
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/85e971223dfe6ace734637db1841678939a76155.1579866752.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/32/kup.h b/arch/powerpc/include/asm/book3s/32/kup.h
index 91c8f1d9bcee..de29fb752cca 100644
--- a/arch/powerpc/include/asm/book3s/32/kup.h
+++ b/arch/powerpc/include/asm/book3s/32/kup.h
@@ -113,7 +113,7 @@ static __always_inline void allow_user_access(void __user *to, const void __user
 
 	addr = (__force u32)to;
 
-	if (!addr || addr >= TASK_SIZE || !size)
+	if (unlikely(addr >= TASK_SIZE || !size))
 		return;
 
 	end = min(addr + size, TASK_SIZE);
@@ -124,16 +124,18 @@ static __always_inline void allow_user_access(void __user *to, const void __user
 static __always_inline void prevent_user_access(void __user *to, const void __user *from,
 						u32 size, unsigned long dir)
 {
-	u32 addr = (__force u32)to;
-	u32 end = min(addr + size, TASK_SIZE);
+	u32 addr, end;
 
 	BUILD_BUG_ON(!__builtin_constant_p(dir));
 	if (!(dir & KUAP_WRITE))
 		return;
 
-	if (!addr || addr >= TASK_SIZE || !size)
+	addr = (__force u32)to;
+
+	if (unlikely(addr >= TASK_SIZE || !size))
 		return;
 
+	end = min(addr + size, TASK_SIZE);
 	current->thread.kuap = 0;
 	kuap_update_sr(mfsrin(addr) | SR_KS, addr, end);	/* set Ks */
 }

commit 1d8f739b07bd538f272f60bf53f10e7e6248d295
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Jan 24 11:54:41 2020 +0000

    powerpc/kuap: Fix set direction in allow/prevent_user_access()
    
    __builtin_constant_p() always return 0 for pointers, so on RADIX
    we always end up opening both direction (by writing 0 in SPR29):
    
      0000000000000170 <._copy_to_user>:
      ...
       1b0: 4c 00 01 2c     isync
       1b4: 39 20 00 00     li      r9,0
       1b8: 7d 3d 03 a6     mtspr   29,r9
       1bc: 4c 00 01 2c     isync
       1c0: 48 00 00 01     bl      1c0 <._copy_to_user+0x50>
                            1c0: R_PPC64_REL24      .__copy_tofrom_user
      ...
      0000000000000220 <._copy_from_user>:
      ...
       2ac: 4c 00 01 2c     isync
       2b0: 39 20 00 00     li      r9,0
       2b4: 7d 3d 03 a6     mtspr   29,r9
       2b8: 4c 00 01 2c     isync
       2bc: 7f c5 f3 78     mr      r5,r30
       2c0: 7f 83 e3 78     mr      r3,r28
       2c4: 48 00 00 01     bl      2c4 <._copy_from_user+0xa4>
                            2c4: R_PPC64_REL24      .__copy_tofrom_user
      ...
    
    Use an explicit parameter for direction selection, so that GCC
    is able to see it is a constant:
    
      00000000000001b0 <._copy_to_user>:
      ...
       1f0: 4c 00 01 2c     isync
       1f4: 3d 20 40 00     lis     r9,16384
       1f8: 79 29 07 c6     rldicr  r9,r9,32,31
       1fc: 7d 3d 03 a6     mtspr   29,r9
       200: 4c 00 01 2c     isync
       204: 48 00 00 01     bl      204 <._copy_to_user+0x54>
                            204: R_PPC64_REL24      .__copy_tofrom_user
      ...
      0000000000000260 <._copy_from_user>:
      ...
       2ec: 4c 00 01 2c     isync
       2f0: 39 20 ff ff     li      r9,-1
       2f4: 79 29 00 04     rldicr  r9,r9,0,0
       2f8: 7d 3d 03 a6     mtspr   29,r9
       2fc: 4c 00 01 2c     isync
       300: 7f c5 f3 78     mr      r5,r30
       304: 7f 83 e3 78     mr      r3,r28
       308: 48 00 00 01     bl      308 <._copy_from_user+0xa8>
                            308: R_PPC64_REL24      .__copy_tofrom_user
      ...
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Spell out the directions, s/KUAP_R/KUAP_READ/ etc.]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/f4e88ec4941d5facb35ce75026b0112f980086c3.1579866752.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/32/kup.h b/arch/powerpc/include/asm/book3s/32/kup.h
index d88008c8eb85..91c8f1d9bcee 100644
--- a/arch/powerpc/include/asm/book3s/32/kup.h
+++ b/arch/powerpc/include/asm/book3s/32/kup.h
@@ -102,11 +102,13 @@ static inline void kuap_update_sr(u32 sr, u32 addr, u32 end)
 	isync();	/* Context sync required after mtsrin() */
 }
 
-static inline void allow_user_access(void __user *to, const void __user *from, u32 size)
+static __always_inline void allow_user_access(void __user *to, const void __user *from,
+					      u32 size, unsigned long dir)
 {
 	u32 addr, end;
 
-	if (__builtin_constant_p(to) && to == NULL)
+	BUILD_BUG_ON(!__builtin_constant_p(dir));
+	if (!(dir & KUAP_WRITE))
 		return;
 
 	addr = (__force u32)to;
@@ -119,11 +121,16 @@ static inline void allow_user_access(void __user *to, const void __user *from, u
 	kuap_update_sr(mfsrin(addr) & ~SR_KS, addr, end);	/* Clear Ks */
 }
 
-static inline void prevent_user_access(void __user *to, const void __user *from, u32 size)
+static __always_inline void prevent_user_access(void __user *to, const void __user *from,
+						u32 size, unsigned long dir)
 {
 	u32 addr = (__force u32)to;
 	u32 end = min(addr + size, TASK_SIZE);
 
+	BUILD_BUG_ON(!__builtin_constant_p(dir));
+	if (!(dir & KUAP_WRITE))
+		return;
+
 	if (!addr || addr >= TASK_SIZE || !size)
 		return;
 

commit 6ec20aa2e510b6297906c45f009aa08b2d97269a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Jan 24 11:54:40 2020 +0000

    powerpc/32s: Fix bad_kuap_fault()
    
    At the moment, bad_kuap_fault() reports a fault only if a bad access
    to userspace occurred while access to userspace was not granted.
    
    But if a fault occurs for a write outside the allowed userspace
    segment(s) that have been unlocked, bad_kuap_fault() fails to
    detect it and the kernel loops forever in do_page_fault().
    
    Fix it by checking that the accessed address is within the allowed
    range.
    
    Fixes: a68c31fc01ef ("powerpc/32s: Implement Kernel Userspace Access Protection")
    Cc: stable@vger.kernel.org # v5.2+
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/f48244e9485ada0a304ed33ccbb8da271180c80d.1579866752.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/32/kup.h b/arch/powerpc/include/asm/book3s/32/kup.h
index f9dc597b0b86..d88008c8eb85 100644
--- a/arch/powerpc/include/asm/book3s/32/kup.h
+++ b/arch/powerpc/include/asm/book3s/32/kup.h
@@ -131,12 +131,17 @@ static inline void prevent_user_access(void __user *to, const void __user *from,
 	kuap_update_sr(mfsrin(addr) | SR_KS, addr, end);	/* set Ks */
 }
 
-static inline bool bad_kuap_fault(struct pt_regs *regs, bool is_write)
+static inline bool
+bad_kuap_fault(struct pt_regs *regs, unsigned long address, bool is_write)
 {
+	unsigned long begin = regs->kuap & 0xf0000000;
+	unsigned long end = regs->kuap << 28;
+
 	if (!is_write)
 		return false;
 
-	return WARN(!regs->kuap, "Bug: write fault blocked by segment registers !");
+	return WARN(address < begin || address >= end,
+		    "Bug: write fault blocked by segment registers !");
 }
 
 #endif /* CONFIG_PPC_KUAP */

commit d10f60ae27d26d811e2a1bb39ded47df96d7499f
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Oct 14 16:51:28 2019 +0000

    powerpc/32s: fix allow/prevent_user_access() when crossing segment boundaries.
    
    Make sure starting addr is aligned to segment boundary so that when
    incrementing the segment, the starting address of the new segment is
    below the end address. Otherwise the last segment might get  missed.
    
    Fixes: a68c31fc01ef ("powerpc/32s: Implement Kernel Userspace Access Protection")
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/067a1b09f15f421d40797c2d04c22d4049a1cee8.1571071875.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/32/kup.h b/arch/powerpc/include/asm/book3s/32/kup.h
index 677e9babef80..f9dc597b0b86 100644
--- a/arch/powerpc/include/asm/book3s/32/kup.h
+++ b/arch/powerpc/include/asm/book3s/32/kup.h
@@ -91,6 +91,7 @@
 
 static inline void kuap_update_sr(u32 sr, u32 addr, u32 end)
 {
+	addr &= 0xf0000000;	/* align addr to start of segment */
 	barrier();	/* make sure thread.kuap is updated before playing with SRs */
 	while (addr < end) {
 		mtsrin(sr, addr);

commit a68c31fc01ef7863acc0fc74694bf279456a58c4
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Mar 11 08:30:38 2019 +0000

    powerpc/32s: Implement Kernel Userspace Access Protection
    
    This patch implements Kernel Userspace Access Protection for
    book3s/32.
    
    Due to limitations of the processor page protection capabilities,
    the protection is only against writing. read protection cannot be
    achieved using page protection.
    
    The previous patch modifies the page protection so that RW user
    pages are RW for Key 0 and RO for Key 1, and it sets Key 0 for
    both user and kernel.
    
    This patch changes userspace segment registers are set to Ku 0
    and Ks 1. When kernel needs to write to RW pages, the associated
    segment register is then changed to Ks 0 in order to allow write
    access to the kernel.
    
    In order to avoid having the read all segment registers when
    locking/unlocking the access, some data is kept in the thread_struct
    and saved on stack on exceptions. The field identifies both the
    first unlocked segment and the first segment following the last
    unlocked one. When no segment is unlocked, it contains value 0.
    
    As the hash_page() function is not able to easily determine if a
    protfault is due to a bad kernel access to userspace, protfaults
    need to be handled by handle_page_fault when KUAP is set.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Drop allow_read/write_to/from_user() as they're now in kup.h,
          and adapt allow_user_access() to do nothing when to == NULL]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/32/kup.h b/arch/powerpc/include/asm/book3s/32/kup.h
index 5f97c742ca71..677e9babef80 100644
--- a/arch/powerpc/include/asm/book3s/32/kup.h
+++ b/arch/powerpc/include/asm/book3s/32/kup.h
@@ -37,6 +37,109 @@
 #endif
 .endm
 
+#ifdef CONFIG_PPC_KUAP
+
+.macro kuap_update_sr	gpr1, gpr2, gpr3	/* NEVER use r0 as gpr2 due to addis */
+101:	mtsrin	\gpr1, \gpr2
+	addi	\gpr1, \gpr1, 0x111		/* next VSID */
+	rlwinm	\gpr1, \gpr1, 0, 0xf0ffffff	/* clear VSID overflow */
+	addis	\gpr2, \gpr2, 0x1000		/* address of next segment */
+	cmplw	\gpr2, \gpr3
+	blt-	101b
+	isync
+.endm
+
+.macro kuap_save_and_lock	sp, thread, gpr1, gpr2, gpr3
+	lwz	\gpr2, KUAP(\thread)
+	rlwinm.	\gpr3, \gpr2, 28, 0xf0000000
+	stw	\gpr2, STACK_REGS_KUAP(\sp)
+	beq+	102f
+	li	\gpr1, 0
+	stw	\gpr1, KUAP(\thread)
+	mfsrin	\gpr1, \gpr2
+	oris	\gpr1, \gpr1, SR_KS@h	/* set Ks */
+	kuap_update_sr	\gpr1, \gpr2, \gpr3
+102:
+.endm
+
+.macro kuap_restore	sp, current, gpr1, gpr2, gpr3
+	lwz	\gpr2, STACK_REGS_KUAP(\sp)
+	rlwinm.	\gpr3, \gpr2, 28, 0xf0000000
+	stw	\gpr2, THREAD + KUAP(\current)
+	beq+	102f
+	mfsrin	\gpr1, \gpr2
+	rlwinm	\gpr1, \gpr1, 0, ~SR_KS	/* Clear Ks */
+	kuap_update_sr	\gpr1, \gpr2, \gpr3
+102:
+.endm
+
+.macro kuap_check	current, gpr
+#ifdef CONFIG_PPC_KUAP_DEBUG
+	lwz	\gpr2, KUAP(thread)
+999:	twnei	\gpr, 0
+	EMIT_BUG_ENTRY 999b, __FILE__, __LINE__, (BUGFLAG_WARNING | BUGFLAG_ONCE)
+#endif
+.endm
+
+#endif /* CONFIG_PPC_KUAP */
+
+#else /* !__ASSEMBLY__ */
+
+#ifdef CONFIG_PPC_KUAP
+
+#include <linux/sched.h>
+
+static inline void kuap_update_sr(u32 sr, u32 addr, u32 end)
+{
+	barrier();	/* make sure thread.kuap is updated before playing with SRs */
+	while (addr < end) {
+		mtsrin(sr, addr);
+		sr += 0x111;		/* next VSID */
+		sr &= 0xf0ffffff;	/* clear VSID overflow */
+		addr += 0x10000000;	/* address of next segment */
+	}
+	isync();	/* Context sync required after mtsrin() */
+}
+
+static inline void allow_user_access(void __user *to, const void __user *from, u32 size)
+{
+	u32 addr, end;
+
+	if (__builtin_constant_p(to) && to == NULL)
+		return;
+
+	addr = (__force u32)to;
+
+	if (!addr || addr >= TASK_SIZE || !size)
+		return;
+
+	end = min(addr + size, TASK_SIZE);
+	current->thread.kuap = (addr & 0xf0000000) | ((((end - 1) >> 28) + 1) & 0xf);
+	kuap_update_sr(mfsrin(addr) & ~SR_KS, addr, end);	/* Clear Ks */
+}
+
+static inline void prevent_user_access(void __user *to, const void __user *from, u32 size)
+{
+	u32 addr = (__force u32)to;
+	u32 end = min(addr + size, TASK_SIZE);
+
+	if (!addr || addr >= TASK_SIZE || !size)
+		return;
+
+	current->thread.kuap = 0;
+	kuap_update_sr(mfsrin(addr) | SR_KS, addr, end);	/* set Ks */
+}
+
+static inline bool bad_kuap_fault(struct pt_regs *regs, bool is_write)
+{
+	if (!is_write)
+		return false;
+
+	return WARN(!regs->kuap, "Bug: write fault blocked by segment registers !");
+}
+
+#endif /* CONFIG_PPC_KUAP */
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* _ASM_POWERPC_BOOK3S_32_KUP_H */

commit 31ed2b13c48d779efc838ad54e30121e088a62af
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Mar 11 08:30:35 2019 +0000

    powerpc/32s: Implement Kernel Userspace Execution Prevention.
    
    To implement Kernel Userspace Execution Prevention, this patch
    sets NX bit on all user segments on kernel entry and clears NX bit
    on all user segments on kernel exit.
    
    Note that powerpc 601 doesn't have the NX bit, so KUEP will not
    work on it. A warning is displayed at startup.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/32/kup.h b/arch/powerpc/include/asm/book3s/32/kup.h
new file mode 100644
index 000000000000..5f97c742ca71
--- /dev/null
+++ b/arch/powerpc/include/asm/book3s/32/kup.h
@@ -0,0 +1,42 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_POWERPC_BOOK3S_32_KUP_H
+#define _ASM_POWERPC_BOOK3S_32_KUP_H
+
+#include <asm/book3s/32/mmu-hash.h>
+
+#ifdef __ASSEMBLY__
+
+.macro kuep_update_sr	gpr1, gpr2		/* NEVER use r0 as gpr2 due to addis */
+101:	mtsrin	\gpr1, \gpr2
+	addi	\gpr1, \gpr1, 0x111		/* next VSID */
+	rlwinm	\gpr1, \gpr1, 0, 0xf0ffffff	/* clear VSID overflow */
+	addis	\gpr2, \gpr2, 0x1000		/* address of next segment */
+	bdnz	101b
+	isync
+.endm
+
+.macro kuep_lock	gpr1, gpr2
+#ifdef CONFIG_PPC_KUEP
+	li	\gpr1, NUM_USER_SEGMENTS
+	li	\gpr2, 0
+	mtctr	\gpr1
+	mfsrin	\gpr1, \gpr2
+	oris	\gpr1, \gpr1, SR_NX@h		/* set Nx */
+	kuep_update_sr \gpr1, \gpr2
+#endif
+.endm
+
+.macro kuep_unlock	gpr1, gpr2
+#ifdef CONFIG_PPC_KUEP
+	li	\gpr1, NUM_USER_SEGMENTS
+	li	\gpr2, 0
+	mtctr	\gpr1
+	mfsrin	\gpr1, \gpr2
+	rlwinm	\gpr1, \gpr1, 0, ~SR_NX		/* Clear Nx */
+	kuep_update_sr \gpr1, \gpr2
+#endif
+.endm
+
+#endif /* __ASSEMBLY__ */
+
+#endif /* _ASM_POWERPC_BOOK3S_32_KUP_H */
