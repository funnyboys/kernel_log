commit d4539074b0e9c5fa6508e8c33aaf51abc8ff6e91
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Apr 29 16:56:54 2020 +1000

    powerpc/64s/kuap: Conditionally restore AMR in kuap_restore_amr asm
    
    Similar to the C code change, make the AMR restore conditional on
    whether the register has changed.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200429065654.1677541-7-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
index 820169bac6c4..3ee1ec60be84 100644
--- a/arch/powerpc/include/asm/book3s/64/kup-radix.h
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -12,13 +12,17 @@
 
 #ifdef __ASSEMBLY__
 
-.macro kuap_restore_amr	gpr
+.macro kuap_restore_amr	gpr1, gpr2
 #ifdef CONFIG_PPC_KUAP
 	BEGIN_MMU_FTR_SECTION_NESTED(67)
-	ld	\gpr, STACK_REGS_KUAP(r1)
+	mfspr	\gpr1, SPRN_AMR
+	ld	\gpr2, STACK_REGS_KUAP(r1)
+	cmpd	\gpr1, \gpr2
+	beq	998f
 	isync
-	mtspr	SPRN_AMR, \gpr
+	mtspr	SPRN_AMR, \gpr2
 	/* No isync required, see kuap_restore_amr() */
+998:
 	END_MMU_FTR_SECTION_NESTED_IFSET(MMU_FTR_RADIX_KUAP, 67)
 #endif
 .endm

commit 579940bb451c2dd33396d2d56ce6ef5d92154b3b
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Apr 29 16:56:51 2020 +1000

    powerpc/64/kuap: Conditionally restore AMR in interrupt exit
    
    The AMR update is made conditional on AMR actually changing, which
    should be the less common case on most workloads (though kernel page
    faults on uaccess could be frequent, this doesn't significantly slow
    down that case).
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200429065654.1677541-4-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
index 101d60f16d46..820169bac6c4 100644
--- a/arch/powerpc/include/asm/book3s/64/kup-radix.h
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -62,9 +62,9 @@
 #include <asm/mmu.h>
 #include <asm/ptrace.h>
 
-static inline void kuap_restore_amr(struct pt_regs *regs)
+static inline void kuap_restore_amr(struct pt_regs *regs, unsigned long amr)
 {
-	if (mmu_has_feature(MMU_FTR_RADIX_KUAP)) {
+	if (mmu_has_feature(MMU_FTR_RADIX_KUAP) && unlikely(regs->kuap != amr)) {
 		isync();
 		mtspr(SPRN_AMR, regs->kuap);
 		/*
@@ -75,6 +75,17 @@ static inline void kuap_restore_amr(struct pt_regs *regs)
 	}
 }
 
+static inline unsigned long kuap_get_and_check_amr(void)
+{
+	if (mmu_has_feature(MMU_FTR_RADIX_KUAP)) {
+		unsigned long amr = mfspr(SPRN_AMR);
+		if (IS_ENABLED(CONFIG_PPC_KUAP_DEBUG)) /* kuap_check_amr() */
+			WARN_ON_ONCE(amr != AMR_KUAP_BLOCKED);
+		return amr;
+	}
+	return 0;
+}
+
 static inline void kuap_check_amr(void)
 {
 	if (IS_ENABLED(CONFIG_PPC_KUAP_DEBUG) && mmu_has_feature(MMU_FTR_RADIX_KUAP))
@@ -151,13 +162,18 @@ bad_kuap_fault(struct pt_regs *regs, unsigned long address, bool is_write)
 		    "Bug: %s fault blocked by AMR!", is_write ? "Write" : "Read");
 }
 #else /* CONFIG_PPC_KUAP */
-static inline void kuap_restore_amr(struct pt_regs *regs)
+static inline void kuap_restore_amr(struct pt_regs *regs, unsigned long amr)
 {
 }
 
 static inline void kuap_check_amr(void)
 {
 }
+
+static inline unsigned long kuap_get_and_check_amr(void)
+{
+	return 0;
+}
 #endif /* CONFIG_PPC_KUAP */
 
 #endif /* __ASSEMBLY__ */

commit cb2b53cbffe3c388cd676b63f34e54ceb2643ae2
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Apr 29 16:56:50 2020 +1000

    powerpc/64s/kuap: Add missing isync to KUAP restore paths
    
    Writing the AMR register is documented to require context
    synchronizing operations before and after, for it to take effect as
    expected. The KUAP restore at interrupt exit time deliberately avoids
    the isync after the AMR update because it only needs to take effect
    after the context synchronizing RFID that soon follows. Add a comment
    for this.
    
    The missing isync before the update doesn't have an obvious
    justification, and seems it could theoretically allow a rogue user
    access to leak past the AMR update. Add isyncs for these.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200429065654.1677541-3-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
index 3bcef989a35d..101d60f16d46 100644
--- a/arch/powerpc/include/asm/book3s/64/kup-radix.h
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -16,7 +16,9 @@
 #ifdef CONFIG_PPC_KUAP
 	BEGIN_MMU_FTR_SECTION_NESTED(67)
 	ld	\gpr, STACK_REGS_KUAP(r1)
+	isync
 	mtspr	SPRN_AMR, \gpr
+	/* No isync required, see kuap_restore_amr() */
 	END_MMU_FTR_SECTION_NESTED_IFSET(MMU_FTR_RADIX_KUAP, 67)
 #endif
 .endm
@@ -62,8 +64,15 @@
 
 static inline void kuap_restore_amr(struct pt_regs *regs)
 {
-	if (mmu_has_feature(MMU_FTR_RADIX_KUAP))
+	if (mmu_has_feature(MMU_FTR_RADIX_KUAP)) {
+		isync();
 		mtspr(SPRN_AMR, regs->kuap);
+		/*
+		 * No isync required here because we are about to RFI back to
+		 * previous context before any user accesses would be made,
+		 * which is a CSI.
+		 */
+	}
 }
 
 static inline void kuap_check_amr(void)

commit 6cc0c16d82f889f0083f3608237189afb55b67be
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 26 03:35:37 2020 +1000

    powerpc/64s: Implement interrupt exit logic in C
    
    Implement the bulk of interrupt return logic in C. The asm return code
    must handle a few cases: restoring full GPRs, and emulating stack
    store.
    
    The stack store emulation is significantly simplfied, rather than
    creating a new return frame and switching to that before performing
    the store, it uses the PACA to keep a scratch register around to
    perform the store.
    
    The asm return code is moved into 64e for now. The new logic has made
    allowance for 64e, but I don't have a full environment that works well
    to test it, and even booting in emulated qemu is not great for stress
    testing. 64e shouldn't be too far off working with this, given a bit
    more testing and auditing of the logic.
    
    This is slightly faster on a POWER9 (page fault speed increases about
    1.1%), probably due to reduced mtmsrd.
    
    mpe: Includes fixes from Nick for _TIF_EMULATE_STACK_STORE
    handling (including the fast_interrupt_return path), to remove
    trace_hardirqs_on(), and fixes the interrupt-return part of the
    MSR_VSX restore bug caught by tm-unavailable selftest.
    
    mpe: Incorporate fix from Nick:
    
    The return-to-kernel path has to replay any soft-pending interrupts if
    it is returning to a context that had interrupts soft-enabled. It has
    to do this carefully and avoid plain enabling interrupts if this is an
    irq context, which can cause multiple nesting of interrupts on the
    stack, and other unexpected issues.
    
    The code which avoided this case got the soft-mask state wrong, and
    marked interrupts as enabled before going around again to retry. This
    seems to be mostly harmless except when PREEMPT=y, this calls
    preempt_schedule_irq with irqs apparently enabled and runs into a BUG
    in kernel/sched/core.c
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Suchanek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200225173541.1549955-29-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
index 71081d90f999..3bcef989a35d 100644
--- a/arch/powerpc/include/asm/book3s/64/kup-radix.h
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -60,6 +60,12 @@
 #include <asm/mmu.h>
 #include <asm/ptrace.h>
 
+static inline void kuap_restore_amr(struct pt_regs *regs)
+{
+	if (mmu_has_feature(MMU_FTR_RADIX_KUAP))
+		mtspr(SPRN_AMR, regs->kuap);
+}
+
 static inline void kuap_check_amr(void)
 {
 	if (IS_ENABLED(CONFIG_PPC_KUAP_DEBUG) && mmu_has_feature(MMU_FTR_RADIX_KUAP))
@@ -136,6 +142,10 @@ bad_kuap_fault(struct pt_regs *regs, unsigned long address, bool is_write)
 		    "Bug: %s fault blocked by AMR!", is_write ? "Write" : "Read");
 }
 #else /* CONFIG_PPC_KUAP */
+static inline void kuap_restore_amr(struct pt_regs *regs)
+{
+}
+
 static inline void kuap_check_amr(void)
 {
 }

commit 68b34588e2027f699a3c034235f21cd19356b2e6
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 26 03:35:34 2020 +1000

    powerpc/64/sycall: Implement syscall entry/exit logic in C
    
    System call entry and particularly exit code is beyond the limit of
    what is reasonable to implement in asm.
    
    This conversion moves all conditional branches out of the asm code,
    except for the case that all GPRs should be restored at exit.
    
    Null syscall test is about 5% faster after this patch, because the
    exit work is handled under local_irq_disable, and the hard mask and
    pending interrupt replay is handled after that, which avoids games
    with MSR.
    
    mpe: Includes subsequent fixes from Nick:
    
    This fixes 4 issues caught by TM selftests. First was a tm-syscall bug
    that hit due to tabort_syscall being called after interrupts were
    reconciled (in a subsequent patch), which led to interrupts being
    enabled before tabort_syscall was called. Rather than going through an
    un-reconciling interrupts for the return, I just go back to putting
    the test early in asm, the C-ification of that wasn't a big win
    anyway.
    
    Second is the syscall return _TIF_USER_WORK_MASK check would go into
    an infinite loop if _TIF_RESTORE_TM became set. The asm code uses
    _TIF_USER_WORK_MASK to brach to slowpath which includes
    restore_tm_state.
    
    Third is system call return was not calling restore_tm_state, I missed
    this completely (alhtough it's in the return from interrupt C
    conversion because when the asm syscall code encountered problems it
    would branch to the interrupt return code.
    
    Fourth is MSR_VEC missing from restore_math, which was caught by
    tm-unavailable selftest taking an unexpected facility unavailable
    interrupt when testing VSX unavailble exception with MSR.FP=1
    MSR.VEC=1. Fourth case also has a fixup in a subsequent patch.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Suchanek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200225173541.1549955-26-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
index 90dd3a3fc8c7..71081d90f999 100644
--- a/arch/powerpc/include/asm/book3s/64/kup-radix.h
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -3,6 +3,7 @@
 #define _ASM_POWERPC_BOOK3S_64_KUP_RADIX_H
 
 #include <linux/const.h>
+#include <asm/reg.h>
 
 #define AMR_KUAP_BLOCK_READ	UL(0x4000000000000000)
 #define AMR_KUAP_BLOCK_WRITE	UL(0x8000000000000000)
@@ -56,7 +57,14 @@
 
 #ifdef CONFIG_PPC_KUAP
 
-#include <asm/reg.h>
+#include <asm/mmu.h>
+#include <asm/ptrace.h>
+
+static inline void kuap_check_amr(void)
+{
+	if (IS_ENABLED(CONFIG_PPC_KUAP_DEBUG) && mmu_has_feature(MMU_FTR_RADIX_KUAP))
+		WARN_ON_ONCE(mfspr(SPRN_AMR) != AMR_KUAP_BLOCKED);
+}
 
 /*
  * We support individually allowing read or write, but we don't support nesting
@@ -127,6 +135,10 @@ bad_kuap_fault(struct pt_regs *regs, unsigned long address, bool is_write)
 		    (regs->kuap & (is_write ? AMR_KUAP_BLOCK_WRITE : AMR_KUAP_BLOCK_READ)),
 		    "Bug: %s fault blocked by AMR!", is_write ? "Write" : "Read");
 }
+#else /* CONFIG_PPC_KUAP */
+static inline void kuap_check_amr(void)
+{
+}
 #endif /* CONFIG_PPC_KUAP */
 
 #endif /* __ASSEMBLY__ */

commit 3d7dfd632f9b60cfce069b4da517e6b1a1c3f613
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Jan 24 11:54:45 2020 +0000

    powerpc: Implement user_access_save() and user_access_restore()
    
    Implement user_access_save() and user_access_restore()
    
    On 8xx and radix:
      - On save, get the value of the associated special register then
        prevent user access.
      - On restore, set back the saved value to the associated special
        register.
    
    On book3s/32:
      - On save, get the value stored in current->thread.kuap and prevent
        user access.
      - On restore, regenerate address range from the stored value and
        reopen read/write access for that range.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/54f2f74938006b33c55a416674807b42ef222068.1579866752.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
index a0263e94df33..90dd3a3fc8c7 100644
--- a/arch/powerpc/include/asm/book3s/64/kup-radix.h
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -63,6 +63,14 @@
  * because that would require an expensive read/modify write of the AMR.
  */
 
+static inline unsigned long get_kuap(void)
+{
+	if (!early_mmu_has_feature(MMU_FTR_RADIX_KUAP))
+		return 0;
+
+	return mfspr(SPRN_AMR);
+}
+
 static inline void set_kuap(unsigned long value)
 {
 	if (!early_mmu_has_feature(MMU_FTR_RADIX_KUAP))
@@ -98,6 +106,20 @@ static inline void prevent_user_access(void __user *to, const void __user *from,
 	set_kuap(AMR_KUAP_BLOCKED);
 }
 
+static inline unsigned long prevent_user_access_return(void)
+{
+	unsigned long flags = get_kuap();
+
+	set_kuap(AMR_KUAP_BLOCKED);
+
+	return flags;
+}
+
+static inline void restore_user_access(unsigned long flags)
+{
+	set_kuap(flags);
+}
+
 static inline bool
 bad_kuap_fault(struct pt_regs *regs, unsigned long address, bool is_write)
 {

commit bedb4dbe443c11ff551b4ae4e48c8676fdc96467
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Jan 24 11:54:43 2020 +0000

    powerpc/32s: Prepare prevent_user_access() for user_access_end()
    
    In preparation of implementing user_access_begin and friends
    on powerpc, the book3s/32 version of prevent_user_access() need
    to be prepared for user_access_end().
    
    user_access_end() doesn't provide the address and size which
    were passed to user_access_begin(), required by prevent_user_access()
    to know which segment to modify.
    
    The list of segments which where unprotected by allow_user_access()
    are available in current->kuap. But we don't want prevent_user_access()
    to read this all the time, especially everytime it is 0 (for instance
    because the access was not a write access).
    
    Implement a special direction named KUAP_CURRENT. In this case only,
    the addr and end are retrieved from current->kuap.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/55bcc1f25d8200892a31f67a0b024ff3b816c3cc.1579866752.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
index c8d1076e0ebb..a0263e94df33 100644
--- a/arch/powerpc/include/asm/book3s/64/kup-radix.h
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -86,8 +86,10 @@ static __always_inline void allow_user_access(void __user *to, const void __user
 		set_kuap(AMR_KUAP_BLOCK_WRITE);
 	else if (dir == KUAP_WRITE)
 		set_kuap(AMR_KUAP_BLOCK_READ);
-	else
+	else if (dir == KUAP_READ_WRITE)
 		set_kuap(0);
+	else
+		BUILD_BUG();
 }
 
 static inline void prevent_user_access(void __user *to, const void __user *from,

commit 1d8f739b07bd538f272f60bf53f10e7e6248d295
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Jan 24 11:54:41 2020 +0000

    powerpc/kuap: Fix set direction in allow/prevent_user_access()
    
    __builtin_constant_p() always return 0 for pointers, so on RADIX
    we always end up opening both direction (by writing 0 in SPR29):
    
      0000000000000170 <._copy_to_user>:
      ...
       1b0: 4c 00 01 2c     isync
       1b4: 39 20 00 00     li      r9,0
       1b8: 7d 3d 03 a6     mtspr   29,r9
       1bc: 4c 00 01 2c     isync
       1c0: 48 00 00 01     bl      1c0 <._copy_to_user+0x50>
                            1c0: R_PPC64_REL24      .__copy_tofrom_user
      ...
      0000000000000220 <._copy_from_user>:
      ...
       2ac: 4c 00 01 2c     isync
       2b0: 39 20 00 00     li      r9,0
       2b4: 7d 3d 03 a6     mtspr   29,r9
       2b8: 4c 00 01 2c     isync
       2bc: 7f c5 f3 78     mr      r5,r30
       2c0: 7f 83 e3 78     mr      r3,r28
       2c4: 48 00 00 01     bl      2c4 <._copy_from_user+0xa4>
                            2c4: R_PPC64_REL24      .__copy_tofrom_user
      ...
    
    Use an explicit parameter for direction selection, so that GCC
    is able to see it is a constant:
    
      00000000000001b0 <._copy_to_user>:
      ...
       1f0: 4c 00 01 2c     isync
       1f4: 3d 20 40 00     lis     r9,16384
       1f8: 79 29 07 c6     rldicr  r9,r9,32,31
       1fc: 7d 3d 03 a6     mtspr   29,r9
       200: 4c 00 01 2c     isync
       204: 48 00 00 01     bl      204 <._copy_to_user+0x54>
                            204: R_PPC64_REL24      .__copy_tofrom_user
      ...
      0000000000000260 <._copy_from_user>:
      ...
       2ec: 4c 00 01 2c     isync
       2f0: 39 20 ff ff     li      r9,-1
       2f4: 79 29 00 04     rldicr  r9,r9,0,0
       2f8: 7d 3d 03 a6     mtspr   29,r9
       2fc: 4c 00 01 2c     isync
       300: 7f c5 f3 78     mr      r5,r30
       304: 7f 83 e3 78     mr      r3,r28
       308: 48 00 00 01     bl      308 <._copy_from_user+0xa8>
                            308: R_PPC64_REL24      .__copy_tofrom_user
      ...
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Spell out the directions, s/KUAP_R/KUAP_READ/ etc.]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/f4e88ec4941d5facb35ce75026b0112f980086c3.1579866752.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
index dbbd22cb80f5..c8d1076e0ebb 100644
--- a/arch/powerpc/include/asm/book3s/64/kup-radix.h
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -77,20 +77,21 @@ static inline void set_kuap(unsigned long value)
 	isync();
 }
 
-static inline void allow_user_access(void __user *to, const void __user *from,
-				     unsigned long size)
+static __always_inline void allow_user_access(void __user *to, const void __user *from,
+					      unsigned long size, unsigned long dir)
 {
 	// This is written so we can resolve to a single case at build time
-	if (__builtin_constant_p(to) && to == NULL)
+	BUILD_BUG_ON(!__builtin_constant_p(dir));
+	if (dir == KUAP_READ)
 		set_kuap(AMR_KUAP_BLOCK_WRITE);
-	else if (__builtin_constant_p(from) && from == NULL)
+	else if (dir == KUAP_WRITE)
 		set_kuap(AMR_KUAP_BLOCK_READ);
 	else
 		set_kuap(0);
 }
 
 static inline void prevent_user_access(void __user *to, const void __user *from,
-				       unsigned long size)
+				       unsigned long size, unsigned long dir)
 {
 	set_kuap(AMR_KUAP_BLOCKED);
 }

commit 6ec20aa2e510b6297906c45f009aa08b2d97269a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Jan 24 11:54:40 2020 +0000

    powerpc/32s: Fix bad_kuap_fault()
    
    At the moment, bad_kuap_fault() reports a fault only if a bad access
    to userspace occurred while access to userspace was not granted.
    
    But if a fault occurs for a write outside the allowed userspace
    segment(s) that have been unlocked, bad_kuap_fault() fails to
    detect it and the kernel loops forever in do_page_fault().
    
    Fix it by checking that the accessed address is within the allowed
    range.
    
    Fixes: a68c31fc01ef ("powerpc/32s: Implement Kernel Userspace Access Protection")
    Cc: stable@vger.kernel.org # v5.2+
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/f48244e9485ada0a304ed33ccbb8da271180c80d.1579866752.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
index f254de956d6a..dbbd22cb80f5 100644
--- a/arch/powerpc/include/asm/book3s/64/kup-radix.h
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -95,7 +95,8 @@ static inline void prevent_user_access(void __user *to, const void __user *from,
 	set_kuap(AMR_KUAP_BLOCKED);
 }
 
-static inline bool bad_kuap_fault(struct pt_regs *regs, bool is_write)
+static inline bool
+bad_kuap_fault(struct pt_regs *regs, unsigned long address, bool is_write)
 {
 	return WARN(mmu_has_feature(MMU_FTR_RADIX_KUAP) &&
 		    (regs->kuap & (is_write ? AMR_KUAP_BLOCK_WRITE : AMR_KUAP_BLOCK_READ)),

commit 8150a153c013aa2dd1ffae43370b89ac1347a7fb
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed May 8 13:06:42 2019 +1000

    powerpc/64s: Use early_mmu_has_feature() in set_kuap()
    
    When implementing the KUAP support on Radix we fixed one case where
    mmu_has_feature() was being called too early in boot via
    __put_user_size().
    
    However since then some new code in linux-next has created a new path
    via which we can end up calling mmu_has_feature() too early.
    
    On P9 this leads to crashes early in boot if we have both PPC_KUAP and
    CONFIG_JUMP_LABEL_FEATURE_CHECK_DEBUG enabled. Our early boot code
    calls printk() which calls probe_kernel_read(), that does a
    __copy_from_user_inatomic() which calls into set_kuap() and that uses
    mmu_has_feature().
    
    At that point in boot we haven't patched MMU features yet so the debug
    code in mmu_has_feature() complains, and calls printk(). At that point
    we recurse, eg:
    
      ...
      dump_stack+0xdc
      probe_kernel_read+0x1a4
      check_pointer+0x58
      ...
      printk+0x40
      dump_stack_print_info+0xbc
      dump_stack+0x8
      probe_kernel_read+0x1a4
      probe_kernel_read+0x19c
      check_pointer+0x58
      ...
      printk+0x40
      cpufeatures_process_feature+0xc8
      scan_cpufeatures_subnodes+0x380
      of_scan_flat_dt_subnodes+0xb4
      dt_cpu_ftrs_scan_callback+0x158
      of_scan_flat_dt+0xf0
      dt_cpu_ftrs_scan+0x3c
      early_init_devtree+0x360
      early_setup+0x9c
    
    And so on for infinity, symptom is a dead system.
    
    Even more fun is what happens when using the hash MMU (ie. p8 or p9
    with Radix disabled), and when we don't have
    CONFIG_JUMP_LABEL_FEATURE_CHECK_DEBUG enabled. With the debug disabled
    we don't check if static keys have been initialised, we just rely on
    the jump label. But the jump label defaults to true so we just whack
    the AMR even though Radix is not enabled.
    
    Clearing the AMR is fine, but after we've done the user copy we write
    (0b11 << 62) into AMR. When using hash that makes all pages with key
    zero no longer readable or writable. All kernel pages implicitly have
    key zero, and so all of a sudden the kernel can't read or write any of
    its memory. Again dead system.
    
    In the medium term we have several options for fixing this.
    probe_kernel_read() doesn't need to touch AMR at all, it's not doing a
    user access after all, but it uses __copy_from_user_inatomic() just
    because it's easy, we could fix that.
    
    It would also be safe to default to not writing to the AMR during
    early boot, until we've detected features. But it's not clear that
    flipping all the MMU features to static_key_false won't introduce
    other bugs.
    
    But for now just switch to early_mmu_has_feature() in set_kuap(), that
    avoids all the problems with jump labels. It adds the overhead of a
    global lookup and test, but that's probably trivial compared to the
    writes to the AMR anyway.
    
    Fixes: 890274c2dc4c ("powerpc/64s: Implement KUAP for Radix MMU")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Russell Currey <ruscur@russell.cc>

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
index 7679bd0c5af0..f254de956d6a 100644
--- a/arch/powerpc/include/asm/book3s/64/kup-radix.h
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -65,7 +65,7 @@
 
 static inline void set_kuap(unsigned long value)
 {
-	if (!mmu_has_feature(MMU_FTR_RADIX_KUAP))
+	if (!early_mmu_has_feature(MMU_FTR_RADIX_KUAP))
 		return;
 
 	/*

commit 5e5be3aed23032d40d5ab7407f344f1a74f2765b
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Apr 18 16:51:25 2019 +1000

    powerpc/mm: Detect bad KUAP faults
    
    When KUAP is enabled we have logic to detect page faults that occur
    outside of a valid user access region and are blocked by the AMR.
    
    What we don't have at the moment is logic to detect a fault *within* a
    valid user access region, that has been incorrectly blocked by AMR.
    This is not meant to ever happen, but it can if we incorrectly
    save/restore the AMR, or if the AMR was overwritten for some other
    reason.
    
    Currently if that happens we assume it's just a regular fault that
    will be corrected by handling the fault normally, so we just return.
    But there is nothing the fault handling code can do to fix it, so the
    fault just happens again and we spin forever, leading to soft lockups.
    
    So add some logic to detect that case and WARN() if we ever see it.
    Arguably it should be a BUG(), but it's more polite to fail the access
    and let the kernel continue, rather than taking down the box. There
    should be no data integrity issue with failing the fault rather than
    BUG'ing, as we're just going to disallow an access that should have
    been allowed.
    
    To make the code a little easier to follow, unroll the condition at
    the end of bad_kernel_fault() and comment each case, before adding the
    call to bad_kuap_fault().
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
index 6d6628424134..7679bd0c5af0 100644
--- a/arch/powerpc/include/asm/book3s/64/kup-radix.h
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -95,6 +95,12 @@ static inline void prevent_user_access(void __user *to, const void __user *from,
 	set_kuap(AMR_KUAP_BLOCKED);
 }
 
+static inline bool bad_kuap_fault(struct pt_regs *regs, bool is_write)
+{
+	return WARN(mmu_has_feature(MMU_FTR_RADIX_KUAP) &&
+		    (regs->kuap & (is_write ? AMR_KUAP_BLOCK_WRITE : AMR_KUAP_BLOCK_READ)),
+		    "Bug: %s fault blocked by AMR!", is_write ? "Write" : "Read");
+}
 #endif /* CONFIG_PPC_KUAP */
 
 #endif /* __ASSEMBLY__ */

commit 890274c2dc4c0a57ae5a12d6a76fa6d05b599d98
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Apr 18 16:51:24 2019 +1000

    powerpc/64s: Implement KUAP for Radix MMU
    
    Kernel Userspace Access Prevention utilises a feature of the Radix MMU
    which disallows read and write access to userspace addresses. By
    utilising this, the kernel is prevented from accessing user data from
    outside of trusted paths that perform proper safety checks, such as
    copy_{to/from}_user() and friends.
    
    Userspace access is disabled from early boot and is only enabled when
    performing an operation like copy_{to/from}_user(). The register that
    controls this (AMR) does not prevent userspace from accessing itself,
    so there is no need to save and restore when entering and exiting
    userspace.
    
    When entering the kernel from the kernel we save AMR and if it is not
    blocking user access (because eg. we faulted doing a user access) we
    reblock user access for the duration of the exception (ie. the page
    fault) and then restore the AMR when returning back to the kernel.
    
    This feature can be tested by using the lkdtm driver (CONFIG_LKDTM=y)
    and performing the following:
    
      # (echo ACCESS_USERSPACE) > [debugfs]/provoke-crash/DIRECT
    
    If enabled, this should send SIGSEGV to the thread.
    
    We also add paranoid checking of AMR in switch and syscall return
    under CONFIG_PPC_KUAP_DEBUG.
    
    Co-authored-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Russell Currey <ruscur@russell.cc>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/kup-radix.h b/arch/powerpc/include/asm/book3s/64/kup-radix.h
new file mode 100644
index 000000000000..6d6628424134
--- /dev/null
+++ b/arch/powerpc/include/asm/book3s/64/kup-radix.h
@@ -0,0 +1,102 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_POWERPC_BOOK3S_64_KUP_RADIX_H
+#define _ASM_POWERPC_BOOK3S_64_KUP_RADIX_H
+
+#include <linux/const.h>
+
+#define AMR_KUAP_BLOCK_READ	UL(0x4000000000000000)
+#define AMR_KUAP_BLOCK_WRITE	UL(0x8000000000000000)
+#define AMR_KUAP_BLOCKED	(AMR_KUAP_BLOCK_READ | AMR_KUAP_BLOCK_WRITE)
+#define AMR_KUAP_SHIFT		62
+
+#ifdef __ASSEMBLY__
+
+.macro kuap_restore_amr	gpr
+#ifdef CONFIG_PPC_KUAP
+	BEGIN_MMU_FTR_SECTION_NESTED(67)
+	ld	\gpr, STACK_REGS_KUAP(r1)
+	mtspr	SPRN_AMR, \gpr
+	END_MMU_FTR_SECTION_NESTED_IFSET(MMU_FTR_RADIX_KUAP, 67)
+#endif
+.endm
+
+.macro kuap_check_amr gpr1, gpr2
+#ifdef CONFIG_PPC_KUAP_DEBUG
+	BEGIN_MMU_FTR_SECTION_NESTED(67)
+	mfspr	\gpr1, SPRN_AMR
+	li	\gpr2, (AMR_KUAP_BLOCKED >> AMR_KUAP_SHIFT)
+	sldi	\gpr2, \gpr2, AMR_KUAP_SHIFT
+999:	tdne	\gpr1, \gpr2
+	EMIT_BUG_ENTRY 999b, __FILE__, __LINE__, (BUGFLAG_WARNING | BUGFLAG_ONCE)
+	END_MMU_FTR_SECTION_NESTED_IFSET(MMU_FTR_RADIX_KUAP, 67)
+#endif
+.endm
+
+.macro kuap_save_amr_and_lock gpr1, gpr2, use_cr, msr_pr_cr
+#ifdef CONFIG_PPC_KUAP
+	BEGIN_MMU_FTR_SECTION_NESTED(67)
+	.ifnb \msr_pr_cr
+	bne	\msr_pr_cr, 99f
+	.endif
+	mfspr	\gpr1, SPRN_AMR
+	std	\gpr1, STACK_REGS_KUAP(r1)
+	li	\gpr2, (AMR_KUAP_BLOCKED >> AMR_KUAP_SHIFT)
+	sldi	\gpr2, \gpr2, AMR_KUAP_SHIFT
+	cmpd	\use_cr, \gpr1, \gpr2
+	beq	\use_cr, 99f
+	// We don't isync here because we very recently entered via rfid
+	mtspr	SPRN_AMR, \gpr2
+	isync
+99:
+	END_MMU_FTR_SECTION_NESTED_IFSET(MMU_FTR_RADIX_KUAP, 67)
+#endif
+.endm
+
+#else /* !__ASSEMBLY__ */
+
+#ifdef CONFIG_PPC_KUAP
+
+#include <asm/reg.h>
+
+/*
+ * We support individually allowing read or write, but we don't support nesting
+ * because that would require an expensive read/modify write of the AMR.
+ */
+
+static inline void set_kuap(unsigned long value)
+{
+	if (!mmu_has_feature(MMU_FTR_RADIX_KUAP))
+		return;
+
+	/*
+	 * ISA v3.0B says we need a CSI (Context Synchronising Instruction) both
+	 * before and after the move to AMR. See table 6 on page 1134.
+	 */
+	isync();
+	mtspr(SPRN_AMR, value);
+	isync();
+}
+
+static inline void allow_user_access(void __user *to, const void __user *from,
+				     unsigned long size)
+{
+	// This is written so we can resolve to a single case at build time
+	if (__builtin_constant_p(to) && to == NULL)
+		set_kuap(AMR_KUAP_BLOCK_WRITE);
+	else if (__builtin_constant_p(from) && from == NULL)
+		set_kuap(AMR_KUAP_BLOCK_READ);
+	else
+		set_kuap(0);
+}
+
+static inline void prevent_user_access(void __user *to, const void __user *from,
+				       unsigned long size)
+{
+	set_kuap(AMR_KUAP_BLOCKED);
+}
+
+#endif /* CONFIG_PPC_KUAP */
+
+#endif /* __ASSEMBLY__ */
+
+#endif /* _ASM_POWERPC_BOOK3S_64_KUP_RADIX_H */
