commit 9384e552aabb647ec22acb00181ca1715b0fcdfe
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Apr 29 17:02:47 2020 +1000

    powerpc/64s: Fix early_init_mmu section mismatch
    
    Christian reports:
    
      MODPOST vmlinux.o
      WARNING: modpost: vmlinux.o(.text.unlikely+0x1a0): Section mismatch in
      reference from the function .early_init_mmu() to the function
      .init.text:.radix__early_init_mmu()
      The function .early_init_mmu() references
      the function __init .radix__early_init_mmu().
      This is often because .early_init_mmu lacks a __init
      annotation or the annotation of .radix__early_init_mmu is wrong.
    
      WARNING: modpost: vmlinux.o(.text.unlikely+0x1ac): Section mismatch in
      reference from the function .early_init_mmu() to the function
      .init.text:.hash__early_init_mmu()
      The function .early_init_mmu() references
      the function __init .hash__early_init_mmu().
      This is often because .early_init_mmu lacks a __init
      annotation or the annotation of .hash__early_init_mmu is wrong.
    
    The compiler is uninlining early_init_mmu and not putting it in an init
    section because there is no annotation. Add it.
    
    Reported-by: Christian Zigotzky <chzigotzky@xenosoft.de>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Tested-by: Christian Zigotzky <chzigotzky@xenosoft.de>
    Link: https://lore.kernel.org/r/20200429070247.1678172-1-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index f0a9ff690881..5393a535240c 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -211,7 +211,7 @@ void hash__early_init_devtree(void);
 void radix__early_init_devtree(void);
 extern void hash__early_init_mmu(void);
 extern void radix__early_init_mmu(void);
-static inline void early_init_mmu(void)
+static inline void __init early_init_mmu(void)
 {
 	if (radix_enabled())
 		return radix__early_init_mmu();

commit c420644c0a8f8839ca7269acbb8a3fc7fe1ec97d
Author: Haren Myneni <haren@linux.ibm.com>
Date:   Wed Apr 15 23:08:11 2020 -0700

    powerpc: Use mm_context vas_windows counter to issue CP_ABORT
    
    set_thread_uses_vas() sets used_vas flag for a process that opened VAS
    window and issue CP_ABORT during context switch for only that process.
    In multi-thread application, windows can be shared. For example Thread
    A can open a window and Thread B can run COPY/PASTE instructions to
    send NX request which may cause corruption or snooping or a covert
    channel Also once this flag is set, continue to run CP_ABORT even the
    VAS window is closed.
    
    So define vas-windows counter in process mm_context, increment this
    counter for each window open and decrement it for window close. If
    vas-windows is set, issue CP_ABORT during context switch. It means
    clear the foreign real address mapping only if the process / thread
    uses COPY/PASTE. Then disable it for that process if windows are not
    open.
    
    Moved set_thread_uses_vas() code to vas_tx_win_open() as this
    functionality is needed only for userspace open windows. We are adding
    VAS userspace support along with this fix. So no need to include this
    fix in stable releases.
    
    Fixes: 9d2a4d71332c ("powerpc: Define set_thread_uses_vas()")
    Signed-off-by: Haren Myneni <haren@linux.ibm.com>
    Reported-by: Nicholas Piggin <npiggin@gmail.com>
    Suggested-by: Milton Miller <miltonm@us.ibm.com>
    Suggested-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/1587017291.2275.1077.camel@hbabu-laptop

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index bb3deb76c951..f0a9ff690881 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -116,6 +116,9 @@ typedef struct {
 	/* Number of users of the external (Nest) MMU */
 	atomic_t copros;
 
+	/* Number of user space windows opened in process mm_context */
+	atomic_t vas_windows;
+
 	struct hash_mm_context *hash_context;
 
 	unsigned long vdso_base;

commit ed6546bdc61b7c4bd926cebd82ba52d056fcefa1
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Sep 3 01:29:26 2019 +1000

    powerpc/64s: remove register_process_table callback
    
    This callback is only required because the partition table init comes
    before process table allocation on powernv (aka bare metal aka native).
    
    Change the order to allocate the process table first, and remove the
    callback.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190902152931.17840-2-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 23b83d3593e2..bb3deb76c951 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -206,7 +206,6 @@ extern int mmu_io_psize;
 void mmu_early_init_devtree(void);
 void hash__early_init_devtree(void);
 void radix__early_init_devtree(void);
-extern void radix_init_native(void);
 extern void hash__early_init_mmu(void);
 extern void radix__early_init_mmu(void);
 static inline void early_init_mmu(void)
@@ -238,9 +237,6 @@ static inline void setup_initial_memory_limit(phys_addr_t first_memblock_base,
 					   first_memblock_size);
 }
 
-extern int (*register_process_table)(unsigned long base, unsigned long page_size,
-				     unsigned long tbl_size);
-
 #ifdef CONFIG_PPC_PSERIES
 extern void radix_init_pseries(void);
 #else

commit 7eb3cf761927b2687164e182efa675e6c09cfe44
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jun 25 16:52:38 2019 +0200

    powerpc/powernv: remove unused NPU DMA code
    
    None of these routines were ever used anywhere in the kernel tree
    since they were added to the kernel.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 74d24201fc4f..23b83d3593e2 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -116,8 +116,6 @@ typedef struct {
 	/* Number of users of the external (Nest) MMU */
 	atomic_t copros;
 
-	/* NPU NMMU context */
-	struct npu_context *npu_context;
 	struct hash_mm_context *hash_context;
 
 	unsigned long vdso_base;

commit 696dffa24bd0e17c8bccb18467555c17cc15e62c
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 15:58:02 2019 +0000

    powerpc/mm: move pgtable_t in asm/mmu.h
    
    pgtable_t is now identical for all subarches, move it to the
    top level asm/mmu.h
    
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 51b2d60efc1b..74d24201fc4f 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -25,14 +25,6 @@ struct mmu_psize_def {
 	};
 };
 extern struct mmu_psize_def mmu_psize_defs[MMU_PAGE_COUNT];
-
-/*
- * For BOOK3s 64 with 4k and 64K linux page size
- * we want to use pointers, because the page table
- * actually store pfn
- */
-typedef pte_t *pgtable_t;
-
 #endif /* __ASSEMBLY__ */
 
 /*

commit 877461210ea1c92f159bf261924e58d7d27edadc
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Apr 25 14:29:31 2019 +0000

    powerpc/mm: get rid of mm_ctx_slice_mask_xxx()
    
    Now that slice_mask_for_size() is in mmu.h, the mm_ctx_slice_mask_xxx()
    are not needed anymore, so drop them. Note that the 8xx ones where
    not used anyway.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index a6d5b5ed1170..51b2d60efc1b 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -179,45 +179,21 @@ static inline void mm_ctx_set_slb_addr_limit(mm_context_t *ctx, unsigned long li
 	ctx->hash_context->slb_addr_limit = limit;
 }
 
-#ifdef CONFIG_PPC_64K_PAGES
-static inline struct slice_mask *mm_ctx_slice_mask_64k(mm_context_t *ctx)
-{
-	return &ctx->hash_context->mask_64k;
-}
-#endif
-
-static inline struct slice_mask *mm_ctx_slice_mask_4k(mm_context_t *ctx)
-{
-	return &ctx->hash_context->mask_4k;
-}
-
-#ifdef CONFIG_HUGETLB_PAGE
-static inline struct slice_mask *mm_ctx_slice_mask_16m(mm_context_t *ctx)
-{
-	return &ctx->hash_context->mask_16m;
-}
-
-static inline struct slice_mask *mm_ctx_slice_mask_16g(mm_context_t *ctx)
-{
-	return &ctx->hash_context->mask_16g;
-}
-#endif
-
 static inline struct slice_mask *slice_mask_for_size(mm_context_t *ctx, int psize)
 {
 #ifdef CONFIG_PPC_64K_PAGES
 	if (psize == MMU_PAGE_64K)
-		return mm_ctx_slice_mask_64k(&ctx);
+		return &ctx->hash_context->mask_64k;
 #endif
 #ifdef CONFIG_HUGETLB_PAGE
 	if (psize == MMU_PAGE_16M)
-		return mm_ctx_slice_mask_16m(&ctx);
+		return &ctx->hash_context->mask_16m;
 	if (psize == MMU_PAGE_16G)
-		return mm_ctx_slice_mask_16g(&ctx);
+		return &ctx->hash_context->mask_16g;
 #endif
 	BUG_ON(psize != MMU_PAGE_4K);
 
-	return mm_ctx_slice_mask_4k(&ctx);
+	return &ctx->hash_context->mask_4k;
 }
 
 #ifdef CONFIG_PPC_SUBPAGE_PROT

commit fca5c1e9eb5e263c1b4def0b5ae4ce5b2e1a9877
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Apr 25 14:29:30 2019 +0000

    powerpc/mm: move slice_mask_for_size() into mmu.h
    
    Move slice_mask_for_size() into subarch mmu.h
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    [mpe: Retain the BUG_ON()s, rather than converting to VM_BUG_ON()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 230a9dec7677..a6d5b5ed1170 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -203,6 +203,23 @@ static inline struct slice_mask *mm_ctx_slice_mask_16g(mm_context_t *ctx)
 }
 #endif
 
+static inline struct slice_mask *slice_mask_for_size(mm_context_t *ctx, int psize)
+{
+#ifdef CONFIG_PPC_64K_PAGES
+	if (psize == MMU_PAGE_64K)
+		return mm_ctx_slice_mask_64k(&ctx);
+#endif
+#ifdef CONFIG_HUGETLB_PAGE
+	if (psize == MMU_PAGE_16M)
+		return mm_ctx_slice_mask_16m(&ctx);
+	if (psize == MMU_PAGE_16G)
+		return mm_ctx_slice_mask_16g(&ctx);
+#endif
+	BUG_ON(psize != MMU_PAGE_4K);
+
+	return mm_ctx_slice_mask_4k(&ctx);
+}
+
 #ifdef CONFIG_PPC_SUBPAGE_PROT
 static inline struct subpage_prot_table *mm_ctx_subpage_prot(mm_context_t *ctx)
 {

commit ef629cc5bf0543eb57d6e344ba776880ac35fd00
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Apr 17 18:33:51 2019 +0530

    powerc/mm/hash: Reduce hash_mm_context size
    
    Allocate subpage protect related variables only if we use the feature.
    This helps in reducing the hash related mm context struct by around 4K
    
    Before the patch
    sizeof(struct hash_mm_context)  = 8288
    
    After the patch
    sizeof(struct hash_mm_context) = 4160
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index e510e46b07ce..230a9dec7677 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -206,7 +206,7 @@ static inline struct slice_mask *mm_ctx_slice_mask_16g(mm_context_t *ctx)
 #ifdef CONFIG_PPC_SUBPAGE_PROT
 static inline struct subpage_prot_table *mm_ctx_subpage_prot(mm_context_t *ctx)
 {
-	return &ctx->hash_context->spt;
+	return ctx->hash_context->spt;
 }
 #endif
 

commit 701101865f5d3e268281ce7a254eb4a97d16cbdc
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Apr 17 18:33:50 2019 +0530

    powerpc/mm: Reduce memory usage for mm_context_t for radix
    
    Currently, our mm_context_t on book3s64 include all hash specific
    context details like slice mask and subpage protection details. We
    can skip allocating these with radix translation. This will help us to save
    8K per mm_context with radix translation.
    
    With the patch applied we have
    
    sizeof(mm_context_t)  = 136
    sizeof(struct hash_mm_context)  = 8288
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index c9f317090620..e510e46b07ce 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -104,16 +104,6 @@ struct spinlock;
 /* Maximum possible number of NPUs in a system. */
 #define NV_MAX_NPUS 8
 
-/*
- * One bit per slice. We have lower slices which cover 256MB segments
- * upto 4G range. That gets us 16 low slices. For the rest we track slices
- * in 1TB size.
- */
-struct slice_mask {
-	u64 low_slices;
-	DECLARE_BITMAP(high_slices, SLICE_NUM_HIGH);
-};
-
 typedef struct {
 	union {
 		/*
@@ -127,7 +117,6 @@ typedef struct {
 		mm_context_id_t id;
 		mm_context_id_t extended_id[TASK_SIZE_USER64/TASK_CONTEXT_SIZE];
 	};
-	u16 user_psize;		/* page size index */
 
 	/* Number of bits in the mm_cpumask */
 	atomic_t active_cpus;
@@ -137,23 +126,9 @@ typedef struct {
 
 	/* NPU NMMU context */
 	struct npu_context *npu_context;
+	struct hash_mm_context *hash_context;
 
-	 /* SLB page size encodings*/
-	unsigned char low_slices_psize[LOW_SLICE_ARRAY_SZ];
-	unsigned char high_slices_psize[SLICE_ARRAY_SIZE];
-	unsigned long slb_addr_limit;
-# ifdef CONFIG_PPC_64K_PAGES
-	struct slice_mask mask_64k;
-# endif
-	struct slice_mask mask_4k;
-# ifdef CONFIG_HUGETLB_PAGE
-	struct slice_mask mask_16m;
-	struct slice_mask mask_16g;
-# endif
 	unsigned long vdso_base;
-#ifdef CONFIG_PPC_SUBPAGE_PROT
-	struct subpage_prot_table spt;
-#endif /* CONFIG_PPC_SUBPAGE_PROT */
 	/*
 	 * pagetable fragment support
 	 */
@@ -176,62 +151,62 @@ typedef struct {
 
 static inline u16 mm_ctx_user_psize(mm_context_t *ctx)
 {
-	return ctx->user_psize;
+	return ctx->hash_context->user_psize;
 }
 
 static inline void mm_ctx_set_user_psize(mm_context_t *ctx, u16 user_psize)
 {
-	ctx->user_psize = user_psize;
+	ctx->hash_context->user_psize = user_psize;
 }
 
 static inline unsigned char *mm_ctx_low_slices(mm_context_t *ctx)
 {
-	return ctx->low_slices_psize;
+	return ctx->hash_context->low_slices_psize;
 }
 
 static inline unsigned char *mm_ctx_high_slices(mm_context_t *ctx)
 {
-	return ctx->high_slices_psize;
+	return ctx->hash_context->high_slices_psize;
 }
 
 static inline unsigned long mm_ctx_slb_addr_limit(mm_context_t *ctx)
 {
-	return ctx->slb_addr_limit;
+	return ctx->hash_context->slb_addr_limit;
 }
 
 static inline void mm_ctx_set_slb_addr_limit(mm_context_t *ctx, unsigned long limit)
 {
-	ctx->slb_addr_limit = limit;
+	ctx->hash_context->slb_addr_limit = limit;
 }
 
 #ifdef CONFIG_PPC_64K_PAGES
 static inline struct slice_mask *mm_ctx_slice_mask_64k(mm_context_t *ctx)
 {
-	return &ctx->mask_64k;
+	return &ctx->hash_context->mask_64k;
 }
 #endif
 
 static inline struct slice_mask *mm_ctx_slice_mask_4k(mm_context_t *ctx)
 {
-	return &ctx->mask_4k;
+	return &ctx->hash_context->mask_4k;
 }
 
 #ifdef CONFIG_HUGETLB_PAGE
 static inline struct slice_mask *mm_ctx_slice_mask_16m(mm_context_t *ctx)
 {
-	return &ctx->mask_16m;
+	return &ctx->hash_context->mask_16m;
 }
 
 static inline struct slice_mask *mm_ctx_slice_mask_16g(mm_context_t *ctx)
 {
-	return &ctx->mask_16g;
+	return &ctx->hash_context->mask_16g;
 }
 #endif
 
 #ifdef CONFIG_PPC_SUBPAGE_PROT
 static inline struct subpage_prot_table *mm_ctx_subpage_prot(mm_context_t *ctx)
 {
-	return &ctx->spt;
+	return &ctx->hash_context->spt;
 }
 #endif
 

commit 60458fba469a695a026334b364cf8adbcd5807e3
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Apr 17 18:33:48 2019 +0530

    powerpc/mm: Add helpers for accessing hash translation related variables
    
    We want to switch to allocating them runtime only when hash translation is
    enabled. Add helpers so that both book3s and nohash can be adapted to
    upcoming change easily.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index afe10dd11c68..c9f317090620 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -139,7 +139,7 @@ typedef struct {
 	struct npu_context *npu_context;
 
 	 /* SLB page size encodings*/
-	unsigned char low_slices_psize[BITS_PER_LONG / BITS_PER_BYTE];
+	unsigned char low_slices_psize[LOW_SLICE_ARRAY_SZ];
 	unsigned char high_slices_psize[SLICE_ARRAY_SIZE];
 	unsigned long slb_addr_limit;
 # ifdef CONFIG_PPC_64K_PAGES
@@ -174,6 +174,67 @@ typedef struct {
 #endif
 } mm_context_t;
 
+static inline u16 mm_ctx_user_psize(mm_context_t *ctx)
+{
+	return ctx->user_psize;
+}
+
+static inline void mm_ctx_set_user_psize(mm_context_t *ctx, u16 user_psize)
+{
+	ctx->user_psize = user_psize;
+}
+
+static inline unsigned char *mm_ctx_low_slices(mm_context_t *ctx)
+{
+	return ctx->low_slices_psize;
+}
+
+static inline unsigned char *mm_ctx_high_slices(mm_context_t *ctx)
+{
+	return ctx->high_slices_psize;
+}
+
+static inline unsigned long mm_ctx_slb_addr_limit(mm_context_t *ctx)
+{
+	return ctx->slb_addr_limit;
+}
+
+static inline void mm_ctx_set_slb_addr_limit(mm_context_t *ctx, unsigned long limit)
+{
+	ctx->slb_addr_limit = limit;
+}
+
+#ifdef CONFIG_PPC_64K_PAGES
+static inline struct slice_mask *mm_ctx_slice_mask_64k(mm_context_t *ctx)
+{
+	return &ctx->mask_64k;
+}
+#endif
+
+static inline struct slice_mask *mm_ctx_slice_mask_4k(mm_context_t *ctx)
+{
+	return &ctx->mask_4k;
+}
+
+#ifdef CONFIG_HUGETLB_PAGE
+static inline struct slice_mask *mm_ctx_slice_mask_16m(mm_context_t *ctx)
+{
+	return &ctx->mask_16m;
+}
+
+static inline struct slice_mask *mm_ctx_slice_mask_16g(mm_context_t *ctx)
+{
+	return &ctx->mask_16g;
+}
+#endif
+
+#ifdef CONFIG_PPC_SUBPAGE_PROT
+static inline struct subpage_prot_table *mm_ctx_subpage_prot(mm_context_t *ctx)
+{
+	return &ctx->spt;
+}
+#endif
+
 /*
  * The current system page and segment sizes
  */

commit 4f40b15f339d896f5726714842947c9339742494
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Apr 17 18:33:47 2019 +0530

    powerpc/mm: Remove PPC_MM_SLICES #ifdef for book3s64
    
    Book3s64 always have PPC_MM_SLICES enabled. So remove the unncessary #ifdef
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index a809bdd77322..afe10dd11c68 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -138,7 +138,6 @@ typedef struct {
 	/* NPU NMMU context */
 	struct npu_context *npu_context;
 
-#ifdef CONFIG_PPC_MM_SLICES
 	 /* SLB page size encodings*/
 	unsigned char low_slices_psize[BITS_PER_LONG / BITS_PER_BYTE];
 	unsigned char high_slices_psize[SLICE_ARRAY_SIZE];
@@ -151,9 +150,6 @@ typedef struct {
 	struct slice_mask mask_16m;
 	struct slice_mask mask_16g;
 # endif
-#else
-	u16 sllp;		/* SLB page size encoding */
-#endif
 	unsigned long vdso_base;
 #ifdef CONFIG_PPC_SUBPAGE_PROT
 	struct subpage_prot_table spt;

commit 6161a37307f3320808b5a7549593b991500f2656
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Apr 3 11:35:14 2019 +0530

    powerpc/mm: Fix build error with FLATMEM book3s64 config
    
    The current value of MAX_PHYSMEM_BITS cannot work with 32 bit configs.
    We used to have MAX_PHYSMEM_BITS not defined without SPARSEMEM and 32
    bit configs never expected a value to be set for MAX_PHYSMEM_BITS.
    
    Dependent code such as zsmalloc derived the right values based on other
    fields. Instead of finding a value that works with different configs,
    use new values only for book3s_64. For 64 bit booke, use the definition
    of MAX_PHYSMEM_BITS as per commit a7df61a0e2b6 ("[PATCH] ppc64: Increase sparsemem defaults")
    That change was done in 2005 and hopefully will work with book3e 64.
    
    Fixes: 8bc086899816 ("powerpc/mm: Only define MAX_PHYSMEM_BITS in SPARSEMEM configurations")
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 1ceee000c18d..a809bdd77322 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -35,6 +35,21 @@ typedef pte_t *pgtable_t;
 
 #endif /* __ASSEMBLY__ */
 
+/*
+ * If we store section details in page->flags we can't increase the MAX_PHYSMEM_BITS
+ * if we increase SECTIONS_WIDTH we will not store node details in page->flags and
+ * page_to_nid does a page->section->node lookup
+ * Hence only increase for VMEMMAP. Further depending on SPARSEMEM_EXTREME reduce
+ * memory requirements with large number of sections.
+ * 51 bits is the max physical real address on POWER9
+ */
+#if defined(CONFIG_SPARSEMEM_VMEMMAP) && defined(CONFIG_SPARSEMEM_EXTREME) &&  \
+	defined(CONFIG_PPC_64K_PAGES)
+#define MAX_PHYSMEM_BITS 51
+#else
+#define MAX_PHYSMEM_BITS 46
+#endif
+
 /* 64-bit classic hash table MMU */
 #include <asm/book3s/64/mmu-hash.h>
 

commit d09780f3a8d48fd49136d7bae8f0ae30de7f261a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Nov 29 14:06:57 2018 +0000

    powerpc/mm: Move pgtable_t into platform headers
    
    This patch move pgtable_t into platform headers.
    
    It gets rid of the CONFIG_PPC_64K_PAGES case for PPC64
    as nohash/64 doesn't support CONFIG_PPC_64K_PAGES.
    
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 6328857f259f..1ceee000c18d 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -2,6 +2,8 @@
 #ifndef _ASM_POWERPC_BOOK3S_64_MMU_H_
 #define _ASM_POWERPC_BOOK3S_64_MMU_H_
 
+#include <asm/page.h>
+
 #ifndef __ASSEMBLY__
 /*
  * Page size definition
@@ -24,6 +26,13 @@ struct mmu_psize_def {
 };
 extern struct mmu_psize_def mmu_psize_defs[MMU_PAGE_COUNT];
 
+/*
+ * For BOOK3s 64 with 4k and 64K linux page size
+ * we want to use pointers, because the page table
+ * actually store pfn
+ */
+typedef pte_t *pgtable_t;
+
 #endif /* __ASSEMBLY__ */
 
 /* 64-bit classic hash table MMU */

commit c9f80734cd552ddba50567bc43b0ff250a4b2c17
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Thu Sep 20 14:03:57 2018 +0530

    powerpc/mm/hash: Rename get_ea_context to get_user_context
    
    We will be adding get_kernel_context later. Update function name to indicate
    this handle context allocation user space address.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 9c8c669a6b6a..6328857f259f 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -208,7 +208,7 @@ extern void radix_init_pseries(void);
 static inline void radix_init_pseries(void) { };
 #endif
 
-static inline int get_ea_context(mm_context_t *ctx, unsigned long ea)
+static inline int get_user_context(mm_context_t *ctx, unsigned long ea)
 {
 	int index = ea >> MAX_EA_BITS_PER_CONTEXT;
 
@@ -223,7 +223,7 @@ static inline int get_ea_context(mm_context_t *ctx, unsigned long ea)
 static inline unsigned long get_user_vsid(mm_context_t *ctx,
 					  unsigned long ea, int ssize)
 {
-	unsigned long context = get_ea_context(ctx, ea);
+	unsigned long context = get_user_context(ctx, ea);
 
 	return get_vsid(context, ea, ssize);
 }

commit 8a6c697b993974eeb05f540fb0ff2515346dbefd
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Apr 16 16:57:22 2018 +0530

    powerpc/mm: Implement helpers for pagetable fragment support at PMD level
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index fde7803a2261..9c8c669a6b6a 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -138,6 +138,7 @@ typedef struct {
 	 * pagetable fragment support
 	 */
 	void *pte_frag;
+	void *pmd_frag;
 #ifdef CONFIG_SPAPR_TCE_IOMMU
 	struct list_head iommu_group_mem_list;
 #endif

commit 1c7ec8a40aa7796a5a996bd985899c93dacaaa35
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Apr 16 16:57:20 2018 +0530

    powerpc/mm/book3s64/4k: Switch 4k pagesize config to use pagetable fragment
    
    4K config use one full page at level 4 of the pagetable. Add support for single
    fragment allocation in pagetable fragment code and and use that for 4K config.
    This makes both 4k and 64k use the same code path. Later we will switch pmd to
    use the page table fragment code. This is done only for 64bit platforms which
    is using page table fragment support.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 5094696eecd6..fde7803a2261 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -134,10 +134,10 @@ typedef struct {
 #ifdef CONFIG_PPC_SUBPAGE_PROT
 	struct subpage_prot_table spt;
 #endif /* CONFIG_PPC_SUBPAGE_PROT */
-#ifdef CONFIG_PPC_64K_PAGES
-	/* for 4K PTE fragment support */
+	/*
+	 * pagetable fragment support
+	 */
 	void *pte_frag;
-#endif
 #ifdef CONFIG_SPAPR_TCE_IOMMU
 	struct list_head iommu_group_mem_list;
 #endif

commit f384796c40dc55b3dba25e0ee9c1afd98c6d24d1
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Mar 26 15:34:48 2018 +0530

    powerpc/mm: Add support for handling > 512TB address in SLB miss
    
    For addresses above 512TB we allocate additional mmu contexts. To make
    it all easy, addresses above 512TB are handled with IR/DR=1 and with
    stack frame setup.
    
    The mmu_context_t is also updated to track the new extended_ids. To
    support upto 4PB we need a total 8 contexts.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [mpe: Minor formatting tweaks and comment wording, switch BUG to WARN
          in get_ea_context().]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index c8c836e8ad1b..5094696eecd6 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -91,7 +91,18 @@ struct slice_mask {
 };
 
 typedef struct {
-	mm_context_id_t id;
+	union {
+		/*
+		 * We use id as the PIDR content for radix. On hash we can use
+		 * more than one id. The extended ids are used when we start
+		 * having address above 512TB. We allocate one extended id
+		 * for each 512TB. The new id is then used with the 49 bit
+		 * EA to build a new VA. We always use ESID_BITS_1T_MASK bits
+		 * from EA and new context ids to build the new VAs.
+		 */
+		mm_context_id_t id;
+		mm_context_id_t extended_id[TASK_SIZE_USER64/TASK_CONTEXT_SIZE];
+	};
 	u16 user_psize;		/* page size index */
 
 	/* Number of bits in the mm_cpumask */
@@ -196,5 +207,25 @@ extern void radix_init_pseries(void);
 static inline void radix_init_pseries(void) { };
 #endif
 
+static inline int get_ea_context(mm_context_t *ctx, unsigned long ea)
+{
+	int index = ea >> MAX_EA_BITS_PER_CONTEXT;
+
+	if (likely(index < ARRAY_SIZE(ctx->extended_id)))
+		return ctx->extended_id[index];
+
+	/* should never happen */
+	WARN_ON(1);
+	return 0;
+}
+
+static inline unsigned long get_user_vsid(mm_context_t *ctx,
+					  unsigned long ea, int ssize)
+{
+	unsigned long context = get_ea_context(ctx, ea);
+
+	return get_vsid(context, ea, ssize);
+}
+
 #endif /* __ASSEMBLY__ */
 #endif /* _ASM_POWERPC_BOOK3S_64_MMU_H_ */

commit 95dff480bb7b7b5ea534ebf00a18ff57eb897669
Merge: c0b346729b5d 52396500f97c
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Mar 28 22:59:50 2018 +1100

    Merge branch 'fixes' into next
    
    Merge our fixes branch from the 4.16 cycle.
    
    There were a number of important fixes merged, in particular some Power9
    workarounds that we want in next for testing purposes. There's also been
    some conflicting changes in the CPU features code which are best merged
    and tested before going upstream.

commit aff6f8cb3e2170b9e58b0932bce7bfb492775e23
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Mar 23 09:29:05 2018 +1100

    powerpc/mm: Add tracking of the number of coprocessors using a context
    
    Currently, when using coprocessors (which use the Nest MMU), we
    simply increment the active_cpu count to force all TLB invalidations
    to be come broadcast.
    
    Unfortunately, due to an errata in POWER9, we will need to know
    more specifically that coprocessors are in use.
    
    This maintains a separate copros counter in the MMU context for
    that purpose.
    
    NB. The commit mentioned in the fixes tag below is not at fault for
    the bug we're fixing in this commit and the next, but this fix applies
    on top the infrastructure it introduced.
    
    Fixes: 03b8abedf4f4 ("cxl: Enable global TLBIs for cxl contexts")
    Cc: stable@vger.kernel.org # v4.15+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Tested-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 0abeb0e2d616..37671feb2bf6 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -87,6 +87,9 @@ typedef struct {
 	/* Number of bits in the mm_cpumask */
 	atomic_t active_cpus;
 
+	/* Number of users of the external (Nest) MMU */
+	atomic_t copros;
+
 	/* NPU NMMU context */
 	struct npu_context *npu_context;
 

commit 5709f7cfd8305252dc327206bd674ad65ca4d77f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Mar 7 11:37:12 2018 +1000

    powerpc/mm/slice: implement a slice mask cache
    
    Calculating the slice mask can become a signifcant overhead for
    get_unmapped_area. This patch adds a struct slice_mask for
    each page size in the mm_context, and keeps these in synch with
    the slices psize arrays and slb_addr_limit.
    
    On Book3S/64 this adds 288 bytes to the mm_context_t for the
    slice mask caches.
    
    On POWER8, this increases vfork+exec+exit performance by 9.9%
    and reduces time to mmap+munmap a 64kB page by 28%.
    
    Reduces time to mmap+munmap by about 10% on 8xx.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index bef6e39ed63a..777778579305 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -80,6 +80,16 @@ struct spinlock;
 /* Maximum possible number of NPUs in a system. */
 #define NV_MAX_NPUS 8
 
+/*
+ * One bit per slice. We have lower slices which cover 256MB segments
+ * upto 4G range. That gets us 16 low slices. For the rest we track slices
+ * in 1TB size.
+ */
+struct slice_mask {
+	u64 low_slices;
+	DECLARE_BITMAP(high_slices, SLICE_NUM_HIGH);
+};
+
 typedef struct {
 	mm_context_id_t id;
 	u16 user_psize;		/* page size index */
@@ -95,6 +105,14 @@ typedef struct {
 	unsigned char low_slices_psize[BITS_PER_LONG / BITS_PER_BYTE];
 	unsigned char high_slices_psize[SLICE_ARRAY_SIZE];
 	unsigned long slb_addr_limit;
+# ifdef CONFIG_PPC_64K_PAGES
+	struct slice_mask mask_64k;
+# endif
+	struct slice_mask mask_4k;
+# ifdef CONFIG_HUGETLB_PAGE
+	struct slice_mask mask_16m;
+	struct slice_mask mask_16g;
+# endif
 #else
 	u16 sllp;		/* SLB page size encoding */
 #endif

commit 15472423ce47d6397d08d48daaae8590c9f9f242
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Feb 22 15:27:28 2018 +0100

    powerpc/mm/slice: Allow up to 64 low slices
    
    While the implementation of the "slices" address space allows
    a significant amount of high slices, it limits the number of
    low slices to 16 due to the use of a single u64 low_slices_psize
    element in struct mm_context_t
    
    On the 8xx, the minimum slice size is the size of the area
    covered by a single PMD entry, ie 4M in 4K pages mode and 64M in
    16K pages mode. This means we could have at least 64 slices.
    
    In order to override this limitation, this patch switches the
    handling of low_slices_psize to char array as done already for
    high_slices_psize.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 0abeb0e2d616..bef6e39ed63a 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -91,7 +91,8 @@ typedef struct {
 	struct npu_context *npu_context;
 
 #ifdef CONFIG_PPC_MM_SLICES
-	u64 low_slices_psize;	/* SLB page size encodings */
+	 /* SLB page size encodings*/
+	unsigned char low_slices_psize[BITS_PER_LONG / BITS_PER_BYTE];
 	unsigned char high_slices_psize[SLICE_ARRAY_SIZE];
 	unsigned long slb_addr_limit;
 #else

commit 5586cf61e108019565bb936daeb296e53df1c1d6
Author: Ram Pai <linuxram@us.ibm.com>
Date:   Thu Jan 18 17:50:32 2018 -0800

    powerpc: introduce execute-only pkey
    
    This patch provides the implementation of execute-only pkey.
    The architecture-independent layer expects the arch-dependent
    layer, to support the ability to create and enable a special
    key which has execute-only permission.
    
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Ram Pai <linuxram@us.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 37ef23cc1136..0abeb0e2d616 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -116,6 +116,7 @@ typedef struct {
 	 * bit unset -> key available for allocation
 	 */
 	u32 pkey_allocation_map;
+	s16 execute_only_pkey; /* key holding execute-only protection */
 #endif
 } mm_context_t;
 

commit 4fb158f65ac5556b9b4a6f63f38272853ed99b22
Author: Ram Pai <linuxram@us.ibm.com>
Date:   Thu Jan 18 17:50:25 2018 -0800

    powerpc: track allocation status of all pkeys
    
    Total 32 keys are available on power7 and above. However
    pkey 0,1 are reserved. So effectively we  have  30 pkeys.
    
    On 4K kernels, we do not  have  5  bits  in  the  PTE to
    represent  all the keys; we only have 3bits. Two of those
    keys are reserved; pkey 0 and pkey 1. So effectively  we
    have 6 pkeys.
    
    This patch keeps track of reserved keys, allocated  keys
    and keys that are currently free.
    
    Also it  adds  skeletal  functions  and macros, that the
    architecture-independent code expects to be available.
    
    Reviewed-by: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
    Signed-off-by: Ram Pai <linuxram@us.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index c9448e19847a..37ef23cc1136 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -108,6 +108,15 @@ typedef struct {
 #ifdef CONFIG_SPAPR_TCE_IOMMU
 	struct list_head iommu_group_mem_list;
 #endif
+
+#ifdef CONFIG_PPC_MEM_KEYS
+	/*
+	 * Each bit represents one protection key.
+	 * bit set   -> key allocated
+	 * bit unset -> key available for allocation
+	 */
+	u32 pkey_allocation_map;
+#endif
 } mm_context_t;
 
 /*

commit 5b0e2cb020085efe202123162502e0b551e49a0e
Merge: 758f875848d7 3ffa9d9e2a7c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 12:47:46 2017 -0800

    Merge tag 'powerpc-4.15-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "A bit of a small release, I suspect in part due to me travelling for
      KS. But my backlog of patches to review is smaller than usual, so I
      think in part folks just didn't send as much this cycle.
    
      Non-highlights:
    
       - Five fixes for the >128T address space handling, both to fix bugs
         in our implementation and to bring the semantics exactly into line
         with x86.
    
      Highlights:
    
       - Support for a new OPAL call on bare metal machines which gives us a
         true NMI (ie. is not masked by MSR[EE]=0) for debugging etc.
    
       - Support for Power9 DD2 in the CXL driver.
    
       - Improvements to machine check handling so that uncorrectable errors
         can be reported into the generic memory_failure() machinery.
    
       - Some fixes and improvements for VPHN, which is used under PowerVM
         to notify the Linux partition of topology changes.
    
       - Plumbing to enable TM (transactional memory) without suspend on
         some Power9 processors (PPC_FEATURE2_HTM_NO_SUSPEND).
    
       - Support for emulating vector loads form cache-inhibited memory, on
         some Power9 revisions.
    
       - Disable the fast-endian switch "syscall" by default (behind a
         CONFIG), we believe it has never had any users.
    
       - A major rework of the API drivers use when initiating and waiting
         for long running operations performed by OPAL firmware, and changes
         to the powernv_flash driver to use the new API.
    
       - Several fixes for the handling of FP/VMX/VSX while processes are
         using transactional memory.
    
       - Optimisations of TLB range flushes when using the radix MMU on
         Power9.
    
       - Improvements to the VAS facility used to access coprocessors on
         Power9, and related improvements to the way the NX crypto driver
         handles requests.
    
       - Implementation of PMEM_API and UACCESS_FLUSHCACHE for 64-bit.
    
      Thanks to: Alexey Kardashevskiy, Alistair Popple, Allen Pais, Andrew
      Donnellan, Aneesh Kumar K.V, Arnd Bergmann, Balbir Singh, Benjamin
      Herrenschmidt, Breno Leitao, Christophe Leroy, Christophe Lombard,
      Cyril Bur, Frederic Barrat, Gautham R. Shenoy, Geert Uytterhoeven,
      Guilherme G. Piccoli, Gustavo Romero, Haren Myneni, Joel Stanley,
      Kamalesh Babulal, Kautuk Consul, Markus Elfring, Masami Hiramatsu,
      Michael Bringmann, Michael Neuling, Michal Suchanek, Naveen N. Rao,
      Nicholas Piggin, Oliver O'Halloran, Paul Mackerras, Pedro Miraglia
      Franco de Carvalho, Philippe Bergheaud, Sandipan Das, Seth Forshee,
      Shriya, Stephen Rothwell, Stewart Smith, Sukadev Bhattiprolu, Tyrel
      Datwyler, Vaibhav Jain, Vaidyanathan Srinivasan, and William A.
      Kennington III"
    
    * tag 'powerpc-4.15-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (151 commits)
      powerpc/64s: Fix Power9 DD2.0 workarounds by adding DD2.1 feature
      powerpc/64s: Fix masking of SRR1 bits on instruction fault
      powerpc/64s: mm_context.addr_limit is only used on hash
      powerpc/64s/radix: Fix 128TB-512TB virtual address boundary case allocation
      powerpc/64s/hash: Allow MAP_FIXED allocations to cross 128TB boundary
      powerpc/64s/hash: Fix fork() with 512TB process address space
      powerpc/64s/hash: Fix 128TB-512TB virtual address boundary case allocation
      powerpc/64s/hash: Fix 512T hint detection to use >= 128T
      powerpc: Fix DABR match on hash based systems
      powerpc/signal: Properly handle return value from uprobe_deny_signal()
      powerpc/fadump: use kstrtoint to handle sysfs store
      powerpc/lib: Implement UACCESS_FLUSHCACHE API
      powerpc/lib: Implement PMEM API
      powerpc/powernv/npu: Don't explicitly flush nmmu tlb
      powerpc/powernv/npu: Use flush_all_mm() instead of flush_tlb_mm()
      powerpc/powernv/idle: Round up latency and residency values
      powerpc/kprobes: refactor kprobe_lookup_name for safer string operations
      powerpc/kprobes: Blacklist emulate_update_regs() from kprobes
      powerpc/kprobes: Do not disable interrupts for optprobes and kprobes_on_ftrace
      powerpc/kprobes: Disable preemption before invoking probe handler for optprobes
      ...

commit 4722476bce283149986a3463f61009dec0e7f6a1
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Nov 10 04:27:40 2017 +1100

    powerpc/64s: mm_context.addr_limit is only used on hash
    
    Radix keeps no meaningful state in addr_limit, so remove it from radix
    code and rename to slb_addr_limit to make it clear it applies to hash
    only.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index c3b00e8ff791..49a07c5d9e50 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -92,7 +92,7 @@ typedef struct {
 #ifdef CONFIG_PPC_MM_SLICES
 	u64 low_slices_psize;	/* SLB page size encodings */
 	unsigned char high_slices_psize[SLICE_ARRAY_SIZE];
-	unsigned long addr_limit;
+	unsigned long slb_addr_limit;
 #else
 	u16 sllp;		/* SLB page size encoding */
 #endif

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index c3b00e8ff791..37fdede5a24c 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_POWERPC_BOOK3S_64_MMU_H_
 #define _ASM_POWERPC_BOOK3S_64_MMU_H_
 

commit a619e59c075c66e530a88e57b45bb0417e2f4fff
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Jul 24 14:28:02 2017 +1000

    powerpc/mm: Optimize detection of thread local mm's
    
    Instead of comparing the whole CPU mask every time, let's
    keep a counter of how many bits are set in the mask. Thus
    testing for a local mm only requires testing if that counter
    is 1 and the current CPU bit is set in the mask.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 1a220cdff923..c3b00e8ff791 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -83,6 +83,9 @@ typedef struct {
 	mm_context_id_t id;
 	u16 user_psize;		/* page size index */
 
+	/* Number of bits in the mm_cpumask */
+	atomic_t active_cpus;
+
 	/* NPU NMMU context */
 	struct npu_context *npu_context;
 

commit 15c659ff9d5b367c886166a9854a89b72c524a68
Merge: 516fa8d0e19d 1a92a80ad386
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Aug 23 22:20:10 2017 +1000

    Merge branch 'fixes' into next
    
    There's a non-trivial dependency between some commits we want to put in
    next and the KVM prefetch work around that went into fixes. So merge
    fixes into next.

commit 6ff4d3e9665274b69cff47f64cc20183465a1de2
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Jul 19 14:49:46 2017 +1000

    powerpc: Remove old unused icswx based coprocessor support
    
    We have a whole pile of unused code to maintain the ACOP register,
    allocate coprocessor PIDs and handle ACOP faults. This mechanism
    was used for the HFI adapter on POWER7 which is dead and gone and
    whose driver never went upstream. It was used on some A2 core based
    stuff that also never saw the light of day.
    
    Take out all that code.
    
    There is still some POWER8 coprocessor code that uses icswx but it's
    kernel only and thus doesn't use any of that infrastructure.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 77529a3e3811..cbf1945f4338 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -96,11 +96,6 @@ typedef struct {
 #ifdef CONFIG_PPC_SUBPAGE_PROT
 	struct subpage_prot_table spt;
 #endif /* CONFIG_PPC_SUBPAGE_PROT */
-#ifdef CONFIG_PPC_ICSWX
-	struct spinlock *cop_lockp; /* guard acop and cop_pid */
-	unsigned long acop;	/* mask of enabled coprocessor types */
-	unsigned int cop_pid;	/* pid value used with coprocessors */
-#endif /* CONFIG_PPC_ICSWX */
 #ifdef CONFIG_PPC_64K_PAGES
 	/* for 4K PTE fragment support */
 	void *pte_frag;

commit a25bd72badfa793ab5aeafd50dbd9db39f8c9179
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Jul 24 14:26:06 2017 +1000

    powerpc/mm/radix: Workaround prefetch issue with KVM
    
    There's a somewhat architectural issue with Radix MMU and KVM.
    
    When coming out of a guest with AIL (Alternate Interrupt Location, ie,
    MMU enabled), we start executing hypervisor code with the PID register
    still containing whatever the guest has been using.
    
    The problem is that the CPU can (and will) then start prefetching or
    speculatively load from whatever host context has that same PID (if
    any), thus bringing translations for that context into the TLB, which
    Linux doesn't know about.
    
    This can cause stale translations and subsequent crashes.
    
    Fixing this in a way that is neither racy nor a huge performance
    impact is difficult. We could just make the host invalidations always
    use broadcast forms but that would hurt single threaded programs for
    example.
    
    We chose to fix it instead by partitioning the PID space between guest
    and host. This is possible because today Linux only use 19 out of the
    20 bits of PID space, so existing guests will work if we make the host
    use the top half of the 20 bits space.
    
    We additionally add support for a property to indicate to Linux the
    size of the PID register which will be useful if we eventually have
    processors with a larger PID space available.
    
    There is still an issue with malicious guests purposefully setting the
    PID register to a value in the hosts PID range. Hopefully future HW
    can prevent that, but in the meantime, we handle it with a pair of
    kludges:
    
     - On the way out of a guest, before we clear the current VCPU in the
       PACA, we check the PID and if it's outside of the permitted range
       we flush the TLB for that PID.
    
     - When context switching, if the mm is "new" on that CPU (the
       corresponding bit was set for the first time in the mm cpumask), we
       check if any sibling thread is in KVM (has a non-NULL VCPU pointer
       in the PACA). If that is the case, we also flush the PID for that
       CPU (core).
    
    This second part is needed to handle the case where a process is
    migrated (or starts a new pthread) on a sibling thread of the CPU
    coming out of KVM, as there's a window where stale translations can
    exist before we detect it and flush them out.
    
    A future optimization could be added by keeping track of whether the
    PID has ever been used and avoid doing that for completely fresh PIDs.
    We could similarily mark PIDs that have been the subject of a global
    invalidation as "fresh". But for now this will do.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [mpe: Rework the asm to build with CONFIG_PPC_RADIX_MMU=n, drop
          unneeded include of kvm_book3s_asm.h]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 77529a3e3811..5b4023c616f7 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -59,13 +59,14 @@ extern struct patb_entry *partition_tb;
 #define PRTS_MASK	0x1f		/* process table size field */
 #define PRTB_MASK	0x0ffffffffffff000UL
 
-/*
- * Limit process table to PAGE_SIZE table. This
- * also limit the max pid we can support.
- * MAX_USER_CONTEXT * 16 bytes of space.
- */
-#define PRTB_SIZE_SHIFT	(CONTEXT_BITS + 4)
-#define PRTB_ENTRIES	(1ul << CONTEXT_BITS)
+/* Number of supported PID bits */
+extern unsigned int mmu_pid_bits;
+
+/* Base PID to allocate from */
+extern unsigned int mmu_base_pid;
+
+#define PRTB_SIZE_SHIFT	(mmu_pid_bits + 4)
+#define PRTB_ENTRIES	(1ul << mmu_pid_bits)
 
 /*
  * Power9 currently only support 64K partition table size.

commit 1ab66d1fbadad86b1f4a9c7857e193af0ee0022c
Author: Alistair Popple <alistair@popple.id.au>
Date:   Mon Apr 3 19:51:44 2017 +1000

    powerpc/powernv: Introduce address translation services for Nvlink2
    
    Nvlink2 supports address translation services (ATS) allowing devices
    to request address translations from an mmu known as the nest MMU
    which is setup to walk the CPU page tables.
    
    To access this functionality certain firmware calls are required to
    setup and manage hardware context tables in the nvlink processing unit
    (NPU). The NPU also manages forwarding of TLB invalidates (known as
    address translation shootdowns/ATSDs) to attached devices.
    
    This patch exports several methods to allow device drivers to register
    a process id (PASID/PID) in the hardware tables and to receive
    notification of when a device should stop issuing address translation
    requests (ATRs). It also adds a fault handler to allow device drivers
    to demand fault pages in.
    
    Signed-off-by: Alistair Popple <alistair@popple.id.au>
    [mpe: Fix up comment formatting, use flush_tlb_mm()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index c4b865112d24..77529a3e3811 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -75,10 +75,16 @@ extern struct patb_entry *partition_tb;
 typedef unsigned long mm_context_id_t;
 struct spinlock;
 
+/* Maximum possible number of NPUs in a system. */
+#define NV_MAX_NPUS 8
+
 typedef struct {
 	mm_context_id_t id;
 	u16 user_psize;		/* page size index */
 
+	/* NPU NMMU context */
+	struct npu_context *npu_context;
+
 #ifdef CONFIG_PPC_MM_SLICES
 	u64 low_slices_psize;	/* SLB page size encodings */
 	unsigned char high_slices_psize[SLICE_ARRAY_SIZE];

commit 957b778a166e32e242a33fdab693ffb256a19cbd
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Mar 22 09:06:58 2017 +0530

    powerpc/mm: Add addr_limit to mm_context and use it to derive max slice index
    
    In the followup patch, we will increase the slice array size to handle
    512TB range, but will limit the max addr to 128TB. Avoid doing
    unnecessary computation and avoid doing slice mask related operation
    above address limit.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index cce434e7de06..c4b865112d24 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -82,6 +82,7 @@ typedef struct {
 #ifdef CONFIG_PPC_MM_SLICES
 	u64 low_slices_psize;	/* SLB page size encodings */
 	unsigned char high_slices_psize[SLICE_ARRAY_SIZE];
+	unsigned long addr_limit;
 #else
 	u16 sllp;		/* SLB page size encoding */
 #endif

commit 760573c1a98cce20b9306f3922d460e58b4ba656
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Mar 29 22:36:56 2017 +1100

    powerpc/mm: Split radix vs hash mm context initialisation
    
    Complete the split of the radix vs hash mm context initialisation.
    
    This is mostly code movement, with the exception that we now limit the
    context allocation to PRTB_ENTRIES - 1 on radix.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 805d4105e9bb..cce434e7de06 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -65,6 +65,8 @@ extern struct patb_entry *partition_tb;
  * MAX_USER_CONTEXT * 16 bytes of space.
  */
 #define PRTB_SIZE_SHIFT	(CONTEXT_BITS + 4)
+#define PRTB_ENTRIES	(1ul << CONTEXT_BITS)
+
 /*
  * Power9 currently only support 64K partition table size.
  */

commit 2d62e0768d3c28536d4cfe4c40ba1e5e8e442a93
Merge: be834aafdf5f 16ce771b93ab
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 4 11:36:19 2017 -0800

    Merge tag 'kvm-4.11-2' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull more KVM updates from Radim Krm:
     "Second batch of KVM changes for the 4.11 merge window:
    
      PPC:
       - correct assumption about ASDR on POWER9
       - fix MMIO emulation on POWER9
    
      x86:
       - add a simple test for ioperm
       - cleanup TSS (going through KVM tree as the whole undertaking was
         caused by VMX's use of TSS)
       - fix nVMX interrupt delivery
       - fix some performance counters in the guest
    
      ... and two cleanup patches"
    
    * tag 'kvm-4.11-2' of git://git.kernel.org/pub/scm/virt/kvm/kvm:
      KVM: nVMX: Fix pending events injection
      x86/kvm/vmx: remove unused variable in segment_base()
      selftests/x86: Add a basic selftest for ioperm
      x86/asm: Tidy up TSS limit code
      kvm: convert kvm.users_count from atomic_t to refcount_t
      KVM: x86: never specify a sample period for virtualized in_tx_cp counters
      KVM: PPC: Book3S HV: Don't use ASDR for real-mode HPT faults on POWER9
      KVM: PPC: Book3S HV: Fix software walk of guest process page tables

commit 70cd4c10b290dd77fff6dc702a9a2c8c679df121
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Feb 27 11:51:37 2017 +1100

    KVM: PPC: Book3S HV: Fix software walk of guest process page tables
    
    This fixes some bugs in the code that walks the guest's page tables.
    These bugs cause MMIO emulation to fail whenever the guest is in
    virtial mode (MMU on), leading to the guest hanging if it tried to
    access a virtio device.
    
    The first bug was that when reading the guest's process table, we were
    using the whole of arch->process_table, not just the field that contains
    the process table base address.  The second bug was that the mask used
    when reading the process table entry to get the radix tree base address,
    RPDB_MASK, had the wrong value.
    
    Fixes: 9e04ba69beec ("KVM: PPC: Book3S HV: Add basic infrastructure for radix guests")
    Fixes: e99833448c5f ("powerpc/mm/radix: Add partition table format & callback")
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index d73e9dfa5237..440f3423e213 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -46,7 +46,7 @@ extern struct patb_entry *partition_tb;
 
 /* Bits in patb0 field */
 #define PATB_HR		(1UL << 63)
-#define RPDB_MASK	0x0ffffffffffff00fUL
+#define RPDB_MASK	0x0fffffffffffff00UL
 #define RPDB_SHIFT	(1UL << 8)
 #define RTS1_SHIFT	61		/* top 2 bits of radix tree size */
 #define RTS1_MASK	(3UL << RTS1_SHIFT)
@@ -57,6 +57,7 @@ extern struct patb_entry *partition_tb;
 /* Bits in patb1 field */
 #define PATB_GR		(1UL << 63)	/* guest uses radix; must match HR */
 #define PRTS_MASK	0x1f		/* process table size field */
+#define PRTB_MASK	0x0ffffffffffff000UL
 
 /*
  * Limit process table to PAGE_SIZE table. This

commit 8ab102d60a0c19df602f3758848d55f0703bf9bb
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Mon Feb 27 14:28:55 2017 -0800

    scripts/spelling.txt: add "partiton" pattern and fix typo instances
    
    Fix typos and add the following to the scripts/spelling.txt:
    
      partiton||partition
    
    Link: http://lkml.kernel.org/r/1481573103-11329-7-git-send-email-yamada.masahiro@socionext.com
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index d73e9dfa5237..1145dc8e726d 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -30,7 +30,7 @@ extern struct mmu_psize_def mmu_psize_defs[MMU_PAGE_COUNT];
 
 #ifndef __ASSEMBLY__
 /*
- * ISA 3.0 partiton and process table entry format
+ * ISA 3.0 partition and process table entry format
  */
 struct prtb_entry {
 	__be64 prtb0;

commit dbcbfee0c81c7938e40d7d6bc659a5191f490b50
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 30 21:21:37 2017 +1100

    powerpc/64: More definitions for POWER9
    
    This adds definitions for bits in the DSISR register which are used
    by POWER9 for various translation-related exception conditions, and
    for some more bits in the partition table entry that will be needed
    by KVM.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index cea522c3bcae..d73e9dfa5237 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -44,10 +44,20 @@ struct patb_entry {
 };
 extern struct patb_entry *partition_tb;
 
+/* Bits in patb0 field */
 #define PATB_HR		(1UL << 63)
-#define PATB_GR		(1UL << 63)
 #define RPDB_MASK	0x0ffffffffffff00fUL
 #define RPDB_SHIFT	(1UL << 8)
+#define RTS1_SHIFT	61		/* top 2 bits of radix tree size */
+#define RTS1_MASK	(3UL << RTS1_SHIFT)
+#define RTS2_SHIFT	5		/* bottom 3 bits of radix tree size */
+#define RTS2_MASK	(7UL << RTS2_SHIFT)
+#define RPDS_MASK	0x1f		/* root page dir. size field */
+
+/* Bits in patb1 field */
+#define PATB_GR		(1UL << 63)	/* guest uses radix; must match HR */
+#define PRTS_MASK	0x1f		/* process table size field */
+
 /*
  * Limit process table to PAGE_SIZE table. This
  * also limit the max pid we can support.

commit cc3d2940133d24000e2866b21e03ce32adfead0a
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 30 21:21:36 2017 +1100

    powerpc/64: Enable use of radix MMU under hypervisor on POWER9
    
    To use radix as a guest, we first need to tell the hypervisor via
    the ibm,client-architecture call first that we support POWER9 and
    architecture v3.00, and that we can do either radix or hash and
    that we would like to choose later using an hcall (the
    H_REGISTER_PROC_TBL hcall).
    
    Then we need to check whether the hypervisor agreed to us using
    radix.  We need to do this very early on in the kernel boot process
    before any of the MMU initialization is done.  If the hypervisor
    doesn't agree, we can't use radix and therefore clear the radix
    MMU feature bit.
    
    Later, when we have set up our process table, which points to the
    radix tree for each process, we need to install that using the
    H_REGISTER_PROC_TBL hcall.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 8afb0e00f7d9..cea522c3bcae 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -138,5 +138,11 @@ static inline void setup_initial_memory_limit(phys_addr_t first_memblock_base,
 extern int (*register_process_table)(unsigned long base, unsigned long page_size,
 				     unsigned long tbl_size);
 
+#ifdef CONFIG_PPC_PSERIES
+extern void radix_init_pseries(void);
+#else
+static inline void radix_init_pseries(void) { };
+#endif
+
 #endif /* __ASSEMBLY__ */
 #endif /* _ASM_POWERPC_BOOK3S_64_MMU_H_ */

commit eea8148c69f3aecbf297b12943a591467a1fb432
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Aug 4 15:32:06 2016 +1000

    powerpc/mm: Move register_process_table() out of ppc_md
    
    We want to initialise register_process_table() before ppc_md is setup,
    so that it can be called as part of MMU init (at least on Radix ATM).
    
    That no longer works because probe_machine() requires that ppc_md be
    empty before it's called, and we now do probe_machine() much later.
    
    So make register_process_table a global for now. It will probably move
    into a mmu_radix_ops struct at some point in the future.
    
    This was broken by me when applying commit 7025776ed1eb "powerpc/mm:
    Move hash table ops to a separate structure" due to conflicts with other
    patches.
    
    Fixes: 7025776ed1eb ("powerpc/mm: Move hash table ops to a separate structure")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 0cbde6a6750b..8afb0e00f7d9 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -134,5 +134,9 @@ static inline void setup_initial_memory_limit(phys_addr_t first_memblock_base,
 	return hash__setup_initial_memory_limit(first_memblock_base,
 					   first_memblock_size);
 }
+
+extern int (*register_process_table)(unsigned long base, unsigned long page_size,
+				     unsigned long tbl_size);
+
 #endif /* __ASSEMBLY__ */
 #endif /* _ASM_POWERPC_BOOK3S_64_MMU_H_ */

commit b8f1b4f8606b40b478072fb551f79e2984d44fad
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Sat Jul 23 14:42:35 2016 +0530

    powerpc/mm: Convert early cpu/mmu feature check to use the new helpers
    
    This switches early feature checks to use the non static key variant of
    the function. In later patches we will be switching cpu_has_feature()
    and mmu_has_feature() to use static keys and we can use them only after
    static key/jump label is initialized. Any check for feature before jump
    label init should be done using this new helper.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 70c995870297..0cbde6a6750b 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -128,7 +128,7 @@ extern void radix__setup_initial_memory_limit(phys_addr_t first_memblock_base,
 static inline void setup_initial_memory_limit(phys_addr_t first_memblock_base,
 					      phys_addr_t first_memblock_size)
 {
-	if (radix_enabled())
+	if (early_radix_enabled())
 		return radix__setup_initial_memory_limit(first_memblock_base,
 						   first_memblock_size);
 	return hash__setup_initial_memory_limit(first_memblock_base,

commit bab4c8de6289b4615c21ddf0400397d03ce1863c
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jul 27 13:37:58 2016 +1000

    powerpc/mm: Define radix_enabled() in one place & use static inline
    
    Currently we have radix_enabled() three times, twice in asm/book3s/64/mmu.h
    and then a fallback in asm/mmu.h.
    
    Consolidate them in asm/mmu.h. While we're at it convert them to be
    static inlines, and change the fallback case to returning a bool, like
    mmu_has_feature().
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index ad2d501cddcf..70c995870297 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -23,13 +23,6 @@ struct mmu_psize_def {
 };
 extern struct mmu_psize_def mmu_psize_defs[MMU_PAGE_COUNT];
 
-#ifdef CONFIG_PPC_RADIX_MMU
-#define radix_enabled() mmu_has_feature(MMU_FTR_TYPE_RADIX)
-#else
-#define radix_enabled() (0)
-#endif
-
-
 #endif /* __ASSEMBLY__ */
 
 /* 64-bit classic hash table MMU */

commit 5a25b6f527f9f5bbf5747b1b97e538e6d61bd2f2
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Jul 27 13:19:01 2016 +1000

    powerpc/mm: Make MMU_FTR_RADIX a MMU family feature
    
    MMU feature bits are defined such that we use the lower half to
    present MMU family features. Remove the strict split of half and
    also move Radix to a mmu family feature. Radix introduce a new MMU
    model and strictly speaking it is a new MMU family. This also free
    up bits which can be used for individual features later.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 9ee00c2576d0..ad2d501cddcf 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -24,7 +24,7 @@ struct mmu_psize_def {
 extern struct mmu_psize_def mmu_psize_defs[MMU_PAGE_COUNT];
 
 #ifdef CONFIG_PPC_RADIX_MMU
-#define radix_enabled() mmu_has_feature(MMU_FTR_RADIX)
+#define radix_enabled() mmu_has_feature(MMU_FTR_TYPE_RADIX)
 #else
 #define radix_enabled() (0)
 #endif

commit 2537b09c939fca26447f0ac3e688691de06eadda
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jul 26 21:55:27 2016 +1000

    powerpc/mm: Do radix device tree scanning earlier
    
    Like we just did for hash, split the device tree scanning parts out and
    call them from mmu_early_init_devtree().
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 358f1410dc0d..9ee00c2576d0 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -109,6 +109,7 @@ extern int mmu_io_psize;
 /* MMU initialization */
 void mmu_early_init_devtree(void);
 void hash__early_init_devtree(void);
+void radix__early_init_devtree(void);
 extern void radix_init_native(void);
 extern void hash__early_init_mmu(void);
 extern void radix__early_init_mmu(void);

commit bacf9cf88303c0df5794ca45dd9f297740a00913
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jul 26 21:31:59 2016 +1000

    powerpc/mm: Do hash device tree scanning earlier
    
    Currently MMU initialisation (early_init_mmu()) consists of a mixture of
    scanning the device tree, setting MMU feature bits, and then also doing
    actual initialisation of MMU data structures.
    
    We'd like to decouple the setting of the MMU features from the actual
    setup. So split out the device tree scanning, and associated code, and
    call it from mmu_init_early_devtree().
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 4eb4bd019716..358f1410dc0d 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -108,6 +108,7 @@ extern int mmu_io_psize;
 
 /* MMU initialization */
 void mmu_early_init_devtree(void);
+void hash__early_init_devtree(void);
 extern void radix_init_native(void);
 extern void hash__early_init_mmu(void);
 extern void radix__early_init_mmu(void);

commit 1a01dc87e09b6e60d22c417e00f470a72f00ec80
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jul 26 20:09:30 2016 +1000

    powerpc/mm: Add mmu_early_init_devtree()
    
    Empty for now, but we'll add to it in the next patch.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index d4eda6420523..4eb4bd019716 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -107,6 +107,7 @@ extern int mmu_vmemmap_psize;
 extern int mmu_io_psize;
 
 /* MMU initialization */
+void mmu_early_init_devtree(void);
 extern void radix_init_native(void);
 extern void hash__early_init_mmu(void);
 extern void radix__early_init_mmu(void);

commit e21fc93b7020905c9f3672969ddd276aa152b9b7
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Jul 13 15:05:23 2016 +0530

    powerpc/mm: Compile out radix related functions if RADIX_MMU is disabled
    
    Currently we depend on mmu_has_feature to evalute to zero based on
    MMU_FTRS_POSSIBLE mask. In a later patch, we want to update
    radix_enabled() to runtime update the conditional operation to a jump
    instruction. This implies we cannot depend on MMU_FTRS_POSSIBLE mask.
    Instead define radix_enabled to return 0 if RADIX_MMU is not enabled.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 5854263d4d6e..d4eda6420523 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -23,7 +23,12 @@ struct mmu_psize_def {
 };
 extern struct mmu_psize_def mmu_psize_defs[MMU_PAGE_COUNT];
 
+#ifdef CONFIG_PPC_RADIX_MMU
 #define radix_enabled() mmu_has_feature(MMU_FTR_RADIX)
+#else
+#define radix_enabled() (0)
+#endif
+
 
 #endif /* __ASSEMBLY__ */
 

commit a8ed87c92adf1fd45142323e04f15b522117d575
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:26:06 2016 +1000

    powerpc/mm/radix: Add MMU_FTR_RADIX
    
    We are going to add asm changes in the follow up patches. Add the
    feature bit now so that we can get it all build.
    
    mpe: When CONFIG_PPC_RADIX_MMU=n we omit MMU_FTR_RADIX from the
    MMU_FTRS_POSSIBLE mask. This allows the compiler to work out that those
    checks will always be false and so the code can be elided completely.
    
    Note we do *not* define MMU_FTR_RADIX to 0 in the RADIX_MMU=n case,
    because that doesn't work with the ASM_FTR patching. In particular an
    IF_SET section will result in a mask and value of zero, which is always
    true, meaning the section *won't* be patched, which is the opposite of
    what we want.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 94d52ff49bbe..5854263d4d6e 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -23,7 +23,8 @@ struct mmu_psize_def {
 };
 extern struct mmu_psize_def mmu_psize_defs[MMU_PAGE_COUNT];
 
-#define radix_enabled() (0)
+#define radix_enabled() mmu_has_feature(MMU_FTR_RADIX)
+
 #endif /* __ASSEMBLY__ */
 
 /* 64-bit classic hash table MMU */

commit 2bfd65e45e877fb5704730244da67c748d28a1b8
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:25:58 2016 +1000

    powerpc/mm/radix: Add radix callbacks for early init routines
    
    This adds routines for early setup for radix. We use device tree
    property "ibm,processor-radix-AP-encodings" to find supported page
    sizes. If we don't find the above we consider 64K and 4K as supported
    page sizes.
    
    We do map vmemap using 2M page size if we can. The linear mapping is
    done such that we use required page size for that range. For example
    memory of 3.5G is mapped such that we use 1G mapping till 3G range and
    use 2M mapping for the rest.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 37943f7af60f..94d52ff49bbe 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -16,7 +16,10 @@ struct mmu_psize_def {
 	int		penc[MMU_PAGE_COUNT];	/* HPTE encoding */
 	unsigned int	tlbiel;	/* tlbiel supported for that page size */
 	unsigned long	avpnm;	/* bits to mask out in AVPN in the HPTE */
-	unsigned long	sllp;	/* SLB L||LP (exact mask to use in slbmte) */
+	union {
+		unsigned long	sllp;	/* SLB L||LP (exact mask to use in slbmte) */
+		unsigned long ap;	/* Ap encoding used by PowerISA 3.0 */
+	};
 };
 extern struct mmu_psize_def mmu_psize_defs[MMU_PAGE_COUNT];
 
@@ -98,22 +101,34 @@ extern int mmu_vmemmap_psize;
 extern int mmu_io_psize;
 
 /* MMU initialization */
+extern void radix_init_native(void);
 extern void hash__early_init_mmu(void);
+extern void radix__early_init_mmu(void);
 static inline void early_init_mmu(void)
 {
+	if (radix_enabled())
+		return radix__early_init_mmu();
 	return hash__early_init_mmu();
 }
 extern void hash__early_init_mmu_secondary(void);
+extern void radix__early_init_mmu_secondary(void);
 static inline void early_init_mmu_secondary(void)
 {
+	if (radix_enabled())
+		return radix__early_init_mmu_secondary();
 	return hash__early_init_mmu_secondary();
 }
 
 extern void hash__setup_initial_memory_limit(phys_addr_t first_memblock_base,
 					 phys_addr_t first_memblock_size);
+extern void radix__setup_initial_memory_limit(phys_addr_t first_memblock_base,
+					 phys_addr_t first_memblock_size);
 static inline void setup_initial_memory_limit(phys_addr_t first_memblock_base,
 					      phys_addr_t first_memblock_size)
 {
+	if (radix_enabled())
+		return radix__setup_initial_memory_limit(first_memblock_base,
+						   first_memblock_size);
 	return hash__setup_initial_memory_limit(first_memblock_base,
 					   first_memblock_size);
 }

commit 756d08d1ba169ed9eae4d48e9a0af014e86593fc
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:25:57 2016 +1000

    powerpc/mm: Abstract early MMU init in preparation for radix
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 593d9e3ce8e7..37943f7af60f 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -97,5 +97,25 @@ extern int mmu_vmalloc_psize;
 extern int mmu_vmemmap_psize;
 extern int mmu_io_psize;
 
+/* MMU initialization */
+extern void hash__early_init_mmu(void);
+static inline void early_init_mmu(void)
+{
+	return hash__early_init_mmu();
+}
+extern void hash__early_init_mmu_secondary(void);
+static inline void early_init_mmu_secondary(void)
+{
+	return hash__early_init_mmu_secondary();
+}
+
+extern void hash__setup_initial_memory_limit(phys_addr_t first_memblock_base,
+					 phys_addr_t first_memblock_size);
+static inline void setup_initial_memory_limit(phys_addr_t first_memblock_base,
+					      phys_addr_t first_memblock_size)
+{
+	return hash__setup_initial_memory_limit(first_memblock_base,
+					   first_memblock_size);
+}
 #endif /* __ASSEMBLY__ */
 #endif /* _ASM_POWERPC_BOOK3S_64_MMU_H_ */

commit 566ca99af026d43c7cf7ab72f78463dbcc7e6ac2
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:25:53 2016 +1000

    powerpc/mm/radix: Add dummy radix_enabled()
    
    In this patch we add the radix Kconfig and conditional check.
    radix_enabled() is written to always return 0 here. Once we have all
    needed radix changes added, we will update this to an mmu_feature check.
    
    We need to add this early so that we can get it all build in the early
    stage.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index b86786f2521c..593d9e3ce8e7 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -19,6 +19,8 @@ struct mmu_psize_def {
 	unsigned long	sllp;	/* SLB L||LP (exact mask to use in slbmte) */
 };
 extern struct mmu_psize_def mmu_psize_defs[MMU_PAGE_COUNT];
+
+#define radix_enabled() (0)
 #endif /* __ASSEMBLY__ */
 
 /* 64-bit classic hash table MMU */

commit e99833448c5fa198a120db6d7087dd17149dc273
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:25:42 2016 +1000

    powerpc/mm/radix: Add partition table format & callback
    
    Add structs and #defines related to the radix MMU partition table
    format. We also add a ppc_md callback for updating a partition table
    entry.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index aadb0bbc5c71..b86786f2521c 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -21,12 +21,39 @@ struct mmu_psize_def {
 extern struct mmu_psize_def mmu_psize_defs[MMU_PAGE_COUNT];
 #endif /* __ASSEMBLY__ */
 
-#ifdef CONFIG_PPC_STD_MMU_64
 /* 64-bit classic hash table MMU */
 #include <asm/book3s/64/mmu-hash.h>
-#endif
 
 #ifndef __ASSEMBLY__
+/*
+ * ISA 3.0 partiton and process table entry format
+ */
+struct prtb_entry {
+	__be64 prtb0;
+	__be64 prtb1;
+};
+extern struct prtb_entry *process_tb;
+
+struct patb_entry {
+	__be64 patb0;
+	__be64 patb1;
+};
+extern struct patb_entry *partition_tb;
+
+#define PATB_HR		(1UL << 63)
+#define PATB_GR		(1UL << 63)
+#define RPDB_MASK	0x0ffffffffffff00fUL
+#define RPDB_SHIFT	(1UL << 8)
+/*
+ * Limit process table to PAGE_SIZE table. This
+ * also limit the max pid we can support.
+ * MAX_USER_CONTEXT * 16 bytes of space.
+ */
+#define PRTB_SIZE_SHIFT	(CONTEXT_BITS + 4)
+/*
+ * Power9 currently only support 64K partition table size.
+ */
+#define PATB_SIZE_SHIFT	16
 
 typedef unsigned long mm_context_id_t;
 struct spinlock;

commit 11a6f6abd74ab80865023200ca33515f6db43d63
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:25:41 2016 +1000

    powerpc/mm: Move radix/hash common data structures to book3s64 headers
    
    Start moving code that is generic between radix and hash to book3s64
    specific headers from the book3s64 hash specific one.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
new file mode 100644
index 000000000000..aadb0bbc5c71
--- /dev/null
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -0,0 +1,72 @@
+#ifndef _ASM_POWERPC_BOOK3S_64_MMU_H_
+#define _ASM_POWERPC_BOOK3S_64_MMU_H_
+
+#ifndef __ASSEMBLY__
+/*
+ * Page size definition
+ *
+ *    shift : is the "PAGE_SHIFT" value for that page size
+ *    sllp  : is a bit mask with the value of SLB L || LP to be or'ed
+ *            directly to a slbmte "vsid" value
+ *    penc  : is the HPTE encoding mask for the "LP" field:
+ *
+ */
+struct mmu_psize_def {
+	unsigned int	shift;	/* number of bits */
+	int		penc[MMU_PAGE_COUNT];	/* HPTE encoding */
+	unsigned int	tlbiel;	/* tlbiel supported for that page size */
+	unsigned long	avpnm;	/* bits to mask out in AVPN in the HPTE */
+	unsigned long	sllp;	/* SLB L||LP (exact mask to use in slbmte) */
+};
+extern struct mmu_psize_def mmu_psize_defs[MMU_PAGE_COUNT];
+#endif /* __ASSEMBLY__ */
+
+#ifdef CONFIG_PPC_STD_MMU_64
+/* 64-bit classic hash table MMU */
+#include <asm/book3s/64/mmu-hash.h>
+#endif
+
+#ifndef __ASSEMBLY__
+
+typedef unsigned long mm_context_id_t;
+struct spinlock;
+
+typedef struct {
+	mm_context_id_t id;
+	u16 user_psize;		/* page size index */
+
+#ifdef CONFIG_PPC_MM_SLICES
+	u64 low_slices_psize;	/* SLB page size encodings */
+	unsigned char high_slices_psize[SLICE_ARRAY_SIZE];
+#else
+	u16 sllp;		/* SLB page size encoding */
+#endif
+	unsigned long vdso_base;
+#ifdef CONFIG_PPC_SUBPAGE_PROT
+	struct subpage_prot_table spt;
+#endif /* CONFIG_PPC_SUBPAGE_PROT */
+#ifdef CONFIG_PPC_ICSWX
+	struct spinlock *cop_lockp; /* guard acop and cop_pid */
+	unsigned long acop;	/* mask of enabled coprocessor types */
+	unsigned int cop_pid;	/* pid value used with coprocessors */
+#endif /* CONFIG_PPC_ICSWX */
+#ifdef CONFIG_PPC_64K_PAGES
+	/* for 4K PTE fragment support */
+	void *pte_frag;
+#endif
+#ifdef CONFIG_SPAPR_TCE_IOMMU
+	struct list_head iommu_group_mem_list;
+#endif
+} mm_context_t;
+
+/*
+ * The current system page and segment sizes
+ */
+extern int mmu_linear_psize;
+extern int mmu_virtual_psize;
+extern int mmu_vmalloc_psize;
+extern int mmu_vmemmap_psize;
+extern int mmu_io_psize;
+
+#endif /* __ASSEMBLY__ */
+#endif /* _ASM_POWERPC_BOOK3S_64_MMU_H_ */
