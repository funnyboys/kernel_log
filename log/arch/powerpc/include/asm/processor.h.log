commit c887ef5707591e84f80271e95e99ff9fb38987b5
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu May 28 00:58:43 2020 +1000

    powerpc/64s: Don't set FSCR bits in INIT_THREAD
    
    Since the previous commit that saves the value of FSCR configured at
    boot into init_task.thread.fscr, the static initialisation in
    INIT_THREAD now no longer has any effect.
    
    So remove it.
    
    For non DT CPU features, the end result is the same, because
    __init_FSCR() is called on all CPUs that have an FSCR (Power8,
    Power9), and it sets FSCR_TAR & FSCR_EBB.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200527145843.2761782-4-mpe@ellerman.id.au

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 4e53df163b92..52a67835057a 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -302,7 +302,6 @@ struct thread_struct {
 	.ksp = INIT_SP, \
 	.addr_limit = KERNEL_DS, \
 	.fpexc_mode = 0, \
-	.fscr = FSCR_TAR | FSCR_EBB \
 }
 #endif
 

commit 74c6881019b7d56c327fffc268d97adb5eb1b4f9
Author: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
Date:   Thu May 14 16:47:38 2020 +0530

    powerpc/watchpoint: Prepare handler to handle more than one watchpoint
    
    Currently we assume that we have only one watchpoint supported by hw.
    Get rid of that assumption and use dynamic loop instead. This should
    make supporting more watchpoints very easy.
    
    With more than one watchpoint, exception handler needs to know which
    DAWR caused the exception, and hw currently does not provide it. So
    we need sw logic for the same. To figure out which DAWR caused the
    exception, check all different combinations of user specified range,
    DAWR address range, actual access range and DAWRX constrains. For ex,
    if user specified range and actual access range overlaps but DAWRX is
    configured for readonly watchpoint and the instruction is store, this
    DAWR must not have caused exception.
    
    Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Reviewed-by: Michael Neuling <mikey@neuling.org>
    [mpe: Unsplit multi-line printk() strings, fix some sparse warnings]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200514111741.97993-14-ravi.bangoria@linux.ibm.com

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index fba6b586e3c8..4e53df163b92 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -185,7 +185,7 @@ struct thread_struct {
 	 * Helps identify source of single-step exception and subsequent
 	 * hw-breakpoint enablement
 	 */
-	struct perf_event *last_hit_ubp;
+	struct perf_event *last_hit_ubp[HBP_NUM_MAX];
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 	struct arch_hw_breakpoint hw_brk[HBP_NUM_MAX]; /* hardware breakpoint info */
 	unsigned long	trap_nr;	/* last trap # on this thread */

commit 303e6a9ddcdc168e92253c78cdb4bbe1e10d78b3
Author: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
Date:   Thu May 14 16:47:34 2020 +0530

    powerpc/watchpoint: Convert thread_struct->hw_brk to an array
    
    So far powerpc hw supported only one watchpoint. But Power10 is
    introducing 2nd DAWR. Convert thread_struct->hw_brk into an array.
    
    Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Michael Neuling <mikey@neuling.org>
    Link: https://lore.kernel.org/r/20200514111741.97993-10-ravi.bangoria@linux.ibm.com

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index f209c5703ee2..fba6b586e3c8 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -187,7 +187,7 @@ struct thread_struct {
 	 */
 	struct perf_event *last_hit_ubp;
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
-	struct arch_hw_breakpoint hw_brk; /* info on the hardware breakpoint */
+	struct arch_hw_breakpoint hw_brk[HBP_NUM_MAX]; /* hardware breakpoint info */
 	unsigned long	trap_nr;	/* last trap # on this thread */
 	u8 load_slb;			/* Ages out SLB preload cache entries */
 	u8 load_fp;

commit a6ba44e8799230e36c8ab06fda7f77f421e9e795
Author: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
Date:   Thu May 14 16:47:28 2020 +0530

    powerpc/watchpoint: Introduce function to get nr watchpoints dynamically
    
    So far we had only one watchpoint, so we have hardcoded HBP_NUM to 1.
    But Power10 is introducing 2nd DAWR and thus kernel should be able to
    dynamically find actual number of watchpoints supported by hw it's
    running on. Introduce function for the same. Also convert HBP_NUM macro
    to HBP_NUM_MAX, which will now represent maximum number of watchpoints
    supported by Powerpc.
    
    Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Michael Neuling <mikey@neuling.org>
    Link: https://lore.kernel.org/r/20200514111741.97993-4-ravi.bangoria@linux.ibm.com

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 5ab202055d5a..f209c5703ee2 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -180,7 +180,7 @@ struct thread_struct {
 	int		fpexc_mode;	/* floating-point exception mode */
 	unsigned int	align_ctl;	/* alignment handling control */
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
-	struct perf_event *ptrace_bps[HBP_NUM];
+	struct perf_event *ptrace_bps[HBP_NUM_MAX];
 	/*
 	 * Helps identify source of single-step exception and subsequent
 	 * hw-breakpoint enablement

commit 24ac99e97fa7b8f0db9b48413a76def9cf73295c
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Apr 28 22:31:52 2020 +1000

    powerpc: Drop unneeded cast in task_pt_regs()
    
    There's no need to cast in task_pt_regs() as tsk->thread.regs should
    already be a struct pt_regs. If someone's using task_pt_regs() on
    something that's not a task but happens to have a thread.regs then
    we'll deal with them later.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200428123152.73566-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index a51964b4ec42..5ab202055d5a 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -306,7 +306,7 @@ struct thread_struct {
 }
 #endif
 
-#define task_pt_regs(tsk)	((struct pt_regs *)(tsk)->thread.regs)
+#define task_pt_regs(tsk)	((tsk)->thread.regs)
 
 unsigned long get_wchan(struct task_struct *p);
 

commit 7ffa8b7dc11752827329e4e84a574ea6aaf24716
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Apr 28 22:31:30 2020 +1000

    powerpc/64: Don't initialise init_task->thread.regs
    
    Aneesh increased the size of struct pt_regs by 16 bytes and started
    seeing this WARN_ON:
    
      smp: Bringing up secondary CPUs ...
      ------------[ cut here ]------------
      WARNING: CPU: 0 PID: 0 at arch/powerpc/kernel/process.c:455 giveup_all+0xb4/0x110
      Modules linked in:
      CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.7.0-rc2-gcc-8.2.0-1.g8f6a41f-default+ #318
      NIP:  c00000000001a2b4 LR: c00000000001a29c CTR: c0000000031d0000
      REGS: c0000000026d3980 TRAP: 0700   Not tainted  (5.7.0-rc2-gcc-8.2.0-1.g8f6a41f-default+)
      MSR:  800000000282b033 <SF,VEC,VSX,EE,FP,ME,IR,DR,RI,LE>  CR: 48048224  XER: 00000000
      CFAR: c000000000019cc8 IRQMASK: 1
      GPR00: c00000000001a264 c0000000026d3c20 c0000000026d7200 800000000280b033
      GPR04: 0000000000000001 0000000000000000 0000000000000077 30206d7372203164
      GPR08: 0000000000002000 0000000002002000 800000000280b033 3230303030303030
      GPR12: 0000000000008800 c0000000031d0000 0000000000800050 0000000002000066
      GPR16: 000000000309a1a0 000000000309a4b0 000000000309a2d8 000000000309a890
      GPR20: 00000000030d0098 c00000000264da40 00000000fd620000 c0000000ff798080
      GPR24: c00000000264edf0 c0000001007469f0 00000000fd620000 c0000000020e5e90
      GPR28: c00000000264edf0 c00000000264d200 000000001db60000 c00000000264d200
      NIP [c00000000001a2b4] giveup_all+0xb4/0x110
      LR [c00000000001a29c] giveup_all+0x9c/0x110
      Call Trace:
      [c0000000026d3c20] [c00000000001a264] giveup_all+0x64/0x110 (unreliable)
      [c0000000026d3c90] [c00000000001ae34] __switch_to+0x104/0x480
      [c0000000026d3cf0] [c000000000e0b8a0] __schedule+0x320/0x970
      [c0000000026d3dd0] [c000000000e0c518] schedule_idle+0x38/0x70
      [c0000000026d3df0] [c00000000019c7c8] do_idle+0x248/0x3f0
      [c0000000026d3e70] [c00000000019cbb8] cpu_startup_entry+0x38/0x40
      [c0000000026d3ea0] [c000000000011bb0] rest_init+0xe0/0xf8
      [c0000000026d3ed0] [c000000002004820] start_kernel+0x990/0x9e0
      [c0000000026d3f90] [c00000000000c49c] start_here_common+0x1c/0x400
    
    Which was unexpected. The warning is checking the thread.regs->msr
    value of the task we are switching from:
    
      usermsr = tsk->thread.regs->msr;
      ...
      WARN_ON((usermsr & MSR_VSX) && !((usermsr & MSR_FP) && (usermsr & MSR_VEC)));
    
    ie. if MSR_VSX is set then both of MSR_FP and MSR_VEC are also set.
    
    Dumping tsk->thread.regs->msr we see that it's: 0x1db60000
    
    Which is not a normal looking MSR, in fact the only valid bit is
    MSR_VSX, all the other bits are reserved in the current definition of
    the MSR.
    
    We can see from the oops that it was swapper/0 that we were switching
    from when we hit the warning, ie. init_task. So its thread.regs points
    to the base (high addresses) in init_stack.
    
    Dumping the content of init_task->thread.regs, with the members of
    pt_regs annotated (the 16 bytes larger version), we see:
    
      0000000000000000 c000000002780080    gpr[0]     gpr[1]
      0000000000000000 c000000002666008    gpr[2]     gpr[3]
      c0000000026d3ed0 0000000000000078    gpr[4]     gpr[5]
      c000000000011b68 c000000002780080    gpr[6]     gpr[7]
      0000000000000000 0000000000000000    gpr[8]     gpr[9]
      c0000000026d3f90 0000800000002200    gpr[10]    gpr[11]
      c000000002004820 c0000000026d7200    gpr[12]    gpr[13]
      000000001db60000 c0000000010aabe8    gpr[14]    gpr[15]
      c0000000010aabe8 c0000000010aabe8    gpr[16]    gpr[17]
      c00000000294d598 0000000000000000    gpr[18]    gpr[19]
      0000000000000000 0000000000001ff8    gpr[20]    gpr[21]
      0000000000000000 c00000000206d608    gpr[22]    gpr[23]
      c00000000278e0cc 0000000000000000    gpr[24]    gpr[25]
      000000002fff0000 c000000000000000    gpr[26]    gpr[27]
      0000000002000000 0000000000000028    gpr[28]    gpr[29]
      000000001db60000 0000000004750000    gpr[30]    gpr[31]
      0000000002000000 000000001db60000    nip        msr
      0000000000000000 0000000000000000    orig_r3    ctr
      c00000000000c49c 0000000000000000    link       xer
      0000000000000000 0000000000000000    ccr        softe
      0000000000000000 0000000000000000    trap       dar
      0000000000000000 0000000000000000    dsisr      result
      0000000000000000 0000000000000000    ppr        kuap
      0000000000000000 0000000000000000    pad[2]     pad[3]
    
    This looks suspiciously like stack frames, not a pt_regs. If we look
    closely we can see return addresses from the stack trace above,
    c000000002004820 (start_kernel) and c00000000000c49c (start_here_common).
    
    init_task->thread.regs is setup at build time in processor.h:
    
      #define INIT_THREAD  { \
            .ksp = INIT_SP, \
            .regs = (struct pt_regs *)INIT_SP - 1, /* XXX bogus, I think */ \
    
    The early boot code where we setup the initial stack is:
    
      LOAD_REG_ADDR(r3,init_thread_union)
    
      /* set up a stack pointer */
      LOAD_REG_IMMEDIATE(r1,THREAD_SIZE)
      add   r1,r3,r1
      li    r0,0
      stdu  r0,-STACK_FRAME_OVERHEAD(r1)
    
    Which creates a stack frame of size 112 bytes (STACK_FRAME_OVERHEAD).
    Which is far too small to contain a pt_regs.
    
    So the result is init_task->thread.regs is pointing at some stack
    frames on the init stack, not at a pt_regs.
    
    We have gotten away with this for so long because with pt_regs at its
    current size the MSR happens to point into the first frame, at a
    location that is not written to by the early asm. With the 16 byte
    expansion the MSR falls into the second frame, which is used by the
    compiler, and collides with a saved register that tends to be
    non-zero.
    
    As far as I can see this has been wrong since the original merge of
    64-bit ppc support, back in 2002.
    
    Conceptually swapper should have no regs, it never entered from
    userspace, and in fact that's what we do on 32-bit. It's also
    presumably what the "bogus" comment is referring to.
    
    So I think the right fix is to just not-initialise regs at all. I'm
    slightly worried this will break some code that isn't prepared for a
    NULL regs, but we'll have to see.
    
    Remove the comment in head_64.S which refers to us setting up the
    regs (even though we never did), and is otherwise not really accurate
    any more.
    
    Reported-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200428123130.73078-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index bfa336fbcfeb..a51964b4ec42 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -300,7 +300,6 @@ struct thread_struct {
 #else
 #define INIT_THREAD  { \
 	.ksp = INIT_SP, \
-	.regs = (struct pt_regs *)INIT_SP - 1, /* XXX bogus, I think */ \
 	.addr_limit = KERNEL_DS, \
 	.fpexc_mode = 0, \
 	.fscr = FSCR_TAR | FSCR_EBB \

commit c420644c0a8f8839ca7269acbb8a3fc7fe1ec97d
Author: Haren Myneni <haren@linux.ibm.com>
Date:   Wed Apr 15 23:08:11 2020 -0700

    powerpc: Use mm_context vas_windows counter to issue CP_ABORT
    
    set_thread_uses_vas() sets used_vas flag for a process that opened VAS
    window and issue CP_ABORT during context switch for only that process.
    In multi-thread application, windows can be shared. For example Thread
    A can open a window and Thread B can run COPY/PASTE instructions to
    send NX request which may cause corruption or snooping or a covert
    channel Also once this flag is set, continue to run CP_ABORT even the
    VAS window is closed.
    
    So define vas-windows counter in process mm_context, increment this
    counter for each window open and decrement it for window close. If
    vas-windows is set, issue CP_ABORT during context switch. It means
    clear the foreign real address mapping only if the process / thread
    uses COPY/PASTE. Then disable it for that process if windows are not
    open.
    
    Moved set_thread_uses_vas() code to vas_tx_win_open() as this
    functionality is needed only for userspace open windows. We are adding
    VAS userspace support along with this fix. So no need to include this
    fix in stable releases.
    
    Fixes: 9d2a4d71332c ("powerpc: Define set_thread_uses_vas()")
    Signed-off-by: Haren Myneni <haren@linux.ibm.com>
    Reported-by: Nicholas Piggin <npiggin@gmail.com>
    Suggested-by: Milton Miller <miltonm@us.ibm.com>
    Suggested-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/1587017291.2275.1077.camel@hbabu-laptop

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index eedcbfb9a6ff..bfa336fbcfeb 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -272,7 +272,6 @@ struct thread_struct {
 	unsigned 	mmcr0;
 
 	unsigned 	used_ebb;
-	unsigned int	used_vas;
 #endif
 };
 

commit 232ca1eecafed8c54491017f0612c33d8c742d74
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Feb 15 10:14:25 2020 +0000

    powerpc/32s: Fix DSI and ISI exceptions for CONFIG_VMAP_STACK
    
    hash_page() needs to read page tables from kernel memory. When entire
    kernel memory is mapped by BATs, which is normally the case when
    CONFIG_STRICT_KERNEL_RWX is not set, it works even if the page hosting
    the page table is not referenced in the MMU hash table.
    
    However, if the page where the page table resides is not covered by
    a BAT, a DSI fault can be encountered from hash_page(), and it loops
    forever. This can happen when CONFIG_STRICT_KERNEL_RWX is selected
    and the alignment of the different regions is too small to allow
    covering the entire memory with BATs. This also happens when
    CONFIG_DEBUG_PAGEALLOC is selected or when booting with 'nobats'
    flag.
    
    Also, if the page containing the kernel stack is not present in the
    MMU hash table, registers cannot be saved and a recursive DSI fault
    is encountered.
    
    To allow hash_page() to properly do its job at all time and load the
    MMU hash table whenever needed, it must run with data MMU disabled.
    This means it must be called before re-enabling data MMU. To allow
    this, registers clobbered by hash_page() and create_hpte() have to
    be saved in the thread struct together with SRR0, SSR1, DAR and DSISR.
    It is also necessary to ensure that DSI prolog doesn't overwrite
    regs saved by prolog of the current running exception. That means:
    - DSI can only use SPRN_SPRG_SCRATCH0
    - Exceptions must free SPRN_SPRG_SCRATCH0 before writing to the stack.
    
    This also fixes the Oops reported by Erhard when create_hpte() is
    called by add_hash_page().
    
    Due to prolog size increase, a few more exceptions had to get split
    in two parts.
    
    Fixes: cd08f109e262 ("powerpc/32s: Enable CONFIG_VMAP_STACK")
    Reported-by: Erhard F. <erhard_f@mailbox.org>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Tested-by: Erhard F. <erhard_f@mailbox.org>
    Tested-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=206501
    Link: https://lore.kernel.org/r/64a4aa44686e9fd4b01333401367029771d9b231.1581761633.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 8387698bd5b6..eedcbfb9a6ff 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -168,6 +168,10 @@ struct thread_struct {
 	unsigned long	srr1;
 	unsigned long	dar;
 	unsigned long	dsisr;
+#ifdef CONFIG_PPC_BOOK3S_32
+	unsigned long	r0, r3, r4, r5, r6, r8, r9, r11;
+	unsigned long	lr, ctr;
+#endif
 #endif
 	/* Debug Registers */
 	struct debug_reg debug;

commit 028474876f472c3b6eee633aed528a1206609657
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Dec 21 08:32:27 2019 +0000

    powerpc/32: prepare for CONFIG_VMAP_STACK
    
    To support CONFIG_VMAP_STACK, the kernel has to activate Data MMU
    Translation for accessing the stack. Before doing that it must save
    SRR0, SRR1 and also DAR and DSISR when relevant, in order to not
    loose them in case there is a Data TLB Miss once the translation is
    reactivated.
    
    This patch adds fields in thread struct for saving those registers.
    It prepares entry_32.S to handle exception entry with
    Data MMU Translation enabled and alters EXCEPTION_PROLOG macros to
    save SRR0, SRR1, DAR and DSISR then reenables Data MMU.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/a775a1fea60f190e0f63503463fb775310a2009b.1576916812.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index e114eb36721c..8387698bd5b6 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -162,6 +162,12 @@ struct thread_struct {
 #endif
 #if defined(CONFIG_PPC_BOOK3S_32) && defined(CONFIG_PPC_KUAP)
 	unsigned long	kuap;		/* opened segments for user access */
+#endif
+#ifdef CONFIG_VMAP_STACK
+	unsigned long	srr0;
+	unsigned long	srr1;
+	unsigned long	dar;
+	unsigned long	dsisr;
 #endif
 	/* Debug Registers */
 	struct debug_reg debug;

commit ed0bc98f8cbe4f8254759d333a47aedc816ff8c5
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Jul 11 12:24:03 2019 +1000

    powerpc/64s: Reimplement power4_idle code in C
    
    This implements the tricky tracing and soft irq handling bits in C,
    leaving the low level bit to asm.
    
    A functional difference is that this redirects the interrupt exit to
    a return stub to execute blr, rather than the lr address itself. This
    is probably barely measurable on real hardware, but it keeps the link
    stack balanced.
    
    Tested with QEMU.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Move power4_fixup_nap back into exceptions-64s.S]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190711022404.18132-1-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index a9993e7a443b..e114eb36721c 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -412,6 +412,9 @@ static inline unsigned long get_clean_sp(unsigned long sp, int is_32)
 extern unsigned long isa300_idle_stop_noloss(unsigned long psscr_val);
 extern unsigned long isa300_idle_stop_mayloss(unsigned long psscr_val);
 extern unsigned long isa206_idle_insn_mayloss(unsigned long type);
+#ifdef CONFIG_PPC_970_NAP
+extern void power4_idle_nap(void);
+#endif
 
 extern unsigned long cpuidle_disable;
 enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};

commit 7928260539f3a13b5b23a3fa0a7c0e4f5255940b
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Sat Jun 8 11:39:05 2019 +0200

    processor: remove spin_cpu_yield
    
    spin_cpu_yield is unused, therefore remove it.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index ef573fe9873e..a9993e7a443b 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -346,8 +346,6 @@ static inline unsigned long __pack_fe01(unsigned int fpmode)
 
 #define spin_cpu_relax()	barrier()
 
-#define spin_cpu_yield()	spin_cpu_relax()
-
 #define spin_end()	HMT_medium()
 
 #define spin_until_cond(cond)					\

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 706ac5df546f..ef573fe9873e 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -1,13 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 #ifndef _ASM_POWERPC_PROCESSOR_H
 #define _ASM_POWERPC_PROCESSOR_H
 
 /*
  * Copyright (C) 2001 PPC 64 Team, IBM Corp
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #include <asm/reg.h>

commit bdc7c970bcdce1c018957e0158230bc025682ba2
Merge: 7ae3f6e130e8 e9cef0189c5b
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Apr 30 22:52:03 2019 +1000

    Merge branch 'topic/ppc-kvm' into next
    
    Merge our topic branch shared with KVM. In particular this includes the
    rewrite of the idle code into C.

commit 10d91611f426d4bafd2a83d966c36da811b2f7ad
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Apr 13 00:30:52 2019 +1000

    powerpc/64s: Reimplement book3s idle code in C
    
    Reimplement Book3S idle code in C, moving POWER7/8/9 implementation
    speific HV idle code to the powernv platform code.
    
    Book3S assembly stubs are kept in common code and used only to save
    the stack frame and non-volatile GPRs before executing architected
    idle instructions, and restoring the stack and reloading GPRs then
    returning to C after waking from idle.
    
    The complex logic dealing with threads and subcores, locking, SPRs,
    HMIs, timebase resync, etc., is all done in C which makes it more
    maintainable.
    
    This is not a strict translation to C code, there are some
    significant differences:
    
    - Idle wakeup no longer uses the ->cpu_restore call to reinit SPRs,
      but saves and restores them itself.
    
    - The optimisation where EC=ESL=0 idle modes did not have to save GPRs
      or change MSR is restored, because it's now simple to do. ESL=1
      sleeps that do not lose GPRs can use this optimization too.
    
    - KVM secondary entry and cede is now more of a call/return style
      rather than branchy. nap_state_lost is not required because KVM
      always returns via NVGPR restoring path.
    
    - KVM secondary wakeup from offline sequence is moved entirely into
      the offline wakeup, which avoids a hwsync in the normal idle wakeup
      path.
    
    Performance measured with context switch ping-pong on different
    threads or cores, is possibly improved a small amount, 1-3% depending
    on stop state and core vs thread test for shallow states. Deep states
    it's in the noise compared with other latencies.
    
    KVM improvements:
    
    - Idle sleepers now always return to caller rather than branch out
      to KVM first.
    
    - This allows optimisations like very fast return to caller when no
      state has been lost.
    
    - KVM no longer requires nap_state_lost because it controls NVGPR
      save/restore itself on the way in and out.
    
    - The heavy idle wakeup KVM request check can be moved out of the
      normal host idle code and into the not-performance-critical offline
      code.
    
    - KVM nap code now returns from where it is called, which makes the
      flow a bit easier to follow.
    
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Squash the KVM changes in]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 3351bcf42f2d..3120cca72e1f 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -411,14 +411,17 @@ static inline unsigned long get_clean_sp(unsigned long sp, int is_32)
 }
 #endif
 
+/* asm stubs */
+extern unsigned long isa300_idle_stop_noloss(unsigned long psscr_val);
+extern unsigned long isa300_idle_stop_mayloss(unsigned long psscr_val);
+extern unsigned long isa206_idle_insn_mayloss(unsigned long type);
+
 extern unsigned long cpuidle_disable;
 enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 
 extern int powersave_nap;	/* set if nap mode can be used in idle loop */
-extern unsigned long power7_idle_insn(unsigned long type); /* PNV_THREAD_NAP/etc*/
+
 extern void power7_idle_type(unsigned long type);
-extern unsigned long power9_idle_stop(unsigned long psscr_val);
-extern unsigned long power9_offline_stop(unsigned long psscr_val);
 extern void power9_idle_type(unsigned long stop_psscr_val,
 			      unsigned long stop_psscr_mask);
 

commit a68c31fc01ef7863acc0fc74694bf279456a58c4
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Mar 11 08:30:38 2019 +0000

    powerpc/32s: Implement Kernel Userspace Access Protection
    
    This patch implements Kernel Userspace Access Protection for
    book3s/32.
    
    Due to limitations of the processor page protection capabilities,
    the protection is only against writing. read protection cannot be
    achieved using page protection.
    
    The previous patch modifies the page protection so that RW user
    pages are RW for Key 0 and RO for Key 1, and it sets Key 0 for
    both user and kernel.
    
    This patch changes userspace segment registers are set to Ku 0
    and Ks 1. When kernel needs to write to RW pages, the associated
    segment register is then changed to Ks 0 in order to allow write
    access to the kernel.
    
    In order to avoid having the read all segment registers when
    locking/unlocking the access, some data is kept in the thread_struct
    and saved on stack on exceptions. The field identifies both the
    first unlocked segment and the first segment following the last
    unlocked one. When no segment is unlocked, it contains value 0.
    
    As the hash_page() function is not able to easily determine if a
    protfault is due to a bad kernel access to userspace, protfaults
    need to be handled by handle_page_fault when KUAP is set.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Drop allow_read/write_to/from_user() as they're now in kup.h,
          and adapt allow_user_access() to do nothing when to == NULL]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 3351bcf42f2d..540949b397d4 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -163,6 +163,9 @@ struct thread_struct {
 #ifdef CONFIG_PPC_RTAS
 	unsigned long	rtas_sp;	/* stack pointer for when in RTAS */
 #endif
+#endif
+#if defined(CONFIG_PPC_BOOK3S_32) && defined(CONFIG_PPC_KUAP)
+	unsigned long	kuap;		/* opened segments for user access */
 #endif
 	/* Debug Registers */
 	struct debug_reg debug;

commit a7916a1de526162d73e894b6d3ebd895d4302078
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:09:00 2019 +0000

    powerpc: regain entire stack space
    
    thread_info is not anymore in the stack, so the entire stack
    can now be used.
    
    There is also no risk anymore of corrupting task_cpu(p) with a
    stack overflow so the patch removes the test.
    
    When doing this, an explicit test for NULL stack pointer is
    needed in validate_sp() as it is not anymore implicitely covered
    by the sizeof(thread_info) gap.
    
    In the meantime, with the previous patch all pointers to the stacks
    are not anymore pointers to thread_info so this patch changes them
    to void*
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 2c740042b8d3..3351bcf42f2d 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -270,8 +270,7 @@ struct thread_struct {
 #define ARCH_MIN_TASKALIGN 16
 
 #define INIT_SP		(sizeof(init_stack) + (unsigned long) &init_stack)
-#define INIT_SP_LIMIT \
-	(_ALIGN_UP(sizeof(struct thread_info), 16) + (unsigned long)&init_stack)
+#define INIT_SP_LIMIT	((unsigned long)&init_stack)
 
 #ifdef CONFIG_SPE
 #define SPEFSCR_INIT \

commit 3733304048feb9bdfc3daff02ca4da8cfc9c4352
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 17 23:27:28 2019 +1100

    powerpc: Use linux/thread_info.h in processor.h
    
    When we enable THREAD_INFO_IN_TASK we will remove our definition of
    current_thread_info(). Instead it will come from linux/thread_info.h
    
    So switch processor.h to include the latter, so that it can continue
    to find current_thread_info().
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Split out of larger patch]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index c406ec3b4b3c..2c740042b8d3 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -40,7 +40,7 @@
 
 #ifndef __ASSEMBLY__
 #include <linux/types.h>
-#include <asm/thread_info.h>
+#include <linux/thread_info.h>
 #include <asm/ptrace.h>
 #include <asm/hw_breakpoint.h>
 

commit 5497c2536f09e733bb68362ffeba147203295ae2
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 17 23:27:40 2019 +1100

    powerpc: Use sizeof(struct thread_info) in INIT_SP_LIMIT
    
    Currently INIT_SP_LIMIT uses sizeof(init_thread_info), but that symbol
    won't exist when we enable THREAD_INFO_IN_TASK. So just use the sizeof
    the type which is the same value but will continue to work.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Split out of larger patch]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 2edab34ee288..c406ec3b4b3c 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -271,7 +271,7 @@ struct thread_struct {
 
 #define INIT_SP		(sizeof(init_stack) + (unsigned long) &init_stack)
 #define INIT_SP_LIMIT \
-	(_ALIGN_UP(sizeof(init_thread_info), 16) + (unsigned long) &init_stack)
+	(_ALIGN_UP(sizeof(struct thread_info), 16) + (unsigned long)&init_stack)
 
 #ifdef CONFIG_SPE
 #define SPEFSCR_INIT \

commit 92ab45c5f2db0caa68243be8cfa5e390a1de8c3a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:08:48 2019 +0000

    powerpc: Avoid circular header inclusion in mmu-hash.h
    
    When activating CONFIG_THREAD_INFO_IN_TASK, linux/sched.h includes
    asm/current.h. This generates a circular dependency. To avoid that,
    asm/processor.h shall not be included in mmu-hash.h.
    
    In order to do that, this patch moves into a new header called
    asm/task_size_64/32.h all the TASK_SIZE related constants, which can
    then be included in mmu-hash.h directly.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Split out all the TASK_SIZE constants not just 64-bit ones]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index e8682122ea3d..2edab34ee288 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -77,105 +77,15 @@ extern int _chrp_type;
 
 #ifdef __KERNEL__
 
-struct task_struct;
-void start_thread(struct pt_regs *regs, unsigned long fdptr, unsigned long sp);
-void release_thread(struct task_struct *);
-
-#ifdef CONFIG_PPC32
-
-#if CONFIG_TASK_SIZE > CONFIG_KERNEL_START
-#error User TASK_SIZE overlaps with KERNEL_START address
-#endif
-#define TASK_SIZE	(CONFIG_TASK_SIZE)
-
-/* This decides where the kernel will search for a free chunk of vm
- * space during mmap's.
- */
-#define TASK_UNMAPPED_BASE	(TASK_SIZE / 8 * 3)
-#endif
-
 #ifdef CONFIG_PPC64
-/*
- * 64-bit user address space can have multiple limits
- * For now supported values are:
- */
-#define TASK_SIZE_64TB  (0x0000400000000000UL)
-#define TASK_SIZE_128TB (0x0000800000000000UL)
-#define TASK_SIZE_512TB (0x0002000000000000UL)
-#define TASK_SIZE_1PB   (0x0004000000000000UL)
-#define TASK_SIZE_2PB   (0x0008000000000000UL)
-/*
- * With 52 bits in the address we can support
- * upto 4PB of range.
- */
-#define TASK_SIZE_4PB   (0x0010000000000000UL)
-
-/*
- * For now 512TB is only supported with book3s and 64K linux page size.
- */
-#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_PPC_64K_PAGES)
-/*
- * Max value currently used:
- */
-#define TASK_SIZE_USER64		TASK_SIZE_4PB
-#define DEFAULT_MAP_WINDOW_USER64	TASK_SIZE_128TB
-#define TASK_CONTEXT_SIZE		TASK_SIZE_512TB
-#else
-#define TASK_SIZE_USER64		TASK_SIZE_64TB
-#define DEFAULT_MAP_WINDOW_USER64	TASK_SIZE_64TB
-/*
- * We don't need to allocate extended context ids for 4K page size, because
- * we limit the max effective address on this config to 64TB.
- */
-#define TASK_CONTEXT_SIZE		TASK_SIZE_64TB
-#endif
-
-/*
- * 32-bit user address space is 4GB - 1 page
- * (this 1 page is needed so referencing of 0xFFFFFFFF generates EFAULT
- */
-#define TASK_SIZE_USER32 (0x0000000100000000UL - (1*PAGE_SIZE))
-
-#define TASK_SIZE_OF(tsk) (test_tsk_thread_flag(tsk, TIF_32BIT) ? \
-		TASK_SIZE_USER32 : TASK_SIZE_USER64)
-#define TASK_SIZE	  TASK_SIZE_OF(current)
-/* This decides where the kernel will search for a free chunk of vm
- * space during mmap's.
- */
-#define TASK_UNMAPPED_BASE_USER32 (PAGE_ALIGN(TASK_SIZE_USER32 / 4))
-#define TASK_UNMAPPED_BASE_USER64 (PAGE_ALIGN(DEFAULT_MAP_WINDOW_USER64 / 4))
-
-#define TASK_UNMAPPED_BASE ((is_32bit_task()) ? \
-		TASK_UNMAPPED_BASE_USER32 : TASK_UNMAPPED_BASE_USER64 )
-#endif
-
-/*
- * Initial task size value for user applications. For book3s 64 we start
- * with 128TB and conditionally enable upto 512TB
- */
-#ifdef CONFIG_PPC_BOOK3S_64
-#define DEFAULT_MAP_WINDOW	((is_32bit_task()) ?			\
-				 TASK_SIZE_USER32 : DEFAULT_MAP_WINDOW_USER64)
+#include <asm/task_size_64.h>
 #else
-#define DEFAULT_MAP_WINDOW	TASK_SIZE
+#include <asm/task_size_32.h>
 #endif
 
-#ifdef __powerpc64__
-
-#define STACK_TOP_USER64 DEFAULT_MAP_WINDOW_USER64
-#define STACK_TOP_USER32 TASK_SIZE_USER32
-
-#define STACK_TOP (is_32bit_task() ? \
-		   STACK_TOP_USER32 : STACK_TOP_USER64)
-
-#define STACK_TOP_MAX TASK_SIZE_USER64
-
-#else /* __powerpc64__ */
-
-#define STACK_TOP TASK_SIZE
-#define STACK_TOP_MAX	STACK_TOP
-
-#endif /* __powerpc64__ */
+struct task_struct;
+void start_thread(struct pt_regs *regs, unsigned long fdptr, unsigned long sp);
+void release_thread(struct task_struct *);
 
 typedef struct {
 	unsigned long seg;

commit 0df977eafc792a5365a7f81d8d5920132e03afad
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Feb 21 10:37:54 2019 +0000

    powerpc/6xx: Don't use SPRN_SPRG2 for storing stack pointer while in RTAS
    
    When calling RTAS, the stack pointer is stored in SPRN_SPRG2
    in order to be able to restore it in case of machine check in RTAS.
    
    As machine check is not a perfomance critical path, this patch
    frees SPRN_SPRG2 by using a field in thread struct instead.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index ee58526cb6c2..e8682122ea3d 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -250,6 +250,9 @@ struct thread_struct {
 #ifdef CONFIG_PPC32
 	void		*pgdir;		/* root of page-table tree */
 	unsigned long	ksp_limit;	/* if ksp <= ksp_limit stack overflow */
+#ifdef CONFIG_PPC_RTAS
+	unsigned long	rtas_sp;	/* stack pointer for when in RTAS */
+#endif
 #endif
 	/* Debug Registers */
 	struct debug_reg debug;

commit de0d22e50cd3d57277f073ccf65d57aa519d6888
Author: Nick Desaulniers <ndesaulniers@google.com>
Date:   Tue Oct 30 15:04:47 2018 -0700

    treewide: remove current_text_addr
    
    Prefer _THIS_IP_ defined in linux/kernel.h.
    
    Most definitions of current_text_addr were the same as _THIS_IP_, but
    a few archs had inline assembly instead.
    
    This patch removes the final call site of current_text_addr, making all
    of the definitions dead code.
    
    [akpm@linux-foundation.org: fix arch/csky/include/asm/processor.h]
    Link: http://lkml.kernel.org/r/20180911182413.180715-1-ndesaulniers@google.com
    Signed-off-by: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 7d04d60a39c9..ee58526cb6c2 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -67,12 +67,6 @@ extern int _chrp_type;
 
 #endif /* defined(__KERNEL__) && defined(CONFIG_PPC32) */
 
-/*
- * Default implementation of macro that returns current
- * instruction pointer ("program counter").
- */
-#define current_text_addr() ({ __label__ _l; _l: &&_l;})
-
 /* Macros for adjusting thread priority (hardware multi-threading) */
 #define HMT_very_low()   asm volatile("or 31,31,31   # very low priority")
 #define HMT_low()	 asm volatile("or 1,1,1	     # low priority")

commit 5434ae74629af58ad0fc27143a9ea435f7734410
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:56 2018 +1000

    powerpc/64s/hash: Add a SLB preload cache
    
    When switching processes, currently all user SLBEs are cleared, and a
    few (exec_base, pc, and stack) are preloaded. In trivial testing with
    small apps, this tends to miss the heap and low 256MB segments, and it
    will also miss commonly accessed segments on large memory workloads.
    
    Add a simple round-robin preload cache that just inserts the last SLB
    miss into the head of the cache and preloads those at context switch
    time. Every 256 context switches, the oldest entry is removed from the
    cache to shrink the cache and require fewer slbmte if they are unused.
    
    Much more could go into this, including into the SLB entry reclaim
    side to track some LRU information etc, which would require a study of
    large memory workloads. But this is a simple thing we can do now that
    is an obvious win for common workloads.
    
    With the full series, process switching speed on the context_switch
    benchmark on POWER9/hash (with kernel speculation security masures
    disabled) increases from 140K/s to 178K/s (27%).
    
    POWER8 does not change much (within 1%), it's unclear why it does not
    see a big gain like POWER9.
    
    Booting to busybox init with 256MB segments has SLB misses go down
    from 945 to 69, and with 1T segments 900 to 21. These could almost all
    be eliminated by preloading a bit more carefully with ELF binary
    loading.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 3fefb8a65b17..7d04d60a39c9 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -273,6 +273,7 @@ struct thread_struct {
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 	struct arch_hw_breakpoint hw_brk; /* info on the hardware breakpoint */
 	unsigned long	trap_nr;	/* last trap # on this thread */
+	u8 load_slb;			/* Ages out SLB preload cache entries */
 	u8 load_fp;
 #ifdef CONFIG_ALTIVEC
 	u8 load_vec;

commit 4c2de74cc8696154b283f241d74ec0bb24438e22
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Oct 13 00:15:16 2018 +1100

    powerpc/64: Interrupts save PPR on stack rather than thread_struct
    
    PPR is the odd register out when it comes to interrupt handling, it is
    saved in current->thread.ppr while all others are saved on the stack.
    
    The difficulty with this is that accessing thread.ppr can cause a SLB
    fault, but the SLB fault handler implementation in C change had
    assumed the normal exception entry handlers would not cause an SLB
    fault.
    
    Fix this by allocating room in the interrupt stack to save PPR.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 52fadded5c1e..3fefb8a65b17 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -32,9 +32,9 @@
 /* Default SMT priority is set to 3. Use 11- 13bits to save priority. */
 #define PPR_PRIORITY 3
 #ifdef __ASSEMBLY__
-#define INIT_PPR (PPR_PRIORITY << 50)
+#define DEFAULT_PPR (PPR_PRIORITY << 50)
 #else
-#define INIT_PPR ((u64)PPR_PRIORITY << 50)
+#define DEFAULT_PPR ((u64)PPR_PRIORITY << 50)
 #endif /* __ASSEMBLY__ */
 #endif /* CONFIG_PPC64 */
 
@@ -341,7 +341,6 @@ struct thread_struct {
 	 * onwards.
 	 */
 	int		dscr_inherit;
-	unsigned long	ppr;	/* used to save/restore SMT priority */
 	unsigned long	tidr;
 #endif
 #ifdef CONFIG_PPC_BOOK3S_64
@@ -389,7 +388,6 @@ struct thread_struct {
 	.regs = (struct pt_regs *)INIT_SP - 1, /* XXX bogus, I think */ \
 	.addr_limit = KERNEL_DS, \
 	.fpexc_mode = 0, \
-	.ppr = INIT_PPR, \
 	.fscr = FSCR_TAR | FSCR_EBB \
 }
 #endif

commit 54be0b9c7c9888ebe63b89a31a17ee3df6a68d61
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Oct 2 23:56:39 2018 +1000

    Revert "convert SLB miss handlers to C" and subsequent commits
    
    This reverts commits:
      5e46e29e6a97 ("powerpc/64s/hash: convert SLB miss handlers to C")
      8fed04d0f6ae ("powerpc/64s/hash: remove user SLB data from the paca")
      655deecf67b2 ("powerpc/64s/hash: SLB allocation status bitmaps")
      2e1626744e8d ("powerpc/64s/hash: provide arch_setup_exec hooks for hash slice setup")
      89ca4e126a3f ("powerpc/64s/hash: Add a SLB preload cache")
    
    This series had a few bugs, and the fixes are not all trivial. So
    revert most of it for now.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 350c584ca179..52fadded5c1e 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -273,7 +273,6 @@ struct thread_struct {
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 	struct arch_hw_breakpoint hw_brk; /* info on the hardware breakpoint */
 	unsigned long	trap_nr;	/* last trap # on this thread */
-	u8 load_slb;			/* Ages out SLB preload cache entries */
 	u8 load_fp;
 #ifdef CONFIG_ALTIVEC
 	u8 load_vec;

commit 89ca4e126a3f519ccbd42670b38d78700802c10b
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:56 2018 +1000

    powerpc/64s/hash: Add a SLB preload cache
    
    When switching processes, currently all user SLBEs are cleared, and a
    few (exec_base, pc, and stack) are preloaded. In trivial testing with
    small apps, this tends to miss the heap and low 256MB segments, and it
    will also miss commonly accessed segments on large memory workloads.
    
    Add a simple round-robin preload cache that just inserts the last SLB
    miss into the head of the cache and preloads those at context switch
    time. Every 256 context switches, the oldest entry is removed from the
    cache to shrink the cache and require fewer slbmte if they are unused.
    
    Much more could go into this, including into the SLB entry reclaim
    side to track some LRU information etc, which would require a study of
    large memory workloads. But this is a simple thing we can do now that
    is an obvious win for common workloads.
    
    With the full series, process switching speed on the context_switch
    benchmark on POWER9/hash (with kernel speculation security masures
    disabled) increases from 140K/s to 178K/s (27%).
    
    POWER8 does not change much (within 1%), it's unclear why it does not
    see a big gain like POWER9.
    
    Booting to busybox init with 256MB segments has SLB misses go down
    from 945 to 69, and with 1T segments 900 to 21. These could almost all
    be eliminated by preloading a bit more carefully with ELF binary
    loading.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 52fadded5c1e..350c584ca179 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -273,6 +273,7 @@ struct thread_struct {
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 	struct arch_hw_breakpoint hw_brk; /* info on the hardware breakpoint */
 	unsigned long	trap_nr;	/* last trap # on this thread */
+	u8 load_slb;			/* Ages out SLB preload cache entries */
 	u8 load_fp;
 #ifdef CONFIG_ALTIVEC
 	u8 load_vec;

commit 62b8426578c414c918468ab4cc7517da7adc31d5
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jul 5 16:25:09 2018 +0000

    powerpc: fix includes in asm/processor.h
    
    Remove superflous includes and add missing ones
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 5debe337ea9d..52fadded5c1e 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -39,10 +39,9 @@
 #endif /* CONFIG_PPC64 */
 
 #ifndef __ASSEMBLY__
-#include <linux/compiler.h>
-#include <linux/cache.h>
+#include <linux/types.h>
+#include <asm/thread_info.h>
 #include <asm/ptrace.h>
-#include <asm/types.h>
 #include <asm/hw_breakpoint.h>
 
 /* We do _not_ want to define new machine types at all, those must die

commit ba0635fcbe8c1ce83523c1ec79753868ce57f7a8
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon May 14 23:03:15 2018 +1000

    powerpc: Rename thread_struct.fs to addr_limit
    
    It's called 'fs' for historical reasons, it's named after the x86 'FS'
    register. But we don't have to use that name for the member of
    thread_struct, and in fact arch/x86 doesn't even call it 'fs' anymore.
    
    So rename it to 'addr_limit', which better reflects what it's used
    for, and is also the name used on other arches.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index eff269adfa71..5debe337ea9d 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -249,7 +249,7 @@ struct thread_struct {
 	unsigned long	ksp_vsid;
 #endif
 	struct pt_regs	*regs;		/* Pointer to saved register state */
-	mm_segment_t	fs;		/* for get_fs() validation */
+	mm_segment_t	addr_limit;	/* for get_fs() validation */
 #ifdef CONFIG_BOOKE
 	/* BookE base exception scratch space; align on cacheline */
 	unsigned long	normsave[8] ____cacheline_aligned;
@@ -379,7 +379,7 @@ struct thread_struct {
 #define INIT_THREAD { \
 	.ksp = INIT_SP, \
 	.ksp_limit = INIT_SP_LIMIT, \
-	.fs = KERNEL_DS, \
+	.addr_limit = KERNEL_DS, \
 	.pgdir = swapper_pg_dir, \
 	.fpexc_mode = MSR_FE0 | MSR_FE1, \
 	SPEFSCR_INIT \
@@ -388,7 +388,7 @@ struct thread_struct {
 #define INIT_THREAD  { \
 	.ksp = INIT_SP, \
 	.regs = (struct pt_regs *)INIT_SP - 1, /* XXX bogus, I think */ \
-	.fs = KERNEL_DS, \
+	.addr_limit = KERNEL_DS, \
 	.fpexc_mode = 0, \
 	.ppr = INIT_PPR, \
 	.fscr = FSCR_TAR | FSCR_EBB \

commit 36d632ea831fd2fa3cb62599a465825f59076f64
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:29 2018 +1000

    powerpc/64: remove start_tb and accum_tb from thread_struct
    
    These fields are only written to.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index c4b36a494a63..eff269adfa71 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -264,10 +264,6 @@ struct thread_struct {
 	struct thread_fp_state	*fp_save_area;
 	int		fpexc_mode;	/* floating-point exception mode */
 	unsigned int	align_ctl;	/* alignment handling control */
-#ifdef CONFIG_PPC64
-	unsigned long	start_tb;	/* Start purr when proc switched in */
-	unsigned long	accum_tb;	/* Total accumulated purr for process */
-#endif
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	struct perf_event *ptrace_bps[HBP_NUM];
 	/*

commit 3d4fbffdd703d2b968db443911f2147c732a4a48
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 18 00:08:05 2017 +1000

    powerpc/64s/idle: POWER9 implement a separate idle stop function for hotplug
    
    Implement a new function to invoke stop, power9_offline_stop, which is
    like power9_idle_stop but used by the cpu hotplug code.
    
    Move KVM secondary state manipulation code to the offline case.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index bb9cb25ffb20..c4b36a494a63 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -518,6 +518,7 @@ extern int powersave_nap;	/* set if nap mode can be used in idle loop */
 extern unsigned long power7_idle_insn(unsigned long type); /* PNV_THREAD_NAP/etc*/
 extern void power7_idle_type(unsigned long type);
 extern unsigned long power9_idle_stop(unsigned long psscr_val);
+extern unsigned long power9_offline_stop(unsigned long psscr_val);
 extern void power9_idle_type(unsigned long stop_psscr_val,
 			      unsigned long stop_psscr_mask);
 

commit c2b4d8b7417a59b7f9a52d0d8402f5257cbbd398
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Mar 26 15:34:49 2018 +0530

    powerpc/mm/hash64: Increase the VA range
    
    This patch increases the max virtual (effective) address value to 4PB.
    With 4K page size config we continue to limit ourself to 64TB.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [mpe: Keep the H_PGTABLE_RANGE test, update it to work]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 75b084486ce1..bb9cb25ffb20 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -109,6 +109,13 @@ void release_thread(struct task_struct *);
 #define TASK_SIZE_64TB  (0x0000400000000000UL)
 #define TASK_SIZE_128TB (0x0000800000000000UL)
 #define TASK_SIZE_512TB (0x0002000000000000UL)
+#define TASK_SIZE_1PB   (0x0004000000000000UL)
+#define TASK_SIZE_2PB   (0x0008000000000000UL)
+/*
+ * With 52 bits in the address we can support
+ * upto 4PB of range.
+ */
+#define TASK_SIZE_4PB   (0x0010000000000000UL)
 
 /*
  * For now 512TB is only supported with book3s and 64K linux page size.
@@ -117,7 +124,7 @@ void release_thread(struct task_struct *);
 /*
  * Max value currently used:
  */
-#define TASK_SIZE_USER64		TASK_SIZE_512TB
+#define TASK_SIZE_USER64		TASK_SIZE_4PB
 #define DEFAULT_MAP_WINDOW_USER64	TASK_SIZE_128TB
 #define TASK_CONTEXT_SIZE		TASK_SIZE_512TB
 #else

commit f384796c40dc55b3dba25e0ee9c1afd98c6d24d1
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Mar 26 15:34:48 2018 +0530

    powerpc/mm: Add support for handling > 512TB address in SLB miss
    
    For addresses above 512TB we allocate additional mmu contexts. To make
    it all easy, addresses above 512TB are handled with IR/DR=1 and with
    stack frame setup.
    
    The mmu_context_t is also updated to track the new extended_ids. To
    support upto 4PB we need a total 8 contexts.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [mpe: Minor formatting tweaks and comment wording, switch BUG to WARN
          in get_ea_context().]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 01299cdc9806..75b084486ce1 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -119,9 +119,15 @@ void release_thread(struct task_struct *);
  */
 #define TASK_SIZE_USER64		TASK_SIZE_512TB
 #define DEFAULT_MAP_WINDOW_USER64	TASK_SIZE_128TB
+#define TASK_CONTEXT_SIZE		TASK_SIZE_512TB
 #else
 #define TASK_SIZE_USER64		TASK_SIZE_64TB
 #define DEFAULT_MAP_WINDOW_USER64	TASK_SIZE_64TB
+/*
+ * We don't need to allocate extended context ids for 4K page size, because
+ * we limit the max effective address on this config to 64TB.
+ */
+#define TASK_CONTEXT_SIZE		TASK_SIZE_64TB
 #endif
 
 /*

commit 06bb53b33804613627c7ca1eda246459a7be2803
Author: Ram Pai <linuxram@us.ibm.com>
Date:   Thu Jan 18 17:50:31 2018 -0800

    powerpc: store and restore the pkey state across context switches
    
    Store and restore the AMR, IAMR and UAMOR register state of the task
    before scheduling out and after scheduling in, respectively.
    
    Signed-off-by: Ram Pai <linuxram@us.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index bdab3b74eb98..01299cdc9806 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -309,6 +309,11 @@ struct thread_struct {
 	struct thread_vr_state ckvr_state; /* Checkpointed VR state */
 	unsigned long	ckvrsave; /* Checkpointed VRSAVE */
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
+#ifdef CONFIG_PPC_MEM_KEYS
+	unsigned long	amr;
+	unsigned long	iamr;
+	unsigned long	uamor;
+#endif
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
 	void*		kvm_shadow_vcpu; /* KVM internal data */
 #endif /* CONFIG_KVM_BOOK3S_32_HANDLER */

commit 9d2a4d71332cfdf4ea90754ad9b2f05a5ee5f6c7
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Tue Nov 7 18:23:54 2017 -0800

    powerpc: Define set_thread_uses_vas()
    
    A CP_ABORT instruction is required in processes that have mapped a VAS
    "paste address" with the intention of using COPY/PASTE instructions.
    But since CP_ABORT is expensive, we want to restrict it to only
    processes that use/intend to use COPY/PASTE.
    
    Define an interface, set_thread_uses_vas(), that VAS can use to
    indicate that the current process opened a send window. During context
    switch, issue CP_ABORT only for processes that have the flag set.
    
    Thanks for input from Nick Piggin, Michael Ellerman.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    [mpe: Fix to not use new_thread after _switch() returns]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 58cc21274423..bdab3b74eb98 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -341,7 +341,9 @@ struct thread_struct {
 	unsigned long	sier;
 	unsigned long	mmcr2;
 	unsigned 	mmcr0;
+
 	unsigned 	used_ebb;
+	unsigned int	used_vas;
 #endif
 };
 

commit ec233ede4c8654894610ea54f4dae7adc954ac62
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Tue Nov 7 18:23:53 2017 -0800

    powerpc: Add support for setting SPRN_TIDR
    
    We need the SPRN_TIDR to be set for use with fast thread-wakeup (core-
    to-core wakeup) and also with CAPI.
    
    Each thread in a process needs to have a unique id within the process.
    But for now, we assign globally unique thread ids to all threads in
    the system.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Philippe Bergheaud <felix@linux.vnet.ibm.com>
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    [mpe: Simplify tidr clearing on fork() and ctx switch code]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index fab7ff877304..58cc21274423 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -329,6 +329,7 @@ struct thread_struct {
 	 */
 	int		dscr_inherit;
 	unsigned long	ppr;	/* used to save/restore SMT priority */
+	unsigned long	tidr;
 #endif
 #ifdef CONFIG_PPC_BOOK3S_64
 	unsigned long	tar;

commit d691b7e7d1b5186eae62fd32adee65d3316bfdf6
Merge: b59eea554f57 1e0fc9d1eb2b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 7 13:55:45 2017 -0700

    Merge tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for STRICT_KERNEL_RWX on 64-bit server CPUs.
    
       - Platform support for FSP2 (476fpe) board
    
       - Enable ZONE_DEVICE on 64-bit server CPUs.
    
       - Generic & powerpc spin loop primitives to optimise busy waiting
    
       - Convert VDSO update function to use new update_vsyscall() interface
    
       - Optimisations to hypercall/syscall/context-switch paths
    
       - Improvements to the CPU idle code on Power8 and Power9.
    
      As well as many other fixes and improvements.
    
      Thanks to: Akshay Adiga, Andrew Donnellan, Andrew Jeffery, Anshuman
      Khandual, Anton Blanchard, Balbir Singh, Benjamin Herrenschmidt,
      Christophe Leroy, Christophe Lombard, Colin Ian King, Dan Carpenter,
      Gautham R. Shenoy, Hari Bathini, Ian Munsie, Ivan Mikhaylov, Javier
      Martinez Canillas, Madhavan Srinivasan, Masahiro Yamada, Matt Brown,
      Michael Neuling, Michal Suchanek, Murilo Opsfelder Araujo, Naveen N.
      Rao, Nicholas Piggin, Oliver O'Halloran, Paul Mackerras, Pavel Machek,
      Russell Currey, Santosh Sivaraj, Stephen Rothwell, Thiago Jung
      Bauermann, Yang Li"
    
    * tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (158 commits)
      powerpc/Kconfig: Enable STRICT_KERNEL_RWX for some configs
      powerpc/mm/radix: Implement STRICT_RWX/mark_rodata_ro() for Radix
      powerpc/mm/hash: Implement mark_rodata_ro() for hash
      powerpc/vmlinux.lds: Align __init_begin to 16M
      powerpc/lib/code-patching: Use alternate map for patch_instruction()
      powerpc/xmon: Add patch_instruction() support for xmon
      powerpc/kprobes/optprobes: Use patch_instruction()
      powerpc/kprobes: Move kprobes over to patch_instruction()
      powerpc/mm/radix: Fix execute permissions for interrupt_vectors
      powerpc/pseries: Fix passing of pp0 in updatepp() and updateboltedpp()
      powerpc/64s: Blacklist rtas entry/exit from kprobes
      powerpc/64s: Blacklist functions invoked on a trap
      powerpc/64s: Un-blacklist system_call() from kprobes
      powerpc/64s: Move system_call() symbol to just after setting MSR_EE
      powerpc/64s: Blacklist system_call() and system_call_common() from kprobes
      powerpc/64s: Convert .L__replay_interrupt_return to a local label
      powerpc64/elfv1: Only dereference function descriptor for non-text symbols
      cxl: Export library to support IBM XSL
      powerpc/dts: Use #include "..." to include local DT
      powerpc/perf/hv-24x7: Aggregate result elements on POWER9 SMT8
      ...

commit 218ea31039e84901b449c3769035456688f6e17d
Merge: 5405c92bc2cd d6bd8194e286
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Jul 3 23:05:43 2017 +1000

    Merge branch 'fixes' into next
    
    Merge our fixes branch, a few of them are tripping people up while
    working on top of next, and we also have a dependency between the CXL
    fixes and new CXL code we want to merge into next.

commit ede8e2bbb0eb3370e4dc5484b40eb22850a09b92
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Jun 6 23:08:31 2017 +1000

    powerpc/64: implement spin loop primitives
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index c49165a7439c..832775771bd3 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -428,6 +428,26 @@ static inline unsigned long __pack_fe01(unsigned int fpmode)
 
 #ifdef CONFIG_PPC64
 #define cpu_relax()	do { HMT_low(); HMT_medium(); barrier(); } while (0)
+
+#define spin_begin()	HMT_low()
+
+#define spin_cpu_relax()	barrier()
+
+#define spin_cpu_yield()	spin_cpu_relax()
+
+#define spin_end()	HMT_medium()
+
+#define spin_until_cond(cond)					\
+do {								\
+	if (unlikely(!(cond))) {				\
+		spin_begin();					\
+		do {						\
+			spin_cpu_relax();			\
+		} while (!(cond));				\
+		spin_end();					\
+	}							\
+} while (0)
+
 #else
 #define cpu_relax()	barrier()
 #endif

commit 6474924e2b5ddb0030c355558966adcbe3b49022
Author: Tobias Klauser <tklauser@distanz.ch>
Date:   Wed Jun 28 15:30:02 2017 +0200

    arch: remove unused macro/function thread_saved_pc()
    
    The only user of thread_saved_pc() in non-arch-specific code was removed
    in commit 8243d5597793 ("sched/core: Remove pointless printout in
    sched_show_task()").  Remove the implementations as well.
    
    Some architectures use thread_saved_pc() in their arch-specific code.
    Leave their thread_saved_pc() intact.
    
    Signed-off-by: Tobias Klauser <tklauser@distanz.ch>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index bb99b651085a..1189d04f3bd1 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -378,12 +378,6 @@ struct thread_struct {
 }
 #endif
 
-/*
- * Return saved PC of a blocked thread. For now, this is the "user" PC
- */
-#define thread_saved_pc(tsk)    \
-        ((tsk)->thread.regs? (tsk)->thread.regs->nip: 0)
-
 #define task_pt_regs(tsk)	((struct pt_regs *)(tsk)->thread.regs)
 
 unsigned long get_wchan(struct task_struct *p);

commit 2201f994a5742c03e660623c385fd6897dd1fa2f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Jun 13 23:05:45 2017 +1000

    powerpc/64s/idle: Move soft interrupt mask logic into C code
    
    This simplifies the asm and fixes irq-off tracing over sleep
    instructions.
    
    Also move powersave_nap check for POWER8 into C code, and move
    PSSCR register value calculation for POWER9 into C.
    
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index a2123f291ab0..c49165a7439c 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -481,11 +481,11 @@ extern unsigned long cpuidle_disable;
 enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 
 extern int powersave_nap;	/* set if nap mode can be used in idle loop */
-extern unsigned long power7_nap(int check_irq);
-extern unsigned long power7_sleep(void);
-extern unsigned long power7_winkle(void);
-extern unsigned long power9_idle_stop(unsigned long stop_psscr_val,
-				      unsigned long stop_psscr_mask);
+extern unsigned long power7_idle_insn(unsigned long type); /* PNV_THREAD_NAP/etc*/
+extern void power7_idle_type(unsigned long type);
+extern unsigned long power9_idle_stop(unsigned long psscr_val);
+extern void power9_idle_type(unsigned long stop_psscr_val,
+			      unsigned long stop_psscr_mask);
 
 extern void flush_instruction_cache(void);
 extern void hard_reset_now(void);

commit 92d9dfda8b547cc292af27e11e11c9eff3bb574f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Jun 1 20:05:04 2017 +0530

    powerpc/mm/4k: Limit 4k page size config to 64TB virtual address space
    
    Supporting 512TB requires us to do a order 3 allocation for level 1 page
    table (pgd). This results in page allocation failures with certain workloads.
    For now limit 4k linux page size config to 64TB.
    
    Fixes: f6eedbba7a26 ("powerpc/mm/hash: Increase VA range to 128TB")
    Reported-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index a2123f291ab0..bb99b651085a 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -110,13 +110,18 @@ void release_thread(struct task_struct *);
 #define TASK_SIZE_128TB (0x0000800000000000UL)
 #define TASK_SIZE_512TB (0x0002000000000000UL)
 
-#ifdef CONFIG_PPC_BOOK3S_64
+/*
+ * For now 512TB is only supported with book3s and 64K linux page size.
+ */
+#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_PPC_64K_PAGES)
 /*
  * Max value currently used:
  */
-#define TASK_SIZE_USER64	TASK_SIZE_512TB
+#define TASK_SIZE_USER64		TASK_SIZE_512TB
+#define DEFAULT_MAP_WINDOW_USER64	TASK_SIZE_128TB
 #else
-#define TASK_SIZE_USER64	TASK_SIZE_64TB
+#define TASK_SIZE_USER64		TASK_SIZE_64TB
+#define DEFAULT_MAP_WINDOW_USER64	TASK_SIZE_64TB
 #endif
 
 /*
@@ -132,7 +137,7 @@ void release_thread(struct task_struct *);
  * space during mmap's.
  */
 #define TASK_UNMAPPED_BASE_USER32 (PAGE_ALIGN(TASK_SIZE_USER32 / 4))
-#define TASK_UNMAPPED_BASE_USER64 (PAGE_ALIGN(TASK_SIZE_128TB / 4))
+#define TASK_UNMAPPED_BASE_USER64 (PAGE_ALIGN(DEFAULT_MAP_WINDOW_USER64 / 4))
 
 #define TASK_UNMAPPED_BASE ((is_32bit_task()) ? \
 		TASK_UNMAPPED_BASE_USER32 : TASK_UNMAPPED_BASE_USER64 )
@@ -143,21 +148,15 @@ void release_thread(struct task_struct *);
  * with 128TB and conditionally enable upto 512TB
  */
 #ifdef CONFIG_PPC_BOOK3S_64
-#define DEFAULT_MAP_WINDOW	((is_32bit_task()) ? \
-				 TASK_SIZE_USER32 : TASK_SIZE_128TB)
+#define DEFAULT_MAP_WINDOW	((is_32bit_task()) ?			\
+				 TASK_SIZE_USER32 : DEFAULT_MAP_WINDOW_USER64)
 #else
 #define DEFAULT_MAP_WINDOW	TASK_SIZE
 #endif
 
 #ifdef __powerpc64__
 
-#ifdef CONFIG_PPC_BOOK3S_64
-/* Limit stack to 128TB */
-#define STACK_TOP_USER64 TASK_SIZE_128TB
-#else
-#define STACK_TOP_USER64 TASK_SIZE_USER64
-#endif
-
+#define STACK_TOP_USER64 DEFAULT_MAP_WINDOW_USER64
 #define STACK_TOP_USER32 TASK_SIZE_USER32
 
 #define STACK_TOP (is_32bit_task() ? \

commit 61baf15555129f69720334f232b153890895ef71
Author: Scott Wood <oss@buserror.net>
Date:   Fri May 5 01:22:06 2017 -0500

    powerpc/64e: Don't place the stack beyond TASK_SIZE
    
    Commit f4ea6dcb08ea ("powerpc/mm: Enable mappings above 128TB") increased
    the task size on book3s, and introduced a mechanism to dynamically
    control whether a task uses these larger addresses.  While the change to
    the task size itself was ifdef-protected to only apply on book3s, the
    change to STACK_TOP_USER64 was not.  On book3e, this had the effect of
    trying to use addresses up to 128TiB for the stack despite a 64TiB task
    size limit -- which broke 64-bit userspace producing the following errors:
    
    Starting init: /sbin/init exists but couldn't execute it (error -14)
    Starting init: /bin/sh exists but couldn't execute it (error -14)
    Kernel panic - not syncing: No working init found.  Try passing init= option to kernel. See Linux Documentation/admin-guide/init.rst for guidance.
    
    Fixes: f4ea6dcb08ea ("powerpc/mm: Enable mappings above 128TB")
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index a4b1d8d6b793..a2123f291ab0 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -151,8 +151,13 @@ void release_thread(struct task_struct *);
 
 #ifdef __powerpc64__
 
+#ifdef CONFIG_PPC_BOOK3S_64
 /* Limit stack to 128TB */
 #define STACK_TOP_USER64 TASK_SIZE_128TB
+#else
+#define STACK_TOP_USER64 TASK_SIZE_USER64
+#endif
+
 #define STACK_TOP_USER32 TASK_SIZE_USER32
 
 #define STACK_TOP (is_32bit_task() ? \

commit f4ea6dcb08ea2c6c996c373573caf74d48d23b84
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Mar 30 16:35:21 2017 +0530

    powerpc/mm: Enable mappings above 128TB
    
    Not all user space application is ready to handle wide addresses. It's
    known that at least some JIT compilers use higher bits in pointers to
    encode their information. It collides with valid pointers with 512TB
    addresses and leads to crashes.
    
    To mitigate this, we are not going to allocate virtual address space
    above 128TB by default.
    
    But userspace can ask for allocation from full address space by
    specifying hint address (with or without MAP_FIXED) above 128TB.
    
    If hint address set above 128TB, but MAP_FIXED is not specified, we try
    to look for unmapped area by specified address. If it's already
    occupied, we look for unmapped area in *full* address space, rather than
    from 128TB window.
    
    This approach helps to easily make application's memory allocator aware
    about large address space without manually tracking allocated virtual
    address space.
    
    This is going to be a per mmap decision. ie, we can have some mmaps with
    larger addresses and other that do not.
    
    A sample memory layout looks like:
    
      10000000-10010000 r-xp 00000000 fc:00 9057045          /home/max_addr_512TB
      10010000-10020000 r--p 00000000 fc:00 9057045          /home/max_addr_512TB
      10020000-10030000 rw-p 00010000 fc:00 9057045          /home/max_addr_512TB
      10029630000-10029660000 rw-p 00000000 00:00 0          [heap]
      7fff834a0000-7fff834b0000 rw-p 00000000 00:00 0
      7fff834b0000-7fff83670000 r-xp 00000000 fc:00 9177190  /lib/powerpc64le-linux-gnu/libc-2.23.so
      7fff83670000-7fff83680000 r--p 001b0000 fc:00 9177190  /lib/powerpc64le-linux-gnu/libc-2.23.so
      7fff83680000-7fff83690000 rw-p 001c0000 fc:00 9177190  /lib/powerpc64le-linux-gnu/libc-2.23.so
      7fff83690000-7fff836a0000 rw-p 00000000 00:00 0
      7fff836a0000-7fff836c0000 r-xp 00000000 00:00 0        [vdso]
      7fff836c0000-7fff83700000 r-xp 00000000 fc:00 9177193  /lib/powerpc64le-linux-gnu/ld-2.23.so
      7fff83700000-7fff83710000 r--p 00030000 fc:00 9177193  /lib/powerpc64le-linux-gnu/ld-2.23.so
      7fff83710000-7fff83720000 rw-p 00040000 fc:00 9177193  /lib/powerpc64le-linux-gnu/ld-2.23.so
      7fffdccf0000-7fffdcd20000 rw-p 00000000 00:00 0        [stack]
      1000000000000-1000000010000 rw-p 00000000 00:00 0
      1ffff83710000-1ffff83720000 rw-p 00000000 00:00 0
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index ea4b6c696f1f..a4b1d8d6b793 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -114,9 +114,9 @@ void release_thread(struct task_struct *);
 /*
  * Max value currently used:
  */
-#define TASK_SIZE_USER64 TASK_SIZE_128TB
+#define TASK_SIZE_USER64	TASK_SIZE_512TB
 #else
-#define TASK_SIZE_USER64 TASK_SIZE_64TB
+#define TASK_SIZE_USER64	TASK_SIZE_64TB
 #endif
 
 /*
@@ -128,26 +128,37 @@ void release_thread(struct task_struct *);
 #define TASK_SIZE_OF(tsk) (test_tsk_thread_flag(tsk, TIF_32BIT) ? \
 		TASK_SIZE_USER32 : TASK_SIZE_USER64)
 #define TASK_SIZE	  TASK_SIZE_OF(current)
-
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap's.
  */
 #define TASK_UNMAPPED_BASE_USER32 (PAGE_ALIGN(TASK_SIZE_USER32 / 4))
-#define TASK_UNMAPPED_BASE_USER64 (PAGE_ALIGN(TASK_SIZE_USER64 / 4))
+#define TASK_UNMAPPED_BASE_USER64 (PAGE_ALIGN(TASK_SIZE_128TB / 4))
 
 #define TASK_UNMAPPED_BASE ((is_32bit_task()) ? \
 		TASK_UNMAPPED_BASE_USER32 : TASK_UNMAPPED_BASE_USER64 )
 #endif
 
+/*
+ * Initial task size value for user applications. For book3s 64 we start
+ * with 128TB and conditionally enable upto 512TB
+ */
+#ifdef CONFIG_PPC_BOOK3S_64
+#define DEFAULT_MAP_WINDOW	((is_32bit_task()) ? \
+				 TASK_SIZE_USER32 : TASK_SIZE_128TB)
+#else
+#define DEFAULT_MAP_WINDOW	TASK_SIZE
+#endif
+
 #ifdef __powerpc64__
 
-#define STACK_TOP_USER64 TASK_SIZE_USER64
+/* Limit stack to 128TB */
+#define STACK_TOP_USER64 TASK_SIZE_128TB
 #define STACK_TOP_USER32 TASK_SIZE_USER32
 
 #define STACK_TOP (is_32bit_task() ? \
 		   STACK_TOP_USER32 : STACK_TOP_USER64)
 
-#define STACK_TOP_MAX STACK_TOP_USER64
+#define STACK_TOP_MAX TASK_SIZE_USER64
 
 #else /* __powerpc64__ */
 

commit f6eedbba7a26fdaee9ea8121336dc86236c136c7
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Mar 22 09:06:57 2017 +0530

    powerpc/mm/hash: Increase VA range to 128TB
    
    We update the hash linux page table layout such that we can support
    512TB. But we limit the TASK_SIZE to 128TB. We can switch to 128TB by
    default without conditional because that is the max virtual address
    supported by other architectures. We will later add a mechanism to
    on-demand increase the application's effective address range to 512TB.
    
    Having the page table layout changed to accommodate 512TB makes testing
    large memory configuration easier with less code changes to kernel
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index e0fecbcea2a2..ea4b6c696f1f 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -102,11 +102,25 @@ void release_thread(struct task_struct *);
 #endif
 
 #ifdef CONFIG_PPC64
-/* 64-bit user address space is 46-bits (64TB user VM) */
-#define TASK_SIZE_USER64 (0x0000400000000000UL)
+/*
+ * 64-bit user address space can have multiple limits
+ * For now supported values are:
+ */
+#define TASK_SIZE_64TB  (0x0000400000000000UL)
+#define TASK_SIZE_128TB (0x0000800000000000UL)
+#define TASK_SIZE_512TB (0x0002000000000000UL)
 
-/* 
- * 32-bit user address space is 4GB - 1 page 
+#ifdef CONFIG_PPC_BOOK3S_64
+/*
+ * Max value currently used:
+ */
+#define TASK_SIZE_USER64 TASK_SIZE_128TB
+#else
+#define TASK_SIZE_USER64 TASK_SIZE_64TB
+#endif
+
+/*
+ * 32-bit user address space is 4GB - 1 page
  * (this 1 page is needed so referencing of 0xFFFFFFFF generates EFAULT
  */
 #define TASK_SIZE_USER32 (0x0000000100000000UL - (1*PAGE_SIZE))

commit 6c8f9ad566ddc63da5da84c3c9e6c1bb0434c64f
Merge: d11914b21c4c 75b824727680
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Feb 18 21:37:14 2017 +1100

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/scottwood/linux into next
    
    Freescale updates from Scott:
    
    "Highlights include 8xx breakpoints and perf, t1042rdb display support,
    and board updates."

commit 09206b600c76f20984e80d99f3b5343c79332a97
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Wed Jan 25 14:06:28 2017 +0530

    powernv: Pass PSSCR value and mask to power9_idle_stop
    
    The power9_idle_stop method currently takes only the requested stop
    level as a parameter and picks up the rest of the PSSCR bits from a
    hand-coded macro. This is not a very flexible design, especially when
    the firmware has the capability to communicate the psscr value and the
    mask associated with a particular stop state via device tree.
    
    This patch modifies the power9_idle_stop API to take as parameters the
    PSSCR value and the PSSCR mask corresponding to the stop state that
    needs to be set. These PSSCR value and mask are respectively obtained
    by parsing the "ibm,cpu-idle-state-psscr" and
    "ibm,cpu-idle-state-psscr-mask" fields from the device tree.
    
    In addition to this, the patch adds support for handling stop states
    for which ESL and EC bits in the PSSCR are zero. As per the
    architecture, a wakeup from these stop states resumes execution from
    the subsequent instruction as opposed to waking up at the System
    Vector.
    
    The older firmware sets only the Requested Level (RL) field in the
    psscr and psscr-mask exposed in the device tree. For older firmware
    where psscr-mask=0xf, this patch will set the default sane values that
    the set for for remaining PSSCR fields (i.e PSLL, MTL, ESL, EC, and
    TR). For the new firmware, the patch will validate that the invariants
    required by the ISA for the psscr values are maintained by the
    firmware.
    
    This skiboot patch that exports fully populated PSSCR values and the
    mask for all the stop states can be found here:
    https://lists.ozlabs.org/pipermail/skiboot/2016-September/004869.html
    
    [Optimize the number of instructions before entering STOP with
    ESL=EC=0, validate the PSSCR values provided by the firimware
    maintains the invariants required as per the ISA suggested by Balbir
    Singh]
    
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 1ba814436c73..21e0b52685b5 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -454,7 +454,8 @@ extern int powersave_nap;	/* set if nap mode can be used in idle loop */
 extern unsigned long power7_nap(int check_irq);
 extern unsigned long power7_sleep(void);
 extern unsigned long power7_winkle(void);
-extern unsigned long power9_idle_stop(unsigned long stop_level);
+extern unsigned long power9_idle_stop(unsigned long stop_psscr_val,
+				      unsigned long stop_psscr_mask);
 
 extern void flush_instruction_cache(void);
 extern void hard_reset_now(void);

commit fa769d3f58e6b0db4ed9f5f05ef1f251692f90c6
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Nov 29 09:52:13 2016 +0100

    powerpc/32: Enable HW_BREAKPOINT on BOOK3S
    
    BOOK3S also has DABR register and capability to handle data
    breakpoints, so this patch enable it on all BOOK3S, not only 64 bits.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 1ba814436c73..2053a4b0914f 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -225,6 +225,7 @@ struct thread_struct {
 #ifdef CONFIG_PPC64
 	unsigned long	start_tb;	/* Start purr when proc switched in */
 	unsigned long	accum_tb;	/* Total accumulated purr for process */
+#endif
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	struct perf_event *ptrace_bps[HBP_NUM];
 	/*
@@ -233,7 +234,6 @@ struct thread_struct {
 	 */
 	struct perf_event *last_hit_ubp;
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
-#endif
 	struct arch_hw_breakpoint hw_brk; /* info on the hardware breakpoint */
 	unsigned long	trap_nr;	/* last trap # on this thread */
 	u8 load_fp;

commit de399813b521ea7e38bbfb5e5b620b5e202e5783
Merge: 57ca04ab4401 c6f6634721c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 16 09:26:42 2016 -0800

    Merge tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for the kexec_file_load() syscall, which is a prereq for
         secure and trusted boot.
    
       - Prevent kernel execution of userspace on P9 Radix (similar to
         SMEP/PXN).
    
       - Sort the exception tables at build time, to save time at boot, and
         store them as relative offsets to save space in the kernel image &
         memory.
    
       - Allow building the kernel with thin archives, which should allow us
         to build an allyesconfig once some other fixes land.
    
       - Build fixes to allow us to correctly rebuild when changing the
         kernel endian from big to little or vice versa.
    
       - Plumbing so that we can avoid doing a full mm TLB flush on P9
         Radix.
    
       - Initial stack protector support (-fstack-protector).
    
       - Support for dumping the radix (aka. Linux) and hash page tables via
         debugfs.
    
       - Fix an oops in cxl coredump generation when cxl_get_fd() is used.
    
       - Freescale updates from Scott: "Highlights include 8xx hugepage
         support, qbman fixes/cleanup, device tree updates, and some misc
         cleanup."
    
       - Many and varied fixes and minor enhancements as always.
    
      Thanks to:
        Alexey Kardashevskiy, Andrew Donnellan, Aneesh Kumar K.V, Anshuman
        Khandual, Anton Blanchard, Balbir Singh, Bartlomiej Zolnierkiewicz,
        Christophe Jaillet, Christophe Leroy, Denis Kirjanov, Elimar
        Riesebieter, Frederic Barrat, Gautham R. Shenoy, Geliang Tang, Geoff
        Levand, Jack Miller, Johan Hovold, Lars-Peter Clausen, Libin,
        Madhavan Srinivasan, Michael Neuling, Nathan Fontenot, Naveen N.
        Rao, Nicholas Piggin, Pan Xinhui, Peter Senna Tschudin, Rashmica
        Gupta, Rui Teng, Russell Currey, Scott Wood, Simon Guo, Suraj
        Jitindar Singh, Thiago Jung Bauermann, Tobias Klauser, Vaibhav Jain"
    
    [ And thanks to Michael, who took time off from a new baby to get this
      pull request done.   - Linus ]
    
    * tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (174 commits)
      powerpc/fsl/dts: add FMan node for t1042d4rdb
      powerpc/fsl/dts: add sg_2500_aqr105_phy4 alias on t1024rdb
      powerpc/fsl/dts: add QMan and BMan nodes on t1024
      powerpc/fsl/dts: add QMan and BMan nodes on t1023
      soc/fsl/qman: test: use DEFINE_SPINLOCK()
      powerpc/fsl-lbc: use DEFINE_SPINLOCK()
      powerpc/8xx: Implement support of hugepages
      powerpc: get hugetlbpage handling more generic
      powerpc: port 64 bits pgtable_cache to 32 bits
      powerpc/boot: Request no dynamic linker for boot wrapper
      soc/fsl/bman: Use resource_size instead of computation
      soc/fsl/qe: use builtin_platform_driver
      powerpc/fsl_pmc: use builtin_platform_driver
      powerpc/83xx/suspend: use builtin_platform_driver
      powerpc/ftrace: Fix the comments for ftrace_modify_code
      powerpc/perf: macros for power9 format encoding
      powerpc/perf: power9 raw event format encoding
      powerpc/perf: update attribute_group data structure
      powerpc/perf: factor out the event format field
      powerpc/mm/iommu, vfio/spapr: Put pages on VFIO container shutdown
      ...

commit 6d0d287891a022ebba572327cbd70b5de69a63a2
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Nov 16 13:23:05 2016 +0100

    locking/core: Provide common cpu_relax_yield() definition
    
    No need to duplicate the same define everywhere. Since
    the only user is stop-machine and the only provider is
    s390, we can use a default implementation of cpu_relax_yield()
    in sched.h.
    
    Suggested-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-s390 <linux-s390@vger.kernel.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: sparclinux@vger.kernel.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1479298985-191589-1-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 5684e6872473..dac83fcb9445 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -404,8 +404,6 @@ static inline unsigned long __pack_fe01(unsigned int fpmode)
 #define cpu_relax()	barrier()
 #endif
 
-#define cpu_relax_yield() cpu_relax()
-
 /* Check that a certain kernel stack pointer is valid in task_struct p */
 int validate_sp(unsigned long sp, struct task_struct *p,
                        unsigned long nbytes);

commit 5bd0b85ba8bb9de6f61f33f3752fc85f4c87fc22
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Oct 25 11:03:15 2016 +0200

    locking/core, arch: Remove cpu_relax_lowlatency()
    
    As there are no users left, we can remove cpu_relax_lowlatency()
    implementations from every architecture.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Cc: <linux-arch@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1477386195-32736-6-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 908fa7cb3fb3..5684e6872473 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -405,7 +405,6 @@ static inline unsigned long __pack_fe01(unsigned int fpmode)
 #endif
 
 #define cpu_relax_yield() cpu_relax()
-#define cpu_relax_lowlatency() cpu_relax()
 
 /* Check that a certain kernel stack pointer is valid in task_struct p */
 int validate_sp(unsigned long sp, struct task_struct *p,

commit 79ab11cdb90d8536817ab7357ecb6b1ff76be26c
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Oct 25 11:03:11 2016 +0200

    locking/core: Introduce cpu_relax_yield()
    
    For spinning loops people do often use barrier() or cpu_relax().
    For most architectures cpu_relax and barrier are the same, but on
    some architectures cpu_relax can add some latency.
    For example on power,sparc64 and arc, cpu_relax can shift the CPU
    towards other hardware threads in an SMT environment.
    On s390 cpu_relax does even more, it uses an hypercall to the
    hypervisor to give up the timeslice.
    In contrast to the SMT yielding this can result in larger latencies.
    In some places this latency is unwanted, so another variant
    "cpu_relax_lowlatency" was introduced. Before this is used in more
    and more places, lets revert the logic and provide a cpu_relax_yield
    that can be called in places where yielding is more important than
    latency. By default this is the same as cpu_relax on all architectures.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1477386195-32736-2-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index c07c31b0e89e..908fa7cb3fb3 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -404,6 +404,7 @@ static inline unsigned long __pack_fe01(unsigned int fpmode)
 #define cpu_relax()	barrier()
 #endif
 
+#define cpu_relax_yield() cpu_relax()
 #define cpu_relax_lowlatency() cpu_relax()
 
 /* Check that a certain kernel stack pointer is valid in task_struct p */

commit 29a969b764817c1dce819c2bc8c00a147529a5ef
Author: Michael Neuling <mikey@neuling.org>
Date:   Mon Oct 31 13:19:39 2016 +1100

    powerpc: Revert Load Monitor Register Support
    
    Load monitored is no longer supported on POWER9 so let's remove the
    code.
    
    This reverts commit bd3ea317fddf ("powerpc: Load Monitor Register
    Support").
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index c07c31b0e89e..5c0a665af1c3 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -312,8 +312,6 @@ struct thread_struct {
 	unsigned long	mmcr2;
 	unsigned 	mmcr0;
 	unsigned 	used_ebb;
-	unsigned long	lmrr;
-	unsigned long	lmser;
 #endif
 };
 

commit 5d176f751ee3c6eededd984ad409bff201f436a7
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Wed Sep 14 18:02:16 2016 +1000

    powerpc: tm: Enable transactional memory (TM) lazily for userspace
    
    Currently the MSR TM bit is always set if the hardware is TM capable.
    This adds extra overhead as it means the TM SPRS (TFHAR, TEXASR and
    TFAIR) must be swapped for each process regardless of if they use TM.
    
    For processes that don't use TM the TM MSR bit can be turned off
    allowing the kernel to avoid the expensive swap of the TM registers.
    
    A TM unavailable exception will occur if a thread does use TM and the
    kernel will enable MSR_TM and leave it so for some time afterwards.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index b3e0cfcc84f6..c07c31b0e89e 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -257,6 +257,7 @@ struct thread_struct {
 	int		used_spe;	/* set if process has used spe */
 #endif /* CONFIG_SPE */
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+	u8	load_tm;
 	u64		tm_tfhar;	/* Transaction fail handler addr */
 	u64		tm_texasr;	/* Transaction exception & summary */
 	u64		tm_tfiar;	/* Transaction fail instr address reg */

commit 000ec280e3dd5c77a5227db27bfda1511e26db9a
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Fri Sep 23 16:18:25 2016 +1000

    powerpc: tm: Rename transct_(*) to ck(\1)_state
    
    Make the structures being used for checkpointed state named
    consistently with the pt_regs/ckpt_regs.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index feab2ce72940..b3e0cfcc84f6 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -147,7 +147,7 @@ typedef struct {
 } mm_segment_t;
 
 #define TS_FPR(i) fp_state.fpr[i][TS_FPROFFSET]
-#define TS_TRANS_FPR(i) transact_fp.fpr[i][TS_FPROFFSET]
+#define TS_CKFPR(i) ckfp_state.fpr[i][TS_FPROFFSET]
 
 /* FP and VSX 0-31 register set */
 struct thread_fp_state {
@@ -275,9 +275,9 @@ struct thread_struct {
 	 *
 	 * These are analogous to how ckpt_regs and pt_regs work
 	 */
-	struct thread_fp_state transact_fp;
-	struct thread_vr_state transact_vr;
-	unsigned long	transact_vrsave;
+	struct thread_fp_state ckfp_state; /* Checkpointed FP state */
+	struct thread_vr_state ckvr_state; /* Checkpointed VR state */
+	unsigned long	ckvrsave; /* Checkpointed VRSAVE */
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
 	void*		kvm_shadow_vcpu; /* KVM internal data */

commit dc3106690b20305c3df06b42456fe386dd632ac9
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Fri Sep 23 16:18:24 2016 +1000

    powerpc: tm: Always use fp_state and vr_state to store live registers
    
    There is currently an inconsistency as to how the entire CPU register
    state is saved and restored when a thread uses transactional memory
    (TM).
    
    Using transactional memory results in the CPU having duplicated
    (almost) all of its register state. This duplication results in a set
    of registers which can be considered 'live', those being currently
    modified by the instructions being executed and another set that is
    frozen at a point in time.
    
    On context switch, both sets of state have to be saved and (later)
    restored. These two states are often called a variety of different
    things. Common terms for the state which only exists after the CPU has
    entered a transaction (performed a TBEGIN instruction) in hardware are
    'transactional' or 'speculative'.
    
    Between a TBEGIN and a TEND or TABORT (or an event that causes the
    hardware to abort), regardless of the use of TSUSPEND the
    transactional state can be referred to as the live state.
    
    The second state is often to referred to as the 'checkpointed' state
    and is a duplication of the live state when the TBEGIN instruction is
    executed. This state is kept in the hardware and will be rolled back
    to on transaction failure.
    
    Currently all the registers stored in pt_regs are ALWAYS the live
    registers, that is, when a thread has transactional registers their
    values are stored in pt_regs and the checkpointed state is in
    ckpt_regs. A strange opposite is true for fp_state/vr_state. When a
    thread is non transactional fp_state/vr_state holds the live
    registers. When a thread has initiated a transaction fp_state/vr_state
    holds the checkpointed state and transact_fp/transact_vr become the
    structure which holds the live state (at this point it is a
    transactional state).
    
    This method creates confusion as to where the live state is, in some
    circumstances it requires extra work to determine where to put the
    live state and prevents the use of common functions designed (probably
    before TM) to save the live state.
    
    With this patch pt_regs, fp_state and vr_state all represent the
    same thing and the other structures [pending rename] are for
    checkpointed state.
    
    Acked-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 68e3bf57b027..feab2ce72940 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -267,16 +267,13 @@ struct thread_struct {
 	unsigned long	tm_dscr;
 
 	/*
-	 * Transactional FP and VSX 0-31 register set.
-	 * NOTE: the sense of these is the opposite of the integer ckpt_regs!
+	 * Checkpointed FP and VSX 0-31 register set.
 	 *
 	 * When a transaction is active/signalled/scheduled etc., *regs is the
 	 * most recent set of/speculated GPRs with ckpt_regs being the older
 	 * checkpointed regs to which we roll back if transaction aborts.
 	 *
-	 * However, fpr[] is the checkpointed 'base state' of FP regs, and
-	 * transact_fpr[] is the new set of transactional values.
-	 * VRs work the same way.
+	 * These are analogous to how ckpt_regs and pt_regs work
 	 */
 	struct thread_fp_state transact_fp;
 	struct thread_vr_state transact_vr;

commit bcef83a00dc44ee25ff4d6e078cf6432ddf74dec
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Fri Jul 8 11:50:49 2016 +0530

    powerpc/powernv: Add platform support for stop instruction
    
    POWER ISA v3 defines a new idle processor core mechanism. In summary,
     a) new instruction named stop is added. This instruction replaces
            instructions like nap, sleep, rvwinkle.
     b) new per thread SPR named Processor Stop Status and Control Register
            (PSSCR) is added which controls the behavior of stop instruction.
    
    PSSCR layout:
    ----------------------------------------------------------
    | PLS | /// | SD | ESL | EC | PSLL | /// | TR | MTL | RL |
    ----------------------------------------------------------
    0      4     41   42    43   44     48    54   56    60
    
    PSSCR key fields:
            Bits 0:3  - Power-Saving Level Status. This field indicates the lowest
            power-saving state the thread entered since stop instruction was last
            executed.
    
            Bit 42 - Enable State Loss
            0 - No state is lost irrespective of other fields
            1 - Allows state loss
    
            Bits 44:47 - Power-Saving Level Limit
            This limits the power-saving level that can be entered into.
    
            Bits 60:63 - Requested Level
            Used to specify which power-saving level must be entered on executing
            stop instruction
    
    This patch adds support for stop instruction and PSSCR handling.
    
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index b5925d5d4985..68e3bf57b027 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -460,6 +460,8 @@ extern int powersave_nap;	/* set if nap mode can be used in idle loop */
 extern unsigned long power7_nap(int check_irq);
 extern unsigned long power7_sleep(void);
 extern unsigned long power7_winkle(void);
+extern unsigned long power9_idle_stop(unsigned long stop_level);
+
 extern void flush_instruction_cache(void);
 extern void hard_reset_now(void);
 extern void poweroff_now(void);

commit bd3ea317fddfd0f2044f94bed294b90c4bc8e69e
Author: Jack Miller <jack@codezen.org>
Date:   Thu Jun 9 12:31:09 2016 +1000

    powerpc: Load Monitor Register Support
    
    This enables new registers, LMRR and LMSER, that can trigger an EBB in
    userspace code when a monitored load (via the new ldmx instruction)
    loads memory from a monitored space. This facility is controlled by a
    new FSCR bit, LM.
    
    This patch disables the FSCR LM control bit on task init and enables
    that bit when a load monitor facility unavailable exception is taken
    for using it. On context switch, this bit is then used to determine
    whether the two relevant registers are saved and restored. This is
    done lazily for performance reasons.
    
    Signed-off-by: Jack Miller <jack@codezen.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index f6b1a5f51d05..b5925d5d4985 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -314,6 +314,8 @@ struct thread_struct {
 	unsigned long	mmcr2;
 	unsigned 	mmcr0;
 	unsigned 	used_ebb;
+	unsigned long	lmrr;
+	unsigned long	lmser;
 #endif
 };
 

commit b57bd2de8c6c9aa03f1b899edd6f5582cc8b5b08
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Jun 9 12:31:08 2016 +1000

    powerpc: Improve FSCR init and context switching
    
    This fixes a few issues with FSCR init and switching.
    
    In commit 152d523e6307 ("powerpc: Create context switch helpers
    save_sprs() and restore_sprs()") we moved the setting of the FSCR
    register from inside an CPU_FTR_ARCH_207S section to inside just a
    CPU_FTR_ARCH_DSCR section. Hence we are setting FSCR on POWER6/7 where
    the FSCR doesn't exist. This is harmless but we shouldn't do it.
    
    Also, we can simplify the FSCR context switch. We don't need to go
    through the calculation involving dscr_inherit. We can just restore
    what we saved last time.
    
    We also set an initial value in INIT_THREAD, so that pid 1 which is
    cloned from that gets a sane value.
    
    Based on patch by Jack Miller.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index c0c27bdbb069..f6b1a5f51d05 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -347,6 +347,7 @@ struct thread_struct {
 	.fs = KERNEL_DS, \
 	.fpexc_mode = 0, \
 	.ppr = INIT_PPR, \
+	.fscr = FSCR_TAR | FSCR_EBB \
 }
 #endif
 

commit 027dfac694fc27ef0273afb810d9b1f9da57d6e1
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jun 1 16:34:37 2016 +1000

    powerpc: Various typo fixes
    
    Signed-off-by: Andrea Gelmini <andrea.gelmini@gelma.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 009fab130cd8..c0c27bdbb069 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -224,7 +224,7 @@ struct thread_struct {
 	unsigned int	align_ctl;	/* alignment handling control */
 #ifdef CONFIG_PPC64
 	unsigned long	start_tb;	/* Start purr when proc switched in */
-	unsigned long	accum_tb;	/* Total accumilated purr for process */
+	unsigned long	accum_tb;	/* Total accumulated purr for process */
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	struct perf_event *ptrace_bps[HBP_NUM];
 	/*

commit 71528d8bd7a8aa920cd69d4223c6c87d5849257d
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Fri Mar 25 01:12:21 2016 +0800

    powerpc: Correct used_vsr comment
    
    The used_vsr flag is set if process has used VSX registers, not Altivec
    registers. But the comment says otherwise, correct the comment.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 8ab8a1a9610a..009fab130cd8 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -246,7 +246,7 @@ struct thread_struct {
 #endif /* CONFIG_ALTIVEC */
 #ifdef CONFIG_VSX
 	/* VSR status */
-	int		used_vsr;	/* set if process has used altivec */
+	int		used_vsr;	/* set if process has used VSX */
 #endif /* CONFIG_VSX */
 #ifdef CONFIG_SPE
 	unsigned long	evr[32];	/* upper 32-bits of SPE regs */

commit 70fe3d980f5f14d8125869125ba9a0ea95e09c6b
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:47 2016 +1100

    powerpc: Restore FPU/VEC/VSX if previously used
    
    Currently the FPU, VEC and VSX facilities are lazily loaded. This is not
    a problem unless a process is using these facilities.
    
    Modern versions of GCC are very good at automatically vectorising code,
    new and modernised workloads make use of floating point and vector
    facilities, even the kernel makes use of vectorised memcpy.
    
    All this combined greatly increases the cost of a syscall since the
    kernel uses the facilities sometimes even in syscall fast-path making it
    increasingly common for a thread to take an *_unavailable exception soon
    after a syscall, not to mention potentially taking all three.
    
    The obvious overcompensation to this problem is to simply always load
    all the facilities on every exit to userspace. Loading up all FPU, VEC
    and VSX registers every time can be expensive and if a workload does
    avoid using them, it should not be forced to incur this penalty.
    
    An 8bit counter is used to detect if the registers have been used in the
    past and the registers are always loaded until the value wraps to back
    to zero.
    
    Several versions of the assembly in entry_64.S were tested:
    
      1. Always calling C.
      2. Performing a common case check and then calling C.
      3. A complex check in asm.
    
    After some benchmarking it was determined that avoiding C in the common
    case is a performance benefit (option 2). The full check in asm (option
    3) greatly complicated that codepath for a negligible performance gain
    and the trade-off was deemed not worth it.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    [mpe: Move load_vec in the struct to fill an existing hole, reword change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    
    fixup

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index ac2330820b9a..8ab8a1a9610a 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -236,7 +236,9 @@ struct thread_struct {
 #endif
 	struct arch_hw_breakpoint hw_brk; /* info on the hardware breakpoint */
 	unsigned long	trap_nr;	/* last trap # on this thread */
+	u8 load_fp;
 #ifdef CONFIG_ALTIVEC
+	u8 load_vec;
 	struct thread_vr_state vr_state;
 	struct thread_vr_state *vr_save_area;
 	unsigned long	vrsave;

commit 1f2e25b2d552cade43eacb2edc4e7f01c1cfecb3
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:07 2015 +1100

    powerpc: Remove fp_enable() and vec_enable(), use msr_check_and_{set, clear}()
    
    More consolidation of our MSR available bit handling.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index a2e891840806..ac2330820b9a 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -380,8 +380,6 @@ extern int set_endian(struct task_struct *tsk, unsigned int val);
 extern int get_unalign_ctl(struct task_struct *tsk, unsigned long adr);
 extern int set_unalign_ctl(struct task_struct *tsk, unsigned int val);
 
-extern void fp_enable(void);
-extern void vec_enable(void);
 extern void load_fp_state(struct thread_fp_state *fp);
 extern void store_fp_state(struct thread_fp_state *fp);
 extern void load_vr_state(struct thread_vr_state *vr);

commit af1bbc3dd3d501d27da72e1764afe5f5b0d3882d
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:43:57 2015 +1100

    powerpc: Remove UP only lazy floating point and vector optimisations
    
    The UP only lazy floating point and vector optimisations were written
    back when SMP was not common, and neither glibc nor gcc used vector
    instructions. Now SMP is very common, glibc aggressively uses vector
    instructions and gcc autovectorises.
    
    We want to add new optimisations that apply to both UP and SMP, but
    in preparation for that remove these UP only optimisations.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index c273f3e0ba84..a2e891840806 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -88,12 +88,6 @@ struct task_struct;
 void start_thread(struct pt_regs *regs, unsigned long fdptr, unsigned long sp);
 void release_thread(struct task_struct *);
 
-/* Lazy FPU handling on uni-processor */
-extern struct task_struct *last_task_used_math;
-extern struct task_struct *last_task_used_altivec;
-extern struct task_struct *last_task_used_vsx;
-extern struct task_struct *last_task_used_spe;
-
 #ifdef CONFIG_PPC32
 
 #if CONFIG_TASK_SIZE > CONFIG_KERNEL_START

commit 152d523e6307c7152f9986a542f873b5c5863937
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:43:55 2015 +1100

    powerpc: Create context switch helpers save_sprs() and restore_sprs()
    
    Move all our context switch SPR save and restore code into two
    helpers. We do a few optimisations:
    
    - Group all mfsprs and all mtsprs. In many cases an mtspr sets a
    scoreboarding bit that an mfspr waits on, so the current practise of
    mfspr A; mtspr A; mfpsr B; mtspr B is the worst scheduling we can
    do.
    
    - SPR writes are slow, so check that the value is changing before
    writing it.
    
    A context switch microbenchmark using yield():
    
    http://ozlabs.org/~anton/junkcode/context_switch2.c
    
    ./context_switch2 --test=yield 0 0
    
    shows an improvement of almost 10% on POWER8.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 5afea361beaa..c273f3e0ba84 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -294,6 +294,7 @@ struct thread_struct {
 #endif
 #ifdef CONFIG_PPC64
 	unsigned long	dscr;
+	unsigned long	fscr;
 	/*
 	 * This member element dscr_inherit indicates that the process
 	 * has explicitly attempted and changed the DSCR register value

commit 829023df86d4ec39b110860cd5f106b7ac58f772
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Mon Jul 6 16:24:10 2015 +0530

    powerpc/tm: Drop tm_orig_msr from thread_struct
    
    Currently tm_orig_msr is getting used during process context switch only.
    Then there is ckpt_regs which saves the checkpointed userspace context
    The MSR slot contained in ckpt_regs structure can be used during process
    context switch instead of tm_orig_msr, thus allowing us to drop it from
    thread_struct structure. This patch does that change.
    
    Acked-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 28ded5d9b579..5afea361beaa 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -264,7 +264,6 @@ struct thread_struct {
 	u64		tm_tfhar;	/* Transaction fail handler addr */
 	u64		tm_texasr;	/* Transaction exception & summary */
 	u64		tm_tfiar;	/* Transaction fail instr address reg */
-	unsigned long	tm_orig_msr;	/* Thread's MSR on ctx switch */
 	struct pt_regs	ckpt_regs;	/* Checkpointed registers */
 
 	unsigned long	tm_tar;

commit d3cb06e0cdc9841b289a0d68acaffd2868504902
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Thu May 21 12:13:04 2015 +0530

    powerpc/dscr: Add some in-code documentation
    
    This patch adds some in-code documentation to the DSCR related code to
    make it more readable without having any functional change to it.
    
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index bf117d8fb45f..28ded5d9b579 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -295,6 +295,15 @@ struct thread_struct {
 #endif
 #ifdef CONFIG_PPC64
 	unsigned long	dscr;
+	/*
+	 * This member element dscr_inherit indicates that the process
+	 * has explicitly attempted and changed the DSCR register value
+	 * for itself. Hence kernel wont use the default CPU DSCR value
+	 * contained in the PACA structure anymore during process context
+	 * switch. Once this variable is set, this behaviour will also be
+	 * inherited to all the children of this process from that point
+	 * onwards.
+	 */
 	int		dscr_inherit;
 	unsigned long	ppr;	/* used to save/restore SMT priority */
 #endif

commit 77b54e9f213f76a23736940cf94bcd765fc00f40
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Wed Dec 10 00:26:53 2014 +0530

    powernv/powerpc: Add winkle support for offline cpus
    
    Winkle is a deep idle state supported in power8 chips. A core enters
    winkle when all the threads of the core enter winkle. In this state
    power supply to the entire chiplet i.e core, private L2 and private L3
    is turned off. As a result it gives higher powersavings compared to
    sleep.
    
    But entering winkle results in a total hypervisor state loss. Hence the
    hypervisor context has to be preserved before entering winkle and
    restored upon wake up.
    
    Power-on Reset Engine (PORE) is a dedicated engine which is responsible
    for powering on the chiplet during wake up. It can be programmed to
    restore the register contests of a few specific registers. This patch
    uses PORE to restore register state wherever possible and uses stack to
    save and restore rest of the necessary registers.
    
    With hypervisor state restore things fall under three categories-
    per-core state, per-subcore state and per-thread state. To manage this,
    extend the infrastructure introduced for sleep. Mainly we add a paca
    variable subcore_sibling_mask. Using this and the core_idle_state we can
    distingush first thread in core and subcore.
    
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index f5c45b37c0d4..bf117d8fb45f 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -453,6 +453,7 @@ enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 extern int powersave_nap;	/* set if nap mode can be used in idle loop */
 extern unsigned long power7_nap(int check_irq);
 extern unsigned long power7_sleep(void);
+extern unsigned long power7_winkle(void);
 extern void flush_instruction_cache(void);
 extern void hard_reset_now(void);
 extern void poweroff_now(void);

commit 7cba160ad789a3ad7e68b92bf20eaad6ed171f80
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Wed Dec 10 00:26:52 2014 +0530

    powernv/cpuidle: Redesign idle states management
    
    Deep idle states like sleep and winkle are per core idle states. A core
    enters these states only when all the threads enter either the
    particular idle state or a deeper one. There are tasks like fastsleep
    hardware bug workaround and hypervisor core state save which have to be
    done only by the last thread of the core entering deep idle state and
    similarly tasks like timebase resync, hypervisor core register restore
    that have to be done only by the first thread waking up from these
    state.
    
    The current idle state management does not have a way to distinguish the
    first/last thread of the core waking/entering idle states. Tasks like
    timebase resync are done for all the threads. This is not only is
    suboptimal, but can cause functionality issues when subcores and kvm is
    involved.
    
    This patch adds the necessary infrastructure to track idle states of
    threads in a per-core structure. It uses this info to perform tasks like
    fastsleep workaround and timebase resync only once per core.
    
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Originally-by: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: linux-pm@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 29c3798cf800..f5c45b37c0d4 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -452,7 +452,7 @@ enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 
 extern int powersave_nap;	/* set if nap mode can be used in idle loop */
 extern unsigned long power7_nap(int check_irq);
-extern void power7_sleep(void);
+extern unsigned long power7_sleep(void);
 extern void flush_instruction_cache(void);
 extern void hard_reset_now(void);
 extern void poweroff_now(void);

commit 56548fc0e86cb9156af7a7e1f15ba78f251dafaf
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Dec 3 14:48:40 2014 +1100

    powerpc/powernv: Return to cpu offline loop when finished in KVM guest
    
    When a secondary hardware thread has finished running a KVM guest, we
    currently put that thread into nap mode using a nap instruction in
    the KVM code.  This changes the code so that instead of doing a nap
    instruction directly, we instead cause the call to power7_nap() that
    put the thread into nap mode to return.  The reason for doing this is
    to avoid having the KVM code having to know what low-power mode to
    put the thread into.
    
    In the case of a secondary thread used to run a KVM guest, the thread
    will be offline from the point of view of the host kernel, and the
    relevant power7_nap() call is the one in pnv_smp_cpu_disable().
    In this case we don't want to clear pending IPIs in the offline loop
    in that function, since that might cause us to miss the wakeup for
    the next time the thread needs to run a guest.  To tell whether or
    not to clear the interrupt, we use the SRR1 value returned from
    power7_nap(), and check if it indicates an external interrupt.  We
    arrange that the return from power7_nap() when we have finished running
    a guest returns 0, so pending interrupts don't get flushed in that
    case.
    
    Note that it is important a secondary thread that has finished
    executing in the guest, or that didn't have a guest to run, should
    not return to power7_nap's caller while the kvm_hstate.hwthread_req
    flag in the PACA is non-zero, because the return from power7_nap
    will reenable the MMU, and the MMU might still be in guest context.
    In this situation we spin at low priority in real mode waiting for
    hwthread_req to become zero.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index dda7ac4c80bd..29c3798cf800 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -451,7 +451,7 @@ extern unsigned long cpuidle_disable;
 enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 
 extern int powersave_nap;	/* set if nap mode can be used in idle loop */
-extern void power7_nap(int check_irq);
+extern unsigned long power7_nap(int check_irq);
 extern void power7_sleep(void);
 extern void flush_instruction_cache(void);
 extern void hard_reset_now(void);

commit 3a6bfbc91df04b081a44d419e0260bad54abddf7
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Sun Jun 29 15:09:33 2014 -0700

    arch, locking: Ciao arch_mutex_cpu_relax()
    
    The arch_mutex_cpu_relax() function, introduced by 34b133f, is
    hacky and ugly. It was added a few years ago to address the fact
    that common cpu_relax() calls include yielding on s390, and thus
    impact the optimistic spinning functionality of mutexes. Nowadays
    we use this function well beyond mutexes: rwsem, qrwlock, mcs and
    lockref. Since the macro that defines the call is in the mutex header,
    any users must include mutex.h and the naming is misleading as well.
    
    This patch (i) renames the call to cpu_relax_lowlatency  ("relax, but
    only if you can do it with very low latency") and (ii) defines it in
    each arch's asm/processor.h local header, just like for regular cpu_relax
    functions. On all archs, except s390, cpu_relax_lowlatency is simply cpu_relax,
    and thus we can take it out of mutex.h. While this can seem redundant,
    I believe it is a good choice as it allows us to move out arch specific
    logic from generic locking primitives and enables future(?) archs to
    transparently define it, similarly to System Z.
    
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Bharat Bhushan <r65777@freescale.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Cc: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Joseph Myers <joseph@codesourcery.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Nicolas Pitre <nico@linaro.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Qais Yousef <qais.yousef@imgtec.com>
    Cc: Qiaowei Ren <qiaowei.ren@intel.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Stratos Karafotis <stratosk@semaphore.gr>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vasily Kulikov <segoon@openwall.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Vineet Gupta <Vineet.Gupta1@synopsys.com>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Wolfram Sang <wsa@the-dreams.de>
    Cc: adi-buildroot-devel@lists.sourceforge.net
    Cc: linux390@de.ibm.com
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-am33-list@redhat.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-cris-kernel@axis.com
    Cc: linux-hexagon@vger.kernel.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux@lists.openrisc.net
    Cc: linux-m32r-ja@ml.linux-m32r.org
    Cc: linux-m32r@ml.linux-m32r.org
    Cc: linux-m68k@lists.linux-m68k.org
    Cc: linux-metag@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Cc: linux-parisc@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-s390@vger.kernel.org
    Cc: linux-sh@vger.kernel.org
    Cc: linux-xtensa@linux-xtensa.org
    Cc: sparclinux@vger.kernel.org
    Link: http://lkml.kernel.org/r/1404079773.2619.4.camel@buesod1.americas.hpqcorp.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 6d59072e13a7..dda7ac4c80bd 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -400,6 +400,8 @@ static inline unsigned long __pack_fe01(unsigned int fpmode)
 #define cpu_relax()	barrier()
 #endif
 
+#define cpu_relax_lowlatency() cpu_relax()
+
 /* Check that a certain kernel stack pointer is valid in task_struct p */
 int validate_sp(unsigned long sp, struct task_struct *p,
                        unsigned long nbytes);

commit 8d6f7c5aa3db6f3e5e43d09f8a0166c7d96f33f3
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri May 23 18:15:26 2014 +1000

    powerpc/powernv: Make it possible to skip the IRQHAPPENED check in power7_nap()
    
    To support split core we need to be able to force all secondaries into
    nap, so the core can detect they are idle and do an unsplit.
    
    Currently power7_nap() will return without napping if there is an irq
    pending. We want to ignore the pending irq and nap anyway, we will deal
    with the interrupt later.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index d660dc36831a..6d59072e13a7 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -449,7 +449,7 @@ extern unsigned long cpuidle_disable;
 enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 
 extern int powersave_nap;	/* set if nap mode can be used in idle loop */
-extern void power7_nap(void);
+extern void power7_nap(int check_irq);
 extern void power7_sleep(void);
 extern void flush_instruction_cache(void);
 extern void hard_reset_now(void);

commit aca79d2b6ec2c0b955b22abb71c6dab90fa1d4d5
Author: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
Date:   Wed Feb 26 05:38:25 2014 +0530

    powerpc/powernv: Add context management for Fast Sleep
    
    Before adding Fast-Sleep into the cpuidle framework, some low level
    support needs to be added to enable it. This includes saving and
    restoring of certain registers at entry and exit time of this state
    respectively just like we do in the NAP idle state.
    
    Signed-off-by: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    [Changelog modified by Preeti U. Murthy <preeti@linux.vnet.ibm.com>]
    Signed-off-by: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index b62de43ae5f3..d660dc36831a 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -450,6 +450,7 @@ enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 
 extern int powersave_nap;	/* set if nap mode can be used in idle loop */
 extern void power7_nap(void);
+extern void power7_sleep(void);
 extern void flush_instruction_cache(void);
 extern void hard_reset_now(void);
 extern void poweroff_now(void);

commit 3fa8cad82b94d0bed002571bd246f2299ffc876b
Author: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
Date:   Tue Jan 14 16:26:38 2014 +0530

    powerpc/pseries/cpuidle: smt-snooze-delay cleanup.
    
    smt-snooze-delay was designed to disable NAP state or delay the entry
    to the NAP state prior to adoption of cpuidle framework. This
    is per-cpu variable. With the coming of CPUIDLE framework,
    states can be disabled on per-cpu basis using the cpuidle/enable
    sysfs entry.
    
    Also, with the coming of cpuidle driver each state's target residency
    is per-driver unlike earlier which was per-device. Therefore,
    the per-cpu sysfs smt-snooze-delay which decides the target residency
    of the idle state on a particular cpu causes more confusion to the user
    as we cannot have different smt-snooze-delay (target residency)
    values for each cpu.
    
    In the current code, smt-snooze-delay functionality is completely broken.
    It makes sense to remove smt-snooze-delay from idle driver with the
    coming of cpuidle framework.
    However, sysfs files are retained as ppc64_util currently
    utilises it. Once we fix ppc64_util, propose to clean
    up the kernel code.
    
    Signed-off-by: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index c2c0f4478be3..b62de43ae5f3 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -450,13 +450,6 @@ enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 
 extern int powersave_nap;	/* set if nap mode can be used in idle loop */
 extern void power7_nap(void);
-
-#ifdef CONFIG_PSERIES_CPUIDLE
-extern void update_smt_snooze_delay(int cpu, int residency);
-#else
-static inline void update_smt_snooze_delay(int cpu, int residency) {}
-#endif
-
 extern void flush_instruction_cache(void);
 extern void hard_reset_now(void);
 extern void poweroff_now(void);

commit 962e7bd4976516c34fc9ef51d536aab801980767
Author: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
Date:   Tue Jan 14 16:26:02 2014 +0530

    powerpc/pseries/cpuidle: Move processor_idle.c to drivers/cpuidle.
    
    Move the file from arch specific pseries/processor_idle.c
    to drivers/cpuidle/cpuidle-pseries.c
    Make the relevant Makefile and Kconfig changes.
    Also, introduce Kconfig.powerpc in drivers/cpuidle
    for all powerpc cpuidle drivers.
    
    Signed-off-by: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 8ca20ac28dc2..c2c0f4478be3 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -451,7 +451,7 @@ enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 extern int powersave_nap;	/* set if nap mode can be used in idle loop */
 extern void power7_nap(void);
 
-#ifdef CONFIG_PSERIES_IDLE
+#ifdef CONFIG_PSERIES_CPUIDLE
 extern void update_smt_snooze_delay(int cpu, int residency);
 #else
 static inline void update_smt_snooze_delay(int cpu, int residency) {}

commit fac515db45207718168cb55ca4d0a390e43b61af
Merge: 3ac8ff1c475b d064f30e5063
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Jan 15 14:22:35 2014 +1100

    Merge remote-tracking branch 'scott/next' into next
    
    Freescale updates from Scott:
    
    <<
    Highlights include 32-bit booke relocatable support, e6500 hardware
    tablewalk support, various e500 SPE fixes, some new/revived boards, and
    e6500 deeper idle and altivec powerdown modes.
    >>

commit d31626f70b6103f4d9153b75d07e0e8795728cc9
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jan 13 15:56:29 2014 +1100

    powerpc: Don't corrupt transactional state when using FP/VMX in kernel
    
    Currently, when we have a process using the transactional memory
    facilities on POWER8 (that is, the processor is in transactional
    or suspended state), and the process enters the kernel and the
    kernel then uses the floating-point or vector (VMX/Altivec) facility,
    we end up corrupting the user-visible FP/VMX/VSX state.  This
    happens, for example, if a page fault causes a copy-on-write
    operation, because the copy_page function will use VMX to do the
    copy on POWER8.  The test program below demonstrates the bug.
    
    The bug happens because when FP/VMX state for a transactional process
    is stored in the thread_struct, we store the checkpointed state in
    .fp_state/.vr_state and the transactional (current) state in
    .transact_fp/.transact_vr.  However, when the kernel wants to use
    FP/VMX, it calls enable_kernel_fp() or enable_kernel_altivec(),
    which saves the current state in .fp_state/.vr_state.  Furthermore,
    when we return to the user process we return with FP/VMX/VSX
    disabled.  The next time the process uses FP/VMX/VSX, we don't know
    which set of state (the current register values, .fp_state/.vr_state,
    or .transact_fp/.transact_vr) we should be using, since we have no
    way to tell if we are still in the same transaction, and if not,
    whether the previous transaction succeeded or failed.
    
    Thus it is necessary to strictly adhere to the rule that if FP has
    been enabled at any point in a transaction, we must keep FP enabled
    for the user process with the current transactional state in the
    FP registers, until we detect that it is no longer in a transaction.
    Similarly for VMX; once enabled it must stay enabled until the
    process is no longer transactional.
    
    In order to keep this rule, we add a new thread_info flag which we
    test when returning from the kernel to userspace, called TIF_RESTORE_TM.
    This flag indicates that there is FP/VMX/VSX state to be restored
    before entering userspace, and when it is set the .tm_orig_msr field
    in the thread_struct indicates what state needs to be restored.
    The restoration is done by restore_tm_state().  The TIF_RESTORE_TM
    bit is set by new giveup_fpu/altivec_maybe_transactional helpers,
    which are called from enable_kernel_fp/altivec, giveup_vsx, and
    flush_fp/altivec_to_thread instead of giveup_fpu/altivec.
    
    The other thing to be done is to get the transactional FP/VMX/VSX
    state from .fp_state/.vr_state when doing reclaim, if that state
    has been saved there by giveup_fpu/altivec_maybe_transactional.
    Having done this, we set the FP/VMX bit in the thread's MSR after
    reclaim to indicate that that part of the state is now valid
    (having been reclaimed from the processor's checkpointed state).
    
    Finally, in the signal handling code, we move the clearing of the
    transactional state bits in the thread's MSR a bit earlier, before
    calling flush_fp_to_thread(), so that we don't unnecessarily set
    the TIF_RESTORE_TM bit.
    
    This is the test program:
    
    /* Michael Neuling 4/12/2013
     *
     * See if the altivec state is leaked out of an aborted transaction due to
     * kernel vmx copy loops.
     *
     *   gcc -m64 htm_vmxcopy.c -o htm_vmxcopy
     *
     */
    
    /* We don't use all of these, but for reference: */
    
    int main(int argc, char *argv[])
    {
            long double vecin = 1.3;
            long double vecout;
            unsigned long pgsize = getpagesize();
            int i;
            int fd;
            int size = pgsize*16;
            char tmpfile[] = "/tmp/page_faultXXXXXX";
            char buf[pgsize];
            char *a;
            uint64_t aborted = 0;
    
            fd = mkstemp(tmpfile);
            assert(fd >= 0);
    
            memset(buf, 0, pgsize);
            for (i = 0; i < size; i += pgsize)
                    assert(write(fd, buf, pgsize) == pgsize);
    
            unlink(tmpfile);
    
            a = mmap(NULL, size, PROT_READ|PROT_WRITE, MAP_PRIVATE, fd, 0);
            assert(a != MAP_FAILED);
    
            asm __volatile__(
                    "lxvd2x 40,0,%[vecinptr] ; " // set 40 to initial value
                    TBEGIN
                    "beq    3f ;"
                    TSUSPEND
                    "xxlxor 40,40,40 ; " // set 40 to 0
                    "std    5, 0(%[map]) ;" // cause kernel vmx copy page
                    TABORT
                    TRESUME
                    TEND
                    "li     %[res], 0 ;"
                    "b      5f ;"
                    "3: ;" // Abort handler
                    "li     %[res], 1 ;"
                    "5: ;"
                    "stxvd2x 40,0,%[vecoutptr] ; "
                    : [res]"=r"(aborted)
                    : [vecinptr]"r"(&vecin),
                      [vecoutptr]"r"(&vecout),
                      [map]"r"(a)
                    : "memory", "r0", "r3", "r4", "r5", "r6", "r7");
    
            if (aborted && (vecin != vecout)){
                    printf("FAILED: vector state leaked on abort %f != %f\n",
                           (double)vecin, (double)vecout);
                    exit(1);
            }
    
            munmap(a, size);
    
            close(fd);
    
            printf("PASSED!\n");
            return 0;
    }
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index fc14a38c7ccf..232a2fa5b483 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -373,6 +373,8 @@ extern int set_endian(struct task_struct *tsk, unsigned int val);
 extern int get_unalign_ctl(struct task_struct *tsk, unsigned long adr);
 extern int set_unalign_ctl(struct task_struct *tsk, unsigned int val);
 
+extern void fp_enable(void);
+extern void vec_enable(void);
 extern void load_fp_state(struct thread_fp_state *fp);
 extern void store_fp_state(struct thread_fp_state *fp);
 extern void load_vr_state(struct thread_vr_state *vr);

commit 640e922501103aaf2e0abb4cf4de5d49fa8342f7
Author: Joseph Myers <joseph@codesourcery.com>
Date:   Tue Dec 10 23:07:45 2013 +0000

    powerpc: fix exception clearing in e500 SPE float emulation
    
    The e500 SPE floating-point emulation code clears existing exceptions
    (__FPU_FPSCR &= ~FP_EX_MASK;) before ORing in the exceptions from the
    emulated operation.  However, these exception bits are the "sticky",
    cumulative exception bits, and should only be cleared by the user
    program setting SPEFSCR, not implicitly by any floating-point
    instruction (whether executed purely by the hardware or emulated).
    The spurious clearing of these bits shows up as missing exceptions in
    glibc testing.
    
    Fixing this, however, is not as simple as just not clearing the bits,
    because while the bits may be from previous floating-point operations
    (in which case they should not be cleared), the processor can also set
    the sticky bits itself before the interrupt for an exception occurs,
    and this can happen in cases when IEEE 754 semantics are that the
    sticky bit should not be set.  Specifically, the "invalid" sticky bit
    is set in various cases with non-finite operands, where IEEE 754
    semantics do not involve raising such an exception, and the
    "underflow" sticky bit is set in cases of exact underflow, whereas
    IEEE 754 semantics are that this flag is set only for inexact
    underflow.  Thus, for correct emulation the kernel needs to know the
    setting of these two sticky bits before the instruction being
    emulated.
    
    When a floating-point operation raises an exception, the kernel can
    note the state of the sticky bits immediately afterwards.  Some
    <fenv.h> functions that affect the state of these bits, such as
    fesetenv and feholdexcept, need to use prctl with PR_GET_FPEXC and
    PR_SET_FPEXC anyway, and so it is natural to record the state of those
    bits during that call into the kernel and so avoid any need for a
    separate call into the kernel to inform it of a change to those bits.
    Thus, the interface I chose to use (in this patch and the glibc port)
    is that one of those prctl calls must be made after any userspace
    change to those sticky bits, other than through a floating-point
    operation that traps into the kernel anyway.  feclearexcept and
    fesetexceptflag duly make those calls, which would not be required
    were it not for this issue.
    
    The previous EGLIBC port, and the uClibc code copied from it, is
    fundamentally broken as regards any use of prctl for floating-point
    exceptions because it didn't use the PR_FP_EXC_SW_ENABLE bit in its
    prctl calls (and did various worse things, such as passing a pointer
    when prctl expected an integer).  If you avoid anything where prctl is
    used, the clearing of sticky bits still means it will never give
    anything approximating correct exception semantics with existing
    kernels.  I don't believe the patch makes things any worse for
    existing code that doesn't try to inform the kernel of changes to
    sticky bits - such code may get incorrect exceptions in some cases,
    but it would have done so anyway in other cases.
    
    Signed-off-by: Joseph Myers <joseph@codesourcery.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index fc14a38c7ccf..91441d9cbaae 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -256,6 +256,8 @@ struct thread_struct {
 	unsigned long	evr[32];	/* upper 32-bits of SPE regs */
 	u64		acc;		/* Accumulator */
 	unsigned long	spefscr;	/* SPE & eFP status */
+	unsigned long	spefscr_last;	/* SPEFSCR value on last prctl
+					   call or trap return */
 	int		used_spe;	/* set if process has used spe */
 #endif /* CONFIG_SPE */
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
@@ -317,7 +319,9 @@ struct thread_struct {
 	(_ALIGN_UP(sizeof(init_thread_info), 16) + (unsigned long) &init_stack)
 
 #ifdef CONFIG_SPE
-#define SPEFSCR_INIT .spefscr = SPEFSCR_FINVE | SPEFSCR_FDBZE | SPEFSCR_FUNFE | SPEFSCR_FOVFE,
+#define SPEFSCR_INIT \
+	.spefscr = SPEFSCR_FINVE | SPEFSCR_FDBZE | SPEFSCR_FUNFE | SPEFSCR_FOVFE, \
+	.spefscr_last = SPEFSCR_FINVE | SPEFSCR_FDBZE | SPEFSCR_FUNFE | SPEFSCR_FOVFE,
 #else
 #define SPEFSCR_INIT
 #endif

commit f080480488028bcc25357f85e8ae54ccc3bb7173
Merge: eda670c626a4 e504c9098ed6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 15 13:51:36 2013 +0900

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM changes from Paolo Bonzini:
     "Here are the 3.13 KVM changes.  There was a lot of work on the PPC
      side: the HV and emulation flavors can now coexist in a single kernel
      is probably the most interesting change from a user point of view.
    
      On the x86 side there are nested virtualization improvements and a few
      bugfixes.
    
      ARM got transparent huge page support, improved overcommit, and
      support for big endian guests.
    
      Finally, there is a new interface to connect KVM with VFIO.  This
      helps with devices that use NoSnoop PCI transactions, letting the
      driver in the guest execute WBINVD instructions.  This includes some
      nVidia cards on Windows, that fail to start without these patches and
      the corresponding userspace changes"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (146 commits)
      kvm, vmx: Fix lazy FPU on nested guest
      arm/arm64: KVM: PSCI: propagate caller endianness to the incoming vcpu
      arm/arm64: KVM: MMIO support for BE guest
      kvm, cpuid: Fix sparse warning
      kvm: Delete prototype for non-existent function kvm_check_iopl
      kvm: Delete prototype for non-existent function complete_pio
      hung_task: add method to reset detector
      pvclock: detect watchdog reset at pvclock read
      kvm: optimize out smp_mb after srcu_read_unlock
      srcu: API for barrier after srcu read unlock
      KVM: remove vm mmap method
      KVM: IOMMU: hva align mapping page size
      KVM: x86: trace cpuid emulation when called from emulator
      KVM: emulator: cleanup decode_register_operand() a bit
      KVM: emulator: check rex prefix inside decode_register()
      KVM: x86: fix emulation of "movzbl %bpl, %eax"
      kvm_host: typo fix
      KVM: x86: emulate SAHF instruction
      MAINTAINERS: add tree for kvm.git
      Documentation/kvm: add a 00-INDEX file
      ...

commit 95f328d3ad1a8e4e3175a18546fb35c495e31130
Merge: daf727225b8a a78b55d1c021
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Nov 4 10:20:57 2013 +0200

    Merge branch 'kvm-ppc-queue' of git://github.com/agraf/linux-2.6 into queue
    
    Conflicts:
            arch/powerpc/include/asm/processor.h

commit 51ae8d4a2b9e4aa9a502061b9c39168e08829b94
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Thu Jul 4 11:45:46 2013 +0530

    powerpc: move debug registers in a structure
    
    This way we can use same data type struct with KVM and
    also help in using other debug related function.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Acked-by: Michael Neuling <mikey@neuling.org>
    [scottwood@freescale.com: removed obvious debug_reg comment]
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index c1583070937d..7794b2b04eb2 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -167,21 +167,7 @@ struct thread_vr_state {
 	vector128	vscr __attribute__((aligned(16)));
 };
 
-struct thread_struct {
-	unsigned long	ksp;		/* Kernel stack pointer */
-#ifdef CONFIG_PPC64
-	unsigned long	ksp_vsid;
-#endif
-	struct pt_regs	*regs;		/* Pointer to saved register state */
-	mm_segment_t	fs;		/* for get_fs() validation */
-#ifdef CONFIG_BOOKE
-	/* BookE base exception scratch space; align on cacheline */
-	unsigned long	normsave[8] ____cacheline_aligned;
-#endif
-#ifdef CONFIG_PPC32
-	void		*pgdir;		/* root of page-table tree */
-	unsigned long	ksp_limit;	/* if ksp <= ksp_limit stack overflow */
-#endif
+struct debug_reg {
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	/*
 	 * The following help to manage the use of Debug Control Registers
@@ -218,6 +204,24 @@ struct thread_struct {
 	unsigned long	dvc2;
 #endif
 #endif
+};
+
+struct thread_struct {
+	unsigned long	ksp;		/* Kernel stack pointer */
+#ifdef CONFIG_PPC64
+	unsigned long	ksp_vsid;
+#endif
+	struct pt_regs	*regs;		/* Pointer to saved register state */
+	mm_segment_t	fs;		/* for get_fs() validation */
+#ifdef CONFIG_BOOKE
+	/* BookE base exception scratch space; align on cacheline */
+	unsigned long	normsave[8] ____cacheline_aligned;
+#endif
+#ifdef CONFIG_PPC32
+	void		*pgdir;		/* root of page-table tree */
+	unsigned long	ksp_limit;	/* if ksp <= ksp_limit stack overflow */
+#endif
+	struct debug_reg debug;
 	struct thread_fp_state	fp_state;
 	struct thread_fp_state	*fp_save_area;
 	int		fpexc_mode;	/* floating-point exception mode */

commit 95791988fec645d196e746fcc0e329e19f7b1347
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed Jun 26 11:12:22 2013 +0530

    powerpc: move debug registers in a structure
    
    This way we can use same data type struct with KVM and
    also help in using other debug related function.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index e378cccfca55..b43844442a6c 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -147,22 +147,7 @@ typedef struct {
 #define TS_FPR(i) fpr[i][TS_FPROFFSET]
 #define TS_TRANS_FPR(i) transact_fpr[i][TS_FPROFFSET]
 
-struct thread_struct {
-	unsigned long	ksp;		/* Kernel stack pointer */
-	unsigned long	ksp_limit;	/* if ksp <= ksp_limit stack overflow */
-
-#ifdef CONFIG_PPC64
-	unsigned long	ksp_vsid;
-#endif
-	struct pt_regs	*regs;		/* Pointer to saved register state */
-	mm_segment_t	fs;		/* for get_fs() validation */
-#ifdef CONFIG_BOOKE
-	/* BookE base exception scratch space; align on cacheline */
-	unsigned long	normsave[8] ____cacheline_aligned;
-#endif
-#ifdef CONFIG_PPC32
-	void		*pgdir;		/* root of page-table tree */
-#endif
+struct debug_reg {
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	/*
 	 * The following help to manage the use of Debug Control Registers
@@ -199,6 +184,27 @@ struct thread_struct {
 	unsigned long	dvc2;
 #endif
 #endif
+};
+
+struct thread_struct {
+	unsigned long	ksp;		/* Kernel stack pointer */
+	unsigned long	ksp_limit;	/* if ksp <= ksp_limit stack overflow */
+
+#ifdef CONFIG_PPC64
+	unsigned long	ksp_vsid;
+#endif
+	struct pt_regs	*regs;		/* Pointer to saved register state */
+	mm_segment_t	fs;		/* for get_fs() validation */
+#ifdef CONFIG_BOOKE
+	/* BookE base exception scratch space; align on cacheline */
+	unsigned long	normsave[8] ____cacheline_aligned;
+#endif
+#ifdef CONFIG_PPC32
+	void		*pgdir;		/* root of page-table tree */
+#endif
+	/* Debug Registers */
+	struct debug_reg debug;
+
 	/* FP and VSX 0-31 register set */
 	double		fpr[32][TS_FPRWIDTH] __attribute__((aligned(16)));
 	struct {

commit 3ad26e5c4459d3793ad65bc8929037c70515df83
Merge: 5293bf97a27e 18461960cbf5
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Oct 11 18:23:53 2013 +1100

    Merge branch 'for-kvm' into next
    
    Topic branch for commits that the KVM tree might want to pull
    in separately.
    
    Hand merged a few files due to conflicts with the LE stuff
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 18461960cbf50bf345ef0667d45d5f64de8fb893
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 10 20:21:10 2013 +1000

    powerpc: Provide for giveup_fpu/altivec to save state in alternate location
    
    This provides a facility which is intended for use by KVM, where the
    contents of the FP/VSX and VMX (Altivec) registers can be saved away
    to somewhere other than the thread_struct when kernel code wants to
    use floating point or VMX instructions.  This is done by providing a
    pointer in the thread_struct to indicate where the state should be
    saved to.  The giveup_fpu() and giveup_altivec() functions test these
    pointers and save state to the indicated location if they are non-NULL.
    Note that the MSR_FP/VEC bits in task->thread.regs->msr are still used
    to indicate whether the CPU register state is live, even when an
    alternate save location is being used.
    
    This also provides load_fp_state() and load_vr_state() functions, which
    load up FP/VSX and VMX state from memory into the CPU registers, and
    corresponding store_fp_state() and store_vr_state() functions, which
    store FP/VSX and VMX state into memory from the CPU registers.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index afe695e9feb8..ea88e7bd4a34 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -211,6 +211,7 @@ struct thread_struct {
 #endif
 #endif
 	struct thread_fp_state	fp_state;
+	struct thread_fp_state	*fp_save_area;
 	int		fpexc_mode;	/* floating-point exception mode */
 	unsigned int	align_ctl;	/* alignment handling control */
 #ifdef CONFIG_PPC64
@@ -229,6 +230,7 @@ struct thread_struct {
 	unsigned long	trap_nr;	/* last trap # on this thread */
 #ifdef CONFIG_ALTIVEC
 	struct thread_vr_state vr_state;
+	struct thread_vr_state *vr_save_area;
 	unsigned long	vrsave;
 	int		used_vr;	/* set if process has used altivec */
 #endif /* CONFIG_ALTIVEC */
@@ -357,6 +359,11 @@ extern int set_endian(struct task_struct *tsk, unsigned int val);
 extern int get_unalign_ctl(struct task_struct *tsk, unsigned long adr);
 extern int set_unalign_ctl(struct task_struct *tsk, unsigned int val);
 
+extern void load_fp_state(struct thread_fp_state *fp);
+extern void store_fp_state(struct thread_fp_state *fp);
+extern void load_vr_state(struct thread_vr_state *vr);
+extern void store_vr_state(struct thread_vr_state *vr);
+
 static inline unsigned int __unpack_fe01(unsigned long msr_bits)
 {
 	return ((msr_bits & MSR_FE0) >> 10) | ((msr_bits & MSR_FE1) >> 8);

commit de79f7b9f6f92ec1bd6f61fa1f20de60728a5b5e
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 10 20:20:42 2013 +1000

    powerpc: Put FP/VSX and VR state into structures
    
    This creates new 'thread_fp_state' and 'thread_vr_state' structures
    to store FP/VSX state (including FPSCR) and Altivec/VSX state
    (including VSCR), and uses them in the thread_struct.  In the
    thread_fp_state, the FPRs and VSRs are represented as u64 rather
    than double, since we rarely perform floating-point computations
    on the values, and this will enable the structures to be used
    in KVM code as well.  Similarly FPSCR is now a u64 rather than
    a structure of two 32-bit values.
    
    This takes the offsets out of the macros such as SAVE_32FPRS,
    REST_32FPRS, etc.  This enables the same macros to be used for normal
    and transactional state, enabling us to delete the transactional
    versions of the macros.   This also removes the unused do_load_up_fpu
    and do_load_up_altivec, which were in fact buggy since they didn't
    create large enough stack frames to account for the fact that
    load_up_fpu and load_up_altivec are not designed to be called from C
    and assume that their caller's stack frame is an interrupt frame.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index ce4de5aed7b5..afe695e9feb8 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -144,8 +144,20 @@ typedef struct {
 
 #define TS_FPROFFSET 0
 #define TS_VSRLOWOFFSET 1
-#define TS_FPR(i) fpr[i][TS_FPROFFSET]
-#define TS_TRANS_FPR(i) transact_fpr[i][TS_FPROFFSET]
+#define TS_FPR(i) fp_state.fpr[i][TS_FPROFFSET]
+#define TS_TRANS_FPR(i) transact_fp.fpr[i][TS_FPROFFSET]
+
+/* FP and VSX 0-31 register set */
+struct thread_fp_state {
+	u64	fpr[32][TS_FPRWIDTH] __attribute__((aligned(16)));
+	u64	fpscr;		/* Floating point status */
+};
+
+/* Complete AltiVec register set including VSCR */
+struct thread_vr_state {
+	vector128	vr[32] __attribute__((aligned(16)));
+	vector128	vscr __attribute__((aligned(16)));
+};
 
 struct thread_struct {
 	unsigned long	ksp;		/* Kernel stack pointer */
@@ -198,13 +210,7 @@ struct thread_struct {
 	unsigned long	dvc2;
 #endif
 #endif
-	/* FP and VSX 0-31 register set */
-	double		fpr[32][TS_FPRWIDTH] __attribute__((aligned(16)));
-	struct {
-
-		unsigned int pad;
-		unsigned int val;	/* Floating point status */
-	} fpscr;
+	struct thread_fp_state	fp_state;
 	int		fpexc_mode;	/* floating-point exception mode */
 	unsigned int	align_ctl;	/* alignment handling control */
 #ifdef CONFIG_PPC64
@@ -222,10 +228,7 @@ struct thread_struct {
 	struct arch_hw_breakpoint hw_brk; /* info on the hardware breakpoint */
 	unsigned long	trap_nr;	/* last trap # on this thread */
 #ifdef CONFIG_ALTIVEC
-	/* Complete AltiVec register set */
-	vector128	vr[32] __attribute__((aligned(16)));
-	/* AltiVec status */
-	vector128	vscr __attribute__((aligned(16)));
+	struct thread_vr_state vr_state;
 	unsigned long	vrsave;
 	int		used_vr;	/* set if process has used altivec */
 #endif /* CONFIG_ALTIVEC */
@@ -262,13 +265,8 @@ struct thread_struct {
 	 * transact_fpr[] is the new set of transactional values.
 	 * VRs work the same way.
 	 */
-	double		transact_fpr[32][TS_FPRWIDTH];
-	struct {
-		unsigned int pad;
-		unsigned int val;	/* Floating point status */
-	} transact_fpscr;
-	vector128	transact_vr[32] __attribute__((aligned(16)));
-	vector128	transact_vscr __attribute__((aligned(16)));
+	struct thread_fp_state transact_fp;
+	struct thread_vr_state transact_vr;
 	unsigned long	transact_vrsave;
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
@@ -322,8 +320,6 @@ struct thread_struct {
 	.ksp = INIT_SP, \
 	.regs = (struct pt_regs *)INIT_SP - 1, /* XXX bogus, I think */ \
 	.fs = KERNEL_DS, \
-	.fpr = {{0}}, \
-	.fpscr = { .val = 0, }, \
 	.fpexc_mode = 0, \
 	.ppr = INIT_PPR, \
 }

commit e156bd8ad76939a9bcd66d85cf06f8cde1fb8030
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Sep 23 12:04:37 2013 +1000

    powerpc: Fix offset of FPRs in VSX registers in little endian builds
    
    The FPRs overlap the high doublewords of the first 32 VSX registers.
    Fix TS_FPROFFSET and TS_VSRLOWOFFSET so we access the correct fields
    in little endian mode.
    
    If VSX is disabled the FPRs are only one doubleword in length so
    TS_FPROFFSET needs adjusting in little endian.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index ce4de5aed7b5..82c6ee9df9a1 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -14,8 +14,18 @@
 
 #ifdef CONFIG_VSX
 #define TS_FPRWIDTH 2
+
+#ifdef __BIG_ENDIAN__
+#define TS_FPROFFSET 0
+#define TS_VSRLOWOFFSET 1
+#else
+#define TS_FPROFFSET 1
+#define TS_VSRLOWOFFSET 0
+#endif
+
 #else
 #define TS_FPRWIDTH 1
+#define TS_FPROFFSET 0
 #endif
 
 #ifdef CONFIG_PPC64
@@ -142,8 +152,6 @@ typedef struct {
 	unsigned long seg;
 } mm_segment_t;
 
-#define TS_FPROFFSET 0
-#define TS_VSRLOWOFFSET 1
 #define TS_FPR(i) fpr[i][TS_FPROFFSET]
 #define TS_TRANS_FPR(i) transact_fpr[i][TS_FPROFFSET]
 

commit cbc9565ee82694dec31d8137dec975b83175183b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Sep 24 15:17:21 2013 +1000

    powerpc: Remove ksp_limit on ppc64
    
    We've been keeping that field in thread_struct for a while, it contains
    the "limit" of the current stack pointer and is meant to be used for
    detecting stack overflows.
    
    It has a few problems however:
    
     - First, it was never actually *used* on 64-bit. Set and updated but
    not actually exploited
    
     - When switching stack to/from irq and softirq stacks, it's update
    is racy unless we hard disable interrupts, which is costly. This
    is fine on 32-bit as we don't soft-disable there but not on 64-bit.
    
    Thus rather than fixing 2 in order to implement 1 in some hypothetical
    future, let's remove the code completely from 64-bit. In order to avoid
    a clutter of ifdef's, we remove the updates from C code completely
    during interrupt stack switching, and instead maintain it from the
    asm helper that is used to do the stack switching in the first place.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index e378cccfca55..ce4de5aed7b5 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -149,8 +149,6 @@ typedef struct {
 
 struct thread_struct {
 	unsigned long	ksp;		/* Kernel stack pointer */
-	unsigned long	ksp_limit;	/* if ksp <= ksp_limit stack overflow */
-
 #ifdef CONFIG_PPC64
 	unsigned long	ksp_vsid;
 #endif
@@ -162,6 +160,7 @@ struct thread_struct {
 #endif
 #ifdef CONFIG_PPC32
 	void		*pgdir;		/* root of page-table tree */
+	unsigned long	ksp_limit;	/* if ksp <= ksp_limit stack overflow */
 #endif
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	/*
@@ -321,7 +320,6 @@ struct thread_struct {
 #else
 #define INIT_THREAD  { \
 	.ksp = INIT_SP, \
-	.ksp_limit = INIT_SP_LIMIT, \
 	.regs = (struct pt_regs *)INIT_SP - 1, /* XXX bogus, I think */ \
 	.fs = KERNEL_DS, \
 	.fpr = {{0}}, \

commit 28e61cc466d8daace4b0f04ba2b83e0bd68f5832
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Aug 9 17:29:31 2013 +1000

    powerpc/tm: Fix context switching TAR, PPR and DSCR SPRs
    
    If a transaction is rolled back, the Target Address Register (TAR), Processor
    Priority Register (PPR) and Data Stream Control Register (DSCR) should be
    restored to the checkpointed values before the transaction began.  Any changes
    to these SPRs inside the transaction should not be visible in the abort
    handler.
    
    Currently Linux doesn't save or restore the checkpointed TAR, PPR or DSCR.  If
    we preempt a processes inside a transaction which has modified any of these, on
    process restore, that same transaction may be aborted we but we won't see the
    checkpointed versions of these SPRs.
    
    This adds checkpointed versions of these SPRs to the thread_struct and adds the
    save/restore of these three SPRs to the treclaim/trechkpt code.
    
    Without this if any of these SPRs are modified during a transaction, users may
    incorrectly see a speculated SPR value even if the transaction is aborted.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: <stable@vger.kernel.org> [v3.10]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 47a35b08b963..e378cccfca55 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -247,6 +247,10 @@ struct thread_struct {
 	unsigned long	tm_orig_msr;	/* Thread's MSR on ctx switch */
 	struct pt_regs	ckpt_regs;	/* Checkpointed registers */
 
+	unsigned long	tm_tar;
+	unsigned long	tm_ppr;
+	unsigned long	tm_dscr;
+
 	/*
 	 * Transactional FP and VSX 0-31 register set.
 	 * NOTE: the sense of these is the opposite of the integer ckpt_regs!

commit 330a1eb7775ba876dbd46b9885556e57f705e3d4
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Jun 28 18:15:16 2013 +1000

    powerpc/perf: Core EBB support for 64-bit book3s
    
    Add support for EBB (Event Based Branches) on 64-bit book3s. See the
    included documentation for more details.
    
    EBBs are a feature which allows the hardware to branch directly to a
    specified user space address when a PMU event overflows. This can be
    used by programs for self-monitoring with no kernel involvement in the
    inner loop.
    
    Most of the logic is in the generic book3s code, primarily to avoid a
    proliferation of PMU callbacks.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 3f19df3cc7a3..47a35b08b963 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -287,8 +287,9 @@ struct thread_struct {
 	unsigned long	siar;
 	unsigned long	sdar;
 	unsigned long	sier;
-	unsigned long	mmcr0;
 	unsigned long	mmcr2;
+	unsigned 	mmcr0;
+	unsigned 	used_ebb;
 #endif
 };
 

commit 2ac138ca21ad26c988ce7c91d27327f85beb7519
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Jun 28 18:15:15 2013 +1000

    powerpc/perf: Drop MMCRA from thread_struct
    
    In commit 59affcd "Context switch more PMU related SPRs" I added more
    PMU SPRs to thread_struct, later modified in commit b11ae95. To add
    insult to injury it turns out we don't need to switch MMCRA as it's
    only user readable, and the value is recomputed by the PMU code.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 9fe1129efa02..3f19df3cc7a3 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -289,7 +289,6 @@ struct thread_struct {
 	unsigned long	sier;
 	unsigned long	mmcr0;
 	unsigned long	mmcr2;
-	unsigned long	mmcra;
 #endif
 };
 

commit 475e68cfdde39e6d95055999b0cb42fdb2bea0ca
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Jun 5 13:02:26 2013 +1000

    powerpc: Align thread->fpr to 16 bytes
    
    On newer CPUs we use VSX loads and stores to the thread->fpr array.
    For best performance we need to ensure 16 byte alignment.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 2b5a39c67df4..9fe1129efa02 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -200,7 +200,7 @@ struct thread_struct {
 #endif
 #endif
 	/* FP and VSX 0-31 register set */
-	double		fpr[32][TS_FPRWIDTH];
+	double		fpr[32][TS_FPRWIDTH] __attribute__((aligned(16)));
 	struct {
 
 		unsigned int pad;

commit d8899bb2be91b3a19ebf82b138232919ffcf833a
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed May 22 09:50:58 2013 +0530

    powerpc: Debug control and status registers are 32bit
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 7135a257f7cb..2b5a39c67df4 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -168,10 +168,10 @@ struct thread_struct {
 	 * The following help to manage the use of Debug Control Registers
 	 * om the BookE platforms.
 	 */
-	unsigned long	dbcr0;
-	unsigned long	dbcr1;
+	uint32_t	dbcr0;
+	uint32_t	dbcr1;
 #ifdef CONFIG_BOOKE
-	unsigned long	dbcr2;
+	uint32_t	dbcr2;
 #endif
 	/*
 	 * The stored value of the DBSR register will be the value at the
@@ -179,7 +179,7 @@ struct thread_struct {
 	 * user (will never be written to) and has value while helping to
 	 * describe the reason for the last debug trap.  Torez
 	 */
-	unsigned long	dbsr;
+	uint32_t	dbsr;
 	/*
 	 * The following will contain addresses used by debug applications
 	 * to help trace and trap on particular address locations.

commit d5d8ec895ca599fbde43efe3a2f9714315e3d298
Author: Daniel Walker <dwalker@fifo99.com>
Date:   Tue Apr 23 17:50:33 2013 -0700

    powerpc/mm: Make mmap_64.c compile on 32bit powerpc
    
    There appears to be no good reason to keep this as 64bit only. It works
    on 32bit also, and has checks so that it can work correctly with 32bit
    binaries on 64bit hardware which is why I think this works.
    
    I tested this on qemu using the virtex-ml507 machine type.
    
    Before,
    
    /bin2 # ./test & cat /proc/${!}/maps
    00100000-00103000 r-xp 00000000 00:00 0          [vdso]
    10000000-10007000 r-xp 00000000 00:01 454        /bin2/test
    10017000-10018000 rw-p 00007000 00:01 454        /bin2/test
    48000000-48020000 r-xp 00000000 00:01 224        /lib/ld-2.11.3.so
    48021000-48023000 rw-p 00021000 00:01 224        /lib/ld-2.11.3.so
    bfd03000-bfd24000 rw-p 00000000 00:00 0          [stack]
    /bin2 # ./test & cat /proc/${!}/maps
    00100000-00103000 r-xp 00000000 00:00 0          [vdso]
    0fe6e000-0ffd8000 r-xp 00000000 00:01 214        /lib/libc-2.11.3.so
    0ffd8000-0ffe8000 ---p 0016a000 00:01 214        /lib/libc-2.11.3.so
    0ffe8000-0ffed000 rw-p 0016a000 00:01 214        /lib/libc-2.11.3.so
    0ffed000-0fff0000 rw-p 00000000 00:00 0
    10000000-10007000 r-xp 00000000 00:01 454        /bin2/test
    10017000-10018000 rw-p 00007000 00:01 454        /bin2/test
    48000000-48020000 r-xp 00000000 00:01 224        /lib/ld-2.11.3.so
    48020000-48021000 rw-p 00000000 00:00 0
    48021000-48023000 rw-p 00021000 00:01 224        /lib/ld-2.11.3.so
    bf98a000-bf9ab000 rw-p 00000000 00:00 0          [stack]
    /bin2 # ./test & cat /proc/${!}/maps
    00100000-00103000 r-xp 00000000 00:00 0          [vdso]
    0fe6e000-0ffd8000 r-xp 00000000 00:01 214        /lib/libc-2.11.3.so
    0ffd8000-0ffe8000 ---p 0016a000 00:01 214        /lib/libc-2.11.3.so
    0ffe8000-0ffed000 rw-p 0016a000 00:01 214        /lib/libc-2.11.3.so
    0ffed000-0fff0000 rw-p 00000000 00:00 0
    10000000-10007000 r-xp 00000000 00:01 454        /bin2/test
    10017000-10018000 rw-p 00007000 00:01 454        /bin2/test
    48000000-48020000 r-xp 00000000 00:01 224        /lib/ld-2.11.3.so
    48020000-48021000 rw-p 00000000 00:00 0
    48021000-48023000 rw-p 00021000 00:01 224        /lib/ld-2.11.3.so
    bfa54000-bfa75000 rw-p 00000000 00:00 0          [stack]
    
    After,
    
    bash-4.1# ./test & cat /proc/${!}/maps
    [7] 803
    00100000-00103000 r-xp 00000000 00:00 0          [vdso]
    10000000-10007000 r-xp 00000000 00:01 454        /bin2/test
    10017000-10018000 rw-p 00007000 00:01 454        /bin2/test
    b7eb0000-b7ed0000 r-xp 00000000 00:01 224        /lib/ld-2.11.3.so
    b7ed1000-b7ed3000 rw-p 00021000 00:01 224        /lib/ld-2.11.3.so
    bfbc0000-bfbe1000 rw-p 00000000 00:00 0          [stack]
    bash-4.1# ./test & cat /proc/${!}/maps
    [8] 805
    00100000-00103000 r-xp 00000000 00:00 0          [vdso]
    10000000-10007000 r-xp 00000000 00:01 454        /bin2/test
    10017000-10018000 rw-p 00007000 00:01 454        /bin2/test
    b7b03000-b7b23000 r-xp 00000000 00:01 224        /lib/ld-2.11.3.so
    b7b24000-b7b26000 rw-p 00021000 00:01 224        /lib/ld-2.11.3.so
    bfc27000-bfc48000 rw-p 00000000 00:00 0          [stack]
    bash-4.1# ./test & cat /proc/${!}/maps
    [9] 807
    00100000-00103000 r-xp 00000000 00:00 0          [vdso]
    10000000-10007000 r-xp 00000000 00:01 454        /bin2/test
    10017000-10018000 rw-p 00007000 00:01 454        /bin2/test
    b7f37000-b7f57000 r-xp 00000000 00:01 224        /lib/ld-2.11.3.so
    b7f58000-b7f5a000 rw-p 00021000 00:01 224        /lib/ld-2.11.3.so
    bff96000-bffb7000 rw-p 00000000 00:00 0          [stack]
    
    Signed-off-by: Daniel Walker <dwalker@fifo90.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 14a658363698..7135a257f7cb 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -404,9 +404,7 @@ static inline void prefetchw(const void *x)
 
 #define spin_lock_prefetch(x)	prefetchw(x)
 
-#ifdef CONFIG_PPC64
 #define HAVE_ARCH_PICK_MMAP_LAYOUT
-#endif
 
 #ifdef CONFIG_PPC64
 static inline unsigned long get_clean_sp(unsigned long sp, int is_32)

commit 2b3f8e87cf99a33fb6faf5026d7147748bbd77b6
Author: Michael Neuling <mikey@neuling.org>
Date:   Sun May 26 18:09:41 2013 +0000

    powerpc/tm: Fix userspace stack corruption on signal delivery for active transactions
    
    When in an active transaction that takes a signal, we need to be careful with
    the stack.  It's possible that the stack has moved back up after the tbegin.
    The obvious case here is when the tbegin is called inside a function that
    returns before a tend.  In this case, the stack is part of the checkpointed
    transactional memory state.  If we write over this non transactionally or in
    suspend, we are in trouble because if we get a tm abort, the program counter
    and stack pointer will be back at the tbegin but our in memory stack won't be
    valid anymore.
    
    To avoid this, when taking a signal in an active transaction, we need to use
    the stack pointer from the checkpointed state, rather than the speculated
    state.  This ensures that the signal context (written tm suspended) will be
    written below the stack required for the rollback.  The transaction is aborted
    becuase of the treclaim, so any memory written between the tbegin and the
    signal will be rolled back anyway.
    
    For signals taken in non-TM or suspended mode, we use the
    normal/non-checkpointed stack pointer.
    
    Tested with 64 and 32 bit signals
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: <stable@vger.kernel.org> # v3.9
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 594db6bc093c..14a658363698 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -409,21 +409,16 @@ static inline void prefetchw(const void *x)
 #endif
 
 #ifdef CONFIG_PPC64
-static inline unsigned long get_clean_sp(struct pt_regs *regs, int is_32)
+static inline unsigned long get_clean_sp(unsigned long sp, int is_32)
 {
-	unsigned long sp;
-
 	if (is_32)
-		sp = regs->gpr[1] & 0x0ffffffffUL;
-	else
-		sp = regs->gpr[1];
-
+		return sp & 0x0ffffffffUL;
 	return sp;
 }
 #else
-static inline unsigned long get_clean_sp(struct pt_regs *regs, int is_32)
+static inline unsigned long get_clean_sp(unsigned long sp, int is_32)
 {
-	return regs->gpr[1];
+	return sp;
 }
 #endif
 

commit 59affcd3e460b97492bc1aa2b843bafe7c54f596
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue May 21 16:31:12 2013 +0000

    powerpc: Context switch more PMU related SPRs
    
    In commit 9353374 "Context switch the new EBB SPRs" we added support for
    context switching some new EBB SPRs. However despite four of us signing
    off on that patch we missed some. To be fair these are not actually new
    SPRs, but they are now potentially user accessible so need to be context
    switched.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index d7e67ca8b4a6..594db6bc093c 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -284,6 +284,12 @@ struct thread_struct {
 	unsigned long	ebbrr;
 	unsigned long	ebbhr;
 	unsigned long	bescr;
+	unsigned long	siar;
+	unsigned long	sdar;
+	unsigned long	sier;
+	unsigned long	mmcr0;
+	unsigned long	mmcr2;
+	unsigned long	mmcra;
 #endif
 };
 

commit 9353374b8e1585d5fa47a1e5c1d3e9155dd0eb7c
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Apr 30 20:17:04 2013 +0000

    powerpc: Context switch the new EBB SPRs
    
    This context switches the new Event Based Branching (EBB) SPRs.  The three new
    SPRs are:
      - Event Based Branch Handler Register (EBBHR)
      - Event Based Branch Return Register (EBBRR)
      - Branch Event Status and Control Register (BESCR)
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 0a4cc5d649e1..d7e67ca8b4a6 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -281,6 +281,9 @@ struct thread_struct {
 #endif
 #ifdef CONFIG_PPC_BOOK3S_64
 	unsigned long	tar;
+	unsigned long	ebbrr;
+	unsigned long	ebbhr;
+	unsigned long	bescr;
 #endif
 };
 

commit 933ee7119fb14156f46dc8bce8218f62db13c568
Author: Paul Bolle <pebolle@tiscali.nl>
Date:   Wed Mar 27 00:47:03 2013 +0000

    powerpc: remove PReP platform
    
    PPC_PREP is marked as BROKEN since v2.6.15. Remove all PReP specific
    code now.
    
    Signed-off-by: Paul Bolle <pebolle@tiscali.nl>
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 7ff9eaa3ea6c..0a4cc5d649e1 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -40,7 +40,7 @@
  * -- BenH.
  */
 
-/* PREP sub-platform types see residual.h for these */
+/* PREP sub-platform types. Unused */
 #define _PREP_Motorola	0x01	/* motorola prep */
 #define _PREP_Firm	0x02	/* firmworks prep */
 #define _PREP_IBM	0x00	/* ibm prep */
@@ -56,13 +56,6 @@
 
 extern int _chrp_type;
 
-#ifdef CONFIG_PPC_PREP
-
-/* what kind of prep workstation we are */
-extern int _prep_type;
-
-#endif /* CONFIG_PPC_PREP */
-
 #endif /* defined(__KERNEL__) && defined(CONFIG_PPC32) */
 
 /*

commit 8b3c34cf0e0ab334a24aad7367cd06a5ba09a898
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Feb 13 16:21:32 2013 +0000

    powerpc: New macros for transactional memory support
    
    This adds new macros for saving and restoring checkpointed architected state
    from and to the thread_struct.
    
    It also adds some debugging macros for when your brain explodes trying to debug
    your transactional memory enabled kernel.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index fc41ab3aa114..7ff9eaa3ea6c 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -152,6 +152,7 @@ typedef struct {
 #define TS_FPROFFSET 0
 #define TS_VSRLOWOFFSET 1
 #define TS_FPR(i) fpr[i][TS_FPROFFSET]
+#define TS_TRANS_FPR(i) transact_fpr[i][TS_FPROFFSET]
 
 struct thread_struct {
 	unsigned long	ksp;		/* Kernel stack pointer */

commit f4c3aff2230bfe696a0683073b77446248d7895c
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Feb 13 16:21:31 2013 +0000

    powerpc: Add additional state needed for transactional memory to thread struct
    
    Set of new archtected state for saving away on context switch.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 42ac53cebacd..fc41ab3aa114 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -246,6 +246,34 @@ struct thread_struct {
 	unsigned long	spefscr;	/* SPE & eFP status */
 	int		used_spe;	/* set if process has used spe */
 #endif /* CONFIG_SPE */
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+	u64		tm_tfhar;	/* Transaction fail handler addr */
+	u64		tm_texasr;	/* Transaction exception & summary */
+	u64		tm_tfiar;	/* Transaction fail instr address reg */
+	unsigned long	tm_orig_msr;	/* Thread's MSR on ctx switch */
+	struct pt_regs	ckpt_regs;	/* Checkpointed registers */
+
+	/*
+	 * Transactional FP and VSX 0-31 register set.
+	 * NOTE: the sense of these is the opposite of the integer ckpt_regs!
+	 *
+	 * When a transaction is active/signalled/scheduled etc., *regs is the
+	 * most recent set of/speculated GPRs with ckpt_regs being the older
+	 * checkpointed regs to which we roll back if transaction aborts.
+	 *
+	 * However, fpr[] is the checkpointed 'base state' of FP regs, and
+	 * transact_fpr[] is the new set of transactional values.
+	 * VRs work the same way.
+	 */
+	double		transact_fpr[32][TS_FPRWIDTH];
+	struct {
+		unsigned int pad;
+		unsigned int val;	/* Floating point status */
+	} transact_fpscr;
+	vector128	transact_vr[32] __attribute__((aligned(16)));
+	vector128	transact_vscr __attribute__((aligned(16)));
+	unsigned long	transact_vrsave;
+#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
 	void*		kvm_shadow_vcpu; /* KVM internal data */
 #endif /* CONFIG_KVM_BOOK3S_32_HANDLER */

commit 2468dcf641e4f3e1b0153e3e11ca20740b2f4ce8
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Thu Feb 7 15:46:58 2013 +0000

    powerpc: Add support for context switching the TAR register
    
    This patch adds support for enabling and context switching the Target
    Address Register in Power8. The TAR is a new special purpose register
    that can be used for computed branches with the bctar[l] (branch
    conditional to TAR) instruction in the same manner as the count and link
    registers.
    
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 7938658c168d..42ac53cebacd 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -257,6 +257,9 @@ struct thread_struct {
 	int		dscr_inherit;
 	unsigned long	ppr;	/* used to save/restore SMT priority */
 #endif
+#ifdef CONFIG_PPC_BOOK3S_64
+	unsigned long	tar;
+#endif
 };
 
 #define ARCH_MIN_TASKALIGN 16

commit 9422de3e953d0e60eb95f5430a9dd803eec1c6d7
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Dec 20 14:06:44 2012 +0000

    powerpc: Hardware breakpoints rewrite to handle non DABR breakpoint registers
    
    This is a rewrite so that we don't assume we are using the DABR throughout the
    code.  We now use the arch_hw_breakpoint to store the breakpoint in a generic
    manner in the thread_struct, rather than storing the raw DABR value.
    
    The ptrace GET/SET_DEBUGREG interface currently passes the raw DABR in from
    userspace.  We keep this functionality, so that future changes (like the POWER8
    DAWR), will still fake the DABR to userspace.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 37f87f069cbf..7938658c168d 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -33,6 +33,7 @@
 #include <linux/cache.h>
 #include <asm/ptrace.h>
 #include <asm/types.h>
+#include <asm/hw_breakpoint.h>
 
 /* We do _not_ want to define new machine types at all, those must die
  * in favor of using the device-tree
@@ -225,8 +226,7 @@ struct thread_struct {
 	struct perf_event *last_hit_ubp;
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
-	unsigned long	dabr;		/* Data address breakpoint register */
-	unsigned long	dabrx;		/*      ... extension  */
+	struct arch_hw_breakpoint hw_brk; /* info on the hardware breakpoint */
 	unsigned long	trap_nr;	/* last trap # on this thread */
 #ifdef CONFIG_ALTIVEC
 	/* Complete AltiVec register set */

commit 92779245599bb3d7fb48066b11c4bfd6aa477198
Author: Haren Myneni <haren@linux.vnet.ibm.com>
Date:   Thu Dec 6 21:49:56 2012 +0000

    powerpc: Define ppr in thread_struct
    
    [PATCH 4/6] powerpc: Define ppr in thread_struct
    
    ppr in thread_struct is used to save PPR and restore it before process exits
    from kernel.
    
    This patch sets the default priority to 3 when tasks are created such
    that users can use 4 for higher priority tasks.
    
    Signed-off-by: Haren Myneni <haren@us.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 87502046c0dc..37f87f069cbf 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -18,6 +18,16 @@
 #define TS_FPRWIDTH 1
 #endif
 
+#ifdef CONFIG_PPC64
+/* Default SMT priority is set to 3. Use 11- 13bits to save priority. */
+#define PPR_PRIORITY 3
+#ifdef __ASSEMBLY__
+#define INIT_PPR (PPR_PRIORITY << 50)
+#else
+#define INIT_PPR ((u64)PPR_PRIORITY << 50)
+#endif /* __ASSEMBLY__ */
+#endif /* CONFIG_PPC64 */
+
 #ifndef __ASSEMBLY__
 #include <linux/compiler.h>
 #include <linux/cache.h>
@@ -245,6 +255,7 @@ struct thread_struct {
 #ifdef CONFIG_PPC64
 	unsigned long	dscr;
 	int		dscr_inherit;
+	unsigned long	ppr;	/* used to save/restore SMT priority */
 #endif
 };
 
@@ -278,6 +289,7 @@ struct thread_struct {
 	.fpr = {{0}}, \
 	.fpscr = { .val = 0, }, \
 	.fpexc_mode = 0, \
+	.ppr = INIT_PPR, \
 }
 #endif
 

commit 8ea959a17fe6e27f7954dddad5b17b0e33f0d7ee
Author: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
Date:   Wed Oct 3 18:42:18 2012 +0000

    cpuidle/powerpc: Fix smt_snooze_delay functionality.
    
    smt_snooze_delay was designed to  delay idle loop's nap entry
    in the native idle code before it got  ported over to use as part of
    the cpuidle framework.
    
    A -ve value  assigned to smt_snooze_delay should result in
    busy looping, in other words disabling the entry to nap state.
    
            - https://lists.ozlabs.org/pipermail/linuxppc-dev/2010-May/082450.html
    
    This particular functionality can be achieved currently by
    echo 1 > /sys/devices/system/cpu/cpu*/state1/disable
    but it is broken when one assigns -ve value to  the smt_snooze_delay
    variable either via sysfs entry or ppc64_cpu util.
    
    This patch aims to fix this, by disabling nap state when smt_snooze_delay
    variable is set to -ve value.
    
    Signed-off-by: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 8734b3855272..87502046c0dc 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -388,9 +388,9 @@ extern int powersave_nap;	/* set if nap mode can be used in idle loop */
 extern void power7_nap(void);
 
 #ifdef CONFIG_PSERIES_IDLE
-extern void update_smt_snooze_delay(int snooze);
+extern void update_smt_snooze_delay(int cpu, int residency);
 #else
-static inline void update_smt_snooze_delay(int snooze) {}
+static inline void update_smt_snooze_delay(int cpu, int residency) {}
 #endif
 
 extern void flush_instruction_cache(void);

commit 8213a2f3eeafdecf06dd718cb4130372263f6067
Merge: 40924754f2ca 12f79be93d94
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 12 10:49:08 2012 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull pile 2 of execve and kernel_thread unification work from Al Viro:
     "Stuff in there: kernel_thread/kernel_execve/sys_execve conversions for
      several more architectures plus assorted signal fixes and cleanups.
    
      There'll be more (in particular, real fixes for the alpha
      do_notify_resume() irq mess)..."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (43 commits)
      alpha: don't open-code trace_report_syscall_{enter,exit}
      Uninclude linux/freezer.h
      m32r: trim masks
      avr32: trim masks
      tile: don't bother with SIGTRAP in setup_frame
      microblaze: don't bother with SIGTRAP in setup_rt_frame()
      mn10300: don't bother with SIGTRAP in setup_frame()
      frv: no need to raise SIGTRAP in setup_frame()
      x86: get rid of duplicate code in case of CONFIG_VM86
      unicore32: remove pointless test
      h8300: trim _TIF_WORK_MASK
      parisc: decide whether to go to slow path (tracesys) based on thread flags
      parisc: don't bother looping in do_signal()
      parisc: fix double restarts
      bury the rest of TIF_IRET
      sanitize tsk_is_polling()
      bury _TIF_RESTORE_SIGMASK
      unicore32: unobfuscate _TIF_WORK_MASK
      mips: NOTIFY_RESUME is not needed in TIF masks
      mips: merge the identical "return from syscall" per-ABI code
      ...
    
    Conflicts:
            arch/arm/include/asm/thread_info.h

commit 58254e1002a82eb383c5977ad9fd5a451b91fe29
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Sep 12 18:32:42 2012 -0400

    powerpc: split ret_from_fork
    
    ... and get rid of in-kernel syscalls in kernel_thread()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 54b73a28c205..5376453d90cc 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -74,9 +74,6 @@ struct task_struct;
 void start_thread(struct pt_regs *regs, unsigned long fdptr, unsigned long sp);
 void release_thread(struct task_struct *);
 
-/* Create a new kernel thread. */
-extern long kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
-
 /* Lazy FPU handling on uni-processor */
 extern struct task_struct *last_task_used_math;
 extern struct task_struct *last_task_used_altivec;

commit 048ee0993ec8360abb0b51bdf8f8721e9ed62ec4
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Sep 10 02:52:55 2012 +0000

    powerpc/mm: Add 64TB support
    
    Increase max addressable range to 64TB. This is not tested on
    real hardware yet.
    
    Reviewed-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 83efc6e81543..9dc5cd1fde1a 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -97,8 +97,8 @@ extern struct task_struct *last_task_used_spe;
 #endif
 
 #ifdef CONFIG_PPC64
-/* 64-bit user address space is 44-bits (16TB user VM) */
-#define TASK_SIZE_USER64 (0x0000100000000000UL)
+/* 64-bit user address space is 46-bits (64TB user VM) */
+#define TASK_SIZE_USER64 (0x0000400000000000UL)
 
 /* 
  * 32-bit user address space is 4GB - 1 page 

commit 4474ef055c5d8cb8eaf002d69e49af71e3aa3a88
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Sep 6 21:24:56 2012 +0000

    powerpc: Rework set_dabr so it can take a DABRX value as well
    
    Rework set_dabr to take a DABRX value as well.
    
    Both the pseries and PS3 hypervisors do some checks on the DABRX
    values that are passed in the hcall.  This patch stops bogus values
    from being passed to hypervisor.  Also, in the case where we are
    clearing the breakpoint, where DABR and DABRX are zero, we modify the
    DABRX value to make it valid so that the hcall won't fail.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 7e7fa13e4667..83efc6e81543 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -219,6 +219,7 @@ struct thread_struct {
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
 	unsigned long	dabr;		/* Data address breakpoint register */
+	unsigned long	dabrx;		/*      ... extension  */
 	unsigned long	trap_nr;	/* last trap # on this thread */
 #ifdef CONFIG_ALTIVEC
 	/* Complete AltiVec register set */

commit fff34b3412b9401a76ba9d021db1bd91cb0e02b6
Merge: 28e1e58fb668 636802ef96ee
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Sep 7 09:48:59 2012 +1000

    Merge branch 'merge' into next
    
    Brings in various bug fixes from 3.6-rcX

commit 375f561a4131a0f501c8845a2a20f2ca1abc8f7a
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jul 26 18:51:09 2012 +0000

    powerpc/powernv: Always go into nap mode when CPU is offline
    
    The CPU hotplug code for the powernv platform currently only puts
    offline CPUs into nap mode if the powersave_nap variable is set.
    However, HV-style KVM on this platform requires secondary CPU threads
    to be offline and in nap mode.  Since we know nap mode works just
    fine on all POWER7 machines, and the only machines that support the
    powernv platform are POWER7 machines, this changes the code to
    always put offline CPUs into nap mode, regardless of powersave_nap.
    Powersave_nap still controls whether or not CPUs go into nap mode
    when idle, as before.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 53b6dfa83344..54b73a28c205 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -386,6 +386,7 @@ extern unsigned long cpuidle_disable;
 enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 
 extern int powersave_nap;	/* set if nap mode can be used in idle loop */
+extern void power7_nap(void);
 
 #ifdef CONFIG_PSERIES_IDLE
 extern void update_smt_snooze_delay(int snooze);

commit 41ab5266c3622354353433618edb92ab278025fa
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Thu Aug 23 21:27:09 2012 +0000

    powerpc: Add trap_nr to thread_struct
    
    Add thread_struct.trap_nr and use it to store the last exception
    the thread experienced. In this patch, we populate the field at
    various places where we force_sig_info() to the process.
    
    This is also used in uprobes to determine if the probed instruction
    caused an exception.
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 53b6dfa83344..e0f6710d1983 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -219,6 +219,7 @@ struct thread_struct {
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
 	unsigned long	dabr;		/* Data address breakpoint register */
+	unsigned long	trap_nr;	/* last trap # on this thread */
 #ifdef CONFIG_ALTIVEC
 	/* Complete AltiVec register set */
 	vector128	vr[32] __attribute__((aligned(16)));

commit 16aaaff68440dd95de98adb075303355814be6e0
Author: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
Date:   Sun May 20 18:34:27 2012 +0000

    powerpc/pseries/cpuidle: Replace pseries_notify_cpuidle_add call with notifier
    
    The following patch is to remove the pseries_notify_add_cpu() call
    and replace it by a hot plug notifier.
    
    This would prevent cpuidle resources being released and allocated each
    time cpu comes online on pseries.
    
    The earlier design was causing a lockdep problem
    in start_secondary as reported on this thread
            -https://lkml.org/lkml/2012/5/17/2
    
    This applies on 3.4-rc7
    
    Signed-off-by: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 413a5eaef56c..53b6dfa83344 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -389,10 +389,8 @@ extern int powersave_nap;	/* set if nap mode can be used in idle loop */
 
 #ifdef CONFIG_PSERIES_IDLE
 extern void update_smt_snooze_delay(int snooze);
-extern int pseries_notify_cpuidle_add_cpu(int cpu);
 #else
 static inline void update_smt_snooze_delay(int snooze) {}
-static inline int pseries_notify_cpuidle_add_cpu(int cpu) { return 0; }
 #endif
 
 extern void flush_instruction_cache(void);

commit 07acfc2a9349a8ce45b236c2624dad452001966b
Merge: b5f4035adfff 322728e55aa7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 24 16:17:30 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM changes from Avi Kivity:
     "Changes include additional instruction emulation, page-crossing MMIO,
      faster dirty logging, preventing the watchdog from killing a stopped
      guest, module autoload, a new MSI ABI, and some minor optimizations
      and fixes.  Outside x86 we have a small s390 and a very large ppc
      update.
    
      Regarding the new (for kvm) rebaseless workflow, some of the patches
      that were merged before we switch trees had to be rebased, while
      others are true pulls.  In either case the signoffs should be correct
      now."
    
    Fix up trivial conflicts in Documentation/feature-removal-schedule.txt
    arch/powerpc/kvm/book3s_segment.S and arch/x86/include/asm/kvm_para.h.
    
    I suspect the kvm_para.h resolution ends up doing the "do I have cpuid"
    check effectively twice (it was done differently in two different
    commits), but better safe than sorry ;)
    
    * 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (125 commits)
      KVM: make asm-generic/kvm_para.h have an ifdef __KERNEL__ block
      KVM: s390: onereg for timer related registers
      KVM: s390: epoch difference and TOD programmable field
      KVM: s390: KVM_GET/SET_ONEREG for s390
      KVM: s390: add capability indicating COW support
      KVM: Fix mmu_reload() clash with nested vmx event injection
      KVM: MMU: Don't use RCU for lockless shadow walking
      KVM: VMX: Optimize %ds, %es reload
      KVM: VMX: Fix %ds/%es clobber
      KVM: x86 emulator: convert bsf/bsr instructions to emulate_2op_SrcV_nobyte()
      KVM: VMX: unlike vmcs on fail path
      KVM: PPC: Emulator: clean up SPR reads and writes
      KVM: PPC: Emulator: clean up instruction parsing
      kvm/powerpc: Add new ioctl to retreive server MMU infos
      kvm/book3s: Make kernel emulated H_PUT_TCE available for "PR" KVM
      KVM: PPC: bookehv: Fix r8/r13 storing in level exception handler
      KVM: PPC: Book3S: Enable IRQs during exit handling
      KVM: PPC: Fix PR KVM on POWER7 bare metal
      KVM: PPC: Fix stbux emulation
      KVM: PPC: bookehv: Use lwz/stw instead of PPC_LL/PPC_STL for 32-bit fields
      ...

commit ec0d7f18ab7b5097d7c0c8f3d909ca1031b9d5cd
Merge: 269af9a1a08d 1dcc8d7ba235
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 23 10:59:07 2012 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull fpu state cleanups from Ingo Molnar:
     "This tree streamlines further aspects of FPU handling by eliminating
      the prepare_to_copy() complication and moving that logic to
      arch_dup_task_struct().
    
      It also fixes the FPU dumps in threaded core dumps, removes and old
      (and now invalid) assumption plus micro-optimizes the exit path by
      avoiding an FPU save for dead tasks."
    
    Fixed up trivial add-add conflict in arch/sh/kernel/process.c that came
    in because we now do the FPU handling in arch_dup_task_struct() rather
    than the legacy (and now gone) prepare_to_copy().
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, fpu: drop the fpu state during thread exit
      x86, xsave: remove thread_has_fpu() bug check in __sanitize_i387_state()
      coredump: ensure the fpu state is flushed for proper multi-threaded core dump
      fork: move the real prepare_to_copy() users to arch_dup_task_struct()

commit 55ccf3fe3f9a3441731aa79cf42a628fc4ecace9
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed May 16 15:03:51 2012 -0700

    fork: move the real prepare_to_copy() users to arch_dup_task_struct()
    
    Historical prepare_to_copy() is mostly a no-op, duplicated for majority of
    the architectures and the rest following the x86 model of flushing the extended
    register state like fpu there.
    
    Remove it and use the arch_dup_task_struct() instead.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1336692811-30576-1-git-send-email-suresh.b.siddha@intel.com
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chen Liqin <liqin.chen@sunplusct.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 8e2d0371fe1e..854f899d0c34 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -74,9 +74,6 @@ struct task_struct;
 void start_thread(struct pt_regs *regs, unsigned long fdptr, unsigned long sp);
 void release_thread(struct task_struct *);
 
-/* Prepare to copy thread state - unlazy all lazy status */
-extern void prepare_to_copy(struct task_struct *tsk);
-
 /* Create a new kernel thread. */
 extern long kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
 

commit c9b92b840705542a1ae50b5407154a5595d17359
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 7 17:59:50 2012 +0000

    powerpc: Remove unused cpu_idle_wait()
    
    cpuidle uses a generic function now. Remove the cruft.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Link: http://lkml.kernel.org/r/20120507175652.330322737@linutronix.de

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 8e2d0371fe1e..48a26d379222 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -386,7 +386,6 @@ extern unsigned long cpuidle_disable;
 enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 
 extern int powersave_nap;	/* set if nap mode can be used in idle loop */
-void cpu_idle_wait(void);
 
 #ifdef CONFIG_PSERIES_IDLE
 extern void update_smt_snooze_delay(int snooze);

commit d30f6e480055e5be12e7a03fd11ea912a451daa5
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Dec 20 15:34:43 2011 +0000

    KVM: PPC: booke: category E.HV (GS-mode) support
    
    Chips such as e500mc that implement category E.HV in Power ISA 2.06
    provide hardware virtualization features, including a new MSR mode for
    guest state.  The guest OS can perform many operations without trapping
    into the hypervisor, including transitions to and from guest userspace.
    
    Since we can use SRR1[GS] to reliably tell whether an exception came from
    guest state, instead of messing around with IVPR, we use DO_KVM similarly
    to book3s.
    
    Current issues include:
     - Machine checks from guest state are not routed to the host handler.
     - The guest can cause a host oops by executing an emulated instruction
       in a page that lacks read permission.  Existing e500/4xx support has
       the same problem.
    
    Includes work by Ashish Kalra <Ashish.Kalra@freescale.com>,
    Varun Sethi <Varun.Sethi@freescale.com>, and
    Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: remove pt_regs usage]
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 8e2d0371fe1e..2a25ab0f5896 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -243,6 +243,9 @@ struct thread_struct {
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
 	void*		kvm_shadow_vcpu; /* KVM internal data */
 #endif /* CONFIG_KVM_BOOK3S_32_HANDLER */
+#if defined(CONFIG_KVM) && defined(CONFIG_BOOKE)
+	struct kvm_vcpu	*kvm_vcpu;
+#endif
 #ifdef CONFIG_PPC64
 	unsigned long	dscr;
 	int		dscr_inherit;

commit ae3a197e3d0bfe3f4bf1693723e82dc018c096f3
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:02 2012 +0100

    Disintegrate asm/system.h for PowerPC
    
    Disintegrate asm/system.h for PowerPC.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    cc: linuxppc-dev@lists.ozlabs.org

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index b585bff1a022..8e2d0371fe1e 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -385,6 +385,36 @@ static inline unsigned long get_clean_sp(struct pt_regs *regs, int is_32)
 extern unsigned long cpuidle_disable;
 enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 
+extern int powersave_nap;	/* set if nap mode can be used in idle loop */
+void cpu_idle_wait(void);
+
+#ifdef CONFIG_PSERIES_IDLE
+extern void update_smt_snooze_delay(int snooze);
+extern int pseries_notify_cpuidle_add_cpu(int cpu);
+#else
+static inline void update_smt_snooze_delay(int snooze) {}
+static inline int pseries_notify_cpuidle_add_cpu(int cpu) { return 0; }
+#endif
+
+extern void flush_instruction_cache(void);
+extern void hard_reset_now(void);
+extern void poweroff_now(void);
+extern int fix_alignment(struct pt_regs *);
+extern void cvt_fd(float *from, double *to);
+extern void cvt_df(double *from, float *to);
+extern void _nmask_and_or_msr(unsigned long nmask, unsigned long or_val);
+
+#ifdef CONFIG_PPC64
+/*
+ * We handle most unaligned accesses in hardware. On the other hand 
+ * unaligned DMA can be very expensive on some ppc64 IO chips (it does
+ * powers of 2 writes until it reaches sufficient alignment).
+ *
+ * Based on this we disable the IP header alignment in network drivers.
+ */
+#define NET_IP_ALIGN	0
+#endif
+
 #endif /* __KERNEL__ */
 #endif /* __ASSEMBLY__ */
 #endif /* _ASM_POWERPC_PROCESSOR_H */

commit e8bb3e00cff93ef2a0cfc09c3294aa37b4737e09
Author: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
Date:   Wed Nov 30 02:47:03 2011 +0000

    powerpc/cpuidle: Handle power_save=off
    
    This patch makes pseries_idle_driver not to be registered when
    power_save=off kernel boot option is specified. The
    cpuidle_disable variable used here is similar to
    its usage on x86. If cpuidle_disable is set then
    sysfs entries for cpuidle framework are not created
    and the required drivers are not loaded.
    
    Signed-off-by: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Signed-off-by: Trinabh Gupta <g.trinabh@gmail.com>
    Signed-off-by: Arun R Bharadwaj <arun.r.bharadwaj@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 811b7e744378..b585bff1a022 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -382,6 +382,7 @@ static inline unsigned long get_clean_sp(struct pt_regs *regs, int is_32)
 }
 #endif
 
+extern unsigned long cpuidle_disable;
 enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
 
 #endif /* __KERNEL__ */

commit 771dae81896855d25f7f8746aaf56c0238deafb6
Author: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
Date:   Wed Nov 30 02:46:31 2011 +0000

    powerpc/cpuidle: Add cpu_idle_wait() to allow switching of idle routines
    
    This patch provides cpu_idle_wait() routine for the powerpc
    platform which is required by the cpuidle subsystem. This
    routine is required to change the idle handler on SMP systems.
    The equivalent routine for x86 is in arch/x86/kernel/process.c
    but the powerpc implementation is different.
    
    cpuidle_disable variable is to enable/disable cpuidle
    framework if power_save option is set during the boot
    time.
    
    Signed-off-by: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Signed-off-by: Trinabh Gupta <g.trinabh@gmail.com>
    Signed-off-by: Arun R Bharadwaj <arun.r.bharadwaj@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index eb11a446720e..811b7e744378 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -382,6 +382,8 @@ static inline unsigned long get_clean_sp(struct pt_regs *regs, int is_32)
 }
 #endif
 
+enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_POWERSAVE_OFF};
+
 #endif /* __KERNEL__ */
 #endif /* __ASSEMBLY__ */
 #endif /* _ASM_POWERPC_PROCESSOR_H */

commit 1325a684b553d4b5c41ae0482f8991b43f945746
Author: Ashish Kalra <Ashish.Kalra@freescale.com>
Date:   Fri Apr 22 16:48:27 2011 -0500

    powerpc/85xx: Save scratch registers to thread info instead of using SPRGs.
    
    We expect this is actually faster, and we end up needing more space than we
    can get from the SPRGs in some instances.  This is also useful when running
    as a guest OS - SPRGs4-7 do not have guest versions.
    
    8 slots are allocated in thread_info for this even though we only actually
    use 4 of them - this allows space for future code to have more scratch
    space (and we know we'll need it for things like hugetlb).
    
    Signed-off-by: Ashish Kalra <Ashish.Kalra@freescale.com>
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index d50c2b6d9bc3..eb11a446720e 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -20,6 +20,7 @@
 
 #ifndef __ASSEMBLY__
 #include <linux/compiler.h>
+#include <linux/cache.h>
 #include <asm/ptrace.h>
 #include <asm/types.h>
 
@@ -156,6 +157,10 @@ struct thread_struct {
 #endif
 	struct pt_regs	*regs;		/* Pointer to saved register state */
 	mm_segment_t	fs;		/* for get_fs() validation */
+#ifdef CONFIG_BOOKE
+	/* BookE base exception scratch space; align on cacheline */
+	unsigned long	normsave[8] ____cacheline_aligned;
+#endif
 #ifdef CONFIG_PPC32
 	void		*pgdir;		/* root of page-table tree */
 #endif

commit efcac6589a277c10060e4be44b9455cf43838dc1
Author: Alexey Kardashevskiy <aik@au1.ibm.com>
Date:   Wed Mar 2 15:18:48 2011 +0000

    powerpc: Per process DSCR + some fixes (try#4)
    
    The DSCR (aka Data Stream Control Register) is supported on some
    server PowerPC chips and allow some control over the prefetch
    of data streams.
    
    This patch allows the value to be specified per thread by emulating
    the corresponding mfspr and mtspr instructions. Children of such
    threads inherit the value. Other threads use a default value that
    can be specified in sysfs - /sys/devices/system/cpu/dscr_default.
    
    If a thread starts with non default value in the sysfs entry,
    all children threads inherit this non default value even if
    the sysfs value is changed later.
    
    Signed-off-by: Alexey Kardashevskiy <aik@au1.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index de1967a1ff57..d50c2b6d9bc3 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -238,6 +238,10 @@ struct thread_struct {
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
 	void*		kvm_shadow_vcpu; /* KVM internal data */
 #endif /* CONFIG_KVM_BOOK3S_32_HANDLER */
+#ifdef CONFIG_PPC64
+	unsigned long	dscr;
+	int		dscr_inherit;
+#endif
 };
 
 #define ARCH_MIN_TASKALIGN 16

commit 56e640de12c4d9902493cd819c63cacf66515686
Author: Christian Dietrich <qy03fugy@stud.informatik.uni-erlangen.de>
Date:   Mon Sep 6 04:36:12 2010 +0000

    powerpc: Removing undead ifdef __KERNEL__
    
    The __KERNEL__ ifdef isn't necessary at this point, because it is
    checked in an outer ifdef level already and has no effect here.
    
    Signed-off-by: Christian Dietrich <qy03fugy@stud.informatik.uni-erlangen.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 4c14187ba02d..de1967a1ff57 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -122,7 +122,6 @@ extern struct task_struct *last_task_used_spe;
 		TASK_UNMAPPED_BASE_USER32 : TASK_UNMAPPED_BASE_USER64 )
 #endif
 
-#ifdef __KERNEL__
 #ifdef __powerpc64__
 
 #define STACK_TOP_USER64 TASK_SIZE_USER64
@@ -139,7 +138,6 @@ extern struct task_struct *last_task_used_spe;
 #define STACK_TOP_MAX	STACK_TOP
 
 #endif /* __powerpc64__ */
-#endif /* __KERNEL__ */
 
 typedef struct {
 	unsigned long seg;

commit cab175f9fa2973f0deb1580fca3c966fe1d3981e
Author: Denis Kirjanov <dkirjanov@kernel.org>
Date:   Fri Aug 27 03:49:11 2010 +0000

    powerpc: Use is_32bit_task() helper to test 32-bit binary
    
    This patch removes all explicit tests for the TIF_32BIT flag
    
    Signed-off-by: Denis Kirjanov <dkirjanov@kernel.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 19c05b0f74be..4c14187ba02d 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -118,7 +118,7 @@ extern struct task_struct *last_task_used_spe;
 #define TASK_UNMAPPED_BASE_USER32 (PAGE_ALIGN(TASK_SIZE_USER32 / 4))
 #define TASK_UNMAPPED_BASE_USER64 (PAGE_ALIGN(TASK_SIZE_USER64 / 4))
 
-#define TASK_UNMAPPED_BASE ((test_thread_flag(TIF_32BIT)) ? \
+#define TASK_UNMAPPED_BASE ((is_32bit_task()) ? \
 		TASK_UNMAPPED_BASE_USER32 : TASK_UNMAPPED_BASE_USER64 )
 #endif
 
@@ -128,7 +128,7 @@ extern struct task_struct *last_task_used_spe;
 #define STACK_TOP_USER64 TASK_SIZE_USER64
 #define STACK_TOP_USER32 TASK_SIZE_USER32
 
-#define STACK_TOP (test_thread_flag(TIF_32BIT) ? \
+#define STACK_TOP (is_32bit_task() ? \
 		   STACK_TOP_USER32 : STACK_TOP_USER64)
 
 #define STACK_TOP_MAX STACK_TOP_USER64

commit 5aae8a53708025d4e718f0d2e7c2f766779ddc71
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Tue Jun 15 11:35:19 2010 +0530

    powerpc, hw_breakpoints: Implement hw_breakpoints for 64-bit server processors
    
    Implement perf-events based hw-breakpoint interfaces for PowerPC
    64-bit server (Book III S) processors.  This allows access to a
    given location to be used as an event that can be counted or
    profiled by the perf_events subsystem.
    
    This is done using the DABR (data breakpoint register), which can
    also be used for process debugging via ptrace.  When perf_event
    hw_breakpoint support is configured in, the perf_event subsystem
    manages the DABR and arbitrates access to it, and ptrace then
    creates a perf_event when it is requested to set a data breakpoint.
    
    [Adopted suggestions from Paul Mackerras <paulus@samba.org> to
    - emulate_step() all system-wide breakpoints and single-step only the
      per-task breakpoints
    - perform arch-specific cleanup before unregistration through
      arch_unregister_hw_breakpoint()
    ]
    
    Signed-off-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 7492fe8ad6e4..19c05b0f74be 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -209,6 +209,14 @@ struct thread_struct {
 #ifdef CONFIG_PPC64
 	unsigned long	start_tb;	/* Start purr when proc switched in */
 	unsigned long	accum_tb;	/* Total accumilated purr for process */
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+	struct perf_event *ptrace_bps[HBP_NUM];
+	/*
+	 * Helps identify source of single-step exception and subsequent
+	 * hw-breakpoint enablement
+	 */
+	struct perf_event *last_hit_ubp;
+#endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
 	unsigned long	dabr;		/* Data address breakpoint register */
 #ifdef CONFIG_ALTIVEC

commit 97e492558f423d99c51eb934506b7a3d7c64613b
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:51 2010 +0200

    KVM: PPC: Add SVCPU to Book3S_32
    
    We need to keep the pointer to the shadow vcpu somewhere accessible from
    within really early interrupt code. The best fit I found was the thread
    struct, as that resides in an SPRG.
    
    So let's put a pointer to the shadow vcpu in the thread struct and add
    an asm-offset so we can find it.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 221ba6240464..7492fe8ad6e4 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -229,6 +229,9 @@ struct thread_struct {
 	unsigned long	spefscr;	/* SPE & eFP status */
 	int		used_spe;	/* set if process has used spe */
 #endif /* CONFIG_SPE */
+#ifdef CONFIG_KVM_BOOK3S_32_HANDLER
+	void*		kvm_shadow_vcpu; /* KVM internal data */
+#endif /* CONFIG_KVM_BOOK3S_32_HANDLER */
 };
 
 #define ARCH_MIN_TASKALIGN 16

commit 99396ac105f54fe3584374c7c70a5cb6def766e6
Author: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
Date:   Mon Feb 8 11:53:26 2010 +0000

    powerpc/booke: Add definitions for advanced debug registers
    
    powerpc/booke: Add definitions for advanced debug registers
    
    From: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    
    Based on patches originally written by Torez Smith.
    
    This patch adds additional definitions for BookE Debug Registers
    to the reg_booke.h header file.
    
    Signed-off-by: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Acked-by: David Gibson <dwg@au1.ibm.com>
    Cc: Torez Smith  <lnxtorez@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Josh Boyer <jwboyer@linux.vnet.ibm.com>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Cc: Sergio Durigan Junior <sergiodj@br.ibm.com>
    Cc: Thiago Jung Bauermann <bauerman@br.ibm.com>
    Cc: linuxppc-dev list <Linuxppc-dev@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 9eed29eee604..221ba6240464 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -161,9 +161,41 @@ struct thread_struct {
 #ifdef CONFIG_PPC32
 	void		*pgdir;		/* root of page-table tree */
 #endif
-#if defined(CONFIG_4xx) || defined (CONFIG_BOOKE)
-	unsigned long	dbcr0;		/* debug control register values */
+#ifdef CONFIG_PPC_ADV_DEBUG_REGS
+	/*
+	 * The following help to manage the use of Debug Control Registers
+	 * om the BookE platforms.
+	 */
+	unsigned long	dbcr0;
 	unsigned long	dbcr1;
+#ifdef CONFIG_BOOKE
+	unsigned long	dbcr2;
+#endif
+	/*
+	 * The stored value of the DBSR register will be the value at the
+	 * last debug interrupt. This register can only be read from the
+	 * user (will never be written to) and has value while helping to
+	 * describe the reason for the last debug trap.  Torez
+	 */
+	unsigned long	dbsr;
+	/*
+	 * The following will contain addresses used by debug applications
+	 * to help trace and trap on particular address locations.
+	 * The bits in the Debug Control Registers above help define which
+	 * of the following registers will contain valid data and/or addresses.
+	 */
+	unsigned long	iac1;
+	unsigned long	iac2;
+#if CONFIG_PPC_ADV_DEBUG_IACS > 2
+	unsigned long	iac3;
+	unsigned long	iac4;
+#endif
+	unsigned long	dac1;
+	unsigned long	dac2;
+#if CONFIG_PPC_ADV_DEBUG_DVCS > 0
+	unsigned long	dvc1;
+	unsigned long	dvc2;
+#endif
 #endif
 	/* FP and VSX 0-31 register set */
 	double		fpr[32][TS_FPRWIDTH];

commit efbda86098455da014be849713df6498cefc5a2a
Author: Josh Boyer <jwboyer@linux.vnet.ibm.com>
Date:   Wed Mar 25 06:23:59 2009 +0000

    powerpc: Sanitize stack pointer in signal handling code
    
    On powerpc64 machines running 32-bit userspace, we can get garbage bits in the
    stack pointer passed into the kernel.  Most places handle this correctly, but
    the signal handling code uses the passed value directly for allocating signal
    stack frames.
    
    This fixes the issue by introducing a get_clean_sp function that returns a
    sanitized stack pointer.  For 32-bit tasks on a 64-bit kernel, the stack
    pointer is masked correctly.  In all other cases, the stack pointer is simply
    returned.
    
    Additionally, we pass an 'is_32' parameter to get_sigframe now in order to
    get the properly sanitized stack.  The callers are know to be 32 or 64-bit
    statically.
    
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index d3466490104a..9eed29eee604 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -313,6 +313,25 @@ static inline void prefetchw(const void *x)
 #define HAVE_ARCH_PICK_MMAP_LAYOUT
 #endif
 
+#ifdef CONFIG_PPC64
+static inline unsigned long get_clean_sp(struct pt_regs *regs, int is_32)
+{
+	unsigned long sp;
+
+	if (is_32)
+		sp = regs->gpr[1] & 0x0ffffffffUL;
+	else
+		sp = regs->gpr[1];
+
+	return sp;
+}
+#else
+static inline unsigned long get_clean_sp(struct pt_regs *regs, int is_32)
+{
+	return regs->gpr[1];
+}
+#endif
+
 #endif /* __KERNEL__ */
 #endif /* __ASSEMBLY__ */
 #endif /* _ASM_POWERPC_PROCESSOR_H */

commit 6b82b3e4b54b2fce2ca11976c535012b836b2016
Author: Anton Vorontsov <avorontsov@ru.mvista.com>
Date:   Tue Dec 9 09:47:29 2008 +0000

    powerpc: Remove `have_of' global variable
    
    The `have_of' variable is a relic from the arch/ppc time, it isn't
    useful nowadays.
    
    Signed-off-by: Anton Vorontsov <avorontsov@ru.mvista.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index cd7a47860e5a..d3466490104a 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -69,8 +69,6 @@ extern int _prep_type;
 
 #ifdef __KERNEL__
 
-extern int have_of;
-
 struct task_struct;
 void start_thread(struct pt_regs *regs, unsigned long fdptr, unsigned long sp);
 void release_thread(struct task_struct *);

commit 6a800f36acd5bf06b5fe2cb27c4d0524d60c3df5
Author: Liu Yu <yu.liu@freescale.com>
Date:   Tue Oct 28 11:50:21 2008 +0800

    powerpc: Add SPE/EFP math emulation for E500v1/v2 processors.
    
    This patch add the handlers of SPE/EFP exceptions.
    The code is used to emulate float point arithmetic,
    when MSR(SPE) is enabled and receive EFP data interrupt or EFP round interrupt.
    
    This patch has no conflict with or dependence on FP math-emu.
    
    The code has been tested by TestFloat.
    
    Now the code doesn't support SPE/EFP instructions emulation
    (it won't be called when receive program interrupt),
    but it could be easily added.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
index 101ed87f7d84..cd7a47860e5a 100644
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@ -207,6 +207,11 @@ struct thread_struct {
 #define INIT_SP_LIMIT \
 	(_ALIGN_UP(sizeof(init_thread_info), 16) + (unsigned long) &init_stack)
 
+#ifdef CONFIG_SPE
+#define SPEFSCR_INIT .spefscr = SPEFSCR_FINVE | SPEFSCR_FDBZE | SPEFSCR_FUNFE | SPEFSCR_FOVFE,
+#else
+#define SPEFSCR_INIT
+#endif
 
 #ifdef CONFIG_PPC32
 #define INIT_THREAD { \
@@ -215,6 +220,7 @@ struct thread_struct {
 	.fs = KERNEL_DS, \
 	.pgdir = swapper_pg_dir, \
 	.fpexc_mode = MSR_FE0 | MSR_FE1, \
+	SPEFSCR_INIT \
 }
 #else
 #define INIT_THREAD  { \

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/processor.h b/arch/powerpc/include/asm/processor.h
new file mode 100644
index 000000000000..101ed87f7d84
--- /dev/null
+++ b/arch/powerpc/include/asm/processor.h
@@ -0,0 +1,314 @@
+#ifndef _ASM_POWERPC_PROCESSOR_H
+#define _ASM_POWERPC_PROCESSOR_H
+
+/*
+ * Copyright (C) 2001 PPC 64 Team, IBM Corp
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <asm/reg.h>
+
+#ifdef CONFIG_VSX
+#define TS_FPRWIDTH 2
+#else
+#define TS_FPRWIDTH 1
+#endif
+
+#ifndef __ASSEMBLY__
+#include <linux/compiler.h>
+#include <asm/ptrace.h>
+#include <asm/types.h>
+
+/* We do _not_ want to define new machine types at all, those must die
+ * in favor of using the device-tree
+ * -- BenH.
+ */
+
+/* PREP sub-platform types see residual.h for these */
+#define _PREP_Motorola	0x01	/* motorola prep */
+#define _PREP_Firm	0x02	/* firmworks prep */
+#define _PREP_IBM	0x00	/* ibm prep */
+#define _PREP_Bull	0x03	/* bull prep */
+
+/* CHRP sub-platform types. These are arbitrary */
+#define _CHRP_Motorola	0x04	/* motorola chrp, the cobra */
+#define _CHRP_IBM	0x05	/* IBM chrp, the longtrail and longtrail 2 */
+#define _CHRP_Pegasos	0x06	/* Genesi/bplan's Pegasos and Pegasos2 */
+#define _CHRP_briq	0x07	/* TotalImpact's briQ */
+
+#if defined(__KERNEL__) && defined(CONFIG_PPC32)
+
+extern int _chrp_type;
+
+#ifdef CONFIG_PPC_PREP
+
+/* what kind of prep workstation we are */
+extern int _prep_type;
+
+#endif /* CONFIG_PPC_PREP */
+
+#endif /* defined(__KERNEL__) && defined(CONFIG_PPC32) */
+
+/*
+ * Default implementation of macro that returns current
+ * instruction pointer ("program counter").
+ */
+#define current_text_addr() ({ __label__ _l; _l: &&_l;})
+
+/* Macros for adjusting thread priority (hardware multi-threading) */
+#define HMT_very_low()   asm volatile("or 31,31,31   # very low priority")
+#define HMT_low()	 asm volatile("or 1,1,1	     # low priority")
+#define HMT_medium_low() asm volatile("or 6,6,6      # medium low priority")
+#define HMT_medium()	 asm volatile("or 2,2,2	     # medium priority")
+#define HMT_medium_high() asm volatile("or 5,5,5      # medium high priority")
+#define HMT_high()	 asm volatile("or 3,3,3	     # high priority")
+
+#ifdef __KERNEL__
+
+extern int have_of;
+
+struct task_struct;
+void start_thread(struct pt_regs *regs, unsigned long fdptr, unsigned long sp);
+void release_thread(struct task_struct *);
+
+/* Prepare to copy thread state - unlazy all lazy status */
+extern void prepare_to_copy(struct task_struct *tsk);
+
+/* Create a new kernel thread. */
+extern long kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
+
+/* Lazy FPU handling on uni-processor */
+extern struct task_struct *last_task_used_math;
+extern struct task_struct *last_task_used_altivec;
+extern struct task_struct *last_task_used_vsx;
+extern struct task_struct *last_task_used_spe;
+
+#ifdef CONFIG_PPC32
+
+#if CONFIG_TASK_SIZE > CONFIG_KERNEL_START
+#error User TASK_SIZE overlaps with KERNEL_START address
+#endif
+#define TASK_SIZE	(CONFIG_TASK_SIZE)
+
+/* This decides where the kernel will search for a free chunk of vm
+ * space during mmap's.
+ */
+#define TASK_UNMAPPED_BASE	(TASK_SIZE / 8 * 3)
+#endif
+
+#ifdef CONFIG_PPC64
+/* 64-bit user address space is 44-bits (16TB user VM) */
+#define TASK_SIZE_USER64 (0x0000100000000000UL)
+
+/* 
+ * 32-bit user address space is 4GB - 1 page 
+ * (this 1 page is needed so referencing of 0xFFFFFFFF generates EFAULT
+ */
+#define TASK_SIZE_USER32 (0x0000000100000000UL - (1*PAGE_SIZE))
+
+#define TASK_SIZE_OF(tsk) (test_tsk_thread_flag(tsk, TIF_32BIT) ? \
+		TASK_SIZE_USER32 : TASK_SIZE_USER64)
+#define TASK_SIZE	  TASK_SIZE_OF(current)
+
+/* This decides where the kernel will search for a free chunk of vm
+ * space during mmap's.
+ */
+#define TASK_UNMAPPED_BASE_USER32 (PAGE_ALIGN(TASK_SIZE_USER32 / 4))
+#define TASK_UNMAPPED_BASE_USER64 (PAGE_ALIGN(TASK_SIZE_USER64 / 4))
+
+#define TASK_UNMAPPED_BASE ((test_thread_flag(TIF_32BIT)) ? \
+		TASK_UNMAPPED_BASE_USER32 : TASK_UNMAPPED_BASE_USER64 )
+#endif
+
+#ifdef __KERNEL__
+#ifdef __powerpc64__
+
+#define STACK_TOP_USER64 TASK_SIZE_USER64
+#define STACK_TOP_USER32 TASK_SIZE_USER32
+
+#define STACK_TOP (test_thread_flag(TIF_32BIT) ? \
+		   STACK_TOP_USER32 : STACK_TOP_USER64)
+
+#define STACK_TOP_MAX STACK_TOP_USER64
+
+#else /* __powerpc64__ */
+
+#define STACK_TOP TASK_SIZE
+#define STACK_TOP_MAX	STACK_TOP
+
+#endif /* __powerpc64__ */
+#endif /* __KERNEL__ */
+
+typedef struct {
+	unsigned long seg;
+} mm_segment_t;
+
+#define TS_FPROFFSET 0
+#define TS_VSRLOWOFFSET 1
+#define TS_FPR(i) fpr[i][TS_FPROFFSET]
+
+struct thread_struct {
+	unsigned long	ksp;		/* Kernel stack pointer */
+	unsigned long	ksp_limit;	/* if ksp <= ksp_limit stack overflow */
+
+#ifdef CONFIG_PPC64
+	unsigned long	ksp_vsid;
+#endif
+	struct pt_regs	*regs;		/* Pointer to saved register state */
+	mm_segment_t	fs;		/* for get_fs() validation */
+#ifdef CONFIG_PPC32
+	void		*pgdir;		/* root of page-table tree */
+#endif
+#if defined(CONFIG_4xx) || defined (CONFIG_BOOKE)
+	unsigned long	dbcr0;		/* debug control register values */
+	unsigned long	dbcr1;
+#endif
+	/* FP and VSX 0-31 register set */
+	double		fpr[32][TS_FPRWIDTH];
+	struct {
+
+		unsigned int pad;
+		unsigned int val;	/* Floating point status */
+	} fpscr;
+	int		fpexc_mode;	/* floating-point exception mode */
+	unsigned int	align_ctl;	/* alignment handling control */
+#ifdef CONFIG_PPC64
+	unsigned long	start_tb;	/* Start purr when proc switched in */
+	unsigned long	accum_tb;	/* Total accumilated purr for process */
+#endif
+	unsigned long	dabr;		/* Data address breakpoint register */
+#ifdef CONFIG_ALTIVEC
+	/* Complete AltiVec register set */
+	vector128	vr[32] __attribute__((aligned(16)));
+	/* AltiVec status */
+	vector128	vscr __attribute__((aligned(16)));
+	unsigned long	vrsave;
+	int		used_vr;	/* set if process has used altivec */
+#endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_VSX
+	/* VSR status */
+	int		used_vsr;	/* set if process has used altivec */
+#endif /* CONFIG_VSX */
+#ifdef CONFIG_SPE
+	unsigned long	evr[32];	/* upper 32-bits of SPE regs */
+	u64		acc;		/* Accumulator */
+	unsigned long	spefscr;	/* SPE & eFP status */
+	int		used_spe;	/* set if process has used spe */
+#endif /* CONFIG_SPE */
+};
+
+#define ARCH_MIN_TASKALIGN 16
+
+#define INIT_SP		(sizeof(init_stack) + (unsigned long) &init_stack)
+#define INIT_SP_LIMIT \
+	(_ALIGN_UP(sizeof(init_thread_info), 16) + (unsigned long) &init_stack)
+
+
+#ifdef CONFIG_PPC32
+#define INIT_THREAD { \
+	.ksp = INIT_SP, \
+	.ksp_limit = INIT_SP_LIMIT, \
+	.fs = KERNEL_DS, \
+	.pgdir = swapper_pg_dir, \
+	.fpexc_mode = MSR_FE0 | MSR_FE1, \
+}
+#else
+#define INIT_THREAD  { \
+	.ksp = INIT_SP, \
+	.ksp_limit = INIT_SP_LIMIT, \
+	.regs = (struct pt_regs *)INIT_SP - 1, /* XXX bogus, I think */ \
+	.fs = KERNEL_DS, \
+	.fpr = {{0}}, \
+	.fpscr = { .val = 0, }, \
+	.fpexc_mode = 0, \
+}
+#endif
+
+/*
+ * Return saved PC of a blocked thread. For now, this is the "user" PC
+ */
+#define thread_saved_pc(tsk)    \
+        ((tsk)->thread.regs? (tsk)->thread.regs->nip: 0)
+
+#define task_pt_regs(tsk)	((struct pt_regs *)(tsk)->thread.regs)
+
+unsigned long get_wchan(struct task_struct *p);
+
+#define KSTK_EIP(tsk)  ((tsk)->thread.regs? (tsk)->thread.regs->nip: 0)
+#define KSTK_ESP(tsk)  ((tsk)->thread.regs? (tsk)->thread.regs->gpr[1]: 0)
+
+/* Get/set floating-point exception mode */
+#define GET_FPEXC_CTL(tsk, adr) get_fpexc_mode((tsk), (adr))
+#define SET_FPEXC_CTL(tsk, val) set_fpexc_mode((tsk), (val))
+
+extern int get_fpexc_mode(struct task_struct *tsk, unsigned long adr);
+extern int set_fpexc_mode(struct task_struct *tsk, unsigned int val);
+
+#define GET_ENDIAN(tsk, adr) get_endian((tsk), (adr))
+#define SET_ENDIAN(tsk, val) set_endian((tsk), (val))
+
+extern int get_endian(struct task_struct *tsk, unsigned long adr);
+extern int set_endian(struct task_struct *tsk, unsigned int val);
+
+#define GET_UNALIGN_CTL(tsk, adr)	get_unalign_ctl((tsk), (adr))
+#define SET_UNALIGN_CTL(tsk, val)	set_unalign_ctl((tsk), (val))
+
+extern int get_unalign_ctl(struct task_struct *tsk, unsigned long adr);
+extern int set_unalign_ctl(struct task_struct *tsk, unsigned int val);
+
+static inline unsigned int __unpack_fe01(unsigned long msr_bits)
+{
+	return ((msr_bits & MSR_FE0) >> 10) | ((msr_bits & MSR_FE1) >> 8);
+}
+
+static inline unsigned long __pack_fe01(unsigned int fpmode)
+{
+	return ((fpmode << 10) & MSR_FE0) | ((fpmode << 8) & MSR_FE1);
+}
+
+#ifdef CONFIG_PPC64
+#define cpu_relax()	do { HMT_low(); HMT_medium(); barrier(); } while (0)
+#else
+#define cpu_relax()	barrier()
+#endif
+
+/* Check that a certain kernel stack pointer is valid in task_struct p */
+int validate_sp(unsigned long sp, struct task_struct *p,
+                       unsigned long nbytes);
+
+/*
+ * Prefetch macros.
+ */
+#define ARCH_HAS_PREFETCH
+#define ARCH_HAS_PREFETCHW
+#define ARCH_HAS_SPINLOCK_PREFETCH
+
+static inline void prefetch(const void *x)
+{
+	if (unlikely(!x))
+		return;
+
+	__asm__ __volatile__ ("dcbt 0,%0" : : "r" (x));
+}
+
+static inline void prefetchw(const void *x)
+{
+	if (unlikely(!x))
+		return;
+
+	__asm__ __volatile__ ("dcbtst 0,%0" : : "r" (x));
+}
+
+#define spin_lock_prefetch(x)	prefetchw(x)
+
+#ifdef CONFIG_PPC64
+#define HAVE_ARCH_PICK_MMAP_LAYOUT
+#endif
+
+#endif /* __KERNEL__ */
+#endif /* __ASSEMBLY__ */
+#endif /* _ASM_POWERPC_PROCESSOR_H */
