commit a3ea40d5c7365e7e5c7c85b6f30b15142b397571
Author: Alistair Popple <alistair@popple.id.au>
Date:   Thu May 21 11:43:41 2020 +1000

    powerpc: Add POWER10 architected mode
    
    PVR value of 0x0F000006 means we are arch v3.1 compliant (i.e.
    POWER10). This is used by phyp and kvm when booting as a pseries guest
    to detect the presence of new P10 features and to enable the
    appropriate hwcap and facility bits.
    
    Signed-off-by: Alistair Popple <alistair@popple.id.au>
    Signed-off-by: CÃ©dric Le Goater <clg@kaod.org>
    [mpe: Fall through to __init_FSCR rather than duplicating it, drop
          hack to set current->thread.fscr now that is handled elsewhere.]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200521014341.29095-8-alistair@popple.id.au

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index cf2a08bfd5cd..f4ac25d4df05 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -122,6 +122,7 @@
 #define MMU_FTRS_POWER7		MMU_FTRS_POWER6
 #define MMU_FTRS_POWER8		MMU_FTRS_POWER6
 #define MMU_FTRS_POWER9		MMU_FTRS_POWER6
+#define MMU_FTRS_POWER10	MMU_FTRS_POWER6
 #define MMU_FTRS_CELL		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \
 				MMU_FTR_CI_LARGE_PAGE
 #define MMU_FTRS_PA6T		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \

commit fe4a6856cb4f4353a6cb8d3629bcfe9204e3d57d
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Tue May 5 12:47:08 2020 +0530

    powerpc/pkeys: Avoid using lockless page table walk
    
    Fetch pkey from vma instead of linux page table. Also document the fact that in
    some cases the pkey returned in siginfo won't be the same as the one we took
    keyfault on. Even with linux page table walk, we can end up in a similar scenario.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200505071729.54912-2-aneesh.kumar@linux.ibm.com

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 0699cfeeb8c9..cf2a08bfd5cd 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -291,15 +291,6 @@ static inline bool early_radix_enabled(void)
 }
 #endif
 
-#ifdef CONFIG_PPC_MEM_KEYS
-extern u16 get_mm_addr_key(struct mm_struct *mm, unsigned long address);
-#else
-static inline u16 get_mm_addr_key(struct mm_struct *mm, unsigned long address)
-{
-	return 0;
-}
-#endif /* CONFIG_PPC_MEM_KEYS */
-
 #ifdef CONFIG_STRICT_KERNEL_RWX
 static inline bool strict_kernel_rwx_enabled(void)
 {

commit fd13daea5f72605a0a7386ebedbb5ff2b2a48da4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Sep 3 01:29:28 2019 +1000

    powerpc/64s: make mmu_partition_table_set_entry TLB flush optional
    
    No functional change.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190902152931.17840-4-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index ba94ce8c22d7..0699cfeeb8c9 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -257,7 +257,7 @@ extern void radix__mmu_cleanup_all(void);
 /* Functions for creating and updating partition table on POWER9 */
 extern void mmu_partition_table_init(void);
 extern void mmu_partition_table_set_entry(unsigned int lpid, unsigned long dw0,
-					  unsigned long dw1);
+					  unsigned long dw1, bool flush);
 #endif /* CONFIG_PPC64 */
 
 struct mm_struct;

commit 696dffa24bd0e17c8bccb18467555c17cc15e62c
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 15:58:02 2019 +0000

    powerpc/mm: move pgtable_t in asm/mmu.h
    
    pgtable_t is now identical for all subarches, move it to the
    top level asm/mmu.h
    
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index d86c5641bd97..ba94ce8c22d7 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -129,6 +129,9 @@
 #ifndef __ASSEMBLY__
 #include <linux/bug.h>
 #include <asm/cputable.h>
+#include <asm/page.h>
+
+typedef pte_t *pgtable_t;
 
 #ifdef CONFIG_PPC_FSL_BOOK3E
 #include <asm/percpu.h>

commit 6161a37307f3320808b5a7549593b991500f2656
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Apr 3 11:35:14 2019 +0530

    powerpc/mm: Fix build error with FLATMEM book3s64 config
    
    The current value of MAX_PHYSMEM_BITS cannot work with 32 bit configs.
    We used to have MAX_PHYSMEM_BITS not defined without SPARSEMEM and 32
    bit configs never expected a value to be set for MAX_PHYSMEM_BITS.
    
    Dependent code such as zsmalloc derived the right values based on other
    fields. Instead of finding a value that works with different configs,
    use new values only for book3s_64. For 64 bit booke, use the definition
    of MAX_PHYSMEM_BITS as per commit a7df61a0e2b6 ("[PATCH] ppc64: Increase sparsemem defaults")
    That change was done in 2005 and hopefully will work with book3e 64.
    
    Fixes: 8bc086899816 ("powerpc/mm: Only define MAX_PHYSMEM_BITS in SPARSEMEM configurations")
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 38d21adfde40..d86c5641bd97 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -349,21 +349,6 @@ static inline bool strict_kernel_rwx_enabled(void)
  */
 #define MMU_PAGE_COUNT	16
 
-/*
- * If we store section details in page->flags we can't increase the MAX_PHYSMEM_BITS
- * if we increase SECTIONS_WIDTH we will not store node details in page->flags and
- * page_to_nid does a page->section->node lookup
- * Hence only increase for VMEMMAP. Further depending on SPARSEMEM_EXTREME reduce
- * memory requirements with large number of sections.
- * 51 bits is the max physical real address on POWER9
- */
-#if defined(CONFIG_SPARSEMEM_VMEMMAP) && defined(CONFIG_SPARSEMEM_EXTREME) &&	\
-	defined (CONFIG_PPC_64K_PAGES)
-#define MAX_PHYSMEM_BITS        51
-#elif defined(CONFIG_PPC64)
-#define MAX_PHYSMEM_BITS        46
-#endif
-
 #ifdef CONFIG_PPC_BOOK3S_64
 #include <asm/book3s/64/mmu.h>
 #else /* CONFIG_PPC_BOOK3S_64 */

commit 890274c2dc4c0a57ae5a12d6a76fa6d05b599d98
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Apr 18 16:51:24 2019 +1000

    powerpc/64s: Implement KUAP for Radix MMU
    
    Kernel Userspace Access Prevention utilises a feature of the Radix MMU
    which disallows read and write access to userspace addresses. By
    utilising this, the kernel is prevented from accessing user data from
    outside of trusted paths that perform proper safety checks, such as
    copy_{to/from}_user() and friends.
    
    Userspace access is disabled from early boot and is only enabled when
    performing an operation like copy_{to/from}_user(). The register that
    controls this (AMR) does not prevent userspace from accessing itself,
    so there is no need to save and restore when entering and exiting
    userspace.
    
    When entering the kernel from the kernel we save AMR and if it is not
    blocking user access (because eg. we faulted doing a user access) we
    reblock user access for the duration of the exception (ie. the page
    fault) and then restore the AMR when returning back to the kernel.
    
    This feature can be tested by using the lkdtm driver (CONFIG_LKDTM=y)
    and performing the following:
    
      # (echo ACCESS_USERSPACE) > [debugfs]/provoke-crash/DIRECT
    
    If enabled, this should send SIGSEGV to the thread.
    
    We also add paranoid checking of AMR in switch and syscall return
    under CONFIG_PPC_KUAP_DEBUG.
    
    Co-authored-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Russell Currey <ruscur@russell.cc>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 8ddd4a91bdc1..38d21adfde40 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -107,6 +107,11 @@
  */
 #define MMU_FTR_1T_SEGMENT		ASM_CONST(0x40000000)
 
+/*
+ * Supports KUAP (key 0 controlling userspace addresses) on radix
+ */
+#define MMU_FTR_RADIX_KUAP		ASM_CONST(0x80000000)
+
 /* MMU feature bit sets for various CPUs */
 #define MMU_FTRS_DEFAULT_HPTE_ARCH_V2	\
 	MMU_FTR_HPTE_TABLE | MMU_FTR_PPCAS_ARCH_V2
@@ -164,7 +169,10 @@ enum {
 #endif
 #ifdef CONFIG_PPC_RADIX_MMU
 		MMU_FTR_TYPE_RADIX |
-#endif
+#ifdef CONFIG_PPC_KUAP
+		MMU_FTR_RADIX_KUAP |
+#endif /* CONFIG_PPC_KUAP */
+#endif /* CONFIG_PPC_RADIX_MMU */
 		0,
 };
 

commit cf7cf6977f531acd5dfe55250d0ee8cbbb6f1ae8
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Apr 9 15:43:11 2019 +1000

    powerpc/mm: Define MAX_PHYSMEM_BITS for all 64-bit configs
    
    The recent commit 8bc086899816 ("powerpc/mm: Only define
    MAX_PHYSMEM_BITS in SPARSEMEM configurations") removed our definition
    of MAX_PHYSMEM_BITS when SPARSEMEM is disabled.
    
    This inadvertently broke some 64-bit FLATMEM using configs with eg:
    
      arch/powerpc/include/asm/book3s/64/mmu-hash.h:584:6: error: "MAX_PHYSMEM_BITS" is not defined, evaluates to 0
       #if (MAX_PHYSMEM_BITS > MAX_EA_BITS_PER_CONTEXT)
            ^~~~~~~~~~~~~~~~
    
    Fix it by making sure we define MAX_PHYSMEM_BITS for all 64-bit
    configs regardless of SPARSEMEM.
    
    Fixes: 8bc086899816 ("powerpc/mm: Only define MAX_PHYSMEM_BITS in SPARSEMEM configurations")
    Reported-by: Andreas Schwab <schwab@linux-m68k.org>
    Reported-by: Hugh Dickins <hughd@google.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 598cdcdd1355..8ddd4a91bdc1 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -352,7 +352,7 @@ static inline bool strict_kernel_rwx_enabled(void)
 #if defined(CONFIG_SPARSEMEM_VMEMMAP) && defined(CONFIG_SPARSEMEM_EXTREME) &&	\
 	defined (CONFIG_PPC_64K_PAGES)
 #define MAX_PHYSMEM_BITS        51
-#elif defined(CONFIG_SPARSEMEM)
+#elif defined(CONFIG_PPC64)
 #define MAX_PHYSMEM_BITS        46
 #endif
 

commit 8bc086899816214fbc6047c9c7e15fcab49552bf
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Sun Mar 17 01:17:56 2019 +0000

    powerpc/mm: Only define MAX_PHYSMEM_BITS in SPARSEMEM configurations
    
    MAX_PHYSMEM_BITS only needs to be defined if CONFIG_SPARSEMEM is
    enabled, and that was the case before commit 4ffe713b7587
    ("powerpc/mm: Increase the max addressable memory to 2PB").
    
    On 32-bit systems, where CONFIG_SPARSEMEM is not enabled, we now
    define it as 46.  That is larger than the real number of physical
    address bits, and breaks calculations in zsmalloc:
    
      mm/zsmalloc.c:130:49: warning: right shift count is negative
        MAX(32, (ZS_MAX_PAGES_PER_ZSPAGE << PAGE_SHIFT >> OBJ_INDEX_BITS))
                                                       ^~
      ...
      mm/zsmalloc.c:253:21: error: variably modified 'size_class' at file scope
        struct size_class *size_class[ZS_SIZE_CLASSES];
                           ^~~~~~~~~~
    
    Fixes: 4ffe713b7587 ("powerpc/mm: Increase the max addressable memory to 2PB")
    Cc: stable@vger.kernel.org # v4.20+
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index d34ad1657d7b..598cdcdd1355 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -352,7 +352,7 @@ static inline bool strict_kernel_rwx_enabled(void)
 #if defined(CONFIG_SPARSEMEM_VMEMMAP) && defined(CONFIG_SPARSEMEM_EXTREME) &&	\
 	defined (CONFIG_PPC_64K_PAGES)
 #define MAX_PHYSMEM_BITS        51
-#else
+#elif defined(CONFIG_SPARSEMEM)
 #define MAX_PHYSMEM_BITS        46
 #endif
 

commit 28ea38b9cba68eec55cf550acd6b36b6f507cd17
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Feb 21 19:08:45 2019 +0000

    powerpc/mmu: add is_strict_kernel_rwx() helper
    
    Add a helper to know whether STRICT_KERNEL_RWX is enabled.
    
    This is based on rodata_enabled flag which is defined only
    when CONFIG_STRICT_KERNEL_RWX is selected.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 6d22a8e78fe2..d34ad1657d7b 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -289,6 +289,17 @@ static inline u16 get_mm_addr_key(struct mm_struct *mm, unsigned long address)
 }
 #endif /* CONFIG_PPC_MEM_KEYS */
 
+#ifdef CONFIG_STRICT_KERNEL_RWX
+static inline bool strict_kernel_rwx_enabled(void)
+{
+	return rodata_enabled;
+}
+#else
+static inline bool strict_kernel_rwx_enabled(void)
+{
+	return false;
+}
+#endif
 #endif /* !__ASSEMBLY__ */
 
 /* The kernel use the constants below to index in the page sizes array.

commit 40058337f23f79212f92ed5ef066e90a032905b1
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Feb 21 10:37:53 2019 +0000

    powerpc: simplify BDI switch
    
    There is no reason to re-read each time the pointer at
    location 0xf0 as it is fixed and known.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 25607604a7a5..6d22a8e78fe2 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -356,6 +356,8 @@ extern void early_init_mmu_secondary(void);
 extern void setup_initial_memory_limit(phys_addr_t first_memblock_base,
 				       phys_addr_t first_memblock_size);
 static inline void mmu_early_init_devtree(void) { }
+
+extern void *abatron_pteptrs[2];
 #endif /* __ASSEMBLY__ */
 #endif
 

commit 712877f8740471d96f703647d813d8a74fccc816
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Nov 16 17:08:03 2018 +0000

    powerpc/mm: Eliminate not possible mmu features at compile time
    
    Depending on the CONFIG selected, many of the MMU features are
    not possible. Lets only get the possible ones in MMU_FTRS_POSSIBLE.
    
    This allows gcc to get rid at compile time of code related to
    not possible features.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 9667921bf420..25607604a7a5 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -131,16 +131,37 @@ DECLARE_PER_CPU(int, next_tlbcam_idx);
 #endif
 
 enum {
-	MMU_FTRS_POSSIBLE = MMU_FTR_HPTE_TABLE | MMU_FTR_TYPE_8xx |
-		MMU_FTR_TYPE_40x | MMU_FTR_TYPE_44x | MMU_FTR_TYPE_FSL_E |
-		MMU_FTR_TYPE_47x | MMU_FTR_USE_HIGH_BATS | MMU_FTR_BIG_PHYS |
-		MMU_FTR_USE_TLBIVAX_BCAST | MMU_FTR_USE_TLBILX |
-		MMU_FTR_LOCK_BCAST_INVAL | MMU_FTR_NEED_DTLB_SW_LRU |
+	MMU_FTRS_POSSIBLE =
+#ifdef CONFIG_PPC_BOOK3S
+		MMU_FTR_HPTE_TABLE |
+#endif
+#ifdef CONFIG_PPC_8xx
+		MMU_FTR_TYPE_8xx |
+#endif
+#ifdef CONFIG_40x
+		MMU_FTR_TYPE_40x |
+#endif
+#ifdef CONFIG_44x
+		MMU_FTR_TYPE_44x |
+#endif
+#if defined(CONFIG_E200) || defined(CONFIG_E500)
+		MMU_FTR_TYPE_FSL_E | MMU_FTR_BIG_PHYS | MMU_FTR_USE_TLBILX |
+#endif
+#ifdef CONFIG_PPC_47x
+		MMU_FTR_TYPE_47x | MMU_FTR_USE_TLBIVAX_BCAST | MMU_FTR_LOCK_BCAST_INVAL |
+#endif
+#ifdef CONFIG_PPC_BOOK3S_32
+		MMU_FTR_USE_HIGH_BATS | MMU_FTR_NEED_DTLB_SW_LRU |
+#endif
+#ifdef CONFIG_PPC_BOOK3E_64
 		MMU_FTR_USE_TLBRSRV | MMU_FTR_USE_PAIRED_MAS |
+#endif
+#ifdef CONFIG_PPC_BOOK3S_64
 		MMU_FTR_NO_SLBIE_B | MMU_FTR_16M_PAGE | MMU_FTR_TLBIEL |
 		MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_CI_LARGE_PAGE |
 		MMU_FTR_1T_SEGMENT | MMU_FTR_TLBIE_CROP_VA |
 		MMU_FTR_KERNEL_RO | MMU_FTR_68_BIT_VA |
+#endif
 #ifdef CONFIG_PPC_RADIX_MMU
 		MMU_FTR_TYPE_RADIX |
 #endif

commit 994da93d196866f914c9d64aafb86e95e3decbb2
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Nov 29 14:06:55 2018 +0000

    powerpc/mm: move platform specific mmu-xxx.h in platform directories
    
    The purpose of this patch is to move platform specific
    mmu-xxx.h files in platform directories like pte-xxx.h files.
    
    In the meantime this patch creates common nohash and
    nohash/32 + nohash/64 mmu.h files for future common parts.
    
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 8a6d3b129c6d..9667921bf420 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -341,18 +341,8 @@ static inline void mmu_early_init_devtree(void) { }
 #if defined(CONFIG_PPC_BOOK3S_32)
 /* 32-bit classic hash table MMU */
 #include <asm/book3s/32/mmu-hash.h>
-#elif defined(CONFIG_40x)
-/* 40x-style software loaded TLB */
-#  include <asm/mmu-40x.h>
-#elif defined(CONFIG_44x)
-/* 44x-style software loaded TLB */
-#  include <asm/mmu-44x.h>
-#elif defined(CONFIG_PPC_BOOK3E_MMU)
-/* Freescale Book-E software loaded TLB or Book-3e (ISA 2.06+) MMU */
-#  include <asm/mmu-book3e.h>
-#elif defined (CONFIG_PPC_8xx)
-/* Motorola/Freescale 8xx software loaded TLB */
-#  include <asm/mmu-8xx.h>
+#elif defined(CONFIG_PPC_MMU_NOHASH)
+#include <asm/nohash/mmu.h>
 #endif
 
 #endif /* __KERNEL__ */

commit 68289ae935da5a8488c0268111631f644d27b683
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Nov 17 10:25:02 2018 +0000

    powerpc: change CONFIG_PPC_STD_MMU_32 to CONFIG_PPC_BOOK3S_32
    
    Today we have:
    
    config PPC_BOOK3S_32
            bool "512x/52xx/6xx/7xx/74xx/82xx/83xx/86xx"
            [depends on PPC32 within a choice]
    
    config PPC_BOOK3S
            def_bool y
            depends on PPC_BOOK3S_32 || PPC_BOOK3S_64
    
    config PPC_STD_MMU
            def_bool y
            depends on PPC_BOOK3S
    
    config PPC_STD_MMU_32
            def_bool y
            depends on PPC_STD_MMU && PPC32
    
    PPC_STD_MMU_32 is therefore redundant with PPC_BOOK3S_32.
    
    In order to make the code clearer, lets use preferably PPC_BOOK3S_32.
    This will allow to remove CONFIG_PPC_STD_MMU_32 in a later patch.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 47b651deff5b..8a6d3b129c6d 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -338,7 +338,7 @@ static inline void mmu_early_init_devtree(void) { }
 #endif /* __ASSEMBLY__ */
 #endif
 
-#if defined(CONFIG_PPC_STD_MMU_32)
+#if defined(CONFIG_PPC_BOOK3S_32)
 /* 32-bit classic hash table MMU */
 #include <asm/book3s/32/mmu-hash.h>
 #elif defined(CONFIG_40x)

commit d7cceda96badc1bd444cff27ab9c375a1277c1e3
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Nov 17 10:24:56 2018 +0000

    powerpc: change CONFIG_6xx to CONFIG_PPC_BOOK3S_32
    
    Today we have:
    
    config PPC_BOOK3S_32
            bool "512x/52xx/6xx/7xx/74xx/82xx/83xx/86xx"
            [depends on PPC32 within a choice]
    
    config PPC_BOOK3S
            def_bool y
            depends on PPC_BOOK3S_32 || PPC_BOOK3S_64
    
    config 6xx
            def_bool y
            depends on PPC32 && PPC_BOOK3S
    
    6xx is therefore redundant with PPC_BOOK3S_32.
    
    In order to make the code clearer, lets use preferably PPC_BOOK3S_32.
    This will allow to remove CONFIG_6xx in a later patch.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index eb20eb3b8fb0..47b651deff5b 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -48,7 +48,7 @@
 #define MMU_FTR_USE_HIGH_BATS		ASM_CONST(0x00010000)
 
 /* Enable >32-bit physical addresses on 32-bit processor, only used
- * by CONFIG_6xx currently as BookE supports that from day 1
+ * by CONFIG_PPC_BOOK3S_32 currently as BookE supports that from day 1
  */
 #define MMU_FTR_BIG_PHYS		ASM_CONST(0x00020000)
 

commit 4ffe713b7587b14695c9bec26a000fc88ef54895
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Thu Sep 20 14:03:58 2018 +0530

    powerpc/mm: Increase the max addressable memory to 2PB
    
    Currently we limit the max addressable memory to 128TB. This patch increase the
    limit to 2PB. We can have devices like nvdimm which adds memory above 512TB
    limit.
    
    We still don't support regular system ram above 512TB. One of the challenge with
    that is the percpu allocator, that allocates per node memory and use the max
    distance between them as the percpu offsets. This means with large gap in
    address space ( system ram above 1PB) we will run out of vmalloc space to map
    the percpu allocation.
    
    In order to support addressable memory above 512TB, kernel should be able to
    linear map this range. To do that with hash translation we now add 4 context
    to kernel linear map region. Our per context addressable range is 512TB. We
    still keep VMALLOC and VMEMMAP region to old size. SLB miss handlers is updated
    to validate these limit.
    
    We also limit this update to SPARSEMEM_VMEMMAP and SPARSEMEM_EXTREME
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 13ea441ac531..eb20eb3b8fb0 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -309,6 +309,21 @@ static inline u16 get_mm_addr_key(struct mm_struct *mm, unsigned long address)
  */
 #define MMU_PAGE_COUNT	16
 
+/*
+ * If we store section details in page->flags we can't increase the MAX_PHYSMEM_BITS
+ * if we increase SECTIONS_WIDTH we will not store node details in page->flags and
+ * page_to_nid does a page->section->node lookup
+ * Hence only increase for VMEMMAP. Further depending on SPARSEMEM_EXTREME reduce
+ * memory requirements with large number of sections.
+ * 51 bits is the max physical real address on POWER9
+ */
+#if defined(CONFIG_SPARSEMEM_VMEMMAP) && defined(CONFIG_SPARSEMEM_EXTREME) &&	\
+	defined (CONFIG_PPC_64K_PAGES)
+#define MAX_PHYSMEM_BITS        51
+#else
+#define MAX_PHYSMEM_BITS        46
+#endif
+
 #ifdef CONFIG_PPC_BOOK3S_64
 #include <asm/book3s/64/mmu.h>
 #else /* CONFIG_PPC_BOOK3S_64 */

commit 2c86cd188f8a5631f3d75a1dea14d22df85189b4
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jul 5 16:25:01 2018 +0000

    powerpc: clean inclusions of asm/feature-fixups.h
    
    files not using feature fixup don't need asm/feature-fixups.h
    files using feature fixup need asm/feature-fixups.h
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 8418d83b5eb0..13ea441ac531 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -5,7 +5,6 @@
 
 #include <linux/types.h>
 
-#include <asm/feature-fixups.h>
 #include <asm/asm-const.h>
 
 /*

commit ec0c464cdbf38bf6ddabec8bfa595bd421cab203
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jul 5 16:24:57 2018 +0000

    powerpc: move ASM_CONST and stringify_in_c() into asm-const.h
    
    This patch moves ASM_CONST() and stringify_in_c() into
    dedicated asm-const.h, then cleans all related inclusions.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: asm-compat.h should include asm-const.h]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 61d15ce92278..8418d83b5eb0 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -5,8 +5,8 @@
 
 #include <linux/types.h>
 
-#include <asm/asm-compat.h>
 #include <asm/feature-fixups.h>
+#include <asm/asm-const.h>
 
 /*
  * MMU features bit definitions

commit 471d7ff8b51b63521c8ea35c51966ab4caa434ee
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 21 05:08:29 2018 +1000

    powerpc/64s: Remove POWER4 support
    
    POWER4 has been broken since at least the change 49d09bf2a6
    ("powerpc/64s: Optimise MSR handling in exception handling"), which
    requires mtmsrd L=1 support. This was introduced in ISA v2.01, and
    POWER4 supports ISA v2.00.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index bb38312cff28..61d15ce92278 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -111,9 +111,9 @@
 /* MMU feature bit sets for various CPUs */
 #define MMU_FTRS_DEFAULT_HPTE_ARCH_V2	\
 	MMU_FTR_HPTE_TABLE | MMU_FTR_PPCAS_ARCH_V2
-#define MMU_FTRS_POWER4		MMU_FTRS_DEFAULT_HPTE_ARCH_V2
-#define MMU_FTRS_PPC970		MMU_FTRS_POWER4 | MMU_FTR_TLBIE_CROP_VA
-#define MMU_FTRS_POWER5		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
+#define MMU_FTRS_POWER		MMU_FTRS_DEFAULT_HPTE_ARCH_V2
+#define MMU_FTRS_PPC970		MMU_FTRS_POWER | MMU_FTR_TLBIE_CROP_VA
+#define MMU_FTRS_POWER5		MMU_FTRS_POWER | MMU_FTR_LOCKLESS_TLBIE
 #define MMU_FTRS_POWER6		MMU_FTRS_POWER5 | MMU_FTR_KERNEL_RO | MMU_FTR_68_BIT_VA
 #define MMU_FTRS_POWER7		MMU_FTRS_POWER6
 #define MMU_FTRS_POWER8		MMU_FTRS_POWER6

commit 087003e9ef7c1c5bec932387e47511429eff2a54
Author: Ram Pai <linuxram@us.ibm.com>
Date:   Thu Jan 18 17:50:41 2018 -0800

    powerpc: introduce get_mm_addr_key() helper
    
    get_mm_addr_key() helper returns the pkey associated with
    an address corresponding to a given mm_struct.
    
    Signed-off-by: Ram Pai <linuxram@us.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 6364f5c2cc3e..bb38312cff28 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -260,6 +260,15 @@ static inline bool early_radix_enabled(void)
 }
 #endif
 
+#ifdef CONFIG_PPC_MEM_KEYS
+extern u16 get_mm_addr_key(struct mm_struct *mm, unsigned long address);
+#else
+static inline u16 get_mm_addr_key(struct mm_struct *mm, unsigned long address)
+{
+	return 0;
+}
+#endif /* CONFIG_PPC_MEM_KEYS */
+
 #endif /* !__ASSEMBLY__ */
 
 /* The kernel use the constants below to index in the page sizes array.

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 78260409dc9c..6364f5c2cc3e 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_POWERPC_MMU_H_
 #define _ASM_POWERPC_MMU_H_
 #ifdef __KERNEL__

commit e6f81a92015b2c1d12fad32b8456f99855da6e13
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Mar 29 17:21:53 2017 +1100

    powerpc/mm/hash: Support 68 bit VA
    
    Inorder to support large effective address range (512TB), we want to
    increase the virtual address bits to 68. But we do have platforms like
    p4 and p5 that can only do 65 bit VA. We support those platforms by
    limiting context bits on them to 16.
    
    The protovsid -> vsid conversion is verified to work with both 65 and 68
    bit va values. I also documented the restrictions in a table format as
    part of code comments.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 065e762fae85..78260409dc9c 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -28,6 +28,10 @@
  * Individual features below.
  */
 
+/*
+ * Support for 68 bit VA space. We added that from ISA 2.05
+ */
+#define MMU_FTR_68_BIT_VA		ASM_CONST(0x00002000)
 /*
  * Kernel read only support.
  * We added the ppp value 0b110 in ISA 2.04.
@@ -109,10 +113,10 @@
 #define MMU_FTRS_POWER4		MMU_FTRS_DEFAULT_HPTE_ARCH_V2
 #define MMU_FTRS_PPC970		MMU_FTRS_POWER4 | MMU_FTR_TLBIE_CROP_VA
 #define MMU_FTRS_POWER5		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
-#define MMU_FTRS_POWER6		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_KERNEL_RO
-#define MMU_FTRS_POWER7		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_KERNEL_RO
-#define MMU_FTRS_POWER8		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_KERNEL_RO
-#define MMU_FTRS_POWER9		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_KERNEL_RO
+#define MMU_FTRS_POWER6		MMU_FTRS_POWER5 | MMU_FTR_KERNEL_RO | MMU_FTR_68_BIT_VA
+#define MMU_FTRS_POWER7		MMU_FTRS_POWER6
+#define MMU_FTRS_POWER8		MMU_FTRS_POWER6
+#define MMU_FTRS_POWER9		MMU_FTRS_POWER6
 #define MMU_FTRS_CELL		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \
 				MMU_FTR_CI_LARGE_PAGE
 #define MMU_FTRS_PA6T		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \
@@ -136,7 +140,7 @@ enum {
 		MMU_FTR_NO_SLBIE_B | MMU_FTR_16M_PAGE | MMU_FTR_TLBIEL |
 		MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_CI_LARGE_PAGE |
 		MMU_FTR_1T_SEGMENT | MMU_FTR_TLBIE_CROP_VA |
-		MMU_FTR_KERNEL_RO |
+		MMU_FTR_KERNEL_RO | MMU_FTR_68_BIT_VA |
 #ifdef CONFIG_PPC_RADIX_MMU
 		MMU_FTR_TYPE_RADIX |
 #endif
@@ -290,7 +294,10 @@ static inline bool early_radix_enabled(void)
 #define MMU_PAGE_16G	14
 #define MMU_PAGE_64G	15
 
-/* N.B. we need to change the type of hpte_page_sizes if this gets to be > 16 */
+/*
+ * N.B. we need to change the type of hpte_page_sizes if this gets to be > 16
+ * Also we need to change he type of mm_context.low/high_slices_psize.
+ */
 #define MMU_PAGE_COUNT	16
 
 #ifdef CONFIG_PPC_BOOK3S_64

commit 38705613b74ab090eee55c327cd0cb77fb10eb26
Merge: ff47d8c05019 438e69b52be7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 10:30:38 2017 -0800

    Merge tag 'powerpc-4.11-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for direct mapped LPC on POWER9, giving Linux direct access
         to devices that may be on there such as a UART.
    
       - Memory hotplug support for the Power9 Radix MMU.
    
       - Add new AUX vectors describing the processor's cache geometry, to
         be used by glibc.
    
       - The ability for a guest to ask the hypervisor to resize the guest's
         hash table, and in addition support for doing so automatically when
         memory is hotplugged into/out-of the guest. This allows the hash
         table to be sized based on the current memory usage of the guest,
         rather than the maximum possible memory usage.
    
       - Implementation of optprobes (kprobe optimisation) for powerpc.
    
      In addition there's the topic branch shared with the KVM tree, which
      includes support for guests to use the Radix MMU on Power9.
    
      Thanks to:
        Alistair Popple, Andrew Donnellan, Aneesh Kumar K.V, Anju T, Anton
        Blanchard, Benjamin Herrenschmidt, Chris Packham, Daniel Axtens,
        Daniel Borkmann, David Gibson, Finn Thain, Gautham R. Shenoy, Gavin
        Shan, Greg Kurz, Joel Stanley, John Allen, Madhavan Srinivasan,
        Mahesh Salgaonkar, Markus Elfring, Michael Neuling, Nathan Fontenot,
        Naveen N. Rao, Nicholas Piggin, Paul Mackerras, Ravi Bangoria, Reza
        Arbab, Shailendra Singh, Vaibhav Jain, Wei Yongjun"
    
    * tag 'powerpc-4.11-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (129 commits)
      powerpc/mm/radix: Skip ptesync in pte update helpers
      powerpc/mm/radix: Use ptep_get_and_clear_full when clearing pte for full mm
      powerpc/mm/radix: Update pte update sequence for pte clear case
      powerpc/mm: Update PROTFAULT handling in the page fault path
      powerpc/xmon: Fix data-breakpoint
      powerpc/mm: Fix build break with BOOK3S_64=n and MEMORY_HOTPLUG=y
      powerpc/mm: Fix build break when CMA=n && SPAPR_TCE_IOMMU=y
      powerpc/mm: Fix build break with RADIX=y & HUGETLBFS=n
      powerpc/pseries: Fix typo in parameter description
      powerpc/kprobes: Remove kprobe_exceptions_notify()
      kprobes: Introduce weak variant of kprobe_exceptions_notify()
      powerpc/ftrace: Fix confusing help text for DISABLE_MPROFILE_KERNEL
      powerpc/powernv: Fix opal_exit tracepoint opcode
      powerpc: Add a prototype for mcount() so it can be versioned
      powerpc: Drop GPL from of_node_to_nid() export to match other arches
      powerpc/kprobes: Optimize kprobe in kretprobe_trampoline()
      powerpc/kprobes: Implement Optprobes
      powerpc/kprobes: Fixes for kprobe_lookup_name() on BE
      powerpc: Add helper to check if offset is within relative branch range
      powerpc/bpf: Introduce __PPC_SH64()
      ...

commit a5ecdad4847897007399d7a14c9109b65ce4c9b7
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Feb 7 00:09:27 2017 +0530

    powerpc/mm: Add MMU_FTR_KERNEL_RO to possible feature mask
    
    Without this we will always find the feature disabled.
    
    Fixes: 984d7a1ec6 ("powerpc/mm: Fixup kernel read only mapping")
    Cc: stable@vger.kernel.org # v4.7+
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index a34c764ca8dd..e5616bf83623 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -136,6 +136,7 @@ enum {
 		MMU_FTR_NO_SLBIE_B | MMU_FTR_16M_PAGE | MMU_FTR_TLBIEL |
 		MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_CI_LARGE_PAGE |
 		MMU_FTR_1T_SEGMENT | MMU_FTR_TLBIE_CROP_VA |
+		MMU_FTR_KERNEL_RO |
 #ifdef CONFIG_PPC_RADIX_MMU
 		MMU_FTR_TYPE_RADIX |
 #endif

commit b5fa0f7f88edcde37df1807fdf9ff10ec787a60e
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jan 24 16:36:57 2017 +1100

    powerpc: Fix build failure with clang due to BUILD_BUG_ON()
    
    Anton says: In commit 4db7327194db ("powerpc: Add option to use jump
    label for cpu_has_feature()") and commit c12e6f24d413 ("powerpc: Add
    option to use jump label for mmu_has_feature()") we added:
    
      BUILD_BUG_ON(!__builtin_constant_p(feature))
    
    to cpu_has_feature() and mmu_has_feature() in order to catch usage
    issues (such as cpu_has_feature(cpu_has_feature(X), which has happened
    once in the past). Unfortunately LLVM isn't smart enough to resolve
    this, and it errors out.
    
    I work around it in my clang/LLVM builds of the kernel, but I have just
    discovered that it causes a lot of issues for the bcc (eBPF) trace tool
    (which uses LLVM).
    
    For now just #ifdef it away for clang builds.
    
    Fixes: 4db7327194db ("powerpc: Add option to use jump label for cpu_has_feature()")
    Fixes: c12e6f24d413 ("powerpc: Add option to use jump label for mmu_has_feature()")
    Cc: stable@vger.kernel.org # v4.8+
    Reported-by: Anton Blanchard <anton@samba.org>
    Tested-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index a34c764ca8dd..233a7e8cc8e3 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -160,7 +160,9 @@ static __always_inline bool mmu_has_feature(unsigned long feature)
 {
 	int i;
 
+#ifndef __clang__ /* clang can't cope with this */
 	BUILD_BUG_ON(!__builtin_constant_p(feature));
+#endif
 
 #ifdef CONFIG_JUMP_LABEL_FEATURE_CHECK_DEBUG
 	if (!static_key_initialized) {

commit de399813b521ea7e38bbfb5e5b620b5e202e5783
Merge: 57ca04ab4401 c6f6634721c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 16 09:26:42 2016 -0800

    Merge tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for the kexec_file_load() syscall, which is a prereq for
         secure and trusted boot.
    
       - Prevent kernel execution of userspace on P9 Radix (similar to
         SMEP/PXN).
    
       - Sort the exception tables at build time, to save time at boot, and
         store them as relative offsets to save space in the kernel image &
         memory.
    
       - Allow building the kernel with thin archives, which should allow us
         to build an allyesconfig once some other fixes land.
    
       - Build fixes to allow us to correctly rebuild when changing the
         kernel endian from big to little or vice versa.
    
       - Plumbing so that we can avoid doing a full mm TLB flush on P9
         Radix.
    
       - Initial stack protector support (-fstack-protector).
    
       - Support for dumping the radix (aka. Linux) and hash page tables via
         debugfs.
    
       - Fix an oops in cxl coredump generation when cxl_get_fd() is used.
    
       - Freescale updates from Scott: "Highlights include 8xx hugepage
         support, qbman fixes/cleanup, device tree updates, and some misc
         cleanup."
    
       - Many and varied fixes and minor enhancements as always.
    
      Thanks to:
        Alexey Kardashevskiy, Andrew Donnellan, Aneesh Kumar K.V, Anshuman
        Khandual, Anton Blanchard, Balbir Singh, Bartlomiej Zolnierkiewicz,
        Christophe Jaillet, Christophe Leroy, Denis Kirjanov, Elimar
        Riesebieter, Frederic Barrat, Gautham R. Shenoy, Geliang Tang, Geoff
        Levand, Jack Miller, Johan Hovold, Lars-Peter Clausen, Libin,
        Madhavan Srinivasan, Michael Neuling, Nathan Fontenot, Naveen N.
        Rao, Nicholas Piggin, Pan Xinhui, Peter Senna Tschudin, Rashmica
        Gupta, Rui Teng, Russell Currey, Scott Wood, Simon Guo, Suraj
        Jitindar Singh, Thiago Jung Bauermann, Tobias Klauser, Vaibhav Jain"
    
    [ And thanks to Michael, who took time off from a new baby to get this
      pull request done.   - Linus ]
    
    * tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (174 commits)
      powerpc/fsl/dts: add FMan node for t1042d4rdb
      powerpc/fsl/dts: add sg_2500_aqr105_phy4 alias on t1024rdb
      powerpc/fsl/dts: add QMan and BMan nodes on t1024
      powerpc/fsl/dts: add QMan and BMan nodes on t1023
      soc/fsl/qman: test: use DEFINE_SPINLOCK()
      powerpc/fsl-lbc: use DEFINE_SPINLOCK()
      powerpc/8xx: Implement support of hugepages
      powerpc: get hugetlbpage handling more generic
      powerpc: port 64 bits pgtable_cache to 32 bits
      powerpc/boot: Request no dynamic linker for boot wrapper
      soc/fsl/bman: Use resource_size instead of computation
      soc/fsl/qe: use builtin_platform_driver
      powerpc/fsl_pmc: use builtin_platform_driver
      powerpc/83xx/suspend: use builtin_platform_driver
      powerpc/ftrace: Fix the comments for ftrace_modify_code
      powerpc/perf: macros for power9 format encoding
      powerpc/perf: power9 raw event format encoding
      powerpc/perf: update attribute_group data structure
      powerpc/perf: factor out the event format field
      powerpc/mm/iommu, vfio/spapr: Put pages on VFIO container shutdown
      ...

commit c6f6634721c871bfab4235e1cbcad208d3063798
Merge: ff45000fcb56 baae856ebdee
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Dec 16 15:05:38 2016 +1100

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/scottwood/linux into next
    
    Freescale updates from Scott:
    
    "Highlights include 8xx hugepage support, qbman fixes/cleanup, device
    tree updates, and some misc cleanup."

commit 93173b5bf2841da7e3a9b0cb1312ef5c87251524
Merge: 1c59e1edb13d f673b5b2a663
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 13 15:47:02 2016 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small release, the most interesting stuff is x86 nested virt
      improvements.
    
      x86:
       - userspace can now hide nested VMX features from guests
       - nested VMX can now run Hyper-V in a guest
       - support for AVX512_4VNNIW and AVX512_FMAPS in KVM
       - infrastructure support for virtual Intel GPUs.
    
      PPC:
       - support for KVM guests on POWER9
       - improved support for interrupt polling
       - optimizations and cleanups.
    
      s390:
       - two small optimizations, more stuff is in flight and will be in
         4.11.
    
      ARM:
       - support for the GICv3 ITS on 32bit platforms"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (94 commits)
      arm64: KVM: pmu: Reset PMSELR_EL0.SEL to a sane value before entering the guest
      KVM: arm/arm64: timer: Check for properly initialized timer on init
      KVM: arm/arm64: vgic-v2: Limit ITARGETSR bits to number of VCPUs
      KVM: x86: Handle the kthread worker using the new API
      KVM: nVMX: invvpid handling improvements
      KVM: nVMX: check host CR3 on vmentry and vmexit
      KVM: nVMX: introduce nested_vmx_load_cr3 and call it on vmentry
      KVM: nVMX: propagate errors from prepare_vmcs02
      KVM: nVMX: fix CR3 load if L2 uses PAE paging and EPT
      KVM: nVMX: load GUEST_EFER after GUEST_CR0 during emulated VM-entry
      KVM: nVMX: generate MSR_IA32_CR{0,4}_FIXED1 from guest CPUID
      KVM: nVMX: fix checks on CR{0,4} during virtual VMX operation
      KVM: nVMX: support restore of VMX capability MSRs
      KVM: nVMX: generate non-true VMX MSRs based on true versions
      KVM: x86: Do not clear RFLAGS.TF when a singlestep trap occurs.
      KVM: x86: Add kvm_skip_emulated_instruction and use it.
      KVM: VMX: Move skip_emulated_instruction out of nested_vmx_check_vmcs12
      KVM: VMX: Reorder some skip_emulated_instruction calls
      KVM: x86: Add a return value to kvm_emulate_cpuid
      KVM: PPC: Book3S: Move prototypes for KVM functions into kvm_ppc.h
      ...

commit 4b91428699477532ab1255c2dd5819713e9e8985
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Wed Dec 7 08:47:28 2016 +0100

    powerpc/8xx: Implement support of hugepages
    
    8xx uses a two level page table with two different linux page size
    support (4k and 16k). 8xx also support two different hugepage sizes
    512k and 8M. In order to support them on linux we define two different
    page table layout.
    
    The size of pages is in the PGD entry, using PS field (bits 28-29):
    00 : Small pages (4k or 16k)
    01 : 512k pages
    10 : reserved
    11 : 8M pages
    
    For 512K hugepage size a pgd entry have the below format
    [<hugepte address >0101] . The hugepte table allocated will contain 8
    entries pointing to 512K huge pte in 4k pages mode and 64 entries in
    16k pages mode.
    
    For 8M in 16k mode, a pgd entry have the below format
    [<hugepte address >1101] . The hugepte table allocated will contain 8
    entries pointing to 8M huge pte.
    
    For 8M in 4k mode, multiple pgd entries point to the same hugepte
    address and pgd entry will have the below format
    [<hugepte address>1101]. The hugepte table allocated will only have one
    entry.
    
    For the time being, we do not support CPU15 ERRATA when HUGETLB is
    selected
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com> (v3, for the generic bits)
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index e88368354e49..b119bdd6ed27 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -264,19 +264,20 @@ static inline bool early_radix_enabled(void)
 #define MMU_PAGE_64K	2
 #define MMU_PAGE_64K_AP	3	/* "Admixed pages" (hash64 only) */
 #define MMU_PAGE_256K	4
-#define MMU_PAGE_1M	5
-#define MMU_PAGE_2M	6
-#define MMU_PAGE_4M	7
-#define MMU_PAGE_8M	8
-#define MMU_PAGE_16M	9
-#define MMU_PAGE_64M	10
-#define MMU_PAGE_256M	11
-#define MMU_PAGE_1G	12
-#define MMU_PAGE_16G	13
-#define MMU_PAGE_64G	14
+#define MMU_PAGE_512K	5
+#define MMU_PAGE_1M	6
+#define MMU_PAGE_2M	7
+#define MMU_PAGE_4M	8
+#define MMU_PAGE_8M	9
+#define MMU_PAGE_16M	10
+#define MMU_PAGE_64M	11
+#define MMU_PAGE_256M	12
+#define MMU_PAGE_1G	13
+#define MMU_PAGE_16G	14
+#define MMU_PAGE_64G	15
 
 /* N.B. we need to change the type of hpte_page_sizes if this gets to be > 16 */
-#define MMU_PAGE_COUNT	15
+#define MMU_PAGE_COUNT	16
 
 #ifdef CONFIG_PPC_BOOK3S_64
 #include <asm/book3s/64/mmu.h>

commit 984d7a1ec67ce3a46324fa4bcb4c745bbc266cf2
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Nov 24 15:09:54 2016 +0530

    powerpc/mm: Fixup kernel read only mapping
    
    With commit e58e87adc8bf9 ("powerpc/mm: Update _PAGE_KERNEL_RO") we
    started using the ppp value 0b110 to map kernel readonly. But that
    facility was only added as part of ISA 2.04. For earlier ISA version
    only supported ppp bit value for readonly mapping is 0b011. (This
    implies both user and kernel get mapped using the same ppp bit value for
    readonly mapping.).
    Update the code such that for earlier architecture version we use ppp
    value 0b011 for readonly mapping. We don't differentiate between power5+
    and power5 here and apply the new ppp bits only from power6 (ISA 2.05).
    This keep the changes minimal.
    
    This fixes issue with PS3 spu usage reported at
    https://lkml.kernel.org/r/rep.1421449714.geoff@infradead.org
    
    Fixes: e58e87adc8bf9 ("powerpc/mm: Update _PAGE_KERNEL_RO")
    Cc: stable@vger.kernel.org # v4.7+
    Tested-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index e88368354e49..e311c25751a4 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -28,6 +28,12 @@
  * Individual features below.
  */
 
+/*
+ * Kernel read only support.
+ * We added the ppp value 0b110 in ISA 2.04.
+ */
+#define MMU_FTR_KERNEL_RO		ASM_CONST(0x00004000)
+
 /*
  * We need to clear top 16bits of va (from the remaining 64 bits )in
  * tlbie* instructions
@@ -103,10 +109,10 @@
 #define MMU_FTRS_POWER4		MMU_FTRS_DEFAULT_HPTE_ARCH_V2
 #define MMU_FTRS_PPC970		MMU_FTRS_POWER4 | MMU_FTR_TLBIE_CROP_VA
 #define MMU_FTRS_POWER5		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
-#define MMU_FTRS_POWER6		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
-#define MMU_FTRS_POWER7		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
-#define MMU_FTRS_POWER8		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
-#define MMU_FTRS_POWER9		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
+#define MMU_FTRS_POWER6		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_KERNEL_RO
+#define MMU_FTRS_POWER7		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_KERNEL_RO
+#define MMU_FTRS_POWER8		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_KERNEL_RO
+#define MMU_FTRS_POWER9		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_KERNEL_RO
 #define MMU_FTRS_CELL		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \
 				MMU_FTR_CI_LARGE_PAGE
 #define MMU_FTRS_PA6T		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \

commit 9d66195807ac6cb8a14231fd055ff755977c5fca
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Nov 21 16:00:58 2016 +1100

    powerpc/64: Provide functions for accessing POWER9 partition table
    
    POWER9 requires the host to set up a partition table, which is a
    table in memory indexed by logical partition ID (LPID) which
    contains the pointers to page tables and process tables for the
    host and each guest.
    
    This factors out the initialization of the partition table into
    a single function.  This code was previously duplicated between
    hash_utils_64.c and pgtable-radix.c.
    
    This provides a function for setting a partition table entry,
    which is used in early MMU initialization, and will be used by
    KVM whenever a guest is created.  This function includes a tlbie
    instruction which will flush all TLB entries for the LPID and
    all caches of the partition table entry for the LPID, across the
    system.
    
    This also moves a call to memblock_set_current_limit(), which was
    in radix_init_partition_table(), but has nothing to do with the
    partition table.  By analogy with the similar code for hash, the
    call gets moved to near the end of radix__early_init_mmu().  It
    now gets called when running as a guest, whereas previously it
    would only be called if the kernel is running as the host.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index e88368354e49..060b40b1bc3d 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -208,6 +208,11 @@ extern u64 ppc64_rma_size;
 /* Cleanup function used by kexec */
 extern void mmu_cleanup_all(void);
 extern void radix__mmu_cleanup_all(void);
+
+/* Functions for creating and updating partition table on POWER9 */
+extern void mmu_partition_table_init(void);
+extern void mmu_partition_table_set_entry(unsigned int lpid, unsigned long dw0,
+					  unsigned long dw1);
 #endif /* CONFIG_PPC64 */
 
 struct mm_struct;

commit 07021b43597f506cc525d139ed1a94e79cf184f2
Merge: d1f5323370fc b7b7013cac55
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 7 20:19:31 2016 -0700

    Merge tag 'powerpc-4.9-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights:
       - Major rework of Book3S 64-bit exception vectors (Nicholas Piggin)
       - Use gas sections for arranging exception vectors et. al.
       - Large set of TM cleanups and selftests (Cyril Bur)
       - Enable transactional memory (TM) lazily for userspace (Cyril Bur)
       - Support for XZ compression in the zImage wrapper (Oliver
         O'Halloran)
       - Add support for bpf constant blinding (Naveen N. Rao)
       - Beginnings of upstream support for PA Semi Nemo motherboards
         (Darren Stevens)
    
      Fixes:
       - Ensure .mem(init|exit).text are within _stext/_etext (Michael
         Ellerman)
       - xmon: Don't use ld on 32-bit (Michael Ellerman)
       - vdso64: Use double word compare on pointers (Anton Blanchard)
       - powerpc/nvram: Fix an incorrect partition merge (Pan Xinhui)
       - powerpc: Fix usage of _PAGE_RO in hugepage (Christophe Leroy)
       - powerpc/mm: Update FORCE_MAX_ZONEORDER range to allow hugetlb w/4K
         (Aneesh Kumar K.V)
       - Fix memory leak in queue_hotplug_event() error path (Andrew
         Donnellan)
       - Replay hypervisor maintenance interrupt first (Nicholas Piggin)
    
      Various performance optimisations (Anton Blanchard):
       - Align hot loops of memset() and backwards_memcpy()
       - During context switch, check before setting mm_cpumask
       - Remove static branch prediction in atomic{, 64}_add_unless
       - Only disable HAVE_EFFICIENT_UNALIGNED_ACCESS on POWER7 little
         endian
       - Set default CPU type to POWER8 for little endian builds
    
      Cleanups & features:
       - Sparse fixes/cleanups (Daniel Axtens)
       - Preserve CFAR value on SLB miss caused by access to bogus address
         (Paul Mackerras)
       - Radix MMU fixups for POWER9 (Aneesh Kumar K.V)
       - Support for setting used_(vsr|vr|spe) in sigreturn path (for CRIU)
         (Simon Guo)
       - Optimise syscall entry for virtual, relocatable case (Nicholas
         Piggin)
       - Optimise MSR handling in exception handling (Nicholas Piggin)
       - Support for kexec with Radix MMU (Benjamin Herrenschmidt)
       - powernv EEH fixes (Russell Currey)
       - Suprise PCI hotplug support for powernv (Gavin Shan)
       - Endian/sparse fixes for powernv PCI (Gavin Shan)
       - Defconfig updates (Anton Blanchard)
       - KVM: PPC: Book3S HV: Migrate pinned pages out of CMA (Balbir Singh)
       - cxl: Flush PSL cache before resetting the adapter (Frederic Barrat)
       - cxl: replace loop with for_each_child_of_node(), remove unneeded
         of_node_put() (Andrew Donnellan)
       - Fix HV facility unavailable to use correct handler (Nicholas
         Piggin)
       - Remove unnecessary syscall trampoline (Nicholas Piggin)
       - fadump: Fix build break when CONFIG_PROC_VMCORE=n (Michael
         Ellerman)
       - Quieten EEH message when no adapters are found (Anton Blanchard)
       - powernv: Add PHB register dump debugfs handle (Russell Currey)
       - Use kprobe blacklist for exception handlers & asm functions
         (Nicholas Piggin)
       - Document the syscall ABI (Nicholas Piggin)
       - MAINTAINERS: Update cxl maintainers (Michael Neuling)
       - powerpc: Remove all usages of NO_IRQ (Michael Ellerman)
    
      Minor cleanups:
       - Andrew Donnellan, Christophe Leroy, Colin Ian King, Cyril Bur,
         Frederic Barrat, Pan Xinhui, PrasannaKumar Muralidharan, Rui Teng,
         Simon Guo"
    
    * tag 'powerpc-4.9-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (156 commits)
      powerpc/bpf: Add support for bpf constant blinding
      powerpc/bpf: Implement support for tail calls
      powerpc/bpf: Introduce accessors for using the tmp local stack space
      powerpc/fadump: Fix build break when CONFIG_PROC_VMCORE=n
      powerpc: tm: Enable transactional memory (TM) lazily for userspace
      powerpc/tm: Add TM Unavailable Exception
      powerpc: Remove do_load_up_transact_{fpu,altivec}
      powerpc: tm: Rename transct_(*) to ck(\1)_state
      powerpc: tm: Always use fp_state and vr_state to store live registers
      selftests/powerpc: Add checks for transactional VSXs in signal contexts
      selftests/powerpc: Add checks for transactional VMXs in signal contexts
      selftests/powerpc: Add checks for transactional FPUs in signal contexts
      selftests/powerpc: Add checks for transactional GPRs in signal contexts
      selftests/powerpc: Check that signals always get delivered
      selftests/powerpc: Add TM tcheck helpers in C
      selftests/powerpc: Allow tests to extend their kill timeout
      selftests/powerpc: Introduce GPR asm helper header file
      selftests/powerpc: Move VMX stack frame macros to header file
      selftests/powerpc: Rework FPU stack placement macros and move to header file
      selftests/powerpc: Check for VSX preservation across userspace preemption
      ...

commit fe036a0605d60d6c81ffdcd6241e9ae0013fe235
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Aug 19 14:22:37 2016 +0530

    powerpc/64/kexec: Fix MMU cleanup on radix
    
    Just using the hash ops won't work anymore since radix will have
    NULL in there. Instead create an mmu_cleanup_all() function which
    will do the right thing based on the MMU mode.
    
    For Radix, for now I clear UPRT and the PTCR, effectively switching
    back to Radix with no partition table setup.
    
    Currently set it to NULL on BookE thought it might be a good idea
    to wipe the TLB there (Scott ?)
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index e2fb408f8398..79c989a05aa1 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -204,6 +204,10 @@ extern unsigned int __start___mmu_ftr_fixup, __stop___mmu_ftr_fixup;
  * make it match the size our of bolted TLB area
  */
 extern u64 ppc64_rma_size;
+
+/* Cleanup function used by kexec */
+extern void mmu_cleanup_all(void);
+extern void radix__mmu_cleanup_all(void);
 #endif /* CONFIG_PPC64 */
 
 struct mm_struct;

commit 0eeede0c63305a33de31bd90b53b023c1d452c17
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Sep 2 17:20:43 2016 +1000

    powerpc/mm: Speed up computation of base and actual page size for a HPTE
    
    This replaces a 2-D search through an array with a simple 8-bit table
    lookup for determining the actual and/or base page size for a HPT entry.
    
    The encoding in the second doubleword of the HPTE is designed to encode
    the actual and base page sizes without using any more bits than would be
    needed for a 4k page number, by using between 1 and 8 low-order bits of
    the RPN (real page number) field to encode the page sizes.  A single
    "large page" bit in the first doubleword indicates that these low-order
    bits are to be interpreted like this.
    
    We can determine the page sizes by using the low-order 8 bits of the RPN
    to look up a 256-entry table.  For actual page sizes less than 1MB, some
    of the upper bits of these 8 bits are going to be real address bits, but
    we can cope with that by replicating the entries for those smaller page
    sizes.
    
    While we're at it, let's move the hpte_page_size() and hpte_base_page_size()
    functions from a KVM-specific header to a header for 64-bit HPT systems,
    since this computation doesn't have anything specifically to do with KVM.
    
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index e2fb408f8398..b78e8d3377f6 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -271,6 +271,7 @@ static inline bool early_radix_enabled(void)
 #define MMU_PAGE_16G	13
 #define MMU_PAGE_64G	14
 
+/* N.B. we need to change the type of hpte_page_sizes if this gets to be > 16 */
 #define MMU_PAGE_COUNT	15
 
 #ifdef CONFIG_PPC_BOOK3S_64

commit c812c7d8f1470ac9c8aa6d7e29b56e5845ee05fc
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Sat Jul 23 14:42:43 2016 +0530

    powerpc/mm: Catch usage of cpu/mmu_has_feature() before jump label init
    
    This allows us to catch incorrect usage of cpu_has_feature() and
    mmu_has_feature() prior to jump labels being initialised.
    
    mpe: Use printk() and dump_stack() rather than WARN_ON(), because
    WARN_ON() may not work this early in boot. Rename the Kconfig.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 1bf919aafd50..e2fb408f8398 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -156,6 +156,14 @@ static __always_inline bool mmu_has_feature(unsigned long feature)
 
 	BUILD_BUG_ON(!__builtin_constant_p(feature));
 
+#ifdef CONFIG_JUMP_LABEL_FEATURE_CHECK_DEBUG
+	if (!static_key_initialized) {
+		printk("Warning! mmu_has_feature() used prior to jump label init!\n");
+		dump_stack();
+		return early_mmu_has_feature(feature);
+	}
+#endif
+
 	if (!(MMU_FTRS_POSSIBLE & feature))
 		return false;
 

commit c12e6f24d4137822d5019c1f78ac65bd27a3447d
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sat Jul 23 14:42:42 2016 +0530

    powerpc: Add option to use jump label for mmu_has_feature()
    
    As we just did for CPU features.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 37f55169df3e..1bf919aafd50 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -141,6 +141,43 @@ static inline bool early_mmu_has_feature(unsigned long feature)
 	return !!(MMU_FTRS_POSSIBLE & cur_cpu_spec->mmu_features & feature);
 }
 
+#ifdef CONFIG_JUMP_LABEL_FEATURE_CHECKS
+#include <linux/jump_label.h>
+
+#define NUM_MMU_FTR_KEYS	32
+
+extern struct static_key_true mmu_feature_keys[NUM_MMU_FTR_KEYS];
+
+extern void mmu_feature_keys_init(void);
+
+static __always_inline bool mmu_has_feature(unsigned long feature)
+{
+	int i;
+
+	BUILD_BUG_ON(!__builtin_constant_p(feature));
+
+	if (!(MMU_FTRS_POSSIBLE & feature))
+		return false;
+
+	i = __builtin_ctzl(feature);
+	return static_branch_likely(&mmu_feature_keys[i]);
+}
+
+static inline void mmu_clear_feature(unsigned long feature)
+{
+	int i;
+
+	i = __builtin_ctzl(feature);
+	cur_cpu_spec->mmu_features &= ~feature;
+	static_branch_disable(&mmu_feature_keys[i]);
+}
+#else
+
+static inline void mmu_feature_keys_init(void)
+{
+
+}
+
 static inline bool mmu_has_feature(unsigned long feature)
 {
 	return early_mmu_has_feature(feature);
@@ -150,6 +187,7 @@ static inline void mmu_clear_feature(unsigned long feature)
 {
 	cur_cpu_spec->mmu_features &= ~feature;
 }
+#endif /* CONFIG_JUMP_LABEL */
 
 extern unsigned int __start___mmu_ftr_fixup, __stop___mmu_ftr_fixup;
 

commit 4db7327194dba0cb91f274b0f606785a9ee5108d
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sat Jul 23 14:42:41 2016 +0530

    powerpc: Add option to use jump label for cpu_has_feature()
    
    We do binary patching of asm code using CPU features, which is a
    one-time operation, done during early boot. However checks of CPU
    features in C code are currently done at run time, even though the set
    of CPU features can never change after boot.
    
    We can optimise this by using jump labels to implement cpu_has_feature(),
    meaning checks in C code are binary patched into a single nop or branch.
    
    For a C sequence along the lines of:
    
        if (cpu_has_feature(FOO))
             return 2;
    
    The generated code before is roughly:
    
        ld      r9,-27640(r2)
        ld      r9,0(r9)
        lwz     r9,32(r9)
        cmpwi   cr7,r9,0
        bge     cr7, 1f
        li      r3,2
        blr
    1:  ...
    
    After (true):
        nop
        li      r3,2
        blr
    
    After (false):
        b   1f
        li      r3,2
        blr
    1:  ...
    
    mpe: Rename MAX_CPU_FEATURES as we already have a #define with that
    name, and define it simply as a constant, rather than doing tricks with
    sizeof and NULL pointers. Rename the array to cpu_feature_keys. Use the
    kconfig we added to guard it. Add BUILD_BUG_ON() if the feature is not a
    compile time constant. Rewrite the change log.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 08b4c06604d6..37f55169df3e 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -112,6 +112,7 @@
 #define MMU_FTRS_PA6T		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \
 				MMU_FTR_CI_LARGE_PAGE | MMU_FTR_NO_SLBIE_B
 #ifndef __ASSEMBLY__
+#include <linux/bug.h>
 #include <asm/cputable.h>
 
 #ifdef CONFIG_PPC_FSL_BOOK3E

commit a141cca3892bb391d17a73dae917ad51d40ff69a
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jul 27 20:48:36 2016 +1000

    powerpc/mm: Add early_[cpu|mmu]_has_feature()
    
    In later patches, we will be switching CPU and MMU feature checks to
    use static keys.
    
    For checks in early boot before jump label is initialized we need a
    variant of [cpu|mmu]_has_feature() that doesn't use jump labels.
    
    So create those called, unimaginatively, early_[cpu|mmu]_has_feature().
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index f413b3213a3b..08b4c06604d6 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -135,11 +135,16 @@ enum {
 		0,
 };
 
-static inline bool mmu_has_feature(unsigned long feature)
+static inline bool early_mmu_has_feature(unsigned long feature)
 {
 	return !!(MMU_FTRS_POSSIBLE & cur_cpu_spec->mmu_features & feature);
 }
 
+static inline bool mmu_has_feature(unsigned long feature)
+{
+	return early_mmu_has_feature(feature);
+}
+
 static inline void mmu_clear_feature(unsigned long feature)
 {
 	cur_cpu_spec->mmu_features &= ~feature;
@@ -168,11 +173,21 @@ static inline bool radix_enabled(void)
 {
 	return mmu_has_feature(MMU_FTR_TYPE_RADIX);
 }
+
+static inline bool early_radix_enabled(void)
+{
+	return early_mmu_has_feature(MMU_FTR_TYPE_RADIX);
+}
 #else
 static inline bool radix_enabled(void)
 {
 	return false;
 }
+
+static inline bool early_radix_enabled(void)
+{
+	return false;
+}
 #endif
 
 #endif /* !__ASSEMBLY__ */

commit bab4c8de6289b4615c21ddf0400397d03ce1863c
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jul 27 13:37:58 2016 +1000

    powerpc/mm: Define radix_enabled() in one place & use static inline
    
    Currently we have radix_enabled() three times, twice in asm/book3s/64/mmu.h
    and then a fallback in asm/mmu.h.
    
    Consolidate them in asm/mmu.h. While we're at it convert them to be
    static inlines, and change the fallback case to returning a bool, like
    mmu_has_feature().
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index eb942a446969..f413b3213a3b 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -163,6 +163,18 @@ static inline void assert_pte_locked(struct mm_struct *mm, unsigned long addr)
 }
 #endif /* !CONFIG_DEBUG_VM */
 
+#ifdef CONFIG_PPC_RADIX_MMU
+static inline bool radix_enabled(void)
+{
+	return mmu_has_feature(MMU_FTR_TYPE_RADIX);
+}
+#else
+static inline bool radix_enabled(void)
+{
+	return false;
+}
+#endif
+
 #endif /* !__ASSEMBLY__ */
 
 /* The kernel use the constants below to index in the page sizes array.
@@ -230,9 +242,5 @@ static inline void mmu_early_init_devtree(void) { }
 #  include <asm/mmu-8xx.h>
 #endif
 
-#ifndef radix_enabled
-#define radix_enabled() (0)
-#endif
-
 #endif /* __KERNEL__ */
 #endif /* _ASM_POWERPC_MMU_H_ */

commit a81dc9d9957354ee2ebc2e387cbc10cf457b9948
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jul 27 13:39:42 2016 +1000

    powerpc/kernel: Convert mmu_has_feature() to returning bool
    
    The intention is that the result is only used as a boolean, so enforce
    that by changing the return type to bool.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 599781e48552..eb942a446969 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -135,9 +135,9 @@ enum {
 		0,
 };
 
-static inline int mmu_has_feature(unsigned long feature)
+static inline bool mmu_has_feature(unsigned long feature)
 {
-	return (MMU_FTRS_POSSIBLE & cur_cpu_spec->mmu_features & feature);
+	return !!(MMU_FTRS_POSSIBLE & cur_cpu_spec->mmu_features & feature);
 }
 
 static inline void mmu_clear_feature(unsigned long feature)

commit 5a25b6f527f9f5bbf5747b1b97e538e6d61bd2f2
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Jul 27 13:19:01 2016 +1000

    powerpc/mm: Make MMU_FTR_RADIX a MMU family feature
    
    MMU feature bits are defined such that we use the lower half to
    present MMU family features. Remove the strict split of half and
    also move Radix to a mmu family feature. Radix introduce a new MMU
    model and strictly speaking it is a new MMU family. This also free
    up bits which can be used for individual features later.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 14220c5c12c9..599781e48552 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -12,7 +12,7 @@
  */
 
 /*
- * First half is MMU families
+ * MMU families
  */
 #define MMU_FTR_HPTE_TABLE		ASM_CONST(0x00000001)
 #define MMU_FTR_TYPE_8xx		ASM_CONST(0x00000002)
@@ -21,9 +21,13 @@
 #define MMU_FTR_TYPE_FSL_E		ASM_CONST(0x00000010)
 #define MMU_FTR_TYPE_47x		ASM_CONST(0x00000020)
 
+/* Radix page table supported and enabled */
+#define MMU_FTR_TYPE_RADIX		ASM_CONST(0x00000040)
+
 /*
- * This is individual features
+ * Individual features below.
  */
+
 /*
  * We need to clear top 16bits of va (from the remaining 64 bits )in
  * tlbie* instructions
@@ -93,11 +97,6 @@
  */
 #define MMU_FTR_1T_SEGMENT		ASM_CONST(0x40000000)
 
-/*
- * Radix page table available
- */
-#define MMU_FTR_RADIX			ASM_CONST(0x80000000)
-
 /* MMU feature bit sets for various CPUs */
 #define MMU_FTRS_DEFAULT_HPTE_ARCH_V2	\
 	MMU_FTR_HPTE_TABLE | MMU_FTR_PPCAS_ARCH_V2
@@ -131,7 +130,7 @@ enum {
 		MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_CI_LARGE_PAGE |
 		MMU_FTR_1T_SEGMENT | MMU_FTR_TLBIE_CROP_VA |
 #ifdef CONFIG_PPC_RADIX_MMU
-		MMU_FTR_RADIX |
+		MMU_FTR_TYPE_RADIX |
 #endif
 		0,
 };

commit 1a01dc87e09b6e60d22c417e00f470a72f00ec80
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jul 26 20:09:30 2016 +1000

    powerpc/mm: Add mmu_early_init_devtree()
    
    Empty for now, but we'll add to it in the next patch.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 54471228f7b8..14220c5c12c9 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -210,6 +210,7 @@ extern void early_init_mmu(void);
 extern void early_init_mmu_secondary(void);
 extern void setup_initial_memory_limit(phys_addr_t first_memblock_base,
 				       phys_addr_t first_memblock_size);
+static inline void mmu_early_init_devtree(void) { }
 #endif /* __ASSEMBLY__ */
 #endif
 

commit accfad7d0a85c5678eef76083972426032d64469
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Jul 13 15:05:24 2016 +0530

    powerpc/mm: Clear top 16 bits of va only on older cpus
    
    As per ISA, we need to do this only for architecture version 2.02 and
    earlier. This continued to work even for 2.07. But let's not do this for
    anything after 2.02. ISA 3.0 requires these top bits to be not cleared.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index e53ebebff474..54471228f7b8 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -24,6 +24,11 @@
 /*
  * This is individual features
  */
+/*
+ * We need to clear top 16bits of va (from the remaining 64 bits )in
+ * tlbie* instructions
+ */
+#define MMU_FTR_TLBIE_CROP_VA		ASM_CONST(0x00008000)
 
 /* Enable use of high BAT registers */
 #define MMU_FTR_USE_HIGH_BATS		ASM_CONST(0x00010000)
@@ -97,7 +102,7 @@
 #define MMU_FTRS_DEFAULT_HPTE_ARCH_V2	\
 	MMU_FTR_HPTE_TABLE | MMU_FTR_PPCAS_ARCH_V2
 #define MMU_FTRS_POWER4		MMU_FTRS_DEFAULT_HPTE_ARCH_V2
-#define MMU_FTRS_PPC970		MMU_FTRS_POWER4
+#define MMU_FTRS_PPC970		MMU_FTRS_POWER4 | MMU_FTR_TLBIE_CROP_VA
 #define MMU_FTRS_POWER5		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
 #define MMU_FTRS_POWER6		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
 #define MMU_FTRS_POWER7		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
@@ -124,7 +129,7 @@ enum {
 		MMU_FTR_USE_TLBRSRV | MMU_FTR_USE_PAIRED_MAS |
 		MMU_FTR_NO_SLBIE_B | MMU_FTR_16M_PAGE | MMU_FTR_TLBIEL |
 		MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_CI_LARGE_PAGE |
-		MMU_FTR_1T_SEGMENT |
+		MMU_FTR_1T_SEGMENT | MMU_FTR_TLBIE_CROP_VA |
 #ifdef CONFIG_PPC_RADIX_MMU
 		MMU_FTR_RADIX |
 #endif

commit a8ed87c92adf1fd45142323e04f15b522117d575
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:26:06 2016 +1000

    powerpc/mm/radix: Add MMU_FTR_RADIX
    
    We are going to add asm changes in the follow up patches. Add the
    feature bit now so that we can get it all build.
    
    mpe: When CONFIG_PPC_RADIX_MMU=n we omit MMU_FTR_RADIX from the
    MMU_FTRS_POSSIBLE mask. This allows the compiler to work out that those
    checks will always be false and so the code can be elided completely.
    
    Note we do *not* define MMU_FTR_RADIX to 0 in the RADIX_MMU=n case,
    because that doesn't work with the ASM_FTR patching. In particular an
    IF_SET section will result in a mask and value of zero, which is always
    true, meaning the section *won't* be patched, which is the opposite of
    what we want.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index fbd6f351c399..e53ebebff474 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -88,6 +88,11 @@
  */
 #define MMU_FTR_1T_SEGMENT		ASM_CONST(0x40000000)
 
+/*
+ * Radix page table available
+ */
+#define MMU_FTR_RADIX			ASM_CONST(0x80000000)
+
 /* MMU feature bit sets for various CPUs */
 #define MMU_FTRS_DEFAULT_HPTE_ARCH_V2	\
 	MMU_FTR_HPTE_TABLE | MMU_FTR_PPCAS_ARCH_V2
@@ -119,7 +124,11 @@ enum {
 		MMU_FTR_USE_TLBRSRV | MMU_FTR_USE_PAIRED_MAS |
 		MMU_FTR_NO_SLBIE_B | MMU_FTR_16M_PAGE | MMU_FTR_TLBIEL |
 		MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_CI_LARGE_PAGE |
-		MMU_FTR_1T_SEGMENT,
+		MMU_FTR_1T_SEGMENT |
+#ifdef CONFIG_PPC_RADIX_MMU
+		MMU_FTR_RADIX |
+#endif
+		0,
 };
 
 static inline int mmu_has_feature(unsigned long feature)

commit 773edeadf672546cd554e0797fc02a9e6edf01ae
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed May 11 15:30:47 2016 +1000

    powerpc/mm: Add mask of possible MMU features
    
    Follow the example of the cpu feature code, and add a mask of possible
    MMU features, MMU_FTRS_POSSIBLE.
    
    This is used in mmu_has_feature(), which allows the possible mask to act
    as a shortcut for any features that are not possible, but still allows
    the feature bit itself to be defined.
    
    We will use this in the next commit to allow MMU_FTR_RADIX checks to be
    elided when MMU_FTR_RADIX is not possible.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 34070e982f1d..fbd6f351c399 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -110,9 +110,21 @@
 DECLARE_PER_CPU(int, next_tlbcam_idx);
 #endif
 
+enum {
+	MMU_FTRS_POSSIBLE = MMU_FTR_HPTE_TABLE | MMU_FTR_TYPE_8xx |
+		MMU_FTR_TYPE_40x | MMU_FTR_TYPE_44x | MMU_FTR_TYPE_FSL_E |
+		MMU_FTR_TYPE_47x | MMU_FTR_USE_HIGH_BATS | MMU_FTR_BIG_PHYS |
+		MMU_FTR_USE_TLBIVAX_BCAST | MMU_FTR_USE_TLBILX |
+		MMU_FTR_LOCK_BCAST_INVAL | MMU_FTR_NEED_DTLB_SW_LRU |
+		MMU_FTR_USE_TLBRSRV | MMU_FTR_USE_PAIRED_MAS |
+		MMU_FTR_NO_SLBIE_B | MMU_FTR_16M_PAGE | MMU_FTR_TLBIEL |
+		MMU_FTR_LOCKLESS_TLBIE | MMU_FTR_CI_LARGE_PAGE |
+		MMU_FTR_1T_SEGMENT,
+};
+
 static inline int mmu_has_feature(unsigned long feature)
 {
-	return (cur_cpu_spec->mmu_features & feature);
+	return (MMU_FTRS_POSSIBLE & cur_cpu_spec->mmu_features & feature);
 }
 
 static inline void mmu_clear_feature(unsigned long feature)

commit 756d08d1ba169ed9eae4d48e9a0af014e86593fc
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:25:57 2016 +1000

    powerpc/mm: Abstract early MMU init in preparation for radix
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index decde4c4870d..34070e982f1d 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -122,13 +122,6 @@ static inline void mmu_clear_feature(unsigned long feature)
 
 extern unsigned int __start___mmu_ftr_fixup, __stop___mmu_ftr_fixup;
 
-/* MMU initialization */
-extern void early_init_mmu(void);
-extern void early_init_mmu_secondary(void);
-
-extern void setup_initial_memory_limit(phys_addr_t first_memblock_base,
-				       phys_addr_t first_memblock_size);
-
 #ifdef CONFIG_PPC64
 /* This is our real memory area size on ppc64 server, on embedded, we
  * make it match the size our of bolted TLB area
@@ -185,6 +178,13 @@ static inline void assert_pte_locked(struct mm_struct *mm, unsigned long addr)
 #include <asm/book3s/64/mmu.h>
 #else /* CONFIG_PPC_BOOK3S_64 */
 
+#ifndef __ASSEMBLY__
+/* MMU initialization */
+extern void early_init_mmu(void);
+extern void early_init_mmu_secondary(void);
+extern void setup_initial_memory_limit(phys_addr_t first_memblock_base,
+				       phys_addr_t first_memblock_size);
+#endif /* __ASSEMBLY__ */
 #endif
 
 #if defined(CONFIG_PPC_STD_MMU_32)

commit 566ca99af026d43c7cf7ab72f78463dbcc7e6ac2
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:25:53 2016 +1000

    powerpc/mm/radix: Add dummy radix_enabled()
    
    In this patch we add the radix Kconfig and conditional check.
    radix_enabled() is written to always return 0 here. Once we have all
    needed radix changes added, we will update this to an mmu_feature check.
    
    We need to add this early so that we can get it all build in the early
    stage.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 5f55024f9522..decde4c4870d 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -204,6 +204,9 @@ static inline void assert_pte_locked(struct mm_struct *mm, unsigned long addr)
 #  include <asm/mmu-8xx.h>
 #endif
 
+#ifndef radix_enabled
+#define radix_enabled() (0)
+#endif
 
 #endif /* __KERNEL__ */
 #endif /* _ASM_POWERPC_MMU_H_ */

commit 11a6f6abd74ab80865023200ca33515f6db43d63
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:25:41 2016 +1000

    powerpc/mm: Move radix/hash common data structures to book3s64 headers
    
    Start moving code that is generic between radix and hash to book3s64
    specific headers from the book3s64 hash specific one.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 8ca1c983bf6c..5f55024f9522 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -181,10 +181,13 @@ static inline void assert_pte_locked(struct mm_struct *mm, unsigned long addr)
 
 #define MMU_PAGE_COUNT	15
 
-#if defined(CONFIG_PPC_STD_MMU_64)
-/* 64-bit classic hash table MMU */
-#include <asm/book3s/64/mmu-hash.h>
-#elif defined(CONFIG_PPC_STD_MMU_32)
+#ifdef CONFIG_PPC_BOOK3S_64
+#include <asm/book3s/64/mmu.h>
+#else /* CONFIG_PPC_BOOK3S_64 */
+
+#endif
+
+#if defined(CONFIG_PPC_STD_MMU_32)
 /* 32-bit classic hash table MMU */
 #include <asm/book3s/32/mmu-hash.h>
 #elif defined(CONFIG_40x)

commit f64e8084c94bb0449177364856d8117e2f14c4c0
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Mar 1 12:59:20 2016 +0530

    powerpc/mm: Move hash related mmu-*.h headers to book3s/
    
    No code changes.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 54d46504733d..8ca1c983bf6c 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -183,10 +183,10 @@ static inline void assert_pte_locked(struct mm_struct *mm, unsigned long addr)
 
 #if defined(CONFIG_PPC_STD_MMU_64)
 /* 64-bit classic hash table MMU */
-#  include <asm/mmu-hash64.h>
+#include <asm/book3s/64/mmu-hash.h>
 #elif defined(CONFIG_PPC_STD_MMU_32)
 /* 32-bit classic hash table MMU */
-#  include <asm/mmu-hash32.h>
+#include <asm/book3s/32/mmu-hash.h>
 #elif defined(CONFIG_40x)
 /* 40x-style software loaded TLB */
 #  include <asm/mmu-40x.h>

commit c3ab300ea55541014348561e7690c41c79966ac6
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Feb 19 11:16:24 2016 +1100

    powerpc: Add POWER9 cputable entry
    
    Add a cputable entry for POWER9.  More code is required to actually
    boot and run on a POWER9 but this gets the base piece in which we can
    start building on.
    
    Copies over from POWER8 except for:
    - Adds a new CPU_FTR_ARCH_300 bit to start hanging new architecture
       features from (in subsequent patches).
    - Advertises new user features bits PPC_FEATURE2_ARCH_3_00 &
      HAS_IEEE128 when on POWER9.
    - Drops CPU_FTR_SUBCORE.
    - Drops PMU code and machine check.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 3d5abfe6ba67..54d46504733d 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -97,6 +97,7 @@
 #define MMU_FTRS_POWER6		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
 #define MMU_FTRS_POWER7		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
 #define MMU_FTRS_POWER8		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
+#define MMU_FTRS_POWER9		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
 #define MMU_FTRS_CELL		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \
 				MMU_FTR_CI_LARGE_PAGE
 #define MMU_FTRS_PA6T		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \

commit 13b3d13b813ab834fac67dc05f8b86dbcc29c134
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Jul 10 12:29:20 2014 +1000

    powerpc: Remove MMU_FTR_SLB
    
    We now only support cpus that use an SLB, so we don't need an MMU
    feature to indicate that.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index e61f24ed4e65..3d5abfe6ba67 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -64,9 +64,9 @@
  */
 #define MMU_FTR_USE_PAIRED_MAS		ASM_CONST(0x01000000)
 
-/* MMU is SLB-based
+/* Doesn't support the B bit (1T segment) in SLBIE
  */
-#define MMU_FTR_SLB			ASM_CONST(0x02000000)
+#define MMU_FTR_NO_SLBIE_B		ASM_CONST(0x02000000)
 
 /* Support 16M large pages
  */
@@ -88,10 +88,6 @@
  */
 #define MMU_FTR_1T_SEGMENT		ASM_CONST(0x40000000)
 
-/* Doesn't support the B bit (1T segment) in SLBIE
- */
-#define MMU_FTR_NO_SLBIE_B		ASM_CONST(0x80000000)
-
 /* MMU feature bit sets for various CPUs */
 #define MMU_FTRS_DEFAULT_HPTE_ARCH_V2	\
 	MMU_FTR_HPTE_TABLE | MMU_FTR_PPCAS_ARCH_V2

commit cd68098bcedd432f21013b2a3b0737b9222da363
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jul 8 17:10:45 2014 +1000

    powerpc: Clean up MMU_FTRS_A2 and MMU_FTR_TYPE_3E
    
    In fb5a515704d7 "powerpc: Remove platforms/wsp and associated pieces",
    we removed the last user of MMU_FTRS_A2. So remove it.
    
    MMU_FTRS_A2 was the last user of MMU_FTR_TYPE_3E, so remove it also.
    This leaves some unreachable code in mmu_context_nohash.c, so remove
    that also.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index f8d1d6dcf7db..e61f24ed4e65 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -19,8 +19,7 @@
 #define MMU_FTR_TYPE_40x		ASM_CONST(0x00000004)
 #define MMU_FTR_TYPE_44x		ASM_CONST(0x00000008)
 #define MMU_FTR_TYPE_FSL_E		ASM_CONST(0x00000010)
-#define MMU_FTR_TYPE_3E			ASM_CONST(0x00000020)
-#define MMU_FTR_TYPE_47x		ASM_CONST(0x00000040)
+#define MMU_FTR_TYPE_47x		ASM_CONST(0x00000020)
 
 /*
  * This is individual features
@@ -106,13 +105,6 @@
 				MMU_FTR_CI_LARGE_PAGE
 #define MMU_FTRS_PA6T		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \
 				MMU_FTR_CI_LARGE_PAGE | MMU_FTR_NO_SLBIE_B
-#define MMU_FTRS_A2		MMU_FTR_TYPE_3E | MMU_FTR_USE_TLBILX | \
-				MMU_FTR_USE_TLBIVAX_BCAST | \
-				MMU_FTR_LOCK_BCAST_INVAL | \
-				MMU_FTR_USE_TLBRSRV | \
-				MMU_FTR_USE_PAIRED_MAS | \
-				MMU_FTR_TLBIEL | \
-				MMU_FTR_16M_PAGE
 #ifndef __ASSEMBLY__
 #include <asm/cputable.h>
 

commit 28efc35fe68dacbddc4b12c2fa8f2df1593a4ad3
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Oct 11 19:22:38 2013 -0500

    powerpc/e6500: TLB miss handler with hardware tablewalk support
    
    There are a few things that make the existing hw tablewalk handlers
    unsuitable for e6500:
    
     - Indirect entries go in TLB1 (though the resulting direct entries go in
       TLB0).
    
     - It has threads, but no "tlbsrx." -- so we need a spinlock and
       a normal "tlbsx".  Because we need this lock, hardware tablewalk
       is mandatory on e6500 unless we want to add spinlock+tlbsx to
       the normal bolted TLB miss handler.
    
     - TLB1 has no HES (nor next-victim hint) so we need software round robin
       (TODO: integrate this round robin data with hugetlb/KVM)
    
     - The existing tablewalk handlers map half of a page table at a time,
       because IBM hardware has a fixed 1MiB indirect page size.  e6500
       has variable size indirect entries, with a minimum of 2MiB.
       So we can't do the half-page indirect mapping, and even if we
       could it would be less efficient than mapping the full page.
    
     - Like on e5500, the linear mapping is bolted, so we don't need the
       overhead of supporting nested tlb misses.
    
    Note that hardware tablewalk does not work in rev1 of e6500.
    We do not expect to support e6500 rev1 in mainline Linux.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Cc: Mihai Caraman <mihai.caraman@freescale.com>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 691fd8aca939..f8d1d6dcf7db 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -180,16 +180,17 @@ static inline void assert_pte_locked(struct mm_struct *mm, unsigned long addr)
 #define MMU_PAGE_64K_AP	3	/* "Admixed pages" (hash64 only) */
 #define MMU_PAGE_256K	4
 #define MMU_PAGE_1M	5
-#define MMU_PAGE_4M	6
-#define MMU_PAGE_8M	7
-#define MMU_PAGE_16M	8
-#define MMU_PAGE_64M	9
-#define MMU_PAGE_256M	10
-#define MMU_PAGE_1G	11
-#define MMU_PAGE_16G	12
-#define MMU_PAGE_64G	13
-
-#define MMU_PAGE_COUNT	14
+#define MMU_PAGE_2M	6
+#define MMU_PAGE_4M	7
+#define MMU_PAGE_8M	8
+#define MMU_PAGE_16M	9
+#define MMU_PAGE_64M	10
+#define MMU_PAGE_256M	11
+#define MMU_PAGE_1G	12
+#define MMU_PAGE_16G	13
+#define MMU_PAGE_64G	14
+
+#define MMU_PAGE_COUNT	15
 
 #if defined(CONFIG_PPC_STD_MMU_64)
 /* 64-bit classic hash table MMU */

commit 71e184972456a8095657e80fd1470a3857b441a0
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Oct 30 19:34:15 2012 +0000

    powerpc: POWER8 cputable entry
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 5e38eedea218..691fd8aca939 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -101,6 +101,7 @@
 #define MMU_FTRS_POWER5		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
 #define MMU_FTRS_POWER6		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
 #define MMU_FTRS_POWER7		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
+#define MMU_FTRS_POWER8		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
 #define MMU_FTRS_CELL		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \
 				MMU_FTR_CI_LARGE_PAGE
 #define MMU_FTRS_PA6T		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \

commit 78f1dbde9fd020419313c2a0c3b602ea2427118f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Sep 10 02:52:57 2012 +0000

    powerpc/mm: Make some of the PGTABLE_RANGE dependency explicit
    
    slice array size and slice mask size depend on PGTABLE_RANGE.
    
    Reviewed-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index e8a26db2e8f3..5e38eedea218 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -146,6 +146,15 @@ extern void setup_initial_memory_limit(phys_addr_t first_memblock_base,
 extern u64 ppc64_rma_size;
 #endif /* CONFIG_PPC64 */
 
+struct mm_struct;
+#ifdef CONFIG_DEBUG_VM
+extern void assert_pte_locked(struct mm_struct *mm, unsigned long addr);
+#else /* CONFIG_DEBUG_VM */
+static inline void assert_pte_locked(struct mm_struct *mm, unsigned long addr)
+{
+}
+#endif /* !CONFIG_DEBUG_VM */
+
 #endif /* !__ASSEMBLY__ */
 
 /* The kernel use the constants below to index in the page sizes array.

commit a8b91e43afd736fcebb0836359e5ddaeae45b2ab
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Jun 14 13:40:55 2012 +0000

    powerpc/mm: remove obsolete comment about page size name array
    
    The array of names in hugetlbpage.c no longer exists.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index f0145522cfba..e8a26db2e8f3 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -163,12 +163,7 @@ extern u64 ppc64_rma_size;
  * to think about, feedback welcome. --BenH.
  */
 
-/* There are #define as they have to be used in assembly
- *
- * WARNING: If you change this list, make sure to update the array of
- * names currently in arch/powerpc/mm/hugetlbpage.c or bad things will
- * happen
- */
+/* These are #defines as they have to be used in assembly */
 #define MMU_PAGE_4K	0
 #define MMU_PAGE_16K	1
 #define MMU_PAGE_64K	2

commit 41151e77a4d96ea138cede6d84c955aa4769ce74
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Tue Jun 28 09:54:48 2011 +0000

    powerpc: Hugetlb for BookE
    
    Enable hugepages on Freescale BookE processors.  This allows the kernel to
    use huge TLB entries to map pages, which can greatly reduce the number of
    TLB misses and the amount of TLB thrashing experienced by applications with
    large memory footprints.  Care should be taken when using this on FSL
    processors, as the number of large TLB entries supported by the core is low
    (16-64) on current processors.
    
    The supported set of hugepage sizes include 4m, 16m, 64m, 256m, and 1g.
    Page sizes larger than the max zone size are called "gigantic" pages and
    must be allocated on the command line (and cannot be deallocated).
    
    This is currently only fully implemented for Freescale 32-bit BookE
    processors, but there is some infrastructure in the code for
    64-bit BooKE.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 698b30638681..f0145522cfba 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -175,14 +175,16 @@ extern u64 ppc64_rma_size;
 #define MMU_PAGE_64K_AP	3	/* "Admixed pages" (hash64 only) */
 #define MMU_PAGE_256K	4
 #define MMU_PAGE_1M	5
-#define MMU_PAGE_8M	6
-#define MMU_PAGE_16M	7
-#define MMU_PAGE_256M	8
-#define MMU_PAGE_1G	9
-#define MMU_PAGE_16G	10
-#define MMU_PAGE_64G	11
-#define MMU_PAGE_COUNT	12
-
+#define MMU_PAGE_4M	6
+#define MMU_PAGE_8M	7
+#define MMU_PAGE_16M	8
+#define MMU_PAGE_64M	9
+#define MMU_PAGE_256M	10
+#define MMU_PAGE_1G	11
+#define MMU_PAGE_16G	12
+#define MMU_PAGE_64G	13
+
+#define MMU_PAGE_COUNT	14
 
 #if defined(CONFIG_PPC_STD_MMU_64)
 /* 64-bit classic hash table MMU */

commit 4b575f3e8aba07688fc48025efde41036e5d5eee
Merge: f7723f0eaf53 a8e616b9a412
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Jul 22 13:16:41 2011 +1000

    Merge remote-tracking branch 'jwb/next' into next

commit 91b191c71eae79c0e652c52a968d06cd96b539c5
Author: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
Date:   Mon Jul 4 18:38:03 2011 +0000

    powerpc/44x: don't use tlbivax on AMP systems
    
    Since other OS's may be running on the other cores don't use tlbivax
    
    Signed-off-by: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Josh Boyer <jwboyer@linux.vnet.ibm.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 4138b21ae80a..bb5591f424b3 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -120,9 +120,14 @@ static inline int mmu_has_feature(unsigned long feature)
 	return (cur_cpu_spec->mmu_features & feature);
 }
 
+static inline void mmu_clear_feature(unsigned long feature)
+{
+	cur_cpu_spec->mmu_features &= ~feature;
+}
+
 extern unsigned int __start___mmu_ftr_fixup, __stop___mmu_ftr_fixup;
 
-/* MMU initialization (64-bit only fo now) */
+/* MMU initialization */
 extern void early_init_mmu(void);
 extern void early_init_mmu_secondary(void);
 

commit 3160b09796129abc9523ea3cd1633b0faba64a02
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Tue Jun 28 14:54:47 2011 -0500

    powerpc: Create next_tlbcam_idx percpu variable for FSL_BOOKE
    
    This is used to round-robin TLBCAM entries.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 4138b21ae80a..b427a55ce2ab 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -115,6 +115,11 @@
 #ifndef __ASSEMBLY__
 #include <asm/cputable.h>
 
+#ifdef CONFIG_PPC_FSL_BOOK3E
+#include <asm/percpu.h>
+DECLARE_PER_CPU(int, next_tlbcam_idx);
+#endif
+
 static inline int mmu_has_feature(unsigned long feature)
 {
 	return (cur_cpu_spec->mmu_features & feature);

commit a32e252f7cdfb3675a4e50215cfac356ed8952c4
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Apr 6 18:23:29 2011 +0000

    powerpc: Use new CPU feature bit to select 2.06 tlbie
    
    This removes MMU_FTR_TLBIE_206 as we can now use CPU_FTR_HVMODE_206.  It
    also changes the logic to select which tlbie to use to be based on this
    new CPU feature bit.
    
    This also duplicates the ASM_FTR_IF/SET/CLR defines for CPU features
    (copied from MMU features).
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index a39304b74f84..4138b21ae80a 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -56,11 +56,6 @@
  */
 #define MMU_FTR_NEED_DTLB_SW_LRU	ASM_CONST(0x00200000)
 
-/* This indicates that the processor uses the ISA 2.06 server tlbie
- * mnemonics
- */
-#define MMU_FTR_TLBIE_206		ASM_CONST(0x00400000)
-
 /* Enable use of TLB reservation.  Processor should support tlbsrx.
  * instruction and MAS0[WQ].
  */
@@ -105,8 +100,7 @@
 #define MMU_FTRS_PPC970		MMU_FTRS_POWER4
 #define MMU_FTRS_POWER5		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
 #define MMU_FTRS_POWER6		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
-#define MMU_FTRS_POWER7		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE | \
-				MMU_FTR_TLBIE_206
+#define MMU_FTRS_POWER7		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
 #define MMU_FTRS_CELL		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \
 				MMU_FTR_CI_LARGE_PAGE
 #define MMU_FTRS_PA6T		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \

commit 44ae3ab3358e962039c36ad4ae461ae9fb29596c
Author: Matt Evans <matt@ozlabs.org>
Date:   Wed Apr 6 19:48:50 2011 +0000

    powerpc: Free up some CPU feature bits by moving out MMU-related features
    
    Some of the 64bit PPC CPU features are MMU-related, so this patch moves
    them to MMU_FTR_ bits.  All cpu_has_feature()-style tests are moved to
    mmu_has_feature(), and seven feature bits are freed as a result.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index bb40a06d3b77..a39304b74f84 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -70,6 +70,54 @@
  */
 #define MMU_FTR_USE_PAIRED_MAS		ASM_CONST(0x01000000)
 
+/* MMU is SLB-based
+ */
+#define MMU_FTR_SLB			ASM_CONST(0x02000000)
+
+/* Support 16M large pages
+ */
+#define MMU_FTR_16M_PAGE		ASM_CONST(0x04000000)
+
+/* Supports TLBIEL variant
+ */
+#define MMU_FTR_TLBIEL			ASM_CONST(0x08000000)
+
+/* Supports tlbies w/o locking
+ */
+#define MMU_FTR_LOCKLESS_TLBIE		ASM_CONST(0x10000000)
+
+/* Large pages can be marked CI
+ */
+#define MMU_FTR_CI_LARGE_PAGE		ASM_CONST(0x20000000)
+
+/* 1T segments available
+ */
+#define MMU_FTR_1T_SEGMENT		ASM_CONST(0x40000000)
+
+/* Doesn't support the B bit (1T segment) in SLBIE
+ */
+#define MMU_FTR_NO_SLBIE_B		ASM_CONST(0x80000000)
+
+/* MMU feature bit sets for various CPUs */
+#define MMU_FTRS_DEFAULT_HPTE_ARCH_V2	\
+	MMU_FTR_HPTE_TABLE | MMU_FTR_PPCAS_ARCH_V2
+#define MMU_FTRS_POWER4		MMU_FTRS_DEFAULT_HPTE_ARCH_V2
+#define MMU_FTRS_PPC970		MMU_FTRS_POWER4
+#define MMU_FTRS_POWER5		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
+#define MMU_FTRS_POWER6		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE
+#define MMU_FTRS_POWER7		MMU_FTRS_POWER4 | MMU_FTR_LOCKLESS_TLBIE | \
+				MMU_FTR_TLBIE_206
+#define MMU_FTRS_CELL		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \
+				MMU_FTR_CI_LARGE_PAGE
+#define MMU_FTRS_PA6T		MMU_FTRS_DEFAULT_HPTE_ARCH_V2 | \
+				MMU_FTR_CI_LARGE_PAGE | MMU_FTR_NO_SLBIE_B
+#define MMU_FTRS_A2		MMU_FTR_TYPE_3E | MMU_FTR_USE_TLBILX | \
+				MMU_FTR_USE_TLBIVAX_BCAST | \
+				MMU_FTR_LOCK_BCAST_INVAL | \
+				MMU_FTR_USE_TLBRSRV | \
+				MMU_FTR_USE_PAIRED_MAS | \
+				MMU_FTR_TLBIEL | \
+				MMU_FTR_16M_PAGE
 #ifndef __ASSEMBLY__
 #include <asm/cputable.h>
 

commit cd3db0c4ca3d237e7ad20f7107216e575705d2b0
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 6 15:39:02 2010 -0700

    memblock: Remove rmo_size, burry it in arch/powerpc where it belongs
    
    The RMA (RMO is a misnomer) is a concept specific to ppc64 (in fact
    server ppc64 though I hijack it on embedded ppc64 for similar purposes)
    and represents the area of memory that can be accessed in real mode
    (aka with MMU off), or on embedded, from the exception vectors (which
    is bolted in the TLB) which pretty much boils down to the same thing.
    
    We take that out of the generic MEMBLOCK data structure and move it into
    arch/powerpc where it belongs, renaming it to "RMA" while at it.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 7ebf42ed84a2..bb40a06d3b77 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -2,6 +2,8 @@
 #define _ASM_POWERPC_MMU_H_
 #ifdef __KERNEL__
 
+#include <linux/types.h>
+
 #include <asm/asm-compat.h>
 #include <asm/feature-fixups.h>
 
@@ -82,6 +84,16 @@ extern unsigned int __start___mmu_ftr_fixup, __stop___mmu_ftr_fixup;
 extern void early_init_mmu(void);
 extern void early_init_mmu_secondary(void);
 
+extern void setup_initial_memory_limit(phys_addr_t first_memblock_base,
+				       phys_addr_t first_memblock_size);
+
+#ifdef CONFIG_PPC64
+/* This is our real memory area size on ppc64 server, on embedded, we
+ * make it match the size our of bolted TLB area
+ */
+extern u64 ppc64_rma_size;
+#endif /* CONFIG_PPC64 */
+
 #endif /* !__ASSEMBLY__ */
 
 /* The kernel use the constants below to index in the page sizes array.

commit e7f75ad01d590243904c2d95ab47e6b2e9ef6dad
Author: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
Date:   Fri Mar 5 10:43:12 2010 +0000

    powerpc/47x: Base ppc476 support
    
    This patch adds the base support for the 476 processor.  The code was
    primarily written by Ben Herrenschmidt and Torez Smith, but I've been
    maintaining it for a while.
    
    The goal is to have a single binary that will run on 44x and 47x, but
    we still have some details to work out.  The biggest is that the L1 cache
    line size differs on the two platforms, but it's currently a compile-time
    option.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Torez Smith  <lnxtorez@linux.vnet.ibm.com>
    Signed-off-by: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 7ffbb65ff7a9..7ebf42ed84a2 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -18,6 +18,7 @@
 #define MMU_FTR_TYPE_44x		ASM_CONST(0x00000008)
 #define MMU_FTR_TYPE_FSL_E		ASM_CONST(0x00000010)
 #define MMU_FTR_TYPE_3E			ASM_CONST(0x00000020)
+#define MMU_FTR_TYPE_47x		ASM_CONST(0x00000040)
 
 /*
  * This is individual features

commit df5d6ecf8157245ef733db87597adb2c6e2510da
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Mon Aug 24 15:52:48 2009 +0000

    powerpc/mm: Add MMU features for TLB reservation & Paired MAS registers
    
    Support for TLB reservation (or TLB Write Conditional) and Paired MAS
    registers are optional for a processor implementation so we handle
    them via MMU feature sections.
    
    We currently only used paired MAS registers to access the full RPN + perm
    bits that are kept in MAS7||MAS3.  We assume that if an implementation has
    hardware page table at this time it also implements in TLB reservations.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 2fcfefc60894..7ffbb65ff7a9 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -58,6 +58,15 @@
  */
 #define MMU_FTR_TLBIE_206		ASM_CONST(0x00400000)
 
+/* Enable use of TLB reservation.  Processor should support tlbsrx.
+ * instruction and MAS0[WQ].
+ */
+#define MMU_FTR_USE_TLBRSRV		ASM_CONST(0x00800000)
+
+/* Use paired MAS registers (MAS7||MAS3, etc.)
+ */
+#define MMU_FTR_USE_PAIRED_MAS		ASM_CONST(0x01000000)
+
 #ifndef __ASSEMBLY__
 #include <asm/cputable.h>
 

commit 57e2a99f74b0d3720c97a6aadb57ae6aad3c61ea
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 28 11:59:34 2009 +1000

    powerpc: Add memory management headers for new 64-bit BookE
    
    This adds the PTE and pgtable format definitions, along with changes
    to the kernel memory map and other definitions related to implementing
    support for 64-bit Book3E. This also shields some asm-offset bits that
    are currently only relevant on 32-bit
    
    We also move the definition of the "linux" page size constants to
    the common mmu.h file and add a few sizes that are relevant to
    embedded processors.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index fb57ded592f9..2fcfefc60894 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -17,6 +17,7 @@
 #define MMU_FTR_TYPE_40x		ASM_CONST(0x00000004)
 #define MMU_FTR_TYPE_44x		ASM_CONST(0x00000008)
 #define MMU_FTR_TYPE_FSL_E		ASM_CONST(0x00000010)
+#define MMU_FTR_TYPE_3E			ASM_CONST(0x00000020)
 
 /*
  * This is individual features
@@ -73,6 +74,41 @@ extern void early_init_mmu_secondary(void);
 
 #endif /* !__ASSEMBLY__ */
 
+/* The kernel use the constants below to index in the page sizes array.
+ * The use of fixed constants for this purpose is better for performances
+ * of the low level hash refill handlers.
+ *
+ * A non supported page size has a "shift" field set to 0
+ *
+ * Any new page size being implemented can get a new entry in here. Whether
+ * the kernel will use it or not is a different matter though. The actual page
+ * size used by hugetlbfs is not defined here and may be made variable
+ *
+ * Note: This array ended up being a false good idea as it's growing to the
+ * point where I wonder if we should replace it with something different,
+ * to think about, feedback welcome. --BenH.
+ */
+
+/* There are #define as they have to be used in assembly
+ *
+ * WARNING: If you change this list, make sure to update the array of
+ * names currently in arch/powerpc/mm/hugetlbpage.c or bad things will
+ * happen
+ */
+#define MMU_PAGE_4K	0
+#define MMU_PAGE_16K	1
+#define MMU_PAGE_64K	2
+#define MMU_PAGE_64K_AP	3	/* "Admixed pages" (hash64 only) */
+#define MMU_PAGE_256K	4
+#define MMU_PAGE_1M	5
+#define MMU_PAGE_8M	6
+#define MMU_PAGE_16M	7
+#define MMU_PAGE_256M	8
+#define MMU_PAGE_1G	9
+#define MMU_PAGE_16G	10
+#define MMU_PAGE_64G	11
+#define MMU_PAGE_COUNT	12
+
 
 #if defined(CONFIG_PPC_STD_MMU_64)
 /* 64-bit classic hash table MMU */
@@ -94,5 +130,6 @@ extern void early_init_mmu_secondary(void);
 #  include <asm/mmu-8xx.h>
 #endif
 
+
 #endif /* __KERNEL__ */
 #endif /* _ASM_POWERPC_MMU_H_ */

commit 944916858a430a0627e483657d4cfa2cd2dfb4f7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jun 2 21:17:45 2009 +0000

    powerpc: Shield code specific to 64-bit server processors
    
    This is a random collection of added ifdef's around portions of
    code that only mak sense on server processors. Using either
    CONFIG_PPC_STD_MMU_64 or CONFIG_PPC_BOOK3S as seems appropriate.
    
    This is meant to make the future merging of Book3E 64-bit support
    easier.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 325b7208a146..fb57ded592f9 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -74,10 +74,10 @@ extern void early_init_mmu_secondary(void);
 #endif /* !__ASSEMBLY__ */
 
 
-#ifdef CONFIG_PPC64
+#if defined(CONFIG_PPC_STD_MMU_64)
 /* 64-bit classic hash table MMU */
 #  include <asm/mmu-hash64.h>
-#elif defined(CONFIG_PPC_STD_MMU)
+#elif defined(CONFIG_PPC_STD_MMU_32)
 /* 32-bit classic hash table MMU */
 #  include <asm/mmu-hash32.h>
 #elif defined(CONFIG_40x)

commit 60dbf4385130136847ea73657da329f8e7dbe16e
Author: Milton Miller <miltonm@bga.com>
Date:   Wed Apr 29 20:58:01 2009 +0000

    powerpc: Add 2.06 tlbie mnemonics
    
    This adds the PowerPC 2.06 tlbie mnemonics and keeps backwards
    compatibilty for CPUs before 2.06.
    
    Only useful for bare metal systems.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index cbf154387091..325b7208a146 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -52,6 +52,11 @@
  */
 #define MMU_FTR_NEED_DTLB_SW_LRU	ASM_CONST(0x00200000)
 
+/* This indicates that the processor uses the ISA 2.06 server tlbie
+ * mnemonics
+ */
+#define MMU_FTR_TLBIE_206		ASM_CONST(0x00400000)
+
 #ifndef __ASSEMBLY__
 #include <asm/cputable.h>
 

commit 323d23aeac4918c7a540b597a26fa7a67645593a
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Apr 23 08:51:22 2009 -0500

    Revert "powerpc: Add support for early tlbilx opcode"
    
    This reverts commit e9965577406a2148ade97b5e0ce7c448b4ba4ef6.  Our HW
    guys were able to fix this so it never sees the light of day.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 86d2366ab6a1..cbf154387091 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -52,12 +52,6 @@
  */
 #define MMU_FTR_NEED_DTLB_SW_LRU	ASM_CONST(0x00200000)
 
-/* This indicates that the processor uses the wrong opcode for tlbilx
- * instructions.  During the ISA 2.06 development the opcode for tlbilx
- * changed and some early implementations used to old opcode
- */
-#define MMU_FTR_TLBILX_EARLY_OPCODE	ASM_CONST(0x00400000)
-
 #ifndef __ASSEMBLY__
 #include <asm/cputable.h>
 

commit e9965577406a2148ade97b5e0ce7c448b4ba4ef6
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Mon Apr 6 23:36:50 2009 -0500

    powerpc: Add support for early tlbilx opcode
    
    During the ISA 2.06 development the opcode for tlbilx changed and some
    early implementations used to old opcode.  Add support for a MMU_FTR
    fixup to deal with this.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index cbf154387091..86d2366ab6a1 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -52,6 +52,12 @@
  */
 #define MMU_FTR_NEED_DTLB_SW_LRU	ASM_CONST(0x00200000)
 
+/* This indicates that the processor uses the wrong opcode for tlbilx
+ * instructions.  During the ISA 2.06 development the opcode for tlbilx
+ * changed and some early implementations used to old opcode
+ */
+#define MMU_FTR_TLBILX_EARLY_OPCODE	ASM_CONST(0x00400000)
+
 #ifndef __ASSEMBLY__
 #include <asm/cputable.h>
 

commit 757c74d298dc8438760b8dea275c4c6e0ac8a77f
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Mar 19 19:34:16 2009 +0000

    powerpc/mm: Introduce early_init_mmu() on 64-bit
    
    This moves some MMU related init code out of setup_64.c into hash_utils_64.c
    and calls it early_init_mmu() and early_init_mmu_secondary(). This will
    make it easier to plug in a new MMU type.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index c073de4af849..cbf154387091 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -62,6 +62,10 @@ static inline int mmu_has_feature(unsigned long feature)
 
 extern unsigned int __start___mmu_ftr_fixup, __stop___mmu_ftr_fixup;
 
+/* MMU initialization (64-bit only fo now) */
+extern void early_init_mmu(void);
+extern void early_init_mmu_secondary(void);
+
 #endif /* !__ASSEMBLY__ */
 
 

commit 2319f1239592d0de80414ad2338c2bd7384a2a41
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Mar 19 03:55:41 2009 +0000

    powerpc/mm: e300c2/c3/c4 TLB errata workaround
    
    Complete workaround for DTLB errata in e300c2/c3/c4 processors.
    
    Due to the bug, the hardware-implemented LRU algorythm always goes to way
    1 of the TLB. This fix implements the proposed software workaround in
    form of a LRW table for chosing the TLB-way.
    
    Based on patch from David Jander <david@protonic.nl>
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index dc82dcd06aea..c073de4af849 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -46,6 +46,12 @@
  */
 #define MMU_FTR_LOCK_BCAST_INVAL	ASM_CONST(0x00100000)
 
+/* This indicates that the processor doesn't handle way selection
+ * properly and needs SW to track and update the LRU state.  This
+ * is specific to an errata on e300c2/c3/c4 class parts
+ */
+#define MMU_FTR_NEED_DTLB_SW_LRU	ASM_CONST(0x00200000)
+
 #ifndef __ASSEMBLY__
 #include <asm/cputable.h>
 

commit c3071951d0acd33b5c3f820fb5eaa3a9c2a8f212
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Tue Feb 10 22:26:06 2009 -0600

    powerpc/fsl-booke: Add support for tlbilx instructions
    
    The e500mc core supports the new tlbilx instructions that do core
    local invalidates and also provide us the ability to take down
    all TLB entries matching a given PID.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 5c78079cfa3c..dc82dcd06aea 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -36,9 +36,9 @@
  */
 #define MMU_FTR_USE_TLBIVAX_BCAST	ASM_CONST(0x00040000)
 
-/* Enable use of tlbilx invalidate-by-PID variant.
+/* Enable use of tlbilx invalidate instructions.
  */
-#define MMU_FTR_USE_TLBILX_PID		ASM_CONST(0x00080000)
+#define MMU_FTR_USE_TLBILX		ASM_CONST(0x00080000)
 
 /* This indicates that the processor cannot handle multiple outstanding
  * broadcast tlbivax or tlbsync. This makes the code use a spinlock

commit 70fe3af8403f85196bb74f22ce4813db7dfedc1a
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Feb 12 16:12:40 2009 -0600

    powerpc/book-3e: Introduce concept of Book-3e MMU
    
    The Power ISA 2.06 spec introduces a standard MMU programming model that
    is based on the Freescale Book-E MMU programing model.  The Freescale
    version is pretty backwards compatiable with the ISA 2.06 definition so
    we are starting to refactor some of the Freescale code so it can be
    easily shared.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 6e7639911318..5c78079cfa3c 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -71,9 +71,9 @@ extern unsigned int __start___mmu_ftr_fixup, __stop___mmu_ftr_fixup;
 #elif defined(CONFIG_44x)
 /* 44x-style software loaded TLB */
 #  include <asm/mmu-44x.h>
-#elif defined(CONFIG_FSL_BOOKE)
-/* Freescale Book-E software loaded TLB */
-#  include <asm/mmu-fsl-booke.h>
+#elif defined(CONFIG_PPC_BOOK3E_MMU)
+/* Freescale Book-E software loaded TLB or Book-3e (ISA 2.06+) MMU */
+#  include <asm/mmu-book3e.h>
 #elif defined (CONFIG_PPC_8xx)
 /* Motorola/Freescale 8xx software loaded TLB */
 #  include <asm/mmu-8xx.h>

commit f048aace29e007f2b642097e2da8231e0e9cce2d
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Dec 18 19:13:38 2008 +0000

    powerpc/mm: Add SMP support to no-hash TLB handling
    
    This commit moves the whole no-hash TLB handling out of line into a
    new tlb_nohash.c file, and implements some basic SMP support using
    IPIs and/or broadcast tlbivax instructions.
    
    Note that I'm using local invalidations for D->I cache coherency.
    
    At worst, if another processor is trying to execute the same and
    has the old entry in its TLB, it will just take a fault and re-do
    the TLB flush locally (it won't re-do the cache flush in any case).
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index dc8c0aef5e6c..6e7639911318 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -30,6 +30,22 @@
  */
 #define MMU_FTR_BIG_PHYS		ASM_CONST(0x00020000)
 
+/* Enable use of broadcast TLB invalidations. We don't always set it
+ * on processors that support it due to other constraints with the
+ * use of such invalidations
+ */
+#define MMU_FTR_USE_TLBIVAX_BCAST	ASM_CONST(0x00040000)
+
+/* Enable use of tlbilx invalidate-by-PID variant.
+ */
+#define MMU_FTR_USE_TLBILX_PID		ASM_CONST(0x00080000)
+
+/* This indicates that the processor cannot handle multiple outstanding
+ * broadcast tlbivax or tlbsync. This makes the code use a spinlock
+ * around such invalidate forms.
+ */
+#define MMU_FTR_LOCK_BCAST_INVAL	ASM_CONST(0x00100000)
+
 #ifndef __ASSEMBLY__
 #include <asm/cputable.h>
 

commit 7c03d653cd257793dc40520c94e229b5fd0578e7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Dec 18 19:13:32 2008 +0000

    powerpc/mm: Introduce MMU features
    
    We're soon running out of CPU features and I need to add some new
    ones for various MMU related bits, so this patch separates the MMU
    features from the CPU features.  I moved over the 32-bit MMU related
    ones, added base features for MMU type families, but didn't move
    over any 64-bit only feature yet.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
index 4c0e1b4f975c..dc8c0aef5e6c 100644
--- a/arch/powerpc/include/asm/mmu.h
+++ b/arch/powerpc/include/asm/mmu.h
@@ -2,6 +2,47 @@
 #define _ASM_POWERPC_MMU_H_
 #ifdef __KERNEL__
 
+#include <asm/asm-compat.h>
+#include <asm/feature-fixups.h>
+
+/*
+ * MMU features bit definitions
+ */
+
+/*
+ * First half is MMU families
+ */
+#define MMU_FTR_HPTE_TABLE		ASM_CONST(0x00000001)
+#define MMU_FTR_TYPE_8xx		ASM_CONST(0x00000002)
+#define MMU_FTR_TYPE_40x		ASM_CONST(0x00000004)
+#define MMU_FTR_TYPE_44x		ASM_CONST(0x00000008)
+#define MMU_FTR_TYPE_FSL_E		ASM_CONST(0x00000010)
+
+/*
+ * This is individual features
+ */
+
+/* Enable use of high BAT registers */
+#define MMU_FTR_USE_HIGH_BATS		ASM_CONST(0x00010000)
+
+/* Enable >32-bit physical addresses on 32-bit processor, only used
+ * by CONFIG_6xx currently as BookE supports that from day 1
+ */
+#define MMU_FTR_BIG_PHYS		ASM_CONST(0x00020000)
+
+#ifndef __ASSEMBLY__
+#include <asm/cputable.h>
+
+static inline int mmu_has_feature(unsigned long feature)
+{
+	return (cur_cpu_spec->mmu_features & feature);
+}
+
+extern unsigned int __start___mmu_ftr_fixup, __stop___mmu_ftr_fixup;
+
+#endif /* !__ASSEMBLY__ */
+
+
 #ifdef CONFIG_PPC64
 /* 64-bit classic hash table MMU */
 #  include <asm/mmu-hash64.h>

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/mmu.h b/arch/powerpc/include/asm/mmu.h
new file mode 100644
index 000000000000..4c0e1b4f975c
--- /dev/null
+++ b/arch/powerpc/include/asm/mmu.h
@@ -0,0 +1,26 @@
+#ifndef _ASM_POWERPC_MMU_H_
+#define _ASM_POWERPC_MMU_H_
+#ifdef __KERNEL__
+
+#ifdef CONFIG_PPC64
+/* 64-bit classic hash table MMU */
+#  include <asm/mmu-hash64.h>
+#elif defined(CONFIG_PPC_STD_MMU)
+/* 32-bit classic hash table MMU */
+#  include <asm/mmu-hash32.h>
+#elif defined(CONFIG_40x)
+/* 40x-style software loaded TLB */
+#  include <asm/mmu-40x.h>
+#elif defined(CONFIG_44x)
+/* 44x-style software loaded TLB */
+#  include <asm/mmu-44x.h>
+#elif defined(CONFIG_FSL_BOOKE)
+/* Freescale Book-E software loaded TLB */
+#  include <asm/mmu-fsl-booke.h>
+#elif defined (CONFIG_PPC_8xx)
+/* Motorola/Freescale 8xx software loaded TLB */
+#  include <asm/mmu-8xx.h>
+#endif
+
+#endif /* __KERNEL__ */
+#endif /* _ASM_POWERPC_MMU_H_ */
