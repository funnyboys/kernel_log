commit c420644c0a8f8839ca7269acbb8a3fc7fe1ec97d
Author: Haren Myneni <haren@linux.ibm.com>
Date:   Wed Apr 15 23:08:11 2020 -0700

    powerpc: Use mm_context vas_windows counter to issue CP_ABORT
    
    set_thread_uses_vas() sets used_vas flag for a process that opened VAS
    window and issue CP_ABORT during context switch for only that process.
    In multi-thread application, windows can be shared. For example Thread
    A can open a window and Thread B can run COPY/PASTE instructions to
    send NX request which may cause corruption or snooping or a covert
    channel Also once this flag is set, continue to run CP_ABORT even the
    VAS window is closed.
    
    So define vas-windows counter in process mm_context, increment this
    counter for each window open and decrement it for window close. If
    vas-windows is set, issue CP_ABORT during context switch. It means
    clear the foreign real address mapping only if the process / thread
    uses COPY/PASTE. Then disable it for that process if windows are not
    open.
    
    Moved set_thread_uses_vas() code to vas_tx_win_open() as this
    functionality is needed only for userspace open windows. We are adding
    VAS userspace support along with this fix. So no need to include this
    fix in stable releases.
    
    Fixes: 9d2a4d71332c ("powerpc: Define set_thread_uses_vas()")
    Signed-off-by: Haren Myneni <haren@linux.ibm.com>
    Reported-by: Nicholas Piggin <npiggin@gmail.com>
    Suggested-by: Milton Miller <miltonm@us.ibm.com>
    Suggested-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/1587017291.2275.1077.camel@hbabu-laptop

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index b867b58b1093..fdab93428372 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -102,8 +102,6 @@ static inline void clear_task_ebb(struct task_struct *t)
 #endif
 }
 
-extern int set_thread_uses_vas(void);
-
 extern int set_thread_tidr(struct task_struct *t);
 
 #endif /* _ASM_POWERPC_SWITCH_TO_H */

commit 6cc0c16d82f889f0083f3608237189afb55b67be
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 26 03:35:37 2020 +1000

    powerpc/64s: Implement interrupt exit logic in C
    
    Implement the bulk of interrupt return logic in C. The asm return code
    must handle a few cases: restoring full GPRs, and emulating stack
    store.
    
    The stack store emulation is significantly simplfied, rather than
    creating a new return frame and switching to that before performing
    the store, it uses the PACA to keep a scratch register around to
    perform the store.
    
    The asm return code is moved into 64e for now. The new logic has made
    allowance for 64e, but I don't have a full environment that works well
    to test it, and even booting in emulated qemu is not great for stress
    testing. 64e shouldn't be too far off working with this, given a bit
    more testing and auditing of the logic.
    
    This is slightly faster on a POWER9 (page fault speed increases about
    1.1%), probably due to reduced mtmsrd.
    
    mpe: Includes fixes from Nick for _TIF_EMULATE_STACK_STORE
    handling (including the fast_interrupt_return path), to remove
    trace_hardirqs_on(), and fixes the interrupt-return part of the
    MSR_VSX restore bug caught by tm-unavailable selftest.
    
    mpe: Incorporate fix from Nick:
    
    The return-to-kernel path has to replay any soft-pending interrupts if
    it is returning to a context that had interrupts soft-enabled. It has
    to do this carefully and avoid plain enabling interrupts if this is an
    irq context, which can cause multiple nesting of interrupts on the
    stack, and other unexpected issues.
    
    The code which avoided this case got the soft-mask state wrong, and
    marked interrupts as enabled before going around again to retry. This
    seems to be mostly harmless except when PREEMPT=y, this calls
    preempt_schedule_irq with irqs apparently enabled and runs into a BUG
    in kernel/sched/core.c
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Suchanek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200225173541.1549955-29-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 476008bc3d08..b867b58b1093 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -23,7 +23,13 @@ extern void switch_booke_debug_regs(struct debug_reg *new_debug);
 
 extern int emulate_altivec(struct pt_regs *);
 
+#ifdef CONFIG_PPC_BOOK3S_64
 void restore_math(struct pt_regs *regs);
+#else
+static inline void restore_math(struct pt_regs *regs)
+{
+}
+#endif
 
 void restore_tm_state(struct pt_regs *regs);
 

commit 68b34588e2027f699a3c034235f21cd19356b2e6
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 26 03:35:34 2020 +1000

    powerpc/64/sycall: Implement syscall entry/exit logic in C
    
    System call entry and particularly exit code is beyond the limit of
    what is reasonable to implement in asm.
    
    This conversion moves all conditional branches out of the asm code,
    except for the case that all GPRs should be restored at exit.
    
    Null syscall test is about 5% faster after this patch, because the
    exit work is handled under local_irq_disable, and the hard mask and
    pending interrupt replay is handled after that, which avoids games
    with MSR.
    
    mpe: Includes subsequent fixes from Nick:
    
    This fixes 4 issues caught by TM selftests. First was a tm-syscall bug
    that hit due to tabort_syscall being called after interrupts were
    reconciled (in a subsequent patch), which led to interrupts being
    enabled before tabort_syscall was called. Rather than going through an
    un-reconciling interrupts for the return, I just go back to putting
    the test early in asm, the C-ification of that wasn't a big win
    anyway.
    
    Second is the syscall return _TIF_USER_WORK_MASK check would go into
    an infinite loop if _TIF_RESTORE_TM became set. The asm code uses
    _TIF_USER_WORK_MASK to brach to slowpath which includes
    restore_tm_state.
    
    Third is system call return was not calling restore_tm_state, I missed
    this completely (alhtough it's in the return from interrupt C
    conversion because when the asm syscall code encountered problems it
    would branch to the interrupt return code.
    
    Fourth is MSR_VEC missing from restore_math, which was caught by
    tm-unavailable selftest taking an unexpected facility unavailable
    interrupt when testing VSX unavailble exception with MSR.FP=1
    MSR.VEC=1. Fourth case also has a fixup in a subsequent patch.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Suchanek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200225173541.1549955-26-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 5b03d8a82409..476008bc3d08 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -5,6 +5,7 @@
 #ifndef _ASM_POWERPC_SWITCH_TO_H
 #define _ASM_POWERPC_SWITCH_TO_H
 
+#include <linux/sched.h>
 #include <asm/reg.h>
 
 struct thread_struct;
@@ -22,6 +23,10 @@ extern void switch_booke_debug_regs(struct debug_reg *new_debug);
 
 extern int emulate_altivec(struct pt_regs *);
 
+void restore_math(struct pt_regs *regs);
+
+void restore_tm_state(struct pt_regs *regs);
+
 extern void flush_all_to_thread(struct task_struct *);
 extern void giveup_all(struct task_struct *);
 

commit 71cc64a85d8d99936f6851709a07f18c87a0adab
Author: Alastair D'Silva <alastair@d-silva.org>
Date:   Fri May 11 16:12:59 2018 +1000

    powerpc: use task_pid_nr() for TID allocation
    
    The current implementation of TID allocation, using a global IDR, may
    result in an errant process starving the system of available TIDs.
    Instead, use task_pid_nr(), as mentioned by the original author. The
    scenario described which prevented it's use is not applicable, as
    set_thread_tidr can only be called after the task struct has been
    populated.
    
    In the unlikely event that 2 threads share the TID and are waiting,
    all potential outcomes have been determined safe.
    
    Signed-off-by: Alastair D'Silva <alastair@d-silva.org>
    Reviewed-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index be8c9fa23983..5b03d8a82409 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -94,6 +94,5 @@ static inline void clear_task_ebb(struct task_struct *t)
 extern int set_thread_uses_vas(void);
 
 extern int set_thread_tidr(struct task_struct *t);
-extern void clear_thread_tidr(struct task_struct *t);
 
 #endif /* _ASM_POWERPC_SWITCH_TO_H */

commit 1cdf039bf82a39d816ca4b8161b01c0acfca3e62
Author: Mathieu Malaterre <malat@debian.org>
Date:   Sun Feb 25 18:22:23 2018 +0100

    powerpc/kernel: Make function __giveup_fpu() static
    
    __giveup_fpu() is never called outside process.c, so it can be static.
    That also means we don't need an empty definition in switch_to.h
    
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    [mpe: Also drop the empty version, rewrite change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index c3ca42cdc9f5..be8c9fa23983 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -35,7 +35,6 @@ static inline void disable_kernel_fp(void)
 	msr_check_and_clear(MSR_FP);
 }
 #else
-static inline void __giveup_fpu(struct task_struct *t) { }
 static inline void save_fpu(struct task_struct *t) { }
 static inline void flush_fp_to_thread(struct task_struct *t) { }
 #endif

commit 5b0e2cb020085efe202123162502e0b551e49a0e
Merge: 758f875848d7 3ffa9d9e2a7c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 12:47:46 2017 -0800

    Merge tag 'powerpc-4.15-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "A bit of a small release, I suspect in part due to me travelling for
      KS. But my backlog of patches to review is smaller than usual, so I
      think in part folks just didn't send as much this cycle.
    
      Non-highlights:
    
       - Five fixes for the >128T address space handling, both to fix bugs
         in our implementation and to bring the semantics exactly into line
         with x86.
    
      Highlights:
    
       - Support for a new OPAL call on bare metal machines which gives us a
         true NMI (ie. is not masked by MSR[EE]=0) for debugging etc.
    
       - Support for Power9 DD2 in the CXL driver.
    
       - Improvements to machine check handling so that uncorrectable errors
         can be reported into the generic memory_failure() machinery.
    
       - Some fixes and improvements for VPHN, which is used under PowerVM
         to notify the Linux partition of topology changes.
    
       - Plumbing to enable TM (transactional memory) without suspend on
         some Power9 processors (PPC_FEATURE2_HTM_NO_SUSPEND).
    
       - Support for emulating vector loads form cache-inhibited memory, on
         some Power9 revisions.
    
       - Disable the fast-endian switch "syscall" by default (behind a
         CONFIG), we believe it has never had any users.
    
       - A major rework of the API drivers use when initiating and waiting
         for long running operations performed by OPAL firmware, and changes
         to the powernv_flash driver to use the new API.
    
       - Several fixes for the handling of FP/VMX/VSX while processes are
         using transactional memory.
    
       - Optimisations of TLB range flushes when using the radix MMU on
         Power9.
    
       - Improvements to the VAS facility used to access coprocessors on
         Power9, and related improvements to the way the NX crypto driver
         handles requests.
    
       - Implementation of PMEM_API and UACCESS_FLUSHCACHE for 64-bit.
    
      Thanks to: Alexey Kardashevskiy, Alistair Popple, Allen Pais, Andrew
      Donnellan, Aneesh Kumar K.V, Arnd Bergmann, Balbir Singh, Benjamin
      Herrenschmidt, Breno Leitao, Christophe Leroy, Christophe Lombard,
      Cyril Bur, Frederic Barrat, Gautham R. Shenoy, Geert Uytterhoeven,
      Guilherme G. Piccoli, Gustavo Romero, Haren Myneni, Joel Stanley,
      Kamalesh Babulal, Kautuk Consul, Markus Elfring, Masami Hiramatsu,
      Michael Bringmann, Michael Neuling, Michal Suchanek, Naveen N. Rao,
      Nicholas Piggin, Oliver O'Halloran, Paul Mackerras, Pedro Miraglia
      Franco de Carvalho, Philippe Bergheaud, Sandipan Das, Seth Forshee,
      Shriya, Stephen Rothwell, Stewart Smith, Sukadev Bhattiprolu, Tyrel
      Datwyler, Vaibhav Jain, Vaidyanathan Srinivasan, and William A.
      Kennington III"
    
    * tag 'powerpc-4.15-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (151 commits)
      powerpc/64s: Fix Power9 DD2.0 workarounds by adding DD2.1 feature
      powerpc/64s: Fix masking of SRR1 bits on instruction fault
      powerpc/64s: mm_context.addr_limit is only used on hash
      powerpc/64s/radix: Fix 128TB-512TB virtual address boundary case allocation
      powerpc/64s/hash: Allow MAP_FIXED allocations to cross 128TB boundary
      powerpc/64s/hash: Fix fork() with 512TB process address space
      powerpc/64s/hash: Fix 128TB-512TB virtual address boundary case allocation
      powerpc/64s/hash: Fix 512T hint detection to use >= 128T
      powerpc: Fix DABR match on hash based systems
      powerpc/signal: Properly handle return value from uprobe_deny_signal()
      powerpc/fadump: use kstrtoint to handle sysfs store
      powerpc/lib: Implement UACCESS_FLUSHCACHE API
      powerpc/lib: Implement PMEM API
      powerpc/powernv/npu: Don't explicitly flush nmmu tlb
      powerpc/powernv/npu: Use flush_all_mm() instead of flush_tlb_mm()
      powerpc/powernv/idle: Round up latency and residency values
      powerpc/kprobes: refactor kprobe_lookup_name for safer string operations
      powerpc/kprobes: Blacklist emulate_update_regs() from kprobes
      powerpc/kprobes: Do not disable interrupts for optprobes and kprobes_on_ftrace
      powerpc/kprobes: Disable preemption before invoking probe handler for optprobes
      ...

commit 9d2a4d71332cfdf4ea90754ad9b2f05a5ee5f6c7
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Tue Nov 7 18:23:54 2017 -0800

    powerpc: Define set_thread_uses_vas()
    
    A CP_ABORT instruction is required in processes that have mapped a VAS
    "paste address" with the intention of using COPY/PASTE instructions.
    But since CP_ABORT is expensive, we want to restrict it to only
    processes that use/intend to use COPY/PASTE.
    
    Define an interface, set_thread_uses_vas(), that VAS can use to
    indicate that the current process opened a send window. During context
    switch, issue CP_ABORT only for processes that have the flag set.
    
    Thanks for input from Nick Piggin, Michael Ellerman.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    [mpe: Fix to not use new_thread after _switch() returns]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index f5da32f43fed..07ff06e9dae1 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -91,6 +91,8 @@ static inline void clear_task_ebb(struct task_struct *t)
 #endif
 }
 
+extern int set_thread_uses_vas(void);
+
 extern int set_thread_tidr(struct task_struct *t);
 extern void clear_thread_tidr(struct task_struct *t);
 

commit ec233ede4c8654894610ea54f4dae7adc954ac62
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Tue Nov 7 18:23:53 2017 -0800

    powerpc: Add support for setting SPRN_TIDR
    
    We need the SPRN_TIDR to be set for use with fast thread-wakeup (core-
    to-core wakeup) and also with CAPI.
    
    Each thread in a process needs to have a unique id within the process.
    But for now, we assign globally unique thread ids to all threads in
    the system.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Philippe Bergheaud <felix@linux.vnet.ibm.com>
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    [mpe: Simplify tidr clearing on fork() and ctx switch code]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 17c8380673a6..f5da32f43fed 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -91,4 +91,7 @@ static inline void clear_task_ebb(struct task_struct *t)
 #endif
 }
 
+extern int set_thread_tidr(struct task_struct *t);
+extern void clear_thread_tidr(struct task_struct *t);
+
 #endif /* _ASM_POWERPC_SWITCH_TO_H */

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 17c8380673a6..bf820f53e27e 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Copyright (C) 1999 Cort Dougan <cort@cs.nmt.edu>
  */

commit c7a318ba868c61fc9be710a4970172d8c2eeb8b9
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Wed Aug 10 15:44:46 2016 +1000

    powerpc/ptrace: Fix coredump since ptrace TM changes
    
    Commit 8d460f6156cd ("powerpc/process: Add the function
    flush_tmregs_to_thread") added flush_tmregs_to_thread() and included
    the assumption that it would only be called for a task which is not
    current.
    
    Although this is correct for ptrace, when generating a core dump, some
    of the routines which call flush_tmregs_to_thread() are called. This
    leads to a WARNing such as:
    
      Not expecting ptrace on self: TM regs may be incorrect
      ------------[ cut here ]------------
      WARNING: CPU: 123 PID: 7727 at arch/powerpc/kernel/process.c:1088 flush_tmregs_to_thread+0x78/0x80
      CPU: 123 PID: 7727 Comm: libvirtd Not tainted 4.8.0-rc1-gcc6x-g61e8a0d #1
      task: c000000fe631b600 task.stack: c000000fe63b0000
      NIP: c00000000001a1a8 LR: c00000000001a1a4 CTR: c000000000717780
      REGS: c000000fe63b3420 TRAP: 0700   Not tainted  (4.8.0-rc1-gcc6x-g61e8a0d)
      MSR: 900000010282b033 <SF,HV,VEC,VSX,EE,FP,ME,IR,DR,RI,LE,TM[E]>  CR: 28004222  XER: 20000000
      ...
      NIP [c00000000001a1a8] flush_tmregs_to_thread+0x78/0x80
      LR [c00000000001a1a4] flush_tmregs_to_thread+0x74/0x80
      Call Trace:
       flush_tmregs_to_thread+0x74/0x80 (unreliable)
       vsr_get+0x64/0x1a0
       elf_core_dump+0x604/0x1430
       do_coredump+0x5fc/0x1200
       get_signal+0x398/0x740
       do_signal+0x54/0x2b0
       do_notify_resume+0x98/0xb0
       ret_from_except_lite+0x70/0x74
    
    So fix flush_tmregs_to_thread() to detect the case where it is called on
    current, and a transaction is active, and in that case flush the TM regs
    to the thread_struct.
    
    This patch also moves flush_tmregs_to_thread() into ptrace.c as it is
    only called from that file.
    
    Fixes: 8d460f6156cd ("powerpc/process: Add the function flush_tmregs_to_thread")
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    [mpe: Flesh out change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 0a74ebe934e1..17c8380673a6 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -75,14 +75,6 @@ static inline void disable_kernel_spe(void)
 static inline void __giveup_spe(struct task_struct *t) { }
 #endif
 
-#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
-extern void flush_tmregs_to_thread(struct task_struct *);
-#else
-static inline void flush_tmregs_to_thread(struct task_struct *t)
-{
-}
-#endif
-
 static inline void clear_task_ebb(struct task_struct *t)
 {
 #ifdef CONFIG_PPC_BOOK3S_64

commit 8d460f6156cd55d981d109f01b82cbea8cf80e57
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Thu Jul 28 10:57:31 2016 +0800

    powerpc/process: Add the function flush_tmregs_to_thread
    
    This patch creates a function flush_tmregs_to_thread which
    will then be used by subsequent patches in this series. The
    function checks for self tracing ptrace interface attempts
    while in the TM context and logs appropriate warning message.
    
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 17c8380673a6..0a74ebe934e1 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -75,6 +75,14 @@ static inline void disable_kernel_spe(void)
 static inline void __giveup_spe(struct task_struct *t) { }
 #endif
 
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+extern void flush_tmregs_to_thread(struct task_struct *);
+#else
+static inline void flush_tmregs_to_thread(struct task_struct *t)
+{
+}
+#endif
+
 static inline void clear_task_ebb(struct task_struct *t)
 {
 #ifdef CONFIG_PPC_BOOK3S_64

commit bf6a4d5b75d1ea87897fe68d0e45d35a2996c678
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:51 2016 +1100

    powerpc: Add the ability to save VSX without giving it up
    
    This patch adds the ability to be able to save the VSX registers to the
    thread struct without giving up (disabling the facility) next time the
    process returns to userspace.
    
    This patch builds on a previous optimisation for the FPU and VEC registers
    in the thread copy path to avoid a possibly pointless reload of VSX state.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 9028822bb73f..17c8380673a6 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -56,14 +56,10 @@ static inline void __giveup_altivec(struct task_struct *t) { }
 #ifdef CONFIG_VSX
 extern void enable_kernel_vsx(void);
 extern void flush_vsx_to_thread(struct task_struct *);
-extern void giveup_vsx(struct task_struct *);
-extern void __giveup_vsx(struct task_struct *);
 static inline void disable_kernel_vsx(void)
 {
 	msr_check_and_clear(MSR_FP|MSR_VEC|MSR_VSX);
 }
-#else
-static inline void __giveup_vsx(struct task_struct *t) { }
 #endif
 
 #ifdef CONFIG_SPE

commit 6f515d842e8e1b205e54f44b9013bf14870b97a7
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:50 2016 +1100

    powerpc: Add the ability to save Altivec without giving it up
    
    This patch adds the ability to be able to save the VEC registers to the
    thread struct without giving up (disabling the facility) next time the
    process returns to userspace.
    
    This patch builds on a previous optimisation for the FPU registers in the
    thread copy path to avoid a possibly pointless reload of VEC state.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 6a201e8dc947..9028822bb73f 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -43,12 +43,13 @@ static inline void flush_fp_to_thread(struct task_struct *t) { }
 extern void enable_kernel_altivec(void);
 extern void flush_altivec_to_thread(struct task_struct *);
 extern void giveup_altivec(struct task_struct *);
-extern void __giveup_altivec(struct task_struct *);
+extern void save_altivec(struct task_struct *);
 static inline void disable_kernel_altivec(void)
 {
 	msr_check_and_clear(MSR_VEC);
 }
 #else
+static inline void save_altivec(struct task_struct *t) { }
 static inline void __giveup_altivec(struct task_struct *t) { }
 #endif
 

commit 8792468da5e12e77e76e1edf081acf0392abb331
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:49 2016 +1100

    powerpc: Add the ability to save FPU without giving it up
    
    This patch adds the ability to be able to save the FPU registers to the
    thread struct without giving up (disabling the facility) next time the
    process returns to userspace.
    
    This patch optimises the thread copy path (as a result of a fork() or
    clone()) so that the parent thread can return to userspace with hot
    registers avoiding a possibly pointless reload of FPU register state.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 3690041c126a..6a201e8dc947 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -28,13 +28,14 @@ extern void giveup_all(struct task_struct *);
 extern void enable_kernel_fp(void);
 extern void flush_fp_to_thread(struct task_struct *);
 extern void giveup_fpu(struct task_struct *);
-extern void __giveup_fpu(struct task_struct *);
+extern void save_fpu(struct task_struct *);
 static inline void disable_kernel_fp(void)
 {
 	msr_check_and_clear(MSR_FP);
 }
 #else
 static inline void __giveup_fpu(struct task_struct *t) { }
+static inline void save_fpu(struct task_struct *t) { }
 static inline void flush_fp_to_thread(struct task_struct *t) { }
 #endif
 

commit de2a20aa7237b45d3c14a2505804a8daa95a8f53
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:48 2016 +1100

    powerpc: Prepare for splitting giveup_{fpu, altivec, vsx} in two
    
    This prepares for the decoupling of saving {fpu,altivec,vsx} registers and
    marking {fpu,altivec,vsx} as being unused by a thread.
    
    Currently giveup_{fpu,altivec,vsx}() does both however optimisations to
    task switching can be made if these two operations are decoupled.
    save_all() will permit the saving of registers to thread structs and leave
    threads MSR with bits enabled.
    
    This patch introduces no functional change.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 5b268b6be74c..3690041c126a 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -34,6 +34,7 @@ static inline void disable_kernel_fp(void)
 	msr_check_and_clear(MSR_FP);
 }
 #else
+static inline void __giveup_fpu(struct task_struct *t) { }
 static inline void flush_fp_to_thread(struct task_struct *t) { }
 #endif
 
@@ -46,6 +47,8 @@ static inline void disable_kernel_altivec(void)
 {
 	msr_check_and_clear(MSR_VEC);
 }
+#else
+static inline void __giveup_altivec(struct task_struct *t) { }
 #endif
 
 #ifdef CONFIG_VSX
@@ -57,6 +60,8 @@ static inline void disable_kernel_vsx(void)
 {
 	msr_check_and_clear(MSR_FP|MSR_VEC|MSR_VSX);
 }
+#else
+static inline void __giveup_vsx(struct task_struct *t) { }
 #endif
 
 #ifdef CONFIG_SPE
@@ -68,6 +73,8 @@ static inline void disable_kernel_spe(void)
 {
 	msr_check_and_clear(MSR_SPE);
 }
+#else
+static inline void __giveup_spe(struct task_struct *t) { }
 #endif
 
 static inline void clear_task_ebb(struct task_struct *t)

commit d1e1cf2e38def301fde42c1a33f896f974941d7b
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:11 2015 +1100

    powerpc: clean up asm/switch_to.h
    
    Remove a bunch of unnecessary fallback functions and group
    things in a more logical way.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 81d46a433c03..5b268b6be74c 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -14,23 +14,18 @@ extern struct task_struct *__switch_to(struct task_struct *,
 	struct task_struct *);
 #define switch_to(prev, next, last)	((last) = __switch_to((prev), (next)))
 
-struct thread_struct;
 extern struct task_struct *_switch(struct thread_struct *prev,
 				   struct thread_struct *next);
 
-extern void enable_kernel_fp(void);
-extern void enable_kernel_altivec(void);
-extern void enable_kernel_vsx(void);
+extern void switch_booke_debug_regs(struct debug_reg *new_debug);
+
 extern int emulate_altivec(struct pt_regs *);
-extern void __giveup_vsx(struct task_struct *);
-extern void giveup_vsx(struct task_struct *);
-extern void enable_kernel_spe(void);
-extern void load_up_spe(struct task_struct *);
-extern void giveup_all(struct task_struct *);
+
 extern void flush_all_to_thread(struct task_struct *);
-extern void switch_booke_debug_regs(struct debug_reg *new_debug);
+extern void giveup_all(struct task_struct *);
 
 #ifdef CONFIG_PPC_FPU
+extern void enable_kernel_fp(void);
 extern void flush_fp_to_thread(struct task_struct *);
 extern void giveup_fpu(struct task_struct *);
 extern void __giveup_fpu(struct task_struct *);
@@ -38,14 +33,12 @@ static inline void disable_kernel_fp(void)
 {
 	msr_check_and_clear(MSR_FP);
 }
-
 #else
 static inline void flush_fp_to_thread(struct task_struct *t) { }
-static inline void giveup_fpu(struct task_struct *t) { }
-static inline void __giveup_fpu(struct task_struct *t) { }
 #endif
 
 #ifdef CONFIG_ALTIVEC
+extern void enable_kernel_altivec(void);
 extern void flush_altivec_to_thread(struct task_struct *);
 extern void giveup_altivec(struct task_struct *);
 extern void __giveup_altivec(struct task_struct *);
@@ -53,25 +46,21 @@ static inline void disable_kernel_altivec(void)
 {
 	msr_check_and_clear(MSR_VEC);
 }
-#else
-static inline void flush_altivec_to_thread(struct task_struct *t) { }
-static inline void giveup_altivec(struct task_struct *t) { }
-static inline void __giveup_altivec(struct task_struct *t) { }
 #endif
 
 #ifdef CONFIG_VSX
+extern void enable_kernel_vsx(void);
 extern void flush_vsx_to_thread(struct task_struct *);
+extern void giveup_vsx(struct task_struct *);
+extern void __giveup_vsx(struct task_struct *);
 static inline void disable_kernel_vsx(void)
 {
 	msr_check_and_clear(MSR_FP|MSR_VEC|MSR_VSX);
 }
-#else
-static inline void flush_vsx_to_thread(struct task_struct *t)
-{
-}
 #endif
 
 #ifdef CONFIG_SPE
+extern void enable_kernel_spe(void);
 extern void flush_spe_to_thread(struct task_struct *);
 extern void giveup_spe(struct task_struct *);
 extern void __giveup_spe(struct task_struct *);
@@ -79,10 +68,6 @@ static inline void disable_kernel_spe(void)
 {
 	msr_check_and_clear(MSR_SPE);
 }
-#else
-static inline void flush_spe_to_thread(struct task_struct *t) { }
-static inline void giveup_spe(struct task_struct *t) { }
-static inline void __giveup_spe(struct task_struct *t) { }
 #endif
 
 static inline void clear_task_ebb(struct task_struct *t)

commit 579e633e764e6e5f7784b74e7df3e81fe11f40de
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:09 2015 +1100

    powerpc: create flush_all_to_thread()
    
    Create a single function that flushes everything (FP, VMX, VSX, SPE).
    Doing this all at once means we only do one MSR write.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 8f856788a7cf..81d46a433c03 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -27,6 +27,7 @@ extern void giveup_vsx(struct task_struct *);
 extern void enable_kernel_spe(void);
 extern void load_up_spe(struct task_struct *);
 extern void giveup_all(struct task_struct *);
+extern void flush_all_to_thread(struct task_struct *);
 extern void switch_booke_debug_regs(struct debug_reg *new_debug);
 
 #ifdef CONFIG_PPC_FPU

commit c208505900b232ecdc81dee54cb3a032e75d88d6
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:08 2015 +1100

    powerpc: create giveup_all()
    
    Create a single function that gives everything up (FP, VMX, VSX, SPE).
    Doing this all at once means we only do one MSR write.
    
    A context switch microbenchmark using yield():
    
    http://ozlabs.org/~anton/junkcode/context_switch2.c
    
    ./context_switch2 --test=yield --fp --altivec --vector 0 0
    
    shows an improvement of 3% on POWER8.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    [mpe: giveup_all() needs to be EXPORT_SYMBOL'ed]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 9414dcb180d6..8f856788a7cf 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -26,6 +26,7 @@ extern void __giveup_vsx(struct task_struct *);
 extern void giveup_vsx(struct task_struct *);
 extern void enable_kernel_spe(void);
 extern void load_up_spe(struct task_struct *);
+extern void giveup_all(struct task_struct *);
 extern void switch_booke_debug_regs(struct debug_reg *new_debug);
 
 #ifdef CONFIG_PPC_FPU

commit 3eb5d5888dc68c9b187998ca4249b8b9fa481eeb
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:06 2015 +1100

    powerpc: Add ppc_strict_facility_enable boot option
    
    Add a boot option that strictly manages the MSR unavailable bits.
    This catches kernel uses of FP/Altivec/SPE that would otherwise
    corrupt user state.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 438502f59550..9414dcb180d6 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -4,6 +4,8 @@
 #ifndef _ASM_POWERPC_SWITCH_TO_H
 #define _ASM_POWERPC_SWITCH_TO_H
 
+#include <asm/reg.h>
+
 struct thread_struct;
 struct task_struct;
 struct pt_regs;
@@ -26,15 +28,15 @@ extern void enable_kernel_spe(void);
 extern void load_up_spe(struct task_struct *);
 extern void switch_booke_debug_regs(struct debug_reg *new_debug);
 
-static inline void disable_kernel_fp(void) { }
-static inline void disable_kernel_altivec(void) { }
-static inline void disable_kernel_spe(void) { }
-static inline void disable_kernel_vsx(void) { }
-
 #ifdef CONFIG_PPC_FPU
 extern void flush_fp_to_thread(struct task_struct *);
 extern void giveup_fpu(struct task_struct *);
 extern void __giveup_fpu(struct task_struct *);
+static inline void disable_kernel_fp(void)
+{
+	msr_check_and_clear(MSR_FP);
+}
+
 #else
 static inline void flush_fp_to_thread(struct task_struct *t) { }
 static inline void giveup_fpu(struct task_struct *t) { }
@@ -45,6 +47,10 @@ static inline void __giveup_fpu(struct task_struct *t) { }
 extern void flush_altivec_to_thread(struct task_struct *);
 extern void giveup_altivec(struct task_struct *);
 extern void __giveup_altivec(struct task_struct *);
+static inline void disable_kernel_altivec(void)
+{
+	msr_check_and_clear(MSR_VEC);
+}
 #else
 static inline void flush_altivec_to_thread(struct task_struct *t) { }
 static inline void giveup_altivec(struct task_struct *t) { }
@@ -53,6 +59,10 @@ static inline void __giveup_altivec(struct task_struct *t) { }
 
 #ifdef CONFIG_VSX
 extern void flush_vsx_to_thread(struct task_struct *);
+static inline void disable_kernel_vsx(void)
+{
+	msr_check_and_clear(MSR_FP|MSR_VEC|MSR_VSX);
+}
 #else
 static inline void flush_vsx_to_thread(struct task_struct *t)
 {
@@ -63,6 +73,10 @@ static inline void flush_vsx_to_thread(struct task_struct *t)
 extern void flush_spe_to_thread(struct task_struct *);
 extern void giveup_spe(struct task_struct *);
 extern void __giveup_spe(struct task_struct *);
+static inline void disable_kernel_spe(void)
+{
+	msr_check_and_clear(MSR_SPE);
+}
 #else
 static inline void flush_spe_to_thread(struct task_struct *t) { }
 static inline void giveup_spe(struct task_struct *t) { }

commit dc4fbba11e4661a6a77a1f89ba32f9082e6395ff
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:05 2015 +1100

    powerpc: Create disable_kernel_{fp,altivec,vsx,spe}()
    
    The enable_kernel_*() functions leave the relevant MSR bits enabled
    until we exit the kernel sometime later. Create disable versions
    that wrap the kernel use of FP, Altivec VSX or SPE.
    
    While we don't want to disable it normally for performance reasons
    (MSR writes are slow), it will be used for a debug boot option that
    does this and catches bad uses in other areas of the kernel.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index c2678b93bcba..438502f59550 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -26,6 +26,11 @@ extern void enable_kernel_spe(void);
 extern void load_up_spe(struct task_struct *);
 extern void switch_booke_debug_regs(struct debug_reg *new_debug);
 
+static inline void disable_kernel_fp(void) { }
+static inline void disable_kernel_altivec(void) { }
+static inline void disable_kernel_spe(void) { }
+static inline void disable_kernel_vsx(void) { }
+
 #ifdef CONFIG_PPC_FPU
 extern void flush_fp_to_thread(struct task_struct *);
 extern void giveup_fpu(struct task_struct *);

commit 98da581e0846f6d932a4bc46a55458140e20478a
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:01 2015 +1100

    powerpc: Move part of giveup_fpu,altivec,spe into c
    
    Move the MSR modification into new c functions. Removing it from
    the low level functions will allow us to avoid costly MSR writes
    by batching them up.
    
    Move the check_if_tm_restore_required() check into these new functions.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 042aaf05a787..c2678b93bcba 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -23,28 +23,27 @@ extern int emulate_altivec(struct pt_regs *);
 extern void __giveup_vsx(struct task_struct *);
 extern void giveup_vsx(struct task_struct *);
 extern void enable_kernel_spe(void);
-extern void giveup_spe(struct task_struct *);
 extern void load_up_spe(struct task_struct *);
 extern void switch_booke_debug_regs(struct debug_reg *new_debug);
 
 #ifdef CONFIG_PPC_FPU
 extern void flush_fp_to_thread(struct task_struct *);
 extern void giveup_fpu(struct task_struct *);
+extern void __giveup_fpu(struct task_struct *);
 #else
 static inline void flush_fp_to_thread(struct task_struct *t) { }
 static inline void giveup_fpu(struct task_struct *t) { }
+static inline void __giveup_fpu(struct task_struct *t) { }
 #endif
 
 #ifdef CONFIG_ALTIVEC
 extern void flush_altivec_to_thread(struct task_struct *);
 extern void giveup_altivec(struct task_struct *);
+extern void __giveup_altivec(struct task_struct *);
 #else
-static inline void flush_altivec_to_thread(struct task_struct *t)
-{
-}
-static inline void giveup_altivec(struct task_struct *t)
-{
-}
+static inline void flush_altivec_to_thread(struct task_struct *t) { }
+static inline void giveup_altivec(struct task_struct *t) { }
+static inline void __giveup_altivec(struct task_struct *t) { }
 #endif
 
 #ifdef CONFIG_VSX
@@ -57,10 +56,12 @@ static inline void flush_vsx_to_thread(struct task_struct *t)
 
 #ifdef CONFIG_SPE
 extern void flush_spe_to_thread(struct task_struct *);
+extern void giveup_spe(struct task_struct *);
+extern void __giveup_spe(struct task_struct *);
 #else
-static inline void flush_spe_to_thread(struct task_struct *t)
-{
-}
+static inline void flush_spe_to_thread(struct task_struct *t) { }
+static inline void giveup_spe(struct task_struct *t) { }
+static inline void __giveup_spe(struct task_struct *t) { }
 #endif
 
 static inline void clear_task_ebb(struct task_struct *t)

commit b51b1153d0e78a70767441273331d2de066bb929
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:00 2015 +1100

    powerpc: Remove NULL task struct pointer checks in FP and vector code
    
    We used to allow giveup_*() to be called with a NULL task struct
    pointer. Now those cases are handled in the caller we can remove
    the checks. We can also remove giveup_altivec_notask() which is also
    unused.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index bd1d93318350..042aaf05a787 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -38,7 +38,6 @@ static inline void giveup_fpu(struct task_struct *t) { }
 #ifdef CONFIG_ALTIVEC
 extern void flush_altivec_to_thread(struct task_struct *);
 extern void giveup_altivec(struct task_struct *);
-extern void giveup_altivec_notask(void);
 #else
 static inline void flush_altivec_to_thread(struct task_struct *t)
 {

commit af1bbc3dd3d501d27da72e1764afe5f5b0d3882d
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:43:57 2015 +1100

    powerpc: Remove UP only lazy floating point and vector optimisations
    
    The UP only lazy floating point and vector optimisations were written
    back when SMP was not common, and neither glibc nor gcc used vector
    instructions. Now SMP is very common, glibc aggressively uses vector
    instructions and gcc autovectorises.
    
    We want to add new optimisations that apply to both UP and SMP, but
    in preparation for that remove these UP only optimisations.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 33a071d24ba8..bd1d93318350 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -27,14 +27,6 @@ extern void giveup_spe(struct task_struct *);
 extern void load_up_spe(struct task_struct *);
 extern void switch_booke_debug_regs(struct debug_reg *new_debug);
 
-#ifndef CONFIG_SMP
-extern void discard_lazy_cpu_state(void);
-#else
-static inline void discard_lazy_cpu_state(void)
-{
-}
-#endif
-
 #ifdef CONFIG_PPC_FPU
 extern void flush_fp_to_thread(struct task_struct *);
 extern void giveup_fpu(struct task_struct *);

commit 152d523e6307c7152f9986a542f873b5c5863937
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:43:55 2015 +1100

    powerpc: Create context switch helpers save_sprs() and restore_sprs()
    
    Move all our context switch SPR save and restore code into two
    helpers. We do a few optimisations:
    
    - Group all mfsprs and all mtsprs. In many cases an mtspr sets a
    scoreboarding bit that an mfspr waits on, so the current practise of
    mfspr A; mtspr A; mfpsr B; mtspr B is the worst scheduling we can
    do.
    
    - SPR writes are slow, so check that the value is changing before
    writing it.
    
    A context switch microbenchmark using yield():
    
    http://ozlabs.org/~anton/junkcode/context_switch2.c
    
    ./context_switch2 --test=yield 0 0
    
    shows an improvement of almost 10% on POWER8.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 15cca17cba4b..33a071d24ba8 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -15,17 +15,6 @@ extern struct task_struct *__switch_to(struct task_struct *,
 struct thread_struct;
 extern struct task_struct *_switch(struct thread_struct *prev,
 				   struct thread_struct *next);
-#ifdef CONFIG_PPC_BOOK3S_64
-static inline void save_early_sprs(struct thread_struct *prev)
-{
-	if (cpu_has_feature(CPU_FTR_ARCH_207S))
-		prev->tar = mfspr(SPRN_TAR);
-	if (cpu_has_feature(CPU_FTR_DSCR))
-		prev->dscr = mfspr(SPRN_DSCR);
-}
-#else
-static inline void save_early_sprs(struct thread_struct *prev) {}
-#endif
 
 extern void enable_kernel_fp(void);
 extern void enable_kernel_altivec(void);

commit 72cd7b44bc99376b3f3c93cedcd052663fcdf705
Author: Leonidas Da Silva Barbosa <leosilva@linux.vnet.ibm.com>
Date:   Mon Jul 13 13:51:01 2015 -0300

    powerpc: Uncomment and make enable_kernel_vsx() routine available
    
    enable_kernel_vsx() function was commented since anything was using
    it. However, vmx-crypto driver uses VSX instructions which are
    only available if VSX is enable. Otherwise it rises an exception oops.
    
    This patch uncomment enable_kernel_vsx() routine and makes it available.
    
    Signed-off-by: Leonidas S. Barbosa <leosilva@linux.vnet.ibm.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 58abeda64cb7..15cca17cba4b 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -29,6 +29,7 @@ static inline void save_early_sprs(struct thread_struct *prev) {}
 
 extern void enable_kernel_fp(void);
 extern void enable_kernel_altivec(void);
+extern void enable_kernel_vsx(void);
 extern int emulate_altivec(struct pt_regs *);
 extern void __giveup_vsx(struct task_struct *);
 extern void giveup_vsx(struct task_struct *);

commit 3df48c981d5a9610e02e9270b1bc4274fb536710
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jun 10 16:46:21 2014 +1000

    powerpc/perf: Ensure all EBB register state is cleared on fork()
    
    In commit 330a1eb "Core EBB support for 64-bit book3s" I messed up
    clear_task_ebb(). It clears some but not all of the task's Event Based
    Branch (EBB) registers when we duplicate a task struct.
    
    That allows a child task to observe the EBBHR & EBBRR of its parent,
    which it should not be able to do.
    
    Fix it by clearing EBBHR & EBBRR.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: stable@vger.kernel.org [v3.11+]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index d2468eb12639..58abeda64cb7 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -86,6 +86,8 @@ static inline void clear_task_ebb(struct task_struct *t)
 {
 #ifdef CONFIG_PPC_BOOK3S_64
     /* EBB perf events are not inherited, so clear all EBB state. */
+    t->thread.ebbrr = 0;
+    t->thread.ebbhr = 0;
     t->thread.bescr = 0;
     t->thread.mmcr2 = 0;
     t->thread.mmcr0 = 0;

commit 96d016108640bc2b7fb0ee800737f80923847294
Author: Sam bobroff <sam.bobroff@au1.ibm.com>
Date:   Thu Jun 5 16:19:22 2014 +1000

    powerpc: Correct DSCR during TM context switch
    
    Correct the DSCR SPR becoming temporarily corrupted if a task is
    context switched during a transaction.
    
    The problem occurs while suspending the task and is caused by saving
    the DSCR to thread.dscr after it has already been set to the CPU's
    default value:
    
    __switch_to() calls __switch_to_tm()
            which calls tm_reclaim_task()
            which calls tm_reclaim_thread()
            which calls tm_reclaim()
                    where the DSCR is set to the CPU's default
    __switch_to() calls _switch()
                    where thread.dscr is set to the DSCR
    
    When the task is resumed, it's transaction will be doomed (as usual)
    and the DSCR SPR will be corrupted, although the checkpointed value
    will be correct. Therefore the DSCR will be immediately corrected by
    the transaction aborting, unless it has been suspended. In that case
    the incorrect value can be seen by the task until it resumes the
    transaction.
    
    The fix is to treat the DSCR similarly to the TAR and save it early
    in __switch_to().
    
    A program exposing the problem is added to the kernel self tests as:
    tools/testing/selftests/powerpc/tm/tm-resched-dscr.
    
    Signed-off-by: Sam Bobroff <sam.bobroff@au1.ibm.com>
    CC: <stable@vger.kernel.org> [v3.10+]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 0e83e7d8c73f..d2468eb12639 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -16,13 +16,15 @@ struct thread_struct;
 extern struct task_struct *_switch(struct thread_struct *prev,
 				   struct thread_struct *next);
 #ifdef CONFIG_PPC_BOOK3S_64
-static inline void save_tar(struct thread_struct *prev)
+static inline void save_early_sprs(struct thread_struct *prev)
 {
 	if (cpu_has_feature(CPU_FTR_ARCH_207S))
 		prev->tar = mfspr(SPRN_TAR);
+	if (cpu_has_feature(CPU_FTR_DSCR))
+		prev->dscr = mfspr(SPRN_DSCR);
 }
 #else
-static inline void save_tar(struct thread_struct *prev) {}
+static inline void save_early_sprs(struct thread_struct *prev) {}
 #endif
 
 extern void enable_kernel_fp(void);

commit b73117c49364551ff789db7c424a115ac5b77850
Merge: 77f01bdfa5e5 4068890931f6
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jan 29 18:29:01 2014 +0100

    Merge branch 'kvm-ppc-next' of git://github.com/agraf/linux-2.6 into kvm-queue
    
    Conflicts:
            arch/powerpc/kvm/book3s_hv_rmhandlers.S
            arch/powerpc/kvm/booke.c

commit 09548fdaf32ce77a68e7f9a8a3098c1306b04858
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Oct 15 20:43:01 2013 +1100

    KVM: PPC: Use load_fp/vr_state rather than load_up_fpu/altivec
    
    The load_up_fpu and load_up_altivec functions were never intended to
    be called from C, and do things like modifying the MSR value in their
    callers' stack frames, which are assumed to be interrupt frames.  In
    addition, on 32-bit Book S they require the MMU to be off.
    
    This makes KVM use the new load_fp_state() and load_vr_state() functions
    instead of load_up_fpu/altivec.  This means we can remove the assembler
    glue in book3s_rmhandlers.S, and potentially fixes a bug on Book E,
    where load_up_fpu was called directly from C.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 9ee12610af02..971ca336f451 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -25,10 +25,8 @@ static inline void save_tar(struct thread_struct *prev)
 static inline void save_tar(struct thread_struct *prev) {}
 #endif
 
-extern void load_up_fpu(void);
 extern void enable_kernel_fp(void);
 extern void enable_kernel_altivec(void);
-extern void load_up_altivec(struct task_struct *);
 extern int emulate_altivec(struct pt_regs *);
 extern void __giveup_vsx(struct task_struct *);
 extern void giveup_vsx(struct task_struct *);

commit f5f972102d5c12729f0a35fce266b580aaa03f66
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Nov 22 15:52:29 2013 -0600

    powerpc/kvm/booke: Fix build break due to stack frame size warning
    
    Commit ce11e48b7fdd256ec68b932a89b397a790566031 ("KVM: PPC: E500: Add
    userspace debug stub support") added "struct thread_struct" to the
    stack of kvmppc_vcpu_run().  thread_struct is 1152 bytes on my build,
    compared to 48 bytes for the recently-introduced "struct debug_reg".
    Use the latter instead.
    
    This fixes the following error:
    
    cc1: warnings being treated as errors
    arch/powerpc/kvm/booke.c: In function 'kvmppc_vcpu_run':
    arch/powerpc/kvm/booke.c:760:1: error: the frame size of 1424 bytes is larger than 1024 bytes
    make[2]: *** [arch/powerpc/kvm/booke.o] Error 1
    make[1]: *** [arch/powerpc/kvm] Error 2
    make[1]: *** Waiting for unfinished jobs....
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 9ee12610af02..aace90547614 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -35,7 +35,7 @@ extern void giveup_vsx(struct task_struct *);
 extern void enable_kernel_spe(void);
 extern void giveup_spe(struct task_struct *);
 extern void load_up_spe(struct task_struct *);
-extern void switch_booke_debug_regs(struct thread_struct *new_thread);
+extern void switch_booke_debug_regs(struct debug_reg *new_debug);
 
 #ifndef CONFIG_SMP
 extern void discard_lazy_cpu_state(void);

commit 3743c9b8ceb638b6e4b78b42f2262e22aa6359f0
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Thu Jul 4 12:27:44 2013 +0530

    powerpc: export debug registers save function for KVM
    
    KVM need this function when switching from vcpu to user-space
    thread. My subsequent patch will use this function.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Acked-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 2be5618cdec6..9ee12610af02 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -35,6 +35,7 @@ extern void giveup_vsx(struct task_struct *);
 extern void enable_kernel_spe(void);
 extern void giveup_spe(struct task_struct *);
 extern void load_up_spe(struct task_struct *);
+extern void switch_booke_debug_regs(struct thread_struct *new_thread);
 
 #ifndef CONFIG_SMP
 extern void discard_lazy_cpu_state(void);

commit 3f1f4311881b330a7b5429dd101e676df191b159
Merge: 5935ff4343a6 28e61cc466d8
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Aug 27 15:03:30 2013 +1000

    Merge branch 'merge' into next
    
    Merge stuff that already went into Linus via "merge" which
    are pre-reqs for subsequent patches

commit 5f20be4478032eeba66abddc79eae3abea45c5c8
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sun Jul 14 17:02:06 2013 +0800

    powerpc: Remove the empty giveup_fpu() function on 32bit kernel
    
    Instead of implementing an empty giveup_fpu() function for each
    32bit processor type, replace them with an unique empty inline
    function.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 39cf0f6cc373..2c7edde8f1bb 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -16,7 +16,6 @@ struct thread_struct;
 extern struct task_struct *_switch(struct thread_struct *prev,
 				   struct thread_struct *next);
 
-extern void giveup_fpu(struct task_struct *);
 extern void load_up_fpu(void);
 extern void enable_kernel_fp(void);
 extern void enable_kernel_altivec(void);
@@ -38,8 +37,10 @@ static inline void discard_lazy_cpu_state(void)
 
 #ifdef CONFIG_PPC_FPU
 extern void flush_fp_to_thread(struct task_struct *);
+extern void giveup_fpu(struct task_struct *);
 #else
 static inline void flush_fp_to_thread(struct task_struct *t) { }
+static inline void giveup_fpu(struct task_struct *t) { }
 #endif
 
 #ifdef CONFIG_ALTIVEC

commit 037f0eed57c3f35367ac32275e45f24e297549e9
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sun Jul 14 17:02:05 2013 +0800

    powerpc: Make flush_fp_to_thread() nop when CONFIG_PPC_FPU is disabled
    
    In the current kernel, the function flush_fp_to_thread() is not
    dependent on CONFIG_PPC_FPU. So most invocations of this function
    is not wrapped by CONFIG_PPC_FPU. Even through we don't really
    save the FPRs to the thread struct if CONFIG_PPC_FPU is not enabled,
    but there does have some runtime overhead such as the check for
    tsk->thread.regs and preempt disable and enable. It really make
    no sense to do that. So make it a nop when CONFIG_PPC_FPU is
    disabled. Also remove the wrapped #ifdef CONFIG_PPC_FPU
    when invoking this function.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 33e1cd823585..39cf0f6cc373 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -19,7 +19,6 @@ extern struct task_struct *_switch(struct thread_struct *prev,
 extern void giveup_fpu(struct task_struct *);
 extern void load_up_fpu(void);
 extern void enable_kernel_fp(void);
-extern void flush_fp_to_thread(struct task_struct *);
 extern void enable_kernel_altivec(void);
 extern void load_up_altivec(struct task_struct *);
 extern int emulate_altivec(struct pt_regs *);
@@ -37,6 +36,12 @@ static inline void discard_lazy_cpu_state(void)
 }
 #endif
 
+#ifdef CONFIG_PPC_FPU
+extern void flush_fp_to_thread(struct task_struct *);
+#else
+static inline void flush_fp_to_thread(struct task_struct *t) { }
+#endif
+
 #ifdef CONFIG_ALTIVEC
 extern void flush_altivec_to_thread(struct task_struct *);
 extern void giveup_altivec(struct task_struct *);

commit 6ef94ff2e8e593de6de8d8d299a5fffaed35329c
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sun Jul 14 17:02:03 2013 +0800

    powerpc: remove the unused function disable_kernel_fp()
    
    The only using of function disable_kernel_fp() was already dropped
    in the commit 5daf9071 (powerpc: merge align.c).
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 49a13e0ef234..33e1cd823585 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -18,7 +18,6 @@ extern struct task_struct *_switch(struct thread_struct *prev,
 
 extern void giveup_fpu(struct task_struct *);
 extern void load_up_fpu(void);
-extern void disable_kernel_fp(void);
 extern void enable_kernel_fp(void);
 extern void flush_fp_to_thread(struct task_struct *);
 extern void enable_kernel_altivec(void);

commit c2d52644e2da8a07ecab5ca62dd0bc563089e8dc
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Aug 9 17:29:30 2013 +1000

    powerpc: Save the TAR register earlier
    
    This moves us to save the Target Address Register (TAR) a earlier in
    __switch_to.  It introduces a new function save_tar() to do this.
    
    We need to save the TAR earlier as we will overwrite it in the transactional
    memory reclaim/recheckpoint path.  We are going to do this in a subsequent
    patch which will fix saving the TAR register when it's modified inside a
    transaction.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: <stable@vger.kernel.org> [v3.10]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 49a13e0ef234..294c2cedcf7a 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -15,6 +15,15 @@ extern struct task_struct *__switch_to(struct task_struct *,
 struct thread_struct;
 extern struct task_struct *_switch(struct thread_struct *prev,
 				   struct thread_struct *next);
+#ifdef CONFIG_PPC_BOOK3S_64
+static inline void save_tar(struct thread_struct *prev)
+{
+	if (cpu_has_feature(CPU_FTR_ARCH_207S))
+		prev->tar = mfspr(SPRN_TAR);
+}
+#else
+static inline void save_tar(struct thread_struct *prev) {}
+#endif
 
 extern void giveup_fpu(struct task_struct *);
 extern void load_up_fpu(void);

commit 330a1eb7775ba876dbd46b9885556e57f705e3d4
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Jun 28 18:15:16 2013 +1000

    powerpc/perf: Core EBB support for 64-bit book3s
    
    Add support for EBB (Event Based Branches) on 64-bit book3s. See the
    included documentation for more details.
    
    EBBs are a feature which allows the hardware to branch directly to a
    specified user space address when a PMU event overflows. This can be
    used by programs for self-monitoring with no kernel involvement in the
    inner loop.
    
    Most of the logic is in the generic book3s code, primarily to avoid a
    proliferation of PMU callbacks.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 200d763a0a67..49a13e0ef234 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -67,4 +67,18 @@ static inline void flush_spe_to_thread(struct task_struct *t)
 }
 #endif
 
+static inline void clear_task_ebb(struct task_struct *t)
+{
+#ifdef CONFIG_PPC_BOOK3S_64
+    /* EBB perf events are not inherited, so clear all EBB state. */
+    t->thread.bescr = 0;
+    t->thread.mmcr2 = 0;
+    t->thread.mmcr0 = 0;
+    t->thread.siar = 0;
+    t->thread.sdar = 0;
+    t->thread.sier = 0;
+    t->thread.used_ebb = 0;
+#endif
+}
+
 #endif /* _ASM_POWERPC_SWITCH_TO_H */

commit 07acfc2a9349a8ce45b236c2624dad452001966b
Merge: b5f4035adfff 322728e55aa7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 24 16:17:30 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM changes from Avi Kivity:
     "Changes include additional instruction emulation, page-crossing MMIO,
      faster dirty logging, preventing the watchdog from killing a stopped
      guest, module autoload, a new MSI ABI, and some minor optimizations
      and fixes.  Outside x86 we have a small s390 and a very large ppc
      update.
    
      Regarding the new (for kvm) rebaseless workflow, some of the patches
      that were merged before we switch trees had to be rebased, while
      others are true pulls.  In either case the signoffs should be correct
      now."
    
    Fix up trivial conflicts in Documentation/feature-removal-schedule.txt
    arch/powerpc/kvm/book3s_segment.S and arch/x86/include/asm/kvm_para.h.
    
    I suspect the kvm_para.h resolution ends up doing the "do I have cpuid"
    check effectively twice (it was done differently in two different
    commits), but better safe than sorry ;)
    
    * 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (125 commits)
      KVM: make asm-generic/kvm_para.h have an ifdef __KERNEL__ block
      KVM: s390: onereg for timer related registers
      KVM: s390: epoch difference and TOD programmable field
      KVM: s390: KVM_GET/SET_ONEREG for s390
      KVM: s390: add capability indicating COW support
      KVM: Fix mmu_reload() clash with nested vmx event injection
      KVM: MMU: Don't use RCU for lockless shadow walking
      KVM: VMX: Optimize %ds, %es reload
      KVM: VMX: Fix %ds/%es clobber
      KVM: x86 emulator: convert bsf/bsr instructions to emulate_2op_SrcV_nobyte()
      KVM: VMX: unlike vmcs on fail path
      KVM: PPC: Emulator: clean up SPR reads and writes
      KVM: PPC: Emulator: clean up instruction parsing
      kvm/powerpc: Add new ioctl to retreive server MMU infos
      kvm/book3s: Make kernel emulated H_PUT_TCE available for "PR" KVM
      KVM: PPC: bookehv: Fix r8/r13 storing in level exception handler
      KVM: PPC: Book3S: Enable IRQs during exit handling
      KVM: PPC: Fix PR KVM on POWER7 bare metal
      KVM: PPC: Fix stbux emulation
      KVM: PPC: bookehv: Use lwz/stw instead of PPC_LL/PPC_STL for 32-bit fields
      ...

commit 35000870fcfbb28757ad47de77b4645072d916b8
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Apr 15 20:56:45 2012 +0000

    powerpc: Optimise enable_kernel_altivec
    
    Add two optimisations to enable_kernel_altivec:
    
    - enable_kernel_altivec has already determined if we need to
    save the previous task's state but we call giveup_altivec
    in both cases, requiring an extra branch in giveup_altivec. Create
    giveup_altivec_notask which only turns on the VMX bit in the
    MSR.
    
    - We write the VMX MSR bit each time we call enable_kernel_altivec
    even it was already set. Check the bit and branch out if we have
    already set it. The classic case for this is vectored IO
    where we have to copy multiple buffers to or from userspace.
    
    The following testcase was used to confirm this patch improves
    performance:
    
    http://ozlabs.org/~anton/junkcode/copy_to_user.c
    
    Since the current breakpoint for using VMX in copy_tofrom_user is
    4096 bytes, I'm using buffers of 4096 + 1 cacheline (4224) bytes.
    A benchmark of 16 entry readvs (-s 16):
    
    time copy_to_user -l 4224 -s 16 -i 1000000
    
    completes 5.2% faster on a POWER7 PS700.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index 2824609319c7..1a6320290d26 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -40,6 +40,7 @@ static inline void discard_lazy_cpu_state(void)
 #ifdef CONFIG_ALTIVEC
 extern void flush_altivec_to_thread(struct task_struct *);
 extern void giveup_altivec(struct task_struct *);
+extern void giveup_altivec_notask(void);
 #else
 static inline void flush_altivec_to_thread(struct task_struct *t)
 {

commit 8cd3c23df79411f6b24ddb7d2ed58d26e3b06815
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Apr 15 20:54:59 2012 +0000

    powerpc: Remove empty giveup_altivec function on book3e CPUs
    
    Use an empty inline instead of an empty function to implement
    giveup_altivec on book3e CPUs, similar to flush_altivec_to_thread.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index caf82d0a00de..2824609319c7 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -21,7 +21,6 @@ extern void disable_kernel_fp(void);
 extern void enable_kernel_fp(void);
 extern void flush_fp_to_thread(struct task_struct *);
 extern void enable_kernel_altivec(void);
-extern void giveup_altivec(struct task_struct *);
 extern void load_up_altivec(struct task_struct *);
 extern int emulate_altivec(struct pt_regs *);
 extern void __giveup_vsx(struct task_struct *);
@@ -40,10 +39,14 @@ static inline void discard_lazy_cpu_state(void)
 
 #ifdef CONFIG_ALTIVEC
 extern void flush_altivec_to_thread(struct task_struct *);
+extern void giveup_altivec(struct task_struct *);
 #else
 static inline void flush_altivec_to_thread(struct task_struct *t)
 {
 }
+static inline void giveup_altivec(struct task_struct *t)
+{
+}
 #endif
 
 #ifdef CONFIG_VSX

commit 8fae845f4956de0becc115e926d33eff46722e94
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Dec 20 15:34:45 2011 +0000

    KVM: PPC: booke: standard PPC floating point support
    
    e500mc has a normal PPC FPU, rather than SPE which is found
    on e500v1/v2.
    
    Based on code from Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
index caf82d0a00de..1622c356ba90 100644
--- a/arch/powerpc/include/asm/switch_to.h
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -17,6 +17,7 @@ extern struct task_struct *_switch(struct thread_struct *prev,
 				   struct thread_struct *next);
 
 extern void giveup_fpu(struct task_struct *);
+extern void load_up_fpu(void);
 extern void disable_kernel_fp(void);
 extern void enable_kernel_fp(void);
 extern void flush_fp_to_thread(struct task_struct *);

commit ae3a197e3d0bfe3f4bf1693723e82dc018c096f3
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:02 2012 +0100

    Disintegrate asm/system.h for PowerPC
    
    Disintegrate asm/system.h for PowerPC.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    cc: linuxppc-dev@lists.ozlabs.org

diff --git a/arch/powerpc/include/asm/switch_to.h b/arch/powerpc/include/asm/switch_to.h
new file mode 100644
index 000000000000..caf82d0a00de
--- /dev/null
+++ b/arch/powerpc/include/asm/switch_to.h
@@ -0,0 +1,65 @@
+/*
+ * Copyright (C) 1999 Cort Dougan <cort@cs.nmt.edu>
+ */
+#ifndef _ASM_POWERPC_SWITCH_TO_H
+#define _ASM_POWERPC_SWITCH_TO_H
+
+struct thread_struct;
+struct task_struct;
+struct pt_regs;
+
+extern struct task_struct *__switch_to(struct task_struct *,
+	struct task_struct *);
+#define switch_to(prev, next, last)	((last) = __switch_to((prev), (next)))
+
+struct thread_struct;
+extern struct task_struct *_switch(struct thread_struct *prev,
+				   struct thread_struct *next);
+
+extern void giveup_fpu(struct task_struct *);
+extern void disable_kernel_fp(void);
+extern void enable_kernel_fp(void);
+extern void flush_fp_to_thread(struct task_struct *);
+extern void enable_kernel_altivec(void);
+extern void giveup_altivec(struct task_struct *);
+extern void load_up_altivec(struct task_struct *);
+extern int emulate_altivec(struct pt_regs *);
+extern void __giveup_vsx(struct task_struct *);
+extern void giveup_vsx(struct task_struct *);
+extern void enable_kernel_spe(void);
+extern void giveup_spe(struct task_struct *);
+extern void load_up_spe(struct task_struct *);
+
+#ifndef CONFIG_SMP
+extern void discard_lazy_cpu_state(void);
+#else
+static inline void discard_lazy_cpu_state(void)
+{
+}
+#endif
+
+#ifdef CONFIG_ALTIVEC
+extern void flush_altivec_to_thread(struct task_struct *);
+#else
+static inline void flush_altivec_to_thread(struct task_struct *t)
+{
+}
+#endif
+
+#ifdef CONFIG_VSX
+extern void flush_vsx_to_thread(struct task_struct *);
+#else
+static inline void flush_vsx_to_thread(struct task_struct *t)
+{
+}
+#endif
+
+#ifdef CONFIG_SPE
+extern void flush_spe_to_thread(struct task_struct *);
+#else
+static inline void flush_spe_to_thread(struct task_struct *t)
+{
+}
+#endif
+
+#endif /* _ASM_POWERPC_SWITCH_TO_H */
