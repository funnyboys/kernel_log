commit c5f58bd58f432be5d92df33c5458e0bcbee3aadf
Author: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date:   Mon Jan 29 15:20:13 2018 -0500

    membarrier: Provide GLOBAL_EXPEDITED command
    
    Allow expedited membarrier to be used for data shared between processes
    through shared memory.
    
    Processes wishing to receive the membarriers register with
    MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED. Those which want to issue
    membarrier invoke MEMBARRIER_CMD_GLOBAL_EXPEDITED.
    
    This allows extremely simple kernel-level implementation: we have almost
    everything we need with the PRIVATE_EXPEDITED barrier code. All we need
    to do is to add a flag in the mm_struct that will be used to check
    whether we need to send the IPI to the current thread of each CPU.
    
    There is a slight downside to this approach compared to targeting
    specific shared memory users: when performing a membarrier operation,
    all registered "global" receivers will get the barrier, even if they
    don't share a memory mapping with the sender issuing
    MEMBARRIER_CMD_GLOBAL_EXPEDITED.
    
    This registration approach seems to fit the requirement of not
    disturbing processes that really deeply care about real-time: they
    simply should not register with MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED.
    
    In order to align the membarrier command names, the "MEMBARRIER_CMD_SHARED"
    command is renamed to "MEMBARRIER_CMD_GLOBAL", keeping an alias of
    MEMBARRIER_CMD_SHARED to MEMBARRIER_CMD_GLOBAL for UAPI header backward
    compatibility.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrea Parri <parri.andrea@gmail.com>
    Cc: Andrew Hunter <ahh@google.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Avi Kivity <avi@scylladb.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Dave Watson <davejwatson@fb.com>
    Cc: David Sehr <sehr@google.com>
    Cc: Greg Hackmann <ghackmann@google.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Maged Michael <maged.michael@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-api@vger.kernel.org
    Link: http://lkml.kernel.org/r/20180129202020.8515-5-mathieu.desnoyers@efficios.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/include/asm/membarrier.h b/arch/powerpc/include/asm/membarrier.h
index 98ff4f1fcf2b..6e20bb5c74ea 100644
--- a/arch/powerpc/include/asm/membarrier.h
+++ b/arch/powerpc/include/asm/membarrier.h
@@ -13,7 +13,8 @@ static inline void membarrier_arch_switch_mm(struct mm_struct *prev,
 	 * store to rq->curr.
 	 */
 	if (likely(!(atomic_read(&next->membarrier_state) &
-		     MEMBARRIER_STATE_PRIVATE_EXPEDITED) || !prev))
+		     (MEMBARRIER_STATE_PRIVATE_EXPEDITED |
+		      MEMBARRIER_STATE_GLOBAL_EXPEDITED)) || !prev))
 		return;
 
 	/*

commit 3ccfebedd8cf54e291c809c838d8ad5cc00f5688
Author: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date:   Mon Jan 29 15:20:11 2018 -0500

    powerpc, membarrier: Skip memory barrier in switch_mm()
    
    Allow PowerPC to skip the full memory barrier in switch_mm(), and
    only issue the barrier when scheduling into a task belonging to a
    process that has registered to use expedited private.
    
    Threads targeting the same VM but which belong to different thread
    groups is a tricky case. It has a few consequences:
    
    It turns out that we cannot rely on get_nr_threads(p) to count the
    number of threads using a VM. We can use
    (atomic_read(&mm->mm_users) == 1 && get_nr_threads(p) == 1)
    instead to skip the synchronize_sched() for cases where the VM only has
    a single user, and that user only has a single thread.
    
    It also turns out that we cannot use for_each_thread() to set
    thread flags in all threads using a VM, as it only iterates on the
    thread group.
    
    Therefore, test the membarrier state variable directly rather than
    relying on thread flags. This means
    membarrier_register_private_expedited() needs to set the
    MEMBARRIER_STATE_PRIVATE_EXPEDITED flag, issue synchronize_sched(), and
    only then set MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY which allows
    private expedited membarrier commands to succeed.
    membarrier_arch_switch_mm() now tests for the
    MEMBARRIER_STATE_PRIVATE_EXPEDITED flag.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Andrea Parri <parri.andrea@gmail.com>
    Cc: Andrew Hunter <ahh@google.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Avi Kivity <avi@scylladb.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Dave Watson <davejwatson@fb.com>
    Cc: David Sehr <sehr@google.com>
    Cc: Greg Hackmann <ghackmann@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Maged Michael <maged.michael@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-api@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/20180129202020.8515-3-mathieu.desnoyers@efficios.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/include/asm/membarrier.h b/arch/powerpc/include/asm/membarrier.h
new file mode 100644
index 000000000000..98ff4f1fcf2b
--- /dev/null
+++ b/arch/powerpc/include/asm/membarrier.h
@@ -0,0 +1,26 @@
+#ifndef _ASM_POWERPC_MEMBARRIER_H
+#define _ASM_POWERPC_MEMBARRIER_H
+
+static inline void membarrier_arch_switch_mm(struct mm_struct *prev,
+					     struct mm_struct *next,
+					     struct task_struct *tsk)
+{
+	/*
+	 * Only need the full barrier when switching between processes.
+	 * Barrier when switching from kernel to userspace is not
+	 * required here, given that it is implied by mmdrop(). Barrier
+	 * when switching from userspace to kernel is not needed after
+	 * store to rq->curr.
+	 */
+	if (likely(!(atomic_read(&next->membarrier_state) &
+		     MEMBARRIER_STATE_PRIVATE_EXPEDITED) || !prev))
+		return;
+
+	/*
+	 * The membarrier system call requires a full memory barrier
+	 * after storing to rq->curr, before going back to user-space.
+	 */
+	smp_mb();
+}
+
+#endif /* _ASM_POWERPC_MEMBARRIER_H */
