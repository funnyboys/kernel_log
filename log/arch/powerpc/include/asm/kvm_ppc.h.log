commit 8c99d34578628b50233210dae5fc9600eea20b8e
Author: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
Date:   Mon Apr 27 12:35:11 2020 +0800

    KVM: PPC: Clean up redundant 'kvm_run' parameters
    
    In the current kvm version, 'kvm_run' has been included in the 'kvm_vcpu'
    structure. For historical reasons, many kvm-related function parameters
    retain the 'kvm_run' and 'kvm_vcpu' parameters at the same time. This
    patch does a unified cleanup of these remaining redundant parameters.
    
    Signed-off-by: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 94f5a32acaf1..ccf66b3a4c1d 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -58,28 +58,28 @@ enum xlate_readwrite {
 	XLATE_WRITE		/* check for write permissions */
 };
 
-extern int kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
-extern int __kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
+extern int kvmppc_vcpu_run(struct kvm_vcpu *vcpu);
+extern int __kvmppc_vcpu_run(struct kvm_run *run, struct kvm_vcpu *vcpu);
 extern void kvmppc_handler_highmem(void);
 
 extern void kvmppc_dump_vcpu(struct kvm_vcpu *vcpu);
-extern int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+extern int kvmppc_handle_load(struct kvm_vcpu *vcpu,
                               unsigned int rt, unsigned int bytes,
 			      int is_default_endian);
-extern int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
+extern int kvmppc_handle_loads(struct kvm_vcpu *vcpu,
                                unsigned int rt, unsigned int bytes,
 			       int is_default_endian);
-extern int kvmppc_handle_vsx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+extern int kvmppc_handle_vsx_load(struct kvm_vcpu *vcpu,
 				unsigned int rt, unsigned int bytes,
 			int is_default_endian, int mmio_sign_extend);
-extern int kvmppc_handle_vmx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+extern int kvmppc_handle_vmx_load(struct kvm_vcpu *vcpu,
 		unsigned int rt, unsigned int bytes, int is_default_endian);
-extern int kvmppc_handle_vmx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+extern int kvmppc_handle_vmx_store(struct kvm_vcpu *vcpu,
 		unsigned int rs, unsigned int bytes, int is_default_endian);
-extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+extern int kvmppc_handle_store(struct kvm_vcpu *vcpu,
 			       u64 val, unsigned int bytes,
 			       int is_default_endian);
-extern int kvmppc_handle_vsx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+extern int kvmppc_handle_vsx_store(struct kvm_vcpu *vcpu,
 				int rs, unsigned int bytes,
 				int is_default_endian);
 
@@ -90,10 +90,9 @@ extern int kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 		     bool data);
 extern int kvmppc_st(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 		     bool data);
-extern int kvmppc_emulate_instruction(struct kvm_run *run,
-                                      struct kvm_vcpu *vcpu);
+extern int kvmppc_emulate_instruction(struct kvm_vcpu *vcpu);
 extern int kvmppc_emulate_loadstore(struct kvm_vcpu *vcpu);
-extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
+extern int kvmppc_emulate_mmio(struct kvm_vcpu *vcpu);
 extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
 extern u32 kvmppc_get_dec(struct kvm_vcpu *vcpu, u64 tb);
 extern void kvmppc_decrementer_func(struct kvm_vcpu *vcpu);
@@ -267,7 +266,7 @@ struct kvmppc_ops {
 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
 	void (*inject_interrupt)(struct kvm_vcpu *vcpu, int vec, u64 srr1_flags);
 	void (*set_msr)(struct kvm_vcpu *vcpu, u64 msr);
-	int (*vcpu_run)(struct kvm_run *run, struct kvm_vcpu *vcpu);
+	int (*vcpu_run)(struct kvm_vcpu *vcpu);
 	int (*vcpu_create)(struct kvm_vcpu *vcpu);
 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
 	int (*check_requests)(struct kvm_vcpu *vcpu);
@@ -291,7 +290,7 @@ struct kvmppc_ops {
 	int (*init_vm)(struct kvm *kvm);
 	void (*destroy_vm)(struct kvm *kvm);
 	int (*get_smmu_info)(struct kvm *kvm, struct kvm_ppc_smmu_info *info);
-	int (*emulate_op)(struct kvm_run *run, struct kvm_vcpu *vcpu,
+	int (*emulate_op)(struct kvm_vcpu *vcpu,
 			  unsigned int inst, int *advance);
 	int (*emulate_mtspr)(struct kvm_vcpu *vcpu, int sprn, ulong spr_val);
 	int (*emulate_mfspr)(struct kvm_vcpu *vcpu, int sprn, ulong *spr_val);

commit 9a5788c615f52f6d7bf0b61986a632d4ec86791d
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Mar 19 15:29:55 2020 +1100

    KVM: PPC: Book3S HV: Add a capability for enabling secure guests
    
    At present, on Power systems with Protected Execution Facility
    hardware and an ultravisor, a KVM guest can transition to being a
    secure guest at will.  Userspace (QEMU) has no way of knowing
    whether a host system is capable of running secure guests.  This
    will present a problem in future when the ultravisor is capable of
    migrating secure guests from one host to another, because
    virtualization management software will have no way to ensure that
    secure guests only run in domains where all of the hosts can
    support secure guests.
    
    This adds a VM capability which has two functions: (a) userspace
    can query it to find out whether the host can support secure guests,
    and (b) userspace can enable it for a guest, which allows that
    guest to become a secure guest.  If userspace does not enable it,
    KVM will return an error when the ultravisor does the hypercall
    that indicates that the guest is starting to transition to a
    secure guest.  The ultravisor will then abort the transition and
    the guest will terminate.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Ram Pai <linuxram@us.ibm.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index e716862d56b9..94f5a32acaf1 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -313,6 +313,7 @@ struct kvmppc_ops {
 			       int size);
 	int (*store_to_eaddr)(struct kvm_vcpu *vcpu, ulong *eaddr, void *ptr,
 			      int size);
+	int (*enable_svm)(struct kvm *kvm);
 	int (*svm_off)(struct kvm *kvm);
 };
 

commit 6fef0c6bbe4987fc94c14f52782b224ddaf3530b
Author: Greg Kurz <groug@kaod.org>
Date:   Wed Mar 18 18:43:42 2020 +0100

    KVM: PPC: Kill kvmppc_ops::mmu_destroy() and kvmppc_mmu_destroy()
    
    These are only used by HV KVM and BookE, and in both cases they are
    nops.
    
    Signed-off-by: Greg Kurz <groug@kaod.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 0b80e3420fef..e716862d56b9 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -107,7 +107,6 @@ extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gpa_t gpaddr,
                            unsigned int gtlb_idx);
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
-extern void kvmppc_mmu_destroy(struct kvm_vcpu *vcpu);
 extern int kvmppc_mmu_dtlb_index(struct kvm_vcpu *vcpu, gva_t eaddr);
 extern int kvmppc_mmu_itlb_index(struct kvm_vcpu *vcpu, gva_t eaddr);
 extern gpa_t kvmppc_mmu_xlate(struct kvm_vcpu *vcpu, unsigned int gtlb_index,
@@ -288,7 +287,6 @@ struct kvmppc_ops {
 	int (*age_hva)(struct kvm *kvm, unsigned long start, unsigned long end);
 	int (*test_age_hva)(struct kvm *kvm, unsigned long hva);
 	void (*set_spte_hva)(struct kvm *kvm, unsigned long hva, pte_t pte);
-	void (*mmu_destroy)(struct kvm_vcpu *vcpu);
 	void (*free_memslot)(struct kvm_memory_slot *slot);
 	int (*init_vm)(struct kvm *kvm);
 	void (*destroy_vm)(struct kvm *kvm);

commit 3f1268dda8e47f808f4f50f24715b84d4b228bf3
Author: Greg Kurz <groug@kaod.org>
Date:   Wed Mar 18 18:43:36 2020 +0100

    KVM: PPC: Book3S PR: Move kvmppc_mmu_init() into PR KVM
    
    This is only relevant to PR KVM. Make it obvious by moving the
    function declaration to the Book3s header and rename it with
    a _pr suffix.
    
    Signed-off-by: Greg Kurz <groug@kaod.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 406ec46304d5..0b80e3420fef 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -108,7 +108,6 @@ extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gpa_t gpaddr,
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 extern void kvmppc_mmu_destroy(struct kvm_vcpu *vcpu);
-extern int kvmppc_mmu_init(struct kvm_vcpu *vcpu);
 extern int kvmppc_mmu_dtlb_index(struct kvm_vcpu *vcpu, gva_t eaddr);
 extern int kvmppc_mmu_itlb_index(struct kvm_vcpu *vcpu, gva_t eaddr);
 extern gpa_t kvmppc_mmu_xlate(struct kvm_vcpu *vcpu, unsigned int gtlb_index,

commit e96c81ee89d80e1a0fe50a0e9be40c1b77e14aaa
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:27 2020 -0800

    KVM: Simplify kvm_free_memslot() and all its descendents
    
    Now that all callers of kvm_free_memslot() pass NULL for @dont, remove
    the param from the top-level routine and all arch's implementations.
    
    No functional change intended.
    
    Tested-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index d162649430ba..406ec46304d5 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -200,8 +200,7 @@ extern void kvm_free_hpt_cma(struct page *page, unsigned long nr_pages);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
 extern void kvmppc_core_destroy_vm(struct kvm *kvm);
 extern void kvmppc_core_free_memslot(struct kvm *kvm,
-				     struct kvm_memory_slot *free,
-				     struct kvm_memory_slot *dont);
+				     struct kvm_memory_slot *slot);
 extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
 				const struct kvm_userspace_memory_region *mem,
@@ -291,8 +290,7 @@ struct kvmppc_ops {
 	int (*test_age_hva)(struct kvm *kvm, unsigned long hva);
 	void (*set_spte_hva)(struct kvm *kvm, unsigned long hva, pte_t pte);
 	void (*mmu_destroy)(struct kvm_vcpu *vcpu);
-	void (*free_memslot)(struct kvm_memory_slot *free,
-			     struct kvm_memory_slot *dont);
+	void (*free_memslot)(struct kvm_memory_slot *slot);
 	int (*init_vm)(struct kvm *kvm);
 	void (*destroy_vm)(struct kvm *kvm);
 	int (*get_smmu_info)(struct kvm *kvm, struct kvm_ppc_smmu_info *info);

commit 82307e676f9d885871f121e4921b12905a53397d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:18 2020 -0800

    KVM: PPC: Move memslot memory allocation into prepare_memory_region()
    
    Allocate the rmap array during kvm_arch_prepare_memory_region() to pave
    the way for removing kvm_arch_create_memslot() altogether.  Moving PPC's
    memory allocation only changes the order of kernel memory allocations
    between PPC and common KVM code.
    
    No functional change intended.
    
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index bc2494e5710a..d162649430ba 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -202,12 +202,10 @@ extern void kvmppc_core_destroy_vm(struct kvm *kvm);
 extern void kvmppc_core_free_memslot(struct kvm *kvm,
 				     struct kvm_memory_slot *free,
 				     struct kvm_memory_slot *dont);
-extern int kvmppc_core_create_memslot(struct kvm *kvm,
-				      struct kvm_memory_slot *slot,
-				      unsigned long npages);
 extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
-				const struct kvm_userspace_memory_region *mem);
+				const struct kvm_userspace_memory_region *mem,
+				enum kvm_mr_change change);
 extern void kvmppc_core_commit_memory_region(struct kvm *kvm,
 				const struct kvm_userspace_memory_region *mem,
 				const struct kvm_memory_slot *old,
@@ -280,7 +278,8 @@ struct kvmppc_ops {
 	void (*flush_memslot)(struct kvm *kvm, struct kvm_memory_slot *memslot);
 	int (*prepare_memory_region)(struct kvm *kvm,
 				     struct kvm_memory_slot *memslot,
-				     const struct kvm_userspace_memory_region *mem);
+				     const struct kvm_userspace_memory_region *mem,
+				     enum kvm_mr_change change);
 	void (*commit_memory_region)(struct kvm *kvm,
 				     const struct kvm_userspace_memory_region *mem,
 				     const struct kvm_memory_slot *old,
@@ -294,8 +293,6 @@ struct kvmppc_ops {
 	void (*mmu_destroy)(struct kvm_vcpu *vcpu);
 	void (*free_memslot)(struct kvm_memory_slot *free,
 			     struct kvm_memory_slot *dont);
-	int (*create_memslot)(struct kvm_memory_slot *slot,
-			      unsigned long npages);
 	int (*init_vm)(struct kvm *kvm);
 	void (*destroy_vm)(struct kvm *kvm);
 	int (*get_smmu_info)(struct kvm *kvm, struct kvm_ppc_smmu_info *info);

commit ff030fdf55732266c2d35b1a4a0baaf9ce49e9dd
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:00 2019 -0800

    KVM: PPC: Move kvm_vcpu_init() invocation to common code
    
    Move the kvm_cpu_{un}init() calls to common PPC code as an intermediate
    step towards removing kvm_cpu_{un}init() altogether.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 8f77ca5ace6f..bc2494e5710a 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -119,8 +119,7 @@ extern int kvmppc_xlate(struct kvm_vcpu *vcpu, ulong eaddr,
 			enum xlate_instdata xlid, enum xlate_readwrite xlrw,
 			struct kvmppc_pte *pte);
 
-extern int kvmppc_core_vcpu_create(struct kvm *kvm, struct kvm_vcpu *vcpu,
-				   unsigned int id);
+extern int kvmppc_core_vcpu_create(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_vcpu_free(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_vcpu_setup(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_check_processor_compat(void);
@@ -274,8 +273,7 @@ struct kvmppc_ops {
 	void (*inject_interrupt)(struct kvm_vcpu *vcpu, int vec, u64 srr1_flags);
 	void (*set_msr)(struct kvm_vcpu *vcpu, u64 msr);
 	int (*vcpu_run)(struct kvm_run *run, struct kvm_vcpu *vcpu);
-	int (*vcpu_create)(struct kvm *kvm, struct kvm_vcpu *vcpu,
-			   unsigned int id);
+	int (*vcpu_create)(struct kvm_vcpu *vcpu);
 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
 	int (*check_requests)(struct kvm_vcpu *vcpu);
 	int (*get_dirty_log)(struct kvm *kvm, struct kvm_dirty_log *log);

commit c50bfbdc38ec56cf8e53afb4f9ebb600dfcabd49
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:54:57 2019 -0800

    KVM: PPC: Allocate vcpu struct in common PPC code
    
    Move allocation of all flavors of PPC vCPUs to common PPC code.  All
    variants either allocate 'struct kvm_vcpu' directly, or require that
    the embedded 'struct kvm_vcpu' member be located at offset 0, i.e.
    guarantee that the allocation can be directly interpreted as a 'struct
    kvm_vcpu' object.
    
    Remove the message from the build-time assertion regarding placement of
    the struct, as compatibility with the arch usercopy region is no longer
    the sole dependent on 'struct kvm_vcpu' being at offset zero.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 3d2f871241a8..8f77ca5ace6f 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -119,8 +119,8 @@ extern int kvmppc_xlate(struct kvm_vcpu *vcpu, ulong eaddr,
 			enum xlate_instdata xlid, enum xlate_readwrite xlrw,
 			struct kvmppc_pte *pte);
 
-extern struct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm,
-                                                unsigned int id);
+extern int kvmppc_core_vcpu_create(struct kvm *kvm, struct kvm_vcpu *vcpu,
+				   unsigned int id);
 extern void kvmppc_core_vcpu_free(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_vcpu_setup(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_check_processor_compat(void);
@@ -274,7 +274,8 @@ struct kvmppc_ops {
 	void (*inject_interrupt)(struct kvm_vcpu *vcpu, int vec, u64 srr1_flags);
 	void (*set_msr)(struct kvm_vcpu *vcpu, u64 msr);
 	int (*vcpu_run)(struct kvm_run *run, struct kvm_vcpu *vcpu);
-	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned int id);
+	int (*vcpu_create)(struct kvm *kvm, struct kvm_vcpu *vcpu,
+			   unsigned int id);
 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
 	int (*check_requests)(struct kvm_vcpu *vcpu);
 	int (*get_dirty_log)(struct kvm *kvm, struct kvm_dirty_log *log);

commit 22945688acd4d0ec2620b0670a53110401ed9c59
Author: Bharata B Rao <bharata@linux.ibm.com>
Date:   Mon Nov 25 08:36:30 2019 +0530

    KVM: PPC: Book3S HV: Support reset of secure guest
    
    Add support for reset of secure guest via a new ioctl KVM_PPC_SVM_OFF.
    This ioctl will be issued by QEMU during reset and includes the
    the following steps:
    
    - Release all device pages of the secure guest.
    - Ask UV to terminate the guest via UV_SVM_TERMINATE ucall
    - Unpin the VPA pages so that they can be migrated back to secure
      side when guest becomes secure again. This is required because
      pinned pages can't be migrated.
    - Reinit the partition scoped page tables
    
    After these steps, guest is ready to issue UV_ESM call once again
    to switch to secure mode.
    
    Signed-off-by: Bharata B Rao <bharata@linux.ibm.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
            [Implementation of uv_svm_terminate() and its call from
            guest shutdown path]
    Signed-off-by: Ram Pai <linuxram@us.ibm.com>
            [Unpinning of VPA pages]
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index d63f649fe713..3d2f871241a8 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -322,6 +322,7 @@ struct kvmppc_ops {
 			       int size);
 	int (*store_to_eaddr)(struct kvm_vcpu *vcpu, ulong *eaddr, void *ptr,
 			      int size);
+	int (*svm_off)(struct kvm *kvm);
 };
 
 extern struct kvmppc_ops *kvmppc_hv_ops;

commit 87a45e07a5abfec4d6b0e8356718f8919d0a3c20
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Oct 2 16:00:22 2019 +1000

    KVM: PPC: Book3S: Replace reset_msr mmu op with inject_interrupt arch op
    
    reset_msr sets the MSR for interrupt injection, but it's cleaner and
    more flexible to provide a single op to set both MSR and PC for the
    interrupt.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index ee62776e5433..d63f649fe713 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -271,6 +271,7 @@ struct kvmppc_ops {
 			   union kvmppc_one_reg *val);
 	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);
 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
+	void (*inject_interrupt)(struct kvm_vcpu *vcpu, int vec, u64 srr1_flags);
 	void (*set_msr)(struct kvm_vcpu *vcpu, u64 msr);
 	int (*vcpu_run)(struct kvm_run *run, struct kvm_vcpu *vcpu);
 	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned int id);

commit 3a83f677a6eeff65751b29e3648d7c69c3be83f3
Author: Michael Roth <mdroth@linux.vnet.ibm.com>
Date:   Wed Sep 11 17:31:55 2019 -0500

    KVM: PPC: Book3S HV: use smp_mb() when setting/clearing host_ipi flag
    
    On a 2-socket Power9 system with 32 cores/128 threads (SMT4) and 1TB
    of memory running the following guest configs:
    
      guest A:
        - 224GB of memory
        - 56 VCPUs (sockets=1,cores=28,threads=2), where:
          VCPUs 0-1 are pinned to CPUs 0-3,
          VCPUs 2-3 are pinned to CPUs 4-7,
          ...
          VCPUs 54-55 are pinned to CPUs 108-111
    
      guest B:
        - 4GB of memory
        - 4 VCPUs (sockets=1,cores=4,threads=1)
    
    with the following workloads (with KSM and THP enabled in all):
    
      guest A:
        stress --cpu 40 --io 20 --vm 20 --vm-bytes 512M
    
      guest B:
        stress --cpu 4 --io 4 --vm 4 --vm-bytes 512M
    
      host:
        stress --cpu 4 --io 4 --vm 2 --vm-bytes 256M
    
    the below soft-lockup traces were observed after an hour or so and
    persisted until the host was reset (this was found to be reliably
    reproducible for this configuration, for kernels 4.15, 4.18, 5.0,
    and 5.3-rc5):
    
      [ 1253.183290] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1253.183319] rcu:     124-....: (5250 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=1941
      [ 1256.287426] watchdog: BUG: soft lockup - CPU#105 stuck for 23s! [CPU 52/KVM:19709]
      [ 1264.075773] watchdog: BUG: soft lockup - CPU#24 stuck for 23s! [worker:19913]
      [ 1264.079769] watchdog: BUG: soft lockup - CPU#31 stuck for 23s! [worker:20331]
      [ 1264.095770] watchdog: BUG: soft lockup - CPU#45 stuck for 23s! [worker:20338]
      [ 1264.131773] watchdog: BUG: soft lockup - CPU#64 stuck for 23s! [avocado:19525]
      [ 1280.408480] watchdog: BUG: soft lockup - CPU#124 stuck for 22s! [ksmd:791]
      [ 1316.198012] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1316.198032] rcu:     124-....: (21003 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=8243
      [ 1340.411024] watchdog: BUG: soft lockup - CPU#124 stuck for 22s! [ksmd:791]
      [ 1379.212609] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1379.212629] rcu:     124-....: (36756 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=14714
      [ 1404.413615] watchdog: BUG: soft lockup - CPU#124 stuck for 22s! [ksmd:791]
      [ 1442.227095] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1442.227115] rcu:     124-....: (52509 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=21403
      [ 1455.111787] INFO: task worker:19907 blocked for more than 120 seconds.
      [ 1455.111822]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.111833] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.111884] INFO: task worker:19908 blocked for more than 120 seconds.
      [ 1455.111905]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.111925] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.111966] INFO: task worker:20328 blocked for more than 120 seconds.
      [ 1455.111986]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.111998] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112048] INFO: task worker:20330 blocked for more than 120 seconds.
      [ 1455.112068]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112097] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112138] INFO: task worker:20332 blocked for more than 120 seconds.
      [ 1455.112159]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112179] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112210] INFO: task worker:20333 blocked for more than 120 seconds.
      [ 1455.112231]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112242] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112282] INFO: task worker:20335 blocked for more than 120 seconds.
      [ 1455.112303]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112332] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112372] INFO: task worker:20336 blocked for more than 120 seconds.
      [ 1455.112392]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
    
    CPUs 45, 24, and 124 are stuck on spin locks, likely held by
    CPUs 105 and 31.
    
    CPUs 105 and 31 are stuck in smp_call_function_many(), waiting on
    target CPU 42. For instance:
    
      # CPU 105 registers (via xmon)
      R00 = c00000000020b20c   R16 = 00007d1bcd800000
      R01 = c00000363eaa7970   R17 = 0000000000000001
      R02 = c0000000019b3a00   R18 = 000000000000006b
      R03 = 000000000000002a   R19 = 00007d537d7aecf0
      R04 = 000000000000002a   R20 = 60000000000000e0
      R05 = 000000000000002a   R21 = 0801000000000080
      R06 = c0002073fb0caa08   R22 = 0000000000000d60
      R07 = c0000000019ddd78   R23 = 0000000000000001
      R08 = 000000000000002a   R24 = c00000000147a700
      R09 = 0000000000000001   R25 = c0002073fb0ca908
      R10 = c000008ffeb4e660   R26 = 0000000000000000
      R11 = c0002073fb0ca900   R27 = c0000000019e2464
      R12 = c000000000050790   R28 = c0000000000812b0
      R13 = c000207fff623e00   R29 = c0002073fb0ca808
      R14 = 00007d1bbee00000   R30 = c0002073fb0ca800
      R15 = 00007d1bcd600000   R31 = 0000000000000800
      pc  = c00000000020b260 smp_call_function_many+0x3d0/0x460
      cfar= c00000000020b270 smp_call_function_many+0x3e0/0x460
      lr  = c00000000020b20c smp_call_function_many+0x37c/0x460
      msr = 900000010288b033   cr  = 44024824
      ctr = c000000000050790   xer = 0000000000000000   trap =  100
    
    CPU 42 is running normally, doing VCPU work:
    
      # CPU 42 stack trace (via xmon)
      [link register   ] c00800001be17188 kvmppc_book3s_radix_page_fault+0x90/0x2b0 [kvm_hv]
      [c000008ed3343820] c000008ed3343850 (unreliable)
      [c000008ed33438d0] c00800001be11b6c kvmppc_book3s_hv_page_fault+0x264/0xe30 [kvm_hv]
      [c000008ed33439d0] c00800001be0d7b4 kvmppc_vcpu_run_hv+0x8dc/0xb50 [kvm_hv]
      [c000008ed3343ae0] c00800001c10891c kvmppc_vcpu_run+0x34/0x48 [kvm]
      [c000008ed3343b00] c00800001c10475c kvm_arch_vcpu_ioctl_run+0x244/0x420 [kvm]
      [c000008ed3343b90] c00800001c0f5a78 kvm_vcpu_ioctl+0x470/0x7c8 [kvm]
      [c000008ed3343d00] c000000000475450 do_vfs_ioctl+0xe0/0xc70
      [c000008ed3343db0] c0000000004760e4 ksys_ioctl+0x104/0x120
      [c000008ed3343e00] c000000000476128 sys_ioctl+0x28/0x80
      [c000008ed3343e20] c00000000000b388 system_call+0x5c/0x70
      --- Exception: c00 (System Call) at 00007d545cfd7694
      SP (7d53ff7edf50) is in userspace
    
    It was subsequently found that ipi_message[PPC_MSG_CALL_FUNCTION]
    was set for CPU 42 by at least 1 of the CPUs waiting in
    smp_call_function_many(), but somehow the corresponding
    call_single_queue entries were never processed by CPU 42, causing the
    callers to spin in csd_lock_wait() indefinitely.
    
    Nick Piggin suggested something similar to the following sequence as
    a possible explanation (interleaving of CALL_FUNCTION/RESCHEDULE
    IPI messages seems to be most common, but any mix of CALL_FUNCTION and
    !CALL_FUNCTION messages could trigger it):
    
        CPU
          X: smp_muxed_ipi_set_message():
          X:   smp_mb()
          X:   message[RESCHEDULE] = 1
          X: doorbell_global_ipi(42):
          X:   kvmppc_set_host_ipi(42, 1)
          X:   ppc_msgsnd_sync()/smp_mb()
          X:   ppc_msgsnd() -> 42
         42: doorbell_exception(): // from CPU X
         42:   ppc_msgsync()
        105: smp_muxed_ipi_set_message():
        105:   smb_mb()
             // STORE DEFERRED DUE TO RE-ORDERING
      --105:   message[CALL_FUNCTION] = 1
      | 105: doorbell_global_ipi(42):
      | 105:   kvmppc_set_host_ipi(42, 1)
      |  42:   kvmppc_set_host_ipi(42, 0)
      |  42: smp_ipi_demux_relaxed()
      |  42: // returns to executing guest
      |      // RE-ORDERED STORE COMPLETES
      ->105:   message[CALL_FUNCTION] = 1
        105:   ppc_msgsnd_sync()/smp_mb()
        105:   ppc_msgsnd() -> 42
         42: local_paca->kvm_hstate.host_ipi == 0 // IPI ignored
        105: // hangs waiting on 42 to process messages/call_single_queue
    
    This can be prevented with an smp_mb() at the beginning of
    kvmppc_set_host_ipi(), such that stores to message[<type>] (or other
    state indicated by the host_ipi flag) are ordered vs. the store to
    to host_ipi.
    
    However, doing so might still allow for the following scenario (not
    yet observed):
    
        CPU
          X: smp_muxed_ipi_set_message():
          X:   smp_mb()
          X:   message[RESCHEDULE] = 1
          X: doorbell_global_ipi(42):
          X:   kvmppc_set_host_ipi(42, 1)
          X:   ppc_msgsnd_sync()/smp_mb()
          X:   ppc_msgsnd() -> 42
         42: doorbell_exception(): // from CPU X
         42:   ppc_msgsync()
             // STORE DEFERRED DUE TO RE-ORDERING
      -- 42:   kvmppc_set_host_ipi(42, 0)
      |  42: smp_ipi_demux_relaxed()
      | 105: smp_muxed_ipi_set_message():
      | 105:   smb_mb()
      | 105:   message[CALL_FUNCTION] = 1
      | 105: doorbell_global_ipi(42):
      | 105:   kvmppc_set_host_ipi(42, 1)
      |      // RE-ORDERED STORE COMPLETES
      -> 42:   kvmppc_set_host_ipi(42, 0)
         42: // returns to executing guest
        105:   ppc_msgsnd_sync()/smp_mb()
        105:   ppc_msgsnd() -> 42
         42: local_paca->kvm_hstate.host_ipi == 0 // IPI ignored
        105: // hangs waiting on 42 to process messages/call_single_queue
    
    Fixing this scenario would require an smp_mb() *after* clearing
    host_ipi flag in kvmppc_set_host_ipi() to order the store vs.
    subsequent processing of IPI messages.
    
    To handle both cases, this patch splits kvmppc_set_host_ipi() into
    separate set/clear functions, where we execute smp_mb() prior to
    setting host_ipi flag, and after clearing host_ipi flag. These
    functions pair with each other to synchronize the sender and receiver
    sides.
    
    With that change in place the above workload ran for 20 hours without
    triggering any lock-ups.
    
    Fixes: 755563bc79c7 ("powerpc/powernv: Fixes for hypervisor doorbell handling") # v4.0
    Signed-off-by: Michael Roth <mdroth@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190911223155.16045-1-mdroth@linux.vnet.ibm.com

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 8e8514efb124..ee62776e5433 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -452,9 +452,100 @@ static inline u32 kvmppc_get_xics_latch(void)
 	return xirr;
 }
 
-static inline void kvmppc_set_host_ipi(int cpu, u8 host_ipi)
+/*
+ * To avoid the need to unnecessarily exit fully to the host kernel, an IPI to
+ * a CPU thread that's running/napping inside of a guest is by default regarded
+ * as a request to wake the CPU (if needed) and continue execution within the
+ * guest, potentially to process new state like externally-generated
+ * interrupts or IPIs sent from within the guest itself (e.g. H_PROD/H_IPI).
+ *
+ * To force an exit to the host kernel, kvmppc_set_host_ipi() must be called
+ * prior to issuing the IPI to set the corresponding 'host_ipi' flag in the
+ * target CPU's PACA. To avoid unnecessary exits to the host, this flag should
+ * be immediately cleared via kvmppc_clear_host_ipi() by the IPI handler on
+ * the receiving side prior to processing the IPI work.
+ *
+ * NOTE:
+ *
+ * We currently issue an smp_mb() at the beginning of kvmppc_set_host_ipi().
+ * This is to guard against sequences such as the following:
+ *
+ *      CPU
+ *        X: smp_muxed_ipi_set_message():
+ *        X:   smp_mb()
+ *        X:   message[RESCHEDULE] = 1
+ *        X: doorbell_global_ipi(42):
+ *        X:   kvmppc_set_host_ipi(42)
+ *        X:   ppc_msgsnd_sync()/smp_mb()
+ *        X:   ppc_msgsnd() -> 42
+ *       42: doorbell_exception(): // from CPU X
+ *       42:   ppc_msgsync()
+ *      105: smp_muxed_ipi_set_message():
+ *      105:   smb_mb()
+ *           // STORE DEFERRED DUE TO RE-ORDERING
+ *    --105:   message[CALL_FUNCTION] = 1
+ *    | 105: doorbell_global_ipi(42):
+ *    | 105:   kvmppc_set_host_ipi(42)
+ *    |  42:   kvmppc_clear_host_ipi(42)
+ *    |  42: smp_ipi_demux_relaxed()
+ *    |  42: // returns to executing guest
+ *    |      // RE-ORDERED STORE COMPLETES
+ *    ->105:   message[CALL_FUNCTION] = 1
+ *      105:   ppc_msgsnd_sync()/smp_mb()
+ *      105:   ppc_msgsnd() -> 42
+ *       42: local_paca->kvm_hstate.host_ipi == 0 // IPI ignored
+ *      105: // hangs waiting on 42 to process messages/call_single_queue
+ *
+ * We also issue an smp_mb() at the end of kvmppc_clear_host_ipi(). This is
+ * to guard against sequences such as the following (as well as to create
+ * a read-side pairing with the barrier in kvmppc_set_host_ipi()):
+ *
+ *      CPU
+ *        X: smp_muxed_ipi_set_message():
+ *        X:   smp_mb()
+ *        X:   message[RESCHEDULE] = 1
+ *        X: doorbell_global_ipi(42):
+ *        X:   kvmppc_set_host_ipi(42)
+ *        X:   ppc_msgsnd_sync()/smp_mb()
+ *        X:   ppc_msgsnd() -> 42
+ *       42: doorbell_exception(): // from CPU X
+ *       42:   ppc_msgsync()
+ *           // STORE DEFERRED DUE TO RE-ORDERING
+ *    -- 42:   kvmppc_clear_host_ipi(42)
+ *    |  42: smp_ipi_demux_relaxed()
+ *    | 105: smp_muxed_ipi_set_message():
+ *    | 105:   smb_mb()
+ *    | 105:   message[CALL_FUNCTION] = 1
+ *    | 105: doorbell_global_ipi(42):
+ *    | 105:   kvmppc_set_host_ipi(42)
+ *    |      // RE-ORDERED STORE COMPLETES
+ *    -> 42:   kvmppc_clear_host_ipi(42)
+ *       42: // returns to executing guest
+ *      105:   ppc_msgsnd_sync()/smp_mb()
+ *      105:   ppc_msgsnd() -> 42
+ *       42: local_paca->kvm_hstate.host_ipi == 0 // IPI ignored
+ *      105: // hangs waiting on 42 to process messages/call_single_queue
+ */
+static inline void kvmppc_set_host_ipi(int cpu)
 {
-	paca_ptrs[cpu]->kvm_hstate.host_ipi = host_ipi;
+	/*
+	 * order stores of IPI messages vs. setting of host_ipi flag
+	 *
+	 * pairs with the barrier in kvmppc_clear_host_ipi()
+	 */
+	smp_mb();
+	paca_ptrs[cpu]->kvm_hstate.host_ipi = 1;
+}
+
+static inline void kvmppc_clear_host_ipi(int cpu)
+{
+	paca_ptrs[cpu]->kvm_hstate.host_ipi = 0;
+	/*
+	 * order clearing of host_ipi flag vs. processing of IPI messages
+	 *
+	 * pairs with the barrier in kvmppc_set_host_ipi()
+	 */
+	smp_mb();
 }
 
 static inline void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu)
@@ -486,7 +577,10 @@ static inline u32 kvmppc_get_xics_latch(void)
 	return 0;
 }
 
-static inline void kvmppc_set_host_ipi(int cpu, u8 host_ipi)
+static inline void kvmppc_set_host_ipi(int cpu)
+{}
+
+static inline void kvmppc_clear_host_ipi(int cpu)
 {}
 
 static inline void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu)

commit 2ad7a27deaf6d78545d97ab80874584f6990360e
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Aug 26 16:21:21 2019 +1000

    KVM: PPC: Book3S: Enable XIVE native capability only if OPAL has required functions
    
    There are some POWER9 machines where the OPAL firmware does not support
    the OPAL_XIVE_GET_QUEUE_STATE and OPAL_XIVE_SET_QUEUE_STATE calls.
    The impact of this is that a guest using XIVE natively will not be able
    to be migrated successfully.  On the source side, the get_attr operation
    on the KVM native device for the KVM_DEV_XIVE_GRP_EQ_CONFIG attribute
    will fail; on the destination side, the set_attr operation for the same
    attribute will fail.
    
    This adds tests for the existence of the OPAL get/set queue state
    functions, and if they are not supported, the XIVE-native KVM device
    is not created and the KVM_CAP_PPC_IRQ_XIVE capability returns false.
    Userspace can then either provide a software emulation of XIVE, or
    else tell the guest that it does not have a XIVE controller available
    to it.
    
    Cc: stable@vger.kernel.org # v5.2+
    Fixes: 3fab2d10588e ("KVM: PPC: Book3S HV: XIVE: Activate XIVE exploitation mode")
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Cédric Le Goater <clg@kaod.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 2484e6a8f5ca..8e8514efb124 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -598,6 +598,7 @@ extern int kvmppc_xive_native_get_vp(struct kvm_vcpu *vcpu,
 				     union kvmppc_one_reg *val);
 extern int kvmppc_xive_native_set_vp(struct kvm_vcpu *vcpu,
 				     union kvmppc_one_reg *val);
+extern bool kvmppc_xive_native_supported(void);
 
 #else
 static inline int kvmppc_xive_set_xive(struct kvm *kvm, u32 irq, u32 server,

commit d94d71cb45fda694a7189839f1c6aacb4f615f95
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 29 07:12:40 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 266
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not write to the free
      software foundation 51 franklin street fifth floor boston ma 02110
      1301 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 67 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190529141333.953658117@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index bc892380e6cd..2484e6a8f5ca 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -1,16 +1,5 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, version 2, as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
  *
  * Copyright IBM Corp. 2008
  *

commit e4945b9da52b36052b7c509ca31c5ead1d165b24
Author: Cédric Le Goater <clg@kaod.org>
Date:   Thu Apr 18 12:39:35 2019 +0200

    KVM: PPC: Book3S HV: XIVE: Add get/set accessors for the VP XIVE state
    
    The state of the thread interrupt management registers needs to be
    collected for migration. These registers are cached under the
    'xive_saved_state.w01' field of the VCPU when the VPCU context is
    pulled from the HW thread. An OPAL call retrieves the backup of the
    IPB register in the underlying XIVE NVT structure and merges it in the
    KVM state.
    
    Signed-off-by: Cédric Le Goater <clg@kaod.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 9fc2753e516e..bc892380e6cd 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -269,6 +269,7 @@ union kvmppc_one_reg {
 		u64	addr;
 		u64	length;
 	}	vpaval;
+	u64	xive_timaval[2];
 };
 
 struct kvmppc_ops {
@@ -604,6 +605,10 @@ extern int kvmppc_xive_native_connect_vcpu(struct kvm_device *dev,
 extern void kvmppc_xive_native_cleanup_vcpu(struct kvm_vcpu *vcpu);
 extern void kvmppc_xive_native_init_module(void);
 extern void kvmppc_xive_native_exit_module(void);
+extern int kvmppc_xive_native_get_vp(struct kvm_vcpu *vcpu,
+				     union kvmppc_one_reg *val);
+extern int kvmppc_xive_native_set_vp(struct kvm_vcpu *vcpu,
+				     union kvmppc_one_reg *val);
 
 #else
 static inline int kvmppc_xive_set_xive(struct kvm *kvm, u32 irq, u32 server,
@@ -636,6 +641,12 @@ static inline int kvmppc_xive_native_connect_vcpu(struct kvm_device *dev,
 static inline void kvmppc_xive_native_cleanup_vcpu(struct kvm_vcpu *vcpu) { }
 static inline void kvmppc_xive_native_init_module(void) { }
 static inline void kvmppc_xive_native_exit_module(void) { }
+static inline int kvmppc_xive_native_get_vp(struct kvm_vcpu *vcpu,
+					    union kvmppc_one_reg *val)
+{ return 0; }
+static inline int kvmppc_xive_native_set_vp(struct kvm_vcpu *vcpu,
+					    union kvmppc_one_reg *val)
+{ return -ENOENT; }
 
 #endif /* CONFIG_KVM_XIVE */
 

commit eacc56bb9de3e6830ddc169553772cd6de59ee4c
Author: Cédric Le Goater <clg@kaod.org>
Date:   Thu Apr 18 12:39:28 2019 +0200

    KVM: PPC: Book3S HV: XIVE: Introduce a new capability KVM_CAP_PPC_IRQ_XIVE
    
    The user interface exposes a new capability KVM_CAP_PPC_IRQ_XIVE to
    let QEMU connect the vCPU presenters to the XIVE KVM device if
    required. The capability is not advertised for now as the full support
    for the XIVE native exploitation mode is not yet available. When this
    is case, the capability will be advertised on PowerNV Hypervisors
    only. Nested guests (pseries KVM Hypervisor) are not supported.
    
    Internally, the interface to the new KVM device is protected with a
    new interrupt mode: KVMPPC_IRQ_XIVE.
    
    Signed-off-by: Cédric Le Goater <clg@kaod.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index f746d85f36c7..9fc2753e516e 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -594,6 +594,14 @@ extern int kvmppc_xive_set_irq(struct kvm *kvm, int irq_source_id, u32 irq,
 			       int level, bool line_status);
 extern void kvmppc_xive_push_vcpu(struct kvm_vcpu *vcpu);
 
+static inline int kvmppc_xive_enabled(struct kvm_vcpu *vcpu)
+{
+	return vcpu->arch.irq_type == KVMPPC_IRQ_XIVE;
+}
+
+extern int kvmppc_xive_native_connect_vcpu(struct kvm_device *dev,
+					   struct kvm_vcpu *vcpu, u32 cpu);
+extern void kvmppc_xive_native_cleanup_vcpu(struct kvm_vcpu *vcpu);
 extern void kvmppc_xive_native_init_module(void);
 extern void kvmppc_xive_native_exit_module(void);
 
@@ -621,6 +629,11 @@ static inline int kvmppc_xive_set_irq(struct kvm *kvm, int irq_source_id, u32 ir
 				      int level, bool line_status) { return -ENODEV; }
 static inline void kvmppc_xive_push_vcpu(struct kvm_vcpu *vcpu) { }
 
+static inline int kvmppc_xive_enabled(struct kvm_vcpu *vcpu)
+	{ return 0; }
+static inline int kvmppc_xive_native_connect_vcpu(struct kvm_device *dev,
+			  struct kvm_vcpu *vcpu, u32 cpu) { return -EBUSY; }
+static inline void kvmppc_xive_native_cleanup_vcpu(struct kvm_vcpu *vcpu) { }
 static inline void kvmppc_xive_native_init_module(void) { }
 static inline void kvmppc_xive_native_exit_module(void) { }
 

commit 90c73795afa24890bd2ae4f3b359de04b4147d37
Author: Cédric Le Goater <clg@kaod.org>
Date:   Thu Apr 18 12:39:27 2019 +0200

    KVM: PPC: Book3S HV: Add a new KVM device for the XIVE native exploitation mode
    
    This is the basic framework for the new KVM device supporting the XIVE
    native exploitation mode. The user interface exposes a new KVM device
    to be created by QEMU, only available when running on a L0 hypervisor.
    Support for nested guests is not available yet.
    
    The XIVE device reuses the device structure of the XICS-on-XIVE device
    as they have a lot in common. That could possibly change in the future
    if the need arise.
    
    Signed-off-by: Cédric Le Goater <clg@kaod.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 0e99a6f8066f..f746d85f36c7 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -593,6 +593,10 @@ extern int kvmppc_xive_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
 extern int kvmppc_xive_set_irq(struct kvm *kvm, int irq_source_id, u32 irq,
 			       int level, bool line_status);
 extern void kvmppc_xive_push_vcpu(struct kvm_vcpu *vcpu);
+
+extern void kvmppc_xive_native_init_module(void);
+extern void kvmppc_xive_native_exit_module(void);
+
 #else
 static inline int kvmppc_xive_set_xive(struct kvm *kvm, u32 irq, u32 server,
 				       u32 priority) { return -1; }
@@ -616,6 +620,10 @@ static inline int kvmppc_xive_set_icp(struct kvm_vcpu *vcpu, u64 icpval) { retur
 static inline int kvmppc_xive_set_irq(struct kvm *kvm, int irq_source_id, u32 irq,
 				      int level, bool line_status) { return -ENODEV; }
 static inline void kvmppc_xive_push_vcpu(struct kvm_vcpu *vcpu) { }
+
+static inline void kvmppc_xive_native_init_module(void) { }
+static inline void kvmppc_xive_native_exit_module(void) { }
+
 #endif /* CONFIG_KVM_XIVE */
 
 #if defined(CONFIG_PPC_POWERNV) && defined(CONFIG_KVM_BOOK3S_64_HANDLER)

commit 70ea13f6e609e8762d9f57287ebf873a18c91a44
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Apr 29 19:02:58 2019 +1000

    KVM: PPC: Book3S HV: Flush TLB on secondary radix threads
    
    When running on POWER9 with kvm_hv.indep_threads_mode = N and the host
    in SMT1 mode, KVM will run guest VCPUs on offline secondary threads.
    If those guests are in radix mode, we fail to load the LPID and flush
    the TLB if necessary, leading to the guest crashing with an
    unsupported MMU fault.  This arises from commit 9a4506e11b97 ("KVM:
    PPC: Book3S HV: Make radix handle process scoped LPID flush in C,
    with relocation on", 2018-05-17), which didn't consider the case
    where indep_threads_mode = N.
    
    For simplicity, this makes the real-mode guest entry path flush the
    TLB in the same place for both radix and hash guests, as we did before
    9a4506e11b97, though the code is now C code rather than assembly code.
    We also have the radix TLB flush open-coded rather than calling
    radix__local_flush_tlb_lpid_guest(), because the TLB flush can be
    called in real mode, and in real mode we don't want to invoke the
    tracepoint code.
    
    Fixes: 9a4506e11b97 ("KVM: PPC: Book3S HV: Make radix handle process scoped LPID flush in C, with relocation on")
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 0975f5f2fd70..0e99a6f8066f 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -476,7 +476,8 @@ extern void kvm_hv_vm_activated(void);
 extern void kvm_hv_vm_deactivated(void);
 extern bool kvm_hv_mode_active(void);
 
-extern void kvmppc_hpt_check_need_tlb_flush(struct kvm *kvm);
+extern void kvmppc_check_need_tlb_flush(struct kvm *kvm, int pcpu,
+					struct kvm_nested_guest *nested);
 
 #else
 static inline void __init kvm_cma_reserve(void)

commit 2940ba0c48bf18e15e85cbb0f26c0e88e1211587
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Apr 29 19:00:40 2019 +1000

    KVM: PPC: Book3S HV: Move HPT guest TLB flushing to C code
    
    This replaces assembler code in book3s_hv_rmhandlers.S that checks
    the kvm->arch.need_tlb_flush cpumask and optionally does a TLB flush
    with C code in book3s_hv_builtin.c.  Note that unlike the radix
    version, the hash version doesn't do an explicit ERAT invalidation
    because we will invalidate and load up the SLB before entering the
    guest, and that will invalidate the ERAT.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 05128e917a83..0975f5f2fd70 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -476,6 +476,8 @@ extern void kvm_hv_vm_activated(void);
 extern void kvm_hv_vm_deactivated(void);
 extern bool kvm_hv_mode_active(void);
 
+extern void kvmppc_hpt_check_need_tlb_flush(struct kvm *kvm);
+
 #else
 static inline void __init kvm_cma_reserve(void)
 {}

commit e1a1ef84cd07f72ce12f139eb9a37d3f9028e7a7
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Fri Mar 29 16:43:26 2019 +1100

    KVM: PPC: Book3S: Allocate guest TCEs on demand too
    
    We already allocate hardware TCE tables in multiple levels and skip
    intermediate levels when we can, now it is a turn of the KVM TCE tables.
    Thankfully these are allocated already in 2 levels.
    
    This moves the table's last level allocation from the creating helper to
    kvmppc_tce_put() and kvm_spapr_tce_fault(). Since such allocation cannot
    be done in real mode, this creates a virtual mode version of
    kvmppc_tce_put() which handles allocations.
    
    This adds kvmppc_rm_ioba_validate() to do an additional test if
    the consequent kvmppc_tce_put() needs a page which has not been allocated;
    if this is the case, we bail out to virtual mode handlers.
    
    The allocations are protected by a new mutex as kvm->lock is not suitable
    for the task because the fault handler is called with the mmap_sem held
    but kvmhv_setup_mmu() locks kvm->lock and mmap_sem in the reverse order.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 02732eb156ae..05128e917a83 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -197,8 +197,6 @@ extern struct kvmppc_spapr_tce_table *kvmppc_find_table(
 		(iommu_tce_check_ioba((stt)->page_shift, (stt)->offset, \
 				(stt)->size, (ioba), (npages)) ?        \
 				H_PARAMETER : H_SUCCESS)
-extern void kvmppc_tce_put(struct kvmppc_spapr_tce_table *tt,
-		unsigned long idx, unsigned long tce);
 extern long kvmppc_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
 			     unsigned long ioba, unsigned long tce);
 extern long kvmppc_h_put_tce_indirect(struct kvm_vcpu *vcpu,

commit 2001825efcea75e4209e4956f6cd619fbc246d16
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Fri Mar 29 16:42:20 2019 +1100

    KVM: PPC: Book3S HV: Avoid lockdep debugging in TCE realmode handlers
    
    The kvmppc_tce_to_ua() helper is called from real and virtual modes
    and it works fine as long as CONFIG_DEBUG_LOCKDEP is not enabled.
    However if the lockdep debugging is on, the lockdep will most likely break
    in kvm_memslots() because of srcu_dereference_check() so we need to use
    PPC-own kvm_memslots_raw() which uses realmode safe
    rcu_dereference_raw_notrace().
    
    This creates a realmode copy of kvmppc_tce_to_ua() which replaces
    kvm_memslots() with kvm_memslots_raw().
    
    Since kvmppc_rm_tce_to_ua() becomes static and can only be used inside
    HV KVM, this moves it earlier under CONFIG_KVM_BOOK3S_HV_POSSIBLE.
    
    This moves truly virtual-mode kvmppc_tce_to_ua() to where it belongs and
    drops the prmap parameter which was never used in the virtual mode.
    
    Fixes: d3695aa4f452 ("KVM: PPC: Add support for multiple-TCE hcalls", 2016-02-15)
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index df0b173da3e0..02732eb156ae 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -197,8 +197,6 @@ extern struct kvmppc_spapr_tce_table *kvmppc_find_table(
 		(iommu_tce_check_ioba((stt)->page_shift, (stt)->offset, \
 				(stt)->size, (ioba), (npages)) ?        \
 				H_PARAMETER : H_SUCCESS)
-extern long kvmppc_tce_to_ua(struct kvm *kvm, unsigned long tce,
-		unsigned long *ua, unsigned long **prmap);
 extern void kvmppc_tce_put(struct kvmppc_spapr_tce_table *tt,
 		unsigned long idx, unsigned long tce);
 extern long kvmppc_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,

commit eadfb1c5f8c02f428a565e62e908e99900b697e4
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Fri Mar 22 17:05:45 2019 +1100

    KVM: PPC: Book3S HV: Implement real mode H_PAGE_INIT handler
    
    Implement a real mode handler for the H_CALL H_PAGE_INIT which can be
    used to zero or copy a guest page. The page is defined to be 4k and must
    be 4k aligned.
    
    The in-kernel real mode handler halves the time to handle this H_CALL
    compared to handling it in userspace for a hash guest.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index ac22b28ae78d..df0b173da3e0 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -665,6 +665,8 @@ long kvmppc_h_clear_ref(struct kvm_vcpu *vcpu, unsigned long flags,
                         unsigned long pte_index);
 long kvmppc_h_clear_mod(struct kvm_vcpu *vcpu, unsigned long flags,
                         unsigned long pte_index);
+long kvmppc_rm_h_page_init(struct kvm_vcpu *vcpu, unsigned long flags,
+			   unsigned long dest, unsigned long src);
 long kvmppc_hpte_hv_fault(struct kvm_vcpu *vcpu, unsigned long addr,
                           unsigned long slb_v, unsigned int status, bool data);
 unsigned long kvmppc_rm_h_xirr(struct kvm_vcpu *vcpu);

commit e74d53e30e2927fa5b223296ac7922baf15ea89a
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Feb 25 14:35:06 2019 +1100

    KVM: PPC: Fix compilation when KVM is not enabled
    
    Compiling with CONFIG_PPC_POWERNV=y and KVM disabled currently gives
    an error like this:
    
      CC      arch/powerpc/kernel/dbell.o
    In file included from arch/powerpc/kernel/dbell.c:20:0:
    arch/powerpc/include/asm/kvm_ppc.h: In function ‘xics_on_xive’:
    arch/powerpc/include/asm/kvm_ppc.h:625:9: error: implicit declaration of function ‘xive_enabled’ [-Werror=implicit-function-declaration]
      return xive_enabled() && cpu_has_feature(CPU_FTR_HVMODE);
             ^
    cc1: all warnings being treated as errors
    scripts/Makefile.build:276: recipe for target 'arch/powerpc/kernel/dbell.o' failed
    make[3]: *** [arch/powerpc/kernel/dbell.o] Error 1
    
    Fix this by making the xics_on_xive() definition conditional on the
    same symbol (CONFIG_KVM_BOOK3S_64_HANDLER) that determines whether we
    include <asm/xive.h> or not, since that's the header that defines
    xive_enabled().
    
    Fixes: 03f953329bd8 ("KVM: PPC: Book3S: Allow XICS emulation to work in nested hosts using XIVE")
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index d283d3179fbc..ac22b28ae78d 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -619,7 +619,7 @@ static inline int kvmppc_xive_set_irq(struct kvm *kvm, int irq_source_id, u32 ir
 static inline void kvmppc_xive_push_vcpu(struct kvm_vcpu *vcpu) { }
 #endif /* CONFIG_KVM_XIVE */
 
-#ifdef CONFIG_PPC_POWERNV
+#if defined(CONFIG_PPC_POWERNV) && defined(CONFIG_KVM_BOOK3S_64_HANDLER)
 static inline bool xics_on_xive(void)
 {
 	return xive_enabled() && cpu_has_feature(CPU_FTR_HVMODE);

commit 0a0c50f771f577b05f5a1ac9867d296d02a5e51c
Merge: 716cb1160819 c3c7470c7556
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Feb 22 13:52:30 2019 +1100

    Merge remote-tracking branch 'remotes/powerpc/topic/ppc-kvm' into kvm-ppc-next
    
    This merges in the "ppc-kvm" topic branch of the powerpc tree to get a
    series of commits that touch both general arch/powerpc code and KVM
    code.  These commits will be merged both via the KVM tree and the
    powerpc tree.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

commit 884dfb722db899e36d8c382783347aab57f96caa
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Feb 21 13:38:49 2019 +1100

    KVM: PPC: Book3S HV: Simplify machine check handling
    
    This makes the handling of machine check interrupts that occur inside
    a guest simpler and more robust, with less done in assembler code and
    in real mode.
    
    Now, when a machine check occurs inside a guest, we always get the
    machine check event struct and put a copy in the vcpu struct for the
    vcpu where the machine check occurred.  We no longer call
    machine_check_queue_event() from kvmppc_realmode_mc_power7(), because
    on POWER8, when a vcpu is running on an offline secondary thread and
    we call machine_check_queue_event(), that calls irq_work_queue(),
    which doesn't work because the CPU is offline, but instead triggers
    the WARN_ON(lazy_irq_pending()) in pnv_smp_cpu_kill_self() (which
    fires again and again because nothing clears the condition).
    
    All that machine_check_queue_event() actually does is to cause the
    event to be printed to the console.  For a machine check occurring in
    the guest, we now print the event in kvmppc_handle_exit_hv()
    instead.
    
    The assembly code at label machine_check_realmode now just calls C
    code and then continues exiting the guest.  We no longer either
    synthesize a machine check for the guest in assembly code or return
    to the guest without a machine check.
    
    The code in kvmppc_handle_exit_hv() is extended to handle the case
    where the guest is not FWNMI-capable.  In that case we now always
    synthesize a machine check interrupt for the guest.  Previously, if
    the host thinks it has recovered the machine check fully, it would
    return to the guest without any notification that the machine check
    had occurred.  If the machine check was caused by some action of the
    guest (such as creating duplicate SLB entries), it is much better to
    tell the guest that it has caused a problem.  Therefore we now always
    generate a machine check interrupt for guests that are not
    FWNMI-capable.
    
    Reviewed-by: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Reviewed-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index eb0d79f0ca45..a6c8548ed9fa 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -141,6 +141,7 @@ extern void kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu);
 
 extern int kvmppc_core_prepare_to_enter(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_pending_dec(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_queue_machine_check(struct kvm_vcpu *vcpu, ulong flags);
 extern void kvmppc_core_queue_program(struct kvm_vcpu *vcpu, ulong flags);
 extern void kvmppc_core_queue_fpunavail(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_vec_unavail(struct kvm_vcpu *vcpu);
@@ -632,7 +633,7 @@ long int kvmppc_rm_h_confer(struct kvm_vcpu *vcpu, int target,
                             unsigned int yield_count);
 long kvmppc_h_random(struct kvm_vcpu *vcpu);
 void kvmhv_commence_exit(int trap);
-long kvmppc_realmode_machine_check(struct kvm_vcpu *vcpu);
+void kvmppc_realmode_machine_check(struct kvm_vcpu *vcpu);
 void kvmppc_subcore_enter_guest(void);
 void kvmppc_subcore_exit_guest(void);
 long kvmppc_realmode_hmi_handler(void);

commit 03f953329bd872b176e825584d8c0b50685f16ee
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Feb 4 22:07:20 2019 +1100

    KVM: PPC: Book3S: Allow XICS emulation to work in nested hosts using XIVE
    
    Currently, the KVM code assumes that if the host kernel is using the
    XIVE interrupt controller (the new interrupt controller that first
    appeared in POWER9 systems), then the in-kernel XICS emulation will
    use the XIVE hardware to deliver interrupts to the guest.  However,
    this only works when the host is running in hypervisor mode and has
    full access to all of the XIVE functionality.  It doesn't work in any
    nested virtualization scenario, either with PR KVM or nested-HV KVM,
    because the XICS-on-XIVE code calls directly into the native-XIVE
    routines, which are not initialized and cannot function correctly
    because they use OPAL calls, and OPAL is not available in a guest.
    
    This means that using the in-kernel XICS emulation in a nested
    hypervisor that is using XIVE as its interrupt controller will cause a
    (nested) host kernel crash.  To fix this, we change most of the places
    where the current code calls xive_enabled() to select between the
    XICS-on-XIVE emulation and the plain XICS emulation to call a new
    function, xics_on_xive(), which returns false in a guest.
    
    However, there is a further twist.  The plain XICS emulation has some
    functions which are used in real mode and access the underlying XICS
    controller (the interrupt controller of the host) directly.  In the
    case of a nested hypervisor, this means doing XICS hypercalls
    directly.  When the nested host is using XIVE as its interrupt
    controller, these hypercalls will fail.  Therefore this also adds
    checks in the places where the XICS emulation wants to access the
    underlying interrupt controller directly, and if that is XIVE, makes
    the code use the virtual mode fallback paths, which call generic
    kernel infrastructure rather than doing direct XICS access.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: Cédric Le Goater <clg@kaod.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index eb0d79f0ca45..b3bf4f61b30c 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -36,6 +36,8 @@
 #endif
 #ifdef CONFIG_KVM_BOOK3S_64_HANDLER
 #include <asm/paca.h>
+#include <asm/xive.h>
+#include <asm/cpu_has_feature.h>
 #endif
 
 /*
@@ -616,6 +618,18 @@ static inline int kvmppc_xive_set_irq(struct kvm *kvm, int irq_source_id, u32 ir
 static inline void kvmppc_xive_push_vcpu(struct kvm_vcpu *vcpu) { }
 #endif /* CONFIG_KVM_XIVE */
 
+#ifdef CONFIG_PPC_POWERNV
+static inline bool xics_on_xive(void)
+{
+	return xive_enabled() && cpu_has_feature(CPU_FTR_HVMODE);
+}
+#else
+static inline bool xics_on_xive(void)
+{
+	return false;
+}
+#endif
+
 /*
  * Prototypes for functions called only from assembler code.
  * Having prototypes reduces sparse errors.

commit dceadcf91b2e0971abe706b6d605ed25de61db0e
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Fri Dec 14 16:29:06 2018 +1100

    KVM: PPC: Add load_from_eaddr and store_to_eaddr to the kvmppc_ops struct
    
    The kvmppc_ops struct is used to store function pointers to kvm
    implementation specific functions.
    
    Introduce two new functions load_from_eaddr and store_to_eaddr to be
    used to load from and store to a guest effective address respectively.
    
    Also implement these for the kvm-hv module. If we are using the radix
    mmu then we can call the functions to access quadrant 1 and 2.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 04c5b84df83d..eb0d79f0ca45 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -328,6 +328,10 @@ struct kvmppc_ops {
 			    unsigned long flags);
 	void (*giveup_ext)(struct kvm_vcpu *vcpu, ulong msr);
 	int (*enable_nested)(struct kvm *kvm);
+	int (*load_from_eaddr)(struct kvm_vcpu *vcpu, ulong *eaddr, void *ptr,
+			       int size);
+	int (*store_to_eaddr)(struct kvm_vcpu *vcpu, ulong *eaddr, void *ptr,
+			      int size);
 };
 
 extern struct kvmppc_ops *kvmppc_hv_ops;

commit f032b73459eed4897bcafee2b1d37a817f1bb596
Author: Bharata B Rao <bharata@linux.ibm.com>
Date:   Wed Dec 12 15:15:30 2018 +1100

    KVM: PPC: Pass change type down to memslot commit function
    
    Currently, kvm_arch_commit_memory_region() gets called with a
    parameter indicating what type of change is being made to the memslot,
    but it doesn't pass it down to the platform-specific memslot commit
    functions.  This adds the `change' parameter to the lower-level
    functions so that they can use it in future.
    
    [paulus@ozlabs.org - fix book E also.]
    
    Signed-off-by: Bharata B Rao <bharata@linux.vnet.ibm.com>
    Reviewed-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 9b89b1918dfc..04c5b84df83d 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -224,7 +224,8 @@ extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
 extern void kvmppc_core_commit_memory_region(struct kvm *kvm,
 				const struct kvm_userspace_memory_region *mem,
 				const struct kvm_memory_slot *old,
-				const struct kvm_memory_slot *new);
+				const struct kvm_memory_slot *new,
+				enum kvm_mr_change change);
 extern int kvm_vm_ioctl_get_smmu_info(struct kvm *kvm,
 				      struct kvm_ppc_smmu_info *info);
 extern void kvmppc_core_flush_memslot(struct kvm *kvm,
@@ -294,7 +295,8 @@ struct kvmppc_ops {
 	void (*commit_memory_region)(struct kvm *kvm,
 				     const struct kvm_userspace_memory_region *mem,
 				     const struct kvm_memory_slot *old,
-				     const struct kvm_memory_slot *new);
+				     const struct kvm_memory_slot *new,
+				     enum kvm_mr_change change);
 	int (*unmap_hva_range)(struct kvm *kvm, unsigned long start,
 			   unsigned long end);
 	int (*age_hva)(struct kvm *kvm, unsigned long start, unsigned long end);

commit aa069a996951f3e2e38437ef0316685a5893fc7e
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Sep 21 20:02:01 2018 +1000

    KVM: PPC: Book3S HV: Add a VM capability to enable nested virtualization
    
    With this, userspace can enable a KVM-HV guest to run nested guests
    under it.
    
    The administrator can control whether any nested guests can be run;
    setting the "nested" module parameter to false prevents any guests
    becoming nested hypervisors (that is, any attempt to enable the nested
    capability on a guest will fail).  Guests which are already nested
    hypervisors will continue to be so.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 88362ccda549..9b89b1918dfc 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -325,6 +325,7 @@ struct kvmppc_ops {
 	int (*set_smt_mode)(struct kvm *kvm, unsigned long mode,
 			    unsigned long flags);
 	void (*giveup_ext)(struct kvm_vcpu *vcpu, ulong msr);
+	int (*enable_nested)(struct kvm *kvm);
 };
 
 extern struct kvmppc_ops *kvmppc_hv_ops;

commit 95a6432ce903858a2f285d611275340aa574c6ac
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:30:55 2018 +1100

    KVM: PPC: Book3S HV: Streamlined guest entry/exit path on P9 for radix guests
    
    This creates an alternative guest entry/exit path which is used for
    radix guests on POWER9 systems when we have indep_threads_mode=Y.  In
    these circumstances there is exactly one vcpu per vcore and there is
    no coordination required between vcpus or vcores; the vcpu can enter
    the guest without needing to synchronize with anything else.
    
    The new fast path is implemented almost entirely in C in book3s_hv.c
    and runs with the MMU on until the guest is entered.  On guest exit
    we use the existing path until the point where we are committed to
    exiting the guest (as distinct from handling an interrupt in the
    low-level code and returning to the guest) and we have pulled the
    guest context from the XIVE.  At that point we check a flag in the
    stack frame to see whether we came in via the old path and the new
    path; if we came in via the new path then we go back to C code to do
    the rest of the process of saving the guest context and restoring the
    host context.
    
    The C code is split into separate functions for handling the
    OS-accessible state and the hypervisor state, with the idea that the
    latter can be replaced by a hypercall when we implement nested
    virtualization.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    [mpe: Fix CONFIG_ALTIVEC=n build]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 03a60f76f3d7..88362ccda549 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -583,6 +583,7 @@ extern int kvmppc_xive_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
 
 extern int kvmppc_xive_set_irq(struct kvm *kvm, int irq_source_id, u32 irq,
 			       int level, bool line_status);
+extern void kvmppc_xive_push_vcpu(struct kvm_vcpu *vcpu);
 #else
 static inline int kvmppc_xive_set_xive(struct kvm *kvm, u32 irq, u32 server,
 				       u32 priority) { return -1; }
@@ -605,6 +606,7 @@ static inline int kvmppc_xive_set_icp(struct kvm_vcpu *vcpu, u64 icpval) { retur
 
 static inline int kvmppc_xive_set_irq(struct kvm *kvm, int irq_source_id, u32 irq,
 				      int level, bool line_status) { return -ENODEV; }
+static inline void kvmppc_xive_push_vcpu(struct kvm_vcpu *vcpu) { }
 #endif /* CONFIG_KVM_XIVE */
 
 /*

commit f7035ce9f1dfb1042c4acedf5cca6f9af395f110
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:30:50 2018 +1100

    KVM: PPC: Book3S HV: Move interrupt delivery on guest entry to C code
    
    This is based on a patch by Suraj Jitindar Singh.
    
    This moves the code in book3s_hv_rmhandlers.S that generates an
    external, decrementer or privileged doorbell interrupt just before
    entering the guest to C code in book3s_hv_builtin.c.  This is to
    make future maintenance and modification easier.  The algorithm
    expressed in the C code is almost identical to the previous
    algorithm.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 38d03282bb6a..03a60f76f3d7 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -650,6 +650,7 @@ int kvmppc_rm_h_ipi(struct kvm_vcpu *vcpu, unsigned long server,
                     unsigned long mfrr);
 int kvmppc_rm_h_cppr(struct kvm_vcpu *vcpu, unsigned long cppr);
 int kvmppc_rm_h_eoi(struct kvm_vcpu *vcpu, unsigned long xirr);
+void kvmppc_guest_entry_inject_int(struct kvm_vcpu *vcpu);
 
 /*
  * Host-side operations we want to set up while running in real

commit a3ac077b75c5a922dcbafd7e689ee09beefae0f6
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Mon Sep 10 18:29:12 2018 +1000

    KVM: PPC: Remove redundand permission bits removal
    
    The kvmppc_gpa_to_ua() helper itself takes care of the permission
    bits in the TCE and yet every single caller removes them.
    
    This changes semantics of kvmppc_gpa_to_ua() so it takes TCEs
    (which are GPAs + TCE permission bits) to make the callers simpler.
    
    This should cause no behavioural change.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 2f5d431e438b..38d03282bb6a 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -194,7 +194,7 @@ extern struct kvmppc_spapr_tce_table *kvmppc_find_table(
 		(iommu_tce_check_ioba((stt)->page_shift, (stt)->offset, \
 				(stt)->size, (ioba), (npages)) ?        \
 				H_PARAMETER : H_SUCCESS)
-extern long kvmppc_gpa_to_ua(struct kvm *kvm, unsigned long gpa,
+extern long kvmppc_tce_to_ua(struct kvm *kvm, unsigned long tce,
 		unsigned long *ua, unsigned long **prmap);
 extern void kvmppc_tce_put(struct kvmppc_spapr_tce_table *tt,
 		unsigned long idx, unsigned long tce);

commit 42de7b9e216728edbe53e0c4513e06fe3d566c5d
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Mon Sep 10 18:29:10 2018 +1000

    KVM: PPC: Validate TCEs against preregistered memory page sizes
    
    The userspace can request an arbitrary supported page size for a DMA
    window and this works fine as long as the mapped memory is backed with
    the pages of the same or bigger size; if this is not the case,
    mm_iommu_ua_to_hpa{_rm}() fail and tables do not populated with
    dangerously incorrect TCEs.
    
    However since it is quite easy to misconfigure the KVM and we do not do
    reverts to all changes made to TCE tables if an error happens in a middle,
    we better do the acceptable page size validation before we even touch
    the tables.
    
    This enhances kvmppc_tce_validate() to check the hardware IOMMU page sizes
    against the preregistered memory page sizes.
    
    Since the new check uses real/virtual mode helpers, this renames
    kvmppc_tce_validate() to kvmppc_rm_tce_validate() to handle the real mode
    case and mirrors it for the virtual mode under the old name. The real
    mode handler is not used for the virtual mode as:
    1. it uses _lockless() list traversing primitives instead of RCU;
    2. realmode's mm_iommu_ua_to_hpa_rm() uses vmalloc_to_phys() which
    virtual mode does not have to use and since on POWER9+radix only virtual
    mode handlers actually work, we do not want to slow down that path even
    a bit.
    
    This removes EXPORT_SYMBOL_GPL(kvmppc_tce_validate) as the validators
    are static now.
    
    From now on the attempts on mapping IOMMU pages bigger than allowed
    will result in KVM exit.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    [mpe: Fix KVM_HV=n build]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index e991821dd7fa..2f5d431e438b 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -194,8 +194,6 @@ extern struct kvmppc_spapr_tce_table *kvmppc_find_table(
 		(iommu_tce_check_ioba((stt)->page_shift, (stt)->offset, \
 				(stt)->size, (ioba), (npages)) ?        \
 				H_PARAMETER : H_SUCCESS)
-extern long kvmppc_tce_validate(struct kvmppc_spapr_tce_table *tt,
-		unsigned long tce);
 extern long kvmppc_gpa_to_ua(struct kvm *kvm, unsigned long gpa,
 		unsigned long *ua, unsigned long **prmap);
 extern void kvmppc_tce_put(struct kvmppc_spapr_tce_table *tt,

commit acc9eb9305fecd958e2877c4e6cd3284d01c2e82
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 21 13:24:26 2018 +0800

    KVM: PPC: Reimplement LOAD_VMX/STORE_VMX instruction mmio emulation with analyse_instr() input
    
    This patch reimplements LOAD_VMX/STORE_VMX MMIO emulation with
    analyse_instr() input. When emulating the store, the VMX reg will need to
    be flushed so that the right reg val can be retrieved before writing to
    IO MEM.
    
    This patch also adds support for lvebx/lvehx/lvewx/stvebx/stvehx/stvewx
    MMIO emulation. To meet the requirement of handling different element
    sizes, kvmppc_handle_load128_by2x64()/kvmppc_handle_store128_by2x64()
    were replaced with kvmppc_handle_vmx_load()/kvmppc_handle_vmx_store().
    
    The framework used is similar to VSX instruction MMIO emulation.
    
    Suggested-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 1f087c4cb386..e991821dd7fa 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -81,10 +81,10 @@ extern int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
 extern int kvmppc_handle_vsx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 				unsigned int rt, unsigned int bytes,
 			int is_default_endian, int mmio_sign_extend);
-extern int kvmppc_handle_load128_by2x64(struct kvm_run *run,
-		struct kvm_vcpu *vcpu, unsigned int rt, int is_default_endian);
-extern int kvmppc_handle_store128_by2x64(struct kvm_run *run,
-		struct kvm_vcpu *vcpu, unsigned int rs, int is_default_endian);
+extern int kvmppc_handle_vmx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+		unsigned int rt, unsigned int bytes, int is_default_endian);
+extern int kvmppc_handle_vmx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+		unsigned int rs, unsigned int bytes, int is_default_endian);
 extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 			       u64 val, unsigned int bytes,
 			       int is_default_endian);
@@ -265,6 +265,8 @@ union kvmppc_one_reg {
 	vector128 vval;
 	u64	vsxval[2];
 	u32	vsx32val[4];
+	u16	vsx16val[8];
+	u8	vsx8val[16];
 	struct {
 		u64	addr;
 		u64	length;

commit 2e6baa46b4ae785e3e954aaf9d2e8a0bb06ad33a
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 21 13:24:22 2018 +0800

    KVM: PPC: Add giveup_ext() hook to PPC KVM ops
    
    Currently HV will save math regs(FP/VEC/VSX) when trap into host. But
    PR KVM will only save math regs when qemu task switch out of CPU, or
    when returning from qemu code.
    
    To emulate FP/VEC/VSX mmio load, PR KVM need to make sure that math
    regs were flushed firstly and then be able to update saved VCPU
    FPR/VEC/VSX area reasonably.
    
    This patch adds giveup_ext() field to KVM ops. Only PR KVM has non-NULL
    giveup_ext() ops. kvmppc_complete_mmio_load() can invoke that hook
    (when not NULL) to flush math regs accordingly, before updating saved
    register vals.
    
    Math regs flush is also necessary for STORE, which will be covered
    in later patch within this patch series.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 139cdf0abf90..1f087c4cb386 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -324,6 +324,7 @@ struct kvmppc_ops {
 	int (*get_rmmu_info)(struct kvm *kvm, struct kvm_ppc_rmmu_info *info);
 	int (*set_smt_mode)(struct kvm *kvm, unsigned long mode,
 			    unsigned long flags);
+	void (*giveup_ext)(struct kvm_vcpu *vcpu, ulong msr);
 };
 
 extern struct kvmppc_ops *kvmppc_hv_ops;

commit 7092360399644ad4b12ac573c1996536b9e9b4b6
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 21 13:24:21 2018 +0800

    KVM: PPC: Reimplement non-SIMD LOAD/STORE instruction mmio emulation with analyse_instr() input
    
    This patch reimplements non-SIMD LOAD/STORE instruction MMIO emulation
    with analyse_instr() input. It utilizes the BYTEREV/UPDATE/SIGNEXT
    properties exported by analyse_instr() and invokes
    kvmppc_handle_load(s)/kvmppc_handle_store() accordingly.
    
    It also moves CACHEOP type handling into the skeleton.
    
    instruction_type within kvm_ppc.h is renamed to avoid conflict with
    sstep.h.
    
    Suggested-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index abe7032cdb54..139cdf0abf90 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -52,7 +52,7 @@ enum emulation_result {
 	EMULATE_EXIT_USER,    /* emulation requires exit to user-space */
 };
 
-enum instruction_type {
+enum instruction_fetch_type {
 	INST_GENERIC,
 	INST_SC,		/* system call */
 };
@@ -93,7 +93,7 @@ extern int kvmppc_handle_vsx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 				int is_default_endian);
 
 extern int kvmppc_load_last_inst(struct kvm_vcpu *vcpu,
-				 enum instruction_type type, u32 *inst);
+				 enum instruction_fetch_type type, u32 *inst);
 
 extern int kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 		     bool data);
@@ -330,7 +330,7 @@ extern struct kvmppc_ops *kvmppc_hv_ops;
 extern struct kvmppc_ops *kvmppc_pr_ops;
 
 static inline int kvmppc_get_last_inst(struct kvm_vcpu *vcpu,
-					enum instruction_type type, u32 *inst)
+				enum instruction_fetch_type type, u32 *inst)
 {
 	int ret = EMULATE_DONE;
 	u32 fetched_inst;

commit d8312a3f61024352f1c7cb967571fd53631b0d6c
Merge: e9092d0d9796 e01bca2fc698
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 9 11:42:31 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - VHE optimizations
    
       - EL2 address space randomization
    
       - speculative execution mitigations ("variant 3a", aka execution past
         invalid privilege register access)
    
       - bugfixes and cleanups
    
      PPC:
       - improvements for the radix page fault handler for HV KVM on POWER9
    
      s390:
       - more kvm stat counters
    
       - virtio gpu plumbing
    
       - documentation
    
       - facilities improvements
    
      x86:
       - support for VMware magic I/O port and pseudo-PMCs
    
       - AMD pause loop exiting
    
       - support for AMD core performance extensions
    
       - support for synchronous register access
    
       - expose nVMX capabilities to userspace
    
       - support for Hyper-V signaling via eventfd
    
       - use Enlightened VMCS when running on Hyper-V
    
       - allow userspace to disable MWAIT/HLT/PAUSE vmexits
    
       - usual roundup of optimizations and nested virtualization bugfixes
    
      Generic:
       - API selftest infrastructure (though the only tests are for x86 as
         of now)"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (174 commits)
      kvm: x86: fix a prototype warning
      kvm: selftests: add sync_regs_test
      kvm: selftests: add API testing infrastructure
      kvm: x86: fix a compile warning
      KVM: X86: Add Force Emulation Prefix for "emulate the next instruction"
      KVM: X86: Introduce handle_ud()
      KVM: vmx: unify adjacent #ifdefs
      x86: kvm: hide the unused 'cpu' variable
      KVM: VMX: remove bogus WARN_ON in handle_ept_misconfig
      Revert "KVM: X86: Fix SMRAM accessing even if VM is shutdown"
      kvm: Add emulation for movups/movupd
      KVM: VMX: raise internal error for exception during invalid protected mode state
      KVM: nVMX: Optimization: Dont set KVM_REQ_EVENT when VMExit with nested_run_pending
      KVM: nVMX: Require immediate-exit when event reinjected to L2 and L1 event pending
      KVM: x86: Fix misleading comments on handling pending exceptions
      KVM: x86: Rename interrupt.pending to interrupt.injected
      KVM: VMX: No need to clear pending NMI/interrupt on inject realmode interrupt
      x86/kvm: use Enlightened VMCS when running on Hyper-V
      x86/hyper-v: detect nested features
      x86/hyper-v: define struct hv_enlightened_vmcs and clean field bits
      ...

commit d2e60075a3d4422dc54b919f3b125d8066b839d4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:12 2018 +1000

    powerpc/64: Use array of paca pointers and allocate pacas individually
    
    Change the paca array into an array of pointers to pacas. Allocate
    pacas individually.
    
    This allows flexibility in where the PACAs are allocated. Future work
    will allocate them node-local. Platforms that don't have address limits
    on PACAs would be able to defer PACA allocations until later in boot
    rather than allocate all possible ones up-front then freeing unused.
    
    This is slightly more overhead (one additional indirection) for cross
    CPU paca references, but those aren't too common.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 7765a800ddae..b7d066b037da 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -436,15 +436,15 @@ struct openpic;
 extern void kvm_cma_reserve(void) __init;
 static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 {
-	paca[cpu].kvm_hstate.xics_phys = (void __iomem *)addr;
+	paca_ptrs[cpu]->kvm_hstate.xics_phys = (void __iomem *)addr;
 }
 
 static inline void kvmppc_set_xive_tima(int cpu,
 					unsigned long phys_addr,
 					void __iomem *virt_addr)
 {
-	paca[cpu].kvm_hstate.xive_tima_phys = (void __iomem *)phys_addr;
-	paca[cpu].kvm_hstate.xive_tima_virt = virt_addr;
+	paca_ptrs[cpu]->kvm_hstate.xive_tima_phys = (void __iomem *)phys_addr;
+	paca_ptrs[cpu]->kvm_hstate.xive_tima_virt = virt_addr;
 }
 
 static inline u32 kvmppc_get_xics_latch(void)
@@ -458,7 +458,7 @@ static inline u32 kvmppc_get_xics_latch(void)
 
 static inline void kvmppc_set_host_ipi(int cpu, u8 host_ipi)
 {
-	paca[cpu].kvm_hstate.host_ipi = host_ipi;
+	paca_ptrs[cpu]->kvm_hstate.host_ipi = host_ipi;
 }
 
 static inline void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu)

commit 39c983ea0f96a270d4876c4148e3bb2d9cd3294f
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Feb 22 15:16:54 2018 +1100

    KVM: PPC: Remove unused kvm_unmap_hva callback
    
    Since commit fb1522e099f0 ("KVM: update to new mmu_notifier semantic
    v2", 2017-08-31), the MMU notifier code in KVM no longer calls the
    kvm_unmap_hva callback.  This removes the PPC implementations of
    kvm_unmap_hva().
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 7765a800ddae..23cfaef9544e 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -295,7 +295,6 @@ struct kvmppc_ops {
 				     const struct kvm_userspace_memory_region *mem,
 				     const struct kvm_memory_slot *old,
 				     const struct kvm_memory_slot *new);
-	int (*unmap_hva)(struct kvm *kvm, unsigned long hva);
 	int (*unmap_hva_range)(struct kvm *kvm, unsigned long start,
 			   unsigned long end);
 	int (*age_hva)(struct kvm *kvm, unsigned long start, unsigned long end);

commit 15303ba5d1cd9b28d03a980456c0978c0ea3b208
Merge: 9a61df9e5f74 1ab03c072feb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 10 13:16:35 2018 -0800

    Merge tag 'kvm-4.16-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krčmář:
     "ARM:
    
       - icache invalidation optimizations, improving VM startup time
    
       - support for forwarded level-triggered interrupts, improving
         performance for timers and passthrough platform devices
    
       - a small fix for power-management notifiers, and some cosmetic
         changes
    
      PPC:
    
       - add MMIO emulation for vector loads and stores
    
       - allow HPT guests to run on a radix host on POWER9 v2.2 CPUs without
         requiring the complex thread synchronization of older CPU versions
    
       - improve the handling of escalation interrupts with the XIVE
         interrupt controller
    
       - support decrement register migration
    
       - various cleanups and bugfixes.
    
      s390:
    
       - Cornelia Huck passed maintainership to Janosch Frank
    
       - exitless interrupts for emulated devices
    
       - cleanup of cpuflag handling
    
       - kvm_stat counter improvements
    
       - VSIE improvements
    
       - mm cleanup
    
      x86:
    
       - hypervisor part of SEV
    
       - UMIP, RDPID, and MSR_SMI_COUNT emulation
    
       - paravirtualized TLB shootdown using the new KVM_VCPU_PREEMPTED bit
    
       - allow guests to see TOPOEXT, GFNI, VAES, VPCLMULQDQ, and more
         AVX512 features
    
       - show vcpu id in its anonymous inode name
    
       - many fixes and cleanups
    
       - per-VCPU MSR bitmaps (already merged through x86/pti branch)
    
       - stable KVM clock when nesting on Hyper-V (merged through
         x86/hyperv)"
    
    * tag 'kvm-4.16-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (197 commits)
      KVM: PPC: Book3S: Add MMIO emulation for VMX instructions
      KVM: PPC: Book3S HV: Branch inside feature section
      KVM: PPC: Book3S HV: Make HPT resizing work on POWER9
      KVM: PPC: Book3S HV: Fix handling of secondary HPTEG in HPT resizing code
      KVM: PPC: Book3S PR: Fix broken select due to misspelling
      KVM: x86: don't forget vcpu_put() in kvm_arch_vcpu_ioctl_set_sregs()
      KVM: PPC: Book3S PR: Fix svcpu copying with preemption enabled
      KVM: PPC: Book3S HV: Drop locks before reading guest memory
      kvm: x86: remove efer_reload entry in kvm_vcpu_stat
      KVM: x86: AMD Processor Topology Information
      x86/kvm/vmx: do not use vm-exit instruction length for fast MMIO when running nested
      kvm: embed vcpu id to dentry of vcpu anon inode
      kvm: Map PFN-type memory regions as writable (if possible)
      x86/kvm: Make it compile on 32bit and with HYPYERVISOR_GUEST=n
      KVM: arm/arm64: Fixup userspace irqchip static key optimization
      KVM: arm/arm64: Fix userspace_irqchip_in_use counting
      KVM: arm/arm64: Fix incorrect timer_is_pending logic
      MAINTAINERS: update KVM/s390 maintainers
      MAINTAINERS: add Halil as additional vfio-ccw maintainer
      MAINTAINERS: add David as a reviewer for KVM/s390
      ...

commit 09f984961c137c4b252c368adab7e1c9f035fa59
Author: Jose Ricardo Ziviani <joserz@linux.vnet.ibm.com>
Date:   Sat Feb 3 18:24:26 2018 -0200

    KVM: PPC: Book3S: Add MMIO emulation for VMX instructions
    
    This patch provides the MMIO load/store vector indexed
    X-Form emulation.
    
    Instructions implemented:
    lvx: the quadword in storage addressed by the result of EA &
    0xffff_ffff_ffff_fff0 is loaded into VRT.
    
    stvx: the contents of VRS are stored into the quadword in storage
    addressed by the result of EA & 0xffff_ffff_ffff_fff0.
    
    Reported-by: Gopesh Kumar Chaudhary <gopchaud@in.ibm.com>
    Reported-by: Balamuruhan S <bala24@linux.vnet.ibm.com>
    Signed-off-by: Jose Ricardo Ziviani <joserz@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 941c2a3f231b..28c203003519 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -81,6 +81,10 @@ extern int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
 extern int kvmppc_handle_vsx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 				unsigned int rt, unsigned int bytes,
 			int is_default_endian, int mmio_sign_extend);
+extern int kvmppc_handle_load128_by2x64(struct kvm_run *run,
+		struct kvm_vcpu *vcpu, unsigned int rt, int is_default_endian);
+extern int kvmppc_handle_store128_by2x64(struct kvm_run *run,
+		struct kvm_vcpu *vcpu, unsigned int rs, int is_default_endian);
 extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 			       u64 val, unsigned int bytes,
 			       int is_default_endian);

commit 4e26bc4a4ed683c42ba45f09050575a671c6f1f4
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:50 2017 +0530

    powerpc/64: Rename soft_enabled to irq_soft_mask
    
    Rename the paca->soft_enabled to paca->irq_soft_mask as it is no
    longer used as a flag for interrupt state, but a mask.
    
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 369f0640826c..9db18287b5f4 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -873,7 +873,7 @@ static inline void kvmppc_fix_ee_before_entry(void)
 
 	/* Only need to enable IRQs by hard enabling them after this */
 	local_paca->irq_happened = 0;
-	soft_enabled_set(IRQS_ENABLED);
+	irq_soft_mask_set(IRQS_ENABLED);
 #endif
 }
 

commit 0b63acf4a0eb8843f83954ea1bd29ccdfcbaa778
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:45 2017 +0530

    powerpc/64: Move set_soft_enabled() and rename
    
    Move set_soft_enabled() from powerpc/kernel/irq.c to asm/hw_irq.c, to
    encourage updates to paca->soft_enabled done via these access
    function. Add "memory" clobber to hint compiler since
    paca->soft_enabled memory is the target here.
    
    Renaming it as soft_enabled_set() will make namespaces works better as
    prefix than a postfix when new soft_enabled manipulation functions are
    introduced.
    
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 68484d77a3cb..369f0640826c 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -873,7 +873,7 @@ static inline void kvmppc_fix_ee_before_entry(void)
 
 	/* Only need to enable IRQs by hard enabling them after this */
 	local_paca->irq_happened = 0;
-	local_paca->soft_enabled = IRQS_ENABLED;
+	soft_enabled_set(IRQS_ENABLED);
 #endif
 }
 

commit c2e480ba822718190e58849b79a76db13c3dac18
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:42 2017 +0530

    powerpc/64: Add #defines for paca->soft_enabled flags
    
    Two #defines IRQS_ENABLED and IRQS_DISABLED are added to be used when
    updating paca->soft_enabled. Replace the hardcoded values used when
    updating paca->soft_enabled with IRQ_(EN|DIS)ABLED #define. No logic
    change.
    
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 941c2a3f231b..68484d77a3cb 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -873,7 +873,7 @@ static inline void kvmppc_fix_ee_before_entry(void)
 
 	/* Only need to enable IRQs by hard enabling them after this */
 	local_paca->irq_happened = 0;
-	local_paca->soft_enabled = 1;
+	local_paca->soft_enabled = IRQS_ENABLED;
 #endif
 }
 

commit ded13fc11b71fd1351e57c68a130d89a0285f1b6
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Nov 22 14:38:53 2017 +1100

    KVM: PPC: Book3S HV: Fix migration and HPT resizing of HPT guests on radix hosts
    
    This fixes two errors that prevent a guest using the HPT MMU from
    successfully migrating to a POWER9 host in radix MMU mode, or resizing
    its HPT when running on a radix host.
    
    The first bug was that commit 8dc6cca556e4 ("KVM: PPC: Book3S HV:
    Don't rely on host's page size information", 2017-09-11) missed two
    uses of hpte_base_page_size(), one in the HPT rehashing code and
    one in kvm_htab_write() (which is used on the destination side in
    migrating a HPT guest).  Instead we use kvmppc_hpte_base_page_shift().
    Having the shift count means that we can use left and right shifts
    instead of multiplication and division in a few places.
    
    Along the way, this adds a check in kvm_htab_write() to ensure that the
    page size encoding in the incoming HPTEs is recognized, and if not
    return an EINVAL error to userspace.
    
    The second bug was that kvm_htab_write was performing some but not all
    of the functions of kvmhv_setup_mmu(), resulting in the destination VM
    being left in radix mode as far as the hardware is concerned.  The
    simplest fix for now is make kvm_htab_write() call
    kvmppc_setup_partition_table() like kvmppc_hv_setup_htab_rma() does.
    In future it would be better to refactor the code more extensively
    to remove the duplication.
    
    Fixes: 8dc6cca556e4 ("KVM: PPC: Book3S HV: Don't rely on host's page size information")
    Fixes: 7a84084c6054 ("KVM: PPC: Book3S HV: Set partition table rather than SDR1 on POWER9")
    Reported-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Tested-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 96753f3aac6d..941c2a3f231b 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -180,6 +180,7 @@ extern void kvm_spapr_tce_release_iommu_group(struct kvm *kvm,
 		struct iommu_group *grp);
 extern int kvmppc_switch_mmu_to_hpt(struct kvm *kvm);
 extern int kvmppc_switch_mmu_to_radix(struct kvm *kvm);
+extern void kvmppc_setup_partition_table(struct kvm *kvm);
 
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce_64 *args);

commit 18c3640cefc7f1c6986b7be48f5013a8d5e394cb
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Sep 13 16:00:10 2017 +1000

    KVM: PPC: Book3S HV: Add infrastructure for running HPT guests on radix host
    
    This sets up the machinery for switching a guest between HPT (hashed
    page table) and radix MMU modes, so that in future we can run a HPT
    guest on a radix host on POWER9 machines.
    
    * The KVM_PPC_CONFIGURE_V3_MMU ioctl can now specify either HPT or
      radix mode, on a radix host.
    
    * The KVM_CAP_PPC_MMU_HASH_V3 capability now returns 1 on POWER9
      with HV KVM on a radix host.
    
    * The KVM_PPC_GET_SMMU_INFO returns information about the HPT MMU on a
      radix host.
    
    * The KVM_PPC_ALLOCATE_HTAB ioctl on a radix host will switch the
      guest to HPT mode and allocate a HPT.
    
    * For simplicity, we now allocate the rmap array for each memslot,
      even on a radix host, since it will be needed if the guest switches
      to HPT mode.
    
    * Since we cannot yet run a HPT guest on a radix host, the KVM_RUN
      ioctl will return an EINVAL error in that case.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index ba5fadd6f3c9..96753f3aac6d 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -168,6 +168,7 @@ extern int kvmppc_allocate_hpt(struct kvm_hpt_info *info, u32 order);
 extern void kvmppc_set_hpt(struct kvm *kvm, struct kvm_hpt_info *info);
 extern long kvmppc_alloc_reset_hpt(struct kvm *kvm, int order);
 extern void kvmppc_free_hpt(struct kvm_hpt_info *info);
+extern void kvmppc_rmap_reset(struct kvm *kvm);
 extern long kvmppc_prepare_vrma(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);
 extern void kvmppc_map_vrma(struct kvm_vcpu *vcpu,
@@ -177,6 +178,8 @@ extern long kvm_spapr_tce_attach_iommu_group(struct kvm *kvm, int tablefd,
 		struct iommu_group *grp);
 extern void kvm_spapr_tce_release_iommu_group(struct kvm *kvm,
 		struct iommu_group *grp);
+extern int kvmppc_switch_mmu_to_hpt(struct kvm *kvm);
+extern int kvmppc_switch_mmu_to_radix(struct kvm *kvm);
 
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce_64 *args);

commit 3c313524605a6afd8207448a8e9967f5e8cba734
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Feb 6 13:24:41 2017 +1100

    KVM: PPC: Book3S HV: Allow userspace to set the desired SMT mode
    
    This allows userspace to set the desired virtual SMT (simultaneous
    multithreading) mode for a VM, that is, the number of VCPUs that
    get assigned to each virtual core.  Previously, the virtual SMT mode
    was fixed to the number of threads per subcore, and if userspace
    wanted to have fewer vcpus per vcore, then it would achieve that by
    using a sparse CPU numbering.  This had the disadvantage that the
    vcpu numbers can get quite large, particularly for SMT1 guests on
    a POWER8 with 8 threads per core.  With this patch, userspace can
    set its desired virtual SMT mode and then use contiguous vcpu
    numbering.
    
    On POWER8, where the threading mode is "strict", the virtual SMT mode
    must be less than or equal to the number of threads per subcore.  On
    POWER9, which implements a "loose" threading mode, the virtual SMT
    mode can be any power of 2 between 1 and 8, even though there is
    effectively one thread per subcore, since the threads are independent
    and can all be in different partitions.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index e0d88c38602b..ba5fadd6f3c9 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -315,6 +315,8 @@ struct kvmppc_ops {
 					struct irq_bypass_producer *);
 	int (*configure_mmu)(struct kvm *kvm, struct kvm_ppc_mmuv3_cfg *cfg);
 	int (*get_rmmu_info)(struct kvm *kvm, struct kvm_ppc_rmmu_info *info);
+	int (*set_smt_mode)(struct kvm *kvm, unsigned long mode,
+			    unsigned long flags);
 };
 
 extern struct kvmppc_ops *kvmppc_hv_ops;

commit fb7dcf723dd2cb1d5d8f2f49c3023130938848e3
Merge: db4b0dfab7b0 5af50993850a
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Apr 28 08:23:16 2017 +1000

    Merge remote-tracking branch 'remotes/powerpc/topic/xive' into kvm-ppc-next
    
    This merges in the powerpc topic/xive branch to bring in the code for
    the in-kernel XICS interrupt controller emulation to use the new XIVE
    (eXternal Interrupt Virtualization Engine) hardware in the POWER9 chip
    directly, rather than via a XICS emulation in firmware.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

commit 5af50993850a48ba749b122173d789ea90976c72
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:56 2017 +1000

    KVM: PPC: Book3S HV: Native usage of the XIVE interrupt controller
    
    This patch makes KVM capable of using the XIVE interrupt controller
    to provide the standard PAPR "XICS" style hypercalls. It is necessary
    for proper operations when the host uses XIVE natively.
    
    This has been lightly tested on an actual system, including PCI
    pass-through with a TG3 device.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [mpe: Cleanup pr_xxx(), unsplit pr_xxx() strings, etc., fix build
     failures by adding KVM_XIVE which depends on KVM_XICS and XIVE, and
     adding empty stubs for the kvm_xive_xxx() routines, fixup subject,
     integrate fixes from Paul for building PR=y HV=n]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c3877992eff9..ed52b13d9ffb 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -225,6 +225,7 @@ int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq);
 extern int kvm_vm_ioctl_rtas_define_token(struct kvm *kvm, void __user *argp);
 extern int kvmppc_rtas_hcall(struct kvm_vcpu *vcpu);
 extern void kvmppc_rtas_tokens_free(struct kvm *kvm);
+
 extern int kvmppc_xics_set_xive(struct kvm *kvm, u32 irq, u32 server,
 				u32 priority);
 extern int kvmppc_xics_get_xive(struct kvm *kvm, u32 irq, u32 *server,
@@ -412,6 +413,14 @@ static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 	paca[cpu].kvm_hstate.xics_phys = (void __iomem *)addr;
 }
 
+static inline void kvmppc_set_xive_tima(int cpu,
+					unsigned long phys_addr,
+					void __iomem *virt_addr)
+{
+	paca[cpu].kvm_hstate.xive_tima_phys = (void __iomem *)phys_addr;
+	paca[cpu].kvm_hstate.xive_tima_virt = virt_addr;
+}
+
 static inline u32 kvmppc_get_xics_latch(void)
 {
 	u32 xirr;
@@ -442,6 +451,11 @@ static inline void __init kvm_cma_reserve(void)
 static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 {}
 
+static inline void kvmppc_set_xive_tima(int cpu,
+					unsigned long phys_addr,
+					void __iomem *virt_addr)
+{}
+
 static inline u32 kvmppc_get_xics_latch(void)
 {
 	return 0;
@@ -492,6 +506,10 @@ extern long kvmppc_deliver_irq_passthru(struct kvm_vcpu *vcpu, __be32 xirr,
 					struct kvmppc_irq_map *irq_map,
 					struct kvmppc_passthru_irqmap *pimap,
 					bool *again);
+
+extern int kvmppc_xics_set_irq(struct kvm *kvm, int irq_source_id, u32 irq,
+			       int level, bool line_status);
+
 extern int h_ipi_redirect;
 #else
 static inline struct kvmppc_passthru_irqmap *kvmppc_get_passthru_irqmap(
@@ -509,6 +527,60 @@ static inline int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd)
 	{ return 0; }
 #endif
 
+#ifdef CONFIG_KVM_XIVE
+/*
+ * Below the first "xive" is the "eXternal Interrupt Virtualization Engine"
+ * ie. P9 new interrupt controller, while the second "xive" is the legacy
+ * "eXternal Interrupt Vector Entry" which is the configuration of an
+ * interrupt on the "xics" interrupt controller on P8 and earlier. Those
+ * two function consume or produce a legacy "XIVE" state from the
+ * new "XIVE" interrupt controller.
+ */
+extern int kvmppc_xive_set_xive(struct kvm *kvm, u32 irq, u32 server,
+				u32 priority);
+extern int kvmppc_xive_get_xive(struct kvm *kvm, u32 irq, u32 *server,
+				u32 *priority);
+extern int kvmppc_xive_int_on(struct kvm *kvm, u32 irq);
+extern int kvmppc_xive_int_off(struct kvm *kvm, u32 irq);
+extern void kvmppc_xive_init_module(void);
+extern void kvmppc_xive_exit_module(void);
+
+extern int kvmppc_xive_connect_vcpu(struct kvm_device *dev,
+				    struct kvm_vcpu *vcpu, u32 cpu);
+extern void kvmppc_xive_cleanup_vcpu(struct kvm_vcpu *vcpu);
+extern int kvmppc_xive_set_mapped(struct kvm *kvm, unsigned long guest_irq,
+				  struct irq_desc *host_desc);
+extern int kvmppc_xive_clr_mapped(struct kvm *kvm, unsigned long guest_irq,
+				  struct irq_desc *host_desc);
+extern u64 kvmppc_xive_get_icp(struct kvm_vcpu *vcpu);
+extern int kvmppc_xive_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
+
+extern int kvmppc_xive_set_irq(struct kvm *kvm, int irq_source_id, u32 irq,
+			       int level, bool line_status);
+#else
+static inline int kvmppc_xive_set_xive(struct kvm *kvm, u32 irq, u32 server,
+				       u32 priority) { return -1; }
+static inline int kvmppc_xive_get_xive(struct kvm *kvm, u32 irq, u32 *server,
+				       u32 *priority) { return -1; }
+static inline int kvmppc_xive_int_on(struct kvm *kvm, u32 irq) { return -1; }
+static inline int kvmppc_xive_int_off(struct kvm *kvm, u32 irq) { return -1; }
+static inline void kvmppc_xive_init_module(void) { }
+static inline void kvmppc_xive_exit_module(void) { }
+
+static inline int kvmppc_xive_connect_vcpu(struct kvm_device *dev,
+					   struct kvm_vcpu *vcpu, u32 cpu) { return -EBUSY; }
+static inline void kvmppc_xive_cleanup_vcpu(struct kvm_vcpu *vcpu) { }
+static inline int kvmppc_xive_set_mapped(struct kvm *kvm, unsigned long guest_irq,
+					 struct irq_desc *host_desc) { return -ENODEV; }
+static inline int kvmppc_xive_clr_mapped(struct kvm *kvm, unsigned long guest_irq,
+					 struct irq_desc *host_desc) { return -ENODEV; }
+static inline u64 kvmppc_xive_get_icp(struct kvm_vcpu *vcpu) { return 0; }
+static inline int kvmppc_xive_set_icp(struct kvm_vcpu *vcpu, u64 icpval) { return -ENOENT; }
+
+static inline int kvmppc_xive_set_irq(struct kvm *kvm, int irq_source_id, u32 irq,
+				      int level, bool line_status) { return -ENODEV; }
+#endif /* CONFIG_KVM_XIVE */
+
 /*
  * Prototypes for functions called only from assembler code.
  * Having prototypes reduces sparse errors.
@@ -546,6 +618,8 @@ long kvmppc_h_clear_mod(struct kvm_vcpu *vcpu, unsigned long flags,
 long kvmppc_hpte_hv_fault(struct kvm_vcpu *vcpu, unsigned long addr,
                           unsigned long slb_v, unsigned int status, bool data);
 unsigned long kvmppc_rm_h_xirr(struct kvm_vcpu *vcpu);
+unsigned long kvmppc_rm_h_xirr_x(struct kvm_vcpu *vcpu);
+unsigned long kvmppc_rm_h_ipoll(struct kvm_vcpu *vcpu, unsigned long server);
 int kvmppc_rm_h_ipi(struct kvm_vcpu *vcpu, unsigned long server,
                     unsigned long mfrr);
 int kvmppc_rm_h_cppr(struct kvm_vcpu *vcpu, unsigned long cppr);

commit 121f80ba68f1a5779a36d7b3247206e60e0a7418
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Wed Mar 22 15:21:56 2017 +1100

    KVM: PPC: VFIO: Add in-kernel acceleration for VFIO
    
    This allows the host kernel to handle H_PUT_TCE, H_PUT_TCE_INDIRECT
    and H_STUFF_TCE requests targeted an IOMMU TCE table used for VFIO
    without passing them to user space which saves time on switching
    to user space and back.
    
    This adds H_PUT_TCE/H_PUT_TCE_INDIRECT/H_STUFF_TCE handlers to KVM.
    KVM tries to handle a TCE request in the real mode, if failed
    it passes the request to the virtual mode to complete the operation.
    If it a virtual mode handler fails, the request is passed to
    the user space; this is not expected to happen though.
    
    To avoid dealing with page use counters (which is tricky in real mode),
    this only accelerates SPAPR TCE IOMMU v2 clients which are required
    to pre-register the userspace memory. The very first TCE request will
    be handled in the VFIO SPAPR TCE driver anyway as the userspace view
    of the TCE table (iommu_table::it_userspace) is not allocated till
    the very first mapping happens and we cannot call vmalloc in real mode.
    
    If we fail to update a hardware IOMMU table unexpected reason, we just
    clear it and move on as there is nothing really we can do about it -
    for example, if we hot plug a VFIO device to a guest, existing TCE tables
    will be mirrored automatically to the hardware and there is no interface
    to report to the guest about possible failures.
    
    This adds new attribute - KVM_DEV_VFIO_GROUP_SET_SPAPR_TCE - to
    the VFIO KVM device. It takes a VFIO group fd and SPAPR TCE table fd
    and associates a physical IOMMU table with the SPAPR TCE table (which
    is a guest view of the hardware IOMMU table). The iommu_table object
    is cached and referenced so we do not have to look up for it in real mode.
    
    This does not implement the UNSET counterpart as there is no use for it -
    once the acceleration is enabled, the existing userspace won't
    disable it unless a VFIO container is destroyed; this adds necessary
    cleanup to the KVM_DEV_VFIO_GROUP_DEL handler.
    
    This advertises the new KVM_CAP_SPAPR_TCE_VFIO capability to the user
    space.
    
    This adds real mode version of WARN_ON_ONCE() as the generic version
    causes problems with rcu_sched. Since we testing what vmalloc_to_phys()
    returns in the code, this also adds a check for already existing
    vmalloc_to_phys() call in kvmppc_rm_h_put_tce_indirect().
    
    This finally makes use of vfio_external_user_iommu_id() which was
    introduced quite some time ago and was considered for removal.
    
    Tests show that this patch increases transmission speed from 220MB/s
    to 750..1020MB/s on 10Gb network (Chelsea CXGB3 10Gb ethernet card).
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Acked-by: Alex Williamson <alex.williamson@redhat.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 4d079a29eae2..5885d327c025 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -173,6 +173,10 @@ extern long kvmppc_prepare_vrma(struct kvm *kvm,
 extern void kvmppc_map_vrma(struct kvm_vcpu *vcpu,
 			struct kvm_memory_slot *memslot, unsigned long porder);
 extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
+extern long kvm_spapr_tce_attach_iommu_group(struct kvm *kvm, int tablefd,
+		struct iommu_group *grp);
+extern void kvm_spapr_tce_release_iommu_group(struct kvm *kvm,
+		struct iommu_group *grp);
 
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce_64 *args);

commit b1af23d836f811137d504d14d4cbdd01929dec34
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Wed Mar 22 15:21:55 2017 +1100

    KVM: PPC: iommu: Unify TCE checking
    
    This reworks helpers for checking TCE update parameters in way they
    can be used in KVM.
    
    This should cause no behavioral change.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 2e66800a870b..4d079a29eae2 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -178,8 +178,10 @@ extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce_64 *args);
 extern struct kvmppc_spapr_tce_table *kvmppc_find_table(
 		struct kvm *kvm, unsigned long liobn);
-extern long kvmppc_ioba_validate(struct kvmppc_spapr_tce_table *stt,
-		unsigned long ioba, unsigned long npages);
+#define kvmppc_ioba_validate(stt, ioba, npages)                         \
+		(iommu_tce_check_ioba((stt)->page_shift, (stt)->offset, \
+				(stt)->size, (ioba), (npages)) ?        \
+				H_PARAMETER : H_SUCCESS)
 extern long kvmppc_tce_validate(struct kvmppc_spapr_tce_table *tt,
 		unsigned long tce);
 extern long kvmppc_gpa_to_ua(struct kvm *kvm, unsigned long gpa,

commit 503bfcbe18576a79be0bc5173b23b530845e704a
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Wed Mar 22 15:21:53 2017 +1100

    KVM: PPC: Pass kvm* to kvmppc_find_table()
    
    The guest view TCE tables are per KVM anyway (not per VCPU) so pass kvm*
    there. This will be used in the following patches where we will be
    attaching VFIO containers to LIOBNs via ioctl() to KVM (rather than
    to VCPU).
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index bbecec4e753a..2e66800a870b 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -177,7 +177,7 @@ extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce_64 *args);
 extern struct kvmppc_spapr_tce_table *kvmppc_find_table(
-		struct kvm_vcpu *vcpu, unsigned long liobn);
+		struct kvm *kvm, unsigned long liobn);
 extern long kvmppc_ioba_validate(struct kvmppc_spapr_tce_table *stt,
 		unsigned long ioba, unsigned long npages);
 extern long kvmppc_tce_validate(struct kvmppc_spapr_tce_table *tt,

commit 6f63e81bda98cbb549b01faf978884692ded438d
Author: Bin Lu <lblulb@linux.vnet.ibm.com>
Date:   Tue Feb 21 21:12:36 2017 +0800

    KVM: PPC: Book3S: Add MMIO emulation for FP and VSX instructions
    
    This patch provides the MMIO load/store emulation for instructions
    of 'double & vector unsigned char & vector signed char & vector
    unsigned short & vector signed short & vector unsigned int & vector
    signed int & vector double '.
    
    The instructions that this adds emulation for are:
    
    - ldx, ldux, lwax,
    - lfs, lfsx, lfsu, lfsux, lfd, lfdx, lfdu, lfdux,
    - stfs, stfsx, stfsu, stfsux, stfd, stfdx, stfdu, stfdux, stfiwx,
    - lxsdx, lxsspx, lxsiwax, lxsiwzx, lxvd2x, lxvw4x, lxvdsx,
    - stxsdx, stxsspx, stxsiwx, stxvd2x, stxvw4x
    
    [paulus@ozlabs.org - some cleanups, fixes and rework, make it
     compile for Book E, fix build when PR KVM is built in]
    
    Signed-off-by: Bin Lu <lblulb@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 4f1f22fc0ea1..bbecec4e753a 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -78,9 +78,15 @@ extern int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 extern int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
                                unsigned int rt, unsigned int bytes,
 			       int is_default_endian);
+extern int kvmppc_handle_vsx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+				unsigned int rt, unsigned int bytes,
+			int is_default_endian, int mmio_sign_extend);
 extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 			       u64 val, unsigned int bytes,
 			       int is_default_endian);
+extern int kvmppc_handle_vsx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+				int rs, unsigned int bytes,
+				int is_default_endian);
 
 extern int kvmppc_load_last_inst(struct kvm_vcpu *vcpu,
 				 enum instruction_type type, u32 *inst);
@@ -243,6 +249,7 @@ union kvmppc_one_reg {
 	u64	dval;
 	vector128 vval;
 	u64	vsxval[2];
+	u32	vsx32val[4];
 	struct {
 		u64	addr;
 		u64	length;

commit 307d927967007acef98cfd3f0639c7a4bf234ede
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Mar 22 21:02:08 2017 +1100

    KVM: PPC: Provide functions for queueing up FP/VEC/VSX unavailable interrupts
    
    This provides functions that can be used for generating interrupts
    indicating that a given functional unit (floating point, vector, or
    VSX) is unavailable.  These functions will be used in instruction
    emulation code.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index dd11c4c8c56a..4f1f22fc0ea1 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -132,6 +132,9 @@ extern void kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_prepare_to_enter(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_pending_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_program(struct kvm_vcpu *vcpu, ulong flags);
+extern void kvmppc_core_queue_fpunavail(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_queue_vec_unavail(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_queue_vsx_unavail(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_dequeue_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_external(struct kvm_vcpu *vcpu,

commit d381d7caf812f7aa9f05cfeb858c9004ac654412
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:54 2017 +1000

    powerpc: Consolidate variants of real-mode MMIOs
    
    We have all sort of variants of MMIO accessors for the real mode
    instructions. This creates a clean set of accessors based on
    Linux normal naming conventions, replacing all occurrences of
    the old ones in the tree.
    
    I have purposefully removed the "out/in" variants in favor of
    only including __raw variants. Any code using these is already
    pretty much hand tuned to operate in a very specific environment.
    I've fixed up the 2 users (only one of them actually needed
    a barrier in the first place).
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 0c418655ee1f..c3877992eff9 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -409,7 +409,7 @@ struct openpic;
 extern void kvm_cma_reserve(void) __init;
 static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 {
-	paca[cpu].kvm_hstate.xics_phys = addr;
+	paca[cpu].kvm_hstate.xics_phys = (void __iomem *)addr;
 }
 
 static inline u32 kvmppc_get_xics_latch(void)

commit f50d6bd3442c3c1345b0da0885ac9d81fef2bb8e
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:53 2017 +1000

    powerpc/kvm: Remove obsolete kvm_vm_ioctl_xics_irq declaration
    
    The function doesn't exist anymore
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index bfef1ae66ffd..0c418655ee1f 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -478,7 +478,6 @@ extern void kvmppc_free_host_rm_ops(void);
 extern void kvmppc_free_pimap(struct kvm *kvm);
 extern int kvmppc_xics_rm_complete(struct kvm_vcpu *vcpu, u32 hcall);
 extern void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu);
-extern int kvm_vm_ioctl_xics_irq(struct kvm *kvm, struct kvm_irq_level *args);
 extern int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd);
 extern u64 kvmppc_xics_get_icp(struct kvm_vcpu *vcpu);
 extern int kvmppc_xics_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
@@ -506,9 +505,6 @@ static inline int kvmppc_xics_rm_complete(struct kvm_vcpu *vcpu, u32 hcall)
 static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
 	{ return 0; }
 static inline void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu) { }
-static inline int kvm_vm_ioctl_xics_irq(struct kvm *kvm,
-					struct kvm_irq_level *args)
-	{ return -ENOTTY; }
 static inline int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd)
 	{ return 0; }
 #endif

commit 936774cd3fc8152e36c18a129fa940c42c177d14
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:52 2017 +1000

    powerpc/kvm: Make kvmppc_xics_create_icp static
    
    It's only used within the same file it's defined
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index dd11c4c8c56a..bfef1ae66ffd 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -478,7 +478,6 @@ extern void kvmppc_free_host_rm_ops(void);
 extern void kvmppc_free_pimap(struct kvm *kvm);
 extern int kvmppc_xics_rm_complete(struct kvm_vcpu *vcpu, u32 hcall);
 extern void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu);
-extern int kvmppc_xics_create_icp(struct kvm_vcpu *vcpu, unsigned long server);
 extern int kvm_vm_ioctl_xics_irq(struct kvm *kvm, struct kvm_irq_level *args);
 extern int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd);
 extern u64 kvmppc_xics_get_icp(struct kvm_vcpu *vcpu);
@@ -507,9 +506,6 @@ static inline int kvmppc_xics_rm_complete(struct kvm_vcpu *vcpu, u32 hcall)
 static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
 	{ return 0; }
 static inline void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu) { }
-static inline int kvmppc_xics_create_icp(struct kvm_vcpu *vcpu,
-					 unsigned long server)
-	{ return -EINVAL; }
 static inline int kvm_vm_ioctl_xics_irq(struct kvm *kvm,
 					struct kvm_irq_level *args)
 	{ return -ENOTTY; }

commit 5e9859699aba74c0e297645e7d1734cd4b964de7
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Dec 20 16:49:05 2016 +1100

    KVM: PPC: Book3S HV: Outline of KVM-HV HPT resizing implementation
    
    This adds a not yet working outline of the HPT resizing PAPR
    extension.  Specifically it adds the necessary ioctl() functions,
    their basic steps, the work function which will handle preparation for
    the resize, and synchronization between these, the guest page fault
    path and guest HPT update path.
    
    The actual guts of the implementation isn't here yet, so for now the
    calls will always fail.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index cf3ef8d75910..dd11c4c8c56a 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -215,6 +215,10 @@ extern void kvmppc_bookehv_exit(void);
 extern int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu);
 
 extern int kvm_vm_ioctl_get_htab_fd(struct kvm *kvm, struct kvm_get_htab_fd *);
+extern long kvm_vm_ioctl_resize_hpt_prepare(struct kvm *kvm,
+					    struct kvm_ppc_resize_hpt *rhpt);
+extern long kvm_vm_ioctl_resize_hpt_commit(struct kvm *kvm,
+					   struct kvm_ppc_resize_hpt *rhpt);
 
 int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq);
 

commit f98a8bf9ee201b7e22fc05e27150b1e481d4949f
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Dec 20 16:49:03 2016 +1100

    KVM: PPC: Book3S HV: Allow KVM_PPC_ALLOCATE_HTAB ioctl() to change HPT size
    
    The KVM_PPC_ALLOCATE_HTAB ioctl() is used to set the size of hashed page
    table (HPT) that userspace expects a guest VM to have, and is also used to
    clear that HPT when necessary (e.g. guest reboot).
    
    At present, once the ioctl() is called for the first time, the HPT size can
    never be changed thereafter - it will be cleared but always sized as from
    the first call.
    
    With upcoming HPT resize implementation, we're going to need to allow
    userspace to resize the HPT at reset (to change it back to the default size
    if the guest changed it).
    
    So, we need to allow this ioctl() to change the HPT size.
    
    This patch also updates Documentation/virtual/kvm/api.txt to reflect
    the new behaviour.  In fact the documentation was already slightly
    incorrect since 572abd5 "KVM: PPC: Book3S HV: Don't fall back to
    smaller HPT size in allocation ioctl"
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index ba61dec72089..cf3ef8d75910 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -157,7 +157,7 @@ extern void kvmppc_map_magic(struct kvm_vcpu *vcpu);
 
 extern int kvmppc_allocate_hpt(struct kvm_hpt_info *info, u32 order);
 extern void kvmppc_set_hpt(struct kvm *kvm, struct kvm_hpt_info *info);
-extern long kvmppc_alloc_reset_hpt(struct kvm *kvm, u32 *htab_orderp);
+extern long kvmppc_alloc_reset_hpt(struct kvm *kvm, int order);
 extern void kvmppc_free_hpt(struct kvm_hpt_info *info);
 extern long kvmppc_prepare_vrma(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);

commit aae0777f1e8224b4fbb78b2c692060852ee750c8
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Dec 20 16:49:02 2016 +1100

    KVM: PPC: Book3S HV: Split HPT allocation from activation
    
    Currently, kvmppc_alloc_hpt() both allocates a new hashed page table (HPT)
    and sets it up as the active page table for a VM.  For the upcoming HPT
    resize implementation we're going to want to allocate HPTs separately from
    activating them.
    
    So, split the allocation itself out into kvmppc_allocate_hpt() and perform
    the activation with a new kvmppc_set_hpt() function.  Likewise we split
    kvmppc_free_hpt(), which just frees the HPT, from kvmppc_release_hpt()
    which unsets it as an active HPT, then frees it.
    
    We also move the logic to fall back to smaller HPT sizes if the first try
    fails into the single caller which used that behaviour,
    kvmppc_hv_setup_htab_rma().  This introduces a slight semantic change, in
    that previously if the initial attempt at CMA allocation failed, we would
    fall back to attempting smaller sizes with the page allocator.  Now, we
    try first CMA, then the page allocator at each size.  As far as I can tell
    this change should be harmless.
    
    To match, we make kvmppc_free_hpt() just free the actual HPT itself.  The
    call to kvmppc_free_lpid() that was there, we move to the single caller.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 6fad1f12e9ec..ba61dec72089 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -155,9 +155,10 @@ extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 extern int kvmppc_kvm_pv(struct kvm_vcpu *vcpu);
 extern void kvmppc_map_magic(struct kvm_vcpu *vcpu);
 
-extern long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp);
+extern int kvmppc_allocate_hpt(struct kvm_hpt_info *info, u32 order);
+extern void kvmppc_set_hpt(struct kvm *kvm, struct kvm_hpt_info *info);
 extern long kvmppc_alloc_reset_hpt(struct kvm *kvm, u32 *htab_orderp);
-extern void kvmppc_free_hpt(struct kvm *kvm);
+extern void kvmppc_free_hpt(struct kvm_hpt_info *info);
 extern long kvmppc_prepare_vrma(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);
 extern void kvmppc_map_vrma(struct kvm_vcpu *vcpu,

commit db9a290d9c3c596e5325e2a42133594435e5de46
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Dec 20 16:48:59 2016 +1100

    KVM: PPC: Book3S HV: Rename kvm_alloc_hpt() for clarity
    
    The difference between kvm_alloc_hpt() and kvmppc_alloc_hpt() is not at
    all obvious from the name.  In practice kvmppc_alloc_hpt() allocates an HPT
    by whatever means, and calls kvm_alloc_hpt() which will attempt to allocate
    it with CMA only.
    
    To make this less confusing, rename kvm_alloc_hpt() to kvm_alloc_hpt_cma().
    Similarly, kvm_release_hpt() is renamed kvm_free_hpt_cma().
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 48c760f89590..6fad1f12e9ec 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -186,8 +186,8 @@ extern long kvmppc_h_stuff_tce(struct kvm_vcpu *vcpu,
 		unsigned long tce_value, unsigned long npages);
 extern long kvmppc_h_get_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
 			     unsigned long ioba);
-extern struct page *kvm_alloc_hpt(unsigned long nr_pages);
-extern void kvm_release_hpt(struct page *page, unsigned long nr_pages);
+extern struct page *kvm_alloc_hpt_cma(unsigned long nr_pages);
+extern void kvm_free_hpt_cma(struct page *page, unsigned long nr_pages);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
 extern void kvmppc_core_destroy_vm(struct kvm *kvm);
 extern void kvmppc_core_free_memslot(struct kvm *kvm,

commit c92701322711682de89b2bd0f32affad040b6e86
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 30 21:21:41 2017 +1100

    KVM: PPC: Book3S HV: Add userspace interfaces for POWER9 MMU
    
    This adds two capabilities and two ioctls to allow userspace to
    find out about and configure the POWER9 MMU in a guest.  The two
    capabilities tell userspace whether KVM can support a guest using
    the radix MMU, or using the hashed page table (HPT) MMU with a
    process table and segment tables.  (Note that the MMUs in the
    POWER9 processor cores do not use the process and segment tables
    when in HPT mode, but the nest MMU does).
    
    The KVM_PPC_CONFIGURE_V3_MMU ioctl allows userspace to specify
    whether a guest will use the radix MMU or the HPT MMU, and to
    specify the size and location (in guest space) of the process
    table.
    
    The KVM_PPC_GET_RMMU_INFO ioctl gives userspace information about
    the radix MMU.  It returns a list of supported radix tree geometries
    (base page size and number of bits indexed at each level of the
    radix tree) and the encoding used to specify the various page
    sizes for the TLB invalidate entry instruction.
    
    Initially, both capabilities return 0 and the ioctls return -EINVAL,
    until the necessary infrastructure for them to operate correctly
    is added.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 2da67bf1f2ec..48c760f89590 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -291,6 +291,8 @@ struct kvmppc_ops {
 				       struct irq_bypass_producer *);
 	void (*irq_bypass_del_producer)(struct irq_bypass_consumer *,
 					struct irq_bypass_producer *);
+	int (*configure_mmu)(struct kvm *kvm, struct kvm_ppc_mmuv3_cfg *cfg);
+	int (*get_rmmu_info)(struct kvm *kvm, struct kvm_ppc_rmmu_info *info);
 };
 
 extern struct kvmppc_ops *kvmppc_hv_ops;

commit e34af7849014f1d80899b811cf9021588cb8dd88
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Dec 1 14:03:46 2016 +1100

    KVM: PPC: Book3S: Move prototypes for KVM functions into kvm_ppc.h
    
    This moves the prototypes for functions that are only called from
    assembler code out of asm/asm-prototypes.h into asm/kvm_ppc.h.
    The prototypes were added in commit ebe4535fbe7a ("KVM: PPC:
    Book3S HV: sparse: prototypes for functions called from assembler",
    2016-10-10), but given that the functions are KVM functions,
    having them in a KVM header will be better for long-term
    maintenance.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index a5b94bed1423..2da67bf1f2ec 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -510,6 +510,48 @@ static inline int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd)
 	{ return 0; }
 #endif
 
+/*
+ * Prototypes for functions called only from assembler code.
+ * Having prototypes reduces sparse errors.
+ */
+long kvmppc_rm_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
+			 unsigned long ioba, unsigned long tce);
+long kvmppc_rm_h_put_tce_indirect(struct kvm_vcpu *vcpu,
+				  unsigned long liobn, unsigned long ioba,
+				  unsigned long tce_list, unsigned long npages);
+long kvmppc_rm_h_stuff_tce(struct kvm_vcpu *vcpu,
+			   unsigned long liobn, unsigned long ioba,
+			   unsigned long tce_value, unsigned long npages);
+long int kvmppc_rm_h_confer(struct kvm_vcpu *vcpu, int target,
+                            unsigned int yield_count);
+long kvmppc_h_random(struct kvm_vcpu *vcpu);
+void kvmhv_commence_exit(int trap);
+long kvmppc_realmode_machine_check(struct kvm_vcpu *vcpu);
+void kvmppc_subcore_enter_guest(void);
+void kvmppc_subcore_exit_guest(void);
+long kvmppc_realmode_hmi_handler(void);
+long kvmppc_h_enter(struct kvm_vcpu *vcpu, unsigned long flags,
+                    long pte_index, unsigned long pteh, unsigned long ptel);
+long kvmppc_h_remove(struct kvm_vcpu *vcpu, unsigned long flags,
+                     unsigned long pte_index, unsigned long avpn);
+long kvmppc_h_bulk_remove(struct kvm_vcpu *vcpu);
+long kvmppc_h_protect(struct kvm_vcpu *vcpu, unsigned long flags,
+                      unsigned long pte_index, unsigned long avpn,
+                      unsigned long va);
+long kvmppc_h_read(struct kvm_vcpu *vcpu, unsigned long flags,
+                   unsigned long pte_index);
+long kvmppc_h_clear_ref(struct kvm_vcpu *vcpu, unsigned long flags,
+                        unsigned long pte_index);
+long kvmppc_h_clear_mod(struct kvm_vcpu *vcpu, unsigned long flags,
+                        unsigned long pte_index);
+long kvmppc_hpte_hv_fault(struct kvm_vcpu *vcpu, unsigned long addr,
+                          unsigned long slb_v, unsigned int status, bool data);
+unsigned long kvmppc_rm_h_xirr(struct kvm_vcpu *vcpu);
+int kvmppc_rm_h_ipi(struct kvm_vcpu *vcpu, unsigned long server,
+                    unsigned long mfrr);
+int kvmppc_rm_h_cppr(struct kvm_vcpu *vcpu, unsigned long cppr);
+int kvmppc_rm_h_eoi(struct kvm_vcpu *vcpu, unsigned long xirr);
+
 /*
  * Host-side operations we want to set up while running in real
  * mode in the guest operating on the xics.

commit f725758b899f11cac6b375e332e092dc855b9210
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Nov 18 09:02:08 2016 +1100

    KVM: PPC: Book3S HV: Use OPAL XICS emulation on POWER9
    
    POWER9 includes a new interrupt controller, called XIVE, which is
    quite different from the XICS interrupt controller on POWER7 and
    POWER8 machines.  KVM-HV accesses the XICS directly in several places
    in order to send and clear IPIs and handle interrupts from PCI
    devices being passed through to the guest.
    
    In order to make the transition to XIVE easier, OPAL firmware will
    include an emulation of XICS on top of XIVE.  Access to the emulated
    XICS is via OPAL calls.  The one complication is that the EOI
    (end-of-interrupt) function can now return a value indicating that
    another interrupt is pending; in this case, the XIVE will not signal
    an interrupt in hardware to the CPU, and software is supposed to
    acknowledge the new interrupt without waiting for another interrupt
    to be delivered in hardware.
    
    This adapts KVM-HV to use the OPAL calls on machines where there is
    no XICS hardware.  When there is no XICS, we look for a device-tree
    node with "ibm,opal-intc" in its compatible property, which is how
    OPAL indicates that it provides XICS emulation.
    
    In order to handle the EOI return value, kvmppc_read_intr() has
    become kvmppc_read_one_intr(), with a boolean variable passed by
    reference which can be set by the EOI functions to indicate that
    another interrupt is pending.  The new kvmppc_read_intr() keeps
    calling kvmppc_read_one_intr() until there are no more interrupts
    to process.  The return value from kvmppc_read_intr() is the
    largest non-zero value of the returns from kvmppc_read_one_intr().
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index f6e49640dbe1..a5b94bed1423 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -483,9 +483,10 @@ extern void kvmppc_xics_set_mapped(struct kvm *kvm, unsigned long guest_irq,
 				   unsigned long host_irq);
 extern void kvmppc_xics_clr_mapped(struct kvm *kvm, unsigned long guest_irq,
 				   unsigned long host_irq);
-extern long kvmppc_deliver_irq_passthru(struct kvm_vcpu *vcpu, u32 xirr,
-				 struct kvmppc_irq_map *irq_map,
-				 struct kvmppc_passthru_irqmap *pimap);
+extern long kvmppc_deliver_irq_passthru(struct kvm_vcpu *vcpu, __be32 xirr,
+					struct kvmppc_irq_map *irq_map,
+					struct kvmppc_passthru_irqmap *pimap,
+					bool *again);
 extern int h_ipi_redirect;
 #else
 static inline struct kvmppc_passthru_irqmap *kvmppc_get_passthru_irqmap(

commit 5d375199ea963fa2a972eae9c7d83db36ed37082
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Aug 19 15:35:56 2016 +1000

    KVM: PPC: Book3S HV: Set server for passed-through interrupts
    
    When a guest has a PCI pass-through device with an interrupt, it
    will direct the interrupt to a particular guest VCPU.  In fact the
    physical interrupt might arrive on any CPU, and then get
    delivered to the target VCPU in the emulated XICS (guest interrupt
    controller), and eventually delivered to the target VCPU.
    
    Now that we have code to handle device interrupts in real mode
    without exiting to the host kernel, there is an advantage to having
    the device interrupt arrive on the same sub(core) as the target
    VCPU is running on.  In this situation, the interrupt can be
    delivered to the target VCPU without any exit to the host kernel
    (using a hypervisor doorbell interrupt between threads if
    necessary).
    
    This patch aims to get passed-through device interrupts arriving
    on the correct core by setting the interrupt server in the real
    hardware XICS for the interrupt to the first thread in the (sub)core
    where its target VCPU is running.  We do this in the real-mode H_EOI
    code because the H_EOI handler already needs to look at the
    emulated ICS state for the interrupt (whereas the H_XIRR handler
    doesn't), and we know we are running in the target VCPU context
    at that point.
    
    We set the server CPU in hardware using an OPAL call, regardless of
    what the IRQ affinity mask for the interrupt says, and without
    updating the affinity mask.  This amounts to saying that when an
    interrupt is passed through to a guest, as a matter of policy we
    allow the guest's affinity for the interrupt to override the host's.
    
    This is inspired by an earlier patch from Suresh Warrier, although
    none of this code came from that earlier patch.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 97b9bad9ec49..f6e49640dbe1 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -479,6 +479,10 @@ extern int kvmppc_xics_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
 extern int kvmppc_xics_connect_vcpu(struct kvm_device *dev,
 			struct kvm_vcpu *vcpu, u32 cpu);
 extern void kvmppc_xics_ipi_action(void);
+extern void kvmppc_xics_set_mapped(struct kvm *kvm, unsigned long guest_irq,
+				   unsigned long host_irq);
+extern void kvmppc_xics_clr_mapped(struct kvm *kvm, unsigned long guest_irq,
+				   unsigned long host_irq);
 extern long kvmppc_deliver_irq_passthru(struct kvm_vcpu *vcpu, u32 xirr,
 				 struct kvmppc_irq_map *irq_map,
 				 struct kvmppc_passthru_irqmap *pimap);

commit 644abbb254b1ab171f777431b23e6fb5879599d0
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Fri Aug 19 15:35:54 2016 +1000

    KVM: PPC: Book3S HV: Tunable to disable KVM IRQ bypass
    
    Add a  module parameter kvm_irq_bypass for kvm_hv.ko to
    disable IRQ bypass for passthrough interrupts. The default
    value of this tunable is 1 - that is enable the feature.
    
    Since the tunable is used by built-in kernel code, we use
    the module_param_cb macro to achieve this.
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index e0ada3138649..97b9bad9ec49 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -461,7 +461,7 @@ static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
 static inline struct kvmppc_passthru_irqmap *kvmppc_get_passthru_irqmap(
 				struct kvm *kvm)
 {
-	if (kvm)
+	if (kvm && kvm_irq_bypass)
 		return kvm->arch.pimap;
 	return NULL;
 }

commit f7af5209b87c592aad81da65bd104241aa43d36a
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Fri Aug 19 15:35:52 2016 +1000

    KVM: PPC: Book3S HV: Complete passthrough interrupt in host
    
    In existing real mode ICP code, when updating the virtual ICP
    state, if there is a required action that cannot be completely
    handled in real mode, as for instance, a VCPU needs to be woken
    up, flags are set in the ICP to indicate the required action.
    This is checked when returning from hypercalls to decide whether
    the call needs switch back to the host where the action can be
    performed in virtual mode. Note that if h_ipi_redirect is enabled,
    real mode code will first try to message a free host CPU to
    complete this job instead of returning the host to do it ourselves.
    
    Currently, the real mode PCI passthrough interrupt handling code
    checks if any of these flags are set and simply returns to the host.
    This is not good enough as the trap value (0x500) is treated as an
    external interrupt by the host code. It is only when the trap value
    is a hypercall that the host code searches for and acts on unfinished
    work by calling kvmppc_xics_rm_complete.
    
    This patch introduces a special trap BOOK3S_INTERRUPT_HV_RM_HARD
    which is returned by KVM if there is unfinished business to be
    completed in host virtual mode after handling a PCI passthrough
    interrupt. The host checks for this special interrupt condition
    and calls into the kvmppc_xics_rm_complete, which is made an
    exported function for this reason.
    
    [paulus@ozlabs.org - moved logic to set r12 to BOOK3S_INTERRUPT_HV_RM_HARD
     in book3s_hv_rmhandlers.S into the end of kvmppc_check_wake_reason.]
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 4299a1f79a91..e0ada3138649 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -469,6 +469,7 @@ static inline struct kvmppc_passthru_irqmap *kvmppc_get_passthru_irqmap(
 extern void kvmppc_alloc_host_rm_ops(void);
 extern void kvmppc_free_host_rm_ops(void);
 extern void kvmppc_free_pimap(struct kvm *kvm);
+extern int kvmppc_xics_rm_complete(struct kvm_vcpu *vcpu, u32 hcall);
 extern void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu);
 extern int kvmppc_xics_create_icp(struct kvm_vcpu *vcpu, unsigned long server);
 extern int kvm_vm_ioctl_xics_irq(struct kvm *kvm, struct kvm_irq_level *args);
@@ -489,6 +490,8 @@ static inline struct kvmppc_passthru_irqmap *kvmppc_get_passthru_irqmap(
 static inline void kvmppc_alloc_host_rm_ops(void) {};
 static inline void kvmppc_free_host_rm_ops(void) {};
 static inline void kvmppc_free_pimap(struct kvm *kvm) {};
+static inline int kvmppc_xics_rm_complete(struct kvm_vcpu *vcpu, u32 hcall)
+	{ return 0; }
 static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
 	{ return 0; }
 static inline void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu) { }

commit e3c13e56a4717ee334837a20c596e527eb6355e1
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Fri Aug 19 15:35:51 2016 +1000

    KVM: PPC: Book3S HV: Handle passthrough interrupts in guest
    
    Currently, KVM switches back to the host to handle any external
    interrupt (when the interrupt is received while running in the
    guest). This patch updates real-mode KVM to check if an interrupt
    is generated by a passthrough adapter that is owned by this guest.
    If so, the real mode KVM will directly inject the corresponding
    virtual interrupt to the guest VCPU's ICS and also EOI the interrupt
    in hardware. In short, the interrupt is handled entirely in real
    mode in the guest context without switching back to the host.
    
    In some rare cases, the interrupt cannot be completely handled in
    real mode, for instance, a VCPU that is sleeping needs to be woken
    up. In this case, KVM simply switches back to the host with trap
    reason set to 0x500. This works, but it is clearly not very efficient.
    A following patch will distinguish this case and handle it
    correctly in the host. Note that we can use the existing
    check_too_hard() routine even though we are not in a hypercall to
    determine if there is unfinished business that needs to be
    completed in host virtual mode.
    
    The patch assumes that the mapping between hardware interrupt IRQ
    and virtual IRQ to be injected to the guest already exists for the
    PCI passthrough interrupts that need to be handled in real mode.
    If the mapping does not exist, KVM falls back to the default
    existing behavior.
    
    The KVM real mode code reads mappings from the mapped array in the
    passthrough IRQ map without taking any lock.  We carefully order the
    loads and stores of the fields in the kvmppc_irq_map data structure
    using memory barriers to avoid an inconsistent mapping being seen by
    the reader. Thus, although it is possible to miss a map entry, it is
    not possible to read a stale value.
    
    [paulus@ozlabs.org - get irq_chip from irq_map rather than pimap,
     pulled out powernv eoi change into a separate patch, made
     kvmppc_read_intr get the vcpu from the paca rather than being
     passed in, rewrote the logic at the end of kvmppc_read_intr to
     avoid deep indentation, simplified logic in book3s_hv_rmhandlers.S
     since we were always restoring SRR0/1 anyway, get rid of the cached
     array (just use the mapped array), removed the kick_all_cpus_sync()
     call, clear saved_xirr PACA field when we handle the interrupt in
     real mode, fix compilation with CONFIG_KVM_XICS=n.]
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 4ca2ba36a860..4299a1f79a91 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -478,6 +478,9 @@ extern int kvmppc_xics_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
 extern int kvmppc_xics_connect_vcpu(struct kvm_device *dev,
 			struct kvm_vcpu *vcpu, u32 cpu);
 extern void kvmppc_xics_ipi_action(void);
+extern long kvmppc_deliver_irq_passthru(struct kvm_vcpu *vcpu, u32 xirr,
+				 struct kvmppc_irq_map *irq_map,
+				 struct kvmppc_passthru_irqmap *pimap);
 extern int h_ipi_redirect;
 #else
 static inline struct kvmppc_passthru_irqmap *kvmppc_get_passthru_irqmap(

commit 8daaafc88b46fb3af952e92d7c2816a8950e1363
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Fri Aug 19 15:35:48 2016 +1000

    KVM: PPC: Book3S HV: Introduce kvmppc_passthru_irqmap
    
    This patch introduces an IRQ mapping structure, the
    kvmppc_passthru_irqmap structure that is to be used
    to map the real hardware IRQ in the host with the virtual
    hardware IRQ (gsi) that is injected into a guest by KVM for
    passthrough adapters.
    
    Currently, we assume a separate IRQ mapping structure for
    each guest. Each kvmppc_passthru_irqmap has a mapping arrays,
    containing all defined real<->virtual IRQs.
    
    [paulus@ozlabs.org - removed irq_chip field from struct
     kvmppc_passthru_irqmap; changed parameter for
     kvmppc_get_passthru_irqmap from struct kvm_vcpu * to struct
     kvm *, removed small cached array.]
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 94715e22d6a2..4ca2ba36a860 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -457,8 +457,18 @@ static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.irq_type == KVMPPC_IRQ_XICS;
 }
+
+static inline struct kvmppc_passthru_irqmap *kvmppc_get_passthru_irqmap(
+				struct kvm *kvm)
+{
+	if (kvm)
+		return kvm->arch.pimap;
+	return NULL;
+}
+
 extern void kvmppc_alloc_host_rm_ops(void);
 extern void kvmppc_free_host_rm_ops(void);
+extern void kvmppc_free_pimap(struct kvm *kvm);
 extern void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu);
 extern int kvmppc_xics_create_icp(struct kvm_vcpu *vcpu, unsigned long server);
 extern int kvm_vm_ioctl_xics_irq(struct kvm *kvm, struct kvm_irq_level *args);
@@ -470,8 +480,12 @@ extern int kvmppc_xics_connect_vcpu(struct kvm_device *dev,
 extern void kvmppc_xics_ipi_action(void);
 extern int h_ipi_redirect;
 #else
+static inline struct kvmppc_passthru_irqmap *kvmppc_get_passthru_irqmap(
+				struct kvm *kvm)
+	{ return NULL; }
 static inline void kvmppc_alloc_host_rm_ops(void) {};
 static inline void kvmppc_free_host_rm_ops(void) {};
+static inline void kvmppc_free_pimap(struct kvm *kvm) {};
 static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
 	{ return 0; }
 static inline void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu) { }

commit 9576730d0e6e301343c5aead5418ad53fcecfd14
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Fri Aug 19 15:35:47 2016 +1000

    KVM: PPC: select IRQ_BYPASS_MANAGER
    
    Select IRQ_BYPASS_MANAGER for PPC when CONFIG_KVM is set.
    Add the PPC producer functions for add and del producer.
    
    [paulus@ozlabs.org - Moved new functions from book3s.c to powerpc.c
     so booke compiles; added kvm_arch_has_irq_bypass implementation.]
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 2544edabe7f3..94715e22d6a2 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -287,6 +287,10 @@ struct kvmppc_ops {
 	long (*arch_vm_ioctl)(struct file *filp, unsigned int ioctl,
 			      unsigned long arg);
 	int (*hcall_implemented)(unsigned long hcall);
+	int (*irq_bypass_add_producer)(struct irq_bypass_consumer *,
+				       struct irq_bypass_producer *);
+	void (*irq_bypass_del_producer)(struct irq_bypass_consumer *,
+					struct irq_bypass_producer *);
 };
 
 extern struct kvmppc_ops *kvmppc_hv_ops;

commit 58ded4201ff028b15f6b317228faa5f154a0663f
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Tue Mar 1 17:54:40 2016 +1100

    KVM: PPC: Add support for 64bit TCE windows
    
    The existing KVM_CREATE_SPAPR_TCE only supports 32bit windows which is not
    enough for directly mapped windows as the guest can get more than 4GB.
    
    This adds KVM_CREATE_SPAPR_TCE_64 ioctl and advertises it
    via KVM_CAP_SPAPR_TCE_64 capability. The table size is checked against
    the locked memory limit.
    
    Since 64bit windows are to support Dynamic DMA windows (DDW), let's add
    @bus_offset and @page_shift which are also required by DDW.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 197a8aca2871..2544edabe7f3 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -165,7 +165,7 @@ extern void kvmppc_map_vrma(struct kvm_vcpu *vcpu,
 extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
 
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
-				struct kvm_create_spapr_tce *args);
+				struct kvm_create_spapr_tce_64 *args);
 extern struct kvmppc_spapr_tce_table *kvmppc_find_table(
 		struct kvm_vcpu *vcpu, unsigned long liobn);
 extern long kvmppc_ioba_validate(struct kvmppc_spapr_tce_table *stt,

commit 520fe9c607d3acea96391aad27e17518bd7d39bd
Author: Suresh E. Warrier <warrier@linux.vnet.ibm.com>
Date:   Mon Dec 21 16:33:57 2015 -0600

    KVM: PPC: Book3S HV: Add tunable to control H_IPI redirection
    
    Redirecting the wakeup of a VCPU from the H_IPI hypercall to
    a core running in the host is usually a good idea, most workloads
    seemed to benefit. However, in one heavily interrupt-driven SMT1
    workload, some regression was observed. This patch adds a kvm_hv
    module parameter called h_ipi_redirect to control this feature.
    
    The default value for this tunable is 1 - that is enable the feature.
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index bc14e9e0e4fe..197a8aca2871 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -464,6 +464,7 @@ extern int kvmppc_xics_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
 extern int kvmppc_xics_connect_vcpu(struct kvm_device *dev,
 			struct kvm_vcpu *vcpu, u32 cpu);
 extern void kvmppc_xics_ipi_action(void);
+extern int h_ipi_redirect;
 #else
 static inline void kvmppc_alloc_host_rm_ops(void) {};
 static inline void kvmppc_free_host_rm_ops(void) {};

commit 0c2a66062470cd1f6d11ae6db31059f59d3f725f
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Thu Dec 17 14:59:09 2015 -0600

    KVM: PPC: Book3S HV: Host side kick VCPU when poked by real-mode KVM
    
    This patch adds the support for the kick VCPU operation for
    kvmppc_host_rm_ops. The kvmppc_xics_ipi_action() function
    provides the function to be invoked for a host side operation
    when poked by the real mode KVM. This is initiated by KVM by
    sending an IPI to any free host core.
    
    KVM real mode must set the rm_action to XICS_RM_KICK_VCPU and
    rm_data to point to the VCPU to be woken up before sending the IPI.
    Note that we have allocated one kvmppc_host_rm_core structure
    per core. The above values need to be set in the structure
    corresponding to the core to which the IPI will be sent.
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index ded8ddac7dcf..bc14e9e0e4fe 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -463,6 +463,7 @@ extern u64 kvmppc_xics_get_icp(struct kvm_vcpu *vcpu);
 extern int kvmppc_xics_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
 extern int kvmppc_xics_connect_vcpu(struct kvm_device *dev,
 			struct kvm_vcpu *vcpu, u32 cpu);
+extern void kvmppc_xics_ipi_action(void);
 #else
 static inline void kvmppc_alloc_host_rm_ops(void) {};
 static inline void kvmppc_free_host_rm_ops(void) {};

commit 79b6c247e9afe35714c1f83cfcecf40a438ca4a4
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Thu Dec 17 14:59:06 2015 -0600

    KVM: PPC: Book3S HV: Host-side RM data structures
    
    This patch defines the data structures to support the setting up
    of host side operations while running in real mode in the guest,
    and also the functions to allocate and free it.
    
    The operations are for now limited to virtual XICS operations.
    Currently, we have only defined one operation in the data
    structure:
             - Wake up a VCPU sleeping in the host when it
               receives a virtual interrupt
    
    The operations are assigned at the core level because PowerKVM
    requires that the host run in SMT off mode. For each core,
    we will need to manage its state atomically - where the state
    is defined by:
    1. Is the core running in the host?
    2. Is there a Real Mode (RM) operation pending on the host?
    
    Currently, core state is only managed at the whole-core level
    even when the system is in split-core mode. This just limits
    the number of free or "available" cores in the host to perform
    any host-side operations.
    
    The kvmppc_host_rm_core.rm_data allows any data to be passed by
    KVM in real mode to the host core along with the operation to
    be performed.
    
    The kvmppc_host_rm_ops structure is allocated the very first time
    a guest VM is started. Initial core state is also set - all online
    cores are in the host. This structure is never deleted, not even
    when there are no active guests. However, it needs to be freed
    when the module is unloaded because the kvmppc_host_rm_ops_hv
    can contain function pointers to kvm-hv.ko functions for the
    different supported host operations.
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 4cadee590deb..ded8ddac7dcf 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -453,6 +453,8 @@ static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.irq_type == KVMPPC_IRQ_XICS;
 }
+extern void kvmppc_alloc_host_rm_ops(void);
+extern void kvmppc_free_host_rm_ops(void);
 extern void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu);
 extern int kvmppc_xics_create_icp(struct kvm_vcpu *vcpu, unsigned long server);
 extern int kvm_vm_ioctl_xics_irq(struct kvm *kvm, struct kvm_irq_level *args);
@@ -462,6 +464,8 @@ extern int kvmppc_xics_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
 extern int kvmppc_xics_connect_vcpu(struct kvm_device *dev,
 			struct kvm_vcpu *vcpu, u32 cpu);
 #else
+static inline void kvmppc_alloc_host_rm_ops(void) {};
+static inline void kvmppc_free_host_rm_ops(void) {};
 static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
 	{ return 0; }
 static inline void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu) { }
@@ -475,6 +479,33 @@ static inline int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd)
 	{ return 0; }
 #endif
 
+/*
+ * Host-side operations we want to set up while running in real
+ * mode in the guest operating on the xics.
+ * Currently only VCPU wakeup is supported.
+ */
+
+union kvmppc_rm_state {
+	unsigned long raw;
+	struct {
+		u32 in_host;
+		u32 rm_action;
+	};
+};
+
+struct kvmppc_host_rm_core {
+	union kvmppc_rm_state rm_state;
+	void *rm_data;
+	char pad[112];
+};
+
+struct kvmppc_host_rm_ops {
+	struct kvmppc_host_rm_core	*rm_core;
+	void		(*vcpu_kick)(struct kvm_vcpu *vcpu);
+};
+
+extern struct kvmppc_host_rm_ops *kvmppc_host_rm_ops_hv;
+
 static inline unsigned long kvmppc_get_epr(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_KVM_BOOKE_HV

commit d3695aa4f452bc09c834a5010484f65fca37d87c
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Mon Feb 15 12:55:09 2016 +1100

    KVM: PPC: Add support for multiple-TCE hcalls
    
    This adds real and virtual mode handlers for the H_PUT_TCE_INDIRECT and
    H_STUFF_TCE hypercalls for user space emulated devices such as IBMVIO
    devices or emulated PCI. These calls allow adding multiple entries
    (up to 512) into the TCE table in one call which saves time on
    transition between kernel and user space.
    
    The current implementation of kvmppc_h_stuff_tce() allows it to be
    executed in both real and virtual modes so there is one helper.
    The kvmppc_rm_h_put_tce_indirect() needs to translate the guest address
    to the host address and since the translation is different, there are
    2 helpers - one for each mode.
    
    This implements the KVM_CAP_PPC_MULTITCE capability. When present,
    the kernel will try handling H_PUT_TCE_INDIRECT and H_STUFF_TCE if these
    are enabled by the userspace via KVM_CAP_PPC_ENABLE_HCALL.
    If they can not be handled by the kernel, they are passed on to
    the user space. The user space still has to have an implementation
    for these.
    
    Both HV and PR-syle KVM are supported.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 95139111a929..4cadee590deb 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -166,12 +166,24 @@ extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
 
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce *args);
+extern struct kvmppc_spapr_tce_table *kvmppc_find_table(
+		struct kvm_vcpu *vcpu, unsigned long liobn);
 extern long kvmppc_ioba_validate(struct kvmppc_spapr_tce_table *stt,
 		unsigned long ioba, unsigned long npages);
 extern long kvmppc_tce_validate(struct kvmppc_spapr_tce_table *tt,
 		unsigned long tce);
+extern long kvmppc_gpa_to_ua(struct kvm *kvm, unsigned long gpa,
+		unsigned long *ua, unsigned long **prmap);
+extern void kvmppc_tce_put(struct kvmppc_spapr_tce_table *tt,
+		unsigned long idx, unsigned long tce);
 extern long kvmppc_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
 			     unsigned long ioba, unsigned long tce);
+extern long kvmppc_h_put_tce_indirect(struct kvm_vcpu *vcpu,
+		unsigned long liobn, unsigned long ioba,
+		unsigned long tce_list, unsigned long npages);
+extern long kvmppc_h_stuff_tce(struct kvm_vcpu *vcpu,
+		unsigned long liobn, unsigned long ioba,
+		unsigned long tce_value, unsigned long npages);
 extern long kvmppc_h_get_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
 			     unsigned long ioba);
 extern struct page *kvm_alloc_hpt(unsigned long nr_pages);

commit 5ee7af18642ce38c79b35927872f13d292cc3e27
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Mon Feb 15 12:55:08 2016 +1100

    KVM: PPC: Move reusable bits of H_PUT_TCE handler to helpers
    
    Upcoming multi-tce support (H_PUT_TCE_INDIRECT/H_STUFF_TCE hypercalls)
    will validate TCE (not to have unexpected bits) and IO address
    (to be within the DMA window boundaries).
    
    This introduces helpers to validate TCE and IO address. The helpers are
    exported as they compile into vmlinux (to work in realmode) and will be
    used later by KVM kernel module in virtual mode.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 2241d5357129..95139111a929 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -166,6 +166,10 @@ extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
 
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce *args);
+extern long kvmppc_ioba_validate(struct kvmppc_spapr_tce_table *stt,
+		unsigned long ioba, unsigned long npages);
+extern long kvmppc_tce_validate(struct kvmppc_spapr_tce_table *tt,
+		unsigned long tce);
 extern long kvmppc_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
 			     unsigned long ioba, unsigned long tce);
 extern long kvmppc_h_get_tce(struct kvm_vcpu *vcpu, unsigned long liobn,

commit ba049e93aef7e8c571567088b1b73f4f5b99272a
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jan 15 16:56:11 2016 -0800

    kvm: rename pfn_t to kvm_pfn_t
    
    To date, we have implemented two I/O usage models for persistent memory,
    PMEM (a persistent "ram disk") and DAX (mmap persistent memory into
    userspace).  This series adds a third, DAX-GUP, that allows DAX mappings
    to be the target of direct-i/o.  It allows userspace to coordinate
    DMA/RDMA from/to persistent memory.
    
    The implementation leverages the ZONE_DEVICE mm-zone that went into
    4.3-rc1 (also discussed at kernel summit) to flag pages that are owned
    and dynamically mapped by a device driver.  The pmem driver, after
    mapping a persistent memory range into the system memmap via
    devm_memremap_pages(), arranges for DAX to distinguish pfn-only versus
    page-backed pmem-pfns via flags in the new pfn_t type.
    
    The DAX code, upon seeing a PFN_DEV+PFN_MAP flagged pfn, flags the
    resulting pte(s) inserted into the process page tables with a new
    _PAGE_DEVMAP flag.  Later, when get_user_pages() is walking ptes it keys
    off _PAGE_DEVMAP to pin the device hosting the page range active.
    Finally, get_page() and put_page() are modified to take references
    against the device driver established page mapping.
    
    Finally, this need for "struct page" for persistent memory requires
    memory capacity to store the memmap array.  Given the memmap array for a
    large pool of persistent may exhaust available DRAM introduce a
    mechanism to allocate the memmap from persistent memory.  The new
    "struct vmem_altmap *" parameter to devm_memremap_pages() enables
    arch_add_memory() to use reserved pmem capacity rather than the page
    allocator.
    
    This patch (of 18):
    
    The core has developed a need for a "pfn_t" type [1].  Move the existing
    pfn_t in KVM to kvm_pfn_t [2].
    
    [1]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002199.html
    [2]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002218.html
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c6ef05bd0765..2241d5357129 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -515,7 +515,7 @@ void kvmppc_claim_lpid(long lpid);
 void kvmppc_free_lpid(long lpid);
 void kvmppc_init_lpid(unsigned long nr_lpids);
 
-static inline void kvmppc_mmu_flush_icache(pfn_t pfn)
+static inline void kvmppc_mmu_flush_icache(kvm_pfn_t pfn)
 {
 	struct page *page;
 	/*

commit f36f3f2846b5578d62910ee0b6dbef59fdd1cfa4
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 18 13:20:23 2015 +0200

    KVM: add "new" argument to kvm_arch_commit_memory_region
    
    This lets the function access the new memory slot without going through
    kvm_memslots and id_to_memslot.  It will simplify the code when more
    than one address space will be supported.
    
    Unfortunately, the "const"ness of the new argument must be casted
    away in two places.  Fixing KVM to accept const struct kvm_memory_slot
    pointers would require modifications in pretty much all architectures,
    and is left for later.
    
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index aff563b5f001..c6ef05bd0765 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -185,7 +185,8 @@ extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
 				const struct kvm_userspace_memory_region *mem);
 extern void kvmppc_core_commit_memory_region(struct kvm *kvm,
 				const struct kvm_userspace_memory_region *mem,
-				const struct kvm_memory_slot *old);
+				const struct kvm_memory_slot *old,
+				const struct kvm_memory_slot *new);
 extern int kvm_vm_ioctl_get_smmu_info(struct kvm *kvm,
 				      struct kvm_ppc_smmu_info *info);
 extern void kvmppc_core_flush_memslot(struct kvm *kvm,
@@ -246,7 +247,8 @@ struct kvmppc_ops {
 				     const struct kvm_userspace_memory_region *mem);
 	void (*commit_memory_region)(struct kvm *kvm,
 				     const struct kvm_userspace_memory_region *mem,
-				     const struct kvm_memory_slot *old);
+				     const struct kvm_memory_slot *old,
+				     const struct kvm_memory_slot *new);
 	int (*unmap_hva)(struct kvm *kvm, unsigned long hva);
 	int (*unmap_hva_range)(struct kvm *kvm, unsigned long start,
 			   unsigned long end);

commit 09170a49422bd786be3eac5cec1955257c5a34b7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 18 13:59:39 2015 +0200

    KVM: const-ify uses of struct kvm_userspace_memory_region
    
    Architecture-specific helpers are not supposed to muck with
    struct kvm_userspace_memory_region contents.  Add const to
    enforce this.
    
    In order to eliminate the only write in __kvm_set_memory_region,
    the cleaning of deleted slots is pulled up from update_memslots
    to __kvm_set_memory_region.
    
    Reviewed-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index b8475daad884..aff563b5f001 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -182,9 +182,9 @@ extern int kvmppc_core_create_memslot(struct kvm *kvm,
 				      unsigned long npages);
 extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
-				struct kvm_userspace_memory_region *mem);
+				const struct kvm_userspace_memory_region *mem);
 extern void kvmppc_core_commit_memory_region(struct kvm *kvm,
-				struct kvm_userspace_memory_region *mem,
+				const struct kvm_userspace_memory_region *mem,
 				const struct kvm_memory_slot *old);
 extern int kvm_vm_ioctl_get_smmu_info(struct kvm *kvm,
 				      struct kvm_ppc_smmu_info *info);
@@ -243,9 +243,9 @@ struct kvmppc_ops {
 	void (*flush_memslot)(struct kvm *kvm, struct kvm_memory_slot *memslot);
 	int (*prepare_memory_region)(struct kvm *kvm,
 				     struct kvm_memory_slot *memslot,
-				     struct kvm_userspace_memory_region *mem);
+				     const struct kvm_userspace_memory_region *mem);
 	void (*commit_memory_region)(struct kvm *kvm,
-				     struct kvm_userspace_memory_region *mem,
+				     const struct kvm_userspace_memory_region *mem,
 				     const struct kvm_memory_slot *old);
 	int (*unmap_hva)(struct kvm *kvm, unsigned long hva);
 	int (*unmap_hva_range)(struct kvm *kvm, unsigned long start,

commit e928e9cb3601ce240189bfea05b67ebd391c85ae
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Mar 20 20:39:41 2015 +1100

    KVM: PPC: Book3S HV: Add fast real-mode H_RANDOM implementation.
    
    Some PowerNV systems include a hardware random-number generator.
    This HWRNG is present on POWER7+ and POWER8 chips and is capable of
    generating one 64-bit random number every microsecond.  The random
    numbers are produced by sampling a set of 64 unstable high-frequency
    oscillators and are almost completely entropic.
    
    PAPR defines an H_RANDOM hypercall which guests can use to obtain one
    64-bit random sample from the HWRNG.  This adds a real-mode
    implementation of the H_RANDOM hypercall.  This hypercall was
    implemented in real mode because the latency of reading the HWRNG is
    generally small compared to the latency of a guest exit and entry for
    all the threads in the same virtual core.
    
    Userspace can detect the presence of the HWRNG and the H_RANDOM
    implementation by querying the KVM_CAP_PPC_HWRNG capability.  The
    H_RANDOM hypercall implementation will only be invoked when the guest
    does an H_RANDOM hypercall if userspace first enables the in-kernel
    H_RANDOM implementation using the KVM_CAP_PPC_ENABLE_HCALL capability.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 46bf652c9169..b8475daad884 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -302,6 +302,8 @@ static inline bool is_kvmppc_hv_enabled(struct kvm *kvm)
 	return kvm->arch.kvm_ops == kvmppc_hv_ops;
 }
 
+extern int kvmppc_hwrng_present(void);
+
 /*
  * Cuts out inst bits with ordering according to spec.
  * That means the leftmost bit is zero. All given bits are included.

commit c17b98cf6028704e1f953d6a25ed6140425ccfd0
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Dec 3 13:30:38 2014 +1100

    KVM: PPC: Book3S HV: Remove code for PPC970 processors
    
    This removes the code that was added to enable HV KVM to work
    on PPC970 processors.  The PPC970 is an old CPU that doesn't
    support virtualizing guest memory.  Removing PPC970 support also
    lets us remove the code for allocating and managing contiguous
    real-mode areas, the code for the !kvm->arch.using_mmu_notifiers
    case, the code for pinning pages of guest memory when first
    accessed and keeping track of which pages have been pinned, and
    the code for handling H_ENTER hypercalls in virtual mode.
    
    Book3S HV KVM is now supported only on POWER7 and POWER8 processors.
    The KVM_CAP_PPC_RMA capability now always returns 0.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index a6dcdb6d13c1..46bf652c9169 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -170,8 +170,6 @@ extern long kvmppc_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
 			     unsigned long ioba, unsigned long tce);
 extern long kvmppc_h_get_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
 			     unsigned long ioba);
-extern struct kvm_rma_info *kvm_alloc_rma(void);
-extern void kvm_release_rma(struct kvm_rma_info *ri);
 extern struct page *kvm_alloc_hpt(unsigned long nr_pages);
 extern void kvm_release_hpt(struct page *page, unsigned long nr_pages);
 extern int kvmppc_core_init_vm(struct kvm *kvm);

commit 00c027db0cc4b7387b258330482c6e5f5e836b18
Merge: c24ae0dcd3e8 8d0eff638564
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Sep 24 23:19:45 2014 +0200

    Merge tag 'signed-kvm-ppc-next' of git://github.com/agraf/linux-2.6 into kvm-next
    
    Patch queue for ppc - 2014-09-24
    
    New awesome things in this release:
    
      - E500: e6500 core support
      - E500: guest and remote debug support
      - Book3S: remote sw breakpoint support
      - Book3S: HV: Minor bugfixes
    
    Alexander Graf (1):
          KVM: PPC: Pass enum to kvmppc_get_last_inst
    
    Bharat Bhushan (8):
          KVM: PPC: BOOKE: allow debug interrupt at "debug level"
          KVM: PPC: BOOKE : Emulate rfdi instruction
          KVM: PPC: BOOKE: Allow guest to change MSR_DE
          KVM: PPC: BOOKE: Clear guest dbsr in userspace exit KVM_EXIT_DEBUG
          KVM: PPC: BOOKE: Guest and hardware visible debug registers are same
          KVM: PPC: BOOKE: Add one reg interface for DBSR
          KVM: PPC: BOOKE: Add one_reg documentation of SPRG9 and DBSR
          KVM: PPC: BOOKE: Emulate debug registers and exception
    
    Madhavan Srinivasan (2):
          powerpc/kvm: support to handle sw breakpoint
          powerpc/kvm: common sw breakpoint instr across ppc
    
    Michael Neuling (1):
          KVM: PPC: Book3S HV: Add register name when loading toc
    
    Mihai Caraman (10):
          powerpc/booke: Restrict SPE exception handlers to e200/e500 cores
          powerpc/booke: Revert SPE/AltiVec common defines for interrupt numbers
          KVM: PPC: Book3E: Increase FPU laziness
          KVM: PPC: Book3e: Add AltiVec support
          KVM: PPC: Make ONE_REG powerpc generic
          KVM: PPC: Move ONE_REG AltiVec support to powerpc
          KVM: PPC: Remove the tasklet used by the hrtimer
          KVM: PPC: Remove shared defines for SPE and AltiVec interrupts
          KVM: PPC: e500mc: Add support for single threaded vcpus on e6500 core
          KVM: PPC: Book3E: Enable e6500 core
    
    Paul Mackerras (2):
          KVM: PPC: Book3S HV: Increase timeout for grabbing secondary threads
          KVM: PPC: Book3S HV: Only accept host PVR value for guest PVR

commit 57128468080a8b6ea452223036d3e417f748af55
Author: Andres Lagar-Cavilla <andreslc@google.com>
Date:   Mon Sep 22 14:54:42 2014 -0700

    kvm: Fix page ageing bugs
    
    1. We were calling clear_flush_young_notify in unmap_one, but we are
    within an mmu notifier invalidate range scope. The spte exists no more
    (due to range_start) and the accessed bit info has already been
    propagated (due to kvm_pfn_set_accessed). Simply call
    clear_flush_young.
    
    2. We clear_flush_young on a primary MMU PMD, but this may be mapped
    as a collection of PTEs by the secondary MMU (e.g. during log-dirty).
    This required expanding the interface of the clear_flush_young mmu
    notifier, so a lot of code has been trivially touched.
    
    3. In the absence of shadow_accessed_mask (e.g. EPT A bit), we emulate
    the access bit by blowing the spte. This requires proper synchronizing
    with MMU notifier consumers, like every other removal of spte's does.
    
    Signed-off-by: Andres Lagar-Cavilla <andreslc@google.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index fb86a2299d8a..d4a92d7cea6a 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -243,7 +243,7 @@ struct kvmppc_ops {
 	int (*unmap_hva)(struct kvm *kvm, unsigned long hva);
 	int (*unmap_hva_range)(struct kvm *kvm, unsigned long start,
 			   unsigned long end);
-	int (*age_hva)(struct kvm *kvm, unsigned long hva);
+	int (*age_hva)(struct kvm *kvm, unsigned long start, unsigned long end);
 	int (*test_age_hva)(struct kvm *kvm, unsigned long hva);
 	void (*set_spte_hva)(struct kvm *kvm, unsigned long hva, pte_t pte);
 	void (*mmu_destroy)(struct kvm_vcpu *vcpu);

commit a59c1d9e609c4bbad9ec3b238221ecf3b9ca091b
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Tue Sep 9 22:37:35 2014 +0530

    powerpc/kvm: support to handle sw breakpoint
    
    This patch adds kernel side support for software breakpoint.
    Design is that, by using an illegal instruction, we trap to hypervisor
    via Emulation Assistance interrupt, where we check for the illegal instruction
    and accordingly we return to Host or Guest. Patch also adds support for
    software breakpoint in PR KVM.
    
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 73063ef53694..dbd160f16cb0 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -38,6 +38,12 @@
 #include <asm/paca.h>
 #endif
 
+/*
+ * KVMPPC_INST_SW_BREAKPOINT is debug Instruction
+ * for supporting software breakpoint.
+ */
+#define KVMPPC_INST_SW_BREAKPOINT	0x00dddd00
+
 enum emulation_result {
 	EMULATE_DONE,         /* no further processing */
 	EMULATE_DO_MMIO,      /* kvm_run filled with MMIO request */

commit d02d4d156e72baf9a6628c76eb53019124d3c82f
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Mon Sep 1 17:19:56 2014 +0300

    KVM: PPC: Remove the tasklet used by the hrtimer
    
    Powerpc timer implementation is a copycat version of s390. Now that they removed
    the tasklet with commit ea74c0ea1b24a6978a6ebc80ba4dbc7b7848b32d follow this
    optimization.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Bogdan Purcareata <bogdan.purcareata@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 05e58b630601..73063ef53694 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -89,7 +89,7 @@ extern int kvmppc_emulate_loadstore(struct kvm_vcpu *vcpu);
 extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
 extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
 extern u32 kvmppc_get_dec(struct kvm_vcpu *vcpu, u64 tb);
-extern void kvmppc_decrementer_func(unsigned long data);
+extern void kvmppc_decrementer_func(struct kvm_vcpu *vcpu);
 extern int kvmppc_sanity_check(struct kvm_vcpu *vcpu);
 extern int kvmppc_subarch_vcpu_init(struct kvm_vcpu *vcpu);
 extern void kvmppc_subarch_vcpu_uninit(struct kvm_vcpu *vcpu);

commit 2f699a59f399d65d51df6eb916bf2e0f7c6f8148
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Wed Aug 13 14:39:44 2014 +0530

    KVM: PPC: BOOKE: Emulate debug registers and exception
    
    This patch emulates debug registers and debug exception
    to support guest using debug resource. This enables running
    gdb/kgdb etc in guest.
    
    On BOOKE architecture we cannot share debug resources between QEMU and
    guest because:
        When QEMU is using debug resources then debug exception must
        be always enabled. To achieve this we set MSR_DE and also set
        MSRP_DEP so guest cannot change MSR_DE.
    
        When emulating debug resource for guest we want guest
        to control MSR_DE (enable/disable debug interrupt on need).
    
        So above mentioned two configuration cannot be supported
        at the same time. So the result is that we cannot share
        debug resources between QEMU and Guest on BOOKE architecture.
    
    In the current design QEMU gets priority over guest, this means that if
    QEMU is using debug resources then guest cannot use them and if guest is
    using debug resource then QEMU can overwrite them.
    
    Signed-off-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index fb86a2299d8a..05e58b630601 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -206,6 +206,9 @@ extern int kvmppc_xics_get_xive(struct kvm *kvm, u32 irq, u32 *server,
 extern int kvmppc_xics_int_on(struct kvm *kvm, u32 irq);
 extern int kvmppc_xics_int_off(struct kvm *kvm, u32 irq);
 
+void kvmppc_core_dequeue_debug(struct kvm_vcpu *vcpu);
+void kvmppc_core_queue_debug(struct kvm_vcpu *vcpu);
+
 union kvmppc_one_reg {
 	u32	wval;
 	u64	dval;

commit 5a484c7c1efd2c45f8cc726e4d21283a5324e361
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Wed Jul 30 15:03:56 2014 +0530

    KVM: PPC: BOOKEHV: rename e500hv_spr to bookehv_spr
    
    This are not specific to e500hv but applicable for bookehv
    (As per comment from Scott Wood on my patch
    "kvm: ppc: bookehv: Added wrapper macros for shadow registers")
    
    Signed-off-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 8e36c1e2c631..fb86a2299d8a 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -539,16 +539,16 @@ static inline bool kvmppc_shared_big_endian(struct kvm_vcpu *vcpu)
 #endif
 }
 
-#define SPRNG_WRAPPER_GET(reg, e500hv_spr)				\
+#define SPRNG_WRAPPER_GET(reg, bookehv_spr)				\
 static inline ulong kvmppc_get_##reg(struct kvm_vcpu *vcpu)		\
 {									\
-	return mfspr(e500hv_spr);					\
+	return mfspr(bookehv_spr);					\
 }									\
 
-#define SPRNG_WRAPPER_SET(reg, e500hv_spr)				\
+#define SPRNG_WRAPPER_SET(reg, bookehv_spr)				\
 static inline void kvmppc_set_##reg(struct kvm_vcpu *vcpu, ulong val)	\
 {									\
-	mtspr(e500hv_spr, val);						\
+	mtspr(bookehv_spr, val);						\
 }									\
 
 #define SHARED_WRAPPER_GET(reg, size)					\
@@ -573,18 +573,18 @@ static inline void kvmppc_set_##reg(struct kvm_vcpu *vcpu, u##size val)	\
 	SHARED_WRAPPER_GET(reg, size)					\
 	SHARED_WRAPPER_SET(reg, size)					\
 
-#define SPRNG_WRAPPER(reg, e500hv_spr)					\
-	SPRNG_WRAPPER_GET(reg, e500hv_spr)				\
-	SPRNG_WRAPPER_SET(reg, e500hv_spr)				\
+#define SPRNG_WRAPPER(reg, bookehv_spr)					\
+	SPRNG_WRAPPER_GET(reg, bookehv_spr)				\
+	SPRNG_WRAPPER_SET(reg, bookehv_spr)				\
 
 #ifdef CONFIG_KVM_BOOKE_HV
 
-#define SHARED_SPRNG_WRAPPER(reg, size, e500hv_spr)			\
-	SPRNG_WRAPPER(reg, e500hv_spr)					\
+#define SHARED_SPRNG_WRAPPER(reg, size, bookehv_spr)			\
+	SPRNG_WRAPPER(reg, bookehv_spr)					\
 
 #else
 
-#define SHARED_SPRNG_WRAPPER(reg, size, e500hv_spr)			\
+#define SHARED_SPRNG_WRAPPER(reg, size, bookehv_spr)			\
 	SHARED_WRAPPER(reg, size)					\
 
 #endif

commit ce91ddc471b77ec75e5b2a43c803efac605f37b3
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jul 28 19:29:13 2014 +0200

    KVM: PPC: Remove DCR handling
    
    DCR handling was only needed for 440 KVM. Since we removed it, we can also
    remove handling of DCR accesses.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index cbee4538307f..8e36c1e2c631 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -41,7 +41,6 @@
 enum emulation_result {
 	EMULATE_DONE,         /* no further processing */
 	EMULATE_DO_MMIO,      /* kvm_run filled with MMIO request */
-	EMULATE_DO_DCR,       /* kvm_run filled with DCR request */
 	EMULATE_FAIL,         /* can't emulate this instruction */
 	EMULATE_AGAIN,        /* something went wrong. go again */
 	EMULATE_EXIT_USER,    /* emulation requires exit to user-space */

commit 8de12015ff75967b16f70e5938b151390dac9b77
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Jun 18 21:56:55 2014 +0200

    KVM: PPC: Expose helper functions for data/inst faults
    
    We're going to implement guest code interpretation in KVM for some rare
    corner cases. This code needs to be able to inject data and instruction
    faults into the guest when it encounters them.
    
    Expose generic APIs to do this in a reasonably subarch agnostic fashion.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 2214ee61f668..cbee4538307f 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -132,6 +132,14 @@ extern void kvmppc_core_dequeue_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_external(struct kvm_vcpu *vcpu,
                                        struct kvm_interrupt *irq);
 extern void kvmppc_core_dequeue_external(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_queue_dtlb_miss(struct kvm_vcpu *vcpu, ulong dear_flags,
+					ulong esr_flags);
+extern void kvmppc_core_queue_data_storage(struct kvm_vcpu *vcpu,
+					   ulong dear_flags,
+					   ulong esr_flags);
+extern void kvmppc_core_queue_itlb_miss(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_queue_inst_storage(struct kvm_vcpu *vcpu,
+					   ulong esr_flags);
 extern void kvmppc_core_flush_tlb(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_check_requests(struct kvm_vcpu *vcpu);
 

commit d69614a295aef72f8fb22da8e3ccf1a8f19a7ffc
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Jun 18 14:53:49 2014 +0200

    KVM: PPC: Separate loadstore emulation from priv emulation
    
    Today the instruction emulator can get called via 2 separate code paths. It
    can either be called by MMIO emulation detection code or by privileged
    instruction traps.
    
    This is bad, as both code paths prepare the environment differently. For MMIO
    emulation we already know the virtual address we faulted on, so instructions
    there don't have to actually fetch that information.
    
    Split out the two separate use cases into separate files.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 17fa277d297e..2214ee61f668 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -86,6 +86,7 @@ extern int kvmppc_st(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 		     bool data);
 extern int kvmppc_emulate_instruction(struct kvm_run *run,
                                       struct kvm_vcpu *vcpu);
+extern int kvmppc_emulate_loadstore(struct kvm_vcpu *vcpu);
 extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
 extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
 extern u32 kvmppc_get_dec(struct kvm_vcpu *vcpu, u64 tb);

commit 35c4a7330dbe1ae6f590a5645b185e35ddb3f6d9
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jun 20 13:58:16 2014 +0200

    KVM: PPC: Move kvmppc_ld/st to common code
    
    We have enough common infrastructure now to resolve GVA->GPA mappings at
    runtime. With this we can move our book3s specific helpers to load / store
    in guest virtual address space to common code as well.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 1a60af9f2fa1..17fa277d297e 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -80,6 +80,10 @@ extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 extern int kvmppc_load_last_inst(struct kvm_vcpu *vcpu,
 				 enum instruction_type type, u32 *inst);
 
+extern int kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
+		     bool data);
+extern int kvmppc_st(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
+		     bool data);
 extern int kvmppc_emulate_instruction(struct kvm_run *run,
                                       struct kvm_vcpu *vcpu);
 extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);

commit 7d15c06f1abfe4b893c6c2c8a306b02210a6a6db
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jun 20 13:52:36 2014 +0200

    KVM: PPC: Implement kvmppc_xlate for all targets
    
    We have a nice API to find the translated GPAs of a GVA including protection
    flags. So far we only use it on Book3S, but there's no reason the same shouldn't
    be used on BookE as well.
    
    Implement a kvmppc_xlate() version for BookE and clean it up to make it more
    readable in general.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index e38136304c1f..1a60af9f2fa1 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -52,6 +52,16 @@ enum instruction_type {
 	INST_SC,		/* system call */
 };
 
+enum xlate_instdata {
+	XLATE_INST,		/* translate instruction address */
+	XLATE_DATA		/* translate data address */
+};
+
+enum xlate_readwrite {
+	XLATE_READ,		/* check for read permissions */
+	XLATE_WRITE		/* check for write permissions */
+};
+
 extern int kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
 extern int __kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
 extern void kvmppc_handler_highmem(void);
@@ -94,6 +104,9 @@ extern gpa_t kvmppc_mmu_xlate(struct kvm_vcpu *vcpu, unsigned int gtlb_index,
                               gva_t eaddr);
 extern void kvmppc_mmu_dtlb_miss(struct kvm_vcpu *vcpu);
 extern void kvmppc_mmu_itlb_miss(struct kvm_vcpu *vcpu);
+extern int kvmppc_xlate(struct kvm_vcpu *vcpu, ulong eaddr,
+			enum xlate_instdata xlid, enum xlate_readwrite xlrw,
+			struct kvmppc_pte *pte);
 
 extern struct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm,
                                                 unsigned int id);

commit 51f047261e717b74b226f837a16455994b61ae30
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Wed Jul 23 19:06:21 2014 +0300

    KVM: PPC: Allow kvmppc_get_last_inst() to fail
    
    On book3e, guest last instruction is read on the exit path using load
    external pid (lwepx) dedicated instruction. This load operation may fail
    due to TLB eviction and execute-but-not-read entries.
    
    This patch lay down the path for an alternative solution to read the guest
    last instruction, by allowing kvmppc_get_lat_inst() function to fail.
    Architecture specific implmentations of kvmppc_load_last_inst() may read
    last guest instruction and instruct the emulation layer to re-execute the
    guest in case of failure.
    
    Make kvmppc_get_last_inst() definition common between architectures.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 246fb9a7df33..e38136304c1f 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -47,6 +47,11 @@ enum emulation_result {
 	EMULATE_EXIT_USER,    /* emulation requires exit to user-space */
 };
 
+enum instruction_type {
+	INST_GENERIC,
+	INST_SC,		/* system call */
+};
+
 extern int kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
 extern int __kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
 extern void kvmppc_handler_highmem(void);
@@ -62,6 +67,9 @@ extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 			       u64 val, unsigned int bytes,
 			       int is_default_endian);
 
+extern int kvmppc_load_last_inst(struct kvm_vcpu *vcpu,
+				 enum instruction_type type, u32 *inst);
+
 extern int kvmppc_emulate_instruction(struct kvm_run *run,
                                       struct kvm_vcpu *vcpu);
 extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
@@ -234,6 +242,29 @@ struct kvmppc_ops {
 extern struct kvmppc_ops *kvmppc_hv_ops;
 extern struct kvmppc_ops *kvmppc_pr_ops;
 
+static inline int kvmppc_get_last_inst(struct kvm_vcpu *vcpu,
+					enum instruction_type type, u32 *inst)
+{
+	int ret = EMULATE_DONE;
+	u32 fetched_inst;
+
+	/* Load the instruction manually if it failed to do so in the
+	 * exit path */
+	if (vcpu->arch.last_inst == KVM_INST_FETCH_FAILED)
+		ret = kvmppc_load_last_inst(vcpu, type, &vcpu->arch.last_inst);
+
+	/*  Write fetch_failed unswapped if the fetch failed */
+	if (ret == EMULATE_DONE)
+		fetched_inst = kvmppc_need_byteswap(vcpu) ?
+				swab32(vcpu->arch.last_inst) :
+				vcpu->arch.last_inst;
+	else
+		fetched_inst = vcpu->arch.last_inst;
+
+	*inst = fetched_inst;
+	return ret;
+}
+
 static inline bool is_kvmppc_hv_enabled(struct kvm *kvm)
 {
 	return kvm->arch.kvm_ops == kvmppc_hv_ops;

commit 34f754b99e2f642c661967b456764b2c7ccc096e
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Thu Jul 17 17:01:40 2014 +0530

    kvm: ppc: Add SPRN_EPR get helper function
    
    kvmppc_set_epr() is already defined in asm/kvm_ppc.h, So
    rename and move get_epr helper function to same file.
    
    Signed-off-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    [agraf: remove duplicate return]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c95bdbdc9d44..246fb9a7df33 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -392,6 +392,17 @@ static inline int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd)
 	{ return 0; }
 #endif
 
+static inline unsigned long kvmppc_get_epr(struct kvm_vcpu *vcpu)
+{
+#ifdef CONFIG_KVM_BOOKE_HV
+	return mfspr(SPRN_GEPR);
+#elif defined(CONFIG_BOOKE)
+	return vcpu->arch.epr;
+#else
+	return 0;
+#endif
+}
+
 static inline void kvmppc_set_epr(struct kvm_vcpu *vcpu, u32 epr)
 {
 #ifdef CONFIG_KVM_BOOKE_HV

commit dc168549d9a0fb55d3021ee408adf25786cfda23
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Thu Jul 17 17:01:38 2014 +0530

    kvm: ppc: booke: Add shared struct helpers of SPRN_ESR
    
    Add and use kvmppc_set_esr() and kvmppc_get_esr() helper functions
    
    Signed-off-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 6520d09f7a1e..c95bdbdc9d44 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -530,6 +530,7 @@ SHARED_SPRNG_WRAPPER(sprg3, 64, SPRN_GSPRG3)
 SHARED_SPRNG_WRAPPER(srr0, 64, SPRN_GSRR0)
 SHARED_SPRNG_WRAPPER(srr1, 64, SPRN_GSRR1)
 SHARED_SPRNG_WRAPPER(dar, 64, SPRN_GDEAR)
+SHARED_SPRNG_WRAPPER(esr, 64, SPRN_GESR)
 SHARED_WRAPPER_GET(msr, 64)
 static inline void kvmppc_set_msr_fast(struct kvm_vcpu *vcpu, u64 val)
 {

commit 1dc0c5b88cae1c211b37fed9187379a692bb469b
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Thu Jul 17 17:01:35 2014 +0530

    kvm: ppc: bookehv: Added wrapper macros for shadow registers
    
    There are shadow registers like, GSPRG[0-3], GSRR0, GSRR1 etc on
    BOOKE-HV and these shadow registers are guest accessible.
    So these shadow registers needs to be updated on BOOKE-HV.
    This patch adds new macro for get/set helper of shadow register .
    
    Signed-off-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index e2fd5a133b9c..6520d09f7a1e 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -472,8 +472,20 @@ static inline bool kvmppc_shared_big_endian(struct kvm_vcpu *vcpu)
 #endif
 }
 
+#define SPRNG_WRAPPER_GET(reg, e500hv_spr)				\
+static inline ulong kvmppc_get_##reg(struct kvm_vcpu *vcpu)		\
+{									\
+	return mfspr(e500hv_spr);					\
+}									\
+
+#define SPRNG_WRAPPER_SET(reg, e500hv_spr)				\
+static inline void kvmppc_set_##reg(struct kvm_vcpu *vcpu, ulong val)	\
+{									\
+	mtspr(e500hv_spr, val);						\
+}									\
+
 #define SHARED_WRAPPER_GET(reg, size)					\
-static inline u##size kvmppc_get_##reg(struct kvm_vcpu *vcpu)	\
+static inline u##size kvmppc_get_##reg(struct kvm_vcpu *vcpu)		\
 {									\
 	if (kvmppc_shared_big_endian(vcpu))				\
 	       return be##size##_to_cpu(vcpu->arch.shared->reg);	\
@@ -494,14 +506,30 @@ static inline void kvmppc_set_##reg(struct kvm_vcpu *vcpu, u##size val)	\
 	SHARED_WRAPPER_GET(reg, size)					\
 	SHARED_WRAPPER_SET(reg, size)					\
 
+#define SPRNG_WRAPPER(reg, e500hv_spr)					\
+	SPRNG_WRAPPER_GET(reg, e500hv_spr)				\
+	SPRNG_WRAPPER_SET(reg, e500hv_spr)				\
+
+#ifdef CONFIG_KVM_BOOKE_HV
+
+#define SHARED_SPRNG_WRAPPER(reg, size, e500hv_spr)			\
+	SPRNG_WRAPPER(reg, e500hv_spr)					\
+
+#else
+
+#define SHARED_SPRNG_WRAPPER(reg, size, e500hv_spr)			\
+	SHARED_WRAPPER(reg, size)					\
+
+#endif
+
 SHARED_WRAPPER(critical, 64)
-SHARED_WRAPPER(sprg0, 64)
-SHARED_WRAPPER(sprg1, 64)
-SHARED_WRAPPER(sprg2, 64)
-SHARED_WRAPPER(sprg3, 64)
-SHARED_WRAPPER(srr0, 64)
-SHARED_WRAPPER(srr1, 64)
-SHARED_WRAPPER(dar, 64)
+SHARED_SPRNG_WRAPPER(sprg0, 64, SPRN_GSPRG0)
+SHARED_SPRNG_WRAPPER(sprg1, 64, SPRN_GSPRG1)
+SHARED_SPRNG_WRAPPER(sprg2, 64, SPRN_GSPRG2)
+SHARED_SPRNG_WRAPPER(sprg3, 64, SPRN_GSPRG3)
+SHARED_SPRNG_WRAPPER(srr0, 64, SPRN_GSRR0)
+SHARED_SPRNG_WRAPPER(srr1, 64, SPRN_GSRR1)
+SHARED_SPRNG_WRAPPER(dar, 64, SPRN_GDEAR)
 SHARED_WRAPPER_GET(msr, 64)
 static inline void kvmppc_set_msr_fast(struct kvm_vcpu *vcpu, u64 val)
 {

commit ae2113a4f1a6cd5a3cd3d75f394547922758e9ac
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jun 2 11:03:00 2014 +1000

    KVM: PPC: Book3S: Allow only implemented hcalls to be enabled or disabled
    
    This adds code to check that when the KVM_CAP_PPC_ENABLE_HCALL
    capability is used to enable or disable in-kernel handling of an
    hcall, that the hcall is actually implemented by the kernel.
    If not an EINVAL error is returned.
    
    This also checks the default-enabled list of hcalls and prints a
    warning if any hcall there is not actually implemented.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 9c89cdd067a6..e2fd5a133b9c 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -228,7 +228,7 @@ struct kvmppc_ops {
 	void (*fast_vcpu_kick)(struct kvm_vcpu *vcpu);
 	long (*arch_vm_ioctl)(struct file *filp, unsigned int ioctl,
 			      unsigned long arg);
-
+	int (*hcall_implemented)(unsigned long hcall);
 };
 
 extern struct kvmppc_ops *kvmppc_hv_ops;

commit c5aec4c76af1a2d89ee2f2d4d5463b2ad2d85de5
Merge: 2937f5efa575 0c0a3e5a100b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 10 18:54:22 2014 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Ben Herrenschmidt:
     "Here is the bulk of the powerpc changes for this merge window.  It got
      a bit delayed in part because I wasn't paying attention, and in part
      because I discovered I had a core PCI change without a PCI maintainer
      ack in it.  Bjorn eventually agreed it was ok to merge it though we'll
      probably improve it later and I didn't want to rebase to add his ack.
    
      There is going to be a bit more next week, essentially fixes that I
      still want to sort through and test.
    
      The biggest item this time is the support to build the ppc64 LE kernel
      with our new v2 ABI.  We previously supported v2 userspace but the
      kernel itself was a tougher nut to crack.  This is now sorted mostly
      thanks to Anton and Rusty.
    
      We also have a fairly big series from Cedric that add support for
      64-bit LE zImage boot wrapper.  This was made harder by the fact that
      traditionally our zImage wrapper was always 32-bit, but our new LE
      toolchains don't really support 32-bit anymore (it's somewhat there
      but not really "supported") so we didn't want to rely on it.  This
      meant more churn that just endian fixes.
    
      This brings some more LE bits as well, such as the ability to run in
      LE mode without a hypervisor (ie. under OPAL firmware) by doing the
      right OPAL call to reinitialize the CPU to take HV interrupts in the
      right mode and the usual pile of endian fixes.
    
      There's another series from Gavin adding EEH improvements (one day we
      *will* have a release with less than 20 EEH patches, I promise!).
    
      Another highlight is the support for the "Split core" functionality on
      P8 by Michael.  This allows a P8 core to be split into "sub cores" of
      4 threads which allows the subcores to run different guests under KVM
      (the HW still doesn't support a partition per thread).
    
      And then the usual misc bits and fixes ..."
    
    [ Further delayed by gmail deciding that BenH is a dirty spammer.
      Google knows.  ]
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (155 commits)
      powerpc/powernv: Add missing include to LPC code
      selftests/powerpc: Test the THP bug we fixed in the previous commit
      powerpc/mm: Check paca psize is up to date for huge mappings
      powerpc/powernv: Pass buffer size to OPAL validate flash call
      powerpc/pseries: hcall functions are exported to modules, need _GLOBAL_TOC()
      powerpc: Exported functions __clear_user and copy_page use r2 so need _GLOBAL_TOC()
      powerpc/powernv: Set memory_block_size_bytes to 256MB
      powerpc: Allow ppc_md platform hook to override memory_block_size_bytes
      powerpc/powernv: Fix endian issues in memory error handling code
      powerpc/eeh: Skip eeh sysfs when eeh is disabled
      powerpc: 64bit sendfile is capped at 2GB
      powerpc/powernv: Provide debugfs access to the LPC bus via OPAL
      powerpc/serial: Use saner flags when creating legacy ports
      powerpc: Add cpu family documentation
      powerpc/xmon: Fix up xmon format strings
      powerpc/powernv: Add calls to support little endian host
      powerpc: Document sysfs DSCR interface
      powerpc: Fix regression of per-CPU DSCR setting
      powerpc: Split __SYSFS_SPRSETUP macro
      arch: powerpc/fadump: Cleaning up inconsistent NULL checks
      ...

commit 5deb8e7ad8ac7e3fcdfa042acff617f461b361c2
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Apr 24 13:46:24 2014 +0200

    KVM: PPC: Make shared struct aka magic page guest endian
    
    The shared (magic) page is a data structure that contains often used
    supervisor privileged SPRs accessible via memory to the user to reduce
    the number of exits we have to take to read/write them.
    
    When we actually share this structure with the guest we have to maintain
    it in guest endianness, because some of the patch tricks only work with
    native endian load/store operations.
    
    Since we only share the structure with either host or guest in little
    endian on book3s_64 pr mode, we don't have to worry about booke or book3s hv.
    
    For booke, the shared struct stays big endian. For book3s_64 hv we maintain
    the struct in host native endian, since it never gets shared with the guest.
    
    For book3s_64 pr we introduce a variable that tells us which endianness the
    shared struct is in and route every access to it through helper inline
    functions that evaluate this variable.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 4096f16502a9..4a7cc453be0b 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -448,6 +448,84 @@ static inline void kvmppc_mmu_flush_icache(pfn_t pfn)
 	}
 }
 
+/*
+ * Shared struct helpers. The shared struct can be little or big endian,
+ * depending on the guest endianness. So expose helpers to all of them.
+ */
+static inline bool kvmppc_shared_big_endian(struct kvm_vcpu *vcpu)
+{
+#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_KVM_BOOK3S_PR_POSSIBLE)
+	/* Only Book3S_64 PR supports bi-endian for now */
+	return vcpu->arch.shared_big_endian;
+#elif defined(CONFIG_PPC_BOOK3S_64) && defined(__LITTLE_ENDIAN__)
+	/* Book3s_64 HV on little endian is always little endian */
+	return false;
+#else
+	return true;
+#endif
+}
+
+#define SHARED_WRAPPER_GET(reg, size)					\
+static inline u##size kvmppc_get_##reg(struct kvm_vcpu *vcpu)	\
+{									\
+	if (kvmppc_shared_big_endian(vcpu))				\
+	       return be##size##_to_cpu(vcpu->arch.shared->reg);	\
+	else								\
+	       return le##size##_to_cpu(vcpu->arch.shared->reg);	\
+}									\
+
+#define SHARED_WRAPPER_SET(reg, size)					\
+static inline void kvmppc_set_##reg(struct kvm_vcpu *vcpu, u##size val)	\
+{									\
+	if (kvmppc_shared_big_endian(vcpu))				\
+	       vcpu->arch.shared->reg = cpu_to_be##size(val);		\
+	else								\
+	       vcpu->arch.shared->reg = cpu_to_le##size(val);		\
+}									\
+
+#define SHARED_WRAPPER(reg, size)					\
+	SHARED_WRAPPER_GET(reg, size)					\
+	SHARED_WRAPPER_SET(reg, size)					\
+
+SHARED_WRAPPER(critical, 64)
+SHARED_WRAPPER(sprg0, 64)
+SHARED_WRAPPER(sprg1, 64)
+SHARED_WRAPPER(sprg2, 64)
+SHARED_WRAPPER(sprg3, 64)
+SHARED_WRAPPER(srr0, 64)
+SHARED_WRAPPER(srr1, 64)
+SHARED_WRAPPER(dar, 64)
+SHARED_WRAPPER_GET(msr, 64)
+static inline void kvmppc_set_msr_fast(struct kvm_vcpu *vcpu, u64 val)
+{
+	if (kvmppc_shared_big_endian(vcpu))
+	       vcpu->arch.shared->msr = cpu_to_be64(val);
+	else
+	       vcpu->arch.shared->msr = cpu_to_le64(val);
+}
+SHARED_WRAPPER(dsisr, 32)
+SHARED_WRAPPER(int_pending, 32)
+SHARED_WRAPPER(sprg4, 64)
+SHARED_WRAPPER(sprg5, 64)
+SHARED_WRAPPER(sprg6, 64)
+SHARED_WRAPPER(sprg7, 64)
+
+static inline u32 kvmppc_get_sr(struct kvm_vcpu *vcpu, int nr)
+{
+	if (kvmppc_shared_big_endian(vcpu))
+	       return be32_to_cpu(vcpu->arch.shared->sr[nr]);
+	else
+	       return le32_to_cpu(vcpu->arch.shared->sr[nr]);
+}
+
+static inline void kvmppc_set_sr(struct kvm_vcpu *vcpu, int nr, u32 val)
+{
+	if (kvmppc_shared_big_endian(vcpu))
+	       vcpu->arch.shared->sr[nr] = cpu_to_be32(val);
+	else
+	       vcpu->arch.shared->sr[nr] = cpu_to_le32(val);
+}
+
 /*
  * Please call after prepare_to_enter. This function puts the lazy ee and irq
  * disabled tracking state back to normal mode, without actually enabling
@@ -485,7 +563,7 @@ static inline ulong kvmppc_get_ea_indexed(struct kvm_vcpu *vcpu, int ra, int rb)
 	msr_64bit = MSR_SF;
 #endif
 
-	if (!(vcpu->arch.shared->msr & msr_64bit))
+	if (!(kvmppc_get_msr(vcpu) & msr_64bit))
 		ea = (uint32_t)ea;
 
 	return ea;

commit 441c19c8a290f5f1e1b263691641124c84232b6e
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri May 23 18:15:25 2014 +1000

    powerpc/kvm/book3s_hv: Rework the secondary inhibit code
    
    As part of the support for split core on POWER8, we want to be able to
    block splitting of the core while KVM VMs are active.
    
    The logic to do that would be exactly the same as the code we currently
    have for inhibiting onlining of secondaries.
    
    Instead of adding an identical mechanism to block split core, rework the
    secondary inhibit code to be a "HV KVM is active" check. We can then use
    that in both the cpu hotplug code and the upcoming split core code.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Acked-by: Alexander Graf <agraf@suse.de>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 4096f16502a9..2c8e39951ab5 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -337,6 +337,10 @@ static inline void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu)
 	vcpu->kvm->arch.kvm_ops->fast_vcpu_kick(vcpu);
 }
 
+extern void kvm_hv_vm_activated(void);
+extern void kvm_hv_vm_deactivated(void);
+extern bool kvm_hv_mode_active(void);
+
 #else
 static inline void __init kvm_cma_reserve(void)
 {}
@@ -356,6 +360,9 @@ static inline void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu)
 {
 	kvm_vcpu_kick(vcpu);
 }
+
+static inline bool kvm_hv_mode_active(void)		{ return false; }
+
 #endif
 
 #ifdef CONFIG_KVM_XICS

commit 69e9fbb278af8de3059f1d1017b52a32b5f9f0bd
Author: Laurent Dufour <ldufour@linux.vnet.ibm.com>
Date:   Fri Feb 21 16:31:10 2014 +0100

    KVM: PPC: Book3S: Introduce hypervisor call H_GET_TCE
    
    This introduces the H_GET_TCE hypervisor call, which is basically the
    reverse of H_PUT_TCE, as defined in the Power Architecture Platform
    Requirements (PAPR).
    
    The hcall H_GET_TCE is required by the kdump kernel, which uses it to
    retrieve TCEs set up by the previous (panicked) kernel.
    
    Signed-off-by: Laurent Dufour <ldufour@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index fcd53f0d34ba..4096f16502a9 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -129,6 +129,8 @@ extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce *args);
 extern long kvmppc_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
 			     unsigned long ioba, unsigned long tce);
+extern long kvmppc_h_get_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
+			     unsigned long ioba);
 extern struct kvm_rma_info *kvm_alloc_rma(void);
 extern void kvm_release_rma(struct kvm_rma_info *ri);
 extern struct page *kvm_alloc_hpt(unsigned long nr_pages);

commit 6c85f52b10fd60e45c6e30c5b85d116406bd3c9b
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Jan 9 19:18:40 2014 -0600

    kvm/ppc: IRQ disabling cleanup
    
    Simplify the handling of lazy EE by going directly from fully-enabled
    to hard-disabled.  This replaces the lazy_irq_pending() check
    (including its misplaced kvm_guest_exit() call).
    
    As suggested by Tiejun Chen, move the interrupt disabling into
    kvmppc_prepare_to_enter() rather than have each caller do it.  Also
    move the IRQ enabling on heavyweight exit into
    kvmppc_prepare_to_enter().
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 629277df4798..fcd53f0d34ba 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -456,6 +456,12 @@ static inline void kvmppc_fix_ee_before_entry(void)
 	trace_hardirqs_on();
 
 #ifdef CONFIG_PPC64
+	/*
+	 * To avoid races, the caller must have gone directly from having
+	 * interrupts fully-enabled to hard-disabled.
+	 */
+	WARN_ON(local_paca->irq_happened != PACA_IRQ_HARD_DIS);
+
 	/* Only need to enable IRQs by hard enabling them after this */
 	local_paca->irq_happened = 0;
 	local_paca->soft_enabled = 1;

commit 736017752d2f6ed0d64f5e15cf48e79779b11c85
Author: Cédric Le Goater <clg@fr.ibm.com>
Date:   Thu Jan 9 11:51:16 2014 +0100

    KVM: PPC: Book3S: MMIO emulation support for little endian guests
    
    MMIO emulation reads the last instruction executed by the guest
    and then emulates. If the guest is running in Little Endian order,
    or more generally in a different endian order of the host, the
    instruction needs to be byte-swapped before being emulated.
    
    This patch adds a helper routine which tests the endian order of
    the host and the guest in order to decide whether a byteswap is
    needed or not. It is then used to byteswap the last instruction
    of the guest in the endian order of the host before MMIO emulation
    is performed.
    
    Finally, kvmppc_handle_load() of kvmppc_handle_store() are modified
    to reverse the endianness of the MMIO if required.
    
    Signed-off-by: Cédric Le Goater <clg@fr.ibm.com>
    [agraf: add booke handling]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c8317fbf92c4..629277df4798 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -54,12 +54,13 @@ extern void kvmppc_handler_highmem(void);
 extern void kvmppc_dump_vcpu(struct kvm_vcpu *vcpu);
 extern int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
                               unsigned int rt, unsigned int bytes,
-                              int is_bigendian);
+			      int is_default_endian);
 extern int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
                                unsigned int rt, unsigned int bytes,
-                               int is_bigendian);
+			       int is_default_endian);
 extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
-                               u64 val, unsigned int bytes, int is_bigendian);
+			       u64 val, unsigned int bytes,
+			       int is_default_endian);
 
 extern int kvmppc_emulate_instruction(struct kvm_run *run,
                                       struct kvm_vcpu *vcpu);

commit a78b55d1c0218b6d91d504941d20e36435c276f5
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:18:02 2013 +0530

    kvm: powerpc: book3s: drop is_hv_enabled
    
    drop is_hv_enabled, because that should not be a callback property
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 3069cf4dcc88..c8317fbf92c4 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -183,7 +183,6 @@ union kvmppc_one_reg {
 
 struct kvmppc_ops {
 	struct module *owner;
-	bool is_hv_enabled;
 	int (*get_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 	int (*set_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 	int (*get_one_reg)(struct kvm_vcpu *vcpu, u64 id,
@@ -232,6 +231,11 @@ struct kvmppc_ops {
 extern struct kvmppc_ops *kvmppc_hv_ops;
 extern struct kvmppc_ops *kvmppc_pr_ops;
 
+static inline bool is_kvmppc_hv_enabled(struct kvm *kvm)
+{
+	return kvm->arch.kvm_ops == kvmppc_hv_ops;
+}
+
 /*
  * Cuts out inst bits with ordering according to spec.
  * That means the leftmost bit is zero. All given bits are included.

commit cbbc58d4fdfab1a39a6ac1b41fcb17885952157a
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:18:01 2013 +0530

    kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine
    
    This moves the kvmppc_ops callbacks to be a per VM entity. This
    enables us to select HV and PR mode when creating a VM. We also
    allow both kvm-hv and kvm-pr kernel module to be loaded. To
    achieve this we move /dev/kvm ownership to kvm.ko module. Depending on
    which KVM mode we select during VM creation we take a reference
    count on respective module
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [agraf: fix coding style]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 20f461637090..3069cf4dcc88 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -182,6 +182,7 @@ union kvmppc_one_reg {
 };
 
 struct kvmppc_ops {
+	struct module *owner;
 	bool is_hv_enabled;
 	int (*get_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 	int (*set_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
@@ -217,7 +218,6 @@ struct kvmppc_ops {
 			      unsigned long npages);
 	int (*init_vm)(struct kvm *kvm);
 	void (*destroy_vm)(struct kvm *kvm);
-	int (*check_processor_compat)(void);
 	int (*get_smmu_info)(struct kvm *kvm, struct kvm_ppc_smmu_info *info);
 	int (*emulate_op)(struct kvm_run *run, struct kvm_vcpu *vcpu,
 			  unsigned int inst, int *advance);
@@ -229,7 +229,8 @@ struct kvmppc_ops {
 
 };
 
-extern struct kvmppc_ops *kvmppc_ops;
+extern struct kvmppc_ops *kvmppc_hv_ops;
+extern struct kvmppc_ops *kvmppc_pr_ops;
 
 /*
  * Cuts out inst bits with ordering according to spec.
@@ -326,7 +327,7 @@ static inline void kvmppc_set_host_ipi(int cpu, u8 host_ipi)
 
 static inline void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu)
 {
-	kvmppc_ops->fast_vcpu_kick(vcpu);
+	vcpu->kvm->arch.kvm_ops->fast_vcpu_kick(vcpu);
 }
 
 #else

commit 5587027ce9d59a57aecaa190be1c8e560aaff45d
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:18:00 2013 +0530

    kvm: Add struct kvm arg to memslot APIs
    
    We will use that in the later patch to find the kvm ops handler
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c13f15db476c..20f461637090 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -134,9 +134,11 @@ extern struct page *kvm_alloc_hpt(unsigned long nr_pages);
 extern void kvm_release_hpt(struct page *page, unsigned long nr_pages);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
 extern void kvmppc_core_destroy_vm(struct kvm *kvm);
-extern void kvmppc_core_free_memslot(struct kvm_memory_slot *free,
+extern void kvmppc_core_free_memslot(struct kvm *kvm,
+				     struct kvm_memory_slot *free,
 				     struct kvm_memory_slot *dont);
-extern int kvmppc_core_create_memslot(struct kvm_memory_slot *slot,
+extern int kvmppc_core_create_memslot(struct kvm *kvm,
+				      struct kvm_memory_slot *slot,
 				      unsigned long npages);
 extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,

commit 699cc87641c123128bf3a4e12c0a8d739b1ac2f3
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:17:56 2013 +0530

    kvm: powerpc: book3s: Add is_hv_enabled to kvmppc_ops
    
    This help us to identify whether we are running with hypervisor mode KVM
    enabled. The change is needed so that we can have both HV and PR kvm
    enabled in the same kernel.
    
    If both HV and PR KVM are included, interrupts come in to the HV version
    of the kvmppc_interrupt code, which then jumps to the PR handler,
    renamed to kvmppc_interrupt_pr, if the guest is a PR guest.
    
    Allowing both PR and HV in the same kernel required some changes to
    kvm_dev_ioctl_check_extension(), since the values returned now can't
    be selected with #ifdefs as much as previously. We look at is_hv_enabled
    to return the right value when checking for capabilities.For capabilities that
    are only provided by HV KVM, we return the HV value only if
    is_hv_enabled is true. For capabilities provided by PR KVM but not HV,
    we return the PR value only if is_hv_enabled is false.
    
    NOTE: in later patch we replace is_hv_enabled with a static inline
    function comparing kvm_ppc_ops
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 326033c99385..c13f15db476c 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -180,6 +180,7 @@ union kvmppc_one_reg {
 };
 
 struct kvmppc_ops {
+	bool is_hv_enabled;
 	int (*get_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 	int (*set_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 	int (*get_one_reg)(struct kvm_vcpu *vcpu, u64 id,
@@ -309,10 +310,10 @@ static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 
 static inline u32 kvmppc_get_xics_latch(void)
 {
-	u32 xirr = get_paca()->kvm_hstate.saved_xirr;
+	u32 xirr;
 
+	xirr = get_paca()->kvm_hstate.saved_xirr;
 	get_paca()->kvm_hstate.saved_xirr = 0;
-
 	return xirr;
 }
 

commit 3a167beac07cba597856c12b87638a06b0d53db7
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:17:53 2013 +0530

    kvm: powerpc: Add kvmppc_ops callback
    
    This patch add a new callback kvmppc_ops. This will help us in enabling
    both HV and PR KVM together in the same kernel. The actual change to
    enable them together is done in the later patch in the series.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [agraf: squash in booke changes]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 1823f38906c6..326033c99385 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -106,13 +106,6 @@ extern void kvmppc_core_queue_external(struct kvm_vcpu *vcpu,
                                        struct kvm_interrupt *irq);
 extern void kvmppc_core_dequeue_external(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_flush_tlb(struct kvm_vcpu *vcpu);
-
-extern int kvmppc_core_emulate_op(struct kvm_run *run, struct kvm_vcpu *vcpu,
-                                  unsigned int op, int *advance);
-extern int kvmppc_core_emulate_mtspr(struct kvm_vcpu *vcpu, int sprn,
-				     ulong val);
-extern int kvmppc_core_emulate_mfspr(struct kvm_vcpu *vcpu, int sprn,
-				     ulong *val);
 extern int kvmppc_core_check_requests(struct kvm_vcpu *vcpu);
 
 extern int kvmppc_booke_init(void);
@@ -135,8 +128,6 @@ extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce *args);
 extern long kvmppc_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
 			     unsigned long ioba, unsigned long tce);
-extern long kvm_vm_ioctl_allocate_rma(struct kvm *kvm,
-				struct kvm_allocate_rma *rma);
 extern struct kvm_rma_info *kvm_alloc_rma(void);
 extern void kvm_release_rma(struct kvm_rma_info *ri);
 extern struct page *kvm_alloc_hpt(unsigned long nr_pages);
@@ -177,6 +168,66 @@ extern int kvmppc_xics_get_xive(struct kvm *kvm, u32 irq, u32 *server,
 extern int kvmppc_xics_int_on(struct kvm *kvm, u32 irq);
 extern int kvmppc_xics_int_off(struct kvm *kvm, u32 irq);
 
+union kvmppc_one_reg {
+	u32	wval;
+	u64	dval;
+	vector128 vval;
+	u64	vsxval[2];
+	struct {
+		u64	addr;
+		u64	length;
+	}	vpaval;
+};
+
+struct kvmppc_ops {
+	int (*get_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+	int (*set_sregs)(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+	int (*get_one_reg)(struct kvm_vcpu *vcpu, u64 id,
+			   union kvmppc_one_reg *val);
+	int (*set_one_reg)(struct kvm_vcpu *vcpu, u64 id,
+			   union kvmppc_one_reg *val);
+	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);
+	void (*vcpu_put)(struct kvm_vcpu *vcpu);
+	void (*set_msr)(struct kvm_vcpu *vcpu, u64 msr);
+	int (*vcpu_run)(struct kvm_run *run, struct kvm_vcpu *vcpu);
+	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned int id);
+	void (*vcpu_free)(struct kvm_vcpu *vcpu);
+	int (*check_requests)(struct kvm_vcpu *vcpu);
+	int (*get_dirty_log)(struct kvm *kvm, struct kvm_dirty_log *log);
+	void (*flush_memslot)(struct kvm *kvm, struct kvm_memory_slot *memslot);
+	int (*prepare_memory_region)(struct kvm *kvm,
+				     struct kvm_memory_slot *memslot,
+				     struct kvm_userspace_memory_region *mem);
+	void (*commit_memory_region)(struct kvm *kvm,
+				     struct kvm_userspace_memory_region *mem,
+				     const struct kvm_memory_slot *old);
+	int (*unmap_hva)(struct kvm *kvm, unsigned long hva);
+	int (*unmap_hva_range)(struct kvm *kvm, unsigned long start,
+			   unsigned long end);
+	int (*age_hva)(struct kvm *kvm, unsigned long hva);
+	int (*test_age_hva)(struct kvm *kvm, unsigned long hva);
+	void (*set_spte_hva)(struct kvm *kvm, unsigned long hva, pte_t pte);
+	void (*mmu_destroy)(struct kvm_vcpu *vcpu);
+	void (*free_memslot)(struct kvm_memory_slot *free,
+			     struct kvm_memory_slot *dont);
+	int (*create_memslot)(struct kvm_memory_slot *slot,
+			      unsigned long npages);
+	int (*init_vm)(struct kvm *kvm);
+	void (*destroy_vm)(struct kvm *kvm);
+	int (*check_processor_compat)(void);
+	int (*get_smmu_info)(struct kvm *kvm, struct kvm_ppc_smmu_info *info);
+	int (*emulate_op)(struct kvm_run *run, struct kvm_vcpu *vcpu,
+			  unsigned int inst, int *advance);
+	int (*emulate_mtspr)(struct kvm_vcpu *vcpu, int sprn, ulong spr_val);
+	int (*emulate_mfspr)(struct kvm_vcpu *vcpu, int sprn, ulong *spr_val);
+	void (*fast_vcpu_kick)(struct kvm_vcpu *vcpu);
+	long (*arch_vm_ioctl)(struct file *filp, unsigned int ioctl,
+			      unsigned long arg);
+
+};
+
+extern struct kvmppc_ops *kvmppc_ops;
+
 /*
  * Cuts out inst bits with ordering according to spec.
  * That means the leftmost bit is zero. All given bits are included.
@@ -210,17 +261,6 @@ static inline u32 kvmppc_set_field(u64 inst, int msb, int lsb, int value)
 	return r;
 }
 
-union kvmppc_one_reg {
-	u32	wval;
-	u64	dval;
-	vector128 vval;
-	u64	vsxval[2];
-	struct {
-		u64	addr;
-		u64	length;
-	}	vpaval;
-};
-
 #define one_reg_size(id)	\
 	(1ul << (((id) & KVM_REG_SIZE_MASK) >> KVM_REG_SIZE_SHIFT))
 
@@ -245,10 +285,10 @@ union kvmppc_one_reg {
 	__v;					\
 })
 
-void kvmppc_core_get_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+int kvmppc_core_get_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 int kvmppc_core_set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 
-void kvmppc_get_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+int kvmppc_get_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 int kvmppc_set_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 
 int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg);
@@ -281,7 +321,10 @@ static inline void kvmppc_set_host_ipi(int cpu, u8 host_ipi)
 	paca[cpu].kvm_hstate.host_ipi = host_ipi;
 }
 
-extern void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu);
+static inline void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu)
+{
+	kvmppc_ops->fast_vcpu_kick(vcpu);
+}
 
 #else
 static inline void __init kvm_cma_reserve(void)

commit 9975f5e3692d320b4259a4d2edd8a979adb1e535
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:17:52 2013 +0530

    kvm: powerpc: book3s: Add a new config variable CONFIG_KVM_BOOK3S_HV_POSSIBLE
    
    This help ups to select the relevant code in the kernel code
    when we later move HV and PR bits as seperate modules. The patch
    also makes the config options for PR KVM selectable
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index b15554a26c20..1823f38906c6 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -260,7 +260,7 @@ void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 pid);
 
 struct openpic;
 
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 extern void kvm_cma_reserve(void) __init;
 static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 {

commit 5f1c248f52c12e155e0fe6a614178181f7629901
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Jul 10 17:47:39 2013 -0500

    kvm/ppc: Call trace_hardirqs_on before entry
    
    Currently this is only being done on 64-bit.  Rather than just move it
    out of the 64-bit ifdef, move it to kvm_lazy_ee_enable() so that it is
    consistent with lazy ee state, and so that we don't track more host
    code as interrupts-enabled than necessary.
    
    Rename kvm_lazy_ee_enable() to kvm_fix_ee_before_entry() to reflect
    that this function now has a role on 32-bit as well.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 5a26bfcd0bbc..b15554a26c20 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -394,10 +394,15 @@ static inline void kvmppc_mmu_flush_icache(pfn_t pfn)
 	}
 }
 
-/* Please call after prepare_to_enter. This function puts the lazy ee state
-   back to normal mode, without actually enabling interrupts. */
-static inline void kvmppc_lazy_ee_enable(void)
+/*
+ * Please call after prepare_to_enter. This function puts the lazy ee and irq
+ * disabled tracking state back to normal mode, without actually enabling
+ * interrupts.
+ */
+static inline void kvmppc_fix_ee_before_entry(void)
 {
+	trace_hardirqs_on();
+
 #ifdef CONFIG_PPC64
 	/* Only need to enable IRQs by hard enabling them after this */
 	local_paca->irq_happened = 0;

commit 6c45b810989d1c04194499d666f695d3f811965f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 2 11:15:17 2013 +0530

    powerpc/kvm: Contiguous memory allocator based RMA allocation
    
    Older version of power architecture use Real Mode Offset register and Real Mode Limit
    Selector for mapping guest Real Mode Area. The guest RMA should be physically
    contigous since we use the range when address translation is not enabled.
    
    This patch switch RMA allocation code to use contigous memory allocator. The patch
    also remove the the linear allocator which not used any more
    
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index b5ef7a3c606b..5a26bfcd0bbc 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -137,8 +137,8 @@ extern long kvmppc_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
 			     unsigned long ioba, unsigned long tce);
 extern long kvm_vm_ioctl_allocate_rma(struct kvm *kvm,
 				struct kvm_allocate_rma *rma);
-extern struct kvmppc_linear_info *kvm_alloc_rma(void);
-extern void kvm_release_rma(struct kvmppc_linear_info *ri);
+extern struct kvm_rma_info *kvm_alloc_rma(void);
+extern void kvm_release_rma(struct kvm_rma_info *ri);
 extern struct page *kvm_alloc_hpt(unsigned long nr_pages);
 extern void kvm_release_hpt(struct page *page, unsigned long nr_pages);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
@@ -282,7 +282,6 @@ static inline void kvmppc_set_host_ipi(int cpu, u8 host_ipi)
 }
 
 extern void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu);
-extern void kvm_linear_init(void);
 
 #else
 static inline void __init kvm_cma_reserve(void)
@@ -291,9 +290,6 @@ static inline void __init kvm_cma_reserve(void)
 static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 {}
 
-static inline void kvm_linear_init(void)
-{}
-
 static inline u32 kvmppc_get_xics_latch(void)
 {
 	return 0;

commit fa61a4e376d2129690c82dfb05b31705a67d6e0b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 2 11:15:16 2013 +0530

    powerpc/kvm: Contiguous memory allocator based hash page table allocation
    
    Powerpc architecture uses a hash based page table mechanism for mapping virtual
    addresses to physical address. The architecture require this hash page table to
    be physically contiguous. With KVM on Powerpc currently we use early reservation
    mechanism for allocating guest hash page table. This implies that we need to
    reserve a big memory region to ensure we can create large number of guest
    simultaneously with KVM on Power. Another disadvantage is that the reserved memory
    is not available to rest of the subsystems and and that implies we limit the total
    available memory in the host.
    
    This patch series switch the guest hash page table allocation to use
    contiguous memory allocator.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index a5287fe03d77..b5ef7a3c606b 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -139,8 +139,8 @@ extern long kvm_vm_ioctl_allocate_rma(struct kvm *kvm,
 				struct kvm_allocate_rma *rma);
 extern struct kvmppc_linear_info *kvm_alloc_rma(void);
 extern void kvm_release_rma(struct kvmppc_linear_info *ri);
-extern struct kvmppc_linear_info *kvm_alloc_hpt(void);
-extern void kvm_release_hpt(struct kvmppc_linear_info *li);
+extern struct page *kvm_alloc_hpt(unsigned long nr_pages);
+extern void kvm_release_hpt(struct page *page, unsigned long nr_pages);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
 extern void kvmppc_core_destroy_vm(struct kvm *kvm);
 extern void kvmppc_core_free_memslot(struct kvm_memory_slot *free,
@@ -261,6 +261,7 @@ void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 pid);
 struct openpic;
 
 #ifdef CONFIG_KVM_BOOK3S_64_HV
+extern void kvm_cma_reserve(void) __init;
 static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 {
 	paca[cpu].kvm_hstate.xics_phys = addr;
@@ -284,6 +285,9 @@ extern void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu);
 extern void kvm_linear_init(void);
 
 #else
+static inline void __init kvm_cma_reserve(void)
+{}
+
 static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 {}
 

commit 5975a2e0950291a6bfe9fd5880e7952ff87764be
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Apr 27 00:28:37 2013 +0000

    KVM: PPC: Book3S: Add API for in-kernel XICS emulation
    
    This adds the API for userspace to instantiate an XICS device in a VM
    and connect VCPUs to it.  The API consists of a new device type for
    the KVM_CREATE_DEVICE ioctl, a new capability KVM_CAP_IRQ_XICS, which
    functions similarly to KVM_CAP_IRQ_MPIC, and the KVM_IRQ_LINE ioctl,
    which is used to assert and deassert interrupt inputs of the XICS.
    
    The XICS device has one attribute group, KVM_DEV_XICS_GRP_SOURCES.
    Each attribute within this group corresponds to the state of one
    interrupt source.  The attribute number is the same as the interrupt
    source number.
    
    This does not support irq routing or irqfd yet.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index d7339df19259..a5287fe03d77 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -315,6 +315,8 @@ extern int kvm_vm_ioctl_xics_irq(struct kvm *kvm, struct kvm_irq_level *args);
 extern int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd);
 extern u64 kvmppc_xics_get_icp(struct kvm_vcpu *vcpu);
 extern int kvmppc_xics_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
+extern int kvmppc_xics_connect_vcpu(struct kvm_device *dev,
+			struct kvm_vcpu *vcpu, u32 cpu);
 #else
 static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
 	{ return 0; }

commit 8b78645c93b5d469e8006d68dbc92edc2640c654
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Apr 17 20:32:26 2013 +0000

    KVM: PPC: Book3S: Facilities to save/restore XICS presentation ctrler state
    
    This adds the ability for userspace to save and restore the state
    of the XICS interrupt presentation controllers (ICPs) via the
    KVM_GET/SET_ONE_REG interface.  Since there is one ICP per vcpu, we
    simply define a new 64-bit register in the ONE_REG space for the ICP
    state.  The state includes the CPU priority setting, the pending IPI
    priority, and the priority and source number of any pending external
    interrupt.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index cfaa47995c0e..d7339df19259 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -313,6 +313,8 @@ extern void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu);
 extern int kvmppc_xics_create_icp(struct kvm_vcpu *vcpu, unsigned long server);
 extern int kvm_vm_ioctl_xics_irq(struct kvm *kvm, struct kvm_irq_level *args);
 extern int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd);
+extern u64 kvmppc_xics_get_icp(struct kvm_vcpu *vcpu);
+extern int kvmppc_xics_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
 #else
 static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
 	{ return 0; }

commit d19bd86204f85d42873e07bb64a27587fc380b5b
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Apr 17 20:32:04 2013 +0000

    KVM: PPC: Book3S: Add support for ibm,int-on/off RTAS calls
    
    This adds support for the ibm,int-on and ibm,int-off RTAS calls to the
    in-kernel XICS emulation and corrects the handling of the saved
    priority by the ibm,set-xive RTAS call.  With this, ibm,int-off sets
    the specified interrupt's priority in its saved_priority field and
    sets the priority to 0xff (the least favoured value).  ibm,int-on
    restores the saved_priority to the priority field, and ibm,set-xive
    sets both the priority and the saved_priority to the specified
    priority value.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 1589fd8bf063..cfaa47995c0e 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -174,6 +174,8 @@ extern int kvmppc_xics_set_xive(struct kvm *kvm, u32 irq, u32 server,
 				u32 priority);
 extern int kvmppc_xics_get_xive(struct kvm *kvm, u32 irq, u32 *server,
 				u32 *priority);
+extern int kvmppc_xics_int_on(struct kvm *kvm, u32 irq);
+extern int kvmppc_xics_int_off(struct kvm *kvm, u32 irq);
 
 /*
  * Cuts out inst bits with ordering according to spec.

commit 54695c3088a74e25474db8eb6b490b45d1aeb0ca
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 17 20:30:50 2013 +0000

    KVM: PPC: Book3S HV: Speed up wakeups of CPUs on HV KVM
    
    Currently, we wake up a CPU by sending a host IPI with
    smp_send_reschedule() to thread 0 of that core, which will take all
    threads out of the guest, and cause them to re-evaluate their
    interrupt status on the way back in.
    
    This adds a mechanism to differentiate real host IPIs from IPIs sent
    by KVM for guest threads to poke each other, in order to target the
    guest threads precisely when possible and avoid that global switch of
    the core to host state.
    
    We then use this new facility in the in-kernel XICS code.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 6582eed321ba..1589fd8bf063 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -264,6 +264,21 @@ static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 	paca[cpu].kvm_hstate.xics_phys = addr;
 }
 
+static inline u32 kvmppc_get_xics_latch(void)
+{
+	u32 xirr = get_paca()->kvm_hstate.saved_xirr;
+
+	get_paca()->kvm_hstate.saved_xirr = 0;
+
+	return xirr;
+}
+
+static inline void kvmppc_set_host_ipi(int cpu, u8 host_ipi)
+{
+	paca[cpu].kvm_hstate.host_ipi = host_ipi;
+}
+
+extern void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu);
 extern void kvm_linear_init(void);
 
 #else
@@ -273,6 +288,18 @@ static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 static inline void kvm_linear_init(void)
 {}
 
+static inline u32 kvmppc_get_xics_latch(void)
+{
+	return 0;
+}
+
+static inline void kvmppc_set_host_ipi(int cpu, u8 host_ipi)
+{}
+
+static inline void kvmppc_fast_vcpu_kick(struct kvm_vcpu *vcpu)
+{
+	kvm_vcpu_kick(vcpu);
+}
 #endif
 
 #ifdef CONFIG_KVM_XICS
@@ -393,4 +420,6 @@ static inline ulong kvmppc_get_ea_indexed(struct kvm_vcpu *vcpu, int ra, int rb)
 	return ea;
 }
 
+extern void xics_wake_cpu(int cpu);
+
 #endif /* __POWERPC_KVM_PPC_H__ */

commit bc5ad3f3701116e7db57268e6f89010ec714697e
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 17 20:30:26 2013 +0000

    KVM: PPC: Book3S: Add kernel emulation for the XICS interrupt controller
    
    This adds in-kernel emulation of the XICS (eXternal Interrupt
    Controller Specification) interrupt controller specified by PAPR, for
    both HV and PR KVM guests.
    
    The XICS emulation supports up to 1048560 interrupt sources.
    Interrupt source numbers below 16 are reserved; 0 is used to mean no
    interrupt and 2 is used for IPIs.  Internally these are represented in
    blocks of 1024, called ICS (interrupt controller source) entities, but
    that is not visible to userspace.
    
    Each vcpu gets one ICP (interrupt controller presentation) entity,
    used to store the per-vcpu state such as vcpu priority, pending
    interrupt state, IPI request, etc.
    
    This does not include any API or any way to connect vcpus to their
    ICP state; that will be added in later patches.
    
    This is based on an initial implementation by Michael Ellerman
    <michael@ellerman.id.au> reworked by Benjamin Herrenschmidt and
    Paul Mackerras.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix typo, add dependency on !KVM_MPIC]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 8a30eb7f2bec..6582eed321ba 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -130,6 +130,7 @@ extern long kvmppc_prepare_vrma(struct kvm *kvm,
 extern void kvmppc_map_vrma(struct kvm_vcpu *vcpu,
 			struct kvm_memory_slot *memslot, unsigned long porder);
 extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
+
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce *args);
 extern long kvmppc_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
@@ -169,6 +170,10 @@ int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq);
 extern int kvm_vm_ioctl_rtas_define_token(struct kvm *kvm, void __user *argp);
 extern int kvmppc_rtas_hcall(struct kvm_vcpu *vcpu);
 extern void kvmppc_rtas_tokens_free(struct kvm *kvm);
+extern int kvmppc_xics_set_xive(struct kvm *kvm, u32 irq, u32 server,
+				u32 priority);
+extern int kvmppc_xics_get_xive(struct kvm *kvm, u32 irq, u32 *server,
+				u32 *priority);
 
 /*
  * Cuts out inst bits with ordering according to spec.
@@ -267,6 +272,30 @@ static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 
 static inline void kvm_linear_init(void)
 {}
+
+#endif
+
+#ifdef CONFIG_KVM_XICS
+static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
+{
+	return vcpu->arch.irq_type == KVMPPC_IRQ_XICS;
+}
+extern void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu);
+extern int kvmppc_xics_create_icp(struct kvm_vcpu *vcpu, unsigned long server);
+extern int kvm_vm_ioctl_xics_irq(struct kvm *kvm, struct kvm_irq_level *args);
+extern int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd);
+#else
+static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
+	{ return 0; }
+static inline void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu) { }
+static inline int kvmppc_xics_create_icp(struct kvm_vcpu *vcpu,
+					 unsigned long server)
+	{ return -EINVAL; }
+static inline int kvm_vm_ioctl_xics_irq(struct kvm *kvm,
+					struct kvm_irq_level *args)
+	{ return -ENOTTY; }
+static inline int kvmppc_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd)
+	{ return 0; }
 #endif
 
 static inline void kvmppc_set_epr(struct kvm_vcpu *vcpu, u32 epr)

commit 8e591cb7204739efa8e15967ea334eb367039dde
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed Apr 17 20:30:00 2013 +0000

    KVM: PPC: Book3S: Add infrastructure to implement kernel-side RTAS calls
    
    For pseries machine emulation, in order to move the interrupt
    controller code to the kernel, we need to intercept some RTAS
    calls in the kernel itself.  This adds an infrastructure to allow
    in-kernel handlers to be registered for RTAS services by name.
    A new ioctl, KVM_PPC_RTAS_DEFINE_TOKEN, then allows userspace to
    associate token values with those service names.  Then, when the
    guest requests an RTAS service with one of those token values, it
    will be handled by the relevant in-kernel handler rather than being
    passed up to userspace as at present.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix warning]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index df9c80b37905..8a30eb7f2bec 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -166,6 +166,10 @@ extern int kvm_vm_ioctl_get_htab_fd(struct kvm *kvm, struct kvm_get_htab_fd *);
 
 int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq);
 
+extern int kvm_vm_ioctl_rtas_define_token(struct kvm *kvm, void __user *argp);
+extern int kvmppc_rtas_hcall(struct kvm_vcpu *vcpu);
+extern void kvmppc_rtas_tokens_free(struct kvm *kvm);
+
 /*
  * Cuts out inst bits with ordering according to spec.
  * That means the leftmost bit is zero. All given bits are included.

commit eb1e4f43e0f47f2655372c7d32c43db9711c278e
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Apr 12 14:08:47 2013 +0000

    kvm/ppc/mpic: add KVM_CAP_IRQ_MPIC
    
    Enabling this capability connects the vcpu to the designated in-kernel
    MPIC.  Using explicit connections between vcpus and irqchips allows
    for flexibility, but the main benefit at the moment is that it
    simplifies the code -- KVM doesn't need vm-global state to remember
    which MPIC object is associated with this vm, and it doesn't need to
    care about ordering between irqchip creation and vcpu creation.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: add stub functions for kvmppc_mpic_{dis,}connect_vcpu]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 3810f9c7616c..df9c80b37905 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -248,7 +248,6 @@ int kvmppc_set_one_reg(struct kvm_vcpu *vcpu, u64 id, union kvmppc_one_reg *);
 void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 pid);
 
 struct openpic;
-void kvmppc_mpic_put(struct openpic *opp);
 
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
@@ -278,6 +277,9 @@ static inline void kvmppc_set_epr(struct kvm_vcpu *vcpu, u32 epr)
 #ifdef CONFIG_KVM_MPIC
 
 void kvmppc_mpic_set_epr(struct kvm_vcpu *vcpu);
+int kvmppc_mpic_connect_vcpu(struct kvm_device *dev, struct kvm_vcpu *vcpu,
+			     u32 cpu);
+void kvmppc_mpic_disconnect_vcpu(struct openpic *opp, struct kvm_vcpu *vcpu);
 
 #else
 
@@ -285,6 +287,17 @@ static inline void kvmppc_mpic_set_epr(struct kvm_vcpu *vcpu)
 {
 }
 
+static inline int kvmppc_mpic_connect_vcpu(struct kvm_device *dev,
+		struct kvm_vcpu *vcpu, u32 cpu)
+{
+	return -EINVAL;
+}
+
+static inline void kvmppc_mpic_disconnect_vcpu(struct openpic *opp,
+		struct kvm_vcpu *vcpu)
+{
+}
+
 #endif /* CONFIG_KVM_MPIC */
 
 int kvm_vcpu_ioctl_config_tlb(struct kvm_vcpu *vcpu,

commit 5df554ad5b7522ea62b0ff9d5be35183494efc21
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Apr 12 14:08:46 2013 +0000

    kvm/ppc/mpic: in-kernel MPIC emulation
    
    Hook the MPIC code up to the KVM interfaces, add locking, etc.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: add stub function for kvmppc_mpic_set_epr, non-booke, 64bit]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index bcc68b1afc66..3810f9c7616c 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -164,6 +164,8 @@ extern int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu);
 
 extern int kvm_vm_ioctl_get_htab_fd(struct kvm *kvm, struct kvm_get_htab_fd *);
 
+int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq);
+
 /*
  * Cuts out inst bits with ordering according to spec.
  * That means the leftmost bit is zero. All given bits are included.
@@ -245,6 +247,9 @@ int kvmppc_set_one_reg(struct kvm_vcpu *vcpu, u64 id, union kvmppc_one_reg *);
 
 void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 pid);
 
+struct openpic;
+void kvmppc_mpic_put(struct openpic *opp);
+
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 {
@@ -270,6 +275,18 @@ static inline void kvmppc_set_epr(struct kvm_vcpu *vcpu, u32 epr)
 #endif
 }
 
+#ifdef CONFIG_KVM_MPIC
+
+void kvmppc_mpic_set_epr(struct kvm_vcpu *vcpu);
+
+#else
+
+static inline void kvmppc_mpic_set_epr(struct kvm_vcpu *vcpu)
+{
+}
+
+#endif /* CONFIG_KVM_MPIC */
+
 int kvm_vcpu_ioctl_config_tlb(struct kvm_vcpu *vcpu,
 			      struct kvm_config_tlb *cfg);
 int kvm_vcpu_ioctl_dirty_tlb(struct kvm_vcpu *vcpu,

commit c402a3f457b9689451c4e422781026633a5b6287
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Mon Apr 8 00:32:13 2013 +0000

    Rename EMULATE_DO_PAPR to EMULATE_EXIT_USER
    
    Instruction emulation return EMULATE_DO_PAPR when it requires
    exit to userspace on book3s. Similar return is required
    for booke. EMULATE_DO_PAPR reads out to be confusing so it is
    renamed to EMULATE_EXIT_USER.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 4794de6ea379..bcc68b1afc66 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -44,7 +44,7 @@ enum emulation_result {
 	EMULATE_DO_DCR,       /* kvm_run filled with DCR request */
 	EMULATE_FAIL,         /* can't emulate this instruction */
 	EMULATE_AGAIN,        /* something went wrong. go again */
-	EMULATE_DO_PAPR,      /* kvm_run filled with PAPR request */
+	EMULATE_EXIT_USER,    /* emulation requires exit to user-space */
 };
 
 extern int kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);

commit adccf65ca431b41733483f476e8de9e3cf171c44
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Thu Apr 25 06:33:57 2013 +0000

    KVM: PPC: cache flush for kernel managed pages
    
    Kernel can only access pages which maps as memory.
    So flush only the valid kernel pages.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index f58930779ae8..4794de6ea379 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -282,8 +282,15 @@ void kvmppc_init_lpid(unsigned long nr_lpids);
 
 static inline void kvmppc_mmu_flush_icache(pfn_t pfn)
 {
-	/* Clear i-cache for new pages */
 	struct page *page;
+	/*
+	 * We can only access pages that the kernel maps
+	 * as memory. Bail out for unmapped ones.
+	 */
+	if (!pfn_valid(pfn))
+		return;
+
+	/* Clear i-cache for new pages */
 	page = pfn_to_page(pfn);
 	if (!test_bit(PG_arch_1, &page->flags)) {
 		flush_dcache_icache_page(page);

commit 4fe27d2addda8af7714546a69369fb92dddcf9a3
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Feb 14 14:00:25 2013 +0000

    KVM: PPC: Remove unused argument to kvmppc_core_dequeue_external
    
    Currently kvmppc_core_dequeue_external() takes a struct kvm_interrupt *
    argument and does nothing with it, in any of its implementations.
    This removes it in order to make things easier for forthcoming
    in-kernel interrupt controller emulation code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 44fa9ad1d62c..f58930779ae8 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -104,8 +104,7 @@ extern void kvmppc_core_queue_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_dequeue_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_external(struct kvm_vcpu *vcpu,
                                        struct kvm_interrupt *irq);
-extern void kvmppc_core_dequeue_external(struct kvm_vcpu *vcpu,
-                                         struct kvm_interrupt *irq);
+extern void kvmppc_core_dequeue_external(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_flush_tlb(struct kvm_vcpu *vcpu);
 
 extern int kvmppc_core_emulate_op(struct kvm_run *run, struct kvm_vcpu *vcpu,

commit 8482644aea11e0647867732319ccf35879a9acc2
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:45:25 2013 +0900

    KVM: set_memory_region: Refactor commit_memory_region()
    
    This patch makes the parameter old a const pointer to the old memory
    slot and adds a new parameter named change to know the change being
    requested: the former is for removing extra copying and the latter is
    for cleaning up the code.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 44a657adf416..44fa9ad1d62c 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -152,7 +152,7 @@ extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);
 extern void kvmppc_core_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
-				struct kvm_memory_slot old);
+				const struct kvm_memory_slot *old);
 extern int kvm_vm_ioctl_get_smmu_info(struct kvm *kvm,
 				      struct kvm_ppc_smmu_info *info);
 extern void kvmppc_core_flush_memslot(struct kvm *kvm,

commit 1d542d9c2bbca9b99835fef6a938b9ae9dd7ca2a
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Tue Jan 15 22:24:39 2013 +0000

    KVM: PPC: booke: Allow multiple exception types
    
    Current kvmppc_booke_handlers uses the same macro (KVM_HANDLER) and
    all handlers are considered to be the same size. This will not be
    the case if we want to use different macros for different handlers.
    
    This patch improves the kvmppc_booke_handler so that it can
    support different macros for different handlers.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    [bharat.bhushan@freescale.com: Substantial changes]
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 493630e209c8..44a657adf416 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -49,8 +49,6 @@ enum emulation_result {
 
 extern int kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
 extern int __kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
-extern char kvmppc_handlers_start[];
-extern unsigned long kvmppc_handler_len;
 extern void kvmppc_handler_highmem(void);
 
 extern void kvmppc_dump_vcpu(struct kvm_vcpu *vcpu);

commit 1c810636556c8d53a37406b34a64d9b9b0161aa6
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 4 18:12:48 2013 +0100

    KVM: PPC: BookE: Implement EPR exit
    
    The External Proxy Facility in FSL BookE chips allows the interrupt
    controller to automatically acknowledge an interrupt as soon as a
    core gets its pending external interrupt delivered.
    
    Today, user space implements the interrupt controller, so we need to
    check on it during such a cycle.
    
    This patch implements logic for user space to enable EPR exiting,
    disable EPR exiting and EPR exiting itself, so that user space can
    acknowledge an interrupt when an external interrupt has successfully
    been delivered into the guest vcpu.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 5f5f69abd281..493630e209c8 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -264,6 +264,15 @@ static inline void kvm_linear_init(void)
 {}
 #endif
 
+static inline void kvmppc_set_epr(struct kvm_vcpu *vcpu, u32 epr)
+{
+#ifdef CONFIG_KVM_BOOKE_HV
+	mtspr(SPRN_GEPR, epr);
+#elif defined(CONFIG_BOOKE)
+	vcpu->arch.epr = epr;
+#endif
+}
+
 int kvm_vcpu_ioctl_config_tlb(struct kvm_vcpu *vcpu,
 			      struct kvm_config_tlb *cfg);
 int kvm_vcpu_ioctl_dirty_tlb(struct kvm_vcpu *vcpu,

commit 50c7bb80b5bd5a9962905306dd2292eeb9857d46
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Dec 14 23:42:05 2012 +0100

    KVM: PPC: Book3S: PR: Enable alternative instruction for SC 1
    
    When running on top of pHyp, the hypercall instruction "sc 1" goes
    straight into pHyp without trapping in supervisor mode.
    
    So if we want to support PAPR guest in this configuration we need to
    add a second way of accessing PAPR hypercalls, preferably with the
    exact same semantics except for the instruction.
    
    So let's overlay an officially reserved instruction and emulate PAPR
    hypercalls whenever we hit that one.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 572aa7530619..5f5f69abd281 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -44,6 +44,7 @@ enum emulation_result {
 	EMULATE_DO_DCR,       /* kvm_run filled with DCR request */
 	EMULATE_FAIL,         /* can't emulate this instruction */
 	EMULATE_AGAIN,        /* something went wrong. go again */
+	EMULATE_DO_PAPR,      /* kvm_run filled with PAPR request */
 };
 
 extern int kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);

commit 8823a8fd0d730612f12a87102503622c01eb2468
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Thu Oct 11 06:13:23 2012 +0000

    KVM: PPC: Mask ea's high 32-bits in 32/64 instr emulation
    
    Mask high 32 bits of effective address in emulation layer for guests running
    in 32-bit mode.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    [agraf: fix indent]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index d55a2b28706e..572aa7530619 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -298,11 +298,21 @@ static inline void kvmppc_lazy_ee_enable(void)
 static inline ulong kvmppc_get_ea_indexed(struct kvm_vcpu *vcpu, int ra, int rb)
 {
 	ulong ea;
+	ulong msr_64bit = 0;
 
 	ea = kvmppc_get_gpr(vcpu, rb);
 	if (ra)
 		ea += kvmppc_get_gpr(vcpu, ra);
 
+#if defined(CONFIG_PPC_BOOK3E_64)
+	msr_64bit = MSR_CM;
+#elif defined(CONFIG_PPC_BOOK3S_64)
+	msr_64bit = MSR_SF;
+#endif
+
+	if (!(vcpu->arch.shared->msr & msr_64bit))
+		ea = (uint32_t)ea;
+
 	return ea;
 }
 

commit 7cdd7a95c66a6309ae6156471033fb5375cbcfca
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Thu Oct 11 06:13:22 2012 +0000

    KVM: PPC: e500: Add emulation helper for getting instruction ea
    
    Add emulation helper for getting instruction ea and refactor tlb instruction
    emulation to use it.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    [agraf: keep rt variable around]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 1ca31e92ee75..d55a2b28706e 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -295,4 +295,15 @@ static inline void kvmppc_lazy_ee_enable(void)
 #endif
 }
 
+static inline ulong kvmppc_get_ea_indexed(struct kvm_vcpu *vcpu, int ra, int rb)
+{
+	ulong ea;
+
+	ea = kvmppc_get_gpr(vcpu, rb);
+	if (ra)
+		ea += kvmppc_get_gpr(vcpu, ra);
+
+	return ea;
+}
+
 #endif /* __POWERPC_KVM_PPC_H__ */

commit a2932923ccf63c419c77aaa18ac09be98f2c94d8
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Nov 19 22:57:20 2012 +0000

    KVM: PPC: Book3S HV: Provide a method for userspace to read and write the HPT
    
    A new ioctl, KVM_PPC_GET_HTAB_FD, returns a file descriptor.  Reads on
    this fd return the contents of the HPT (hashed page table), writes
    create and/or remove entries in the HPT.  There is a new capability,
    KVM_CAP_PPC_HTAB_FD, to indicate the presence of the ioctl.  The ioctl
    takes an argument structure with the index of the first HPT entry to
    read out and a set of flags.  The flags indicate whether the user is
    intending to read or write the HPT, and whether to return all entries
    or only the "bolted" entries (those with the bolted bit, 0x10, set in
    the first doubleword).
    
    This is intended for use in implementing qemu's savevm/loadvm and for
    live migration.  Therefore, on reads, the first pass returns information
    about all HPTEs (or all bolted HPTEs).  When the first pass reaches the
    end of the HPT, it returns from the read.  Subsequent reads only return
    information about HPTEs that have changed since they were last read.
    A read that finds no changed HPTEs in the HPT following where the last
    read finished will return 0 bytes.
    
    The format of the data provides a simple run-length compression of the
    invalid entries.  Each block of data starts with a header that indicates
    the index (position in the HPT, which is just an array), the number of
    valid entries starting at that index (may be zero), and the number of
    invalid entries following those valid entries.  The valid entries, 16
    bytes each, follow the header.  The invalid entries are not explicitly
    represented.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix documentation]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 609cca3e9426..1ca31e92ee75 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -164,6 +164,8 @@ extern void kvmppc_bookehv_exit(void);
 
 extern int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu);
 
+extern int kvm_vm_ioctl_get_htab_fd(struct kvm *kvm, struct kvm_get_htab_fd *);
+
 /*
  * Cuts out inst bits with ordering according to spec.
  * That means the leftmost bit is zero. All given bits are included.

commit 55b665b0263ae88a776071306ef1eee4b769016b
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 25 20:33:06 2012 +0000

    KVM: PPC: Book3S HV: Provide a way for userspace to get/set per-vCPU areas
    
    The PAPR paravirtualization interface lets guests register three
    different types of per-vCPU buffer areas in its memory for communication
    with the hypervisor.  These are called virtual processor areas (VPAs).
    Currently the hypercalls to register and unregister VPAs are handled
    by KVM in the kernel, and userspace has no way to know about or save
    and restore these registrations across a migration.
    
    This adds "register" codes for these three areas that userspace can
    use with the KVM_GET/SET_ONE_REG ioctls to see what addresses have
    been registered, and to register or unregister them.  This will be
    needed for guest hibernation and migration, and is also needed so
    that userspace can unregister them on reset (otherwise we corrupt
    guest memory after reboot by writing to the VPAs registered by the
    previous kernel).
    
    The "register" for the VPA is a 64-bit value containing the address,
    since the length of the VPA is fixed.  The "registers" for the SLB
    shadow buffer and dispatch trace log (DTL) are 128 bits long,
    consisting of the guest physical address in the high (first) 64 bits
    and the length in the low 64 bits.
    
    This also fixes a bug where we were calling init_vpa unconditionally,
    leading to an oops when unregistering the VPA.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 51604a16c8a5..609cca3e9426 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -202,6 +202,10 @@ union kvmppc_one_reg {
 	u64	dval;
 	vector128 vval;
 	u64	vsxval[2];
+	struct {
+		u64	addr;
+		u64	length;
+	}	vpaval;
 };
 
 #define one_reg_size(id)	\

commit a8bd19ef4dd49f0eef86a4a8eb43d60f967236b8
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 25 20:32:30 2012 +0000

    KVM: PPC: Book3S: Get/set guest FP regs using the GET/SET_ONE_REG interface
    
    This enables userspace to get and set all the guest floating-point
    state using the KVM_[GS]ET_ONE_REG ioctls.  The floating-point state
    includes all of the traditional floating-point registers and the
    FPSCR (floating point status/control register), all the VMX/Altivec
    vector registers and the VSCR (vector status/control register), and
    on POWER7, the vector-scalar registers (note that each FP register
    is the high-order half of the corresponding VSR).
    
    Most of these are implemented in common Book 3S code, except for VSX
    on POWER7.  Because HV and PR differ in how they store the FP and VSX
    registers on POWER7, the code for these cases is not common.  On POWER7,
    the FP registers are the upper halves of the VSX registers vsr0 - vsr31.
    PR KVM stores vsr0 - vsr31 in two halves, with the upper halves in the
    arch.fpr[] array and the lower halves in the arch.vsr[] array, whereas
    HV KVM on POWER7 stores the whole VSX register in arch.vsr[].
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix whitespace, vsx compilation]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 709f0ddae1f1..51604a16c8a5 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -200,6 +200,8 @@ static inline u32 kvmppc_set_field(u64 inst, int msb, int lsb, int value)
 union kvmppc_one_reg {
 	u32	wval;
 	u64	dval;
+	vector128 vval;
+	u64	vsxval[2];
 };
 
 #define one_reg_size(id)	\

commit a136a8bdc02fc14625ac45ee846cc646fc46597e
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 25 20:31:56 2012 +0000

    KVM: PPC: Book3S: Get/set guest SPRs using the GET/SET_ONE_REG interface
    
    This enables userspace to get and set various SPRs (special-purpose
    registers) using the KVM_[GS]ET_ONE_REG ioctls.  With this, userspace
    can get and set all the SPRs that are part of the guest state, either
    through the KVM_[GS]ET_REGS ioctls, the KVM_[GS]ET_SREGS ioctls, or
    the KVM_[GS]ET_ONE_REG ioctls.
    
    The SPRs that are added here are:
    
    - DABR:  Data address breakpoint register
    - DSCR:  Data stream control register
    - PURR:  Processor utilization of resources register
    - SPURR: Scaled PURR
    - DAR:   Data address register
    - DSISR: Data storage interrupt status register
    - AMR:   Authority mask register
    - UAMOR: User authority mask override register
    - MMCR0, MMCR1, MMCRA: Performance monitor unit control registers
    - PMC1..PMC8: Performance monitor unit counter registers
    
    In order to reduce code duplication between PR and HV KVM code, this
    moves the kvm_vcpu_ioctl_[gs]et_one_reg functions into book3s.c and
    centralizes the copying between user and kernel space there.  The
    registers that are handled differently between PR and HV, and those
    that exist only in one flavor, are handled in kvmppc_[gs]et_one_reg()
    functions that are specific to each flavor.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: minimal style fixes]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 3fb980d293e5..709f0ddae1f1 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -28,6 +28,7 @@
 #include <linux/types.h>
 #include <linux/kvm_types.h>
 #include <linux/kvm_host.h>
+#include <linux/bug.h>
 #ifdef CONFIG_PPC_BOOK3S
 #include <asm/kvm_book3s.h>
 #else
@@ -196,6 +197,35 @@ static inline u32 kvmppc_set_field(u64 inst, int msb, int lsb, int value)
 	return r;
 }
 
+union kvmppc_one_reg {
+	u32	wval;
+	u64	dval;
+};
+
+#define one_reg_size(id)	\
+	(1ul << (((id) & KVM_REG_SIZE_MASK) >> KVM_REG_SIZE_SHIFT))
+
+#define get_reg_val(id, reg)	({		\
+	union kvmppc_one_reg __u;		\
+	switch (one_reg_size(id)) {		\
+	case 4: __u.wval = (reg); break;	\
+	case 8: __u.dval = (reg); break;	\
+	default: BUG();				\
+	}					\
+	__u;					\
+})
+
+
+#define set_reg_val(id, val)	({		\
+	u64 __v;				\
+	switch (one_reg_size(id)) {		\
+	case 4: __v = (val).wval; break;	\
+	case 8: __v = (val).dval; break;	\
+	default: BUG();				\
+	}					\
+	__v;					\
+})
+
 void kvmppc_core_get_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 int kvmppc_core_set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 
@@ -204,6 +234,8 @@ int kvmppc_set_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 
 int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg);
 int kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg);
+int kvmppc_get_one_reg(struct kvm_vcpu *vcpu, u64 id, union kvmppc_one_reg *);
+int kvmppc_set_one_reg(struct kvm_vcpu *vcpu, u64 id, union kvmppc_one_reg *);
 
 void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 pid);
 

commit dfe49dbd1fc7310a4e0e2f83ae737cd7d34fa0cd
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 11 13:28:18 2012 +0000

    KVM: PPC: Book3S HV: Handle memory slot deletion and modification correctly
    
    This adds an implementation of kvm_arch_flush_shadow_memslot for
    Book3S HV, and arranges for kvmppc_core_commit_memory_region to
    flush the dirty log when modifying an existing slot.  With this,
    we can handle deletion and modification of memory slots.
    
    kvm_arch_flush_shadow_memslot calls kvmppc_core_flush_memslot, which
    on Book3S HV now traverses the reverse map chains to remove any HPT
    (hashed page table) entries referring to pages in the memslot.  This
    gets called by generic code whenever deleting a memslot or changing
    the guest physical address for a memslot.
    
    We flush the dirty log in kvmppc_core_commit_memory_region for
    consistency with what x86 does.  We only need to flush when an
    existing memslot is being modified, because for a new memslot the
    rmap array (which stores the dirty bits) is all zero, meaning that
    every page is considered clean already, and when deleting a memslot
    we obviously don't care about the dirty bits any more.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 41a00eae68c7..3fb980d293e5 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -151,9 +151,12 @@ extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
 				struct kvm_userspace_memory_region *mem);
 extern void kvmppc_core_commit_memory_region(struct kvm *kvm,
-				struct kvm_userspace_memory_region *mem);
+				struct kvm_userspace_memory_region *mem,
+				struct kvm_memory_slot old);
 extern int kvm_vm_ioctl_get_smmu_info(struct kvm *kvm,
 				      struct kvm_ppc_smmu_info *info);
+extern void kvmppc_core_flush_memslot(struct kvm *kvm,
+				      struct kvm_memory_slot *memslot);
 
 extern int kvmppc_bookehv_init(void);
 extern void kvmppc_bookehv_exit(void);

commit a66b48c3a39fa1c4223d4f847fdc7a04ed1618de
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 11 13:27:46 2012 +0000

    KVM: PPC: Move kvm->arch.slot_phys into memslot.arch
    
    Now that we have an architecture-specific field in the kvm_memory_slot
    structure, we can use it to store the array of page physical addresses
    that we need for Book3S HV KVM on PPC970 processors.  This reduces the
    size of struct kvm_arch for Book3S HV, and also reduces the size of
    struct kvm_arch_memory_slot for other PPC KVM variants since the fields
    in it are now only compiled in for Book3S HV.
    
    This necessitates making the kvm_arch_create_memslot and
    kvm_arch_free_memslot operations specific to each PPC KVM variant.
    That in turn means that we now don't allocate the rmap arrays on
    Book3S PR and Book E.
    
    Since we now unpin pages and free the slot_phys array in
    kvmppc_core_free_memslot, we no longer need to do it in
    kvmppc_core_destroy_vm, since the generic code takes care to free
    all the memslots when destroying a VM.
    
    We now need the new memslot to be passed in to
    kvmppc_core_prepare_memory_region, since we need to initialize its
    arch.slot_phys member on Book3S HV.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c06a64b53362..41a00eae68c7 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -143,7 +143,12 @@ extern struct kvmppc_linear_info *kvm_alloc_hpt(void);
 extern void kvm_release_hpt(struct kvmppc_linear_info *li);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
 extern void kvmppc_core_destroy_vm(struct kvm *kvm);
+extern void kvmppc_core_free_memslot(struct kvm_memory_slot *free,
+				     struct kvm_memory_slot *dont);
+extern int kvmppc_core_create_memslot(struct kvm_memory_slot *slot,
+				      unsigned long npages);
 extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
+				struct kvm_memory_slot *memslot,
 				struct kvm_userspace_memory_region *mem);
 extern void kvmppc_core_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);

commit f61c94bb99ca4253ac5dd57750e1af209a4beb7a
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed Aug 8 20:38:19 2012 +0000

    KVM: PPC: booke: Add watchdog emulation
    
    This patch adds the watchdog emulation in KVM. The watchdog
    emulation is enabled by KVM_ENABLE_CAP(KVM_CAP_PPC_BOOKE_WATCHDOG) ioctl.
    The kernel timer are used for watchdog emulation and emulates
    h/w watchdog state machine. On watchdog timer expiry, it exit to QEMU
    if TCR.WRC is non ZERO. QEMU can reset/shutdown etc depending upon how
    it is configured.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [bharat.bhushan@freescale.com: reworked patch]
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    [agraf: adjust to new request framework]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 3dfc437fb9d9..c06a64b53362 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -68,6 +68,8 @@ extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
 extern u32 kvmppc_get_dec(struct kvm_vcpu *vcpu, u64 tb);
 extern void kvmppc_decrementer_func(unsigned long data);
 extern int kvmppc_sanity_check(struct kvm_vcpu *vcpu);
+extern int kvmppc_subarch_vcpu_init(struct kvm_vcpu *vcpu);
+extern void kvmppc_subarch_vcpu_uninit(struct kvm_vcpu *vcpu);
 
 /* Core-specific hooks */
 

commit 7c973a2ebb8fb9c8ee2ae9647f9ad7b0ad58a3e6
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 13 12:50:35 2012 +0200

    KVM: PPC: Add return value to core_check_requests
    
    Requests may want to tell us that we need to go back into host state,
    so add a return value for the checks.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 545936428bf6..3dfc437fb9d9 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -112,7 +112,7 @@ extern int kvmppc_core_emulate_mtspr(struct kvm_vcpu *vcpu, int sprn,
 				     ulong val);
 extern int kvmppc_core_emulate_mfspr(struct kvm_vcpu *vcpu, int sprn,
 				     ulong *val);
-extern void kvmppc_core_check_requests(struct kvm_vcpu *vcpu);
+extern int kvmppc_core_check_requests(struct kvm_vcpu *vcpu);
 
 extern int kvmppc_booke_init(void);
 extern void kvmppc_booke_exit(void);

commit bd2be6836ee493d41fe42367a2b129aa771185c1
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 13 01:04:19 2012 +0200

    KVM: PPC: Book3S: PR: Rework irq disabling
    
    Today, we disable preemption while inside guest context, because we need
    to expose to the world that we are not in a preemptible context. However,
    during that time we already have interrupts disabled, which would indicate
    that we are in a non-preemptible context.
    
    The reason the checks for irqs_disabled() fail for us though is that we
    manually control hard IRQs and ignore all the lazy EE framework. Let's
    stop doing that. Instead, let's always use lazy EE to indicate when we
    want to disable IRQs, but do a special final switch that gets us into
    EE disabled, but soft enabled state. That way when we get back out of
    guest state, we are immediately ready to process interrupts.
    
    This simplifies the code drastically and reduces the time that we appear
    as preempt disabled.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 59b7c87e47f7..545936428bf6 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -234,5 +234,15 @@ static inline void kvmppc_mmu_flush_icache(pfn_t pfn)
 	}
 }
 
+/* Please call after prepare_to_enter. This function puts the lazy ee state
+   back to normal mode, without actually enabling interrupts. */
+static inline void kvmppc_lazy_ee_enable(void)
+{
+#ifdef CONFIG_PPC64
+	/* Only need to enable IRQs by hard enabling them after this */
+	local_paca->irq_happened = 0;
+	local_paca->soft_enabled = 1;
+#endif
+}
 
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 03d25c5bd5c3125055bd36f4813ddb817def19dd
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Aug 10 12:28:50 2012 +0200

    KVM: PPC: Use same kvmppc_prepare_to_enter code for booke and book3s_pr
    
    We need to do the same things when preparing to enter a guest for booke and
    book3s_pr cores. Fold the generic code into a generic function that both call.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 88de3146838b..59b7c87e47f7 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -112,6 +112,7 @@ extern int kvmppc_core_emulate_mtspr(struct kvm_vcpu *vcpu, int sprn,
 				     ulong val);
 extern int kvmppc_core_emulate_mfspr(struct kvm_vcpu *vcpu, int sprn,
 				     ulong *val);
+extern void kvmppc_core_check_requests(struct kvm_vcpu *vcpu);
 
 extern int kvmppc_booke_init(void);
 extern void kvmppc_booke_exit(void);
@@ -150,6 +151,8 @@ extern int kvm_vm_ioctl_get_smmu_info(struct kvm *kvm,
 extern int kvmppc_bookehv_init(void);
 extern void kvmppc_bookehv_exit(void);
 
+extern int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu);
+
 /*
  * Cuts out inst bits with ordering according to spec.
  * That means the leftmost bit is zero. All given bits are included.

commit 862d31f788f9a249f7656d02d8d4006e306108ce
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Jul 31 00:19:50 2012 +0200

    KVM: PPC: E500: Implement MMU notifiers
    
    The e500 target has lived without mmu notifiers ever since it got
    introduced, but fails for the user space check on them with hugetlbfs.
    
    So in order to get that one working, implement mmu notifiers in a
    reasonably dumb fashion and be happy. On embedded hardware, we almost
    never end up with mmu notifier calls, since most people don't overcommit.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index e006f0bdea95..88de3146838b 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -104,6 +104,7 @@ extern void kvmppc_core_queue_external(struct kvm_vcpu *vcpu,
                                        struct kvm_interrupt *irq);
 extern void kvmppc_core_dequeue_external(struct kvm_vcpu *vcpu,
                                          struct kvm_interrupt *irq);
+extern void kvmppc_core_flush_tlb(struct kvm_vcpu *vcpu);
 
 extern int kvmppc_core_emulate_op(struct kvm_run *run, struct kvm_vcpu *vcpu,
                                   unsigned int op, int *advance);

commit 249ba1ee0f8fcb4e40caa5fbea11dafde201cc46
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Aug 3 13:56:33 2012 +0200

    KVM: PPC: Add cache flush on page map
    
    When we map a page that wasn't icache cleared before, do so when first
    mapping it in KVM using the same information bits as the Linux mapping
    logic. That way we are 100% sure that any page we map does not have stale
    entries in the icache.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 0124937a23b9..e006f0bdea95 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -219,4 +219,16 @@ void kvmppc_claim_lpid(long lpid);
 void kvmppc_free_lpid(long lpid);
 void kvmppc_init_lpid(unsigned long nr_lpids);
 
+static inline void kvmppc_mmu_flush_icache(pfn_t pfn)
+{
+	/* Clear i-cache for new pages */
+	struct page *page;
+	page = pfn_to_page(pfn);
+	if (!test_bit(PG_arch_1, &page->flags)) {
+		flush_dcache_icache_page(page);
+		set_bit(PG_arch_1, &page->flags);
+	}
+}
+
+
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 32fad281c0680ed0ccade7dda85a2121cf9b1d06
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri May 4 02:32:53 2012 +0000

    KVM: PPC: Book3S HV: Make the guest hash table size configurable
    
    This adds a new ioctl to enable userspace to control the size of the guest
    hashed page table (HPT) and to clear it out when resetting the guest.
    The KVM_PPC_ALLOCATE_HTAB ioctl is a VM ioctl and takes as its parameter
    a pointer to a u32 containing the desired order of the HPT (log base 2
    of the size in bytes), which is updated on successful return to the
    actual order of the HPT which was allocated.
    
    There must be no vcpus running at the time of this ioctl.  To enforce
    this, we now keep a count of the number of vcpus running in
    kvm->arch.vcpus_running.
    
    If the ioctl is called when a HPT has already been allocated, we don't
    reallocate the HPT but just clear it out.  We first clear the
    kvm->arch.rma_setup_done flag, which has two effects: (a) since we hold
    the kvm->lock mutex, it will prevent any vcpus from starting to run until
    we're done, and (b) it means that the first vcpu to run after we're done
    will re-establish the VRMA if necessary.
    
    If userspace doesn't call this ioctl before running the first vcpu, the
    kernel will allocate a default-sized HPT at that point.  We do it then
    rather than when creating the VM, as the code did previously, so that
    userspace has a chance to do the ioctl if it wants.
    
    When allocating the HPT, we can allocate either from the kernel page
    allocator, or from the preallocated pool.  If userspace is asking for
    a different size from the preallocated HPTs, we first try to allocate
    using the kernel page allocator.  Then we try to allocate from the
    preallocated pool, and then if that fails, we try allocating decreasing
    sizes from the kernel page allocator, down to the minimum size allowed
    (256kB).  Note that the kernel page allocator limits allocations to
    1 << CONFIG_FORCE_MAX_ZONEORDER pages, which by default corresponds to
    16MB (on 64-bit powerpc, at least).
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix module compilation]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index f68c22fa2fce..0124937a23b9 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -119,7 +119,8 @@ extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 extern int kvmppc_kvm_pv(struct kvm_vcpu *vcpu);
 extern void kvmppc_map_magic(struct kvm_vcpu *vcpu);
 
-extern long kvmppc_alloc_hpt(struct kvm *kvm);
+extern long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp);
+extern long kvmppc_alloc_reset_hpt(struct kvm *kvm, u32 *htab_orderp);
 extern void kvmppc_free_hpt(struct kvm *kvm);
 extern long kvmppc_prepare_vrma(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);

commit 54771e6217ce05a474827d9b23ff03de9d2ef2a0
Author: Alexander Graf <agraf@suse.de>
Date:   Fri May 4 14:55:12 2012 +0200

    KVM: PPC: Emulator: clean up SPR reads and writes
    
    When reading and writing SPRs, every SPR emulation piece had to read
    or write the respective GPR the value was read from or stored in itself.
    
    This approach is pretty prone to failure. What if we accidentally
    implement mfspr emulation where we just do "break" and nothing else?
    Suddenly we would get a random value in the return register - which is
    always a bad idea.
    
    So let's consolidate the generic code paths and only give the core
    specific SPR handling code readily made variables to read/write from/to.
    
    Functionally, this patch doesn't change anything, but it increases the
    readability of the code and makes is less prone to bugs.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c87e3b503fdc..f68c22fa2fce 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -107,8 +107,10 @@ extern void kvmppc_core_dequeue_external(struct kvm_vcpu *vcpu,
 
 extern int kvmppc_core_emulate_op(struct kvm_run *run, struct kvm_vcpu *vcpu,
                                   unsigned int op, int *advance);
-extern int kvmppc_core_emulate_mtspr(struct kvm_vcpu *vcpu, int sprn, int rs);
-extern int kvmppc_core_emulate_mfspr(struct kvm_vcpu *vcpu, int sprn, int rt);
+extern int kvmppc_core_emulate_mtspr(struct kvm_vcpu *vcpu, int sprn,
+				     ulong val);
+extern int kvmppc_core_emulate_mfspr(struct kvm_vcpu *vcpu, int sprn,
+				     ulong *val);
 
 extern int kvmppc_booke_init(void);
 extern void kvmppc_booke_exit(void);

commit 5b74716ebab10e7bce960d148fe6d8f6920451e5
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Apr 26 19:43:42 2012 +0000

    kvm/powerpc: Add new ioctl to retreive server MMU infos
    
    This is necessary for qemu to be able to pass the right information
    to the guest, such as the supported page sizes and corresponding
    encodings in the SLB and hash table, which can vary depending
    on the processor type, the type of KVM used (PR vs HV) and the
    version of KVM
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [agraf: fix compilation on hv, adjust for newer ioctl numbers]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c1069f63dcaf..c87e3b503fdc 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -140,6 +140,8 @@ extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);
 extern void kvmppc_core_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);
+extern int kvm_vm_ioctl_get_smmu_info(struct kvm *kvm,
+				      struct kvm_ppc_smmu_info *info);
 
 extern int kvmppc_bookehv_init(void);
 extern void kvmppc_bookehv_exit(void);

commit f31e65e1170edba4a86bd8cba0318e251d3746d0
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Mar 15 21:58:34 2012 +0000

    kvm/book3s: Make kernel emulated H_PUT_TCE available for "PR" KVM
    
    There is nothing in the code for emulating TCE tables in the kernel
    that prevents it from working on "PR" KVM... other than ifdef's and
    location of the code.
    
    This and moves the bulk of the code there to a new file called
    book3s_64_vio.c.
    
    This speeds things up a bit on my G5.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [agraf: fix for hv kvm, 32bit, whitespace]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 7f0a3dae7cde..c1069f63dcaf 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -126,6 +126,8 @@ extern void kvmppc_map_vrma(struct kvm_vcpu *vcpu,
 extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce *args);
+extern long kvmppc_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,
+			     unsigned long ioba, unsigned long tce);
 extern long kvm_vm_ioctl_allocate_rma(struct kvm *kvm,
 				struct kvm_allocate_rma *rma);
 extern struct kvmppc_linear_info *kvm_alloc_rma(void);

commit a8e4ef841429d338b8700998afb3dfc18c1f25d9
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Feb 16 14:07:37 2012 +0000

    KVM: PPC: booke: rework rescheduling checks
    
    Instead of checking whether we should reschedule only when we exited
    due to an interrupt, let's always check before entering the guest back
    again. This gets the target more in line with the other archs.
    
    Also while at it, generalize the whole thing so that eventually we could
    have a single kvmppc_prepare_to_enter function for all ppc targets that
    does signal and reschedule checking for us.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index e709975702a6..7f0a3dae7cde 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -95,7 +95,7 @@ extern int kvmppc_core_vcpu_translate(struct kvm_vcpu *vcpu,
 extern void kvmppc_core_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 extern void kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu);
 
-extern void kvmppc_core_prepare_to_enter(struct kvm_vcpu *vcpu);
+extern int kvmppc_core_prepare_to_enter(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_pending_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_program(struct kvm_vcpu *vcpu, ulong flags);
 extern void kvmppc_core_queue_dec(struct kvm_vcpu *vcpu);

commit d30f6e480055e5be12e7a03fd11ea912a451daa5
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Dec 20 15:34:43 2011 +0000

    KVM: PPC: booke: category E.HV (GS-mode) support
    
    Chips such as e500mc that implement category E.HV in Power ISA 2.06
    provide hardware virtualization features, including a new MSR mode for
    guest state.  The guest OS can perform many operations without trapping
    into the hypervisor, including transitions to and from guest userspace.
    
    Since we can use SRR1[GS] to reliably tell whether an exception came from
    guest state, instead of messing around with IVPR, we use DO_KVM similarly
    to book3s.
    
    Current issues include:
     - Machine checks from guest state are not routed to the host handler.
     - The guest can cause a host oops by executing an emulated instruction
       in a page that lacks read permission.  Existing e500/4xx support has
       the same problem.
    
    Includes work by Ashish Kalra <Ashish.Kalra@freescale.com>,
    Varun Sethi <Varun.Sethi@freescale.com>, and
    Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: remove pt_regs usage]
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 731e920eda1e..e709975702a6 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -139,6 +139,9 @@ extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
 extern void kvmppc_core_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);
 
+extern int kvmppc_bookehv_init(void);
+extern void kvmppc_bookehv_exit(void);
+
 /*
  * Cuts out inst bits with ordering according to spec.
  * That means the leftmost bit is zero. All given bits are included.

commit 043cc4d724da6bb9e4f417c735accec58dfa40bf
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Dec 20 15:34:20 2011 +0000

    KVM: PPC: factor out lpid allocator from book3s_64_mmu_hv
    
    We'll use it on e500mc as well.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 9d6dee0f7d48..731e920eda1e 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -204,4 +204,9 @@ int kvm_vcpu_ioctl_config_tlb(struct kvm_vcpu *vcpu,
 int kvm_vcpu_ioctl_dirty_tlb(struct kvm_vcpu *vcpu,
 			     struct kvm_dirty_tlb *cfg);
 
+long kvmppc_alloc_lpid(void);
+void kvmppc_claim_lpid(long lpid);
+void kvmppc_free_lpid(long lpid);
+void kvmppc_init_lpid(unsigned long nr_lpids);
+
 #endif /* __POWERPC_KVM_PPC_H__ */

commit d2a1b483a4a3f4bbb5fec1877f716c15ac7fa405
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jan 16 19:12:11 2012 +0100

    KVM: PPC: Add HPT preallocator
    
    We're currently allocating 16MB of linear memory on demand when creating
    a guest. That does work some times, but finding 16MB of linear memory
    available in the system at runtime is definitely not a given.
    
    So let's add another command line option similar to the RMA preallocator,
    that we can use to keep a pool of page tables around. Now, when a guest
    gets created it has a pretty low chance of receiving an OOM.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 1c37a2f8d0f4..9d6dee0f7d48 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -130,6 +130,8 @@ extern long kvm_vm_ioctl_allocate_rma(struct kvm *kvm,
 				struct kvm_allocate_rma *rma);
 extern struct kvmppc_linear_info *kvm_alloc_rma(void);
 extern void kvm_release_rma(struct kvmppc_linear_info *ri);
+extern struct kvmppc_linear_info *kvm_alloc_hpt(void);
+extern void kvm_release_hpt(struct kvmppc_linear_info *li);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
 extern void kvmppc_core_destroy_vm(struct kvm *kvm);
 extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,

commit b4e706111d501991c59d2af23a299ab52a06b03d
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jan 16 16:50:10 2012 +0100

    KVM: PPC: Convert RMA allocation into generic code
    
    We have code to allocate big chunks of linear memory on bootup for later use.
    This code is currently used for RMA allocation, but can be useful beyond that
    extent.
    
    Make it generic so we can reuse it for other stuff later.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index a61b5b5047d6..1c37a2f8d0f4 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -128,8 +128,8 @@ extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce *args);
 extern long kvm_vm_ioctl_allocate_rma(struct kvm *kvm,
 				struct kvm_allocate_rma *rma);
-extern struct kvmppc_rma_info *kvm_alloc_rma(void);
-extern void kvm_release_rma(struct kvmppc_rma_info *ri);
+extern struct kvmppc_linear_info *kvm_alloc_rma(void);
+extern void kvm_release_rma(struct kvmppc_linear_info *ri);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
 extern void kvmppc_core_destroy_vm(struct kvm *kvm);
 extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
@@ -187,13 +187,13 @@ static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 	paca[cpu].kvm_hstate.xics_phys = addr;
 }
 
-extern void kvm_rma_init(void);
+extern void kvm_linear_init(void);
 
 #else
 static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 {}
 
-static inline void kvm_rma_init(void)
+static inline void kvm_linear_init(void)
 {}
 #endif
 

commit 31f3438eca2fc90dc892e0e9963ba4b93a2c8383
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:26:50 2011 +0000

    KVM: PPC: Move kvm_vcpu_ioctl_[gs]et_one_reg down to platform-specific code
    
    This moves the get/set_one_reg implementation down from powerpc.c into
    booke.c, book3s_pr.c and book3s_hv.c.  This avoids #ifdefs in C code,
    but more importantly, it fixes a bug on Book3s HV where we were
    accessing beyond the end of the kvm_vcpu struct (via the to_book3s()
    macro) and corrupting memory, causing random crashes and file corruption.
    
    On Book3s HV we only accept setting the HIOR to zero, since the guest
    runs in supervisor mode and its vectors are never offset from zero.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    [agraf update to apply on top of changed ONE_REG patches]
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index fb70414db90c..a61b5b5047d6 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -176,6 +176,9 @@ int kvmppc_core_set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 void kvmppc_get_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 int kvmppc_set_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 
+int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg);
+int kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg);
+
 void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 pid);
 
 #ifdef CONFIG_KVM_BOOK3S_64_HV

commit da9d1d7f2875cc8c1ffbce8f3501d0b33f4e7a4d
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:31:41 2011 +0000

    KVM: PPC: Allow use of small pages to back Book3S HV guests
    
    This relaxes the requirement that the guest memory be provided as
    16MB huge pages, allowing it to be provided as normal memory, i.e.
    in pages of PAGE_SIZE bytes (4k or 64k).  To allow this, we index
    the kvm->arch.slot_phys[] arrays with a small page index, even if
    huge pages are being used, and use the low-order 5 bits of each
    entry to store the order of the enclosing page with respect to
    normal pages, i.e. log_2(enclosing_page_size / PAGE_SIZE).
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 1458c6740ea3..fb70414db90c 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -122,7 +122,7 @@ extern void kvmppc_free_hpt(struct kvm *kvm);
 extern long kvmppc_prepare_vrma(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);
 extern void kvmppc_map_vrma(struct kvm_vcpu *vcpu,
-			    struct kvm_memory_slot *memslot);
+			struct kvm_memory_slot *memslot, unsigned long porder);
 extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce *args);

commit c77162dee7aff6ab5f075da9b60f649cbbeb86cc
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:31:00 2011 +0000

    KVM: PPC: Only get pages when actually needed, not in prepare_memory_region()
    
    This removes the code from kvmppc_core_prepare_memory_region() that
    looked up the VMA for the region being added and called hva_to_page
    to get the pfns for the memory.  We have no guarantee that there will
    be anything mapped there at the time of the KVM_SET_USER_MEMORY_REGION
    ioctl call; userspace can do that ioctl and then map memory into the
    region later.
    
    Instead we defer looking up the pfn for each memory page until it is
    needed, which generally means when the guest does an H_ENTER hcall on
    the page.  Since we can't call get_user_pages in real mode, if we don't
    already have the pfn for the page, kvmppc_h_enter() will return
    H_TOO_HARD and we then call kvmppc_virtmode_h_enter() once we get back
    to kernel context.  That calls kvmppc_get_guest_page() to get the pfn
    for the page, and then calls back to kvmppc_h_enter() to redo the HPTE
    insertion.
    
    When the first vcpu starts executing, we need to have the RMO or VRMA
    region mapped so that the guest's real mode accesses will work.  Thus
    we now have a check in kvmppc_vcpu_run() to see if the RMO/VRMA is set
    up and if not, call kvmppc_hv_setup_rma().  It checks if the memslot
    starting at guest physical 0 now has RMO memory mapped there; if so it
    sets it up for the guest, otherwise on POWER7 it sets up the VRMA.
    The function that does that, kvmppc_map_vrma, is now a bit simpler,
    as it calls kvmppc_virtmode_h_enter instead of creating the HPTE itself.
    
    Since we are now potentially updating entries in the slot_phys[]
    arrays from multiple vcpu threads, we now have a spinlock protecting
    those updates to ensure that we don't lose track of any references
    to pages.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 5192c2e70583..1458c6740ea3 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -121,8 +121,8 @@ extern long kvmppc_alloc_hpt(struct kvm *kvm);
 extern void kvmppc_free_hpt(struct kvm *kvm);
 extern long kvmppc_prepare_vrma(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);
-extern void kvmppc_map_vrma(struct kvm *kvm,
-			    struct kvm_userspace_memory_region *mem);
+extern void kvmppc_map_vrma(struct kvm_vcpu *vcpu,
+			    struct kvm_memory_slot *memslot);
 extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce *args);

commit dfd4d47e9a71c5a35eb67a44cd311efbe1846b7e
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Nov 17 12:39:59 2011 +0000

    KVM: PPC: booke: Improve timer register emulation
    
    Decrementers are now properly driven by TCR/TSR, and the guest
    has full read/write access to these registers.
    
    The decrementer keeps ticking (and setting the TSR bit) regardless of
    whether the interrupts are enabled with TCR.
    
    The decrementer stops at zero, rather than going negative.
    
    Decrementers (and FITs, once implemented) are delivered as
    level-triggered interrupts -- dequeued when the TSR bit is cleared, not
    on delivery.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    [scottwood@freescale.com: significant changes]
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c089927f64cc..5192c2e70583 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -66,6 +66,7 @@ extern int kvmppc_emulate_instruction(struct kvm_run *run,
 extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
 extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
 extern u32 kvmppc_get_dec(struct kvm_vcpu *vcpu, u64 tb);
+extern void kvmppc_decrementer_func(unsigned long data);
 extern int kvmppc_sanity_check(struct kvm_vcpu *vcpu);
 
 /* Core-specific hooks */

commit 7e28e60ef974d0eeb43112ef264d8c130f7b7bf4
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Nov 8 18:23:20 2011 -0600

    KVM: PPC: Rename deliver_interrupts to prepare_to_enter
    
    This function also updates paravirt int_pending, so rename it
    to be more obvious that this is a collection of checks run prior
    to (re)entering a guest.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index a284f209e2df..c089927f64cc 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -94,7 +94,7 @@ extern int kvmppc_core_vcpu_translate(struct kvm_vcpu *vcpu,
 extern void kvmppc_core_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 extern void kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu);
 
-extern void kvmppc_core_deliver_interrupts(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_prepare_to_enter(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_pending_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_program(struct kvm_vcpu *vcpu, ulong flags);
 extern void kvmppc_core_queue_dec(struct kvm_vcpu *vcpu);

commit dc83b8bc0256ee682506ed83853a98eaba529c6f
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Aug 18 15:25:21 2011 -0500

    KVM: PPC: e500: MMU API
    
    This implements a shared-memory API for giving host userspace access to
    the guest's TLB.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 46efd1a265c9..a284f209e2df 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -193,4 +193,9 @@ static inline void kvm_rma_init(void)
 {}
 #endif
 
+int kvm_vcpu_ioctl_config_tlb(struct kvm_vcpu *vcpu,
+			      struct kvm_config_tlb *cfg);
+int kvm_vcpu_ioctl_dirty_tlb(struct kvm_vcpu *vcpu,
+			     struct kvm_dirty_tlb *cfg);
+
 #endif /* __POWERPC_KVM_PPC_H__ */

commit af8f38b3499f0d4a3c354df2435f0fb2dded250a
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Aug 10 13:57:08 2011 +0200

    KVM: PPC: Add sanity checking to vcpu_run
    
    There are multiple features in PowerPC KVM that can now be enabled
    depending on the user's wishes. Some of the combinations don't make
    sense or don't work though.
    
    So this patch adds a way to check if the executing environment would
    actually be able to run the guest properly. It also adds sanity
    checks if PVR is set (should always be true given the current code
    flow), if PAPR is only used with book3s_64 where it works and that
    HV KVM is only used in PAPR mode.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index d121f49d62b8..46efd1a265c9 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -66,6 +66,7 @@ extern int kvmppc_emulate_instruction(struct kvm_run *run,
 extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
 extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
 extern u32 kvmppc_get_dec(struct kvm_vcpu *vcpu, u64 tb);
+extern int kvmppc_sanity_check(struct kvm_vcpu *vcpu);
 
 /* Core-specific hooks */
 

commit aa04b4cc5be64b4fb9ef4e0fdf2418e2f4737fb2
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:25:44 2011 +0000

    KVM: PPC: Allocate RMAs (Real Mode Areas) at boot for use by guests
    
    This adds infrastructure which will be needed to allow book3s_hv KVM to
    run on older POWER processors, including PPC970, which don't support
    the Virtual Real Mode Area (VRMA) facility, but only the Real Mode
    Offset (RMO) facility.  These processors require a physically
    contiguous, aligned area of memory for each guest.  When the guest does
    an access in real mode (MMU off), the address is compared against a
    limit value, and if it is lower, the address is ORed with an offset
    value (from the Real Mode Offset Register (RMOR)) and the result becomes
    the real address for the access.  The size of the RMA has to be one of
    a set of supported values, which usually includes 64MB, 128MB, 256MB
    and some larger powers of 2.
    
    Since we are unlikely to be able to allocate 64MB or more of physically
    contiguous memory after the kernel has been running for a while, we
    allocate a pool of RMAs at boot time using the bootmem allocator.  The
    size and number of the RMAs can be set using the kvm_rma_size=xx and
    kvm_rma_count=xx kernel command line options.
    
    KVM exports a new capability, KVM_CAP_PPC_RMA, to signal the availability
    of the pool of preallocated RMAs.  The capability value is 1 if the
    processor can use an RMA but doesn't require one (because it supports
    the VRMA facility), or 2 if the processor requires an RMA for each guest.
    
    This adds a new ioctl, KVM_ALLOCATE_RMA, which allocates an RMA from the
    pool and returns a file descriptor which can be used to map the RMA.  It
    also returns the size of the RMA in the argument structure.
    
    Having an RMA means we will get multiple KMV_SET_USER_MEMORY_REGION
    ioctl calls from userspace.  To cope with this, we now preallocate the
    kvm->arch.ram_pginfo array when the VM is created with a size sufficient
    for up to 64GB of guest memory.  Subsequently we will get rid of this
    array and use memory associated with each memslot instead.
    
    This moves most of the code that translates the user addresses into
    host pfns (page frame numbers) out of kvmppc_prepare_vrma up one level
    to kvmppc_core_prepare_memory_region.  Also, instead of having to look
    up the VMA for each page in order to check the page size, we now check
    that the pages we get are compound pages of 16MB.  However, if we are
    adding memory that is mapped to an RMA, we don't bother with calling
    get_user_pages_fast and instead just offset from the base pfn for the
    RMA.
    
    Typically the RMA gets added after vcpus are created, which makes it
    inconvenient to have the LPCR (logical partition control register) value
    in the vcpu->arch struct, since the LPCR controls whether the processor
    uses RMA or VRMA for the guest.  This moves the LPCR value into the
    kvm->arch struct and arranges for the MER (mediated external request)
    bit, which is the only bit that varies between vcpus, to be set in
    assembly code when going into the guest if there is a pending external
    interrupt request.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 6ef734428634..d121f49d62b8 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -124,6 +124,10 @@ extern void kvmppc_map_vrma(struct kvm *kvm,
 extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
 extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 				struct kvm_create_spapr_tce *args);
+extern long kvm_vm_ioctl_allocate_rma(struct kvm *kvm,
+				struct kvm_allocate_rma *rma);
+extern struct kvmppc_rma_info *kvm_alloc_rma(void);
+extern void kvm_release_rma(struct kvmppc_rma_info *ri);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
 extern void kvmppc_core_destroy_vm(struct kvm *kvm);
 extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
@@ -177,9 +181,15 @@ static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 {
 	paca[cpu].kvm_hstate.xics_phys = addr;
 }
+
+extern void kvm_rma_init(void);
+
 #else
 static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
 {}
+
+static inline void kvm_rma_init(void)
+{}
 #endif
 
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 371fefd6f2dc46668e00871930dde613b88d4bde
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:23:08 2011 +0000

    KVM: PPC: Allow book3s_hv guests to use SMT processor modes
    
    This lifts the restriction that book3s_hv guests can only run one
    hardware thread per core, and allows them to use up to 4 threads
    per core on POWER7.  The host still has to run single-threaded.
    
    This capability is advertised to qemu through a new KVM_CAP_PPC_SMT
    capability.  The return value of the ioctl querying this capability
    is the number of vcpus per virtual CPU core (vcore), currently 4.
    
    To use this, the host kernel should be booted with all threads
    active, and then all the secondary threads should be offlined.
    This will put the secondary threads into nap mode.  KVM will then
    wake them from nap mode and use them for running guest code (while
    they are still offline).  To wake the secondary threads, we send
    them an IPI using a new xics_wake_cpu() function, implemented in
    arch/powerpc/sysdev/xics/icp-native.c.  In other words, at this stage
    we assume that the platform has a XICS interrupt controller and
    we are using icp-native.c to drive it.  Since the woken thread will
    need to acknowledge and clear the IPI, we also export the base
    physical address of the XICS registers using kvmppc_set_xics_phys()
    for use in the low-level KVM book3s code.
    
    When a vcpu is created, it is assigned to a virtual CPU core.
    The vcore number is obtained by dividing the vcpu number by the
    number of threads per core in the host.  This number is exported
    to userspace via the KVM_CAP_PPC_SMT capability.  If qemu wishes
    to run the guest in single-threaded mode, it should make all vcpu
    numbers be multiples of the number of threads per core.
    
    We distinguish three states of a vcpu: runnable (i.e., ready to execute
    the guest), blocked (that is, idle), and busy in host.  We currently
    implement a policy that the vcore can run only when all its threads
    are runnable or blocked.  This way, if a vcpu needs to execute elsewhere
    in the kernel or in qemu, it can do so without being starved of CPU
    by the other vcpus.
    
    When a vcore starts to run, it executes in the context of one of the
    vcpu threads.  The other vcpu threads all go to sleep and stay asleep
    until something happens requiring the vcpu thread to return to qemu,
    or to wake up to run the vcore (this can happen when another vcpu
    thread goes from busy in host state to blocked).
    
    It can happen that a vcpu goes from blocked to runnable state (e.g.
    because of an interrupt), and the vcore it belongs to is already
    running.  In that case it can start to run immediately as long as
    the none of the vcpus in the vcore have started to exit the guest.
    We send the next free thread in the vcore an IPI to get it to start
    to execute the guest.  It synchronizes with the other threads via
    the vcore->entry_exit_count field to make sure that it doesn't go
    into the guest if the other vcpus are exiting by the time that it
    is ready to actually enter the guest.
    
    Note that there is no fixed relationship between the hardware thread
    number and the vcpu number.  Hardware threads are assigned to vcpus
    as they become runnable, so we will always use the lower-numbered
    hardware threads in preference to higher-numbered threads if not all
    the vcpus in the vcore are runnable, regardless of which vcpus are
    runnable.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 99f6fcf4cf88..6ef734428634 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -33,6 +33,9 @@
 #else
 #include <asm/kvm_booke.h>
 #endif
+#ifdef CONFIG_KVM_BOOK3S_64_HANDLER
+#include <asm/paca.h>
+#endif
 
 enum emulation_result {
 	EMULATE_DONE,         /* no further processing */
@@ -169,4 +172,14 @@ int kvmppc_set_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
 
 void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 pid);
 
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
+{
+	paca[cpu].kvm_hstate.xics_phys = addr;
+}
+#else
+static inline void kvmppc_set_xics_phys(int cpu, unsigned long addr)
+{}
+#endif
+
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 54738c097163c3f01e67ccc85462b78d4d4f495f
Author: David Gibson <dwg@au1.ibm.com>
Date:   Wed Jun 29 00:22:41 2011 +0000

    KVM: PPC: Accelerate H_PUT_TCE by implementing it in real mode
    
    This improves I/O performance for guests using the PAPR
    paravirtualization interface by making the H_PUT_TCE hcall faster, by
    implementing it in real mode.  H_PUT_TCE is used for updating virtual
    IOMMU tables, and is used both for virtual I/O and for real I/O in the
    PAPR interface.
    
    Since this moves the IOMMU tables into the kernel, we define a new
    KVM_CREATE_SPAPR_TCE ioctl to allow qemu to create the tables.  The
    ioctl returns a file descriptor which can be used to mmap the newly
    created table.  The qemu driver models use them in the same way as
    userspace managed tables, but they can be updated directly by the
    guest with a real-mode H_PUT_TCE implementation, reducing the number
    of host/guest context switches during guest IO.
    
    There are certain circumstances where it is useful for userland qemu
    to write to the TCE table even if the kernel H_PUT_TCE path is used
    most of the time.  Specifically, allowing this will avoid awkwardness
    when we need to reset the table.  More importantly, we will in the
    future need to write the table in order to restore its state after a
    checkpoint resume or migration.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 2afe92e6f625..99f6fcf4cf88 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -119,6 +119,8 @@ extern long kvmppc_prepare_vrma(struct kvm *kvm,
 extern void kvmppc_map_vrma(struct kvm *kvm,
 			    struct kvm_userspace_memory_region *mem);
 extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
+extern long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
+				struct kvm_create_spapr_tce *args);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
 extern void kvmppc_core_destroy_vm(struct kvm *kvm);
 extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,

commit a8606e20e41a8149456bafdf76ad29d47672027c
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:22:05 2011 +0000

    KVM: PPC: Handle some PAPR hcalls in the kernel
    
    This adds the infrastructure for handling PAPR hcalls in the kernel,
    either early in the guest exit path while we are still in real mode,
    or later once the MMU has been turned back on and we are in the full
    kernel context.  The advantage of handling hcalls in real mode if
    possible is that we avoid two partition switches -- and this will
    become more important when we support SMT4 guests, since a partition
    switch means we have to pull all of the threads in the core out of
    the guest.  The disadvantage is that we can only access the kernel
    linear mapping, not anything vmalloced or ioremapped, since the MMU
    is off.
    
    This also adds code to handle the following hcalls in real mode:
    
    H_ENTER       Add an HPTE to the hashed page table
    H_REMOVE      Remove an HPTE from the hashed page table
    H_READ        Read HPTEs from the hashed page table
    H_PROTECT     Change the protection bits in an HPTE
    H_BULK_REMOVE Remove up to 4 HPTEs from the hashed page table
    H_SET_DABR    Set the data address breakpoint register
    
    Plus code to handle the following hcalls in the kernel:
    
    H_CEDE        Idle the vcpu until an interrupt or H_PROD hcall arrives
    H_PROD        Wake up a ceded vcpu
    H_REGISTER_VPA Register a virtual processor area (VPA)
    
    The code that runs in real mode has to be in the base kernel, not in
    the module, if KVM is compiled as a module.  The real-mode code can
    only access the kernel linear mapping, not vmalloc or ioremap space.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 0dafd53c30ed..2afe92e6f625 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -118,6 +118,7 @@ extern long kvmppc_prepare_vrma(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);
 extern void kvmppc_map_vrma(struct kvm *kvm,
 			    struct kvm_userspace_memory_region *mem);
+extern int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
 extern void kvmppc_core_destroy_vm(struct kvm *kvm);
 extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,

commit de56a948b9182fbcf92cb8212f114de096c2d574
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:21:34 2011 +0000

    KVM: PPC: Add support for Book3S processors in hypervisor mode
    
    This adds support for KVM running on 64-bit Book 3S processors,
    specifically POWER7, in hypervisor mode.  Using hypervisor mode means
    that the guest can use the processor's supervisor mode.  That means
    that the guest can execute privileged instructions and access privileged
    registers itself without trapping to the host.  This gives excellent
    performance, but does mean that KVM cannot emulate a processor
    architecture other than the one that the hardware implements.
    
    This code assumes that the guest is running paravirtualized using the
    PAPR (Power Architecture Platform Requirements) interface, which is the
    interface that IBM's PowerVM hypervisor uses.  That means that existing
    Linux distributions that run on IBM pSeries machines will also run
    under KVM without modification.  In order to communicate the PAPR
    hypercalls to qemu, this adds a new KVM_EXIT_PAPR_HCALL exit code
    to include/linux/kvm.h.
    
    Currently the choice between book3s_hv support and book3s_pr support
    (i.e. the existing code, which runs the guest in user mode) has to be
    made at kernel configuration time, so a given kernel binary can only
    do one or the other.
    
    This new book3s_hv code doesn't support MMIO emulation at present.
    Since we are running paravirtualized guests, this isn't a serious
    restriction.
    
    With the guest running in supervisor mode, most exceptions go straight
    to the guest.  We will never get data or instruction storage or segment
    interrupts, alignment interrupts, decrementer interrupts, program
    interrupts, single-step interrupts, etc., coming to the hypervisor from
    the guest.  Therefore this introduces a new KVMTEST_NONHV macro for the
    exception entry path so that we don't have to do the KVM test on entry
    to those exception handlers.
    
    We do however get hypervisor decrementer, hypervisor data storage,
    hypervisor instruction storage, and hypervisor emulation assist
    interrupts, so we have to handle those.
    
    In hypervisor mode, real-mode accesses can access all of RAM, not just
    a limited amount.  Therefore we put all the guest state in the vcpu.arch
    and use the shadow_vcpu in the PACA only for temporary scratch space.
    We allocate the vcpu with kzalloc rather than vzalloc, and we don't use
    anything in the kvmppc_vcpu_book3s struct, so we don't allocate it.
    We don't have a shared page with the guest, but we still need a
    kvm_vcpu_arch_shared struct to store the values of various registers,
    so we include one in the vcpu_arch struct.
    
    The POWER7 processor has a restriction that all threads in a core have
    to be in the same partition.  MMU-on kernel code counts as a partition
    (partition 0), so we have to do a partition switch on every entry to and
    exit from the guest.  At present we require the host and guest to run
    in single-thread mode because of this hardware restriction.
    
    This code allocates a hashed page table for the guest and initializes
    it with HPTEs for the guest's Virtual Real Memory Area (VRMA).  We
    require that the guest memory is allocated using 16MB huge pages, in
    order to simplify the low-level memory management.  This also means that
    we can get away without tracking paging activity in the host for now,
    since huge pages can't be paged or swapped.
    
    This also adds a few new exports needed by the book3s_hv code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 48b7ab76de2d..0dafd53c30ed 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -112,6 +112,12 @@ extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 extern int kvmppc_kvm_pv(struct kvm_vcpu *vcpu);
 extern void kvmppc_map_magic(struct kvm_vcpu *vcpu);
 
+extern long kvmppc_alloc_hpt(struct kvm *kvm);
+extern void kvmppc_free_hpt(struct kvm *kvm);
+extern long kvmppc_prepare_vrma(struct kvm *kvm,
+				struct kvm_userspace_memory_region *mem);
+extern void kvmppc_map_vrma(struct kvm *kvm,
+			    struct kvm_userspace_memory_region *mem);
 extern int kvmppc_core_init_vm(struct kvm *kvm);
 extern void kvmppc_core_destroy_vm(struct kvm *kvm);
 extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,

commit df6909e5d52f67be01862c5cb453e509aee661f1
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:19:50 2011 +0000

    KVM: PPC: Move guest enter/exit down into subarch-specific code
    
    Instead of doing the kvm_guest_enter/exit() and local_irq_dis/enable()
    calls in powerpc.c, this moves them down into the subarch-specific
    book3s_pr.c and booke.c.  This eliminates an extra local_irq_enable()
    call in book3s_pr.c, and will be needed for when we do SMT4 guest
    support in the book3s hypervisor mode code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 9b6f3f99c5eb..48b7ab76de2d 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -42,6 +42,7 @@ enum emulation_result {
 	EMULATE_AGAIN,        /* something went wrong. go again */
 };
 
+extern int kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
 extern int __kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
 extern char kvmppc_handlers_start[];
 extern unsigned long kvmppc_handler_len;

commit f9e0554deca54a42fb2cf7f68c05a4a37461c205
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:19:22 2011 +0000

    KVM: PPC: Pass init/destroy vm and prepare/commit memory region ops down
    
    This arranges for the top-level arch/powerpc/kvm/powerpc.c file to
    pass down some of the calls it gets to the lower-level subarchitecture
    specific code.  The lower-level implementations (in booke.c and book3s.c)
    are no-ops.  The coming book3s_hv.c will need this.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c662f140283a..9b6f3f99c5eb 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -111,6 +111,13 @@ extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 extern int kvmppc_kvm_pv(struct kvm_vcpu *vcpu);
 extern void kvmppc_map_magic(struct kvm_vcpu *vcpu);
 
+extern int kvmppc_core_init_vm(struct kvm *kvm);
+extern void kvmppc_core_destroy_vm(struct kvm *kvm);
+extern int kvmppc_core_prepare_memory_region(struct kvm *kvm,
+				struct kvm_userspace_memory_region *mem);
+extern void kvmppc_core_commit_memory_region(struct kvm *kvm,
+				struct kvm_userspace_memory_region *mem);
+
 /*
  * Cuts out inst bits with ordering according to spec.
  * That means the leftmost bit is zero. All given bits are included.

commit a4cd8b23ac5786943202c0174c717956947db43c
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Jun 14 18:34:41 2011 -0500

    KVM: PPC: e500: enable magic page
    
    This is a shared page used for paravirtualization.  It is always present
    in the guest kernel's effective address space at the address indicated
    by the hypercall that enables it.
    
    The physical address specified by the hypercall is not used, as
    e500 does not have real mode.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 9345238edecf..c662f140283a 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -109,6 +109,7 @@ extern void kvmppc_booke_exit(void);
 
 extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 extern int kvmppc_kvm_pv(struct kvm_vcpu *vcpu);
+extern void kvmppc_map_magic(struct kvm_vcpu *vcpu);
 
 /*
  * Cuts out inst bits with ordering according to spec.

commit 5ce941ee4258b836cf818d2ac159d8cf3ebad648
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Apr 27 17:24:21 2011 -0500

    KVM: PPC: booke: add sregs support
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index ecb3bc74c344..9345238edecf 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -61,6 +61,7 @@ extern int kvmppc_emulate_instruction(struct kvm_run *run,
                                       struct kvm_vcpu *vcpu);
 extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
 extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
+extern u32 kvmppc_get_dec(struct kvm_vcpu *vcpu, u64 tb);
 
 /* Core-specific hooks */
 
@@ -142,4 +143,12 @@ static inline u32 kvmppc_set_field(u64 inst, int msb, int lsb, int value)
 	return r;
 }
 
+void kvmppc_core_get_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+int kvmppc_core_set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+
+void kvmppc_get_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+int kvmppc_set_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs);
+
+void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 pid);
+
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 2a342ed57756ad5d8af5456959433884367e5ab2
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:48 2010 +0200

    KVM: PPC: Implement hypervisor interface
    
    To communicate with KVM directly we need to plumb some sort of interface
    between the guest and KVM. Usually those interfaces use hypercalls.
    
    This hypercall implementation is described in the last patch of the series
    in a special documentation file. Please read that for further information.
    
    This patch implements stubs to handle KVM PPC hypercalls on the host and
    guest side alike.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 18d139ec2d22..ecb3bc74c344 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -107,6 +107,7 @@ extern int kvmppc_booke_init(void);
 extern void kvmppc_booke_exit(void);
 
 extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
+extern int kvmppc_kvm_pv(struct kvm_vcpu *vcpu);
 
 /*
  * Cuts out inst bits with ordering according to spec.

commit 9cc5e9538ab7cbbfb1d7263373d2f58ab2af2bad
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:45 2010 +0200

    KVM: PPC: Extract MMU init
    
    The host shadow mmu code needs to get initialized. It needs to fetch a
    segment it can use to put shadow PTEs into.
    
    That initialization code was in generic code, which is icky. Let's move
    it over to the respective MMU file.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index edade847b8f8..18d139ec2d22 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -69,6 +69,7 @@ extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gpa_t gpaddr,
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 extern void kvmppc_mmu_destroy(struct kvm_vcpu *vcpu);
+extern int kvmppc_mmu_init(struct kvm_vcpu *vcpu);
 extern int kvmppc_mmu_dtlb_index(struct kvm_vcpu *vcpu, gva_t eaddr);
 extern int kvmppc_mmu_itlb_index(struct kvm_vcpu *vcpu, gva_t eaddr);
 extern gpa_t kvmppc_mmu_xlate(struct kvm_vcpu *vcpu, unsigned int gtlb_index,

commit c7f38f46f2a98d232147e47284cb4e7363296a3e
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:40 2010 +0200

    KVM: PPC: Improve indirect svcpu accessors
    
    We already have some inline fuctions we use to access vcpu or svcpu structs,
    depending on whether we're on booke or book3s. Since we just put a few more
    registers into the svcpu, we also need to make sure the respective callbacks
    are available and get used.
    
    So this patch moves direct use of the now in the svcpu struct fields to
    inline function calls. While at it, it also moves the definition of those
    inline function calls to respective header files for booke and book3s,
    greatly improving readability.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 6a2464e4d6b9..edade847b8f8 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -30,6 +30,8 @@
 #include <linux/kvm_host.h>
 #ifdef CONFIG_PPC_BOOK3S
 #include <asm/kvm_book3s.h>
+#else
+#include <asm/kvm_booke.h>
 #endif
 
 enum emulation_result {
@@ -138,81 +140,4 @@ static inline u32 kvmppc_set_field(u64 inst, int msb, int lsb, int value)
 	return r;
 }
 
-#ifdef CONFIG_PPC_BOOK3S
-
-/* We assume we're always acting on the current vcpu */
-
-static inline void kvmppc_set_gpr(struct kvm_vcpu *vcpu, int num, ulong val)
-{
-	if ( num < 14 ) {
-		get_paca()->shadow_vcpu.gpr[num] = val;
-		to_book3s(vcpu)->shadow_vcpu.gpr[num] = val;
-	} else
-		vcpu->arch.gpr[num] = val;
-}
-
-static inline ulong kvmppc_get_gpr(struct kvm_vcpu *vcpu, int num)
-{
-	if ( num < 14 )
-		return get_paca()->shadow_vcpu.gpr[num];
-	else
-		return vcpu->arch.gpr[num];
-}
-
-static inline void kvmppc_set_cr(struct kvm_vcpu *vcpu, u32 val)
-{
-	get_paca()->shadow_vcpu.cr = val;
-	to_book3s(vcpu)->shadow_vcpu.cr = val;
-}
-
-static inline u32 kvmppc_get_cr(struct kvm_vcpu *vcpu)
-{
-	return get_paca()->shadow_vcpu.cr;
-}
-
-static inline void kvmppc_set_xer(struct kvm_vcpu *vcpu, u32 val)
-{
-	get_paca()->shadow_vcpu.xer = val;
-	to_book3s(vcpu)->shadow_vcpu.xer = val;
-}
-
-static inline u32 kvmppc_get_xer(struct kvm_vcpu *vcpu)
-{
-	return get_paca()->shadow_vcpu.xer;
-}
-
-#else
-
-static inline void kvmppc_set_gpr(struct kvm_vcpu *vcpu, int num, ulong val)
-{
-	vcpu->arch.gpr[num] = val;
-}
-
-static inline ulong kvmppc_get_gpr(struct kvm_vcpu *vcpu, int num)
-{
-	return vcpu->arch.gpr[num];
-}
-
-static inline void kvmppc_set_cr(struct kvm_vcpu *vcpu, u32 val)
-{
-	vcpu->arch.cr = val;
-}
-
-static inline u32 kvmppc_get_cr(struct kvm_vcpu *vcpu)
-{
-	return vcpu->arch.cr;
-}
-
-static inline void kvmppc_set_xer(struct kvm_vcpu *vcpu, u32 val)
-{
-	vcpu->arch.xer = val;
-}
-
-static inline u32 kvmppc_get_xer(struct kvm_vcpu *vcpu)
-{
-	return vcpu->arch.xer;
-}
-
-#endif
-
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 18978768d89f638165646718c50ced19f2a10164
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Mar 24 21:48:18 2010 +0100

    KVM: PPC: Allow userspace to unset the IRQ line
    
    Userspace can tell us that it wants to trigger an interrupt. But
    so far it can't tell us that it wants to stop triggering one.
    
    So let's interpret the parameter to the ioctl that we have anyways
    to tell us if we want to raise or lower the interrupt line.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    
    v2 -> v3:
    
     - Add CAP for unset irq
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c7fcdd751f14..6a2464e4d6b9 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -92,6 +92,8 @@ extern void kvmppc_core_queue_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_dequeue_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_external(struct kvm_vcpu *vcpu,
                                        struct kvm_interrupt *irq);
+extern void kvmppc_core_dequeue_external(struct kvm_vcpu *vcpu,
+                                         struct kvm_interrupt *irq);
 
 extern int kvmppc_core_emulate_op(struct kvm_run *run, struct kvm_vcpu *vcpu,
                                   unsigned int op, int *advance);

commit 0564ee8a8611326f28bae2a0455182b458826762
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Feb 19 11:00:42 2010 +0100

    KVM: PPC: Add helpers to modify ppc fields
    
    The PowerPC specification always lists bits from MSB to LSB. That is
    really confusing when you're trying to write C code, because it fits
    in pretty badly with the normal (1 << xx) schemes.
    
    So I came up with some nice wrappers that allow to get and set fields
    in a u64 with bit numbers exactly as given in the spec. That makes the
    code in KVM and the spec easier comparable.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 07612189eb8b..c7fcdd751f14 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -103,6 +103,39 @@ extern void kvmppc_booke_exit(void);
 
 extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 
+/*
+ * Cuts out inst bits with ordering according to spec.
+ * That means the leftmost bit is zero. All given bits are included.
+ */
+static inline u32 kvmppc_get_field(u64 inst, int msb, int lsb)
+{
+	u32 r;
+	u32 mask;
+
+	BUG_ON(msb > lsb);
+
+	mask = (1 << (lsb - msb + 1)) - 1;
+	r = (inst >> (63 - lsb)) & mask;
+
+	return r;
+}
+
+/*
+ * Replaces inst bits with ordering according to spec.
+ */
+static inline u32 kvmppc_set_field(u64 inst, int msb, int lsb, int value)
+{
+	u32 r;
+	u32 mask;
+
+	BUG_ON(msb > lsb);
+
+	mask = ((1 << (lsb - msb + 1)) - 1) << (63 - lsb);
+	r = (inst & ~mask) | ((value << (63 - lsb)) & mask);
+
+	return r;
+}
+
 #ifdef CONFIG_PPC_BOOK3S
 
 /* We assume we're always acting on the current vcpu */

commit 37f5bca64e206ed97e53f734d7de5b7c5ade3578
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Feb 19 11:00:31 2010 +0100

    KVM: PPC: Add AGAIN type for emulation return
    
    Emulation of an instruction can have different outcomes. It can succeed,
    fail, require MMIO, do funky BookE stuff - or it can just realize something's
    odd and will be fixed the next time around.
    
    Exactly that is what EMULATE_AGAIN means. Using that flag we can now tell
    the caller that nothing happened, but we still want to go back to the
    guest and see what happens next time we come around.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index a288dd2fbb2c..07612189eb8b 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -37,6 +37,7 @@ enum emulation_result {
 	EMULATE_DO_MMIO,      /* kvm_run filled with MMIO request */
 	EMULATE_DO_DCR,       /* kvm_run filled with DCR request */
 	EMULATE_FAIL,         /* can't emulate this instruction */
+	EMULATE_AGAIN,        /* something went wrong. go again */
 };
 
 extern int __kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);

commit 3587d5348ced089666c51411bd9d771fb0b072cf
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Feb 19 11:00:30 2010 +0100

    KVM: PPC: Teach MMIO Signedness
    
    The guest I was trying to get to run uses the LHA and LHAU instructions.
    Those instructions basically do a load, but also sign extend the result.
    
    Since we need to fill our registers by hand when doing MMIO, we also need
    to sign extend manually.
    
    This patch implements sign extended MMIO and the LHA(U) instructions.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index c011170f572b..a288dd2fbb2c 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -48,6 +48,9 @@ extern void kvmppc_dump_vcpu(struct kvm_vcpu *vcpu);
 extern int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
                               unsigned int rt, unsigned int bytes,
                               int is_bigendian);
+extern int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
+                               unsigned int rt, unsigned int bytes,
+                               int is_bigendian);
 extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
                                u64 val, unsigned int bytes, int is_bigendian);
 

commit b104d06632d08957f384ff7403f609fb5dfb9cbd
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Feb 19 11:00:29 2010 +0100

    KVM: PPC: Enable MMIO to do 64 bits, fprs and qprs
    
    Right now MMIO access can only happen for GPRs and is at most 32 bit wide.
    That's actually enough for almost all types of hardware out there.
    
    Unfortunately, the guest I was using used FPU writes to MMIO regions, so
    it ended up writing 64 bit MMIOs using FPRs and QPRs.
    
    So let's add code to handle those odd cases too.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index e2642829e435..c011170f572b 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -49,7 +49,7 @@ extern int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
                               unsigned int rt, unsigned int bytes,
                               int is_bigendian);
 extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
-                               u32 val, unsigned int bytes, int is_bigendian);
+                               u64 val, unsigned int bytes, int is_bigendian);
 
 extern int kvmppc_emulate_instruction(struct kvm_run *run,
                                       struct kvm_vcpu *vcpu);

commit 1c0006d8d131585095c4a27dbfcfb3970807a35e
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 15 14:49:12 2010 +0100

    KVM: PPC: Fix initial GPR settings
    
    Commit 7d01b4c3ed2bb33ceaf2d270cb4831a67a76b51b introduced PACA backed vcpu
    values. With this patch, when a userspace app was setting GPRs before it was
    actually first loaded, the set values get discarded.
    
    This is because vcpu_load loads them from the vcpu backing store that we use
    whenever we're not owning the PACA.
    
    That behavior is not really a major problem, because we don't need it for
    qemu. Other users (like kvmctl) do have problems with it though, so let's
    better do it right.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 09816da9e950..e2642829e435 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -28,6 +28,9 @@
 #include <linux/types.h>
 #include <linux/kvm_types.h>
 #include <linux/kvm_host.h>
+#ifdef CONFIG_PPC_BOOK3S
+#include <asm/kvm_book3s.h>
+#endif
 
 enum emulation_result {
 	EMULATE_DONE,         /* no further processing */
@@ -102,9 +105,10 @@ extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 
 static inline void kvmppc_set_gpr(struct kvm_vcpu *vcpu, int num, ulong val)
 {
-	if ( num < 14 )
+	if ( num < 14 ) {
 		get_paca()->shadow_vcpu.gpr[num] = val;
-	else
+		to_book3s(vcpu)->shadow_vcpu.gpr[num] = val;
+	} else
 		vcpu->arch.gpr[num] = val;
 }
 
@@ -119,6 +123,7 @@ static inline ulong kvmppc_get_gpr(struct kvm_vcpu *vcpu, int num)
 static inline void kvmppc_set_cr(struct kvm_vcpu *vcpu, u32 val)
 {
 	get_paca()->shadow_vcpu.cr = val;
+	to_book3s(vcpu)->shadow_vcpu.cr = val;
 }
 
 static inline u32 kvmppc_get_cr(struct kvm_vcpu *vcpu)
@@ -129,6 +134,7 @@ static inline u32 kvmppc_get_cr(struct kvm_vcpu *vcpu)
 static inline void kvmppc_set_xer(struct kvm_vcpu *vcpu, u32 val)
 {
 	get_paca()->shadow_vcpu.xer = val;
+	to_book3s(vcpu)->shadow_vcpu.xer = val;
 }
 
 static inline u32 kvmppc_get_xer(struct kvm_vcpu *vcpu)

commit 25a8a02d26a71c28e26417a3520c653c2d40af6b
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 8 02:58:07 2010 +0100

    KVM: PPC: Emulate trap SRR1 flags properly
    
    Book3S needs some flags in SRR1 to get to know details about an interrupt.
    
    One such example is the trap instruction. It tells the guest kernel that
    a program interrupt is due to a trap using a bit in SRR1.
    
    This patch implements above behavior, making WARN_ON behave like WARN_ON.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 89c5d79c3479..09816da9e950 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -80,7 +80,7 @@ extern void kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu);
 
 extern void kvmppc_core_deliver_interrupts(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_pending_dec(struct kvm_vcpu *vcpu);
-extern void kvmppc_core_queue_program(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_queue_program(struct kvm_vcpu *vcpu, ulong flags);
 extern void kvmppc_core_queue_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_dequeue_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_external(struct kvm_vcpu *vcpu,

commit 7e57cba06074da84d7c24d8c3f44040d2d8c88ac
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 8 02:58:03 2010 +0100

    KVM: PPC: Use PACA backed shadow vcpu
    
    We're being horribly racy right now. All the entry and exit code hijacks
    random fields from the PACA that could easily be used by different code in
    case we get interrupted, for example by a #MC or even page fault.
    
    After discussing this with Ben, we figured it's best to reserve some more
    space in the PACA and just shove off some vcpu state to there.
    
    That way we can drastically improve the readability of the code, make it
    less racy and less complex.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index d60b2f0cdcf2..89c5d79c3479 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -98,34 +98,42 @@ extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 
 #ifdef CONFIG_PPC_BOOK3S
 
+/* We assume we're always acting on the current vcpu */
+
 static inline void kvmppc_set_gpr(struct kvm_vcpu *vcpu, int num, ulong val)
 {
-	vcpu->arch.gpr[num] = val;
+	if ( num < 14 )
+		get_paca()->shadow_vcpu.gpr[num] = val;
+	else
+		vcpu->arch.gpr[num] = val;
 }
 
 static inline ulong kvmppc_get_gpr(struct kvm_vcpu *vcpu, int num)
 {
-	return vcpu->arch.gpr[num];
+	if ( num < 14 )
+		return get_paca()->shadow_vcpu.gpr[num];
+	else
+		return vcpu->arch.gpr[num];
 }
 
 static inline void kvmppc_set_cr(struct kvm_vcpu *vcpu, u32 val)
 {
-	vcpu->arch.cr = val;
+	get_paca()->shadow_vcpu.cr = val;
 }
 
 static inline u32 kvmppc_get_cr(struct kvm_vcpu *vcpu)
 {
-	return vcpu->arch.cr;
+	return get_paca()->shadow_vcpu.cr;
 }
 
 static inline void kvmppc_set_xer(struct kvm_vcpu *vcpu, u32 val)
 {
-	vcpu->arch.xer = val;
+	get_paca()->shadow_vcpu.xer = val;
 }
 
 static inline u32 kvmppc_get_xer(struct kvm_vcpu *vcpu)
 {
-	return vcpu->arch.xer;
+	return get_paca()->shadow_vcpu.xer;
 }
 
 #else

commit 992b5b29b5ae254c416c62faf98d59a6cf970027
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 8 02:58:02 2010 +0100

    KVM: PPC: Add helpers for CR, XER
    
    We now have helpers for the GPRs, so let's also add some for CR and XER.
    
    Having them in the PACA simplifies code a lot, as we don't need to care
    about where to store CC or not to overflow any integers.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index ba01b9c1d388..d60b2f0cdcf2 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -108,6 +108,26 @@ static inline ulong kvmppc_get_gpr(struct kvm_vcpu *vcpu, int num)
 	return vcpu->arch.gpr[num];
 }
 
+static inline void kvmppc_set_cr(struct kvm_vcpu *vcpu, u32 val)
+{
+	vcpu->arch.cr = val;
+}
+
+static inline u32 kvmppc_get_cr(struct kvm_vcpu *vcpu)
+{
+	return vcpu->arch.cr;
+}
+
+static inline void kvmppc_set_xer(struct kvm_vcpu *vcpu, u32 val)
+{
+	vcpu->arch.xer = val;
+}
+
+static inline u32 kvmppc_get_xer(struct kvm_vcpu *vcpu)
+{
+	return vcpu->arch.xer;
+}
+
 #else
 
 static inline void kvmppc_set_gpr(struct kvm_vcpu *vcpu, int num, ulong val)
@@ -120,6 +140,26 @@ static inline ulong kvmppc_get_gpr(struct kvm_vcpu *vcpu, int num)
 	return vcpu->arch.gpr[num];
 }
 
+static inline void kvmppc_set_cr(struct kvm_vcpu *vcpu, u32 val)
+{
+	vcpu->arch.cr = val;
+}
+
+static inline u32 kvmppc_get_cr(struct kvm_vcpu *vcpu)
+{
+	return vcpu->arch.cr;
+}
+
+static inline void kvmppc_set_xer(struct kvm_vcpu *vcpu, u32 val)
+{
+	vcpu->arch.xer = val;
+}
+
+static inline u32 kvmppc_get_xer(struct kvm_vcpu *vcpu)
+{
+	return vcpu->arch.xer;
+}
+
 #endif
 
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 8e5b26b55a8b6aee2c789b1d20ec715f9e4bea5c
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 8 02:58:01 2010 +0100

    KVM: PPC: Use accessor functions for GPR access
    
    All code in PPC KVM currently accesses gprs in the vcpu struct directly.
    
    While there's nothing wrong with that wrt the current way gprs are stored
    and loaded, it doesn't suffice for the PACA acceleration that will follow
    in this patchset.
    
    So let's just create little wrapper inline functions that we call whenever
    a GPR needs to be read from or written to. The compiled code shouldn't really
    change at all for now.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index abfd0c4d567b..ba01b9c1d388 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -96,4 +96,30 @@ extern void kvmppc_booke_exit(void);
 
 extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 
+#ifdef CONFIG_PPC_BOOK3S
+
+static inline void kvmppc_set_gpr(struct kvm_vcpu *vcpu, int num, ulong val)
+{
+	vcpu->arch.gpr[num] = val;
+}
+
+static inline ulong kvmppc_get_gpr(struct kvm_vcpu *vcpu, int num)
+{
+	return vcpu->arch.gpr[num];
+}
+
+#else
+
+static inline void kvmppc_set_gpr(struct kvm_vcpu *vcpu, int num, ulong val)
+{
+	vcpu->arch.gpr[num] = val;
+}
+
+static inline ulong kvmppc_get_gpr(struct kvm_vcpu *vcpu, int num)
+{
+	return vcpu->arch.gpr[num];
+}
+
+#endif
+
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 7706664d39a8eb8555408a24b1f17bd2086189c6
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Dec 21 20:21:24 2009 +0100

    KVM: powerpc: Improve DEC handling
    
    We treated the DEC interrupt like an edge based one. This is not true for
    Book3s. The DEC keeps firing until mtdec is issued again and thus clears
    the interrupt line.
    
    So let's implement this logic in KVM too. This patch moves the line clearing
    from the firing of the interrupt to the mtdec emulation.
    
    This makes PPC64 guests work without AGGRESSIVE_DEC defined.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Acked-by: Hollis Blanchard <hollis@penguinppc.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 269ee46ab028..abfd0c4d567b 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -82,6 +82,7 @@ extern void kvmppc_core_deliver_interrupts(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_pending_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_program(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_dec(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_dequeue_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_external(struct kvm_vcpu *vcpu,
                                        struct kvm_interrupt *irq);
 

commit 29eb61bca1e82dc59e4d9c575e6f21ce7a36d61d
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Oct 30 05:47:07 2009 +0000

    Add book3s_64 highmem asm code
    
    This is the of entry / exit code. In order to switch between host and guest
    context, we need to switch register state and call the exit code handler on
    exit.
    
    This assembly file does exactly that. To finally enter the guest it calls
    into book3s_64_slb.S. On exit it gets jumped at from book3s_64_slb.S too.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 2c6ee349df5e..269ee46ab028 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -39,6 +39,7 @@ enum emulation_result {
 extern int __kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
 extern char kvmppc_handlers_start[];
 extern unsigned long kvmppc_handler_len;
+extern void kvmppc_handler_highmem(void);
 
 extern void kvmppc_dump_vcpu(struct kvm_vcpu *vcpu);
 extern int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,

commit f5d0906b5bafd7faea553ed1cc92bd06755b66b9
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sun Jan 4 13:51:09 2009 -0600

    KVM: ppc: remove debug support broken by KVM debug rewrite
    
    After the rewrite of KVM's debug support, this code doesn't even build any
    more.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 6052779dbb56..2c6ee349df5e 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -77,9 +77,6 @@ extern int kvmppc_core_vcpu_translate(struct kvm_vcpu *vcpu,
 extern void kvmppc_core_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 extern void kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu);
 
-extern void kvmppc_core_load_guest_debugstate(struct kvm_vcpu *vcpu);
-extern void kvmppc_core_load_host_debugstate(struct kvm_vcpu *vcpu);
-
 extern void kvmppc_core_deliver_interrupts(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_pending_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_program(struct kvm_vcpu *vcpu);

commit b52a638c391c5c7b013180f5374274698b8535c8
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sat Jan 3 16:23:11 2009 -0600

    KVM: ppc: Add kvmppc_mmu_dtlb/itlb_miss for booke
    
    When itlb or dtlb miss happens, E500 needs to update some mmu registers.
    So that the auto-load mechanism can work on E500 when write a tlb entry.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 5e80a20f32b8..6052779dbb56 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -63,6 +63,8 @@ extern int kvmppc_mmu_dtlb_index(struct kvm_vcpu *vcpu, gva_t eaddr);
 extern int kvmppc_mmu_itlb_index(struct kvm_vcpu *vcpu, gva_t eaddr);
 extern gpa_t kvmppc_mmu_xlate(struct kvm_vcpu *vcpu, unsigned int gtlb_index,
                               gva_t eaddr);
+extern void kvmppc_mmu_dtlb_miss(struct kvm_vcpu *vcpu);
+extern void kvmppc_mmu_itlb_miss(struct kvm_vcpu *vcpu);
 
 extern struct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm,
                                                 unsigned int id);

commit fa86b8dda2e0faccefbeda61edc02a50bd588f4f
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sat Jan 3 16:23:03 2009 -0600

    KVM: ppc: rename 44x MMU functions used in booke.c
    
    e500 will provide its own implementation of these.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 9fd70aa916d9..5e80a20f32b8 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -59,6 +59,8 @@ extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gpa_t gpaddr,
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 extern void kvmppc_mmu_destroy(struct kvm_vcpu *vcpu);
+extern int kvmppc_mmu_dtlb_index(struct kvm_vcpu *vcpu, gva_t eaddr);
+extern int kvmppc_mmu_itlb_index(struct kvm_vcpu *vcpu, gva_t eaddr);
 extern gpa_t kvmppc_mmu_xlate(struct kvm_vcpu *vcpu, unsigned int gtlb_index,
                               gva_t eaddr);
 

commit be8d1cae07d5acf4a61046d7def5eda40f0981e1
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sat Jan 3 16:23:02 2009 -0600

    KVM: ppc: turn tlb_xlate() into a per-core hook (and give it a better name)
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index f661f8ba3ab8..9fd70aa916d9 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -59,6 +59,8 @@ extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gpa_t gpaddr,
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 extern void kvmppc_mmu_destroy(struct kvm_vcpu *vcpu);
+extern gpa_t kvmppc_mmu_xlate(struct kvm_vcpu *vcpu, unsigned int gtlb_index,
+                              gva_t eaddr);
 
 extern struct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm,
                                                 unsigned int id);

commit 58a96214a306fc7fc66105097eea9c4f3bfa35bc
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sat Jan 3 16:23:01 2009 -0600

    KVM: ppc: change kvmppc_mmu_map() parameters
    
    Passing just the TLB index will ease an e500 implementation.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 7ba95d28b837..f661f8ba3ab8 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -55,7 +55,6 @@ extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
 /* Core-specific hooks */
 
 extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gpa_t gpaddr,
-                           u64 asid, u32 flags, u32 max_bytes,
                            unsigned int gtlb_idx);
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);

commit ecc0981ff07cbe7cdf95de20be5b24fee8e49cb5
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sat Jan 3 16:22:59 2009 -0600

    KVM: ppc: cosmetic changes to mmu hook names
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 36d2a50a8487..7ba95d28b837 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -52,13 +52,14 @@ extern int kvmppc_emulate_instruction(struct kvm_run *run,
 extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
 extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
 
+/* Core-specific hooks */
+
 extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gpa_t gpaddr,
                            u64 asid, u32 flags, u32 max_bytes,
                            unsigned int gtlb_idx);
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
-
-/* Core-specific hooks */
+extern void kvmppc_mmu_destroy(struct kvm_vcpu *vcpu);
 
 extern struct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm,
                                                 unsigned int id);

commit 7924bd41097ae8991c6d38cef8b1e4058e30d198
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Tue Dec 2 15:51:55 2008 -0600

    KVM: ppc: directly insert shadow mappings into the hardware TLB
    
    Formerly, we used to maintain a per-vcpu shadow TLB and on every entry to the
    guest would load this array into the hardware TLB. This consumed 1280 bytes of
    memory (64 entries of 16 bytes plus a struct page pointer each), and also
    required some assembly to loop over the array on every entry.
    
    Instead of saving a copy in memory, we can just store shadow mappings directly
    into the hardware TLB, accepting that the host kernel will clobber these as
    part of the normal 440 TLB round robin. When we do that we need less than half
    the memory, and we have decreased the exit handling time for all guest exits,
    at the cost of increased number of TLB misses because the host overwrites some
    guest entries.
    
    These savings will be increased on processors with larger TLBs or which
    implement intelligent flush instructions like tlbivax (which will avoid the
    need to walk arrays in software).
    
    In addition to that and to the code simplification, we have a greater chance of
    leaving other host userspace mappings in the TLB, instead of forcing all
    subsequent tasks to re-fault all their mappings.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 5bb29267d6a6..36d2a50a8487 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -53,7 +53,8 @@ extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
 extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
 
 extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gpa_t gpaddr,
-                           u64 asid, u32 flags, u32 max_bytes);
+                           u64 asid, u32 flags, u32 max_bytes,
+                           unsigned int gtlb_idx);
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 

commit 891686188f69d330f7eeeec8e6642ccfb7453106
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Tue Dec 2 15:51:53 2008 -0600

    KVM: ppc: support large host pages
    
    KVM on 440 has always been able to handle large guest mappings with 4K host
    pages -- we must, since the guest kernel uses 256MB mappings.
    
    This patch makes KVM work when the host has large pages too (tested with 64K).
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 844f683302f6..5bb29267d6a6 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -52,8 +52,8 @@ extern int kvmppc_emulate_instruction(struct kvm_run *run,
 extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
 extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
 
-extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gfn_t gfn,
-                           u64 asid, u32 flags);
+extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gpa_t gpaddr,
+                           u64 asid, u32 flags, u32 max_bytes);
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 

commit d4cf3892e50b8e35341086a4fe2bb8a3989b55d4
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:23 2008 -0600

    KVM: ppc: optimize irq delivery path
    
    In kvmppc_deliver_interrupt is just one case left in the switch and it is a
    rare one (less than 8%) when looking at the exit numbers. Therefore we can
    at least drop the switch/case and if an if. I inserted an unlikely too, but
    that's open for discussion.
    
    In kvmppc_can_deliver_interrupt all frequent cases are in the default case.
    I know compilers are smart but we can make it easier for them. By writing
    down all options and removing the default case combined with the fact that
    ithe values are constants 0..15 should allow the compiler to write an easy
    jump table.
    Modifying kvmppc_can_deliver_interrupt pointed me to the fact that gcc seems
    to be unable to reduce priority_exception[x] to a build time constant.
    Therefore I changed the usage of the translation arrays in the interrupt
    delivery path completely. It is now using priority without translation to irq
    on the full irq delivery path.
    To be able to do that ivpr regs are stored by their priority now.
    
    Additionally the decision made in kvmppc_can_deliver_interrupt is already
    sufficient to get the value of interrupt_msr_mask[x]. Therefore we can replace
    the 16x4byte array used here with a single 4byte variable (might still be one
    miss, but the chance to find this in cache should be better than the right
    entry of the whole array).
    
    Signed-off-by: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 976ecc4b322e..844f683302f6 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -36,9 +36,6 @@ enum emulation_result {
 	EMULATE_FAIL,         /* can't emulate this instruction */
 };
 
-extern const unsigned char exception_priority[];
-extern const unsigned char priority_exception[];
-
 extern int __kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
 extern char kvmppc_handlers_start[];
 extern unsigned long kvmppc_handler_len;

commit db93f5745d836f81cef0b4101a7c2685eeb55efb
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:18 2008 -0600

    KVM: ppc: create struct kvm_vcpu_44x and introduce container_of() accessor
    
    This patch doesn't yet move all 44x-specific data into the new structure, but
    is the first step down that path. In the future we may also want to create a
    struct kvm_vcpu_booke.
    
    Based on patch from Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index d59332575b4d..976ecc4b322e 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -62,6 +62,9 @@ extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 
 /* Core-specific hooks */
 
+extern struct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm,
+                                                unsigned int id);
+extern void kvmppc_core_vcpu_free(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_vcpu_setup(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_check_processor_compat(void);
 extern int kvmppc_core_vcpu_translate(struct kvm_vcpu *vcpu,
@@ -85,6 +88,9 @@ extern int kvmppc_core_emulate_op(struct kvm_run *run, struct kvm_vcpu *vcpu,
 extern int kvmppc_core_emulate_mtspr(struct kvm_vcpu *vcpu, int sprn, int rs);
 extern int kvmppc_core_emulate_mfspr(struct kvm_vcpu *vcpu, int sprn, int rt);
 
+extern int kvmppc_booke_init(void);
+extern void kvmppc_booke_exit(void);
+
 extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 5cbb5106f50b4515815cd32cf944958c0d4da83f
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:17 2008 -0600

    KVM: ppc: Move the last bits of 44x code out of booke.c
    
    Needed to port to other Book E processors.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index aecf95d5fede..d59332575b4d 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -62,7 +62,10 @@ extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 
 /* Core-specific hooks */
 
+extern int kvmppc_core_vcpu_setup(struct kvm_vcpu *vcpu);
 extern int kvmppc_core_check_processor_compat(void);
+extern int kvmppc_core_vcpu_translate(struct kvm_vcpu *vcpu,
+                                      struct kvm_translation *tr);
 
 extern void kvmppc_core_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 extern void kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu);

commit 75f74f0dbe086c239b4b0cc5ed75b903ea3e663f
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:16 2008 -0600

    KVM: ppc: refactor instruction emulation into generic and core-specific pieces
    
    Cores provide 3 emulation hooks, implemented for example in the new
    4xx_emulate.c:
    kvmppc_core_emulate_op
    kvmppc_core_emulate_mtspr
    kvmppc_core_emulate_mfspr
    
    Strictly speaking the last two aren't necessary, but provide for more
    informative error reporting ("unknown SPR").
    
    Long term I'd like to have instruction decoding autogenerated from tables of
    opcodes, and that way we could aggregate universal, Book E, and core-specific
    instructions more easily and without redundant switch statements.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 96d5de90ac5a..aecf95d5fede 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -53,35 +53,13 @@ extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 extern int kvmppc_emulate_instruction(struct kvm_run *run,
                                       struct kvm_vcpu *vcpu);
 extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
-extern int kvmppc_emul_tlbwe(struct kvm_vcpu *vcpu, u8 ra, u8 rs, u8 ws);
-extern int kvmppc_emul_tlbsx(struct kvm_vcpu *vcpu, u8 rt, u8 ra, u8 rb, u8 rc);
+extern void kvmppc_emulate_dec(struct kvm_vcpu *vcpu);
 
 extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gfn_t gfn,
                            u64 asid, u32 flags);
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 
-/* Helper function for "full" MSR writes. No need to call this if only EE is
- * changing. */
-static inline void kvmppc_set_msr(struct kvm_vcpu *vcpu, u32 new_msr)
-{
-	if ((new_msr & MSR_PR) != (vcpu->arch.msr & MSR_PR))
-		kvmppc_mmu_priv_switch(vcpu, new_msr & MSR_PR);
-
-	vcpu->arch.msr = new_msr;
-
-	if (vcpu->arch.msr & MSR_WE)
-		kvm_vcpu_block(vcpu);
-}
-
-static inline void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 new_pid)
-{
-	if (vcpu->arch.pid != new_pid) {
-		vcpu->arch.pid = new_pid;
-		vcpu->arch.swap_pid = 1;
-	}
-}
-
 /* Core-specific hooks */
 
 extern int kvmppc_core_check_processor_compat(void);
@@ -99,6 +77,11 @@ extern void kvmppc_core_queue_dec(struct kvm_vcpu *vcpu);
 extern void kvmppc_core_queue_external(struct kvm_vcpu *vcpu,
                                        struct kvm_interrupt *irq);
 
+extern int kvmppc_core_emulate_op(struct kvm_run *run, struct kvm_vcpu *vcpu,
+                                  unsigned int op, int *advance);
+extern int kvmppc_core_emulate_mtspr(struct kvm_vcpu *vcpu, int sprn, int rs);
+extern int kvmppc_core_emulate_mfspr(struct kvm_vcpu *vcpu, int sprn, int rt);
+
 extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 9dd921cfea734409a931ccc6eafd7f09850311e9
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:14 2008 -0600

    KVM: ppc: Refactor powerpc.c to relocate 440-specific code
    
    This introduces a set of core-provided hooks. For 440, some of these are
    implemented by booke.c, with the rest in (the new) 44x.c.
    
    Note that these hooks are link-time, not run-time. Since it is not possible to
    build a single kernel for both e500 and 440 (for example), using function
    pointers would only add overhead.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 39daeaa82b53..96d5de90ac5a 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -61,23 +61,6 @@ extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gfn_t gfn,
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 
-/* XXX Book E specific */
-extern void kvmppc_tlbe_set_modified(struct kvm_vcpu *vcpu, unsigned int i);
-
-extern void kvmppc_check_and_deliver_interrupts(struct kvm_vcpu *vcpu);
-
-static inline void kvmppc_queue_exception(struct kvm_vcpu *vcpu, int exception)
-{
-	unsigned int priority = exception_priority[exception];
-	set_bit(priority, &vcpu->arch.pending_exceptions);
-}
-
-static inline void kvmppc_clear_exception(struct kvm_vcpu *vcpu, int exception)
-{
-	unsigned int priority = exception_priority[exception];
-	clear_bit(priority, &vcpu->arch.pending_exceptions);
-}
-
 /* Helper function for "full" MSR writes. No need to call this if only EE is
  * changing. */
 static inline void kvmppc_set_msr(struct kvm_vcpu *vcpu, u32 new_msr)
@@ -99,6 +82,23 @@ static inline void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 new_pid)
 	}
 }
 
+/* Core-specific hooks */
+
+extern int kvmppc_core_check_processor_compat(void);
+
+extern void kvmppc_core_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
+extern void kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu);
+
+extern void kvmppc_core_load_guest_debugstate(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_load_host_debugstate(struct kvm_vcpu *vcpu);
+
+extern void kvmppc_core_deliver_interrupts(struct kvm_vcpu *vcpu);
+extern int kvmppc_core_pending_dec(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_queue_program(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_queue_dec(struct kvm_vcpu *vcpu);
+extern void kvmppc_core_queue_external(struct kvm_vcpu *vcpu,
+                                       struct kvm_interrupt *irq);
+
 extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 0f55dc481ea5c4f87fc0161cb1b8c6e2cafae8fc
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:12 2008 -0600

    KVM: ppc: Rename "struct tlbe" to "struct kvmppc_44x_tlbe"
    
    This will ease ports to other cores.
    
    Also remove unused "struct kvm_tlb" while we're at it.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 4adb4a397508..39daeaa82b53 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -29,11 +29,6 @@
 #include <linux/kvm_types.h>
 #include <linux/kvm_host.h>
 
-struct kvm_tlb {
-	struct tlbe guest_tlb[PPC44x_TLB_SIZE];
-	struct tlbe shadow_tlb[PPC44x_TLB_SIZE];
-};
-
 enum emulation_result {
 	EMULATE_DONE,         /* no further processing */
 	EMULATE_DO_MMIO,      /* kvm_run filled with MMIO request */

commit a0d7b9f246074fab1f42678d203ef4ba281505f2
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:11 2008 -0600

    KVM: ppc: Move 440-specific TLB code into 44x_tlb.c
    
    This will make it easier to provide implementations for other cores.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index bb62ad876de3..4adb4a397508 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -58,11 +58,11 @@ extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 extern int kvmppc_emulate_instruction(struct kvm_run *run,
                                       struct kvm_vcpu *vcpu);
 extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
+extern int kvmppc_emul_tlbwe(struct kvm_vcpu *vcpu, u8 ra, u8 rs, u8 ws);
+extern int kvmppc_emul_tlbsx(struct kvm_vcpu *vcpu, u8 rt, u8 ra, u8 rb, u8 rc);
 
 extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gfn_t gfn,
                            u64 asid, u32 flags);
-extern void kvmppc_mmu_invalidate(struct kvm_vcpu *vcpu, gva_t eaddr,
-                                  gva_t eend, u32 asid);
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 

commit c30f8a6c6d74f67bc2107726cc61a1e7c71e9740
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Mon Nov 24 11:37:38 2008 -0600

    KVM: ppc: stop leaking host memory on VM exit
    
    When the VM exits, we must call put_page() for every page referenced in the
    shadow TLB.
    
    Without this patch, we usually leak 30-50 host pages (120 - 200 KiB with 4 KiB
    pages). The maximum number of pages leaked is the size of our shadow TLB, 64
    pages.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 8931ba729d2b..bb62ad876de3 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -104,4 +104,6 @@ static inline void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 new_pid)
 	}
 }
 
+extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
+
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 49dd2c492895828a90ecdf889e7fe9cfb40a82a7
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Jul 25 13:54:53 2008 -0500

    KVM: powerpc: Map guest userspace with TID=0 mappings
    
    When we use TID=N userspace mappings, we must ensure that kernel mappings have
    been destroyed when entering userspace. Using TID=1/TID=0 for kernel/user
    mappings and running userspace with PID=0 means that userspace can't access the
    kernel mappings, but the kernel can directly access userspace.
    
    The net is that we don't need to flush the TLB on privilege switches, but we do
    on guest context switches (which are far more infrequent). Guest boot time
    performance improvement: about 30%.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 8e7e42959903..8931ba729d2b 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -64,6 +64,7 @@ extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gfn_t gfn,
 extern void kvmppc_mmu_invalidate(struct kvm_vcpu *vcpu, gva_t eaddr,
                                   gva_t eend, u32 asid);
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
+extern void kvmppc_mmu_switch_pid(struct kvm_vcpu *vcpu, u32 pid);
 
 /* XXX Book E specific */
 extern void kvmppc_tlbe_set_modified(struct kvm_vcpu *vcpu, unsigned int i);
@@ -95,4 +96,12 @@ static inline void kvmppc_set_msr(struct kvm_vcpu *vcpu, u32 new_msr)
 		kvm_vcpu_block(vcpu);
 }
 
+static inline void kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 new_pid)
+{
+	if (vcpu->arch.pid != new_pid) {
+		vcpu->arch.pid = new_pid;
+		vcpu->arch.swap_pid = 1;
+	}
+}
+
 #endif /* __POWERPC_KVM_PPC_H__ */

commit 83aae4a8098eb8a40a2e9dab3714354182143b4f
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Jul 25 13:54:52 2008 -0500

    KVM: ppc: Write only modified shadow entries into the TLB on exit
    
    Track which TLB entries need to be written, instead of overwriting everything
    below the high water mark. Typically only a single guest TLB entry will be
    modified in a single exit.
    
    Guest boot time performance improvement: about 15%.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index a8b068792260..8e7e42959903 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -65,6 +65,9 @@ extern void kvmppc_mmu_invalidate(struct kvm_vcpu *vcpu, gva_t eaddr,
                                   gva_t eend, u32 asid);
 extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
 
+/* XXX Book E specific */
+extern void kvmppc_tlbe_set_modified(struct kvm_vcpu *vcpu, unsigned int i);
+
 extern void kvmppc_check_and_deliver_interrupts(struct kvm_vcpu *vcpu);
 
 static inline void kvmppc_queue_exception(struct kvm_vcpu *vcpu, int exception)

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
new file mode 100644
index 000000000000..a8b068792260
--- /dev/null
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -0,0 +1,95 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+ *
+ * Copyright IBM Corp. 2008
+ *
+ * Authors: Hollis Blanchard <hollisb@us.ibm.com>
+ */
+
+#ifndef __POWERPC_KVM_PPC_H__
+#define __POWERPC_KVM_PPC_H__
+
+/* This file exists just so we can dereference kvm_vcpu, avoiding nested header
+ * dependencies. */
+
+#include <linux/mutex.h>
+#include <linux/timer.h>
+#include <linux/types.h>
+#include <linux/kvm_types.h>
+#include <linux/kvm_host.h>
+
+struct kvm_tlb {
+	struct tlbe guest_tlb[PPC44x_TLB_SIZE];
+	struct tlbe shadow_tlb[PPC44x_TLB_SIZE];
+};
+
+enum emulation_result {
+	EMULATE_DONE,         /* no further processing */
+	EMULATE_DO_MMIO,      /* kvm_run filled with MMIO request */
+	EMULATE_DO_DCR,       /* kvm_run filled with DCR request */
+	EMULATE_FAIL,         /* can't emulate this instruction */
+};
+
+extern const unsigned char exception_priority[];
+extern const unsigned char priority_exception[];
+
+extern int __kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu);
+extern char kvmppc_handlers_start[];
+extern unsigned long kvmppc_handler_len;
+
+extern void kvmppc_dump_vcpu(struct kvm_vcpu *vcpu);
+extern int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+                              unsigned int rt, unsigned int bytes,
+                              int is_bigendian);
+extern int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+                               u32 val, unsigned int bytes, int is_bigendian);
+
+extern int kvmppc_emulate_instruction(struct kvm_run *run,
+                                      struct kvm_vcpu *vcpu);
+extern int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu);
+
+extern void kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 gvaddr, gfn_t gfn,
+                           u64 asid, u32 flags);
+extern void kvmppc_mmu_invalidate(struct kvm_vcpu *vcpu, gva_t eaddr,
+                                  gva_t eend, u32 asid);
+extern void kvmppc_mmu_priv_switch(struct kvm_vcpu *vcpu, int usermode);
+
+extern void kvmppc_check_and_deliver_interrupts(struct kvm_vcpu *vcpu);
+
+static inline void kvmppc_queue_exception(struct kvm_vcpu *vcpu, int exception)
+{
+	unsigned int priority = exception_priority[exception];
+	set_bit(priority, &vcpu->arch.pending_exceptions);
+}
+
+static inline void kvmppc_clear_exception(struct kvm_vcpu *vcpu, int exception)
+{
+	unsigned int priority = exception_priority[exception];
+	clear_bit(priority, &vcpu->arch.pending_exceptions);
+}
+
+/* Helper function for "full" MSR writes. No need to call this if only EE is
+ * changing. */
+static inline void kvmppc_set_msr(struct kvm_vcpu *vcpu, u32 new_msr)
+{
+	if ((new_msr & MSR_PR) != (vcpu->arch.msr & MSR_PR))
+		kvmppc_mmu_priv_switch(vcpu, new_msr & MSR_PR);
+
+	vcpu->arch.msr = new_msr;
+
+	if (vcpu->arch.msr & MSR_WE)
+		kvm_vcpu_block(vcpu);
+}
+
+#endif /* __POWERPC_KVM_PPC_H__ */
