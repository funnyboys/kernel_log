commit 9a5788c615f52f6d7bf0b61986a632d4ec86791d
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Mar 19 15:29:55 2020 +1100

    KVM: PPC: Book3S HV: Add a capability for enabling secure guests
    
    At present, on Power systems with Protected Execution Facility
    hardware and an ultravisor, a KVM guest can transition to being a
    secure guest at will.  Userspace (QEMU) has no way of knowing
    whether a host system is capable of running secure guests.  This
    will present a problem in future when the ultravisor is capable of
    migrating secure guests from one host to another, because
    virtualization management software will have no way to ensure that
    secure guests only run in domains where all of the hosts can
    support secure guests.
    
    This adds a VM capability which has two functions: (a) userspace
    can query it to find out whether the host can support secure guests,
    and (b) userspace can enable it for a guest, which allows that
    guest to become a secure guest.  If userspace does not enable it,
    KVM will return an error when the ultravisor does the hypercall
    that indicates that the guest is starting to transition to a
    secure guest.  The ultravisor will then abort the transition and
    the guest will terminate.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Ram Pai <linuxram@us.ibm.com>

diff --git a/arch/powerpc/include/asm/kvm_book3s_uvmem.h b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
index 5a9834e0e2d1..9cb7d8be2366 100644
--- a/arch/powerpc/include/asm/kvm_book3s_uvmem.h
+++ b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
@@ -5,6 +5,7 @@
 #ifdef CONFIG_PPC_UV
 int kvmppc_uvmem_init(void);
 void kvmppc_uvmem_free(void);
+bool kvmppc_uvmem_available(void);
 int kvmppc_uvmem_slot_init(struct kvm *kvm, const struct kvm_memory_slot *slot);
 void kvmppc_uvmem_slot_free(struct kvm *kvm,
 			    const struct kvm_memory_slot *slot);
@@ -30,6 +31,11 @@ static inline int kvmppc_uvmem_init(void)
 
 static inline void kvmppc_uvmem_free(void) { }
 
+static inline bool kvmppc_uvmem_available(void)
+{
+	return false;
+}
+
 static inline int
 kvmppc_uvmem_slot_init(struct kvm *kvm, const struct kvm_memory_slot *slot)
 {

commit 3a43970d55e9fd5475d3c4e5fe398ab831ec6c3a
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Mon Jan 6 18:02:37 2020 -0800

    KVM: PPC: Book3S HV: Implement H_SVM_INIT_ABORT hcall
    
    Implement the H_SVM_INIT_ABORT hcall which the Ultravisor can use to
    abort an SVM after it has issued the H_SVM_INIT_START and before the
    H_SVM_INIT_DONE hcalls. This hcall could be used when Ultravisor
    encounters security violations or other errors when starting an SVM.
    
    Note that this hcall is different from UV_SVM_TERMINATE ucall which
    is used by HV to terminate/cleanup an VM that has becore secure.
    
    The H_SVM_INIT_ABORT basically undoes operations that were done
    since the H_SVM_INIT_START hcall - i.e page-out all the VM pages back
    to normal memory, and terminate the SVM.
    
    (If we do not bring the pages back to normal memory, the text/data
    of the VM would be stuck in secure memory and since the SVM did not
    go secure, its MSR_S bit will be clear and the VM wont be able to
    access its pages even to do a clean exit).
    
    Based on patches and discussion with Paul Mackerras, Ram Pai and
    Bharata Rao.
    
    Signed-off-by: Ram Pai <linuxram@linux.ibm.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.ibm.com>
    Signed-off-by: Bharata B Rao <bharata@linux.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_book3s_uvmem.h b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
index 3cf8425b9838..5a9834e0e2d1 100644
--- a/arch/powerpc/include/asm/kvm_book3s_uvmem.h
+++ b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
@@ -19,6 +19,7 @@ unsigned long kvmppc_h_svm_page_out(struct kvm *kvm,
 unsigned long kvmppc_h_svm_init_start(struct kvm *kvm);
 unsigned long kvmppc_h_svm_init_done(struct kvm *kvm);
 int kvmppc_send_page_to_uv(struct kvm *kvm, unsigned long gfn);
+unsigned long kvmppc_h_svm_init_abort(struct kvm *kvm);
 void kvmppc_uvmem_drop_pages(const struct kvm_memory_slot *free,
 			     struct kvm *kvm, bool skip_page_out);
 #else
@@ -62,6 +63,11 @@ static inline unsigned long kvmppc_h_svm_init_done(struct kvm *kvm)
 	return H_UNSUPPORTED;
 }
 
+static inline unsigned long kvmppc_h_svm_init_abort(struct kvm *kvm)
+{
+	return H_UNSUPPORTED;
+}
+
 static inline int kvmppc_send_page_to_uv(struct kvm *kvm, unsigned long gfn)
 {
 	return -EFAULT;

commit ce477a7a1cdfc9aaafcfd03b45bde131a88d51de
Author: Sukadev Bhattiprolu <sukadev@linux.ibm.com>
Date:   Thu Dec 19 13:51:45 2019 -0800

    KVM: PPC: Add skip_page_out parameter to uvmem functions
    
    Add 'skip_page_out' parameter to kvmppc_uvmem_drop_pages() so the
    callers can specify whetheter or not to skip paging out pages. This
    will be needed in a follow-on patch that implements H_SVM_INIT_ABORT
    hcall.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_book3s_uvmem.h b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
index 50204e228f16..3cf8425b9838 100644
--- a/arch/powerpc/include/asm/kvm_book3s_uvmem.h
+++ b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
@@ -20,7 +20,7 @@ unsigned long kvmppc_h_svm_init_start(struct kvm *kvm);
 unsigned long kvmppc_h_svm_init_done(struct kvm *kvm);
 int kvmppc_send_page_to_uv(struct kvm *kvm, unsigned long gfn);
 void kvmppc_uvmem_drop_pages(const struct kvm_memory_slot *free,
-			     struct kvm *kvm);
+			     struct kvm *kvm, bool skip_page_out);
 #else
 static inline int kvmppc_uvmem_init(void)
 {
@@ -69,6 +69,6 @@ static inline int kvmppc_send_page_to_uv(struct kvm *kvm, unsigned long gfn)
 
 static inline void
 kvmppc_uvmem_drop_pages(const struct kvm_memory_slot *free,
-			struct kvm *kvm) { }
+			struct kvm *kvm, bool skip_page_out) { }
 #endif /* CONFIG_PPC_UV */
 #endif /* __ASM_KVM_BOOK3S_UVMEM_H__ */

commit c32622575dd0ecb6fd0b41e3a451bd58152971ba
Author: Bharata B Rao <bharata@linux.ibm.com>
Date:   Mon Nov 25 08:36:29 2019 +0530

    KVM: PPC: Book3S HV: Handle memory plug/unplug to secure VM
    
    Register the new memslot with UV during plug and unregister
    the memslot during unplug. In addition, release all the
    device pages during unplug.
    
    Signed-off-by: Bharata B Rao <bharata@linux.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_book3s_uvmem.h b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
index 3033a9585b43..50204e228f16 100644
--- a/arch/powerpc/include/asm/kvm_book3s_uvmem.h
+++ b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
@@ -19,6 +19,8 @@ unsigned long kvmppc_h_svm_page_out(struct kvm *kvm,
 unsigned long kvmppc_h_svm_init_start(struct kvm *kvm);
 unsigned long kvmppc_h_svm_init_done(struct kvm *kvm);
 int kvmppc_send_page_to_uv(struct kvm *kvm, unsigned long gfn);
+void kvmppc_uvmem_drop_pages(const struct kvm_memory_slot *free,
+			     struct kvm *kvm);
 #else
 static inline int kvmppc_uvmem_init(void)
 {
@@ -64,5 +66,9 @@ static inline int kvmppc_send_page_to_uv(struct kvm *kvm, unsigned long gfn)
 {
 	return -EFAULT;
 }
+
+static inline void
+kvmppc_uvmem_drop_pages(const struct kvm_memory_slot *free,
+			struct kvm *kvm) { }
 #endif /* CONFIG_PPC_UV */
 #endif /* __ASM_KVM_BOOK3S_UVMEM_H__ */

commit 008e359c76d85facb10d10fa21fd5bc8c3a4e5d6
Author: Bharata B Rao <bharata@linux.ibm.com>
Date:   Mon Nov 25 08:36:28 2019 +0530

    KVM: PPC: Book3S HV: Radix changes for secure guest
    
    - After the guest becomes secure, when we handle a page fault of a page
      belonging to SVM in HV, send that page to UV via UV_PAGE_IN.
    - Whenever a page is unmapped on the HV side, inform UV via UV_PAGE_INVAL.
    - Ensure all those routines that walk the secondary page tables of
      the guest don't do so in case of secure VM. For secure guest, the
      active secondary page tables are in secure memory and the secondary
      page tables in HV are freed when guest becomes secure.
    
    Signed-off-by: Bharata B Rao <bharata@linux.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_book3s_uvmem.h b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
index 95f389c2937b..3033a9585b43 100644
--- a/arch/powerpc/include/asm/kvm_book3s_uvmem.h
+++ b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
@@ -18,6 +18,7 @@ unsigned long kvmppc_h_svm_page_out(struct kvm *kvm,
 				    unsigned long page_shift);
 unsigned long kvmppc_h_svm_init_start(struct kvm *kvm);
 unsigned long kvmppc_h_svm_init_done(struct kvm *kvm);
+int kvmppc_send_page_to_uv(struct kvm *kvm, unsigned long gfn);
 #else
 static inline int kvmppc_uvmem_init(void)
 {
@@ -58,5 +59,10 @@ static inline unsigned long kvmppc_h_svm_init_done(struct kvm *kvm)
 {
 	return H_UNSUPPORTED;
 }
+
+static inline int kvmppc_send_page_to_uv(struct kvm *kvm, unsigned long gfn)
+{
+	return -EFAULT;
+}
 #endif /* CONFIG_PPC_UV */
 #endif /* __ASM_KVM_BOOK3S_UVMEM_H__ */

commit ca9f4942670c37407bb109090eaf776ce2ccc54c
Author: Bharata B Rao <bharata@linux.ibm.com>
Date:   Mon Nov 25 08:36:26 2019 +0530

    KVM: PPC: Book3S HV: Support for running secure guests
    
    A pseries guest can be run as secure guest on Ultravisor-enabled
    POWER platforms. On such platforms, this driver will be used to manage
    the movement of guest pages between the normal memory managed by
    hypervisor (HV) and secure memory managed by Ultravisor (UV).
    
    HV is informed about the guest's transition to secure mode via hcalls:
    
    H_SVM_INIT_START: Initiate securing a VM
    H_SVM_INIT_DONE: Conclude securing a VM
    
    As part of H_SVM_INIT_START, register all existing memslots with
    the UV. H_SVM_INIT_DONE call by UV informs HV that transition of
    the guest to secure mode is complete.
    
    These two states (transition to secure mode STARTED and transition
    to secure mode COMPLETED) are recorded in kvm->arch.secure_guest.
    Setting these states will cause the assembly code that enters the
    guest to call the UV_RETURN ucall instead of trying to enter the
    guest directly.
    
    Migration of pages betwen normal and secure memory of secure
    guest is implemented in H_SVM_PAGE_IN and H_SVM_PAGE_OUT hcalls.
    
    H_SVM_PAGE_IN: Move the content of a normal page to secure page
    H_SVM_PAGE_OUT: Move the content of a secure page to normal page
    
    Private ZONE_DEVICE memory equal to the amount of secure memory
    available in the platform for running secure guests is created.
    Whenever a page belonging to the guest becomes secure, a page from
    this private device memory is used to represent and track that secure
    page on the HV side. The movement of pages between normal and secure
    memory is done via migrate_vma_pages() using UV_PAGE_IN and
    UV_PAGE_OUT ucalls.
    
    In order to prevent the device private pages (that correspond to pages
    of secure guest) from participating in KSM merging, H_SVM_PAGE_IN
    calls ksm_madvise() under read version of mmap_sem. However
    ksm_madvise() needs to be under write lock.  Hence we call
    kvmppc_svm_page_in with mmap_sem held for writing, and it then
    downgrades to a read lock after calling ksm_madvise.
    
    [paulus@ozlabs.org - roll in patch "KVM: PPC: Book3S HV: Take write
     mmap_sem when calling ksm_madvise"]
    
    Signed-off-by: Bharata B Rao <bharata@linux.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/kvm_book3s_uvmem.h b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
new file mode 100644
index 000000000000..95f389c2937b
--- /dev/null
+++ b/arch/powerpc/include/asm/kvm_book3s_uvmem.h
@@ -0,0 +1,62 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __ASM_KVM_BOOK3S_UVMEM_H__
+#define __ASM_KVM_BOOK3S_UVMEM_H__
+
+#ifdef CONFIG_PPC_UV
+int kvmppc_uvmem_init(void);
+void kvmppc_uvmem_free(void);
+int kvmppc_uvmem_slot_init(struct kvm *kvm, const struct kvm_memory_slot *slot);
+void kvmppc_uvmem_slot_free(struct kvm *kvm,
+			    const struct kvm_memory_slot *slot);
+unsigned long kvmppc_h_svm_page_in(struct kvm *kvm,
+				   unsigned long gra,
+				   unsigned long flags,
+				   unsigned long page_shift);
+unsigned long kvmppc_h_svm_page_out(struct kvm *kvm,
+				    unsigned long gra,
+				    unsigned long flags,
+				    unsigned long page_shift);
+unsigned long kvmppc_h_svm_init_start(struct kvm *kvm);
+unsigned long kvmppc_h_svm_init_done(struct kvm *kvm);
+#else
+static inline int kvmppc_uvmem_init(void)
+{
+	return 0;
+}
+
+static inline void kvmppc_uvmem_free(void) { }
+
+static inline int
+kvmppc_uvmem_slot_init(struct kvm *kvm, const struct kvm_memory_slot *slot)
+{
+	return 0;
+}
+
+static inline void
+kvmppc_uvmem_slot_free(struct kvm *kvm, const struct kvm_memory_slot *slot) { }
+
+static inline unsigned long
+kvmppc_h_svm_page_in(struct kvm *kvm, unsigned long gra,
+		     unsigned long flags, unsigned long page_shift)
+{
+	return H_UNSUPPORTED;
+}
+
+static inline unsigned long
+kvmppc_h_svm_page_out(struct kvm *kvm, unsigned long gra,
+		      unsigned long flags, unsigned long page_shift)
+{
+	return H_UNSUPPORTED;
+}
+
+static inline unsigned long kvmppc_h_svm_init_start(struct kvm *kvm)
+{
+	return H_UNSUPPORTED;
+}
+
+static inline unsigned long kvmppc_h_svm_init_done(struct kvm *kvm)
+{
+	return H_UNSUPPORTED;
+}
+#endif /* CONFIG_PPC_UV */
+#endif /* __ASM_KVM_BOOK3S_UVMEM_H__ */
