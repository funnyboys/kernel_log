commit 4e0e45b07d790253643ee05300784ab2156e2d5e
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu May 7 22:13:32 2020 +1000

    powerpc: Use trap metadata to prevent double restart rather than zeroing trap
    
    It's not very nice to zero trap for this, because then system calls no
    longer have trap_is_syscall(regs) invariant, and we can't distinguish
    between sc and scv system calls (in a later patch).
    
    Take one last unused bit from the low bits of the pt_regs.trap word
    for this instead. There is not a really good reason why it should be
    in trap as opposed to another field, but trap has some concept of
    flags and it exists. Ideally I think we would move trap to 2-byte
    field and have 2 more bytes available independently.
    
    Add a selftests case for this, which can be seen to fail if
    trap_norestart() is changed to return false.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Make them static inlines]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200507121332.2233629-4-mpe@ellerman.id.au

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 5db45790a087..ac3970fff0d5 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -182,13 +182,13 @@ extern int ptrace_put_reg(struct task_struct *task, int regno,
 
 #ifdef __powerpc64__
 #ifdef CONFIG_PPC_BOOK3S
-#define TRAP_FLAGS_MASK		0
-#define TRAP(regs)		((regs)->trap)
+#define TRAP_FLAGS_MASK		0x10
+#define TRAP(regs)		((regs)->trap & ~TRAP_FLAGS_MASK)
 #define FULL_REGS(regs)		true
 #define SET_FULL_REGS(regs)	do { } while (0)
 #else
-#define TRAP_FLAGS_MASK		0x1
-#define TRAP(regs)		((regs)->trap & ~0x1)
+#define TRAP_FLAGS_MASK		0x11
+#define TRAP(regs)		((regs)->trap & ~TRAP_FLAGS_MASK)
 #define FULL_REGS(regs)		(((regs)->trap & 1) == 0)
 #define SET_FULL_REGS(regs)	((regs)->trap |= 1)
 #endif
@@ -202,8 +202,8 @@ extern int ptrace_put_reg(struct task_struct *task, int regno,
  * On 4xx we use the next bit to indicate whether the exception
  * is a critical exception (1 means it is).
  */
-#define TRAP_FLAGS_MASK		0xF
-#define TRAP(regs)		((regs)->trap & ~0xF)
+#define TRAP_FLAGS_MASK		0x1F
+#define TRAP(regs)		((regs)->trap & ~TRAP_FLAGS_MASK)
 #define FULL_REGS(regs)		(((regs)->trap & 1) == 0)
 #define SET_FULL_REGS(regs)	((regs)->trap |= 1)
 #define IS_CRITICAL_EXC(regs)	(((regs)->trap & 2) != 0)
@@ -227,6 +227,16 @@ static inline bool trap_is_syscall(struct pt_regs *regs)
 	return TRAP(regs) == 0xc00;
 }
 
+static inline bool trap_norestart(struct pt_regs *regs)
+{
+	return regs->trap & 0x10;
+}
+
+static inline void set_trap_norestart(struct pt_regs *regs)
+{
+	regs->trap |= 0x10;
+}
+
 #define arch_has_single_step()	(1)
 #ifndef CONFIG_BOOK3S_601
 #define arch_has_block_step()	(true)

commit 912237ea166428edcbf3c137adf12cb987c477f2
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu May 7 22:13:31 2020 +1000

    powerpc: trap_is_syscall() helper to hide syscall trap number
    
    A new system call interrupt will be added with a new trap number.
    Hide the explicit 0xc00 test behind an accessor to reduce churn
    in callers.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Make it a static inline]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200507121332.2233629-3-mpe@ellerman.id.au

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 7c585bddc06e..5db45790a087 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -222,6 +222,11 @@ static inline void set_trap(struct pt_regs *regs, unsigned long val)
 	regs->trap = (regs->trap & TRAP_FLAGS_MASK) | (val & ~TRAP_FLAGS_MASK);
 }
 
+static inline bool trap_is_syscall(struct pt_regs *regs)
+{
+	return TRAP(regs) == 0xc00;
+}
+
 #define arch_has_single_step()	(1)
 #ifndef CONFIG_BOOK3S_601
 #define arch_has_block_step()	(true)

commit db30144b5c9cfb09c6b8b2fa7a9c351c94aa3433
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu May 7 22:13:30 2020 +1000

    powerpc: Use set_trap() and avoid open-coding trap masking
    
    The pt_regs.trap field keeps 4 low bits for some metadata about the
    trap or how it was handled, which is masked off in order to test the
    architectural trap number.
    
    Add a set_trap() accessor to set this, equivalent to TRAP() for
    returning it. This is actually not quite the equivalent of TRAP()
    because it always clears the low bits, which may be harmless if
    it can only be updated via ptrace syscall, but it seems dangerous.
    
    In fact settting TRAP from ptrace doesn't seem like a great idea
    so maybe it's better deleted.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Make it a static inline rather than a shouty macro]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200507121332.2233629-2-mpe@ellerman.id.au

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 89f31d5a8062..7c585bddc06e 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -182,10 +182,12 @@ extern int ptrace_put_reg(struct task_struct *task, int regno,
 
 #ifdef __powerpc64__
 #ifdef CONFIG_PPC_BOOK3S
+#define TRAP_FLAGS_MASK		0
 #define TRAP(regs)		((regs)->trap)
 #define FULL_REGS(regs)		true
 #define SET_FULL_REGS(regs)	do { } while (0)
 #else
+#define TRAP_FLAGS_MASK		0x1
 #define TRAP(regs)		((regs)->trap & ~0x1)
 #define FULL_REGS(regs)		(((regs)->trap & 1) == 0)
 #define SET_FULL_REGS(regs)	((regs)->trap |= 1)
@@ -200,6 +202,7 @@ extern int ptrace_put_reg(struct task_struct *task, int regno,
  * On 4xx we use the next bit to indicate whether the exception
  * is a critical exception (1 means it is).
  */
+#define TRAP_FLAGS_MASK		0xF
 #define TRAP(regs)		((regs)->trap & ~0xF)
 #define FULL_REGS(regs)		(((regs)->trap & 1) == 0)
 #define SET_FULL_REGS(regs)	((regs)->trap |= 1)
@@ -214,6 +217,11 @@ do {									      \
 } while (0)
 #endif /* __powerpc64__ */
 
+static inline void set_trap(struct pt_regs *regs, unsigned long val)
+{
+	regs->trap = (regs->trap & TRAP_FLAGS_MASK) | (val & ~TRAP_FLAGS_MASK);
+}
+
 #define arch_has_single_step()	(1)
 #ifndef CONFIG_BOOK3S_601
 #define arch_has_block_step()	(true)

commit feb9df3462e688d073848d85c8bb132fe8fd9ae5
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu May 7 22:13:29 2020 +1000

    powerpc/64s: Always has full regs, so remove remnant checks
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200507121332.2233629-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index e0195e6b892b..89f31d5a8062 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -179,6 +179,20 @@ extern int ptrace_put_reg(struct task_struct *task, int regno,
 
 #define current_pt_regs() \
 	((struct pt_regs *)((unsigned long)task_stack_page(current) + THREAD_SIZE) - 1)
+
+#ifdef __powerpc64__
+#ifdef CONFIG_PPC_BOOK3S
+#define TRAP(regs)		((regs)->trap)
+#define FULL_REGS(regs)		true
+#define SET_FULL_REGS(regs)	do { } while (0)
+#else
+#define TRAP(regs)		((regs)->trap & ~0x1)
+#define FULL_REGS(regs)		(((regs)->trap & 1) == 0)
+#define SET_FULL_REGS(regs)	((regs)->trap |= 1)
+#endif
+#define CHECK_FULL_REGS(regs)	BUG_ON(!FULL_REGS(regs))
+#define NV_REG_POISON		0xdeadbeefdeadbeefUL
+#else
 /*
  * We use the least-significant bit of the trap field to indicate
  * whether we have saved the full set of registers, or only a
@@ -186,17 +200,12 @@ extern int ptrace_put_reg(struct task_struct *task, int regno,
  * On 4xx we use the next bit to indicate whether the exception
  * is a critical exception (1 means it is).
  */
+#define TRAP(regs)		((regs)->trap & ~0xF)
 #define FULL_REGS(regs)		(((regs)->trap & 1) == 0)
-#ifndef __powerpc64__
+#define SET_FULL_REGS(regs)	((regs)->trap |= 1)
 #define IS_CRITICAL_EXC(regs)	(((regs)->trap & 2) != 0)
 #define IS_MCHECK_EXC(regs)	(((regs)->trap & 4) != 0)
 #define IS_DEBUG_EXC(regs)	(((regs)->trap & 8) != 0)
-#endif /* ! __powerpc64__ */
-#define TRAP(regs)		((regs)->trap & ~0xF)
-#ifdef __powerpc64__
-#define NV_REG_POISON		0xdeadbeefdeadbeefUL
-#define CHECK_FULL_REGS(regs)	BUG_ON(regs->trap & 1)
-#else
 #define NV_REG_POISON		0xdeadbeef
 #define CHECK_FULL_REGS(regs)						      \
 do {									      \

commit f1763e623c69bcec2c1e739e990058de41d45030
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Feb 28 00:14:39 2020 +0000

    powerpc/ptrace: drop unnecessary #ifdefs CONFIG_PPC64
    
    Drop a bunch of #ifdefs CONFIG_PPC64 that are not vital.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/af38b87a7e1e3efe4f9b664eaeb029e6e7d69fdb.1582848567.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 082a40153b94..e0195e6b892b 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -279,6 +279,8 @@ static inline unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,
 #endif /* __ASSEMBLY__ */
 
 #ifndef __powerpc64__
+/* We need PT_SOFTE defined at all time to avoid #ifdefs */
+#define PT_SOFTE PT_MQ
 #else /* __powerpc64__ */
 #define PT_FPSCR32 (PT_FPR0 + 2*32 + 1)	/* each FP reg occupies 2 32-bit userspace slots */
 #define PT_VR0_32 164	/* each Vector reg occupies 4 slots in 32-bit */

commit 68b34588e2027f699a3c034235f21cd19356b2e6
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 26 03:35:34 2020 +1000

    powerpc/64/sycall: Implement syscall entry/exit logic in C
    
    System call entry and particularly exit code is beyond the limit of
    what is reasonable to implement in asm.
    
    This conversion moves all conditional branches out of the asm code,
    except for the case that all GPRs should be restored at exit.
    
    Null syscall test is about 5% faster after this patch, because the
    exit work is handled under local_irq_disable, and the hard mask and
    pending interrupt replay is handled after that, which avoids games
    with MSR.
    
    mpe: Includes subsequent fixes from Nick:
    
    This fixes 4 issues caught by TM selftests. First was a tm-syscall bug
    that hit due to tabort_syscall being called after interrupts were
    reconciled (in a subsequent patch), which led to interrupts being
    enabled before tabort_syscall was called. Rather than going through an
    un-reconciling interrupts for the return, I just go back to putting
    the test early in asm, the C-ification of that wasn't a big win
    anyway.
    
    Second is the syscall return _TIF_USER_WORK_MASK check would go into
    an infinite loop if _TIF_RESTORE_TM became set. The asm code uses
    _TIF_USER_WORK_MASK to brach to slowpath which includes
    restore_tm_state.
    
    Third is system call return was not calling restore_tm_state, I missed
    this completely (alhtough it's in the return from interrupt C
    conversion because when the asm syscall code encountered problems it
    would branch to the interrupt return code.
    
    Fourth is MSR_VEC missing from restore_math, which was caught by
    tm-unavailable selftest taking an unexpected facility unavailable
    interrupt when testing VSX unavailble exception with MSR.FP=1
    MSR.VEC=1. Fourth case also has a fixup in a subsequent patch.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Suchanek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200225173541.1549955-26-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index ee3ada66deb5..082a40153b94 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -138,6 +138,9 @@ extern unsigned long profile_pc(struct pt_regs *regs);
 #define profile_pc(regs) instruction_pointer(regs)
 #endif
 
+long do_syscall_trace_enter(struct pt_regs *regs);
+void do_syscall_trace_leave(struct pt_regs *regs);
+
 #define kernel_stack_pointer(regs) ((regs)->gpr[1])
 static inline int is_syscall_success(struct pt_regs *regs)
 {

commit 12c3f1fd87bf4e55f06d079a45d6f15e2f6f9750
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Aug 26 15:52:14 2019 +0000

    powerpc/32s: get rid of CPU_FTR_601 feature
    
    Now that 601 is exclusive from other 6xx, CPU_FTR_601 and
    associated fixups are useless.
    
    Drop this feature and use #ifdefs instead.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/ecdb7194a17dbfa01865df6a82979533adc2c70b.1566834712.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index feee1b21bbd5..ee3ada66deb5 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -203,7 +203,11 @@ do {									      \
 #endif /* __powerpc64__ */
 
 #define arch_has_single_step()	(1)
-#define arch_has_block_step()	(!cpu_has_feature(CPU_FTR_601))
+#ifndef CONFIG_BOOK3S_601
+#define arch_has_block_step()	(true)
+#else
+#define arch_has_block_step()	(false)
+#endif
 #define ARCH_HAS_USER_SINGLE_STEP_REPORT
 
 /*

commit b42dfdea6052f7e8880f78e8e17881b30fefb840
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 24 07:47:25 2019 +0200

    powerpc: don't use asm-generic/ptrace.h
    
    Doing the indirection through macros for the regs accessors just
    makes them harder to read, so implement the helpers directly.
    
    Note that only the helpers actually used are implemented now.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index faa5a338ac5a..feee1b21bbd5 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -111,18 +111,33 @@ struct pt_regs
 
 #ifndef __ASSEMBLY__
 
-#define GET_IP(regs)		((regs)->nip)
-#define GET_USP(regs)		((regs)->gpr[1])
-#define GET_FP(regs)		(0)
-#define SET_FP(regs, val)
+static inline unsigned long instruction_pointer(struct pt_regs *regs)
+{
+	return regs->nip;
+}
+
+static inline void instruction_pointer_set(struct pt_regs *regs,
+		unsigned long val)
+{
+	regs->nip = val;
+}
+
+static inline unsigned long user_stack_pointer(struct pt_regs *regs)
+{
+	return regs->gpr[1];
+}
+
+static inline unsigned long frame_pointer(struct pt_regs *regs)
+{
+	return 0;
+}
 
 #ifdef CONFIG_SMP
 extern unsigned long profile_pc(struct pt_regs *regs);
-#define profile_pc profile_pc
+#else
+#define profile_pc(regs) instruction_pointer(regs)
 #endif
 
-#include <asm-generic/ptrace.h>
-
 #define kernel_stack_pointer(regs) ((regs)->gpr[1])
 static inline int is_syscall_success(struct pt_regs *regs)
 {

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 6f047730e642..faa5a338ac5a 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * Copyright (C) 2001 PPC64 Team, IBM Corp
  *
@@ -14,11 +15,6 @@
  *
  * Note that the offsets of the fields in this struct correspond with
  * the PT_* values below.  This simplifies arch/powerpc/kernel/ptrace.c.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 #ifndef _ASM_POWERPC_PTRACE_H
 #define _ASM_POWERPC_PTRACE_H

commit de78a9c42a790011f179bc94a7da3f5d8721f4cc
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Apr 18 16:51:20 2019 +1000

    powerpc: Add a framework for Kernel Userspace Access Protection
    
    This patch implements a framework for Kernel Userspace Access
    Protection.
    
    Then subarches will have the possibility to provide their own
    implementation by providing setup_kuap() and
    allow/prevent_user_access().
    
    Some platforms will need to know the area accessed and whether it is
    accessed from read, write or both. Therefore source, destination and
    size and handed over to the two functions.
    
    mpe: Rename to allow/prevent rather than unlock/lock, and add
    read/write wrappers. Drop the 32-bit code for now until we have an
    implementation for it. Add kuap to pt_regs for 64-bit as well as
    32-bit. Don't split strings, use pr_crit_ratelimited().
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Russell Currey <ruscur@russell.cc>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 64271e562fed..6f047730e642 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -52,10 +52,17 @@ struct pt_regs
 		};
 	};
 
+	union {
+		struct {
 #ifdef CONFIG_PPC64
-	unsigned long ppr;
-	unsigned long __pad;	/* Maintain 16 byte interrupt stack alignment */
+			unsigned long ppr;
+#endif
+#ifdef CONFIG_PPC_KUAP
+			unsigned long kuap;
 #endif
+		};
+		unsigned long __pad[2];	/* Maintain 16 byte interrupt stack alignment */
+	};
 };
 #endif
 

commit b72cc2e7aea1e42a82358bdc6c41dfaf7a5fa742
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Jan 18 18:40:34 2019 +1100

    powerpc: Use task_stack_page() in current_pt_regs()
    
    Change current_pt_regs() to use task_stack_page() rather than
    current_thread_info() so that it keeps working once we enable
    THREAD_INFO_IN_TASK.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Split out of large patch]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 0b8a735b6d85..64271e562fed 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -157,7 +157,7 @@ extern int ptrace_put_reg(struct task_struct *task, int regno,
 			  unsigned long data);
 
 #define current_pt_regs() \
-	((struct pt_regs *)((unsigned long)current_thread_info() + THREAD_SIZE) - 1)
+	((struct pt_regs *)((unsigned long)task_stack_page(current) + THREAD_SIZE) - 1)
 /*
  * We use the least-significant bit of the trap field to indicate
  * whether we have saved the full set of registers, or only a

commit 66f93c5a02d5ba6ef17fef459143961382593212
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Nov 15 12:34:27 2018 +1000

    powerpc/64: Fix kernel stack 16-byte alignment
    
    Commit 4c2de74cc869 ("powerpc/64: Interrupts save PPR on stack rather
    than thread_struct") changed sizeof(struct pt_regs) % 16 from 0 to 8,
    which causes the interrupt frame allocation on kernel entry to put the
    kernel stack out of alignment.
    
    Quadword (16-byte) alignment for the stack is required by both the
    64-bit v1 ABI (v1.9 § 3.2.2) and the 64-bit v2 ABI (v1.1 § 2.2.2.1).
    
    Add a pad field to fix alignment, and add a BUILD_BUG_ON to catch this
    in future.
    
    Fixes: 4c2de74cc869 ("powerpc/64: Interrupts save PPR on stack rather than thread_struct")
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index f73886a1a7f5..0b8a735b6d85 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -54,6 +54,7 @@ struct pt_regs
 
 #ifdef CONFIG_PPC64
 	unsigned long ppr;
+	unsigned long __pad;	/* Maintain 16 byte interrupt stack alignment */
 #endif
 };
 #endif

commit 685f7e4f161425b137056abe35ba8ef7b669d83d
Merge: c7a2c49ea6c9 58cfbac25b1f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 26 14:36:21 2018 -0700

    Merge tag 'powerpc-4.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - A large series to rewrite our SLB miss handling, replacing a lot of
         fairly complicated asm with much fewer lines of C.
    
       - Following on from that, we now maintain a cache of SLB entries for
         each process and preload them on context switch. Leading to a 27%
         speedup for our context switch benchmark on Power9.
    
       - Improvements to our handling of SLB multi-hit errors. We now print
         more debug information when they occur, and try to continue running
         by flushing the SLB and reloading, rather than treating them as
         fatal.
    
       - Enable THP migration on 64-bit Book3S machines (eg. Power7/8/9).
    
       - Add support for physical memory up to 2PB in the linear mapping on
         64-bit Book3S. We only support up to 512TB as regular system
         memory, otherwise the percpu allocator runs out of vmalloc space.
    
       - Add stack protector support for 32 and 64-bit, with a per-task
         canary.
    
       - Add support for PTRACE_SYSEMU and PTRACE_SYSEMU_SINGLESTEP.
    
       - Support recognising "big cores" on Power9, where two SMT4 cores are
         presented to us as a single SMT8 core.
    
       - A large series to cleanup some of our ioremap handling and PTE
         flags.
    
       - Add a driver for the PAPR SCM (storage class memory) interface,
         allowing guests to operate on SCM devices (acked by Dan).
    
       - Changes to our ftrace code to handle very large kernels, where we
         need to use a trampoline to get to ftrace_caller().
    
      And many other smaller enhancements and cleanups.
    
      Thanks to: Alan Modra, Alistair Popple, Aneesh Kumar K.V, Anton
      Blanchard, Aravinda Prasad, Bartlomiej Zolnierkiewicz, Benjamin
      Herrenschmidt, Breno Leitao, Cédric Le Goater, Christophe Leroy,
      Christophe Lombard, Dan Carpenter, Daniel Axtens, Finn Thain, Gautham
      R. Shenoy, Gustavo Romero, Haren Myneni, Hari Bathini, Jia Hongtao,
      Joel Stanley, John Allen, Laurent Dufour, Madhavan Srinivasan, Mahesh
      Salgaonkar, Mark Hairgrove, Masahiro Yamada, Michael Bringmann,
      Michael Neuling, Michal Suchanek, Murilo Opsfelder Araujo, Nathan
      Fontenot, Naveen N. Rao, Nicholas Piggin, Nick Desaulniers, Oliver
      O'Halloran, Paul Mackerras, Petr Vorel, Rashmica Gupta, Reza Arbab,
      Rob Herring, Sam Bobroff, Samuel Mendoza-Jonas, Scott Wood, Stan
      Johnson, Stephen Rothwell, Stewart Smith, Suraj Jitindar Singh, Tyrel
      Datwyler, Vaibhav Jain, Vasant Hegde, YueHaibing, zhong jiang"
    
    * tag 'powerpc-4.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (221 commits)
      Revert "selftests/powerpc: Fix out-of-tree build errors"
      powerpc/msi: Fix compile error on mpc83xx
      powerpc: Fix stack protector crashes on CPU hotplug
      powerpc/traps: restore recoverability of machine_check interrupts
      powerpc/64/module: REL32 relocation range check
      powerpc/64s/radix: Fix radix__flush_tlb_collapsed_pmd double flushing pmd
      selftests/powerpc: Add a test of wild bctr
      powerpc/mm: Fix page table dump to work on Radix
      powerpc/mm/radix: Display if mappings are exec or not
      powerpc/mm/radix: Simplify split mapping logic
      powerpc/mm/radix: Remove the retry in the split mapping logic
      powerpc/mm/radix: Fix small page at boundary when splitting
      powerpc/mm/radix: Fix overuse of small pages in splitting logic
      powerpc/mm/radix: Fix off-by-one in split mapping logic
      powerpc/ftrace: Handle large kernel configs
      powerpc/mm: Fix WARN_ON with THP NUMA migration
      selftests/powerpc: Fix out-of-tree build errors
      powerpc/time: no steal_time when CONFIG_PPC_SPLPAR is not selected
      powerpc/time: Only set CONFIG_ARCH_HAS_SCALED_CPUTIME on PPC64
      powerpc/time: isolate scaled cputime accounting in dedicated functions.
      ...

commit 7cd01b08d35f1b7d55686ed8cd57c94d3406ec8f
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Thu Jun 7 15:22:02 2018 +0530

    powerpc: Add support for function error injection
    
    We implement regs_set_return_value() and override_function_with_return()
    for this purpose.
    
    On powerpc, a return from a function (blr) just branches to the location
    contained in the link register. So, we can just update pt_regs rather
    than redirecting execution to a dummy function that returns.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Reviewed-by: Samuel Mendoza-Jonas <sam@mendozajonas.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 2ba2a1e52291..33196b311964 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -133,6 +133,11 @@ static inline long regs_return_value(struct pt_regs *regs)
 		return -regs->gpr[3];
 }
 
+static inline void regs_set_return_value(struct pt_regs *regs, unsigned long rc)
+{
+	regs->gpr[3] = rc;
+}
+
 #ifdef __powerpc64__
 #define user_mode(regs) ((((regs)->msr) >> MSR_PR_LG) & 0x1)
 #else

commit 4c2de74cc8696154b283f241d74ec0bb24438e22
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Oct 13 00:15:16 2018 +1100

    powerpc/64: Interrupts save PPR on stack rather than thread_struct
    
    PPR is the odd register out when it comes to interrupt handling, it is
    saved in current->thread.ppr while all others are saved on the stack.
    
    The difficulty with this is that accessing thread.ppr can cause a SLB
    fault, but the SLB fault handler implementation in C change had
    assumed the normal exception entry handlers would not cause an SLB
    fault.
    
    Fix this by allocating room in the interrupt stack to save PPR.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 3dd15024db93..2ba2a1e52291 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -51,6 +51,10 @@ struct pt_regs
 			unsigned long result;
 		};
 	};
+
+#ifdef CONFIG_PPC64
+	unsigned long ppr;
+#endif
 };
 #endif
 

commit 002af9391bfbe84f8e491bb10bd9c6001a6becee
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Oct 12 23:13:17 2018 +1100

    powerpc: Split user/kernel definitions of struct pt_regs
    
    We use a shared definition for struct pt_regs in uapi/asm/ptrace.h.
    That means the layout of the structure is ABI, ie. we can't change it.
    
    That would be fine if it was only used to describe the user-visible
    register state of a process, but it's also the struct we use in the
    kernel to describe the registers saved in an interrupt frame.
    
    We'd like more flexibility in the content (and possibly layout) of the
    kernel version of the struct, but currently that's not possible.
    
    So split the definition into a user-visible definition which remains
    unchanged, and a kernel internal one.
    
    At the moment they're still identical, and we check that at build
    time. That's because we have code (in ptrace etc.) that assumes that
    they are the same. We will fix that code in future patches, and then
    we can break the strict symmetry between the two structs.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 447cbd1bee99..3dd15024db93 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -26,6 +26,33 @@
 #include <uapi/asm/ptrace.h>
 #include <asm/asm-const.h>
 
+#ifndef __ASSEMBLY__
+struct pt_regs
+{
+	union {
+		struct user_pt_regs user_regs;
+		struct {
+			unsigned long gpr[32];
+			unsigned long nip;
+			unsigned long msr;
+			unsigned long orig_gpr3;
+			unsigned long ctr;
+			unsigned long link;
+			unsigned long xer;
+			unsigned long ccr;
+#ifdef CONFIG_PPC64
+			unsigned long softe;
+#else
+			unsigned long mq;
+#endif
+			unsigned long trap;
+			unsigned long dar;
+			unsigned long dsisr;
+			unsigned long result;
+		};
+	};
+};
+#endif
 
 #ifdef __powerpc64__
 

commit efc463adbccf709c5dbaf6cfbf84b7e94b62810a
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Apr 16 14:18:26 2018 -0500

    signal: Simplify tracehook_report_syscall_exit
    
    Replace user_single_step_siginfo with user_single_step_report
    that allocates siginfo structure on the stack and sends it.
    
    This allows tracehook_report_syscall_exit to become a simple
    if statement that calls user_single_step_report or ptrace_report_syscall
    depending on the value of step.
    
    Update the default helper function now called user_single_step_report
    to explicitly set si_code to SI_USER and to set si_uid and si_pid to 0.
    The default helper has always been doing this (using memset) but it
    was far from obvious.
    
    The powerpc helper can now just call force_sig_fault.
    The x86 helper can now just call send_sigtrap.
    
    Unfortunately the default implementation of user_single_step_report
    can not use force_sig_fault as it does not use a SIGTRAP si_code.
    So it has to carefully setup the siginfo and use use force_sig_info.
    
    The net result is code that is easier to understand and simpler
    to maintain.
    
    Ref: 85ec7fd9f8e5 ("ptrace: introduce user_single_step_siginfo() helper")
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 447cbd1bee99..5b480e1d5909 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -149,7 +149,7 @@ do {									      \
 
 #define arch_has_single_step()	(1)
 #define arch_has_block_step()	(!cpu_has_feature(CPU_FTR_601))
-#define ARCH_HAS_USER_SINGLE_STEP_INFO
+#define ARCH_HAS_USER_SINGLE_STEP_REPORT
 
 /*
  * kprobe-based event tracer support

commit ec0c464cdbf38bf6ddabec8bfa595bd421cab203
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jul 5 16:24:57 2018 +0000

    powerpc: move ASM_CONST and stringify_in_c() into asm-const.h
    
    This patch moves ASM_CONST() and stringify_in_c() into
    dedicated asm-const.h, then cleans all related inclusions.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: asm-compat.h should include asm-const.h]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index e4923686e43a..447cbd1bee99 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -24,6 +24,7 @@
 #define _ASM_POWERPC_PTRACE_H
 
 #include <uapi/asm/ptrace.h>
+#include <asm/asm-const.h>
 
 
 #ifdef __powerpc64__

commit f55d966536034d33476fdd43c45d47225344469f
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Jun 6 22:26:10 2016 +0530

    powerpc: Define and use PPC64_ELF_ABI_v2/v1
    
    We're approaching 20 locations where we need to check for ELF ABI v2.
    That's fine, except the logic is a bit awkward, because we have to check
    that _CALL_ELF is defined and then what its value is.
    
    So check it once in asm/types.h and define PPC64_ELF_ABI_v2 when ELF ABI
    v2 is detected.
    
    We also have a few places where what we're really trying to check is
    that we are using the 64-bit v1 ABI, ie. function descriptors. So also
    add a #define for that, which simplifies several checks.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index c0c61fa9cd9e..e4923686e43a 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -47,7 +47,7 @@
 				 STACK_FRAME_OVERHEAD + KERNEL_REDZONE_SIZE)
 #define STACK_FRAME_MARKER	12
 
-#if defined(_CALL_ELF) && _CALL_ELF == 2
+#ifdef PPC64_ELF_ABI_v2
 #define STACK_FRAME_MIN_SIZE	32
 #else
 #define STACK_FRAME_MIN_SIZE	STACK_FRAME_OVERHEAD

commit 85101af13bb854a6572fa540df7c7201958624b9
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Aug 26 12:44:15 2014 +1000

    powerpc/perf: Fix ABIv2 kernel backtraces
    
    ABIv2 kernels are failing to backtrace through the kernel. An example:
    
    39.30%  readseek2_proce  [kernel.kallsyms]    [k] find_get_entry
                |
                --- find_get_entry
                   __GI___libc_read
    
    The problem is in valid_next_sp() where we check that the new stack
    pointer is at least STACK_FRAME_OVERHEAD below the previous one.
    
    ABIv1 has a minimum stack frame size of 112 bytes consisting of 48 bytes
    and 64 bytes of parameter save area. ABIv2 changes that to 32 bytes
    with no paramter save area.
    
    STACK_FRAME_OVERHEAD is in theory the minimum stack frame size,
    but we over 240 uses of it, some of which assume that it includes
    space for the parameter area.
    
    We need to work through all our stack defines and rationalise them
    but let's fix perf now by creating STACK_FRAME_MIN_SIZE and using
    in valid_next_sp(). This fixes the issue:
    
    30.64%  readseek2_proce  [kernel.kallsyms]    [k] find_get_entry
                |
                --- find_get_entry
                   pagecache_get_page
                   generic_file_read_iter
                   new_sync_read
                   vfs_read
                   sys_read
                   syscall_exit
                   __GI___libc_read
    
    Cc: stable@vger.kernel.org # 3.16+
    Reported-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Anton Blanchard <anton@samba.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 279b80f3bb29..c0c61fa9cd9e 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -47,6 +47,12 @@
 				 STACK_FRAME_OVERHEAD + KERNEL_REDZONE_SIZE)
 #define STACK_FRAME_MARKER	12
 
+#if defined(_CALL_ELF) && _CALL_ELF == 2
+#define STACK_FRAME_MIN_SIZE	32
+#else
+#define STACK_FRAME_MIN_SIZE	STACK_FRAME_OVERHEAD
+#endif
+
 /* Size of dummy stack frame allocated when calling signal handler. */
 #define __SIGNAL_FRAMESIZE	128
 #define __SIGNAL_FRAMESIZE32	64
@@ -60,6 +66,7 @@
 #define STACK_FRAME_REGS_MARKER	ASM_CONST(0x72656773)
 #define STACK_INT_FRAME_SIZE	(sizeof(struct pt_regs) + STACK_FRAME_OVERHEAD)
 #define STACK_FRAME_MARKER	2
+#define STACK_FRAME_MIN_SIZE	STACK_FRAME_OVERHEAD
 
 /* Size of stack frame allocated when calling signal handler. */
 #define __SIGNAL_FRAMESIZE	64

commit 573ebfa6601fa58b439e7f15828762839ccd306a
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Feb 26 17:07:38 2014 +1100

    powerpc: Increase stack redzone for 64-bit userspace to 512 bytes
    
    The new ELFv2 little-endian ABI increases the stack redzone -- the
    area below the stack pointer that can be used for storing data --
    from 288 bytes to 512 bytes.  This means that we need to allow more
    space on the user stack when delivering a signal to a 64-bit process.
    
    To make the code a bit clearer, we define new USER_REDZONE_SIZE and
    KERNEL_REDZONE_SIZE symbols in ptrace.h.  For now, we leave the
    kernel redzone size at 288 bytes, since increasing it to 512 bytes
    would increase the size of interrupt stack frames correspondingly.
    
    Gcc currently only makes use of 288 bytes of redzone even when
    compiling for the new little-endian ABI, and the kernel cannot
    currently be compiled with the new ABI anyway.
    
    In the future, hopefully gcc will provide an option to control the
    amount of redzone used, and then we could reduce it even more.
    
    This also changes the code in arch_compat_alloc_user_space() to
    preserve the expanded redzone.  It is not clear why this function would
    ever be used on a 64-bit process, though.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    CC: <stable@vger.kernel.org> [v3.13]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index becc08e6a65c..279b80f3bb29 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -28,11 +28,23 @@
 
 #ifdef __powerpc64__
 
+/*
+ * Size of redzone that userspace is allowed to use below the stack
+ * pointer.  This is 288 in the 64-bit big-endian ELF ABI, and 512 in
+ * the new ELFv2 little-endian ABI, so we allow the larger amount.
+ *
+ * For kernel code we allow a 288-byte redzone, in order to conserve
+ * kernel stack space; gcc currently only uses 288 bytes, and will
+ * hopefully allow explicit control of the redzone size in future.
+ */
+#define USER_REDZONE_SIZE	512
+#define KERNEL_REDZONE_SIZE	288
+
 #define STACK_FRAME_OVERHEAD	112	/* size of minimum stack frame */
 #define STACK_FRAME_LR_SAVE	2	/* Location of LR in stack frame */
 #define STACK_FRAME_REGS_MARKER	ASM_CONST(0x7265677368657265)
 #define STACK_INT_FRAME_SIZE	(sizeof(struct pt_regs) + \
-					STACK_FRAME_OVERHEAD + 288)
+				 STACK_FRAME_OVERHEAD + KERNEL_REDZONE_SIZE)
 #define STACK_FRAME_MARKER	12
 
 /* Size of dummy stack frame allocated when calling signal handler. */
@@ -41,6 +53,8 @@
 
 #else /* __powerpc64__ */
 
+#define USER_REDZONE_SIZE	0
+#define KERNEL_REDZONE_SIZE	0
 #define STACK_FRAME_OVERHEAD	16	/* size of minimum stack frame */
 #define STACK_FRAME_LR_SAVE	1	/* Location of LR in stack frame */
 #define STACK_FRAME_REGS_MARKER	ASM_CONST(0x72656773)

commit ee4a3916614829914830bc4371358f4d4a63c4d9
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Thu Feb 14 17:44:23 2013 +0000

    powerpc: fixing ptrace_get_reg to return an error
    
    Currently ptrace_get_reg returns error as a value
    what make impossible to tell whether it is a correct value or error code.
    
    The patch adds a parameter which points to the real return data and
    returns an error code.
    
    As get_user_msr() never fails and it is used in multiple places so it has not
    been changed by this patch.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Acked-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 5f995681bc1d..becc08e6a65c 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -92,7 +92,8 @@ static inline long regs_return_value(struct pt_regs *regs)
 	} while(0)
 
 struct task_struct;
-extern unsigned long ptrace_get_reg(struct task_struct *task, int regno);
+extern int ptrace_get_reg(struct task_struct *task, int regno,
+			  unsigned long *data);
 extern int ptrace_put_reg(struct task_struct *task, int regno,
 			  unsigned long data);
 

commit b6897130f0ac8ac33043be736770eb5f58164044
Merge: b4fe19f78e84 fd3bc66f1dd5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 13 11:21:15 2012 +0900

    Merge branch 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc uapi disintegration from Benjamin Herrenschmidt.
    
    * 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc:
      UAPI: (Scripted) Disintegrate arch/powerpc/include/asm

commit c3617f72036c909e1f6086b5b9e364e0ef90a6da
Author: David Howells <dhowells@redhat.com>
Date:   Tue Oct 9 09:47:26 2012 +0100

    UAPI: (Scripted) Disintegrate arch/powerpc/include/asm
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 9c21ed42aba6..55380dc16f91 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -1,6 +1,3 @@
-#ifndef _ASM_POWERPC_PTRACE_H
-#define _ASM_POWERPC_PTRACE_H
-
 /*
  * Copyright (C) 2001 PPC64 Team, IBM Corp
  *
@@ -23,37 +20,11 @@
  * as published by the Free Software Foundation; either version
  * 2 of the License, or (at your option) any later version.
  */
+#ifndef _ASM_POWERPC_PTRACE_H
+#define _ASM_POWERPC_PTRACE_H
 
-#include <linux/types.h>
-
-#ifndef __ASSEMBLY__
-
-struct pt_regs {
-	unsigned long gpr[32];
-	unsigned long nip;
-	unsigned long msr;
-	unsigned long orig_gpr3;	/* Used for restarting system calls */
-	unsigned long ctr;
-	unsigned long link;
-	unsigned long xer;
-	unsigned long ccr;
-#ifdef __powerpc64__
-	unsigned long softe;		/* Soft enabled/disabled */
-#else
-	unsigned long mq;		/* 601 only (not used at present) */
-					/* Used on APUS to hold IPL value. */
-#endif
-	unsigned long trap;		/* Reason for being here */
-	/* N.B. for critical exceptions on 4xx, the dar and dsisr
-	   fields are overloaded to hold srr0 and srr1. */
-	unsigned long dar;		/* Fault registers */
-	unsigned long dsisr;		/* on 4xx/Book-E used for ESR */
-	unsigned long result;		/* Result of a system call */
-};
-
-#endif /* __ASSEMBLY__ */
+#include <uapi/asm/ptrace.h>
 
-#ifdef __KERNEL__
 
 #ifdef __powerpc64__
 
@@ -220,219 +191,12 @@ static inline unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,
 
 #endif /* __ASSEMBLY__ */
 
-#endif /* __KERNEL__ */
-
-/*
- * Offsets used by 'ptrace' system call interface.
- * These can't be changed without breaking binary compatibility
- * with MkLinux, etc.
- */
-#define PT_R0	0
-#define PT_R1	1
-#define PT_R2	2
-#define PT_R3	3
-#define PT_R4	4
-#define PT_R5	5
-#define PT_R6	6
-#define PT_R7	7
-#define PT_R8	8
-#define PT_R9	9
-#define PT_R10	10
-#define PT_R11	11
-#define PT_R12	12
-#define PT_R13	13
-#define PT_R14	14
-#define PT_R15	15
-#define PT_R16	16
-#define PT_R17	17
-#define PT_R18	18
-#define PT_R19	19
-#define PT_R20	20
-#define PT_R21	21
-#define PT_R22	22
-#define PT_R23	23
-#define PT_R24	24
-#define PT_R25	25
-#define PT_R26	26
-#define PT_R27	27
-#define PT_R28	28
-#define PT_R29	29
-#define PT_R30	30
-#define PT_R31	31
-
-#define PT_NIP	32
-#define PT_MSR	33
-#define PT_ORIG_R3 34
-#define PT_CTR	35
-#define PT_LNK	36
-#define PT_XER	37
-#define PT_CCR	38
-#ifndef __powerpc64__
-#define PT_MQ	39
-#else
-#define PT_SOFTE 39
-#endif
-#define PT_TRAP	40
-#define PT_DAR	41
-#define PT_DSISR 42
-#define PT_RESULT 43
-#define PT_REGS_COUNT 44
-
-#define PT_FPR0	48	/* each FP reg occupies 2 slots in this space */
-
 #ifndef __powerpc64__
-
-#define PT_FPR31 (PT_FPR0 + 2*31)
-#define PT_FPSCR (PT_FPR0 + 2*32 + 1)
-
 #else /* __powerpc64__ */
-
-#define PT_FPSCR (PT_FPR0 + 32)	/* each FP reg occupies 1 slot in 64-bit space */
-
-#ifdef __KERNEL__
 #define PT_FPSCR32 (PT_FPR0 + 2*32 + 1)	/* each FP reg occupies 2 32-bit userspace slots */
-#endif
-
-#define PT_VR0 82	/* each Vector reg occupies 2 slots in 64-bit */
-#define PT_VSCR (PT_VR0 + 32*2 + 1)
-#define PT_VRSAVE (PT_VR0 + 33*2)
-
-#ifdef __KERNEL__
 #define PT_VR0_32 164	/* each Vector reg occupies 4 slots in 32-bit */
 #define PT_VSCR_32 (PT_VR0 + 32*4 + 3)
 #define PT_VRSAVE_32 (PT_VR0 + 33*4)
-#endif
-
-/*
- * Only store first 32 VSRs here. The second 32 VSRs in VR0-31
- */
-#define PT_VSR0 150	/* each VSR reg occupies 2 slots in 64-bit */
-#define PT_VSR31 (PT_VSR0 + 2*31)
-#ifdef __KERNEL__
 #define PT_VSR0_32 300 	/* each VSR reg occupies 4 slots in 32-bit */
-#endif
 #endif /* __powerpc64__ */
-
-/*
- * Get/set all the altivec registers vr0..vr31, vscr, vrsave, in one go.
- * The transfer totals 34 quadword.  Quadwords 0-31 contain the
- * corresponding vector registers.  Quadword 32 contains the vscr as the
- * last word (offset 12) within that quadword.  Quadword 33 contains the
- * vrsave as the first word (offset 0) within the quadword.
- *
- * This definition of the VMX state is compatible with the current PPC32
- * ptrace interface.  This allows signal handling and ptrace to use the same
- * structures.  This also simplifies the implementation of a bi-arch
- * (combined (32- and 64-bit) gdb.
- */
-#define PTRACE_GETVRREGS	18
-#define PTRACE_SETVRREGS	19
-
-/* Get/set all the upper 32-bits of the SPE registers, accumulator, and
- * spefscr, in one go */
-#define PTRACE_GETEVRREGS	20
-#define PTRACE_SETEVRREGS	21
-
-/* Get the first 32 128bit VSX registers */
-#define PTRACE_GETVSRREGS	27
-#define PTRACE_SETVSRREGS	28
-
-/*
- * Get or set a debug register. The first 16 are DABR registers and the
- * second 16 are IABR registers.
- */
-#define PTRACE_GET_DEBUGREG	25
-#define PTRACE_SET_DEBUGREG	26
-
-/* (new) PTRACE requests using the same numbers as x86 and the same
- * argument ordering. Additionally, they support more registers too
- */
-#define PTRACE_GETREGS            12
-#define PTRACE_SETREGS            13
-#define PTRACE_GETFPREGS          14
-#define PTRACE_SETFPREGS          15
-#define PTRACE_GETREGS64	  22
-#define PTRACE_SETREGS64	  23
-
-/* Calls to trace a 64bit program from a 32bit program */
-#define PPC_PTRACE_PEEKTEXT_3264 0x95
-#define PPC_PTRACE_PEEKDATA_3264 0x94
-#define PPC_PTRACE_POKETEXT_3264 0x93
-#define PPC_PTRACE_POKEDATA_3264 0x92
-#define PPC_PTRACE_PEEKUSR_3264  0x91
-#define PPC_PTRACE_POKEUSR_3264  0x90
-
-#define PTRACE_SINGLEBLOCK	0x100	/* resume execution until next branch */
-
-#define PPC_PTRACE_GETHWDBGINFO	0x89
-#define PPC_PTRACE_SETHWDEBUG	0x88
-#define PPC_PTRACE_DELHWDEBUG	0x87
-
-#ifndef __ASSEMBLY__
-
-struct ppc_debug_info {
-	__u32 version;			/* Only version 1 exists to date */
-	__u32 num_instruction_bps;
-	__u32 num_data_bps;
-	__u32 num_condition_regs;
-	__u32 data_bp_alignment;
-	__u32 sizeof_condition;		/* size of the DVC register */
-	__u64 features;
-};
-
-#endif /* __ASSEMBLY__ */
-
-/*
- * features will have bits indication whether there is support for:
- */
-#define PPC_DEBUG_FEATURE_INSN_BP_RANGE		0x0000000000000001
-#define PPC_DEBUG_FEATURE_INSN_BP_MASK		0x0000000000000002
-#define PPC_DEBUG_FEATURE_DATA_BP_RANGE		0x0000000000000004
-#define PPC_DEBUG_FEATURE_DATA_BP_MASK		0x0000000000000008
-
-#ifndef __ASSEMBLY__
-
-struct ppc_hw_breakpoint {
-	__u32 version;		/* currently, version must be 1 */
-	__u32 trigger_type;	/* only some combinations allowed */
-	__u32 addr_mode;	/* address match mode */
-	__u32 condition_mode;	/* break/watchpoint condition flags */
-	__u64 addr;		/* break/watchpoint address */
-	__u64 addr2;		/* range end or mask */
-	__u64 condition_value;	/* contents of the DVC register */
-};
-
-#endif /* __ASSEMBLY__ */
-
-/*
- * Trigger Type
- */
-#define PPC_BREAKPOINT_TRIGGER_EXECUTE	0x00000001
-#define PPC_BREAKPOINT_TRIGGER_READ	0x00000002
-#define PPC_BREAKPOINT_TRIGGER_WRITE	0x00000004
-#define PPC_BREAKPOINT_TRIGGER_RW	\
-	(PPC_BREAKPOINT_TRIGGER_READ | PPC_BREAKPOINT_TRIGGER_WRITE)
-
-/*
- * Address Mode
- */
-#define PPC_BREAKPOINT_MODE_EXACT		0x00000000
-#define PPC_BREAKPOINT_MODE_RANGE_INCLUSIVE	0x00000001
-#define PPC_BREAKPOINT_MODE_RANGE_EXCLUSIVE	0x00000002
-#define PPC_BREAKPOINT_MODE_MASK		0x00000003
-
-/*
- * Condition Mode
- */
-#define PPC_BREAKPOINT_CONDITION_MODE	0x00000003
-#define PPC_BREAKPOINT_CONDITION_NONE	0x00000000
-#define PPC_BREAKPOINT_CONDITION_AND	0x00000001
-#define PPC_BREAKPOINT_CONDITION_EXACT	PPC_BREAKPOINT_CONDITION_AND
-#define PPC_BREAKPOINT_CONDITION_OR	0x00000002
-#define PPC_BREAKPOINT_CONDITION_AND_OR	0x00000003
-#define PPC_BREAKPOINT_CONDITION_BE_ALL	0x00ff0000
-#define PPC_BREAKPOINT_CONDITION_BE_SHIFT	16
-#define PPC_BREAKPOINT_CONDITION_BE(n)	\
-	(1<<((n)+PPC_BREAKPOINT_CONDITION_BE_SHIFT))
-
 #endif /* _ASM_POWERPC_PTRACE_H */

commit be6abfa769fa07ce89ac73273360b335ae978805
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Aug 31 15:48:05 2012 -0400

    powerpc: switch to generic sys_execve()/kernel_execve()
    
    the only non-obvious part is that current_pt_regs() is really needed
    here - task_pt_regs() is NULL for kernel threads; it's OK for ptrace
    uses (the thing task_pt_regs() is intended for), but not for us.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 9c21ed42aba6..f76b88c367d1 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -125,6 +125,8 @@ extern unsigned long ptrace_get_reg(struct task_struct *task, int regno);
 extern int ptrace_put_reg(struct task_struct *task, int regno,
 			  unsigned long data);
 
+#define current_pt_regs() \
+	((struct pt_regs *)((unsigned long)current_thread_info() + THREAD_SIZE) - 1)
 /*
  * We use the least-significant bit of the trap field to indicate
  * whether we have saved the full set of registers, or only a

commit ec34a6814988f17506733c1e8b058ce46602591d
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Apr 17 19:36:21 2012 +0000

    powerpc: Remove old powerpc specific ptrace getregs/setregs calls
    
    PowerPC has non standard getregs calls that only dump the GPRs or
    FPRs and have their arguments reversed. commit e17666ba48f7 (ptrace
    updates & new, better requests) in 2.6.3 deprecated them and introduced
    more standard versions.
    
    It's been about 5 years and I know of no users of the old calls so
    lets remove them.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 84cc7840cd18..9c21ed42aba6 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -354,12 +354,6 @@ static inline unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,
 #define PTRACE_GETREGS64	  22
 #define PTRACE_SETREGS64	  23
 
-/* (old) PTRACE requests with inverted arguments */
-#define PPC_PTRACE_GETREGS	0x99	/* Get GPRs 0 - 31 */
-#define PPC_PTRACE_SETREGS	0x98	/* Set GPRs 0 - 31 */
-#define PPC_PTRACE_GETFPREGS	0x97	/* Get FPRs 0 - 31 */
-#define PPC_PTRACE_SETFPREGS	0x96	/* Set FPRs 0 - 31 */
-
 /* Calls to trace a 64bit program from a 32bit program */
 #define PPC_PTRACE_PEEKTEXT_3264 0x95
 #define PPC_PTRACE_PEEKDATA_3264 0x94

commit e62894273c7572cb1bec39096df605f42a66e964
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Wed Feb 8 04:53:13 2012 +0000

    powerpc: Implement GET_IP/SET_IP
    
    With this change, helpers such as instruction_pointer() et al, get defined
    in the generic header in terms of GET_IP
    
    Removed the unnecessary definition of profile_pc in !CONFIG_SMP case as
    suggested by Mike Frysinger.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 78a205162fd7..84cc7840cd18 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -83,8 +83,18 @@ struct pt_regs {
 
 #ifndef __ASSEMBLY__
 
-#define instruction_pointer(regs) ((regs)->nip)
-#define user_stack_pointer(regs) ((regs)->gpr[1])
+#define GET_IP(regs)		((regs)->nip)
+#define GET_USP(regs)		((regs)->gpr[1])
+#define GET_FP(regs)		(0)
+#define SET_FP(regs, val)
+
+#ifdef CONFIG_SMP
+extern unsigned long profile_pc(struct pt_regs *regs);
+#define profile_pc profile_pc
+#endif
+
+#include <asm-generic/ptrace.h>
+
 #define kernel_stack_pointer(regs) ((regs)->gpr[1])
 static inline int is_syscall_success(struct pt_regs *regs)
 {
@@ -99,12 +109,6 @@ static inline long regs_return_value(struct pt_regs *regs)
 		return -regs->gpr[3];
 }
 
-#ifdef CONFIG_SMP
-extern unsigned long profile_pc(struct pt_regs *regs);
-#else
-#define profile_pc(regs) instruction_pointer(regs)
-#endif
-
 #ifdef __powerpc64__
 #define user_mode(regs) ((((regs)->msr) >> MSR_PR_LG) & 0x1)
 #else

commit d7e7528bcd456f5c36ad4a202ccfb43c5aa98bc4
Author: Eric Paris <eparis@redhat.com>
Date:   Tue Jan 3 14:23:06 2012 -0500

    Audit: push audit success and retcode into arch ptrace.h
    
    The audit system previously expected arches calling to audit_syscall_exit to
    supply as arguments if the syscall was a success and what the return code was.
    Audit also provides a helper AUDITSC_RESULT which was supposed to simplify things
    by converting from negative retcodes to an audit internal magic value stating
    success or failure.  This helper was wrong and could indicate that a valid
    pointer returned to userspace was a failed syscall.  The fix is to fix the
    layering foolishness.  We now pass audit_syscall_exit a struct pt_reg and it
    in turns calls back into arch code to collect the return value and to
    determine if the syscall was a success or failure.  We also define a generic
    is_syscall_success() macro which determines success/failure based on if the
    value is < -MAX_ERRNO.  This works for arches like x86 which do not use a
    separate mechanism to indicate syscall failure.
    
    We make both the is_syscall_success() and regs_return_value() static inlines
    instead of macros.  The reason is because the audit function must take a void*
    for the regs.  (uml calls theirs struct uml_pt_regs instead of just struct
    pt_regs so audit_syscall_exit can't take a struct pt_regs).  Since the audit
    function takes a void* we need to use static inlines to cast it back to the
    arch correct structure to dereference it.
    
    The other major change is that on some arches, like ia64, MIPS and ppc, we
    change regs_return_value() to give us the negative value on syscall failure.
    THE only other user of this macro, kretprobe_example.c, won't notice and it
    makes the value signed consistently for the audit functions across all archs.
    
    In arch/sh/kernel/ptrace_64.c I see that we were using regs[9] in the old
    audit code as the return value.  But the ptrace_64.h code defined the macro
    regs_return_value() as regs[3].  I have no idea which one is correct, but this
    patch now uses the regs_return_value() function, so it now uses regs[3].
    
    For powerpc we previously used regs->result but now use the
    regs_return_value() function which uses regs->gprs[3].  regs->gprs[3] is
    always positive so the regs_return_value(), much like ia64 makes it negative
    before calling the audit code when appropriate.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com> [for x86 portion]
    Acked-by: Tony Luck <tony.luck@intel.com> [for ia64]
    Acked-by: Richard Weinberger <richard@nod.at> [for uml]
    Acked-by: David S. Miller <davem@davemloft.net> [for sparc]
    Acked-by: Ralf Baechle <ralf@linux-mips.org> [for mips]
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org> [for ppc]

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 48223f9b8728..78a205162fd7 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -86,7 +86,18 @@ struct pt_regs {
 #define instruction_pointer(regs) ((regs)->nip)
 #define user_stack_pointer(regs) ((regs)->gpr[1])
 #define kernel_stack_pointer(regs) ((regs)->gpr[1])
-#define regs_return_value(regs) ((regs)->gpr[3])
+static inline int is_syscall_success(struct pt_regs *regs)
+{
+	return !(regs->ccr & 0x10000000);
+}
+
+static inline long regs_return_value(struct pt_regs *regs)
+{
+	if (is_syscall_success(regs))
+		return regs->gpr[3];
+	else
+		return -regs->gpr[3];
+}
 
 #ifdef CONFIG_SMP
 extern unsigned long profile_pc(struct pt_regs *regs);

commit a71f5d5d279375205009a4be56a3cf6682921292
Author: Mike Wolf <mjw@linux.vnet.ibm.com>
Date:   Mon Mar 21 11:14:53 2011 +1100

    powerpc/ptrace: Remove BUG_ON when full register set not available
    
    In some cases during a threaded core dump not all the threads will have
    a full register set. This happens when the signal causing the core dump
    races with a thread exiting.  The race happens when the exiting thread
    has entered the kernel for the last time before the signal arrives, but
    doesn't get far enough through the exit code to avoid being included
    in the core dump.
    
    So we get a thread included in the core dump which is never going to go
    out to userspace again and only has a partial register set recorded
    
    Normally we would catch each thread as it is about to go into userspace
    and capture the full register set then.
    
    However, this exiting thread is never going to go out to userspace
    again, so we have no way to capture its full register set.  It doesn't
    really matter, though, as this is a thread which is effectively
    already dead.
    
    So instead of hitting a BUG() in this case (a really bad choice of
    action in the first place), we use a poison value for the register
    values.
    
    [BenH]: Some cosmetic/stylistic changes and fix build on ppc32
    
    Signed-off-by: Mike Wolf <mjw@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 0175a676b34b..48223f9b8728 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -125,8 +125,10 @@ extern int ptrace_put_reg(struct task_struct *task, int regno,
 #endif /* ! __powerpc64__ */
 #define TRAP(regs)		((regs)->trap & ~0xF)
 #ifdef __powerpc64__
+#define NV_REG_POISON		0xdeadbeefdeadbeefUL
 #define CHECK_FULL_REGS(regs)	BUG_ON(regs->trap & 1)
 #else
+#define NV_REG_POISON		0xdeadbeef
 #define CHECK_FULL_REGS(regs)						      \
 do {									      \
 	if ((regs)->trap & 1)						      \

commit bf23690b89b731468478c21eb07bbb645cc66ead
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Sun May 9 08:52:31 2010 +0200

    powerpc: Fix userspace build of ptrace.h
    
    Build of ptrace.h failed for assembly because it
    pulls in stdint.h.
    Use exportable types (__u32, __u64) to avoid the dependency
    on stdint.h.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Cc: Andrey Volkov <avolkov@varma-el.com>
    Cc: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 5d8be0416227..0175a676b34b 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -24,11 +24,7 @@
  * 2 of the License, or (at your option) any later version.
  */
 
-#ifdef __KERNEL__
 #include <linux/types.h>
-#else
-#include <stdint.h>
-#endif
 
 #ifndef __ASSEMBLY__
 
@@ -364,13 +360,13 @@ static inline unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,
 #ifndef __ASSEMBLY__
 
 struct ppc_debug_info {
-	uint32_t version;		/* Only version 1 exists to date */
-	uint32_t num_instruction_bps;
-	uint32_t num_data_bps;
-	uint32_t num_condition_regs;
-	uint32_t data_bp_alignment;
-	uint32_t sizeof_condition;	/* size of the DVC register */
-	uint64_t features;
+	__u32 version;			/* Only version 1 exists to date */
+	__u32 num_instruction_bps;
+	__u32 num_data_bps;
+	__u32 num_condition_regs;
+	__u32 data_bp_alignment;
+	__u32 sizeof_condition;		/* size of the DVC register */
+	__u64 features;
 };
 
 #endif /* __ASSEMBLY__ */
@@ -386,13 +382,13 @@ struct ppc_debug_info {
 #ifndef __ASSEMBLY__
 
 struct ppc_hw_breakpoint {
-	uint32_t version;		/* currently, version must be 1 */
-	uint32_t trigger_type;		/* only some combinations allowed */
-	uint32_t addr_mode;		/* address match mode */
-	uint32_t condition_mode;	/* break/watchpoint condition flags */
-	uint64_t addr;			/* break/watchpoint address */
-	uint64_t addr2;			/* range end or mask */
-	uint64_t condition_value;	/* contents of the DVC register */
+	__u32 version;		/* currently, version must be 1 */
+	__u32 trigger_type;	/* only some combinations allowed */
+	__u32 addr_mode;	/* address match mode */
+	__u32 condition_mode;	/* break/watchpoint condition flags */
+	__u64 addr;		/* break/watchpoint address */
+	__u64 addr2;		/* range end or mask */
+	__u64 condition_value;	/* contents of the DVC register */
 };
 
 #endif /* __ASSEMBLY__ */

commit 359e4284a3f37aba7fd06d993863de2509d86f54
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Wed Apr 7 18:10:20 2010 +1000

    powerpc: Add kprobe-based event tracer
    
    This patch ports the kprobe-based event tracer to powerpc. This patch
    is based on x86 port. This brings powerpc on par with x86.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 9e2d84c06b74..5d8be0416227 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -89,6 +89,7 @@ struct pt_regs {
 
 #define instruction_pointer(regs) ((regs)->nip)
 #define user_stack_pointer(regs) ((regs)->gpr[1])
+#define kernel_stack_pointer(regs) ((regs)->gpr[1])
 #define regs_return_value(regs) ((regs)->gpr[3])
 
 #ifdef CONFIG_SMP
@@ -141,6 +142,69 @@ do {									      \
 #define arch_has_block_step()	(!cpu_has_feature(CPU_FTR_601))
 #define ARCH_HAS_USER_SINGLE_STEP_INFO
 
+/*
+ * kprobe-based event tracer support
+ */
+
+#include <linux/stddef.h>
+#include <linux/thread_info.h>
+extern int regs_query_register_offset(const char *name);
+extern const char *regs_query_register_name(unsigned int offset);
+#define MAX_REG_OFFSET (offsetof(struct pt_regs, dsisr))
+
+/**
+ * regs_get_register() - get register value from its offset
+ * @regs:	   pt_regs from which register value is gotten
+ * @offset:    offset number of the register.
+ *
+ * regs_get_register returns the value of a register whose offset from @regs.
+ * The @offset is the offset of the register in struct pt_regs.
+ * If @offset is bigger than MAX_REG_OFFSET, this returns 0.
+ */
+static inline unsigned long regs_get_register(struct pt_regs *regs,
+						unsigned int offset)
+{
+	if (unlikely(offset > MAX_REG_OFFSET))
+		return 0;
+	return *(unsigned long *)((unsigned long)regs + offset);
+}
+
+/**
+ * regs_within_kernel_stack() - check the address in the stack
+ * @regs:      pt_regs which contains kernel stack pointer.
+ * @addr:      address which is checked.
+ *
+ * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).
+ * If @addr is within the kernel stack, it returns true. If not, returns false.
+ */
+
+static inline bool regs_within_kernel_stack(struct pt_regs *regs,
+						unsigned long addr)
+{
+	return ((addr & ~(THREAD_SIZE - 1))  ==
+		(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1)));
+}
+
+/**
+ * regs_get_kernel_stack_nth() - get Nth entry of the stack
+ * @regs:	pt_regs which contains kernel stack pointer.
+ * @n:		stack entry number.
+ *
+ * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which
+ * is specified by @regs. If the @n th entry is NOT in the kernel stack,
+ * this returns 0.
+ */
+static inline unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,
+						      unsigned int n)
+{
+	unsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);
+	addr += n;
+	if (regs_within_kernel_stack(regs, (unsigned long)addr))
+		return *addr;
+	else
+		return 0;
+}
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* __KERNEL__ */

commit dacbe41f776db0a5a9aee1e41594f405c95778a5
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Mar 10 15:22:46 2010 -0800

    ptrace: move user_enable_single_step & co prototypes to linux/ptrace.h
    
    While in theory user_enable_single_step/user_disable_single_step/
    user_enable_blockstep could also be provided as an inline or macro there's
    no good reason to do so, and having the prototype in one places keeps code
    size and confusion down.
    
    Roland said:
    
      The original thought there was that user_enable_single_step() et al
      might well be only an instruction or three on a sane machine (as if we
      have any of those!), and since there is only one call site inlining
      would be beneficial.  But I agree that there is no strong reason to care
      about inlining it.
    
      As to the arch changes, there is only one thought I'd add to the
      record.  It was always my thinking that for an arch where
      PTRACE_SINGLESTEP does text-modifying breakpoint insertion,
      user_enable_single_step() should not be provided.  That is,
      arch_has_single_step()=>true means that there is an arch facility with
      "pure" semantics that does not have any unexpected side effects.
      Inserting a breakpoint might do very unexpected strange things in
      multi-threaded situations.  Aside from that, it is a peculiar side
      effect that user_{enable,disable}_single_step() should cause COW
      de-sharing of text pages and so forth.  For PTRACE_SINGLESTEP, all these
      peculiarities are the status quo ante for that arch, so having
      arch_ptrace() itself do those is one thing.  But for building other
      things in the future, it is nicer to have a uniform "pure" semantics
      that arch-independent code can expect.
    
      OTOH, all such arch issues are really up to the arch maintainer.  As
      of today, there is nothing but ptrace using user_enable_single_step() et
      al so it's a distinction without a practical difference.  If/when there
      are other facilities that use user_enable_single_step() and might care,
      the affected arch's can revisit the question when someone cares about
      the quality of the arch support for said new facility.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index b45108126562..9e2d84c06b74 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -137,15 +137,8 @@ do {									      \
 } while (0)
 #endif /* __powerpc64__ */
 
-/*
- * These are defined as per linux/ptrace.h, which see.
- */
 #define arch_has_single_step()	(1)
 #define arch_has_block_step()	(!cpu_has_feature(CPU_FTR_601))
-extern void user_enable_single_step(struct task_struct *);
-extern void user_enable_block_step(struct task_struct *);
-extern void user_disable_single_step(struct task_struct *);
-
 #define ARCH_HAS_USER_SINGLE_STEP_INFO
 
 #endif /* __ASSEMBLY__ */

commit 3162d92dfb79a0b5fc03380b8819fa5f870ebf1e
Author: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
Date:   Mon Feb 8 11:51:05 2010 +0000

    powerpc: Extended ptrace interface
    
    powerpc: Extended ptrace interface
    
    From: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    
    Based on patches originally written by Torez Smith.
    
    Add a new extended ptrace interface so that user-space has a single
    interface for powerpc, without having to know the specific layout
    of the debug registers.
    
    Implement:
    PPC_PTRACE_GETHWDEBUGINFO
    PPC_PTRACE_SETHWDEBUG
    PPC_PTRACE_DELHWDEBUG
    
    Signed-off-by: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Acked-by: David Gibson <dwg@au1.ibm.com>
    Cc: Torez Smith  <lnxtorez@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Josh Boyer <jwboyer@linux.vnet.ibm.com>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Cc: Sergio Durigan Junior <sergiodj@br.ibm.com>
    Cc: Thiago Jung Bauermann <bauerman@br.ibm.com>
    Cc: linuxppc-dev list <Linuxppc-dev@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index cbd759e3cd78..b45108126562 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -24,6 +24,12 @@
  * 2 of the License, or (at your option) any later version.
  */
 
+#ifdef __KERNEL__
+#include <linux/types.h>
+#else
+#include <stdint.h>
+#endif
+
 #ifndef __ASSEMBLY__
 
 struct pt_regs {
@@ -294,4 +300,75 @@ extern void user_disable_single_step(struct task_struct *);
 
 #define PTRACE_SINGLEBLOCK	0x100	/* resume execution until next branch */
 
+#define PPC_PTRACE_GETHWDBGINFO	0x89
+#define PPC_PTRACE_SETHWDEBUG	0x88
+#define PPC_PTRACE_DELHWDEBUG	0x87
+
+#ifndef __ASSEMBLY__
+
+struct ppc_debug_info {
+	uint32_t version;		/* Only version 1 exists to date */
+	uint32_t num_instruction_bps;
+	uint32_t num_data_bps;
+	uint32_t num_condition_regs;
+	uint32_t data_bp_alignment;
+	uint32_t sizeof_condition;	/* size of the DVC register */
+	uint64_t features;
+};
+
+#endif /* __ASSEMBLY__ */
+
+/*
+ * features will have bits indication whether there is support for:
+ */
+#define PPC_DEBUG_FEATURE_INSN_BP_RANGE		0x0000000000000001
+#define PPC_DEBUG_FEATURE_INSN_BP_MASK		0x0000000000000002
+#define PPC_DEBUG_FEATURE_DATA_BP_RANGE		0x0000000000000004
+#define PPC_DEBUG_FEATURE_DATA_BP_MASK		0x0000000000000008
+
+#ifndef __ASSEMBLY__
+
+struct ppc_hw_breakpoint {
+	uint32_t version;		/* currently, version must be 1 */
+	uint32_t trigger_type;		/* only some combinations allowed */
+	uint32_t addr_mode;		/* address match mode */
+	uint32_t condition_mode;	/* break/watchpoint condition flags */
+	uint64_t addr;			/* break/watchpoint address */
+	uint64_t addr2;			/* range end or mask */
+	uint64_t condition_value;	/* contents of the DVC register */
+};
+
+#endif /* __ASSEMBLY__ */
+
+/*
+ * Trigger Type
+ */
+#define PPC_BREAKPOINT_TRIGGER_EXECUTE	0x00000001
+#define PPC_BREAKPOINT_TRIGGER_READ	0x00000002
+#define PPC_BREAKPOINT_TRIGGER_WRITE	0x00000004
+#define PPC_BREAKPOINT_TRIGGER_RW	\
+	(PPC_BREAKPOINT_TRIGGER_READ | PPC_BREAKPOINT_TRIGGER_WRITE)
+
+/*
+ * Address Mode
+ */
+#define PPC_BREAKPOINT_MODE_EXACT		0x00000000
+#define PPC_BREAKPOINT_MODE_RANGE_INCLUSIVE	0x00000001
+#define PPC_BREAKPOINT_MODE_RANGE_EXCLUSIVE	0x00000002
+#define PPC_BREAKPOINT_MODE_MASK		0x00000003
+
+/*
+ * Condition Mode
+ */
+#define PPC_BREAKPOINT_CONDITION_MODE	0x00000003
+#define PPC_BREAKPOINT_CONDITION_NONE	0x00000000
+#define PPC_BREAKPOINT_CONDITION_AND	0x00000001
+#define PPC_BREAKPOINT_CONDITION_EXACT	PPC_BREAKPOINT_CONDITION_AND
+#define PPC_BREAKPOINT_CONDITION_OR	0x00000002
+#define PPC_BREAKPOINT_CONDITION_AND_OR	0x00000003
+#define PPC_BREAKPOINT_CONDITION_BE_ALL	0x00ff0000
+#define PPC_BREAKPOINT_CONDITION_BE_SHIFT	16
+#define PPC_BREAKPOINT_CONDITION_BE(n)	\
+	(1<<((n)+PPC_BREAKPOINT_CONDITION_BE_SHIFT))
+
 #endif /* _ASM_POWERPC_PTRACE_H */

commit 25baa35befeebe6a4a8d8d12a4fc5b95918bda54
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Dec 15 16:47:18 2009 -0800

    ptrace: powerpc: implement user_single_step_siginfo()
    
    Suggested by Roland.
    
    Implement user_single_step_siginfo() for powerpc.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: <linux-arch@vger.kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 8c341490cfc5..cbd759e3cd78 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -140,6 +140,8 @@ extern void user_enable_single_step(struct task_struct *);
 extern void user_enable_block_step(struct task_struct *);
 extern void user_disable_single_step(struct task_struct *);
 
+#define ARCH_HAS_USER_SINGLE_STEP_INFO
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* __KERNEL__ */

commit ec097c84dff17511f2693e6ef6c3064dfbf0a3af
Author: Roland McGrath <roland@redhat.com>
Date:   Thu May 28 21:26:38 2009 +0000

    powerpc: Add PTRACE_SINGLEBLOCK support
    
    Reworked by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    
    This adds block-step support on powerpc, including a PTRACE_SINGLEBLOCK
    request for ptrace.
    
    The BookE implementation is tweaked to fire a single step after a
    block step in order to mimmic the server behaviour.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index c9c678fb2538..8c341490cfc5 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -135,7 +135,9 @@ do {									      \
  * These are defined as per linux/ptrace.h, which see.
  */
 #define arch_has_single_step()	(1)
+#define arch_has_block_step()	(!cpu_has_feature(CPU_FTR_601))
 extern void user_enable_single_step(struct task_struct *);
+extern void user_enable_block_step(struct task_struct *);
 extern void user_disable_single_step(struct task_struct *);
 
 #endif /* __ASSEMBLY__ */
@@ -288,4 +290,6 @@ extern void user_disable_single_step(struct task_struct *);
 #define PPC_PTRACE_PEEKUSR_3264  0x91
 #define PPC_PTRACE_POKEUSR_3264  0x90
 
+#define PTRACE_SINGLEBLOCK	0x100	/* resume execution until next branch */
+
 #endif /* _ASM_POWERPC_PTRACE_H */

commit 96b8936a9ed08746e47081458a5eb9e43a751e24
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Nov 25 08:10:03 2008 +0100

    remove __ARCH_WANT_COMPAT_SYS_PTRACE
    
    All architectures now use the generic compat_sys_ptrace, as should every
    new architecture that needs 32bit compat (if we'll ever get another).
    
    Remove the now superflous __ARCH_WANT_COMPAT_SYS_PTRACE define, and also
    kill a comment about __ARCH_SYS_PTRACE that was added after
    __ARCH_SYS_PTRACE was already gone.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 280a90cc9894..c9c678fb2538 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -55,8 +55,6 @@ struct pt_regs {
 
 #ifdef __powerpc64__
 
-#define __ARCH_WANT_COMPAT_SYS_PTRACE
-
 #define STACK_FRAME_OVERHEAD	112	/* size of minimum stack frame */
 #define STACK_FRAME_LR_SAVE	2	/* Location of LR in stack frame */
 #define STACK_FRAME_REGS_MARKER	ASM_CONST(0x7265677368657265)

commit 653c03168348ac7aebb969931f87ba281749d7dd
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Mon Oct 20 16:00:08 2008 -0700

    misc: replace remaining __FUNCTION__ with __func__
    
    __FUNCTION__ is gcc-specific, use __func__
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Acked-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
index 734e0754fb9b..280a90cc9894 100644
--- a/arch/powerpc/include/asm/ptrace.h
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -129,7 +129,7 @@ extern int ptrace_put_reg(struct task_struct *task, int regno,
 #define CHECK_FULL_REGS(regs)						      \
 do {									      \
 	if ((regs)->trap & 1)						      \
-		printk(KERN_CRIT "%s: partial register set\n", __FUNCTION__); \
+		printk(KERN_CRIT "%s: partial register set\n", __func__); \
 } while (0)
 #endif /* __powerpc64__ */
 

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/ptrace.h b/arch/powerpc/include/asm/ptrace.h
new file mode 100644
index 000000000000..734e0754fb9b
--- /dev/null
+++ b/arch/powerpc/include/asm/ptrace.h
@@ -0,0 +1,293 @@
+#ifndef _ASM_POWERPC_PTRACE_H
+#define _ASM_POWERPC_PTRACE_H
+
+/*
+ * Copyright (C) 2001 PPC64 Team, IBM Corp
+ *
+ * This struct defines the way the registers are stored on the
+ * kernel stack during a system call or other kernel entry.
+ *
+ * this should only contain volatile regs
+ * since we can keep non-volatile in the thread_struct
+ * should set this up when only volatiles are saved
+ * by intr code.
+ *
+ * Since this is going on the stack, *CARE MUST BE TAKEN* to insure
+ * that the overall structure is a multiple of 16 bytes in length.
+ *
+ * Note that the offsets of the fields in this struct correspond with
+ * the PT_* values below.  This simplifies arch/powerpc/kernel/ptrace.c.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#ifndef __ASSEMBLY__
+
+struct pt_regs {
+	unsigned long gpr[32];
+	unsigned long nip;
+	unsigned long msr;
+	unsigned long orig_gpr3;	/* Used for restarting system calls */
+	unsigned long ctr;
+	unsigned long link;
+	unsigned long xer;
+	unsigned long ccr;
+#ifdef __powerpc64__
+	unsigned long softe;		/* Soft enabled/disabled */
+#else
+	unsigned long mq;		/* 601 only (not used at present) */
+					/* Used on APUS to hold IPL value. */
+#endif
+	unsigned long trap;		/* Reason for being here */
+	/* N.B. for critical exceptions on 4xx, the dar and dsisr
+	   fields are overloaded to hold srr0 and srr1. */
+	unsigned long dar;		/* Fault registers */
+	unsigned long dsisr;		/* on 4xx/Book-E used for ESR */
+	unsigned long result;		/* Result of a system call */
+};
+
+#endif /* __ASSEMBLY__ */
+
+#ifdef __KERNEL__
+
+#ifdef __powerpc64__
+
+#define __ARCH_WANT_COMPAT_SYS_PTRACE
+
+#define STACK_FRAME_OVERHEAD	112	/* size of minimum stack frame */
+#define STACK_FRAME_LR_SAVE	2	/* Location of LR in stack frame */
+#define STACK_FRAME_REGS_MARKER	ASM_CONST(0x7265677368657265)
+#define STACK_INT_FRAME_SIZE	(sizeof(struct pt_regs) + \
+					STACK_FRAME_OVERHEAD + 288)
+#define STACK_FRAME_MARKER	12
+
+/* Size of dummy stack frame allocated when calling signal handler. */
+#define __SIGNAL_FRAMESIZE	128
+#define __SIGNAL_FRAMESIZE32	64
+
+#else /* __powerpc64__ */
+
+#define STACK_FRAME_OVERHEAD	16	/* size of minimum stack frame */
+#define STACK_FRAME_LR_SAVE	1	/* Location of LR in stack frame */
+#define STACK_FRAME_REGS_MARKER	ASM_CONST(0x72656773)
+#define STACK_INT_FRAME_SIZE	(sizeof(struct pt_regs) + STACK_FRAME_OVERHEAD)
+#define STACK_FRAME_MARKER	2
+
+/* Size of stack frame allocated when calling signal handler. */
+#define __SIGNAL_FRAMESIZE	64
+
+#endif /* __powerpc64__ */
+
+#ifndef __ASSEMBLY__
+
+#define instruction_pointer(regs) ((regs)->nip)
+#define user_stack_pointer(regs) ((regs)->gpr[1])
+#define regs_return_value(regs) ((regs)->gpr[3])
+
+#ifdef CONFIG_SMP
+extern unsigned long profile_pc(struct pt_regs *regs);
+#else
+#define profile_pc(regs) instruction_pointer(regs)
+#endif
+
+#ifdef __powerpc64__
+#define user_mode(regs) ((((regs)->msr) >> MSR_PR_LG) & 0x1)
+#else
+#define user_mode(regs) (((regs)->msr & MSR_PR) != 0)
+#endif
+
+#define force_successful_syscall_return()   \
+	do { \
+		set_thread_flag(TIF_NOERROR); \
+	} while(0)
+
+struct task_struct;
+extern unsigned long ptrace_get_reg(struct task_struct *task, int regno);
+extern int ptrace_put_reg(struct task_struct *task, int regno,
+			  unsigned long data);
+
+/*
+ * We use the least-significant bit of the trap field to indicate
+ * whether we have saved the full set of registers, or only a
+ * partial set.  A 1 there means the partial set.
+ * On 4xx we use the next bit to indicate whether the exception
+ * is a critical exception (1 means it is).
+ */
+#define FULL_REGS(regs)		(((regs)->trap & 1) == 0)
+#ifndef __powerpc64__
+#define IS_CRITICAL_EXC(regs)	(((regs)->trap & 2) != 0)
+#define IS_MCHECK_EXC(regs)	(((regs)->trap & 4) != 0)
+#define IS_DEBUG_EXC(regs)	(((regs)->trap & 8) != 0)
+#endif /* ! __powerpc64__ */
+#define TRAP(regs)		((regs)->trap & ~0xF)
+#ifdef __powerpc64__
+#define CHECK_FULL_REGS(regs)	BUG_ON(regs->trap & 1)
+#else
+#define CHECK_FULL_REGS(regs)						      \
+do {									      \
+	if ((regs)->trap & 1)						      \
+		printk(KERN_CRIT "%s: partial register set\n", __FUNCTION__); \
+} while (0)
+#endif /* __powerpc64__ */
+
+/*
+ * These are defined as per linux/ptrace.h, which see.
+ */
+#define arch_has_single_step()	(1)
+extern void user_enable_single_step(struct task_struct *);
+extern void user_disable_single_step(struct task_struct *);
+
+#endif /* __ASSEMBLY__ */
+
+#endif /* __KERNEL__ */
+
+/*
+ * Offsets used by 'ptrace' system call interface.
+ * These can't be changed without breaking binary compatibility
+ * with MkLinux, etc.
+ */
+#define PT_R0	0
+#define PT_R1	1
+#define PT_R2	2
+#define PT_R3	3
+#define PT_R4	4
+#define PT_R5	5
+#define PT_R6	6
+#define PT_R7	7
+#define PT_R8	8
+#define PT_R9	9
+#define PT_R10	10
+#define PT_R11	11
+#define PT_R12	12
+#define PT_R13	13
+#define PT_R14	14
+#define PT_R15	15
+#define PT_R16	16
+#define PT_R17	17
+#define PT_R18	18
+#define PT_R19	19
+#define PT_R20	20
+#define PT_R21	21
+#define PT_R22	22
+#define PT_R23	23
+#define PT_R24	24
+#define PT_R25	25
+#define PT_R26	26
+#define PT_R27	27
+#define PT_R28	28
+#define PT_R29	29
+#define PT_R30	30
+#define PT_R31	31
+
+#define PT_NIP	32
+#define PT_MSR	33
+#define PT_ORIG_R3 34
+#define PT_CTR	35
+#define PT_LNK	36
+#define PT_XER	37
+#define PT_CCR	38
+#ifndef __powerpc64__
+#define PT_MQ	39
+#else
+#define PT_SOFTE 39
+#endif
+#define PT_TRAP	40
+#define PT_DAR	41
+#define PT_DSISR 42
+#define PT_RESULT 43
+#define PT_REGS_COUNT 44
+
+#define PT_FPR0	48	/* each FP reg occupies 2 slots in this space */
+
+#ifndef __powerpc64__
+
+#define PT_FPR31 (PT_FPR0 + 2*31)
+#define PT_FPSCR (PT_FPR0 + 2*32 + 1)
+
+#else /* __powerpc64__ */
+
+#define PT_FPSCR (PT_FPR0 + 32)	/* each FP reg occupies 1 slot in 64-bit space */
+
+#ifdef __KERNEL__
+#define PT_FPSCR32 (PT_FPR0 + 2*32 + 1)	/* each FP reg occupies 2 32-bit userspace slots */
+#endif
+
+#define PT_VR0 82	/* each Vector reg occupies 2 slots in 64-bit */
+#define PT_VSCR (PT_VR0 + 32*2 + 1)
+#define PT_VRSAVE (PT_VR0 + 33*2)
+
+#ifdef __KERNEL__
+#define PT_VR0_32 164	/* each Vector reg occupies 4 slots in 32-bit */
+#define PT_VSCR_32 (PT_VR0 + 32*4 + 3)
+#define PT_VRSAVE_32 (PT_VR0 + 33*4)
+#endif
+
+/*
+ * Only store first 32 VSRs here. The second 32 VSRs in VR0-31
+ */
+#define PT_VSR0 150	/* each VSR reg occupies 2 slots in 64-bit */
+#define PT_VSR31 (PT_VSR0 + 2*31)
+#ifdef __KERNEL__
+#define PT_VSR0_32 300 	/* each VSR reg occupies 4 slots in 32-bit */
+#endif
+#endif /* __powerpc64__ */
+
+/*
+ * Get/set all the altivec registers vr0..vr31, vscr, vrsave, in one go.
+ * The transfer totals 34 quadword.  Quadwords 0-31 contain the
+ * corresponding vector registers.  Quadword 32 contains the vscr as the
+ * last word (offset 12) within that quadword.  Quadword 33 contains the
+ * vrsave as the first word (offset 0) within the quadword.
+ *
+ * This definition of the VMX state is compatible with the current PPC32
+ * ptrace interface.  This allows signal handling and ptrace to use the same
+ * structures.  This also simplifies the implementation of a bi-arch
+ * (combined (32- and 64-bit) gdb.
+ */
+#define PTRACE_GETVRREGS	18
+#define PTRACE_SETVRREGS	19
+
+/* Get/set all the upper 32-bits of the SPE registers, accumulator, and
+ * spefscr, in one go */
+#define PTRACE_GETEVRREGS	20
+#define PTRACE_SETEVRREGS	21
+
+/* Get the first 32 128bit VSX registers */
+#define PTRACE_GETVSRREGS	27
+#define PTRACE_SETVSRREGS	28
+
+/*
+ * Get or set a debug register. The first 16 are DABR registers and the
+ * second 16 are IABR registers.
+ */
+#define PTRACE_GET_DEBUGREG	25
+#define PTRACE_SET_DEBUGREG	26
+
+/* (new) PTRACE requests using the same numbers as x86 and the same
+ * argument ordering. Additionally, they support more registers too
+ */
+#define PTRACE_GETREGS            12
+#define PTRACE_SETREGS            13
+#define PTRACE_GETFPREGS          14
+#define PTRACE_SETFPREGS          15
+#define PTRACE_GETREGS64	  22
+#define PTRACE_SETREGS64	  23
+
+/* (old) PTRACE requests with inverted arguments */
+#define PPC_PTRACE_GETREGS	0x99	/* Get GPRs 0 - 31 */
+#define PPC_PTRACE_SETREGS	0x98	/* Set GPRs 0 - 31 */
+#define PPC_PTRACE_GETFPREGS	0x97	/* Get FPRs 0 - 31 */
+#define PPC_PTRACE_SETFPREGS	0x96	/* Set FPRs 0 - 31 */
+
+/* Calls to trace a 64bit program from a 32bit program */
+#define PPC_PTRACE_PEEKTEXT_3264 0x95
+#define PPC_PTRACE_PEEKDATA_3264 0x94
+#define PPC_PTRACE_POKETEXT_3264 0x93
+#define PPC_PTRACE_POKEDATA_3264 0x92
+#define PPC_PTRACE_PEEKUSR_3264  0x91
+#define PPC_PTRACE_POKEUSR_3264  0x90
+
+#endif /* _ASM_POWERPC_PTRACE_H */
