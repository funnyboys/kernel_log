commit 7ae77150d94d3b535c7b85e6b3647113095e79bf
Merge: 084623e468d5 1395375c5927
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 5 12:39:30 2020 -0700

    Merge tag 'powerpc-5.8-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
    
     - Support for userspace to send requests directly to the on-chip GZIP
       accelerator on Power9.
    
     - Rework of our lockless page table walking (__find_linux_pte()) to
       make it safe against parallel page table manipulations without
       relying on an IPI for serialisation.
    
     - A series of fixes & enhancements to make our machine check handling
       more robust.
    
     - Lots of plumbing to add support for "prefixed" (64-bit) instructions
       on Power10.
    
     - Support for using huge pages for the linear mapping on 8xx (32-bit).
    
     - Remove obsolete Xilinx PPC405/PPC440 support, and an associated sound
       driver.
    
     - Removal of some obsolete 40x platforms and associated cruft.
    
     - Initial support for booting on Power10.
    
     - Lots of other small features, cleanups & fixes.
    
    Thanks to: Alexey Kardashevskiy, Alistair Popple, Andrew Donnellan,
    Andrey Abramov, Aneesh Kumar K.V, Balamuruhan S, Bharata B Rao, Bulent
    Abali, Cédric Le Goater, Chen Zhou, Christian Zigotzky, Christophe
    JAILLET, Christophe Leroy, Dmitry Torokhov, Emmanuel Nicolet, Erhard F.,
    Gautham R. Shenoy, Geoff Levand, George Spelvin, Greg Kurz, Gustavo A.
    R. Silva, Gustavo Walbon, Haren Myneni, Hari Bathini, Joel Stanley,
    Jordan Niethe, Kajol Jain, Kees Cook, Leonardo Bras, Madhavan
    Srinivasan., Mahesh Salgaonkar, Markus Elfring, Michael Neuling, Michal
    Simek, Nathan Chancellor, Nathan Lynch, Naveen N. Rao, Nicholas Piggin,
    Oliver O'Halloran, Paul Mackerras, Pingfan Liu, Qian Cai, Ram Pai,
    Raphael Moreira Zinsly, Ravi Bangoria, Sam Bobroff, Sandipan Das, Segher
    Boessenkool, Stephen Rothwell, Sukadev Bhattiprolu, Tyrel Datwyler,
    Wolfram Sang, Xiongfeng Wang.
    
    * tag 'powerpc-5.8-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (299 commits)
      powerpc/pseries: Make vio and ibmebus initcalls pseries specific
      cxl: Remove dead Kconfig options
      powerpc: Add POWER10 architected mode
      powerpc/dt_cpu_ftrs: Add MMA feature
      powerpc/dt_cpu_ftrs: Enable Prefixed Instructions
      powerpc/dt_cpu_ftrs: Advertise support for ISA v3.1 if selected
      powerpc: Add support for ISA v3.1
      powerpc: Add new HWCAP bits
      powerpc/64s: Don't set FSCR bits in INIT_THREAD
      powerpc/64s: Save FSCR to init_task.thread.fscr after feature init
      powerpc/64s: Don't let DT CPU features set FSCR_DSCR
      powerpc/64s: Don't init FSCR_DSCR in __init_FSCR()
      powerpc/32s: Fix another build failure with CONFIG_PPC_KUAP_DEBUG
      powerpc/module_64: Use special stub for _mcount() with -mprofile-kernel
      powerpc/module_64: Simplify check for -mprofile-kernel ftrace relocations
      powerpc/module_64: Consolidate ftrace code
      powerpc/32: Disable KASAN with pages bigger than 16k
      powerpc/uaccess: Don't set KUEP by default on book3s/32
      powerpc/uaccess: Don't set KUAP by default on book3s/32
      powerpc/8xx: Reduce time spent in allow_user_access() and friends
      ...

commit 5be993432821750f382809df5e20bf4c129b24f7
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Wed Jun 3 16:01:05 2020 -0700

    mm/hugetlb: define a generic fallback for arch_clear_hugepage_flags()
    
    There are multiple similar definitions for arch_clear_hugepage_flags() on
    various platforms.  Lets just add it's generic fallback definition for
    platforms that do not override.  This help reduce code duplication.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Palmer Dabbelt <palmer@dabbelt.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1588907271-11920-4-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index b167c869d72d..e6dfa63da552 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -61,10 +61,6 @@ int huge_ptep_set_access_flags(struct vm_area_struct *vma,
 			       unsigned long addr, pte_t *ptep,
 			       pte_t pte, int dirty);
 
-static inline void arch_clear_hugepage_flags(struct page *page)
-{
-}
-
 #include <asm-generic/hugetlb.h>
 
 #else /* ! CONFIG_HUGETLB_PAGE */

commit b0eae98c66fe4ccac3a5a79a1479c057f2c7cdd7
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Wed Jun 3 16:01:01 2020 -0700

    mm/hugetlb: define a generic fallback for is_hugepage_only_range()
    
    There are multiple similar definitions for is_hugepage_only_range() on
    various platforms.  Lets just add it's generic fallback definition for
    platforms that do not override.  This help reduce code duplication.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Palmer Dabbelt <palmer@dabbelt.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1588907271-11920-3-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index bd6504c28c2f..b167c869d72d 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -30,6 +30,7 @@ static inline int is_hugepage_only_range(struct mm_struct *mm,
 		return slice_is_hugepage_only_range(mm, addr, len);
 	return 0;
 }
+#define is_hugepage_only_range is_hugepage_only_range
 
 #define __HAVE_ARCH_HUGETLB_FREE_PGD_RANGE
 void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,

commit 06f52524870122fb43b214d27e8f4546da36f8ba
Author: Christophe Leroy <christophe.leroy@csgroup.eu>
Date:   Tue May 19 05:49:04 2020 +0000

    powerpc/mm: Standardise pte_update() prototype between PPC32 and PPC64
    
    PPC64 takes 3 additional parameters compared to PPC32:
    - mm
    - address
    - huge
    
    These 3 parameters will be needed in order to perform different
    action depending on the page size on the 8xx.
    
    Make pte_update() prototype identical for PPC32 and PPC64.
    
    This allows dropping an #ifdef in huge_ptep_get_and_clear().
    
    Signed-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/38111acf6841047a8addde37c63e92d611ee38c2.1589866984.git.christophe.leroy@csgroup.eu

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index bd6504c28c2f..e4276af034e9 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -40,11 +40,7 @@ void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
 static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 					    unsigned long addr, pte_t *ptep)
 {
-#ifdef CONFIG_PPC64
 	return __pte(pte_update(mm, addr, ptep, ~0UL, 0, 1));
-#else
-	return __pte(pte_update(ptep, ~0UL, 0));
-#endif
 }
 
 #define __HAVE_ARCH_HUGE_PTEP_CLEAR_FLUSH

commit 4c1616ef036ffaaea95a29d7b6abf9d3e8eb9d92
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Aug 16 05:41:41 2019 +0000

    powerpc/mm: move FSL_BOOK3 version of update_mmu_cache()
    
    Move FSL_BOOK3E version of update_mmu_cache() at the same
    place as book3e_hugetlb_preload() as update_mmu_cache() is
    the only user of book3e_hugetlb_preload().
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/4d69fdc86df9c74adc71a60331a86f6afb8b5e9e.1565933217.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 20a101046cff..bd6504c28c2f 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -31,9 +31,6 @@ static inline int is_hugepage_only_range(struct mm_struct *mm,
 	return 0;
 }
 
-void book3e_hugetlb_preload(struct vm_area_struct *vma, unsigned long ea,
-			    pte_t pte);
-
 #define __HAVE_ARCH_HUGETLB_FREE_PGD_RANGE
 void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
 			    unsigned long end, unsigned long floor,

commit c5710cd20735037ba9be0e95530f0d3795ce07e6
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 05:59:48 2019 +0000

    powerpc/mm: cleanup HPAGE_SHIFT setup
    
    Only book3s/64 may select default among several HPAGE_SHIFT at runtime.
    8xx always defines 512K pages as default
    FSL_BOOK3E always defines 4M pages as default
    
    This patch limits HUGETLB_PAGE_SIZE_VARIABLE to book3s/64
    moves the definitions in subarches files.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 84598c6b0959..20a101046cff 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -15,6 +15,8 @@
 
 extern bool hugetlb_disabled;
 
+void hugetlbpage_init_default(void);
+
 void flush_dcache_icache_hugepage(struct page *page);
 
 int slice_is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,

commit 45d0ba527b575d47b2be75dd517b57cceda04bfe
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 05:59:47 2019 +0000

    powerpc/mm: move hugetlb_disabled into asm/hugetlb.h
    
    No need to have this in asm/page.h, move it into asm/hugetlb.h
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index fd5c0873a57d..84598c6b0959 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -13,6 +13,8 @@
 #include <asm/nohash/32/hugetlb-8xx.h>
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
+extern bool hugetlb_disabled;
+
 void flush_dcache_icache_hugepage(struct page *page);
 
 int slice_is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,

commit 8197af22be01e7c9ab476138652e0dc8cd22a207
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 05:59:44 2019 +0000

    powerpc/mm: split asm/hugetlb.h into dedicated subarch files
    
    Three subarches support hugepages:
      - fsl book3e
      - book3s/64
      - 8xx
    
    This patch splits asm/hugetlb.h to reduce the #ifdef mess.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 8d40565ad0c3..fd5c0873a57d 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -6,83 +6,13 @@
 #include <asm/page.h>
 
 #ifdef CONFIG_PPC_BOOK3S_64
-
 #include <asm/book3s/64/hugetlb.h>
-/*
- * This should work for other subarchs too. But right now we use the
- * new format only for 64bit book3s
- */
-static inline pte_t *hugepd_page(hugepd_t hpd)
-{
-	BUG_ON(!hugepd_ok(hpd));
-	/*
-	 * We have only four bits to encode, MMU page size
-	 */
-	BUILD_BUG_ON((MMU_PAGE_COUNT - 1) > 0xf);
-	return __va(hpd_val(hpd) & HUGEPD_ADDR_MASK);
-}
-
-static inline unsigned int hugepd_mmu_psize(hugepd_t hpd)
-{
-	return (hpd_val(hpd) & HUGEPD_SHIFT_MASK) >> 2;
-}
-
-static inline unsigned int hugepd_shift(hugepd_t hpd)
-{
-	return mmu_psize_to_shift(hugepd_mmu_psize(hpd));
-}
-static inline void flush_hugetlb_page(struct vm_area_struct *vma,
-				      unsigned long vmaddr)
-{
-	if (radix_enabled())
-		return radix__flush_hugetlb_page(vma, vmaddr);
-}
-
-#else
-
-static inline pte_t *hugepd_page(hugepd_t hpd)
-{
-	BUG_ON(!hugepd_ok(hpd));
-#ifdef CONFIG_PPC_8xx
-	return (pte_t *)__va(hpd_val(hpd) & ~HUGEPD_SHIFT_MASK);
-#else
-	return (pte_t *)((hpd_val(hpd) &
-			  ~HUGEPD_SHIFT_MASK) | PD_HUGE);
-#endif
-}
-
-static inline unsigned int hugepd_shift(hugepd_t hpd)
-{
-#ifdef CONFIG_PPC_8xx
-	return ((hpd_val(hpd) & _PMD_PAGE_MASK) >> 1) + 17;
-#else
-	return hpd_val(hpd) & HUGEPD_SHIFT_MASK;
-#endif
-}
-
+#elif defined(CONFIG_PPC_FSL_BOOK3E)
+#include <asm/nohash/hugetlb-book3e.h>
+#elif defined(CONFIG_PPC_8xx)
+#include <asm/nohash/32/hugetlb-8xx.h>
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
-
-static inline pte_t *hugepte_offset(hugepd_t hpd, unsigned long addr,
-				    unsigned pdshift)
-{
-	/*
-	 * On FSL BookE, we have multiple higher-level table entries that
-	 * point to the same hugepte.  Just use the first one since they're all
-	 * identical.  So for that case, idx=0.
-	 */
-	unsigned long idx = 0;
-
-	pte_t *dir = hugepd_page(hpd);
-#ifdef CONFIG_PPC_8xx
-	idx = (addr & ((1UL << pdshift) - 1)) >> PAGE_SHIFT;
-#elif !defined(CONFIG_PPC_FSL_BOOK3E)
-	idx = (addr & ((1UL << pdshift) - 1)) >> hugepd_shift(hpd);
-#endif
-
-	return dir + idx;
-}
-
 void flush_dcache_icache_hugepage(struct page *page);
 
 int slice_is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,
@@ -99,15 +29,6 @@ static inline int is_hugepage_only_range(struct mm_struct *mm,
 
 void book3e_hugetlb_preload(struct vm_area_struct *vma, unsigned long ea,
 			    pte_t pte);
-#ifdef CONFIG_PPC_8xx
-static inline void flush_hugetlb_page(struct vm_area_struct *vma,
-				      unsigned long vmaddr)
-{
-	flush_tlb_page(vma, vmaddr);
-}
-#else
-void flush_hugetlb_page(struct vm_area_struct *vma, unsigned long vmaddr);
-#endif
 
 #define __HAVE_ARCH_HUGETLB_FREE_PGD_RANGE
 void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,

commit 584dbc7727a22459dfcb6e4c9faef6467d08ab5e
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Oct 18 05:22:06 2018 +0000

    powerpc/mm: remove unused variable
    
    In file included from ./include/linux/hugetlb.h:445:0,
                     from arch/powerpc/kernel/setup-common.c:37:
    ./arch/powerpc/include/asm/hugetlb.h: In function ‘huge_ptep_clear_flush’:
    ./arch/powerpc/include/asm/hugetlb.h:154:8: error: variable ‘pte’ set but not used [-Werror=unused-but-set-variable]
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 62a0ca02ca7d..8d40565ad0c3 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -129,8 +129,7 @@ static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
 					 unsigned long addr, pte_t *ptep)
 {
-	pte_t pte;
-	pte = huge_ptep_get_and_clear(vma->vm_mm, addr, ptep);
+	huge_ptep_get_and_clear(vma->vm_mm, addr, ptep);
 	flush_hugetlb_page(vma, addr);
 }
 

commit 3fb69c6a1a13afc5f64ef84e96d69a4be199d485
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Nov 29 14:07:19 2018 +0000

    powerpc/8xx: Enable 512k hugepage support with HW assistance
    
    For using 512k pages with hardware assistance, the PTEs have to be spread
    every 128 bytes in the L2 table.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index dfb8bf236586..62a0ca02ca7d 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -74,7 +74,9 @@ static inline pte_t *hugepte_offset(hugepd_t hpd, unsigned long addr,
 	unsigned long idx = 0;
 
 	pte_t *dir = hugepd_page(hpd);
-#ifndef CONFIG_PPC_FSL_BOOK3E
+#ifdef CONFIG_PPC_8xx
+	idx = (addr & ((1UL << pdshift) - 1)) >> PAGE_SHIFT;
+#elif !defined(CONFIG_PPC_FSL_BOOK3E)
 	idx = (addr & ((1UL << pdshift) - 1)) >> hugepd_shift(hpd);
 #endif
 

commit 0356656284ca750f882b7eb81b9612bdbc90b2c4
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Nov 29 14:07:05 2018 +0000

    powerpc/mm: replace hugetlb_cache by PGT_CACHE(PTE_T_ORDER)
    
    Instead of opencoding cache handling for the special case
    of hugepage tables having a single pte_t element, this
    patch makes use of the common pgtable_cache helpers
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 98004262bc87..dfb8bf236586 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -5,8 +5,6 @@
 #ifdef CONFIG_HUGETLB_PAGE
 #include <asm/page.h>
 
-extern struct kmem_cache *hugepte_cache;
-
 #ifdef CONFIG_PPC_BOOK3S_64
 
 #include <asm/book3s/64/hugetlb.h>

commit bce85a167b336da80642952b222588c25c8613d7
Author: Breno Leitao <leitao@debian.org>
Date:   Wed Oct 31 11:24:11 2018 -0300

    powerpc/mm: Remove extern from function definition
    
    Function huge_ptep_set_access_flags() has the 'extern' keyword in the
    function definition and also in the function declaration. This causes a
    warning in 'sparse' since the 'extern' storage class should not be used
    in the function definition.
    
            arch/powerpc/mm/pgtable.c:232:12: warning: function 'huge_ptep_set_access_flags' with external linkage has definition
    
    This patch removes the keyword from the definition part. It also removes
    the extern keyword from the declaration part, since checkpatch --strict
    complains about it.
    
    Suggested-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Breno Leitao <leitao@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 383da1ab9e23..98004262bc87 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -135,9 +135,9 @@ static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
 }
 
 #define __HAVE_ARCH_HUGE_PTEP_SET_ACCESS_FLAGS
-extern int huge_ptep_set_access_flags(struct vm_area_struct *vma,
-				      unsigned long addr, pte_t *ptep,
-				      pte_t pte, int dirty);
+int huge_ptep_set_access_flags(struct vm_area_struct *vma,
+			       unsigned long addr, pte_t *ptep,
+			       pte_t pte, int dirty);
 
 static inline void arch_clear_hugepage_flags(struct page *page)
 {

commit 544db7597ad0fd1ceebf656e2808a209d46ccd9e
Author: Alexandre Ghiti <alex@ghiti.fr>
Date:   Fri Oct 26 15:08:43 2018 -0700

    hugetlb: introduce generic version of huge_ptep_get
    
    ia64, mips, parisc, powerpc, sh, sparc, x86 architectures use the same
    version of huge_ptep_get, so move this generic implementation into
    asm-generic/hugetlb.h.
    
    [arnd@arndb.de: fix ARM 3level page tables]
      Link: http://lkml.kernel.org/r/20181005161722.904274-1-arnd@arndb.de
    Link: http://lkml.kernel.org/r/20180920060358.16606-12-alex@ghiti.fr
    Signed-off-by: Alexandre Ghiti <alex@ghiti.fr>
    Reviewed-by: Luiz Capitulino <lcapitulino@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Helge Deller <deller@gmx.de>                 [parisc]
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Ingo Molnar <mingo@kernel.org>                [x86]
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index d4d9cf6cb846..383da1ab9e23 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -139,11 +139,6 @@ extern int huge_ptep_set_access_flags(struct vm_area_struct *vma,
 				      unsigned long addr, pte_t *ptep,
 				      pte_t pte, int dirty);
 
-static inline pte_t huge_ptep_get(pte_t *ptep)
-{
-	return *ptep;
-}
-
 static inline void arch_clear_hugepage_flags(struct page *page)
 {
 }

commit facf6d5b8b6c4212519acfefbf3a0e0aeeb5b77e
Author: Alexandre Ghiti <alex@ghiti.fr>
Date:   Fri Oct 26 15:08:39 2018 -0700

    hugetlb: introduce generic version of huge_ptep_set_access_flags()
    
    arm, ia64, sh, x86 architectures use the same version
    of huge_ptep_set_access_flags, so move this generic implementation
    into asm-generic/hugetlb.h.
    
    Link: http://lkml.kernel.org/r/20180920060358.16606-11-alex@ghiti.fr
    Signed-off-by: Alexandre Ghiti <alex@ghiti.fr>
    Reviewed-by: Luiz Capitulino <lcapitulino@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Helge Deller <deller@gmx.de>                 [parisc]
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Ingo Molnar <mingo@kernel.org>                [x86]
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 2a90f387880a..d4d9cf6cb846 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -134,6 +134,7 @@ static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
 	flush_hugetlb_page(vma, addr);
 }
 
+#define __HAVE_ARCH_HUGE_PTEP_SET_ACCESS_FLAGS
 extern int huge_ptep_set_access_flags(struct vm_area_struct *vma,
 				      unsigned long addr, pte_t *ptep,
 				      pte_t pte, int dirty);

commit 78d6e4e8ea8700fb3973661eb3774f632bab5842
Author: Alexandre Ghiti <alex@ghiti.fr>
Date:   Fri Oct 26 15:08:31 2018 -0700

    hugetlb: introduce generic version of prepare_hugepage_range
    
    arm, arm64, powerpc, sparc, x86 architectures use the same version of
    prepare_hugepage_range, so move this generic implementation into
    asm-generic/hugetlb.h.
    
    Link: http://lkml.kernel.org/r/20180920060358.16606-9-alex@ghiti.fr
    Signed-off-by: Alexandre Ghiti <alex@ghiti.fr>
    Reviewed-by: Luiz Capitulino <lcapitulino@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Helge Deller <deller@gmx.de>                 [parisc]
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Ingo Molnar <mingo@kernel.org>                [x86]
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index b5b57b309564..2a90f387880a 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -114,21 +114,6 @@ void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
 			    unsigned long end, unsigned long floor,
 			    unsigned long ceiling);
 
-/*
- * If the arch doesn't supply something else, assume that hugepage
- * size aligned regions are ok without further preparation.
- */
-static inline int prepare_hugepage_range(struct file *file,
-			unsigned long addr, unsigned long len)
-{
-	struct hstate *h = hstate_file(file);
-	if (len & ~huge_page_mask(h))
-		return -EINVAL;
-	if (addr & ~huge_page_mask(h))
-		return -EINVAL;
-	return 0;
-}
-
 #define __HAVE_ARCH_HUGE_PTEP_GET_AND_CLEAR
 static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 					    unsigned long addr, pte_t *ptep)

commit c4916a008665a6650834a736f80559a7f90e5857
Author: Alexandre Ghiti <alex@ghiti.fr>
Date:   Fri Oct 26 15:08:26 2018 -0700

    hugetlb: introduce generic version of huge_pte_wrprotect
    
    arm, arm64, ia64, mips, parisc, powerpc, sh, sparc, x86 architectures use
    the same version of huge_pte_wrprotect, so move this generic
    implementation into asm-generic/hugetlb.h.
    
    Link: http://lkml.kernel.org/r/20180920060358.16606-8-alex@ghiti.fr
    Signed-off-by: Alexandre Ghiti <alex@ghiti.fr>
    Reviewed-by: Luiz Capitulino <lcapitulino@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Helge Deller <deller@gmx.de>                 [parisc]
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Ingo Molnar <mingo@kernel.org>                [x86]
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 6a534353c8eb..b5b57b309564 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -149,11 +149,6 @@ static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
 	flush_hugetlb_page(vma, addr);
 }
 
-static inline pte_t huge_pte_wrprotect(pte_t pte)
-{
-	return pte_wrprotect(pte);
-}
-
 extern int huge_ptep_set_access_flags(struct vm_area_struct *vma,
 				      unsigned long addr, pte_t *ptep,
 				      pte_t pte, int dirty);

commit cae72abc1af0c1ef4cb39e7187c5af44a7ce17a4
Author: Alexandre Ghiti <alex@ghiti.fr>
Date:   Fri Oct 26 15:08:22 2018 -0700

    hugetlb: introduce generic version of huge_pte_none()
    
    arm, arm64, ia64, mips, parisc, powerpc, sh, sparc, x86 architectures use
    the same version of huge_pte_none, so move this generic implementation
    into asm-generic/hugetlb.h.
    
    Link: http://lkml.kernel.org/r/20180920060358.16606-7-alex@ghiti.fr
    Signed-off-by: Alexandre Ghiti <alex@ghiti.fr>
    Reviewed-by: Luiz Capitulino <lcapitulino@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Helge Deller <deller@gmx.de>                 [parisc]
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Ingo Molnar <mingo@kernel.org>                [x86]
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 1eb3e131cab4..6a534353c8eb 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -149,11 +149,6 @@ static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
 	flush_hugetlb_page(vma, addr);
 }
 
-static inline int huge_pte_none(pte_t pte)
-{
-	return pte_none(pte);
-}
-
 static inline pte_t huge_pte_wrprotect(pte_t pte)
 {
 	return pte_wrprotect(pte);

commit fe632225bdbd49f5620e7ccfa03268fec9a3e370
Author: Alexandre Ghiti <alex@ghiti.fr>
Date:   Fri Oct 26 15:08:17 2018 -0700

    hugetlb: introduce generic version of huge_ptep_clear_flush
    
    arm, x86 architectures use the same version of huge_ptep_clear_flush, so
    move this generic implementation into asm-generic/hugetlb.h.
    
    Link: http://lkml.kernel.org/r/20180920060358.16606-6-alex@ghiti.fr
    Signed-off-by: Alexandre Ghiti <alex@ghiti.fr>
    Reviewed-by: Luiz Capitulino <lcapitulino@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Helge Deller <deller@gmx.de>                 [parisc]
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Ingo Molnar <mingo@kernel.org>                [x86]
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 91bdc84b76ce..1eb3e131cab4 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -140,6 +140,7 @@ static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 #endif
 }
 
+#define __HAVE_ARCH_HUGE_PTEP_CLEAR_FLUSH
 static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
 					 unsigned long addr, pte_t *ptep)
 {

commit a4d838536c6e5c1807a863c860d9e767d55ba681
Author: Alexandre Ghiti <alex@ghiti.fr>
Date:   Fri Oct 26 15:08:12 2018 -0700

    hugetlb: introduce generic version of huge_ptep_get_and_clear()
    
    arm, ia64, sh, x86 architectures use the same version of
    huge_ptep_get_and_clear, so move this generic implementation into
    asm-generic/hugetlb.h.
    
    Link: http://lkml.kernel.org/r/20180920060358.16606-5-alex@ghiti.fr
    Signed-off-by: Alexandre Ghiti <alex@ghiti.fr>
    Reviewed-by: Luiz Capitulino <lcapitulino@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Helge Deller <deller@gmx.de>                 [parisc]
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Ingo Molnar <mingo@kernel.org>                [x86]
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 33b899624922..91bdc84b76ce 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -129,6 +129,7 @@ static inline int prepare_hugepage_range(struct file *file,
 	return 0;
 }
 
+#define __HAVE_ARCH_HUGE_PTEP_GET_AND_CLEAR
 static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 					    unsigned long addr, pte_t *ptep)
 {

commit cea685d556330b0265ca62c0d820f687cc9a38d6
Author: Alexandre Ghiti <alex@ghiti.fr>
Date:   Fri Oct 26 15:08:07 2018 -0700

    hugetlb: introduce generic version of set_huge_pte_at()
    
    arm, ia64, mips, powerpc, sh, x86 architectures use the same version of
    set_huge_pte_at, so move this generic implementation into
    asm-generic/hugetlb.h.
    
    Link: http://lkml.kernel.org/r/20180920060358.16606-4-alex@ghiti.fr
    Signed-off-by: Alexandre Ghiti <alex@ghiti.fr>
    Reviewed-by: Luiz Capitulino <lcapitulino@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Helge Deller <deller@gmx.de>                 [parisc]
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Ingo Molnar <mingo@kernel.org>                [x86]
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 2ab028b73a43..33b899624922 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -129,12 +129,6 @@ static inline int prepare_hugepage_range(struct file *file,
 	return 0;
 }
 
-static inline void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
-				   pte_t *ptep, pte_t pte)
-{
-	set_pte_at(mm, addr, ptep, pte);
-}
-
 static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 					    unsigned long addr, pte_t *ptep)
 {

commit 1e5f50fc9d0a653c910df2291f245a8fa7beed11
Author: Alexandre Ghiti <alex@ghiti.fr>
Date:   Fri Oct 26 15:08:03 2018 -0700

    hugetlb: introduce generic version of hugetlb_free_pgd_range
    
    arm, arm64, mips, parisc, sh, x86 architectures use the same version of
    hugetlb_free_pgd_range, so move this generic implementation into
    asm-generic/hugetlb.h.
    
    Link: http://lkml.kernel.org/r/20180920060358.16606-3-alex@ghiti.fr
    Signed-off-by: Alexandre Ghiti <alex@ghiti.fr>
    Reviewed-by: Luiz Capitulino <lcapitulino@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Helge Deller <deller@gmx.de>                 [parisc]
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Ingo Molnar <mingo@kernel.org>                [x86]
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 2d00cc530083..2ab028b73a43 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -4,7 +4,6 @@
 
 #ifdef CONFIG_HUGETLB_PAGE
 #include <asm/page.h>
-#include <asm-generic/hugetlb.h>
 
 extern struct kmem_cache *hugepte_cache;
 
@@ -110,6 +109,7 @@ static inline void flush_hugetlb_page(struct vm_area_struct *vma,
 void flush_hugetlb_page(struct vm_area_struct *vma, unsigned long vmaddr);
 #endif
 
+#define __HAVE_ARCH_HUGETLB_FREE_PGD_RANGE
 void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
 			    unsigned long end, unsigned long floor,
 			    unsigned long ceiling);
@@ -176,6 +176,8 @@ static inline void arch_clear_hugepage_flags(struct page *page)
 {
 }
 
+#include <asm-generic/hugetlb.h>
+
 #else /* ! CONFIG_HUGETLB_PAGE */
 static inline void flush_hugetlb_page(struct vm_area_struct *vma,
 				      unsigned long vmaddr)

commit 646dbe40fa2a54118975792fa9b98cefac7d232e
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Wed Aug 8 15:36:34 2018 +0000

    powerpc/mm: remove huge_pte_offset_and_shift() prototype
    
    huge_pte_offset_and_shift() has never existed
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 3225eb6402cc..2d00cc530083 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -84,9 +84,6 @@ static inline pte_t *hugepte_offset(hugepd_t hpd, unsigned long addr,
 	return dir + idx;
 }
 
-pte_t *huge_pte_offset_and_shift(struct mm_struct *mm,
-				 unsigned long addr, unsigned *shift);
-
 void flush_dcache_icache_hugepage(struct page *page);
 
 int slice_is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,

commit f069ff396d657ac7bdb5de866c3ec28b8d08d953
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Tue May 29 19:58:38 2018 +0530

    powerpc/mm/hugetlb: Update huge_ptep_set_access_flags to call __ptep_set_access_flags directly
    
    In a later patch, we want to update __ptep_set_access_flags take page size
    arg. This makes ptep_set_access_flags only work with mmu_virtual_psize.
    To simplify the code make huge_ptep_set_access_flags directly call
    __ptep_set_access_flags so that we can compute the hugetlb page size in
    hugetlb function.
    
    Now that ptep_set_access_flags won't be called for hugetlb remove
    the is_vm_hugetlb_page() check and add the assert of pte lock
    unconditionally.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 96444bc08034..3225eb6402cc 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -166,22 +166,9 @@ static inline pte_t huge_pte_wrprotect(pte_t pte)
 	return pte_wrprotect(pte);
 }
 
-static inline int huge_ptep_set_access_flags(struct vm_area_struct *vma,
-					     unsigned long addr, pte_t *ptep,
-					     pte_t pte, int dirty)
-{
-#ifdef HUGETLB_NEED_PRELOAD
-	/*
-	 * The "return 1" forces a call of update_mmu_cache, which will write a
-	 * TLB entry.  Without this, platforms that don't do a write of the TLB
-	 * entry in the TLB miss handler asm will fault ad infinitum.
-	 */
-	ptep_set_access_flags(vma, addr, ptep, pte, dirty);
-	return 1;
-#else
-	return ptep_set_access_flags(vma, addr, ptep, pte, dirty);
-#endif
-}
+extern int huge_ptep_set_access_flags(struct vm_area_struct *vma,
+				      unsigned long addr, pte_t *ptep,
+				      pte_t pte, int dirty);
 
 static inline pte_t huge_ptep_get(pte_t *ptep)
 {

commit d8731527acee53b4d46d59ff0b5fc36931ad0451
Author: Mathieu Malaterre <malat@debian.org>
Date:   Fri Apr 13 20:41:43 2018 +0200

    powerpc/sparse: Fix plain integer as NULL pointer warning
    
    Trivial fix to remove the following sparse warnings:
    
      arch/powerpc/kernel/module_32.c:112:74: warning: Using plain integer as NULL pointer
      arch/powerpc/kernel/module_32.c:117:74: warning: Using plain integer as NULL pointer
      drivers/macintosh/via-pmu.c:1155:28: warning: Using plain integer as NULL pointer
      drivers/macintosh/via-pmu.c:1230:20: warning: Using plain integer as NULL pointer
      drivers/macintosh/via-pmu.c:1385:36: warning: Using plain integer as NULL pointer
      drivers/macintosh/via-pmu.c:1752:23: warning: Using plain integer as NULL pointer
      drivers/macintosh/via-pmu.c:2084:19: warning: Using plain integer as NULL pointer
      drivers/macintosh/via-pmu.c:2110:32: warning: Using plain integer as NULL pointer
      drivers/macintosh/via-pmu.c:2167:19: warning: Using plain integer as NULL pointer
      drivers/macintosh/via-pmu.c:2183:19: warning: Using plain integer as NULL pointer
      drivers/macintosh/via-pmu.c:277:20: warning: Using plain integer as NULL pointer
      arch/powerpc/platforms/powermac/setup.c:155:67: warning: Using plain integer as NULL pointer
      arch/powerpc/platforms/powermac/setup.c:247:27: warning: Using plain integer as NULL pointer
      arch/powerpc/platforms/powermac/setup.c:249:27: warning: Using plain integer as NULL pointer
      arch/powerpc/platforms/powermac/setup.c:252:37: warning: Using plain integer as NULL pointer
      arch/powerpc/mm/tlb_hash32.c:127:21: warning: Using plain integer as NULL pointer
      arch/powerpc/mm/tlb_hash32.c:148:21: warning: Using plain integer as NULL pointer
      arch/powerpc/mm/tlb_hash32.c:44:21: warning: Using plain integer as NULL pointer
      arch/powerpc/mm/tlb_hash32.c:57:21: warning: Using plain integer as NULL pointer
      arch/powerpc/mm/tlb_hash32.c:87:21: warning: Using plain integer as NULL pointer
      arch/powerpc/kernel/btext.c:160:31: warning: Using plain integer as NULL pointer
      arch/powerpc/kernel/btext.c:167:22: warning: Using plain integer as NULL pointer
      arch/powerpc/kernel/btext.c:274:21: warning: Using plain integer as NULL pointer
      arch/powerpc/kernel/btext.c:285:31: warning: Using plain integer as NULL pointer
      arch/powerpc/include/asm/hugetlb.h:204:16: warning: Using plain integer as NULL pointer
      arch/powerpc/mm/ppc_mmu_32.c:170:21: warning: Using plain integer as NULL pointer
      arch/powerpc/platforms/powermac/pci.c:1227:23: warning: Using plain integer as NULL pointer
      arch/powerpc/platforms/powermac/pci.c:65:24: warning: Using plain integer as NULL pointer
    
    Also use `--fix` command line option from `script/checkpatch --strict` to
    remove the following:
    
      CHECK: Comparison to NULL could be written "!dispDeviceBase"
      #72: FILE: arch/powerpc/kernel/btext.c:160:
      +     if (dispDeviceBase == NULL)
    
      CHECK: Comparison to NULL could be written "!vbase"
      #80: FILE: arch/powerpc/kernel/btext.c:167:
      +     if (vbase == NULL)
    
      CHECK: Comparison to NULL could be written "!base"
      #89: FILE: arch/powerpc/kernel/btext.c:274:
      +     if (base == NULL)
    
      CHECK: Comparison to NULL could be written "!dispDeviceBase"
      #98: FILE: arch/powerpc/kernel/btext.c:285:
      +     if (dispDeviceBase == NULL)
    
      CHECK: Comparison to NULL could be written "strstr"
      #117: FILE: arch/powerpc/kernel/module_32.c:117:
      +             if (strstr(secstrings + sechdrs[i].sh_name, ".debug") != NULL)
    
      CHECK: Comparison to NULL could be written "!Hash"
      #130: FILE: arch/powerpc/mm/ppc_mmu_32.c:170:
      +     if (Hash == NULL)
    
      CHECK: Comparison to NULL could be written "Hash"
      #143: FILE: arch/powerpc/mm/tlb_hash32.c:44:
      +     if (Hash != NULL) {
    
      CHECK: Comparison to NULL could be written "!Hash"
      #152: FILE: arch/powerpc/mm/tlb_hash32.c:57:
      +     if (Hash == NULL) {
    
      CHECK: Comparison to NULL could be written "!Hash"
      #161: FILE: arch/powerpc/mm/tlb_hash32.c:87:
      +     if (Hash == NULL) {
    
      CHECK: Comparison to NULL could be written "!Hash"
      #170: FILE: arch/powerpc/mm/tlb_hash32.c:127:
      +     if (Hash == NULL) {
    
      CHECK: Comparison to NULL could be written "!Hash"
      #179: FILE: arch/powerpc/mm/tlb_hash32.c:148:
      +     if (Hash == NULL) {
    
      ERROR: space required after that ';' (ctx:VxV)
      #192: FILE: arch/powerpc/platforms/powermac/pci.c:65:
      +     for (; node != NULL;node = node->sibling) {
    
      CHECK: Comparison to NULL could be written "node"
      #192: FILE: arch/powerpc/platforms/powermac/pci.c:65:
      +     for (; node != NULL;node = node->sibling) {
    
      CHECK: Comparison to NULL could be written "!region"
      #201: FILE: arch/powerpc/platforms/powermac/pci.c:1227:
      +     if (region == NULL)
    
      CHECK: Comparison to NULL could be written "of_get_property"
      #214: FILE: arch/powerpc/platforms/powermac/setup.c:155:
      +             if (of_get_property(np, "cache-unified", NULL) != NULL && dc) {
    
      CHECK: Comparison to NULL could be written "!np"
      #223: FILE: arch/powerpc/platforms/powermac/setup.c:247:
      +             if (np == NULL)
    
      CHECK: Comparison to NULL could be written "np"
      #226: FILE: arch/powerpc/platforms/powermac/setup.c:249:
      +             if (np != NULL) {
    
      CHECK: Comparison to NULL could be written "l2cr"
      #230: FILE: arch/powerpc/platforms/powermac/setup.c:252:
      +                     if (l2cr != NULL) {
    
      CHECK: Comparison to NULL could be written "via"
      #243: FILE: drivers/macintosh/via-pmu.c:277:
      +     if (via != NULL)
    
      CHECK: Comparison to NULL could be written "current_req"
      #252: FILE: drivers/macintosh/via-pmu.c:1155:
      +     if (current_req != NULL) {
    
      CHECK: Comparison to NULL could be written "!req"
      #261: FILE: drivers/macintosh/via-pmu.c:1230:
      +     if (req == NULL || pmu_state != idle
    
      CHECK: Comparison to NULL could be written "!req"
      #270: FILE: drivers/macintosh/via-pmu.c:1385:
      +                     if (req == NULL) {
    
      CHECK: Comparison to NULL could be written "!pp"
      #288: FILE: drivers/macintosh/via-pmu.c:2084:
      +     if (pp == NULL)
    
      CHECK: Comparison to NULL could be written "!pp"
      #297: FILE: drivers/macintosh/via-pmu.c:2110:
      +     if (count < 1 || pp == NULL)
    
      CHECK: Comparison to NULL could be written "!pp"
      #306: FILE: drivers/macintosh/via-pmu.c:2167:
      +     if (pp == NULL)
    
      CHECK: Comparison to NULL could be written "pp"
      #315: FILE: drivers/macintosh/via-pmu.c:2183:
      +     if (pp != NULL) {
    
    Link: https://github.com/linuxppc/linux/issues/37
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 78540c074d70..96444bc08034 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -202,7 +202,7 @@ static inline void flush_hugetlb_page(struct vm_area_struct *vma,
 static inline pte_t *hugepte_offset(hugepd_t hpd, unsigned long addr,
 				    unsigned pdshift)
 {
-	return 0;
+	return NULL;
 }
 #endif /* CONFIG_HUGETLB_PAGE */
 

commit 49a695ba723224875df50e327bd7b0b65dd9a56b
Merge: 299f89d53e61 c1b25a17d249
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Apr 7 12:08:19 2018 -0700

    Merge tag 'powerpc-4.17-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - Support for 4PB user address space on 64-bit, opt-in via mmap().
    
       - Removal of POWER4 support, which was accidentally broken in 2016
         and no one noticed, and blocked use of some modern instructions.
    
       - Workarounds so that the hypervisor can enable Transactional Memory
         on Power9.
    
       - A series to disable the DAWR (Data Address Watchpoint Register) on
         Power9.
    
       - More information displayed in the meltdown/spectre_v1/v2 sysfs
         files.
    
       - A vpermxor (Power8 Altivec) implementation for the raid6 Q
         Syndrome.
    
       - A big series to make the allocation of our pacas (per cpu area),
         kernel page tables, and per-cpu stacks NUMA aware when using the
         Radix MMU on Power9.
    
      And as usual many fixes, reworks and cleanups.
    
      Thanks to: Aaro Koskinen, Alexandre Belloni, Alexey Kardashevskiy,
      Alistair Popple, Andy Shevchenko, Aneesh Kumar K.V, Anshuman Khandual,
      Balbir Singh, Benjamin Herrenschmidt, Christophe Leroy, Christophe
      Lombard, Cyril Bur, Daniel Axtens, Dave Young, Finn Thain, Frederic
      Barrat, Gustavo Romero, Horia Geantă, Jonathan Neuschäfer, Kees Cook,
      Larry Finger, Laurent Dufour, Laurent Vivier, Logan Gunthorpe,
      Madhavan Srinivasan, Mark Greer, Mark Hairgrove, Markus Elfring,
      Mathieu Malaterre, Matt Brown, Matt Evans, Mauricio Faria de Oliveira,
      Michael Neuling, Naveen N. Rao, Nicholas Piggin, Paul Mackerras,
      Philippe Bergheaud, Ram Pai, Rob Herring, Sam Bobroff, Segher
      Boessenkool, Simon Guo, Simon Horman, Stewart Smith, Sukadev
      Bhattiprolu, Suraj Jitindar Singh, Thiago Jung Bauermann, Vaibhav
      Jain, Vaidyanathan Srinivasan, Vasant Hegde, Wei Yongjun"
    
    * tag 'powerpc-4.17-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (207 commits)
      powerpc/64s/idle: Fix restore of AMOR on POWER9 after deep sleep
      powerpc/64s: Fix POWER9 DD2.2 and above in cputable features
      powerpc/64s: Fix pkey support in dt_cpu_ftrs, add CPU_FTR_PKEY bit
      powerpc/64s: Fix dt_cpu_ftrs to have restore_cpu clear unwanted LPCR bits
      Revert "powerpc/64s/idle: POWER9 ESL=0 stop avoid save/restore overhead"
      powerpc: iomap.c: introduce io{read|write}64_{lo_hi|hi_lo}
      powerpc: io.h: move iomap.h include so that it can use readq/writeq defs
      cxl: Fix possible deadlock when processing page faults from cxllib
      powerpc/hw_breakpoint: Only disable hw breakpoint if cpu supports it
      powerpc/mm/radix: Update command line parsing for disable_radix
      powerpc/mm/radix: Parse disable_radix commandline correctly.
      powerpc/mm/hugetlb: initialize the pagetable cache correctly for hugetlb
      powerpc/mm/radix: Update pte fragment count from 16 to 256 on radix
      powerpc/mm/keys: Update documentation and remove unnecessary check
      powerpc/64s/idle: POWER9 ESL=0 stop avoid save/restore overhead
      powerpc/64s/idle: Consolidate power9_offline_stop()/power9_idle_stop()
      powerpc/powernv: Always stop secondaries before reboot/shutdown
      powerpc: hard disable irqs in smp_send_stop loop
      powerpc: use NMI IPI for smp_send_stop
      powerpc/powernv: Fix SMT4 forcing idle code
      ...

commit 09135cc594d141cd279c32a18b91cb3bd3fe8cc5
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Apr 5 16:24:21 2018 -0700

    mm, powerpc: use vma_kernel_pagesize() in vma_mmu_pagesize()
    
    Patch series "mm, smaps: MMUPageSize for device-dax", v3.
    
    Similar to commit 31383c6865a5 ("mm, hugetlbfs: introduce ->split() to
    vm_operations_struct") here is another occasion where we want
    special-case hugetlbfs/hstate enabling to also apply to device-dax.
    
    This prompts the question what other hstate conversions we might do
    beyond ->split() and ->pagesize(), but this appears to be the last of
    the usages of hstate_vma() in generic/non-hugetlbfs specific code paths.
    
    This patch (of 3):
    
    The current powerpc definition of vma_mmu_pagesize() open codes looking
    up the page size via hstate.  It is identical to the generic
    vma_kernel_pagesize() implementation.
    
    Now, vma_kernel_pagesize() is growing support for determining the page
    size of Device-DAX vmas in addition to the existing Hugetlbfs page size
    determination.
    
    Ideally, if the powerpc vma_mmu_pagesize() used vma_kernel_pagesize() it
    would automatically benefit from any new vma-type support that is added
    to vma_kernel_pagesize().  However, the powerpc vma_mmu_pagesize() is
    prevented from calling vma_kernel_pagesize() due to a circular header
    dependency that requires vma_mmu_pagesize() to be defined before
    including <linux/hugetlb.h>.
    
    Break this circular dependency by defining the default vma_mmu_pagesize()
    as a __weak symbol to be overridden by the powerpc version.
    
    Link: http://lkml.kernel.org/r/151996254179.27922.2213728278535578744.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Jane Chu <jane.chu@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 1a4847f67ea8..6f6751d3eba9 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -117,12 +117,6 @@ void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
 			    unsigned long end, unsigned long floor,
 			    unsigned long ceiling);
 
-/*
- * The version of vma_mmu_pagesize() in arch/powerpc/mm/hugetlbpage.c needs
- * to override the version in mm/hugetlb.c
- */
-#define vma_mmu_pagesize vma_mmu_pagesize
-
 /*
  * If the arch doesn't supply something else, assume that hugepage
  * size aligned regions are ok without further preparation.

commit 014a32b30e9d81b47ef82b9995b52c3a0c8b4082
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Mar 7 11:37:17 2018 +1000

    powerpc/mm/slice: remove radix calls to the slice code
    
    This is a tidy up which removes radix MMU calls into the slice
    code.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 1a4847f67ea8..48f2ed2a71ae 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -89,17 +89,17 @@ pte_t *huge_pte_offset_and_shift(struct mm_struct *mm,
 
 void flush_dcache_icache_hugepage(struct page *page);
 
-#if defined(CONFIG_PPC_MM_SLICES)
-int is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,
+int slice_is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,
 			   unsigned long len);
-#else
+
 static inline int is_hugepage_only_range(struct mm_struct *mm,
 					 unsigned long addr,
 					 unsigned long len)
 {
+	if (IS_ENABLED(CONFIG_PPC_MM_SLICES) && !radix_enabled())
+		return slice_is_hugepage_only_range(mm, addr, len);
 	return 0;
 }
-#endif
 
 void book3e_hugetlb_preload(struct vm_area_struct *vma, unsigned long ea,
 			    pte_t pte);

commit de0f93873937e999fadaba011d368bc042af37b2
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Jan 12 13:45:31 2018 +0100

    powerpc/8xx: Remove _PAGE_USER and handle user access at PMD level
    
    As Linux kernel separates KERNEL and USER address spaces, there is
    therefore no need to flag USER access at page level.
    
    Today, the 8xx TLB handlers already handle user access in the L1 entry
    through Access Protection Groups, it is then natural to move the user
    access handling at PMD level once _PAGE_NA allows to handle PAGE_NONE
    protection without _PAGE_USER
    
    In the mean time, as we free up one bit in the PTE, we can use it to
    include SPS (page size flag) in the PTE and avoid handling it at every
    TLB miss hence removing special handling based on compiled page size.
    
    For _PAGE_EXEC, we rework it to use PP PTE bits, avoiding the copy
    of _PAGE_EXEC bit into the L1 entry. Unfortunatly we are not
    able to put it at the correct location as it conflicts with
    NA/RO/RW bits for data entries.
    
    Upper bits of APG in L1 entry overlap with PMD base address. In
    order to avoid having to filter that out, we set up all groups so that
    upper bits can have any value.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 14c9d44f355b..1a4847f67ea8 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -47,8 +47,7 @@ static inline pte_t *hugepd_page(hugepd_t hpd)
 {
 	BUG_ON(!hugepd_ok(hpd));
 #ifdef CONFIG_PPC_8xx
-	return (pte_t *)__va(hpd_val(hpd) &
-			     ~(_PMD_PAGE_MASK | _PMD_PRESENT_MASK));
+	return (pte_t *)__va(hpd_val(hpd) & ~HUGEPD_SHIFT_MASK);
 #else
 	return (pte_t *)((hpd_val(hpd) &
 			  ~HUGEPD_SHIFT_MASK) | PD_HUGE);

commit 5b0e2cb020085efe202123162502e0b551e49a0e
Merge: 758f875848d7 3ffa9d9e2a7c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 12:47:46 2017 -0800

    Merge tag 'powerpc-4.15-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "A bit of a small release, I suspect in part due to me travelling for
      KS. But my backlog of patches to review is smaller than usual, so I
      think in part folks just didn't send as much this cycle.
    
      Non-highlights:
    
       - Five fixes for the >128T address space handling, both to fix bugs
         in our implementation and to bring the semantics exactly into line
         with x86.
    
      Highlights:
    
       - Support for a new OPAL call on bare metal machines which gives us a
         true NMI (ie. is not masked by MSR[EE]=0) for debugging etc.
    
       - Support for Power9 DD2 in the CXL driver.
    
       - Improvements to machine check handling so that uncorrectable errors
         can be reported into the generic memory_failure() machinery.
    
       - Some fixes and improvements for VPHN, which is used under PowerVM
         to notify the Linux partition of topology changes.
    
       - Plumbing to enable TM (transactional memory) without suspend on
         some Power9 processors (PPC_FEATURE2_HTM_NO_SUSPEND).
    
       - Support for emulating vector loads form cache-inhibited memory, on
         some Power9 revisions.
    
       - Disable the fast-endian switch "syscall" by default (behind a
         CONFIG), we believe it has never had any users.
    
       - A major rework of the API drivers use when initiating and waiting
         for long running operations performed by OPAL firmware, and changes
         to the powernv_flash driver to use the new API.
    
       - Several fixes for the handling of FP/VMX/VSX while processes are
         using transactional memory.
    
       - Optimisations of TLB range flushes when using the radix MMU on
         Power9.
    
       - Improvements to the VAS facility used to access coprocessors on
         Power9, and related improvements to the way the NX crypto driver
         handles requests.
    
       - Implementation of PMEM_API and UACCESS_FLUSHCACHE for 64-bit.
    
      Thanks to: Alexey Kardashevskiy, Alistair Popple, Allen Pais, Andrew
      Donnellan, Aneesh Kumar K.V, Arnd Bergmann, Balbir Singh, Benjamin
      Herrenschmidt, Breno Leitao, Christophe Leroy, Christophe Lombard,
      Cyril Bur, Frederic Barrat, Gautham R. Shenoy, Geert Uytterhoeven,
      Guilherme G. Piccoli, Gustavo Romero, Haren Myneni, Joel Stanley,
      Kamalesh Babulal, Kautuk Consul, Markus Elfring, Masami Hiramatsu,
      Michael Bringmann, Michael Neuling, Michal Suchanek, Naveen N. Rao,
      Nicholas Piggin, Oliver O'Halloran, Paul Mackerras, Pedro Miraglia
      Franco de Carvalho, Philippe Bergheaud, Sandipan Das, Seth Forshee,
      Shriya, Stephen Rothwell, Stewart Smith, Sukadev Bhattiprolu, Tyrel
      Datwyler, Vaibhav Jain, Vaidyanathan Srinivasan, and William A.
      Kennington III"
    
    * tag 'powerpc-4.15-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (151 commits)
      powerpc/64s: Fix Power9 DD2.0 workarounds by adding DD2.1 feature
      powerpc/64s: Fix masking of SRR1 bits on instruction fault
      powerpc/64s: mm_context.addr_limit is only used on hash
      powerpc/64s/radix: Fix 128TB-512TB virtual address boundary case allocation
      powerpc/64s/hash: Allow MAP_FIXED allocations to cross 128TB boundary
      powerpc/64s/hash: Fix fork() with 512TB process address space
      powerpc/64s/hash: Fix 128TB-512TB virtual address boundary case allocation
      powerpc/64s/hash: Fix 512T hint detection to use >= 128T
      powerpc: Fix DABR match on hash based systems
      powerpc/signal: Properly handle return value from uprobe_deny_signal()
      powerpc/fadump: use kstrtoint to handle sysfs store
      powerpc/lib: Implement UACCESS_FLUSHCACHE API
      powerpc/lib: Implement PMEM API
      powerpc/powernv/npu: Don't explicitly flush nmmu tlb
      powerpc/powernv/npu: Use flush_all_mm() instead of flush_tlb_mm()
      powerpc/powernv/idle: Round up latency and residency values
      powerpc/kprobes: refactor kprobe_lookup_name for safer string operations
      powerpc/kprobes: Blacklist emulate_update_regs() from kprobes
      powerpc/kprobes: Do not disable interrupts for optprobes and kprobes_on_ftrace
      powerpc/kprobes: Disable preemption before invoking probe handler for optprobes
      ...

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index b8a0fb442c64..93f98239159f 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_POWERPC_HUGETLB_H
 #define _ASM_POWERPC_HUGETLB_H
 

commit 6773027205ea4ccf17055d7f0a0cd813f22fe127
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Oct 16 12:41:00 2017 +0530

    powerpc/mm/radix: Drop unneeded NULL check
    
    We call these functions with non-NULL mm or vma. Hence we can skip the
    NULL check in these functions. We also remove now unused function
    __local_flush_hugetlb_page().
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [mpe: Drop the checks with is_vm_hugetlb_page() as noticed by Nick]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index b8a0fb442c64..795d825c2edd 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -40,12 +40,6 @@ static inline void flush_hugetlb_page(struct vm_area_struct *vma,
 		return radix__flush_hugetlb_page(vma, vmaddr);
 }
 
-static inline void __local_flush_hugetlb_page(struct vm_area_struct *vma,
-					      unsigned long vmaddr)
-{
-	if (radix_enabled())
-		return radix__local_flush_hugetlb_page(vma, vmaddr);
-}
 #else
 
 static inline pte_t *hugepd_page(hugepd_t hpd)

commit 79cc38ded1e1ac86e69c90f604efadd50b0b3762
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Jul 28 10:31:26 2017 +0530

    powerpc/mm/hugetlb: Add support for reserving gigantic huge pages via kernel command line
    
    With commit aa888a74977a8 ("hugetlb: support larger than MAX_ORDER") we added
    support for allocating gigantic hugepages via kernel command line. Switch
    ppc64 arch specific code to use that.
    
    W.r.t FSL support, we now limit our allocation range using BOOTMEM_ALLOC_ACCESSIBLE.
    
    We use the kernel command line to do reservation of hugetlb pages on powernv
    platforms. On pseries hash mmu mode the supported gigantic huge page size is
    16GB and that can only be allocated with hypervisor assist. For pseries the
    command line option doesn't do the allocation. Instead pseries does gigantic
    hugepage allocation based on hypervisor hint that is specified via
    "ibm,expected#pages" property of the memory node.
    
    Cc: Scott Wood <oss@buserror.net>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 7f4025a6c69e..b8a0fb442c64 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -218,18 +218,4 @@ static inline pte_t *hugepte_offset(hugepd_t hpd, unsigned long addr,
 }
 #endif /* CONFIG_HUGETLB_PAGE */
 
-/*
- * FSL Book3E platforms require special gpage handling - the gpages
- * are reserved early in the boot process by memblock instead of via
- * the .dts as on IBM platforms.
- */
-#if defined(CONFIG_HUGETLB_PAGE) && (defined(CONFIG_PPC_FSL_BOOK3E) || \
-    defined(CONFIG_PPC_8xx))
-extern void __init reserve_hugetlb_gpages(void);
-#else
-static inline void reserve_hugetlb_gpages(void)
-{
-}
-#endif
-
 #endif /* _ASM_POWERPC_HUGETLB_H */

commit 20717e1ff52672e31f9399c45d88936bbbc7e175
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Dec 14 10:07:53 2016 +0530

    powerpc/mm: Fix little-endian 4K hugetlb
    
    When we switched to big endian page table, we never updated the hugepd
    format such that it can work for both big endian and little endian
    config. This patch series update hugepd format such that it is looked at
    as __be64 value in big endian page table config.
    
    This patch also switch hugepd_t.pd from signed long to unsigned long.
    I did update the FSL hugepd_ok check to check for the top bit instead
    of checking > 0.
    
    Fixes: 5dc1ef858c12 ("powerpc/mm: Use big endian Linux page tables for book3s 64")
    Cc: stable@vger.kernel.org # v4.7+
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index ede215167d1a..7f4025a6c69e 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -21,12 +21,12 @@ static inline pte_t *hugepd_page(hugepd_t hpd)
 	 * We have only four bits to encode, MMU page size
 	 */
 	BUILD_BUG_ON((MMU_PAGE_COUNT - 1) > 0xf);
-	return __va(hpd.pd & HUGEPD_ADDR_MASK);
+	return __va(hpd_val(hpd) & HUGEPD_ADDR_MASK);
 }
 
 static inline unsigned int hugepd_mmu_psize(hugepd_t hpd)
 {
-	return (hpd.pd & HUGEPD_SHIFT_MASK) >> 2;
+	return (hpd_val(hpd) & HUGEPD_SHIFT_MASK) >> 2;
 }
 
 static inline unsigned int hugepd_shift(hugepd_t hpd)
@@ -52,18 +52,20 @@ static inline pte_t *hugepd_page(hugepd_t hpd)
 {
 	BUG_ON(!hugepd_ok(hpd));
 #ifdef CONFIG_PPC_8xx
-	return (pte_t *)__va(hpd.pd & ~(_PMD_PAGE_MASK | _PMD_PRESENT_MASK));
+	return (pte_t *)__va(hpd_val(hpd) &
+			     ~(_PMD_PAGE_MASK | _PMD_PRESENT_MASK));
 #else
-	return (pte_t *)((hpd.pd & ~HUGEPD_SHIFT_MASK) | PD_HUGE);
+	return (pte_t *)((hpd_val(hpd) &
+			  ~HUGEPD_SHIFT_MASK) | PD_HUGE);
 #endif
 }
 
 static inline unsigned int hugepd_shift(hugepd_t hpd)
 {
 #ifdef CONFIG_PPC_8xx
-	return ((hpd.pd & _PMD_PAGE_MASK) >> 1) + 17;
+	return ((hpd_val(hpd) & _PMD_PAGE_MASK) >> 1) + 17;
 #else
-	return hpd.pd & HUGEPD_SHIFT_MASK;
+	return hpd_val(hpd) & HUGEPD_SHIFT_MASK;
 #endif
 }
 

commit c6f6634721c871bfab4235e1cbcad208d3063798
Merge: ff45000fcb56 baae856ebdee
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Dec 16 15:05:38 2016 +1100

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/scottwood/linux into next
    
    Freescale updates from Scott:
    
    "Highlights include 8xx hugepage support, qbman fixes/cleanup, device
    tree updates, and some misc cleanup."

commit 4b91428699477532ab1255c2dd5819713e9e8985
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Wed Dec 7 08:47:28 2016 +0100

    powerpc/8xx: Implement support of hugepages
    
    8xx uses a two level page table with two different linux page size
    support (4k and 16k). 8xx also support two different hugepage sizes
    512k and 8M. In order to support them on linux we define two different
    page table layout.
    
    The size of pages is in the PGD entry, using PS field (bits 28-29):
    00 : Small pages (4k or 16k)
    01 : 512k pages
    10 : reserved
    11 : 8M pages
    
    For 512K hugepage size a pgd entry have the below format
    [<hugepte address >0101] . The hugepte table allocated will contain 8
    entries pointing to 512K huge pte in 4k pages mode and 64 entries in
    16k pages mode.
    
    For 8M in 16k mode, a pgd entry have the below format
    [<hugepte address >1101] . The hugepte table allocated will contain 8
    entries pointing to 8M huge pte.
    
    For 8M in 4k mode, multiple pgd entries point to the same hugepte
    address and pgd entry will have the below format
    [<hugepte address>1101]. The hugepte table allocated will only have one
    entry.
    
    For the time being, we do not support CPU15 ERRATA when HUGETLB is
    selected
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com> (v3, for the generic bits)
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index c5517f463ec7..3facdd41709c 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -51,12 +51,20 @@ static inline void __local_flush_hugetlb_page(struct vm_area_struct *vma,
 static inline pte_t *hugepd_page(hugepd_t hpd)
 {
 	BUG_ON(!hugepd_ok(hpd));
+#ifdef CONFIG_PPC_8xx
+	return (pte_t *)__va(hpd.pd & ~(_PMD_PAGE_MASK | _PMD_PRESENT_MASK));
+#else
 	return (pte_t *)((hpd.pd & ~HUGEPD_SHIFT_MASK) | PD_HUGE);
+#endif
 }
 
 static inline unsigned int hugepd_shift(hugepd_t hpd)
 {
+#ifdef CONFIG_PPC_8xx
+	return ((hpd.pd & _PMD_PAGE_MASK) >> 1) + 17;
+#else
 	return hpd.pd & HUGEPD_SHIFT_MASK;
+#endif
 }
 
 #endif /* CONFIG_PPC_BOOK3S_64 */
@@ -99,7 +107,15 @@ static inline int is_hugepage_only_range(struct mm_struct *mm,
 
 void book3e_hugetlb_preload(struct vm_area_struct *vma, unsigned long ea,
 			    pte_t pte);
+#ifdef CONFIG_PPC_8xx
+static inline void flush_hugetlb_page(struct vm_area_struct *vma,
+				      unsigned long vmaddr)
+{
+	flush_tlb_page(vma, vmaddr);
+}
+#else
 void flush_hugetlb_page(struct vm_area_struct *vma, unsigned long vmaddr);
+#endif
 
 void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
 			    unsigned long end, unsigned long floor,
@@ -205,7 +221,8 @@ static inline pte_t *hugepte_offset(hugepd_t hpd, unsigned long addr,
  * are reserved early in the boot process by memblock instead of via
  * the .dts as on IBM platforms.
  */
-#if defined(CONFIG_HUGETLB_PAGE) && defined(CONFIG_PPC_FSL_BOOK3E)
+#if defined(CONFIG_HUGETLB_PAGE) && (defined(CONFIG_PPC_FSL_BOOK3E) || \
+    defined(CONFIG_PPC_8xx))
 extern void __init reserve_hugetlb_gpages(void);
 #else
 static inline void reserve_hugetlb_gpages(void)

commit bee8b3b56d1dfc4075254a61340ee3898a732e63
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Nov 28 11:46:58 2016 +0530

    powerpc/mm: Rename hugetlb-radix.h to hugetlb.h
    
    We will start moving some book3s specific hugetlb functions there.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index c5517f463ec7..c03e0a3dd4d8 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -9,7 +9,7 @@ extern struct kmem_cache *hugepte_cache;
 
 #ifdef CONFIG_PPC_BOOK3S_64
 
-#include <asm/book3s/64/hugetlb-radix.h>
+#include <asm/book3s/64/hugetlb.h>
 /*
  * This should work for other subarchs too. But right now we use the
  * new format only for 64bit book3s

commit 13dce033637b8e4d3266667c4df4d72fc37dc0a9
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Jul 13 15:06:38 2016 +0530

    powerpc/mm: Use hugetlb flush functions
    
    Use flush_hugetlb_page instead of flush_tlb_page when we clear flush the
    pte.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index e2d9f4996e5c..c5517f463ec7 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -147,7 +147,7 @@ static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
 {
 	pte_t pte;
 	pte = huge_ptep_get_and_clear(vma->vm_mm, addr, ptep);
-	flush_tlb_page(vma, addr);
+	flush_hugetlb_page(vma, addr);
 }
 
 static inline int huge_pte_none(pte_t pte)

commit 484837601d4dd048d1f8d2cedd39934bb3ad6d59
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:26:25 2016 +1000

    powerpc/mm: Add radix support for hugetlb
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 42814f0567cc..e2d9f4996e5c 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -8,6 +8,8 @@
 extern struct kmem_cache *hugepte_cache;
 
 #ifdef CONFIG_PPC_BOOK3S_64
+
+#include <asm/book3s/64/hugetlb-radix.h>
 /*
  * This should work for other subarchs too. But right now we use the
  * new format only for 64bit book3s
@@ -31,7 +33,19 @@ static inline unsigned int hugepd_shift(hugepd_t hpd)
 {
 	return mmu_psize_to_shift(hugepd_mmu_psize(hpd));
 }
+static inline void flush_hugetlb_page(struct vm_area_struct *vma,
+				      unsigned long vmaddr)
+{
+	if (radix_enabled())
+		return radix__flush_hugetlb_page(vma, vmaddr);
+}
 
+static inline void __local_flush_hugetlb_page(struct vm_area_struct *vma,
+					      unsigned long vmaddr)
+{
+	if (radix_enabled())
+		return radix__local_flush_hugetlb_page(vma, vmaddr);
+}
 #else
 
 static inline pte_t *hugepd_page(hugepd_t hpd)

commit c61a8843124e353f4ba27c073133868da00e0335
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Tue Feb 23 13:36:17 2016 +1100

    powerpc/mm/book3s-64: Use physical addresses in upper page table tree levels
    
    This changes the Linux page tables to store physical addresses
    rather than kernel virtual addresses in the upper levels of the
    tree (pgd, pud and pmd) for 64-bit Book 3S machines.
    
    This also changes the hugepd pointers used to implement hugepages
    when the base page size is 4k to store physical addresses rather than
    virtual addresses (again just for 64-bit Book3S machines).
    
    This frees up some high order bits, and will be needed with
    PowerISA v3.0 machines which read the page table tree in hardware
    in radix mode.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 7eac89b9f02e..42814f0567cc 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -19,7 +19,7 @@ static inline pte_t *hugepd_page(hugepd_t hpd)
 	 * We have only four bits to encode, MMU page size
 	 */
 	BUILD_BUG_ON((MMU_PAGE_COUNT - 1) > 0xf);
-	return (pte_t *)(hpd.pd & ~HUGEPD_SHIFT_MASK);
+	return __va(hpd.pd & HUGEPD_ADDR_MASK);
 }
 
 static inline unsigned int hugepd_mmu_psize(hugepd_t hpd)

commit 08bd4fc15683b9a26b9be7048d151f4ddadad96f
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu Jun 25 14:59:44 2015 -0700

    mm/hugetlb: remove arch_prepare/release_hugepage from arch headers
    
    Nobody used these hooks so they were removed from common code, and can now
    be removed from the architectures.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 4bbd3c8c2888..7eac89b9f02e 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -168,15 +168,6 @@ static inline pte_t huge_ptep_get(pte_t *ptep)
 	return *ptep;
 }
 
-static inline int arch_prepare_hugepage(struct page *page)
-{
-	return 0;
-}
-
-static inline void arch_release_hugepage(struct page *page)
-{
-}
-
 static inline void arch_clear_hugepage_flags(struct page *page)
 {
 }

commit a67a31fa308a9032ead31b0501dafdb44ccf5a12
Author: Zhang Zhen <zhenzhang.zhang@huawei.com>
Date:   Wed Jun 24 16:56:25 2015 -0700

    mm/hugetlb: reduce arch dependent code about hugetlb_prefault_arch_hook
    
    Currently we have many duplicates in definitions of
    hugetlb_prefault_arch_hook.  In all architectures this function is empty.
    
    Signed-off-by: Zhang Zhen <zhenzhang.zhang@huawei.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 1d53a65b4ec1..4bbd3c8c2888 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -112,11 +112,6 @@ static inline int prepare_hugepage_range(struct file *file,
 	return 0;
 }
 
-static inline void hugetlb_prefault_arch_hook(struct mm_struct *mm)
-{
-}
-
-
 static inline void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
 				   pte_t *ptep, pte_t pte)
 {

commit b30e759072c182538abb6908681cfd49978ba5e2
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Nov 5 21:57:41 2014 +0530

    powerpc/mm: Switch to generic RCU get_user_pages_fast
    
    This patch switch the ppc arch to use the generic RCU based
    gup implementation.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 766b77d527ac..1d53a65b4ec1 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -48,7 +48,7 @@ static inline unsigned int hugepd_shift(hugepd_t hpd)
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
 
-static inline pte_t *hugepte_offset(hugepd_t *hpdp, unsigned long addr,
+static inline pte_t *hugepte_offset(hugepd_t hpd, unsigned long addr,
 				    unsigned pdshift)
 {
 	/*
@@ -58,9 +58,9 @@ static inline pte_t *hugepte_offset(hugepd_t *hpdp, unsigned long addr,
 	 */
 	unsigned long idx = 0;
 
-	pte_t *dir = hugepd_page(*hpdp);
+	pte_t *dir = hugepd_page(hpd);
 #ifndef CONFIG_PPC_FSL_BOOK3E
-	idx = (addr & ((1UL << pdshift) - 1)) >> hugepd_shift(*hpdp);
+	idx = (addr & ((1UL << pdshift) - 1)) >> hugepd_shift(hpd);
 #endif
 
 	return dir + idx;
@@ -193,7 +193,7 @@ static inline void flush_hugetlb_page(struct vm_area_struct *vma,
 }
 
 #define hugepd_shift(x) 0
-static inline pte_t *hugepte_offset(hugepd_t *hpdp, unsigned long addr,
+static inline pte_t *hugepte_offset(hugepd_t hpd, unsigned long addr,
 				    unsigned pdshift)
 {
 	return 0;

commit ca5f1d16a51900d23d2cf834d3aa62dfe3a37881
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Oct 21 14:25:59 2014 +1100

    powerpc/mm: Remove redundant #if case
    
    Remove the check of CONFIG_PPC_SUBPAGE_PROT when deciding if
    is_hugepage_only_range() is extern or inline. The extern version is in
    slice.c and is built if CONFIG_PPC_MM_SLICES=y.
    
    There was no build break possible because CONFIG_PPC_SUBPAGE_PROT is
    only selectable under conditions which also mean CONFIG_PPC_MM_SLICES
    will be selected.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 623f2971ce0e..766b77d527ac 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -71,7 +71,7 @@ pte_t *huge_pte_offset_and_shift(struct mm_struct *mm,
 
 void flush_dcache_icache_hugepage(struct page *page);
 
-#if defined(CONFIG_PPC_MM_SLICES) || defined(CONFIG_PPC_SUBPAGE_PROT)
+#if defined(CONFIG_PPC_MM_SLICES)
 int is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,
 			   unsigned long len);
 #else

commit 88247e8d7ba6639f2c199e147ebbc91f7673150c
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Feb 12 09:13:36 2014 +0530

    powerpc/mm: Add new "set" flag argument to pte/pmd update function
    
    pte_update() is a powerpc-ism used to change the bits of a PTE
    when the access permission is being restricted (a flush is
    potentially needed).
    
    It uses atomic operations on when needed and handles the hash
    synchronization on hash based processors.
    
    It is currently only used to clear PTE bits and so the current
    implementation doesn't provide a way to also set PTE bits.
    
    The new _PAGE_NUMA bit, when set, is actually restricting access
    so it must use that function too, so this change adds the ability
    for pte_update() to also set bits.
    
    We will use this later to set the _PAGE_NUMA bit.
    
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index d750336b171d..623f2971ce0e 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -127,7 +127,7 @@ static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 					    unsigned long addr, pte_t *ptep)
 {
 #ifdef CONFIG_PPC64
-	return __pte(pte_update(mm, addr, ptep, ~0UL, 1));
+	return __pte(pte_update(mm, addr, ptep, ~0UL, 0, 1));
 #else
 	return __pte(pte_update(ptep, ~0UL, 0));
 #endif

commit 29409997f8d06d693d82127d200eeaf48989fdd2
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Jun 20 14:30:16 2013 +0530

    powerpc: move find_linux_pte_or_hugepte and gup_hugepte to common code
    
    We will use this in the later patch for handling THP pages
    
    Reviewed-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index f2498c8e595d..d750336b171d 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -191,8 +191,14 @@ static inline void flush_hugetlb_page(struct vm_area_struct *vma,
 				      unsigned long vmaddr)
 {
 }
-#endif /* CONFIG_HUGETLB_PAGE */
 
+#define hugepd_shift(x) 0
+static inline pte_t *hugepte_offset(hugepd_t *hpdp, unsigned long addr,
+				    unsigned pdshift)
+{
+	return 0;
+}
+#endif /* CONFIG_HUGETLB_PAGE */
 
 /*
  * FSL Book3E platforms require special gpage handling - the gpages

commit 5a148af66932c31814e263366094b5812210b501
Merge: 99c6bcf46d22 54d5999d98f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 2 10:16:16 2013 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc update from Benjamin Herrenschmidt:
     "The main highlights this time around are:
    
       - A pile of addition POWER8 bits and nits, such as updated
         performance counter support (Michael Ellerman), new branch history
         buffer support (Anshuman Khandual), base support for the new PCI
         host bridge when not using the hypervisor (Gavin Shan) and other
         random related bits and fixes from various contributors.
    
       - Some rework of our page table format by Aneesh Kumar which fixes a
         thing or two and paves the way for THP support.  THP itself will
         not make it this time around however.
    
       - More Freescale updates, including Altivec support on the new e6500
         cores, new PCI controller support, and a pile of new boards support
         and updates.
    
       - The usual batch of trivial cleanups & fixes"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (156 commits)
      powerpc: Fix build error for book3e
      powerpc: Context switch the new EBB SPRs
      powerpc: Turn on the EBB H/FSCR bits
      powerpc: Replace CPU_FTR_BCTAR with CPU_FTR_ARCH_207S
      powerpc: Setup BHRB instructions facility in HFSCR for POWER8
      powerpc: Fix interrupt range check on debug exception
      powerpc: Update tlbie/tlbiel as per ISA doc
      powerpc: Print page size info during boot
      powerpc: print both base and actual page size on hash failure
      powerpc: Fix hpte_decode to use the correct decoding for page sizes
      powerpc: Decode the pte-lp-encoding bits correctly.
      powerpc: Use encode avpn where we need only avpn values
      powerpc: Reduce PTE table memory wastage
      powerpc: Move the pte free routines from common header
      powerpc: Reduce the PTE_INDEX_SIZE
      powerpc: Switch 16GB and 16MB explicit hugepages to a different page table format
      powerpc: New hugepage directory format
      powerpc: Don't truncate pgd_index wrongly
      powerpc: Don't hard code the size of pte page
      powerpc: Save DAR and DSISR in pt_regs on MCE
      ...

commit cf9427b85e90bb1ff90e2397ff419691d983c68b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Sun Apr 28 09:37:29 2013 +0000

    powerpc: New hugepage directory format
    
    Change the hugepage directory format so that we can have leaf ptes directly
    at page directory avoiding the allocation of hugepage directory.
    
    With the new table format we have 3 cases for pgds and pmds:
    (1) invalid (all zeroes)
    (2) pointer to next table, as normal; bottom 6 bits == 0
    (4) hugepd pointer, bottom two bits == 00, next 4 bits indicate size of table
    
    Instead of storing shift value in hugepd pointer we use mmu_psize_def index
    so that we can fit all the supported hugepage size in 4 bits
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 62e11a32c4c2..4daf7e684f58 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -6,6 +6,33 @@
 
 extern struct kmem_cache *hugepte_cache;
 
+#ifdef CONFIG_PPC_BOOK3S_64
+/*
+ * This should work for other subarchs too. But right now we use the
+ * new format only for 64bit book3s
+ */
+static inline pte_t *hugepd_page(hugepd_t hpd)
+{
+	BUG_ON(!hugepd_ok(hpd));
+	/*
+	 * We have only four bits to encode, MMU page size
+	 */
+	BUILD_BUG_ON((MMU_PAGE_COUNT - 1) > 0xf);
+	return (pte_t *)(hpd.pd & ~HUGEPD_SHIFT_MASK);
+}
+
+static inline unsigned int hugepd_mmu_psize(hugepd_t hpd)
+{
+	return (hpd.pd & HUGEPD_SHIFT_MASK) >> 2;
+}
+
+static inline unsigned int hugepd_shift(hugepd_t hpd)
+{
+	return mmu_psize_to_shift(hugepd_mmu_psize(hpd));
+}
+
+#else
+
 static inline pte_t *hugepd_page(hugepd_t hpd)
 {
 	BUG_ON(!hugepd_ok(hpd));
@@ -17,6 +44,9 @@ static inline unsigned int hugepd_shift(hugepd_t hpd)
 	return hpd.pd & HUGEPD_SHIFT_MASK;
 }
 
+#endif /* CONFIG_PPC_BOOK3S_64 */
+
+
 static inline pte_t *hugepte_offset(hugepd_t *hpdp, unsigned long addr,
 				    unsigned pdshift)
 {

commit 106c992a5ebef28193cf5958e49ceff5e4aebb04
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Mon Apr 29 15:07:23 2013 -0700

    mm/hugetlb: add more arch-defined huge_pte functions
    
    Commit abf09bed3cce ("s390/mm: implement software dirty bits")
    introduced another difference in the pte layout vs.  the pmd layout on
    s390, thoroughly breaking the s390 support for hugetlbfs.  This requires
    replacing some more pte_xxx functions in mm/hugetlbfs.c with a
    huge_pte_xxx version.
    
    This patch introduces those huge_pte_xxx functions and their generic
    implementation in asm-generic/hugetlb.h, which will now be included on
    all architectures supporting hugetlbfs apart from s390.  This change
    will be a no-op for those architectures.
    
    [akpm@linux-foundation.org: fix warning]
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Acked-by: Michal Hocko <mhocko@suse.cz> [for !s390 parts]
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 62e11a32c4c2..4fcbd6b14a3a 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -3,6 +3,7 @@
 
 #ifdef CONFIG_HUGETLB_PAGE
 #include <asm/page.h>
+#include <asm-generic/hugetlb.h>
 
 extern struct kmem_cache *hugepte_cache;
 

commit 5d3a551c28c6669dc43be40d8fafafbc2ec8f42b
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Oct 8 16:29:32 2012 -0700

    mm: hugetlb: add arch hook for clearing page flags before entering pool
    
    The core page allocator ensures that page flags are zeroed when freeing
    pages via free_pages_check.  A number of architectures (ARM, PPC, MIPS)
    rely on this property to treat new pages as dirty with respect to the data
    cache and perform the appropriate flushing before mapping the pages into
    userspace.
    
    This can lead to cache synchronisation problems when using hugepages,
    since the allocator keeps its own pool of pages above the usual page
    allocator and does not reset the page flags when freeing a page into the
    pool.
    
    This patch adds a new architecture hook, arch_clear_hugepage_flags, so
    that architectures which rely on the page flags being in a particular
    state for fresh allocations can adjust the flags accordingly when a page
    is freed into the pool.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index dfdb95bc59a5..62e11a32c4c2 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -151,6 +151,10 @@ static inline void arch_release_hugepage(struct page *page)
 {
 }
 
+static inline void arch_clear_hugepage_flags(struct page *page)
+{
+}
+
 #else /* ! CONFIG_HUGETLB_PAGE */
 static inline void flush_hugetlb_page(struct vm_area_struct *vma,
 				      unsigned long vmaddr)

commit 1f6820b4c1d09b017625ce32ba23fa39ebfdb27a
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Tue Nov 29 15:10:39 2011 +0000

    powerpc: Define/use HUGETLB_NEED_PRELOAD insead of complicated #if
    
    Define HUGETLB_NEED_PRELOAD in mmu-book3e.h for CONFIG_PPC64 instead
    of having a much more complicated #if block.  This is easier to read
    and maintain.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 863f49d5ea35..dfdb95bc59a5 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -124,8 +124,7 @@ static inline int huge_ptep_set_access_flags(struct vm_area_struct *vma,
 					     unsigned long addr, pte_t *ptep,
 					     pte_t pte, int dirty)
 {
-#if defined(CONFIG_PPC_MMU_NOHASH) && \
-	!(defined(CONFIG_PPC_FSL_BOOK3E) && defined(CONFIG_PPC32))
+#ifdef HUGETLB_NEED_PRELOAD
 	/*
 	 * The "return 1" forces a call of update_mmu_cache, which will write a
 	 * TLB entry.  Without this, platforms that don't do a write of the TLB

commit d93e4d7d72037d8c9405e8d404ecb2ee162adc25
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Mon Nov 28 14:43:33 2011 +0000

    powerpc/book3e: Change hugetlb preload to take vma argument
    
    This avoids an extra find_vma() and is less error-prone.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 555044c310b2..863f49d5ea35 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -52,7 +52,8 @@ static inline int is_hugepage_only_range(struct mm_struct *mm,
 }
 #endif
 
-void book3e_hugetlb_preload(struct mm_struct *mm, unsigned long ea, pte_t pte);
+void book3e_hugetlb_preload(struct vm_area_struct *vma, unsigned long ea,
+			    pte_t pte);
 void flush_hugetlb_page(struct vm_area_struct *vma, unsigned long vmaddr);
 
 void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,

commit a6146888be0aa80ea41c99178d7d2e08efc776b5
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Mon Oct 10 10:50:43 2011 +0000

    powerpc: Add gpages reservation code for 64-bit FSL BOOKE
    
    For 64-bit FSL_BOOKE implementations, gigantic pages need to be
    reserved at boot time by the memblock code based on the command line.
    This adds the call that handles the reservation, and fixes some code
    comments.
    
    It also removes the previous pr_err when reserve_hugetlb_gpages
    is called on a system without hugetlb enabled - the way the code is
    structured, the call is unconditional and the resulting error message
    spurious and confusing.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 273acfad65ae..555044c310b2 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -5,7 +5,6 @@
 #include <asm/page.h>
 
 extern struct kmem_cache *hugepte_cache;
-extern void __init reserve_hugetlb_gpages(void);
 
 static inline pte_t *hugepd_page(hugepd_t hpd)
 {
@@ -153,14 +152,24 @@ static inline void arch_release_hugepage(struct page *page)
 }
 
 #else /* ! CONFIG_HUGETLB_PAGE */
-static inline void reserve_hugetlb_gpages(void)
-{
-	pr_err("Cannot reserve gpages without hugetlb enabled\n");
-}
 static inline void flush_hugetlb_page(struct vm_area_struct *vma,
 				      unsigned long vmaddr)
 {
 }
+#endif /* CONFIG_HUGETLB_PAGE */
+
+
+/*
+ * FSL Book3E platforms require special gpage handling - the gpages
+ * are reserved early in the boot process by memblock instead of via
+ * the .dts as on IBM platforms.
+ */
+#if defined(CONFIG_HUGETLB_PAGE) && defined(CONFIG_PPC_FSL_BOOK3E)
+extern void __init reserve_hugetlb_gpages(void);
+#else
+static inline void reserve_hugetlb_gpages(void)
+{
+}
 #endif
 
 #endif /* _ASM_POWERPC_HUGETLB_H */

commit 881fde1db591628db0494e77cd9002b0ba8b04b7
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Mon Oct 10 10:50:40 2011 +0000

    powerpc: hugetlb: modify include usage for FSL BookE code
    
    The original 32-bit hugetlb implementation used PPC64 vs PPC32 to
    determine which code path to take.  However, the final hugetlb
    implementation for 64-bit FSL ended up shared with the FSL
    32-bit code so the actual check needs to be FSL_BOOK3E vs
    everything else.  This patch changes the include protections to
    reflect this.
    
    There are also a couple of related comment fixes.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 70f9885f5c0f..273acfad65ae 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -22,14 +22,14 @@ static inline pte_t *hugepte_offset(hugepd_t *hpdp, unsigned long addr,
 				    unsigned pdshift)
 {
 	/*
-	 * On 32-bit, we have multiple higher-level table entries that point to
-	 * the same hugepte.  Just use the first one since they're all
+	 * On FSL BookE, we have multiple higher-level table entries that
+	 * point to the same hugepte.  Just use the first one since they're all
 	 * identical.  So for that case, idx=0.
 	 */
 	unsigned long idx = 0;
 
 	pte_t *dir = hugepd_page(*hpdp);
-#ifdef CONFIG_PPC64
+#ifndef CONFIG_PPC_FSL_BOOK3E
 	idx = (addr & ((1UL << pdshift) - 1)) >> hugepd_shift(*hpdp);
 #endif
 

commit 97632e6fbea5b996669ffee21d869ed09848e1ec
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Mon Oct 10 10:50:37 2011 +0000

    powerpc: hugetlb: fix huge_ptep_set_access_flags return value
    
    There was an unconditional return of "1" in the original code
    from David Gibson, and I dropped it because it wasn't needed
    for FSL BOOKE 32-bit.  However, not all systems (including 64-bit
    FSL BOOKE) do loading of the hpte from the fault handler asm
    and depend on this function returning 1, which causes a call
    to update_mmu_cache() that writes an entry into the tlb.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 86004930a78e..70f9885f5c0f 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -124,7 +124,18 @@ static inline int huge_ptep_set_access_flags(struct vm_area_struct *vma,
 					     unsigned long addr, pte_t *ptep,
 					     pte_t pte, int dirty)
 {
+#if defined(CONFIG_PPC_MMU_NOHASH) && \
+	!(defined(CONFIG_PPC_FSL_BOOK3E) && defined(CONFIG_PPC32))
+	/*
+	 * The "return 1" forces a call of update_mmu_cache, which will write a
+	 * TLB entry.  Without this, platforms that don't do a write of the TLB
+	 * entry in the TLB miss handler asm will fault ad infinitum.
+	 */
+	ptep_set_access_flags(vma, addr, ptep, pte, dirty);
+	return 1;
+#else
 	return ptep_set_access_flags(vma, addr, ptep, pte, dirty);
+#endif
 }
 
 static inline pte_t huge_ptep_get(pte_t *ptep)

commit 41151e77a4d96ea138cede6d84c955aa4769ce74
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Tue Jun 28 09:54:48 2011 +0000

    powerpc: Hugetlb for BookE
    
    Enable hugepages on Freescale BookE processors.  This allows the kernel to
    use huge TLB entries to map pages, which can greatly reduce the number of
    TLB misses and the amount of TLB thrashing experienced by applications with
    large memory footprints.  Care should be taken when using this on FSL
    processors, as the number of large TLB entries supported by the core is low
    (16-64) on current processors.
    
    The supported set of hugepage sizes include 4m, 16m, 64m, 256m, and 1g.
    Page sizes larger than the max zone size are called "gigantic" pages and
    must be allocated on the command line (and cannot be deallocated).
    
    This is currently only fully implemented for Freescale 32-bit BookE
    processors, but there is some infrastructure in the code for
    64-bit BooKE.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 5856a66ab404..86004930a78e 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -1,15 +1,60 @@
 #ifndef _ASM_POWERPC_HUGETLB_H
 #define _ASM_POWERPC_HUGETLB_H
 
+#ifdef CONFIG_HUGETLB_PAGE
 #include <asm/page.h>
 
+extern struct kmem_cache *hugepte_cache;
+extern void __init reserve_hugetlb_gpages(void);
+
+static inline pte_t *hugepd_page(hugepd_t hpd)
+{
+	BUG_ON(!hugepd_ok(hpd));
+	return (pte_t *)((hpd.pd & ~HUGEPD_SHIFT_MASK) | PD_HUGE);
+}
+
+static inline unsigned int hugepd_shift(hugepd_t hpd)
+{
+	return hpd.pd & HUGEPD_SHIFT_MASK;
+}
+
+static inline pte_t *hugepte_offset(hugepd_t *hpdp, unsigned long addr,
+				    unsigned pdshift)
+{
+	/*
+	 * On 32-bit, we have multiple higher-level table entries that point to
+	 * the same hugepte.  Just use the first one since they're all
+	 * identical.  So for that case, idx=0.
+	 */
+	unsigned long idx = 0;
+
+	pte_t *dir = hugepd_page(*hpdp);
+#ifdef CONFIG_PPC64
+	idx = (addr & ((1UL << pdshift) - 1)) >> hugepd_shift(*hpdp);
+#endif
+
+	return dir + idx;
+}
+
 pte_t *huge_pte_offset_and_shift(struct mm_struct *mm,
 				 unsigned long addr, unsigned *shift);
 
 void flush_dcache_icache_hugepage(struct page *page);
 
+#if defined(CONFIG_PPC_MM_SLICES) || defined(CONFIG_PPC_SUBPAGE_PROT)
 int is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,
 			   unsigned long len);
+#else
+static inline int is_hugepage_only_range(struct mm_struct *mm,
+					 unsigned long addr,
+					 unsigned long len)
+{
+	return 0;
+}
+#endif
+
+void book3e_hugetlb_preload(struct mm_struct *mm, unsigned long ea, pte_t pte);
+void flush_hugetlb_page(struct vm_area_struct *vma, unsigned long vmaddr);
 
 void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
 			    unsigned long end, unsigned long floor,
@@ -50,8 +95,11 @@ static inline void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
 static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 					    unsigned long addr, pte_t *ptep)
 {
-	unsigned long old = pte_update(mm, addr, ptep, ~0UL, 1);
-	return __pte(old);
+#ifdef CONFIG_PPC64
+	return __pte(pte_update(mm, addr, ptep, ~0UL, 1));
+#else
+	return __pte(pte_update(ptep, ~0UL, 0));
+#endif
 }
 
 static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
@@ -93,4 +141,15 @@ static inline void arch_release_hugepage(struct page *page)
 {
 }
 
+#else /* ! CONFIG_HUGETLB_PAGE */
+static inline void reserve_hugetlb_gpages(void)
+{
+	pr_err("Cannot reserve gpages without hugetlb enabled\n");
+}
+static inline void flush_hugetlb_page(struct vm_area_struct *vma,
+				      unsigned long vmaddr)
+{
+}
+#endif
+
 #endif /* _ASM_POWERPC_HUGETLB_H */

commit 0895ecda79428df48501e48dd0a868e0c8e1aae2
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Mon Oct 26 19:24:31 2009 +0000

    powerpc/mm: Bring hugepage PTE accessor functions back into sync with normal accessors
    
    The hugepage arch code provides a number of hook functions/macros
    which mirror the functionality of various normal page pte access
    functions.  Various changes in the normal page accessors (in
    particular BenH's recent changes to the handling of lazy icache
    flushing and PAGE_EXEC) have caused the hugepage versions to get out
    of sync with the originals.  In some cases, this is a bug, at least on
    some MMU types.
    
    One of the reasons that some hooks were not identical to the normal
    page versions, is that the fact we're dealing with a hugepage needed
    to be passed down do use the correct dcache-icache flush function.
    This patch makes the main flush_dcache_icache_page() function hugepage
    aware (by checking for the PageCompound flag).  That in turn means we
    can make set_huge_pte_at() just a call to set_pte_at() bringing it
    back into sync.  As a bonus, this lets us remove the
    hash_huge_page_do_lazy_icache() function, replacing it with a call to
    the hash_page_do_lazy_icache() function it was based on.
    
    Some other hugepage pte access hooks - huge_ptep_get_and_clear() and
    huge_ptep_clear_flush() - are not so easily unified, but this patch at
    least brings them back into sync with the current versions of the
    corresponding normal page functions.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 038886834da5..5856a66ab404 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -6,6 +6,8 @@
 pte_t *huge_pte_offset_and_shift(struct mm_struct *mm,
 				 unsigned long addr, unsigned *shift);
 
+void flush_dcache_icache_hugepage(struct page *page);
+
 int is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,
 			   unsigned long len);
 
@@ -13,12 +15,6 @@ void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
 			    unsigned long end, unsigned long floor,
 			    unsigned long ceiling);
 
-void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
-		     pte_t *ptep, pte_t pte);
-
-pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
-			      pte_t *ptep);
-
 /*
  * The version of vma_mmu_pagesize() in arch/powerpc/mm/hugetlbpage.c needs
  * to override the version in mm/hugetlb.c
@@ -44,9 +40,26 @@ static inline void hugetlb_prefault_arch_hook(struct mm_struct *mm)
 {
 }
 
+
+static inline void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
+				   pte_t *ptep, pte_t pte)
+{
+	set_pte_at(mm, addr, ptep, pte);
+}
+
+static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
+					    unsigned long addr, pte_t *ptep)
+{
+	unsigned long old = pte_update(mm, addr, ptep, ~0UL, 1);
+	return __pte(old);
+}
+
 static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
 					 unsigned long addr, pte_t *ptep)
 {
+	pte_t pte;
+	pte = huge_ptep_get_and_clear(vma->vm_mm, addr, ptep);
+	flush_tlb_page(vma, addr);
 }
 
 static inline int huge_pte_none(pte_t pte)

commit 883a3e523672ebba2ec3969837ba02af4f70fae2
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Mon Oct 26 19:24:31 2009 +0000

    powerpc/mm: Split hash MMU specific hugepage code into a new file
    
    This patch separates the parts of hugetlbpage.c which are inherently
    specific to the hash MMU into a new hugelbpage-hash64.c file.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index a4f08f10fe1f..038886834da5 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -3,6 +3,9 @@
 
 #include <asm/page.h>
 
+pte_t *huge_pte_offset_and_shift(struct mm_struct *mm,
+				 unsigned long addr, unsigned *shift);
+
 int is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,
 			   unsigned long len);
 

commit a4fe3ce7699bfe1bd88f816b55d42d8fe1dac655
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Mon Oct 26 19:24:31 2009 +0000

    powerpc/mm: Allow more flexible layouts for hugepage pagetables
    
    Currently each available hugepage size uses a slightly different
    pagetable layout: that is, the bottem level table of pointers to
    hugepages is a different size, and may branch off from the normal page
    tables at a different level.  Every hugepage aware path that needs to
    walk the pagetables must therefore look up the hugepage size from the
    slice info first, and work out the correct way to walk the pagetables
    accordingly.  Future hardware is likely to add more possible hugepage
    sizes, more layout options and more mess.
    
    This patch, therefore reworks the handling of hugepage pagetables to
    reduce this complexity.  In the new scheme, instead of having to
    consult the slice mask, pagetable walking code can check a flag in the
    PGD/PUD/PMD entries to see where to branch off to hugepage pagetables,
    and the entry also contains the information (eseentially hugepage
    shift) necessary to then interpret that table without recourse to the
    slice mask.  This scheme can be extended neatly to handle multiple
    levels of self-describing "special" hugepage pagetables, although for
    now we assume only one level exists.
    
    This approach means that only the pagetable allocation path needs to
    know how the pagetables should be set out.  All other (hugepage)
    pagetable walking paths can just interpret the structure as they go.
    
    There already was a flag bit in PGD/PUD/PMD entries for hugepage
    directory pointers, but it was only used for debug.  We alter that
    flag bit to instead be a 0 in the MSB to indicate a hugepage pagetable
    pointer (normally it would be 1 since the pointer lies in the linear
    mapping).  This means that asm pagetable walking can test for (and
    punt on) hugepage pointers with the same test that checks for
    unpopulated page directory entries (beq becomes bge), since hugepage
    pointers will always be positive, and normal pointers always negative.
    
    While we're at it, we get rid of the confusing (and grep defeating)
    #defining of hugepte_shift to be the same thing as mmu_huge_psizes.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index b1dafb6a9743..a4f08f10fe1f 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -3,7 +3,6 @@
 
 #include <asm/page.h>
 
-
 int is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,
 			   unsigned long len);
 

commit 3340289ddf29ca75c3acfb3a6b72f234b2f74d5c
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Jan 6 14:38:54 2009 -0800

    mm: report the MMU pagesize in /proc/pid/smaps
    
    The KernelPageSize entry in /proc/pid/smaps is the pagesize used by the
    kernel to back a VMA.  This matches the size used by the MMU in the
    majority of cases.  However, one counter-example occurs on PPC64 kernels
    whereby a kernel using 64K as a base pagesize may still use 4K pages for
    the MMU on older processor.  To distinguish, this patch reports
    MMUPageSize as the pagesize used by the MMU in /proc/pid/smaps.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: "KOSAKI Motohiro" <kosaki.motohiro@jp.fujitsu.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
index 26f0d0ab27a5..b1dafb6a9743 100644
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -17,6 +17,12 @@ void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
 pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep);
 
+/*
+ * The version of vma_mmu_pagesize() in arch/powerpc/mm/hugetlbpage.c needs
+ * to override the version in mm/hugetlb.c
+ */
+#define vma_mmu_pagesize vma_mmu_pagesize
+
 /*
  * If the arch doesn't supply something else, assume that hugepage
  * size aligned regions are ok without further preparation.

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/hugetlb.h b/arch/powerpc/include/asm/hugetlb.h
new file mode 100644
index 000000000000..26f0d0ab27a5
--- /dev/null
+++ b/arch/powerpc/include/asm/hugetlb.h
@@ -0,0 +1,75 @@
+#ifndef _ASM_POWERPC_HUGETLB_H
+#define _ASM_POWERPC_HUGETLB_H
+
+#include <asm/page.h>
+
+
+int is_hugepage_only_range(struct mm_struct *mm, unsigned long addr,
+			   unsigned long len);
+
+void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
+			    unsigned long end, unsigned long floor,
+			    unsigned long ceiling);
+
+void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
+		     pte_t *ptep, pte_t pte);
+
+pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
+			      pte_t *ptep);
+
+/*
+ * If the arch doesn't supply something else, assume that hugepage
+ * size aligned regions are ok without further preparation.
+ */
+static inline int prepare_hugepage_range(struct file *file,
+			unsigned long addr, unsigned long len)
+{
+	struct hstate *h = hstate_file(file);
+	if (len & ~huge_page_mask(h))
+		return -EINVAL;
+	if (addr & ~huge_page_mask(h))
+		return -EINVAL;
+	return 0;
+}
+
+static inline void hugetlb_prefault_arch_hook(struct mm_struct *mm)
+{
+}
+
+static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
+					 unsigned long addr, pte_t *ptep)
+{
+}
+
+static inline int huge_pte_none(pte_t pte)
+{
+	return pte_none(pte);
+}
+
+static inline pte_t huge_pte_wrprotect(pte_t pte)
+{
+	return pte_wrprotect(pte);
+}
+
+static inline int huge_ptep_set_access_flags(struct vm_area_struct *vma,
+					     unsigned long addr, pte_t *ptep,
+					     pte_t pte, int dirty)
+{
+	return ptep_set_access_flags(vma, addr, ptep, pte, dirty);
+}
+
+static inline pte_t huge_ptep_get(pte_t *ptep)
+{
+	return *ptep;
+}
+
+static inline int arch_prepare_hugepage(struct page *page)
+{
+	return 0;
+}
+
+static inline void arch_release_hugepage(struct page *page)
+{
+}
+
+#endif /* _ASM_POWERPC_HUGETLB_H */
