commit 4cdb2da654033d76e1b1cb4ac427d9193dce816b
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Apr 20 18:36:38 2020 +0000

    powerpc: Remove _ALIGN_UP(), _ALIGN_DOWN() and _ALIGN()
    
    These three powerpc macros have been replaced by
    equivalent generic macros and are not used anymore.
    
    Remove them.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Joel Stanley <joel@jms.id.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/bb0a6081f7b95ee64ca20f92483e5b9661cbacb2.1587407777.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 3ee8df0f66e0..a63fe6f3a0ff 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -249,13 +249,6 @@ static inline bool pfn_valid(unsigned long pfn)
 #include <asm/page_32.h>
 #endif
 
-/* align addr on a size boundary - adjust address up/down if needed */
-#define _ALIGN_UP(addr, size)   __ALIGN_KERNEL(addr, size)
-#define _ALIGN_DOWN(addr, size)	((addr)&(~((typeof(addr))(size)-1)))
-
-/* align addr on a size boundary - adjust address up if needed */
-#define _ALIGN(addr,size)     _ALIGN_UP(addr,size)
-
 /*
  * Don't compare things with KERNELBASE or PAGE_OFFSET to test for
  * "kernelness", use is_kernel_addr() - it should do what you want.

commit c62da0c35d58518ddb26ff641d2485596567fd96
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Fri Apr 10 14:33:05 2020 -0700

    mm/vma: define a default value for VM_DATA_DEFAULT_FLAGS
    
    There are many platforms with exact same value for VM_DATA_DEFAULT_FLAGS
    This creates a default value for VM_DATA_DEFAULT_FLAGS in line with the
    existing VM_STACK_DEFAULT_FLAGS.  While here, also define some more
    macros with standard VMA access flag combinations that are used
    frequently across many platforms.  Apart from simplification, this
    reduces code duplication as well.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paulburton@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Chris Zankel <chris@zankel.net>
    Link: http://lkml.kernel.org/r/1583391014-8170-2-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 080a0bf8e54b..3ee8df0f66e0 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -240,13 +240,8 @@ static inline bool pfn_valid(unsigned long pfn)
  * and needs to be executable.  This means the whole heap ends
  * up being executable.
  */
-#define VM_DATA_DEFAULT_FLAGS32 \
-	(((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0) | \
-				 VM_READ | VM_WRITE | \
-				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
-
-#define VM_DATA_DEFAULT_FLAGS64	(VM_READ | VM_WRITE | \
-				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+#define VM_DATA_DEFAULT_FLAGS32	VM_DATA_FLAGS_TSK_EXEC
+#define VM_DATA_DEFAULT_FLAGS64	VM_DATA_FLAGS_NON_EXEC
 
 #ifdef __powerpc64__
 #include <asm/page_64.h>

commit 50a175dd18de7a647e72aca7daf4744e3a5a81e3
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sun Feb 9 16:02:41 2020 +0000

    powerpc/hugetlb: Fix 8M hugepages on 8xx
    
    With HW assistance all page tables must be 4k aligned, the 8xx drops
    the last 12 bits during the walk.
    
    Redefine HUGEPD_SHIFT_MASK to mask last 12 bits out. HUGEPD_SHIFT_MASK
    is used to for alignment of page table cache.
    
    Fixes: 22569b881d37 ("powerpc/8xx: Enable 8M hugepage support with HW assistance")
    Cc: stable@vger.kernel.org # v5.0+
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/778b1a248c4c7ca79640eeff7740044da6a220a0.1581264115.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 86332080399a..080a0bf8e54b 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -295,8 +295,13 @@ static inline bool pfn_valid(unsigned long pfn)
 /*
  * Some number of bits at the level of the page table that points to
  * a hugepte are used to encode the size.  This masks those bits.
+ * On 8xx, HW assistance requires 4k alignment for the hugepte.
  */
+#ifdef CONFIG_PPC_8xx
+#define HUGEPD_SHIFT_MASK     0xfff
+#else
 #define HUGEPD_SHIFT_MASK     0x3f
+#endif
 
 #ifndef __ASSEMBLY__
 

commit 6ad4afc97bc6c5cca9786030492ddfab871ce79e
Author: Bai Yingjie <byj.tea@gmail.com>
Date:   Mon Jan 6 12:29:53 2020 +0800

    powerpc32/booke: consistently return phys_addr_t in __pa()
    
    When CONFIG_RELOCATABLE=y is set, VIRT_PHYS_OFFSET is a 64bit variable,
    thus __pa() returns as 64bit value.
    But when CONFIG_RELOCATABLE=n, __pa() returns 32bit value.
    
    When CONFIG_PHYS_64BIT is set, __pa() should consistently return as
    64bit value irrelevant to CONFIG_RELOCATABLE.
    So we'd make __pa() consistently return phys_addr_t, which is 64bit
    when CONFIG_PHYS_64BIT is set.
    
    Signed-off-by: Bai Yingjie <byj.tea@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200106042957.26494-1-yingjie_bai@126.com

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 7f1fd41e3065..86332080399a 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -209,7 +209,7 @@ static inline bool pfn_valid(unsigned long pfn)
  */
 #if defined(CONFIG_PPC32) && defined(CONFIG_BOOKE)
 #define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) + VIRT_PHYS_OFFSET))
-#define __pa(x) ((unsigned long)(x) - VIRT_PHYS_OFFSET)
+#define __pa(x) ((phys_addr_t)(unsigned long)(x) - VIRT_PHYS_OFFSET)
 #else
 #ifdef CONFIG_PPC64
 /*

commit 7794b1d4185e2587af46435e3e2f6696dae314c7
Merge: 9dd0013824fc 2807273f5e88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 30 14:35:43 2019 -0800

    Merge tag 'powerpc-5.5-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights:
    
       - Infrastructure for secure boot on some bare metal Power9 machines.
         The firmware support is still in development, so the code here
         won't actually activate secure boot on any existing systems.
    
       - A change to xmon (our crash handler / pseudo-debugger) to restrict
         it to read-only mode when the kernel is lockdown'ed, otherwise it's
         trivial to drop into xmon and modify kernel data, such as the
         lockdown state.
    
       - Support for KASLR on 32-bit BookE machines (Freescale / NXP).
    
       - Fixes for our flush_icache_range() and __kernel_sync_dicache()
         (VDSO) to work with memory ranges >4GB.
    
       - Some reworks of the pseries CMM (Cooperative Memory Management)
         driver to make it behave more like other balloon drivers and enable
         some cleanups of generic mm code.
    
       - A series of fixes to our hardware breakpoint support to properly
         handle unaligned watchpoint addresses.
    
      Plus a bunch of other smaller improvements, fixes and cleanups.
    
      Thanks to: Alastair D'Silva, Andrew Donnellan, Aneesh Kumar K.V,
      Anthony Steinhauser, CÃ©dric Le Goater, Chris Packham, Chris Smart,
      Christophe Leroy, Christopher M. Riedl, Christoph Hellwig, Claudio
      Carvalho, Daniel Axtens, David Hildenbrand, Deb McLemore, Diana
      Craciun, Eric Richter, Geert Uytterhoeven, Greg Kroah-Hartman, Greg
      Kurz, Gustavo L. F. Walbon, Hari Bathini, Harish, Jason Yan, Krzysztof
      Kozlowski, Leonardo Bras, Mathieu Malaterre, Mauro S. M. Rodrigues,
      Michal Suchanek, Mimi Zohar, Nathan Chancellor, Nathan Lynch, Nayna
      Jain, Nick Desaulniers, Oliver O'Halloran, Qian Cai, Rasmus Villemoes,
      Ravi Bangoria, Sam Bobroff, Santosh Sivaraj, Scott Wood, Thomas Huth,
      Tyrel Datwyler, Vaibhav Jain, Valentin Longchamp, YueHaibing"
    
    * tag 'powerpc-5.5-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (144 commits)
      powerpc/fixmap: fix crash with HIGHMEM
      x86/efi: remove unused variables
      powerpc: Define arch_is_kernel_initmem_freed() for lockdep
      powerpc/prom_init: Use -ffreestanding to avoid a reference to bcmp
      powerpc: Avoid clang warnings around setjmp and longjmp
      powerpc: Don't add -mabi= flags when building with Clang
      powerpc: Fix Kconfig indentation
      powerpc/fixmap: don't clear fixmap area in paging_init()
      selftests/powerpc: spectre_v2 test must be built 64-bit
      powerpc/powernv: Disable native PCIe port management
      powerpc/kexec: Move kexec files into a dedicated subdir.
      powerpc/32: Split kexec low level code out of misc_32.S
      powerpc/sysdev: drop simple gpio
      powerpc/83xx: map IMMR with a BAT.
      powerpc/32s: automatically allocate BAT in setbat()
      powerpc/ioremap: warn on early use of ioremap()
      powerpc: Add support for GENERIC_EARLY_IOREMAP
      powerpc/fixmap: Use __fix_to_virt() instead of fix_to_virt()
      powerpc/8xx: use the fixmapped IMMR in cpm_reset()
      powerpc/8xx: add __init to cpm1 init functions
      ...

commit 921a79b7802078fab3787c7eae561536906cb8f3
Author: Jason Yan <yanaijie@huawei.com>
Date:   Fri Sep 20 17:45:44 2019 +0800

    powerpc/fsl_booke/kaslr: dump out kernel offset information on panic
    
    When kaslr is enabled, the kernel offset is different for every boot.
    This brings some difficult to debug the kernel. Dump out the kernel
    offset when panic so that we can easily debug the kernel.
    
    This code is derived from x86/arm64 which has similar functionality.
    
    Signed-off-by: Jason Yan <yanaijie@huawei.com>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Diana Craciun <diana.craciun@nxp.com>
    Tested-by: Diana Craciun <diana.craciun@nxp.com>
    Signed-off-by: Scott Wood <oss@buserror.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 88fa53f89f5a..f2f3ed5a8969 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -327,6 +327,11 @@ struct vm_area_struct;
 
 extern unsigned long kernstart_virt_addr;
 
+static inline unsigned long kaslr_offset(void)
+{
+	return kernstart_virt_addr - KERNELBASE;
+}
+
 #include <asm-generic/memory_model.h>
 #endif /* __ASSEMBLY__ */
 #include <asm/slice.h>

commit 39f4b7bf7571a9c6529b0bb3de49c9bb0998f194
Author: Jason Yan <yanaijie@huawei.com>
Date:   Fri Sep 20 17:45:37 2019 +0800

    powerpc: introduce kernstart_virt_addr to store the kernel base
    
    Now the kernel base is a fixed value - KERNELBASE. To support KASLR, we
    need a variable to store the kernel base.
    
    Signed-off-by: Jason Yan <yanaijie@huawei.com>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Diana Craciun <diana.craciun@nxp.com>
    Tested-by: Diana Craciun <diana.craciun@nxp.com>
    Signed-off-by: Scott Wood <oss@buserror.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index c8bb14ff4713..88fa53f89f5a 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -325,6 +325,8 @@ void arch_free_page(struct page *page, int order);
 
 struct vm_area_struct;
 
+extern unsigned long kernstart_virt_addr;
+
 #include <asm-generic/memory_model.h>
 #endif /* __ASSEMBLY__ */
 #include <asm/slice.h>

commit 8b5369ea580964dbc982781bfb9fb93459fc5e8d
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Mon Oct 14 20:31:03 2019 +0200

    dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
    
    Some architectures, notably ARM, are interested in tweaking this
    depending on their runtime DMA addressing limitations.
    
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index c8bb14ff4713..f6c562acc3f8 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -329,13 +329,4 @@ struct vm_area_struct;
 #endif /* __ASSEMBLY__ */
 #include <asm/slice.h>
 
-/*
- * Allow 30-bit DMA for very limited Broadcom wifi chips on many powerbooks.
- */
-#ifdef CONFIG_PPC32
-#define ARCH_ZONE_DMA_BITS 30
-#else
-#define ARCH_ZONE_DMA_BITS 31
-#endif
-
 #endif /* _ASM_POWERPC_PAGE_H */

commit 4dd7554a6456d124c85e0a4ad156625b71390b5c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Jul 24 18:46:37 2019 +1000

    powerpc/64: Add VIRTUAL_BUG_ON checks for __va and __pa addresses
    
    Ensure __va is given a physical address below PAGE_OFFSET, and __pa is
    given a virtual address above PAGE_OFFSET.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190724084638.24982-4-npiggin@gmail.com

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 0d52f57fca04..c8bb14ff4713 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -215,9 +215,19 @@ static inline bool pfn_valid(unsigned long pfn)
 /*
  * gcc miscompiles (unsigned long)(&static_var) - PAGE_OFFSET
  * with -mcmodel=medium, so we use & and | instead of - and + on 64-bit.
+ * This also results in better code generation.
  */
-#define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) | PAGE_OFFSET))
-#define __pa(x) ((unsigned long)(x) & 0x0fffffffffffffffUL)
+#define __va(x)								\
+({									\
+	VIRTUAL_BUG_ON((unsigned long)(x) >= PAGE_OFFSET);		\
+	(void *)(unsigned long)((phys_addr_t)(x) | PAGE_OFFSET);	\
+})
+
+#define __pa(x)								\
+({									\
+	VIRTUAL_BUG_ON((unsigned long)(x) < PAGE_OFFSET);		\
+	(unsigned long)(x) & 0x0fffffffffffffffUL;			\
+})
 
 #else /* 32-bit, non book E */
 #define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) + PAGE_OFFSET - MEMORY_START))

commit a8282bf087bcfb348ad97c8ed1f457bc11fd9709
Merge: 693cd8ce3f88 500871125920
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 22 09:09:42 2019 -0700

    Merge tag 'powerpc-5.2-5' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc fixes from Michael Ellerman:
     "This is a frustratingly large batch at rc5. Some of these were sent
      earlier but were missed by me due to being distracted by other things,
      and some took a while to track down due to needing manual bisection on
      old hardware. But still we clearly need to improve our testing of KVM,
      and of 32-bit, so that we catch these earlier.
    
      Summary: seven fixes, all for bugs introduced this cycle.
    
       - The commit to add KASAN support broke booting on 32-bit SMP
         machines, due to a refactoring that moved some setup out of the
         secondary CPU path.
    
       - A fix for another 32-bit SMP bug introduced by the fast syscall
         entry implementation for 32-bit BOOKE. And a build fix for the same
         commit.
    
       - Our change to allow the DAWR to be force enabled on Power9
         introduced a bug in KVM, where we clobber r3 leading to a host
         crash.
    
       - The same commit also exposed a previously unreachable bug in the
         nested KVM handling of DAWR, which could lead to an oops in a
         nested host.
    
       - One of the DMA reworks broke the b43legacy WiFi driver on some
         people's powermacs, fix it by enabling a 30-bit ZONE_DMA on 32-bit.
    
       - A fix for TLB flushing in KVM introduced a new bug, as it neglected
         to also flush the ERAT, this could lead to memory corruption in the
         guest.
    
      Thanks to: Aaro Koskinen, Christoph Hellwig, Christophe Leroy, Larry
      Finger, Michael Neuling, Suraj Jitindar Singh"
    
    * tag 'powerpc-5.2-5' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:
      KVM: PPC: Book3S HV: Invalidate ERAT when flushing guest TLB entries
      powerpc: enable a 30-bit ZONE_DMA for 32-bit pmac
      KVM: PPC: Book3S HV: Only write DAWR[X] when handling h_set_dawr in real mode
      KVM: PPC: Book3S HV: Fix r3 corruption in h_set_dabr()
      powerpc/32: fix build failure on book3e with KVM
      powerpc/booke: fix fast syscall entry on SMP
      powerpc/32s: fix initial setup of segment registers on secondary CPU

commit 9739ab7eda459f0669ec9807e0d9be5020bab88c
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 13 10:24:46 2019 +0200

    powerpc: enable a 30-bit ZONE_DMA for 32-bit pmac
    
    With the strict dma mask checking introduced with the switch to
    the generic DMA direct code common wifi chips on 32-bit powerbooks
    stopped working.  Add a 30-bit ZONE_DMA to the 32-bit pmac builds
    to allow them to reliably allocate dma coherent memory.
    
    Fixes: 65a21b71f948 ("powerpc/dma: remove dma_nommu_dma_supported")
    Reported-by: Aaro Koskinen <aaro.koskinen@iki.fi>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Larry Finger <Larry.Finger@lwfinger.net>
    Acked-by: Larry Finger <Larry.Finger@lwfinger.net>
    Tested-by: Aaro Koskinen <aaro.koskinen@iki.fi>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index dbc8c0679480..3d013e4696e9 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -323,6 +323,13 @@ struct vm_area_struct;
 #endif /* __ASSEMBLY__ */
 #include <asm/slice.h>
 
+/*
+ * Allow 30-bit DMA for very limited Broadcom wifi chips on many powerbooks.
+ */
+#ifdef CONFIG_PPC32
+#define ARCH_ZONE_DMA_BITS 30
+#else
 #define ARCH_ZONE_DMA_BITS 31
+#endif
 
 #endif /* _ASM_POWERPC_PAGE_H */

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index dbc8c0679480..b8286a2013b4 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -1,13 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 #ifndef _ASM_POWERPC_PAGE_H
 #define _ASM_POWERPC_PAGE_H
 
 /*
  * Copyright (C) 2001,2005 IBM Corporation.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #ifndef __ASSEMBLY__

commit c5710cd20735037ba9be0e95530f0d3795ce07e6
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 05:59:48 2019 +0000

    powerpc/mm: cleanup HPAGE_SHIFT setup
    
    Only book3s/64 may select default among several HPAGE_SHIFT at runtime.
    8xx always defines 512K pages as default
    FSL_BOOK3E always defines 4M pages as default
    
    This patch limits HUGETLB_PAGE_SIZE_VARIABLE to book3s/64
    moves the definitions in subarches files.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 6b508420d92b..dbc8c0679480 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -28,10 +28,15 @@
 #define PAGE_SIZE		(ASM_CONST(1) << PAGE_SHIFT)
 
 #ifndef __ASSEMBLY__
-#ifdef CONFIG_HUGETLB_PAGE
-extern unsigned int HPAGE_SHIFT;
-#else
+#ifndef CONFIG_HUGETLB_PAGE
 #define HPAGE_SHIFT PAGE_SHIFT
+#elif defined(CONFIG_PPC_BOOK3S_64)
+extern unsigned int hpage_shift;
+#define HPAGE_SHIFT hpage_shift
+#elif defined(CONFIG_PPC_8xx)
+#define HPAGE_SHIFT		19	/* 512k pages */
+#elif defined(CONFIG_PPC_FSL_BOOK3E)
+#define HPAGE_SHIFT		22	/* 4M pages */
 #endif
 #define HPAGE_SIZE		((1UL) << HPAGE_SHIFT)
 #define HPAGE_MASK		(~(HPAGE_SIZE - 1))

commit 45d0ba527b575d47b2be75dd517b57cceda04bfe
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 05:59:47 2019 +0000

    powerpc/mm: move hugetlb_disabled into asm/hugetlb.h
    
    No need to have this in asm/page.h, move it into asm/hugetlb.h
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 748f5db2e2b7..6b508420d92b 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -29,7 +29,6 @@
 
 #ifndef __ASSEMBLY__
 #ifdef CONFIG_HUGETLB_PAGE
-extern bool hugetlb_disabled;
 extern unsigned int HPAGE_SHIFT;
 #else
 #define HPAGE_SHIFT PAGE_SHIFT

commit 53ed7a5947de2e19c270a0bc0c29257c6d004b0f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Apr 17 18:29:16 2019 +0530

    powerpc/mm: Drop the unnecessary region check
    
    All the regions are now mapped with top nibble 0xc. Hence the region id
    check is not needed for virt_addr_valid()
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 918228f2205b..748f5db2e2b7 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -132,19 +132,7 @@ static inline bool pfn_valid(unsigned long pfn)
 #define virt_to_page(kaddr)	pfn_to_page(virt_to_pfn(kaddr))
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 
-#ifdef CONFIG_PPC_BOOK3S_64
-/*
- * On hash the vmalloc and other regions alias to the kernel region when passed
- * through __pa(), which virt_to_pfn() uses. That means virt_addr_valid() can
- * return true for some vmalloc addresses, which is incorrect. So explicitly
- * check that the address is in the kernel region.
- */
-/* may be can drop get_region_id */
-#define virt_addr_valid(kaddr) (get_region_id((unsigned long)kaddr) == KERNEL_REGION_ID && \
-				pfn_valid(virt_to_pfn(kaddr)))
-#else
 #define virt_addr_valid(kaddr)	pfn_valid(virt_to_pfn(kaddr))
-#endif
 
 /*
  * On Book-E parts we need __va to parse the device tree and we can't

commit 0034d395f89d9c092bb15adbabdca5283e258b41
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Apr 17 18:29:14 2019 +0530

    powerpc/mm/hash64: Map all the kernel regions in the same 0xc range
    
    This patch maps vmalloc, IO and vmemap regions in the 0xc address range
    instead of the current 0xd and 0xf range. This brings the mapping closer
    to radix translation mode.
    
    With hash 64K page size each of this region is 512TB whereas with 4K config
    we are limited by the max page table range of 64TB and hence there regions
    are of 16TB size.
    
    The kernel mapping is now:
    
     On 4K hash
    
         kernel_region_map_size = 16TB
         kernel vmalloc start   = 0xc000100000000000
         kernel IO start        = 0xc000200000000000
         kernel vmemmap start   = 0xc000300000000000
    
    64K hash, 64K radix and 4k radix:
    
         kernel_region_map_size = 512TB
         kernel vmalloc start   = 0xc008000000000000
         kernel IO start        = 0xc00a000000000000
         kernel vmemmap start   = 0xc00c000000000000
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index ed870468ef6f..918228f2205b 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -139,7 +139,8 @@ static inline bool pfn_valid(unsigned long pfn)
  * return true for some vmalloc addresses, which is incorrect. So explicitly
  * check that the address is in the kernel region.
  */
-#define virt_addr_valid(kaddr) (REGION_ID(kaddr) == KERNEL_REGION_ID && \
+/* may be can drop get_region_id */
+#define virt_addr_valid(kaddr) (get_region_id((unsigned long)kaddr) == KERNEL_REGION_ID && \
 				pfn_valid(virt_to_pfn(kaddr)))
 #else
 #define virt_addr_valid(kaddr)	pfn_valid(virt_to_pfn(kaddr))

commit 555f4fdb93e70d39e664fcc52cda23c5b62a46cc
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Feb 21 19:08:46 2019 +0000

    powerpc/kconfig: define PAGE_SHIFT inside Kconfig
    
    This patch defined CONFIG_PPC_PAGE_SHIFT in order
    to be able to use PAGE_SHIFT value inside Kconfig.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index aa4497175bd3..ed870468ef6f 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -20,20 +20,11 @@
 
 /*
  * On regular PPC32 page size is 4K (but we support 4K/16K/64K/256K pages
- * on PPC44x). For PPC64 we support either 4K or 64K software
+ * on PPC44x and 4K/16K on 8xx). For PPC64 we support either 4K or 64K software
  * page size. When using 64K pages however, whether we are really supporting
  * 64K pages in HW or not is irrelevant to those definitions.
  */
-#if defined(CONFIG_PPC_256K_PAGES)
-#define PAGE_SHIFT		18
-#elif defined(CONFIG_PPC_64K_PAGES)
-#define PAGE_SHIFT		16
-#elif defined(CONFIG_PPC_16K_PAGES)
-#define PAGE_SHIFT		14
-#else
-#define PAGE_SHIFT		12
-#endif
-
+#define PAGE_SHIFT		CONFIG_PPC_PAGE_SHIFT
 #define PAGE_SIZE		(ASM_CONST(1) << PAGE_SHIFT)
 
 #ifndef __ASSEMBLY__

commit 26b523356f49a0117c8f9e32ca98aa6d6e496e1a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Feb 1 10:46:52 2019 +0000

    powerpc: Drop page_is_ram() and walk_system_ram_range()
    
    Since commit c40dd2f76644 ("powerpc: Add System RAM to /proc/iomem")
    it is possible to use the generic walk_system_ram_range() and
    the generic page_is_ram().
    
    To enable the use of walk_system_ram_range() by the IBM EHEA ethernet
    driver, we still need an export of the generic function.
    
    As powerpc was the only user of CONFIG_ARCH_HAS_WALK_MEMORY, the
    ifdef around the generic walk_system_ram_range() has become useless
    and can be dropped.
    
    Fixes: c40dd2f76644 ("powerpc: Add System RAM to /proc/iomem")
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Keep the EXPORT_SYMBOL_GPL in powerpc code]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 5c5ea2413413..aa4497175bd3 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -326,7 +326,6 @@ struct page;
 extern void clear_user_page(void *page, unsigned long vaddr, struct page *pg);
 extern void copy_user_page(void *to, void *from, unsigned long vaddr,
 		struct page *p);
-extern int page_is_ram(unsigned long pfn);
 extern int devmem_is_allowed(unsigned long pfn);
 
 #ifdef CONFIG_PPC_SMLPAR

commit 25078dc1f74be16b858e914f52cc8f4d03c2271a
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Dec 16 17:53:49 2018 +0100

    powerpc: use mm zones more sensibly
    
    Powerpc has somewhat odd usage where ZONE_DMA is used for all memory on
    common 64-bit configfs, and ZONE_DMA32 is used for 31-bit schemes.
    
    Move to a scheme closer to what other architectures use (and I dare to
    say the intent of the system):
    
     - ZONE_DMA: optionally for memory < 31-bit (64-bit embedded only)
     - ZONE_NORMAL: everything addressable by the kernel
     - ZONE_HIGHMEM: memory > 32-bit for 32-bit kernels
    
    Also provide information on how ZONE_DMA is used by defining
    ARCH_ZONE_DMA_BITS.
    
    Contains various fixes from Benjamin Herrenschmidt.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index a7624a3b1435..5c5ea2413413 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -340,4 +340,6 @@ struct vm_area_struct;
 #endif /* __ASSEMBLY__ */
 #include <asm/slice.h>
 
+#define ARCH_ZONE_DMA_BITS 31
+
 #endif /* _ASM_POWERPC_PAGE_H */

commit d09780f3a8d48fd49136d7bae8f0ae30de7f261a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Nov 29 14:06:57 2018 +0000

    powerpc/mm: Move pgtable_t into platform headers
    
    This patch move pgtable_t into platform headers.
    
    It gets rid of the CONFIG_PPC_64K_PAGES case for PPC64
    as nohash/64 doesn't support CONFIG_PPC_64K_PAGES.
    
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 9ea903221a9f..a7624a3b1435 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -335,20 +335,6 @@ void arch_free_page(struct page *page, int order);
 #endif
 
 struct vm_area_struct;
-#ifdef CONFIG_PPC_BOOK3S_64
-/*
- * For BOOK3s 64 with 4k and 64K linux page size
- * we want to use pointers, because the page table
- * actually store pfn
- */
-typedef pte_t *pgtable_t;
-#else
-#if defined(CONFIG_PPC_64K_PAGES) && defined(CONFIG_PPC64)
-typedef pte_t *pgtable_t;
-#else
-typedef struct page *pgtable_t;
-#endif
-#endif
 
 #include <asm-generic/memory_model.h>
 #endif /* __ASSEMBLY__ */

commit d456f3529a7a980ca7410c62e9e3496d828858f2
Author: Daniel Axtens <dja@axtens.net>
Date:   Wed Nov 7 01:48:03 2018 +1100

    powerpc: mark 64-bit PD_HUGE constant as unsigned long
    
    When compiled for 64-bit, the PD_HUGE constant is a 64-bit integer.
    Mark it as an unsigned long.
    
    This squashes over a thousand sparse warnings on my minimal T4240RDB
    (e6500, ppc64be) config, of the following 2 forms:
    
    arch/powerpc/include/asm/hugetlb.h:52:49: warning: constant 0x8000000000000000 is so big it is unsigned long
    arch/powerpc/include/asm/nohash/pgtable.h:269:49: warning: constant 0x8000000000000000 is so big it is unsigned long
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index f6a1265face2..9ea903221a9f 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -289,7 +289,7 @@ static inline bool pfn_valid(unsigned long pfn)
  * page tables at arbitrary addresses, this breaks and will have to change.
  */
 #ifdef CONFIG_PPC64
-#define PD_HUGE 0x8000000000000000
+#define PD_HUGE 0x8000000000000000UL
 #else
 #define PD_HUGE 0x80000000
 #endif

commit ec0c464cdbf38bf6ddabec8bfa595bd421cab203
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jul 5 16:24:57 2018 +0000

    powerpc: move ASM_CONST and stringify_in_c() into asm-const.h
    
    This patch moves ASM_CONST() and stringify_in_c() into
    dedicated asm-const.h, then cleans all related inclusions.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: asm-compat.h should include asm-const.h]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index a9fbefaacf10..f6a1265face2 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -16,7 +16,7 @@
 #else
 #include <asm/types.h>
 #endif
-#include <asm/asm-compat.h>
+#include <asm/asm-const.h>
 
 /*
  * On regular PPC32 page size is 4K (but we support 4K/16K/64K/256K pages

commit db0a2b633da4216b767d7aed95ffe30d37409c7a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jul 5 16:24:51 2018 +0000

    powerpc: remove kdump.h from page.h
    
    page.h doesn't need kdump.h
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index db7be0779d55..a9fbefaacf10 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -17,7 +17,6 @@
 #include <asm/types.h>
 #endif
 #include <asm/asm-compat.h>
-#include <asm/kdump.h>
 
 /*
  * On regular PPC32 page size is 4K (but we support 4K/16K/64K/256K pages

commit 8597538712ebd90bc83dfb0b3b40398a0c53ad5b
Author: Hari Bathini <hbathini@linux.vnet.ibm.com>
Date:   Tue Apr 10 19:11:31 2018 +0530

    powerpc/fadump: Do not use hugepages when fadump is active
    
    FADump capture kernel boots in restricted memory environment preserving
    the context of previous kernel to save vmcore. Supporting hugepages in
    such environment makes things unnecessarily complicated, as hugepages
    need memory set aside for them. This means most of the capture kernel's
    memory is used in supporting hugepages. In most cases, this results in
    out-of-memory issues while booting FADump capture kernel. But hugepages
    are not of much use in capture kernel whose only job is to save vmcore.
    So, disabling hugepages support, when fadump is active, is a reliable
    solution for the out of memory issues. Introducing a flag variable to
    disable HugeTLB support when fadump is active.
    
    Signed-off-by: Hari Bathini <hbathini@linux.vnet.ibm.com>
    Reviewed-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index dec9ce5ba8af..db7be0779d55 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -39,6 +39,7 @@
 
 #ifndef __ASSEMBLY__
 #ifdef CONFIG_HUGETLB_PAGE
+extern bool hugetlb_disabled;
 extern unsigned int HPAGE_SHIFT;
 #else
 #define HPAGE_SHIFT PAGE_SHIFT

commit 603b892200e653dd7e86a0e4a315561534d97441
Author: Mathieu Malaterre <malat@debian.org>
Date:   Wed Mar 7 21:34:35 2018 +0100

    powerpc: Avoid comparison of unsigned long >= 0 in pfn_valid()
    
    Rewrite comparison since all values compared are of type `unsigned long`.
    
    Instead of using unsigned properties and rewriting the original code as:
    (originally suggested by Segher Boessenkool <segher@kernel.crashing.org>)
    
      #define pfn_valid(pfn) \
                   (((pfn) - ARCH_PFN_OFFSET) < (max_mapnr - ARCH_PFN_OFFSET))
    
    Prefer a static inline function to make code as readable as possible.
    
    Fix a warning (treated as error in W=1):
      arch/powerpc/include/asm/page.h:129:32: error: comparison of unsigned expression >= 0 is always true [-Werror=type-limits]
      #define pfn_valid(pfn)  ((pfn) >= ARCH_PFN_OFFSET && (pfn) < max_mapnr)
                                      ^
    
    Suggested-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index d5f1c41b7dba..dec9ce5ba8af 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -126,7 +126,15 @@ extern long long virt_phys_offset;
 
 #ifdef CONFIG_FLATMEM
 #define ARCH_PFN_OFFSET		((unsigned long)(MEMORY_START >> PAGE_SHIFT))
-#define pfn_valid(pfn)		((pfn) >= ARCH_PFN_OFFSET && (pfn) < max_mapnr)
+#ifndef __ASSEMBLY__
+extern unsigned long max_mapnr;
+static inline bool pfn_valid(unsigned long pfn)
+{
+	unsigned long min_pfn = ARCH_PFN_OFFSET;
+
+	return pfn >= min_pfn && pfn < max_mapnr;
+}
+#endif
 #endif
 
 #define virt_to_pfn(kaddr)	(__pa(kaddr) >> PAGE_SHIFT)

commit a3286f05bc5a5bc7fc73a9783ec89de78fcd07f8
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Feb 22 15:27:22 2018 +0100

    powerpc/mm/slice: create header files dedicated to slices
    
    In preparation for the following patch which will enhance 'slices'
    for supporting PPC32 in order to fix an issue on hugepages on 8xx,
    this patch takes out of page*.h all bits related to 'slices' and put
    them into newly created slice.h header files.
    While common parts go into asm/slice.h, subarch specific
    parts go into respective books3s/64/slice.c and nohash/64/slice.c
    'slices'
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 8da5d4c1cab2..d5f1c41b7dba 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -344,5 +344,6 @@ typedef struct page *pgtable_t;
 
 #include <asm-generic/memory_model.h>
 #endif /* __ASSEMBLY__ */
+#include <asm/slice.h>
 
 #endif /* _ASM_POWERPC_PAGE_H */

commit e41e53cd4fe331d0d1f06f8e4ed7e2cc63ee2c34
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu May 18 20:37:31 2017 +1000

    powerpc/mm: Fix virt_addr_valid() etc. on 64-bit hash
    
    virt_addr_valid() is supposed to tell you if it's OK to call virt_to_page() on
    an address. What this means in practice is that it should only return true for
    addresses in the linear mapping which are backed by a valid PFN.
    
    We are failing to properly check that the address is in the linear mapping,
    because virt_to_pfn() will return a valid looking PFN for more or less any
    address. That bug is actually caused by __pa(), used in virt_to_pfn().
    
    eg: __pa(0xc000000000010000) = 0x10000  # Good
        __pa(0xd000000000010000) = 0x10000  # Bad!
        __pa(0x0000000000010000) = 0x10000  # Bad!
    
    This started happening after commit bdbc29c19b26 ("powerpc: Work around gcc
    miscompilation of __pa() on 64-bit") (Aug 2013), where we changed the definition
    of __pa() to work around a GCC bug. Prior to that we subtracted PAGE_OFFSET from
    the value passed to __pa(), meaning __pa() of a 0xd or 0x0 address would give
    you something bogus back.
    
    Until we can verify if that GCC bug is no longer an issue, or come up with
    another solution, this commit does the minimal fix to make virt_addr_valid()
    work, by explicitly checking that the address is in the linear mapping region.
    
    Fixes: bdbc29c19b26 ("powerpc: Work around gcc miscompilation of __pa() on 64-bit")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: Balbir Singh <bsingharora@gmail.com>
    Tested-by: Breno Leitao <breno.leitao@gmail.com>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 2a32483c7b6c..8da5d4c1cab2 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -132,7 +132,19 @@ extern long long virt_phys_offset;
 #define virt_to_pfn(kaddr)	(__pa(kaddr) >> PAGE_SHIFT)
 #define virt_to_page(kaddr)	pfn_to_page(virt_to_pfn(kaddr))
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
+
+#ifdef CONFIG_PPC_BOOK3S_64
+/*
+ * On hash the vmalloc and other regions alias to the kernel region when passed
+ * through __pa(), which virt_to_pfn() uses. That means virt_addr_valid() can
+ * return true for some vmalloc addresses, which is incorrect. So explicitly
+ * check that the address is in the kernel region.
+ */
+#define virt_addr_valid(kaddr) (REGION_ID(kaddr) == KERNEL_REGION_ID && \
+				pfn_valid(virt_to_pfn(kaddr)))
+#else
 #define virt_addr_valid(kaddr)	pfn_valid(virt_to_pfn(kaddr))
+#endif
 
 /*
  * On Book-E parts we need __va to parse the device tree and we can't

commit 16e72e9b30986ee15f17fbb68189ca842c32af58
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Wed Feb 22 15:45:16 2017 -0800

    powerpc: do not make the entire heap executable
    
    On 32-bit powerpc the ELF PLT sections of binaries (built with
    --bss-plt, or with a toolchain which defaults to it) look like this:
    
      [17] .sbss             NOBITS          0002aff8 01aff8 000014 00  WA  0   0  4
      [18] .plt              NOBITS          0002b00c 01aff8 000084 00 WAX  0   0  4
      [19] .bss              NOBITS          0002b090 01aff8 0000a4 00  WA  0   0  4
    
    Which results in an ELF load header:
    
      Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
      LOAD           0x019c70 0x00029c70 0x00029c70 0x01388 0x014c4 RWE 0x10000
    
    This is all correct, the load region containing the PLT is marked as
    executable.  Note that the PLT starts at 0002b00c but the file mapping
    ends at 0002aff8, so the PLT falls in the 0 fill section described by
    the load header, and after a page boundary.
    
    Unfortunately the generic ELF loader ignores the X bit in the load
    headers when it creates the 0 filled non-file backed mappings.  It
    assumes all of these mappings are RW BSS sections, which is not the case
    for PPC.
    
    gcc/ld has an option (--secure-plt) to not do this, this is said to
    incur a small performance penalty.
    
    Currently, to support 32-bit binaries with PLT in BSS kernel maps
    *entire brk area* with executable rights for all binaries, even
    --secure-plt ones.
    
    Stop doing that.
    
    Teach the ELF loader to check the X bit in the relevant load header and
    create 0 filled anonymous mappings that are executable if the load
    header requests that.
    
    Test program showing the difference in /proc/$PID/maps:
    
    int main() {
            char buf[16*1024];
            char *p = malloc(123); /* make "[heap]" mapping appear */
            int fd = open("/proc/self/maps", O_RDONLY);
            int len = read(fd, buf, sizeof(buf));
            write(1, buf, len);
            printf("%p\n", p);
            return 0;
    }
    
    Compiled using: gcc -mbss-plt -m32 -Os test.c -otest
    
    Unpatched ppc64 kernel:
    00100000-00120000 r-xp 00000000 00:00 0                                  [vdso]
    0fe10000-0ffd0000 r-xp 00000000 fd:00 67898094                           /usr/lib/libc-2.17.so
    0ffd0000-0ffe0000 r--p 001b0000 fd:00 67898094                           /usr/lib/libc-2.17.so
    0ffe0000-0fff0000 rw-p 001c0000 fd:00 67898094                           /usr/lib/libc-2.17.so
    10000000-10010000 r-xp 00000000 fd:00 100674505                          /home/user/test
    10010000-10020000 r--p 00000000 fd:00 100674505                          /home/user/test
    10020000-10030000 rw-p 00010000 fd:00 100674505                          /home/user/test
    10690000-106c0000 rwxp 00000000 00:00 0                                  [heap]
    f7f70000-f7fa0000 r-xp 00000000 fd:00 67898089                           /usr/lib/ld-2.17.so
    f7fa0000-f7fb0000 r--p 00020000 fd:00 67898089                           /usr/lib/ld-2.17.so
    f7fb0000-f7fc0000 rw-p 00030000 fd:00 67898089                           /usr/lib/ld-2.17.so
    ffa90000-ffac0000 rw-p 00000000 00:00 0                                  [stack]
    0x10690008
    
    Patched ppc64 kernel:
    00100000-00120000 r-xp 00000000 00:00 0                                  [vdso]
    0fe10000-0ffd0000 r-xp 00000000 fd:00 67898094                           /usr/lib/libc-2.17.so
    0ffd0000-0ffe0000 r--p 001b0000 fd:00 67898094                           /usr/lib/libc-2.17.so
    0ffe0000-0fff0000 rw-p 001c0000 fd:00 67898094                           /usr/lib/libc-2.17.so
    10000000-10010000 r-xp 00000000 fd:00 100674505                          /home/user/test
    10010000-10020000 r--p 00000000 fd:00 100674505                          /home/user/test
    10020000-10030000 rw-p 00010000 fd:00 100674505                          /home/user/test
    10180000-101b0000 rw-p 00000000 00:00 0                                  [heap]
                      ^^^^ this has changed
    f7c60000-f7c90000 r-xp 00000000 fd:00 67898089                           /usr/lib/ld-2.17.so
    f7c90000-f7ca0000 r--p 00020000 fd:00 67898089                           /usr/lib/ld-2.17.so
    f7ca0000-f7cb0000 rw-p 00030000 fd:00 67898089                           /usr/lib/ld-2.17.so
    ff860000-ff890000 rw-p 00000000 00:00 0                                  [stack]
    0x10180008
    
    The patch was originally posted in 2012 by Jason Gunthorpe
    and apparently ignored:
    
    https://lkml.org/lkml/2012/9/30/138
    
    Lightly run-tested.
    
    Link: http://lkml.kernel.org/r/20161215131950.23054-1-dvlasenk@redhat.com
    Signed-off-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Tested-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Florian Weimer <fweimer@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 47120bf2670c..2a32483c7b6c 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -230,7 +230,9 @@ extern long long virt_phys_offset;
  * and needs to be executable.  This means the whole heap ends
  * up being executable.
  */
-#define VM_DATA_DEFAULT_FLAGS32	(VM_READ | VM_WRITE | VM_EXEC | \
+#define VM_DATA_DEFAULT_FLAGS32 \
+	(((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0) | \
+				 VM_READ | VM_WRITE | \
 				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
 #define VM_DATA_DEFAULT_FLAGS64	(VM_READ | VM_WRITE | \

commit 20717e1ff52672e31f9399c45d88936bbbc7e175
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Dec 14 10:07:53 2016 +0530

    powerpc/mm: Fix little-endian 4K hugetlb
    
    When we switched to big endian page table, we never updated the hugepd
    format such that it can work for both big endian and little endian
    config. This patch series update hugepd format such that it is looked at
    as __be64 value in big endian page table config.
    
    This patch also switch hugepd_t.pd from signed long to unsigned long.
    I did update the FSL hugepd_ok check to check for the top bit instead
    of checking > 0.
    
    Fixes: 5dc1ef858c12 ("powerpc/mm: Use big endian Linux page tables for book3s 64")
    Cc: stable@vger.kernel.org # v4.7+
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 56398e7e6100..47120bf2670c 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -294,15 +294,12 @@ extern long long virt_phys_offset;
 #include <asm/pgtable-types.h>
 #endif
 
-typedef struct { signed long pd; } hugepd_t;
 
 #ifndef CONFIG_HUGETLB_PAGE
 #define is_hugepd(pdep)		(0)
 #define pgd_huge(pgd)		(0)
 #endif /* CONFIG_HUGETLB_PAGE */
 
-#define __hugepd(x) ((hugepd_t) { (x) })
-
 struct page;
 extern void clear_user_page(void *page, unsigned long vaddr, struct page *pg);
 extern void copy_user_page(void *to, void *from, unsigned long vaddr,

commit 27d1149667352772240655b65372a4294f992ea7
Author: Kevin Hao <haokexin@gmail.com>
Date:   Wed Jul 13 09:14:40 2016 +0800

    powerpc/32: Remove RELOCATABLE_PPC32
    
    It is seldom used in the kernel code and can be easily replaced by
    either RELOCATABLE or PPC32. So there is no reason to keep a separate
    kernel option for this.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 51db3a37bced..56398e7e6100 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -96,7 +96,7 @@ extern unsigned int HPAGE_SHIFT;
 extern phys_addr_t memstart_addr;
 extern phys_addr_t kernstart_addr;
 
-#ifdef CONFIG_RELOCATABLE_PPC32
+#if defined(CONFIG_RELOCATABLE) && defined(CONFIG_PPC32)
 extern long long virt_phys_offset;
 #endif
 
@@ -139,9 +139,9 @@ extern long long virt_phys_offset;
  * determine MEMORY_START until then.  However we can determine PHYSICAL_START
  * from information at hand (program counter, TLB lookup).
  *
- * On BookE with RELOCATABLE (RELOCATABLE_PPC32)
+ * On BookE with RELOCATABLE && PPC32
  *
- *   With RELOCATABLE_PPC32,  we support loading the kernel at any physical 
+ *   With RELOCATABLE && PPC32,  we support loading the kernel at any physical
  *   address without any restriction on the page alignment.
  *
  *   We find the runtime address of _stext and relocate ourselves based on 

commit 934828edfadc43be07e53429ce501741bedf4a5e
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:26:18 2016 +1000

    powerpc/mm: Make 4K and 64K use pte_t for pgtable_t
    
    This patch switches 4K Linux page size config to use pte_t * type
    instead of struct page * for pgtable_t. This simplifies the code a lot
    and helps in consolidating both 64K and 4K page allocator routines. The
    changes should not have any impact, because we already store physical
    address in the upper level page table tree and that implies we already
    do struct page * to physical address conversion.
    
    One change to note here is we move the pgtable_page_dtor() call for
    nohash to pte_fragment_free_mm(). The nohash related change is due to
    the related changes in pgtable_64.c.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 158574d2acf4..51db3a37bced 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -316,12 +316,20 @@ void arch_free_page(struct page *page, int order);
 #endif
 
 struct vm_area_struct;
-
+#ifdef CONFIG_PPC_BOOK3S_64
+/*
+ * For BOOK3s 64 with 4k and 64K linux page size
+ * we want to use pointers, because the page table
+ * actually store pfn
+ */
+typedef pte_t *pgtable_t;
+#else
 #if defined(CONFIG_PPC_64K_PAGES) && defined(CONFIG_PPC64)
 typedef pte_t *pgtable_t;
 #else
 typedef struct page *pgtable_t;
 #endif
+#endif
 
 #include <asm-generic/memory_model.h>
 #endif /* __ASSEMBLY__ */

commit 5dc1ef858c12f865e3676727e03519bece4ce6c1
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:25:28 2016 +1000

    powerpc/mm: Use big endian Linux page tables for book3s 64
    
    Traditionally Power server machines have used the Hashed Page Table MMU
    mode. In this mode Linux manages its own tree of nested page tables,
    aka. "the Linux page tables", which are not used by the hardware
    directly, and software loads translations into the hash page table for
    use by the hardware.
    
    Power ISA 3.0 defines a new MMU mode, known as Radix Tree Translation,
    where the hardware can directly operate on the Linux page tables.
    However the hardware requires that the page tables be in big endian
    format.
    
    To accommodate this, switch the pgtable types to __be64 and add
    appropriate endian conversions.
    
    Because we will be supporting a single kernel binary that boots using
    either radix or hash mode, we always store the Linux page tables big
    endian, even in hash mode where they are not actually used by the
    hardware.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [mpe: Fix sparse errors, flesh out change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index ab3d8977bacd..158574d2acf4 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -288,7 +288,11 @@ extern long long virt_phys_offset;
 
 #ifndef __ASSEMBLY__
 
+#ifdef CONFIG_PPC_BOOK3S_64
+#include <asm/pgtable-be-types.h>
+#else
 #include <asm/pgtable-types.h>
+#endif
 
 typedef struct { signed long pd; } hugepd_t;
 

commit 2bf59916ef033edb9f8e968ee27e486ccb8ba1ce
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Mar 1 09:45:11 2016 +0530

    powerpc/mm: Split pgtable types to separate header
    
    We move the page table accessors into a separate header. We will
    later add a big endian variant of the table which is needed for radix.
    No functionality change only code movement.
    
    Reviewed-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index af7a3422a3ef..ab3d8977bacd 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -288,109 +288,7 @@ extern long long virt_phys_offset;
 
 #ifndef __ASSEMBLY__
 
-#ifdef CONFIG_STRICT_MM_TYPECHECKS
-/* These are used to make use of C type-checking. */
-
-/* PTE level */
-typedef struct { pte_basic_t pte; } pte_t;
-#define __pte(x)	((pte_t) { (x) })
-static inline pte_basic_t pte_val(pte_t x)
-{
-	return x.pte;
-}
-
-/* 64k pages additionally define a bigger "real PTE" type that gathers
- * the "second half" part of the PTE for pseudo 64k pages
- */
-#if defined(CONFIG_PPC_64K_PAGES) && defined(CONFIG_PPC_STD_MMU_64)
-typedef struct { pte_t pte; unsigned long hidx; } real_pte_t;
-#else
-typedef struct { pte_t pte; } real_pte_t;
-#endif
-
-/* PMD level */
-#ifdef CONFIG_PPC64
-typedef struct { unsigned long pmd; } pmd_t;
-#define __pmd(x)	((pmd_t) { (x) })
-static inline unsigned long pmd_val(pmd_t x)
-{
-	return x.pmd;
-}
-
-/* PUD level exusts only on 4k pages */
-#ifndef CONFIG_PPC_64K_PAGES
-typedef struct { unsigned long pud; } pud_t;
-#define __pud(x)	((pud_t) { (x) })
-static inline unsigned long pud_val(pud_t x)
-{
-	return x.pud;
-}
-#endif /* !CONFIG_PPC_64K_PAGES */
-#endif /* CONFIG_PPC64 */
-
-/* PGD level */
-typedef struct { unsigned long pgd; } pgd_t;
-#define __pgd(x)	((pgd_t) { (x) })
-static inline unsigned long pgd_val(pgd_t x)
-{
-	return x.pgd;
-}
-
-/* Page protection bits */
-typedef struct { unsigned long pgprot; } pgprot_t;
-#define pgprot_val(x)	((x).pgprot)
-#define __pgprot(x)	((pgprot_t) { (x) })
-
-#else
-
-/*
- * .. while these make it easier on the compiler
- */
-
-typedef pte_basic_t pte_t;
-#define __pte(x)	(x)
-static inline pte_basic_t pte_val(pte_t pte)
-{
-	return pte;
-}
-
-#if defined(CONFIG_PPC_64K_PAGES) && defined(CONFIG_PPC_STD_MMU_64)
-typedef struct { pte_t pte; unsigned long hidx; } real_pte_t;
-#else
-typedef pte_t real_pte_t;
-#endif
-
-
-#ifdef CONFIG_PPC64
-typedef unsigned long pmd_t;
-#define __pmd(x)	(x)
-static inline unsigned long pmd_val(pmd_t pmd)
-{
-	return pmd;
-}
-
-#ifndef CONFIG_PPC_64K_PAGES
-typedef unsigned long pud_t;
-#define __pud(x)	(x)
-static inline unsigned long pud_val(pud_t pud)
-{
-	return pud;
-}
-#endif /* !CONFIG_PPC_64K_PAGES */
-#endif /* CONFIG_PPC64 */
-
-typedef unsigned long pgd_t;
-#define __pgd(x)	(x)
-static inline unsigned long pgd_val(pgd_t pgd)
-{
-	return pgd;
-}
-
-typedef unsigned long pgprot_t;
-#define pgprot_val(x)	(x)
-#define __pgprot(x)	(x)
-
-#endif
+#include <asm/pgtable-types.h>
 
 typedef struct { signed long pd; } hugepd_t;
 

commit c61a8843124e353f4ba27c073133868da00e0335
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Tue Feb 23 13:36:17 2016 +1100

    powerpc/mm/book3s-64: Use physical addresses in upper page table tree levels
    
    This changes the Linux page tables to store physical addresses
    rather than kernel virtual addresses in the upper levels of the
    tree (pgd, pud and pmd) for 64-bit Book 3S machines.
    
    This also changes the hugepd pointers used to implement hugepages
    when the base page size is 4k to store physical addresses rather than
    virtual addresses (again just for 64-bit Book3S machines).
    
    This frees up some high order bits, and will be needed with
    PowerISA v3.0 machines which read the page table tree in hardware
    in radix mode.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index e34124f6fbf2..af7a3422a3ef 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -271,6 +271,13 @@ extern long long virt_phys_offset;
 #else
 #define PD_HUGE 0x80000000
 #endif
+
+#else	/* CONFIG_PPC_BOOK3S_64 */
+/*
+ * Book3S 64 stores real addresses in the hugepd entries to
+ * avoid overlaps with _PAGE_PRESENT and _PAGE_PTE.
+ */
+#define HUGEPD_ADDR_MASK	(0x0ffffffffffffffful & ~HUGEPD_SHIFT_MASK)
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
 /*

commit 26a344aea48c99cfd80d292a470a480e1c2bd5d9
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Dec 1 09:06:52 2015 +0530

    powerpc/mm: Move hugetlb related headers
    
    W.r.t hugetlb, we support two format for pmd. With book3s_64 and
    64K linux page size, we can have pte at the pmd level. Hence we
    don't need to support hugepd there. For everything else hugepd
    is supported and pmd_huge is (0).
    
    Acked-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 5a3e7c643d73..e34124f6fbf2 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -387,45 +387,11 @@ typedef unsigned long pgprot_t;
 
 typedef struct { signed long pd; } hugepd_t;
 
-#ifdef CONFIG_HUGETLB_PAGE
-#ifdef CONFIG_PPC_BOOK3S_64
-#ifdef CONFIG_PPC_64K_PAGES
-/*
- * With 64k page size, we have hugepage ptes in the pgd and pmd entries. We don't
- * need to setup hugepage directory for them. Our pte and page directory format
- * enable us to have this enabled. But to avoid errors when implementing new
- * features disable hugepd for 64K. We enable a debug version here, So we catch
- * wrong usage.
- */
-#ifdef CONFIG_DEBUG_VM
-extern int hugepd_ok(hugepd_t hpd);
-#else
-#define hugepd_ok(x)	(0)
-#endif
-#else
-static inline int hugepd_ok(hugepd_t hpd)
-{
-	/*
-	 * hugepd pointer, bottom two bits == 00 and next 4 bits
-	 * indicate size of table
-	 */
-	return (((hpd.pd & 0x3) == 0x0) && ((hpd.pd & HUGEPD_SHIFT_MASK) != 0));
-}
-#endif
-#else
-static inline int hugepd_ok(hugepd_t hpd)
-{
-	return (hpd.pd > 0);
-}
-#endif
-
-#define is_hugepd(hpd)               (hugepd_ok(hpd))
-#define pgd_huge pgd_huge
-int pgd_huge(pgd_t pgd);
-#else /* CONFIG_HUGETLB_PAGE */
-#define is_hugepd(pdep)			0
-#define pgd_huge(pgd)			0
+#ifndef CONFIG_HUGETLB_PAGE
+#define is_hugepd(pdep)		(0)
+#define pgd_huge(pgd)		(0)
 #endif /* CONFIG_HUGETLB_PAGE */
+
 #define __hugepd(x) ((hugepd_t) { (x) })
 
 struct page;

commit f281b5d50c87ecca108dcbf8f791bd8923fde3de
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Dec 1 09:06:35 2015 +0530

    powerpc/mm: Don't use pmd_val, pud_val and pgd_val as lvalue
    
    We convert them static inline function here as we did with pte_val in
    the previous patch
    
    Acked-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 3ce534140390..5a3e7c643d73 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -304,21 +304,30 @@ typedef struct { pte_t pte; } real_pte_t;
 /* PMD level */
 #ifdef CONFIG_PPC64
 typedef struct { unsigned long pmd; } pmd_t;
-#define pmd_val(x)	((x).pmd)
 #define __pmd(x)	((pmd_t) { (x) })
+static inline unsigned long pmd_val(pmd_t x)
+{
+	return x.pmd;
+}
 
 /* PUD level exusts only on 4k pages */
 #ifndef CONFIG_PPC_64K_PAGES
 typedef struct { unsigned long pud; } pud_t;
-#define pud_val(x)	((x).pud)
 #define __pud(x)	((pud_t) { (x) })
+static inline unsigned long pud_val(pud_t x)
+{
+	return x.pud;
+}
 #endif /* !CONFIG_PPC_64K_PAGES */
 #endif /* CONFIG_PPC64 */
 
 /* PGD level */
 typedef struct { unsigned long pgd; } pgd_t;
-#define pgd_val(x)	((x).pgd)
 #define __pgd(x)	((pgd_t) { (x) })
+static inline unsigned long pgd_val(pgd_t x)
+{
+	return x.pgd;
+}
 
 /* Page protection bits */
 typedef struct { unsigned long pgprot; } pgprot_t;
@@ -347,22 +356,31 @@ typedef pte_t real_pte_t;
 
 #ifdef CONFIG_PPC64
 typedef unsigned long pmd_t;
-#define pmd_val(x)	(x)
 #define __pmd(x)	(x)
+static inline unsigned long pmd_val(pmd_t pmd)
+{
+	return pmd;
+}
 
 #ifndef CONFIG_PPC_64K_PAGES
 typedef unsigned long pud_t;
-#define pud_val(x)	(x)
 #define __pud(x)	(x)
+static inline unsigned long pud_val(pud_t pud)
+{
+	return pud;
+}
 #endif /* !CONFIG_PPC_64K_PAGES */
 #endif /* CONFIG_PPC64 */
 
 typedef unsigned long pgd_t;
-#define pgd_val(x)	(x)
-#define pgprot_val(x)	(x)
+#define __pgd(x)	(x)
+static inline unsigned long pgd_val(pgd_t pgd)
+{
+	return pgd;
+}
 
 typedef unsigned long pgprot_t;
-#define __pgd(x)	(x)
+#define pgprot_val(x)	(x)
 #define __pgprot(x)	(x)
 
 #endif

commit 10bd3808dfd067d6d6c941cc6e1b13be165f6a70
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Dec 1 09:06:34 2015 +0530

    powerpc/mm: Don't use pte_val as lvalue
    
    We also convert few #define to static inline in this patch for better
    type checking
    
    Acked-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 3140c19c448c..3ce534140390 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -286,8 +286,11 @@ extern long long virt_phys_offset;
 
 /* PTE level */
 typedef struct { pte_basic_t pte; } pte_t;
-#define pte_val(x)	((x).pte)
 #define __pte(x)	((pte_t) { (x) })
+static inline pte_basic_t pte_val(pte_t x)
+{
+	return x.pte;
+}
 
 /* 64k pages additionally define a bigger "real PTE" type that gathers
  * the "second half" part of the PTE for pseudo 64k pages
@@ -329,8 +332,11 @@ typedef struct { unsigned long pgprot; } pgprot_t;
  */
 
 typedef pte_basic_t pte_t;
-#define pte_val(x)	(x)
 #define __pte(x)	(x)
+static inline pte_basic_t pte_val(pte_t pte)
+{
+	return pte;
+}
 
 #if defined(CONFIG_PPC_64K_PAGES) && defined(CONFIG_PPC_STD_MMU_64)
 typedef struct { pte_t pte; unsigned long hidx; } real_pte_t;

commit ffda09a9941c18d9f08d1176d55588d505f62912
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Oct 6 22:48:20 2015 -0500

    powerpc/booke: Only use VIRT_PHYS_OFFSET on booke32
    
    The way VIRT_PHYS_OFFSET is not correct on book3e-64, because
    it does not account for CONFIG_RELOCATABLE other than via the
    32-bit-only virt_phys_offset.
    
    book3e-64 can (and if the comment about a GCC miscompilation is still
    relevant, should) use the normal ppc64 __va/__pa.
    
    At this point, only booke-32 will use VIRT_PHYS_OFFSET, so given the
    issues with its calculation, restrict its definition to booke-32.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 96534b4e5a64..3140c19c448c 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -108,12 +108,13 @@ extern long long virt_phys_offset;
 #endif
 
 /* See Description below for VIRT_PHYS_OFFSET */
-#ifdef CONFIG_RELOCATABLE_PPC32
+#if defined(CONFIG_PPC32) && defined(CONFIG_BOOKE)
+#ifdef CONFIG_RELOCATABLE
 #define VIRT_PHYS_OFFSET virt_phys_offset
 #else
 #define VIRT_PHYS_OFFSET (KERNELBASE - PHYSICAL_START)
 #endif
-
+#endif
 
 #ifdef CONFIG_PPC64
 #define MEMORY_START	0UL
@@ -206,7 +207,7 @@ extern long long virt_phys_offset;
  * On non-Book-E PPC64 PAGE_OFFSET and MEMORY_START are constants so use
  * the other definitions for __va & __pa.
  */
-#ifdef CONFIG_BOOKE
+#if defined(CONFIG_PPC32) && defined(CONFIG_BOOKE)
 #define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) + VIRT_PHYS_OFFSET))
 #define __pa(x) ((unsigned long)(x) - VIRT_PHYS_OFFSET)
 #else

commit ec2640b114d535ba7d895b6ee353791d542f2407
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Sep 7 12:53:53 2015 +0530

    powerpc/mm: Disable hugepd for 64K page size.
    
    After commit e2b3d202d1dba8f3546ed28224ce485bc50010be
    ("powerpc: Switch 16GB and 16MB explicit hugepages to a
    different page table format"), we don't need to support
    is_hugepd() for 64K page size.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 73b58ff6c451..96534b4e5a64 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -364,6 +364,20 @@ typedef struct { signed long pd; } hugepd_t;
 
 #ifdef CONFIG_HUGETLB_PAGE
 #ifdef CONFIG_PPC_BOOK3S_64
+#ifdef CONFIG_PPC_64K_PAGES
+/*
+ * With 64k page size, we have hugepage ptes in the pgd and pmd entries. We don't
+ * need to setup hugepage directory for them. Our pte and page directory format
+ * enable us to have this enabled. But to avoid errors when implementing new
+ * features disable hugepd for 64K. We enable a debug version here, So we catch
+ * wrong usage.
+ */
+#ifdef CONFIG_DEBUG_VM
+extern int hugepd_ok(hugepd_t hpd);
+#else
+#define hugepd_ok(x)	(0)
+#endif
+#else
 static inline int hugepd_ok(hugepd_t hpd)
 {
 	/*
@@ -372,6 +386,7 @@ static inline int hugepd_ok(hugepd_t hpd)
 	 */
 	return (((hpd.pd & 0x3) == 0x0) && ((hpd.pd & HUGEPD_SHIFT_MASK) != 0));
 }
+#endif
 #else
 static inline int hugepd_ok(hugepd_t hpd)
 {

commit f78f7ed72632a0794161f290b89e2cd752f55202
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Oct 8 13:29:28 2015 +0530

    powerpc: Fix _ALIGN_* errors due to type difference.
    
    This avoid errors like
    
            unsigned int usize = 1 << 30;
            int size = 1 << 30;
            unsigned long addr = 64UL << 30 ;
    
            value = _ALIGN_DOWN(addr, usize); -> 0
            value = _ALIGN_DOWN(addr, size);  -> 0x1000000000
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 168ca67e39b3..73b58ff6c451 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -12,6 +12,7 @@
 
 #ifndef __ASSEMBLY__
 #include <linux/types.h>
+#include <linux/kernel.h>
 #else
 #include <asm/types.h>
 #endif
@@ -241,8 +242,8 @@ extern long long virt_phys_offset;
 #endif
 
 /* align addr on a size boundary - adjust address up/down if needed */
-#define _ALIGN_UP(addr,size)	(((addr)+((size)-1))&(~((size)-1)))
-#define _ALIGN_DOWN(addr,size)	((addr)&(~((size)-1)))
+#define _ALIGN_UP(addr, size)   __ALIGN_KERNEL(addr, size)
+#define _ALIGN_DOWN(addr, size)	((addr)&(~((typeof(addr))(size)-1)))
 
 /* align addr on a size boundary - adjust address up if needed */
 #define _ALIGN(addr,size)     _ALIGN_UP(addr,size)

commit 65d3223a853ac8598694064c1d37b0955e7d99cc
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Sep 3 13:20:56 2015 +0530

    powerpc/mm: Add virt_to_pfn and use this instead of opencoding
    
    This add helper virt_to_pfn and remove the opencoded usage of the
    same.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 71294a6e976e..168ca67e39b3 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -127,9 +127,10 @@ extern long long virt_phys_offset;
 #define pfn_valid(pfn)		((pfn) >= ARCH_PFN_OFFSET && (pfn) < max_mapnr)
 #endif
 
-#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#define virt_to_pfn(kaddr)	(__pa(kaddr) >> PAGE_SHIFT)
+#define virt_to_page(kaddr)	pfn_to_page(virt_to_pfn(kaddr))
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
-#define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+#define virt_addr_valid(kaddr)	pfn_valid(virt_to_pfn(kaddr))
 
 /*
  * On Book-E parts we need __va to parse the device tree and we can't

commit f1e7c202a98cb87cc650d99d014f87e6248ae530
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Mar 25 20:11:56 2015 +1100

    powerpc: Make STRICT_MM_TYPECHECKS a config option
    
    The STRICT_MM_TYPECHECKS code has bit-rotted over the years. To make it
    possible to easily build test it, make it a CONFIG option.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 69c059887a2c..71294a6e976e 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -278,9 +278,7 @@ extern long long virt_phys_offset;
 
 #ifndef __ASSEMBLY__
 
-#undef STRICT_MM_TYPECHECKS
-
-#ifdef STRICT_MM_TYPECHECKS
+#ifdef CONFIG_STRICT_MM_TYPECHECKS
 /* These are used to make use of C type-checking. */
 
 /* PTE level */

commit b30e759072c182538abb6908681cfd49978ba5e2
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Nov 5 21:57:41 2014 +0530

    powerpc/mm: Switch to generic RCU get_user_pages_fast
    
    This patch switch the ppc arch to use the generic RCU based
    gup implementation.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index f973fce73a43..69c059887a2c 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -379,13 +379,14 @@ static inline int hugepd_ok(hugepd_t hpd)
 }
 #endif
 
-#define is_hugepd(pdep)               (hugepd_ok(*((hugepd_t *)(pdep))))
+#define is_hugepd(hpd)               (hugepd_ok(hpd))
 #define pgd_huge pgd_huge
 int pgd_huge(pgd_t pgd);
 #else /* CONFIG_HUGETLB_PAGE */
 #define is_hugepd(pdep)			0
 #define pgd_huge(pgd)			0
 #endif /* CONFIG_HUGETLB_PAGE */
+#define __hugepd(x) ((hugepd_t) { (x) })
 
 struct page;
 extern void clear_user_page(void *page, unsigned long vaddr, struct page *pg);

commit f30c59e921f12b209852e1ddc197dc6a8fb0142b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Nov 5 21:57:40 2014 +0530

    mm: Update generic gup implementation to handle hugepage directory
    
    Update generic gup implementation with powerpc specific details.
    On powerpc at pmd level we can have hugepte, normal pmd pointer
    or a pointer to the hugepage directory.
    
    Tested-by: Steve Capper <steve.capper@linaro.org>
    Acked-by: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 26fe1ae15212..f973fce73a43 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -380,6 +380,7 @@ static inline int hugepd_ok(hugepd_t hpd)
 #endif
 
 #define is_hugepd(pdep)               (hugepd_ok(*((hugepd_t *)(pdep))))
+#define pgd_huge pgd_huge
 int pgd_huge(pgd_t pgd);
 #else /* CONFIG_HUGETLB_PAGE */
 #define is_hugepd(pdep)			0

commit a6c19dfe39941a5d3f4d072121c0a4841e7e26fd
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Aug 8 14:23:40 2014 -0700

    arm64,ia64,ppc,s390,sh,tile,um,x86,mm: remove default gate area
    
    The core mm code will provide a default gate area based on
    FIXADDR_USER_START and FIXADDR_USER_END if
    !defined(__HAVE_ARCH_GATE_AREA) && defined(AT_SYSINFO_EHDR).
    
    This default is only useful for ia64.  arm64, ppc, s390, sh, tile, 64-bit
    UML, and x86_32 have their own code just to disable it.  arm, 32-bit UML,
    and x86_64 have gate areas, but they have their own implementations.
    
    This gets rid of the default and moves the code into ia64.
    
    This should save some code on architectures without a gate area: it's now
    possible to inline the gate_area functions in the default case.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Acked-by: Nathan Lynch <nathan_lynch@mentor.com>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org> [in principle]
    Acked-by: Richard Weinberger <richard@nod.at> [for um]
    Acked-by: Will Deacon <will.deacon@arm.com> [for arm64]
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Nathan Lynch <Nathan_Lynch@mentor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 32e4e212b9c1..26fe1ae15212 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -48,9 +48,6 @@ extern unsigned int HPAGE_SHIFT;
 #define HUGE_MAX_HSTATE		(MMU_PAGE_COUNT-1)
 #endif
 
-/* We do define AT_SYSINFO_EHDR but don't use the gate mechanism */
-#define __HAVE_ARCH_GATE_AREA		1
-
 /*
  * Subtle: (1 << PAGE_SHIFT) is an int, not an unsigned long. So if we
  * assign PAGE_MASK to a larger type it gets extended the way we want

commit ecb35c3943040f4db735c7d14c24ee7750cbb482
Author: Alistair Popple <alistair@popple.id.au>
Date:   Thu Oct 17 17:08:28 2013 +1100

    powerpc: Fix 64K page size support for PPC44x
    
    PPC44x supports page sizes other than 4K however when 64K page sizes
    are selected compilation fails. This is due to a change in the
    definition of pgtable_t introduced by the following patch:
    
    commit 5c1f6ee9a31cbdac90bbb8ae1ba4475031ac74b4
    Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    powerpc: Reduce PTE table memory wastage
    
    The above patch only implements the new layout for PPC64 so it doesn't
    compile for PPC32 with a 64K page size. Ideally we should implement
    the same layout for PPC32 however for the meantime this patch reverts
    the definition of pgtable_t for PPC32.
    
    Signed-off-by: Alistair Popple <alistair@popple.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 753c66206a15..32e4e212b9c1 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -403,7 +403,7 @@ void arch_free_page(struct page *page, int order);
 
 struct vm_area_struct;
 
-#ifdef CONFIG_PPC_64K_PAGES
+#if defined(CONFIG_PPC_64K_PAGES) && defined(CONFIG_PPC64)
 typedef pte_t *pgtable_t;
 #else
 typedef struct page *pgtable_t;

commit b83941798c35f9cffba36927011df2b53c3884d8
Author: Vaishnavi Bhat <vaishnavi@linux.vnet.ibm.com>
Date:   Sun Oct 27 11:47:19 2013 +0530

    powerpc: Fix a typo in comments of va to pa conversion
    
    This patch fixes typo in comments virtual to physical
    address conversion.
    
    Signed-off-by: Vaishnavi Bhat <vaishnavi@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index b9f426212d3a..753c66206a15 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -78,7 +78,7 @@ extern unsigned int HPAGE_SHIFT;
  *
  * Also, KERNELBASE >= PAGE_OFFSET and PHYSICAL_START >= MEMORY_START
  *
- * There are two was to determine a physical address from a virtual one:
+ * There are two ways to determine a physical address from a virtual one:
  * va = pa + PAGE_OFFSET - MEMORY_START
  * va = pa + KERNELBASE - PHYSICAL_START
  *

commit bdbc29c19b2633b1d9c52638fb732bcde7a2031a
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Aug 27 16:07:49 2013 +1000

    powerpc: Work around gcc miscompilation of __pa() on 64-bit
    
    On 64-bit, __pa(&static_var) gets miscompiled by recent versions of
    gcc as something like:
    
            addis 3,2,.LANCHOR1+4611686018427387904@toc@ha
            addi 3,3,.LANCHOR1+4611686018427387904@toc@l
    
    This ends up effectively ignoring the offset, since its bottom 32 bits
    are zero, and means that the result of __pa() still has 0xC in the top
    nibble.  This happens with gcc 4.8.1, at least.
    
    To work around this, for 64-bit we make __pa() use an AND operator,
    and for symmetry, we make __va() use an OR operator.  Using an AND
    operator rather than a subtraction ends up with slightly shorter code
    since it can be done with a single clrldi instruction, whereas it
    takes three instructions to form the constant (-PAGE_OFFSET) and add
    it on.  (Note that MEMORY_START is always 0 on 64-bit.)
    
    CC: <stable@vger.kernel.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 988c812aab5b..b9f426212d3a 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -211,9 +211,19 @@ extern long long virt_phys_offset;
 #define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) + VIRT_PHYS_OFFSET))
 #define __pa(x) ((unsigned long)(x) - VIRT_PHYS_OFFSET)
 #else
+#ifdef CONFIG_PPC64
+/*
+ * gcc miscompiles (unsigned long)(&static_var) - PAGE_OFFSET
+ * with -mcmodel=medium, so we use & and | instead of - and + on 64-bit.
+ */
+#define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) | PAGE_OFFSET))
+#define __pa(x) ((unsigned long)(x) & 0x0fffffffffffffffUL)
+
+#else /* 32-bit, non book E */
 #define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) + PAGE_OFFSET - MEMORY_START))
 #define __pa(x) ((unsigned long)(x) - PAGE_OFFSET + MEMORY_START)
 #endif
+#endif
 
 /*
  * Unfortunately the PLT is in the BSS in the PPC32 ELF ABI,

commit 5c1f6ee9a31cbdac90bbb8ae1ba4475031ac74b4
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Sun Apr 28 09:37:33 2013 +0000

    powerpc: Reduce PTE table memory wastage
    
    We allocate one page for the last level of linux page table. With THP and
    large page size of 16MB, that would mean we are wasting large part
    of that page. To map 16MB area, we only need a PTE space of 2K with 64K
    page size. This patch reduce the space wastage by sharing the page
    allocated for the last level of linux page table with multiple pmd
    entries. We call these smaller chunks PTE page fragments and allocated
    page, PTE page.
    
    In order to support systems which doesn't have 64K HPTE support, we also
    add another 2K to PTE page fragment. The second half of the PTE fragments
    is used for storing slot and secondary bit information of an HPTE. With this
    we now have a 4K PTE fragment.
    
    We use a simple approach to share the PTE page. On allocation, we bump the
    PTE page refcount to 16 and share the PTE page with the next 16 pte alloc
    request. This should help in the node locality of the PTE page fragment,
    assuming that the immediate pte alloc request will mostly come from the
    same NUMA node. We don't try to reuse the freed PTE page fragment. Hence
    we could be waisting some space.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 711e83a79e3d..988c812aab5b 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -393,7 +393,11 @@ void arch_free_page(struct page *page, int order);
 
 struct vm_area_struct;
 
+#ifdef CONFIG_PPC_64K_PAGES
+typedef pte_t *pgtable_t;
+#else
 typedef struct page *pgtable_t;
+#endif
 
 #include <asm-generic/memory_model.h>
 #endif /* __ASSEMBLY__ */

commit e2b3d202d1dba8f3546ed28224ce485bc50010be
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Sun Apr 28 09:37:30 2013 +0000

    powerpc: Switch 16GB and 16MB explicit hugepages to a different page table format
    
    We will be switching PMD_SHIFT to 24 bits to facilitate THP impmenetation.
    With PMD_SHIFT set to 24, we now have 16MB huge pages allocated at PGD level.
    That means with 32 bit process we cannot allocate normal pages at
    all, because we cover the entire address space with one pgd entry. Fix this
    by switching to a new page table format for hugepages. With the new page table
    format for 16GB and 16MB hugepages we won't allocate hugepage directory. Instead
    we encode the PTE information directly at the directory level. This forces 16MB
    hugepage at PMD level. This will also make the page take walk much simpler later
    when we add the THP support.
    
    With the new table format we have 4 cases for pgds and pmds:
    (1) invalid (all zeroes)
    (2) pointer to next table, as normal; bottom 6 bits == 0
    (3) leaf pte for huge page, bottom two bits != 00
    (4) hugepd pointer, bottom two bits == 00, next 4 bits indicate size of table
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 652719ccd2e9..711e83a79e3d 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -373,8 +373,10 @@ static inline int hugepd_ok(hugepd_t hpd)
 #endif
 
 #define is_hugepd(pdep)               (hugepd_ok(*((hugepd_t *)(pdep))))
+int pgd_huge(pgd_t pgd);
 #else /* CONFIG_HUGETLB_PAGE */
 #define is_hugepd(pdep)			0
+#define pgd_huge(pgd)			0
 #endif /* CONFIG_HUGETLB_PAGE */
 
 struct page;

commit cf9427b85e90bb1ff90e2397ff419691d983c68b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Sun Apr 28 09:37:29 2013 +0000

    powerpc: New hugepage directory format
    
    Change the hugepage directory format so that we can have leaf ptes directly
    at page directory avoiding the allocation of hugepage directory.
    
    With the new table format we have 3 cases for pgds and pmds:
    (1) invalid (all zeroes)
    (2) pointer to next table, as normal; bottom 6 bits == 0
    (4) hugepd pointer, bottom two bits == 00, next 4 bits indicate size of table
    
    Instead of storing shift value in hugepd pointer we use mmu_psize_def index
    so that we can fit all the supported hugepage size in 4 bits
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index f072e974f8a2..652719ccd2e9 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -249,6 +249,7 @@ extern long long virt_phys_offset;
 #define is_kernel_addr(x)	((x) >= PAGE_OFFSET)
 #endif
 
+#ifndef CONFIG_PPC_BOOK3S_64
 /*
  * Use the top bit of the higher-level page table entries to indicate whether
  * the entries we point to contain hugepages.  This works because we know that
@@ -260,6 +261,7 @@ extern long long virt_phys_offset;
 #else
 #define PD_HUGE 0x80000000
 #endif
+#endif /* CONFIG_PPC_BOOK3S_64 */
 
 /*
  * Some number of bits at the level of the page table that points to
@@ -354,10 +356,21 @@ typedef unsigned long pgprot_t;
 typedef struct { signed long pd; } hugepd_t;
 
 #ifdef CONFIG_HUGETLB_PAGE
+#ifdef CONFIG_PPC_BOOK3S_64
+static inline int hugepd_ok(hugepd_t hpd)
+{
+	/*
+	 * hugepd pointer, bottom two bits == 00 and next 4 bits
+	 * indicate size of table
+	 */
+	return (((hpd.pd & 0x3) == 0x0) && ((hpd.pd & HUGEPD_SHIFT_MASK) != 0));
+}
+#else
 static inline int hugepd_ok(hugepd_t hpd)
 {
 	return (hpd.pd > 0);
 }
+#endif
 
 #define is_hugepd(pdep)               (hugepd_ok(*((hugepd_t *)(pdep))))
 #else /* CONFIG_HUGETLB_PAGE */

commit 368ff8f14d6ed8e9fd3b7c2156f2607719bf5a7a
Author: Suzuki Poulose <suzuki@in.ibm.com>
Date:   Wed Dec 14 22:58:37 2011 +0000

    powerpc: Define virtual-physical translations for RELOCATABLE
    
    We find the runtime address of _stext and relocate ourselves based
    on the following calculation.
    
            virtual_base = ALIGN(KERNELBASE,KERNEL_TLB_PIN_SIZE) +
                            MODULO(_stext.run,KERNEL_TLB_PIN_SIZE)
    
    relocate() is called with the Effective Virtual Base Address (as
    shown below)
    
                | Phys. Addr| Virt. Addr |
    Page        |------------------------|
    Boundary    |           |            |
                |           |            |
                |           |            |
    Kernel Load |___________|_ __ _ _ _ _|<- Effective
    Addr(_stext)|           |      ^     |Virt. Base Addr
                |           |      |     |
                |           |      |     |
                |           |reloc_offset|
                |           |      |     |
                |           |      |     |
                |           |______v_____|<-(KERNELBASE)%TLB_SIZE
                |           |            |
                |           |            |
                |           |            |
    Page        |-----------|------------|
    Boundary    |           |            |
    
    On BookE, we need __va() & __pa() early in the boot process to access
    the device tree.
    
    Currently this has been defined as :
    
    #define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) -
                                                    PHYSICAL_START + KERNELBASE)
    where:
     PHYSICAL_START is kernstart_addr - a variable updated at runtime.
     KERNELBASE     is the compile time Virtual base address of kernel.
    
    This won't work for us, as kernstart_addr is dynamic and will yield different
    results for __va()/__pa() for same mapping.
    
    e.g.,
    
    Let the kernel be loaded at 64MB and KERNELBASE be 0xc0000000 (same as
    PAGE_OFFSET).
    
    In this case, we would be mapping 0 to 0xc0000000, and kernstart_addr = 64M
    
    Now __va(1MB) = (0x100000) - (0x4000000) + 0xc0000000
                    = 0xbc100000 , which is wrong.
    
    it should be : 0xc0000000 + 0x100000 = 0xc0100000
    
    On platforms which support AMP, like PPC_47x (based on 44x), the kernel
    could be loaded at highmem. Hence we cannot always depend on the compile
    time constants for mapping.
    
    Here are the possible solutions:
    
    1) Update kernstart_addr(PHSYICAL_START) to match the Physical address of
    compile time KERNELBASE value, instead of the actual Physical_Address(_stext).
    
    The disadvantage is that we may break other users of PHYSICAL_START. They
    could be replaced with __pa(_stext).
    
    2) Redefine __va() & __pa() with relocation offset
    
    #ifdef  CONFIG_RELOCATABLE_PPC32
    #define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) - PHYSICAL_START + (KERNELBASE + RELOC_OFFSET)))
    #define __pa(x) ((unsigned long)(x) + PHYSICAL_START - (KERNELBASE + RELOC_OFFSET))
    #endif
    
    where, RELOC_OFFSET could be
    
      a) A variable, say relocation_offset (like kernstart_addr), updated
         at boot time. This impacts performance, as we have to load an additional
         variable from memory.
    
                    OR
    
      b) #define RELOC_OFFSET ((PHYSICAL_START & PPC_PIN_SIZE_OFFSET_MASK) - \
                          (KERNELBASE & PPC_PIN_SIZE_OFFSET_MASK))
    
       This introduces more calculations for doing the translation.
    
    3) Redefine __va() & __pa() with a new variable
    
    i.e,
    
    #define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) + VIRT_PHYS_OFFSET))
    
    where VIRT_PHYS_OFFSET :
    
    #ifdef CONFIG_RELOCATABLE_PPC32
    #define VIRT_PHYS_OFFSET virt_phys_offset
    #else
    #define VIRT_PHYS_OFFSET (KERNELBASE - PHYSICAL_START)
    #endif /* CONFIG_RELOCATABLE_PPC32 */
    
    where virt_phy_offset is updated at runtime to :
    
            Effective KERNELBASE - kernstart_addr.
    
    Taking our example, above:
    
    virt_phys_offset = effective_kernelstart_vaddr - kernstart_addr
                     = 0xc0400000 - 0x400000
                     = 0xc0000000
            and
    
            __va(0x100000) = 0xc0000000 + 0x100000 = 0xc0100000
             which is what we want.
    
    I have implemented (3) in the following patch which has same cost of
    operation as the existing one.
    
    I have tested the patches on 440x platforms only. However this should
    work fine for PPC_47x also, as we only depend on the runtime address
    and the current TLB XLAT entry for the startup code, which is available
    in r25. I don't have access to a 47x board yet. So, it would be great if
    somebody could test this on 47x.
    
    Signed-off-by: Suzuki K. Poulose <suzuki@in.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Cc: linuxppc-dev <linuxppc-dev@lists.ozlabs.org>
    Signed-off-by: Josh Boyer <jwboyer@gmail.com>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index f149967ee6b5..f072e974f8a2 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -97,12 +97,26 @@ extern unsigned int HPAGE_SHIFT;
 
 extern phys_addr_t memstart_addr;
 extern phys_addr_t kernstart_addr;
+
+#ifdef CONFIG_RELOCATABLE_PPC32
+extern long long virt_phys_offset;
 #endif
+
+#endif /* __ASSEMBLY__ */
 #define PHYSICAL_START	kernstart_addr
-#else
+
+#else	/* !CONFIG_NONSTATIC_KERNEL */
 #define PHYSICAL_START	ASM_CONST(CONFIG_PHYSICAL_START)
 #endif
 
+/* See Description below for VIRT_PHYS_OFFSET */
+#ifdef CONFIG_RELOCATABLE_PPC32
+#define VIRT_PHYS_OFFSET virt_phys_offset
+#else
+#define VIRT_PHYS_OFFSET (KERNELBASE - PHYSICAL_START)
+#endif
+
+
 #ifdef CONFIG_PPC64
 #define MEMORY_START	0UL
 #elif defined(CONFIG_NONSTATIC_KERNEL)
@@ -125,12 +139,77 @@ extern phys_addr_t kernstart_addr;
  * determine MEMORY_START until then.  However we can determine PHYSICAL_START
  * from information at hand (program counter, TLB lookup).
  *
+ * On BookE with RELOCATABLE (RELOCATABLE_PPC32)
+ *
+ *   With RELOCATABLE_PPC32,  we support loading the kernel at any physical 
+ *   address without any restriction on the page alignment.
+ *
+ *   We find the runtime address of _stext and relocate ourselves based on 
+ *   the following calculation:
+ *
+ *  	  virtual_base = ALIGN_DOWN(KERNELBASE,256M) +
+ *  				MODULO(_stext.run,256M)
+ *   and create the following mapping:
+ *
+ * 	  ALIGN_DOWN(_stext.run,256M) => ALIGN_DOWN(KERNELBASE,256M)
+ *
+ *   When we process relocations, we cannot depend on the
+ *   existing equation for the __va()/__pa() translations:
+ *
+ * 	   __va(x) = (x)  - PHYSICAL_START + KERNELBASE
+ *
+ *   Where:
+ *   	 PHYSICAL_START = kernstart_addr = Physical address of _stext
+ *  	 KERNELBASE = Compiled virtual address of _stext.
+ *
+ *   This formula holds true iff, kernel load address is TLB page aligned.
+ *
+ *   In our case, we need to also account for the shift in the kernel Virtual 
+ *   address.
+ *
+ *   E.g.,
+ *
+ *   Let the kernel be loaded at 64MB and KERNELBASE be 0xc0000000 (same as PAGE_OFFSET).
+ *   In this case, we would be mapping 0 to 0xc0000000, and kernstart_addr = 64M
+ *
+ *   Now __va(1MB) = (0x100000) - (0x4000000) + 0xc0000000
+ *                 = 0xbc100000 , which is wrong.
+ *
+ *   Rather, it should be : 0xc0000000 + 0x100000 = 0xc0100000
+ *      	according to our mapping.
+ *
+ *   Hence we use the following formula to get the translations right:
+ *
+ * 	  __va(x) = (x) - [ PHYSICAL_START - Effective KERNELBASE ]
+ *
+ * 	  Where :
+ * 		PHYSICAL_START = dynamic load address.(kernstart_addr variable)
+ * 		Effective KERNELBASE = virtual_base =
+ * 				     = ALIGN_DOWN(KERNELBASE,256M) +
+ * 						MODULO(PHYSICAL_START,256M)
+ *
+ * 	To make the cost of __va() / __pa() more light weight, we introduce
+ * 	a new variable virt_phys_offset, which will hold :
+ *
+ * 	virt_phys_offset = Effective KERNELBASE - PHYSICAL_START
+ * 			 = ALIGN_DOWN(KERNELBASE,256M) - 
+ * 			 	ALIGN_DOWN(PHYSICALSTART,256M)
+ *
+ * 	Hence :
+ *
+ * 	__va(x) = x - PHYSICAL_START + Effective KERNELBASE
+ * 		= x + virt_phys_offset
+ *
+ * 		and
+ * 	__pa(x) = x + PHYSICAL_START - Effective KERNELBASE
+ * 		= x - virt_phys_offset
+ * 		
  * On non-Book-E PPC64 PAGE_OFFSET and MEMORY_START are constants so use
  * the other definitions for __va & __pa.
  */
 #ifdef CONFIG_BOOKE
-#define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) - PHYSICAL_START + KERNELBASE))
-#define __pa(x) ((unsigned long)(x) + PHYSICAL_START - KERNELBASE)
+#define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) + VIRT_PHYS_OFFSET))
+#define __pa(x) ((unsigned long)(x) - VIRT_PHYS_OFFSET)
 #else
 #define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) + PAGE_OFFSET - MEMORY_START))
 #define __pa(x) ((unsigned long)(x) - PAGE_OFFSET + MEMORY_START)

commit 0f890c8d205e47f7cb0d381ffba582a170fd4f72
Author: Suzuki Poulose <suzuki@in.ibm.com>
Date:   Wed Dec 14 22:57:15 2011 +0000

    powerpc: Rename mapping based RELOCATABLE to DYNAMIC_MEMSTART for BookE
    
    The current implementation of CONFIG_RELOCATABLE in BookE is based
    on mapping the page aligned kernel load address to KERNELBASE. This
    approach however is not enough for platforms, where the TLB page size
    is large (e.g, 256M on 44x). So we are renaming the RELOCATABLE used
    currently in BookE to DYNAMIC_MEMSTART to reflect the actual method.
    
    The CONFIG_RELOCATABLE for PPC32(BookE) based on processing of the
    dynamic relocations will be introduced in the later in the patch series.
    
    This change would allow the use of the old method of RELOCATABLE for
    platforms which can afford to enforce the page alignment (platforms with
    smaller TLB size).
    
    Changes since v3:
    
    * Introduced a new config, NONSTATIC_KERNEL, to denote a kernel which is
      either a RELOCATABLE or DYNAMIC_MEMSTART(Suggested by: Josh Boyer)
    
    Suggested-by: Scott Wood <scottwood@freescale.com>
    Tested-by: Scott Wood <scottwood@freescale.com>
    
    Signed-off-by: Suzuki K. Poulose <suzuki@in.ibm.com>
    Cc: Scott Wood <scottwood@freescale.com>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Cc: Josh Boyer <jwboyer@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: linux ppc dev <linuxppc-dev@lists.ozlabs.org>
    Signed-off-by: Josh Boyer <jwboyer@gmail.com>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 9d7485c7e6f8..f149967ee6b5 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -92,7 +92,7 @@ extern unsigned int HPAGE_SHIFT;
 #define PAGE_OFFSET	ASM_CONST(CONFIG_PAGE_OFFSET)
 #define LOAD_OFFSET	ASM_CONST((CONFIG_KERNEL_START-CONFIG_PHYSICAL_START))
 
-#if defined(CONFIG_RELOCATABLE)
+#if defined(CONFIG_NONSTATIC_KERNEL)
 #ifndef __ASSEMBLY__
 
 extern phys_addr_t memstart_addr;
@@ -105,7 +105,7 @@ extern phys_addr_t kernstart_addr;
 
 #ifdef CONFIG_PPC64
 #define MEMORY_START	0UL
-#elif defined(CONFIG_RELOCATABLE)
+#elif defined(CONFIG_NONSTATIC_KERNEL)
 #define MEMORY_START	memstart_addr
 #else
 #define MEMORY_START	(PHYSICAL_START + PAGE_OFFSET - KERNELBASE)

commit 1d54cf2b973a6265789b382b7d305771321b9b57
Author: sukadev@linux.vnet.ibm.com <sukadev@linux.vnet.ibm.com>
Date:   Tue Aug 30 09:19:17 2011 +0000

    powerpc: Implement CONFIG_STRICT_DEVMEM
    
    As described in the help text in the patch, this token restricts general
    access to /dev/mem as a way of increasing the security. Specifically, access
    to exclusive IOMEM and kernel RAM is denied unless CONFIG_STRICT_DEVMEM is
    set to 'n'.
    
    Implement the 'devmem_is_allowed()' interface for Powerpc. It will be
    called from range_is_allowed() when userpsace attempts to access /dev/mem.
    
    This patch is based on an earlier patch from Steve Best and with input from
    Paul Mackerras and Scott Wood.
    
    [BenH] Fixed a typo or two and removed the generic change which should
           be submitted as a separate patch
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index dd9c4fd038e0..9d7485c7e6f8 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -290,6 +290,7 @@ extern void clear_user_page(void *page, unsigned long vaddr, struct page *pg);
 extern void copy_user_page(void *to, void *from, unsigned long vaddr,
 		struct page *p);
 extern int page_is_ram(unsigned long pfn);
+extern int devmem_is_allowed(unsigned long pfn);
 
 #ifdef CONFIG_PPC_SMLPAR
 void arch_free_page(struct page *page, int order);

commit 41151e77a4d96ea138cede6d84c955aa4769ce74
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Tue Jun 28 09:54:48 2011 +0000

    powerpc: Hugetlb for BookE
    
    Enable hugepages on Freescale BookE processors.  This allows the kernel to
    use huge TLB entries to map pages, which can greatly reduce the number of
    TLB misses and the amount of TLB thrashing experienced by applications with
    large memory footprints.  Care should be taken when using this on FSL
    processors, as the number of large TLB entries supported by the core is low
    (16-64) on current processors.
    
    The supported set of hugepage sizes include 4m, 16m, 64m, 256m, and 1g.
    Page sizes larger than the max zone size are called "gigantic" pages and
    must be allocated on the command line (and cannot be deallocated).
    
    This is currently only fully implemented for Freescale 32-bit BookE
    processors, but there is some infrastructure in the code for
    64-bit BooKE.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 2cd664ef0a5e..dd9c4fd038e0 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -36,6 +36,18 @@
 
 #define PAGE_SIZE		(ASM_CONST(1) << PAGE_SHIFT)
 
+#ifndef __ASSEMBLY__
+#ifdef CONFIG_HUGETLB_PAGE
+extern unsigned int HPAGE_SHIFT;
+#else
+#define HPAGE_SHIFT PAGE_SHIFT
+#endif
+#define HPAGE_SIZE		((1UL) << HPAGE_SHIFT)
+#define HPAGE_MASK		(~(HPAGE_SIZE - 1))
+#define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
+#define HUGE_MAX_HSTATE		(MMU_PAGE_COUNT-1)
+#endif
+
 /* We do define AT_SYSINFO_EHDR but don't use the gate mechanism */
 #define __HAVE_ARCH_GATE_AREA		1
 
@@ -158,6 +170,24 @@ extern phys_addr_t kernstart_addr;
 #define is_kernel_addr(x)	((x) >= PAGE_OFFSET)
 #endif
 
+/*
+ * Use the top bit of the higher-level page table entries to indicate whether
+ * the entries we point to contain hugepages.  This works because we know that
+ * the page tables live in kernel space.  If we ever decide to support having
+ * page tables at arbitrary addresses, this breaks and will have to change.
+ */
+#ifdef CONFIG_PPC64
+#define PD_HUGE 0x8000000000000000
+#else
+#define PD_HUGE 0x80000000
+#endif
+
+/*
+ * Some number of bits at the level of the page table that points to
+ * a hugepte are used to encode the size.  This masks those bits.
+ */
+#define HUGEPD_SHIFT_MASK     0x3f
+
 #ifndef __ASSEMBLY__
 
 #undef STRICT_MM_TYPECHECKS
@@ -243,7 +273,6 @@ typedef unsigned long pgprot_t;
 #endif
 
 typedef struct { signed long pd; } hugepd_t;
-#define HUGEPD_SHIFT_MASK     0x3f
 
 #ifdef CONFIG_HUGETLB_PAGE
 static inline int hugepd_ok(hugepd_t hpd)

commit 67eb54944bb9de5f568ea2c28d4f20a5b7d2611d
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Mar 24 11:51:19 2011 +0000

    powerpc: ARCH_PFN_OFFSET should be unsigned long
    
    pfns are unsigned long, but MEMORY_START is phys_addr_t.  This leads
    to page_to_pfn() returning phys_addr_t, and thus type mismatches in a few
    print statements.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index da4b20008541..2cd664ef0a5e 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -100,7 +100,7 @@ extern phys_addr_t kernstart_addr;
 #endif
 
 #ifdef CONFIG_FLATMEM
-#define ARCH_PFN_OFFSET		(MEMORY_START >> PAGE_SHIFT)
+#define ARCH_PFN_OFFSET		((unsigned long)(MEMORY_START >> PAGE_SHIFT))
 #define pfn_valid(pfn)		((pfn) >= ARCH_PFN_OFFSET && (pfn) < max_mapnr)
 #endif
 

commit 81c386cc7f4c22b81ba94709d2d58754282ea05e
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Jan 27 10:31:38 2011 +0000

    powerpc: Fix pfn_valid() when memory starts at a non-zero address
    
    max_mapnr is a pfn, not an index innto mem_map[].  So don't add
    ARCH_PFN_OFFSET a second time.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 53b64be40eb2..da4b20008541 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -101,7 +101,7 @@ extern phys_addr_t kernstart_addr;
 
 #ifdef CONFIG_FLATMEM
 #define ARCH_PFN_OFFSET		(MEMORY_START >> PAGE_SHIFT)
-#define pfn_valid(pfn)		((pfn) >= ARCH_PFN_OFFSET && (pfn) < (ARCH_PFN_OFFSET + max_mapnr))
+#define pfn_valid(pfn)		((pfn) >= ARCH_PFN_OFFSET && (pfn) < max_mapnr)
 #endif
 
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)

commit dbc9632a8c25c6efcc1ca3f3a2177c855b6e053e
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Apr 21 02:12:58 2010 -0500

    powerpc/fsl-booke: Fix CONFIG_RELOCATABLE support on FSL Book-E ppc32
    
    The following commit broke CONFIG_RELOCATABLE support on FSL Book-E
    parts:
    
    commit 549e8152de8039506f69c677a4546e5427aa6ae7
    Author: Paul Mackerras <paulus@samba.org>
    Date:   Sat Aug 30 11:43:47 2008 +1000
    
        powerpc: Make the 64-bit kernel as a position-independent executable
    
    The change to __va and __pa to use PAGE_OFFSET & MEMORY_START causes
    problems on the Book-E parts because we don't know MEMORY_START until
    after we parse the device tree.  We need __va to work properly to even
    parse the device tree so we have a chicken an egg.  So go back to using
    he other definition of __va/__pa on CONFIG_BOOKE and use the
    PAGE_OFFSET/MEMORY_START version on "Classic" PPC64.
    
    Also updated casts to handle phys_addr_t being a different size from
    unsigned long (ie 36-bit physical on PPC32).
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index e96d52a516ba..53b64be40eb2 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -108,8 +108,21 @@ extern phys_addr_t kernstart_addr;
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 #define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 
-#define __va(x) ((void *)((unsigned long)(x) + PAGE_OFFSET - MEMORY_START))
+/*
+ * On Book-E parts we need __va to parse the device tree and we can't
+ * determine MEMORY_START until then.  However we can determine PHYSICAL_START
+ * from information at hand (program counter, TLB lookup).
+ *
+ * On non-Book-E PPC64 PAGE_OFFSET and MEMORY_START are constants so use
+ * the other definitions for __va & __pa.
+ */
+#ifdef CONFIG_BOOKE
+#define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) - PHYSICAL_START + KERNELBASE))
+#define __pa(x) ((unsigned long)(x) + PHYSICAL_START - KERNELBASE)
+#else
+#define __va(x) ((void *)(unsigned long)((phys_addr_t)(x) + PAGE_OFFSET - MEMORY_START))
 #define __pa(x) ((unsigned long)(x) - PAGE_OFFSET + MEMORY_START)
+#endif
 
 /*
  * Unfortunately the PLT is in the BSS in the PPC32 ELF ABI,

commit a4fe3ce7699bfe1bd88f816b55d42d8fe1dac655
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Mon Oct 26 19:24:31 2009 +0000

    powerpc/mm: Allow more flexible layouts for hugepage pagetables
    
    Currently each available hugepage size uses a slightly different
    pagetable layout: that is, the bottem level table of pointers to
    hugepages is a different size, and may branch off from the normal page
    tables at a different level.  Every hugepage aware path that needs to
    walk the pagetables must therefore look up the hugepage size from the
    slice info first, and work out the correct way to walk the pagetables
    accordingly.  Future hardware is likely to add more possible hugepage
    sizes, more layout options and more mess.
    
    This patch, therefore reworks the handling of hugepage pagetables to
    reduce this complexity.  In the new scheme, instead of having to
    consult the slice mask, pagetable walking code can check a flag in the
    PGD/PUD/PMD entries to see where to branch off to hugepage pagetables,
    and the entry also contains the information (eseentially hugepage
    shift) necessary to then interpret that table without recourse to the
    slice mask.  This scheme can be extended neatly to handle multiple
    levels of self-describing "special" hugepage pagetables, although for
    now we assume only one level exists.
    
    This approach means that only the pagetable allocation path needs to
    know how the pagetables should be set out.  All other (hugepage)
    pagetable walking paths can just interpret the structure as they go.
    
    There already was a flag bit in PGD/PUD/PMD entries for hugepage
    directory pointers, but it was only used for debug.  We alter that
    flag bit to instead be a 0 in the MSB to indicate a hugepage pagetable
    pointer (normally it would be 1 since the pointer lies in the linear
    mapping).  This means that asm pagetable walking can test for (and
    punt on) hugepage pointers with the same test that checks for
    unpopulated page directory entries (beq becomes bge), since hugepage
    pointers will always be positive, and normal pointers always negative.
    
    While we're at it, we get rid of the confusing (and grep defeating)
    #defining of hugepte_shift to be the same thing as mmu_huge_psizes.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index ff24254990e1..e96d52a516ba 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -229,6 +229,20 @@ typedef unsigned long pgprot_t;
 
 #endif
 
+typedef struct { signed long pd; } hugepd_t;
+#define HUGEPD_SHIFT_MASK     0x3f
+
+#ifdef CONFIG_HUGETLB_PAGE
+static inline int hugepd_ok(hugepd_t hpd)
+{
+	return (hpd.pd > 0);
+}
+
+#define is_hugepd(pdep)               (hugepd_ok(*((hugepd_t *)(pdep))))
+#else /* CONFIG_HUGETLB_PAGE */
+#define is_hugepd(pdep)			0
+#endif /* CONFIG_HUGETLB_PAGE */
+
 struct page;
 extern void clear_user_page(void *page, unsigned long vaddr, struct page *pg);
 extern void copy_user_page(void *to, void *from, unsigned long vaddr,

commit 57e2a99f74b0d3720c97a6aadb57ae6aad3c61ea
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 28 11:59:34 2009 +1000

    powerpc: Add memory management headers for new 64-bit BookE
    
    This adds the PTE and pgtable format definitions, along with changes
    to the kernel memory map and other definitions related to implementing
    support for 64-bit Book3E. This also shields some asm-offset bits that
    are currently only relevant on 32-bit
    
    We also move the definition of the "linux" page size constants to
    the common mmu.h file and add a few sizes that are relevant to
    embedded processors.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 4940662ee87e..ff24254990e1 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -139,7 +139,11 @@ extern phys_addr_t kernstart_addr;
  * Don't compare things with KERNELBASE or PAGE_OFFSET to test for
  * "kernelness", use is_kernel_addr() - it should do what you want.
  */
+#ifdef CONFIG_PPC_BOOK3E_64
+#define is_kernel_addr(x)	((x) >= 0x8000000000000000ul)
+#else
 #define is_kernel_addr(x)	((x) >= PAGE_OFFSET)
+#endif
 
 #ifndef __ASSEMBLY__
 

commit 14f966e79445015cd89d0fa0ceb6b33702e951b6
Author: Robert Jennings <rcj@linux.vnet.ibm.com>
Date:   Wed Apr 15 05:55:32 2009 +0000

    powerpc/pseries: CMO unused page hinting
    
    Adds support for the "unused" page hint which can be used in shared
    memory partitions to flag pages not in use, which will then be stolen
    before active pages by the hypervisor when memory needs to be moved to
    LPARs in need of additional memory.  Failure to mark pages as 'unused'
    makes the LPAR slower to give up unused memory to other partitions.
    
    This adds the kernel parameter 'cmo_free_hint' to disable this
    functionality.
    
    Signed-off-by: Brian King <brking@linux.vnet.ibm.com>
    Signed-off-by: Robert Jennings <rcj@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 32cbf16f10ea..4940662ee87e 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -231,6 +231,11 @@ extern void copy_user_page(void *to, void *from, unsigned long vaddr,
 		struct page *p);
 extern int page_is_ram(unsigned long pfn);
 
+#ifdef CONFIG_PPC_SMLPAR
+void arch_free_page(struct page *page, int order);
+#define HAVE_ARCH_FREE_PAGE
+#endif
+
 struct vm_area_struct;
 
 typedef struct page *pgtable_t;

commit e12401222f749c37277a313d631dc024bbfd3b00
Author: Yuri Tikhonov <yur@emcraft.com>
Date:   Thu Jan 29 01:40:44 2009 +0000

    powerpc/44x: Support for 256KB PAGE_SIZE
    
    This patch adds support for 256KB pages on ppc44x-based boards.
    
    For simplification of implementation with 256KB pages we still assume
    2-level paging. As a side effect this leads to wasting extra memory space
    reserved for PTE tables: only 1/4 of pages allocated for PTEs are
    actually used. But this may be an acceptable trade-off to achieve the
    high performance we have with big PAGE_SIZEs in some applications (e.g.
    RAID).
    
    Also with 256KB PAGE_SIZE we increase THREAD_SIZE up to 32KB to minimize
    the risk of stack overflows in the cases of on-stack arrays, which size
    depends on the page size (e.g. multipage BIOs, NTFS, etc.).
    
    With 256KB PAGE_SIZE we need to decrease the PKMAP_ORDER at least down
    to 9, otherwise all high memory (2 ^ 10 * PAGE_SIZE == 256MB) we'll be
    occupied by PKMAP addresses leaving no place for vmalloc. We do not
    separate PKMAP_ORDER for 256K from 16K/64K PAGE_SIZE here; actually that
    value of 10 in support for 16K/64K had been selected rather intuitively.
    Thus now for all cases of PAGE_SIZE on ppc44x (including the default, 4KB,
    one) we have 512 pages for PKMAP.
    
    Because ELF standard supports only page sizes up to 64K, then you should
    use binutils later than 2.17.50.0.3 with '-zmax-page-size' set to 256K
    for building applications, which are to be run with the 256KB-page sized
    kernel. If using the older binutils, then you should patch them like follows:
    
            --- binutils/bfd/elf32-ppc.c.orig
            +++ binutils/bfd/elf32-ppc.c
    
            -#define ELF_MAXPAGESIZE                0x10000
            +#define ELF_MAXPAGESIZE                0x40000
    
    One more restriction we currently have with 256KB page sizes is inability
    to use shmem safely, so, for now, the 256KB is available only if you turn
    the CONFIG_SHMEM option off (another variant is to use BROKEN).
    Though, if you need shmem with 256KB pages, you can always remove the !SHMEM
    dependency in 'config PPC_256K_PAGES', and use the workaround available here:
     http://lkml.org/lkml/2008/12/19/20
    
    Signed-off-by: Yuri Tikhonov <yur@emcraft.com>
    Signed-off-by: Ilya Yanok <yanok@emcraft.com>
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 197d569f5bd3..32cbf16f10ea 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -19,12 +19,14 @@
 #include <asm/kdump.h>
 
 /*
- * On regular PPC32 page size is 4K (but we support 4K/16K/64K pages
+ * On regular PPC32 page size is 4K (but we support 4K/16K/64K/256K pages
  * on PPC44x). For PPC64 we support either 4K or 64K software
  * page size. When using 64K pages however, whether we are really supporting
  * 64K pages in HW or not is irrelevant to those definitions.
  */
-#if defined(CONFIG_PPC_64K_PAGES)
+#if defined(CONFIG_PPC_256K_PAGES)
+#define PAGE_SHIFT		18
+#elif defined(CONFIG_PPC_64K_PAGES)
 #define PAGE_SHIFT		16
 #elif defined(CONFIG_PPC_16K_PAGES)
 #define PAGE_SHIFT		14

commit ca9153a3a2a7556d091dfe080e42b0e67881fff6
Author: Ilya Yanok <yanok@emcraft.com>
Date:   Thu Dec 11 04:55:41 2008 +0300

    powerpc/44x: Support 16K/64K base page sizes on 44x
    
    This adds support for 16k and 64k page sizes on PowerPC 44x processors.
    
    The PGDIR table is much smaller than a page when using 16k or 64k
    pages (512 and 32 bytes respectively) so we allocate the PGDIR with
    kzalloc() instead of __get_free_pages().
    
    One PTE table covers rather a large memory area when using 16k or 64k
    pages (32MB or 512MB respectively), so we can easily put FIXMAP and
    PKMAP in the area covered by one PTE table.
    
    Signed-off-by: Yuri Tikhonov <yur@emcraft.com>
    Signed-off-by: Vladimir Panfilov <pvr@emcraft.com>
    Signed-off-by: Ilya Yanok <yanok@emcraft.com>
    Acked-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index c0b8d4a29a91..197d569f5bd3 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -19,12 +19,15 @@
 #include <asm/kdump.h>
 
 /*
- * On PPC32 page size is 4K. For PPC64 we support either 4K or 64K software
+ * On regular PPC32 page size is 4K (but we support 4K/16K/64K pages
+ * on PPC44x). For PPC64 we support either 4K or 64K software
  * page size. When using 64K pages however, whether we are really supporting
  * 64K pages in HW or not is irrelevant to those definitions.
  */
-#ifdef CONFIG_PPC_64K_PAGES
+#if defined(CONFIG_PPC_64K_PAGES)
 #define PAGE_SHIFT		16
+#elif defined(CONFIG_PPC_16K_PAGES)
+#define PAGE_SHIFT		14
 #else
 #define PAGE_SHIFT		12
 #endif
@@ -151,7 +154,7 @@ typedef struct { pte_basic_t pte; } pte_t;
 /* 64k pages additionally define a bigger "real PTE" type that gathers
  * the "second half" part of the PTE for pseudo 64k pages
  */
-#ifdef CONFIG_PPC_64K_PAGES
+#if defined(CONFIG_PPC_64K_PAGES) && defined(CONFIG_PPC_STD_MMU_64)
 typedef struct { pte_t pte; unsigned long hidx; } real_pte_t;
 #else
 typedef struct { pte_t pte; } real_pte_t;
@@ -191,10 +194,10 @@ typedef pte_basic_t pte_t;
 #define pte_val(x)	(x)
 #define __pte(x)	(x)
 
-#ifdef CONFIG_PPC_64K_PAGES
+#if defined(CONFIG_PPC_64K_PAGES) && defined(CONFIG_PPC_STD_MMU_64)
 typedef struct { pte_t pte; unsigned long hidx; } real_pte_t;
 #else
-typedef unsigned long real_pte_t;
+typedef pte_t real_pte_t;
 #endif
 
 

commit a02efb906d12c9d4eb2ab7c59049ba9545e5412d
Merge: 84dfcb4b3184 2515ddc6db8e
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Oct 21 15:52:04 2008 +1100

    Merge commit 'origin' into master
    
    Manual merge of:
    
            arch/powerpc/Kconfig
            arch/powerpc/include/asm/page.h

commit a3ba68f969b2407b6809a840f6ff45ab0eb06f84
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Mon Oct 20 03:16:55 2008 +0000

    powerpc: Fix build issue with CONFIG_RELOCATABLE=y
    
    There are two issues when we enable CONFIG_RELOCATABLE.  The first is due
    to the fact that phys_addr_t is now defined in linux/types.h.  The second
    is due to the fact that the DMA code changes expose memstart_addr to
    prom_init.c
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index 64e144505f65..120f4d494257 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -12,7 +12,9 @@
 
 #include <asm/asm-compat.h>
 #include <asm/kdump.h>
-#include <asm/types.h>
+#ifndef __ASSEMBLY__
+#include <linux/types.h>
+#endif
 
 /*
  * On PPC32 page size is 4K. For PPC64 we support either 4K or 64K software
@@ -73,6 +75,7 @@
 
 #if defined(CONFIG_RELOCATABLE)
 #ifndef __ASSEMBLY__
+
 extern phys_addr_t memstart_addr;
 extern phys_addr_t kernstart_addr;
 #endif

commit 7110879cf2afbfb7af79675f5ff109e63d631c25
Merge: 99ebcf8285df 3baf63a50709
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 20 13:21:24 2008 -0700

    Merge branch 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      m32r: fix build due to notify_cpu_starting() change
      powerpc: fix linux-next build failure

commit 463baa8a0947f858d6db1c56d87eeaf1176ba7bb
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Thu Oct 16 20:29:07 2008 +1100

    powerpc: fix linux-next build failure
    
    Today's linux-next build (powerpc allyesconfig) failed like this:
    
    In file included from arch/powerpc/include/asm/mmu-hash64.h:17,
                     from arch/powerpc/include/asm/mmu.h:8,
                     from arch/powerpc/include/asm/pgtable.h:8,
                     from arch/powerpc/mm/slb.c:20:
    arch/powerpc/include/asm/page.h:76: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'memstart_addr'
    arch/powerpc/include/asm/page.h:77: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'kernstart_addr'
    
    Caused by commit 600715dcdf567c86f8b2c6173fcfb4b873e25a19 ("generic: add
    phys_addr_t for holding physical addresses") from the tip-core tree.
    This only fails if CONFIG_RELOCATABLE is set.
    
    So include that instead of asm/types.h in asm/page.h for
    the CONFIG_RELOCATABLE case.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: ppc-dev <linuxppc-dev@ozlabs.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index e088545cb3f5..94fe5138b30f 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -10,9 +10,13 @@
  * 2 of the License, or (at your option) any later version.
  */
 
+#ifndef __ASSEMBLY__
+#include <linux/types.h>
+#else
+#include <asm/types.h>
+#endif
 #include <asm/asm-compat.h>
 #include <asm/kdump.h>
-#include <asm/types.h>
 
 /*
  * On PPC32 page size is 4K. For PPC64 we support either 4K or 64K software

commit 549e8152de8039506f69c677a4546e5427aa6ae7
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Aug 30 11:43:47 2008 +1000

    powerpc: Make the 64-bit kernel as a position-independent executable
    
    This implements CONFIG_RELOCATABLE for 64-bit by making the kernel as
    a position-independent executable (PIE) when it is set.  This involves
    processing the dynamic relocations in the image in the early stages of
    booting, even if the kernel is being run at the address it is linked at,
    since the linker does not necessarily fill in words in the image for
    which there are dynamic relocations.  (In fact the linker does fill in
    such words for 64-bit executables, though not for 32-bit executables,
    so in principle we could avoid calling relocate() entirely when we're
    running a 64-bit kernel at the linked address.)
    
    The dynamic relocations are processed by a new function relocate(addr),
    where the addr parameter is the virtual address where the image will be
    run.  In fact we call it twice; once before calling prom_init, and again
    when starting the main kernel.  This means that reloc_offset() returns
    0 in prom_init (since it has been relocated to the address it is running
    at), which necessitated a few adjustments.
    
    This also changes __va and __pa to use an equivalent definition that is
    simpler.  With the relocatable kernel, PAGE_OFFSET and MEMORY_START are
    constants (for 64-bit) whereas PHYSICAL_START is a variable (and
    KERNELBASE ideally should be too, but isn't yet).
    
    With this, relocatable kernels still copy themselves down to physical
    address 0 and run there.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
index e088545cb3f5..64e144505f65 100644
--- a/arch/powerpc/include/asm/page.h
+++ b/arch/powerpc/include/asm/page.h
@@ -71,15 +71,21 @@
 #define PAGE_OFFSET	ASM_CONST(CONFIG_PAGE_OFFSET)
 #define LOAD_OFFSET	ASM_CONST((CONFIG_KERNEL_START-CONFIG_PHYSICAL_START))
 
-#if defined(CONFIG_RELOCATABLE) && defined(CONFIG_FLATMEM)
+#if defined(CONFIG_RELOCATABLE)
 #ifndef __ASSEMBLY__
 extern phys_addr_t memstart_addr;
 extern phys_addr_t kernstart_addr;
 #endif
 #define PHYSICAL_START	kernstart_addr
-#define MEMORY_START	memstart_addr
 #else
 #define PHYSICAL_START	ASM_CONST(CONFIG_PHYSICAL_START)
+#endif
+
+#ifdef CONFIG_PPC64
+#define MEMORY_START	0UL
+#elif defined(CONFIG_RELOCATABLE)
+#define MEMORY_START	memstart_addr
+#else
 #define MEMORY_START	(PHYSICAL_START + PAGE_OFFSET - KERNELBASE)
 #endif
 
@@ -92,8 +98,8 @@ extern phys_addr_t kernstart_addr;
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 #define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 
-#define __va(x) ((void *)((unsigned long)(x) - PHYSICAL_START + KERNELBASE))
-#define __pa(x) ((unsigned long)(x) + PHYSICAL_START - KERNELBASE)
+#define __va(x) ((void *)((unsigned long)(x) + PAGE_OFFSET - MEMORY_START))
+#define __pa(x) ((unsigned long)(x) - PAGE_OFFSET + MEMORY_START)
 
 /*
  * Unfortunately the PLT is in the BSS in the PPC32 ELF ABI,

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/page.h b/arch/powerpc/include/asm/page.h
new file mode 100644
index 000000000000..e088545cb3f5
--- /dev/null
+++ b/arch/powerpc/include/asm/page.h
@@ -0,0 +1,225 @@
+#ifndef _ASM_POWERPC_PAGE_H
+#define _ASM_POWERPC_PAGE_H
+
+/*
+ * Copyright (C) 2001,2005 IBM Corporation.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <asm/asm-compat.h>
+#include <asm/kdump.h>
+#include <asm/types.h>
+
+/*
+ * On PPC32 page size is 4K. For PPC64 we support either 4K or 64K software
+ * page size. When using 64K pages however, whether we are really supporting
+ * 64K pages in HW or not is irrelevant to those definitions.
+ */
+#ifdef CONFIG_PPC_64K_PAGES
+#define PAGE_SHIFT		16
+#else
+#define PAGE_SHIFT		12
+#endif
+
+#define PAGE_SIZE		(ASM_CONST(1) << PAGE_SHIFT)
+
+/* We do define AT_SYSINFO_EHDR but don't use the gate mechanism */
+#define __HAVE_ARCH_GATE_AREA		1
+
+/*
+ * Subtle: (1 << PAGE_SHIFT) is an int, not an unsigned long. So if we
+ * assign PAGE_MASK to a larger type it gets extended the way we want
+ * (i.e. with 1s in the high bits)
+ */
+#define PAGE_MASK      (~((1 << PAGE_SHIFT) - 1))
+
+/*
+ * KERNELBASE is the virtual address of the start of the kernel, it's often
+ * the same as PAGE_OFFSET, but _might not be_.
+ *
+ * The kdump dump kernel is one example where KERNELBASE != PAGE_OFFSET.
+ *
+ * PAGE_OFFSET is the virtual address of the start of lowmem.
+ *
+ * PHYSICAL_START is the physical address of the start of the kernel.
+ *
+ * MEMORY_START is the physical address of the start of lowmem.
+ *
+ * KERNELBASE, PAGE_OFFSET, and PHYSICAL_START are all configurable on
+ * ppc32 and based on how they are set we determine MEMORY_START.
+ *
+ * For the linear mapping the following equation should be true:
+ * KERNELBASE - PAGE_OFFSET = PHYSICAL_START - MEMORY_START
+ *
+ * Also, KERNELBASE >= PAGE_OFFSET and PHYSICAL_START >= MEMORY_START
+ *
+ * There are two was to determine a physical address from a virtual one:
+ * va = pa + PAGE_OFFSET - MEMORY_START
+ * va = pa + KERNELBASE - PHYSICAL_START
+ *
+ * If you want to know something's offset from the start of the kernel you
+ * should subtract KERNELBASE.
+ *
+ * If you want to test if something's a kernel address, use is_kernel_addr().
+ */
+
+#define KERNELBASE      ASM_CONST(CONFIG_KERNEL_START)
+#define PAGE_OFFSET	ASM_CONST(CONFIG_PAGE_OFFSET)
+#define LOAD_OFFSET	ASM_CONST((CONFIG_KERNEL_START-CONFIG_PHYSICAL_START))
+
+#if defined(CONFIG_RELOCATABLE) && defined(CONFIG_FLATMEM)
+#ifndef __ASSEMBLY__
+extern phys_addr_t memstart_addr;
+extern phys_addr_t kernstart_addr;
+#endif
+#define PHYSICAL_START	kernstart_addr
+#define MEMORY_START	memstart_addr
+#else
+#define PHYSICAL_START	ASM_CONST(CONFIG_PHYSICAL_START)
+#define MEMORY_START	(PHYSICAL_START + PAGE_OFFSET - KERNELBASE)
+#endif
+
+#ifdef CONFIG_FLATMEM
+#define ARCH_PFN_OFFSET		(MEMORY_START >> PAGE_SHIFT)
+#define pfn_valid(pfn)		((pfn) >= ARCH_PFN_OFFSET && (pfn) < (ARCH_PFN_OFFSET + max_mapnr))
+#endif
+
+#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
+#define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+
+#define __va(x) ((void *)((unsigned long)(x) - PHYSICAL_START + KERNELBASE))
+#define __pa(x) ((unsigned long)(x) + PHYSICAL_START - KERNELBASE)
+
+/*
+ * Unfortunately the PLT is in the BSS in the PPC32 ELF ABI,
+ * and needs to be executable.  This means the whole heap ends
+ * up being executable.
+ */
+#define VM_DATA_DEFAULT_FLAGS32	(VM_READ | VM_WRITE | VM_EXEC | \
+				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+
+#define VM_DATA_DEFAULT_FLAGS64	(VM_READ | VM_WRITE | \
+				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+
+#ifdef __powerpc64__
+#include <asm/page_64.h>
+#else
+#include <asm/page_32.h>
+#endif
+
+/* align addr on a size boundary - adjust address up/down if needed */
+#define _ALIGN_UP(addr,size)	(((addr)+((size)-1))&(~((size)-1)))
+#define _ALIGN_DOWN(addr,size)	((addr)&(~((size)-1)))
+
+/* align addr on a size boundary - adjust address up if needed */
+#define _ALIGN(addr,size)     _ALIGN_UP(addr,size)
+
+/*
+ * Don't compare things with KERNELBASE or PAGE_OFFSET to test for
+ * "kernelness", use is_kernel_addr() - it should do what you want.
+ */
+#define is_kernel_addr(x)	((x) >= PAGE_OFFSET)
+
+#ifndef __ASSEMBLY__
+
+#undef STRICT_MM_TYPECHECKS
+
+#ifdef STRICT_MM_TYPECHECKS
+/* These are used to make use of C type-checking. */
+
+/* PTE level */
+typedef struct { pte_basic_t pte; } pte_t;
+#define pte_val(x)	((x).pte)
+#define __pte(x)	((pte_t) { (x) })
+
+/* 64k pages additionally define a bigger "real PTE" type that gathers
+ * the "second half" part of the PTE for pseudo 64k pages
+ */
+#ifdef CONFIG_PPC_64K_PAGES
+typedef struct { pte_t pte; unsigned long hidx; } real_pte_t;
+#else
+typedef struct { pte_t pte; } real_pte_t;
+#endif
+
+/* PMD level */
+#ifdef CONFIG_PPC64
+typedef struct { unsigned long pmd; } pmd_t;
+#define pmd_val(x)	((x).pmd)
+#define __pmd(x)	((pmd_t) { (x) })
+
+/* PUD level exusts only on 4k pages */
+#ifndef CONFIG_PPC_64K_PAGES
+typedef struct { unsigned long pud; } pud_t;
+#define pud_val(x)	((x).pud)
+#define __pud(x)	((pud_t) { (x) })
+#endif /* !CONFIG_PPC_64K_PAGES */
+#endif /* CONFIG_PPC64 */
+
+/* PGD level */
+typedef struct { unsigned long pgd; } pgd_t;
+#define pgd_val(x)	((x).pgd)
+#define __pgd(x)	((pgd_t) { (x) })
+
+/* Page protection bits */
+typedef struct { unsigned long pgprot; } pgprot_t;
+#define pgprot_val(x)	((x).pgprot)
+#define __pgprot(x)	((pgprot_t) { (x) })
+
+#else
+
+/*
+ * .. while these make it easier on the compiler
+ */
+
+typedef pte_basic_t pte_t;
+#define pte_val(x)	(x)
+#define __pte(x)	(x)
+
+#ifdef CONFIG_PPC_64K_PAGES
+typedef struct { pte_t pte; unsigned long hidx; } real_pte_t;
+#else
+typedef unsigned long real_pte_t;
+#endif
+
+
+#ifdef CONFIG_PPC64
+typedef unsigned long pmd_t;
+#define pmd_val(x)	(x)
+#define __pmd(x)	(x)
+
+#ifndef CONFIG_PPC_64K_PAGES
+typedef unsigned long pud_t;
+#define pud_val(x)	(x)
+#define __pud(x)	(x)
+#endif /* !CONFIG_PPC_64K_PAGES */
+#endif /* CONFIG_PPC64 */
+
+typedef unsigned long pgd_t;
+#define pgd_val(x)	(x)
+#define pgprot_val(x)	(x)
+
+typedef unsigned long pgprot_t;
+#define __pgd(x)	(x)
+#define __pgprot(x)	(x)
+
+#endif
+
+struct page;
+extern void clear_user_page(void *page, unsigned long vaddr, struct page *pg);
+extern void copy_user_page(void *to, void *from, unsigned long vaddr,
+		struct page *p);
+extern int page_is_ram(unsigned long pfn);
+
+struct vm_area_struct;
+
+typedef struct page *pgtable_t;
+
+#include <asm-generic/memory_model.h>
+#endif /* __ASSEMBLY__ */
+
+#endif /* _ASM_POWERPC_PAGE_H */
