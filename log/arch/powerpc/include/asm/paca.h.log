commit b664db8e3f976d9233cc9ea5e3f8a8c0bcabeb48
Author: Leonardo Bras <leobras.c@gmail.com>
Date:   Mon May 18 20:42:45 2020 -0300

    powerpc/rtas: Implement reentrant rtas call
    
    Implement rtas_call_reentrant() for reentrant rtas-calls:
    "ibm,int-on", "ibm,int-off",ibm,get-xive" and  "ibm,set-xive".
    
    On LoPAPR Version 1.1 (March 24, 2016), from 7.3.10.1 to 7.3.10.4,
    items 2 and 3 say:
    
    2 - For the PowerPC External Interrupt option: The * call must be
    reentrant to the number of processors on the platform.
    3 - For the PowerPC External Interrupt option: The * argument call
    buffer for each simultaneous call must be physically unique.
    
    So, these rtas-calls can be called in a lockless way, if using
    a different buffer for each cpu doing such rtas call.
    
    For this, it was suggested to add the buffer (struct rtas_args)
    in the PACA struct, so each cpu can have it's own buffer.
    The PACA struct received a pointer to rtas buffer, which is
    allocated in the memory range available to rtas 32-bit.
    
    Reentrant rtas calls are useful to avoid deadlocks in crashing,
    where rtas-calls are needed, but some other thread crashed holding
    the rtas.lock.
    
    This is a backtrace of a deadlock from a kdump testing environment:
    
      #0 arch_spin_lock
      #1  lock_rtas ()
      #2  rtas_call (token=8204, nargs=1, nret=1, outputs=0x0)
      #3  ics_rtas_mask_real_irq (hw_irq=4100)
      #4  machine_kexec_mask_interrupts
      #5  default_machine_crash_shutdown
      #6  machine_crash_shutdown
      #7  __crash_kexec
      #8  crash_kexec
      #9  oops_end
    
    Signed-off-by: Leonardo Bras <leobras.c@gmail.com>
    [mpe: Move under #ifdef PSERIES to avoid build breakage]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200518234245.200672-3-leobras.c@gmail.com

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index e3cc9eb9204d..45a839a7c6cf 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -29,6 +29,7 @@
 #include <asm/hmi.h>
 #include <asm/cpuidle.h>
 #include <asm/atomic.h>
+#include <asm/rtas-types.h>
 
 #include <asm-generic/mmiowb_types.h>
 
@@ -256,6 +257,7 @@ struct paca_struct {
 	u64 l1d_flush_size;
 #endif
 #ifdef CONFIG_PPC_PSERIES
+	struct rtas_args *rtas_args_reentrant;
 	u8 *mce_data_buf;		/* buffer to hold per cpu rtas errlog */
 #endif /* CONFIG_PPC_PSERIES */
 

commit 192f0f8e9db7efe4ac98d47f5fa4334e43c1204d
Merge: ec9249752465 f5a9e488d623
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 13 16:08:36 2019 -0700

    Merge tag 'powerpc-5.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - Removal of the NPU DMA code, used by the out-of-tree Nvidia driver,
         as well as some other functions only used by drivers that haven't
         (yet?) made it upstream.
    
       - A fix for a bug in our handling of hardware watchpoints (eg. perf
         record -e mem: ...) which could lead to register corruption and
         kernel crashes.
    
       - Enable HAVE_ARCH_HUGE_VMAP, which allows us to use large pages for
         vmalloc when using the Radix MMU.
    
       - A large but incremental rewrite of our exception handling code to
         use gas macros rather than multiple levels of nested CPP macros.
    
      And the usual small fixes, cleanups and improvements.
    
      Thanks to: Alastair D'Silva, Alexey Kardashevskiy, Andreas Schwab,
      Aneesh Kumar K.V, Anju T Sudhakar, Anton Blanchard, Arnd Bergmann,
      Athira Rajeev, Cédric Le Goater, Christian Lamparter, Christophe
      Leroy, Christophe Lombard, Christoph Hellwig, Daniel Axtens, Denis
      Efremov, Enrico Weigelt, Frederic Barrat, Gautham R. Shenoy, Geert
      Uytterhoeven, Geliang Tang, Gen Zhang, Greg Kroah-Hartman, Greg Kurz,
      Gustavo Romero, Krzysztof Kozlowski, Madhavan Srinivasan, Masahiro
      Yamada, Mathieu Malaterre, Michael Neuling, Nathan Lynch, Naveen N.
      Rao, Nicholas Piggin, Nishad Kamdar, Oliver O'Halloran, Qian Cai, Ravi
      Bangoria, Sachin Sant, Sam Bobroff, Satheesh Rajendran, Segher
      Boessenkool, Shaokun Zhang, Shawn Anastasio, Stewart Smith, Suraj
      Jitindar Singh, Thiago Jung Bauermann, YueHaibing"
    
    * tag 'powerpc-5.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (163 commits)
      powerpc/powernv/idle: Fix restore of SPRN_LDBAR for POWER9 stop state.
      powerpc/eeh: Handle hugepages in ioremap space
      ocxl: Update for AFU descriptor template version 1.1
      powerpc/boot: pass CONFIG options in a simpler and more robust way
      powerpc/boot: add {get, put}_unaligned_be32 to xz_config.h
      powerpc/irq: Don't WARN continuously in arch_local_irq_restore()
      powerpc/module64: Use symbolic instructions names.
      powerpc/module32: Use symbolic instructions names.
      powerpc: Move PPC_HA() PPC_HI() and PPC_LO() to ppc-opcode.h
      powerpc/module64: Fix comment in R_PPC64_ENTRY handling
      powerpc/boot: Add lzo support for uImage
      powerpc/boot: Add lzma support for uImage
      powerpc/boot: don't force gzipped uImage
      powerpc/8xx: Add microcode patch to move SMC parameter RAM.
      powerpc/8xx: Use IO accessors in microcode programming.
      powerpc/8xx: replace #ifdefs by IS_ENABLED() in microcode.c
      powerpc/8xx: refactor programming of microcode CPM params.
      powerpc/8xx: refactor printing of microcode patch name.
      powerpc/8xx: Refactor microcode write
      powerpc/8xx: refactor writing of CPM microcode arrays
      ...

commit 0a882e28468f48ab3d9a36dde0a5723ea29ed1ed
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Jun 28 16:33:18 2019 +1000

    powerpc/64s/exception: remove bad stack branch
    
    The bad stack test in interrupt handlers has a few problems. For
    performance it is taken in the common case, which is a fetch bubble
    and a waste of i-cache.
    
    For code development and maintainence, it requires yet another stack
    frame setup routine, and that constrains all exception handlers to
    follow the same register save pattern which inhibits future
    optimisation.
    
    Remove the test/branch and replace it with a trap. Teach the program
    check handler to use the emergency stack for this case.
    
    This does not result in quite so nice a message, however the SRR0 and
    SRR1 of the crashed interrupt can be seen in r11 and r12, as is the
    original r1 (adjusted by INT_FRAME_SIZE). These are the most important
    parts to debugging the issue.
    
    The original r9-12 and cr0 is lost, which is the main downside.
    
      kernel BUG at linux/arch/powerpc/kernel/exceptions-64s.S:847!
      Oops: Exception in kernel mode, sig: 5 [#1]
      BE SMP NR_CPUS=2048 NUMA PowerNV
      Modules linked in:
      CPU: 0 PID: 1 Comm: swapper/0 Not tainted
      NIP:  c000000000009108 LR: c000000000cadbcc CTR: c0000000000090f0
      REGS: c0000000fffcbd70 TRAP: 0700   Not tainted
      MSR:  9000000000021032 <SF,HV,ME,IR,DR,RI>  CR: 28222448  XER: 20040000
      CFAR: c000000000009100 IRQMASK: 0
      GPR00: 000000000000003d fffffffffffffd00 c0000000018cfb00 c0000000f02b3166
      GPR04: fffffffffffffffd 0000000000000007 fffffffffffffffb 0000000000000030
      GPR08: 0000000000000037 0000000028222448 0000000000000000 c000000000ca8de0
      GPR12: 9000000002009032 c000000001ae0000 c000000000010a00 0000000000000000
      GPR16: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
      GPR20: c0000000f00322c0 c000000000f85200 0000000000000004 ffffffffffffffff
      GPR24: fffffffffffffffe 0000000000000000 0000000000000000 000000000000000a
      GPR28: 0000000000000000 0000000000000000 c0000000f02b391c c0000000f02b3167
      NIP [c000000000009108] decrementer_common+0x18/0x160
      LR [c000000000cadbcc] .vsnprintf+0x3ec/0x4f0
      Call Trace:
      Instruction dump:
      996d098a 994d098b 38610070 480246ed 48005518 60000000 38200000 718a4000
      7c2a0b78 3821fd00 41c20008 e82d0970 <0981fd00> f92101a0 f9610170 f9810178
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 62f27e0aef7c..a2f713034ed3 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -170,7 +170,9 @@ struct paca_struct {
 	u64 kstack;			/* Saved Kernel stack addr */
 	u64 saved_r1;			/* r1 save for RTAS calls or PM or EE=0 */
 	u64 saved_msr;			/* MSR saved here by enter_rtas */
+#ifdef CONFIG_PPC_BOOK3E
 	u16 trap_save;			/* Used when bad stack is encountered */
+#endif
 	u8 irq_soft_mask;		/* mask for irq soft masking */
 	u8 irq_happened;		/* irq happened while soft-disabled */
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 62f27e0aef7c..9bd2326bef6f 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -1,14 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * This control block defines the PACA which defines the processor
  * specific data for each logical processor on the system.
  * There are some pointers defined that are utilized by PLIC.
  *
  * C 2001 PPC 64 Team, IBM Corp
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 #ifndef _ASM_POWERPC_PACA_H
 #define _ASM_POWERPC_PACA_H

commit b970afcfcabd63cd3832e95db096439c177c3592
Merge: 8ea5b2abd07e 8150a153c013
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 10 05:29:27 2019 -0700

    Merge tag 'powerpc-5.2-1' of ssh://gitolite.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Slightly delayed due to the issue with printk() calling
      probe_kernel_read() interacting with our new user access prevention
      stuff, but all fixed now.
    
      The only out-of-area changes are the addition of a cpuhp_state, small
      additions to Documentation and MAINTAINERS updates.
    
      Highlights:
    
       - Support for Kernel Userspace Access/Execution Prevention (like
         SMAP/SMEP/PAN/PXN) on some 64-bit and 32-bit CPUs. This prevents
         the kernel from accidentally accessing userspace outside
         copy_to/from_user(), or ever executing userspace.
    
       - KASAN support on 32-bit.
    
       - Rework of where we map the kernel, vmalloc, etc. on 64-bit hash to
         use the same address ranges we use with the Radix MMU.
    
       - A rewrite into C of large parts of our idle handling code for
         64-bit Book3S (ie. power8 & power9).
    
       - A fast path entry for syscalls on 32-bit CPUs, for a 12-17% speedup
         in the null_syscall benchmark.
    
       - On 64-bit bare metal we have support for recovering from errors
         with the time base (our clocksource), however if that fails
         currently we hang in __delay() and never crash. We now have support
         for detecting that case and short circuiting __delay() so we at
         least panic() and reboot.
    
       - Add support for optionally enabling the DAWR on Power9, which had
         to be disabled by default due to a hardware erratum. This has the
         effect of enabling hardware breakpoints for GDB, the downside is a
         badly behaved program could crash the machine by pointing the DAWR
         at cache inhibited memory. This is opt-in obviously.
    
       - xmon, our crash handler, gets support for a read only mode where
         operations that could change memory or otherwise disturb the system
         are disabled.
    
      Plus many clean-ups, reworks and minor fixes etc.
    
      Thanks to: Christophe Leroy, Akshay Adiga, Alastair D'Silva, Alexey
      Kardashevskiy, Andrew Donnellan, Aneesh Kumar K.V, Anju T Sudhakar,
      Anton Blanchard, Ben Hutchings, Bo YU, Breno Leitao, Cédric Le Goater,
      Christopher M. Riedl, Christoph Hellwig, Colin Ian King, David Gibson,
      Ganesh Goudar, Gautham R. Shenoy, George Spelvin, Greg Kroah-Hartman,
      Greg Kurz, Horia Geantă, Jagadeesh Pagadala, Joel Stanley, Joe
      Perches, Julia Lawall, Laurentiu Tudor, Laurent Vivier, Lukas Bulwahn,
      Madhavan Srinivasan, Mahesh Salgaonkar, Mathieu Malaterre, Michael
      Neuling, Mukesh Ojha, Nathan Fontenot, Nathan Lynch, Nicholas Piggin,
      Nick Desaulniers, Oliver O'Halloran, Peng Hao, Qian Cai, Ravi
      Bangoria, Rick Lindsley, Russell Currey, Sachin Sant, Stewart Smith,
      Sukadev Bhattiprolu, Thomas Huth, Tobin C. Harding, Tyrel Datwyler,
      Valentin Schneider, Wei Yongjun, Wen Yang, YueHaibing"
    
    * tag 'powerpc-5.2-1' of ssh://gitolite.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (205 commits)
      powerpc/64s: Use early_mmu_has_feature() in set_kuap()
      powerpc/book3s/64: check for NULL pointer in pgd_alloc()
      powerpc/mm: Fix hugetlb page initialization
      ocxl: Fix return value check in afu_ioctl()
      powerpc/mm: fix section mismatch for setup_kup()
      powerpc/mm: fix redundant inclusion of pgtable-frag.o in Makefile
      powerpc/mm: Fix makefile for KASAN
      powerpc/kasan: add missing/lost Makefile
      selftests/powerpc: Add a signal fuzzer selftest
      powerpc/booke64: set RI in default MSR
      ocxl: Provide global MMIO accessors for external drivers
      ocxl: move event_fd handling to frontend
      ocxl: afu_irq only deals with IRQ IDs, not offsets
      ocxl: Allow external drivers to use OpenCAPI contexts
      ocxl: Create a clear delineation between ocxl backend & frontend
      ocxl: Don't pass pci_dev around
      ocxl: Split pci.c
      ocxl: Remove some unused exported symbols
      ocxl: Remove superfluous 'extern' from headers
      ocxl: read_pasid never returns an error, so make it void
      ...

commit 10d91611f426d4bafd2a83d966c36da811b2f7ad
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Apr 13 00:30:52 2019 +1000

    powerpc/64s: Reimplement book3s idle code in C
    
    Reimplement Book3S idle code in C, moving POWER7/8/9 implementation
    speific HV idle code to the powernv platform code.
    
    Book3S assembly stubs are kept in common code and used only to save
    the stack frame and non-volatile GPRs before executing architected
    idle instructions, and restoring the stack and reloading GPRs then
    returning to C after waking from idle.
    
    The complex logic dealing with threads and subcores, locking, SPRs,
    HMIs, timebase resync, etc., is all done in C which makes it more
    maintainable.
    
    This is not a strict translation to C code, there are some
    significant differences:
    
    - Idle wakeup no longer uses the ->cpu_restore call to reinit SPRs,
      but saves and restores them itself.
    
    - The optimisation where EC=ESL=0 idle modes did not have to save GPRs
      or change MSR is restored, because it's now simple to do. ESL=1
      sleeps that do not lose GPRs can use this optimization too.
    
    - KVM secondary entry and cede is now more of a call/return style
      rather than branchy. nap_state_lost is not required because KVM
      always returns via NVGPR restoring path.
    
    - KVM secondary wakeup from offline sequence is moved entirely into
      the offline wakeup, which avoids a hwsync in the normal idle wakeup
      path.
    
    Performance measured with context switch ping-pong on different
    threads or cores, is possibly improved a small amount, 1-3% depending
    on stop state and core vs thread test for shallow states. Deep states
    it's in the noise compared with other latencies.
    
    KVM improvements:
    
    - Idle sleepers now always return to caller rather than branch out
      to KVM first.
    
    - This allows optimisations like very fast return to caller when no
      state has been lost.
    
    - KVM no longer requires nap_state_lost because it controls NVGPR
      save/restore itself on the way in and out.
    
    - The heavy idle wakeup KVM request check can be moved out of the
      normal host idle code and into the not-performance-critical offline
      code.
    
    - KVM nap code now returns from where it is called, which makes the
      flow a bit easier to follow.
    
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Squash the KVM changes in]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index e843bc5d1a0f..245d11a71784 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -173,7 +173,6 @@ struct paca_struct {
 	u8 irq_happened;		/* irq happened while soft-disabled */
 	u8 io_sync;			/* writel() needs spin_unlock sync */
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */
-	u8 nap_state_lost;		/* NV GPR values lost in power7_idle */
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	u8 pmcregs_in_use;		/* pseries puts this in lppaca */
 #endif
@@ -183,23 +182,28 @@ struct paca_struct {
 #endif
 
 #ifdef CONFIG_PPC_POWERNV
-	/* Per-core mask tracking idle threads and a lock bit-[L][TTTTTTTT] */
-	u32 *core_idle_state_ptr;
-	u8 thread_idle_state;		/* PNV_THREAD_RUNNING/NAP/SLEEP	*/
-	/* Mask to indicate thread id in core */
-	u8 thread_mask;
-	/* Mask to denote subcore sibling threads */
-	u8 subcore_sibling_mask;
-	/* Flag to request this thread not to stop */
-	atomic_t dont_stop;
-	/* The PSSCR value that the kernel requested before going to stop */
-	u64 requested_psscr;
-
-	/*
-	 * Save area for additional SPRs that need to be
-	 * saved/restored during cpuidle stop.
-	 */
-	struct stop_sprs stop_sprs;
+	/* PowerNV idle fields */
+	/* PNV_CORE_IDLE_* bits, all siblings work on thread 0 paca */
+	unsigned long idle_state;
+	union {
+		/* P7/P8 specific fields */
+		struct {
+			/* PNV_THREAD_RUNNING/NAP/SLEEP	*/
+			u8 thread_idle_state;
+			/* Mask to denote subcore sibling threads */
+			u8 subcore_sibling_mask;
+		};
+
+		/* P9 specific fields */
+		struct {
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+			/* The PSSCR value that the kernel requested before going to stop */
+			u64 requested_psscr;
+			/* Flag to request this thread not to stop */
+			atomic_t dont_stop;
+#endif
+		};
+	};
 #endif
 
 #ifdef CONFIG_PPC_BOOK3S_64

commit 420af1554790a95e6813f56f63b6d2361614082b
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Feb 22 14:45:42 2019 +0000

    powerpc/mmiowb: Hook up mmwiob() implementation to asm-generic code
    
    In a bid to kill off explicit mmiowb() usage in driver code, hook up
    the asm-generic mmiowb() tracking code but provide a definition of
    arch_mmiowb_state() so that the tracking data can remain in the paca
    as it does at present
    
    This replaces the existing (flawed) implementation.
    
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index e843bc5d1a0f..134e912d403f 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -34,6 +34,8 @@
 #include <asm/cpuidle.h>
 #include <asm/atomic.h>
 
+#include <asm-generic/mmiowb_types.h>
+
 register struct paca_struct *local_paca asm("r13");
 
 #if defined(CONFIG_DEBUG_PREEMPT) && defined(CONFIG_SMP)
@@ -171,7 +173,6 @@ struct paca_struct {
 	u16 trap_save;			/* Used when bad stack is encountered */
 	u8 irq_soft_mask;		/* mask for irq soft masking */
 	u8 irq_happened;		/* irq happened while soft-disabled */
-	u8 io_sync;			/* writel() needs spin_unlock sync */
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */
 	u8 nap_state_lost;		/* NV GPR values lost in power7_idle */
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
@@ -264,6 +265,9 @@ struct paca_struct {
 #ifdef CONFIG_STACKPROTECTOR
 	unsigned long canary;
 #endif
+#ifdef CONFIG_MMIOWB
+	struct mmiowb_state mmiowb_state;
+#endif
 } ____cacheline_aligned;
 
 extern void copy_mm_to_paca(struct mm_struct *mm);

commit e15a4fea4dee2771c6989862527546b2b3326799
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Oct 3 00:27:59 2018 +1000

    powerpc/64s/hash: Add some SLB debugging tests
    
    This adds CONFIG_DEBUG_VM checks to ensure:
      - The kernel stack is in the SLB after it's flushed and bolted.
      - We don't insert an SLB for an address that is aleady in the SLB.
      - The kernel SLB miss handler does not take an SLB miss.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 97e8a57a4998..e843bc5d1a0f 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -115,6 +115,9 @@ struct paca_struct {
 	u16 vmalloc_sllp;
 	u8 slb_cache_ptr;
 	u8 stab_rr;			/* stab/slb round-robin counter */
+#ifdef CONFIG_DEBUG_VM
+	u8 in_kernel_slb_handler;
+#endif
 	u32 slb_used_bitmap;		/* Bitmaps for first 32 SLB entries. */
 	u32 slb_kern_bitmap;
 	u32 slb_cache[SLB_CACHE_ENTRIES];

commit 126b11b294d15a90e38eb2dcded2433619b2c794
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:53 2018 +1000

    powerpc/64s/hash: Add SLB allocation status bitmaps
    
    Add 32-entry bitmaps to track the allocation status of the first 32
    SLB entries, and whether they are user or kernel entries. These are
    used to allocate free SLB entries first, before resorting to the round
    robin allocator.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index c6d01f0aa898..97e8a57a4998 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -113,7 +113,10 @@ struct paca_struct {
  				 * on the linear mapping */
 	/* SLB related definitions */
 	u16 vmalloc_sllp;
-	u16 slb_cache_ptr;
+	u8 slb_cache_ptr;
+	u8 stab_rr;			/* stab/slb round-robin counter */
+	u32 slb_used_bitmap;		/* Bitmaps for first 32 SLB entries. */
+	u32 slb_kern_bitmap;
 	u32 slb_cache[SLB_CACHE_ENTRIES];
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
@@ -160,7 +163,6 @@ struct paca_struct {
 	 */
 	struct task_struct *__current;	/* Pointer to current */
 	u64 kstack;			/* Saved Kernel stack addr */
-	u64 stab_rr;			/* stab/slb round-robin counter */
 	u64 saved_r1;			/* r1 save for RTAS calls or PM or EE=0 */
 	u64 saved_msr;			/* MSR saved here by enter_rtas */
 	u16 trap_save;			/* Used when bad stack is encountered */

commit 06ec27aea9fc84d9c6d879eb64b5bcf28a8a1eb7
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Sep 27 07:05:55 2018 +0000

    powerpc/64: add stack protector support
    
    On PPC64, as register r13 points to the paca_struct at all time,
    this patch adds a copy of the canary there, which is copied at
    task_switch.
    That new canary is then used by using the following GCC options:
    -mstack-protector-guard=tls
    -mstack-protector-guard-reg=r13
    -mstack-protector-guard-offset=offsetof(struct paca_struct, canary))
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 7b6e23af3808..c6d01f0aa898 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -256,6 +256,9 @@ struct paca_struct {
 	struct slb_entry *mce_faulty_slbs;
 	u16 slb_save_cache_ptr;
 #endif /* CONFIG_PPC_BOOK3S_64 */
+#ifdef CONFIG_STACKPROTECTOR
+	unsigned long canary;
+#endif
 } ____cacheline_aligned;
 
 extern void copy_mm_to_paca(struct mm_struct *mm);

commit 54be0b9c7c9888ebe63b89a31a17ee3df6a68d61
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Oct 2 23:56:39 2018 +1000

    Revert "convert SLB miss handlers to C" and subsequent commits
    
    This reverts commits:
      5e46e29e6a97 ("powerpc/64s/hash: convert SLB miss handlers to C")
      8fed04d0f6ae ("powerpc/64s/hash: remove user SLB data from the paca")
      655deecf67b2 ("powerpc/64s/hash: SLB allocation status bitmaps")
      2e1626744e8d ("powerpc/64s/hash: provide arch_setup_exec hooks for hash slice setup")
      89ca4e126a3f ("powerpc/64s/hash: Add a SLB preload cache")
    
    This series had a few bugs, and the fixes are not all trivial. So
    revert most of it for now.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 6d6b3706232c..7b6e23af3808 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -113,10 +113,7 @@ struct paca_struct {
  				 * on the linear mapping */
 	/* SLB related definitions */
 	u16 vmalloc_sllp;
-	u8 slb_cache_ptr;
-	u8 stab_rr;			/* stab/slb round-robin counter */
-	u32 slb_used_bitmap;		/* Bitmaps for first 32 SLB entries. */
-	u32 slb_kern_bitmap;
+	u16 slb_cache_ptr;
 	u32 slb_cache[SLB_CACHE_ENTRIES];
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
@@ -146,11 +143,24 @@ struct paca_struct {
 	struct tlb_core_data tcd;
 #endif /* CONFIG_PPC_BOOK3E */
 
+#ifdef CONFIG_PPC_BOOK3S
+	mm_context_id_t mm_ctx_id;
+#ifdef CONFIG_PPC_MM_SLICES
+	unsigned char mm_ctx_low_slices_psize[BITS_PER_LONG / BITS_PER_BYTE];
+	unsigned char mm_ctx_high_slices_psize[SLICE_ARRAY_SIZE];
+	unsigned long mm_ctx_slb_addr_limit;
+#else
+	u16 mm_ctx_user_psize;
+	u16 mm_ctx_sllp;
+#endif
+#endif
+
 	/*
 	 * then miscellaneous read-write fields
 	 */
 	struct task_struct *__current;	/* Pointer to current */
 	u64 kstack;			/* Saved Kernel stack addr */
+	u64 stab_rr;			/* stab/slb round-robin counter */
 	u64 saved_r1;			/* r1 save for RTAS calls or PM or EE=0 */
 	u64 saved_msr;			/* MSR saved here by enter_rtas */
 	u16 trap_save;			/* Used when bad stack is encountered */
@@ -248,6 +258,7 @@ struct paca_struct {
 #endif /* CONFIG_PPC_BOOK3S_64 */
 } ____cacheline_aligned;
 
+extern void copy_mm_to_paca(struct mm_struct *mm);
 extern struct paca_struct **paca_ptrs;
 extern void initialise_paca(struct paca_struct *new_paca, int cpu);
 extern void setup_paca(struct paca_struct *new_paca);

commit 655deecf67b240bf7bb4e73df4e1235900c26a01
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:53 2018 +1000

    powerpc/64s/hash: SLB allocation status bitmaps
    
    Add 32-entry bitmaps to track the allocation status of the first 32
    SLB entries, and whether they are user or kernel entries. These are
    used to allocate free SLB entries first, before resorting to the round
    robin allocator.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 8144d673541a..6d6b3706232c 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -113,7 +113,10 @@ struct paca_struct {
  				 * on the linear mapping */
 	/* SLB related definitions */
 	u16 vmalloc_sllp;
-	u16 slb_cache_ptr;
+	u8 slb_cache_ptr;
+	u8 stab_rr;			/* stab/slb round-robin counter */
+	u32 slb_used_bitmap;		/* Bitmaps for first 32 SLB entries. */
+	u32 slb_kern_bitmap;
 	u32 slb_cache[SLB_CACHE_ENTRIES];
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
@@ -148,7 +151,6 @@ struct paca_struct {
 	 */
 	struct task_struct *__current;	/* Pointer to current */
 	u64 kstack;			/* Saved Kernel stack addr */
-	u64 stab_rr;			/* stab/slb round-robin counter */
 	u64 saved_r1;			/* r1 save for RTAS calls or PM or EE=0 */
 	u64 saved_msr;			/* MSR saved here by enter_rtas */
 	u16 trap_save;			/* Used when bad stack is encountered */

commit 8fed04d0f6aedf99b3d811ba58d38bb7f938a47a
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:52 2018 +1000

    powerpc/64s/hash: remove user SLB data from the paca
    
    User SLB mappig data is copied into the PACA from the mm->context so
    it can be accessed by the SLB miss handlers.
    
    After the C conversion, SLB miss handlers now run with relocation on,
    and user SLB misses are able to take recursive kernel SLB misses, so
    the user SLB mapping data can be removed from the paca and accessed
    directly.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 7b6e23af3808..8144d673541a 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -143,18 +143,6 @@ struct paca_struct {
 	struct tlb_core_data tcd;
 #endif /* CONFIG_PPC_BOOK3E */
 
-#ifdef CONFIG_PPC_BOOK3S
-	mm_context_id_t mm_ctx_id;
-#ifdef CONFIG_PPC_MM_SLICES
-	unsigned char mm_ctx_low_slices_psize[BITS_PER_LONG / BITS_PER_BYTE];
-	unsigned char mm_ctx_high_slices_psize[SLICE_ARRAY_SIZE];
-	unsigned long mm_ctx_slb_addr_limit;
-#else
-	u16 mm_ctx_user_psize;
-	u16 mm_ctx_sllp;
-#endif
-#endif
-
 	/*
 	 * then miscellaneous read-write fields
 	 */
@@ -258,7 +246,6 @@ struct paca_struct {
 #endif /* CONFIG_PPC_BOOK3S_64 */
 } ____cacheline_aligned;
 
-extern void copy_mm_to_paca(struct mm_struct *mm);
 extern struct paca_struct **paca_ptrs;
 extern void initialise_paca(struct paca_struct *new_paca, int cpu);
 extern void setup_paca(struct paca_struct *new_paca);

commit c6d15258cdf1c197cad7b11b9848e79068dd21e0
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Tue Sep 11 19:57:15 2018 +0530

    powerpc/pseries: Dump the SLB contents on SLB MCE errors.
    
    If we get a machine check exceptions due to SLB errors then dump the
    current SLB contents which will be very much helpful in debugging the
    root cause of SLB errors. Introduce an exclusive buffer per cpu to hold
    faulty SLB entries. In real mode mce handler saves the old SLB contents
    into this buffer accessible through paca and print it out later in virtual
    mode.
    
    With this patch the console will log SLB contents like below on SLB MCE
    errors:
    
    [  507.297236] SLB contents of cpu 0x1
    [  507.297237] Last SLB entry inserted at slot 16
    [  507.297238] 00 c000000008000000 400ea1b217000500
    [  507.297239]   1T  ESID=   c00000  VSID=      ea1b217 LLP:100
    [  507.297240] 01 d000000008000000 400d43642f000510
    [  507.297242]   1T  ESID=   d00000  VSID=      d43642f LLP:110
    [  507.297243] 11 f000000008000000 400a86c85f000500
    [  507.297244]   1T  ESID=   f00000  VSID=      a86c85f LLP:100
    [  507.297245] 12 00007f0008000000 4008119624000d90
    [  507.297246]   1T  ESID=       7f  VSID=      8119624 LLP:110
    [  507.297247] 13 0000000018000000 00092885f5150d90
    [  507.297247]  256M ESID=        1  VSID=   92885f5150 LLP:110
    [  507.297248] 14 0000010008000000 4009e7cb50000d90
    [  507.297249]   1T  ESID=        1  VSID=      9e7cb50 LLP:110
    [  507.297250] 15 d000000008000000 400d43642f000510
    [  507.297251]   1T  ESID=   d00000  VSID=      d43642f LLP:110
    [  507.297252] 16 d000000008000000 400d43642f000510
    [  507.297253]   1T  ESID=   d00000  VSID=      d43642f LLP:110
    [  507.297253] ----------------------------------
    [  507.297254] SLB cache ptr value = 3
    [  507.297254] Valid SLB cache entries:
    [  507.297255] 00 EA[0-35]=    7f000
    [  507.297256] 01 EA[0-35]=        1
    [  507.297257] 02 EA[0-35]=     1000
    [  507.297257] Rest of SLB cache entries:
    [  507.297258] 03 EA[0-35]=    7f000
    [  507.297258] 04 EA[0-35]=        1
    [  507.297259] 05 EA[0-35]=     1000
    [  507.297260] 06 EA[0-35]=       12
    [  507.297260] 07 EA[0-35]=    7f000
    
    Suggested-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Suggested-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index ad4f16164619..7b6e23af3808 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -250,6 +250,12 @@ struct paca_struct {
 #ifdef CONFIG_PPC_PSERIES
 	u8 *mce_data_buf;		/* buffer to hold per cpu rtas errlog */
 #endif /* CONFIG_PPC_PSERIES */
+
+#ifdef CONFIG_PPC_BOOK3S_64
+	/* Capture SLB related old contents in MCE handler. */
+	struct slb_entry *mce_faulty_slbs;
+	u16 slb_save_cache_ptr;
+#endif /* CONFIG_PPC_BOOK3S_64 */
 } ____cacheline_aligned;
 
 extern void copy_mm_to_paca(struct mm_struct *mm);

commit 94675cceacaec27a30eefb142c4c59a9d3131742
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Wed Jul 4 23:27:21 2018 +0530

    powerpc/pseries: Defer the logging of rtas error to irq work queue.
    
    rtas_log_buf is a buffer to hold RTAS event data that are communicated
    to kernel by hypervisor. This buffer is then used to pass RTAS event
    data to user through proc fs. This buffer is allocated from
    vmalloc (non-linear mapping) area.
    
    On Machine check interrupt, register r3 points to RTAS extended event
    log passed by hypervisor that contains the MCE event. The pseries
    machine check handler then logs this error into rtas_log_buf. The
    rtas_log_buf is a vmalloc-ed (non-linear) buffer we end up taking up a
    page fault (vector 0x300) while accessing it. Since machine check
    interrupt handler runs in NMI context we can not afford to take any
    page fault. Page faults are not honored in NMI context and causes
    kernel panic. Apart from that, as Nick pointed out,
    pSeries_log_error() also takes a spin_lock while logging error which
    is not safe in NMI context. It may endup in deadlock if we get another
    MCE before releasing the lock. Fix this by deferring the logging of
    rtas error to irq work queue.
    
    Current implementation uses two different buffers to hold rtas error
    log depending on whether extended log is provided or not. This makes
    bit difficult to identify which buffer has valid data that needs to
    logged later in irq work. Simplify this using single buffer, one per
    paca, and copy rtas log to it irrespective of whether extended log is
    provided or not. Allocate this buffer below RMA region so that it can
    be accessed in real mode mce handler.
    
    Fixes: b96672dd840f ("powerpc: Machine check interrupt is a non-maskable interrupt")
    Cc: stable@vger.kernel.org # v4.14+
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 4e9cede5a7e7..ad4f16164619 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -247,6 +247,9 @@ struct paca_struct {
 	void *rfi_flush_fallback_area;
 	u64 l1d_flush_size;
 #endif
+#ifdef CONFIG_PPC_PSERIES
+	u8 *mce_data_buf;		/* buffer to hold per cpu rtas errlog */
+#endif /* CONFIG_PPC_PSERIES */
 } ____cacheline_aligned;
 
 extern void copy_mm_to_paca(struct mm_struct *mm);

commit 2bf1071a8d50928a4ae366bb3108833166c2b70c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Jul 5 18:47:00 2018 +1000

    powerpc/64s: Remove POWER9 DD1 support
    
    POWER9 DD1 was never a product. It is no longer supported by upstream
    firmware, and it is not effectively supported in Linux due to lack of
    testing.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Michael Ellerman <mpe@ellerman.id.au>
    [mpe: Remove arch_make_huge_pte() entirely]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 6d34bd71139d..4e9cede5a7e7 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -187,11 +187,6 @@ struct paca_struct {
 	u8 subcore_sibling_mask;
 	/* Flag to request this thread not to stop */
 	atomic_t dont_stop;
-	/*
-	 * Pointer to an array which contains pointer
-	 * to the sibling threads' paca.
-	 */
-	struct paca_struct **thread_sibling_pacas;
 	/* The PSSCR value that the kernel requested before going to stop */
 	u64 requested_psscr;
 

commit 7b08729cb272b4cd5c657cd5ac0dddae15a593ff
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed May 2 23:07:26 2018 +1000

    powerpc/64: Save stack pointer when we hard disable interrupts
    
    A CPU that gets stuck with interrupts hard disable can be difficult to
    debug, as on some platforms we have no way to interrupt the CPU to
    find out what it's doing.
    
    A stop-gap is to have the CPU save it's stack pointer (r1) in its paca
    when it hard disables interrupts. That way if we can't interrupt it,
    we can at least trace the stack based on where it last disabled
    interrupts.
    
    In some cases that will be total junk, but the stack trace code should
    handle that. In the simple case of a CPU that disable interrupts and
    then gets stuck in a loop, the stack trace should be informative.
    
    We could clear the saved stack pointer when we enable interrupts, but
    that loses information which could be useful if we have nothing else
    to go on.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 81471bd08f66..6d34bd71139d 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -161,7 +161,7 @@ struct paca_struct {
 	struct task_struct *__current;	/* Pointer to current */
 	u64 kstack;			/* Saved Kernel stack addr */
 	u64 stab_rr;			/* stab/slb round-robin counter */
-	u64 saved_r1;			/* r1 save for RTAS calls or PM */
+	u64 saved_r1;			/* r1 save for RTAS calls or PM or EE=0 */
 	u64 saved_msr;			/* MSR saved here by enter_rtas */
 	u16 trap_save;			/* Used when bad stack is encountered */
 	u8 irq_soft_mask;		/* mask for irq soft masking */

commit f1079d3a3d1f468f8faa3f506c163d44fac01912
Merge: b5240b14396d faf37c44a105
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sun Jun 3 20:32:02 2018 +1000

    Merge branch 'fixes' into next
    
    We ended up with an ugly conflict between fixes and next in ftrace.h
    involving multiple nested ifdefs, and the automatic resolution is
    wrong. So merge fixes into next so we can fix it up.

commit c4ec1f0353b342473b93637fd0c3fb524bedbb2d
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed May 2 16:57:54 2018 +1000

    powerpc/64: Remove unused paca->soft_enabled
    
    In commit 4e26bc4a4ed6 ("powerpc/64: Rename soft_enabled to
    irq_soft_mask") we renamed paca->soft_enabled. But then in commit
    8e0b634b1327 ("powerpc/64s: Do not allocate lppaca if we are not
    virtualized") we added it back. Oops. This happened because the two
    patches were in flight at the same time and rebased vs each other
    multiple times, and we missed it in review.
    
    Fixes: 8e0b634b1327 ("powerpc/64s: Do not allocate lppaca if we are not virtualized")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 4185f1c96125..3f109a3e3edb 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -165,7 +165,6 @@ struct paca_struct {
 	u64 saved_msr;			/* MSR saved here by enter_rtas */
 	u16 trap_save;			/* Used when bad stack is encountered */
 	u8 irq_soft_mask;		/* mask for irq soft masking */
-	u8 soft_enabled;		/* irq soft-enable flag */
 	u8 irq_happened;		/* irq happened while soft-disabled */
 	u8 io_sync;			/* writel() needs spin_unlock sync */
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */

commit ea678ac627e01daf5b4f1da24bf1d0c500e10898
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Thu Apr 19 12:34:00 2018 +0530

    powerpc64/ftrace: Add a field in paca to disable ftrace in unsafe code paths
    
    We have some C code that we call into from real mode where we cannot
    take any exceptions. Though the C functions themselves are mostly safe,
    if these functions are traced, there is a possibility that we may take
    an exception. For instance, in certain conditions, the ftrace code uses
    WARN(), which uses a 'trap' to do its job.
    
    For such scenarios, introduce a new field in paca 'ftrace_enabled',
    which is checked on ftrace entry before continuing. This field can then
    be set to zero to disable/pause ftrace, and set to a non-zero value to
    resume ftrace.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 4185f1c96125..163f13f31255 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -223,6 +223,7 @@ struct paca_struct {
 	u8 hmi_event_available;		/* HMI event is available */
 	u8 hmi_p9_special_emu;		/* HMI P9 special emulation */
 #endif
+	u8 ftrace_enabled;		/* Hard disable ftrace */
 
 	/* Stuff for accurate time accounting */
 	struct cpu_accounting_data accounting;

commit f437c51748fa1dd423a878c870ad203843a51c8d
Merge: 872a100a49c3 29ab6c4708a5
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Mar 31 00:11:24 2018 +1100

    Merge branch 'topic/paca' into next
    
    Bring in yet another series that touches KVM code, and might need to
    be merged into the kvm-ppc branch to resolve conflicts.
    
    This required some changes in pnv_power9_force_smt4_catch/release()
    due to the paca array becomming an array of pointers.

commit 59f577743d71bf796ceac10961bf6cfa5ca26786
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:19 2018 +1000

    powerpc/64: Defer paca allocation until memory topology is discovered
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Rename the dummy allocate_pacas() to fix 32-bit build]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index e89887f5e56f..2d04c5575631 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -255,12 +255,14 @@ extern void copy_mm_to_paca(struct mm_struct *mm);
 extern struct paca_struct **paca_ptrs;
 extern void initialise_paca(struct paca_struct *new_paca, int cpu);
 extern void setup_paca(struct paca_struct *new_paca);
-extern void allocate_pacas(void);
+extern void allocate_paca_ptrs(void);
+extern void allocate_paca(int cpu);
 extern void free_unused_pacas(void);
 
 #else /* CONFIG_PPC64 */
 
-static inline void allocate_pacas(void) { };
+static inline void allocate_paca_ptrs(void) { };
+static inline void allocate_paca(int cpu) { };
 static inline void free_unused_pacas(void) { };
 
 #endif /* CONFIG_PPC64 */

commit d2e60075a3d4422dc54b919f3b125d8066b839d4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:12 2018 +1000

    powerpc/64: Use array of paca pointers and allocate pacas individually
    
    Change the paca array into an array of pointers to pacas. Allocate
    pacas individually.
    
    This allows flexibility in where the PACAs are allocated. Future work
    will allocate them node-local. Platforms that don't have address limits
    on PACAs would be able to defer PACA allocations until later in boot
    rather than allocate all possible ones up-front then freeing unused.
    
    This is slightly more overhead (one additional indirection) for cross
    CPU paca references, but those aren't too common.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 6db5ab2a29a3..e89887f5e56f 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -249,10 +249,10 @@ struct paca_struct {
 	void *rfi_flush_fallback_area;
 	u64 l1d_flush_size;
 #endif
-};
+} ____cacheline_aligned;
 
 extern void copy_mm_to_paca(struct mm_struct *mm);
-extern struct paca_struct *paca;
+extern struct paca_struct **paca_ptrs;
 extern void initialise_paca(struct paca_struct *new_paca, int cpu);
 extern void setup_paca(struct paca_struct *new_paca);
 extern void allocate_pacas(void);

commit 8e0b634b132752ec3eba50afb952502b1a87d6ba
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:11 2018 +1000

    powerpc/64s: Do not allocate lppaca if we are not virtualized
    
    The "lppaca" is a structure registered with the hypervisor. This is
    unnecessary when running on non-virtualised platforms. One field from
    the lppaca (pmcregs_in_use) is also used by the host, so move the host
    part out into the paca (lppaca field is still updated in
    guest mode).
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Fix non-pseries build with some #ifdefs]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index b62c31037cad..6db5ab2a29a3 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -46,7 +46,10 @@ extern unsigned int debug_smp_processor_id(void); /* from linux/smp.h */
 #define get_paca()	local_paca
 #endif
 
+#ifdef CONFIG_PPC_PSERIES
 #define get_lppaca()	(get_paca()->lppaca_ptr)
+#endif
+
 #define get_slb_shadow()	(get_paca()->slb_shadow_ptr)
 
 struct task_struct;
@@ -58,7 +61,7 @@ struct task_struct;
  * processor.
  */
 struct paca_struct {
-#ifdef CONFIG_PPC_BOOK3S
+#ifdef CONFIG_PPC_PSERIES
 	/*
 	 * Because hw_cpu_id, unlike other paca fields, is accessed
 	 * routinely from other CPUs (from the IRQ code), we stick to
@@ -67,7 +70,8 @@ struct paca_struct {
 	 */
 
 	struct lppaca *lppaca_ptr;	/* Pointer to LpPaca for PLIC */
-#endif /* CONFIG_PPC_BOOK3S */
+#endif /* CONFIG_PPC_PSERIES */
+
 	/*
 	 * MAGIC: the spinlock functions in arch/powerpc/lib/locks.c 
 	 * load lock_token and paca_index with a single lwz
@@ -160,10 +164,14 @@ struct paca_struct {
 	u64 saved_msr;			/* MSR saved here by enter_rtas */
 	u16 trap_save;			/* Used when bad stack is encountered */
 	u8 irq_soft_mask;		/* mask for irq soft masking */
+	u8 soft_enabled;		/* irq soft-enable flag */
 	u8 irq_happened;		/* irq happened while soft-disabled */
 	u8 io_sync;			/* writel() needs spin_unlock sync */
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */
 	u8 nap_state_lost;		/* NV GPR values lost in power7_idle */
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	u8 pmcregs_in_use;		/* pseries puts this in lppaca */
+#endif
 	u64 sprg_vdso;			/* Saved user-visible sprg */
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 	u64 tm_scratch;                 /* TM scratch area for reclaim */

commit a26cf1c9fe3c2e3b671b490aeb708ea72fb5ac0a
Merge: 78e5dfea84dc 681c617b7c42
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Mar 24 08:43:18 2018 +1100

    Merge branch 'topic/ppc-kvm' into next
    
    This brings in two series from Paul, one of which touches KVM code and
    may need to be merged into the kvm-ppc tree to resolve conflicts.

commit 7672691a08c886e53ccbf8cdca406f8c92ec7a20
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Mar 21 21:32:00 2018 +1100

    powerpc/powernv: Provide a way to force a core into SMT4 mode
    
    POWER9 processors up to and including "Nimbus" v2.2 have hardware
    bugs relating to transactional memory and thread reconfiguration.
    One of these bugs has a workaround which is to get the core into
    SMT4 state temporarily.  This workaround is only needed when
    running bare-metal.
    
    This patch provides a function which gets the core into SMT4 mode
    by preventing threads from going to a stop state, and waking up
    those which are already in a stop state.  Once at least 3 threads
    are not in a stop state, the core will be in SMT4 and we can
    continue.
    
    To do this, we add a "dont_stop" flag to the paca to tell the
    thread not to go into a stop state.  If this flag is set,
    power9_idle_stop() just returns immediately with a return value
    of 0.  The pnv_power9_force_smt4_catch() function does the following:
    
    1. Set the dont_stop flag for each thread in the core, except
       ourselves (in fact we use an atomic_inc() in case more than
       one thread is calling this function concurrently).
    2. See how many threads are awake, indicated by their
       requested_psscr field in the paca being 0.  If this is at
       least 3, skip to step 5.
    3. Send a doorbell interrupt to each thread that was seen as
       being in a stop state in step 2.
    4. Until at least 3 threads are awake, scan the threads to which
       we sent a doorbell interrupt and check if they are awake now.
    
    This relies on the following properties:
    
    - Once dont_stop is non-zero, requested_psccr can't go from zero to
      non-zero, except transiently (and without the thread doing stop).
    - requested_psscr being zero guarantees that the thread isn't in
      a state-losing stop state where thread reconfiguration could occur.
    - Doing stop with a PSSCR value of 0 won't be a state-losing stop
      and thus won't allow thread reconfiguration.
    - Once threads_per_core/2 + 1 (i.e. 3) threads are awake, the core
      must be in SMT4 mode, since SMT modes are powers of 2.
    
    This does add a sync to power9_idle_stop(), which is necessary to
    provide the correct ordering between setting requested_psscr and
    checking dont_stop.  The overhead of the sync should be unnoticeable
    compared to the latency of going into and out of a stop state.
    
    Because some objected to incurring this extra latency on systems where
    the XER[SO] bug is not relevant, I have put the test in
    power9_idle_stop inside a feature section.  This means that
    pnv_power9_force_smt4_catch() WILL NOT WORK correctly on systems
    without the CPU_FTR_P9_TM_XER_SO_BUG feature bit set, and will
    probably hang the system.
    
    In order to cater for uses where the caller has an operation that
    has to be done while the core is in SMT4, the core continues to be
    kept in SMT4 after pnv_power9_force_smt4_catch() function returns,
    until the pnv_power9_force_smt4_release() function is called.
    It undoes the effect of step 1 above and allows the other threads
    to go into a stop state.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index b62c31037cad..4803cc1b011b 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -32,6 +32,7 @@
 #include <asm/accounting.h>
 #include <asm/hmi.h>
 #include <asm/cpuidle.h>
+#include <asm/atomic.h>
 
 register struct paca_struct *local_paca asm("r13");
 
@@ -177,6 +178,8 @@ struct paca_struct {
 	u8 thread_mask;
 	/* Mask to denote subcore sibling threads */
 	u8 subcore_sibling_mask;
+	/* Flag to request this thread not to stop */
+	atomic_t dont_stop;
 	/*
 	 * Pointer to an array which contains pointer
 	 * to the sibling threads' paca.

commit 15472423ce47d6397d08d48daaae8590c9f9f242
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Feb 22 15:27:28 2018 +0100

    powerpc/mm/slice: Allow up to 64 low slices
    
    While the implementation of the "slices" address space allows
    a significant amount of high slices, it limits the number of
    low slices to 16 due to the use of a single u64 low_slices_psize
    element in struct mm_context_t
    
    On the 8xx, the minimum slice size is the size of the area
    covered by a single PMD entry, ie 4M in 4K pages mode and 64M in
    16K pages mode. This means we could have at least 64 slices.
    
    In order to override this limitation, this patch switches the
    handling of low_slices_psize to char array as done already for
    high_slices_psize.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index b62c31037cad..d2bf71dddbef 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -141,7 +141,7 @@ struct paca_struct {
 #ifdef CONFIG_PPC_BOOK3S
 	mm_context_id_t mm_ctx_id;
 #ifdef CONFIG_PPC_MM_SLICES
-	u64 mm_ctx_low_slices_psize;
+	unsigned char mm_ctx_low_slices_psize[BITS_PER_LONG / BITS_PER_BYTE];
 	unsigned char mm_ctx_high_slices_psize[SLICE_ARRAY_SIZE];
 	unsigned long mm_ctx_slb_addr_limit;
 #else

commit bdcb1aefc5b3f7d0f1dc8b02673602bca2ff7a4b
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Jan 17 23:58:18 2018 +1000

    powerpc/64s: Improve RFI L1-D cache flush fallback
    
    The fallback RFI flush is used when firmware does not provide a way
    to flush the cache. It's a "displacement flush" that evicts useful
    data by displacing it with an uninteresting buffer.
    
    The flush has to take care to work with implementation specific cache
    replacment policies, so the recipe has been in flux. The initial
    slow but conservative approach is to touch all lines of a congruence
    class, with dependencies between each load. It has since been
    determined that a linear pattern of loads without dependencies is
    sufficient, and is significantly faster.
    
    Measuring the speed of a null syscall with RFI fallback flush enabled
    gives the relative improvement:
    
    P8 - 1.83x
    P9 - 1.75x
    
    The flush also becomes simpler and more adaptable to different cache
    geometries.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 1b7fd535393b..b62c31037cad 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -239,8 +239,7 @@ struct paca_struct {
 	 */
 	u64 exrfi[EX_SIZE] __aligned(0x80);
 	void *rfi_flush_fallback_area;
-	u64 l1d_flush_congruence;
-	u64 l1d_flush_sets;
+	u64 l1d_flush_size;
 #endif
 };
 

commit ebf0b6a8b1e445d2be66087732aafcda12ab9f59
Merge: 5400fc229e60 1b689a95ce74
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sun Jan 21 23:21:14 2018 +1100

    Merge branch 'fixes' into next
    
    Merge our fixes branch from the 4.15 cycle.
    
    Unusually the fixes branch saw some significant features merged,
    notably the RFI flush patches, so we want the code in next to be
    tested against that, to avoid any surprises when the two are merged.
    
    There's also some other work on the panic handling that was reverted
    in fixes and we now want to do properly in next, which would conflict.
    
    And we also fix a few other minor merge conflicts.

commit 4e26bc4a4ed683c42ba45f09050575a671c6f1f4
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:50 2017 +0530

    powerpc/64: Rename soft_enabled to irq_soft_mask
    
    Rename the paca->soft_enabled to paca->irq_soft_mask as it is no
    longer used as a flag for interrupt state, but a mask.
    
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 3892db93b837..e2ee193eb24d 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -159,7 +159,7 @@ struct paca_struct {
 	u64 saved_r1;			/* r1 save for RTAS calls or PM */
 	u64 saved_msr;			/* MSR saved here by enter_rtas */
 	u16 trap_save;			/* Used when bad stack is encountered */
-	u8 soft_enabled;		/* irq soft-enable flag */
+	u8 irq_soft_mask;		/* mask for irq soft masking */
 	u8 irq_happened;		/* irq happened while soft-disabled */
 	u8 io_sync;			/* writel() needs spin_unlock sync */
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */

commit aa8a5e0062ac940f7659394f4817c948dc8c0667
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jan 10 03:07:15 2018 +1100

    powerpc/64s: Add support for RFI flush of L1-D cache
    
    On some CPUs we can prevent the Meltdown vulnerability by flushing the
    L1-D cache on exit from kernel to user mode, and from hypervisor to
    guest.
    
    This is known to be the case on at least Power7, Power8 and Power9. At
    this time we do not know the status of the vulnerability on other CPUs
    such as the 970 (Apple G5), pasemi CPUs (AmigaOne X1000) or Freescale
    CPUs. As more information comes to light we can enable this, or other
    mechanisms on those CPUs.
    
    The vulnerability occurs when the load of an architecturally
    inaccessible memory region (eg. userspace load of kernel memory) is
    speculatively executed to the point where its result can influence the
    address of a subsequent speculatively executed load.
    
    In order for that to happen, the first load must hit in the L1,
    because before the load is sent to the L2 the permission check is
    performed. Therefore if no kernel addresses hit in the L1 the
    vulnerability can not occur. We can ensure that is the case by
    flushing the L1 whenever we return to userspace. Similarly for
    hypervisor vs guest.
    
    In order to flush the L1-D cache on exit, we add a section of nops at
    each (h)rfi location that returns to a lower privileged context, and
    patch that with some sequence. Newer firmwares are able to advertise
    to us that there is a special nop instruction that flushes the L1-D.
    If we do not see that advertised, we fall back to doing a displacement
    flush in software.
    
    For guest kernels we support migration between some CPU versions, and
    different CPUs may use different flush instructions. So that we are
    prepared to migrate to a machine with a different flush instruction
    activated, we may have to patch more than one flush instruction at
    boot if the hypervisor tells us to.
    
    In the end this patch is mostly the work of Nicholas Piggin and
    Michael Ellerman. However a cast of thousands contributed to analysis
    of the issue, earlier versions of the patch, back ports testing etc.
    Many thanks to all of them.
    
    Tested-by: Jon Masters <jcm@redhat.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 3892db93b837..23ac7fc0af23 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -232,6 +232,16 @@ struct paca_struct {
 	struct sibling_subcore_state *sibling_subcore_state;
 #endif
 #endif
+#ifdef CONFIG_PPC_BOOK3S_64
+	/*
+	 * rfi fallback flush must be in its own cacheline to prevent
+	 * other paca data leaking into the L1d
+	 */
+	u64 exrfi[EX_SIZE] __aligned(0x80);
+	void *rfi_flush_fallback_area;
+	u64 l1d_flush_congruence;
+	u64 l1d_flush_sets;
+#endif
 };
 
 extern void copy_mm_to_paca(struct mm_struct *mm);

commit 4722476bce283149986a3463f61009dec0e7f6a1
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Nov 10 04:27:40 2017 +1100

    powerpc/64s: mm_context.addr_limit is only used on hash
    
    Radix keeps no meaningful state in addr_limit, so remove it from radix
    code and rename to slb_addr_limit to make it clear it applies to hash
    only.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index c907ae23c956..3892db93b837 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -143,7 +143,7 @@ struct paca_struct {
 #ifdef CONFIG_PPC_MM_SLICES
 	u64 mm_ctx_low_slices_psize;
 	unsigned char mm_ctx_high_slices_psize[SLICE_ARRAY_SIZE];
-	unsigned long addr_limit;
+	unsigned long mm_ctx_slb_addr_limit;
 #else
 	u16 mm_ctx_user_psize;
 	u16 mm_ctx_sllp;

commit 4e003747043d57aa75c9762fa148ef38afe68dd8
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Oct 19 15:08:43 2017 +1100

    powerpc/64s: Replace CONFIG_PPC_STD_MMU_64 with CONFIG_PPC_BOOK3S_64
    
    CONFIG_PPC_STD_MMU_64 indicates support for the "standard" powerpc MMU
    on 64-bit CPUs. The "standard" MMU refers to the hash page table MMU
    found in "server" processors, from IBM mainly.
    
    Currently CONFIG_PPC_STD_MMU_64 is == CONFIG_PPC_BOOK3S_64. While it's
    annoying to have two symbols that always have the same value, it's not
    quite annoying enough to bother removing one.
    
    However with the arrival of Power9, we now have the situation where
    CONFIG_PPC_STD_MMU_64 is enabled, but the kernel is running using the
    Radix MMU - *not* the "standard" MMU. So it is now actively confusing
    to use it, because it implies that code is disabled or inactive when
    the Radix MMU is in use, however that is not necessarily true.
    
    So s/CONFIG_PPC_STD_MMU_64/CONFIG_PPC_BOOK3S_64/, and do some minor
    formatting updates of some of the affected lines.
    
    This will be a pain for backports, but c'est la vie.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 1e06310ccc09..c907ae23c956 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -91,14 +91,14 @@ struct paca_struct {
 	u8 cpu_start;			/* At startup, processor spins until */
 					/* this becomes non-zero. */
 	u8 kexec_state;		/* set when kexec down has irqs off */
-#ifdef CONFIG_PPC_STD_MMU_64
+#ifdef CONFIG_PPC_BOOK3S_64
 	struct slb_shadow *slb_shadow_ptr;
 	struct dtl_entry *dispatch_log;
 	struct dtl_entry *dispatch_log_end;
-#endif /* CONFIG_PPC_STD_MMU_64 */
+#endif
 	u64 dscr_default;		/* per-CPU default DSCR */
 
-#ifdef CONFIG_PPC_STD_MMU_64
+#ifdef CONFIG_PPC_BOOK3S_64
 	/*
 	 * Now, starting in cacheline 2, the exception save areas
 	 */
@@ -110,7 +110,7 @@ struct paca_struct {
 	u16 vmalloc_sllp;
 	u16 slb_cache_ptr;
 	u32 slb_cache[SLB_CACHE_ENTRIES];
-#endif /* CONFIG_PPC_STD_MMU_64 */
+#endif /* CONFIG_PPC_BOOK3S_64 */
 
 #ifdef CONFIG_PPC_BOOK3E
 	u64 exgen[8] __aligned(0x40);
@@ -192,7 +192,7 @@ struct paca_struct {
 	struct stop_sprs stop_sprs;
 #endif
 
-#ifdef CONFIG_PPC_STD_MMU_64
+#ifdef CONFIG_PPC_BOOK3S_64
 	/* Non-maskable exceptions that are not performance critical */
 	u64 exnmi[EX_SIZE];	/* used for system reset (nmi) */
 	u64 exmc[EX_SIZE];	/* used for machine checks */

commit 5080332c2c893118dbc18755f35c8b0131cf0fc4
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Sep 15 15:25:48 2017 +1000

    powerpc/64s: Add workaround for P9 vector CI load issue
    
    POWER9 DD2.1 and earlier has an issue where some cache inhibited
    vector load will return bad data. The workaround is two part, one
    firmware/microcode part triggers HMI interrupts when hitting such
    loads, the other part is this patch which then emulates the
    instructions in Linux.
    
    The affected instructions are limited to lxvd2x, lxvw4x, lxvb16x and
    lxvh8x.
    
    When an instruction triggers the HMI, all threads in the core will be
    sent to the HMI handler, not just the one running the vector load.
    
    In general, these spurious HMIs are detected by the emulation code and
    we just return back to the running process. Unfortunately, if a
    spurious interrupt occurs on a vector load that's to normal memory we
    have no way to detect that it's spurious (unless we walk the page
    tables, which is very expensive). In this case we emulate the load but
    we need do so using a vector load itself to ensure 128bit atomicity is
    preserved.
    
    Some additional debugfs emulated instruction counters are added also.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [mpe: Switch CONFIG_PPC_BOOK3S_64 to CONFIG_VSX to unbreak the build]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 04b60af027ae..1e06310ccc09 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -210,6 +210,7 @@ struct paca_struct {
 	 */
 	u16 in_mce;
 	u8 hmi_event_available;		/* HMI event is available */
+	u8 hmi_p9_special_emu;		/* HMI P9 special emulation */
 #endif
 
 	/* Stuff for accurate time accounting */

commit e1c1cfed54326fd2b17c78f0c85092167fc0783b
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Fri Jul 21 16:11:37 2017 +0530

    powerpc/powernv: Save/Restore additional SPRs for stop4 cpuidle
    
    The stop4 idle state on POWER9 is a deep idle state which loses
    hypervisor resources, but whose latency is low enough that it can be
    exposed via cpuidle.
    
    Until now, the deep idle states which lose hypervisor resources (eg:
    winkle) were only exposed via CPU-Hotplug.  Hence currently on wakeup
    from such states, barring a few SPRs which need to be restored to
    their older value, rest of the SPRS are reinitialized to their values
    corresponding to that at boot time.
    
    When stop4 is used in the context of cpuidle, we want these additional
    SPRs to be restored to their older value, to ensure that the context
    on the CPU coming back from idle is same as it was before going idle.
    
    In this patch, we define a SPR save area in PACA (since we have used
    up the volatile register space in the stack) and on POWER9, we restore
    SPRN_PID, SPRN_LDBAR, SPRN_FSCR, SPRN_HFSCR, SPRN_MMCRA, SPRN_MMCR1,
    SPRN_MMCR2 to the values they had before entering stop.
    
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index dc88a31cc79a..04b60af027ae 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -31,6 +31,7 @@
 #endif
 #include <asm/accounting.h>
 #include <asm/hmi.h>
+#include <asm/cpuidle.h>
 
 register struct paca_struct *local_paca asm("r13");
 
@@ -183,6 +184,12 @@ struct paca_struct {
 	struct paca_struct **thread_sibling_pacas;
 	/* The PSSCR value that the kernel requested before going to stop */
 	u64 requested_psscr;
+
+	/*
+	 * Save area for additional SPRs that need to be
+	 * saved/restored during cpuidle stop.
+	 */
+	struct stop_sprs stop_sprs;
 #endif
 
 #ifdef CONFIG_PPC_STD_MMU_64

commit 8c3885141537966065e3d2b9be03e574ae381c79
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sun May 21 23:15:46 2017 +1000

    powerpc/64s: Add EX_SIZE definition for paca exception save areas
    
    Rather than open-coding it 4 times.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Move __ASSEMBLY__ guards into head-64.h where they're really needed]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 77f60a0f1405..dc88a31cc79a 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -21,7 +21,11 @@
 #include <asm/lppaca.h>
 #include <asm/mmu.h>
 #include <asm/page.h>
+#ifdef CONFIG_PPC_BOOK3E
 #include <asm/exception-64e.h>
+#else
+#include <asm/exception-64s.h>
+#endif
 #ifdef CONFIG_KVM_BOOK3S_64_HANDLER
 #include <asm/kvm_book3s_asm.h>
 #endif
@@ -98,8 +102,8 @@ struct paca_struct {
 	 * Now, starting in cacheline 2, the exception save areas
 	 */
 	/* used for most interrupts/exceptions */
-	u64 exgen[13] __attribute__((aligned(0x80)));
-	u64 exslb[13];		/* used for SLB/segment table misses
+	u64 exgen[EX_SIZE] __attribute__((aligned(0x80)));
+	u64 exslb[EX_SIZE];	/* used for SLB/segment table misses
  				 * on the linear mapping */
 	/* SLB related definitions */
 	u16 vmalloc_sllp;
@@ -183,8 +187,8 @@ struct paca_struct {
 
 #ifdef CONFIG_PPC_STD_MMU_64
 	/* Non-maskable exceptions that are not performance critical */
-	u64 exnmi[13];		/* used for system reset (nmi) */
-	u64 exmc[13];		/* used for machine checks */
+	u64 exnmi[EX_SIZE];	/* used for system reset (nmi) */
+	u64 exmc[EX_SIZE];	/* used for machine checks */
 #endif
 #ifdef CONFIG_PPC_BOOK3S_64
 	/* Exclusive stacks for system reset and machine check exception. */

commit 22c6663dc69a042a7b4158f162582b4b1ba7a4b7
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Tue May 16 14:19:47 2017 +0530

    powerpc/powernv/idle: Use Requested Level for restoring state on P9 DD1
    
    On Power9 DD1 due to a hardware bug the Power-Saving Level Status
    field (PLS) of the PSSCR for a thread waking up from a deep state can
    under-report if some other thread in the core is in a shallow stop
    state. The scenario in which this can manifest is as follows:
    
       1) All the threads of the core are in deep stop.
       2) One of the threads is woken up. The PLS for this thread will
          correctly reflect that it is waking up from deep stop.
       3) The thread that has woken up now executes a shallow stop.
       4) When some other thread in the core is woken, its PLS will reflect
          the shallow stop state.
    
    Thus, the subsequent thread for which the PLS is under-reporting the
    wakeup state will not restore the hypervisor resources.
    
    Hence, on DD1 systems, use the Requested Level (RL) field as a
    workaround to restore the contents of the hypervisor resources on the
    wakeup from the stop state.
    
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 1c09f8fe2ee8..77f60a0f1405 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -177,6 +177,8 @@ struct paca_struct {
 	 * to the sibling threads' paca.
 	 */
 	struct paca_struct **thread_sibling_pacas;
+	/* The PSSCR value that the kernel requested before going to stop */
+	u64 requested_psscr;
 #endif
 
 #ifdef CONFIG_PPC_STD_MMU_64

commit b1ee8a3de5790777f325416ad97340428d8ae25f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Dec 20 04:30:06 2016 +1000

    powerpc/64s: Dedicated system reset interrupt stack
    
    The system reset interrupt is used for crash/debug situations, so it is
    desirable to have as little impact on the normal state of the system as
    possible.
    
    Currently it uses the current kernel stack to process the exception.
    This stores into the stack which may be involved with the crash. The
    stack pointer may be corrupted, or it may have overflowed.
    
    Avoid or minimise these problems by creating a dedicated NMI stack for
    the system reset interrupt to use.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index b1197c1affac..1c09f8fe2ee8 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -185,7 +185,8 @@ struct paca_struct {
 	u64 exmc[13];		/* used for machine checks */
 #endif
 #ifdef CONFIG_PPC_BOOK3S_64
-	/* Exclusive emergency stack pointer for machine check exception. */
+	/* Exclusive stacks for system reset and machine check exception. */
+	void *nmi_emergency_sp;
 	void *mc_emergency_sp;
 
 	u16 in_nmi;			/* In nmi handler */

commit c4f3b52ce7b16824befb16ab3d045c891b08b7db
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Dec 20 04:30:05 2016 +1000

    powerpc/64s: Disallow system reset vs system reset reentrancy
    
    In preparation for using a dedicated stack for system reset interrupts,
    prevent a nested system reset from recovering, in order to simplify
    code that is called in crash/debug path. This allows a system reset
    interrupt to just use the base stack pointer.
    
    Keep an in_nmi nesting counter similarly to the in_mce counter. Consider
    the interrrupt non-recoverable if it is taken inside another system
    reset.
    
    Interrupt nesting could be allowed similarly to MCE, but system reset
    is a special case that's not for normal operation, so simplicity wins
    until there is requirement for nested system reset interrupts.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 601e2327dd8c..b1197c1affac 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -187,12 +187,15 @@ struct paca_struct {
 #ifdef CONFIG_PPC_BOOK3S_64
 	/* Exclusive emergency stack pointer for machine check exception. */
 	void *mc_emergency_sp;
+
+	u16 in_nmi;			/* In nmi handler */
+
 	/*
 	 * Flag to check whether we are in machine check early handler
 	 * and already using emergency stack.
 	 */
 	u16 in_mce;
-	u8 hmi_event_available;		 /* HMI event is available */
+	u8 hmi_event_available;		/* HMI event is available */
 #endif
 
 	/* Stuff for accurate time accounting */

commit a3d96f70c14773d0928c6a54fd278138f0868572
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Dec 20 04:30:04 2016 +1000

    powerpc/64s: Fix system reset vs general interrupt reentrancy
    
    The system reset interrupt can occur when MSR_EE=0, and it currently
    uses the PACA_EXGEN save area.
    
    Some PACA_EXGEN interrupts have a window where MSR_RI=1 and MSR_EE=0
    when the save area is still in use. A system reset interrupt in this
    window can lead to undetected corruption when the save area gets
    overwritten.
    
    This patch introduces PACA_EXNMI save area for system reset exceptions,
    which closes this corruption window. It's also helpful to retain the
    EXGEN state for debugging situations, even if not considering the
    recoverability aspect.
    
    This patch also moves the PACA_EXMC area down to a less frequently used
    part of the paca with the new save area.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 140ddb9ae5a8..601e2327dd8c 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -99,7 +99,6 @@ struct paca_struct {
 	 */
 	/* used for most interrupts/exceptions */
 	u64 exgen[13] __attribute__((aligned(0x80)));
-	u64 exmc[13];		/* used for machine checks */
 	u64 exslb[13];		/* used for SLB/segment table misses
  				 * on the linear mapping */
 	/* SLB related definitions */
@@ -180,6 +179,11 @@ struct paca_struct {
 	struct paca_struct **thread_sibling_pacas;
 #endif
 
+#ifdef CONFIG_PPC_STD_MMU_64
+	/* Non-maskable exceptions that are not performance critical */
+	u64 exnmi[13];		/* used for system reset (nmi) */
+	u64 exmc[13];		/* used for machine checks */
+#endif
 #ifdef CONFIG_PPC_BOOK3S_64
 	/* Exclusive emergency stack pointer for machine check exception. */
 	void *mc_emergency_sp;

commit 17ed4c8f81da2bf340d33a8c875f4d6b1dfd9398
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Wed Mar 22 20:34:17 2017 +0530

    powerpc/powernv: Recover correct PACA on wakeup from a stop on P9 DD1
    
    POWER9 DD1.0 hardware has a bug where the SPRs of a thread waking up
    from stop 0,1,2 with ESL=1 can endup being misplaced in the core. Thus
    the HSPRG0 of a thread waking up from can contain the paca pointer of
    its sibling.
    
    This patch implements a context recovery framework within threads of a
    core, by provisioning space in paca_struct for saving every sibling
    threads's paca pointers. Basically, we should be able to arrive at the
    right paca pointer from any of the thread's existing paca pointer.
    
    At bootup, during powernv idle-init, we save the paca address of every
    CPU in each one its siblings paca_struct in the slot corresponding to
    this CPU's index in the core.
    
    On wakeup from a stop, the thread will determine its index in the core
    from the TIR register and recover its PACA pointer by indexing into
    the correct slot in the provisioned space in the current PACA.
    
    Furthermore, ensure that the NVGPRs are restored from the stack on the
    way out by setting the NAPSTATELOST in paca.
    
    [Changelog written with inputs from svaidy@linux.vnet.ibm.com]
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Call it a bug]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 65d0ffb2aa33..140ddb9ae5a8 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -173,6 +173,11 @@ struct paca_struct {
 	u8 thread_mask;
 	/* Mask to denote subcore sibling threads */
 	u8 subcore_sibling_mask;
+	/*
+	 * Pointer to an array which contains pointer
+	 * to the sibling threads' paca.
+	 */
+	struct paca_struct **thread_sibling_pacas;
 #endif
 
 #ifdef CONFIG_PPC_BOOK3S_64

commit bb1832217a859f6dbe4a45ff2ba7fdcab0bb3958
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Mar 22 09:06:59 2017 +0530

    powerpc/mm/hash: Store addr_limit in PACA
    
    We optmize the slice page size array copy to paca by copying only the
    range based on addr_limit. This will require us to not look at page size
    array beyond addr_limit in PACA on slb fault. To enable that copy task
    size to paca which will be used during slb fault.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [mpe: Rename from task_size to addr_limit, consolidate #ifdefs]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index f48c250339fd..65d0ffb2aa33 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -139,6 +139,7 @@ struct paca_struct {
 #ifdef CONFIG_PPC_MM_SLICES
 	u64 mm_ctx_low_slices_psize;
 	unsigned char mm_ctx_high_slices_psize[SLICE_ARRAY_SIZE];
+	unsigned long addr_limit;
 #else
 	u16 mm_ctx_user_psize;
 	u16 mm_ctx_sllp;

commit 52b1e66587b6b4f2e5a09102b3a40eed8ec3675f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Mar 22 09:06:49 2017 +0530

    powerpc/mm: Move copy_mm_to_paca to paca.c
    
    We also update the function arg to struct mm_struct. Move this so that function
    finds the definition of struct mm_struct. No functional change in this patch.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 708c3e592eeb..f48c250339fd 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -206,23 +206,7 @@ struct paca_struct {
 #endif
 };
 
-#ifdef CONFIG_PPC_BOOK3S
-static inline void copy_mm_to_paca(mm_context_t *context)
-{
-	get_paca()->mm_ctx_id = context->id;
-#ifdef CONFIG_PPC_MM_SLICES
-	get_paca()->mm_ctx_low_slices_psize = context->low_slices_psize;
-	memcpy(&get_paca()->mm_ctx_high_slices_psize,
-	       &context->high_slices_psize, SLICE_ARRAY_SIZE);
-#else
-	get_paca()->mm_ctx_user_psize = context->user_psize;
-	get_paca()->mm_ctx_sllp = context->sllp;
-#endif
-}
-#else
-static inline void copy_mm_to_paca(mm_context_t *context){}
-#endif
-
+extern void copy_mm_to_paca(struct mm_struct *mm);
 extern struct paca_struct *paca;
 extern void initialise_paca(struct paca_struct *new_paca, int cpu);
 extern void setup_paca(struct paca_struct *new_paca);

commit f828c3d0aebab130a19d36336b50afa3414fa0bc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jan 5 18:11:46 2017 +0100

    sched/cputime, powerpc: Migrate stolen_time field to the accounting structure
    
    That in order to gather all cputime accumulation to the same place.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1483636310-6557-7-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 6a6792bb39fb..708c3e592eeb 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -187,7 +187,6 @@ struct paca_struct {
 
 	/* Stuff for accurate time accounting */
 	struct cpu_accounting_data accounting;
-	u64 stolen_time;		/* TB ticks taken by hypervisor */
 	u64 dtl_ridx;			/* read index in dispatch log */
 	struct dtl_entry *dtl_curr;	/* pointer corresponding to dtl_ridx */
 

commit 7c379526d7e71d2041c8ca0bd9af4888916a14b9
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Aug 11 15:07:43 2016 +0200

    powerpc: move hmi.c to arch/powerpc/kvm/
    
    hmi.c functions are unused unless sibling_subcore_state is nonzero, and
    that in turn happens only if KVM is in use.  So move the code to
    arch/powerpc/kvm/, putting it under CONFIG_KVM_BOOK3S_HV_POSSIBLE
    rather than CONFIG_PPC_BOOK3S_64.  The sibling_subcore_state is also
    included in struct paca_struct only if KVM is supported by the kernel.
    
    Cc: Daniel Axtens <dja@axtens.net>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: kvm-ppc@vger.kernel.org
    Cc: kvm@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 148303e7771f..6a6792bb39fb 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -183,11 +183,6 @@ struct paca_struct {
 	 */
 	u16 in_mce;
 	u8 hmi_event_available;		 /* HMI event is available */
-	/*
-	 * Bitmap for sibling subcore status. See kvm/book3s_hv_ras.c for
-	 * more details
-	 */
-	struct sibling_subcore_state *sibling_subcore_state;
 #endif
 
 	/* Stuff for accurate time accounting */
@@ -202,6 +197,13 @@ struct paca_struct {
 	struct kvmppc_book3s_shadow_vcpu shadow_vcpu;
 #endif
 	struct kvmppc_host_state kvm_hstate;
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	/*
+	 * Bitmap for sibling subcore status. See kvm/book3s_hv_ras.c for
+	 * more details
+	 */
+	struct sibling_subcore_state *sibling_subcore_state;
+#endif
 #endif
 };
 

commit 221bb8a46e230b9824204ae86537183d9991ff2a
Merge: f7b32e4c021f 23528bb21ee2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 2 16:11:27 2016 -0400

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
    
     - ARM: GICv3 ITS emulation and various fixes.  Removal of the
       old VGIC implementation.
    
     - s390: support for trapping software breakpoints, nested
       virtualization (vSIE), the STHYI opcode, initial extensions
       for CPU model support.
    
     - MIPS: support for MIPS64 hosts (32-bit guests only) and lots
       of cleanups, preliminary to this and the upcoming support for
       hardware virtualization extensions.
    
     - x86: support for execute-only mappings in nested EPT; reduced
       vmexit latency for TSC deadline timer (by about 30%) on Intel
       hosts; support for more than 255 vCPUs.
    
     - PPC: bugfixes.
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (302 commits)
      KVM: PPC: Introduce KVM_CAP_PPC_HTM
      MIPS: Select HAVE_KVM for MIPS64_R{2,6}
      MIPS: KVM: Reset CP0_PageMask during host TLB flush
      MIPS: KVM: Fix ptr->int cast via KVM_GUEST_KSEGX()
      MIPS: KVM: Sign extend MFC0/RDHWR results
      MIPS: KVM: Fix 64-bit big endian dynamic translation
      MIPS: KVM: Fail if ebase doesn't fit in CP0_EBase
      MIPS: KVM: Use 64-bit CP0_EBase when appropriate
      MIPS: KVM: Set CP0_Status.KX on MIPS64
      MIPS: KVM: Make entry code MIPS64 friendly
      MIPS: KVM: Use kmap instead of CKSEG0ADDR()
      MIPS: KVM: Use virt_to_phys() to get commpage PFN
      MIPS: Fix definition of KSEGX() for 64-bit
      KVM: VMX: Add VMCS to CPU's loaded VMCSs before VMPTRLD
      kvm: x86: nVMX: maintain internal copy of current VMCS
      KVM: PPC: Book3S HV: Save/restore TM state in H_CEDE
      KVM: PPC: Book3S HV: Pull out TM state save/restore into separate procedures
      KVM: arm64: vgic-its: Simplify MAPI error handling
      KVM: arm64: vgic-its: Make vgic_its_cmd_handle_mapi similar to other handlers
      KVM: arm64: vgic-its: Turn device_id validation into generic ID validation
      ...

commit c223c90386bc2306510e0ceacd768a0123ff2a2f
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue May 17 08:33:46 2016 +0200

    powerpc32: provide VIRT_CPU_ACCOUNTING
    
    This patch provides VIRT_CPU_ACCOUTING to PPC32 architecture.
    PPC32 doesn't have the PACA structure, so we use the task_info
    structure to store the accounting data.
    
    In order to reuse on PPC32 the PPC64 functions, all u64 data has
    been replaced by 'unsigned long' so that it is u32 on PPC32 and
    u64 on PPC64
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 546540b91095..ad171e979ab0 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -25,6 +25,7 @@
 #ifdef CONFIG_KVM_BOOK3S_64_HANDLER
 #include <asm/kvm_book3s_asm.h>
 #endif
+#include <asm/accounting.h>
 
 register struct paca_struct *local_paca asm("r13");
 
@@ -184,13 +185,7 @@ struct paca_struct {
 #endif
 
 	/* Stuff for accurate time accounting */
-	u64 user_time;			/* accumulated usermode TB ticks */
-	u64 system_time;		/* accumulated system TB ticks */
-	u64 user_time_scaled;		/* accumulated usermode SPURR ticks */
-	u64 starttime;			/* TB value snapshot */
-	u64 starttime_user;		/* TB value on exit to usermode */
-	u64 startspurr;			/* SPURR value snapshot */
-	u64 utime_sspurr;		/* ->user_time when ->startspurr set */
+	struct cpu_accounting_data accounting;
 	u64 stolen_time;		/* TB ticks taken by hypervisor */
 	u64 dtl_ridx;			/* read index in dispatch log */
 	struct dtl_entry *dtl_curr;	/* pointer corresponding to dtl_ridx */

commit fd7bacbca47a86a6f418440d8a5d7b7edbb2f8f9
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Sun May 15 09:44:26 2016 +0530

    KVM: PPC: Book3S HV: Fix TB corruption in guest exit path on HMI interrupt
    
    When a guest is assigned to a core it converts the host Timebase (TB)
    into guest TB by adding guest timebase offset before entering into
    guest. During guest exit it restores the guest TB to host TB. This means
    under certain conditions (Guest migration) host TB and guest TB can differ.
    
    When we get an HMI for TB related issues the opal HMI handler would
    try fixing errors and restore the correct host TB value. With no guest
    running, we don't have any issues. But with guest running on the core
    we run into TB corruption issues.
    
    If we get an HMI while in the guest, the current HMI handler invokes opal
    hmi handler before forcing guest to exit. The guest exit path subtracts
    the guest TB offset from the current TB value which may have already
    been restored with host value by opal hmi handler. This leads to incorrect
    host and guest TB values.
    
    With split-core, things become more complex. With split-core, TB also gets
    split and each subcore gets its own TB register. When a hmi handler fixes
    a TB error and restores the TB value, it affects all the TB values of
    sibling subcores on the same core. On TB errors all the thread in the core
    gets HMI. With existing code, the individual threads call opal hmi handle
    independently which can easily throw TB out of sync if we have guest
    running on subcores. Hence we will need to co-ordinate with all the
    threads before making opal hmi handler call followed by TB resync.
    
    This patch introduces a sibling subcore state structure (shared by all
    threads in the core) in paca which holds information about whether sibling
    subcores are in Guest mode or host mode. An array in_guest[] of size
    MAX_SUBCORE_PER_CORE=4 is used to maintain the state of each subcore.
    The subcore id is used as index into in_guest[] array. Only primary
    thread entering/exiting the guest is responsible to set/unset its
    designated array element.
    
    On TB error, we get HMI interrupt on every thread on the core. Upon HMI,
    this patch will now force guest to vacate the core/subcore. Primary
    thread from each subcore will then turn off its respective bit
    from the above bitmap during the guest exit path just after the
    guest->host partition switch is complete.
    
    All other threads that have just exited the guest OR were already in host
    will wait until all other subcores clears their respective bit.
    Once all the subcores turn off their respective bit, all threads will
    will make call to opal hmi handler.
    
    It is not necessary that opal hmi handler would resync the TB value for
    every HMI interrupts. It would do so only for the HMI caused due to
    TB errors. For rest, it would not touch TB value. Hence to make things
    simpler, primary thread would call TB resync explicitly once for each
    core immediately after opal hmi handler instead of subtracting guest
    offset from TB. TB resync call will restore the TB with host value.
    Thus we can be sure about the TB state.
    
    One of the primary threads exiting the guest will take up the
    responsibility of calling TB resync. It will use one of the top bits
    (bit 63) from subcore state flags bitmap to make the decision. The first
    primary thread (among the subcores) that is able to set the bit will
    have to call the TB resync. Rest all other threads will wait until TB
    resync is complete.  Once TB resync is complete all threads will then
    proceed.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 546540b91095..4b17bd058e01 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -25,6 +25,7 @@
 #ifdef CONFIG_KVM_BOOK3S_64_HANDLER
 #include <asm/kvm_book3s_asm.h>
 #endif
+#include <asm/hmi.h>
 
 register struct paca_struct *local_paca asm("r13");
 
@@ -181,6 +182,11 @@ struct paca_struct {
 	 */
 	u16 in_mce;
 	u8 hmi_event_available;		 /* HMI event is available */
+	/*
+	 * Bitmap for sibling subcore status. See kvm/book3s_hv_ras.c for
+	 * more details
+	 */
+	struct sibling_subcore_state *sibling_subcore_state;
 #endif
 
 	/* Stuff for accurate time accounting */

commit c33e54fafacaf83b3e345aae0e241c1f152224a0
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Jan 9 08:25:01 2016 +1100

    powerpc: Fix build break due to paca mm_context_t changes
    
    Commit 2fc251a8dda5 ("powerpc: Copy only required pieces of the
    mm_context_t to the paca") broke the build for CONFIG_PPC_STD_MMU_64=y
    and CONFIG_PPC_MM_SLICES=n.
    
    That only happens for a kernel built with 4K pages and HUGETLB disabled,
    which is why we missed it.
    
    Fix it by adding a mm_ctx_user_psize member to the paca and populating
    it in the appropriate places.
    
    Fixes: 2fc251a8dda5 ("powerpc: Copy only required pieces of the mm_context_t to the paca")
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index ef78c288c712..546540b91095 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -138,6 +138,7 @@ struct paca_struct {
 	u64 mm_ctx_low_slices_psize;
 	unsigned char mm_ctx_high_slices_psize[SLICE_ARRAY_SIZE];
 #else
+	u16 mm_ctx_user_psize;
 	u16 mm_ctx_sllp;
 #endif
 #endif
@@ -212,6 +213,7 @@ static inline void copy_mm_to_paca(mm_context_t *context)
 	memcpy(&get_paca()->mm_ctx_high_slices_psize,
 	       &context->high_slices_psize, SLICE_ARRAY_SIZE);
 #else
+	get_paca()->mm_ctx_user_psize = context->user_psize;
 	get_paca()->mm_ctx_sllp = context->sllp;
 #endif
 }

commit 2fc251a8dda56b71ec491bee4c6897e3e12c0739
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Dec 11 09:34:42 2015 +1100

    powerpc: Copy only required pieces of the mm_context_t to the paca
    
    Currently we copy the whole mm_context_t to the paca but only access a
    few bits of it.  This is wasteful of space paca and also takes quite
    some time in the hot path of context switching.
    
    This patch pulls in only the required bits from the mm_context_t to
    the paca and on context switch, copies only those.
    
    Benchmarking this (On top of Anton's recent MSR context switching
    changes [1]) using processes and yield shows an improvement of almost
    3% on POWER8:
    
      http://ozlabs.org/~anton/junkcode/context_switch2.c
      ./context_switch2 --test=yield --process 0 0
    
    1. https://lists.ozlabs.org/pipermail/linuxppc-dev/2015-October/135700.html
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    [mpe: Rename paca fields to be mm_ctx_foo rather than context_foo]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 1cc6e0828907..ef78c288c712 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -16,6 +16,7 @@
 
 #ifdef CONFIG_PPC64
 
+#include <linux/string.h>
 #include <asm/types.h>
 #include <asm/lppaca.h>
 #include <asm/mmu.h>
@@ -132,7 +133,13 @@ struct paca_struct {
 #endif /* CONFIG_PPC_BOOK3E */
 
 #ifdef CONFIG_PPC_BOOK3S
-	mm_context_t context;
+	mm_context_id_t mm_ctx_id;
+#ifdef CONFIG_PPC_MM_SLICES
+	u64 mm_ctx_low_slices_psize;
+	unsigned char mm_ctx_high_slices_psize[SLICE_ARRAY_SIZE];
+#else
+	u16 mm_ctx_sllp;
+#endif
 #endif
 
 	/*
@@ -199,7 +206,14 @@ struct paca_struct {
 #ifdef CONFIG_PPC_BOOK3S
 static inline void copy_mm_to_paca(mm_context_t *context)
 {
-	get_paca()->context = *context;
+	get_paca()->mm_ctx_id = context->id;
+#ifdef CONFIG_PPC_MM_SLICES
+	get_paca()->mm_ctx_low_slices_psize = context->low_slices_psize;
+	memcpy(&get_paca()->mm_ctx_high_slices_psize,
+	       &context->high_slices_psize, SLICE_ARRAY_SIZE);
+#else
+	get_paca()->mm_ctx_sllp = context->sllp;
+#endif
 }
 #else
 static inline void copy_mm_to_paca(mm_context_t *context){}

commit c395465da68bfc3a238d5bc15f862e33e6e9ecec
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Oct 28 15:54:06 2015 +1100

    powerpc: Add function to copy mm_context_t to the paca
    
    This adds a function to copy the mm->context to the paca.  This is
    only a basic conversion for now but will be used more extensively in
    the next patch.
    
    This also adds #ifdef CONFIG_PPC_BOOK3S around this code since it's
    not used elsewhere.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 70bd4381f8e6..1cc6e0828907 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -131,7 +131,9 @@ struct paca_struct {
 	struct tlb_core_data tcd;
 #endif /* CONFIG_PPC_BOOK3E */
 
+#ifdef CONFIG_PPC_BOOK3S
 	mm_context_t context;
+#endif
 
 	/*
 	 * then miscellaneous read-write fields
@@ -194,6 +196,15 @@ struct paca_struct {
 #endif
 };
 
+#ifdef CONFIG_PPC_BOOK3S
+static inline void copy_mm_to_paca(mm_context_t *context)
+{
+	get_paca()->context = *context;
+}
+#else
+static inline void copy_mm_to_paca(mm_context_t *context){}
+#endif
+
 extern struct paca_struct *paca;
 extern void initialise_paca(struct paca_struct *new_paca, int cpu);
 extern void setup_paca(struct paca_struct *new_paca);

commit 016f8cf0d87bb2bda15ccb8708748a013c27423f
Author: Kevin Hao <haokexin@gmail.com>
Date:   Tue Mar 10 20:41:31 2015 +0800

    powerpc: book3e_64: fix the align size for paca_struct
    
    All the cache line size of the current book3e 64bit SoCs are 64 bytes.
    So we should use this size to align the member of paca_struct.
    This only change the paca_struct's members which are private to book3e
    CPUs, and should not have any effect to book3s ones. With this, we save
    192 bytes. Also change it to __aligned(size) since it is preferred over
    __attribute__((aligned(size))).
    
    Before:
            /* size: 1920, cachelines: 30, members: 46 */
            /* sum members: 1667, holes: 6, sum holes: 141 */
            /* padding: 112 */
    
    After:
            /* size: 1728, cachelines: 27, members: 46 */
            /* sum members: 1667, holes: 4, sum holes: 13 */
            /* padding: 48 */
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index e5f22c6c4bf9..70bd4381f8e6 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -106,9 +106,9 @@ struct paca_struct {
 #endif /* CONFIG_PPC_STD_MMU_64 */
 
 #ifdef CONFIG_PPC_BOOK3E
-	u64 exgen[8] __attribute__((aligned(0x80)));
+	u64 exgen[8] __aligned(0x40);
 	/* Keep pgd in the same cacheline as the start of extlb */
-	pgd_t *pgd __attribute__((aligned(0x80))); /* Current PGD */
+	pgd_t *pgd __aligned(0x40); /* Current PGD */
 	pgd_t *kernel_pgd;		/* Kernel PGD */
 
 	/* Shared by all threads of a core -- points to tcd of first thread */

commit 77b54e9f213f76a23736940cf94bcd765fc00f40
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Wed Dec 10 00:26:53 2014 +0530

    powernv/powerpc: Add winkle support for offline cpus
    
    Winkle is a deep idle state supported in power8 chips. A core enters
    winkle when all the threads of the core enter winkle. In this state
    power supply to the entire chiplet i.e core, private L2 and private L3
    is turned off. As a result it gives higher powersavings compared to
    sleep.
    
    But entering winkle results in a total hypervisor state loss. Hence the
    hypervisor context has to be preserved before entering winkle and
    restored upon wake up.
    
    Power-on Reset Engine (PORE) is a dedicated engine which is responsible
    for powering on the chiplet during wake up. It can be programmed to
    restore the register contests of a few specific registers. This patch
    uses PORE to restore register state wherever possible and uses stack to
    save and restore rest of the necessary registers.
    
    With hypervisor state restore things fall under three categories-
    per-core state, per-subcore state and per-thread state. To manage this,
    extend the infrastructure introduced for sleep. Mainly we add a paca
    variable subcore_sibling_mask. Using this and the core_idle_state we can
    distingush first thread in core and subcore.
    
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index a0a16847bd40..e5f22c6c4bf9 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -158,6 +158,8 @@ struct paca_struct {
 	u8 thread_idle_state;		/* PNV_THREAD_RUNNING/NAP/SLEEP	*/
 	/* Mask to indicate thread id in core */
 	u8 thread_mask;
+	/* Mask to denote subcore sibling threads */
+	u8 subcore_sibling_mask;
 #endif
 
 #ifdef CONFIG_PPC_BOOK3S_64

commit 7cba160ad789a3ad7e68b92bf20eaad6ed171f80
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Wed Dec 10 00:26:52 2014 +0530

    powernv/cpuidle: Redesign idle states management
    
    Deep idle states like sleep and winkle are per core idle states. A core
    enters these states only when all the threads enter either the
    particular idle state or a deeper one. There are tasks like fastsleep
    hardware bug workaround and hypervisor core state save which have to be
    done only by the last thread of the core entering deep idle state and
    similarly tasks like timebase resync, hypervisor core register restore
    that have to be done only by the first thread waking up from these
    state.
    
    The current idle state management does not have a way to distinguish the
    first/last thread of the core waking/entering idle states. Tasks like
    timebase resync are done for all the threads. This is not only is
    suboptimal, but can cause functionality issues when subcores and kvm is
    involved.
    
    This patch adds the necessary infrastructure to track idle states of
    threads in a per-core structure. It uses this info to perform tasks like
    fastsleep workaround and timebase resync only once per core.
    
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Originally-by: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: linux-pm@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 24a386cbb928..a0a16847bd40 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -152,6 +152,14 @@ struct paca_struct {
 	u64 tm_scratch;                 /* TM scratch area for reclaim */
 #endif
 
+#ifdef CONFIG_PPC_POWERNV
+	/* Per-core mask tracking idle threads and a lock bit-[L][TTTTTTTT] */
+	u32 *core_idle_state_ptr;
+	u8 thread_idle_state;		/* PNV_THREAD_RUNNING/NAP/SLEEP	*/
+	/* Mask to indicate thread id in core */
+	u8 thread_mask;
+#endif
+
 #ifdef CONFIG_PPC_BOOK3S_64
 	/* Exclusive emergency stack pointer for machine check exception. */
 	void *mc_emergency_sp;

commit 6d626c5ea3d8411cc2a72d7cabe70f01dfc32d1d
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Mon Nov 24 21:59:26 2014 +0530

    powerpc/powernv: Cleanup unused MCE definitions/declarations.
    
    Cleanup OpalMCE_* definitions/declarations and other related code which
    is not used anymore.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Acked-by: Benjamin Herrrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index a5139ea6910b..24a386cbb928 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -42,7 +42,6 @@ extern unsigned int debug_smp_processor_id(void); /* from linux/smp.h */
 #define get_slb_shadow()	(get_paca()->slb_shadow_ptr)
 
 struct task_struct;
-struct opal_machine_check_event;
 
 /*
  * Defines the layout of the paca.
@@ -153,12 +152,6 @@ struct paca_struct {
 	u64 tm_scratch;                 /* TM scratch area for reclaim */
 #endif
 
-#ifdef CONFIG_PPC_POWERNV
-	/* Pointer to OPAL machine check event structure set by the
-	 * early exception handler for use by high level C handler
-	 */
-	struct opal_machine_check_event *opal_mc_evt;
-#endif
 #ifdef CONFIG_PPC_BOOK3S_64
 	/* Exclusive emergency stack pointer for machine check exception. */
 	void *mc_emergency_sp;

commit 0ef95b411e73d8789100d017c02c1329c5055802
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Tue Jul 29 18:40:07 2014 +0530

    powerpc/powernv: Invoke opal call to handle hmi.
    
    When we hit the HMI in Linux, invoke opal call to handle/recover from HMI
    errors in real mode and then in virtual mode during check_irq_replay()
    invoke opal_poll_events()/opal_do_notifier() to retrieve HMI event from
    OPAL and act accordingly.
    
    Now that we are ready to handle HMI interrupt directly in linux, remove
    the HMI interrupt registration with firmware.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 5abde4e223bb..a5139ea6910b 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -167,6 +167,7 @@ struct paca_struct {
 	 * and already using emergency stack.
 	 */
 	u16 in_mce;
+	u8 hmi_event_available;		 /* HMI event is available */
 #endif
 
 	/* Stuff for accurate time accounting */

commit 376af5947c0e441ccbf98f0212d4ffbf171528f6
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Jul 10 12:29:19 2014 +1000

    powerpc: Remove STAB code
    
    Old cpus didn't have a Segment Lookaside Buffer (SLB), instead they had
    a Segment Table (STAB). Now that we've dropped support for those cpus,
    we can remove the STAB support entirely.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index bb0bd25f20d0..5abde4e223bb 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -78,10 +78,6 @@ struct paca_struct {
 	u64 kernel_toc;			/* Kernel TOC address */
 	u64 kernelbase;			/* Base address of kernel */
 	u64 kernel_msr;			/* MSR while running in kernel */
-#ifdef CONFIG_PPC_STD_MMU_64
-	u64 stab_real;			/* Absolute address of segment table */
-	u64 stab_addr;			/* Virtual address of segment table */
-#endif /* CONFIG_PPC_STD_MMU_64 */
 	void *emergency_sp;		/* pointer to emergency stack */
 	u64 data_offset;		/* per cpu data offset */
 	s16 hw_cpu_id;			/* Physical processor number */

commit 1739ea9e13e636590dd56c2f4ca85e783da512e7
Author: Sam bobroff <sam.bobroff@au1.ibm.com>
Date:   Wed May 21 16:32:38 2014 +1000

    powerpc: Fix regression of per-CPU DSCR setting
    
    Since commit "efcac65 powerpc: Per process DSCR + some fixes (try#4)"
    it is no longer possible to set the DSCR on a per-CPU basis.
    
    The old behaviour was to minipulate the DSCR SPR directly but this is no
    longer sufficient: the value is quickly overwritten by context switching.
    
    This patch stores the per-CPU DSCR value in a kernel variable rather than
    directly in the SPR and it is used whenever a process has not set the DSCR
    itself. The sysfs interface (/sys/devices/system/cpu/cpuN/dscr) is unchanged.
    
    Writes to the old global default (/sys/devices/system/cpu/dscr_default)
    now set all of the per-CPU values and reads return the last written value.
    
    The new per-CPU default is added to the paca_struct and is used everywhere
    outside of sysfs.c instead of the old global default.
    
    Signed-off-by: Sam Bobroff <sam.bobroff@au1.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 8e956a0b6e85..bb0bd25f20d0 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -92,7 +92,10 @@ struct paca_struct {
 	struct slb_shadow *slb_shadow_ptr;
 	struct dtl_entry *dispatch_log;
 	struct dtl_entry *dispatch_log_end;
+#endif /* CONFIG_PPC_STD_MMU_64 */
+	u64 dscr_default;		/* per-CPU default DSCR */
 
+#ifdef CONFIG_PPC_STD_MMU_64
 	/*
 	 * Now, starting in cacheline 2, the exception save areas
 	 */

commit 609af38f8fc0f1dab993b2c67f90d07f761ea902
Author: Scott Wood <scottwood@freescale.com>
Date:   Mon Mar 10 17:29:38 2014 -0500

    powerpc/booke64: Critical and machine check exception support
    
    Add special state saving for critical and machine check exceptions.
    
    Most of this code could be used to handle debug exceptions taken from
    kernel space, but actually doing so is outside the scope of this patch.
    
    The various critical and machine check exceptions now point to their
    real handlers, rather than hanging the kernel.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 948f01a04cc3..8e956a0b6e85 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -116,8 +116,11 @@ struct paca_struct {
 	/* Shared by all threads of a core -- points to tcd of first thread */
 	struct tlb_core_data *tcd_ptr;
 
-	/* We can have up to 3 levels of reentrancy in the TLB miss handler */
-	u64 extlb[3][EX_TLB_SIZE / sizeof(u64)];
+	/*
+	 * We can have up to 3 levels of reentrancy in the TLB miss handler,
+	 * in each of four exception levels (normal, crit, mcheck, debug).
+	 */
+	u64 extlb[12][EX_TLB_SIZE / sizeof(u64)];
 	u64 exmc[8];		/* used for machine checks */
 	u64 excrit[8];		/* used for crit interrupts */
 	u64 exdbg[8];		/* used for debug interrupts */

commit 9d378dfac885f72b8b369d08fc61bef36e2f2dd1
Author: Scott Wood <scottwood@freescale.com>
Date:   Mon Mar 10 17:29:38 2014 -0500

    powerpc/booke64: Use SPRG7 for VDSO
    
    Previously SPRG3 was marked for use by both VDSO and critical
    interrupts (though critical interrupts were not fully implemented).
    
    In commit 8b64a9dfb091f1eca8b7e58da82f1e7d1d5fe0ad ("powerpc/booke64:
    Use SPRG0/3 scratch for bolted TLB miss & crit int"), Mihai Caraman
    made an attempt to resolve this conflict by restoring the VDSO value
    early in the critical interrupt, but this has some issues:
    
     - It's incompatible with EXCEPTION_COMMON which restores r13 from the
       by-then-overwritten scratch (this cost me some debugging time).
     - It forces critical exceptions to be a special case handled
       differently from even machine check and debug level exceptions.
     - It didn't occur to me that it was possible to make this work at all
       (by doing a final "ld r13, PACA_EXCRIT+EX_R13(r13)") until after
       I made (most of) this patch. :-)
    
    It might be worth investigating using a load rather than SPRG on return
    from all exceptions (except TLB misses where the scratch never leaves
    the SPRG) -- it could save a few cycles.  Until then, let's stick with
    SPRG for all exceptions.
    
    Since we cannot use SPRG4-7 for scratch without corrupting the state of
    a KVM guest, move VDSO to SPRG7 on book3e.  Since neither SPRG4-7 nor
    critical interrupts exist on book3s, SPRG3 is still used for VDSO
    there.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Cc: Mihai Caraman <mihai.caraman@freescale.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: kvm-ppc@vger.kernel.org

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 9c5dbc3833fb..948f01a04cc3 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -146,7 +146,7 @@ struct paca_struct {
 	u8 io_sync;			/* writel() needs spin_unlock sync */
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */
 	u8 nap_state_lost;		/* NV GPR values lost in power7_idle */
-	u64 sprg3;			/* Saved user-visible sprg */
+	u64 sprg_vdso;			/* Saved user-visible sprg */
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 	u64 tm_scratch;                 /* TM scratch area for reclaim */
 #endif

commit fac515db45207718168cb55ca4d0a390e43b61af
Merge: 3ac8ff1c475b d064f30e5063
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Jan 15 14:22:35 2014 +1100

    Merge remote-tracking branch 'scott/next' into next
    
    Freescale updates from Scott:
    
    <<
    Highlights include 32-bit booke relocatable support, e6500 hardware
    tablewalk support, various e500 SPE fixes, some new/revived boards, and
    e6500 deeper idle and altivec powerdown modes.
    >>

commit c141611fb1ee2cfc374cf9be5327e97f361c4bed
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu Jan 9 00:44:29 2014 -0500

    powerpc: Delete non-required instances of include <linux/init.h>
    
    None of these files are actually using any __init type directives
    and hence don't need to include <linux/init.h>.  Most are just a
    left over from __devinit and __cpuinit removal, or simply due to
    code getting copied from one driver to the next.
    
    The one instance where we add an include for init.h covers off
    a case where that file was implicitly getting it from another
    header which itself didn't need it.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index c3523d1dda58..68bee4636045 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -16,7 +16,6 @@
 
 #ifdef CONFIG_PPC64
 
-#include <linux/init.h>
 #include <asm/types.h>
 #include <asm/lppaca.h>
 #include <asm/mmu.h>

commit 28efc35fe68dacbddc4b12c2fa8f2df1593a4ad3
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Oct 11 19:22:38 2013 -0500

    powerpc/e6500: TLB miss handler with hardware tablewalk support
    
    There are a few things that make the existing hw tablewalk handlers
    unsuitable for e6500:
    
     - Indirect entries go in TLB1 (though the resulting direct entries go in
       TLB0).
    
     - It has threads, but no "tlbsrx." -- so we need a spinlock and
       a normal "tlbsx".  Because we need this lock, hardware tablewalk
       is mandatory on e6500 unless we want to add spinlock+tlbsx to
       the normal bolted TLB miss handler.
    
     - TLB1 has no HES (nor next-victim hint) so we need software round robin
       (TODO: integrate this round robin data with hugetlb/KVM)
    
     - The existing tablewalk handlers map half of a page table at a time,
       because IBM hardware has a fixed 1MiB indirect page size.  e6500
       has variable size indirect entries, with a minimum of 2MiB.
       So we can't do the half-page indirect mapping, and even if we
       could it would be less efficient than mapping the full page.
    
     - Like on e5500, the linear mapping is bolted, so we don't need the
       overhead of supporting nested tlb misses.
    
    Note that hardware tablewalk does not work in rev1 of e6500.
    We do not expect to support e6500 rev1 in mainline Linux.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Cc: Mihai Caraman <mihai.caraman@freescale.com>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index c3523d1dda58..e81731c62a7f 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -113,6 +113,10 @@ struct paca_struct {
 	/* Keep pgd in the same cacheline as the start of extlb */
 	pgd_t *pgd __attribute__((aligned(0x80))); /* Current PGD */
 	pgd_t *kernel_pgd;		/* Kernel PGD */
+
+	/* Shared by all threads of a core -- points to tcd of first thread */
+	struct tlb_core_data *tcd_ptr;
+
 	/* We can have up to 3 levels of reentrancy in the TLB miss handler */
 	u64 extlb[3][EX_TLB_SIZE / sizeof(u64)];
 	u64 exmc[8];		/* used for machine checks */
@@ -123,6 +127,8 @@ struct paca_struct {
 	void *mc_kstack;
 	void *crit_kstack;
 	void *dbg_kstack;
+
+	struct tlb_core_data tcd;
 #endif /* CONFIG_PPC_BOOK3E */
 
 	mm_context_t context;

commit 729b0f715371ce1e7636b4958fc45d6882442456
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Wed Oct 30 20:04:00 2013 +0530

    powerpc/book3s: Introduce exclusive emergency stack for machine check exception.
    
    This patch introduces exclusive emergency stack for machine check exception.
    We use emergency stack to handle machine check exception so that we can save
    MCE information (srr1, srr0, dar and dsisr) before turning on ME bit and be
    ready for re-entrancy. This helps us to prevent clobbering of MCE information
    in case of nested machine checks.
    
    The reason for using emergency stack over normal kernel stack is that the
    machine check might occur in the middle of setting up a stack frame which may
    result into improper use of kernel stack.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index b6ea9e068c13..c3523d1dda58 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -152,6 +152,15 @@ struct paca_struct {
 	 */
 	struct opal_machine_check_event *opal_mc_evt;
 #endif
+#ifdef CONFIG_PPC_BOOK3S_64
+	/* Exclusive emergency stack pointer for machine check exception. */
+	void *mc_emergency_sp;
+	/*
+	 * Flag to check whether we are in machine check early handler
+	 * and already using emergency stack.
+	 */
+	u16 in_mce;
+#endif
 
 	/* Stuff for accurate time accounting */
 	u64 user_time;			/* accumulated usermode TB ticks */

commit 7aa79938f7d76f5865d0b2a2d9bbe2337560261f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:17:51 2013 +0530

    kvm: powerpc: book3s: pr: Rename KVM_BOOK3S_PR to KVM_BOOK3S_PR_POSSIBLE
    
    With later patches supporting PR kvm as a kernel module, the changes
    that has to be built into the main kernel binary to enable PR KVM module
    is now selected via KVM_BOOK3S_PR_POSSIBLE
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index a5954cebbc55..b6ea9e068c13 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -166,7 +166,7 @@ struct paca_struct {
 	struct dtl_entry *dtl_curr;	/* pointer corresponding to dtl_ridx */
 
 #ifdef CONFIG_KVM_BOOK3S_HANDLER
-#ifdef CONFIG_KVM_BOOK3S_PR
+#ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
 	/* We use this to store guest state in */
 	struct kvmppc_book3s_shadow_vcpu shadow_vcpu;
 #endif

commit 54bb7f4bda0ee49f39dc593c2d73fe6053a99dbb
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 7 02:01:51 2013 +1000

    powerpc: Make rwlocks endian safe
    
    Our ppc64 spinlocks and rwlocks use a trick where a lock token and
    the paca index are placed in the lock with a single store. Since we
    are using two u16s they need adjusting for little endian.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 17d40a2a00c4..a5954cebbc55 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -68,8 +68,13 @@ struct paca_struct {
 	 * instruction.  They must travel together and be properly
 	 * aligned.
 	 */
+#ifdef __BIG_ENDIAN__
 	u16 lock_token;			/* Constant 0x8000, used in locks */
 	u16 paca_index;			/* Logical processor number */
+#else
+	u16 paca_index;			/* Logical processor number */
+	u16 lock_token;			/* Constant 0x8000, used in locks */
+#endif
 
 	u64 kernel_toc;			/* Kernel TOC address */
 	u64 kernelbase;			/* Base address of kernel */

commit bc2e6c6ac21183a6102a926f83186d9cac6713f8
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Aug 13 15:54:52 2013 +1000

    powerpc: Avoid link stack corruption for MMU on exceptions
    
    When we have MMU on exceptions (POWER8) and a relocatable kernel, we
    need to branch from the initial exception vectors at 0x0 to up high
    where the kernel might be located.  Currently we do this using the link
    register.
    
    Unfortunately this corrupts the link stack and instead we should use the
    count register.  We did this for the syscall entry path in:
      6a40480 powerpc: Avoid link stack corruption in MMU on syscall entry path
    but I stupidly forgot to do the same for other exceptions.
    
    This patch changes the initial exception vectors to use the count
    register instead of the link register when we need to branch up to the
    relocated kernel.
    
    I have a dodgy userspace test which loops calling a function that reads
    the PVR (mfpvr in userspace will be emulated by the kernel via the
    program check exception).  On POWER8 and with CONFIG_RELOCATABLE=y, I
    get a ~10% performance improvement with my userspace test with this
    patch.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 77c91e74b612..17d40a2a00c4 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -93,9 +93,9 @@ struct paca_struct {
 	 * Now, starting in cacheline 2, the exception save areas
 	 */
 	/* used for most interrupts/exceptions */
-	u64 exgen[12] __attribute__((aligned(0x80)));
-	u64 exmc[12];		/* used for machine checks */
-	u64 exslb[12];		/* used for SLB/segment table misses
+	u64 exgen[13] __attribute__((aligned(0x80)));
+	u64 exmc[13];		/* used for machine checks */
+	u64 exslb[13];		/* used for SLB/segment table misses
  				 * on the linear mapping */
 	/* SLB related definitions */
 	u16 vmalloc_sllp;

commit afc07701ced6463786d09a3b9baf894c1397e991
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Feb 13 16:21:34 2013 +0000

    powerpc: Add transactional memory paca scratch register to show_regs
    
    Add transactional memory paca scratch register to show_regs.  This is useful
    for debugging.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 4b74c6c9d82a..77c91e74b612 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -137,6 +137,9 @@ struct paca_struct {
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */
 	u8 nap_state_lost;		/* NV GPR values lost in power7_idle */
 	u64 sprg3;			/* Saved user-visible sprg */
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+	u64 tm_scratch;                 /* TM scratch area for reclaim */
+#endif
 
 #ifdef CONFIG_PPC_POWERNV
 	/* Pointer to OPAL machine check event structure set by the

commit 6a7e406419d8b176efbc5be41a82299025ad1b43
Author: Geoff Levand <geoff@infradead.org>
Date:   Wed Feb 13 17:03:16 2013 +0000

    powerpc: Move boot_paca into early_setup
    
    The powerpc boot_paca symbol is now only used within the
    early_setup() routine, so move it from its global definition
    into early_setup().
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index c47d687ab01b..4b74c6c9d82a 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -167,7 +167,6 @@ struct paca_struct {
 };
 
 extern struct paca_struct *paca;
-extern __initdata struct paca_struct boot_paca;
 extern void initialise_paca(struct paca_struct *new_paca, int cpu);
 extern void setup_paca(struct paca_struct *new_paca);
 extern void allocate_pacas(void);

commit a09688cd23d477ebd9c8f57881bfe907240cb0a1
Author: Haren Myneni <haren@linux.vnet.ibm.com>
Date:   Thu Dec 6 21:48:26 2012 +0000

    powerpc: Increase exceptions arrays in paca struct to save PPR
    
    [PATCH 3/6] powerpc: Increase exceptions arrays in paca struct to save PPR
    
    Using paca to save user defined PPR value in the first level exception vector.
    
    Signed-off-by: Haren Myneni <haren@us.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index e9e7a6999bb8..c47d687ab01b 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -93,9 +93,9 @@ struct paca_struct {
 	 * Now, starting in cacheline 2, the exception save areas
 	 */
 	/* used for most interrupts/exceptions */
-	u64 exgen[11] __attribute__((aligned(0x80)));
-	u64 exmc[11];		/* used for machine checks */
-	u64 exslb[11];		/* used for SLB/segment table misses
+	u64 exgen[12] __attribute__((aligned(0x80)));
+	u64 exmc[12];		/* used for machine checks */
+	u64 exslb[12];		/* used for SLB/segment table misses
  				 * on the linear mapping */
 	/* SLB related definitions */
 	u16 vmalloc_sllp;

commit 735cafc32b661f08a266a8d754e6cfbd82c11704
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Sep 10 02:52:54 2012 +0000

    powerpc/mm: Use 32bit array for slb cache
    
    With larger vsid we need to track more bits of ESID in slb cache
    for slb invalidate.
    
    Reviewed-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 7796519fd238..e9e7a6999bb8 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -100,7 +100,7 @@ struct paca_struct {
 	/* SLB related definitions */
 	u16 vmalloc_sllp;
 	u16 slb_cache_ptr;
-	u16 slb_cache[SLB_CACHE_ENTRIES];
+	u32 slb_cache[SLB_CACHE_ENTRIES];
 #endif /* CONFIG_PPC_STD_MMU_64 */
 
 #ifdef CONFIG_PPC_BOOK3E

commit 0127262c01f0beb485f917c720d1d95d165dfdbf
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Thu Sep 6 02:49:44 2012 +0000

    powerpc: Restore VDSO information on critical exception om BookE
    
    Critical exception on 64-bit booke uses user-visible SPRG3 as scratch.
    Restore VDSO information in SPRG3 on exception prolog.
    
    Use a common sprg3 field in PACA for all powerpc64 architectures.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index daf813fea91f..7796519fd238 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -136,6 +136,7 @@ struct paca_struct {
 	u8 io_sync;			/* writel() needs spin_unlock sync */
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */
 	u8 nap_state_lost;		/* NV GPR values lost in power7_idle */
+	u64 sprg3;			/* Saved user-visible sprg */
 
 #ifdef CONFIG_PPC_POWERNV
 	/* Pointer to OPAL machine check event structure set by the

commit 7230c5644188cd9e3fb380cc97dde00c464a3ba7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Mar 6 18:27:59 2012 +1100

    powerpc: Rework lazy-interrupt handling
    
    The current implementation of lazy interrupts handling has some
    issues that this tries to address.
    
    We don't do the various workarounds we need to do when re-enabling
    interrupts in some cases such as when returning from an interrupt
    and thus we may still lose or get delayed decrementer or doorbell
    interrupts.
    
    The current scheme also makes it much harder to handle the external
    "edge" interrupts provided by some BookE processors when using the
    EPR facility (External Proxy) and the Freescale Hypervisor.
    
    Additionally, we tend to keep interrupts hard disabled in a number
    of cases, such as decrementer interrupts, external interrupts, or
    when a masked decrementer interrupt is pending. This is sub-optimal.
    
    This is an attempt at fixing it all in one go by reworking the way
    we do the lazy interrupt disabling from the ground up.
    
    The base idea is to replace the "hard_enabled" field with a
    "irq_happened" field in which we store a bit mask of what interrupt
    occurred while soft-disabled.
    
    When re-enabling, either via arch_local_irq_restore() or when returning
    from an interrupt, we can now decide what to do by testing bits in that
    field.
    
    We then implement replaying of the missed interrupts either by
    re-using the existing exception frame (in exception exit case) or via
    the creation of a new one from an assembly trampoline (in the
    arch_local_irq_enable case).
    
    This removes the need to play with the decrementer to try to create
    fake interrupts, among others.
    
    In addition, this adds a few refinements:
    
     - We no longer  hard disable decrementer interrupts that occur
    while soft-disabled. We now simply bump the decrementer back to max
    (on BookS) or leave it stopped (on BookE) and continue with hard interrupts
    enabled, which means that we'll potentially get better sample quality from
    performance monitor interrupts.
    
     - Timer, decrementer and doorbell interrupts now hard-enable
    shortly after removing the source of the interrupt, which means
    they no longer run entirely hard disabled. Again, this will improve
    perf sample quality.
    
     - On Book3E 64-bit, we now make the performance monitor interrupt
    act as an NMI like Book3S (the necessary C code for that to work
    appear to already be present in the FSL perf code, notably calling
    nmi_enter instead of irq_enter). (This also fixes a bug where BookE
    perfmon interrupts could clobber r14 ... oops)
    
     - We could make "masked" decrementer interrupts act as NMIs when doing
    timer-based perf sampling to improve the sample quality.
    
    Signed-off-by-yet: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    ---
    
    v2:
    
    - Add hard-enable to decrementer, timer and doorbells
    - Fix CR clobber in masked irq handling on BookE
    - Make embedded perf interrupt act as an NMI
    - Add a PACA_HAPPENED_EE_EDGE for use by FSL if they want
      to retrigger an interrupt without preventing hard-enable
    
    v3:
    
     - Fix or vs. ori bug on Book3E
     - Fix enabling of interrupts for some exceptions on Book3E
    
    v4:
    
     - Fix resend of doorbells on return from interrupt on Book3E
    
    v5:
    
     - Rebased on top of my latest series, which involves some significant
    rework of some aspects of the patch.
    
    v6:
     - 32-bit compile fix
     - more compile fixes with various .config combos
     - factor out the asm code to soft-disable interrupts
     - remove the C wrapper around preempt_schedule_irq
    
    v7:
     - Fix a bug with hard irq state tracking on native power7

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 269c05a36d91..daf813fea91f 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -132,7 +132,7 @@ struct paca_struct {
 	u64 saved_msr;			/* MSR saved here by enter_rtas */
 	u16 trap_save;			/* Used when bad stack is encountered */
 	u8 soft_enabled;		/* irq soft-enable flag */
-	u8 hard_enabled;		/* set if irqs are enabled in MSR */
+	u8 irq_happened;		/* irq happened while soft-disabled */
 	u8 io_sync;			/* writel() needs spin_unlock sync */
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */
 	u8 nap_state_lost;		/* NV GPR values lost in power7_idle */

commit 2fde6d20bb75b53f1ead383b4713f95d0d6d9f59
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 5 19:47:26 2011 +0000

    powerpc: Provide a way for KVM to indicate that NV GPR values are lost
    
    This fixes a problem where a CPU thread coming out of nap mode can
    think it has valid values in the nonvolatile GPRs (r14 - r31) as saved
    away in power7_idle, but in fact the values have been trashed because
    the thread was used for KVM in the mean time.  The result is that the
    thread crashes because code that called power7_idle (e.g.,
    pnv_smp_cpu_kill_self()) goes to use values in registers that have
    been trashed.
    
    The bit field in SRR1 that tells whether state was lost only reflects
    the most recent nap, which may not have been the nap instruction in
    power7_idle.  So we need an extra PACA field to indicate that state
    has been lost even if SRR1 indicates that the most recent nap didn't
    lose state.  We clear this field when saving the state in power7_idle,
    we set it to a non-zero value when we use the thread for KVM, and we
    test it in power7_wakeup_noloss.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 17722c73ba2e..269c05a36d91 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -135,6 +135,7 @@ struct paca_struct {
 	u8 hard_enabled;		/* set if irqs are enabled in MSR */
 	u8 io_sync;			/* writel() needs spin_unlock sync */
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */
+	u8 nap_state_lost;		/* NV GPR values lost in power7_idle */
 
 #ifdef CONFIG_PPC_POWERNV
 	/* Pointer to OPAL machine check event structure set by the

commit ed79ba9e15f84cef05aba5cbfe6e93f9b43c31f4
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Sep 19 17:45:04 2011 +0000

    powerpc/powernv: Machine check and other system interrupts
    
    OPAL can handle various interrupt for us such as Machine Checks (it
    performs all sorts of recovery tasks and passes back control to us with
    informations about the error), Hardware Management Interrupts and Softpatch
    interrupts.
    
    This wires up the mechanisms and prints out specific informations returned
    by HAL when a machine check occurs.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 516bfb3f47d9..17722c73ba2e 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -43,6 +43,7 @@ extern unsigned int debug_smp_processor_id(void); /* from linux/smp.h */
 #define get_slb_shadow()	(get_paca()->slb_shadow_ptr)
 
 struct task_struct;
+struct opal_machine_check_event;
 
 /*
  * Defines the layout of the paca.
@@ -135,6 +136,13 @@ struct paca_struct {
 	u8 io_sync;			/* writel() needs spin_unlock sync */
 	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */
 
+#ifdef CONFIG_PPC_POWERNV
+	/* Pointer to OPAL machine check event structure set by the
+	 * early exception handler for use by high level C handler
+	 */
+	struct opal_machine_check_event *opal_mc_evt;
+#endif
+
 	/* Stuff for accurate time accounting */
 	u64 user_time;			/* accumulated usermode TB ticks */
 	u64 system_time;		/* accumulated system TB ticks */

commit 184475029a724b6b900d88fc3a5f462a6107d5af
Merge: 3b76eefe0f97 f1f4ee01c0d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 22:59:39 2011 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (99 commits)
      drivers/virt: add missing linux/interrupt.h to fsl_hypervisor.c
      powerpc/85xx: fix mpic configuration in CAMP mode
      powerpc: Copy back TIF flags on return from softirq stack
      powerpc/64: Make server perfmon only built on ppc64 server devices
      powerpc/pseries: Fix hvc_vio.c build due to recent changes
      powerpc: Exporting boot_cpuid_phys
      powerpc: Add CFAR to oops output
      hvc_console: Add kdb support
      powerpc/pseries: Fix hvterm_raw_get_chars to accept < 16 chars, fixing xmon
      powerpc/irq: Quieten irq mapping printks
      powerpc: Enable lockup and hung task detectors in pseries and ppc64 defeconfigs
      powerpc: Add mpt2sas driver to pseries and ppc64 defconfig
      powerpc: Disable IRQs off tracer in ppc64 defconfig
      powerpc: Sync pseries and ppc64 defconfigs
      powerpc/pseries/hvconsole: Fix dropped console output
      hvc_console: Improve tty/console put_chars handling
      powerpc/kdump: Fix timeout in crash_kexec_wait_realmode
      powerpc/mm: Fix output of total_ram.
      powerpc/cpufreq: Add cpufreq driver for Momentum Maple boards
      powerpc: Correct annotations of pmu registration functions
      ...
    
    Fix up trivial Kconfig/Makefile conflicts in arch/powerpc, drivers, and
    drivers/cpufreq

commit de56a948b9182fbcf92cb8212f114de096c2d574
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:21:34 2011 +0000

    KVM: PPC: Add support for Book3S processors in hypervisor mode
    
    This adds support for KVM running on 64-bit Book 3S processors,
    specifically POWER7, in hypervisor mode.  Using hypervisor mode means
    that the guest can use the processor's supervisor mode.  That means
    that the guest can execute privileged instructions and access privileged
    registers itself without trapping to the host.  This gives excellent
    performance, but does mean that KVM cannot emulate a processor
    architecture other than the one that the hardware implements.
    
    This code assumes that the guest is running paravirtualized using the
    PAPR (Power Architecture Platform Requirements) interface, which is the
    interface that IBM's PowerVM hypervisor uses.  That means that existing
    Linux distributions that run on IBM pSeries machines will also run
    under KVM without modification.  In order to communicate the PAPR
    hypercalls to qemu, this adds a new KVM_EXIT_PAPR_HCALL exit code
    to include/linux/kvm.h.
    
    Currently the choice between book3s_hv support and book3s_pr support
    (i.e. the existing code, which runs the guest in user mode) has to be
    made at kernel configuration time, so a given kernel binary can only
    do one or the other.
    
    This new book3s_hv code doesn't support MMIO emulation at present.
    Since we are running paravirtualized guests, this isn't a serious
    restriction.
    
    With the guest running in supervisor mode, most exceptions go straight
    to the guest.  We will never get data or instruction storage or segment
    interrupts, alignment interrupts, decrementer interrupts, program
    interrupts, single-step interrupts, etc., coming to the hypervisor from
    the guest.  Therefore this introduces a new KVMTEST_NONHV macro for the
    exception entry path so that we don't have to do the KVM test on entry
    to those exception handlers.
    
    We do however get hypervisor decrementer, hypervisor data storage,
    hypervisor instruction storage, and hypervisor emulation assist
    interrupts, so we have to handle those.
    
    In hypervisor mode, real-mode accesses can access all of RAM, not just
    a limited amount.  Therefore we put all the guest state in the vcpu.arch
    and use the shadow_vcpu in the PACA only for temporary scratch space.
    We allocate the vcpu with kzalloc rather than vzalloc, and we don't use
    anything in the kvmppc_vcpu_book3s struct, so we don't allocate it.
    We don't have a shared page with the guest, but we still need a
    kvm_vcpu_arch_shared struct to store the values of various registers,
    so we include one in the vcpu_arch struct.
    
    The POWER7 processor has a restriction that all threads in a core have
    to be in the same partition.  MMU-on kernel code counts as a partition
    (partition 0), so we have to do a partition switch on every entry to and
    exit from the guest.  At present we require the host and guest to run
    in single-thread mode because of this hardware restriction.
    
    This code allocates a hashed page table for the guest and initializes
    it with HPTEs for the guest's Virtual Real Memory Area (VRMA).  We
    require that the guest memory is allocated using 16MB huge pages, in
    order to simplify the low-level memory management.  This also means that
    we can get away without tracking paging activity in the host for now,
    since huge pages can't be paged or swapped.
    
    This also adds a few new exports needed by the book3s_hv code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 58f4a18ef60c..a6da12859959 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -147,8 +147,10 @@ struct paca_struct {
 	struct dtl_entry *dtl_curr;	/* pointer corresponding to dtl_ridx */
 
 #ifdef CONFIG_KVM_BOOK3S_HANDLER
+#ifdef CONFIG_KVM_BOOK3S_PR
 	/* We use this to store guest state in */
 	struct kvmppc_book3s_shadow_vcpu shadow_vcpu;
+#endif
 	struct kvmppc_host_state kvm_hstate;
 #endif
 };

commit 3c42bf8a717cb636e0ed2ed77194669e2ac3ed56
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:20:58 2011 +0000

    KVM: PPC: Split host-state fields out of kvmppc_book3s_shadow_vcpu
    
    There are several fields in struct kvmppc_book3s_shadow_vcpu that
    temporarily store bits of host state while a guest is running,
    rather than anything relating to the particular guest or vcpu.
    This splits them out into a new kvmppc_host_state structure and
    modifies the definitions in asm-offsets.c to suit.
    
    On 32-bit, we have a kvmppc_host_state structure inside the
    kvmppc_book3s_shadow_vcpu since the assembly code needs to be able
    to get to them both with one pointer.  On 64-bit they are separate
    fields in the PACA.  This means that on 64-bit we don't need to
    copy the kvmppc_host_state in and out on vcpu load/unload, and
    in future will mean that the book3s_hv code doesn't need a
    shadow_vcpu struct in the PACA at all.  That does mean that we
    have to be careful not to rely on any values persisting in the
    hstate field of the paca across any point where we could block
    or get preempted.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 74126765106a..58f4a18ef60c 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -149,6 +149,7 @@ struct paca_struct {
 #ifdef CONFIG_KVM_BOOK3S_HANDLER
 	/* We use this to store guest state in */
 	struct kvmppc_book3s_shadow_vcpu shadow_vcpu;
+	struct kvmppc_host_state kvm_hstate;
 #endif
 };
 

commit f67f4ef5fcdfdeeddcb0ed4ab2c85d9bb4185d5f
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Jun 22 11:25:42 2011 +0000

    powerpc/book3e-64: use a separate TLB handler when linear map is bolted
    
    On MMUs such as FSL where we can guarantee the entire linear mapping is
    bolted, we don't need to worry about linear TLB misses.  If on top of
    that we do a full table walk, we get rid of all recursive TLB faults, and
    can dispense with some state saving.  This gains a few percent on
    TLB-miss-heavy workloads, and around 50% on a benchmark that had a high
    rate of virtual page table faults under the normal handler.
    
    While touching the EX_TLB layout, remove EX_TLB_MMUCR0, EX_TLB_SRR0, and
    EX_TLB_SRR1 as they're not used.
    
    [BenH: Fixed build with 64K pages (wsp config)]
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 74126765106a..c1f65f597920 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -103,11 +103,12 @@ struct paca_struct {
 #endif /* CONFIG_PPC_STD_MMU_64 */
 
 #ifdef CONFIG_PPC_BOOK3E
-	pgd_t *pgd;			/* Current PGD */
-	pgd_t *kernel_pgd;		/* Kernel PGD */
 	u64 exgen[8] __attribute__((aligned(0x80)));
+	/* Keep pgd in the same cacheline as the start of extlb */
+	pgd_t *pgd __attribute__((aligned(0x80))); /* Current PGD */
+	pgd_t *kernel_pgd;		/* Kernel PGD */
 	/* We can have up to 3 levels of reentrancy in the TLB miss handler */
-	u64 extlb[3][EX_TLB_SIZE / sizeof(u64)] __attribute__((aligned(0x80)));
+	u64 extlb[3][EX_TLB_SIZE / sizeof(u64)];
 	u64 exmc[8];		/* used for machine checks */
 	u64 excrit[8];		/* used for crit interrupts */
 	u64 exdbg[8];		/* used for debug interrupts */

commit 48404f2e95ef0ffd8134d89c8abcd1a15e15f1b0
Author: Paul Mackerras <paulus@samba.org>
Date:   Sun May 1 19:48:20 2011 +0000

    powerpc: Save Come-From Address Register (CFAR) in exception frame
    
    Recent 64-bit server processors (POWER6 and POWER7) have a "Come-From
    Address Register" (CFAR), that records the address of the most recent
    branch or rfid (return from interrupt) instruction for debugging purposes.
    
    This saves the value of the CFAR in the exception entry code and stores
    it in the exception frame.  We also make xmon print the CFAR value in
    its register dump code.
    
    Rather than extend the pt_regs struct at this time, we steal the orig_gpr3
    field, which is only used for system calls, and use it for the CFAR value
    for all exceptions/interrupts other than system calls.  This means we
    don't save the CFAR on system calls, which is not a great problem since
    system calls tend not to happen unexpectedly, and also avoids adding the
    overhead of reading the CFAR to the system call entry path.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 65c13c48db43..74126765106a 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -92,9 +92,9 @@ struct paca_struct {
 	 * Now, starting in cacheline 2, the exception save areas
 	 */
 	/* used for most interrupts/exceptions */
-	u64 exgen[10] __attribute__((aligned(0x80)));
-	u64 exmc[10];		/* used for machine checks */
-	u64 exslb[10];		/* used for SLB/segment table misses
+	u64 exgen[11] __attribute__((aligned(0x80)));
+	u64 exmc[11];		/* used for machine checks */
+	u64 exslb[11];		/* used for SLB/segment table misses
  				 * on the linear mapping */
 	/* SLB related definitions */
 	u16 vmalloc_sllp;

commit a7b8ad405862fb10e496ce839d423dfc94ac821b
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Apr 7 21:22:23 2011 +0000

    powerpc/book3e: Fix extlb size
    
    The calculation of the size for the exception save area of the TLB
    miss handler is wrong, luckily it's too big not too small.
    
    Rework it to make it a bit clearer, and also correct. We want 3 save
    areas, each EX_TLB_SIZE _bytes_.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index f6da4f517fca..65c13c48db43 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -106,7 +106,8 @@ struct paca_struct {
 	pgd_t *pgd;			/* Current PGD */
 	pgd_t *kernel_pgd;		/* Kernel PGD */
 	u64 exgen[8] __attribute__((aligned(0x80)));
-	u64 extlb[EX_TLB_SIZE*3] __attribute__((aligned(0x80)));
+	/* We can have up to 3 levels of reentrancy in the TLB miss handler */
+	u64 extlb[3][EX_TLB_SIZE / sizeof(u64)] __attribute__((aligned(0x80)));
 	u64 exmc[8];		/* used for machine checks */
 	u64 excrit[8];		/* used for crit interrupts */
 	u64 exdbg[8];		/* used for debug interrupts */

commit 948cf67c4726cca2fc57533dccadfb54d890689d
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Jan 24 18:42:41 2011 +1100

    powerpc: Add NAP mode support on Power7 in HV mode
    
    Wakeup comes from the system reset handler with a potential loss of
    the non-hypervisor CPU state. We save the non-volatile state on the
    stack and a pointer to it in the PACA, which the system reset handler
    uses to restore things
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index ec57540cd7af..f6da4f517fca 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -125,7 +125,7 @@ struct paca_struct {
 	struct task_struct *__current;	/* Pointer to current */
 	u64 kstack;			/* Saved Kernel stack addr */
 	u64 stab_rr;			/* stab/slb round-robin counter */
-	u64 saved_r1;			/* r1 save for RTAS calls */
+	u64 saved_r1;			/* r1 save for RTAS calls or PM */
 	u64 saved_msr;			/* MSR saved here by enter_rtas */
 	u16 trap_save;			/* Used when bad stack is encountered */
 	u8 soft_enabled;		/* irq soft-enable flag */

commit d4429f608abde89e8bc1e24b43cd503feb95c496
Merge: e10117d36ef7 6a1c9dfe4186
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 21:19:54 2010 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (71 commits)
      powerpc/44x: Update ppc44x_defconfig
      powerpc/watchdog: Make default timeout for Book-E watchdog a Kconfig option
      fsl_rio: Add comments for sRIO registers.
      powerpc/fsl-booke: Add e55xx (64-bit) smp defconfig
      powerpc/fsl-booke: Add p5020 DS board support
      powerpc/fsl-booke64: Use TLB CAMs to cover linear mapping on FSL 64-bit chips
      powerpc/fsl-booke: Add support for FSL Arch v1.0 MMU in setup_page_sizes
      powerpc/fsl-booke: Add support for FSL 64-bit e5500 core
      powerpc/85xx: add cache-sram support
      powerpc/85xx: add ngPIXIS FPGA device tree node to the P1022DS board
      powerpc: Fix compile error with paca code on ppc64e
      powerpc/fsl-booke: Add p3041 DS board support
      oprofile/fsl emb: Don't set MSR[PMM] until after clearing the interrupt.
      powerpc/fsl-booke: Add PCI device ids for P2040/P3041/P5010/P5020 QoirQ chips
      powerpc/mpc8xxx_gpio: Add support for 'qoriq-gpio' controllers
      powerpc/fsl_booke: Add support to boot from core other than 0
      powerpc/p1022: Add probing for individual DMA channels
      powerpc/fsl_soc: Search all global-utilities nodes for rstccr
      powerpc: Fix invalid page flags in create TLB CAM path for PTE_64BIT
      powerpc/mpc83xx: Support for MPC8308 P1M board
      ...
    
    Fix up conflict with the generic irq_work changes in arch/powerpc/kernel/time.c

commit e360adbe29241a0194e10e20595360dd7b98a2b3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 14 14:01:34 2010 +0800

    irq_work: Add generic hardirq context callbacks
    
    Provide a mechanism that allows running code in IRQ context. It is
    most useful for NMI code that needs to interact with the rest of the
    system -- like wakeup a task to drain buffers.
    
    Perf currently has such a mechanism, so extract that and provide it as
    a generic feature, independent of perf so that others may also
    benefit.
    
    The IRQ context callback is generated through self-IPIs where
    possible, or on architectures like powerpc the decrementer (the
    built-in timer facility) is set to generate an interrupt immediately.
    
    Architectures that don't have anything like this get to do with a
    callback from the timer tick. These architectures can call
    irq_work_run() at the tail of any IRQ handlers that might enqueue such
    work (like the perf IRQ handler) to avoid undue latencies in
    processing the work.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Kyle McMartin <kyle@mcmartin.ca>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    [ various fixes ]
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    LKML-Reference: <1287036094.7768.291.camel@yhuang-dev>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 1ff6662f7faf..9b287fdd8ea3 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -129,7 +129,7 @@ struct paca_struct {
 	u8 soft_enabled;		/* irq soft-enable flag */
 	u8 hard_enabled;		/* set if irqs are enabled in MSR */
 	u8 io_sync;			/* writel() needs spin_unlock sync */
-	u8 perf_event_pending;		/* PM interrupt while soft-disabled */
+	u8 irq_work_pending;		/* IRQ_WORK interrupt while soft-disable */
 
 	/* Stuff for accurate time accounting */
 	u64 user_time;			/* accumulated usermode TB ticks */

commit cf9efce0ce3136fa076f53e53154e98455229514
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Aug 26 19:56:43 2010 +0000

    powerpc: Account time using timebase rather than PURR
    
    Currently, when CONFIG_VIRT_CPU_ACCOUNTING is enabled, we use the
    PURR register for measuring the user and system time used by
    processes, as well as other related times such as hardirq and
    softirq times.  This turns out to be quite confusing for users
    because it means that a program will often be measured as taking
    less time when run on a multi-threaded processor (SMT2 or SMT4 mode)
    than it does when run on a single-threaded processor (ST mode), even
    though the program takes longer to finish.  The discrepancy is
    accounted for as stolen time, which is also confusing, particularly
    when there are no other partitions running.
    
    This changes the accounting to use the timebase instead, meaning that
    the reported user and system times are the actual number of real-time
    seconds that the program was executing on the processor thread,
    regardless of which SMT mode the processor is in.  Thus a program will
    generally show greater user and system times when run on a
    multi-threaded processor than on a single-threaded processor.
    
    On pSeries systems on POWER5 or later processors, we measure the
    stolen time (time when this partition wasn't running) using the
    hypervisor dispatch trace log.  We check for new entries in the
    log on every entry from user mode and on every transition from
    kernel process context to soft or hard IRQ context (i.e. when
    account_system_vtime() gets called).  So that we can correctly
    distinguish time stolen from user time and time stolen from system
    time, without having to check the log on every exit to user mode,
    we store separate timestamps for exit to user mode and entry from
    user mode.
    
    On systems that have a SPURR (POWER6 and POWER7), we read the SPURR
    in account_system_vtime() (as before), and then apportion the SPURR
    ticks since the last time we read it between scaled user time and
    scaled system time according to the relative proportions of user
    time and system time over the same interval.  This avoids having to
    read the SPURR on every kernel entry and exit.  On systems that have
    PURR but not SPURR (i.e., POWER5), we do the same using the PURR
    rather than the SPURR.
    
    This disables the DTL user interface in /sys/debug/kernel/powerpc/dtl
    for now since it conflicts with the use of the dispatch trace log
    by the time accounting code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 1ff6662f7faf..6af6c1613409 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -85,6 +85,8 @@ struct paca_struct {
 	u8 kexec_state;		/* set when kexec down has irqs off */
 #ifdef CONFIG_PPC_STD_MMU_64
 	struct slb_shadow *slb_shadow_ptr;
+	struct dtl_entry *dispatch_log;
+	struct dtl_entry *dispatch_log_end;
 
 	/*
 	 * Now, starting in cacheline 2, the exception save areas
@@ -134,8 +136,14 @@ struct paca_struct {
 	/* Stuff for accurate time accounting */
 	u64 user_time;			/* accumulated usermode TB ticks */
 	u64 system_time;		/* accumulated system TB ticks */
-	u64 startpurr;			/* PURR/TB value snapshot */
+	u64 user_time_scaled;		/* accumulated usermode SPURR ticks */
+	u64 starttime;			/* TB value snapshot */
+	u64 starttime_user;		/* TB value on exit to usermode */
 	u64 startspurr;			/* SPURR value snapshot */
+	u64 utime_sspurr;		/* ->user_time when ->startspurr set */
+	u64 stolen_time;		/* TB ticks taken by hypervisor */
+	u64 dtl_ridx;			/* read index in dispatch log */
+	struct dtl_entry *dtl_curr;	/* pointer corresponding to dtl_ridx */
 
 #ifdef CONFIG_KVM_BOOK3S_HANDLER
 	/* We use this to store guest state in */

commit fc53b4202e61c7e9008c241933ae282aab8a6082
Author: Matt Evans <matt@ozlabs.org>
Date:   Wed Jul 7 21:55:37 2010 +0000

    powerpc/kexec: Switch to a static PACA on the way out
    
    With dynamic PACAs, the kexecing CPU's PACA won't lie within the kernel
    static data and there is a chance that something may stomp it when preparing
    to kexec.  This patch switches this final CPU to a static PACA just before
    we pull the switch.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 8ce7963ad41d..1ff6662f7faf 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -146,7 +146,7 @@ struct paca_struct {
 extern struct paca_struct *paca;
 extern __initdata struct paca_struct boot_paca;
 extern void initialise_paca(struct paca_struct *new_paca, int cpu);
-
+extern void setup_paca(struct paca_struct *new_paca);
 extern void allocate_pacas(void);
 extern void free_unused_pacas(void);
 

commit 98edb6ca4174f17a64890a02f44c211c8b44fb3c
Merge: a8251096b427 8fbf065d6256
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 21 17:16:21 2010 -0700

    Merge branch 'kvm-updates/2.6.35' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    * 'kvm-updates/2.6.35' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (269 commits)
      KVM: x86: Add missing locking to arch specific vcpu ioctls
      KVM: PPC: Add missing vcpu_load()/vcpu_put() in vcpu ioctls
      KVM: MMU: Segregate shadow pages with different cr0.wp
      KVM: x86: Check LMA bit before set_efer
      KVM: Don't allow lmsw to clear cr0.pe
      KVM: Add cpuid.txt file
      KVM: x86: Tell the guest we'll warn it about tsc stability
      x86, paravirt: don't compute pvclock adjustments if we trust the tsc
      x86: KVM guest: Try using new kvm clock msrs
      KVM: x86: export paravirtual cpuid flags in KVM_GET_SUPPORTED_CPUID
      KVM: x86: add new KVMCLOCK cpuid feature
      KVM: x86: change msr numbers for kvmclock
      x86, paravirt: Add a global synchronization point for pvclock
      x86, paravirt: Enable pvclock flags in vcpu_time_info structure
      KVM: x86: Inject #GP with the right rip on efer writes
      KVM: SVM: Don't allow nested guest to VMMCALL into host
      KVM: x86: Fix exception reinjection forced to true
      KVM: Fix wallclock version writing race
      KVM: MMU: Don't read pdptrs with mmu spinlock held in mmu_alloc_roots
      KVM: VMX: enable VMXON check with SMX enabled (Intel TXT)
      ...

commit 1fc711f7ffb01089efc58042cfdbac8573d1b59a
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu May 13 19:40:11 2010 +0000

    powerpc/kexec: Fix race in kexec shutdown
    
    In kexec_prepare_cpus, the primary CPU IPIs the secondary CPUs to
    kexec_smp_down().  kexec_smp_down() calls kexec_smp_wait() which sets
    the hw_cpu_id() to -1.  The primary does this while leaving IRQs on
    which means the primary can take a timer interrupt which can lead to
    the IPIing one of the secondary CPUs (say, for a scheduler re-balance)
    but since the secondary CPU now has a hw_cpu_id = -1, we IPI CPU
    -1... Kaboom!
    
    We are hitting this case regularly on POWER7 machines.
    
    There is also a second race, where the primary will tear down the MMU
    mappings before knowing the secondaries have entered real mode.
    
    Also, the secondaries are clearing out any pending IPIs before
    guaranteeing that no more will be received.
    
    This changes kexec_prepare_cpus() so that we turn off IRQs in the
    primary CPU much earlier.  It adds a paca flag to say that the
    secondaries have entered the kexec_smp_down() IPI and turned off IRQs,
    rather than overloading hw_cpu_id with -1.  This new paca flag is
    again used to in indicate when the secondaries has entered real mode.
    
    It also ensures that all CPUs have their IRQs off before we clear out
    any pending IPI requests (in kexec_cpu_down()) to ensure there are no
    trailing IPIs left unacknowledged.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index a011603d4079..971dfa4815f0 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -82,6 +82,7 @@ struct paca_struct {
 	s16 hw_cpu_id;			/* Physical processor number */
 	u8 cpu_start;			/* At startup, processor spins until */
 					/* this becomes non-zero. */
+	u8 kexec_state;		/* set when kexec down has irqs off */
 #ifdef CONFIG_PPC_STD_MMU_64
 	struct slb_shadow *slb_shadow_ptr;
 

commit 0604675fe17f68741730cebe74422605bb79d972
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:44 2010 +0200

    KVM: PPC: Use now shadowed vcpu fields
    
    The shadow vcpu now contains some fields we don't use from the vcpu anymore.
    Access to them happens using inline functions that happily use the shadow
    vcpu fields.
    
    So let's now ifdef them out to booke only and add asm-offsets.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 33347ea4b47a..224eb371ca1d 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -137,14 +137,8 @@ struct paca_struct {
 	u64 startspurr;			/* SPURR value snapshot */
 
 #ifdef CONFIG_KVM_BOOK3S_HANDLER
-	struct  {
-		u64     esid;
-		u64     vsid;
-	} kvm_slb[64];			/* guest SLB */
 	/* We use this to store guest state in */
 	struct kvmppc_book3s_shadow_vcpu shadow_vcpu;
-	u8 kvm_slb_max;			/* highest used guest slb entry */
-	u8 kvm_in_guest;		/* are we inside the guest? */
 #endif
 };
 

commit c14dea04a248a59fe01f1b49ac94615042016558
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:41 2010 +0200

    KVM: PPC: Use KVM_BOOK3S_HANDLER
    
    So far we had a lot of conditional code on CONFIG_KVM_BOOK3S_64_HANDLER.
    As we're moving towards common code between 32 and 64 bits, most of
    these ifdefs can be moved to a more generic term define, called
    CONFIG_KVM_BOOK3S_HANDLER.
    
    This patch adds the new generic config option and moves ifdefs over.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index dc3ccdf81996..33347ea4b47a 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -136,7 +136,7 @@ struct paca_struct {
 	u64 startpurr;			/* PURR/TB value snapshot */
 	u64 startspurr;			/* SPURR value snapshot */
 
-#ifdef CONFIG_KVM_BOOK3S_64_HANDLER
+#ifdef CONFIG_KVM_BOOK3S_HANDLER
 	struct  {
 		u64     esid;
 		u64     vsid;

commit 2191d657c9eaa4c444c33e014199ed9de1ac339d
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:32 2010 +0200

    KVM: PPC: Name generic 64-bit code generic
    
    We have quite some code that can be used by Book3S_32 and Book3S_64 alike,
    so let's call it "Book3S" instead of "Book3S_64", so we can later on
    use it from the 32 bit port too.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index a011603d4079..dc3ccdf81996 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -23,7 +23,7 @@
 #include <asm/page.h>
 #include <asm/exception-64e.h>
 #ifdef CONFIG_KVM_BOOK3S_64_HANDLER
-#include <asm/kvm_book3s_64_asm.h>
+#include <asm/kvm_book3s_asm.h>
 #endif
 
 register struct paca_struct *local_paca asm("r13");

commit 1426d5a3bd07589534286375998c0c8c6fdc5260
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Jan 28 13:23:22 2010 +0000

    powerpc: Dynamically allocate pacas
    
    On 64-bit kernels we currently have a 512 byte struct paca_struct for
    each cpu (usually just called "the paca"). Currently they are statically
    allocated, which means a kernel built for a large number of cpus will
    waste a lot of space if it's booted on a machine with few cpus.
    
    We can avoid that by only allocating the number of pacas we need at
    boot. However this is complicated by the fact that we need to access
    the paca before we know how many cpus there are in the system.
    
    The solution is to dynamically allocate enough space for NR_CPUS pacas,
    but then later in boot when we know how many cpus we have, we free any
    unused pacas.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index d8a693109c82..a011603d4079 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -14,6 +14,9 @@
 #define _ASM_POWERPC_PACA_H
 #ifdef __KERNEL__
 
+#ifdef CONFIG_PPC64
+
+#include <linux/init.h>
 #include <asm/types.h>
 #include <asm/lppaca.h>
 #include <asm/mmu.h>
@@ -145,8 +148,19 @@ struct paca_struct {
 #endif
 };
 
-extern struct paca_struct paca[];
-extern void initialise_pacas(void);
+extern struct paca_struct *paca;
+extern __initdata struct paca_struct boot_paca;
+extern void initialise_paca(struct paca_struct *new_paca, int cpu);
+
+extern void allocate_pacas(void);
+extern void free_unused_pacas(void);
+
+#else /* CONFIG_PPC64 */
+
+static inline void allocate_pacas(void) { };
+static inline void free_unused_pacas(void) { };
+
+#endif /* CONFIG_PPC64 */
 
 #endif /* __KERNEL__ */
 #endif /* _ASM_POWERPC_PACA_H */

commit 7e57cba06074da84d7c24d8c3f44040d2d8c88ac
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 8 02:58:03 2010 +0100

    KVM: PPC: Use PACA backed shadow vcpu
    
    We're being horribly racy right now. All the entry and exit code hijacks
    random fields from the PACA that could easily be used by different code in
    case we get interrupted, for example by a #MC or even page fault.
    
    After discussing this with Ben, we figured it's best to reserve some more
    space in the PACA and just shove off some vcpu state to there.
    
    That way we can drastically improve the readability of the code, make it
    less racy and less complex.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 5e9b4ef71415..d8a693109c82 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -19,6 +19,9 @@
 #include <asm/mmu.h>
 #include <asm/page.h>
 #include <asm/exception-64e.h>
+#ifdef CONFIG_KVM_BOOK3S_64_HANDLER
+#include <asm/kvm_book3s_64_asm.h>
+#endif
 
 register struct paca_struct *local_paca asm("r13");
 
@@ -135,6 +138,8 @@ struct paca_struct {
 		u64     esid;
 		u64     vsid;
 	} kvm_slb[64];			/* guest SLB */
+	/* We use this to store guest state in */
+	struct kvmppc_book3s_shadow_vcpu shadow_vcpu;
 	u8 kvm_slb_max;			/* highest used guest slb entry */
 	u8 kvm_in_guest;		/* are we inside the guest? */
 #endif

commit 4b7ae55df3621dd9eef56a6fde953ec9c73ac596
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Oct 30 05:47:22 2009 +0000

    Add fields to PACA
    
    For KVM we need to store some information in the PACA, so we
    need to extend it.
    
    This patch adds KVM SLB shadow related entries to the PACA and
    a field that indicates if we're inside a guest.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 7d8514ceceae..5e9b4ef71415 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -129,6 +129,15 @@ struct paca_struct {
 	u64 system_time;		/* accumulated system TB ticks */
 	u64 startpurr;			/* PURR/TB value snapshot */
 	u64 startspurr;			/* SPURR value snapshot */
+
+#ifdef CONFIG_KVM_BOOK3S_64_HANDLER
+	struct  {
+		u64     esid;
+		u64     vsid;
+	} kvm_slb[64];			/* guest SLB */
+	u8 kvm_slb_max;			/* highest used guest slb entry */
+	u8 kvm_in_guest;		/* are we inside the guest? */
+#endif
 };
 
 extern struct paca_struct paca[];

commit 57c0c15b5244320065374ad2c54f4fbec77a6428
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:20:38 2009 +0200

    perf: Tidy up after the big rename
    
     - provide compatibility Kconfig entry for existing PERF_COUNTERS .config's
    
     - provide courtesy copy of old perf_counter.h, for user-space projects
    
     - small indentation fixups
    
     - fix up MAINTAINERS
    
     - fix small x86 printout fallout
    
     - fix up small PowerPC comment fallout (use 'counter' as in register)
    
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 154f405b642f..7d8514ceceae 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -122,7 +122,7 @@ struct paca_struct {
 	u8 soft_enabled;		/* irq soft-enable flag */
 	u8 hard_enabled;		/* set if irqs are enabled in MSR */
 	u8 io_sync;			/* writel() needs spin_unlock sync */
-	u8 perf_event_pending;	/* PM interrupt while soft-disabled */
+	u8 perf_event_pending;		/* PM interrupt while soft-disabled */
 
 	/* Stuff for accurate time accounting */
 	u64 user_time;			/* accumulated usermode TB ticks */

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index b634456ea893..154f405b642f 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -122,7 +122,7 @@ struct paca_struct {
 	u8 soft_enabled;		/* irq soft-enable flag */
 	u8 hard_enabled;		/* set if irqs are enabled in MSR */
 	u8 io_sync;			/* writel() needs spin_unlock sync */
-	u8 perf_counter_pending;	/* PM interrupt while soft-disabled */
+	u8 perf_event_pending;	/* PM interrupt while soft-disabled */
 
 	/* Stuff for accurate time accounting */
 	u64 user_time;			/* accumulated usermode TB ticks */

commit dce6670aaa7efece0558010b48d5ef9d421154be
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jul 23 23:15:42 2009 +0000

    powerpc: Add PACA fields specific to 64-bit Book3E processors
    
    This adds various fields in the PACA that are for use specifically
    by Book3E processors, such as exception save areas, current pgd
    pointer, special exceptions kernel stacks etc...
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index c8a3cbfe02ff..b634456ea893 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -14,9 +14,11 @@
 #define _ASM_POWERPC_PACA_H
 #ifdef __KERNEL__
 
-#include	<asm/types.h>
-#include	<asm/lppaca.h>
-#include	<asm/mmu.h>
+#include <asm/types.h>
+#include <asm/lppaca.h>
+#include <asm/mmu.h>
+#include <asm/page.h>
+#include <asm/exception-64e.h>
 
 register struct paca_struct *local_paca asm("r13");
 
@@ -91,6 +93,21 @@ struct paca_struct {
 	u16 slb_cache[SLB_CACHE_ENTRIES];
 #endif /* CONFIG_PPC_STD_MMU_64 */
 
+#ifdef CONFIG_PPC_BOOK3E
+	pgd_t *pgd;			/* Current PGD */
+	pgd_t *kernel_pgd;		/* Kernel PGD */
+	u64 exgen[8] __attribute__((aligned(0x80)));
+	u64 extlb[EX_TLB_SIZE*3] __attribute__((aligned(0x80)));
+	u64 exmc[8];		/* used for machine checks */
+	u64 excrit[8];		/* used for crit interrupts */
+	u64 exdbg[8];		/* used for debug interrupts */
+
+	/* Kernel stack pointers for use by special exceptions */
+	void *mc_kstack;
+	void *crit_kstack;
+	void *dbg_kstack;
+#endif /* CONFIG_PPC_BOOK3E */
+
 	mm_context_t context;
 
 	/*

commit bc47ab0241c7c86da4f5e5f82fbca7d45387c18d
Merge: 37f9ef553bed 8ebf975608aa
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Jun 12 16:53:38 2009 +1000

    Merge commit 'origin/master' into next
    
    Manual merge of:
            arch/powerpc/kernel/asm-offsets.c

commit 91c60b5b8209627590b31c07262e40c27d27d272
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jun 2 21:17:41 2009 +0000

    powerpc: Separate PACA fields for server CPUs
    
    This patch has no effect other than re-ordering PACA fields on
    current server CPUs. It however is a pre-requisite for future
    support of BookE 64-bit processors. Various parts of the PACA
    struct are now moved under some ifdef's, either the new
    CONFIG_PPC_BOOK3S or CONFIG_PPC_STD_MMU_64, whatever seems more
    appropriate.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.craashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 082b3aedf145..0ea1985bdb8b 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -43,6 +43,7 @@ struct task_struct;
  * processor.
  */
 struct paca_struct {
+#ifdef CONFIG_PPC_BOOK3S
 	/*
 	 * Because hw_cpu_id, unlike other paca fields, is accessed
 	 * routinely from other CPUs (from the IRQ code), we stick to
@@ -51,7 +52,7 @@ struct paca_struct {
 	 */
 
 	struct lppaca *lppaca_ptr;	/* Pointer to LpPaca for PLIC */
-
+#endif /* CONFIG_PPC_BOOK3S */
 	/*
 	 * MAGIC: the spinlock functions in arch/powerpc/lib/locks.c 
 	 * load lock_token and paca_index with a single lwz
@@ -64,13 +65,16 @@ struct paca_struct {
 	u64 kernel_toc;			/* Kernel TOC address */
 	u64 kernelbase;			/* Base address of kernel */
 	u64 kernel_msr;			/* MSR while running in kernel */
+#ifdef CONFIG_PPC_STD_MMU_64
 	u64 stab_real;			/* Absolute address of segment table */
 	u64 stab_addr;			/* Virtual address of segment table */
+#endif /* CONFIG_PPC_STD_MMU_64 */
 	void *emergency_sp;		/* pointer to emergency stack */
 	u64 data_offset;		/* per cpu data offset */
 	s16 hw_cpu_id;			/* Physical processor number */
 	u8 cpu_start;			/* At startup, processor spins until */
 					/* this becomes non-zero. */
+#ifdef CONFIG_PPC_STD_MMU_64
 	struct slb_shadow *slb_shadow_ptr;
 
 	/*
@@ -81,11 +85,13 @@ struct paca_struct {
 	u64 exmc[10];		/* used for machine checks */
 	u64 exslb[10];		/* used for SLB/segment table misses
  				 * on the linear mapping */
-
-	mm_context_t context;
+	/* SLB related definitions */
 	u16 vmalloc_sllp;
 	u16 slb_cache_ptr;
 	u16 slb_cache[SLB_CACHE_ENTRIES];
+#endif /* CONFIG_PPC_STD_MMU_64 */
+
+	mm_context_t context;
 
 	/*
 	 * then miscellaneous read-write fields

commit 93a6d3ce6962044fe9badf528fed46b455d58292
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Jan 9 16:52:19 2009 +1100

    powerpc: Provide a way to defer perf counter work until interrupts are enabled
    
    Because 64-bit powerpc uses lazy (soft) interrupt disabling, it is
    possible for a performance monitor exception to come in when the
    kernel thinks interrupts are disabled (i.e. when they are
    soft-disabled but hard-enabled).  In such a situation the performance
    monitor exception handler might have some processing to do (such as
    process wakeups) which can't be done in what is effectively an NMI
    handler.
    
    This provides a way to defer that work until interrupts get enabled,
    either in raw_local_irq_restore() or by returning from an interrupt
    handler to code that had interrupts enabled.  We have a per-processor
    flag that indicates that there is work pending to do when interrupts
    subsequently get re-enabled.  This flag is checked in the interrupt
    return path and in raw_local_irq_restore(), and if it is set,
    perf_counter_do_pending() is called to do the pending work.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 082b3aedf145..6ef055723019 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -99,6 +99,7 @@ struct paca_struct {
 	u8 soft_enabled;		/* irq soft-enable flag */
 	u8 hard_enabled;		/* set if irqs are enabled in MSR */
 	u8 io_sync;			/* writel() needs spin_unlock sync */
+	u8 perf_counter_pending;	/* PM interrupt while soft-disabled */
 
 	/* Stuff for accurate time accounting */
 	u64 user_time;			/* accumulated usermode TB ticks */

commit 1f6a93e4c35e75d547b51f56ba8139ab1a91628c
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Aug 30 11:40:24 2008 +1000

    powerpc: Make it possible to move the interrupt handlers away from the kernel
    
    This changes the way that the exception prologs transfer control to
    the handlers in 64-bit kernels with the aim of making it possible to
    have the prologs separate from the main body of the kernel.  Now,
    instead of computing the address of the handler by taking the top
    32 bits of the paca address (to get the 0xc0000000........ part) and
    ORing in something in the bottom 16 bits, we get the base address of
    the kernel by doing a load from the paca and add an offset.
    
    This also replaces an mfmsr and an ori to compute the MSR value for
    the handler with a load from the paca.  That makes it unnecessary to
    have a separate version of EXCEPTION_PROLOG_PSERIES that forces 64-bit
    mode.
    
    We can no longer use a direct branches in the exception prolog code,
    which means that the SLB miss handlers can't branch directly to
    .slb_miss_realmode any more.  Instead we have to compute the address
    and do an indirect branch.  This is conditional on CONFIG_RELOCATABLE;
    for non-relocatable kernels we use a direct branch as before.  (A later
    change will allow CONFIG_RELOCATABLE to be set on 64-bit powerpc.)
    
    Since the secondary CPUs on pSeries start execution in the first 0x100
    bytes of real memory and then have to get to wherever the kernel is,
    we can't use a direct branch to get there.  Instead this changes
    __secondary_hold_spinloop from a flag to a function pointer.  When it
    is set to a non-NULL value, the secondary CPUs jump to the function
    pointed to by that value.
    
    Finally this eliminates one code difference between 32-bit and 64-bit
    by making __secondary_hold be the text address of the secondary CPU
    spinloop rather than a function descriptor for it.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
index 6493a395508b..082b3aedf145 100644
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@ -62,6 +62,8 @@ struct paca_struct {
 	u16 paca_index;			/* Logical processor number */
 
 	u64 kernel_toc;			/* Kernel TOC address */
+	u64 kernelbase;			/* Base address of kernel */
+	u64 kernel_msr;			/* MSR while running in kernel */
 	u64 stab_real;			/* Absolute address of segment table */
 	u64 stab_addr;			/* Virtual address of segment table */
 	void *emergency_sp;		/* pointer to emergency stack */

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h
new file mode 100644
index 000000000000..6493a395508b
--- /dev/null
+++ b/arch/powerpc/include/asm/paca.h
@@ -0,0 +1,112 @@
+/*
+ * This control block defines the PACA which defines the processor
+ * specific data for each logical processor on the system.
+ * There are some pointers defined that are utilized by PLIC.
+ *
+ * C 2001 PPC 64 Team, IBM Corp
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+#ifndef _ASM_POWERPC_PACA_H
+#define _ASM_POWERPC_PACA_H
+#ifdef __KERNEL__
+
+#include	<asm/types.h>
+#include	<asm/lppaca.h>
+#include	<asm/mmu.h>
+
+register struct paca_struct *local_paca asm("r13");
+
+#if defined(CONFIG_DEBUG_PREEMPT) && defined(CONFIG_SMP)
+extern unsigned int debug_smp_processor_id(void); /* from linux/smp.h */
+/*
+ * Add standard checks that preemption cannot occur when using get_paca():
+ * otherwise the paca_struct it points to may be the wrong one just after.
+ */
+#define get_paca()	((void) debug_smp_processor_id(), local_paca)
+#else
+#define get_paca()	local_paca
+#endif
+
+#define get_lppaca()	(get_paca()->lppaca_ptr)
+#define get_slb_shadow()	(get_paca()->slb_shadow_ptr)
+
+struct task_struct;
+
+/*
+ * Defines the layout of the paca.
+ *
+ * This structure is not directly accessed by firmware or the service
+ * processor.
+ */
+struct paca_struct {
+	/*
+	 * Because hw_cpu_id, unlike other paca fields, is accessed
+	 * routinely from other CPUs (from the IRQ code), we stick to
+	 * read-only (after boot) fields in the first cacheline to
+	 * avoid cacheline bouncing.
+	 */
+
+	struct lppaca *lppaca_ptr;	/* Pointer to LpPaca for PLIC */
+
+	/*
+	 * MAGIC: the spinlock functions in arch/powerpc/lib/locks.c 
+	 * load lock_token and paca_index with a single lwz
+	 * instruction.  They must travel together and be properly
+	 * aligned.
+	 */
+	u16 lock_token;			/* Constant 0x8000, used in locks */
+	u16 paca_index;			/* Logical processor number */
+
+	u64 kernel_toc;			/* Kernel TOC address */
+	u64 stab_real;			/* Absolute address of segment table */
+	u64 stab_addr;			/* Virtual address of segment table */
+	void *emergency_sp;		/* pointer to emergency stack */
+	u64 data_offset;		/* per cpu data offset */
+	s16 hw_cpu_id;			/* Physical processor number */
+	u8 cpu_start;			/* At startup, processor spins until */
+					/* this becomes non-zero. */
+	struct slb_shadow *slb_shadow_ptr;
+
+	/*
+	 * Now, starting in cacheline 2, the exception save areas
+	 */
+	/* used for most interrupts/exceptions */
+	u64 exgen[10] __attribute__((aligned(0x80)));
+	u64 exmc[10];		/* used for machine checks */
+	u64 exslb[10];		/* used for SLB/segment table misses
+ 				 * on the linear mapping */
+
+	mm_context_t context;
+	u16 vmalloc_sllp;
+	u16 slb_cache_ptr;
+	u16 slb_cache[SLB_CACHE_ENTRIES];
+
+	/*
+	 * then miscellaneous read-write fields
+	 */
+	struct task_struct *__current;	/* Pointer to current */
+	u64 kstack;			/* Saved Kernel stack addr */
+	u64 stab_rr;			/* stab/slb round-robin counter */
+	u64 saved_r1;			/* r1 save for RTAS calls */
+	u64 saved_msr;			/* MSR saved here by enter_rtas */
+	u16 trap_save;			/* Used when bad stack is encountered */
+	u8 soft_enabled;		/* irq soft-enable flag */
+	u8 hard_enabled;		/* set if irqs are enabled in MSR */
+	u8 io_sync;			/* writel() needs spin_unlock sync */
+
+	/* Stuff for accurate time accounting */
+	u64 user_time;			/* accumulated usermode TB ticks */
+	u64 system_time;		/* accumulated system TB ticks */
+	u64 startpurr;			/* PURR/TB value snapshot */
+	u64 startspurr;			/* SPURR value snapshot */
+};
+
+extern struct paca_struct paca[];
+extern void initialise_pacas(void);
+
+#endif /* __KERNEL__ */
+#endif /* _ASM_POWERPC_PACA_H */
