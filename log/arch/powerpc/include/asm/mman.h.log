commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
index e3f1b5ba5d5c..d610c2e07b28 100644
--- a/arch/powerpc/include/asm/mman.h
+++ b/arch/powerpc/include/asm/mman.h
@@ -1,8 +1,5 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 #ifndef _ASM_POWERPC_MMAN_H
 #define _ASM_POWERPC_MMAN_H

commit 9035cf9a97e429e6b5291841da81c433879f5658
Author: Khalid Aziz <khalid.aziz@oracle.com>
Date:   Wed Feb 21 10:15:49 2018 -0700

    mm: Add address parameter to arch_validate_prot()
    
    A protection flag may not be valid across entire address space and
    hence arch_validate_prot() might need the address a protection bit is
    being set on to ensure it is a valid protection flag. For example, sparc
    processors support memory corruption detection (as part of ADI feature)
    flag on memory addresses mapped on to physical RAM but not on PFN mapped
    pages or addresses mapped on to devices. This patch adds address to the
    parameters being passed to arch_validate_prot() so protection bits can
    be validated in the relevant context.
    
    Signed-off-by: Khalid Aziz <khalid.aziz@oracle.com>
    Cc: Khalid Aziz <khalid@gonehiking.org>
    Reviewed-by: Anthony Yznaga <anthony.yznaga@oracle.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
index 07e3f54de9e3..e3f1b5ba5d5c 100644
--- a/arch/powerpc/include/asm/mman.h
+++ b/arch/powerpc/include/asm/mman.h
@@ -43,7 +43,7 @@ static inline pgprot_t arch_vm_get_page_prot(unsigned long vm_flags)
 }
 #define arch_vm_get_page_prot(vm_flags) arch_vm_get_page_prot(vm_flags)
 
-static inline bool arch_validate_prot(unsigned long prot)
+static inline bool arch_validate_prot(unsigned long prot, unsigned long addr)
 {
 	if (prot & ~(PROT_READ | PROT_WRITE | PROT_EXEC | PROT_SEM | PROT_SAO))
 		return false;
@@ -51,7 +51,7 @@ static inline bool arch_validate_prot(unsigned long prot)
 		return false;
 	return true;
 }
-#define arch_validate_prot(prot) arch_validate_prot(prot)
+#define arch_validate_prot arch_validate_prot
 
 #endif /* CONFIG_PPC64 */
 #endif	/* _ASM_POWERPC_MMAN_H */

commit eb95d016ce3d3404e061c7c85d4362094dc34b3f
Author: Ram Pai <linuxram@us.ibm.com>
Date:   Thu Jan 18 17:50:35 2018 -0800

    powerpc: map vma key-protection bits to pte key bits.
    
    Map  the  key  protection  bits of the vma to the pkey bits in
    the PTE.
    
    The PTE  bits used  for pkey  are  3,4,5,6  and 57. The  first
    four bits are the same four bits that were freed up  initially
    in this patch series. remember? :-) Without those four bits
    this patch wouldn't be possible.
    
    BUT, on 4k kernel, bit 3, and 4 could not be freed up. remember?
    Hence we have to be satisfied with 5, 6 and 7.
    
    Signed-off-by: Ram Pai <linuxram@us.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
index 29994788b9f3..07e3f54de9e3 100644
--- a/arch/powerpc/include/asm/mman.h
+++ b/arch/powerpc/include/asm/mman.h
@@ -33,7 +33,13 @@ static inline unsigned long arch_calc_vm_prot_bits(unsigned long prot,
 
 static inline pgprot_t arch_vm_get_page_prot(unsigned long vm_flags)
 {
+#ifdef CONFIG_PPC_MEM_KEYS
+	return (vm_flags & VM_SAO) ?
+		__pgprot(_PAGE_SAO | vmflag_to_pte_pkey_bits(vm_flags)) :
+		__pgprot(0 | vmflag_to_pte_pkey_bits(vm_flags));
+#else
 	return (vm_flags & VM_SAO) ? __pgprot(_PAGE_SAO) : __pgprot(0);
+#endif
 }
 #define arch_vm_get_page_prot(vm_flags) arch_vm_get_page_prot(vm_flags)
 

commit 013a91b39c2d5158cdc5529803f245bbb0526c7c
Author: Ram Pai <linuxram@us.ibm.com>
Date:   Thu Jan 18 17:50:33 2018 -0800

    powerpc: ability to associate pkey to a vma
    
    arch-independent code expects the arch to  map
    a  pkey  into the vma's protection bit setting.
    The patch provides that ability.
    
    Signed-off-by: Ram Pai <linuxram@us.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
index 30922f699341..29994788b9f3 100644
--- a/arch/powerpc/include/asm/mman.h
+++ b/arch/powerpc/include/asm/mman.h
@@ -13,6 +13,7 @@
 
 #include <asm/cputable.h>
 #include <linux/mm.h>
+#include <linux/pkeys.h>
 #include <asm/cpu_has_feature.h>
 
 /*
@@ -22,7 +23,11 @@
 static inline unsigned long arch_calc_vm_prot_bits(unsigned long prot,
 		unsigned long pkey)
 {
-	return (prot & PROT_SAO) ? VM_SAO : 0;
+#ifdef CONFIG_PPC_MEM_KEYS
+	return (((prot & PROT_SAO) ? VM_SAO : 0) | pkey_to_vmflag_bits(pkey));
+#else
+	return ((prot & PROT_SAO) ? VM_SAO : 0);
+#endif
 }
 #define arch_calc_vm_prot_bits(prot, pkey) arch_calc_vm_prot_bits(prot, pkey)
 

commit 2cfd716d2777489db54a237f466a1c42700879c6
Merge: 755b20f49220 eea8148c69f3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 5 09:00:54 2016 -0400

    Merge tag 'powerpc-4.8-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull more powerpc updates from Michael Ellerman:
     "These were delayed for various reasons, so I let them sit in next a
      bit longer, rather than including them in my first pull request.
    
      Fixes:
       - Fix early access to cpu_spec relocation from Benjamin Herrenschmidt
       - Fix incorrect event codes in power9-event-list from Madhavan Srinivasan
       - Move register_process_table() out of ppc_md from Michael Ellerman
    
      Use jump_label use for [cpu|mmu]_has_feature():
       - Add mmu_early_init_devtree() from Michael Ellerman
       - Move disable_radix handling into mmu_early_init_devtree() from Michael Ellerman
       - Do hash device tree scanning earlier from Michael Ellerman
       - Do radix device tree scanning earlier from Michael Ellerman
       - Do feature patching before MMU init from Michael Ellerman
       - Check features don't change after patching from Michael Ellerman
       - Make MMU_FTR_RADIX a MMU family feature from Aneesh Kumar K.V
       - Convert mmu_has_feature() to returning bool from Michael Ellerman
       - Convert cpu_has_feature() to returning bool from Michael Ellerman
       - Define radix_enabled() in one place & use static inline from Michael Ellerman
       - Add early_[cpu|mmu]_has_feature() from Michael Ellerman
       - Convert early cpu/mmu feature check to use the new helpers from Aneesh Kumar K.V
       - jump_label: Make it possible for arches to invoke jump_label_init() earlier from Kevin Hao
       - Call jump_label_init() in apply_feature_fixups() from Aneesh Kumar K.V
       - Remove mfvtb() from Kevin Hao
       - Move cpu_has_feature() to a separate file from Kevin Hao
       - Add kconfig option to use jump labels for cpu/mmu_has_feature() from Michael Ellerman
       - Add option to use jump label for cpu_has_feature() from Kevin Hao
       - Add option to use jump label for mmu_has_feature() from Kevin Hao
       - Catch usage of cpu/mmu_has_feature() before jump label init from Aneesh Kumar K.V
       - Annotate jump label assembly from Michael Ellerman
    
      TLB flush enhancements from Aneesh Kumar K.V:
       - radix: Implement tlb mmu gather flush efficiently
       - Add helper for finding SLBE LLP encoding
       - Use hugetlb flush functions
       - Drop multiple definition of mm_is_core_local
       - radix: Add tlb flush of THP ptes
       - radix: Rename function and drop unused arg
       - radix/hugetlb: Add helper for finding page size
       - hugetlb: Add flush_hugetlb_tlb_range
       - remove flush_tlb_page_nohash
    
      Add new ptrace regsets from Anshuman Khandual and Simon Guo:
       - elf: Add powerpc specific core note sections
       - Add the function flush_tmregs_to_thread
       - Enable in transaction NT_PRFPREG ptrace requests
       - Enable in transaction NT_PPC_VMX ptrace requests
       - Enable in transaction NT_PPC_VSX ptrace requests
       - Adapt gpr32_get, gpr32_set functions for transaction
       - Enable support for NT_PPC_CGPR
       - Enable support for NT_PPC_CFPR
       - Enable support for NT_PPC_CVMX
       - Enable support for NT_PPC_CVSX
       - Enable support for TM SPR state
       - Enable NT_PPC_TM_CTAR, NT_PPC_TM_CPPR, NT_PPC_TM_CDSCR
       - Enable support for NT_PPPC_TAR, NT_PPC_PPR, NT_PPC_DSCR
       - Enable support for EBB registers
       - Enable support for Performance Monitor registers"
    
    * tag 'powerpc-4.8-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (48 commits)
      powerpc/mm: Move register_process_table() out of ppc_md
      powerpc/perf: Fix incorrect event codes in power9-event-list
      powerpc/32: Fix early access to cpu_spec relocation
      powerpc/ptrace: Enable support for Performance Monitor registers
      powerpc/ptrace: Enable support for EBB registers
      powerpc/ptrace: Enable support for NT_PPPC_TAR, NT_PPC_PPR, NT_PPC_DSCR
      powerpc/ptrace: Enable NT_PPC_TM_CTAR, NT_PPC_TM_CPPR, NT_PPC_TM_CDSCR
      powerpc/ptrace: Enable support for TM SPR state
      powerpc/ptrace: Enable support for NT_PPC_CVSX
      powerpc/ptrace: Enable support for NT_PPC_CVMX
      powerpc/ptrace: Enable support for NT_PPC_CFPR
      powerpc/ptrace: Enable support for NT_PPC_CGPR
      powerpc/ptrace: Adapt gpr32_get, gpr32_set functions for transaction
      powerpc/ptrace: Enable in transaction NT_PPC_VSX ptrace requests
      powerpc/ptrace: Enable in transaction NT_PPC_VMX ptrace requests
      powerpc/ptrace: Enable in transaction NT_PRFPREG ptrace requests
      powerpc/process: Add the function flush_tmregs_to_thread
      elf: Add powerpc specific core note sections
      powerpc/mm: remove flush_tlb_page_nohash
      powerpc/mm/hugetlb: Add flush_hugetlb_tlb_range
      ...

commit 949bed2f5764435715e3d6dd3ab6dd4dbd890a71
Author: Chen Gang <chengang@emindsoft.com.cn>
Date:   Tue Aug 2 14:03:42 2016 -0700

    include: mman: use bool instead of int for the return value of arch_validate_prot
    
    For pure bool function's return value, bool is a little better more or
    less than int.
    
    Link: http://lkml.kernel.org/r/1469331815-2026-1-git-send-email-chengang@emindsoft.com.cn
    Signed-off-by: Chen Gang <gang.chen.5i5j@gmail.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
index 2563c435a4b1..fc420cedecae 100644
--- a/arch/powerpc/include/asm/mman.h
+++ b/arch/powerpc/include/asm/mman.h
@@ -31,13 +31,13 @@ static inline pgprot_t arch_vm_get_page_prot(unsigned long vm_flags)
 }
 #define arch_vm_get_page_prot(vm_flags) arch_vm_get_page_prot(vm_flags)
 
-static inline int arch_validate_prot(unsigned long prot)
+static inline bool arch_validate_prot(unsigned long prot)
 {
 	if (prot & ~(PROT_READ | PROT_WRITE | PROT_EXEC | PROT_SEM | PROT_SAO))
-		return 0;
+		return false;
 	if ((prot & PROT_SAO) && !cpu_has_feature(CPU_FTR_SAO))
-		return 0;
-	return 1;
+		return false;
+	return true;
 }
 #define arch_validate_prot(prot) arch_validate_prot(prot)
 

commit b92a226e528423b8d249dd09bb450d53361fbfcb
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sat Jul 23 14:42:40 2016 +0530

    powerpc: Move cpu_has_feature() to a separate file
    
    We plan to use jump label for cpu_has_feature(). In order to implement
    this we need to include the linux/jump_label.h in asm/cputable.h.
    
    Unfortunately if we do that it leads to an include loop. The root of the
    problem seems to be that reg.h needs cputable.h (for CPU_FTRs), and then
    cputable.h via jump_label.h eventually pulls in hw_irq.h which needs
    reg.h (for MSR_EE).
    
    So move cpu_has_feature() to a separate file on its own.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [mpe: Rename to cpu_has_feature.h and flesh out change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
index 2563c435a4b1..ef2d9ac1bc52 100644
--- a/arch/powerpc/include/asm/mman.h
+++ b/arch/powerpc/include/asm/mman.h
@@ -13,6 +13,7 @@
 
 #include <asm/cputable.h>
 #include <linux/mm.h>
+#include <asm/cpu_has_feature.h>
 
 /*
  * This file is included by linux/mman.h, so we can't use cacl_vm_prot_bits()

commit e6bfb70959a0ca6ddedb29e779a293c6f71ed0e7
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Feb 12 13:02:31 2016 -0800

    mm/core, arch, powerpc: Pass a protection key in to calc_vm_flag_bits()
    
    This plumbs a protection key through calc_vm_flag_bits().  We
    could have done this in calc_vm_prot_bits(), but I did not feel
    super strongly which way to go.  It was pretty arbitrary which
    one to use.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arve Hjønnevåg <arve@android.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Chen Gang <gang.chen.5i5j@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Geliang Tang <geliangtang@163.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Leon Romanovsky <leon@leon.nu>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Maxime Coquelin <mcoquelin.stm32@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Riley Andrews <riandrews@android.com>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: devel@driverdev.osuosl.org
    Cc: linux-api@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/20160212210231.E6F1F0D6@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
index 8565c254151a..2563c435a4b1 100644
--- a/arch/powerpc/include/asm/mman.h
+++ b/arch/powerpc/include/asm/mman.h
@@ -18,11 +18,12 @@
  * This file is included by linux/mman.h, so we can't use cacl_vm_prot_bits()
  * here.  How important is the optimization?
  */
-static inline unsigned long arch_calc_vm_prot_bits(unsigned long prot)
+static inline unsigned long arch_calc_vm_prot_bits(unsigned long prot,
+		unsigned long pkey)
 {
 	return (prot & PROT_SAO) ? VM_SAO : 0;
 }
-#define arch_calc_vm_prot_bits(prot) arch_calc_vm_prot_bits(prot)
+#define arch_calc_vm_prot_bits(prot, pkey) arch_calc_vm_prot_bits(prot, pkey)
 
 static inline pgprot_t arch_vm_get_page_prot(unsigned long vm_flags)
 {

commit c3617f72036c909e1f6086b5b9e364e0ef90a6da
Author: David Howells <dhowells@redhat.com>
Date:   Tue Oct 9 09:47:26 2012 +0100

    UAPI: (Scripted) Disintegrate arch/powerpc/include/asm
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
index d4a7f645c5db..8565c254151a 100644
--- a/arch/powerpc/include/asm/mman.h
+++ b/arch/powerpc/include/asm/mman.h
@@ -1,34 +1,14 @@
-#ifndef _ASM_POWERPC_MMAN_H
-#define _ASM_POWERPC_MMAN_H
-
-#include <asm-generic/mman-common.h>
-
 /*
  * This program is free software; you can redistribute it and/or
  * modify it under the terms of the GNU General Public License
  * as published by the Free Software Foundation; either version
  * 2 of the License, or (at your option) any later version.
  */
+#ifndef _ASM_POWERPC_MMAN_H
+#define _ASM_POWERPC_MMAN_H
 
-#define PROT_SAO	0x10		/* Strong Access Ordering */
-
-#define MAP_RENAME      MAP_ANONYMOUS   /* In SunOS terminology */
-#define MAP_NORESERVE   0x40            /* don't reserve swap pages */
-#define MAP_LOCKED	0x80
-
-#define MAP_GROWSDOWN	0x0100		/* stack-like segment */
-#define MAP_DENYWRITE	0x0800		/* ETXTBSY */
-#define MAP_EXECUTABLE	0x1000		/* mark it as an executable */
-
-#define MCL_CURRENT     0x2000          /* lock all currently mapped pages */
-#define MCL_FUTURE      0x4000          /* lock all additions to address space */
-
-#define MAP_POPULATE	0x8000		/* populate (prefault) pagetables */
-#define MAP_NONBLOCK	0x10000		/* do not block on IO */
-#define MAP_STACK	0x20000		/* give out an address that is best suited for process/thread stacks */
-#define MAP_HUGETLB	0x40000		/* create a huge page mapping */
+#include <uapi/asm/mman.h>
 
-#ifdef __KERNEL__
 #ifdef CONFIG_PPC64
 
 #include <asm/cputable.h>
@@ -61,5 +41,4 @@ static inline int arch_validate_prot(unsigned long prot)
 #define arch_validate_prot(prot) arch_validate_prot(prot)
 
 #endif /* CONFIG_PPC64 */
-#endif /* __KERNEL__ */
 #endif	/* _ASM_POWERPC_MMAN_H */

commit 90f72aa58bbf076b68e289fbd71eb829bc505923
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Sep 21 17:03:45 2009 -0700

    mm: add MAP_HUGETLB for mmaping pseudo-anonymous huge page regions
    
    Add a flag for mmap that will be used to request a huge page region that
    will look like anonymous memory to user space.  This is accomplished by
    using a file on the internal vfsmount.  MAP_HUGETLB is a modifier of
    MAP_ANONYMOUS and so must be specified with it.  The region will behave
    the same as a MAP_ANONYMOUS region using small pages.
    
    The patch also adds the MAP_STACK flag, which was previously defined only
    on some architectures but not on others.  Since MAP_STACK is meant to be a
    hint only, architectures can define it without assigning a specific
    meaning to it.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Eric B Munson <ebmunson@us.ibm.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: David Rientjes <rientjes@google.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
index 7b1c49811a24..d4a7f645c5db 100644
--- a/arch/powerpc/include/asm/mman.h
+++ b/arch/powerpc/include/asm/mman.h
@@ -25,6 +25,8 @@
 
 #define MAP_POPULATE	0x8000		/* populate (prefault) pagetables */
 #define MAP_NONBLOCK	0x10000		/* do not block on IO */
+#define MAP_STACK	0x20000		/* give out an address that is best suited for process/thread stacks */
+#define MAP_HUGETLB	0x40000		/* create a huge page mapping */
 
 #ifdef __KERNEL__
 #ifdef CONFIG_PPC64

commit 63b852a6b67d0820d388b0ecd0da83ccb4048b8d
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed May 13 22:56:24 2009 +0000

    asm-generic: rename termios.h, signal.h and mman.h
    
    The existing asm-generic versions are incomplete and included
    by some architectures. New architectures should be able
    to use a generic version, so rename the existing files and
    change all users, which lets us add the new files.
    
    Signed-off-by: Remis Lima Baima <remis.developer@googlemail.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
index e7b99bac9f48..7b1c49811a24 100644
--- a/arch/powerpc/include/asm/mman.h
+++ b/arch/powerpc/include/asm/mman.h
@@ -1,7 +1,7 @@
 #ifndef _ASM_POWERPC_MMAN_H
 #define _ASM_POWERPC_MMAN_H
 
-#include <asm-generic/mman.h>
+#include <asm-generic/mman-common.h>
 
 /*
  * This program is free software; you can redistribute it and/or

commit f5ea64dcbad89875d130596df14c9b25d994a737
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Sun Oct 12 17:54:24 2008 +0000

    powerpc: Get USE_STRICT_MM_TYPECHECKS working again
    
    The typesafe version of the powerpc pagetable handling (with
    USE_STRICT_MM_TYPECHECKS defined) has bitrotted again.  This patch
    makes a bunch of small fixes to get it back to building status.
    
    It's still not enabled by default as gcc still generates worse
    code with it for some reason.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
index 9209f755763e..e7b99bac9f48 100644
--- a/arch/powerpc/include/asm/mman.h
+++ b/arch/powerpc/include/asm/mman.h
@@ -44,7 +44,7 @@ static inline unsigned long arch_calc_vm_prot_bits(unsigned long prot)
 
 static inline pgprot_t arch_vm_get_page_prot(unsigned long vm_flags)
 {
-	return (vm_flags & VM_SAO) ? __pgprot(_PAGE_SAO) : 0;
+	return (vm_flags & VM_SAO) ? __pgprot(_PAGE_SAO) : __pgprot(0);
 }
 #define arch_vm_get_page_prot(vm_flags) arch_vm_get_page_prot(vm_flags)
 

commit b8b572e1015f81b4e748417be2629dfe51ab99f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Aug 1 15:20:30 2008 +1000

    powerpc: Move include files to arch/powerpc/include/asm
    
    from include/asm-powerpc.  This is the result of a
    
    mkdir arch/powerpc/include/asm
    git mv include/asm-powerpc/* arch/powerpc/include/asm
    
    Followed by a few documentation/comment fixups and a couple of places
    where <asm-powepc/...> was being used explicitly.  Of the latter only
    one was outside the arch code and it is a driver only built for powerpc.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/include/asm/mman.h b/arch/powerpc/include/asm/mman.h
new file mode 100644
index 000000000000..9209f755763e
--- /dev/null
+++ b/arch/powerpc/include/asm/mman.h
@@ -0,0 +1,63 @@
+#ifndef _ASM_POWERPC_MMAN_H
+#define _ASM_POWERPC_MMAN_H
+
+#include <asm-generic/mman.h>
+
+/*
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#define PROT_SAO	0x10		/* Strong Access Ordering */
+
+#define MAP_RENAME      MAP_ANONYMOUS   /* In SunOS terminology */
+#define MAP_NORESERVE   0x40            /* don't reserve swap pages */
+#define MAP_LOCKED	0x80
+
+#define MAP_GROWSDOWN	0x0100		/* stack-like segment */
+#define MAP_DENYWRITE	0x0800		/* ETXTBSY */
+#define MAP_EXECUTABLE	0x1000		/* mark it as an executable */
+
+#define MCL_CURRENT     0x2000          /* lock all currently mapped pages */
+#define MCL_FUTURE      0x4000          /* lock all additions to address space */
+
+#define MAP_POPULATE	0x8000		/* populate (prefault) pagetables */
+#define MAP_NONBLOCK	0x10000		/* do not block on IO */
+
+#ifdef __KERNEL__
+#ifdef CONFIG_PPC64
+
+#include <asm/cputable.h>
+#include <linux/mm.h>
+
+/*
+ * This file is included by linux/mman.h, so we can't use cacl_vm_prot_bits()
+ * here.  How important is the optimization?
+ */
+static inline unsigned long arch_calc_vm_prot_bits(unsigned long prot)
+{
+	return (prot & PROT_SAO) ? VM_SAO : 0;
+}
+#define arch_calc_vm_prot_bits(prot) arch_calc_vm_prot_bits(prot)
+
+static inline pgprot_t arch_vm_get_page_prot(unsigned long vm_flags)
+{
+	return (vm_flags & VM_SAO) ? __pgprot(_PAGE_SAO) : 0;
+}
+#define arch_vm_get_page_prot(vm_flags) arch_vm_get_page_prot(vm_flags)
+
+static inline int arch_validate_prot(unsigned long prot)
+{
+	if (prot & ~(PROT_READ | PROT_WRITE | PROT_EXEC | PROT_SEM | PROT_SAO))
+		return 0;
+	if ((prot & PROT_SAO) && !cpu_has_feature(CPU_FTR_SAO))
+		return 0;
+	return 1;
+}
+#define arch_validate_prot(prot) arch_validate_prot(prot)
+
+#endif /* CONFIG_PPC64 */
+#endif /* __KERNEL__ */
+#endif	/* _ASM_POWERPC_MMAN_H */
