commit 3d13e839e801e081bdece0127c2affa33d0f77cf
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Feb 20 22:51:37 2020 +1100

    powerpc: Rename current_stack_pointer() to current_stack_frame()
    
    current_stack_pointer(), which was called __get_SP(), used to just
    return the value in r1.
    
    But that caused problems in some cases, so it was turned into a
    function in commit bfe9a2cfe91a ("powerpc: Reimplement __get_SP() as a
    function not a define").
    
    Because it's a function in a separate compilation unit to all its
    callers, it has the effect of causing a stack frame to be created, and
    then returns the address of that frame. This is good in some cases
    like those described in the above commit, but in other cases it's
    overkill, we just need to know what stack page we're on.
    
    On some other arches current_stack_pointer is just a register global
    giving the stack pointer, and we'd like to do that too. So rename our
    current_stack_pointer() to current_stack_frame() to make that
    possible.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Link: https://lore.kernel.org/r/20200220115141.2707-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
index 7426d7a90e1e..eed3954082fa 100644
--- a/arch/powerpc/include/asm/perf_event.h
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -32,7 +32,7 @@
 	do {							\
 		(regs)->result = 0;				\
 		(regs)->nip = __ip;				\
-		(regs)->gpr[1] = current_stack_pointer();	\
+		(regs)->gpr[1] = current_stack_frame();		\
 		asm volatile("mfmsr %0" : "=r" ((regs)->msr));	\
 	} while (0)
 

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
index 35926cd6cd0b..7426d7a90e1e 100644
--- a/arch/powerpc/include/asm/perf_event.h
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * Performance event support - hardware-specific disambiguation
  *
@@ -7,11 +8,6 @@
  * devices other than the core which provide their own performance counters.
  *
  * Copyright 2010 Freescale Semiconductor, Inc.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #ifdef CONFIG_PPC_PERF_CTRS

commit 333804dc3b7a92158ab63a48febff0d8ef89ada3
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Sun Dec 9 14:55:35 2018 +0530

    powerpc/perf: Update perf_regs structure to include SIER
    
    On each sample, Sample Instruction Event Register (SIER) content
    is saved in pt_regs. SIER does not have a entry as-is in the pt_regs
    but instead, SIER content is saved in the "dar" register of pt_regs.
    
    Patch adds another entry to the perf_regs structure to include the "SIER"
    printing which internally maps to the "dar" of pt_regs.
    
    It also check for the SIER availability in the platform and present
    value accordingly
    
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
index 16a49819da9a..35926cd6cd0b 100644
--- a/arch/powerpc/include/asm/perf_event.h
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -39,4 +39,7 @@
 		(regs)->gpr[1] = current_stack_pointer();	\
 		asm volatile("mfmsr %0" : "=r" ((regs)->msr));	\
 	} while (0)
+
+/* To support perf_regs sier update */
+extern bool is_sier_available(void);
 #endif

commit a6460b03f945ee216dbf42a0d9ee78d52fd470c2
Author: Sandipan Das <sandipan@linux.ibm.com>
Date:   Thu Dec 6 14:57:01 2018 +0530

    powerpc/bpf: Fix broken uapi for BPF_PROG_TYPE_PERF_EVENT
    
    Now that there are different variants of pt_regs for userspace and
    kernel, the uapi for the BPF_PROG_TYPE_PERF_EVENT program type must be
    changed by exporting the user_pt_regs structure instead of the pt_regs
    structure that is in-kernel only.
    
    Fixes: 002af9391bfb ("powerpc: Split user/kernel definitions of struct pt_regs")
    Signed-off-by: Sandipan Das <sandipan@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
index 8bf1b6351716..16a49819da9a 100644
--- a/arch/powerpc/include/asm/perf_event.h
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -26,6 +26,8 @@
 #include <asm/ptrace.h>
 #include <asm/reg.h>
 
+#define perf_arch_bpf_user_pt_regs(regs) &regs->user_regs
+
 /*
  * Overload regs->result to specify whether we should use the MSR (result
  * is zero) or the SIAR (result is non zero).

commit acf620ecf56cfc4edaffaf158250e128539cdd26
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Oct 13 19:41:39 2014 +1100

    powerpc: Rename __get_SP() to current_stack_pointer()
    
    Michael points out that __get_SP() is a pretty horrible
    function name. Let's give it a better name.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
index b058568a850d..8bf1b6351716 100644
--- a/arch/powerpc/include/asm/perf_event.h
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -34,7 +34,7 @@
 	do {							\
 		(regs)->result = 0;				\
 		(regs)->nip = __ip;				\
-		(regs)->gpr[1] = __get_SP();			\
+		(regs)->gpr[1] = current_stack_pointer();	\
 		asm volatile("mfmsr %0" : "=r" ((regs)->msr));	\
 	} while (0)
 #endif

commit bfe9a2cfe91a1c920f152ce5fd0a9ad74b3daf12
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Oct 13 19:41:38 2014 +1100

    powerpc: Reimplement __get_SP() as a function not a define
    
    Li Zhong points out an issue with our current __get_SP()
    implementation. If ftrace function tracing is enabled (ie -pg
    profiling using _mcount) we spill a stack frame on 64bit all the
    time.
    
    If a function calls __get_SP() and later calls a function that is
    tail call optimised, we will pop the stack frame and the value
    returned by __get_SP() is no longer valid. An example from Li can
    be found in save_stack_trace -> save_context_stack:
    
    c0000000000432c0 <.save_stack_trace>:
    c0000000000432c0:       mflr    r0
    c0000000000432c4:       std     r0,16(r1)
    c0000000000432c8:       stdu    r1,-128(r1) <-- stack frame for _mcount
    c0000000000432cc:       std     r3,112(r1)
    c0000000000432d0:       bl      <._mcount>
    c0000000000432d4:       nop
    
    c0000000000432d8:       mr      r4,r1 <-- __get_SP()
    
    c0000000000432dc:       ld      r5,632(r13)
    c0000000000432e0:       ld      r3,112(r1)
    c0000000000432e4:       li      r6,1
    
    c0000000000432e8:       addi    r1,r1,128 <-- pop stack frame
    
    c0000000000432ec:       ld      r0,16(r1)
    c0000000000432f0:       mtlr    r0
    c0000000000432f4:       b       <.save_context_stack> <-- tail call optimized
    
    save_context_stack ends up with a stack pointer below the current
    one, and it is likely to be scribbled over.
    
    Fix this by making __get_SP() a function which returns the
    callers stack frame. Also replace inline assembly which grabs
    the stack pointer in save_stack_trace and show_stack with
    __get_SP().
    
    This also fixes an issue with perf_arch_fetch_caller_regs().
    It currently unwinds the stack once, which will skip a
    valid stack frame on a leaf function. With the __get_SP() fixes
    in this patch, we never need to unwind the stack frame to get
    to the first interesting frame.
    
    We have to export __get_SP() because perf_arch_fetch_caller_regs()
    (which is used in modules) calls it from a header file.
    
    Reported-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
index 0bb23725b1e7..b058568a850d 100644
--- a/arch/powerpc/include/asm/perf_event.h
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -34,7 +34,7 @@
 	do {							\
 		(regs)->result = 0;				\
 		(regs)->nip = __ip;				\
-		(regs)->gpr[1] = *(unsigned long *)__get_SP();	\
+		(regs)->gpr[1] = __get_SP();			\
 		asm volatile("mfmsr %0" : "=r" ((regs)->msr));	\
 	} while (0)
 #endif

commit 75382aa72f06823db7312ad069c3bae2eb3f8548
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Jun 26 01:01:36 2012 +0000

    powerpc/perf: Move code to select SIAR or pt_regs into perf_read_regs
    
    The logic to choose whether to use the SIAR or get the information
    out of pt_regs is going to get more complicated, so do it once in
    perf_read_regs.
    
    We overload regs->result which is gross but we are already doing it
    with regs->dsisr.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
index 5c16b891d501..0bb23725b1e7 100644
--- a/arch/powerpc/include/asm/perf_event.h
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -26,8 +26,13 @@
 #include <asm/ptrace.h>
 #include <asm/reg.h>
 
+/*
+ * Overload regs->result to specify whether we should use the MSR (result
+ * is zero) or the SIAR (result is non zero).
+ */
 #define perf_arch_fetch_caller_regs(regs, __ip)			\
 	do {							\
+		(regs)->result = 0;				\
 		(regs)->nip = __ip;				\
 		(regs)->gpr[1] = *(unsigned long *)__get_SP();	\
 		asm volatile("mfmsr %0" : "=r" ((regs)->msr));	\

commit b0f82b81fe6bbcf78d478071f33e44554726bc81
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 20 07:47:21 2010 +0200

    perf: Drop the skip argument from perf_arch_fetch_regs_caller
    
    Drop this argument now that we always want to rewind only to the
    state of the first caller.
    It means frame pointers are not necessary anymore to reliably get
    the source of an event. But this also means we need this helper
    to be a macro now, as an inline function is not an option since
    we need to know when to provide a default implentation.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
index e6d4ce69b126..5c16b891d501 100644
--- a/arch/powerpc/include/asm/perf_event.h
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -21,3 +21,15 @@
 #ifdef CONFIG_FSL_EMB_PERF_EVENT
 #include <asm/perf_event_fsl_emb.h>
 #endif
+
+#ifdef CONFIG_PERF_EVENTS
+#include <asm/ptrace.h>
+#include <asm/reg.h>
+
+#define perf_arch_fetch_caller_regs(regs, __ip)			\
+	do {							\
+		(regs)->nip = __ip;				\
+		(regs)->gpr[1] = *(unsigned long *)__get_SP();	\
+		asm volatile("mfmsr %0" : "=r" ((regs)->msr));	\
+	} while (0)
+#endif

commit a11106544f33c104706ae42d27219a409b67478e
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Feb 25 18:09:45 2010 -0600

    powerpc/perf: e500 support
    
    This implements perf_event support for the Freescale embedded performance
    monitor, based on the existing perf_event.c that supports server/classic
    chips.
    
    Some limitations:
    - Performance monitor interrupts are regular EE interrupts, and thus you
      can't profile places with interrupts disabled.  We may want to implement
      soft IRQ-disabling, with perfmon interrupts exempted and treated as NMIs.
    - When trying to schedule multiple event groups at once, and using
      restricted events, situations could arise where scheduling fails even
      though it would be possible.  Consider three groups, each with two events.
      One group has restricted events, the others don't.  The two non-restricted
      groups are scheduled, then one is removed, which happens to occupy the two
      counters that can't do restricted events.  The remaining non-restricted
      group will not be moved to the non-restricted-capable counters to make
      room if the restricted group tries to be scheduled.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
index 3288ce3997e0..e6d4ce69b126 100644
--- a/arch/powerpc/include/asm/perf_event.h
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -1,110 +1,23 @@
 /*
- * Performance event support - PowerPC-specific definitions.
+ * Performance event support - hardware-specific disambiguation
  *
- * Copyright 2008-2009 Paul Mackerras, IBM Corporation.
+ * For now this is a compile-time decision, but eventually it should be
+ * runtime.  This would allow multiplatform perf event support for e300 (fsl
+ * embedded perf counters) plus server/classic, and would accommodate
+ * devices other than the core which provide their own performance counters.
+ *
+ * Copyright 2010 Freescale Semiconductor, Inc.
  *
  * This program is free software; you can redistribute it and/or
  * modify it under the terms of the GNU General Public License
  * as published by the Free Software Foundation; either version
  * 2 of the License, or (at your option) any later version.
  */
-#include <linux/types.h>
-
-#include <asm/hw_irq.h>
-
-#define MAX_HWEVENTS		8
-#define MAX_EVENT_ALTERNATIVES	8
-#define MAX_LIMITED_HWCOUNTERS	2
-
-/*
- * This struct provides the constants and functions needed to
- * describe the PMU on a particular POWER-family CPU.
- */
-struct power_pmu {
-	const char	*name;
-	int		n_counter;
-	int		max_alternatives;
-	unsigned long	add_fields;
-	unsigned long	test_adder;
-	int		(*compute_mmcr)(u64 events[], int n_ev,
-				unsigned int hwc[], unsigned long mmcr[]);
-	int		(*get_constraint)(u64 event_id, unsigned long *mskp,
-				unsigned long *valp);
-	int		(*get_alternatives)(u64 event_id, unsigned int flags,
-				u64 alt[]);
-	void		(*disable_pmc)(unsigned int pmc, unsigned long mmcr[]);
-	int		(*limited_pmc_event)(u64 event_id);
-	u32		flags;
-	int		n_generic;
-	int		*generic_events;
-	int		(*cache_events)[PERF_COUNT_HW_CACHE_MAX]
-			       [PERF_COUNT_HW_CACHE_OP_MAX]
-			       [PERF_COUNT_HW_CACHE_RESULT_MAX];
-};
-
-/*
- * Values for power_pmu.flags
- */
-#define PPMU_LIMITED_PMC5_6	1	/* PMC5/6 have limited function */
-#define PPMU_ALT_SIPR		2	/* uses alternate posn for SIPR/HV */
-
-/*
- * Values for flags to get_alternatives()
- */
-#define PPMU_LIMITED_PMC_OK	1	/* can put this on a limited PMC */
-#define PPMU_LIMITED_PMC_REQD	2	/* have to put this on a limited PMC */
-#define PPMU_ONLY_COUNT_RUN	4	/* only counting in run state */
-
-extern int register_power_pmu(struct power_pmu *);
 
-struct pt_regs;
-extern unsigned long perf_misc_flags(struct pt_regs *regs);
-extern unsigned long perf_instruction_pointer(struct pt_regs *regs);
-
-#define PERF_EVENT_INDEX_OFFSET	1
-
-/*
- * Only override the default definitions in include/linux/perf_event.h
- * if we have hardware PMU support.
- */
 #ifdef CONFIG_PPC_PERF_CTRS
-#define perf_misc_flags(regs)	perf_misc_flags(regs)
+#include <asm/perf_event_server.h>
 #endif
 
-/*
- * The power_pmu.get_constraint function returns a 32/64-bit value and
- * a 32/64-bit mask that express the constraints between this event_id and
- * other events.
- *
- * The value and mask are divided up into (non-overlapping) bitfields
- * of three different types:
- *
- * Select field: this expresses the constraint that some set of bits
- * in MMCR* needs to be set to a specific value for this event_id.  For a
- * select field, the mask contains 1s in every bit of the field, and
- * the value contains a unique value for each possible setting of the
- * MMCR* bits.  The constraint checking code will ensure that two events
- * that set the same field in their masks have the same value in their
- * value dwords.
- *
- * Add field: this expresses the constraint that there can be at most
- * N events in a particular class.  A field of k bits can be used for
- * N <= 2^(k-1) - 1.  The mask has the most significant bit of the field
- * set (and the other bits 0), and the value has only the least significant
- * bit of the field set.  In addition, the 'add_fields' and 'test_adder'
- * in the struct power_pmu for this processor come into play.  The
- * add_fields value contains 1 in the LSB of the field, and the
- * test_adder contains 2^(k-1) - 1 - N in the field.
- *
- * NAND field: this expresses the constraint that you may not have events
- * in all of a set of classes.  (For example, on PPC970, you can't select
- * events from the FPU, ISU and IDU simultaneously, although any two are
- * possible.)  For N classes, the field is N+1 bits wide, and each class
- * is assigned one bit from the least-significant N bits.  The mask has
- * only the most-significant bit set, and the value has only the bit
- * for the event_id's class set.  The test_adder has the least significant
- * bit set in the field.
- *
- * If an event_id is not subject to the constraint expressed by a particular
- * field, then it will have 0 in both the mask and value for that field.
- */
+#ifdef CONFIG_FSL_EMB_PERF_EVENT
+#include <asm/perf_event_fsl_emb.h>
+#endif

commit a8f90e906783f1f815120eefe813b23cb396e9bd
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 22 09:48:08 2009 +1000

    perf_event, powerpc: Fix compilation after big perf_counter rename
    
    This fixes two places in the powerpc perf_event (perf_counter) code
    where 'list_entry' needs to be changed to 'group_entry', but were
    missed in commit 65abc865 ("perf_counter: Rename list_entry ->
    group_entry, counter_list -> group_list").
    
    This also changes 'event' back to 'counter' in a couple of
    contexts:
    
    * Field and function names that deal with the limited-function
      counters: it's really the hardware counters whose function is
      limited, not the events that they count.  Hence:
    
      MAX_LIMITED_HWEVENTS -> MAX_LIMITED_HWCOUNTERS
      limited_event -> limited_counter
      freeze/thaw_limited_events -> freeze/thaw_limited_counters
    
    * The machine-specific PMU description struct (struct power_pmu): this
      renames 'n_event' back to 'n_counter' since it really describes how
      many hardware counters the machine has.  (Renaming this back avoids
      a compile error in each of the machine-specific PMU back-ends where
      they initialize their power_pmu struct.)
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Cc: linuxppc-dev@ozlabs.org
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <19128.4280.813369.589704@cargo.ozlabs.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
index 2499aaadaeb9..3288ce3997e0 100644
--- a/arch/powerpc/include/asm/perf_event.h
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -14,7 +14,7 @@
 
 #define MAX_HWEVENTS		8
 #define MAX_EVENT_ALTERNATIVES	8
-#define MAX_LIMITED_HWEVENTS	2
+#define MAX_LIMITED_HWCOUNTERS	2
 
 /*
  * This struct provides the constants and functions needed to
@@ -22,7 +22,7 @@
  */
 struct power_pmu {
 	const char	*name;
-	int		n_event;
+	int		n_counter;
 	int		max_alternatives;
 	unsigned long	add_fields;
 	unsigned long	test_adder;

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
new file mode 100644
index 000000000000..2499aaadaeb9
--- /dev/null
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -0,0 +1,110 @@
+/*
+ * Performance event support - PowerPC-specific definitions.
+ *
+ * Copyright 2008-2009 Paul Mackerras, IBM Corporation.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+#include <linux/types.h>
+
+#include <asm/hw_irq.h>
+
+#define MAX_HWEVENTS		8
+#define MAX_EVENT_ALTERNATIVES	8
+#define MAX_LIMITED_HWEVENTS	2
+
+/*
+ * This struct provides the constants and functions needed to
+ * describe the PMU on a particular POWER-family CPU.
+ */
+struct power_pmu {
+	const char	*name;
+	int		n_event;
+	int		max_alternatives;
+	unsigned long	add_fields;
+	unsigned long	test_adder;
+	int		(*compute_mmcr)(u64 events[], int n_ev,
+				unsigned int hwc[], unsigned long mmcr[]);
+	int		(*get_constraint)(u64 event_id, unsigned long *mskp,
+				unsigned long *valp);
+	int		(*get_alternatives)(u64 event_id, unsigned int flags,
+				u64 alt[]);
+	void		(*disable_pmc)(unsigned int pmc, unsigned long mmcr[]);
+	int		(*limited_pmc_event)(u64 event_id);
+	u32		flags;
+	int		n_generic;
+	int		*generic_events;
+	int		(*cache_events)[PERF_COUNT_HW_CACHE_MAX]
+			       [PERF_COUNT_HW_CACHE_OP_MAX]
+			       [PERF_COUNT_HW_CACHE_RESULT_MAX];
+};
+
+/*
+ * Values for power_pmu.flags
+ */
+#define PPMU_LIMITED_PMC5_6	1	/* PMC5/6 have limited function */
+#define PPMU_ALT_SIPR		2	/* uses alternate posn for SIPR/HV */
+
+/*
+ * Values for flags to get_alternatives()
+ */
+#define PPMU_LIMITED_PMC_OK	1	/* can put this on a limited PMC */
+#define PPMU_LIMITED_PMC_REQD	2	/* have to put this on a limited PMC */
+#define PPMU_ONLY_COUNT_RUN	4	/* only counting in run state */
+
+extern int register_power_pmu(struct power_pmu *);
+
+struct pt_regs;
+extern unsigned long perf_misc_flags(struct pt_regs *regs);
+extern unsigned long perf_instruction_pointer(struct pt_regs *regs);
+
+#define PERF_EVENT_INDEX_OFFSET	1
+
+/*
+ * Only override the default definitions in include/linux/perf_event.h
+ * if we have hardware PMU support.
+ */
+#ifdef CONFIG_PPC_PERF_CTRS
+#define perf_misc_flags(regs)	perf_misc_flags(regs)
+#endif
+
+/*
+ * The power_pmu.get_constraint function returns a 32/64-bit value and
+ * a 32/64-bit mask that express the constraints between this event_id and
+ * other events.
+ *
+ * The value and mask are divided up into (non-overlapping) bitfields
+ * of three different types:
+ *
+ * Select field: this expresses the constraint that some set of bits
+ * in MMCR* needs to be set to a specific value for this event_id.  For a
+ * select field, the mask contains 1s in every bit of the field, and
+ * the value contains a unique value for each possible setting of the
+ * MMCR* bits.  The constraint checking code will ensure that two events
+ * that set the same field in their masks have the same value in their
+ * value dwords.
+ *
+ * Add field: this expresses the constraint that there can be at most
+ * N events in a particular class.  A field of k bits can be used for
+ * N <= 2^(k-1) - 1.  The mask has the most significant bit of the field
+ * set (and the other bits 0), and the value has only the least significant
+ * bit of the field set.  In addition, the 'add_fields' and 'test_adder'
+ * in the struct power_pmu for this processor come into play.  The
+ * add_fields value contains 1 in the LSB of the field, and the
+ * test_adder contains 2^(k-1) - 1 - N in the field.
+ *
+ * NAND field: this expresses the constraint that you may not have events
+ * in all of a set of classes.  (For example, on PPC970, you can't select
+ * events from the FPU, ISU and IDU simultaneously, although any two are
+ * possible.)  For N classes, the field is N+1 bits wide, and each class
+ * is assigned one bit from the least-significant N bits.  The mask has
+ * only the most-significant bit set, and the value has only the bit
+ * for the event_id's class set.  The test_adder has the least significant
+ * bit set in the field.
+ *
+ * If an event_id is not subject to the constraint expressed by a particular
+ * field, then it will have 0 in both the mask and value for that field.
+ */
