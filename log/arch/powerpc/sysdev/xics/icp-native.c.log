commit 3a83f677a6eeff65751b29e3648d7c69c3be83f3
Author: Michael Roth <mdroth@linux.vnet.ibm.com>
Date:   Wed Sep 11 17:31:55 2019 -0500

    KVM: PPC: Book3S HV: use smp_mb() when setting/clearing host_ipi flag
    
    On a 2-socket Power9 system with 32 cores/128 threads (SMT4) and 1TB
    of memory running the following guest configs:
    
      guest A:
        - 224GB of memory
        - 56 VCPUs (sockets=1,cores=28,threads=2), where:
          VCPUs 0-1 are pinned to CPUs 0-3,
          VCPUs 2-3 are pinned to CPUs 4-7,
          ...
          VCPUs 54-55 are pinned to CPUs 108-111
    
      guest B:
        - 4GB of memory
        - 4 VCPUs (sockets=1,cores=4,threads=1)
    
    with the following workloads (with KSM and THP enabled in all):
    
      guest A:
        stress --cpu 40 --io 20 --vm 20 --vm-bytes 512M
    
      guest B:
        stress --cpu 4 --io 4 --vm 4 --vm-bytes 512M
    
      host:
        stress --cpu 4 --io 4 --vm 2 --vm-bytes 256M
    
    the below soft-lockup traces were observed after an hour or so and
    persisted until the host was reset (this was found to be reliably
    reproducible for this configuration, for kernels 4.15, 4.18, 5.0,
    and 5.3-rc5):
    
      [ 1253.183290] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1253.183319] rcu:     124-....: (5250 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=1941
      [ 1256.287426] watchdog: BUG: soft lockup - CPU#105 stuck for 23s! [CPU 52/KVM:19709]
      [ 1264.075773] watchdog: BUG: soft lockup - CPU#24 stuck for 23s! [worker:19913]
      [ 1264.079769] watchdog: BUG: soft lockup - CPU#31 stuck for 23s! [worker:20331]
      [ 1264.095770] watchdog: BUG: soft lockup - CPU#45 stuck for 23s! [worker:20338]
      [ 1264.131773] watchdog: BUG: soft lockup - CPU#64 stuck for 23s! [avocado:19525]
      [ 1280.408480] watchdog: BUG: soft lockup - CPU#124 stuck for 22s! [ksmd:791]
      [ 1316.198012] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1316.198032] rcu:     124-....: (21003 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=8243
      [ 1340.411024] watchdog: BUG: soft lockup - CPU#124 stuck for 22s! [ksmd:791]
      [ 1379.212609] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1379.212629] rcu:     124-....: (36756 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=14714
      [ 1404.413615] watchdog: BUG: soft lockup - CPU#124 stuck for 22s! [ksmd:791]
      [ 1442.227095] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1442.227115] rcu:     124-....: (52509 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=21403
      [ 1455.111787] INFO: task worker:19907 blocked for more than 120 seconds.
      [ 1455.111822]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.111833] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.111884] INFO: task worker:19908 blocked for more than 120 seconds.
      [ 1455.111905]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.111925] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.111966] INFO: task worker:20328 blocked for more than 120 seconds.
      [ 1455.111986]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.111998] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112048] INFO: task worker:20330 blocked for more than 120 seconds.
      [ 1455.112068]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112097] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112138] INFO: task worker:20332 blocked for more than 120 seconds.
      [ 1455.112159]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112179] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112210] INFO: task worker:20333 blocked for more than 120 seconds.
      [ 1455.112231]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112242] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112282] INFO: task worker:20335 blocked for more than 120 seconds.
      [ 1455.112303]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112332] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112372] INFO: task worker:20336 blocked for more than 120 seconds.
      [ 1455.112392]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
    
    CPUs 45, 24, and 124 are stuck on spin locks, likely held by
    CPUs 105 and 31.
    
    CPUs 105 and 31 are stuck in smp_call_function_many(), waiting on
    target CPU 42. For instance:
    
      # CPU 105 registers (via xmon)
      R00 = c00000000020b20c   R16 = 00007d1bcd800000
      R01 = c00000363eaa7970   R17 = 0000000000000001
      R02 = c0000000019b3a00   R18 = 000000000000006b
      R03 = 000000000000002a   R19 = 00007d537d7aecf0
      R04 = 000000000000002a   R20 = 60000000000000e0
      R05 = 000000000000002a   R21 = 0801000000000080
      R06 = c0002073fb0caa08   R22 = 0000000000000d60
      R07 = c0000000019ddd78   R23 = 0000000000000001
      R08 = 000000000000002a   R24 = c00000000147a700
      R09 = 0000000000000001   R25 = c0002073fb0ca908
      R10 = c000008ffeb4e660   R26 = 0000000000000000
      R11 = c0002073fb0ca900   R27 = c0000000019e2464
      R12 = c000000000050790   R28 = c0000000000812b0
      R13 = c000207fff623e00   R29 = c0002073fb0ca808
      R14 = 00007d1bbee00000   R30 = c0002073fb0ca800
      R15 = 00007d1bcd600000   R31 = 0000000000000800
      pc  = c00000000020b260 smp_call_function_many+0x3d0/0x460
      cfar= c00000000020b270 smp_call_function_many+0x3e0/0x460
      lr  = c00000000020b20c smp_call_function_many+0x37c/0x460
      msr = 900000010288b033   cr  = 44024824
      ctr = c000000000050790   xer = 0000000000000000   trap =  100
    
    CPU 42 is running normally, doing VCPU work:
    
      # CPU 42 stack trace (via xmon)
      [link register   ] c00800001be17188 kvmppc_book3s_radix_page_fault+0x90/0x2b0 [kvm_hv]
      [c000008ed3343820] c000008ed3343850 (unreliable)
      [c000008ed33438d0] c00800001be11b6c kvmppc_book3s_hv_page_fault+0x264/0xe30 [kvm_hv]
      [c000008ed33439d0] c00800001be0d7b4 kvmppc_vcpu_run_hv+0x8dc/0xb50 [kvm_hv]
      [c000008ed3343ae0] c00800001c10891c kvmppc_vcpu_run+0x34/0x48 [kvm]
      [c000008ed3343b00] c00800001c10475c kvm_arch_vcpu_ioctl_run+0x244/0x420 [kvm]
      [c000008ed3343b90] c00800001c0f5a78 kvm_vcpu_ioctl+0x470/0x7c8 [kvm]
      [c000008ed3343d00] c000000000475450 do_vfs_ioctl+0xe0/0xc70
      [c000008ed3343db0] c0000000004760e4 ksys_ioctl+0x104/0x120
      [c000008ed3343e00] c000000000476128 sys_ioctl+0x28/0x80
      [c000008ed3343e20] c00000000000b388 system_call+0x5c/0x70
      --- Exception: c00 (System Call) at 00007d545cfd7694
      SP (7d53ff7edf50) is in userspace
    
    It was subsequently found that ipi_message[PPC_MSG_CALL_FUNCTION]
    was set for CPU 42 by at least 1 of the CPUs waiting in
    smp_call_function_many(), but somehow the corresponding
    call_single_queue entries were never processed by CPU 42, causing the
    callers to spin in csd_lock_wait() indefinitely.
    
    Nick Piggin suggested something similar to the following sequence as
    a possible explanation (interleaving of CALL_FUNCTION/RESCHEDULE
    IPI messages seems to be most common, but any mix of CALL_FUNCTION and
    !CALL_FUNCTION messages could trigger it):
    
        CPU
          X: smp_muxed_ipi_set_message():
          X:   smp_mb()
          X:   message[RESCHEDULE] = 1
          X: doorbell_global_ipi(42):
          X:   kvmppc_set_host_ipi(42, 1)
          X:   ppc_msgsnd_sync()/smp_mb()
          X:   ppc_msgsnd() -> 42
         42: doorbell_exception(): // from CPU X
         42:   ppc_msgsync()
        105: smp_muxed_ipi_set_message():
        105:   smb_mb()
             // STORE DEFERRED DUE TO RE-ORDERING
      --105:   message[CALL_FUNCTION] = 1
      | 105: doorbell_global_ipi(42):
      | 105:   kvmppc_set_host_ipi(42, 1)
      |  42:   kvmppc_set_host_ipi(42, 0)
      |  42: smp_ipi_demux_relaxed()
      |  42: // returns to executing guest
      |      // RE-ORDERED STORE COMPLETES
      ->105:   message[CALL_FUNCTION] = 1
        105:   ppc_msgsnd_sync()/smp_mb()
        105:   ppc_msgsnd() -> 42
         42: local_paca->kvm_hstate.host_ipi == 0 // IPI ignored
        105: // hangs waiting on 42 to process messages/call_single_queue
    
    This can be prevented with an smp_mb() at the beginning of
    kvmppc_set_host_ipi(), such that stores to message[<type>] (or other
    state indicated by the host_ipi flag) are ordered vs. the store to
    to host_ipi.
    
    However, doing so might still allow for the following scenario (not
    yet observed):
    
        CPU
          X: smp_muxed_ipi_set_message():
          X:   smp_mb()
          X:   message[RESCHEDULE] = 1
          X: doorbell_global_ipi(42):
          X:   kvmppc_set_host_ipi(42, 1)
          X:   ppc_msgsnd_sync()/smp_mb()
          X:   ppc_msgsnd() -> 42
         42: doorbell_exception(): // from CPU X
         42:   ppc_msgsync()
             // STORE DEFERRED DUE TO RE-ORDERING
      -- 42:   kvmppc_set_host_ipi(42, 0)
      |  42: smp_ipi_demux_relaxed()
      | 105: smp_muxed_ipi_set_message():
      | 105:   smb_mb()
      | 105:   message[CALL_FUNCTION] = 1
      | 105: doorbell_global_ipi(42):
      | 105:   kvmppc_set_host_ipi(42, 1)
      |      // RE-ORDERED STORE COMPLETES
      -> 42:   kvmppc_set_host_ipi(42, 0)
         42: // returns to executing guest
        105:   ppc_msgsnd_sync()/smp_mb()
        105:   ppc_msgsnd() -> 42
         42: local_paca->kvm_hstate.host_ipi == 0 // IPI ignored
        105: // hangs waiting on 42 to process messages/call_single_queue
    
    Fixing this scenario would require an smp_mb() *after* clearing
    host_ipi flag in kvmppc_set_host_ipi() to order the store vs.
    subsequent processing of IPI messages.
    
    To handle both cases, this patch splits kvmppc_set_host_ipi() into
    separate set/clear functions, where we execute smp_mb() prior to
    setting host_ipi flag, and after clearing host_ipi flag. These
    functions pair with each other to synchronize the sender and receiver
    sides.
    
    With that change in place the above workload ran for 20 hours without
    triggering any lock-ups.
    
    Fixes: 755563bc79c7 ("powerpc/powernv: Fixes for hypervisor doorbell handling") # v4.0
    Signed-off-by: Michael Roth <mdroth@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190911223155.16045-1-mdroth@linux.vnet.ibm.com

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 485569ff7ef1..7d13d2ef5a90 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -140,7 +140,7 @@ static unsigned int icp_native_get_irq(void)
 
 static void icp_native_cause_ipi(int cpu)
 {
-	kvmppc_set_host_ipi(cpu, 1);
+	kvmppc_set_host_ipi(cpu);
 	icp_native_set_qirr(cpu, IPI_PRIORITY);
 }
 
@@ -179,7 +179,7 @@ void icp_native_flush_interrupt(void)
 	if (vec == XICS_IPI) {
 		/* Clear pending IPI */
 		int cpu = smp_processor_id();
-		kvmppc_set_host_ipi(cpu, 0);
+		kvmppc_clear_host_ipi(cpu);
 		icp_native_set_qirr(cpu, 0xff);
 	} else {
 		pr_err("XICS: hw interrupt 0x%x to offline cpu, disabling\n",
@@ -200,7 +200,7 @@ static irqreturn_t icp_native_ipi_action(int irq, void *dev_id)
 {
 	int cpu = smp_processor_id();
 
-	kvmppc_set_host_ipi(cpu, 0);
+	kvmppc_clear_host_ipi(cpu);
 	icp_native_set_qirr(cpu, 0xff);
 
 	return smp_ipi_demux();

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 37bfbc54aacb..485569ff7ef1 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -1,11 +1,6 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * Copyright 2011 IBM Corporation.
- *
- *  This program is free software; you can redistribute it and/or
- *  modify it under the terms of the GNU General Public License
- *  as published by the Free Software Foundation; either version
- *  2 of the License, or (at your option) any later version.
- *
  */
 
 #include <linux/types.h>

commit d2e60075a3d4422dc54b919f3b125d8066b839d4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:12 2018 +1000

    powerpc/64: Use array of paca pointers and allocate pacas individually
    
    Change the paca array into an array of pointers to pacas. Allocate
    pacas individually.
    
    This allows flexibility in where the PACAs are allocated. Future work
    will allocate them node-local. Platforms that don't have address limits
    on PACAs would be able to defer PACA allocations until later in boot
    rather than allocate all possible ones up-front then freeing unused.
    
    This is slightly more overhead (one additional indirection) for cross
    CPU paca references, but those aren't too common.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 1459f4e8b698..37bfbc54aacb 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -164,7 +164,7 @@ void icp_native_cause_ipi_rm(int cpu)
 	 * Just like the cause_ipi functions, it is required to
 	 * include a full barrier before causing the IPI.
 	 */
-	xics_phys = paca[cpu].kvm_hstate.xics_phys;
+	xics_phys = paca_ptrs[cpu]->kvm_hstate.xics_phys;
 	mb();
 	__raw_rm_writeb(IPI_PRIORITY, xics_phys + XICS_MFRR);
 }

commit f2c2cbcc35d47f1471a04155ac357521f5170371
Author: Joe Perches <joe@perches.com>
Date:   Mon Oct 24 21:00:08 2016 -0700

    powerpc: Use pr_warn instead of pr_warning
    
    At some point, pr_warning will be removed so all logging messages use
    a consistent <prefix>_warn style.
    
    Update arch/powerpc/
    
    Miscellanea:
    
    o Coalesce formats
    o Realign arguments
    o Use %s, __func__ instead of embedded function names
    o Remove unnecessary line continuations
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Acked-by: Geoff Levand <geoff@infradead.org>
    [mpe: Rebase due to some %pOF changes.]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 2bfb9968d562..1459f4e8b698 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -241,18 +241,16 @@ static int __init icp_native_map_one_cpu(int hw_id, unsigned long addr,
 			  cpu, hw_id);
 
 	if (!request_mem_region(addr, size, rname)) {
-		pr_warning("icp_native: Could not reserve ICP MMIO"
-			   " for CPU %d, interrupt server #0x%x\n",
-			   cpu, hw_id);
+		pr_warn("icp_native: Could not reserve ICP MMIO for CPU %d, interrupt server #0x%x\n",
+			cpu, hw_id);
 		return -EBUSY;
 	}
 
 	icp_native_regs[cpu] = ioremap(addr, size);
 	kvmppc_set_xics_phys(cpu, addr);
 	if (!icp_native_regs[cpu]) {
-		pr_warning("icp_native: Failed ioremap for CPU %d, "
-			   "interrupt server #0x%x, addr %#lx\n",
-			   cpu, hw_id, addr);
+		pr_warn("icp_native: Failed ioremap for CPU %d, interrupt server #0x%x, addr %#lx\n",
+			cpu, hw_id, addr);
 		release_mem_region(addr, size);
 		return -ENOMEM;
 	}

commit b866cc2199d6a6cdcefe4acfe4cfca3ac3c6d38e
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Apr 13 20:16:21 2017 +1000

    powerpc: Change the doorbell IPI calling convention
    
    Change the doorbell callers to know about their msgsnd addressing,
    rather than have them set a per-cpu target data tag at boot that gets
    sent to the cause_ipi functions. The data is only used for doorbell IPI
    functions, no other IPI types, so it makes sense to keep that detail
    local to doorbell.
    
    Have the platform code understand doorbell IPIs, rather than the
    interrupt controller code understand them. Platform code can look at
    capabilities it has available and decide which to use.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index f0f3f47a3fc9..2bfb9968d562 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -143,19 +143,9 @@ static unsigned int icp_native_get_irq(void)
 
 #ifdef CONFIG_SMP
 
-static void icp_native_cause_ipi(int cpu, unsigned long data)
+static void icp_native_cause_ipi(int cpu)
 {
 	kvmppc_set_host_ipi(cpu, 1);
-#ifdef CONFIG_PPC_DOORBELL
-	if (cpu_has_feature(CPU_FTR_DBELL)) {
-		if (cpumask_test_cpu(cpu, cpu_sibling_mask(get_cpu()))) {
-			doorbell_cause_ipi(cpu, data);
-			put_cpu();
-			return;
-		}
-		put_cpu();
-	}
-#endif
 	icp_native_set_qirr(cpu, IPI_PRIORITY);
 }
 

commit d381d7caf812f7aa9f05cfeb858c9004ac654412
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:54 2017 +1000

    powerpc: Consolidate variants of real-mode MMIOs
    
    We have all sort of variants of MMIO accessors for the real mode
    instructions. This creates a clean set of accessors based on
    Linux normal naming conventions, replacing all occurrences of
    the old ones in the tree.
    
    I have purposefully removed the "out/in" variants in favor of
    only including __raw variants. Any code using these is already
    pretty much hand tuned to operate in a very specific environment.
    I've fixed up the 2 users (only one of them actually needed
    a barrier in the first place).
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 8a6a043e239b..f0f3f47a3fc9 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -168,15 +168,15 @@ void icp_native_cause_ipi_rm(int cpu)
 	 * Need the physical address of the XICS to be
 	 * previously saved in kvm_hstate in the paca.
 	 */
-	unsigned long xics_phys;
+	void __iomem *xics_phys;
 
 	/*
 	 * Just like the cause_ipi functions, it is required to
-	 * include a full barrier (out8 includes a sync) before
-	 * causing the IPI.
+	 * include a full barrier before causing the IPI.
 	 */
 	xics_phys = paca[cpu].kvm_hstate.xics_phys;
-	out_rm8((u8 *)(xics_phys + XICS_MFRR), IPI_PRIORITY);
+	mb();
+	__raw_rm_writeb(IPI_PRIORITY, xics_phys + XICS_MFRR);
 }
 #endif
 

commit ef24ba7091517d2bbf9ba2cb4256c0dccd51d248
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Sep 6 21:53:24 2016 +1000

    powerpc: Remove all usages of NO_IRQ
    
    NO_IRQ has been == 0 on powerpc for just over ten years (since commit
    0ebfff1491ef ("[POWERPC] Add new interrupt mapping core and change
    platforms to use it")). It's also 0 on most other arches.
    
    Although it's fairly harmless, every now and then it causes confusion
    when a driver is built on powerpc and another arch which doesn't define
    NO_IRQ. There's at least 6 definitions of NO_IRQ in drivers/, at least
    some of which are to work around that problem.
    
    So we'd like to remove it. This is fairly trivial in the arch code, we
    just convert:
    
        if (irq == NO_IRQ)  to      if (!irq)
        if (irq != NO_IRQ)  to      if (irq)
        irq = NO_IRQ;       to      irq = 0;
        return NO_IRQ;      to      return 0;
    
    And a few other odd cases as well.
    
    At least for now we keep the #define NO_IRQ, because there is driver
    code that uses NO_IRQ and the fixes to remove those will go via other
    trees.
    
    Note we also change some occurrences in PPC sound drivers, drivers/ps3,
    and drivers/macintosh.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index afdf62f2a695..8a6a043e239b 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -124,10 +124,10 @@ static unsigned int icp_native_get_irq(void)
 	unsigned int irq;
 
 	if (vec == XICS_IRQ_SPURIOUS)
-		return NO_IRQ;
+		return 0;
 
 	irq = irq_find_mapping(xics_host, vec);
-	if (likely(irq != NO_IRQ)) {
+	if (likely(irq)) {
 		xics_push_cppr(vec);
 		return irq;
 	}
@@ -138,7 +138,7 @@ static unsigned int icp_native_get_irq(void)
 	/* We might learn about it later, so EOI it */
 	icp_native_set_xirr(xirr);
 
-	return NO_IRQ;
+	return 0;
 }
 
 #ifdef CONFIG_SMP

commit ec13e9b6b13d66c54951fec7f1158bf85f68fecd
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Thu Dec 17 14:59:05 2015 -0600

    powerpc/xics: Add icp_native_cause_ipi_rm
    
    Function to cause an IPI by directly updating the MFFR register
    in the XICS. The function is meant for real-mode callers since
    they cannot use the smp_ops->cause_ipi function which uses an
    ioremapped address.
    
    Normal usage is for the the KVM real mode code to set the IPI message
    using smp_muxed_ipi_message_pass and then invoke icp_native_cause_ipi_rm
    to cause the actual IPI.
    
    The function requires kvm_hstate.xics_phys to have been initialized
    with the physical address of XICS.
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index eae32654bdf2..afdf62f2a695 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -159,6 +159,27 @@ static void icp_native_cause_ipi(int cpu, unsigned long data)
 	icp_native_set_qirr(cpu, IPI_PRIORITY);
 }
 
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+void icp_native_cause_ipi_rm(int cpu)
+{
+	/*
+	 * Currently not used to send IPIs to another CPU
+	 * on the same core. Only caller is KVM real mode.
+	 * Need the physical address of the XICS to be
+	 * previously saved in kvm_hstate in the paca.
+	 */
+	unsigned long xics_phys;
+
+	/*
+	 * Just like the cause_ipi functions, it is required to
+	 * include a full barrier (out8 includes a sync) before
+	 * causing the IPI.
+	 */
+	xics_phys = paca[cpu].kvm_hstate.xics_phys;
+	out_rm8((u8 *)(xics_phys + XICS_MFRR), IPI_PRIORITY);
+}
+#endif
+
 /*
  * Called when an interrupt is received on an off-line CPU to
  * clear the interrupt, so that the CPU can go back to nap mode.

commit 3609d819a36c65857816ca1278d80767d6d9b990
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Wed May 20 00:30:14 2015 +0530

    powerpc: Make doorbell check preemption safe
    
    Doorbell can be used to cause ipi on cpus which are sibling threads on
    the same core. So icp_native_cause_ipi checks if the destination cpu
    is a sibling thread of the current cpu and uses doorbell in such cases.
    
    But while running with CONFIG_PREEMPT=y, since this section is
    preemtible, we can run into issues if after we check if the destination
    cpu is a sibling cpu, the task gets migrated from a sibling cpu to a
    cpu on another core.
    
    Fix this by using get_cpu()/ put_cpu()
    
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 2fc4cf1b7557..eae32654bdf2 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -147,12 +147,16 @@ static void icp_native_cause_ipi(int cpu, unsigned long data)
 {
 	kvmppc_set_host_ipi(cpu, 1);
 #ifdef CONFIG_PPC_DOORBELL
-	if (cpu_has_feature(CPU_FTR_DBELL) &&
-	    (cpumask_test_cpu(cpu, cpu_sibling_mask(smp_processor_id()))))
-		doorbell_cause_ipi(cpu, data);
-	else
+	if (cpu_has_feature(CPU_FTR_DBELL)) {
+		if (cpumask_test_cpu(cpu, cpu_sibling_mask(get_cpu()))) {
+			doorbell_cause_ipi(cpu, data);
+			put_cpu();
+			return;
+		}
+		put_cpu();
+	}
 #endif
-		icp_native_set_qirr(cpu, IPI_PRIORITY);
+	icp_native_set_qirr(cpu, IPI_PRIORITY);
 }
 
 /*

commit d6a4f70909d279004a2b3d539e240e07b1ecc1cb
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 2 14:23:16 2014 +1000

    powerpc/powernv: Don't call generic code on offline cpus
    
    On PowerNV platforms, when a CPU is offline, we put it into nap mode.
    It's possible that the CPU wakes up from nap mode while it is still
    offline due to a stray IPI.  A misdirected device interrupt could also
    potentially cause it to wake up.  In that circumstance, we need to clear
    the interrupt so that the CPU can go back to nap mode.
    
    In the past the clearing of the interrupt was accomplished by briefly
    enabling interrupts and allowing the normal interrupt handling code
    (do_IRQ() etc.) to handle the interrupt.  This has the problem that
    this code calls irq_enter() and irq_exit(), which call functions such
    as account_system_vtime() which use RCU internally.  Use of RCU is not
    permitted on offline CPUs and will trigger errors if RCU checking is
    enabled.
    
    To avoid calling into any generic code which might use RCU, we adopt
    a different method of clearing interrupts on offline CPUs.  Since we
    are on the PowerNV platform, we know that the system interrupt
    controller is a XICS being driven directly (i.e. not via hcalls) by
    the kernel.  Hence this adds a new icp_native_flush_interrupt()
    function to the native-mode XICS driver and arranges to call that
    when an offline CPU is woken from nap.  This new function reads the
    interrupt from the XICS.  If it is an IPI, it clears the IPI; if it
    is a device interrupt, it prints a warning and disables the source.
    Then it does the end-of-interrupt processing for the interrupt.
    
    The other thing that briefly enabling interrupts did was to check and
    clear the irq_happened flag in this CPU's PACA.  Therefore, after
    flushing the interrupt from the XICS, we also clear all bits except
    the PACA_IRQ_HARD_DIS (interrupts are hard disabled) bit from the
    irq_happened flag.  The PACA_IRQ_HARD_DIS flag is set by power7_nap()
    and is left set to indicate that interrupts are hard disabled.  This
    means we then have to ignore that flag in power7_nap(), which is
    reasonable since it doesn't indicate that any interrupt event needs
    servicing.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index de8d9483bbe8..2fc4cf1b7557 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -155,6 +155,31 @@ static void icp_native_cause_ipi(int cpu, unsigned long data)
 		icp_native_set_qirr(cpu, IPI_PRIORITY);
 }
 
+/*
+ * Called when an interrupt is received on an off-line CPU to
+ * clear the interrupt, so that the CPU can go back to nap mode.
+ */
+void icp_native_flush_interrupt(void)
+{
+	unsigned int xirr = icp_native_get_xirr();
+	unsigned int vec = xirr & 0x00ffffff;
+
+	if (vec == XICS_IRQ_SPURIOUS)
+		return;
+	if (vec == XICS_IPI) {
+		/* Clear pending IPI */
+		int cpu = smp_processor_id();
+		kvmppc_set_host_ipi(cpu, 0);
+		icp_native_set_qirr(cpu, 0xff);
+	} else {
+		pr_err("XICS: hw interrupt 0x%x to offline cpu, disabling\n",
+		       vec);
+		xics_mask_unknown_vec(vec);
+	}
+	/* EOI the interrupt */
+	icp_native_set_xirr(xirr);
+}
+
 void xics_wake_cpu(int cpu)
 {
 	icp_native_set_qirr(cpu, IPI_PRIORITY);

commit d4e58e5928f8c6c49228451dd03e0714cbab299a
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Jun 11 15:59:28 2014 +1000

    powerpc/powernv: Enable POWER8 doorbell IPIs
    
    This patch enables POWER8 doorbell IPIs on powernv.
    
    Since doorbells can only IPI within a core, we test to see when we can use
    doorbells and if not we fall back to XICS.  This also enables hypervisor
    doorbells to wakeup us up from nap/sleep via the LPCR PECEDH bit.
    
    Based on tests by Anton, the best case IPI latency between two threads dropped
    from 894ns to 512ns.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 9dee47071af8..de8d9483bbe8 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -26,6 +26,7 @@
 #include <asm/errno.h>
 #include <asm/xics.h>
 #include <asm/kvm_ppc.h>
+#include <asm/dbell.h>
 
 struct icp_ipl {
 	union {
@@ -145,7 +146,13 @@ static unsigned int icp_native_get_irq(void)
 static void icp_native_cause_ipi(int cpu, unsigned long data)
 {
 	kvmppc_set_host_ipi(cpu, 1);
-	icp_native_set_qirr(cpu, IPI_PRIORITY);
+#ifdef CONFIG_PPC_DOORBELL
+	if (cpu_has_feature(CPU_FTR_DBELL) &&
+	    (cpumask_test_cpu(cpu, cpu_sibling_mask(smp_processor_id()))))
+		doorbell_cause_ipi(cpu, data);
+	else
+#endif
+		icp_native_set_qirr(cpu, IPI_PRIORITY);
 }
 
 void xics_wake_cpu(int cpu)

commit 6f7aba7bb4bd09824992cda7f77d4e8b1704a04c
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 7 02:01:34 2013 +1000

    powerpc: Add some endian annotations to time and xics code
    
    Fix a couple of sparse warnings.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 7cd728b3b5e4..9dee47071af8 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -216,7 +216,7 @@ static int __init icp_native_init_one_node(struct device_node *np,
 					   unsigned int *indx)
 {
 	unsigned int ilen;
-	const u32 *ireg;
+	const __be32 *ireg;
 	int i;
 	int reg_tuple_size;
 	int num_servers = 0;

commit 01227a889ed56ae53aeebb9f93be9d54dd8b2de8
Merge: 9e6879460c8e db6ae6158186
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 5 14:47:31 2013 -0700

    Merge tag 'kvm-3.10-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Gleb Natapov:
     "Highlights of the updates are:
    
      general:
       - new emulated device API
       - legacy device assignment is now optional
       - irqfd interface is more generic and can be shared between arches
    
      x86:
       - VMCS shadow support and other nested VMX improvements
       - APIC virtualization and Posted Interrupt hardware support
       - Optimize mmio spte zapping
    
      ppc:
        - BookE: in-kernel MPIC emulation with irqfd support
        - Book3S: in-kernel XICS emulation (incomplete)
        - Book3S: HV: migration fixes
        - BookE: more debug support preparation
        - BookE: e6500 support
    
      ARM:
       - reworking of Hyp idmaps
    
      s390:
       - ioeventfd for virtio-ccw
    
      And many other bug fixes, cleanups and improvements"
    
    * tag 'kvm-3.10-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (204 commits)
      kvm: Add compat_ioctl for device control API
      KVM: x86: Account for failing enable_irq_window for NMI window request
      KVM: PPC: Book3S: Add API for in-kernel XICS emulation
      kvm/ppc/mpic: fix missing unlock in set_base_addr()
      kvm/ppc: Hold srcu lock when calling kvm_io_bus_read/write
      kvm/ppc/mpic: remove users
      kvm/ppc/mpic: fix mmio region lists when multiple guests used
      kvm/ppc/mpic: remove default routes from documentation
      kvm: KVM_CAP_IOMMU only available with device assignment
      ARM: KVM: iterate over all CPUs for CPU compatibility check
      KVM: ARM: Fix spelling in error message
      ARM: KVM: define KVM_ARM_MAX_VCPUS unconditionally
      KVM: ARM: Fix API documentation for ONE_REG encoding
      ARM: KVM: promote vfp_host pointer to generic host cpu context
      ARM: KVM: add architecture specific hook for capabilities
      ARM: KVM: perform HYP initilization for hotplugged CPUs
      ARM: KVM: switch to a dual-step HYP init code
      ARM: KVM: rework HYP page table freeing
      ARM: KVM: enforce maximum size for identity mapped code
      ARM: KVM: move to a KVM provided HYP idmap
      ...

commit 54695c3088a74e25474db8eb6b490b45d1aeb0ca
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 17 20:30:50 2013 +0000

    KVM: PPC: Book3S HV: Speed up wakeups of CPUs on HV KVM
    
    Currently, we wake up a CPU by sending a host IPI with
    smp_send_reschedule() to thread 0 of that core, which will take all
    threads out of the guest, and cause them to re-evaluate their
    interrupt status on the way back in.
    
    This adds a mechanism to differentiate real host IPIs from IPIs sent
    by KVM for guest threads to poke each other, in order to target the
    guest threads precisely when possible and avoid that global switch of
    the core to host state.
    
    We then use this new facility in the in-kernel XICS code.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 48861d3fcd07..20b328bb494d 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -51,6 +51,12 @@ static struct icp_ipl __iomem *icp_native_regs[NR_CPUS];
 static inline unsigned int icp_native_get_xirr(void)
 {
 	int cpu = smp_processor_id();
+	unsigned int xirr;
+
+	/* Handled an interrupt latched by KVM */
+	xirr = kvmppc_get_xics_latch();
+	if (xirr)
+		return xirr;
 
 	return in_be32(&icp_native_regs[cpu]->xirr.word);
 }
@@ -138,6 +144,7 @@ static unsigned int icp_native_get_irq(void)
 
 static void icp_native_cause_ipi(int cpu, unsigned long data)
 {
+	kvmppc_set_host_ipi(cpu, 1);
 	icp_native_set_qirr(cpu, IPI_PRIORITY);
 }
 
@@ -151,6 +158,7 @@ static irqreturn_t icp_native_ipi_action(int irq, void *dev_id)
 {
 	int cpu = smp_processor_id();
 
+	kvmppc_set_host_ipi(cpu, 0);
 	icp_native_set_qirr(cpu, 0xff);
 
 	return smp_ipi_demux();

commit 137436c9a6ee385c883db09e41af763888ee7642
Author: Gavin Shan <shangw@linux.vnet.ibm.com>
Date:   Thu Apr 25 19:20:59 2013 +0000

    powerpc/powernv: Patch MSI EOI handler on P8
    
    The EOI handler of MSI/MSI-X interrupts for P8 (PHB3) need additional
    steps to handle the P/Q bits in IVE before EOIing the corresponding
    interrupt. The patch changes the EOI handler to cover that. we have
    individual IRQ chip in each PHB instance. During the MSI IRQ setup
    time, the IRQ chip is copied over from the original one for that IRQ,
    and the EOI handler is patched with the one that will handle the P/Q
    bits (As Ben suggested).
    
    Signed-off-by: Gavin Shan <shangw@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 48861d3fcd07..89db29d17c25 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -81,7 +81,7 @@ static void icp_native_set_cpu_priority(unsigned char cppr)
 	iosync();
 }
 
-static void icp_native_eoi(struct irq_data *d)
+void icp_native_eoi(struct irq_data *d)
 {
 	unsigned int hw_irq = (unsigned int)irqd_to_hwirq(d);
 

commit d6b0d1f7058f7cf818138cd7fd116dca3f3576d9
Author: Grant Likely <grant.likely@secretlab.ca>
Date:   Sun Jun 3 22:04:37 2012 -0700

    irqdomain: Eliminate dedicated radix lookup functions
    
    In preparation to remove the slow revmap path, eliminate the public
    radix revmap lookup functions.  This simplifies the code and makes the
    slowpath removal patch a lot simpler.
    
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Rob Herring <rob.herring@calxeda.com>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 4c79b6fbee1c..48861d3fcd07 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -119,7 +119,7 @@ static unsigned int icp_native_get_irq(void)
 	if (vec == XICS_IRQ_SPURIOUS)
 		return NO_IRQ;
 
-	irq = irq_radix_revmap_lookup(xics_host, vec);
+	irq = irq_find_mapping(xics_host, vec);
 	if (likely(irq != NO_IRQ)) {
 		xics_push_cppr(vec);
 		return irq;

commit cf01a404d96f9bf6f514074579f0a2d355bfa38e
Author: Arnaud Lacombe <lacombar@gmail.com>
Date:   Thu Aug 25 06:07:13 2011 +0000

    powerpc/xics: Add __init to marker icp_native_init()
    
    This should fix the following warning:
    
     LD      arch/powerpc/sysdev/xics/built-in.o
    WARNING: arch/powerpc/sysdev/xics/built-in.o(.text+0x1310): Section mismatch in
    reference from the function .icp_native_init() to the function
    .init.text:.icp_native_init_one_node()
    The function .icp_native_init() references
    the function __init .icp_native_init_one_node().
    This is often because .icp_native_init lacks a __init
    annotation or the annotation of .icp_native_init_one_node is wrong.
    
    icp_native_init() is only referenced in `arch/powerpc/sysdev/xics/xics-common.c'
    by xics_init() which is itself marked with __init.
    
    = not built-tested =
    
    Reported-by: Timur Tabi <timur@freescale.com>
    Signed-off-by: Arnaud Lacombe <lacombar@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 50e32afe392e..4c79b6fbee1c 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -276,7 +276,7 @@ static const struct icp_ops icp_native_ops = {
 #endif
 };
 
-int icp_native_init(void)
+int __init icp_native_init(void)
 {
 	struct device_node *np;
 	u32 indx = 0;

commit d3ec4844d449cf7af9e749f73ba2052fb7b72fc2
Merge: 0003230e8200 df2e301fee3c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 13:56:39 2011 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (43 commits)
      fs: Merge split strings
      treewide: fix potentially dangerous trailing ';' in #defined values/expressions
      uwb: Fix misspelling of neighbourhood in comment
      net, netfilter: Remove redundant goto in ebt_ulog_packet
      trivial: don't touch files that are removed in the staging tree
      lib/vsprintf: replace link to Draft by final RFC number
      doc: Kconfig: `to be' -> `be'
      doc: Kconfig: Typo: square -> squared
      doc: Konfig: Documentation/power/{pm => apm-acpi}.txt
      drivers/net: static should be at beginning of declaration
      drivers/media: static should be at beginning of declaration
      drivers/i2c: static should be at beginning of declaration
      XTENSA: static should be at beginning of declaration
      SH: static should be at beginning of declaration
      MIPS: static should be at beginning of declaration
      ARM: static should be at beginning of declaration
      rcu: treewide: Do not use rcu_read_lock_held when calling rcu_dereference_check
      Update my e-mail address
      PCIe ASPM: forcedly -> forcibly
      gma500: push through device driver tree
      ...
    
    Fix up trivial conflicts:
     - arch/arm/mach-ep93xx/dma-m2p.c (deleted)
     - drivers/gpio/gpio-ep93xx.c (renamed and context nearby)
     - drivers/net/r8169.c (just context changes)

commit 371fefd6f2dc46668e00871930dde613b88d4bde
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:23:08 2011 +0000

    KVM: PPC: Allow book3s_hv guests to use SMT processor modes
    
    This lifts the restriction that book3s_hv guests can only run one
    hardware thread per core, and allows them to use up to 4 threads
    per core on POWER7.  The host still has to run single-threaded.
    
    This capability is advertised to qemu through a new KVM_CAP_PPC_SMT
    capability.  The return value of the ioctl querying this capability
    is the number of vcpus per virtual CPU core (vcore), currently 4.
    
    To use this, the host kernel should be booted with all threads
    active, and then all the secondary threads should be offlined.
    This will put the secondary threads into nap mode.  KVM will then
    wake them from nap mode and use them for running guest code (while
    they are still offline).  To wake the secondary threads, we send
    them an IPI using a new xics_wake_cpu() function, implemented in
    arch/powerpc/sysdev/xics/icp-native.c.  In other words, at this stage
    we assume that the platform has a XICS interrupt controller and
    we are using icp-native.c to drive it.  Since the woken thread will
    need to acknowledge and clear the IPI, we also export the base
    physical address of the XICS registers using kvmppc_set_xics_phys()
    for use in the low-level KVM book3s code.
    
    When a vcpu is created, it is assigned to a virtual CPU core.
    The vcore number is obtained by dividing the vcpu number by the
    number of threads per core in the host.  This number is exported
    to userspace via the KVM_CAP_PPC_SMT capability.  If qemu wishes
    to run the guest in single-threaded mode, it should make all vcpu
    numbers be multiples of the number of threads per core.
    
    We distinguish three states of a vcpu: runnable (i.e., ready to execute
    the guest), blocked (that is, idle), and busy in host.  We currently
    implement a policy that the vcore can run only when all its threads
    are runnable or blocked.  This way, if a vcpu needs to execute elsewhere
    in the kernel or in qemu, it can do so without being starved of CPU
    by the other vcpus.
    
    When a vcore starts to run, it executes in the context of one of the
    vcpu threads.  The other vcpu threads all go to sleep and stay asleep
    until something happens requiring the vcpu thread to return to qemu,
    or to wake up to run the vcore (this can happen when another vcpu
    thread goes from busy in host state to blocked).
    
    It can happen that a vcpu goes from blocked to runnable state (e.g.
    because of an interrupt), and the vcore it belongs to is already
    running.  In that case it can start to run immediately as long as
    the none of the vcpus in the vcore have started to exit the guest.
    We send the next free thread in the vcore an IPI to get it to start
    to execute the guest.  It synchronizes with the other threads via
    the vcore->entry_exit_count field to make sure that it doesn't go
    into the guest if the other vcpus are exiting by the time that it
    is ready to actually enter the guest.
    
    Note that there is no fixed relationship between the hardware thread
    number and the vcpu number.  Hardware threads are assigned to vcpus
    as they become runnable, so we will always use the lower-numbered
    hardware threads in preference to higher-numbered threads if not all
    the vcpus in the vcore are runnable, regardless of which vcpus are
    runnable.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 1f15ad436140..ba382b59b926 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -17,6 +17,7 @@
 #include <linux/cpu.h>
 #include <linux/of.h>
 #include <linux/spinlock.h>
+#include <linux/module.h>
 
 #include <asm/prom.h>
 #include <asm/io.h>
@@ -24,6 +25,7 @@
 #include <asm/irq.h>
 #include <asm/errno.h>
 #include <asm/xics.h>
+#include <asm/kvm_ppc.h>
 
 struct icp_ipl {
 	union {
@@ -139,6 +141,12 @@ static void icp_native_cause_ipi(int cpu, unsigned long data)
 	icp_native_set_qirr(cpu, IPI_PRIORITY);
 }
 
+void xics_wake_cpu(int cpu)
+{
+	icp_native_set_qirr(cpu, IPI_PRIORITY);
+}
+EXPORT_SYMBOL_GPL(xics_wake_cpu);
+
 static irqreturn_t icp_native_ipi_action(int irq, void *dev_id)
 {
 	int cpu = smp_processor_id();
@@ -185,6 +193,7 @@ static int __init icp_native_map_one_cpu(int hw_id, unsigned long addr,
 	}
 
 	icp_native_regs[cpu] = ioremap(addr, size);
+	kvmppc_set_xics_phys(cpu, addr);
 	if (!icp_native_regs[cpu]) {
 		pr_warning("icp_native: Failed ioremap for CPU %d, "
 			   "interrupt server #0x%x, addr %#lx\n",

commit 28f65c11f2ffb3957259dece647a24f8ad2e241b
Author: Joe Perches <joe@perches.com>
Date:   Thu Jun 9 09:13:32 2011 -0700

    treewide: Convert uses of struct resource to resource_size(ptr)
    
    Several fixes as well where the +1 was missing.
    
    Done via coccinelle scripts like:
    
    @@
    struct resource *ptr;
    @@
    
    - ptr->end - ptr->start + 1
    + resource_size(ptr)
    
    and some grep and typing.
    
    Mostly uncompiled, no cross-compilers.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 1f15ad436140..039a7820ba7f 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -247,7 +247,7 @@ static int __init icp_native_init_one_node(struct device_node *np,
 			return -1;
 		}
 
-		if (icp_native_map_one_cpu(*indx, r.start, r.end - r.start))
+		if (icp_native_map_one_cpu(*indx, r.start, resource_size(&r)))
 			return -1;
 
 		(*indx)++;

commit 23d72bfd8f9f24aa9efafed3586a99f5669c23d7
Author: Milton Miller <miltonm@bga.com>
Date:   Tue May 10 19:29:39 2011 +0000

    powerpc: Consolidate ipi message mux and demux
    
    Consolidate the mux and demux of ipi messages into smp.c and call
    a new smp_ops callback to actually trigger the ipi.
    
    The powerpc architecture code is optimised for having 4 distinct
    ipi triggers, which are mapped to 4 distinct messages (ipi many, ipi
    single, scheduler ipi, and enter debugger).  However, several interrupt
    controllers only provide a single software triggered interrupt that
    can be delivered to each cpu.  To resolve this limitation, each smp_ops
    implementation created a per-cpu variable that is manipulated with atomic
    bitops.  Since these lines will be contended they are optimialy marked as
    shared_aligned and take a full cache line for each cpu.  Distro kernels
    may have 2 or 3 of these in their config, each taking per-cpu space
    even though at most one will be in use.
    
    This consolidation removes smp_message_recv and replaces the single call
    actions cases with direct calls from the common message recognition loop.
    The complicated debugger ipi case with its muxed crash handling code is
    moved to debug_ipi_action which is now called from the demux code (instead
    of the multi-message action calling smp_message_recv).
    
    I put a call to reschedule_action to increase the likelyhood of correctly
    merging the anticipated scheduler_ipi() hook coming from the scheduler
    tree; that single required call can be inlined later.
    
    The actual message decode is a copy of the old pseries xics code with its
    memory barriers and cache line spacing, augmented with a per-cpu unsigned
    long based on the book-e doorbell code.  The optional data is set via a
    callback from the implementation and is passed to the new cause-ipi hook
    along with the logical cpu number.  While currently only the doorbell
    implemntation uses this data it should be almost zero cost to retrieve and
    pass it -- it adds a single register load for the argument from the same
    cache line to which we just completed a store and the register is dead
    on return from the call.  I extended the data element from unsigned int
    to unsigned long in case some other code wanted to associate a pointer.
    
    The doorbell check_self is replaced by a call to smp_muxed_ipi_resend,
    conditioned on the CPU_DBELL feature.  The ifdef guard could be relaxed
    to CONFIG_SMP but I left it with BOOKE for now.
    
    Also, the doorbell interrupt vector for book-e was not calling irq_enter
    and irq_exit, which throws off cpu accounting and causes code to not
    realize it is running in interrupt context.  Add the missing calls.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 246500eefbfd..1f15ad436140 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -134,12 +134,8 @@ static unsigned int icp_native_get_irq(void)
 
 #ifdef CONFIG_SMP
 
-static void icp_native_message_pass(int cpu, int msg)
+static void icp_native_cause_ipi(int cpu, unsigned long data)
 {
-	unsigned long *tgt = &per_cpu(xics_ipi_message, cpu);
-
-	set_bit(msg, tgt);
-	mb();
 	icp_native_set_qirr(cpu, IPI_PRIORITY);
 }
 
@@ -149,7 +145,7 @@ static irqreturn_t icp_native_ipi_action(int irq, void *dev_id)
 
 	icp_native_set_qirr(cpu, 0xff);
 
-	return xics_ipi_dispatch(cpu);
+	return smp_ipi_demux();
 }
 
 #endif /* CONFIG_SMP */
@@ -267,7 +263,7 @@ static const struct icp_ops icp_native_ops = {
 	.flush_ipi	= icp_native_flush_ipi,
 #ifdef CONFIG_SMP
 	.ipi_action	= icp_native_ipi_action,
-	.message_pass	= icp_native_message_pass,
+	.cause_ipi	= icp_native_cause_ipi,
 #endif
 };
 

commit f1072939b6dd01d038d47db0bdc01b33e5f90f28
Author: Milton Miller <miltonm@bga.com>
Date:   Tue May 10 19:29:10 2011 +0000

    powerpc: Remove checks for MSG_ALL and MSG_ALL_BUT_SELF
    
    Now that smp_ops->smp_message_pass is always called with an (online) cpu
    number for the target remove the checks for MSG_ALL and MSG_ALL_BUT_SELF.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index 3508321c4501..246500eefbfd 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -134,7 +134,7 @@ static unsigned int icp_native_get_irq(void)
 
 #ifdef CONFIG_SMP
 
-static inline void icp_native_do_message(int cpu, int msg)
+static void icp_native_message_pass(int cpu, int msg)
 {
 	unsigned long *tgt = &per_cpu(xics_ipi_message, cpu);
 
@@ -143,22 +143,6 @@ static inline void icp_native_do_message(int cpu, int msg)
 	icp_native_set_qirr(cpu, IPI_PRIORITY);
 }
 
-static void icp_native_message_pass(int target, int msg)
-{
-	unsigned int i;
-
-	if (target < NR_CPUS) {
-		icp_native_do_message(target, msg);
-	} else {
-		for_each_online_cpu(i) {
-			if (target == MSG_ALL_BUT_SELF
-			    && i == smp_processor_id())
-				continue;
-			icp_native_do_message(i, msg);
-		}
-	}
-}
-
 static irqreturn_t icp_native_ipi_action(int irq, void *dev_id)
 {
 	int cpu = smp_processor_id();

commit a1d0d98daf6ce580d017a43b09fe30a375cde3e8
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Thu Apr 14 22:32:06 2011 +0000

    powerpc: Add WSP platform
    
    Add a platform for the Wire Speed Processor, based on the PPC A2.
    
    This includes code for the ICS & OPB interrupt controllers, as well
    as a SCOM backend, and SCOM based cpu bringup.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Jack Miller <jack@codezen.org>
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index d9e0515592c4..3508321c4501 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -7,6 +7,7 @@
  *  2 of the License, or (at your option) any later version.
  *
  */
+
 #include <linux/types.h>
 #include <linux/kernel.h>
 #include <linux/irq.h>

commit 476eb4912601a8c01e6702b9a029f476b4b131d2
Author: Grant Likely <grant.likely@secretlab.ca>
Date:   Wed May 4 15:02:15 2011 +1000

    powerpc/irq: Stop exporting irq_map
    
    First step in eliminating irq_map[] table entirely
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
index be5e3d748edb..d9e0515592c4 100644
--- a/arch/powerpc/sysdev/xics/icp-native.c
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -80,7 +80,7 @@ static void icp_native_set_cpu_priority(unsigned char cppr)
 
 static void icp_native_eoi(struct irq_data *d)
 {
-	unsigned int hw_irq = (unsigned int)irq_data_to_hw(d);
+	unsigned int hw_irq = (unsigned int)irqd_to_hwirq(d);
 
 	iosync();
 	icp_native_set_xirr((xics_pop_cppr() << 24) | hw_irq);

commit 0b05ac6e24807f0c26f763b3a546c0bcbf84125f
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Apr 4 13:46:58 2011 +1000

    powerpc/xics: Rewrite XICS driver
    
    This is a significant rework of the XICS driver, too significant to
    conveniently break it up into a series of smaller patches to be honest.
    
    The driver is moved to a more generic location to allow new platforms
    to use it, and is broken up into separate ICP and ICS "backends". For
    now we have the native and "hypervisor" ICP backends and one common
    RTAS ICS backend.
    
    The driver supports one ICP backend instanciation, and many ICS ones,
    in order to accomodate future platforms with multiple possibly different
    interrupt "sources" mechanisms.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/sysdev/xics/icp-native.c b/arch/powerpc/sysdev/xics/icp-native.c
new file mode 100644
index 000000000000..be5e3d748edb
--- /dev/null
+++ b/arch/powerpc/sysdev/xics/icp-native.c
@@ -0,0 +1,312 @@
+/*
+ * Copyright 2011 IBM Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or
+ *  modify it under the terms of the GNU General Public License
+ *  as published by the Free Software Foundation; either version
+ *  2 of the License, or (at your option) any later version.
+ *
+ */
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/irq.h>
+#include <linux/smp.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/cpu.h>
+#include <linux/of.h>
+#include <linux/spinlock.h>
+
+#include <asm/prom.h>
+#include <asm/io.h>
+#include <asm/smp.h>
+#include <asm/irq.h>
+#include <asm/errno.h>
+#include <asm/xics.h>
+
+struct icp_ipl {
+	union {
+		u32 word;
+		u8 bytes[4];
+	} xirr_poll;
+	union {
+		u32 word;
+		u8 bytes[4];
+	} xirr;
+	u32 dummy;
+	union {
+		u32 word;
+		u8 bytes[4];
+	} qirr;
+	u32 link_a;
+	u32 link_b;
+	u32 link_c;
+};
+
+static struct icp_ipl __iomem *icp_native_regs[NR_CPUS];
+
+static inline unsigned int icp_native_get_xirr(void)
+{
+	int cpu = smp_processor_id();
+
+	return in_be32(&icp_native_regs[cpu]->xirr.word);
+}
+
+static inline void icp_native_set_xirr(unsigned int value)
+{
+	int cpu = smp_processor_id();
+
+	out_be32(&icp_native_regs[cpu]->xirr.word, value);
+}
+
+static inline void icp_native_set_cppr(u8 value)
+{
+	int cpu = smp_processor_id();
+
+	out_8(&icp_native_regs[cpu]->xirr.bytes[0], value);
+}
+
+static inline void icp_native_set_qirr(int n_cpu, u8 value)
+{
+	out_8(&icp_native_regs[n_cpu]->qirr.bytes[0], value);
+}
+
+static void icp_native_set_cpu_priority(unsigned char cppr)
+{
+	xics_set_base_cppr(cppr);
+	icp_native_set_cppr(cppr);
+	iosync();
+}
+
+static void icp_native_eoi(struct irq_data *d)
+{
+	unsigned int hw_irq = (unsigned int)irq_data_to_hw(d);
+
+	iosync();
+	icp_native_set_xirr((xics_pop_cppr() << 24) | hw_irq);
+}
+
+static void icp_native_teardown_cpu(void)
+{
+	int cpu = smp_processor_id();
+
+	/* Clear any pending IPI */
+	icp_native_set_qirr(cpu, 0xff);
+}
+
+static void icp_native_flush_ipi(void)
+{
+	/* We take the ipi irq but and never return so we
+	 * need to EOI the IPI, but want to leave our priority 0
+	 *
+	 * should we check all the other interrupts too?
+	 * should we be flagging idle loop instead?
+	 * or creating some task to be scheduled?
+	 */
+
+	icp_native_set_xirr((0x00 << 24) | XICS_IPI);
+}
+
+static unsigned int icp_native_get_irq(void)
+{
+	unsigned int xirr = icp_native_get_xirr();
+	unsigned int vec = xirr & 0x00ffffff;
+	unsigned int irq;
+
+	if (vec == XICS_IRQ_SPURIOUS)
+		return NO_IRQ;
+
+	irq = irq_radix_revmap_lookup(xics_host, vec);
+	if (likely(irq != NO_IRQ)) {
+		xics_push_cppr(vec);
+		return irq;
+	}
+
+	/* We don't have a linux mapping, so have rtas mask it. */
+	xics_mask_unknown_vec(vec);
+
+	/* We might learn about it later, so EOI it */
+	icp_native_set_xirr(xirr);
+
+	return NO_IRQ;
+}
+
+#ifdef CONFIG_SMP
+
+static inline void icp_native_do_message(int cpu, int msg)
+{
+	unsigned long *tgt = &per_cpu(xics_ipi_message, cpu);
+
+	set_bit(msg, tgt);
+	mb();
+	icp_native_set_qirr(cpu, IPI_PRIORITY);
+}
+
+static void icp_native_message_pass(int target, int msg)
+{
+	unsigned int i;
+
+	if (target < NR_CPUS) {
+		icp_native_do_message(target, msg);
+	} else {
+		for_each_online_cpu(i) {
+			if (target == MSG_ALL_BUT_SELF
+			    && i == smp_processor_id())
+				continue;
+			icp_native_do_message(i, msg);
+		}
+	}
+}
+
+static irqreturn_t icp_native_ipi_action(int irq, void *dev_id)
+{
+	int cpu = smp_processor_id();
+
+	icp_native_set_qirr(cpu, 0xff);
+
+	return xics_ipi_dispatch(cpu);
+}
+
+#endif /* CONFIG_SMP */
+
+static int __init icp_native_map_one_cpu(int hw_id, unsigned long addr,
+					 unsigned long size)
+{
+	char *rname;
+	int i, cpu = -1;
+
+	/* This may look gross but it's good enough for now, we don't quite
+	 * have a hard -> linux processor id matching.
+	 */
+	for_each_possible_cpu(i) {
+		if (!cpu_present(i))
+			continue;
+		if (hw_id == get_hard_smp_processor_id(i)) {
+			cpu = i;
+			break;
+		}
+	}
+
+	/* Fail, skip that CPU. Don't print, it's normal, some XICS come up
+	 * with way more entries in there than you have CPUs
+	 */
+	if (cpu == -1)
+		return 0;
+
+	rname = kasprintf(GFP_KERNEL, "CPU %d [0x%x] Interrupt Presentation",
+			  cpu, hw_id);
+
+	if (!request_mem_region(addr, size, rname)) {
+		pr_warning("icp_native: Could not reserve ICP MMIO"
+			   " for CPU %d, interrupt server #0x%x\n",
+			   cpu, hw_id);
+		return -EBUSY;
+	}
+
+	icp_native_regs[cpu] = ioremap(addr, size);
+	if (!icp_native_regs[cpu]) {
+		pr_warning("icp_native: Failed ioremap for CPU %d, "
+			   "interrupt server #0x%x, addr %#lx\n",
+			   cpu, hw_id, addr);
+		release_mem_region(addr, size);
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+static int __init icp_native_init_one_node(struct device_node *np,
+					   unsigned int *indx)
+{
+	unsigned int ilen;
+	const u32 *ireg;
+	int i;
+	int reg_tuple_size;
+	int num_servers = 0;
+
+	/* This code does the theorically broken assumption that the interrupt
+	 * server numbers are the same as the hard CPU numbers.
+	 * This happens to be the case so far but we are playing with fire...
+	 * should be fixed one of these days. -BenH.
+	 */
+	ireg = of_get_property(np, "ibm,interrupt-server-ranges", &ilen);
+
+	/* Do that ever happen ? we'll know soon enough... but even good'old
+	 * f80 does have that property ..
+	 */
+	WARN_ON((ireg == NULL) || (ilen != 2*sizeof(u32)));
+
+	if (ireg) {
+		*indx = of_read_number(ireg, 1);
+		if (ilen >= 2*sizeof(u32))
+			num_servers = of_read_number(ireg + 1, 1);
+	}
+
+	ireg = of_get_property(np, "reg", &ilen);
+	if (!ireg) {
+		pr_err("icp_native: Can't find interrupt reg property");
+		return -1;
+	}
+
+	reg_tuple_size = (of_n_addr_cells(np) + of_n_size_cells(np)) * 4;
+	if (((ilen % reg_tuple_size) != 0)
+	    || (num_servers && (num_servers != (ilen / reg_tuple_size)))) {
+		pr_err("icp_native: ICP reg len (%d) != num servers (%d)",
+		       ilen / reg_tuple_size, num_servers);
+		return -1;
+	}
+
+	for (i = 0; i < (ilen / reg_tuple_size); i++) {
+		struct resource r;
+		int err;
+
+		err = of_address_to_resource(np, i, &r);
+		if (err) {
+			pr_err("icp_native: Could not translate ICP MMIO"
+			       " for interrupt server 0x%x (%d)\n", *indx, err);
+			return -1;
+		}
+
+		if (icp_native_map_one_cpu(*indx, r.start, r.end - r.start))
+			return -1;
+
+		(*indx)++;
+	}
+	return 0;
+}
+
+static const struct icp_ops icp_native_ops = {
+	.get_irq	= icp_native_get_irq,
+	.eoi		= icp_native_eoi,
+	.set_priority	= icp_native_set_cpu_priority,
+	.teardown_cpu	= icp_native_teardown_cpu,
+	.flush_ipi	= icp_native_flush_ipi,
+#ifdef CONFIG_SMP
+	.ipi_action	= icp_native_ipi_action,
+	.message_pass	= icp_native_message_pass,
+#endif
+};
+
+int icp_native_init(void)
+{
+	struct device_node *np;
+	u32 indx = 0;
+	int found = 0;
+
+	for_each_compatible_node(np, NULL, "ibm,ppc-xicp")
+		if (icp_native_init_one_node(np, &indx) == 0)
+			found = 1;
+	if (!found) {
+		for_each_node_by_type(np,
+			"PowerPC-External-Interrupt-Presentation") {
+				if (icp_native_init_one_node(np, &indx) == 0)
+					found = 1;
+		}
+	}
+
+	if (found == 0)
+		return -ENODEV;
+
+	icp_ops = &icp_native_ops;
+
+	return 0;
+}
