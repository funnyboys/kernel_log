commit 3cf5d076fb4d48979f382bc9452765bf8b79e740
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 23 10:17:27 2019 -0500

    signal: Remove task parameter from force_sig
    
    All of the remaining callers pass current into force_sig so
    remove the task parameter to make this obvious and to make
    misuse more difficult in the future.
    
    This also makes it clear force_sig passes current into force_sig_info.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 07f82d7395ff..3f2380f40f99 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -443,7 +443,7 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 
 	else if (unlikely((status & SPU_STATUS_STOPPED_BY_STOP)
 	    && (status >> SPU_STOP_STATUS_SHIFT) == 0x3fff)) {
-		force_sig(SIGTRAP, current);
+		force_sig(SIGTRAP);
 		ret = -ERESTARTSYS;
 	}
 

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index cfacbee24d7b..07f82d7395ff 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 #define DEBUG
 
 #include <linux/wait.h>

commit 027dfac694fc27ef0273afb810d9b1f9da57d6e1
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jun 1 16:34:37 2016 +1000

    powerpc: Various typo fixes
    
    Signed-off-by: Andrea Gelmini <andrea.gelmini@gelma.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 9f79004e6d6f..cfacbee24d7b 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -435,7 +435,7 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 
 	/* Note: we don't need to force_sig SIGTRAP on single-step
 	 * since we have TIF_SINGLESTEP set, thus the kernel will do
-	 * it upon return from the syscall anyawy
+	 * it upon return from the syscall anyway.
 	 */
 	if (unlikely(status & SPU_STATUS_SINGLE_STEP))
 		ret = -ERESTARTSYS;

commit f43194e45852b0455d2a3e3730f70daa76958423
Author: Rashmica Gupta <rashmicy@gmail.com>
Date:   Thu Nov 19 17:04:53 2015 +1100

    powerpc: Standardise on NR_syscalls rather than __NR_syscalls.
    
    Most architectures use NR_syscalls as the #define for the number of syscalls.
    
    We use __NR_syscalls, and then define NR_syscalls as __NR_syscalls.
    
    __NR_syscalls is not used outside arch code, whereas NR_syscalls is. So as
    NR_syscalls must be defined and __NR_syscalls does not, replace __NR_syscalls
    with NR_syscalls.
    
    Signed-off-by: Rashmica Gupta <rashmicy@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 4ddf769a64e5..9f79004e6d6f 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -326,7 +326,7 @@ static int spu_process_callback(struct spu_context *ctx)
 	spu_ret = -ENOSYS;
 	npc += 4;
 
-	if (s.nr_ret < __NR_syscalls) {
+	if (s.nr_ret < NR_syscalls) {
 		spu_release(ctx);
 		/* do actual system call from here */
 		spu_ret = spu_sys_callback(&s);

commit 3688b46b89a1a64d5eeb0df5f9d82e109712fffe
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Tue Feb 17 11:44:14 2009 +1100

    powerpc/spufs: Clear purge status before setting up isolated mode
    
    Currently, we may setup the MFC for isolated mode initilaisation with
    the purge still active. This means that DMAs required to perform the
    init do not happen.
    
    This change clears the purge status after doing the purge, so that
    the isolated init can proceed.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index c58bd36b0c5b..4ddf769a64e5 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -117,6 +117,9 @@ static int spu_setup_isolated(struct spu_context *ctx)
 		cond_resched();
 	}
 
+	/* clear purge status */
+	out_be64(mfc_cntl, 0);
+
 	/* put the SPE in kernel mode to allow access to the loader */
 	sr1 = spu_mfc_sr1_get(ctx->spu);
 	sr1 &= ~MFC_STATE1_PROBLEM_STATE_MASK;

commit f5ed0eb6fe131e8f3847323b4aa569a6f7b36f56
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Thu Oct 16 10:03:46 2008 +1100

    powerpc/spufs: Use state_mutex for switch_log locking, and prevent multiple openers
    
    Currently, we use ctx->mapping_lock and ctx->switch_log->lock for the
    context switch log. The mapping lock only prevents concurrent open()s,
    so we require the switch_lock->lock for reads.
    
    Since writes to the switch log buffer occur on context switches, we're
    better off synchronising with the state_mutex, which is held during a
    switch. Since we're serialised througout the buffer reads and writes,
    we can use the state mutex to protect open and release too, and
    can now kfree() the log buffer on release. This allows us to perform
    the switch log notify without taking any extra locks.
    
    Because the buffer is only present while the file is open, we can use
    it to prevent multiple simultaneous openers.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index c9bb7cfd3dca..c58bd36b0c5b 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -249,6 +249,7 @@ static int spu_run_fini(struct spu_context *ctx, u32 *npc,
 
 	spuctx_switch_state(ctx, SPU_UTIL_IDLE_LOADED);
 	clear_bit(SPU_SCHED_SPU_RUN, &ctx->sched_flags);
+	spu_switch_log_notify(NULL, ctx, SWITCH_LOG_EXIT, *status);
 	spu_release(ctx);
 
 	if (signal_pending(current))
@@ -417,8 +418,6 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 	ret = spu_run_fini(ctx, npc, &status);
 	spu_yield(ctx);
 
-	spu_switch_log_notify(NULL, ctx, SWITCH_LOG_EXIT, status);
-
 	if ((status & SPU_STATUS_STOPPED_BY_STOP) &&
 	    (((status >> SPU_STOP_STATUS_SHIFT) & 0x3f00) == 0x2100))
 		ctx->stats.libassist++;

commit d9dd421fd6ed17af55d27c8d93a9f561be0ff50f
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Wed Aug 13 11:29:31 2008 +1000

    powerpc/spufs: fix npc setting for NOSCHED contexts
    
    Currently, spu_run ignores the npc argument for contexts created with
    SPU_CREATE_NOSCHED. While this is correct for isolated contexts,
    there's no need to enforce the npc restriction on non-isolated NOSCHED
    contexts.
    
    This means that NOSCHED contexts can only ever run with an entry point
    of 0x0.
    
    This change to spu_run_init allows setting of the npc (and, while we're
    at it, the privcntl) for non-isolated NOSCHED contexts. This allows
    us to run NOSCHED contexts from any entry point.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index f7edba6cb795..c9bb7cfd3dca 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -206,11 +206,6 @@ static int spu_run_init(struct spu_context *ctx, u32 *npc)
 			(SPU_RUNCNTL_RUNNABLE | SPU_RUNCNTL_ISOLATE);
 		if (runcntl == 0)
 			runcntl = SPU_RUNCNTL_RUNNABLE;
-	}
-
-	if (ctx->flags & SPU_CREATE_NOSCHED) {
-		spuctx_switch_state(ctx, SPU_UTIL_USER);
-		ctx->ops->runcntl_write(ctx, runcntl);
 	} else {
 		unsigned long privcntl;
 
@@ -219,9 +214,15 @@ static int spu_run_init(struct spu_context *ctx, u32 *npc)
 		else
 			privcntl = SPU_PRIVCNTL_MODE_NORMAL;
 
-		ctx->ops->npc_write(ctx, *npc);
 		ctx->ops->privcntl_write(ctx, privcntl);
-		ctx->ops->runcntl_write(ctx, runcntl);
+		ctx->ops->npc_write(ctx, *npc);
+	}
+
+	ctx->ops->runcntl_write(ctx, runcntl);
+
+	if (ctx->flags & SPU_CREATE_NOSCHED) {
+		spuctx_switch_state(ctx, SPU_UTIL_USER);
+	} else {
 
 		if (ctx->state == SPU_STATE_SAVED) {
 			ret = spu_activate(ctx, 0);

commit 1f64643aa5f5a17f1723f7ea0f17b7a3a8f632b3
Author: Luke Browning <lukebrowning@us.ibm.com>
Date:   Thu Jun 5 17:30:25 2008 +0800

    powerpc/spufs: remove class_0_dsisr from spu exception handling
    
    According to the CBEA, the SPU dsisr is not updated for class 0
    exceptions.
    
    spu_stopped() is testing the dsisr that was passed to it from the class
    0 exception handler, so we return a false positive here.
    
    This patch cleans up the interrupt handler and erroneous tests in
    spu_stopped. It also removes the fields from the csa since it is not
    needed to process class 0 events.
    
    Signed-off-by: Luke Browning <lukebrowning@us.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 0046bcfe495a..f7edba6cb795 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -27,7 +27,6 @@ void spufs_stop_callback(struct spu *spu, int irq)
 		switch(irq) {
 		case 0 :
 			ctx->csa.class_0_pending = spu->class_0_pending;
-			ctx->csa.class_0_dsisr = spu->class_0_dsisr;
 			ctx->csa.class_0_dar = spu->class_0_dar;
 			break;
 		case 1 :
@@ -69,10 +68,6 @@ int spu_stopped(struct spu_context *ctx, u32 *stat)
 	if (test_bit(SPU_SCHED_NOTIFY_ACTIVE, &ctx->sched_flags))
 		return 1;
 
-	dsisr = ctx->csa.class_0_dsisr;
-	if (dsisr & (MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED))
-		return 1;
-
 	dsisr = ctx->csa.class_1_dsisr;
 	if (dsisr & (MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED))
 		return 1;

commit d84050f48ebba73994b93ccf61cea2364dac8d75
Author: Luke Browning <lukebrowning@us.ibm.com>
Date:   Thu May 29 17:46:10 2008 -0300

    powerpc/spufs: wait for stable spu status in spu_stopped()
    
    If the spu is stopping (ie, the SPU_STATUS_RUNNING bit is still set),
    re-read the register to get the final stopped value.
    
    Signed-off-by: Luke Browning <lukebrowning@us.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index b7493b865812..0046bcfe495a 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -51,14 +51,22 @@ int spu_stopped(struct spu_context *ctx, u32 *stat)
 	u64 dsisr;
 	u32 stopped;
 
-	*stat = ctx->ops->status_read(ctx);
+	stopped = SPU_STATUS_INVALID_INSTR | SPU_STATUS_SINGLE_STEP |
+		SPU_STATUS_STOPPED_BY_HALT | SPU_STATUS_STOPPED_BY_STOP;
 
-	if (test_bit(SPU_SCHED_NOTIFY_ACTIVE, &ctx->sched_flags))
+top:
+	*stat = ctx->ops->status_read(ctx);
+	if (*stat & stopped) {
+		/*
+		 * If the spu hasn't finished stopping, we need to
+		 * re-read the register to get the stopped value.
+		 */
+		if (*stat & SPU_STATUS_RUNNING)
+			goto top;
 		return 1;
+	}
 
-	stopped = SPU_STATUS_INVALID_INSTR | SPU_STATUS_SINGLE_STEP |
-		SPU_STATUS_STOPPED_BY_HALT | SPU_STATUS_STOPPED_BY_STOP;
-	if (!(*stat & SPU_STATUS_RUNNING) && (*stat & stopped))
+	if (test_bit(SPU_SCHED_NOTIFY_ACTIVE, &ctx->sched_flags))
 		return 1;
 
 	dsisr = ctx->csa.class_0_dsisr;

commit f3d69e0507f84903059d456c5d19f10b2df3ac69
Author: Luke Browning <lukebr@linux.vnet.ibm.com>
Date:   Sun Apr 27 18:41:55 2008 +0000

    [POWERPC] spufs: fix concurrent delivery of class 0 & 1 exceptions
    
    SPU class 0 & 1 exceptions may occur in parallel, so we may end up
    overwriting csa.dsisr.
    
    This change adds dedicated fields for each class to the spu and the spu
    context so that fault data is not overwritten.
    
    Signed-off-by: Luke Browning <lukebr@linux.vnet.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index e764a43b544e..b7493b865812 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -11,7 +11,7 @@
 #include "spufs.h"
 
 /* interrupt-level stop callback function. */
-void spufs_stop_callback(struct spu *spu)
+void spufs_stop_callback(struct spu *spu, int irq)
 {
 	struct spu_context *ctx = spu->ctx;
 
@@ -24,9 +24,19 @@ void spufs_stop_callback(struct spu *spu)
 	 */
 	if (ctx) {
 		/* Copy exception arguments into module specific structure */
-		ctx->csa.class_0_pending = spu->class_0_pending;
-		ctx->csa.dsisr = spu->dsisr;
-		ctx->csa.dar = spu->dar;
+		switch(irq) {
+		case 0 :
+			ctx->csa.class_0_pending = spu->class_0_pending;
+			ctx->csa.class_0_dsisr = spu->class_0_dsisr;
+			ctx->csa.class_0_dar = spu->class_0_dar;
+			break;
+		case 1 :
+			ctx->csa.class_1_dsisr = spu->class_1_dsisr;
+			ctx->csa.class_1_dar = spu->class_1_dar;
+			break;
+		case 2 :
+			break;
+		}
 
 		/* ensure that the exception status has hit memory before a
 		 * thread waiting on the context's stop queue is woken */
@@ -34,11 +44,6 @@ void spufs_stop_callback(struct spu *spu)
 
 		wake_up_all(&ctx->stop_wq);
 	}
-
-	/* Clear callback arguments from spu structure */
-	spu->class_0_pending = 0;
-	spu->dsisr = 0;
-	spu->dar = 0;
 }
 
 int spu_stopped(struct spu_context *ctx, u32 *stat)
@@ -56,7 +61,11 @@ int spu_stopped(struct spu_context *ctx, u32 *stat)
 	if (!(*stat & SPU_STATUS_RUNNING) && (*stat & stopped))
 		return 1;
 
-	dsisr = ctx->csa.dsisr;
+	dsisr = ctx->csa.class_0_dsisr;
+	if (dsisr & (MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED))
+		return 1;
+
+	dsisr = ctx->csa.class_1_dsisr;
 	if (dsisr & (MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED))
 		return 1;
 

commit d29694f0681e188cf75477f20904041744e78ef7
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Wed Apr 23 16:02:10 2008 +1000

    [POWERPC] spufs: don't acquire state_mutex interruptible while performing callback
    
    There's currently no way to tell if spu_process_callback has
    returned with the state mutex held, as -EINTR may be returned
    by either the syscall or the spu_acquire fail case.
    
    Instead, just do a non-interruptible mutex_lock here.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 0634fecfe54c..e764a43b544e 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -294,7 +294,7 @@ static int spu_process_callback(struct spu_context *ctx)
 	u32 ls_pointer, npc;
 	void __iomem *ls;
 	long spu_ret;
-	int ret, ret2;
+	int ret;
 
 	/* get syscall block from local store */
 	npc = ctx->ops->npc_read(ctx) & ~3;
@@ -316,11 +316,9 @@ static int spu_process_callback(struct spu_context *ctx)
 		if (spu_ret <= -ERESTARTSYS) {
 			ret = spu_handle_restartsys(ctx, &spu_ret, &npc);
 		}
-		ret2 = spu_acquire(ctx);
+		mutex_lock(&ctx->state_mutex);
 		if (ret == -ERESTARTSYS)
 			return ret;
-		if (ret2)
-			return -EINTR;
 	}
 
 	/* need to re-get the ls, as it may have changed when we released the

commit c0bace5c7029ef7cbb6e6ffaa0469a3c042e5029
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Wed Apr 23 14:24:27 2008 +1000

    [POWERPC] spufs: update master runcntl with context lock held
    
    Currently, we update the SPU master run control bit (ie,
    spu_enable_spu) in spufs_run_spu before we grab the context mutex. This
    can result in races with other processes accessing this context's
    resources.
    
    This change moves the spu_enable_spu to after we have acquired the
    context lock.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index a9c35b7b719f..0634fecfe54c 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -343,13 +343,14 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 	if (mutex_lock_interruptible(&ctx->run_mutex))
 		return -ERESTARTSYS;
 
-	spu_enable_spu(ctx);
 	ctx->event_return = 0;
 
 	ret = spu_acquire(ctx);
 	if (ret)
 		goto out_unlock;
 
+	spu_enable_spu(ctx);
+
 	spu_update_sched_info(ctx);
 
 	ret = spu_run_init(ctx, npc);

commit 5158e9b5218bd3799c9fa8c401ad24d7f0c0a0a1
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 29 17:08:38 2008 +1000

    [POWERPC] spufs: add context switch notification log
    
    There are userspace instrumentation tools that need to monitor spu
    context switches. This patch adds a new file called 'switch_log' to
    each spufs context directory that can be used to monitor the context
    switches.
    
    Context switch in/out and exit from spu_run are monitored after the
    file was first opened and can be read from it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 96bf7c2b86fc..a9c35b7b719f 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -405,6 +405,8 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 	ret = spu_run_fini(ctx, npc, &status);
 	spu_yield(ctx);
 
+	spu_switch_log_notify(NULL, ctx, SWITCH_LOG_EXIT, status);
+
 	if ((status & SPU_STATUS_STOPPED_BY_STOP) &&
 	    (((status >> SPU_STOP_STATUS_SHIFT) & 0x3f00) == 0x2100))
 		ctx->stats.libassist++;

commit 858c52d15f186e055dc33a1872f5b45a66d42296
Merge: 771168494719 ada397e93d60
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Apr 18 13:33:43 2008 +1000

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/jk/spufs

commit e48b1b452ff630288c930fd8e0c2d808bc15f7ad
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Sat Mar 29 08:21:07 2008 +1100

    [POWERPC] Replace remaining __FUNCTION__ occurrences
    
    __FUNCTION__ is gcc-specific, use __func__
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index cac69e116776..5986d08115af 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -98,7 +98,7 @@ static int spu_setup_isolated(struct spu_context *ctx)
 			!= MFC_CNTL_PURGE_DMA_COMPLETE) {
 		if (time_after(jiffies, timeout)) {
 			printk(KERN_ERR "%s: timeout flushing MFC DMA queue\n",
-					__FUNCTION__);
+					__func__);
 			ret = -EIO;
 			goto out;
 		}
@@ -124,7 +124,7 @@ static int spu_setup_isolated(struct spu_context *ctx)
 				status_loading) {
 		if (time_after(jiffies, timeout)) {
 			printk(KERN_ERR "%s: timeout waiting for loader\n",
-					__FUNCTION__);
+					__func__);
 			ret = -EIO;
 			goto out_drop_priv;
 		}
@@ -134,7 +134,7 @@ static int spu_setup_isolated(struct spu_context *ctx)
 	if (!(status & SPU_STATUS_RUNNING)) {
 		/* If isolated LOAD has failed: run SPU, we will get a stop-and
 		 * signal later. */
-		pr_debug("%s: isolated LOAD failed\n", __FUNCTION__);
+		pr_debug("%s: isolated LOAD failed\n", __func__);
 		ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_RUNNABLE);
 		ret = -EACCES;
 		goto out_drop_priv;
@@ -142,7 +142,7 @@ static int spu_setup_isolated(struct spu_context *ctx)
 
 	if (!(status & SPU_STATUS_ISOLATED_STATE)) {
 		/* This isn't allowed by the CBEA, but check anyway */
-		pr_debug("%s: SPU fell out of isolated mode?\n", __FUNCTION__);
+		pr_debug("%s: SPU fell out of isolated mode?\n", __func__);
 		ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_STOP);
 		ret = -EINVAL;
 		goto out_drop_priv;
@@ -282,7 +282,7 @@ static int spu_handle_restartsys(struct spu_context *ctx, long *spu_ret,
 		break;
 	default:
 		printk(KERN_WARNING "%s: unexpected return code %ld\n",
-			__FUNCTION__, *spu_ret);
+			__func__, *spu_ret);
 		ret = 0;
 	}
 	return ret;

commit 4eb5aef583aab29dc125b6c2feb0b31f2e350c45
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Tue Mar 25 13:32:03 2008 +1100

    [POWERPC] spufs: reacquire LS pointer in spu_process_callback
    
    During spu_process callback, we release then acquire the SPU, but keep a
    pointer to the local store memory. Since the context may have been
    scheduled out during the callback, the ls pointer may become invalid.
    
    This change reacquires the pointer to the context local store after
    spu_acquire()-ing, so that it isn't invalidated by a context switch.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index cac69e116776..2619f14546a3 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -323,6 +323,10 @@ static int spu_process_callback(struct spu_context *ctx)
 			return -EINTR;
 	}
 
+	/* need to re-get the ls, as it may have changed when we released the
+	 * spu */
+	ls = (void __iomem *)ctx->ops->get_ls(ctx);
+
 	/* write result, jump over indirect pointer */
 	memcpy_toio(ls + ls_pointer, &spu_ret, sizeof(spu_ret));
 	ctx->ops->npc_write(ctx, npc);

commit ce7c191bca88aa2f942f70a6d6c6315739a81a32
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Tue Mar 4 20:17:02 2008 +1100

    [POWERPC] spufs: don't (ab)use SCHED_IDLE
    
    commit 4ef11014 introduced a usage of SCHED_IDLE to detect when
    a context is within spu_run.
    
    Instead of SCHED_IDLE (which has other meaning), add a flag to
    sched_flags to tell if a context should be running.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 6221968c2a3c..cac69e116776 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -220,6 +220,7 @@ static int spu_run_init(struct spu_context *ctx, u32 *npc)
 		}
 	}
 
+	set_bit(SPU_SCHED_SPU_RUN, &ctx->sched_flags);
 	return 0;
 }
 
@@ -234,7 +235,7 @@ static int spu_run_fini(struct spu_context *ctx, u32 *npc,
 	*npc = ctx->ops->npc_read(ctx);
 
 	spuctx_switch_state(ctx, SPU_UTIL_IDLE_LOADED);
-	ctx->policy = SCHED_IDLE;
+	clear_bit(SPU_SCHED_SPU_RUN, &ctx->sched_flags);
 	spu_release(ctx);
 
 	if (signal_pending(current))

commit 4ef110141b3e0758fe30d686417b5686b87eb25b
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Tue Feb 19 10:05:35 2008 +1100

    [POWERPC] spufs: fix scheduler starvation by idle contexts
    
    2.6.25 has a regression where we can starve the scheduler by creating
    (N_SPES+1) contexts, then running them one at a time.
    
    The final context will never be run, as the other contexts are loaded on
    the SPEs, none of which are repoted as free (ie, spu->alloc_state !=
    SPU_FREE), so spu_get_idle() doesn't give us a spu to run on. Because
    all of the contexts are stopped, none are descheduled by the scheduler
    tick, as spusched_tick returns if spu_stopped(ctx).
    
    This change replaces the spu_stopped() check with checking for SCHED_IDLE
    in ctx->policy. We set a context's policy to SCHED_IDLE when we're not
    in spu_run(). We also favour SCHED_IDLE contexts when looking for contexts
    to unbind, but leave their timeslice intact for later resumption.
    
    This patch fixes the following test in the spufs-testsuite:
      tests/20-scheduler/02-yield-starvation
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index fca22e18069a..6221968c2a3c 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -234,6 +234,7 @@ static int spu_run_fini(struct spu_context *ctx, u32 *npc,
 	*npc = ctx->ops->npc_read(ctx);
 
 	spuctx_switch_state(ctx, SPU_UTIL_IDLE_LOADED);
+	ctx->policy = SCHED_IDLE;
 	spu_release(ctx);
 
 	if (signal_pending(current))

commit 85687ff2b4eab47f4d637a0d3a482bb955d3cbd4
Author: Luke Browning <lukebr@linux.vnet.ibm.com>
Date:   Fri Feb 8 15:50:41 2008 +1100

    [POWERPC] spufs: Fix timing dependent false return from spufs_run_spu
    
    Stop bits are only valid when the running bit is not set.  Status bits
    carry over from one invocation of spufs_run_spu() to another, so the
    RUNNING bit gets added to the previous state of the register which may
    have been a remote library call.  In this case, it looks like another
    library routine should be invoked, but the spe is actually running.
    
    This fixes a problem with a testcase that exercises the scheduler.
    
    Signed-off-by: Luke Browning <lukebrowning@us.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index f401e51a7979..fca22e18069a 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -53,7 +53,7 @@ int spu_stopped(struct spu_context *ctx, u32 *stat)
 
 	stopped = SPU_STATUS_INVALID_INSTR | SPU_STATUS_SINGLE_STEP |
 		SPU_STATUS_STOPPED_BY_HALT | SPU_STATUS_STOPPED_BY_STOP;
-	if (*stat & stopped)
+	if (!(*stat & SPU_STATUS_RUNNING) && (*stat & stopped))
 		return 1;
 
 	dsisr = ctx->csa.dsisr;

commit e66686b414f10f1ef2cd0aa77a03a67e17304773
Author: Luke Browning <lukebr@linux.vnet.ibm.com>
Date:   Fri Feb 8 15:50:41 2008 +1100

    [POWERPC] spufs: No need to have a runnable SPU for libassist update
    
    We don't need to update the libassist statistic with the context in a
    runnable state, so do it after spu_disable_spu().
    
    Signed-off-by: Luke Browning <lukebrowning@us.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index e9c61a1a8f98..f401e51a7979 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -395,16 +395,14 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 				      SPU_STATUS_STOPPED_BY_HALT |
 				       SPU_STATUS_SINGLE_STEP)));
 
-	if ((status & SPU_STATUS_STOPPED_BY_STOP) &&
-	    (((status >> SPU_STOP_STATUS_SHIFT) & 0x3f00) == 0x2100) &&
-	    (ctx->state == SPU_STATE_RUNNABLE))
-		ctx->stats.libassist++;
-
-
 	spu_disable_spu(ctx);
 	ret = spu_run_fini(ctx, npc, &status);
 	spu_yield(ctx);
 
+	if ((status & SPU_STATUS_STOPPED_BY_STOP) &&
+	    (((status >> SPU_STOP_STATUS_SHIFT) & 0x3f00) == 0x2100))
+		ctx->stats.libassist++;
+
 	if ((ret == 0) ||
 	    ((ret == -ERESTARTSYS) &&
 	     ((status & SPU_STATUS_STOPPED_BY_HALT) ||

commit eebead5b8ff89340dc18ceec996157d0eb7d0287
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Feb 8 15:50:41 2008 +1100

    [POWERPC] spufs: Fix state_mutex leaks
    
    Fix various state_mutex leaks.  The worst one was introduced by the
    interrutible state_mutex conversion but there've been a few before
    too.  Notably spufs_wait now returns without the state_mutex held
    when returning an error, which actually cleans up some code.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Luke Browning <lukebrowning@us.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index b4814c740d8a..e9c61a1a8f98 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -354,8 +354,15 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 
 	do {
 		ret = spufs_wait(ctx->stop_wq, spu_stopped(ctx, &status));
-		if (unlikely(ret))
+		if (unlikely(ret)) {
+			/*
+			 * This is nasty: we need the state_mutex for all the
+			 * bookkeeping even if the syscall was interrupted by
+			 * a signal. ewww.
+			 */
+			mutex_lock(&ctx->state_mutex);
 			break;
+		}
 		spu = ctx->spu;
 		if (unlikely(test_and_clear_bit(SPU_SCHED_NOTIFY_ACTIVE,
 						&ctx->sched_flags))) {

commit 60cf54db47727935af1c439f59c636a96a8cbd4b
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Fri Jan 11 15:03:26 2008 +1100

    [POWERPC] spufs: Fix SPE single-step mode
    
    Currently we only catch debug events through the 0x3fff status;
    spufs_run_spu doesn't handle single-step SPE events.
    
    This change adds a handler for conditions where the SPE is stopped due
    to single-step-mode.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index c01a09da1e56..b4814c740d8a 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -410,8 +410,11 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 	 * since we have TIF_SINGLESTEP set, thus the kernel will do
 	 * it upon return from the syscall anyawy
 	 */
-	if ((status & SPU_STATUS_STOPPED_BY_STOP)
-	    && (status >> SPU_STOP_STATUS_SHIFT) == 0x3fff) {
+	if (unlikely(status & SPU_STATUS_SINGLE_STEP))
+		ret = -ERESTARTSYS;
+
+	else if (unlikely((status & SPU_STATUS_STOPPED_BY_STOP)
+	    && (status >> SPU_STOP_STATUS_SHIFT) == 0x3fff)) {
 		force_sig(SIGTRAP, current);
 		ret = -ERESTARTSYS;
 	}

commit c9101bdb1b0c56a75a4618542d368fe5013946b9
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 20 16:39:59 2007 +0900

    [POWERPC] spufs: make state_mutex interruptible
    
    Make most places that use spu_acquire/spu_acquire_saved interruptible,
    this allows getting out of the spufs code when e.g. pressing ctrl+c.
    There are a few places where we get called e.g. from spufs teardown
    routines were we can't simply err out so these are left with a comment.
    For now I've also not touched the poll routines because it's open what
    libspe would expect in terms of interrupted system calls.
    
    Acked-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index b380050cdbc7..c01a09da1e56 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -292,7 +292,7 @@ static int spu_process_callback(struct spu_context *ctx)
 	u32 ls_pointer, npc;
 	void __iomem *ls;
 	long spu_ret;
-	int ret;
+	int ret, ret2;
 
 	/* get syscall block from local store */
 	npc = ctx->ops->npc_read(ctx) & ~3;
@@ -314,9 +314,11 @@ static int spu_process_callback(struct spu_context *ctx)
 		if (spu_ret <= -ERESTARTSYS) {
 			ret = spu_handle_restartsys(ctx, &spu_ret, &npc);
 		}
-		spu_acquire(ctx);
+		ret2 = spu_acquire(ctx);
 		if (ret == -ERESTARTSYS)
 			return ret;
+		if (ret2)
+			return -EINTR;
 	}
 
 	/* write result, jump over indirect pointer */
@@ -338,7 +340,9 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 	spu_enable_spu(ctx);
 	ctx->event_return = 0;
 
-	spu_acquire(ctx);
+	ret = spu_acquire(ctx);
+	if (ret)
+		goto out_unlock;
 
 	spu_update_sched_info(ctx);
 
@@ -414,6 +418,7 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 
 out:
 	*event = ctx->event_return;
+out_unlock:
 	mutex_unlock(&ctx->run_mutex);
 	return ret;
 }

commit e65c2f6fcebb9af0c3f53c796aff730dd657f5e7
Author: Luke Browning <lukebr@linux.vnet.ibm.com>
Date:   Thu Dec 20 16:39:59 2007 +0900

    [POWERPC] spufs: decouple spu scheduler from spufs_spu_run (asynchronous scheduling)
    
    Change spufs_spu_run so that the context is queued directly to the
    scheduler and the controlling thread advances directly to spufs_wait()
    for spe errors and exceptions.
    
    nosched contexts are treated the same as before.
    
    Fixes from Christoph Hellwig <hch@lst.de>
    
    Signed-off-by: Luke Browning <lukebr@linux.vnet.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 652ae1366dc8..b380050cdbc7 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -41,21 +41,29 @@ void spufs_stop_callback(struct spu *spu)
 	spu->dar = 0;
 }
 
-static inline int spu_stopped(struct spu_context *ctx, u32 *stat)
+int spu_stopped(struct spu_context *ctx, u32 *stat)
 {
-	struct spu *spu;
-	u64 pte_fault;
+	u64 dsisr;
+	u32 stopped;
 
 	*stat = ctx->ops->status_read(ctx);
 
-	spu = ctx->spu;
-	if (ctx->state != SPU_STATE_RUNNABLE ||
-	    test_bit(SPU_SCHED_NOTIFY_ACTIVE, &ctx->sched_flags))
+	if (test_bit(SPU_SCHED_NOTIFY_ACTIVE, &ctx->sched_flags))
+		return 1;
+
+	stopped = SPU_STATUS_INVALID_INSTR | SPU_STATUS_SINGLE_STEP |
+		SPU_STATUS_STOPPED_BY_HALT | SPU_STATUS_STOPPED_BY_STOP;
+	if (*stat & stopped)
+		return 1;
+
+	dsisr = ctx->csa.dsisr;
+	if (dsisr & (MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED))
 		return 1;
-	pte_fault = ctx->csa.dsisr &
-	    (MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED);
-	return (!(*stat & SPU_STATUS_RUNNING) || pte_fault || ctx->csa.class_0_pending) ?
-		1 : 0;
+
+	if (ctx->csa.class_0_pending)
+		return 1;
+
+	return 0;
 }
 
 static int spu_setup_isolated(struct spu_context *ctx)
@@ -151,24 +159,27 @@ static int spu_setup_isolated(struct spu_context *ctx)
 
 static int spu_run_init(struct spu_context *ctx, u32 *npc)
 {
-	unsigned long runcntl;
+	unsigned long runcntl = SPU_RUNCNTL_RUNNABLE;
 	int ret;
 
 	spuctx_switch_state(ctx, SPU_UTIL_SYSTEM);
 
-	if (ctx->flags & SPU_CREATE_ISOLATE) {
-		/*
-		 * Force activation of spu.  Isolated state assumes that
-		 * special loader context is loaded and running on spu.
-		 */
+	/*
+	 * NOSCHED is synchronous scheduling with respect to the caller.
+	 * The caller waits for the context to be loaded.
+	 */
+	if (ctx->flags & SPU_CREATE_NOSCHED) {
 		if (ctx->state == SPU_STATE_SAVED) {
-			spu_set_timeslice(ctx);
-
 			ret = spu_activate(ctx, 0);
 			if (ret)
 				return ret;
 		}
+	}
 
+	/*
+	 * Apply special setup as required.
+	 */
+	if (ctx->flags & SPU_CREATE_ISOLATE) {
 		if (!(ctx->ops->status_read(ctx) & SPU_STATUS_ISOLATED_STATE)) {
 			ret = spu_setup_isolated(ctx);
 			if (ret)
@@ -183,10 +194,11 @@ static int spu_run_init(struct spu_context *ctx, u32 *npc)
 			(SPU_RUNCNTL_RUNNABLE | SPU_RUNCNTL_ISOLATE);
 		if (runcntl == 0)
 			runcntl = SPU_RUNCNTL_RUNNABLE;
+	}
 
+	if (ctx->flags & SPU_CREATE_NOSCHED) {
 		spuctx_switch_state(ctx, SPU_UTIL_USER);
 		ctx->ops->runcntl_write(ctx, runcntl);
-
 	} else {
 		unsigned long privcntl;
 
@@ -194,20 +206,18 @@ static int spu_run_init(struct spu_context *ctx, u32 *npc)
 			privcntl = SPU_PRIVCNTL_MODE_SINGLE_STEP;
 		else
 			privcntl = SPU_PRIVCNTL_MODE_NORMAL;
-		runcntl = SPU_RUNCNTL_RUNNABLE;
 
 		ctx->ops->npc_write(ctx, *npc);
 		ctx->ops->privcntl_write(ctx, privcntl);
+		ctx->ops->runcntl_write(ctx, runcntl);
 
 		if (ctx->state == SPU_STATE_SAVED) {
-			spu_set_timeslice(ctx);
 			ret = spu_activate(ctx, 0);
 			if (ret)
 				return ret;
+		} else {
+			spuctx_switch_state(ctx, SPU_UTIL_USER);
 		}
-
-		spuctx_switch_state(ctx, SPU_UTIL_USER);
-		ctx->ops->runcntl_write(ctx, runcntl);
 	}
 
 	return 0;
@@ -218,6 +228,8 @@ static int spu_run_fini(struct spu_context *ctx, u32 *npc,
 {
 	int ret = 0;
 
+	spu_del_from_rq(ctx);
+
 	*status = ctx->ops->status_read(ctx);
 	*npc = ctx->ops->npc_read(ctx);
 
@@ -230,26 +242,6 @@ static int spu_run_fini(struct spu_context *ctx, u32 *npc,
 	return ret;
 }
 
-static int spu_reacquire_runnable(struct spu_context *ctx, u32 *npc,
-				         u32 *status)
-{
-	int ret;
-
-	ret = spu_run_fini(ctx, npc, status);
-	if (ret)
-		return ret;
-
-	if (*status & (SPU_STATUS_STOPPED_BY_STOP | SPU_STATUS_STOPPED_BY_HALT))
-		return *status;
-
-	ret = spu_acquire_runnable(ctx, 0);
-	if (ret)
-		return ret;
-
-	spuctx_switch_state(ctx, SPU_UTIL_USER);
-	return 0;
-}
-
 /*
  * SPU syscall restarting is tricky because we violate the basic
  * assumption that the signal handler is running on the interrupted
@@ -386,17 +378,8 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 		if (ret)
 			break;
 
-		if (unlikely(ctx->state != SPU_STATE_RUNNABLE)) {
-			ret = spu_reacquire_runnable(ctx, npc, &status);
-			if (ret)
-				goto out2;
-			continue;
-		}
-
 		if (signal_pending(current))
 			ret = -ERESTARTSYS;
-
-
 	} while (!ret && !(status & (SPU_STATUS_STOPPED_BY_STOP |
 				      SPU_STATUS_STOPPED_BY_HALT |
 				       SPU_STATUS_SINGLE_STEP)));
@@ -411,7 +394,6 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 	ret = spu_run_fini(ctx, npc, &status);
 	spu_yield(ctx);
 
-out2:
 	if ((ret == 0) ||
 	    ((ret == -ERESTARTSYS) &&
 	     ((status & SPU_STATUS_STOPPED_BY_HALT) ||

commit 91569531d1297db42d68136ac0c85cd85223d0b9
Author: Luke Browning <lukebr@linux.vnet.ibm.com>
Date:   Thu Dec 20 16:39:59 2007 +0900

    [POWERPC] spufs: reorganize spu_run_init
    
    This cleans up spu_run_init so that it does all of the spu
    initialization for spufs_run_spu.  It initializes the spu context as
    much as possible before it activates the spu and writes the runcntl
    register.
    
    Signed-off-by: Luke Browning <lukebr@linux.vnet.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 3b3de6c7ee5b..652ae1366dc8 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -152,23 +152,41 @@ static int spu_setup_isolated(struct spu_context *ctx)
 static int spu_run_init(struct spu_context *ctx, u32 *npc)
 {
 	unsigned long runcntl;
+	int ret;
 
 	spuctx_switch_state(ctx, SPU_UTIL_SYSTEM);
 
 	if (ctx->flags & SPU_CREATE_ISOLATE) {
+		/*
+		 * Force activation of spu.  Isolated state assumes that
+		 * special loader context is loaded and running on spu.
+		 */
+		if (ctx->state == SPU_STATE_SAVED) {
+			spu_set_timeslice(ctx);
+
+			ret = spu_activate(ctx, 0);
+			if (ret)
+				return ret;
+		}
 
 		if (!(ctx->ops->status_read(ctx) & SPU_STATUS_ISOLATED_STATE)) {
-			int ret = spu_setup_isolated(ctx);
+			ret = spu_setup_isolated(ctx);
 			if (ret)
 				return ret;
 		}
 
-		/* if userspace has set the runcntrl register (eg, to issue an
-		 * isolated exit), we need to re-set it here */
+		/*
+		 * If userspace has set the runcntrl register (eg, to
+		 * issue an isolated exit), we need to re-set it here
+		 */
 		runcntl = ctx->ops->runcntl_read(ctx) &
 			(SPU_RUNCNTL_RUNNABLE | SPU_RUNCNTL_ISOLATE);
 		if (runcntl == 0)
 			runcntl = SPU_RUNCNTL_RUNNABLE;
+
+		spuctx_switch_state(ctx, SPU_UTIL_USER);
+		ctx->ops->runcntl_write(ctx, runcntl);
+
 	} else {
 		unsigned long privcntl;
 
@@ -180,11 +198,17 @@ static int spu_run_init(struct spu_context *ctx, u32 *npc)
 
 		ctx->ops->npc_write(ctx, *npc);
 		ctx->ops->privcntl_write(ctx, privcntl);
-	}
 
-	ctx->ops->runcntl_write(ctx, runcntl);
+		if (ctx->state == SPU_STATE_SAVED) {
+			spu_set_timeslice(ctx);
+			ret = spu_activate(ctx, 0);
+			if (ret)
+				return ret;
+		}
 
-	spuctx_switch_state(ctx, SPU_UTIL_USER);
+		spuctx_switch_state(ctx, SPU_UTIL_USER);
+		ctx->ops->runcntl_write(ctx, runcntl);
+	}
 
 	return 0;
 }
@@ -323,25 +347,8 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 	ctx->event_return = 0;
 
 	spu_acquire(ctx);
-	if (ctx->state == SPU_STATE_SAVED) {
-		__spu_update_sched_info(ctx);
-		spu_set_timeslice(ctx);
 
-		ret = spu_activate(ctx, 0);
-		if (ret) {
-			spu_release(ctx);
-			goto out;
-		}
-	} else {
-		/*
-		 * We have to update the scheduling priority under active_mutex
-		 * to protect against find_victim().
-		 *
-		 * No need to update the timeslice ASAP, it will get updated
-		 * once the current one has expired.
-		 */
-		spu_update_sched_info(ctx);
-	}
+	spu_update_sched_info(ctx);
 
 	ret = spu_run_init(ctx, npc);
 	if (ret) {

commit d6ad39bc53521275d14fde86bfb94d9b2ddb7a08
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Thu Dec 20 16:39:59 2007 +0900

    [POWERPC] spufs: rework class 0 and 1 interrupt handling
    
    Based on original patches from
     Arnd Bergmann <arnd.bergman@de.ibm.com>; and
     Luke Browning <lukebr@linux.vnet.ibm.com>
    
    Currently, spu contexts need to be loaded to the SPU in order to take
    class 0 and class 1 exceptions.
    
    This change makes the actual interrupt-handlers much simpler (ie,
    set the exception information in the context save area), and defers the
    handling code to the spufs_handle_class[01] functions, called from
    spufs_run_spu.
    
    This should improve the concurrency of the spu scheduling leading to
    greater SPU utilization when SPUs are overcommited.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index b3cc1dd72185..3b3de6c7ee5b 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -15,7 +15,30 @@ void spufs_stop_callback(struct spu *spu)
 {
 	struct spu_context *ctx = spu->ctx;
 
-	wake_up_all(&ctx->stop_wq);
+	/*
+	 * It should be impossible to preempt a context while an exception
+	 * is being processed, since the context switch code is specially
+	 * coded to deal with interrupts ... But, just in case, sanity check
+	 * the context pointer.  It is OK to return doing nothing since
+	 * the exception will be regenerated when the context is resumed.
+	 */
+	if (ctx) {
+		/* Copy exception arguments into module specific structure */
+		ctx->csa.class_0_pending = spu->class_0_pending;
+		ctx->csa.dsisr = spu->dsisr;
+		ctx->csa.dar = spu->dar;
+
+		/* ensure that the exception status has hit memory before a
+		 * thread waiting on the context's stop queue is woken */
+		smp_wmb();
+
+		wake_up_all(&ctx->stop_wq);
+	}
+
+	/* Clear callback arguments from spu structure */
+	spu->class_0_pending = 0;
+	spu->dsisr = 0;
+	spu->dar = 0;
 }
 
 static inline int spu_stopped(struct spu_context *ctx, u32 *stat)
@@ -29,9 +52,9 @@ static inline int spu_stopped(struct spu_context *ctx, u32 *stat)
 	if (ctx->state != SPU_STATE_RUNNABLE ||
 	    test_bit(SPU_SCHED_NOTIFY_ACTIVE, &ctx->sched_flags))
 		return 1;
-	pte_fault = spu->dsisr &
+	pte_fault = ctx->csa.dsisr &
 	    (MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED);
-	return (!(*stat & SPU_STATUS_RUNNING) || pte_fault || spu->class_0_pending) ?
+	return (!(*stat & SPU_STATUS_RUNNING) || pte_fault || ctx->csa.class_0_pending) ?
 		1 : 0;
 }
 
@@ -287,18 +310,6 @@ static int spu_process_callback(struct spu_context *ctx)
 	return ret;
 }
 
-static inline int spu_process_events(struct spu_context *ctx)
-{
-	struct spu *spu = ctx->spu;
-	int ret = 0;
-
-	if (spu->class_0_pending)
-		ret = spu_irq_class_0_bottom(spu);
-	if (!ret && signal_pending(current))
-		ret = -ERESTARTSYS;
-	return ret;
-}
-
 long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 {
 	int ret;
@@ -364,13 +375,20 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 		if (ret)
 			break;
 
+		ret = spufs_handle_class0(ctx);
+		if (ret)
+			break;
+
 		if (unlikely(ctx->state != SPU_STATE_RUNNABLE)) {
 			ret = spu_reacquire_runnable(ctx, npc, &status);
 			if (ret)
 				goto out2;
 			continue;
 		}
-		ret = spu_process_events(ctx);
+
+		if (signal_pending(current))
+			ret = -ERESTARTSYS;
+
 
 	} while (!ret && !(status & (SPU_STATUS_STOPPED_BY_STOP |
 				      SPU_STATUS_STOPPED_BY_HALT |

commit cc210b3ec5e4d9c690549e07aa5853e8de8dbff9
Author: Luke Browning <lukebr@linux.vnet.ibm.com>
Date:   Thu Dec 20 16:39:59 2007 +0900

    [POWERPC] spufs: add backing ops for privcntl register
    
    This change encapsulates the spu_privcntl_RW register so that it can
    be written through backing ops.  This is necessary so that spu contexts
    can be initialized and queued to the scheduler in spufs_run_spu.
    
    Signed-off-by: Luke Browning <lukebr@linux.vnet.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index aad163ff46f4..b3cc1dd72185 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -128,10 +128,11 @@ static int spu_setup_isolated(struct spu_context *ctx)
 
 static int spu_run_init(struct spu_context *ctx, u32 *npc)
 {
+	unsigned long runcntl;
+
 	spuctx_switch_state(ctx, SPU_UTIL_SYSTEM);
 
 	if (ctx->flags & SPU_CREATE_ISOLATE) {
-		unsigned long runcntl;
 
 		if (!(ctx->ops->status_read(ctx) & SPU_STATUS_ISOLATED_STATE)) {
 			int ret = spu_setup_isolated(ctx);
@@ -145,16 +146,21 @@ static int spu_run_init(struct spu_context *ctx, u32 *npc)
 			(SPU_RUNCNTL_RUNNABLE | SPU_RUNCNTL_ISOLATE);
 		if (runcntl == 0)
 			runcntl = SPU_RUNCNTL_RUNNABLE;
-		ctx->ops->runcntl_write(ctx, runcntl);
 	} else {
-		unsigned long mode = SPU_PRIVCNTL_MODE_NORMAL;
-		ctx->ops->npc_write(ctx, *npc);
+		unsigned long privcntl;
+
 		if (test_thread_flag(TIF_SINGLESTEP))
-			mode = SPU_PRIVCNTL_MODE_SINGLE_STEP;
-		out_be64(&ctx->spu->priv2->spu_privcntl_RW, mode);
-		ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_RUNNABLE);
+			privcntl = SPU_PRIVCNTL_MODE_SINGLE_STEP;
+		else
+			privcntl = SPU_PRIVCNTL_MODE_NORMAL;
+		runcntl = SPU_RUNCNTL_RUNNABLE;
+
+		ctx->ops->npc_write(ctx, *npc);
+		ctx->ops->privcntl_write(ctx, privcntl);
 	}
 
+	ctx->ops->runcntl_write(ctx, runcntl);
+
 	spuctx_switch_state(ctx, SPU_UTIL_USER);
 
 	return 0;

commit c25620d7663fef41c373d42c4923c1d6b9847684
Author: Masato Noguchi <Masato.Noguchi@jp.sony.com>
Date:   Wed Dec 5 13:49:31 2007 +1100

    [POWERPC] cell: wrap master run control bit
    
    Add platform specific SPU run control routines to the spufs.  The current
    spufs implementation uses the SPU master run control bit (MFC_SR1[S]) to
    control SPE execution, but the PS3 hypervisor does not support the use of
    this feature.
    
    This change adds the run control wrapper routies spu_enable_spu() and
    spu_disable_spu().  The bare metal routines use the master run control
    bit, and the PS3 specific routines use the priv2 run control register.
    
    An outstanding enhancement for the PS3 would be to add a guard to check
    for incorrect access to the spu problem state when the spu context is
    disabled.  This check could be implemented with a flag added to the spu
    context that would inhibit mapping problem state pages, and a routine
    to unmap spu problem state pages.  When the spu is enabled with
    ps3_enable_spu() the flag would be set allowing pages to be mapped,
    and when the spu is disabled with ps3_disable_spu() the flag would be
    cleared and mapped problem state pages would be unmapped.
    
    Signed-off-by: Masato Noguchi <Masato.Noguchi@jp.sony.com>
    Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 1ce5e22ea5f4..aad163ff46f4 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -302,7 +302,7 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 	if (mutex_lock_interruptible(&ctx->run_mutex))
 		return -ERESTARTSYS;
 
-	ctx->ops->master_start(ctx);
+	spu_enable_spu(ctx);
 	ctx->event_return = 0;
 
 	spu_acquire(ctx);
@@ -376,7 +376,7 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 		ctx->stats.libassist++;
 
 
-	ctx->ops->master_stop(ctx);
+	spu_disable_spu(ctx);
 	ret = spu_run_fini(ctx, npc, &status);
 	spu_yield(ctx);
 

commit 1238819a41b6e38e1560afe8f33bbc815671b4f7
Author: Sebastian Siewior <cbe-oss-dev@ml.breakpoint.cc>
Date:   Wed Sep 19 14:38:12 2007 +1000

    [POWERPC] spufs: Make file-internal functions & variables static
    
    There are a few symbols used only in one file within spufs; this change
    makes them static where suitable.
    
    Signed-off-by: Sebastian Siewior <sebastian@breakpoint.cc>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 958f10e90fdd..1ce5e22ea5f4 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -205,7 +205,7 @@ static int spu_reacquire_runnable(struct spu_context *ctx, u32 *npc,
  * This means we can only do a very rough approximation of POSIX
  * signal semantics.
  */
-int spu_handle_restartsys(struct spu_context *ctx, long *spu_ret,
+static int spu_handle_restartsys(struct spu_context *ctx, long *spu_ret,
 			  unsigned int *npc)
 {
 	int ret;
@@ -241,7 +241,7 @@ int spu_handle_restartsys(struct spu_context *ctx, long *spu_ret,
 	return ret;
 }
 
-int spu_process_callback(struct spu_context *ctx)
+static int spu_process_callback(struct spu_context *ctx)
 {
 	struct spu_syscall_block s;
 	u32 ls_pointer, npc;

commit ada83daab31c3ec35845785482124373a62f430c
Author: Andre Detsch <adetsch@br.ibm.com>
Date:   Tue Aug 21 10:06:22 2007 +0800

    [POWERPC] spufs: Don't call spu_run_init from spu_reacquire_runnable
    
    This fixes a major bug which was happening when a SPU thread advances
    its execution right after being restored to a SPU.  A potentially
    outdated NPC value was being (re)written to the SPU.
    
    So, spu_run_init, in this case, was either not doing anything relevant,
    or breaking the execution of the SPU thread.
    
    This fixes a common problem of losing a mailbox write when it was done
    to a saved context.
    
    Signed-off-by: Andre Detsch <adetsch@br.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 6abdd8fe2098..958f10e90fdd 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -193,11 +193,7 @@ static int spu_reacquire_runnable(struct spu_context *ctx, u32 *npc,
 	if (ret)
 		return ret;
 
-	ret = spu_run_init(ctx, npc);
-	if (ret) {
-		spu_release(ctx);
-		return ret;
-	}
+	spuctx_switch_state(ctx, SPU_UTIL_USER);
 	return 0;
 }
 

commit 9d78592ed72dbff1d8825207f8def07858a49768
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jul 25 21:31:09 2007 +1000

    [POWERPC] spusched: Fix initial timeslice calculation
    
    Currently we calculate the first timeslice for every context
    incorrectly - alloc_spu_context calls spu_set_timeslice before we set
    ctx->prio so we always calculate the longest possible timeslice for the
    lowest possible priority.
    
    This patch makes sure to update the schedule-related fields before
    calculating the timeslice and also makes sure we update the timeslice for
    a non-running context when entering spu_run so a priority change affects
    the context as soon as possible.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 0b50fa5cb39d..6abdd8fe2098 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -312,6 +312,7 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 	spu_acquire(ctx);
 	if (ctx->state == SPU_STATE_SAVED) {
 		__spu_update_sched_info(ctx);
+		spu_set_timeslice(ctx);
 
 		ret = spu_activate(ctx, 0);
 		if (ret) {
@@ -322,6 +323,9 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 		/*
 		 * We have to update the scheduling priority under active_mutex
 		 * to protect against find_victim().
+		 *
+		 * No need to update the timeslice ASAP, it will get updated
+		 * once the current one has expired.
 		 */
 		spu_update_sched_info(ctx);
 	}

commit 36aaccc1e96481e8310b1d13600096da0f24ff43
Author: Bob Nelson <rrnelson@linux.vnet.ibm.com>
Date:   Fri Jul 20 21:39:52 2007 +0200

    [CELL] oprofile: enable SPU switch notification to detect currently active SPU tasks
    
    From: Maynard Johnson <mpjohn@us.ibm.com>
    
    This patch adds to the capability of spu_switch_event_register so that
    the caller is also notified of currently active SPU tasks.
    Exports spu_switch_event_register and spu_switch_event_unregister so
    that OProfile can get access to the notifications provided.
    
    Signed-off-by: Maynard Johnson <mpjohn@us.ibm.com>
    Signed-off-by: Carl Love <carll@us.ibm.com>
    Signed-off-by: Bob Nelson <rrnelson@us.ibm.com>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index c0238ea5b55a..0b50fa5cb39d 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -18,15 +18,17 @@ void spufs_stop_callback(struct spu *spu)
 	wake_up_all(&ctx->stop_wq);
 }
 
-static inline int spu_stopped(struct spu_context *ctx, u32 * stat)
+static inline int spu_stopped(struct spu_context *ctx, u32 *stat)
 {
 	struct spu *spu;
 	u64 pte_fault;
 
 	*stat = ctx->ops->status_read(ctx);
-	if (ctx->state != SPU_STATE_RUNNABLE)
-		return 1;
+
 	spu = ctx->spu;
+	if (ctx->state != SPU_STATE_RUNNABLE ||
+	    test_bit(SPU_SCHED_NOTIFY_ACTIVE, &ctx->sched_flags))
+		return 1;
 	pte_fault = spu->dsisr &
 	    (MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED);
 	return (!(*stat & SPU_STATUS_RUNNING) || pte_fault || spu->class_0_pending) ?
@@ -124,7 +126,7 @@ static int spu_setup_isolated(struct spu_context *ctx)
 	return ret;
 }
 
-static int spu_run_init(struct spu_context *ctx, u32 * npc)
+static int spu_run_init(struct spu_context *ctx, u32 *npc)
 {
 	spuctx_switch_state(ctx, SPU_UTIL_SYSTEM);
 
@@ -158,8 +160,8 @@ static int spu_run_init(struct spu_context *ctx, u32 * npc)
 	return 0;
 }
 
-static int spu_run_fini(struct spu_context *ctx, u32 * npc,
-			       u32 * status)
+static int spu_run_fini(struct spu_context *ctx, u32 *npc,
+			       u32 *status)
 {
 	int ret = 0;
 
@@ -298,6 +300,7 @@ static inline int spu_process_events(struct spu_context *ctx)
 long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 {
 	int ret;
+	struct spu *spu;
 	u32 status;
 
 	if (mutex_lock_interruptible(&ctx->run_mutex))
@@ -333,6 +336,14 @@ long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 		ret = spufs_wait(ctx->stop_wq, spu_stopped(ctx, &status));
 		if (unlikely(ret))
 			break;
+		spu = ctx->spu;
+		if (unlikely(test_and_clear_bit(SPU_SCHED_NOTIFY_ACTIVE,
+						&ctx->sched_flags))) {
+			if (!(status & SPU_STATUS_STOPPED_BY_STOP)) {
+				spu_switch_notify(spu, ctx);
+				continue;
+			}
+		}
 
 		spuctx_switch_state(ctx, SPU_UTIL_SYSTEM);
 

commit 50af32a94beef566664022254d677504e51b6139
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Fri Jul 20 21:39:42 2007 +0200

    [CELL] spufs: remove unused file argument from spufs_run_spu()
    
    From: Sebastian Siewior <cbe-oss-dev@ml.breakpoint.cc>
    
    The 'file' argument is unused in spufs_run_spu(). This change removes
    it.
    
    Signed-off-by: Sebastian Siewior <sebastian@breakpoint.cc>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 8c91b3f93152..c0238ea5b55a 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -295,8 +295,7 @@ static inline int spu_process_events(struct spu_context *ctx)
 	return ret;
 }
 
-long spufs_run_spu(struct file *file, struct spu_context *ctx,
-		   u32 *npc, u32 *event)
+long spufs_run_spu(struct spu_context *ctx, u32 *npc, u32 *event)
 {
 	int ret;
 	u32 status;

commit 27ec41d3a1d4df2b7cd190e93aad22ab86a72aa1
Author: Andre Detsch <adetsch@br.ibm.com>
Date:   Fri Jul 20 21:39:33 2007 +0200

    [CELL] spufs: add spu stats in sysfs and ctx stat file in spufs
    
    This patch exports per-context statistics in spufs as long as spu
    statistics in sysfs.
    
    It was formed by merging:
    "spufs: add spu stats in sysfs"   From: Christoph Hellwig
    "spufs: add stat file to spufs"   From: Christoph Hellwig
    "spufs: fix libassist accounting" From: Jeremy Kerr
    "spusched: fix spu utilization statistics" From: Luke Browning
    And some adjustments by myself, after suggestions on cbe-oss-dev.
    
    Having separate patches was making the review process harder
    than it should, as we end up integrating spus and ctx statistics
    accounting much more than it was on the first implementation.
    
    Signed-off-by: Andre Detsch <adetsch@br.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 58ae13b7de84..8c91b3f93152 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -126,6 +126,8 @@ static int spu_setup_isolated(struct spu_context *ctx)
 
 static int spu_run_init(struct spu_context *ctx, u32 * npc)
 {
+	spuctx_switch_state(ctx, SPU_UTIL_SYSTEM);
+
 	if (ctx->flags & SPU_CREATE_ISOLATE) {
 		unsigned long runcntl;
 
@@ -151,6 +153,8 @@ static int spu_run_init(struct spu_context *ctx, u32 * npc)
 		ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_RUNNABLE);
 	}
 
+	spuctx_switch_state(ctx, SPU_UTIL_USER);
+
 	return 0;
 }
 
@@ -161,6 +165,8 @@ static int spu_run_fini(struct spu_context *ctx, u32 * npc,
 
 	*status = ctx->ops->status_read(ctx);
 	*npc = ctx->ops->npc_read(ctx);
+
+	spuctx_switch_state(ctx, SPU_UTIL_IDLE_LOADED);
 	spu_release(ctx);
 
 	if (signal_pending(current))
@@ -328,6 +334,9 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 		ret = spufs_wait(ctx->stop_wq, spu_stopped(ctx, &status));
 		if (unlikely(ret))
 			break;
+
+		spuctx_switch_state(ctx, SPU_UTIL_SYSTEM);
+
 		if ((status & SPU_STATUS_STOPPED_BY_STOP) &&
 		    (status >> SPU_STOP_STATUS_SHIFT == 0x2104)) {
 			ret = spu_process_callback(ctx);
@@ -356,6 +365,7 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 	    (ctx->state == SPU_STATE_RUNNABLE))
 		ctx->stats.libassist++;
 
+
 	ctx->ops->master_stop(ctx);
 	ret = spu_run_fini(ctx, npc, &status);
 	spu_yield(ctx);

commit fe2f896d67b89a409c366c9a69e30291ab124467
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 29 10:58:07 2007 +1000

    [POWERPC] spufs: Add spu stats in sysfs
    
    Export spu statistics in sysfs.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 05cf815dbdad..58ae13b7de84 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -352,7 +352,8 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 				       SPU_STATUS_SINGLE_STEP)));
 
 	if ((status & SPU_STATUS_STOPPED_BY_STOP) &&
-	    (((status >> SPU_STOP_STATUS_SHIFT) & 0x3f00) == 0x2100))
+	    (((status >> SPU_STOP_STATUS_SHIFT) & 0x3f00) == 0x2100) &&
+	    (ctx->state == SPU_STATE_RUNNABLE))
 		ctx->stats.libassist++;
 
 	ctx->ops->master_stop(ctx);

commit 08c9692b168240729cf89c69c4ad722760a5690c
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Fri Jun 29 10:58:04 2007 +1000

    [POWERPC] spufs: Fix libassist accounting
    
    We're currently too permissive with counting libassist calls - fix the
    check on the SPE stop-and-signal status.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index c69f63dd5f0a..05cf815dbdad 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -352,7 +352,7 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 				       SPU_STATUS_SINGLE_STEP)));
 
 	if ((status & SPU_STATUS_STOPPED_BY_STOP) &&
-	    ((status >> SPU_STOP_STATUS_SHIFT) & 0x2100))
+	    (((status >> SPU_STOP_STATUS_SHIFT) & 0x3f00) == 0x2100))
 		ctx->stats.libassist++;
 
 	ctx->ops->master_stop(ctx);

commit e9f8a0b65ac716fd7974159240ce34bddea780b3
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 29 10:58:03 2007 +1000

    [POWERPC] spufs: Add stat file to spufs
    
    Export per-context statistics in spufs.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 4e0db6ae0d5e..c69f63dd5f0a 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -351,6 +351,10 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 				      SPU_STATUS_STOPPED_BY_HALT |
 				       SPU_STATUS_SINGLE_STEP)));
 
+	if ((status & SPU_STATUS_STOPPED_BY_STOP) &&
+	    ((status >> SPU_STOP_STATUS_SHIFT) & 0x2100))
+		ctx->stats.libassist++;
+
 	ctx->ops->master_stop(ctx);
 	ret = spu_run_fini(ctx, npc, &status);
 	spu_yield(ctx);

commit 2cf2b3b49f10d2f4a0703070fc54ce1cd84a6cda
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 29 10:57:55 2007 +1000

    [POWERPC] spusched: Update scheduling paramters on every spu_run
    
    Update scheduling information on every spu_run to allow for setting
    threads to realtime priority just before running them.  This requires
    some slightly ugly code in spufs_run_spu because we can just update
    the information unlocked if the spu is not runnable, but we need to
    acquire the active_mutex when it is runnable to protect against
    find_victim.  This locking scheme requires opencoding
    spu_acquire_runnable in spufs_run_spu which actually is a nice cleanup
    all by itself.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 89b02b6bfc55..4e0db6ae0d5e 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -301,9 +301,22 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 	ctx->ops->master_start(ctx);
 	ctx->event_return = 0;
 
-	ret = spu_acquire_runnable(ctx, 0);
-	if (ret)
-		return ret;
+	spu_acquire(ctx);
+	if (ctx->state == SPU_STATE_SAVED) {
+		__spu_update_sched_info(ctx);
+
+		ret = spu_activate(ctx, 0);
+		if (ret) {
+			spu_release(ctx);
+			goto out;
+		}
+	} else {
+		/*
+		 * We have to update the scheduling priority under active_mutex
+		 * to protect against find_victim().
+		 */
+		spu_update_sched_info(ctx);
+	}
 
 	ret = spu_run_init(ctx, npc);
 	if (ret) {

commit 379018022071489a7dffee74db2a267465dab561
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 29 10:57:51 2007 +1000

    [POWERPC] spusched: Switch from workqueues to kthread + timer tick
    
    Get rid of the scheduler workqueues that complicated things a lot to
    a dedicated spu scheduler thread that gets woken by a traditional
    scheduler tick.  By default this scheduler tick runs a HZ * 10, aka
    one spu scheduler tick for every 10 cpu ticks.
    
    Currently the tick is not disabled when we have less context than
    available spus, but I will implement this later.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 3ba30cea764a..89b02b6bfc55 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -144,7 +144,6 @@ static int spu_run_init(struct spu_context *ctx, u32 * npc)
 		ctx->ops->runcntl_write(ctx, runcntl);
 	} else {
 		unsigned long mode = SPU_PRIVCNTL_MODE_NORMAL;
-		spu_start_tick(ctx);
 		ctx->ops->npc_write(ctx, *npc);
 		if (test_thread_flag(TIF_SINGLESTEP))
 			mode = SPU_PRIVCNTL_MODE_SINGLE_STEP;
@@ -160,7 +159,6 @@ static int spu_run_fini(struct spu_context *ctx, u32 * npc,
 {
 	int ret = 0;
 
-	spu_stop_tick(ctx);
 	*status = ctx->ops->status_read(ctx);
 	*npc = ctx->ops->npc_read(ctx);
 	spu_release(ctx);
@@ -330,10 +328,8 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 
 		if (unlikely(ctx->state != SPU_STATE_RUNNABLE)) {
 			ret = spu_reacquire_runnable(ctx, npc, &status);
-			if (ret) {
-				spu_stop_tick(ctx);
+			if (ret)
 				goto out2;
-			}
 			continue;
 		}
 		ret = spu_process_events(ctx);

commit be7031773eded128675de6da778234a935c8d8ea
Author: Sebastian Siewior <bigeasy@linux.vnet.ibm.com>
Date:   Fri Jun 29 10:57:50 2007 +1000

    [POWERPC] spufs: Add bit definition
    
    Add a bit define from book, and replace one hex number with a
    symbol, for clarity.
    
    Signed-off-by: Sebastian Siewior <bigeasy@linux.vnet.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 6625ed2a7fdd..3ba30cea764a 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -29,7 +29,8 @@ static inline int spu_stopped(struct spu_context *ctx, u32 * stat)
 	spu = ctx->spu;
 	pte_fault = spu->dsisr &
 	    (MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED);
-	return (!(*stat & 0x1) || pte_fault || spu->class_0_pending) ? 1 : 0;
+	return (!(*stat & SPU_STATUS_RUNNING) || pte_fault || spu->class_0_pending) ?
+		1 : 0;
 }
 
 static int spu_setup_isolated(struct spu_context *ctx)

commit 05169237b55058a3993fb4804d00b65dfa3e4a0c
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Jun 4 15:15:37 2007 +1000

    [POWERPC] spufs: Add support for SPU single stepping
    
    This patch adds support for SPU single stepping. The single
    step bit is set in the SPU when the current process is
    being single-stepped via ptrace. The spu then stops and
    returns with a specific flag set and the syscall exit code
    will generate the SIGTRAP.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 57626600b1a4..6625ed2a7fdd 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -142,8 +142,12 @@ static int spu_run_init(struct spu_context *ctx, u32 * npc)
 			runcntl = SPU_RUNCNTL_RUNNABLE;
 		ctx->ops->runcntl_write(ctx, runcntl);
 	} else {
+		unsigned long mode = SPU_PRIVCNTL_MODE_NORMAL;
 		spu_start_tick(ctx);
 		ctx->ops->npc_write(ctx, *npc);
+		if (test_thread_flag(TIF_SINGLESTEP))
+			mode = SPU_PRIVCNTL_MODE_SINGLE_STEP;
+		out_be64(&ctx->spu->priv2->spu_privcntl_RW, mode);
 		ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_RUNNABLE);
 	}
 
@@ -334,7 +338,8 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 		ret = spu_process_events(ctx);
 
 	} while (!ret && !(status & (SPU_STATUS_STOPPED_BY_STOP |
-				      SPU_STATUS_STOPPED_BY_HALT)));
+				      SPU_STATUS_STOPPED_BY_HALT |
+				       SPU_STATUS_SINGLE_STEP)));
 
 	ctx->ops->master_stop(ctx);
 	ret = spu_run_fini(ctx, npc, &status);
@@ -344,10 +349,15 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 	if ((ret == 0) ||
 	    ((ret == -ERESTARTSYS) &&
 	     ((status & SPU_STATUS_STOPPED_BY_HALT) ||
+	      (status & SPU_STATUS_SINGLE_STEP) ||
 	      ((status & SPU_STATUS_STOPPED_BY_STOP) &&
 	       (status >> SPU_STOP_STATUS_SHIFT != 0x2104)))))
 		ret = status;
 
+	/* Note: we don't need to force_sig SIGTRAP on single-step
+	 * since we have TIF_SINGLESTEP set, thus the kernel will do
+	 * it upon return from the syscall anyawy
+	 */
 	if ((status & SPU_STATUS_STOPPED_BY_STOP)
 	    && (status >> SPU_STOP_STATUS_SHIFT) == 0x3fff) {
 		force_sig(SIGTRAP, current);

commit aa45e2569ffe963dfbbbfddfdccd12afe69b2d65
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 23 21:08:27 2007 +0200

    [POWERPC] spufs: various run.c cleanups
    
     - remove the spu_acquire_runnable from spu_run_init.  I need to
       opencode it in spufs_run_spu in the next patch
     - remove various inline attributes, we don't really want to inline
       long functions with multiple callsites
     - cleanup return values and runcntl_write calls in spu_run_init
     - use normal kernel codingstyle in spu_reacquire_runnable
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index b5e7bdee1bc2..57626600b1a4 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -123,20 +123,15 @@ static int spu_setup_isolated(struct spu_context *ctx)
 	return ret;
 }
 
-static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
+static int spu_run_init(struct spu_context *ctx, u32 * npc)
 {
-	int ret;
-	unsigned long runcntl = SPU_RUNCNTL_RUNNABLE;
-
-	ret = spu_acquire_runnable(ctx, 0);
-	if (ret)
-		return ret;
-
 	if (ctx->flags & SPU_CREATE_ISOLATE) {
+		unsigned long runcntl;
+
 		if (!(ctx->ops->status_read(ctx) & SPU_STATUS_ISOLATED_STATE)) {
-			ret = spu_setup_isolated(ctx);
+			int ret = spu_setup_isolated(ctx);
 			if (ret)
-				spu_release(ctx);
+				return ret;
 		}
 
 		/* if userspace has set the runcntrl register (eg, to issue an
@@ -145,16 +140,17 @@ static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
 			(SPU_RUNCNTL_RUNNABLE | SPU_RUNCNTL_ISOLATE);
 		if (runcntl == 0)
 			runcntl = SPU_RUNCNTL_RUNNABLE;
+		ctx->ops->runcntl_write(ctx, runcntl);
 	} else {
 		spu_start_tick(ctx);
 		ctx->ops->npc_write(ctx, *npc);
+		ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_RUNNABLE);
 	}
 
-	ctx->ops->runcntl_write(ctx, runcntl);
-	return ret;
+	return 0;
 }
 
-static inline int spu_run_fini(struct spu_context *ctx, u32 * npc,
+static int spu_run_fini(struct spu_context *ctx, u32 * npc,
 			       u32 * status)
 {
 	int ret = 0;
@@ -170,19 +166,27 @@ static inline int spu_run_fini(struct spu_context *ctx, u32 * npc,
 	return ret;
 }
 
-static inline int spu_reacquire_runnable(struct spu_context *ctx, u32 *npc,
+static int spu_reacquire_runnable(struct spu_context *ctx, u32 *npc,
 				         u32 *status)
 {
 	int ret;
 
-	if ((ret = spu_run_fini(ctx, npc, status)) != 0)
+	ret = spu_run_fini(ctx, npc, status);
+	if (ret)
 		return ret;
-	if (*status & (SPU_STATUS_STOPPED_BY_STOP |
-		       SPU_STATUS_STOPPED_BY_HALT)) {
+
+	if (*status & (SPU_STATUS_STOPPED_BY_STOP | SPU_STATUS_STOPPED_BY_HALT))
 		return *status;
-	}
-	if ((ret = spu_run_init(ctx, npc)) != 0)
+
+	ret = spu_acquire_runnable(ctx, 0);
+	if (ret)
 		return ret;
+
+	ret = spu_run_init(ctx, npc);
+	if (ret) {
+		spu_release(ctx);
+		return ret;
+	}
 	return 0;
 }
 
@@ -293,9 +297,16 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 
 	ctx->ops->master_start(ctx);
 	ctx->event_return = 0;
-	ret = spu_run_init(ctx, npc);
+
+	ret = spu_acquire_runnable(ctx, 0);
 	if (ret)
+		return ret;
+
+	ret = spu_run_init(ctx, npc);
+	if (ret) {
+		spu_release(ctx);
 		goto out;
+	}
 
 	do {
 		ret = spufs_wait(ctx->stop_wq, spu_stopped(ctx, &status));

commit 9e2fe2ce4e957a79d3dc5d813e0cfb10d79b79b3
Author: Akinobu Mita <mita@fixstars.com>
Date:   Mon Apr 23 21:08:22 2007 +0200

    [POWERPC] spufs: use memcpy_fromio() to copy from local store
    
    GCC may generates inline copy loop to handle memcpy() function
    instead of kernel defined memcpy(). But this inlined version of memcpy()
    causes an alignment interrupt when copying from local store.
    
    This patch uses memcpy_fromio() and memcpy_toio to copy local store
    to prevent memcpy() being inlined.
    
    Signed-off-by: Akinobu Mita <mita@fixstars.com>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 7cf5b298fa13..b5e7bdee1bc2 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -234,17 +234,17 @@ int spu_process_callback(struct spu_context *ctx)
 {
 	struct spu_syscall_block s;
 	u32 ls_pointer, npc;
-	char *ls;
+	void __iomem *ls;
 	long spu_ret;
 	int ret;
 
 	/* get syscall block from local store */
-	npc = ctx->ops->npc_read(ctx);
-	ls = ctx->ops->get_ls(ctx);
-	ls_pointer = *(u32*)(ls + npc);
+	npc = ctx->ops->npc_read(ctx) & ~3;
+	ls = (void __iomem *)ctx->ops->get_ls(ctx);
+	ls_pointer = in_be32(ls + npc);
 	if (ls_pointer > (LS_SIZE - sizeof(s)))
 		return -EFAULT;
-	memcpy(&s, ls + ls_pointer, sizeof (s));
+	memcpy_fromio(&s, ls + ls_pointer, sizeof(s));
 
 	/* do actual syscall without pinning the spu */
 	ret = 0;
@@ -264,7 +264,7 @@ int spu_process_callback(struct spu_context *ctx)
 	}
 
 	/* write result, jump over indirect pointer */
-	memcpy(ls + ls_pointer, &spu_ret, sizeof (spu_ret));
+	memcpy_toio(ls + ls_pointer, &spu_ret, sizeof(spu_ret));
 	ctx->ops->npc_write(ctx, npc);
 	ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_RUNNABLE);
 	return ret;

commit e45d48a34d4d1862d28d22c2533b8c6bb83b8c1f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 23 21:08:17 2007 +0200

    [POWERPC] spufs: turn run_sema into run_mutex
    
    There is no reason for run_sema to be a struct semaphore.  Changing
    it to a mutex and rename it accordingly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 1a8195bf75d5..7cf5b298fa13 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -288,7 +288,7 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 	int ret;
 	u32 status;
 
-	if (down_interruptible(&ctx->run_sema))
+	if (mutex_lock_interruptible(&ctx->run_mutex))
 		return -ERESTARTSYS;
 
 	ctx->ops->master_start(ctx);
@@ -345,6 +345,6 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 
 out:
 	*event = ctx->event_return;
-	up(&ctx->run_sema);
+	mutex_unlock(&ctx->run_mutex);
 	return ret;
 }

commit 57dace2391ba10135e38457904121e7ef34d0c83
Author: Arnd Bergmann <arnd.bergmann@de.ibm.com>
Date:   Mon Apr 23 21:08:15 2007 +0200

    [POWERPC] spufs: make spu page faults not block scheduling
    
    Until now, we have always entered the spu page fault handler
    with a mutex for the spu context held. This has multiple
    bad side-effects:
    - it becomes impossible to suspend the context during
      page faults
    - if an spu program attempts to access its own mmio
      areas through DMA, we get an immediate livelock when
      the nopage function tries to acquire the same mutex
    
    This patch makes the page fault logic operate on a
    struct spu_context instead of a struct spu, and moves it
    from spu_base.c to a new file fault.c inside of spufs.
    
    We now also need to copy the dar and dsisr contents
    of the last fault into the saved context to have it
    accessible in case we schedule out the context before
    activating the page fault handler.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 7df5202c9a90..1a8195bf75d5 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -18,27 +18,6 @@ void spufs_stop_callback(struct spu *spu)
 	wake_up_all(&ctx->stop_wq);
 }
 
-void spufs_dma_callback(struct spu *spu, int type)
-{
-	struct spu_context *ctx = spu->ctx;
-
-	if (ctx->flags & SPU_CREATE_EVENTS_ENABLED) {
-		ctx->event_return |= type;
-		wake_up_all(&ctx->stop_wq);
-	} else {
-		switch (type) {
-		case SPE_EVENT_DMA_ALIGNMENT:
-		case SPE_EVENT_SPE_DATA_STORAGE:
-		case SPE_EVENT_INVALID_DMA:
-			force_sig(SIGBUS, /* info, */ current);
-			break;
-		case SPE_EVENT_SPE_ERROR:
-			force_sig(SIGILL, /* info */ current);
-			break;
-		}
-	}
-}
-
 static inline int spu_stopped(struct spu_context *ctx, u32 * stat)
 {
 	struct spu *spu;
@@ -294,11 +273,8 @@ int spu_process_callback(struct spu_context *ctx)
 static inline int spu_process_events(struct spu_context *ctx)
 {
 	struct spu *spu = ctx->spu;
-	u64 pte_fault = MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED;
 	int ret = 0;
 
-	if (spu->dsisr & pte_fault)
-		ret = spu_irq_class_1_bottom(spu);
 	if (spu->class_0_pending)
 		ret = spu_irq_class_0_bottom(spu);
 	if (!ret && signal_pending(current))
@@ -332,6 +308,10 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 				break;
 			status &= ~SPU_STATUS_STOPPED_BY_STOP;
 		}
+		ret = spufs_handle_class1(ctx);
+		if (ret)
+			break;
+
 		if (unlikely(ctx->state != SPU_STATE_RUNNABLE)) {
 			ret = spu_reacquire_runnable(ctx, npc, &status);
 			if (ret) {

commit 7ec18ab923a2e377ecb05c74a2d38f457f79950f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 23 21:08:12 2007 +0200

    [POWERPC] spufs: streamline locking for isolated spu setup
    
    For quite a while now spu state is protected by a simple mutex instead
    of the old rw_semaphore, and this means we can simplify the locking
    around spu_setup_isolated a lot.
    
    Instead of doing an spu_release before entering spu_setup_isolated and
    then calling the complicated spu_acquire_exclusive we can now simply
    enter the function locked an in guaranteed runnable state, so that the
    only bit of spu_acquire_exclusive that's left is the call to
    spu_unmap_mappings.
    
    Similarly there's no more need to unlock and reacquire the state_mutex
    when spu_setup_isolated is done, but we can always return with the
    lock held and only drop it in spu_run_init in the failure case.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index f95a611ca362..7df5202c9a90 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -63,13 +63,18 @@ static int spu_setup_isolated(struct spu_context *ctx)
 	const u32 status_loading = SPU_STATUS_RUNNING
 		| SPU_STATUS_ISOLATED_STATE | SPU_STATUS_ISOLATED_LOAD_STATUS;
 
+	ret = -ENODEV;
 	if (!isolated_loader)
-		return -ENODEV;
-
-	ret = spu_acquire_exclusive(ctx);
-	if (ret)
 		goto out;
 
+	/*
+	 * We need to exclude userspace access to the context.
+	 *
+	 * To protect against memory access we invalidate all ptes
+	 * and make sure the pagefault handlers block on the mutex.
+	 */
+	spu_unmap_mappings(ctx);
+
 	mfc_cntl = &ctx->spu->priv2->mfc_control_RW;
 
 	/* purge the MFC DMA queue to ensure no spurious accesses before we
@@ -82,7 +87,7 @@ static int spu_setup_isolated(struct spu_context *ctx)
 			printk(KERN_ERR "%s: timeout flushing MFC DMA queue\n",
 					__FUNCTION__);
 			ret = -EIO;
-			goto out_unlock;
+			goto out;
 		}
 		cond_resched();
 	}
@@ -119,12 +124,15 @@ static int spu_setup_isolated(struct spu_context *ctx)
 		pr_debug("%s: isolated LOAD failed\n", __FUNCTION__);
 		ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_RUNNABLE);
 		ret = -EACCES;
+		goto out_drop_priv;
+	}
 
-	} else if (!(status & SPU_STATUS_ISOLATED_STATE)) {
+	if (!(status & SPU_STATUS_ISOLATED_STATE)) {
 		/* This isn't allowed by the CBEA, but check anyway */
 		pr_debug("%s: SPU fell out of isolated mode?\n", __FUNCTION__);
 		ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_STOP);
 		ret = -EINVAL;
+		goto out_drop_priv;
 	}
 
 out_drop_priv:
@@ -132,8 +140,6 @@ static int spu_setup_isolated(struct spu_context *ctx)
 	sr1 |= MFC_STATE1_PROBLEM_STATE_MASK;
 	spu_mfc_sr1_set(ctx->spu, sr1);
 
-out_unlock:
-	spu_release(ctx);
 out:
 	return ret;
 }
@@ -149,13 +155,9 @@ static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
 
 	if (ctx->flags & SPU_CREATE_ISOLATE) {
 		if (!(ctx->ops->status_read(ctx) & SPU_STATUS_ISOLATED_STATE)) {
-			/* Need to release ctx, because spu_setup_isolated will
-			 * acquire it exclusively.
-			 */
-			spu_release(ctx);
 			ret = spu_setup_isolated(ctx);
-			if (!ret)
-				ret = spu_acquire_runnable(ctx, 0);
+			if (ret)
+				spu_release(ctx);
 		}
 
 		/* if userspace has set the runcntrl register (eg, to issue an

commit 50b520d4efbce45281f58112789470ec7965fd33
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Mar 10 00:05:36 2007 +0100

    [POWERPC] avoid SPU_ACTIVATE_NOWAKE optimization
    
    This optimization was added recently but is still buggy,
    so back it out for now.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 353a8fa07ab8..f95a611ca362 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -143,7 +143,7 @@ static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
 	int ret;
 	unsigned long runcntl = SPU_RUNCNTL_RUNNABLE;
 
-	ret = spu_acquire_runnable(ctx, SPU_ACTIVATE_NOWAKE);
+	ret = spu_acquire_runnable(ctx, 0);
 	if (ret)
 		return ret;
 
@@ -155,7 +155,7 @@ static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
 			spu_release(ctx);
 			ret = spu_setup_isolated(ctx);
 			if (!ret)
-				ret = spu_acquire_runnable(ctx, SPU_ACTIVATE_NOWAKE);
+				ret = spu_acquire_runnable(ctx, 0);
 		}
 
 		/* if userspace has set the runcntrl register (eg, to issue an

commit 2eb1b12049844a8ebc670e0e4fc908bc3f8933d3
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:54:29 2007 +0100

    [POWERPC] spu sched: static timeslicing for SCHED_RR contexts
    
    For SCHED_RR tasks we can do some really trivial timeslicing.  Basically
    we fire up a time for every scheduler tick that searches for a higher
    or same priority thread that is on the runqueue and if there is one
    context switches to it.  Because we can't lock spus from timer context
    we actually run this from a delayed runqueue instead of a timer.
    
    A nice optimization would be to skip the actual priority bitmap search
    when there are less contexts than physical spus available.  To implement
    this I need a so far unpublished patch from Andre, and it will be added
    after we have that patch in.
    
    Note that right now we only do the time slicing for SCHED_RR tasks.
    The code would work for SCHED_OTHER tasks aswell, but their prio
    value is defered from the one the PPU thread has at time of spu_run,
    and using this for spu scheduling decisions would make the code very
    unfair.  SCHED_OTHER support will be enabled once we the spu scheduler
    knows how to calculcate cpu_context.prio (very soon)
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index a973e79e9fdc..353a8fa07ab8 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -164,8 +164,10 @@ static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
 			(SPU_RUNCNTL_RUNNABLE | SPU_RUNCNTL_ISOLATE);
 		if (runcntl == 0)
 			runcntl = SPU_RUNCNTL_RUNNABLE;
-	} else
+	} else {
+		spu_start_tick(ctx);
 		ctx->ops->npc_write(ctx, *npc);
+	}
 
 	ctx->ops->runcntl_write(ctx, runcntl);
 	return ret;
@@ -176,6 +178,7 @@ static inline int spu_run_fini(struct spu_context *ctx, u32 * npc,
 {
 	int ret = 0;
 
+	spu_stop_tick(ctx);
 	*status = ctx->ops->status_read(ctx);
 	*npc = ctx->ops->npc_read(ctx);
 	spu_release(ctx);
@@ -329,8 +332,10 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 		}
 		if (unlikely(ctx->state != SPU_STATE_RUNNABLE)) {
 			ret = spu_reacquire_runnable(ctx, npc, &status);
-			if (ret)
+			if (ret) {
+				spu_stop_tick(ctx);
 				goto out2;
+			}
 			continue;
 		}
 		ret = spu_process_events(ctx);

commit 26bec67386dbf6ef887254e815398842e182cdcd
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:54:24 2007 +0100

    [POWERPC] spufs: optimize spu_run
    
    There is no need to directly wake up contexts in spu_activate when
    called from spu_run, so add a flag to surpress this wakeup.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index e1647311044b..a973e79e9fdc 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -143,7 +143,7 @@ static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
 	int ret;
 	unsigned long runcntl = SPU_RUNCNTL_RUNNABLE;
 
-	ret = spu_acquire_runnable(ctx);
+	ret = spu_acquire_runnable(ctx, SPU_ACTIVATE_NOWAKE);
 	if (ret)
 		return ret;
 
@@ -155,7 +155,7 @@ static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
 			spu_release(ctx);
 			ret = spu_setup_isolated(ctx);
 			if (!ret)
-				ret = spu_acquire_runnable(ctx);
+				ret = spu_acquire_runnable(ctx, SPU_ACTIVATE_NOWAKE);
 		}
 
 		/* if userspace has set the runcntrl register (eg, to issue an

commit 6a0641e51011def4e308fd07387047f5ee50647f
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:54:21 2007 +0100

    [POWERPC] spufs: state_mutex cleanup
    
    Various cleanups in code surrounding the state semaphore:
    
     - inline spu_acquire/spu_release
     - cleanup spu_acquire_* and add kerneldoc comments to these functions
     - remove spu_release_exclusive and replace it with spu_release
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 51b78da2f0d3..e1647311044b 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -133,7 +133,7 @@ static int spu_setup_isolated(struct spu_context *ctx)
 	spu_mfc_sr1_set(ctx->spu, sr1);
 
 out_unlock:
-	spu_release_exclusive(ctx);
+	spu_release(ctx);
 out:
 	return ret;
 }

commit 5cb23afc9e64841adb43d46160a5c63a80ebfd54
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:36:46 2007 +0100

    [POWERPC] spufs: remove empty last line in run.c
    
    Remove the empty last line in arch/powerpc/platforms/cell/spufs/run.c.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 1acc2ffef8c8..51b78da2f0d3 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -361,4 +361,3 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 	up(&ctx->run_sema);
 	return ret;
 }
-

commit c2b2226c7e46549c26fd5f5f40122536bc91ba0d
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Nov 27 19:18:53 2006 +0100

    [POWERPC] spufs: always send sigtrap on breakpoint
    
    Currently, we only send a sigtrap if the current task is being ptraced.
    This is somewhat inconsistant, and it breaks utrace support in fedora.
    Removing the check should do the right thing in all cases.
    
    Cc: Ulrich Weigand <ulrich.weigand@de.ibm.com>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 1be4e3339d8e..1acc2ffef8c8 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -350,12 +350,10 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 	       (status >> SPU_STOP_STATUS_SHIFT != 0x2104)))))
 		ret = status;
 
-	if (unlikely(current->ptrace & PT_PTRACED)) {
-		if ((status & SPU_STATUS_STOPPED_BY_STOP)
-		    && (status >> SPU_STOP_STATUS_SHIFT) == 0x3fff) {
-			force_sig(SIGTRAP, current);
-			ret = -ERESTARTSYS;
-		}
+	if ((status & SPU_STATUS_STOPPED_BY_STOP)
+	    && (status >> SPU_STOP_STATUS_SHIFT) == 0x3fff) {
+		force_sig(SIGTRAP, current);
+		ret = -ERESTARTSYS;
 	}
 
 out:

commit c6730ed4c280ff9e55766796523c94a7d111da09
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Mon Nov 20 18:45:10 2006 +0100

    [POWERPC] spufs: Load isolation kernel from spu_run
    
    In order to fit with the "don't-run-spus-outside-of-spu_run" model, this
    patch starts the isolated-mode loader in spu_run, rather than
    spu_create. If spu_run is passed an isolated-mode context that isn't in
    isolated mode state, it will run the loader.
    
    This fixes potential races with the isolated SPE app doing a
    stop-and-signal before the PPE has called spu_run: bugzilla #29111.
    Also (in conjunction with a mambo patch), this addresses #28565, as we
    always set the runcntrl register when entering spu_run.
    
    It is up to libspe to ensure that isolated-mode apps are cleaned up
    after running to completion - ie, put the app through the "ISOLATE EXIT"
    state (see Ch11 of the CBEA).
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 212b9c2f04ab..1be4e3339d8e 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -4,6 +4,8 @@
 #include <linux/ptrace.h>
 
 #include <asm/spu.h>
+#include <asm/spu_priv1.h>
+#include <asm/io.h>
 #include <asm/unistd.h>
 
 #include "spufs.h"
@@ -51,21 +53,122 @@ static inline int spu_stopped(struct spu_context *ctx, u32 * stat)
 	return (!(*stat & 0x1) || pte_fault || spu->class_0_pending) ? 1 : 0;
 }
 
+static int spu_setup_isolated(struct spu_context *ctx)
+{
+	int ret;
+	u64 __iomem *mfc_cntl;
+	u64 sr1;
+	u32 status;
+	unsigned long timeout;
+	const u32 status_loading = SPU_STATUS_RUNNING
+		| SPU_STATUS_ISOLATED_STATE | SPU_STATUS_ISOLATED_LOAD_STATUS;
+
+	if (!isolated_loader)
+		return -ENODEV;
+
+	ret = spu_acquire_exclusive(ctx);
+	if (ret)
+		goto out;
+
+	mfc_cntl = &ctx->spu->priv2->mfc_control_RW;
+
+	/* purge the MFC DMA queue to ensure no spurious accesses before we
+	 * enter kernel mode */
+	timeout = jiffies + HZ;
+	out_be64(mfc_cntl, MFC_CNTL_PURGE_DMA_REQUEST);
+	while ((in_be64(mfc_cntl) & MFC_CNTL_PURGE_DMA_STATUS_MASK)
+			!= MFC_CNTL_PURGE_DMA_COMPLETE) {
+		if (time_after(jiffies, timeout)) {
+			printk(KERN_ERR "%s: timeout flushing MFC DMA queue\n",
+					__FUNCTION__);
+			ret = -EIO;
+			goto out_unlock;
+		}
+		cond_resched();
+	}
+
+	/* put the SPE in kernel mode to allow access to the loader */
+	sr1 = spu_mfc_sr1_get(ctx->spu);
+	sr1 &= ~MFC_STATE1_PROBLEM_STATE_MASK;
+	spu_mfc_sr1_set(ctx->spu, sr1);
+
+	/* start the loader */
+	ctx->ops->signal1_write(ctx, (unsigned long)isolated_loader >> 32);
+	ctx->ops->signal2_write(ctx,
+			(unsigned long)isolated_loader & 0xffffffff);
+
+	ctx->ops->runcntl_write(ctx,
+			SPU_RUNCNTL_RUNNABLE | SPU_RUNCNTL_ISOLATE);
+
+	ret = 0;
+	timeout = jiffies + HZ;
+	while (((status = ctx->ops->status_read(ctx)) & status_loading) ==
+				status_loading) {
+		if (time_after(jiffies, timeout)) {
+			printk(KERN_ERR "%s: timeout waiting for loader\n",
+					__FUNCTION__);
+			ret = -EIO;
+			goto out_drop_priv;
+		}
+		cond_resched();
+	}
+
+	if (!(status & SPU_STATUS_RUNNING)) {
+		/* If isolated LOAD has failed: run SPU, we will get a stop-and
+		 * signal later. */
+		pr_debug("%s: isolated LOAD failed\n", __FUNCTION__);
+		ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_RUNNABLE);
+		ret = -EACCES;
+
+	} else if (!(status & SPU_STATUS_ISOLATED_STATE)) {
+		/* This isn't allowed by the CBEA, but check anyway */
+		pr_debug("%s: SPU fell out of isolated mode?\n", __FUNCTION__);
+		ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_STOP);
+		ret = -EINVAL;
+	}
+
+out_drop_priv:
+	/* Finished accessing the loader. Drop kernel mode */
+	sr1 |= MFC_STATE1_PROBLEM_STATE_MASK;
+	spu_mfc_sr1_set(ctx->spu, sr1);
+
+out_unlock:
+	spu_release_exclusive(ctx);
+out:
+	return ret;
+}
+
 static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
 {
 	int ret;
 	unsigned long runcntl = SPU_RUNCNTL_RUNNABLE;
 
-	if ((ret = spu_acquire_runnable(ctx)) != 0)
+	ret = spu_acquire_runnable(ctx);
+	if (ret)
 		return ret;
 
-	/* if we're in isolated mode, we would have started the SPU
-	 * earlier, so don't do it again now. */
-	if (!(ctx->flags & SPU_CREATE_ISOLATE)) {
+	if (ctx->flags & SPU_CREATE_ISOLATE) {
+		if (!(ctx->ops->status_read(ctx) & SPU_STATUS_ISOLATED_STATE)) {
+			/* Need to release ctx, because spu_setup_isolated will
+			 * acquire it exclusively.
+			 */
+			spu_release(ctx);
+			ret = spu_setup_isolated(ctx);
+			if (!ret)
+				ret = spu_acquire_runnable(ctx);
+		}
+
+		/* if userspace has set the runcntrl register (eg, to issue an
+		 * isolated exit), we need to re-set it here */
+		runcntl = ctx->ops->runcntl_read(ctx) &
+			(SPU_RUNCNTL_RUNNABLE | SPU_RUNCNTL_ISOLATE);
+		if (runcntl == 0)
+			runcntl = SPU_RUNCNTL_RUNNABLE;
+	} else
 		ctx->ops->npc_write(ctx, *npc);
-		ctx->ops->runcntl_write(ctx, runcntl);
-	}
-	return 0;
+
+	ctx->ops->runcntl_write(ctx, runcntl);
+	return ret;
 }
 
 static inline int spu_run_fini(struct spu_context *ctx, u32 * npc,

commit ee2d7340cbf3b123e1c3b7454f3e2b7e65d33bb2
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Nov 20 18:45:08 2006 +0100

    [POWERPC] spufs: Use SPU master control to prevent wild SPU execution
    
    When the user changes the runcontrol register, an SPU might be
    running without a process being attached to it and waiting for
    events. In order to prevent this, make sure we always disable
    the priv1 master control when we're not inside of spu_run.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index c88fd7f9ea74..212b9c2f04ab 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -207,6 +207,7 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 	if (down_interruptible(&ctx->run_sema))
 		return -ERESTARTSYS;
 
+	ctx->ops->master_start(ctx);
 	ctx->event_return = 0;
 	ret = spu_run_init(ctx, npc);
 	if (ret)
@@ -234,7 +235,7 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 	} while (!ret && !(status & (SPU_STATUS_STOPPED_BY_STOP |
 				      SPU_STATUS_STOPPED_BY_HALT)));
 
-	ctx->ops->runcntl_stop(ctx);
+	ctx->ops->master_stop(ctx);
 	ret = spu_run_fini(ctx, npc, &status);
 	spu_yield(ctx);
 

commit 2ebb2477f9a61b436dd22b75189857df1a77e585
Author: Masato Noguchi <Masato.Noguchi@jp.sony.com>
Date:   Mon Nov 20 18:45:04 2006 +0100

    [POWERPC] spufs: Fix missing stop-and-signal
    
    When there is pending signals, current spufs_run_spu() always returns
    -ERESTARTSYS and it is called again automatically.
    But, if spe already stopped by stop-and-signal or halt instruction,
    returning -ERESTARTSYS makes stop-and-signal/halt lost and
    spu run over the end-point.
    
    For your convenience, I attached a sample code to restage this bug.
    If there is no bug, printed NPC will be 0x4000.
    
    Signed-off-by: Masato Noguchi <Masato.Noguchi@jp.sony.com>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 88a41d83a79b..c88fd7f9ea74 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -79,13 +79,7 @@ static inline int spu_run_fini(struct spu_context *ctx, u32 * npc,
 
 	if (signal_pending(current))
 		ret = -ERESTARTSYS;
-	if (unlikely(current->ptrace & PT_PTRACED)) {
-		if ((*status & SPU_STATUS_STOPPED_BY_STOP)
-		    && (*status >> SPU_STOP_STATUS_SHIFT) == 0x3fff) {
-			force_sig(SIGTRAP, current);
-			ret = -ERESTARTSYS;
-		}
-	}
+
 	return ret;
 }
 
@@ -232,7 +226,7 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 		if (unlikely(ctx->state != SPU_STATE_RUNNABLE)) {
 			ret = spu_reacquire_runnable(ctx, npc, &status);
 			if (ret)
-				goto out;
+				goto out2;
 			continue;
 		}
 		ret = spu_process_events(ctx);
@@ -242,10 +236,24 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 
 	ctx->ops->runcntl_stop(ctx);
 	ret = spu_run_fini(ctx, npc, &status);
-	if (!ret)
-		ret = status;
 	spu_yield(ctx);
 
+out2:
+	if ((ret == 0) ||
+	    ((ret == -ERESTARTSYS) &&
+	     ((status & SPU_STATUS_STOPPED_BY_HALT) ||
+	      ((status & SPU_STATUS_STOPPED_BY_STOP) &&
+	       (status >> SPU_STOP_STATUS_SHIFT != 0x2104)))))
+		ret = status;
+
+	if (unlikely(current->ptrace & PT_PTRACED)) {
+		if ((status & SPU_STATUS_STOPPED_BY_STOP)
+		    && (status >> SPU_STOP_STATUS_SHIFT) == 0x3fff) {
+			force_sig(SIGTRAP, current);
+			ret = -ERESTARTSYS;
+		}
+	}
+
 out:
 	*event = ctx->event_return;
 	up(&ctx->run_sema);

commit 453d9f72a91d798c3e3c4b4bed26210926dfb57b
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Nov 20 18:45:03 2006 +0100

    [POWERPC] spufs: Return correct event for data storage interrupt
    
    When we attempt an MFC DMA to an unmapped address, the event
    returned from spu_run should be SPE_EVENT_SPE_DATA_STORAGE,
    not SPE_EVENT_INVALID_DMA.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index a4a0080c2233..88a41d83a79b 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -26,6 +26,7 @@ void spufs_dma_callback(struct spu *spu, int type)
 	} else {
 		switch (type) {
 		case SPE_EVENT_DMA_ALIGNMENT:
+		case SPE_EVENT_SPE_DATA_STORAGE:
 		case SPE_EVENT_INVALID_DMA:
 			force_sig(SIGBUS, /* info, */ current);
 			break;

commit 0afacde3df4c9980f505d9afd7cb0058389732ca
Author: arnd@arndb.de <arnd@arndb.de>
Date:   Tue Oct 24 18:31:18 2006 +0200

    [POWERPC] spufs: allow isolated mode apps by starting the SPE loader
    
    This patch adds general support for isolated mode SPE apps.
    
    Isolated apps are started indirectly, by a dedicated loader "kernel".
    This patch starts the loader when spe_create is invoked with the
    ISOLATE flag. We do this at spe_create time to allow libspe to pass the
    isolated app in before calling spe_run.
    
    The loader is read from the device tree, at the location
    "/spu-isolation/loader". If the loader is not present, an attempt to
    start an isolated SPE binary will fail with -ENODEV.
    
    Update: loader needs to be correctly aligned - copy to a kmalloced buf.
    Update: remove workaround for systemsim/spurom 'L-bit' bug, which has
            been fixed.
    Update: don't write to runcntl on spu_run_init: SPU is already running.
    Update: do spu_setup_isolated earlier
    
    Tested on systemsim.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 0c03a04b6a30..a4a0080c2233 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -1,3 +1,5 @@
+#define DEBUG
+
 #include <linux/wait.h>
 #include <linux/ptrace.h>
 
@@ -56,12 +58,12 @@ static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
 	if ((ret = spu_acquire_runnable(ctx)) != 0)
 		return ret;
 
-	if (ctx->flags & SPU_CREATE_ISOLATE)
-		runcntl |= SPU_RUNCNTL_ISOLATE;
-	else
+	/* if we're in isolated mode, we would have started the SPU
+	 * earlier, so don't do it again now. */
+	if (!(ctx->flags & SPU_CREATE_ISOLATE)) {
 		ctx->ops->npc_write(ctx, *npc);
-
-	ctx->ops->runcntl_write(ctx, runcntl);
+		ctx->ops->runcntl_write(ctx, runcntl);
+	}
 	return 0;
 }
 

commit 5737edd1ddbde5ab7f63bb3cb36015edbdb7c295
Author: Mark Nutter <mnutter@us.ibm.com>
Date:   Tue Oct 24 18:31:16 2006 +0200

    [POWERPC] spufs: add support for nonschedulable contexts
    
    This adds two new flags to spu_create:
    
    SPU_CREATE_NONSCHED: create a context that is never moved
    away from an SPE once it has started running. This flag
    can only be used by tasks with the CAP_SYS_NICE capability.
    
    SPU_CREATE_ISOLATED: create a nonschedulable context that
    enters isolation mode upon first run. This requires the
    SPU_CREATE_NONSCHED flag.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 63df8cf4ba16..0c03a04b6a30 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -51,11 +51,17 @@ static inline int spu_stopped(struct spu_context *ctx, u32 * stat)
 static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
 {
 	int ret;
+	unsigned long runcntl = SPU_RUNCNTL_RUNNABLE;
 
 	if ((ret = spu_acquire_runnable(ctx)) != 0)
 		return ret;
-	ctx->ops->npc_write(ctx, *npc);
-	ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_RUNNABLE);
+
+	if (ctx->flags & SPU_CREATE_ISOLATE)
+		runcntl |= SPU_RUNCNTL_ISOLATE;
+	else
+		ctx->ops->npc_write(ctx, *npc);
+
+	ctx->ops->runcntl_write(ctx, runcntl);
 	return 0;
 }
 

commit 9add11daeee2f6d69f6b86237f197824332a4a3b
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Oct 4 17:26:14 2006 +0200

    [POWERPC] spufs: implement error event delivery to user space
    
    This tries to fix spufs so we have an interface closer to what is
    specified in the man page for events returned in the third argument of
    spu_run.
    
    Fortunately, libspe has never been using the returned contents of that
    register, as they were the same as the return code of spu_run (duh!).
    
    Unlike the specification that we never implemented correctly, we now
    require a SPU_CREATE_EVENTS_ENABLED flag passed to spu_create, in
    order to get the new behavior. When this flag is not passed, spu_run
    will simply ignore the third argument now.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 483c8b76232c..63df8cf4ba16 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -14,6 +14,26 @@ void spufs_stop_callback(struct spu *spu)
 	wake_up_all(&ctx->stop_wq);
 }
 
+void spufs_dma_callback(struct spu *spu, int type)
+{
+	struct spu_context *ctx = spu->ctx;
+
+	if (ctx->flags & SPU_CREATE_EVENTS_ENABLED) {
+		ctx->event_return |= type;
+		wake_up_all(&ctx->stop_wq);
+	} else {
+		switch (type) {
+		case SPE_EVENT_DMA_ALIGNMENT:
+		case SPE_EVENT_INVALID_DMA:
+			force_sig(SIGBUS, /* info, */ current);
+			break;
+		case SPE_EVENT_SPE_ERROR:
+			force_sig(SIGILL, /* info */ current);
+			break;
+		}
+	}
+}
+
 static inline int spu_stopped(struct spu_context *ctx, u32 * stat)
 {
 	struct spu *spu;
@@ -28,8 +48,7 @@ static inline int spu_stopped(struct spu_context *ctx, u32 * stat)
 	return (!(*stat & 0x1) || pte_fault || spu->class_0_pending) ? 1 : 0;
 }
 
-static inline int spu_run_init(struct spu_context *ctx, u32 * npc,
-			       u32 * status)
+static inline int spu_run_init(struct spu_context *ctx, u32 * npc)
 {
 	int ret;
 
@@ -72,7 +91,7 @@ static inline int spu_reacquire_runnable(struct spu_context *ctx, u32 *npc,
 		       SPU_STATUS_STOPPED_BY_HALT)) {
 		return *status;
 	}
-	if ((ret = spu_run_init(ctx, npc, status)) != 0)
+	if ((ret = spu_run_init(ctx, npc)) != 0)
 		return ret;
 	return 0;
 }
@@ -177,46 +196,49 @@ static inline int spu_process_events(struct spu_context *ctx)
 }
 
 long spufs_run_spu(struct file *file, struct spu_context *ctx,
-		   u32 * npc, u32 * status)
+		   u32 *npc, u32 *event)
 {
 	int ret;
+	u32 status;
 
 	if (down_interruptible(&ctx->run_sema))
 		return -ERESTARTSYS;
 
-	ret = spu_run_init(ctx, npc, status);
+	ctx->event_return = 0;
+	ret = spu_run_init(ctx, npc);
 	if (ret)
 		goto out;
 
 	do {
-		ret = spufs_wait(ctx->stop_wq, spu_stopped(ctx, status));
+		ret = spufs_wait(ctx->stop_wq, spu_stopped(ctx, &status));
 		if (unlikely(ret))
 			break;
-		if ((*status & SPU_STATUS_STOPPED_BY_STOP) &&
-		    (*status >> SPU_STOP_STATUS_SHIFT == 0x2104)) {
+		if ((status & SPU_STATUS_STOPPED_BY_STOP) &&
+		    (status >> SPU_STOP_STATUS_SHIFT == 0x2104)) {
 			ret = spu_process_callback(ctx);
 			if (ret)
 				break;
-			*status &= ~SPU_STATUS_STOPPED_BY_STOP;
+			status &= ~SPU_STATUS_STOPPED_BY_STOP;
 		}
 		if (unlikely(ctx->state != SPU_STATE_RUNNABLE)) {
-			ret = spu_reacquire_runnable(ctx, npc, status);
+			ret = spu_reacquire_runnable(ctx, npc, &status);
 			if (ret)
 				goto out;
 			continue;
 		}
 		ret = spu_process_events(ctx);
 
-	} while (!ret && !(*status & (SPU_STATUS_STOPPED_BY_STOP |
+	} while (!ret && !(status & (SPU_STATUS_STOPPED_BY_STOP |
 				      SPU_STATUS_STOPPED_BY_HALT)));
 
 	ctx->ops->runcntl_stop(ctx);
-	ret = spu_run_fini(ctx, npc, status);
+	ret = spu_run_fini(ctx, npc, &status);
 	if (!ret)
-		ret = *status;
+		ret = status;
 	spu_yield(ctx);
 
 out:
+	*event = ctx->event_return;
 	up(&ctx->run_sema);
 	return ret;
 }

commit cfff5b23bad2b7a4754f7c126a174a5867dfcca5
Author: Dave Jones <davej@redhat.com>
Date:   Fri Mar 31 23:53:09 2006 -0500

    [PATCH] powerpc/cell: compile fixes
    
    Missing include for __NR_syscalls, and missing sys_splice() that
    causes build-time failure due to compile-time bounds check on
    spu_syscall_table.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index c04e078c0fe5..483c8b76232c 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -2,6 +2,7 @@
 #include <linux/ptrace.h>
 
 #include <asm/spu.h>
+#include <asm/unistd.h>
 
 #include "spufs.h"
 

commit 2dd14934c9138c562d93c501e88c6d6f061eb8ba
Author: Arnd Bergmann <abergman@de.ibm.com>
Date:   Thu Mar 23 00:00:09 2006 +0100

    [PATCH] spufs: allow SPU code to do syscalls
    
    An SPU does not have a way to implement system calls
    itself, but it can create intercepts to the kernel.
    
    This patch uses the method defined by the JSRE interface
    for C99 host library calls from an SPU to implement
    Linux system calls. It uses the reserved SPU stop code
    0x2104 for this, using the structure layout and syscall
    numbers for ppc64-linux.
    
    I'm still undecided wether it is better to have a list
    of allowed syscalls or a list of forbidden syscalls,
    since we can't allow an SPU to call all syscalls that
    are defined for ppc64-linux.
    
    This patch implements the easier choice of them, with a
    blacklist that only prevents an SPU from calling anything
    that interacts with its own execution, e.g fork, execve,
    clone, vfork, exit, spu_run and spu_create and everything
    that deals with signals.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
index 18ea8866c61a..c04e078c0fe5 100644
--- a/arch/powerpc/platforms/cell/spufs/run.c
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -76,6 +76,90 @@ static inline int spu_reacquire_runnable(struct spu_context *ctx, u32 *npc,
 	return 0;
 }
 
+/*
+ * SPU syscall restarting is tricky because we violate the basic
+ * assumption that the signal handler is running on the interrupted
+ * thread. Here instead, the handler runs on PowerPC user space code,
+ * while the syscall was called from the SPU.
+ * This means we can only do a very rough approximation of POSIX
+ * signal semantics.
+ */
+int spu_handle_restartsys(struct spu_context *ctx, long *spu_ret,
+			  unsigned int *npc)
+{
+	int ret;
+
+	switch (*spu_ret) {
+	case -ERESTARTSYS:
+	case -ERESTARTNOINTR:
+		/*
+		 * Enter the regular syscall restarting for
+		 * sys_spu_run, then restart the SPU syscall
+		 * callback.
+		 */
+		*npc -= 8;
+		ret = -ERESTARTSYS;
+		break;
+	case -ERESTARTNOHAND:
+	case -ERESTART_RESTARTBLOCK:
+		/*
+		 * Restart block is too hard for now, just return -EINTR
+		 * to the SPU.
+		 * ERESTARTNOHAND comes from sys_pause, we also return
+		 * -EINTR from there.
+		 * Assume that we need to be restarted ourselves though.
+		 */
+		*spu_ret = -EINTR;
+		ret = -ERESTARTSYS;
+		break;
+	default:
+		printk(KERN_WARNING "%s: unexpected return code %ld\n",
+			__FUNCTION__, *spu_ret);
+		ret = 0;
+	}
+	return ret;
+}
+
+int spu_process_callback(struct spu_context *ctx)
+{
+	struct spu_syscall_block s;
+	u32 ls_pointer, npc;
+	char *ls;
+	long spu_ret;
+	int ret;
+
+	/* get syscall block from local store */
+	npc = ctx->ops->npc_read(ctx);
+	ls = ctx->ops->get_ls(ctx);
+	ls_pointer = *(u32*)(ls + npc);
+	if (ls_pointer > (LS_SIZE - sizeof(s)))
+		return -EFAULT;
+	memcpy(&s, ls + ls_pointer, sizeof (s));
+
+	/* do actual syscall without pinning the spu */
+	ret = 0;
+	spu_ret = -ENOSYS;
+	npc += 4;
+
+	if (s.nr_ret < __NR_syscalls) {
+		spu_release(ctx);
+		/* do actual system call from here */
+		spu_ret = spu_sys_callback(&s);
+		if (spu_ret <= -ERESTARTSYS) {
+			ret = spu_handle_restartsys(ctx, &spu_ret, &npc);
+		}
+		spu_acquire(ctx);
+		if (ret == -ERESTARTSYS)
+			return ret;
+	}
+
+	/* write result, jump over indirect pointer */
+	memcpy(ls + ls_pointer, &spu_ret, sizeof (spu_ret));
+	ctx->ops->npc_write(ctx, npc);
+	ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_RUNNABLE);
+	return ret;
+}
+
 static inline int spu_process_events(struct spu_context *ctx)
 {
 	struct spu *spu = ctx->spu;
@@ -107,6 +191,13 @@ long spufs_run_spu(struct file *file, struct spu_context *ctx,
 		ret = spufs_wait(ctx->stop_wq, spu_stopped(ctx, status));
 		if (unlikely(ret))
 			break;
+		if ((*status & SPU_STATUS_STOPPED_BY_STOP) &&
+		    (*status >> SPU_STOP_STATUS_SHIFT == 0x2104)) {
+			ret = spu_process_callback(ctx);
+			if (ret)
+				break;
+			*status &= ~SPU_STATUS_STOPPED_BY_STOP;
+		}
 		if (unlikely(ctx->state != SPU_STATE_RUNNABLE)) {
 			ret = spu_reacquire_runnable(ctx, npc, status);
 			if (ret)

commit ce8ab8541203f6c7be5b2eeaa97f14f1d8d44e4f
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Jan 4 20:31:29 2006 +0100

    [PATCH] spufs: move spu_run call to its own file
    
    The logic for sys_spu_run keeps growing and it does
    not really belong into file.c any more since we
    moved away from using regular file operations to our
    own syscall.
    
    No functional change in here.
    
    Signed-off-by: Arnd Bergmann <arndb@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/run.c b/arch/powerpc/platforms/cell/spufs/run.c
new file mode 100644
index 000000000000..18ea8866c61a
--- /dev/null
+++ b/arch/powerpc/platforms/cell/spufs/run.c
@@ -0,0 +1,131 @@
+#include <linux/wait.h>
+#include <linux/ptrace.h>
+
+#include <asm/spu.h>
+
+#include "spufs.h"
+
+/* interrupt-level stop callback function. */
+void spufs_stop_callback(struct spu *spu)
+{
+	struct spu_context *ctx = spu->ctx;
+
+	wake_up_all(&ctx->stop_wq);
+}
+
+static inline int spu_stopped(struct spu_context *ctx, u32 * stat)
+{
+	struct spu *spu;
+	u64 pte_fault;
+
+	*stat = ctx->ops->status_read(ctx);
+	if (ctx->state != SPU_STATE_RUNNABLE)
+		return 1;
+	spu = ctx->spu;
+	pte_fault = spu->dsisr &
+	    (MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED);
+	return (!(*stat & 0x1) || pte_fault || spu->class_0_pending) ? 1 : 0;
+}
+
+static inline int spu_run_init(struct spu_context *ctx, u32 * npc,
+			       u32 * status)
+{
+	int ret;
+
+	if ((ret = spu_acquire_runnable(ctx)) != 0)
+		return ret;
+	ctx->ops->npc_write(ctx, *npc);
+	ctx->ops->runcntl_write(ctx, SPU_RUNCNTL_RUNNABLE);
+	return 0;
+}
+
+static inline int spu_run_fini(struct spu_context *ctx, u32 * npc,
+			       u32 * status)
+{
+	int ret = 0;
+
+	*status = ctx->ops->status_read(ctx);
+	*npc = ctx->ops->npc_read(ctx);
+	spu_release(ctx);
+
+	if (signal_pending(current))
+		ret = -ERESTARTSYS;
+	if (unlikely(current->ptrace & PT_PTRACED)) {
+		if ((*status & SPU_STATUS_STOPPED_BY_STOP)
+		    && (*status >> SPU_STOP_STATUS_SHIFT) == 0x3fff) {
+			force_sig(SIGTRAP, current);
+			ret = -ERESTARTSYS;
+		}
+	}
+	return ret;
+}
+
+static inline int spu_reacquire_runnable(struct spu_context *ctx, u32 *npc,
+				         u32 *status)
+{
+	int ret;
+
+	if ((ret = spu_run_fini(ctx, npc, status)) != 0)
+		return ret;
+	if (*status & (SPU_STATUS_STOPPED_BY_STOP |
+		       SPU_STATUS_STOPPED_BY_HALT)) {
+		return *status;
+	}
+	if ((ret = spu_run_init(ctx, npc, status)) != 0)
+		return ret;
+	return 0;
+}
+
+static inline int spu_process_events(struct spu_context *ctx)
+{
+	struct spu *spu = ctx->spu;
+	u64 pte_fault = MFC_DSISR_PTE_NOT_FOUND | MFC_DSISR_ACCESS_DENIED;
+	int ret = 0;
+
+	if (spu->dsisr & pte_fault)
+		ret = spu_irq_class_1_bottom(spu);
+	if (spu->class_0_pending)
+		ret = spu_irq_class_0_bottom(spu);
+	if (!ret && signal_pending(current))
+		ret = -ERESTARTSYS;
+	return ret;
+}
+
+long spufs_run_spu(struct file *file, struct spu_context *ctx,
+		   u32 * npc, u32 * status)
+{
+	int ret;
+
+	if (down_interruptible(&ctx->run_sema))
+		return -ERESTARTSYS;
+
+	ret = spu_run_init(ctx, npc, status);
+	if (ret)
+		goto out;
+
+	do {
+		ret = spufs_wait(ctx->stop_wq, spu_stopped(ctx, status));
+		if (unlikely(ret))
+			break;
+		if (unlikely(ctx->state != SPU_STATE_RUNNABLE)) {
+			ret = spu_reacquire_runnable(ctx, npc, status);
+			if (ret)
+				goto out;
+			continue;
+		}
+		ret = spu_process_events(ctx);
+
+	} while (!ret && !(*status & (SPU_STATUS_STOPPED_BY_STOP |
+				      SPU_STATUS_STOPPED_BY_HALT)));
+
+	ctx->ops->runcntl_stop(ctx);
+	ret = spu_run_fini(ctx, npc, status);
+	if (!ret)
+		ret = *status;
+	spu_yield(ctx);
+
+out:
+	up(&ctx->run_sema);
+	return ret;
+}
+
