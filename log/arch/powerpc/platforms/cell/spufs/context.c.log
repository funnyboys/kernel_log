commit de6cc6515a445d5d81cad2dee899a0be1a6317f8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:02 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 153
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 or at your option any
      later version this program is distributed in the hope that it will
      be useful but without any warranty without even the implied warranty
      of merchantability or fitness for a particular purpose see the gnu
      general public license for more details you should have received a
      copy of the gnu general public license along with this program if
      not write to the free software foundation inc 675 mass ave cambridge
      ma 02139 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 77 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Armijn Hemel <armijn@tjaldur.nl>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.837555891@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index b500b17254a0..7a39cc414f09 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -1,23 +1,10 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * SPU file system -- SPU context management
  *
  * (C) Copyright IBM Deutschland Entwicklung GmbH 2005
  *
  * Author: Arnd Bergmann <arndb@de.ibm.com>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2, or (at your option)
- * any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  */
 
 #include <linux/fs.h>

commit 6e84f31522f931027bf695752087ece278c10d3f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:29 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/mm.h>
    
    We are going to split <linux/sched/mm.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/mm.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    The APIs that are going to be moved first are:
    
       mm_alloc()
       __mmdrop()
       mmdrop()
       mmdrop_async_fn()
       mmdrop_async()
       mmget_not_zero()
       mmput()
       mmput_async()
       get_task_mm()
       mm_access()
       mm_release()
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 3b4152faeb1f..b500b17254a0 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -25,6 +25,8 @@
 #include <linux/slab.h>
 #include <linux/atomic.h>
 #include <linux/sched.h>
+#include <linux/sched/mm.h>
+
 #include <asm/spu.h>
 #include <asm/spu_csa.h>
 #include "spufs.h"

commit f2dec1eae8029bb056a3c1b2d3373681fa8e3e7a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 16 21:04:38 2014 +0000

    powerpc: cell: Use ktime_get_ns()
    
    Replace the ever recurring:
            ts = ktime_get_ts();
            ns = timespec_to_ns(&ts);
    with
            ns = ktime_get_ns();
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 9c6790d17eda..3b4152faeb1f 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -36,7 +36,6 @@ atomic_t nr_spu_contexts = ATOMIC_INIT(0);
 struct spu_context *alloc_spu_context(struct spu_gang *gang)
 {
 	struct spu_context *ctx;
-	struct timespec ts;
 
 	ctx = kzalloc(sizeof *ctx, GFP_KERNEL);
 	if (!ctx)
@@ -67,8 +66,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	__spu_update_sched_info(ctx);
 	spu_set_timeslice(ctx);
 	ctx->stats.util_state = SPU_UTIL_IDLE_LOADED;
-	ktime_get_ts(&ts);
-	ctx->stats.tstamp = timespec_to_ns(&ts);
+	ctx->stats.tstamp = ktime_get_ns();
 
 	atomic_inc(&nr_spu_contexts);
 	goto out;

commit ead53f22dc646d91a1b6201b9f44dd47d7d88c34
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jul 22 14:24:04 2011 -0400

    powerpc: remove non-required uses of include <linux/module.h>
    
    None of the files touched here are modules, and they are not
    exporting any symbols either -- so there is no need to be including
    the module.h.  Builds of all the files remains successful.
    
    Even kernel/module.c does not need to include it, since it includes
    linux/moduleloader.h instead.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 0430cb278156..9c6790d17eda 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -22,7 +22,6 @@
 
 #include <linux/fs.h>
 #include <linux/mm.h>
-#include <linux/module.h>
 #include <linux/slab.h>
 #include <linux/atomic.h>
 #include <linux/sched.h>

commit 62fe91bba2325593e00698f902b3201629dad571
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri May 27 14:25:11 2011 -0400

    powerpc: Fix up implicit sched.h users
    
    They are getting it through device.h --> module.h path, but we want
    to clean that up.  This is a sample of what will happen if we don't:
    
      pseries/iommu.c: In function 'tce_build_pSeriesLP':
      pseries/iommu.c:136: error: implicit declaration of function 'show_stack'
    
      pseries/eeh.c: In function 'eeh_token_to_phys':
      pseries/eeh.c:359: error: 'init_mm' undeclared (first use in this function)
    
      pseries/eeh_event.c: In function 'eeh_event_handler':
      pseries/eeh_event.c:63: error: implicit declaration of function 'daemonize'
      pseries/eeh_event.c:64: error: implicit declaration of function 'set_current_state'
      pseries/eeh_event.c:64: error: 'TASK_INTERRUPTIBLE' undeclared (first use in this function)
      pseries/eeh_event.c:64: error: (Each undeclared identifier is reported only once
      pseries/eeh_event.c:64: error: for each function it appears in.)
      pseries/eeh_event.c: In function 'eeh_thread_launcher':
      pseries/eeh_event.c:109: error: 'CLONE_KERNEL' undeclared (first use in this function)
    
      hotplug-cpu.c: In function 'pseries_mach_cpu_die':
      hotplug-cpu.c:115: error: implicit declaration of function 'idle_task_exit'
    
      kernel/swsusp_64.c: In function 'do_after_copyback':
      kernel/swsusp_64.c:17: error: implicit declaration of function 'touch_softlockup_watchdog'
    
      cell/spufs/context.c: In function 'alloc_spu_context':
      cell/spufs/context.c:60: error: implicit declaration of function 'get_task_mm'
      cell/spufs/context.c:60: warning: assignment makes pointer from integer without a cast
      cell/spufs/context.c: In function 'spu_forget':
      cell/spufs/context.c:127: error: implicit declaration of function 'mmput'
    
      pasemi/dma_lib.c: In function 'pasemi_dma_stop_chan':
      pasemi/dma_lib.c:332: error: implicit declaration of function 'cond_resched'
    
      sysdev/fsl_lbc.c: In function 'fsl_lbc_ctrl_irq':
      sysdev/fsl_lbc.c:247: error: 'TASK_NORMAL' undeclared (first use in this function)
    
    Add in sched.h so these get the definitions they are looking for.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index bf4d41d8fa14..0430cb278156 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -25,6 +25,7 @@
 #include <linux/module.h>
 #include <linux/slab.h>
 #include <linux/atomic.h>
+#include <linux/sched.h>
 #include <asm/spu.h>
 #include <asm/spu_csa.h>
 #include "spufs.h"

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 0c87bcd2452a..bf4d41d8fa14 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -24,7 +24,7 @@
 #include <linux/mm.h>
 #include <linux/module.h>
 #include <linux/slab.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #include <asm/spu.h>
 #include <asm/spu_csa.h>
 #include "spufs.h"

commit ae142e0c52b38e44d28b12f77c6e7faa96f7a069
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 12 04:31:52 2009 +0000

    powerpc/sputrace: Use the generic event tracer
    
    I wrote sputrace before generic tracing infrastrucure was available.
    Now that we have the generic event tracer we can convert it over and
    remove a lot of code:
    
      8 files changed, 45 insertions(+), 285 deletions(-)
    
    To use it make sure CONFIG_EVENT_TRACING is enabled and then enable
    the spufs trace channel by
    
      echo 1 > /sys/kernel/debug/tracing/events/spufs/spufs_context/enable
    
    and then read the trace records using e.g.
    
      cat /sys/kernel/debug/tracing/trace
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index db5398c0339f..0c87bcd2452a 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -28,6 +28,7 @@
 #include <asm/spu.h>
 #include <asm/spu_csa.h>
 #include "spufs.h"
+#include "sputrace.h"
 
 
 atomic_t nr_spu_contexts = ATOMIC_INIT(0);

commit 8f748aae4b5eda6a6ec3ab3554e7e19c7702ccc2
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Feb 23 21:44:37 2009 +0000

    powerpc/spufs: Initialize ctx->stats.tstamp correctly
    
    spuctx_switch_state() warns if ktime goes backwards, but it
    sometimes compares an uninitialized value, which showed that
    the data was unreliable when we actually saw the warning.
    
    Initialize it to the current time in order to get correct data.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 6653ddbed048..db5398c0339f 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -35,6 +35,8 @@ atomic_t nr_spu_contexts = ATOMIC_INIT(0);
 struct spu_context *alloc_spu_context(struct spu_gang *gang)
 {
 	struct spu_context *ctx;
+	struct timespec ts;
+
 	ctx = kzalloc(sizeof *ctx, GFP_KERNEL);
 	if (!ctx)
 		goto out;
@@ -64,6 +66,8 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	__spu_update_sched_info(ctx);
 	spu_set_timeslice(ctx);
 	ctx->stats.util_state = SPU_UTIL_IDLE_LOADED;
+	ktime_get_ts(&ts);
+	ctx->stats.tstamp = timespec_to_ns(&ts);
 
 	atomic_inc(&nr_spu_contexts);
 	goto out;

commit 87ff6090bfe416c71730654ab53cd4ecffdd675e
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Tue Jul 1 10:22:50 2008 +1000

    powerpc/spufs: avoid magic numbers for mapping sizes
    
    Use a set of #defines for the size of context mappings, instead of
    magic numbers.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 177735f79317..6653ddbed048 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -130,17 +130,17 @@ void spu_unmap_mappings(struct spu_context *ctx)
 	if (ctx->local_store)
 		unmap_mapping_range(ctx->local_store, 0, LS_SIZE, 1);
 	if (ctx->mfc)
-		unmap_mapping_range(ctx->mfc, 0, 0x1000, 1);
+		unmap_mapping_range(ctx->mfc, 0, SPUFS_MFC_MAP_SIZE, 1);
 	if (ctx->cntl)
-		unmap_mapping_range(ctx->cntl, 0, 0x1000, 1);
+		unmap_mapping_range(ctx->cntl, 0, SPUFS_CNTL_MAP_SIZE, 1);
 	if (ctx->signal1)
-		unmap_mapping_range(ctx->signal1, 0, PAGE_SIZE, 1);
+		unmap_mapping_range(ctx->signal1, 0, SPUFS_SIGNAL_MAP_SIZE, 1);
 	if (ctx->signal2)
-		unmap_mapping_range(ctx->signal2, 0, PAGE_SIZE, 1);
+		unmap_mapping_range(ctx->signal2, 0, SPUFS_SIGNAL_MAP_SIZE, 1);
 	if (ctx->mss)
-		unmap_mapping_range(ctx->mss, 0, 0x1000, 1);
+		unmap_mapping_range(ctx->mss, 0, SPUFS_MSS_MAP_SIZE, 1);
 	if (ctx->psmap)
-		unmap_mapping_range(ctx->psmap, 0, 0x20000, 1);
+		unmap_mapping_range(ctx->psmap, 0, SPUFS_PS_MAP_SIZE, 1);
 	mutex_unlock(&ctx->mapping_lock);
 }
 

commit 3734dfc68b64d8ca202c799280daf28c2424659d
Author: Julio M. Merino Vidal <jmerino@ac.upc.edu>
Date:   Wed Apr 30 15:21:17 2008 +1000

    [POWERPC] spufs: trace spu_acquire_saved events
    
    The sputrace module contained a trace entry for spu_acquire_saved, but
    this marker was not placed anywhere. Fix this by adding a marker to the
    routine.
    
    Signed-off-by: Julio M. Merino Vidal <jmerino@ac.upc.edu>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 5c5010078042..177735f79317 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -152,6 +152,8 @@ int spu_acquire_saved(struct spu_context *ctx)
 {
 	int ret;
 
+	spu_context_nospu_trace(spu_acquire_saved__enter, ctx);
+
 	ret = spu_acquire(ctx);
 	if (ret)
 		return ret;

commit 534578816f028b0dc675fdcc3314016d98588327
Author: Julio M. Merino Vidal <jmerino@ac.upc.edu>
Date:   Wed Apr 30 15:12:30 2008 +1000

    [POWERPC] spufs: add marker for destroy_spu_context
    
    The sputrace module contained a reference to a marker for
    destroy_spu_context, but this marker did not appear in the code. Fix
    this by adding a marker in the function.
    
    Signed-off-by: Julio M. Merino Vidal <jmerino@ac.upc.edu>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 91f2d4d9aefd..5c5010078042 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -78,6 +78,7 @@ void destroy_spu_context(struct kref *kref)
 {
 	struct spu_context *ctx;
 	ctx = container_of(kref, struct spu_context, kref);
+	spu_context_nospu_trace(destroy_spu_context__enter, ctx);
 	mutex_lock(&ctx->state_mutex);
 	spu_deactivate(ctx);
 	mutex_unlock(&ctx->state_mutex);

commit 5158e9b5218bd3799c9fa8c401ad24d7f0c0a0a1
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 29 17:08:38 2008 +1000

    [POWERPC] spufs: add context switch notification log
    
    There are userspace instrumentation tools that need to monitor spu
    context switches. This patch adds a new file called 'switch_log' to
    each spufs context directory that can be used to monitor the context
    switches.
    
    Context switch in/out and exit from spu_run are monitored after the
    file was first opened and can be read from it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 0ad83aeb70b1..91f2d4d9aefd 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -88,6 +88,7 @@ void destroy_spu_context(struct kref *kref)
 		kref_put(ctx->prof_priv_kref, ctx->prof_priv_release);
 	BUG_ON(!list_empty(&ctx->rq));
 	atomic_dec(&nr_spu_contexts);
+	kfree(ctx->switch_log);
 	kfree(ctx);
 }
 

commit c368392a9951e6e25e2e2f9268153f1e9365e2c2
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Tue Mar 11 12:46:18 2008 +1100

    [POWERPC] spufs: fix rescheduling of non-runnable contexts
    
    At present, we can hit the BUG_ON in __spu_update_sched_info by reading
    the regs file of a context between two calls to spu_run. The
    spu_release_saved called by spufs_regs_read() is resulting in the (now
    non-runnable) context being placed back on the run queue, so the next
    call to spu_run ends up in the bug condition.
    
    This change uses the SPU_SCHED_SPU_RUN flag to only reschedule a context
    if it's still in spu_run().
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index cf6c2c89211d..0ad83aeb70b1 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -170,7 +170,8 @@ void spu_release_saved(struct spu_context *ctx)
 {
 	BUG_ON(ctx->state != SPU_STATE_SAVED);
 
-	if (test_and_clear_bit(SPU_SCHED_WAS_ACTIVE, &ctx->sched_flags))
+	if (test_and_clear_bit(SPU_SCHED_WAS_ACTIVE, &ctx->sched_flags) &&
+			test_bit(SPU_SCHED_SPU_RUN, &ctx->sched_flags))
 		spu_activate(ctx, 0);
 
 	spu_release(ctx);

commit 0111a701867a796a7ca6ecbc385e4befc9f35066
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Wed Feb 27 19:08:13 2008 +1100

    [POWERPC] spufs: fix invalid scheduling of forgotten contexts
    
    At present, we have a situation where a context with no owner is
    re-scheduled by spu_forget:
    
            Thread 1: reading regs file     Thread 2: context owner
    
                                            spu_forget()
                                                    - ctx->owner = NULL
                                                    - set SPU_SCHED_WAS_ACTIVE
    
            spu_acquire_saved()
            - context is in saved state
    
            spu_release_saved()
            - SPU_SCHED_WAS_ACTIVE is set,
              so spu_activate() the context,
              which now has no owner
    
    In spu_forget(), we shouldn't be requesting a re-schedule by setting
    SPU_SCHED_WAS_ACTIVE. This change removes the set_bit in spu_forget(),
    so that spu_release_saved() doesn't reinsert this destroyed context on
    to the run queue.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 133995ed5cc7..cf6c2c89211d 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -109,13 +109,12 @@ void spu_forget(struct spu_context *ctx)
 
 	/*
 	 * This is basically an open-coded spu_acquire_saved, except that
-	 * we don't acquire the state mutex interruptible.
+	 * we don't acquire the state mutex interruptible, and we don't
+	 * want this context to be rescheduled on release.
 	 */
 	mutex_lock(&ctx->state_mutex);
-	if (ctx->state != SPU_STATE_SAVED) {
-		set_bit(SPU_SCHED_WAS_ACTIVE, &ctx->sched_flags);
+	if (ctx->state != SPU_STATE_SAVED)
 		spu_deactivate(ctx);
-	}
 
 	mm = ctx->owner;
 	ctx->owner = NULL;

commit 9156ad48338e0306e508ead5c0d9986050744475
Merge: fa28237cfcc5 8f7b3d156d34
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jan 24 10:07:21 2008 +1100

    Merge branch 'linux-2.6'

commit aed3a8c9bb1a8623a618232087c5ff62718e3b9a
Author: Bob Nelson <rrnelson@linux.vnet.ibm.com>
Date:   Sat Dec 15 01:27:30 2007 +1100

    [POWERPC] Oprofile: Remove dependency on spufs module
    
    This removes an OProfile dependency on the spufs module.  This
    dependency was causing a problem for multiplatform systems that are
    built with support for Oprofile on Cell but try to load the oprofile
    module on a non-Cell system.
    
    Signed-off-by: Bob Nelson <rrnelson@us.ibm.com>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Acked-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 9cb081c26e71..adf0a030d6fe 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -190,19 +190,3 @@ void spu_release_saved(struct spu_context *ctx)
 	spu_release(ctx);
 }
 
-void spu_set_profile_private_kref(struct spu_context *ctx,
-				  struct kref *prof_info_kref,
-				  void ( * prof_info_release) (struct kref *kref))
-{
-	ctx->prof_priv_kref = prof_info_kref;
-	ctx->prof_priv_release = prof_info_release;
-}
-EXPORT_SYMBOL_GPL(spu_set_profile_private_kref);
-
-void *spu_get_profile_private_kref(struct spu_context *ctx)
-{
-	return ctx->prof_priv_kref;
-}
-EXPORT_SYMBOL_GPL(spu_get_profile_private_kref);
-
-

commit c9101bdb1b0c56a75a4618542d368fe5013946b9
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 20 16:39:59 2007 +0900

    [POWERPC] spufs: make state_mutex interruptible
    
    Make most places that use spu_acquire/spu_acquire_saved interruptible,
    this allows getting out of the spufs code when e.g. pressing ctrl+c.
    There are a few places where we get called e.g. from spufs teardown
    routines were we can't simply err out so these are left with a comment.
    For now I've also not touched the poll routines because it's open what
    libspe would expect in terms of interrupted system calls.
    
    Acked-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 290b10e45105..237e152d31dc 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -106,7 +106,17 @@ int put_spu_context(struct spu_context *ctx)
 void spu_forget(struct spu_context *ctx)
 {
 	struct mm_struct *mm;
-	spu_acquire_saved(ctx);
+
+	/*
+	 * This is basically an open-coded spu_acquire_saved, except that
+	 * we don't acquire the state mutex interruptible.
+	 */
+	mutex_lock(&ctx->state_mutex);
+	if (ctx->state != SPU_STATE_SAVED) {
+		set_bit(SPU_SCHED_WAS_ACTIVE, &ctx->sched_flags);
+		spu_deactivate(ctx);
+	}
+
 	mm = ctx->owner;
 	ctx->owner = NULL;
 	mmput(mm);
@@ -137,13 +147,20 @@ void spu_unmap_mappings(struct spu_context *ctx)
  * spu_acquire_saved - lock spu contex and make sure it is in saved state
  * @ctx:	spu contex to lock
  */
-void spu_acquire_saved(struct spu_context *ctx)
+int spu_acquire_saved(struct spu_context *ctx)
 {
-	spu_acquire(ctx);
+	int ret;
+
+	ret = spu_acquire(ctx);
+	if (ret)
+		return ret;
+
 	if (ctx->state != SPU_STATE_SAVED) {
 		set_bit(SPU_SCHED_WAS_ACTIVE, &ctx->sched_flags);
 		spu_deactivate(ctx);
 	}
+
+	return 0;
 }
 
 /**

commit e65c2f6fcebb9af0c3f53c796aff730dd657f5e7
Author: Luke Browning <lukebr@linux.vnet.ibm.com>
Date:   Thu Dec 20 16:39:59 2007 +0900

    [POWERPC] spufs: decouple spu scheduler from spufs_spu_run (asynchronous scheduling)
    
    Change spufs_spu_run so that the context is queued directly to the
    scheduler and the controlling thread advances directly to spufs_wait()
    for spe errors and exceptions.
    
    nosched contexts are treated the same as before.
    
    Fixes from Christoph Hellwig <hch@lst.de>
    
    Signed-off-by: Luke Browning <lukebr@linux.vnet.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 6fa24d38706e..290b10e45105 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -133,37 +133,6 @@ void spu_unmap_mappings(struct spu_context *ctx)
 	mutex_unlock(&ctx->mapping_lock);
 }
 
-/**
- * spu_acquire_runnable - lock spu contex and make sure it is in runnable state
- * @ctx:	spu contex to lock
- *
- * Note:
- *	Returns 0 and with the context locked on success
- *	Returns negative error and with the context _unlocked_ on failure.
- */
-int spu_acquire_runnable(struct spu_context *ctx, unsigned long flags)
-{
-	int ret = -EINVAL;
-
-	spu_acquire(ctx);
-	if (ctx->state == SPU_STATE_SAVED) {
-		/*
-		 * Context is about to be freed, so we can't acquire it anymore.
-		 */
-		if (!ctx->owner)
-			goto out_unlock;
-		ret = spu_activate(ctx, flags);
-		if (ret)
-			goto out_unlock;
-	}
-
-	return 0;
-
- out_unlock:
-	spu_release(ctx);
-	return ret;
-}
-
 /**
  * spu_acquire_saved - lock spu contex and make sure it is in saved state
  * @ctx:	spu contex to lock

commit 33bfd7a73861c3727482c6c1c1c2ef40054060b7
Author: Arnd Bergmann <arnd.bergmann@de.ibm.com>
Date:   Thu Dec 20 16:39:59 2007 +0900

    [POWERPC] spufs: block fault handlers in spu_acquire_runnable
    
    This change disables the logic that faults-in spu contexts under the
    covers from the page fault handler.  When a fault requires a runnable
    context, the handler will block until the context is scheduled by
    other means.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 9cb081c26e71..6fa24d38706e 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -52,6 +52,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	init_waitqueue_head(&ctx->wbox_wq);
 	init_waitqueue_head(&ctx->stop_wq);
 	init_waitqueue_head(&ctx->mfc_wq);
+	init_waitqueue_head(&ctx->run_wq);
 	ctx->state = SPU_STATE_SAVED;
 	ctx->ops = &spu_backing_ops;
 	ctx->owner = get_task_mm(current);

commit 9d78592ed72dbff1d8825207f8def07858a49768
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jul 25 21:31:09 2007 +1000

    [POWERPC] spusched: Fix initial timeslice calculation
    
    Currently we calculate the first timeslice for every context
    incorrectly - alloc_spu_context calls spu_set_timeslice before we set
    ctx->prio so we always calculate the longest possible timeslice for the
    lowest possible priority.
    
    This patch makes sure to update the schedule-related fields before
    calculating the timeslice and also makes sure we update the timeslice for
    a non-running context when entering spu_run so a priority change affects
    the context as soon as possible.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 6694f86d7000..9cb081c26e71 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -59,7 +59,8 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	INIT_LIST_HEAD(&ctx->aff_list);
 	if (gang)
 		spu_gang_add_ctx(gang, ctx);
-	ctx->cpus_allowed = current->cpus_allowed;
+
+	__spu_update_sched_info(ctx);
 	spu_set_timeslice(ctx);
 	ctx->stats.util_state = SPU_UTIL_IDLE_LOADED;
 

commit 1474855d0878cced6f39f51f3c2bd7428b44cb1e
Author: Bob Nelson <rrnelson@linux.vnet.ibm.com>
Date:   Fri Jul 20 21:39:53 2007 +0200

    [CELL] oprofile: add support to OProfile for profiling CELL BE SPUs
    
    From: Maynard Johnson <mpjohn@us.ibm.com>
    
    This patch updates the existing arch/powerpc/oprofile/op_model_cell.c
    to add in the SPU profiling capabilities.  In addition, a 'cell' subdirectory
    was added to arch/powerpc/oprofile to hold Cell-specific SPU profiling code.
    Exports spu_set_profile_private_kref and spu_get_profile_private_kref which
    are used by OProfile to store private profile information in spufs data
    structures.
    
    Also incorporated several fixes from other patches (rrn).  Check pointer
    returned from kzalloc.  Eliminated unnecessary cast.  Better error
    handling and cleanup in the related area.  64-bit unsigned long parameter
    was being demoted to 32-bit unsigned int and eventually promoted back to
    unsigned long.
    
    Signed-off-by: Carl Love <carll@us.ibm.com>
    Signed-off-by: Maynard Johnson <mpjohn@us.ibm.com>
    Signed-off-by: Bob Nelson <rrnelson@us.ibm.com>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index a7efb999d65e..6694f86d7000 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -22,6 +22,7 @@
 
 #include <linux/fs.h>
 #include <linux/mm.h>
+#include <linux/module.h>
 #include <linux/slab.h>
 #include <asm/atomic.h>
 #include <asm/spu.h>
@@ -81,6 +82,8 @@ void destroy_spu_context(struct kref *kref)
 	spu_fini_csa(&ctx->csa);
 	if (ctx->gang)
 		spu_gang_remove_ctx(ctx->gang, ctx);
+	if (ctx->prof_priv_kref)
+		kref_put(ctx->prof_priv_kref, ctx->prof_priv_release);
 	BUG_ON(!list_empty(&ctx->rq));
 	atomic_dec(&nr_spu_contexts);
 	kfree(ctx);
@@ -185,3 +188,20 @@ void spu_release_saved(struct spu_context *ctx)
 
 	spu_release(ctx);
 }
+
+void spu_set_profile_private_kref(struct spu_context *ctx,
+				  struct kref *prof_info_kref,
+				  void ( * prof_info_release) (struct kref *kref))
+{
+	ctx->prof_priv_kref = prof_info_kref;
+	ctx->prof_priv_release = prof_info_release;
+}
+EXPORT_SYMBOL_GPL(spu_set_profile_private_kref);
+
+void *spu_get_profile_private_kref(struct spu_context *ctx)
+{
+	return ctx->prof_priv_kref;
+}
+EXPORT_SYMBOL_GPL(spu_get_profile_private_kref);
+
+

commit 8e68e2f248332a9c3fd4f08258f488c209bd3e0c
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jul 20 21:39:47 2007 +0200

    [CELL] spufs: extension of spu_create to support affinity definition
    
    This patch adds support for additional flags at spu_create, which relate
    to the establishment of affinity between contexts and contexts to memory.
    A fourth, optional, parameter is supported. This parameter represent
    a affinity neighbor of the context being created, and is used when defining
    SPU-SPU affinity.
    Affinity is represented as a doubly linked list of spu_contexts.
    
    Signed-off-by: Andre Detsch <adetsch@br.ibm.com>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 6b091ea1d192..a7efb999d65e 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -55,6 +55,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	ctx->ops = &spu_backing_ops;
 	ctx->owner = get_task_mm(current);
 	INIT_LIST_HEAD(&ctx->rq);
+	INIT_LIST_HEAD(&ctx->aff_list);
 	if (gang)
 		spu_gang_add_ctx(gang, ctx);
 	ctx->cpus_allowed = current->cpus_allowed;

commit 27b1ea091f0c088ecad0d492f37fbe7b8d54d7dc
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jul 20 21:39:34 2007 +0200

    [CELL] spufs: make sure context are scheduled again after spu_acquire_saved
    
    Currently a process is removed from the physical spu when spu_acquire_saved
    is saved but never put back.  This patch adds a new spu_release_saved
    that is to be paired with spu_acquire_saved and put the process back if
    it has been in RUNNABLE state before.
    
    Niether Jeremy not be are entirely happy about this exact patch because
    it adds another spu_activate call outside of the owner thread, but I
    feel this is the best short-term fix we can come up with.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 0e5e55f53c8b..6b091ea1d192 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -165,6 +165,22 @@ int spu_acquire_runnable(struct spu_context *ctx, unsigned long flags)
 void spu_acquire_saved(struct spu_context *ctx)
 {
 	spu_acquire(ctx);
-	if (ctx->state != SPU_STATE_SAVED)
+	if (ctx->state != SPU_STATE_SAVED) {
+		set_bit(SPU_SCHED_WAS_ACTIVE, &ctx->sched_flags);
 		spu_deactivate(ctx);
+	}
+}
+
+/**
+ * spu_release_saved - unlock spu context and return it to the runqueue
+ * @ctx:	context to unlock
+ */
+void spu_release_saved(struct spu_context *ctx)
+{
+	BUG_ON(ctx->state != SPU_STATE_SAVED);
+
+	if (test_and_clear_bit(SPU_SCHED_WAS_ACTIVE, &ctx->sched_flags))
+		spu_activate(ctx, 0);
+
+	spu_release(ctx);
 }

commit 27ec41d3a1d4df2b7cd190e93aad22ab86a72aa1
Author: Andre Detsch <adetsch@br.ibm.com>
Date:   Fri Jul 20 21:39:33 2007 +0200

    [CELL] spufs: add spu stats in sysfs and ctx stat file in spufs
    
    This patch exports per-context statistics in spufs as long as spu
    statistics in sysfs.
    
    It was formed by merging:
    "spufs: add spu stats in sysfs"   From: Christoph Hellwig
    "spufs: add stat file to spufs"   From: Christoph Hellwig
    "spufs: fix libassist accounting" From: Jeremy Kerr
    "spusched: fix spu utilization statistics" From: Luke Browning
    And some adjustments by myself, after suggestions on cbe-oss-dev.
    
    Having separate patches was making the review process harder
    than it should, as we end up integrating spus and ctx statistics
    accounting much more than it was on the first implementation.
    
    Signed-off-by: Andre Detsch <adetsch@br.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 6d7bd60f5380..0e5e55f53c8b 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -59,8 +59,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 		spu_gang_add_ctx(gang, ctx);
 	ctx->cpus_allowed = current->cpus_allowed;
 	spu_set_timeslice(ctx);
-	ctx->stats.execution_state = SPUCTX_UTIL_USER;
-	ctx->stats.tstamp = jiffies;
+	ctx->stats.util_state = SPU_UTIL_IDLE_LOADED;
 
 	atomic_inc(&nr_spu_contexts);
 	goto out;

commit e9f8a0b65ac716fd7974159240ce34bddea780b3
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 29 10:58:03 2007 +1000

    [POWERPC] spufs: Add stat file to spufs
    
    Export per-context statistics in spufs.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index f623d963fdc7..6d7bd60f5380 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -59,6 +59,8 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 		spu_gang_add_ctx(gang, ctx);
 	ctx->cpus_allowed = current->cpus_allowed;
 	spu_set_timeslice(ctx);
+	ctx->stats.execution_state = SPUCTX_UTIL_USER;
+	ctx->stats.tstamp = jiffies;
 
 	atomic_inc(&nr_spu_contexts);
 	goto out;

commit 65de66f0b8bcb7431d9df82cf32b002062b3a611
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 29 10:58:02 2007 +1000

    [POWERPC] spufs: Implement /proc/spu_loadavg
    
    Provide load average information for spu context.  The format
    is identical to /proc/loadavg, which is also where a lot of code
    and concepts is borrowed from.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 6ff2a75589f3..f623d963fdc7 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -23,10 +23,14 @@
 #include <linux/fs.h>
 #include <linux/mm.h>
 #include <linux/slab.h>
+#include <asm/atomic.h>
 #include <asm/spu.h>
 #include <asm/spu_csa.h>
 #include "spufs.h"
 
+
+atomic_t nr_spu_contexts = ATOMIC_INIT(0);
+
 struct spu_context *alloc_spu_context(struct spu_gang *gang)
 {
 	struct spu_context *ctx;
@@ -55,6 +59,8 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 		spu_gang_add_ctx(gang, ctx);
 	ctx->cpus_allowed = current->cpus_allowed;
 	spu_set_timeslice(ctx);
+
+	atomic_inc(&nr_spu_contexts);
 	goto out;
 out_free:
 	kfree(ctx);
@@ -74,6 +80,7 @@ void destroy_spu_context(struct kref *kref)
 	if (ctx->gang)
 		spu_gang_remove_ctx(ctx->gang, ctx);
 	BUG_ON(!list_empty(&ctx->rq));
+	atomic_dec(&nr_spu_contexts);
 	kfree(ctx);
 }
 

commit ea1ae5949d7fcd2e622226ba71741a0f43b6ef0a
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 29 10:57:56 2007 +1000

    [POWERPC] spusched: fix cpu/node binding
    
    Add a cpus_allowed allowed filed to struct spu_context so that we always
    use the cpu mask of the owning thread instead of the one happening to
    call into the scheduler.  Also use this information in
    grab_runnable_context to avoid spurious wakeups.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index c778d9178e0f..6ff2a75589f3 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -53,7 +53,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	INIT_LIST_HEAD(&ctx->rq);
 	if (gang)
 		spu_gang_add_ctx(gang, ctx);
-
+	ctx->cpus_allowed = current->cpus_allowed;
 	spu_set_timeslice(ctx);
 	goto out;
 out_free:

commit 2cf2b3b49f10d2f4a0703070fc54ce1cd84a6cda
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 29 10:57:55 2007 +1000

    [POWERPC] spusched: Update scheduling paramters on every spu_run
    
    Update scheduling information on every spu_run to allow for setting
    threads to realtime priority just before running them.  This requires
    some slightly ugly code in spufs_run_spu because we can just update
    the information unlocked if the spu is not runnable, but we need to
    acquire the active_mutex when it is runnable to protect against
    find_victim.  This locking scheme requires opencoding
    spu_acquire_runnable in spufs_run_spu which actually is a nice cleanup
    all by itself.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index c5ec7cfc24b5..c778d9178e0f 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -54,17 +54,6 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	if (gang)
 		spu_gang_add_ctx(gang, ctx);
 
-	/*
-	 * We do our own priority calculations, so we normally want
-	 * ->static_prio to start with. Unfortunately thies field
-	 * contains junk for threads with a realtime scheduling
-	 * policy so we have to look at ->prio in this case.
-	 */
-	if (rt_prio(current->prio))
-		ctx->prio = current->prio;
-	else
-		ctx->prio = current->static_prio;
-	ctx->policy = current->policy;
 	spu_set_timeslice(ctx);
 	goto out;
 out_free:

commit fe443ef2ac421c9c652e251e8733e2479d8e411a
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 29 10:57:52 2007 +1000

    [POWERPC] spusched: Dynamic timeslicing for SCHED_OTHER
    
    Enable preemptive scheduling for non-RT contexts.
    
    We use the same algorithms as the CPU scheduler to calculate the time
    slice length, and for now we also use the same timeslice length as the
    CPU scheduler. This might be not enough for good performance and can be
    changed after some benchmarking.
    
    Note that currently we do not boost the priority for contexts waiting
    on the runqueue for a long time, so contexts with a higher nice value
    could starve ones with less priority.  This could easily be fixed once
    the rework of the spu lists that Luke and I discussed is done.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index f084667e4f50..c5ec7cfc24b5 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -53,10 +53,19 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	INIT_LIST_HEAD(&ctx->rq);
 	if (gang)
 		spu_gang_add_ctx(gang, ctx);
-	ctx->rt_priority = current->rt_priority;
+
+	/*
+	 * We do our own priority calculations, so we normally want
+	 * ->static_prio to start with. Unfortunately thies field
+	 * contains junk for threads with a realtime scheduling
+	 * policy so we have to look at ->prio in this case.
+	 */
+	if (rt_prio(current->prio))
+		ctx->prio = current->prio;
+	else
+		ctx->prio = current->static_prio;
 	ctx->policy = current->policy;
-	ctx->prio = current->prio;
-	ctx->time_slice = SPU_DEF_TIMESLICE;
+	spu_set_timeslice(ctx);
 	goto out;
 out_free:
 	kfree(ctx);

commit 379018022071489a7dffee74db2a267465dab561
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 29 10:57:51 2007 +1000

    [POWERPC] spusched: Switch from workqueues to kthread + timer tick
    
    Get rid of the scheduler workqueues that complicated things a lot to
    a dedicated spu scheduler thread that gets woken by a traditional
    scheduler tick.  By default this scheduler tick runs a HZ * 10, aka
    one spu scheduler tick for every 10 cpu ticks.
    
    Currently the tick is not disabled when we have less context than
    available spus, but I will implement this later.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 7c51cb54bca1..f084667e4f50 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -56,7 +56,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	ctx->rt_priority = current->rt_priority;
 	ctx->policy = current->policy;
 	ctx->prio = current->prio;
-	INIT_DELAYED_WORK(&ctx->sched_work, spu_sched_tick);
+	ctx->time_slice = SPU_DEF_TIMESLICE;
 	goto out;
 out_free:
 	kfree(ctx);

commit 47d3a5faa3f72186f769ed9579c630afb8433f2b
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 4 23:26:51 2007 +1000

    [POWERPC] spufs: Synchronize pte invalidation vs ps close
    
    Make sure the mapping_lock also protects access to the various address_space
    pointers used for tearing down the ptes on a spu context switch.
    
    Because unmap_mapping_range can sleep we need to turn mapping_lock from
    a spinlock into a sleeping mutex.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 8654749e317b..7c51cb54bca1 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -39,7 +39,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	if (spu_init_csa(&ctx->csa))
 		goto out_free;
 	spin_lock_init(&ctx->mmio_lock);
-	spin_lock_init(&ctx->mapping_lock);
+	mutex_init(&ctx->mapping_lock);
 	kref_init(&ctx->kref);
 	mutex_init(&ctx->state_mutex);
 	mutex_init(&ctx->run_mutex);
@@ -103,6 +103,7 @@ void spu_forget(struct spu_context *ctx)
 
 void spu_unmap_mappings(struct spu_context *ctx)
 {
+	mutex_lock(&ctx->mapping_lock);
 	if (ctx->local_store)
 		unmap_mapping_range(ctx->local_store, 0, LS_SIZE, 1);
 	if (ctx->mfc)
@@ -117,6 +118,7 @@ void spu_unmap_mappings(struct spu_context *ctx)
 		unmap_mapping_range(ctx->mss, 0, 0x1000, 1);
 	if (ctx->psmap)
 		unmap_mapping_range(ctx->psmap, 0, 0x20000, 1);
+	mutex_unlock(&ctx->mapping_lock);
 }
 
 /**

commit f1fa74f4afe96b0e4ac2beaa61fa4f4667acdcbb
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue May 8 16:27:29 2007 +1000

    [POWERPC] Spufs support for 64K LS mappings on 4K kernels
    
    This adds an option to spufs when the kernel is configured for
    4K page to give it the ability to use 64K pages for SPE local store
    mappings.
    
    Currently, we are optimistic and try order 4 allocations when creating
    contexts. If that fails, the code will fallback to 4K automatically.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index a87d9ca3dba2..8654749e317b 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -36,10 +36,8 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	/* Binding to physical processor deferred
 	 * until spu_activate().
 	 */
-	spu_init_csa(&ctx->csa);
-	if (!ctx->csa.lscsa) {
+	if (spu_init_csa(&ctx->csa))
 		goto out_free;
-	}
 	spin_lock_init(&ctx->mmio_lock);
 	spin_lock_init(&ctx->mapping_lock);
 	kref_init(&ctx->kref);

commit e45d48a34d4d1862d28d22c2533b8c6bb83b8c1f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 23 21:08:17 2007 +0200

    [POWERPC] spufs: turn run_sema into run_mutex
    
    There is no reason for run_sema to be a struct semaphore.  Changing
    it to a mutex and rename it accordingly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index ce17a2847184..a87d9ca3dba2 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -44,7 +44,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	spin_lock_init(&ctx->mapping_lock);
 	kref_init(&ctx->kref);
 	mutex_init(&ctx->state_mutex);
-	init_MUTEX(&ctx->run_sema);
+	mutex_init(&ctx->run_mutex);
 	init_waitqueue_head(&ctx->ibox_wq);
 	init_waitqueue_head(&ctx->wbox_wq);
 	init_waitqueue_head(&ctx->stop_wq);

commit 7ec18ab923a2e377ecb05c74a2d38f457f79950f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 23 21:08:12 2007 +0200

    [POWERPC] spufs: streamline locking for isolated spu setup
    
    For quite a while now spu state is protected by a simple mutex instead
    of the old rw_semaphore, and this means we can simplify the locking
    around spu_setup_isolated a lot.
    
    Instead of doing an spu_release before entering spu_setup_isolated and
    then calling the complicated spu_acquire_exclusive we can now simply
    enter the function locked an in guaranteed runnable state, so that the
    only bit of spu_acquire_exclusive that's left is the call to
    spu_unmap_mappings.
    
    Similarly there's no more need to unlock and reacquire the state_mutex
    when spu_setup_isolated is done, but we can always return with the
    lock held and only drop it in spu_run_init in the failure case.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 065147fb1cc2..ce17a2847184 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -121,46 +121,6 @@ void spu_unmap_mappings(struct spu_context *ctx)
 		unmap_mapping_range(ctx->psmap, 0, 0x20000, 1);
 }
 
-/**
- * spu_acquire_exclusive - lock spu contex and protect against userspace access
- * @ctx:	spu contex to lock
- *
- * Note:
- *	Returns 0 and with the context locked on success
- *	Returns negative error and with the context _unlocked_ on failure.
- */
-int spu_acquire_exclusive(struct spu_context *ctx)
-{
-	int ret = -EINVAL;
-
-	spu_acquire(ctx);
-	/*
-	 * Context is about to be freed, so we can't acquire it anymore.
-	 */
-	if (!ctx->owner)
-		goto out_unlock;
-
-	if (ctx->state == SPU_STATE_SAVED) {
-		ret = spu_activate(ctx, 0);
-		if (ret)
-			goto out_unlock;
-	} else {
-		/*
-		 * We need to exclude userspace access to the context.
-		 *
-		 * To protect against memory access we invalidate all ptes
-		 * and make sure the pagefault handlers block on the mutex.
-		 */
-		spu_unmap_mappings(ctx);
-	}
-
-	return 0;
-
- out_unlock:
-	spu_release(ctx);
-	return ret;
-}
-
 /**
  * spu_acquire_runnable - lock spu contex and make sure it is in runnable state
  * @ctx:	spu contex to lock

commit a475c2f43520cb095452201da57395000cfeb94c
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 23 21:08:11 2007 +0200

    [POWERPC] spufs: remove woken threads from the runqueue early
    
    A single context should only be woken once, and we should not have
    more wakeups for a given priority than the number of contexts on
    that runqueue position.
    
    Also add some asserts to trap future problems in this area more
    easily.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index b3954aba424e..065147fb1cc2 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -52,6 +52,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	ctx->state = SPU_STATE_SAVED;
 	ctx->ops = &spu_backing_ops;
 	ctx->owner = get_task_mm(current);
+	INIT_LIST_HEAD(&ctx->rq);
 	if (gang)
 		spu_gang_add_ctx(gang, ctx);
 	ctx->rt_priority = current->rt_priority;
@@ -76,6 +77,7 @@ void destroy_spu_context(struct kref *kref)
 	spu_fini_csa(&ctx->csa);
 	if (ctx->gang)
 		spu_gang_remove_ctx(ctx->gang, ctx);
+	BUG_ON(!list_empty(&ctx->rq));
 	kfree(ctx);
 }
 

commit 43c2bbd932b66403688f3d812065d82f8fb8f4b3
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 23 21:08:07 2007 +0200

    [POWERPC] spufs: clear mapping pointers after last close
    
    Make sure the pointers to various mappings are cleared once the last
    user stopped using them.  This avoids accessing freed memory when
    tearing down the gang directory aswell as optimizing away
    pte invalidations if no one uses these.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 04ad2e364e97..b3954aba424e 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -41,6 +41,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 		goto out_free;
 	}
 	spin_lock_init(&ctx->mmio_lock);
+	spin_lock_init(&ctx->mapping_lock);
 	kref_init(&ctx->kref);
 	mutex_init(&ctx->state_mutex);
 	init_MUTEX(&ctx->run_sema);

commit 2eb1b12049844a8ebc670e0e4fc908bc3f8933d3
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:54:29 2007 +0100

    [POWERPC] spu sched: static timeslicing for SCHED_RR contexts
    
    For SCHED_RR tasks we can do some really trivial timeslicing.  Basically
    we fire up a time for every scheduler tick that searches for a higher
    or same priority thread that is on the runqueue and if there is one
    context switches to it.  Because we can't lock spus from timer context
    we actually run this from a delayed runqueue instead of a timer.
    
    A nice optimization would be to skip the actual priority bitmap search
    when there are less contexts than physical spus available.  To implement
    this I need a so far unpublished patch from Andre, and it will be added
    after we have that patch in.
    
    Note that right now we only do the time slicing for SCHED_RR tasks.
    The code would work for SCHED_OTHER tasks aswell, but their prio
    value is defered from the one the PPU thread has at time of spu_run,
    and using this for spu scheduling decisions would make the code very
    unfair.  SCHED_OTHER support will be enabled once we the spu scheduler
    knows how to calculcate cpu_context.prio (very soon)
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index d581f4ec99ba..04ad2e364e97 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -54,7 +54,9 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	if (gang)
 		spu_gang_add_ctx(gang, ctx);
 	ctx->rt_priority = current->rt_priority;
+	ctx->policy = current->policy;
 	ctx->prio = current->prio;
+	INIT_DELAYED_WORK(&ctx->sched_work, spu_sched_tick);
 	goto out;
 out_free:
 	kfree(ctx);

commit 52f04fcf66a5d5d90790d6cfde52e391ecf2b882
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:54:27 2007 +0100

    [POWERPC] spu sched: forced preemption at execution
    
    If we start a spu context with realtime priority we want it to run
    immediately and not wait until some other lower priority thread has
    finished.  Try to find a suitable victim and use it's spu in this
    case.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 056a8ad02385..d581f4ec99ba 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -53,6 +53,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	ctx->owner = get_task_mm(current);
 	if (gang)
 		spu_gang_add_ctx(gang, ctx);
+	ctx->rt_priority = current->rt_priority;
 	ctx->prio = current->prio;
 	goto out;
 out_free:

commit 26bec67386dbf6ef887254e815398842e182cdcd
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:54:24 2007 +0100

    [POWERPC] spufs: optimize spu_run
    
    There is no need to directly wake up contexts in spu_activate when
    called from spu_run, so add a flag to surpress this wakeup.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 88a887186303..056a8ad02385 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -163,7 +163,7 @@ int spu_acquire_exclusive(struct spu_context *ctx)
  *	Returns 0 and with the context locked on success
  *	Returns negative error and with the context _unlocked_ on failure.
  */
-int spu_acquire_runnable(struct spu_context *ctx)
+int spu_acquire_runnable(struct spu_context *ctx, unsigned long flags)
 {
 	int ret = -EINVAL;
 
@@ -174,7 +174,7 @@ int spu_acquire_runnable(struct spu_context *ctx)
 		 */
 		if (!ctx->owner)
 			goto out_unlock;
-		ret = spu_activate(ctx, 0);
+		ret = spu_activate(ctx, flags);
 		if (ret)
 			goto out_unlock;
 	}

commit 8389998ae9ea2888c86c446f7911ddced50052a1
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:54:22 2007 +0100

    [POWERPC] spufs: move prio to spu_context
    
    It doesn't make any sense to have a priority field in the physical spu
    structure.  Move it into the spu context instead.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index f2630dc0db89..88a887186303 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -53,6 +53,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	ctx->owner = get_task_mm(current);
 	if (gang)
 		spu_gang_add_ctx(gang, ctx);
+	ctx->prio = current->prio;
 	goto out;
 out_free:
 	kfree(ctx);
@@ -176,8 +177,7 @@ int spu_acquire_runnable(struct spu_context *ctx)
 		ret = spu_activate(ctx, 0);
 		if (ret)
 			goto out_unlock;
-	} else
-		ctx->spu->prio = current->prio;
+	}
 
 	return 0;
 

commit 6a0641e51011def4e308fd07387047f5ee50647f
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:54:21 2007 +0100

    [POWERPC] spufs: state_mutex cleanup
    
    Various cleanups in code surrounding the state semaphore:
    
     - inline spu_acquire/spu_release
     - cleanup spu_acquire_* and add kerneldoc comments to these functions
     - remove spu_release_exclusive and replace it with spu_release
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index c9aab9b1cd8a..f2630dc0db89 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -96,16 +96,6 @@ void spu_forget(struct spu_context *ctx)
 	spu_release(ctx);
 }
 
-void spu_acquire(struct spu_context *ctx)
-{
-	mutex_lock(&ctx->state_mutex);
-}
-
-void spu_release(struct spu_context *ctx)
-{
-	mutex_unlock(&ctx->state_mutex);
-}
-
 void spu_unmap_mappings(struct spu_context *ctx)
 {
 	if (ctx->local_store)
@@ -124,66 +114,85 @@ void spu_unmap_mappings(struct spu_context *ctx)
 		unmap_mapping_range(ctx->psmap, 0, 0x20000, 1);
 }
 
+/**
+ * spu_acquire_exclusive - lock spu contex and protect against userspace access
+ * @ctx:	spu contex to lock
+ *
+ * Note:
+ *	Returns 0 and with the context locked on success
+ *	Returns negative error and with the context _unlocked_ on failure.
+ */
 int spu_acquire_exclusive(struct spu_context *ctx)
 {
-	int ret = 0;
+	int ret = -EINVAL;
 
-	mutex_lock(&ctx->state_mutex);
-	/* ctx is about to be freed, can't acquire any more */
-	if (!ctx->owner) {
-		ret = -EINVAL;
-		goto out;
-	}
+	spu_acquire(ctx);
+	/*
+	 * Context is about to be freed, so we can't acquire it anymore.
+	 */
+	if (!ctx->owner)
+		goto out_unlock;
 
 	if (ctx->state == SPU_STATE_SAVED) {
 		ret = spu_activate(ctx, 0);
 		if (ret)
-			goto out;
+			goto out_unlock;
 	} else {
-		/* We need to exclude userspace access to the context. */
+		/*
+		 * We need to exclude userspace access to the context.
+		 *
+		 * To protect against memory access we invalidate all ptes
+		 * and make sure the pagefault handlers block on the mutex.
+		 */
 		spu_unmap_mappings(ctx);
 	}
 
-out:
-	if (ret)
-		mutex_unlock(&ctx->state_mutex);
+	return 0;
+
+ out_unlock:
+	spu_release(ctx);
 	return ret;
 }
 
+/**
+ * spu_acquire_runnable - lock spu contex and make sure it is in runnable state
+ * @ctx:	spu contex to lock
+ *
+ * Note:
+ *	Returns 0 and with the context locked on success
+ *	Returns negative error and with the context _unlocked_ on failure.
+ */
 int spu_acquire_runnable(struct spu_context *ctx)
 {
-	int ret = 0;
-
-	mutex_lock(&ctx->state_mutex);
-	if (ctx->state == SPU_STATE_RUNNABLE) {
-		ctx->spu->prio = current->prio;
-		return 0;
-	}
-
-	/* ctx is about to be freed, can't acquire any more */
-	if (!ctx->owner) {
-		ret = -EINVAL;
-		goto out;
-	}
+	int ret = -EINVAL;
 
+	spu_acquire(ctx);
 	if (ctx->state == SPU_STATE_SAVED) {
+		/*
+		 * Context is about to be freed, so we can't acquire it anymore.
+		 */
+		if (!ctx->owner)
+			goto out_unlock;
 		ret = spu_activate(ctx, 0);
 		if (ret)
-			goto out;
-	}
+			goto out_unlock;
+	} else
+		ctx->spu->prio = current->prio;
 
-	/* On success, we return holding the lock */
-	return ret;
-out:
-	/* Release here, to simplify calling code. */
-	mutex_unlock(&ctx->state_mutex);
+	return 0;
 
+ out_unlock:
+	spu_release(ctx);
 	return ret;
 }
 
+/**
+ * spu_acquire_saved - lock spu contex and make sure it is in saved state
+ * @ctx:	spu contex to lock
+ */
 void spu_acquire_saved(struct spu_context *ctx)
 {
-	mutex_lock(&ctx->state_mutex);
-	if (ctx->state == SPU_STATE_RUNNABLE)
+	spu_acquire(ctx);
+	if (ctx->state != SPU_STATE_SAVED)
 		spu_deactivate(ctx);
 }

commit 650f8b0291ecd0abdeadbd0ff3d70c3538e55405
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:36:50 2007 +0100

    [POWERPC] spufs: simplify state_mutex
    
    The r/w semaphore to lock the spus was overkill and can be replaced
    with a mutex to make it faster, simpler and easier to debug.  It also
    helps to allow making most spufs interruptible in future patches.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index ccffc449763b..c9aab9b1cd8a 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -42,7 +42,7 @@ struct spu_context *alloc_spu_context(struct spu_gang *gang)
 	}
 	spin_lock_init(&ctx->mmio_lock);
 	kref_init(&ctx->kref);
-	init_rwsem(&ctx->state_sema);
+	mutex_init(&ctx->state_mutex);
 	init_MUTEX(&ctx->run_sema);
 	init_waitqueue_head(&ctx->ibox_wq);
 	init_waitqueue_head(&ctx->wbox_wq);
@@ -65,9 +65,9 @@ void destroy_spu_context(struct kref *kref)
 {
 	struct spu_context *ctx;
 	ctx = container_of(kref, struct spu_context, kref);
-	down_write(&ctx->state_sema);
+	mutex_lock(&ctx->state_mutex);
 	spu_deactivate(ctx);
-	up_write(&ctx->state_sema);
+	mutex_unlock(&ctx->state_mutex);
 	spu_fini_csa(&ctx->csa);
 	if (ctx->gang)
 		spu_gang_remove_ctx(ctx->gang, ctx);
@@ -98,12 +98,12 @@ void spu_forget(struct spu_context *ctx)
 
 void spu_acquire(struct spu_context *ctx)
 {
-	down_read(&ctx->state_sema);
+	mutex_lock(&ctx->state_mutex);
 }
 
 void spu_release(struct spu_context *ctx)
 {
-	up_read(&ctx->state_sema);
+	mutex_unlock(&ctx->state_mutex);
 }
 
 void spu_unmap_mappings(struct spu_context *ctx)
@@ -128,7 +128,7 @@ int spu_acquire_exclusive(struct spu_context *ctx)
 {
 	int ret = 0;
 
-	down_write(&ctx->state_sema);
+	mutex_lock(&ctx->state_mutex);
 	/* ctx is about to be freed, can't acquire any more */
 	if (!ctx->owner) {
 		ret = -EINVAL;
@@ -146,7 +146,7 @@ int spu_acquire_exclusive(struct spu_context *ctx)
 
 out:
 	if (ret)
-		up_write(&ctx->state_sema);
+		mutex_unlock(&ctx->state_mutex);
 	return ret;
 }
 
@@ -154,14 +154,12 @@ int spu_acquire_runnable(struct spu_context *ctx)
 {
 	int ret = 0;
 
-	down_read(&ctx->state_sema);
+	mutex_lock(&ctx->state_mutex);
 	if (ctx->state == SPU_STATE_RUNNABLE) {
 		ctx->spu->prio = current->prio;
 		return 0;
 	}
-	up_read(&ctx->state_sema);
 
-	down_write(&ctx->state_sema);
 	/* ctx is about to be freed, can't acquire any more */
 	if (!ctx->owner) {
 		ret = -EINVAL;
@@ -174,29 +172,18 @@ int spu_acquire_runnable(struct spu_context *ctx)
 			goto out;
 	}
 
-	downgrade_write(&ctx->state_sema);
 	/* On success, we return holding the lock */
-
 	return ret;
 out:
 	/* Release here, to simplify calling code. */
-	up_write(&ctx->state_sema);
+	mutex_unlock(&ctx->state_mutex);
 
 	return ret;
 }
 
 void spu_acquire_saved(struct spu_context *ctx)
 {
-	down_read(&ctx->state_sema);
-
-	if (ctx->state == SPU_STATE_SAVED)
-		return;
-
-	up_read(&ctx->state_sema);
-	down_write(&ctx->state_sema);
-
+	mutex_lock(&ctx->state_mutex);
 	if (ctx->state == SPU_STATE_RUNNABLE)
 		spu_deactivate(ctx);
-
-	downgrade_write(&ctx->state_sema);
 }

commit 81998bafe299b8b675157f0a4dfe8dad43215da9
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:36:48 2007 +0100

    [POWERPC] spufs: bind_context sets SPU_STATE_RUNNABLE
    
    Only bind_context/unbind_context change the spu context state.  Thus
    we can move all assignents of SPU_STATE_RUNNABLE into bind_context,
    which parallels the unbind side aswell.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index dd89aa7c1f16..ccffc449763b 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -139,7 +139,6 @@ int spu_acquire_exclusive(struct spu_context *ctx)
 		ret = spu_activate(ctx, 0);
 		if (ret)
 			goto out;
-		ctx->state = SPU_STATE_RUNNABLE;
 	} else {
 		/* We need to exclude userspace access to the context. */
 		spu_unmap_mappings(ctx);
@@ -173,7 +172,6 @@ int spu_acquire_runnable(struct spu_context *ctx)
 		ret = spu_activate(ctx, 0);
 		if (ret)
 			goto out;
-		ctx->state = SPU_STATE_RUNNABLE;
 	}
 
 	downgrade_write(&ctx->state_sema);

commit aa56c16807ba7b8e801216cab012d2f498755ba5
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Feb 13 21:36:47 2007 +0100

    [POWERPC] spufs: remove superfluous SPU_STATE_SAVED assignments
    
    unbind_context already sets the context state to SPU_STATE_SAVED, thus
    the spu_deactivate callers don't need to do it again.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 28c718ca3b51..dd89aa7c1f16 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -197,10 +197,8 @@ void spu_acquire_saved(struct spu_context *ctx)
 	up_read(&ctx->state_sema);
 	down_write(&ctx->state_sema);
 
-	if (ctx->state == SPU_STATE_RUNNABLE) {
+	if (ctx->state == SPU_STATE_RUNNABLE)
 		spu_deactivate(ctx);
-		ctx->state = SPU_STATE_SAVED;
-	}
 
 	downgrade_write(&ctx->state_sema);
 }

commit 17e0e27020d028a790d97699aff85a43af5be472
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Feb 13 11:46:08 2007 +1100

    [POWERPC] spufs: Fix bitrot of the SPU mmap facility
    
    It looks like we've had some serious bitrot there mostly due to tracking
    of address_space's of mmap'ed files getting out of sync with the actual
    mmap code. The mfc, mss and psmap were not tracked properly and thus
    not invalidated on context switches (oops !)
    
    I also removed the various file->f_mapping = inode->i_mapping;
    assignments that were done in the other open() routines since that
    is already done for us by __dentry_open.
    
    One improvement we might want to do later is to assign the various
    ctx-> fields at mmap time instead of file open/close time so that we
    don't call unmap_mapping_range() on thing that have not been mmap'ed
    
    Finally, I added some smp_wmb's after assigning the ctx-> fields to make
    sure they are visible to other CPUs. I don't think this is really
    necessary as I suspect locking in the fs layer will make that happen
    anyway but better safe than sorry.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 0870009f56db..28c718ca3b51 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -111,13 +111,17 @@ void spu_unmap_mappings(struct spu_context *ctx)
 	if (ctx->local_store)
 		unmap_mapping_range(ctx->local_store, 0, LS_SIZE, 1);
 	if (ctx->mfc)
-		unmap_mapping_range(ctx->mfc, 0, 0x4000, 1);
+		unmap_mapping_range(ctx->mfc, 0, 0x1000, 1);
 	if (ctx->cntl)
-		unmap_mapping_range(ctx->cntl, 0, 0x4000, 1);
+		unmap_mapping_range(ctx->cntl, 0, 0x1000, 1);
 	if (ctx->signal1)
-		unmap_mapping_range(ctx->signal1, 0, 0x4000, 1);
+		unmap_mapping_range(ctx->signal1, 0, PAGE_SIZE, 1);
 	if (ctx->signal2)
-		unmap_mapping_range(ctx->signal2, 0, 0x4000, 1);
+		unmap_mapping_range(ctx->signal2, 0, PAGE_SIZE, 1);
+	if (ctx->mss)
+		unmap_mapping_range(ctx->mss, 0, 0x1000, 1);
+	if (ctx->psmap)
+		unmap_mapping_range(ctx->psmap, 0, 0x20000, 1);
 }
 
 int spu_acquire_exclusive(struct spu_context *ctx)

commit ee2d7340cbf3b123e1c3b7454f3e2b7e65d33bb2
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Nov 20 18:45:08 2006 +0100

    [POWERPC] spufs: Use SPU master control to prevent wild SPU execution
    
    When the user changes the runcontrol register, an SPU might be
    running without a process being attached to it and waiting for
    events. In order to prevent this, make sure we always disable
    the priv1 master control when we're not inside of spu_run.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 48eb050bcf4b..0870009f56db 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -122,29 +122,29 @@ void spu_unmap_mappings(struct spu_context *ctx)
 
 int spu_acquire_exclusive(struct spu_context *ctx)
 {
-       int ret = 0;
-
-       down_write(&ctx->state_sema);
-       /* ctx is about to be freed, can't acquire any more */
-       if (!ctx->owner) {
-               ret = -EINVAL;
-               goto out;
-       }
-
-       if (ctx->state == SPU_STATE_SAVED) {
-               ret = spu_activate(ctx, 0);
-               if (ret)
-                       goto out;
-               ctx->state = SPU_STATE_RUNNABLE;
-       } else {
-               /* We need to exclude userspace access to the context. */
-               spu_unmap_mappings(ctx);
-       }
+	int ret = 0;
+
+	down_write(&ctx->state_sema);
+	/* ctx is about to be freed, can't acquire any more */
+	if (!ctx->owner) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (ctx->state == SPU_STATE_SAVED) {
+		ret = spu_activate(ctx, 0);
+		if (ret)
+			goto out;
+		ctx->state = SPU_STATE_RUNNABLE;
+	} else {
+		/* We need to exclude userspace access to the context. */
+		spu_unmap_mappings(ctx);
+	}
 
 out:
-       if (ret)
-               up_write(&ctx->state_sema);
-       return ret;
+	if (ret)
+		up_write(&ctx->state_sema);
+	return ret;
 }
 
 int spu_acquire_runnable(struct spu_context *ctx)

commit 099814bb1f9bd9081d7c85867f8eb8c049abc1b9
Author: Jeremy Kerr <jeremy@au1.ibm.com>
Date:   Tue Oct 24 18:31:19 2006 +0200

    [POWERPC] spufs: Add isolated-mode SPE recycling support
    
    When in isolated mode, SPEs have access to an area of persistent
    storage, which is per-SPE. In order for isolated-mode apps to
    communicate arbitrary data through this storage, we need to ensure that
    isolated physical SPEs can be reused for subsequent applications.
    
    Add a file ("recycle") in a spethread dir to enable isolated-mode
    recycling. By writing to this file, the kernel will reload the
    isolated-mode loader kernel, allowing a new app to be run on the same
    physical SPE.
    
    This requires the spu_acquire_exclusive function to enforce exclusive
    access to the SPE while the loader is initialised.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 034cf6af53a2..48eb050bcf4b 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -120,6 +120,33 @@ void spu_unmap_mappings(struct spu_context *ctx)
 		unmap_mapping_range(ctx->signal2, 0, 0x4000, 1);
 }
 
+int spu_acquire_exclusive(struct spu_context *ctx)
+{
+       int ret = 0;
+
+       down_write(&ctx->state_sema);
+       /* ctx is about to be freed, can't acquire any more */
+       if (!ctx->owner) {
+               ret = -EINVAL;
+               goto out;
+       }
+
+       if (ctx->state == SPU_STATE_SAVED) {
+               ret = spu_activate(ctx, 0);
+               if (ret)
+                       goto out;
+               ctx->state = SPU_STATE_RUNNABLE;
+       } else {
+               /* We need to exclude userspace access to the context. */
+               spu_unmap_mappings(ctx);
+       }
+
+out:
+       if (ret)
+               up_write(&ctx->state_sema);
+       return ret;
+}
+
 int spu_acquire_runnable(struct spu_context *ctx)
 {
 	int ret = 0;

commit 6263203ed6e9ff107129a1ebe613290b342a4465
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Oct 4 17:26:15 2006 +0200

    [POWERPC] spufs: Add infrastructure needed for gang scheduling
    
    Add the concept of a gang to spufs as a new type of object.
    So far, this has no impact whatsover on scheduling, but makes
    it possible to add that later.
    
    A new type of object in spufs is now a spu_gang. It is created
    with the spu_create system call with the flags argument set
    to SPU_CREATE_GANG (0x2). Inside of a spu_gang, it
    is then possible to create spu_context objects, which until
    now was only possible at the root of spufs.
    
    There is a new member in struct spu_context pointing to
    the spu_gang it belongs to, if any. The spu_gang maintains
    a list of spu_context structures that are its children.
    This information can then be used in the scheduler in the
    future.
    
    There is still a bug that needs to be resolved in this
    basic infrastructure regarding the order in which objects
    are removed. When the spu_gang file descriptor is closed
    before the spu_context descriptors, we leak the dentry
    and inode for the gang. Any ideas how to cleanly solve
    this are appreciated.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 36439c5e9f2d..034cf6af53a2 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -27,7 +27,7 @@
 #include <asm/spu_csa.h>
 #include "spufs.h"
 
-struct spu_context *alloc_spu_context(void)
+struct spu_context *alloc_spu_context(struct spu_gang *gang)
 {
 	struct spu_context *ctx;
 	ctx = kzalloc(sizeof *ctx, GFP_KERNEL);
@@ -51,6 +51,8 @@ struct spu_context *alloc_spu_context(void)
 	ctx->state = SPU_STATE_SAVED;
 	ctx->ops = &spu_backing_ops;
 	ctx->owner = get_task_mm(current);
+	if (gang)
+		spu_gang_add_ctx(gang, ctx);
 	goto out;
 out_free:
 	kfree(ctx);
@@ -67,6 +69,8 @@ void destroy_spu_context(struct kref *kref)
 	spu_deactivate(ctx);
 	up_write(&ctx->state_sema);
 	spu_fini_csa(&ctx->csa);
+	if (ctx->gang)
+		spu_gang_remove_ctx(ctx->gang, ctx);
 	kfree(ctx);
 }
 

commit c5c4591375a10f6bc1bc55d86af7764033b10367
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Fri Apr 28 16:37:47 2006 +0800

    [PATCH] powerpc: cell: use kzalloc in alloc_spu_context()
    
    Use kzalloc when allocating a new spu context, rather than kmalloc +
    zeroing.
    
    Booted & tested on cell.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 8bb33abfad17..36439c5e9f2d 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -30,7 +30,7 @@
 struct spu_context *alloc_spu_context(void)
 {
 	struct spu_context *ctx;
-	ctx = kmalloc(sizeof *ctx, GFP_KERNEL);
+	ctx = kzalloc(sizeof *ctx, GFP_KERNEL);
 	if (!ctx)
 		goto out;
 	/* Binding to physical processor deferred
@@ -48,17 +48,7 @@ struct spu_context *alloc_spu_context(void)
 	init_waitqueue_head(&ctx->wbox_wq);
 	init_waitqueue_head(&ctx->stop_wq);
 	init_waitqueue_head(&ctx->mfc_wq);
-	ctx->ibox_fasync = NULL;
-	ctx->wbox_fasync = NULL;
-	ctx->mfc_fasync = NULL;
-	ctx->mfc = NULL;
-	ctx->tagwait = 0;
 	ctx->state = SPU_STATE_SAVED;
-	ctx->local_store = NULL;
-	ctx->cntl = NULL;
-	ctx->signal1 = NULL;
-	ctx->signal2 = NULL;
-	ctx->spu = NULL;
 	ctx->ops = &spu_backing_ops;
 	ctx->owner = get_task_mm(current);
 	goto out;

commit ca3e91cb0cd53da70621f85d24a080b23751a013
Author: Dirk Herrendoerfer <herrendo@de.ibm.com>
Date:   Thu Mar 23 00:00:13 2006 +0100

    [PATCH] spufs: initialize context correctly
    
    the mfc member of a new context was not initialized to zero,
    which potentially leads to wild memory accesses.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 3f75c1e7adea..8bb33abfad17 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -51,6 +51,7 @@ struct spu_context *alloc_spu_context(void)
 	ctx->ibox_fasync = NULL;
 	ctx->wbox_fasync = NULL;
 	ctx->mfc_fasync = NULL;
+	ctx->mfc = NULL;
 	ctx->tagwait = 0;
 	ctx->state = SPU_STATE_SAVED;
 	ctx->local_store = NULL;

commit 6df10a82f8de89c66eb91c371d62d76e87b2cbba
Author: Mark Nutter <mnutter@us.ibm.com>
Date:   Thu Mar 23 00:00:12 2006 +0100

    [PATCH] spufs: enable SPE problem state MMIO access.
    
    This patch is layered on top of CONFIG_SPARSEMEM
    and is patterned after direct mapping of LS.
    
    This patch allows mmap() of the following regions:
    "mfc", which represents the area from [0x3000 - 0x3fff];
    "cntl", which represents the area from [0x4000 - 0x4fff];
    "signal1" which begins at offset 0x14000; "signal2" which
    begins at offset 0x1c000.
    
    The signal1 & signal2 files may be mmap()'d by regular user
    processes.  The cntl and mfc file, on the other hand, may
    only be accessed if the owning process has CAP_SYS_RAWIO,
    because they have the potential to confuse the kernel
    with regard to parallel access to the same files with
    regular file operations: the kernel always holds a spinlock
    when accessing registers in these areas to serialize them,
    which can not be guaranteed with user mmaps,
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 7e016b9eab21..3f75c1e7adea 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -27,7 +27,7 @@
 #include <asm/spu_csa.h>
 #include "spufs.h"
 
-struct spu_context *alloc_spu_context(struct address_space *local_store)
+struct spu_context *alloc_spu_context(void)
 {
 	struct spu_context *ctx;
 	ctx = kmalloc(sizeof *ctx, GFP_KERNEL);
@@ -53,7 +53,10 @@ struct spu_context *alloc_spu_context(struct address_space *local_store)
 	ctx->mfc_fasync = NULL;
 	ctx->tagwait = 0;
 	ctx->state = SPU_STATE_SAVED;
-	ctx->local_store = local_store;
+	ctx->local_store = NULL;
+	ctx->cntl = NULL;
+	ctx->signal1 = NULL;
+	ctx->signal2 = NULL;
 	ctx->spu = NULL;
 	ctx->ops = &spu_backing_ops;
 	ctx->owner = get_task_mm(current);
@@ -110,7 +113,16 @@ void spu_release(struct spu_context *ctx)
 
 void spu_unmap_mappings(struct spu_context *ctx)
 {
-	unmap_mapping_range(ctx->local_store, 0, LS_SIZE, 1);
+	if (ctx->local_store)
+		unmap_mapping_range(ctx->local_store, 0, LS_SIZE, 1);
+	if (ctx->mfc)
+		unmap_mapping_range(ctx->mfc, 0, 0x4000, 1);
+	if (ctx->cntl)
+		unmap_mapping_range(ctx->cntl, 0, 0x4000, 1);
+	if (ctx->signal1)
+		unmap_mapping_range(ctx->signal1, 0, 0x4000, 1);
+	if (ctx->signal2)
+		unmap_mapping_range(ctx->signal2, 0, 0x4000, 1);
 }
 
 int spu_acquire_runnable(struct spu_context *ctx)

commit a33a7d7309d79656bc19a0e96fc4547a1633283e
Author: Arnd Bergmann <abergman@de.ibm.com>
Date:   Thu Mar 23 00:00:11 2006 +0100

    [PATCH] spufs: implement mfc access for PPE-side DMA
    
    This patch adds a new file called 'mfc' to each spufs directory.
    The file accepts DMA commands that are a subset of what would
    be legal DMA commands for problem state register access. Upon
    reading the file, a bitmask is returned with the completed
    tag groups set.
    
    The file is meant to be used from an abstraction in libspe
    that is added by a different patch.
    
    From the kernel perspective, this means a process can now
    offload a memory copy from or into an SPE local store
    without having to run code on the SPE itself.
    
    The transfer will only be performed while the SPE is owned
    by one thread that is waiting in the spu_run system call
    and the data will be transferred into that thread's
    address space, independent of which thread started the
    transfer.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 336f238102fd..7e016b9eab21 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -47,8 +47,11 @@ struct spu_context *alloc_spu_context(struct address_space *local_store)
 	init_waitqueue_head(&ctx->ibox_wq);
 	init_waitqueue_head(&ctx->wbox_wq);
 	init_waitqueue_head(&ctx->stop_wq);
+	init_waitqueue_head(&ctx->mfc_wq);
 	ctx->ibox_fasync = NULL;
 	ctx->wbox_fasync = NULL;
+	ctx->mfc_fasync = NULL;
+	ctx->tagwait = 0;
 	ctx->state = SPU_STATE_SAVED;
 	ctx->local_store = local_store;
 	ctx->spu = NULL;
@@ -68,8 +71,6 @@ void destroy_spu_context(struct kref *kref)
 	ctx = container_of(kref, struct spu_context, kref);
 	down_write(&ctx->state_sema);
 	spu_deactivate(ctx);
-	ctx->ibox_fasync = NULL;
-	ctx->wbox_fasync = NULL;
 	up_write(&ctx->state_sema);
 	spu_fini_csa(&ctx->csa);
 	kfree(ctx);

commit 0106246594a05f02a6be6ee4695c7584c758fa7f
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Jan 4 20:31:25 2006 +0100

    [PATCH] spufs fix spu_acquire_runnable error path
    
    When spu_activate fails in spu_acquire_runnable, the
    state must still be SPU_STATE_SAVED, we were
    incorrectly setting it to SPU_STATE_RUNNABLE.
    
    Signed-off-by: Arnd Bergmann <arndb@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index c5cd55ac848d..336f238102fd 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -132,10 +132,10 @@ int spu_acquire_runnable(struct spu_context *ctx)
 
 	if (ctx->state == SPU_STATE_SAVED) {
 		ret = spu_activate(ctx, 0);
+		if (ret)
+			goto out;
 		ctx->state = SPU_STATE_RUNNABLE;
 	}
-	if (ret)
-		goto out;
 
 	downgrade_write(&ctx->state_sema);
 	/* On success, we return holding the lock */

commit 5ef8224aaa9220bfecb362f0802cf78aad47c02a
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Jan 4 20:31:24 2006 +0100

    [PATCH] spufs: serialize sys_spu_run per spu
    
    During an earlier cleanup, we lost the serialization
    of multiple spu_run calls performed on the same
    spu_context. In order to get this back, introduce a
    mutex in the spu_context that is held inside of spu_run.
    
    Noticed by Al Viro.
    
    Signed-off-by: Arnd Bergmann <arndb@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 903c35d19577..c5cd55ac848d 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -43,6 +43,7 @@ struct spu_context *alloc_spu_context(struct address_space *local_store)
 	spin_lock_init(&ctx->mmio_lock);
 	kref_init(&ctx->kref);
 	init_rwsem(&ctx->state_sema);
+	init_MUTEX(&ctx->run_sema);
 	init_waitqueue_head(&ctx->ibox_wq);
 	init_waitqueue_head(&ctx->wbox_wq);
 	init_waitqueue_head(&ctx->stop_wq);

commit 762cf6dac2623473e83bb271f2bbe97d2355c64d
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Jan 4 20:31:21 2006 +0100

    [PATCH] spufs: fix locking in spu_acquire_runnable
    
    We need to check for validity of owner under down_write,
    down_read is not enough.
    
    Noticed by Al Viro.
    
    Signed-off-by: Arnd Bergmann <arndb@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 1758cec58bc7..903c35d19577 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -120,27 +120,29 @@ int spu_acquire_runnable(struct spu_context *ctx)
 		ctx->spu->prio = current->prio;
 		return 0;
 	}
+	up_read(&ctx->state_sema);
+
+	down_write(&ctx->state_sema);
 	/* ctx is about to be freed, can't acquire any more */
 	if (!ctx->owner) {
 		ret = -EINVAL;
 		goto out;
 	}
-	up_read(&ctx->state_sema);
 
-	down_write(&ctx->state_sema);
 	if (ctx->state == SPU_STATE_SAVED) {
 		ret = spu_activate(ctx, 0);
 		ctx->state = SPU_STATE_RUNNABLE;
 	}
-	downgrade_write(&ctx->state_sema);
 	if (ret)
 		goto out;
 
+	downgrade_write(&ctx->state_sema);
 	/* On success, we return holding the lock */
+
 	return ret;
 out:
 	/* Release here, to simplify calling code. */
-	up_read(&ctx->state_sema);
+	up_write(&ctx->state_sema);
 
 	return ret;
 }

commit 2a911f0bb73e67826062b7d073dd7367ca449724
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Dec 5 22:52:26 2005 -0500

    [PATCH] spufs: Improved SPU preemptability [part 2].
    
    This patch reduces lock complexity of SPU scheduler, particularly
    for involuntary preemptive switches.  As a result the new code
    does a better job of mapping the highest priority tasks to SPUs.
    
    Lock complexity is reduced by using the system default workqueue
    to perform involuntary saves.  In this way we avoid nasty lock
    ordering problems that the previous code had.  A "minimum timeslice"
    for SPU contexts is also introduced.  The intent here is to avoid
    thrashing.
    
    While the new scheduler does a better job at prioritization it
    still does nothing for fairness.
    
    From: Mark Nutter <mnutter@us.ibm.com>
    Signed-off-by: Arnd Bergmann <arndb@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 0d88a1c24f67..1758cec58bc7 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -116,8 +116,10 @@ int spu_acquire_runnable(struct spu_context *ctx)
 	int ret = 0;
 
 	down_read(&ctx->state_sema);
-	if (ctx->state == SPU_STATE_RUNNABLE)
+	if (ctx->state == SPU_STATE_RUNNABLE) {
+		ctx->spu->prio = current->prio;
 		return 0;
+	}
 	/* ctx is about to be freed, can't acquire any more */
 	if (!ctx->owner) {
 		ret = -EINVAL;

commit 5110459f181ef1f11200bb3dec61953f08cc49e7
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Dec 5 22:52:25 2005 -0500

    [PATCH] spufs: Improved SPU preemptability.
    
    This patch makes it easier to preempt an SPU context by
    having the scheduler hold ctx->state_sema for much shorter
    periods of time.
    
    As part of this restructuring, the control logic for the "run"
    operation is moved from arch/ppc64/kernel/spu_base.c to
    fs/spufs/file.c.  Of course the base retains "bottom half"
    handlers for class{0,1} irqs.  The new run loop will re-acquire
    an SPU if preempted.
    
    From: Mark Nutter <mnutter@us.ibm.com>
    Signed-off-by: Arnd Bergmann <arndb@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 5d6195fc107d..0d88a1c24f67 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -45,6 +45,7 @@ struct spu_context *alloc_spu_context(struct address_space *local_store)
 	init_rwsem(&ctx->state_sema);
 	init_waitqueue_head(&ctx->ibox_wq);
 	init_waitqueue_head(&ctx->wbox_wq);
+	init_waitqueue_head(&ctx->stop_wq);
 	ctx->ibox_fasync = NULL;
 	ctx->wbox_fasync = NULL;
 	ctx->state = SPU_STATE_SAVED;
@@ -105,7 +106,7 @@ void spu_release(struct spu_context *ctx)
 	up_read(&ctx->state_sema);
 }
 
-static void spu_unmap_mappings(struct spu_context *ctx)
+void spu_unmap_mappings(struct spu_context *ctx)
 {
 	unmap_mapping_range(ctx->local_store, 0, LS_SIZE, 1);
 }
@@ -126,7 +127,6 @@ int spu_acquire_runnable(struct spu_context *ctx)
 
 	down_write(&ctx->state_sema);
 	if (ctx->state == SPU_STATE_SAVED) {
-		spu_unmap_mappings(ctx);
 		ret = spu_activate(ctx, 0);
 		ctx->state = SPU_STATE_RUNNABLE;
 	}
@@ -154,7 +154,6 @@ void spu_acquire_saved(struct spu_context *ctx)
 	down_write(&ctx->state_sema);
 
 	if (ctx->state == SPU_STATE_RUNNABLE) {
-		spu_unmap_mappings(ctx);
 		spu_deactivate(ctx);
 		ctx->state = SPU_STATE_SAVED;
 	}

commit 8b3d6663c6217e4f50cc3720935a96da9b984117
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Nov 15 15:53:52 2005 -0500

    [PATCH] spufs: cooperative scheduler support
    
    This adds a scheduler for SPUs to make it possible to use
    more logical SPUs than physical ones are present in the
    system.
    
    Currently, there is no support for preempting a running
    SPU thread, they have to leave the SPU by either triggering
    an event on the SPU that causes it to return to the
    owning thread or by sending a signal to it.
    
    This patch also adds operations that enable accessing an SPU
    in either runnable or saved state. We use an RW semaphore
    to protect the state of the SPU from changing underneath
    us, while we are holding it readable. In order to change
    the state, it is acquired writeable and a context save
    or restore is executed before downgrading the semaphore
    to read-only.
    
    From: Mark Nutter <mnutter@us.ibm.com>,
          Uli Weigand <Ulrich.Weigand@de.ibm.com>
    Signed-off-by: Arnd Bergmann <arndb@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index 41eea4576b6d..5d6195fc107d 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -20,39 +20,38 @@
  * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  */
 
+#include <linux/fs.h>
+#include <linux/mm.h>
 #include <linux/slab.h>
 #include <asm/spu.h>
 #include <asm/spu_csa.h>
 #include "spufs.h"
 
-struct spu_context *alloc_spu_context(void)
+struct spu_context *alloc_spu_context(struct address_space *local_store)
 {
 	struct spu_context *ctx;
 	ctx = kmalloc(sizeof *ctx, GFP_KERNEL);
 	if (!ctx)
 		goto out;
-	/* Future enhancement: do not call spu_alloc()
-	 * here.  This step should be deferred until
-	 * spu_run()!!
-	 *
-	 * More work needs to be done to read(),
-	 * write(), mmap(), etc., so that operations
-	 * are performed on CSA when the context is
-	 * not currently being run.  In this way we
-	 * can support arbitrarily large number of
-	 * entries in /spu, allow state queries, etc.
+	/* Binding to physical processor deferred
+	 * until spu_activate().
 	 */
-	ctx->spu = spu_alloc();
-	if (!ctx->spu)
-		goto out_free;
 	spu_init_csa(&ctx->csa);
 	if (!ctx->csa.lscsa) {
-		spu_free(ctx->spu);
 		goto out_free;
 	}
-	init_rwsem(&ctx->backing_sema);
 	spin_lock_init(&ctx->mmio_lock);
 	kref_init(&ctx->kref);
+	init_rwsem(&ctx->state_sema);
+	init_waitqueue_head(&ctx->ibox_wq);
+	init_waitqueue_head(&ctx->wbox_wq);
+	ctx->ibox_fasync = NULL;
+	ctx->wbox_fasync = NULL;
+	ctx->state = SPU_STATE_SAVED;
+	ctx->local_store = local_store;
+	ctx->spu = NULL;
+	ctx->ops = &spu_backing_ops;
+	ctx->owner = get_task_mm(current);
 	goto out;
 out_free:
 	kfree(ctx);
@@ -65,8 +64,11 @@ void destroy_spu_context(struct kref *kref)
 {
 	struct spu_context *ctx;
 	ctx = container_of(kref, struct spu_context, kref);
-	if (ctx->spu)
-		spu_free(ctx->spu);
+	down_write(&ctx->state_sema);
+	spu_deactivate(ctx);
+	ctx->ibox_fasync = NULL;
+	ctx->wbox_fasync = NULL;
+	up_write(&ctx->state_sema);
 	spu_fini_csa(&ctx->csa);
 	kfree(ctx);
 }
@@ -82,4 +84,80 @@ int put_spu_context(struct spu_context *ctx)
 	return kref_put(&ctx->kref, &destroy_spu_context);
 }
 
+/* give up the mm reference when the context is about to be destroyed */
+void spu_forget(struct spu_context *ctx)
+{
+	struct mm_struct *mm;
+	spu_acquire_saved(ctx);
+	mm = ctx->owner;
+	ctx->owner = NULL;
+	mmput(mm);
+	spu_release(ctx);
+}
+
+void spu_acquire(struct spu_context *ctx)
+{
+	down_read(&ctx->state_sema);
+}
+
+void spu_release(struct spu_context *ctx)
+{
+	up_read(&ctx->state_sema);
+}
+
+static void spu_unmap_mappings(struct spu_context *ctx)
+{
+	unmap_mapping_range(ctx->local_store, 0, LS_SIZE, 1);
+}
+
+int spu_acquire_runnable(struct spu_context *ctx)
+{
+	int ret = 0;
 
+	down_read(&ctx->state_sema);
+	if (ctx->state == SPU_STATE_RUNNABLE)
+		return 0;
+	/* ctx is about to be freed, can't acquire any more */
+	if (!ctx->owner) {
+		ret = -EINVAL;
+		goto out;
+	}
+	up_read(&ctx->state_sema);
+
+	down_write(&ctx->state_sema);
+	if (ctx->state == SPU_STATE_SAVED) {
+		spu_unmap_mappings(ctx);
+		ret = spu_activate(ctx, 0);
+		ctx->state = SPU_STATE_RUNNABLE;
+	}
+	downgrade_write(&ctx->state_sema);
+	if (ret)
+		goto out;
+
+	/* On success, we return holding the lock */
+	return ret;
+out:
+	/* Release here, to simplify calling code. */
+	up_read(&ctx->state_sema);
+
+	return ret;
+}
+
+void spu_acquire_saved(struct spu_context *ctx)
+{
+	down_read(&ctx->state_sema);
+
+	if (ctx->state == SPU_STATE_SAVED)
+		return;
+
+	up_read(&ctx->state_sema);
+	down_write(&ctx->state_sema);
+
+	if (ctx->state == SPU_STATE_RUNNABLE) {
+		spu_unmap_mappings(ctx);
+		spu_deactivate(ctx);
+		ctx->state = SPU_STATE_SAVED;
+	}
+
+	downgrade_write(&ctx->state_sema);
+}

commit 5473af049d8b3556874174e61ce1986c9b5e8fa6
Author: Mark Nutter <mnutter@us.ibm.com>
Date:   Tue Nov 15 15:53:49 2005 -0500

    [PATCH] spufs: switchable spu contexts
    
    Add some infrastructure for saving and restoring the context of an
    SPE. This patch creates a new structure that can hold the whole
    state of a physical SPE in memory. It also contains code that
    avoids races during the context switch and the binary code that
    is loaded to the SPU in order to access its registers.
    
    The actual PPE- and SPE-side context switch code are two separate
    patches.
    
    Signed-off-by: Arnd Bergmann <arndb@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
index a69b85e2778a..41eea4576b6d 100644
--- a/arch/powerpc/platforms/cell/spufs/context.c
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -22,6 +22,7 @@
 
 #include <linux/slab.h>
 #include <asm/spu.h>
+#include <asm/spu_csa.h>
 #include "spufs.h"
 
 struct spu_context *alloc_spu_context(void)
@@ -30,9 +31,25 @@ struct spu_context *alloc_spu_context(void)
 	ctx = kmalloc(sizeof *ctx, GFP_KERNEL);
 	if (!ctx)
 		goto out;
+	/* Future enhancement: do not call spu_alloc()
+	 * here.  This step should be deferred until
+	 * spu_run()!!
+	 *
+	 * More work needs to be done to read(),
+	 * write(), mmap(), etc., so that operations
+	 * are performed on CSA when the context is
+	 * not currently being run.  In this way we
+	 * can support arbitrarily large number of
+	 * entries in /spu, allow state queries, etc.
+	 */
 	ctx->spu = spu_alloc();
 	if (!ctx->spu)
 		goto out_free;
+	spu_init_csa(&ctx->csa);
+	if (!ctx->csa.lscsa) {
+		spu_free(ctx->spu);
+		goto out_free;
+	}
 	init_rwsem(&ctx->backing_sema);
 	spin_lock_init(&ctx->mmio_lock);
 	kref_init(&ctx->kref);
@@ -50,6 +67,7 @@ void destroy_spu_context(struct kref *kref)
 	ctx = container_of(kref, struct spu_context, kref);
 	if (ctx->spu)
 		spu_free(ctx->spu);
+	spu_fini_csa(&ctx->csa);
 	kfree(ctx);
 }
 

commit 67207b9664a8d603138ef1556141e6d0a102bea7
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Nov 15 15:53:48 2005 -0500

    [PATCH] spufs: The SPU file system, base
    
    This is the current version of the spu file system, used
    for driving SPEs on the Cell Broadband Engine.
    
    This release is almost identical to the version for the
    2.6.14 kernel posted earlier, which is available as part
    of the Cell BE Linux distribution from
    http://www.bsc.es/projects/deepcomputing/linuxoncell/.
    
    The first patch provides all the interfaces for running
    spu application, but does not have any support for
    debugging SPU tasks or for scheduling. Both these
    functionalities are added in the subsequent patches.
    
    See Documentation/filesystems/spufs.txt on how to use
    spufs.
    
    Signed-off-by: Arnd Bergmann <arndb@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/platforms/cell/spufs/context.c b/arch/powerpc/platforms/cell/spufs/context.c
new file mode 100644
index 000000000000..a69b85e2778a
--- /dev/null
+++ b/arch/powerpc/platforms/cell/spufs/context.c
@@ -0,0 +1,67 @@
+/*
+ * SPU file system -- SPU context management
+ *
+ * (C) Copyright IBM Deutschland Entwicklung GmbH 2005
+ *
+ * Author: Arnd Bergmann <arndb@de.ibm.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2, or (at your option)
+ * any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/slab.h>
+#include <asm/spu.h>
+#include "spufs.h"
+
+struct spu_context *alloc_spu_context(void)
+{
+	struct spu_context *ctx;
+	ctx = kmalloc(sizeof *ctx, GFP_KERNEL);
+	if (!ctx)
+		goto out;
+	ctx->spu = spu_alloc();
+	if (!ctx->spu)
+		goto out_free;
+	init_rwsem(&ctx->backing_sema);
+	spin_lock_init(&ctx->mmio_lock);
+	kref_init(&ctx->kref);
+	goto out;
+out_free:
+	kfree(ctx);
+	ctx = NULL;
+out:
+	return ctx;
+}
+
+void destroy_spu_context(struct kref *kref)
+{
+	struct spu_context *ctx;
+	ctx = container_of(kref, struct spu_context, kref);
+	if (ctx->spu)
+		spu_free(ctx->spu);
+	kfree(ctx);
+}
+
+struct spu_context * get_spu_context(struct spu_context *ctx)
+{
+	kref_get(&ctx->kref);
+	return ctx;
+}
+
+int put_spu_context(struct spu_context *ctx)
+{
+	return kref_put(&ctx->kref, &destroy_spu_context);
+}
+
+
