commit bf2c59fce4074e55d622089b34be3a6bc95484fb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 1 17:40:33 2020 -0400

    sched/core: Fix illegal RCU from offline CPUs
    
    In the CPU-offline process, it calls mmdrop() after idle entry and the
    subsequent call to cpuhp_report_idle_dead(). Once execution passes the
    call to rcu_report_dead(), RCU is ignoring the CPU, which results in
    lockdep complaining when mmdrop() uses RCU from either memcg or
    debugobjects below.
    
    Fix it by cleaning up the active_mm state from BP instead. Every arch
    which has CONFIG_HOTPLUG_CPU should have already called idle_task_exit()
    from AP. The only exception is parisc because it switches them to
    &init_mm unconditionally (see smp_boot_one_cpu() and smp_cpu_init()),
    but the patch will still work there because it calls mmgrab(&init_mm) in
    smp_cpu_init() and then should call mmdrop(&init_mm) in finish_cpu().
    
      WARNING: suspicious RCU usage
      -----------------------------
      kernel/workqueue.c:710 RCU or wq_pool_mutex should be held!
    
      other info that might help us debug this:
    
      RCU used illegally from offline CPU!
      Call Trace:
       dump_stack+0xf4/0x164 (unreliable)
       lockdep_rcu_suspicious+0x140/0x164
       get_work_pool+0x110/0x150
       __queue_work+0x1bc/0xca0
       queue_work_on+0x114/0x120
       css_release+0x9c/0xc0
       percpu_ref_put_many+0x204/0x230
       free_pcp_prepare+0x264/0x570
       free_unref_page+0x38/0xf0
       __mmdrop+0x21c/0x2c0
       idle_task_exit+0x170/0x1b0
       pnv_smp_cpu_kill_self+0x38/0x2e0
       cpu_die+0x48/0x64
       arch_cpu_idle_dead+0x30/0x50
       do_idle+0x2f4/0x470
       cpu_startup_entry+0x38/0x40
       start_secondary+0x7a8/0xa80
       start_secondary_resume+0x10/0x14
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Link: https://lkml.kernel.org/r/20200401214033.8448-1-cai@lca.pw

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 13e251699346..b2ba3e95bda7 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -167,7 +167,6 @@ static void pnv_smp_cpu_kill_self(void)
 	/* Standard hot unplug procedure */
 
 	idle_task_exit();
-	current->active_mm = NULL; /* for sanity */
 	cpu = smp_processor_id();
 	DBG("CPU%d offline\n", cpu);
 	generic_set_cpu_dead(cpu);

commit 7d6475051fb3d9339c5c760ed9883bc0a9048b21
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Oct 22 21:58:14 2019 +1000

    powerpc/powernv: Fix CPU idle to be called with IRQs disabled
    
    Commit e78a7614f3876 ("idle: Prevent late-arriving interrupts from
    disrupting offline") changes arch_cpu_idle_dead to be called with
    interrupts disabled, which triggers the WARN in pnv_smp_cpu_kill_self.
    
    Fix this by fixing up irq_happened after hard disabling, rather than
    requiring there are no pending interrupts, similarly to what was done
    done until commit 2525db04d1cc5 ("powerpc/powernv: Simplify lazy IRQ
    handling in CPU offline").
    
    Fixes: e78a7614f3876 ("idle: Prevent late-arriving interrupts from disrupting offline")
    Reported-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Add unexpected_mask rather than checking for known bad values,
          change the WARN_ON() to a WARN_ON_ONCE()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20191022115814.22456-1-npiggin@gmail.com

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index fbd6e6b7bbf2..13e251699346 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -146,20 +146,25 @@ static int pnv_smp_cpu_disable(void)
 	return 0;
 }
 
+static void pnv_flush_interrupts(void)
+{
+	if (cpu_has_feature(CPU_FTR_ARCH_300)) {
+		if (xive_enabled())
+			xive_flush_interrupt();
+		else
+			icp_opal_flush_interrupt();
+	} else {
+		icp_native_flush_interrupt();
+	}
+}
+
 static void pnv_smp_cpu_kill_self(void)
 {
+	unsigned long srr1, unexpected_mask, wmask;
 	unsigned int cpu;
-	unsigned long srr1, wmask;
 	u64 lpcr_val;
 
 	/* Standard hot unplug procedure */
-	/*
-	 * This hard disables local interurpts, ensuring we have no lazy
-	 * irqs pending.
-	 */
-	WARN_ON(irqs_disabled());
-	hard_irq_disable();
-	WARN_ON(lazy_irq_pending());
 
 	idle_task_exit();
 	current->active_mm = NULL; /* for sanity */
@@ -172,6 +177,27 @@ static void pnv_smp_cpu_kill_self(void)
 	if (cpu_has_feature(CPU_FTR_ARCH_207S))
 		wmask = SRR1_WAKEMASK_P8;
 
+	/*
+	 * This turns the irq soft-disabled state we're called with, into a
+	 * hard-disabled state with pending irq_happened interrupts cleared.
+	 *
+	 * PACA_IRQ_DEC   - Decrementer should be ignored.
+	 * PACA_IRQ_HMI   - Can be ignored, processing is done in real mode.
+	 * PACA_IRQ_DBELL, EE, PMI - Unexpected.
+	 */
+	hard_irq_disable();
+	if (generic_check_cpu_restart(cpu))
+		goto out;
+
+	unexpected_mask = ~(PACA_IRQ_DEC | PACA_IRQ_HMI | PACA_IRQ_HARD_DIS);
+	if (local_paca->irq_happened & unexpected_mask) {
+		if (local_paca->irq_happened & PACA_IRQ_EE)
+			pnv_flush_interrupts();
+		DBG("CPU%d Unexpected exit while offline irq_happened=%lx!\n",
+				cpu, local_paca->irq_happened);
+	}
+	local_paca->irq_happened = PACA_IRQ_HARD_DIS;
+
 	/*
 	 * We don't want to take decrementer interrupts while we are
 	 * offline, so clear LPCR:PECE1. We keep PECE2 (and
@@ -197,6 +223,7 @@ static void pnv_smp_cpu_kill_self(void)
 
 		srr1 = pnv_cpu_offline(cpu);
 
+		WARN_ON_ONCE(!irqs_disabled());
 		WARN_ON(lazy_irq_pending());
 
 		/*
@@ -212,13 +239,7 @@ static void pnv_smp_cpu_kill_self(void)
 		 */
 		if (((srr1 & wmask) == SRR1_WAKEEE) ||
 		    ((srr1 & wmask) == SRR1_WAKEHVI)) {
-			if (cpu_has_feature(CPU_FTR_ARCH_300)) {
-				if (xive_enabled())
-					xive_flush_interrupt();
-				else
-					icp_opal_flush_interrupt();
-			} else
-				icp_native_flush_interrupt();
+			pnv_flush_interrupts();
 		} else if ((srr1 & wmask) == SRR1_WAKEHDBELL) {
 			unsigned long msg = PPC_DBELL_TYPE(PPC_DBELL_SERVER);
 			asm volatile(PPC_MSGCLR(%0) : : "r" (msg));
@@ -266,7 +287,7 @@ static void pnv_smp_cpu_kill_self(void)
 	 */
 	lpcr_val = mfspr(SPRN_LPCR) | (u64)LPCR_PECE1;
 	pnv_program_cpu_hotplug_lpcr(cpu, lpcr_val);
-
+out:
 	DBG("CPU%d coming online...\n", cpu);
 }
 

commit 3a83f677a6eeff65751b29e3648d7c69c3be83f3
Author: Michael Roth <mdroth@linux.vnet.ibm.com>
Date:   Wed Sep 11 17:31:55 2019 -0500

    KVM: PPC: Book3S HV: use smp_mb() when setting/clearing host_ipi flag
    
    On a 2-socket Power9 system with 32 cores/128 threads (SMT4) and 1TB
    of memory running the following guest configs:
    
      guest A:
        - 224GB of memory
        - 56 VCPUs (sockets=1,cores=28,threads=2), where:
          VCPUs 0-1 are pinned to CPUs 0-3,
          VCPUs 2-3 are pinned to CPUs 4-7,
          ...
          VCPUs 54-55 are pinned to CPUs 108-111
    
      guest B:
        - 4GB of memory
        - 4 VCPUs (sockets=1,cores=4,threads=1)
    
    with the following workloads (with KSM and THP enabled in all):
    
      guest A:
        stress --cpu 40 --io 20 --vm 20 --vm-bytes 512M
    
      guest B:
        stress --cpu 4 --io 4 --vm 4 --vm-bytes 512M
    
      host:
        stress --cpu 4 --io 4 --vm 2 --vm-bytes 256M
    
    the below soft-lockup traces were observed after an hour or so and
    persisted until the host was reset (this was found to be reliably
    reproducible for this configuration, for kernels 4.15, 4.18, 5.0,
    and 5.3-rc5):
    
      [ 1253.183290] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1253.183319] rcu:     124-....: (5250 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=1941
      [ 1256.287426] watchdog: BUG: soft lockup - CPU#105 stuck for 23s! [CPU 52/KVM:19709]
      [ 1264.075773] watchdog: BUG: soft lockup - CPU#24 stuck for 23s! [worker:19913]
      [ 1264.079769] watchdog: BUG: soft lockup - CPU#31 stuck for 23s! [worker:20331]
      [ 1264.095770] watchdog: BUG: soft lockup - CPU#45 stuck for 23s! [worker:20338]
      [ 1264.131773] watchdog: BUG: soft lockup - CPU#64 stuck for 23s! [avocado:19525]
      [ 1280.408480] watchdog: BUG: soft lockup - CPU#124 stuck for 22s! [ksmd:791]
      [ 1316.198012] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1316.198032] rcu:     124-....: (21003 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=8243
      [ 1340.411024] watchdog: BUG: soft lockup - CPU#124 stuck for 22s! [ksmd:791]
      [ 1379.212609] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1379.212629] rcu:     124-....: (36756 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=14714
      [ 1404.413615] watchdog: BUG: soft lockup - CPU#124 stuck for 22s! [ksmd:791]
      [ 1442.227095] rcu: INFO: rcu_sched self-detected stall on CPU
      [ 1442.227115] rcu:     124-....: (52509 ticks this GP) idle=10a/1/0x4000000000000002 softirq=5408/5408 fqs=21403
      [ 1455.111787] INFO: task worker:19907 blocked for more than 120 seconds.
      [ 1455.111822]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.111833] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.111884] INFO: task worker:19908 blocked for more than 120 seconds.
      [ 1455.111905]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.111925] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.111966] INFO: task worker:20328 blocked for more than 120 seconds.
      [ 1455.111986]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.111998] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112048] INFO: task worker:20330 blocked for more than 120 seconds.
      [ 1455.112068]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112097] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112138] INFO: task worker:20332 blocked for more than 120 seconds.
      [ 1455.112159]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112179] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112210] INFO: task worker:20333 blocked for more than 120 seconds.
      [ 1455.112231]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112242] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112282] INFO: task worker:20335 blocked for more than 120 seconds.
      [ 1455.112303]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
      [ 1455.112332] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      [ 1455.112372] INFO: task worker:20336 blocked for more than 120 seconds.
      [ 1455.112392]       Tainted: G             L    5.3.0-rc5-mdr-vanilla+ #1
    
    CPUs 45, 24, and 124 are stuck on spin locks, likely held by
    CPUs 105 and 31.
    
    CPUs 105 and 31 are stuck in smp_call_function_many(), waiting on
    target CPU 42. For instance:
    
      # CPU 105 registers (via xmon)
      R00 = c00000000020b20c   R16 = 00007d1bcd800000
      R01 = c00000363eaa7970   R17 = 0000000000000001
      R02 = c0000000019b3a00   R18 = 000000000000006b
      R03 = 000000000000002a   R19 = 00007d537d7aecf0
      R04 = 000000000000002a   R20 = 60000000000000e0
      R05 = 000000000000002a   R21 = 0801000000000080
      R06 = c0002073fb0caa08   R22 = 0000000000000d60
      R07 = c0000000019ddd78   R23 = 0000000000000001
      R08 = 000000000000002a   R24 = c00000000147a700
      R09 = 0000000000000001   R25 = c0002073fb0ca908
      R10 = c000008ffeb4e660   R26 = 0000000000000000
      R11 = c0002073fb0ca900   R27 = c0000000019e2464
      R12 = c000000000050790   R28 = c0000000000812b0
      R13 = c000207fff623e00   R29 = c0002073fb0ca808
      R14 = 00007d1bbee00000   R30 = c0002073fb0ca800
      R15 = 00007d1bcd600000   R31 = 0000000000000800
      pc  = c00000000020b260 smp_call_function_many+0x3d0/0x460
      cfar= c00000000020b270 smp_call_function_many+0x3e0/0x460
      lr  = c00000000020b20c smp_call_function_many+0x37c/0x460
      msr = 900000010288b033   cr  = 44024824
      ctr = c000000000050790   xer = 0000000000000000   trap =  100
    
    CPU 42 is running normally, doing VCPU work:
    
      # CPU 42 stack trace (via xmon)
      [link register   ] c00800001be17188 kvmppc_book3s_radix_page_fault+0x90/0x2b0 [kvm_hv]
      [c000008ed3343820] c000008ed3343850 (unreliable)
      [c000008ed33438d0] c00800001be11b6c kvmppc_book3s_hv_page_fault+0x264/0xe30 [kvm_hv]
      [c000008ed33439d0] c00800001be0d7b4 kvmppc_vcpu_run_hv+0x8dc/0xb50 [kvm_hv]
      [c000008ed3343ae0] c00800001c10891c kvmppc_vcpu_run+0x34/0x48 [kvm]
      [c000008ed3343b00] c00800001c10475c kvm_arch_vcpu_ioctl_run+0x244/0x420 [kvm]
      [c000008ed3343b90] c00800001c0f5a78 kvm_vcpu_ioctl+0x470/0x7c8 [kvm]
      [c000008ed3343d00] c000000000475450 do_vfs_ioctl+0xe0/0xc70
      [c000008ed3343db0] c0000000004760e4 ksys_ioctl+0x104/0x120
      [c000008ed3343e00] c000000000476128 sys_ioctl+0x28/0x80
      [c000008ed3343e20] c00000000000b388 system_call+0x5c/0x70
      --- Exception: c00 (System Call) at 00007d545cfd7694
      SP (7d53ff7edf50) is in userspace
    
    It was subsequently found that ipi_message[PPC_MSG_CALL_FUNCTION]
    was set for CPU 42 by at least 1 of the CPUs waiting in
    smp_call_function_many(), but somehow the corresponding
    call_single_queue entries were never processed by CPU 42, causing the
    callers to spin in csd_lock_wait() indefinitely.
    
    Nick Piggin suggested something similar to the following sequence as
    a possible explanation (interleaving of CALL_FUNCTION/RESCHEDULE
    IPI messages seems to be most common, but any mix of CALL_FUNCTION and
    !CALL_FUNCTION messages could trigger it):
    
        CPU
          X: smp_muxed_ipi_set_message():
          X:   smp_mb()
          X:   message[RESCHEDULE] = 1
          X: doorbell_global_ipi(42):
          X:   kvmppc_set_host_ipi(42, 1)
          X:   ppc_msgsnd_sync()/smp_mb()
          X:   ppc_msgsnd() -> 42
         42: doorbell_exception(): // from CPU X
         42:   ppc_msgsync()
        105: smp_muxed_ipi_set_message():
        105:   smb_mb()
             // STORE DEFERRED DUE TO RE-ORDERING
      --105:   message[CALL_FUNCTION] = 1
      | 105: doorbell_global_ipi(42):
      | 105:   kvmppc_set_host_ipi(42, 1)
      |  42:   kvmppc_set_host_ipi(42, 0)
      |  42: smp_ipi_demux_relaxed()
      |  42: // returns to executing guest
      |      // RE-ORDERED STORE COMPLETES
      ->105:   message[CALL_FUNCTION] = 1
        105:   ppc_msgsnd_sync()/smp_mb()
        105:   ppc_msgsnd() -> 42
         42: local_paca->kvm_hstate.host_ipi == 0 // IPI ignored
        105: // hangs waiting on 42 to process messages/call_single_queue
    
    This can be prevented with an smp_mb() at the beginning of
    kvmppc_set_host_ipi(), such that stores to message[<type>] (or other
    state indicated by the host_ipi flag) are ordered vs. the store to
    to host_ipi.
    
    However, doing so might still allow for the following scenario (not
    yet observed):
    
        CPU
          X: smp_muxed_ipi_set_message():
          X:   smp_mb()
          X:   message[RESCHEDULE] = 1
          X: doorbell_global_ipi(42):
          X:   kvmppc_set_host_ipi(42, 1)
          X:   ppc_msgsnd_sync()/smp_mb()
          X:   ppc_msgsnd() -> 42
         42: doorbell_exception(): // from CPU X
         42:   ppc_msgsync()
             // STORE DEFERRED DUE TO RE-ORDERING
      -- 42:   kvmppc_set_host_ipi(42, 0)
      |  42: smp_ipi_demux_relaxed()
      | 105: smp_muxed_ipi_set_message():
      | 105:   smb_mb()
      | 105:   message[CALL_FUNCTION] = 1
      | 105: doorbell_global_ipi(42):
      | 105:   kvmppc_set_host_ipi(42, 1)
      |      // RE-ORDERED STORE COMPLETES
      -> 42:   kvmppc_set_host_ipi(42, 0)
         42: // returns to executing guest
        105:   ppc_msgsnd_sync()/smp_mb()
        105:   ppc_msgsnd() -> 42
         42: local_paca->kvm_hstate.host_ipi == 0 // IPI ignored
        105: // hangs waiting on 42 to process messages/call_single_queue
    
    Fixing this scenario would require an smp_mb() *after* clearing
    host_ipi flag in kvmppc_set_host_ipi() to order the store vs.
    subsequent processing of IPI messages.
    
    To handle both cases, this patch splits kvmppc_set_host_ipi() into
    separate set/clear functions, where we execute smp_mb() prior to
    setting host_ipi flag, and after clearing host_ipi flag. These
    functions pair with each other to synchronize the sender and receiver
    sides.
    
    With that change in place the above workload ran for 20 hours without
    triggering any lock-ups.
    
    Fixes: 755563bc79c7 ("powerpc/powernv: Fixes for hypervisor doorbell handling") # v4.0
    Signed-off-by: Michael Roth <mdroth@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190911223155.16045-1-mdroth@linux.vnet.ibm.com

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 94cd96b9b7bb..fbd6e6b7bbf2 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -193,7 +193,7 @@ static void pnv_smp_cpu_kill_self(void)
 		 * for coming online, which are handled via
 		 * generic_check_cpu_restart() calls.
 		 */
-		kvmppc_set_host_ipi(cpu, 0);
+		kvmppc_clear_host_ipi(cpu);
 
 		srr1 = pnv_cpu_offline(cpu);
 

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index db09c7022635..94cd96b9b7bb 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -1,12 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * SMP support for PowerNV machines.
  *
  * Copyright 2011 IBM Corp.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #include <linux/kernel.h>

commit 19f8a5b5be2898573a5e1dc1db93e8d40117606a
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Tue Feb 12 11:58:29 2019 +1100

    powerpc/powernv: Don't reprogram SLW image on every KVM guest entry/exit
    
    Commit 24be85a23d1f ("powerpc/powernv: Clear PECE1 in LPCR via stop-api
    only on Hotplug", 2017-07-21) added two calls to opal_slw_set_reg()
    inside pnv_cpu_offline(), with the aim of changing the LPCR value in
    the SLW image to disable wakeups from the decrementer while a CPU is
    offline.  However, pnv_cpu_offline() gets called each time a secondary
    CPU thread is woken up to participate in running a KVM guest, that is,
    not just when a CPU is offlined.
    
    Since opal_slw_set_reg() is a very slow operation (with observed
    execution times around 20 milliseconds), this means that an offline
    secondary CPU can often be busy doing the opal_slw_set_reg() call
    when the primary CPU wants to grab all the secondary threads so that
    it can run a KVM guest.  This leads to messages like "KVM: couldn't
    grab CPU n" being printed and guest execution failing.
    
    There is no need to reprogram the SLW image on every KVM guest entry
    and exit.  So that we do it only when a CPU is really transitioning
    between online and offline, this moves the calls to
    pnv_program_cpu_hotplug_lpcr() into pnv_smp_cpu_kill_self().
    
    Fixes: 24be85a23d1f ("powerpc/powernv: Clear PECE1 in LPCR via stop-api only on Hotplug")
    Cc: stable@vger.kernel.org # v4.14+
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 0d354e19ef92..db09c7022635 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -39,6 +39,7 @@
 #include <asm/cpuidle.h>
 #include <asm/kexec.h>
 #include <asm/reg.h>
+#include <asm/powernv.h>
 
 #include "powernv.h"
 
@@ -153,6 +154,7 @@ static void pnv_smp_cpu_kill_self(void)
 {
 	unsigned int cpu;
 	unsigned long srr1, wmask;
+	u64 lpcr_val;
 
 	/* Standard hot unplug procedure */
 	/*
@@ -174,6 +176,19 @@ static void pnv_smp_cpu_kill_self(void)
 	if (cpu_has_feature(CPU_FTR_ARCH_207S))
 		wmask = SRR1_WAKEMASK_P8;
 
+	/*
+	 * We don't want to take decrementer interrupts while we are
+	 * offline, so clear LPCR:PECE1. We keep PECE2 (and
+	 * LPCR_PECE_HVEE on P9) enabled so as to let IPIs in.
+	 *
+	 * If the CPU gets woken up by a special wakeup, ensure that
+	 * the SLW engine sets LPCR with decrementer bit cleared, else
+	 * the CPU will come back to the kernel due to a spurious
+	 * wakeup.
+	 */
+	lpcr_val = mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1;
+	pnv_program_cpu_hotplug_lpcr(cpu, lpcr_val);
+
 	while (!generic_check_cpu_restart(cpu)) {
 		/*
 		 * Clear IPI flag, since we don't handle IPIs while
@@ -246,6 +261,16 @@ static void pnv_smp_cpu_kill_self(void)
 
 	}
 
+	/*
+	 * Re-enable decrementer interrupts in LPCR.
+	 *
+	 * Further, we want stop states to be woken up by decrementer
+	 * for non-hotplug cases. So program the LPCR via stop api as
+	 * well.
+	 */
+	lpcr_val = mfspr(SPRN_LPCR) | (u64)LPCR_PECE1;
+	pnv_program_cpu_hotplug_lpcr(cpu, lpcr_val);
+
 	DBG("CPU%d coming online...\n", cpu);
 }
 

commit 2bf1071a8d50928a4ae366bb3108833166c2b70c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Jul 5 18:47:00 2018 +1000

    powerpc/64s: Remove POWER9 DD1 support
    
    POWER9 DD1 was never a product. It is no longer supported by upstream
    firmware, and it is not effectively supported in Linux due to lack of
    testing.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Michael Ellerman <mpe@ellerman.id.au>
    [mpe: Remove arch_make_huge_pte() entirely]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index b80909957792..0d354e19ef92 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -283,23 +283,6 @@ static void pnv_cause_ipi(int cpu)
 	ic_cause_ipi(cpu);
 }
 
-static void pnv_p9_dd1_cause_ipi(int cpu)
-{
-	int this_cpu = get_cpu();
-
-	/*
-	 * POWER9 DD1 has a global addressed msgsnd, but for now we restrict
-	 * IPIs to same core, because it requires additional synchronization
-	 * for inter-core doorbells which we do not implement.
-	 */
-	if (cpumask_test_cpu(cpu, cpu_sibling_mask(this_cpu)))
-		doorbell_global_ipi(cpu);
-	else
-		ic_cause_ipi(cpu);
-
-	put_cpu();
-}
-
 static void __init pnv_smp_probe(void)
 {
 	if (xive_enabled())
@@ -311,14 +294,10 @@ static void __init pnv_smp_probe(void)
 		ic_cause_ipi = smp_ops->cause_ipi;
 		WARN_ON(!ic_cause_ipi);
 
-		if (cpu_has_feature(CPU_FTR_ARCH_300)) {
-			if (cpu_has_feature(CPU_FTR_POWER9_DD1))
-				smp_ops->cause_ipi = pnv_p9_dd1_cause_ipi;
-			else
-				smp_ops->cause_ipi = doorbell_global_ipi;
-		} else {
+		if (cpu_has_feature(CPU_FTR_ARCH_300))
+			smp_ops->cause_ipi = doorbell_global_ipi;
+		else
 			smp_ops->cause_ipi = pnv_cause_ipi;
-		}
 	}
 }
 

commit ee03b9b4479d1302d01cebedda3518dc967697b7
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu May 10 22:21:48 2018 +1000

    powerpc/powernv: call OPAL_QUIESCE before OPAL_SIGNAL_SYSTEM_RESET
    
    Although it is often possible to recover a CPU that was interrupted
    from OPAL with a system reset NMI, it's undesirable to interrupt them
    for a few reasons. Firstly because dump/debug code itself needs to
    call firmware, so it could hang on a lock or possibly corrupt a
    per-cpu data structure if it or another CPU was interrupted from
    OPAL. Secondly, the kexec crash dump code will not return from
    interrupt to unwind the OPAL call.
    
    Call OPAL_QUIESCE with QUIESCE_HOLD before sending an NMI IPI to
    another CPU, which wait for it to leave firmware (or time out) to
    avoid this problem in normal conditions. Firmware bugs may still
    result in a timeout and interrupting OPAL, but that is the best
    option (stops the CPU, and possibly allows firmware to be debugged).
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 19af6de6b6f0..b80909957792 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -334,7 +334,16 @@ static int pnv_cause_nmi_ipi(int cpu)
 	int64_t rc;
 
 	if (cpu >= 0) {
-		rc = opal_signal_system_reset(get_hard_smp_processor_id(cpu));
+		int h = get_hard_smp_processor_id(cpu);
+
+		if (opal_check_token(OPAL_QUIESCE))
+			opal_quiesce(QUIESCE_HOLD, h);
+
+		rc = opal_signal_system_reset(h);
+
+		if (opal_check_token(OPAL_QUIESCE))
+			opal_quiesce(QUIESCE_RESUME, h);
+
 		if (rc != OPAL_SUCCESS)
 			return 0;
 		return 1;
@@ -343,6 +352,8 @@ static int pnv_cause_nmi_ipi(int cpu)
 		bool success = true;
 		int c;
 
+		if (opal_check_token(OPAL_QUIESCE))
+			opal_quiesce(QUIESCE_HOLD, -1);
 
 		/*
 		 * We do not use broadcasts (yet), because it's not clear
@@ -358,6 +369,10 @@ static int pnv_cause_nmi_ipi(int cpu)
 			if (rc != OPAL_SUCCESS)
 				success = false;
 		}
+
+		if (opal_check_token(OPAL_QUIESCE))
+			opal_quiesce(QUIESCE_RESUME, -1);
+
 		if (success)
 			return 1;
 

commit d2e60075a3d4422dc54b919f3b125d8066b839d4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:12 2018 +1000

    powerpc/64: Use array of paca pointers and allocate pacas individually
    
    Change the paca array into an array of pointers to pacas. Allocate
    pacas individually.
    
    This allows flexibility in where the PACAs are allocated. Future work
    will allocate them node-local. Platforms that don't have address limits
    on PACAs would be able to defer PACA allocations until later in boot
    rather than allocate all possible ones up-front then freeing unused.
    
    This is slightly more overhead (one additional indirection) for cross
    CPU paca references, but those aren't too common.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 9664c8461f03..19af6de6b6f0 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -80,7 +80,7 @@ static int pnv_smp_kick_cpu(int nr)
 	 * If we already started or OPAL is not supported, we just
 	 * kick the CPU via the PACA
 	 */
-	if (paca[nr].cpu_start || !firmware_has_feature(FW_FEATURE_OPAL))
+	if (paca_ptrs[nr]->cpu_start || !firmware_has_feature(FW_FEATURE_OPAL))
 		goto kick;
 
 	/*

commit 4145f358644b970fcff293c09fdcc7939e8527d2
Author: Balbir Singh <bsingharora@gmail.com>
Date:   Fri Dec 15 19:14:55 2017 +1100

    powernv/kdump: Fix cases where the kdump kernel can get HMI's
    
    Certain HMI's such as malfunction error propagate through
    all threads/core on the system. If a thread was offline
    prior to us crashing the system and jumping to the kdump
    kernel, bad things happen when it wakes up due to an HMI
    in the kdump kernel.
    
    There are several possible ways to solve this problem
    
    1. Put the offline cores in a state such that they are
    not woken up for machine check and HMI errors. This
    does not work, since we might need to wake up offline
    threads to handle TB errors
    2. Ignore HMI errors, setup HMEER to mask HMI errors,
    but this still leads the window open for any MCEs
    and masking them for the duration of the dump might
    be a concern
    3. Wake up offline CPUs, as in send them to
    crash_ipi_callback (not wake them up as in mark them
    online as seen by the hotplug). kexec does a
    wake_online_cpus() call, this patch does something
    similar, but instead sends an IPI and forces them to
    crash_ipi_callback()
    
    This patch takes approach #3.
    
    Care is taken to enable this only for powenv platforms
    via crash_wake_offline (a global value set at setup
    time). The crash code sends out IPI's to all CPU's
    which then move to crash_ipi_callback and kexec_smp_wait().
    
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index ba030669eca1..9664c8461f03 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -37,6 +37,8 @@
 #include <asm/kvm_ppc.h>
 #include <asm/ppc-opcode.h>
 #include <asm/cpuidle.h>
+#include <asm/kexec.h>
+#include <asm/reg.h>
 
 #include "powernv.h"
 
@@ -209,9 +211,32 @@ static void pnv_smp_cpu_kill_self(void)
 		} else if ((srr1 & wmask) == SRR1_WAKEHDBELL) {
 			unsigned long msg = PPC_DBELL_TYPE(PPC_DBELL_SERVER);
 			asm volatile(PPC_MSGCLR(%0) : : "r" (msg));
+		} else if ((srr1 & wmask) == SRR1_WAKERESET) {
+			irq_set_pending_from_srr1(srr1);
+			/* Does not return */
 		}
+
 		smp_mb();
 
+		/*
+		 * For kdump kernels, we process the ipi and jump to
+		 * crash_ipi_callback
+		 */
+		if (kdump_in_progress()) {
+			/*
+			 * If we got to this point, we've not used
+			 * NMI's, otherwise we would have gone
+			 * via the SRR1_WAKERESET path. We are
+			 * using regular IPI's for waking up offline
+			 * threads.
+			 */
+			struct pt_regs regs;
+
+			ppc_save_regs(&regs);
+			crash_ipi_callback(&regs);
+			/* Does not return */
+		}
+
 		if (cpu_core_split_required())
 			continue;
 
@@ -371,5 +396,8 @@ void __init pnv_smp_init(void)
 
 #ifdef CONFIG_HOTPLUG_CPU
 	ppc_md.cpu_die	= pnv_smp_cpu_kill_self;
+#ifdef CONFIG_KEXEC_CORE
+	crash_wake_offline = 1;
+#endif
 #endif
 }

commit e36d0a2ed5019184bb9b94ff1138c87c05905789
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Sep 29 13:29:42 2017 +1000

    powerpc/powernv: Implement NMI IPI with OPAL_SIGNAL_SYSTEM_RESET
    
    This allows MSR[EE]=0 lockups to be detected on an OPAL (bare metal)
    system similarly to the hcall NMI IPI on pseries guests, when the
    platform/firmware supports it.
    
    This is an example of CPU10 spinning with interrupts hard disabled:
    
      Watchdog CPU:32 detected Hard LOCKUP other CPUS:10
      Watchdog CPU:10 Hard LOCKUP
      CPU: 10 PID: 4410 Comm: bash Not tainted 4.13.0-rc7-00074-ge89ce1f89f62-dirty #34
      task: c0000003a82b4400 task.stack: c0000003af55c000
      NIP: c0000000000a7b38 LR: c000000000659044 CTR: c0000000000a7b00
      REGS: c00000000fd23d80 TRAP: 0100   Not tainted  (4.13.0-rc7-00074-ge89ce1f89f62-dirty)
      MSR: 90000000000c1033 <SF,HV,ME,IR,DR,RI,LE>
      CR: 28422222  XER: 20000000
      CFAR: c0000000000a7b38 SOFTE: 0
      GPR00: c000000000659044 c0000003af55fbb0 c000000001072a00 0000000000000078
      GPR04: c0000003c81b5c80 c0000003c81cc7e8 9000000000009033 0000000000000000
      GPR08: 0000000000000000 c0000000000a7b00 0000000000000001 9000000000001003
      GPR12: c0000000000a7b00 c00000000fd83200 0000000010180df8 0000000010189e60
      GPR16: 0000000010189ed8 0000000010151270 000000001018bd88 000000001018de78
      GPR20: 00000000370a0668 0000000000000001 00000000101645e0 0000000010163c10
      GPR24: 00007fffd14d6294 00007fffd14d6290 c000000000fba6f0 0000000000000004
      GPR28: c000000000f351d8 0000000000000078 c000000000f4095c 0000000000000000
      NIP [c0000000000a7b38] sysrq_handle_xmon+0x38/0x40
      LR [c000000000659044] __handle_sysrq+0xe4/0x270
      Call Trace:
      [c0000003af55fbd0] [c000000000659044] __handle_sysrq+0xe4/0x270
      [c0000003af55fc70] [c000000000659810] write_sysrq_trigger+0x70/0xa0
      [c0000003af55fca0] [c0000000003da650] proc_reg_write+0xb0/0x110
      [c0000003af55fcf0] [c0000000003423bc] __vfs_write+0x6c/0x1b0
      [c0000003af55fd90] [c000000000344398] vfs_write+0xd8/0x240
      [c0000003af55fde0] [c00000000034632c] SyS_write+0x6c/0x110
      [c0000003af55fe30] [c00000000000b220] system_call+0x58/0x6c
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Use kernel types for opal_signal_system_reset()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 355d3f99cafb..ba030669eca1 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -297,6 +297,54 @@ static void __init pnv_smp_probe(void)
 	}
 }
 
+static int pnv_system_reset_exception(struct pt_regs *regs)
+{
+	if (smp_handle_nmi_ipi(regs))
+		return 1;
+	return 0;
+}
+
+static int pnv_cause_nmi_ipi(int cpu)
+{
+	int64_t rc;
+
+	if (cpu >= 0) {
+		rc = opal_signal_system_reset(get_hard_smp_processor_id(cpu));
+		if (rc != OPAL_SUCCESS)
+			return 0;
+		return 1;
+
+	} else if (cpu == NMI_IPI_ALL_OTHERS) {
+		bool success = true;
+		int c;
+
+
+		/*
+		 * We do not use broadcasts (yet), because it's not clear
+		 * exactly what semantics Linux wants or the firmware should
+		 * provide.
+		 */
+		for_each_online_cpu(c) {
+			if (c == smp_processor_id())
+				continue;
+
+			rc = opal_signal_system_reset(
+						get_hard_smp_processor_id(c));
+			if (rc != OPAL_SUCCESS)
+				success = false;
+		}
+		if (success)
+			return 1;
+
+		/*
+		 * Caller will fall back to doorbells, which may pick
+		 * up the remainders.
+		 */
+	}
+
+	return 0;
+}
+
 static struct smp_ops_t pnv_smp_ops = {
 	.message_pass	= NULL, /* Use smp_muxed_ipi_message_pass */
 	.cause_ipi	= NULL,	/* Filled at runtime by pnv_smp_probe() */
@@ -315,6 +363,10 @@ static struct smp_ops_t pnv_smp_ops = {
 /* This is called very early during platform setup_arch */
 void __init pnv_smp_init(void)
 {
+	if (opal_check_token(OPAL_SIGNAL_SYSTEM_RESET)) {
+		ppc_md.system_reset_exception = pnv_system_reset_exception;
+		pnv_smp_ops.cause_nmi_ipi = pnv_cause_nmi_ipi;
+	}
 	smp_ops = &pnv_smp_ops;
 
 #ifdef CONFIG_HOTPLUG_CPU

commit 5080332c2c893118dbc18755f35c8b0131cf0fc4
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Sep 15 15:25:48 2017 +1000

    powerpc/64s: Add workaround for P9 vector CI load issue
    
    POWER9 DD2.1 and earlier has an issue where some cache inhibited
    vector load will return bad data. The workaround is two part, one
    firmware/microcode part triggers HMI interrupts when hitting such
    loads, the other part is this patch which then emulates the
    instructions in Linux.
    
    The affected instructions are limited to lxvd2x, lxvw4x, lxvb16x and
    lxvh8x.
    
    When an instruction triggers the HMI, all threads in the core will be
    sent to the HMI handler, not just the one running the vector load.
    
    In general, these spurious HMIs are detected by the emulation code and
    we just return back to the running process. Unfortunately, if a
    spurious interrupt occurs on a vector load that's to normal memory we
    have no way to detect that it's spurious (unless we walk the page
    tables, which is very expensive). In this case we emulate the load but
    we need do so using a vector load itself to ensure 128bit atomicity is
    preserved.
    
    Some additional debugfs emulated instruction counters are added also.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [mpe: Switch CONFIG_PPC_BOOK3S_64 to CONFIG_VSX to unbreak the build]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index c17f81e433f7..355d3f99cafb 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -49,6 +49,13 @@
 
 static void pnv_smp_setup_cpu(int cpu)
 {
+	/*
+	 * P9 workaround for CI vector load (see traps.c),
+	 * enable the corresponding HMI interrupt
+	 */
+	if (pvr_version_is(PVR_POWER9))
+		mtspr(SPRN_HMEER, mfspr(SPRN_HMEER) | PPC_BIT(17));
+
 	if (xive_enabled())
 		xive_smp_setup_cpu();
 	else if (cpu != boot_cpuid)

commit 24be85a23d1fcdc72264a062a2e4ebaaea48feab
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Fri Jul 21 16:31:34 2017 +0530

    powerpc/powernv: Clear PECE1 in LPCR via stop-api only on Hotplug
    
    Currently we use the stop-api provided by the firmware to program the
    SLW engine to restore the values of hypervisor resources that get lost
    on deeper idle states (such as winkle). Since the deep states were
    only used for CPU-Hotplug on POWER8 systems, we would program the LPCR
    to have the PECE1 bit since Hotplugged CPUs shouldn't be spuriously
    woken up by decrementer.
    
    On POWER9, some of the deep platform idle states such as stop4 can be
    used in cpuidle as well. In this case, we want the CPU in stop4 to be
    woken up by the decrementer when some timer on the CPU expires.
    
    In this patch, we program the stop-api for LPCR with PECE1
    bit cleared only when we are offlining the CPU and set it
    back once the CPU is online.
    
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index c6448c7aabe7..c17f81e433f7 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -165,12 +165,6 @@ static void pnv_smp_cpu_kill_self(void)
 	if (cpu_has_feature(CPU_FTR_ARCH_207S))
 		wmask = SRR1_WAKEMASK_P8;
 
-	/* We don't want to take decrementer interrupts while we are offline,
-	 * so clear LPCR:PECE1. We keep PECE2 (and LPCR_PECE_HVEE on P9)
-	 * enabled as to let IPIs in.
-	 */
-	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
-
 	while (!generic_check_cpu_restart(cpu)) {
 		/*
 		 * Clear IPI flag, since we don't handle IPIs while
@@ -220,8 +214,6 @@ static void pnv_smp_cpu_kill_self(void)
 
 	}
 
-	/* Re-enable decrementer interrupts */
-	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) | LPCR_PECE1);
 	DBG("CPU%d coming online...\n", cpu);
 }
 

commit 76d98ab4628bf8d36498e8228a5b92aa95a4b820
Author: Santosh Sivaraj <santosh@fossix.org>
Date:   Tue Jul 4 09:52:46 2017 +0530

    powerpc/powernv: Get cpu only after validity check
    
    Check for validity of cpu before calling get_hard_smp_processor_id().
    
    Found with coverity.
    
    Signed-off-by: Santosh Sivaraj <santosh@fossix.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 40dae96f7e20..c6448c7aabe7 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -57,7 +57,7 @@ static void pnv_smp_setup_cpu(int cpu)
 
 static int pnv_smp_kick_cpu(int nr)
 {
-	unsigned int pcpu = get_hard_smp_processor_id(nr);
+	unsigned int pcpu;
 	unsigned long start_here =
 			__pa(ppc_function_entry(generic_secondary_smp_init));
 	long rc;
@@ -66,6 +66,7 @@ static int pnv_smp_kick_cpu(int nr)
 	if (nr < 0 || nr >= nr_cpu_ids)
 		return -EINVAL;
 
+	pcpu = get_hard_smp_processor_id(nr);
 	/*
 	 * If we already started or OPAL is not supported, we just
 	 * kick the CPU via the PACA

commit c642af9c41f09296997519499d16ff30e700816a
Author: Santosh Sivaraj <santosh@fossix.org>
Date:   Tue Jun 27 12:30:06 2017 +0530

    powerpc/smp: Convert NR_CPUS to nr_cpu_ids
    
    nr_cpu_ids can be limited by nr_cpus boot parameter, whereas NR_CPUS is a
    compile time constant, which shouldn't be compared against during cpu kick.
    
    Signed-off-by: Santosh Sivaraj <santosh@fossix.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 292825f25ffd..40dae96f7e20 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -63,7 +63,7 @@ static int pnv_smp_kick_cpu(int nr)
 	long rc;
 	uint8_t status;
 
-	if (nr < 0 || nr >= NR_CPUS)
+	if (nr < 0 || nr >= nr_cpu_ids)
 		return -EINVAL;
 
 	/*

commit f8d0d5dc641cd405ad40cb2498b04df9716baee6
Author: Santosh Sivaraj <santosh@fossix.org>
Date:   Tue Jun 27 12:30:05 2017 +0530

    powerpc/smp: Do not BUG_ON if invalid CPU during kick
    
    During secondary start, we do not need to BUG_ON if an invalid CPU number
    is passed. We already print an error if secondary cannot be started, so
    just return an error instead.
    
    Signed-off-by: Santosh Sivaraj <santosh@fossix.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index c04c87adad94..292825f25ffd 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -63,7 +63,8 @@ static int pnv_smp_kick_cpu(int nr)
 	long rc;
 	uint8_t status;
 
-	BUG_ON(nr < 0 || nr >= NR_CPUS);
+	if (nr < 0 || nr >= NR_CPUS)
+		return -EINVAL;
 
 	/*
 	 * If we already started or OPAL is not supported, we just

commit 2525db04d1cc53e1951143d1829aa75a78cc7f76
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Jun 13 23:05:46 2017 +1000

    powerpc/powernv: Simplify lazy IRQ handling in CPU offline
    
    Rather than concern ourselves with any soft-mask logic in the CPU
    hotplug handler, just hard disable interrupts. This ensures there
    are no lazy-irqs pending, which means we can call directly to idle
    instruction in order to sleep.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index f8752795decf..c04c87adad94 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -144,7 +144,14 @@ static void pnv_smp_cpu_kill_self(void)
 	unsigned long srr1, wmask;
 
 	/* Standard hot unplug procedure */
-	local_irq_disable();
+	/*
+	 * This hard disables local interurpts, ensuring we have no lazy
+	 * irqs pending.
+	 */
+	WARN_ON(irqs_disabled());
+	hard_irq_disable();
+	WARN_ON(lazy_irq_pending());
+
 	idle_task_exit();
 	current->active_mm = NULL; /* for sanity */
 	cpu = smp_processor_id();
@@ -162,16 +169,6 @@ static void pnv_smp_cpu_kill_self(void)
 	 */
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
 
-	/*
-	 * Hard-disable interrupts, and then clear irq_happened flags
-	 * that we can safely ignore while off-line, since they
-	 * are for things for which we do no processing when off-line
-	 * (or in the case of HMI, all the processing we need to do
-	 * is done in lower-level real-mode code).
-	 */
-	hard_irq_disable();
-	local_paca->irq_happened &= ~(PACA_IRQ_DEC | PACA_IRQ_HMI);
-
 	while (!generic_check_cpu_restart(cpu)) {
 		/*
 		 * Clear IPI flag, since we don't handle IPIs while
@@ -184,6 +181,8 @@ static void pnv_smp_cpu_kill_self(void)
 
 		srr1 = pnv_cpu_offline(cpu);
 
+		WARN_ON(lazy_irq_pending());
+
 		/*
 		 * If the SRR1 value indicates that we woke up due to
 		 * an external interrupt, then clear the interrupt.
@@ -196,8 +195,7 @@ static void pnv_smp_cpu_kill_self(void)
 		 * contains 0.
 		 */
 		if (((srr1 & wmask) == SRR1_WAKEEE) ||
-		    ((srr1 & wmask) == SRR1_WAKEHVI) ||
-		    (local_paca->irq_happened & PACA_IRQ_EE)) {
+		    ((srr1 & wmask) == SRR1_WAKEHVI)) {
 			if (cpu_has_feature(CPU_FTR_ARCH_300)) {
 				if (xive_enabled())
 					xive_flush_interrupt();
@@ -209,14 +207,15 @@ static void pnv_smp_cpu_kill_self(void)
 			unsigned long msg = PPC_DBELL_TYPE(PPC_DBELL_SERVER);
 			asm volatile(PPC_MSGCLR(%0) : : "r" (msg));
 		}
-		local_paca->irq_happened &= ~(PACA_IRQ_EE | PACA_IRQ_DBELL);
 		smp_mb();
 
 		if (cpu_core_split_required())
 			continue;
 
 		if (srr1 && !generic_check_cpu_restart(cpu))
-			DBG("CPU%d Unexpected exit while offline !\n", cpu);
+			DBG("CPU%d Unexpected exit while offline srr1=%lx!\n",
+					cpu, srr1);
+
 	}
 
 	/* Re-enable decrementer interrupts */

commit 2201f994a5742c03e660623c385fd6897dd1fa2f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Jun 13 23:05:45 2017 +1000

    powerpc/64s/idle: Move soft interrupt mask logic into C code
    
    This simplifies the asm and fixes irq-off tracing over sleep
    instructions.
    
    Also move powersave_nap check for POWER8 into C code, and move
    PSSCR register value calculation for POWER9 into C.
    
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 4aff754b6f2c..f8752795decf 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -182,9 +182,7 @@ static void pnv_smp_cpu_kill_self(void)
 		 */
 		kvmppc_set_host_ipi(cpu, 0);
 
-		ppc64_runlatch_off();
 		srr1 = pnv_cpu_offline(cpu);
-		ppc64_runlatch_on();
 
 		/*
 		 * If the SRR1 value indicates that we woke up due to

commit c64af6458e2e2ddf86aff559837d3925fbf9cbb5
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Dec 20 04:30:09 2016 +1000

    powerpc: Add struct smp_ops_t.cause_nmi_ipi operation
    
    Have the NMI IPI code use this op when the platform defines it.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 5189445db164..4aff754b6f2c 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -302,6 +302,7 @@ static void __init pnv_smp_probe(void)
 static struct smp_ops_t pnv_smp_ops = {
 	.message_pass	= NULL, /* Use smp_muxed_ipi_message_pass */
 	.cause_ipi	= NULL,	/* Filled at runtime by pnv_smp_probe() */
+	.cause_nmi_ipi	= NULL,
 	.probe		= pnv_smp_probe,
 	.prepare_cpu	= pnv_smp_prepare_cpu,
 	.kick_cpu	= pnv_smp_kick_cpu,

commit 45b21cfeb22087795f0b49397fbe529efeb99baf
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Apr 26 17:32:38 2017 +1000

    powerpc/powernv: Fix oops on P9 DD1 in cause_ipi()
    
    Recently we merged the native xive support for Power9, and then separately some
    reworks for doorbell IPI support. In isolation both series were OK, but the
    merged result had a bug in one case.
    
    On P9 DD1 we use pnv_p9_dd1_cause_ipi() which tries to use doorbells, and then
    falls back to the interrupt controller. However the fallback is implemented by
    calling icp_ops->cause_ipi. But now that xive support is merged we might be
    using xive, in which case icp_ops is not initialised, it's a xics specific
    structure. This leads to an oops such as:
    
      Unable to handle kernel paging request for data at address 0x00000028
      Oops: Kernel access of bad area, sig: 11 [#1]
      NIP pnv_p9_dd1_cause_ipi+0x74/0xe0
      LR smp_muxed_ipi_message_pass+0x54/0x70
    
    To fix it, rather than using icp_ops which might be NULL, have both xics and
    xive set smp_ops->cause_ipi, and then in the powernv code we save that as
    ic_cause_ipi before overriding smp_ops->cause_ipi. For paranoia add a WARN_ON()
    to check if somehow smp_ops->cause_ipi is NULL.
    
    Fixes: b866cc2199d6 ("powerpc: Change the doorbell IPI calling convention")
    Tested-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 951a2e230cfa..5189445db164 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -249,12 +249,15 @@ static int pnv_smp_prepare_cpu(int cpu)
 	return 0;
 }
 
+/* Cause IPI as setup by the interrupt controller (xics or xive) */
+static void (*ic_cause_ipi)(int cpu);
+
 static void pnv_cause_ipi(int cpu)
 {
 	if (doorbell_try_core_ipi(cpu))
 		return;
 
-	icp_ops->cause_ipi(cpu);
+	ic_cause_ipi(cpu);
 }
 
 static void pnv_p9_dd1_cause_ipi(int cpu)
@@ -269,7 +272,7 @@ static void pnv_p9_dd1_cause_ipi(int cpu)
 	if (cpumask_test_cpu(cpu, cpu_sibling_mask(this_cpu)))
 		doorbell_global_ipi(cpu);
 	else
-		icp_ops->cause_ipi(cpu);
+		ic_cause_ipi(cpu);
 
 	put_cpu();
 }
@@ -282,6 +285,9 @@ static void __init pnv_smp_probe(void)
 		xics_smp_probe();
 
 	if (cpu_has_feature(CPU_FTR_DBELL)) {
+		ic_cause_ipi = smp_ops->cause_ipi;
+		WARN_ON(!ic_cause_ipi);
+
 		if (cpu_has_feature(CPU_FTR_ARCH_300)) {
 			if (cpu_has_feature(CPU_FTR_POWER9_DD1))
 				smp_ops->cause_ipi = pnv_p9_dd1_cause_ipi;
@@ -290,8 +296,6 @@ static void __init pnv_smp_probe(void)
 		} else {
 			smp_ops->cause_ipi = pnv_cause_ipi;
 		}
-	} else {
-		smp_ops->cause_ipi = icp_ops->cause_ipi;
 	}
 }
 

commit 6b3edefefa6752df57ad636f26baa1b0a502ddab
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Apr 13 20:16:24 2017 +1000

    powerpc/powernv: POWER9 support for msgsnd/doorbell IPI
    
    POWER9 requires msgsync for receiver-side synchronization, and a DD1
    workaround restricts IPIs to core-local.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Drop no longer needed asm feature macro changes]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 33bd4dd2cb41..951a2e230cfa 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -257,6 +257,23 @@ static void pnv_cause_ipi(int cpu)
 	icp_ops->cause_ipi(cpu);
 }
 
+static void pnv_p9_dd1_cause_ipi(int cpu)
+{
+	int this_cpu = get_cpu();
+
+	/*
+	 * POWER9 DD1 has a global addressed msgsnd, but for now we restrict
+	 * IPIs to same core, because it requires additional synchronization
+	 * for inter-core doorbells which we do not implement.
+	 */
+	if (cpumask_test_cpu(cpu, cpu_sibling_mask(this_cpu)))
+		doorbell_global_ipi(cpu);
+	else
+		icp_ops->cause_ipi(cpu);
+
+	put_cpu();
+}
+
 static void __init pnv_smp_probe(void)
 {
 	if (xive_enabled())
@@ -264,8 +281,15 @@ static void __init pnv_smp_probe(void)
 	else
 		xics_smp_probe();
 
-	if (cpu_has_feature(CPU_FTR_DBELL) && !cpu_has_feature(CPU_FTR_ARCH_300)) {
-		smp_ops->cause_ipi = pnv_cause_ipi;
+	if (cpu_has_feature(CPU_FTR_DBELL)) {
+		if (cpu_has_feature(CPU_FTR_ARCH_300)) {
+			if (cpu_has_feature(CPU_FTR_POWER9_DD1))
+				smp_ops->cause_ipi = pnv_p9_dd1_cause_ipi;
+			else
+				smp_ops->cause_ipi = doorbell_global_ipi;
+		} else {
+			smp_ops->cause_ipi = pnv_cause_ipi;
+		}
 	} else {
 		smp_ops->cause_ipi = icp_ops->cause_ipi;
 	}

commit b866cc2199d6a6cdcefe4acfe4cfca3ac3c6d38e
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Apr 13 20:16:21 2017 +1000

    powerpc: Change the doorbell IPI calling convention
    
    Change the doorbell callers to know about their msgsnd addressing,
    rather than have them set a per-cpu target data tag at boot that gets
    sent to the cause_ipi functions. The data is only used for doorbell IPI
    functions, no other IPI types, so it makes sense to keep that detail
    local to doorbell.
    
    Have the platform code understand doorbell IPIs, rather than the
    interrupt controller code understand them. Platform code can look at
    capabilities it has available and decide which to use.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 39296bf7009e..33bd4dd2cb41 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -53,11 +53,6 @@ static void pnv_smp_setup_cpu(int cpu)
 		xive_smp_setup_cpu();
 	else if (cpu != boot_cpuid)
 		xics_setup_cpu();
-
-#ifdef CONFIG_PPC_DOORBELL
-	if (cpu_has_feature(CPU_FTR_DBELL))
-		doorbell_setup_this_cpu();
-#endif
 }
 
 static int pnv_smp_kick_cpu(int nr)
@@ -254,17 +249,31 @@ static int pnv_smp_prepare_cpu(int cpu)
 	return 0;
 }
 
+static void pnv_cause_ipi(int cpu)
+{
+	if (doorbell_try_core_ipi(cpu))
+		return;
+
+	icp_ops->cause_ipi(cpu);
+}
+
 static void __init pnv_smp_probe(void)
 {
 	if (xive_enabled())
 		xive_smp_probe();
 	else
 		xics_smp_probe();
+
+	if (cpu_has_feature(CPU_FTR_DBELL) && !cpu_has_feature(CPU_FTR_ARCH_300)) {
+		smp_ops->cause_ipi = pnv_cause_ipi;
+	} else {
+		smp_ops->cause_ipi = icp_ops->cause_ipi;
+	}
 }
 
 static struct smp_ops_t pnv_smp_ops = {
-	.message_pass	= smp_muxed_ipi_message_pass,
-	.cause_ipi	= NULL, /* Filled at runtime by xi{cs,ve}_smp_probe() */
+	.message_pass	= NULL, /* Use smp_muxed_ipi_message_pass */
+	.cause_ipi	= NULL,	/* Filled at runtime by pnv_smp_probe() */
 	.probe		= pnv_smp_probe,
 	.prepare_cpu	= pnv_smp_prepare_cpu,
 	.kick_cpu	= pnv_smp_kick_cpu,

commit 3c19d5ada1bec8b97119215298df7669d3ffb3db
Merge: 17ed4c8f81da 08a1e650cc63
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Apr 12 22:25:02 2017 +1000

    Merge branch 'topic/xive' (early part) into next
    
    This merges the arch part of the XIVE support, leaving the final commit
    with the KVM specific pieces dangling on the branch for Paul to merge
    via the kvm-ppc tree.

commit a7cd88da97040513e17cd77ae3e57764e854bae4
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Wed Mar 22 20:34:14 2017 +0530

    powerpc/powernv: Move CPU-Offline idle state invocation from smp.c to idle.c
    
    Move the piece of code in powernv/smp.c::pnv_smp_cpu_kill_self() which
    transitions the CPU to the deepest available platform idle state to a
    new function named pnv_cpu_offline() in powernv/idle.c. The rationale
    behind this code movement is that the data required to determine the
    deepest available platform state resides in powernv/idle.c.
    
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 8b67e1eefb5c..914b456ef551 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -35,6 +35,7 @@
 #include <asm/dbell.h>
 #include <asm/kvm_ppc.h>
 #include <asm/ppc-opcode.h>
+#include <asm/cpuidle.h>
 
 #include "powernv.h"
 
@@ -140,7 +141,6 @@ static void pnv_smp_cpu_kill_self(void)
 {
 	unsigned int cpu;
 	unsigned long srr1, wmask;
-	u32 idle_states;
 
 	/* Standard hot unplug procedure */
 	local_irq_disable();
@@ -155,8 +155,6 @@ static void pnv_smp_cpu_kill_self(void)
 	if (cpu_has_feature(CPU_FTR_ARCH_207S))
 		wmask = SRR1_WAKEMASK_P8;
 
-	idle_states = pnv_get_supported_cpuidle_states();
-
 	/* We don't want to take decrementer interrupts while we are offline,
 	 * so clear LPCR:PECE1. We keep PECE2 (and LPCR_PECE_HVEE on P9)
 	 * enabled as to let IPIs in.
@@ -184,19 +182,7 @@ static void pnv_smp_cpu_kill_self(void)
 		kvmppc_set_host_ipi(cpu, 0);
 
 		ppc64_runlatch_off();
-
-		if (cpu_has_feature(CPU_FTR_ARCH_300)) {
-			srr1 = power9_idle_stop(pnv_deepest_stop_psscr_val,
-						pnv_deepest_stop_psscr_mask);
-		} else if (idle_states & OPAL_PM_WINKLE_ENABLED) {
-			srr1 = power7_winkle();
-		} else if ((idle_states & OPAL_PM_SLEEP_ENABLED) ||
-			   (idle_states & OPAL_PM_SLEEP_ENABLED_ER1)) {
-			srr1 = power7_sleep();
-		} else {
-			srr1 = power7_nap(1);
-		}
-
+		srr1 = pnv_cpu_offline(cpu);
 		ppc64_runlatch_on();
 
 		/*

commit 243e25112d06b348f087a6f7aba4bbc288285bdd
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:50 2017 +1000

    powerpc/xive: Native exploitation of the XIVE interrupt controller
    
    The XIVE interrupt controller is the new interrupt controller
    found in POWER9. It supports advanced virtualization capabilities
    among other things.
    
    Currently we use a set of firmware calls that simulate the old
    "XICS" interrupt controller but this is fairly inefficient.
    
    This adds the framework for using XIVE along with a native
    backend which OPAL for configuration. Later, a backend allowing
    the use in a KVM or PowerVM guest will also be provided.
    
    This disables some fast path for interrupts in KVM when XIVE is
    enabled as these rely on the firmware emulation code which is no
    longer available when the XIVE is used natively by Linux.
    
    A latter patch will make KVM also directly exploit the XIVE, thus
    recovering the lost performance (and more).
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [mpe: Fixup pr_xxx("XIVE:"...), don't split pr_xxx() strings,
     tweak Kconfig so XIVE_NATIVE selects XIVE and depends on POWERNV,
     fix build errors when SMP=n, fold in fixes from Ben:
       Don't call cpu_online() on an invalid CPU number
       Fix irq target selection returning out of bounds cpu#
       Extra sanity checks on cpu numbers
     ]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 8b67e1eefb5c..f57195588c6c 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -29,6 +29,7 @@
 #include <asm/vdso_datapage.h>
 #include <asm/cputhreads.h>
 #include <asm/xics.h>
+#include <asm/xive.h>
 #include <asm/opal.h>
 #include <asm/runlatch.h>
 #include <asm/code-patching.h>
@@ -47,7 +48,9 @@
 
 static void pnv_smp_setup_cpu(int cpu)
 {
-	if (cpu != boot_cpuid)
+	if (xive_enabled())
+		xive_smp_setup_cpu();
+	else if (cpu != boot_cpuid)
 		xics_setup_cpu();
 
 #ifdef CONFIG_PPC_DOORBELL
@@ -132,7 +135,10 @@ static int pnv_smp_cpu_disable(void)
 	vdso_data->processorCount--;
 	if (cpu == boot_cpuid)
 		boot_cpuid = cpumask_any(cpu_online_mask);
-	xics_migrate_irqs_away();
+	if (xive_enabled())
+		xive_smp_disable_cpu();
+	else
+		xics_migrate_irqs_away();
 	return 0;
 }
 
@@ -213,9 +219,12 @@ static void pnv_smp_cpu_kill_self(void)
 		if (((srr1 & wmask) == SRR1_WAKEEE) ||
 		    ((srr1 & wmask) == SRR1_WAKEHVI) ||
 		    (local_paca->irq_happened & PACA_IRQ_EE)) {
-			if (cpu_has_feature(CPU_FTR_ARCH_300))
-				icp_opal_flush_interrupt();
-			else
+			if (cpu_has_feature(CPU_FTR_ARCH_300)) {
+				if (xive_enabled())
+					xive_flush_interrupt();
+				else
+					icp_opal_flush_interrupt();
+			} else
 				icp_native_flush_interrupt();
 		} else if ((srr1 & wmask) == SRR1_WAKEHDBELL) {
 			unsigned long msg = PPC_DBELL_TYPE(PPC_DBELL_SERVER);
@@ -252,10 +261,26 @@ static int pnv_cpu_bootable(unsigned int nr)
 	return smp_generic_cpu_bootable(nr);
 }
 
+static int pnv_smp_prepare_cpu(int cpu)
+{
+	if (xive_enabled())
+		return xive_smp_prepare_cpu(cpu);
+	return 0;
+}
+
+static void __init pnv_smp_probe(void)
+{
+	if (xive_enabled())
+		xive_smp_probe();
+	else
+		xics_smp_probe();
+}
+
 static struct smp_ops_t pnv_smp_ops = {
 	.message_pass	= smp_muxed_ipi_message_pass,
-	.cause_ipi	= NULL,	/* Filled at runtime by xics_smp_probe() */
-	.probe		= xics_smp_probe,
+	.cause_ipi	= NULL, /* Filled at runtime by xi{cs,ve}_smp_probe() */
+	.probe		= pnv_smp_probe,
+	.prepare_cpu	= pnv_smp_prepare_cpu,
 	.kick_cpu	= pnv_smp_kick_cpu,
 	.setup_cpu	= pnv_smp_setup_cpu,
 	.cpu_bootable	= pnv_cpu_bootable,

commit ef8bd77f332bb0a4e467d7171bbfc6c57aa08a88
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/hotplug.h>
    
    We are going to split <linux/sched/hotplug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/hotplug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index e39e6c428af1..8b67e1eefb5c 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -12,6 +12,7 @@
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/sched.h>
+#include <linux/sched/hotplug.h>
 #include <linux/smp.h>
 #include <linux/interrupt.h>
 #include <linux/delay.h>

commit 38705613b74ab090eee55c327cd0cb77fb10eb26
Merge: ff47d8c05019 438e69b52be7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 10:30:38 2017 -0800

    Merge tag 'powerpc-4.11-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for direct mapped LPC on POWER9, giving Linux direct access
         to devices that may be on there such as a UART.
    
       - Memory hotplug support for the Power9 Radix MMU.
    
       - Add new AUX vectors describing the processor's cache geometry, to
         be used by glibc.
    
       - The ability for a guest to ask the hypervisor to resize the guest's
         hash table, and in addition support for doing so automatically when
         memory is hotplugged into/out-of the guest. This allows the hash
         table to be sized based on the current memory usage of the guest,
         rather than the maximum possible memory usage.
    
       - Implementation of optprobes (kprobe optimisation) for powerpc.
    
      In addition there's the topic branch shared with the KVM tree, which
      includes support for guests to use the Radix MMU on Power9.
    
      Thanks to:
        Alistair Popple, Andrew Donnellan, Aneesh Kumar K.V, Anju T, Anton
        Blanchard, Benjamin Herrenschmidt, Chris Packham, Daniel Axtens,
        Daniel Borkmann, David Gibson, Finn Thain, Gautham R. Shenoy, Gavin
        Shan, Greg Kurz, Joel Stanley, John Allen, Madhavan Srinivasan,
        Mahesh Salgaonkar, Markus Elfring, Michael Neuling, Nathan Fontenot,
        Naveen N. Rao, Nicholas Piggin, Paul Mackerras, Ravi Bangoria, Reza
        Arbab, Shailendra Singh, Vaibhav Jain, Wei Yongjun"
    
    * tag 'powerpc-4.11-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (129 commits)
      powerpc/mm/radix: Skip ptesync in pte update helpers
      powerpc/mm/radix: Use ptep_get_and_clear_full when clearing pte for full mm
      powerpc/mm/radix: Update pte update sequence for pte clear case
      powerpc/mm: Update PROTFAULT handling in the page fault path
      powerpc/xmon: Fix data-breakpoint
      powerpc/mm: Fix build break with BOOK3S_64=n and MEMORY_HOTPLUG=y
      powerpc/mm: Fix build break when CMA=n && SPAPR_TCE_IOMMU=y
      powerpc/mm: Fix build break with RADIX=y & HUGETLBFS=n
      powerpc/pseries: Fix typo in parameter description
      powerpc/kprobes: Remove kprobe_exceptions_notify()
      kprobes: Introduce weak variant of kprobe_exceptions_notify()
      powerpc/ftrace: Fix confusing help text for DISABLE_MPROFILE_KERNEL
      powerpc/powernv: Fix opal_exit tracepoint opcode
      powerpc: Add a prototype for mcount() so it can be versioned
      powerpc: Drop GPL from of_node_to_nid() export to match other arches
      powerpc/kprobes: Optimize kprobe in kretprobe_trampoline()
      powerpc/kprobes: Implement Optprobes
      powerpc/kprobes: Fixes for kprobe_lookup_name() on BE
      powerpc: Add helper to check if offset is within relative branch range
      powerpc/bpf: Introduce __PPC_SH64()
      ...

commit 9b256714979fad61ae11d90b53cf67dd5e6484eb
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Feb 7 11:35:31 2017 +1100

    powerpc/powernv: Fix CPU hotplug to handle waking on HVI
    
    The IPIs come in as HVI not EE, so we need to test the appropriate
    SRR1 bits. The encoding is such that it won't have false positives
    on P7 and P8 so we can just test it like that. We also need to handle
    the icp-opal variant of the flush.
    
    Fixes: d74361881f0d ("powerpc/xics: Add ICP OPAL backend")
    Cc: stable@vger.kernel.org # v4.8+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index c789258ae1e1..eec0e8d0454d 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -155,8 +155,10 @@ static void pnv_smp_cpu_kill_self(void)
 		wmask = SRR1_WAKEMASK_P8;
 
 	idle_states = pnv_get_supported_cpuidle_states();
+
 	/* We don't want to take decrementer interrupts while we are offline,
-	 * so clear LPCR:PECE1. We keep PECE2 enabled.
+	 * so clear LPCR:PECE1. We keep PECE2 (and LPCR_PECE_HVEE on P9)
+	 * enabled as to let IPIs in.
 	 */
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
 
@@ -206,8 +208,12 @@ static void pnv_smp_cpu_kill_self(void)
 		 * contains 0.
 		 */
 		if (((srr1 & wmask) == SRR1_WAKEEE) ||
+		    ((srr1 & wmask) == SRR1_WAKEHVI) ||
 		    (local_paca->irq_happened & PACA_IRQ_EE)) {
-			icp_native_flush_interrupt();
+			if (cpu_has_feature(CPU_FTR_ARCH_300))
+				icp_opal_flush_interrupt();
+			else
+				icp_native_flush_interrupt();
 		} else if ((srr1 & wmask) == SRR1_WAKEHDBELL) {
 			unsigned long msg = PPC_DBELL_TYPE(PPC_DBELL_SERVER);
 			asm volatile(PPC_MSGCLR(%0) : : "r" (msg));
@@ -221,6 +227,8 @@ static void pnv_smp_cpu_kill_self(void)
 		if (srr1 && !generic_check_cpu_restart(cpu))
 			DBG("CPU%d Unexpected exit while offline !\n", cpu);
 	}
+
+	/* Re-enable decrementer interrupts */
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) | LPCR_PECE1);
 	DBG("CPU%d coming online...\n", cpu);
 }

commit 09206b600c76f20984e80d99f3b5343c79332a97
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Wed Jan 25 14:06:28 2017 +0530

    powernv: Pass PSSCR value and mask to power9_idle_stop
    
    The power9_idle_stop method currently takes only the requested stop
    level as a parameter and picks up the rest of the PSSCR bits from a
    hand-coded macro. This is not a very flexible design, especially when
    the firmware has the capability to communicate the psscr value and the
    mask associated with a particular stop state via device tree.
    
    This patch modifies the power9_idle_stop API to take as parameters the
    PSSCR value and the PSSCR mask corresponding to the stop state that
    needs to be set. These PSSCR value and mask are respectively obtained
    by parsing the "ibm,cpu-idle-state-psscr" and
    "ibm,cpu-idle-state-psscr-mask" fields from the device tree.
    
    In addition to this, the patch adds support for handling stop states
    for which ESL and EC bits in the PSSCR are zero. As per the
    architecture, a wakeup from these stop states resumes execution from
    the subsequent instruction as opposed to waking up at the System
    Vector.
    
    The older firmware sets only the Requested Level (RL) field in the
    psscr and psscr-mask exposed in the device tree. For older firmware
    where psscr-mask=0xf, this patch will set the default sane values that
    the set for for remaining PSSCR fields (i.e PSLL, MTL, ESL, EC, and
    TR). For the new firmware, the patch will validate that the invariants
    required by the ISA for the psscr values are maintained by the
    firmware.
    
    This skiboot patch that exports fully populated PSSCR values and the
    mask for all the stop states can be found here:
    https://lists.ozlabs.org/pipermail/skiboot/2016-September/004869.html
    
    [Optimize the number of instructions before entering STOP with
    ESL=EC=0, validate the PSSCR values provided by the firimware
    maintains the invariants required as per the ISA suggested by Balbir
    Singh]
    
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index c789258ae1e1..1c6405fb769a 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -182,15 +182,17 @@ static void pnv_smp_cpu_kill_self(void)
 
 		ppc64_runlatch_off();
 
-		if (cpu_has_feature(CPU_FTR_ARCH_300))
-			srr1 = power9_idle_stop(pnv_deepest_stop_state);
-		else if (idle_states & OPAL_PM_WINKLE_ENABLED)
+		if (cpu_has_feature(CPU_FTR_ARCH_300)) {
+			srr1 = power9_idle_stop(pnv_deepest_stop_psscr_val,
+						pnv_deepest_stop_psscr_mask);
+		} else if (idle_states & OPAL_PM_WINKLE_ENABLED) {
 			srr1 = power7_winkle();
-		else if ((idle_states & OPAL_PM_SLEEP_ENABLED) ||
-				(idle_states & OPAL_PM_SLEEP_ENABLED_ER1))
+		} else if ((idle_states & OPAL_PM_SLEEP_ENABLED) ||
+			   (idle_states & OPAL_PM_SLEEP_ENABLED_ER1)) {
 			srr1 = power7_sleep();
-		else
+		} else {
 			srr1 = power7_nap(1);
+		}
 
 		ppc64_runlatch_on();
 

commit c0691f9dd2066087524d2b4498c0c9331f26dcd5
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Fri Jul 8 11:50:53 2016 +0530

    powerpc/powernv: Use deepest stop state when cpu is offlined
    
    If hardware supports stop state, use the deepest stop state when
    the cpu is offlined.
    
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index ad7b1a3dbed0..c789258ae1e1 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -182,7 +182,9 @@ static void pnv_smp_cpu_kill_self(void)
 
 		ppc64_runlatch_off();
 
-		if (idle_states & OPAL_PM_WINKLE_ENABLED)
+		if (cpu_has_feature(CPU_FTR_ARCH_300))
+			srr1 = power9_idle_stop(pnv_deepest_stop_state);
+		else if (idle_states & OPAL_PM_WINKLE_ENABLED)
 			srr1 = power7_winkle();
 		else if ((idle_states & OPAL_PM_SLEEP_ENABLED) ||
 				(idle_states & OPAL_PM_SLEEP_ENABLED_ER1))

commit e4d54f71d29997344b4c4c8d47708240f9f23a5c
Author: Stewart Smith <stewart@linux.vnet.ibm.com>
Date:   Wed Dec 9 17:18:20 2015 +1100

    powerpc/powernv: remove FW_FEATURE_OPALv3 and just use FW_FEATURE_OPAL
    
    Long ago, only in the lab, there was OPALv1 and OPALv2. Now there is
    just OPALv3, with nobody ever expecting anything on pre-OPALv3 to
    be cared about or supported by mainline kernels.
    
    So, let's remove FW_FEATURE_OPALv3 and instead use FW_FEATURE_OPAL
    exclusively.
    
    Signed-off-by: Stewart Smith <stewart@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 9b968a315103..ad7b1a3dbed0 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -61,14 +61,15 @@ static int pnv_smp_kick_cpu(int nr)
 	unsigned long start_here =
 			__pa(ppc_function_entry(generic_secondary_smp_init));
 	long rc;
+	uint8_t status;
 
 	BUG_ON(nr < 0 || nr >= NR_CPUS);
 
 	/*
-	 * If we already started or OPALv3 is not supported, we just
+	 * If we already started or OPAL is not supported, we just
 	 * kick the CPU via the PACA
 	 */
-	if (paca[nr].cpu_start || !firmware_has_feature(FW_FEATURE_OPALv3))
+	if (paca[nr].cpu_start || !firmware_has_feature(FW_FEATURE_OPAL))
 		goto kick;
 
 	/*
@@ -77,55 +78,42 @@ static int pnv_smp_kick_cpu(int nr)
 	 * first time. OPAL v3 allows us to query OPAL to know if it
 	 * has the CPUs, so we do that
 	 */
-	if (firmware_has_feature(FW_FEATURE_OPALv3)) {
-		uint8_t status;
-
-		rc = opal_query_cpu_status(pcpu, &status);
-		if (rc != OPAL_SUCCESS) {
-			pr_warn("OPAL Error %ld querying CPU %d state\n",
-				rc, nr);
-			return -ENODEV;
-		}
+	rc = opal_query_cpu_status(pcpu, &status);
+	if (rc != OPAL_SUCCESS) {
+		pr_warn("OPAL Error %ld querying CPU %d state\n", rc, nr);
+		return -ENODEV;
+	}
 
-		/*
-		 * Already started, just kick it, probably coming from
-		 * kexec and spinning
-		 */
-		if (status == OPAL_THREAD_STARTED)
-			goto kick;
+	/*
+	 * Already started, just kick it, probably coming from
+	 * kexec and spinning
+	 */
+	if (status == OPAL_THREAD_STARTED)
+		goto kick;
 
-		/*
-		 * Available/inactive, let's kick it
-		 */
-		if (status == OPAL_THREAD_INACTIVE) {
-			pr_devel("OPAL: Starting CPU %d (HW 0x%x)...\n",
-				 nr, pcpu);
-			rc = opal_start_cpu(pcpu, start_here);
-			if (rc != OPAL_SUCCESS) {
-				pr_warn("OPAL Error %ld starting CPU %d\n",
-					rc, nr);
-				return -ENODEV;
-			}
-		} else {
-			/*
-			 * An unavailable CPU (or any other unknown status)
-			 * shouldn't be started. It should also
-			 * not be in the possible map but currently it can
-			 * happen
-			 */
-			pr_devel("OPAL: CPU %d (HW 0x%x) is unavailable"
-				 " (status %d)...\n", nr, pcpu, status);
+	/*
+	 * Available/inactive, let's kick it
+	 */
+	if (status == OPAL_THREAD_INACTIVE) {
+		pr_devel("OPAL: Starting CPU %d (HW 0x%x)...\n", nr, pcpu);
+		rc = opal_start_cpu(pcpu, start_here);
+		if (rc != OPAL_SUCCESS) {
+			pr_warn("OPAL Error %ld starting CPU %d\n", rc, nr);
 			return -ENODEV;
 		}
 	} else {
 		/*
-		 * On OPAL v2, we just kick it and hope for the best,
-		 * we must not test the error from opal_start_cpu() or
-		 * we would fail to get CPUs from kexec.
+		 * An unavailable CPU (or any other unknown status)
+		 * shouldn't be started. It should also
+		 * not be in the possible map but currently it can
+		 * happen
 		 */
-		opal_start_cpu(pcpu, start_here);
+		pr_devel("OPAL: CPU %d (HW 0x%x) is unavailable"
+			 " (status %d)...\n", nr, pcpu, status);
+		return -ENODEV;
 	}
- kick:
+
+kick:
 	return smp_generic_kick_cpu(nr);
 }
 

commit 7261aafc095763b119136a562540dea7b1ccf657
Author: Stewart Smith <stewart@linux.vnet.ibm.com>
Date:   Wed Dec 9 17:18:19 2015 +1100

    powerpc/powernv: Remove OPALv2 firmware define and references
    
    OPALv2 only ever existed in the lab and didn't escape to the world.
    All OPAL systems in the wild are OPALv3.
    
    The probability of there being an OPALv2 system still powered on
    anywhere inside IBM is approximately zero, let alone anyone
    expecting to run mainline kernels.
    
    So, start to remove references to OPALv2.
    
    Signed-off-by: Stewart Smith <stewart@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index ca264833ee64..9b968a315103 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -65,10 +65,10 @@ static int pnv_smp_kick_cpu(int nr)
 	BUG_ON(nr < 0 || nr >= NR_CPUS);
 
 	/*
-	 * If we already started or OPALv2 is not supported, we just
+	 * If we already started or OPALv3 is not supported, we just
 	 * kick the CPU via the PACA
 	 */
-	if (paca[nr].cpu_start || !firmware_has_feature(FW_FEATURE_OPALv2))
+	if (paca[nr].cpu_start || !firmware_has_feature(FW_FEATURE_OPALv3))
 		goto kick;
 
 	/*

commit 53c656c4138511c2ba54df413dc29976cfa9f084
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Oct 21 16:06:24 2015 +1100

    powerpc/powernv: Handle irq_happened flag correctly in off-line loop
    
    This fixes a bug where it is possible for an off-line CPU to fail to go
    into a low-power state (nap/sleep/winkle), and to become unresponsive to
    requests from the KVM subsystem to wake up and run a VCPU. What can
    happen is that a maskable interrupt of some kind (external, decrementer,
    hypervisor doorbell, or HMI) after we have called local_irq_disable() at
    the beginning of pnv_smp_cpu_kill_self() and before interrupts are
    hard-disabled inside power7_nap/sleep/winkle(). In this situation, the
    pending event is marked in the irq_happened flag in the PACA. This
    pending event prevents power7_nap/sleep/winkle from going to the
    requested low-power state; instead they return immediately. We don't
    deal with any of these pending event flags in the off-line loop in
    pnv_smp_cpu_kill_self() because power7_nap et al. return 0 in this case,
    so we will have srr1 == 0, and none of the processing to clear
    interrupts or doorbells will be done.
    
    Usually, the most obvious symptom of this is that a KVM guest will fail
    with a console message saying "KVM: couldn't grab cpu N".
    
    This fixes the problem by making sure we handle the irq_happened flags
    properly. First, we hard-disable before the off-line loop. Once we have
    hard-disabled, the irq_happened flags can't change underneath us. We
    unconditionally clear the DEC and HMI flags: there is no processing of
    timer interrupts while off-line, and the necessary HMI processing is all
    done in lower-level code. We leave the EE and DBELL flags alone for the
    first iteration of the loop, so that we won't fail to respond to a
    split-core request that came in just before hard-disabling. Within the
    loop, we handle external interrupts if the EE bit is set in irq_happened
    as well as if the low-power state was interrupted by an external
    interrupt. (We don't need to do the msgclr for a pending doorbell in
    irq_happened, because doorbells are edge-triggered and don't remain
    pending in hardware.) Then we clear both the EE and DBELL flags, and
    once clear, they cannot be set again (until this CPU comes online again,
    that is).
    
    This also fixes the debug check to not be done when we just ran a KVM
    guest or when the sleep didn't happen because of a pending event in
    irq_happened.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 8f70ba681a78..ca264833ee64 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -171,7 +171,26 @@ static void pnv_smp_cpu_kill_self(void)
 	 * so clear LPCR:PECE1. We keep PECE2 enabled.
 	 */
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
+
+	/*
+	 * Hard-disable interrupts, and then clear irq_happened flags
+	 * that we can safely ignore while off-line, since they
+	 * are for things for which we do no processing when off-line
+	 * (or in the case of HMI, all the processing we need to do
+	 * is done in lower-level real-mode code).
+	 */
+	hard_irq_disable();
+	local_paca->irq_happened &= ~(PACA_IRQ_DEC | PACA_IRQ_HMI);
+
 	while (!generic_check_cpu_restart(cpu)) {
+		/*
+		 * Clear IPI flag, since we don't handle IPIs while
+		 * offline, except for those when changing micro-threading
+		 * mode, which are handled explicitly below, and those
+		 * for coming online, which are handled via
+		 * generic_check_cpu_restart() calls.
+		 */
+		kvmppc_set_host_ipi(cpu, 0);
 
 		ppc64_runlatch_off();
 
@@ -196,20 +215,20 @@ static void pnv_smp_cpu_kill_self(void)
 		 * having finished executing in a KVM guest, then srr1
 		 * contains 0.
 		 */
-		if ((srr1 & wmask) == SRR1_WAKEEE) {
+		if (((srr1 & wmask) == SRR1_WAKEEE) ||
+		    (local_paca->irq_happened & PACA_IRQ_EE)) {
 			icp_native_flush_interrupt();
-			local_paca->irq_happened &= PACA_IRQ_HARD_DIS;
-			smp_mb();
 		} else if ((srr1 & wmask) == SRR1_WAKEHDBELL) {
 			unsigned long msg = PPC_DBELL_TYPE(PPC_DBELL_SERVER);
 			asm volatile(PPC_MSGCLR(%0) : : "r" (msg));
-			kvmppc_set_host_ipi(cpu, 0);
 		}
+		local_paca->irq_happened &= ~(PACA_IRQ_EE | PACA_IRQ_DBELL);
+		smp_mb();
 
 		if (cpu_core_split_required())
 			continue;
 
-		if (!generic_check_cpu_restart(cpu))
+		if (srr1 && !generic_check_cpu_restart(cpu))
 			DBG("CPU%d Unexpected exit while offline !\n", cpu);
 	}
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) | LPCR_PECE1);

commit d19d5efd8c8840aa4f38a6dfbfe500d8cc27de46
Merge: 34c9a0ffc75a 2fe0753d4940
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 16 13:53:32 2015 -0500

    Merge tag 'powerpc-4.1-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux
    
    Pull powerpc updates from Michael Ellerman:
    
     - Numerous minor fixes, cleanups etc.
    
     - More EEH work from Gavin to remove its dependency on device_nodes.
    
     - Memory hotplug implemented entirely in the kernel from Nathan
       Fontenot.
    
     - Removal of redundant CONFIG_PPC_OF by Kevin Hao.
    
     - Rewrite of VPHN parsing logic & tests from Greg Kurz.
    
     - A fix from Nish Aravamudan to reduce memory usage by clamping
       nodes_possible_map.
    
     - Support for pstore on powernv from Hari Bathini.
    
     - Removal of old powerpc specific byte swap routines by David Gibson.
    
     - Fix from Vasant Hegde to prevent the flash driver telling you it was
       flashing your firmware when it wasn't.
    
     - Patch from Ben Herrenschmidt to add an OPAL heartbeat driver.
    
     - Fix for an oops causing get/put_cpu_var() imbalance in perf by Jan
       Stancek.
    
     - Some fixes for migration from Tyrel Datwyler.
    
     - A new syscall to switch the cpu endian by Michael Ellerman.
    
     - Large series from Wei Yang to implement SRIOV, reviewed and acked by
       Bjorn.
    
     - A fix for the OPAL sensor driver from Cdric Le Goater.
    
     - Fixes to get STRICT_MM_TYPECHECKS building again by Michael Ellerman.
    
     - Large series from Daniel Axtens to make our PCI hooks per PHB rather
       than per machine.
    
     - Small patch from Sam Bobroff to explicitly abort non-suspended
       transactions on syscalls, plus a test to exercise it.
    
     - Numerous reworks and fixes for the 24x7 PMU from Sukadev Bhattiprolu.
    
     - Small patch to enable the hard lockup detector from Anton Blanchard.
    
     - Fix from Dave Olson for missing L2 cache information on some CPUs.
    
     - Some fixes from Michael Ellerman to get Cell machines booting again.
    
     - Freescale updates from Scott: Highlights include BMan device tree
       nodes, an MSI erratum workaround, a couple minor performance
       improvements, config updates, and misc fixes/cleanup.
    
    * tag 'powerpc-4.1-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux: (196 commits)
      powerpc/powermac: Fix build error seen with powermac smp builds
      powerpc/pseries: Fix compile of memory hotplug without CONFIG_MEMORY_HOTREMOVE
      powerpc: Remove PPC32 code from pseries specific find_and_init_phbs()
      powerpc/cell: Fix iommu breakage caused by controller_ops change
      powerpc/eeh: Fix crash in eeh_add_device_early() on Cell
      powerpc/perf: Cap 64bit userspace backtraces to PERF_MAX_STACK_DEPTH
      powerpc/perf/hv-24x7: Fail 24x7 initcall if create_events_from_catalog() fails
      powerpc/pseries: Correct memory hotplug locking
      powerpc: Fix missing L2 cache size in /sys/devices/system/cpu
      powerpc: Add ppc64 hard lockup detector support
      oprofile: Disable oprofile NMI timer on ppc64
      powerpc/perf/hv-24x7: Add missing put_cpu_var()
      powerpc/perf/hv-24x7: Break up single_24x7_request
      powerpc/perf/hv-24x7: Define update_event_count()
      powerpc/perf/hv-24x7: Whitespace cleanup
      powerpc/perf/hv-24x7: Define add_event_to_24x7_request()
      powerpc/perf/hv-24x7: Rename hv_24x7_event_update
      powerpc/perf/hv-24x7: Move debug prints to separate function
      powerpc/perf/hv-24x7: Drop event_24x7_request()
      powerpc/perf/hv-24x7: Use pr_devel() to log message
      ...
    
    Conflicts:
            tools/testing/selftests/powerpc/Makefile
            tools/testing/selftests/powerpc/tm/Makefile

commit 646b54f2f2041473495f166479e3e17fd59a9dd1
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Mar 12 17:27:11 2015 +1100

    powerpc/powernv: Remove powernv RTAS support
    
    The powernv code has some conditional support for running on bare metal
    machines that have no OPAL firmware, but provide RTAS.
    
    No released machines ever supported that, and even in the lab it was
    just a transitional hack in the days when OPAL was still being
    developed.
    
    So remove the code.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Stewart Smith <stewart@linux.vnet.ibm.com>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index fc34025ef822..273d7b46f72a 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -25,7 +25,6 @@
 #include <asm/machdep.h>
 #include <asm/cputable.h>
 #include <asm/firmware.h>
-#include <asm/rtas.h>
 #include <asm/vdso_datapage.h>
 #include <asm/cputhreads.h>
 #include <asm/xics.h>
@@ -241,18 +240,6 @@ void __init pnv_smp_init(void)
 {
 	smp_ops = &pnv_smp_ops;
 
-	/* XXX We don't yet have a proper entry point from HAL, for
-	 * now we rely on kexec-style entry from BML
-	 */
-
-#ifdef CONFIG_PPC_RTAS
-	/* Non-lpar has additional take/give timebase */
-	if (rtas_token("freeze-time-base") != RTAS_UNKNOWN_SERVICE) {
-		smp_ops->give_timebase = rtas_give_timebase;
-		smp_ops->take_timebase = rtas_take_timebase;
-	}
-#endif /* CONFIG_PPC_RTAS */
-
 #ifdef CONFIG_HOTPLUG_CPU
 	ppc_md.cpu_die	= pnv_smp_cpu_kill_self;
 #endif

commit 755563bc79c764c90b9f44db5e4fe6c556d3440c
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Mar 19 19:29:01 2015 +1100

    powerpc/powernv: Fixes for hypervisor doorbell handling
    
    Since we can now use hypervisor doorbells for host IPIs, this makes
    sure we clear the host IPI flag when taking a doorbell interrupt, and
    clears any pending doorbell IPI in pnv_smp_cpu_kill_self() (as we
    already do for IPIs sent via the XICS interrupt controller).  Otherwise
    if there did happen to be a leftover pending doorbell interrupt for
    an offline CPU thread for any reason, it would prevent that thread from
    going into a power-saving mode; it would instead keep waking up because
    of the interrupt.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index fc34025ef822..38a45088f633 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -33,6 +33,8 @@
 #include <asm/runlatch.h>
 #include <asm/code-patching.h>
 #include <asm/dbell.h>
+#include <asm/kvm_ppc.h>
+#include <asm/ppc-opcode.h>
 
 #include "powernv.h"
 
@@ -149,7 +151,7 @@ static int pnv_smp_cpu_disable(void)
 static void pnv_smp_cpu_kill_self(void)
 {
 	unsigned int cpu;
-	unsigned long srr1;
+	unsigned long srr1, wmask;
 	u32 idle_states;
 
 	/* Standard hot unplug procedure */
@@ -161,6 +163,10 @@ static void pnv_smp_cpu_kill_self(void)
 	generic_set_cpu_dead(cpu);
 	smp_wmb();
 
+	wmask = SRR1_WAKEMASK;
+	if (cpu_has_feature(CPU_FTR_ARCH_207S))
+		wmask = SRR1_WAKEMASK_P8;
+
 	idle_states = pnv_get_supported_cpuidle_states();
 	/* We don't want to take decrementer interrupts while we are offline,
 	 * so clear LPCR:PECE1. We keep PECE2 enabled.
@@ -191,10 +197,14 @@ static void pnv_smp_cpu_kill_self(void)
 		 * having finished executing in a KVM guest, then srr1
 		 * contains 0.
 		 */
-		if ((srr1 & SRR1_WAKEMASK) == SRR1_WAKEEE) {
+		if ((srr1 & wmask) == SRR1_WAKEEE) {
 			icp_native_flush_interrupt();
 			local_paca->irq_happened &= PACA_IRQ_HARD_DIS;
 			smp_mb();
+		} else if ((srr1 & wmask) == SRR1_WAKEHDBELL) {
+			unsigned long msg = PPC_DBELL_TYPE(PPC_DBELL_SERVER);
+			asm volatile(PPC_MSGCLR(%0) : : "r" (msg));
+			kvmppc_set_host_ipi(cpu, 0);
 		}
 
 		if (cpu_core_split_required())

commit d70a54e2d08510a99b1f10eceeae6f2f7086e226
Author: Greg Kurz <gkurz@linux.vnet.ibm.com>
Date:   Fri Dec 12 12:37:40 2014 +0100

    powerpc/powernv: Ignore smt-enabled on Power8 and later
    
    Starting with POWER8, the subcore logic relies on all threads of a core
    being booted so that they can participate in split mode switches. So on
    those machines we ignore the smt_enabled_at_boot setting (smt-enabled on
    the kernel command line).
    
    Signed-off-by: Greg Kurz <gkurz@linux.vnet.ibm.com>
    [mpe: Update comment and change log to be more precise]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 6c551a28e899..fc34025ef822 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -209,13 +209,27 @@ static void pnv_smp_cpu_kill_self(void)
 
 #endif /* CONFIG_HOTPLUG_CPU */
 
+static int pnv_cpu_bootable(unsigned int nr)
+{
+	/*
+	 * Starting with POWER8, the subcore logic relies on all threads of a
+	 * core being booted so that they can participate in split mode
+	 * switches. So on those machines we ignore the smt_enabled_at_boot
+	 * setting (smt-enabled on the kernel command line).
+	 */
+	if (cpu_has_feature(CPU_FTR_ARCH_207S))
+		return 1;
+
+	return smp_generic_cpu_bootable(nr);
+}
+
 static struct smp_ops_t pnv_smp_ops = {
 	.message_pass	= smp_muxed_ipi_message_pass,
 	.cause_ipi	= NULL,	/* Filled at runtime by xics_smp_probe() */
 	.probe		= xics_smp_probe,
 	.kick_cpu	= pnv_smp_kick_cpu,
 	.setup_cpu	= pnv_smp_setup_cpu,
-	.cpu_bootable	= smp_generic_cpu_bootable,
+	.cpu_bootable	= pnv_cpu_bootable,
 #ifdef CONFIG_HOTPLUG_CPU
 	.cpu_disable	= pnv_smp_cpu_disable,
 	.cpu_die	= generic_cpu_die,

commit 77b54e9f213f76a23736940cf94bcd765fc00f40
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Wed Dec 10 00:26:53 2014 +0530

    powernv/powerpc: Add winkle support for offline cpus
    
    Winkle is a deep idle state supported in power8 chips. A core enters
    winkle when all the threads of the core enter winkle. In this state
    power supply to the entire chiplet i.e core, private L2 and private L3
    is turned off. As a result it gives higher powersavings compared to
    sleep.
    
    But entering winkle results in a total hypervisor state loss. Hence the
    hypervisor context has to be preserved before entering winkle and
    restored upon wake up.
    
    Power-on Reset Engine (PORE) is a dedicated engine which is responsible
    for powering on the chiplet during wake up. It can be programmed to
    restore the register contests of a few specific registers. This patch
    uses PORE to restore register state wherever possible and uses stack to
    save and restore rest of the necessary registers.
    
    With hypervisor state restore things fall under three categories-
    per-core state, per-subcore state and per-thread state. To manage this,
    extend the infrastructure introduced for sleep. Mainly we add a paca
    variable subcore_sibling_mask. Using this and the core_idle_state we can
    distingush first thread in core and subcore.
    
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index c0691d0fb385..6c551a28e899 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -167,12 +167,17 @@ static void pnv_smp_cpu_kill_self(void)
 	 */
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
 	while (!generic_check_cpu_restart(cpu)) {
+
 		ppc64_runlatch_off();
-		if ((idle_states & OPAL_PM_SLEEP_ENABLED) ||
+
+		if (idle_states & OPAL_PM_WINKLE_ENABLED)
+			srr1 = power7_winkle();
+		else if ((idle_states & OPAL_PM_SLEEP_ENABLED) ||
 				(idle_states & OPAL_PM_SLEEP_ENABLED_ER1))
 			srr1 = power7_sleep();
 		else
 			srr1 = power7_nap(1);
+
 		ppc64_runlatch_on();
 
 		/*

commit 7cba160ad789a3ad7e68b92bf20eaad6ed171f80
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Wed Dec 10 00:26:52 2014 +0530

    powernv/cpuidle: Redesign idle states management
    
    Deep idle states like sleep and winkle are per core idle states. A core
    enters these states only when all the threads enter either the
    particular idle state or a deeper one. There are tasks like fastsleep
    hardware bug workaround and hypervisor core state save which have to be
    done only by the last thread of the core entering deep idle state and
    similarly tasks like timebase resync, hypervisor core register restore
    that have to be done only by the first thread waking up from these
    state.
    
    The current idle state management does not have a way to distinguish the
    first/last thread of the core waking/entering idle states. Tasks like
    timebase resync are done for all the threads. This is not only is
    suboptimal, but can cause functionality issues when subcores and kvm is
    involved.
    
    This patch adds the necessary infrastructure to track idle states of
    threads in a per-core structure. It uses this info to perform tasks like
    fastsleep workaround and timebase resync only once per core.
    
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Originally-by: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: linux-pm@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 83299ef2dc3d..c0691d0fb385 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -168,7 +168,8 @@ static void pnv_smp_cpu_kill_self(void)
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
 	while (!generic_check_cpu_restart(cpu)) {
 		ppc64_runlatch_off();
-		if (idle_states & OPAL_PM_SLEEP_ENABLED)
+		if ((idle_states & OPAL_PM_SLEEP_ENABLED) ||
+				(idle_states & OPAL_PM_SLEEP_ENABLED_ER1))
 			srr1 = power7_sleep();
 		else
 			srr1 = power7_nap(1);

commit 8eb8ac89a364305d05ad16be983b7890eb462cc3
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Wed Dec 10 00:26:51 2014 +0530

    powerpc/powernv: Enable Offline CPUs to enter deep idle states
    
    The secondary threads should enter deep idle states so as to gain maximum
    powersavings when the entire core is offline. To do so the offline path
    must be made aware of the available deepest idle state. Hence probe the
    device tree for the possible idle states in powernv core code and
    expose the deepest idle state through flags.
    
    Since the  device tree is probed by the cpuidle driver as well, move
    the parameters required to discover the idle states into an appropriate
    common place to both the driver and the powernv core code.
    
    Another point is that fastsleep idle state may require workarounds in
    the kernel to function properly. This workaround is introduced in the
    subsequent patches. However neither the cpuidle driver or the hotplug
    path need be bothered about this workaround.
    
    They will be taken care of by the core powernv code.
    
    Originally-by: Srivatsa S. Bhat <srivatsa@mit.edu>
    Signed-off-by: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Reviewed-by: Paul Mackerras <paulus@samba.org>
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: linux-pm@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index b716f666e48a..83299ef2dc3d 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -150,6 +150,7 @@ static void pnv_smp_cpu_kill_self(void)
 {
 	unsigned int cpu;
 	unsigned long srr1;
+	u32 idle_states;
 
 	/* Standard hot unplug procedure */
 	local_irq_disable();
@@ -160,13 +161,17 @@ static void pnv_smp_cpu_kill_self(void)
 	generic_set_cpu_dead(cpu);
 	smp_wmb();
 
+	idle_states = pnv_get_supported_cpuidle_states();
 	/* We don't want to take decrementer interrupts while we are offline,
 	 * so clear LPCR:PECE1. We keep PECE2 enabled.
 	 */
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
 	while (!generic_check_cpu_restart(cpu)) {
 		ppc64_runlatch_off();
-		srr1 = power7_nap(1);
+		if (idle_states & OPAL_PM_SLEEP_ENABLED)
+			srr1 = power7_sleep();
+		else
+			srr1 = power7_nap(1);
 		ppc64_runlatch_on();
 
 		/*

commit 56548fc0e86cb9156af7a7e1f15ba78f251dafaf
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Dec 3 14:48:40 2014 +1100

    powerpc/powernv: Return to cpu offline loop when finished in KVM guest
    
    When a secondary hardware thread has finished running a KVM guest, we
    currently put that thread into nap mode using a nap instruction in
    the KVM code.  This changes the code so that instead of doing a nap
    instruction directly, we instead cause the call to power7_nap() that
    put the thread into nap mode to return.  The reason for doing this is
    to avoid having the KVM code having to know what low-power mode to
    put the thread into.
    
    In the case of a secondary thread used to run a KVM guest, the thread
    will be offline from the point of view of the host kernel, and the
    relevant power7_nap() call is the one in pnv_smp_cpu_disable().
    In this case we don't want to clear pending IPIs in the offline loop
    in that function, since that might cause us to miss the wakeup for
    the next time the thread needs to run a guest.  To tell whether or
    not to clear the interrupt, we use the SRR1 value returned from
    power7_nap(), and check if it indicates an external interrupt.  We
    arrange that the return from power7_nap() when we have finished running
    a guest returns 0, so pending interrupts don't get flushed in that
    case.
    
    Note that it is important a secondary thread that has finished
    executing in the guest, or that didn't have a guest to run, should
    not return to power7_nap's caller while the kvm_hstate.hwthread_req
    flag in the PACA is non-zero, because the return from power7_nap
    will reenable the MMU, and the MMU might still be in guest context.
    In this situation we spin at low priority in real mode waiting for
    hwthread_req to become zero.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 4753958cd509..b716f666e48a 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -149,6 +149,7 @@ static int pnv_smp_cpu_disable(void)
 static void pnv_smp_cpu_kill_self(void)
 {
 	unsigned int cpu;
+	unsigned long srr1;
 
 	/* Standard hot unplug procedure */
 	local_irq_disable();
@@ -165,13 +166,25 @@ static void pnv_smp_cpu_kill_self(void)
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
 	while (!generic_check_cpu_restart(cpu)) {
 		ppc64_runlatch_off();
-		power7_nap(1);
+		srr1 = power7_nap(1);
 		ppc64_runlatch_on();
 
-		/* Clear the IPI that woke us up */
-		icp_native_flush_interrupt();
-		local_paca->irq_happened &= PACA_IRQ_HARD_DIS;
-		mb();
+		/*
+		 * If the SRR1 value indicates that we woke up due to
+		 * an external interrupt, then clear the interrupt.
+		 * We clear the interrupt before checking for the
+		 * reason, so as to avoid a race where we wake up for
+		 * some other reason, find nothing and clear the interrupt
+		 * just as some other cpu is sending us an interrupt.
+		 * If we returned from power7_nap as a result of
+		 * having finished executing in a KVM guest, then srr1
+		 * contains 0.
+		 */
+		if ((srr1 & SRR1_WAKEMASK) == SRR1_WAKEEE) {
+			icp_native_flush_interrupt();
+			local_paca->irq_happened &= PACA_IRQ_HARD_DIS;
+			smp_mb();
+		}
 
 		if (cpu_core_split_required())
 			continue;

commit d6a4f70909d279004a2b3d539e240e07b1ecc1cb
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 2 14:23:16 2014 +1000

    powerpc/powernv: Don't call generic code on offline cpus
    
    On PowerNV platforms, when a CPU is offline, we put it into nap mode.
    It's possible that the CPU wakes up from nap mode while it is still
    offline due to a stray IPI.  A misdirected device interrupt could also
    potentially cause it to wake up.  In that circumstance, we need to clear
    the interrupt so that the CPU can go back to nap mode.
    
    In the past the clearing of the interrupt was accomplished by briefly
    enabling interrupts and allowing the normal interrupt handling code
    (do_IRQ() etc.) to handle the interrupt.  This has the problem that
    this code calls irq_enter() and irq_exit(), which call functions such
    as account_system_vtime() which use RCU internally.  Use of RCU is not
    permitted on offline CPUs and will trigger errors if RCU checking is
    enabled.
    
    To avoid calling into any generic code which might use RCU, we adopt
    a different method of clearing interrupts on offline CPUs.  Since we
    are on the PowerNV platform, we know that the system interrupt
    controller is a XICS being driven directly (i.e. not via hcalls) by
    the kernel.  Hence this adds a new icp_native_flush_interrupt()
    function to the native-mode XICS driver and arranges to call that
    when an offline CPU is woken from nap.  This new function reads the
    interrupt from the XICS.  If it is an IPI, it clears the IPI; if it
    is a device interrupt, it prints a warning and disables the source.
    Then it does the end-of-interrupt processing for the interrupt.
    
    The other thing that briefly enabling interrupts did was to check and
    clear the irq_happened flag in this CPU's PACA.  Therefore, after
    flushing the interrupt from the XICS, we also clear all bits except
    the PACA_IRQ_HARD_DIS (interrupts are hard disabled) bit from the
    irq_happened flag.  The PACA_IRQ_HARD_DIS flag is set by power7_nap()
    and is left set to indicate that interrupts are hard disabled.  This
    means we then have to ignore that flag in power7_nap(), which is
    reasonable since it doesn't indicate that any interrupt event needs
    servicing.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index b73adc573031..4753958cd509 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -168,9 +168,9 @@ static void pnv_smp_cpu_kill_self(void)
 		power7_nap(1);
 		ppc64_runlatch_on();
 
-		/* Reenable IRQs briefly to clear the IPI that woke us */
-		local_irq_enable();
-		local_irq_disable();
+		/* Clear the IPI that woke us up */
+		icp_native_flush_interrupt();
+		local_paca->irq_happened &= PACA_IRQ_HARD_DIS;
 		mb();
 
 		if (cpu_core_split_required())

commit e51df2c170efaeadce4d416e1825b0830de0a795
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 20 08:55:18 2014 +1000

    powerpc: Make a bunch of things static
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 5fcfcf44e3a9..b73adc573031 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -54,7 +54,7 @@ static void pnv_smp_setup_cpu(int cpu)
 #endif
 }
 
-int pnv_smp_kick_cpu(int nr)
+static int pnv_smp_kick_cpu(int nr)
 {
 	unsigned int pcpu = get_hard_smp_processor_id(nr);
 	unsigned long start_here =

commit d4e58e5928f8c6c49228451dd03e0714cbab299a
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Jun 11 15:59:28 2014 +1000

    powerpc/powernv: Enable POWER8 doorbell IPIs
    
    This patch enables POWER8 doorbell IPIs on powernv.
    
    Since doorbells can only IPI within a core, we test to see when we can use
    doorbells and if not we fall back to XICS.  This also enables hypervisor
    doorbells to wakeup us up from nap/sleep via the LPCR PECEDH bit.
    
    Based on tests by Anton, the best case IPI latency between two threads dropped
    from 894ns to 512ns.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 0062a43a2e0d..5fcfcf44e3a9 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -32,6 +32,7 @@
 #include <asm/opal.h>
 #include <asm/runlatch.h>
 #include <asm/code-patching.h>
+#include <asm/dbell.h>
 
 #include "powernv.h"
 
@@ -46,6 +47,11 @@ static void pnv_smp_setup_cpu(int cpu)
 {
 	if (cpu != boot_cpuid)
 		xics_setup_cpu();
+
+#ifdef CONFIG_PPC_DOORBELL
+	if (cpu_has_feature(CPU_FTR_DBELL))
+		doorbell_setup_this_cpu();
+#endif
 }
 
 int pnv_smp_kick_cpu(int nr)

commit e2186023f2d81ee7bb42d2a7dec3d889df7cdace
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri May 23 18:15:30 2014 +1000

    powerpc/powernv: Add support for POWER8 split core on powernv
    
    Upcoming POWER8 chips support a concept called split core. This is where the
    core can be split into subcores that although not full cores, are able to
    appear as full cores to a guest.
    
    The splitting & unsplitting procedure is mildly complicated, and explained at
    length in the comments within the patch.
    
    One notable detail is that when splitting or unsplitting we need to pull
    offline cpus out of their offline state to do work as part of the procedure.
    
    The interface for changing the split mode is via a sysfs file, eg:
    
     $ echo 2 > /sys/devices/system/cpu/subcores_per_core
    
    Currently supported values are '1', '2' and '4'. And indicate respectively that
    the core should be unsplit, split in half, and split in quarters. These modes
    correspond to threads_per_subcore of 8, 4 and 2.
    
    We do not allow changing the split mode while KVM VMs are active. This is to
    prevent the value changing while userspace is configuring the VM, and also to
    prevent the mode being changed in such a way that existing guests are unable to
    be run.
    
    CPU hotplug fixes by Srivatsa.  max_cpus fixes by Mahesh.  cpuset fixes by
    benh.  Fix for irq race by paulus.  The rest by mikey and mpe.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 65faf998fe2c..0062a43a2e0d 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -161,15 +161,17 @@ static void pnv_smp_cpu_kill_self(void)
 		ppc64_runlatch_off();
 		power7_nap(1);
 		ppc64_runlatch_on();
-		if (!generic_check_cpu_restart(cpu)) {
+
+		/* Reenable IRQs briefly to clear the IPI that woke us */
+		local_irq_enable();
+		local_irq_disable();
+		mb();
+
+		if (cpu_core_split_required())
+			continue;
+
+		if (!generic_check_cpu_restart(cpu))
 			DBG("CPU%d Unexpected exit while offline !\n", cpu);
-			/* We may be getting an IPI, so we re-enable
-			 * interrupts to process it, it will be ignored
-			 * since we aren't online (hopefully)
-			 */
-			local_irq_enable();
-			local_irq_disable();
-		}
 	}
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) | LPCR_PECE1);
 	DBG("CPU%d coming online...\n", cpu);

commit 8d6f7c5aa3db6f3e5e43d09f8a0166c7d96f33f3
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri May 23 18:15:26 2014 +1000

    powerpc/powernv: Make it possible to skip the IRQHAPPENED check in power7_nap()
    
    To support split core we need to be able to force all secondaries into
    nap, so the core can detect they are idle and do an unsplit.
    
    Currently power7_nap() will return without napping if there is an irq
    pending. We want to ignore the pending irq and nap anyway, we will deal
    with the interrupt later.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 1601a1ea02c4..65faf998fe2c 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -159,7 +159,7 @@ static void pnv_smp_cpu_kill_self(void)
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
 	while (!generic_check_cpu_restart(cpu)) {
 		ppc64_runlatch_off();
-		power7_nap();
+		power7_nap(1);
 		ppc64_runlatch_on();
 		if (!generic_check_cpu_restart(cpu)) {
 			DBG("CPU%d Unexpected exit while offline !\n", cpu);

commit f6869e7fe657bd977e72954cd78c5871a6a4f71d
Merge: 5a4e58bc693f cec4b7eaf09d
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon May 5 20:33:10 2014 +1000

    Merge remote-tracking branch 'anton/abiv2' into next
    
    This series adds support for building the powerpc 64-bit
    LE kernel using the new ABI v2. We already supported
    running ABI v2 userspace programs but this adds support
    for building the kernel itself using the new ABI.

commit f203891117323dd31c11adcbba60452c933d604e
Author: Preeti U Murthy <preeti@linux.vnet.ibm.com>
Date:   Fri Apr 11 16:01:48 2014 +0530

    ppc/powernv: Set the runlatch bits correctly for offline cpus
    
    Up until now we have been setting the runlatch bits for a busy CPU and
    clearing it when a CPU enters idle state. The runlatch bit has thus
    been consistent with the utilization of a CPU as long as the CPU is online.
    
    However when a CPU is hotplugged out the runlatch bit is not cleared. It
    needs to be cleared to indicate an unused CPU. Hence this patch has the
    runlatch bit cleared for an offline CPU just before entering an idle state
    and sets it immediately after it exits the idle state.
    
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 908672bdcea6..bf5fcd452168 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -30,6 +30,7 @@
 #include <asm/cputhreads.h>
 #include <asm/xics.h>
 #include <asm/opal.h>
+#include <asm/runlatch.h>
 
 #include "powernv.h"
 
@@ -156,7 +157,9 @@ static void pnv_smp_cpu_kill_self(void)
 	 */
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
 	while (!generic_check_cpu_restart(cpu)) {
+		ppc64_runlatch_off();
 		power7_nap();
+		ppc64_runlatch_on();
 		if (!generic_check_cpu_restart(cpu)) {
 			DBG("CPU%d Unexpected exit while offline !\n", cpu);
 			/* We may be getting an IPI, so we re-enable

commit 2751b628c97e66e61f482935ca59148751972941
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Mar 11 11:54:06 2014 +1100

    powerpc: Fix SMP issues with ppc64le ABIv2
    
    There is no need to put a function descriptor in
    __secondary_hold_spinloop. Use ppc_function_entry to get the
    instruction address and put it in __secondary_hold_spinloop instead.
    
    Also fix an issue where we assumed cur_cpu_spec held a function
    descriptor.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 908672bdcea6..b370b86263a6 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -30,6 +30,7 @@
 #include <asm/cputhreads.h>
 #include <asm/xics.h>
 #include <asm/opal.h>
+#include <asm/code-patching.h>
 
 #include "powernv.h"
 
@@ -49,8 +50,8 @@ static void pnv_smp_setup_cpu(int cpu)
 int pnv_smp_kick_cpu(int nr)
 {
 	unsigned int pcpu = get_hard_smp_processor_id(nr);
-	unsigned long start_here = __pa(*((unsigned long *)
-					  generic_secondary_smp_init));
+	unsigned long start_here =
+			__pa(ppc_function_entry(generic_secondary_smp_init));
 	long rc;
 
 	BUG_ON(nr < 0 || nr >= NR_CPUS);

commit 39fd40274d1f3a52152ae6fc22f428d93f1a8363
Author: Andy Fleming <afleming@freescale.com>
Date:   Mon Aug 5 14:58:35 2013 -0500

    powerpc: Convert platforms to smp_generic_cpu_bootable
    
    T4, Cell, powernv, and pseries had the same implementation, so switch
    them to use a generic version. A2 apparently had a version, but
    removed it at some point, so we remove the declaration, too.
    
    Signed-off-by: Andy Fleming <afleming@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 89e3857af4e0..908672bdcea6 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -46,22 +46,6 @@ static void pnv_smp_setup_cpu(int cpu)
 		xics_setup_cpu();
 }
 
-static int pnv_smp_cpu_bootable(unsigned int nr)
-{
-	/* Special case - we inhibit secondary thread startup
-	 * during boot if the user requests it.
-	 */
-	if (system_state == SYSTEM_BOOTING && cpu_has_feature(CPU_FTR_SMT)) {
-		if (!smt_enabled_at_boot && cpu_thread_in_core(nr) != 0)
-			return 0;
-		if (smt_enabled_at_boot
-		    && cpu_thread_in_core(nr) >= smt_enabled_at_boot)
-			return 0;
-	}
-
-	return 1;
-}
-
 int pnv_smp_kick_cpu(int nr)
 {
 	unsigned int pcpu = get_hard_smp_processor_id(nr);
@@ -195,7 +179,7 @@ static struct smp_ops_t pnv_smp_ops = {
 	.probe		= xics_smp_probe,
 	.kick_cpu	= pnv_smp_kick_cpu,
 	.setup_cpu	= pnv_smp_setup_cpu,
-	.cpu_bootable	= pnv_smp_cpu_bootable,
+	.cpu_bootable	= smp_generic_cpu_bootable,
 #ifdef CONFIG_HOTPLUG_CPU
 	.cpu_disable	= pnv_smp_cpu_disable,
 	.cpu_die	= generic_cpu_die,

commit 061d19f279f9bebbdb1ee48bef8c25e03de32ae2
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon Jun 24 15:30:09 2013 -0400

    powerpc: Delete __cpuinit usage from all users
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    This removes all the powerpc uses of the __cpuinit macros.  There
    are no __CPUINIT users in assembly files in powerpc.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Josh Boyer <jwboyer@gmail.com>
    Cc: Matt Porter <mporter@kernel.crashing.org>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 77784aead1c2..89e3857af4e0 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -40,7 +40,7 @@
 #define DBG(fmt...)
 #endif
 
-static void __cpuinit pnv_smp_setup_cpu(int cpu)
+static void pnv_smp_setup_cpu(int cpu)
 {
 	if (cpu != boot_cpuid)
 		xics_setup_cpu();

commit a5b45ded097908d40803b5c2770259398811b24e
Author: liguang <lig.fnst@cn.fujitsu.com>
Date:   Thu May 30 14:47:53 2013 +0800

    powerpc/smp: Use '==' instead of '<' for system_state
    
    'system_state < SYSTEM_RUNNING' will have same effect
    with 'system_state == SYSTEM_BOOTING', but the later
    one is more clearer.
    
    Signed-off-by: liguang <lig.fnst@cn.fujitsu.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 88c9459c3e07..77784aead1c2 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -51,7 +51,7 @@ static int pnv_smp_cpu_bootable(unsigned int nr)
 	/* Special case - we inhibit secondary thread startup
 	 * during boot if the user requests it.
 	 */
-	if (system_state < SYSTEM_RUNNING && cpu_has_feature(CPU_FTR_SMT)) {
+	if (system_state == SYSTEM_BOOTING && cpu_has_feature(CPU_FTR_SMT)) {
 		if (!smt_enabled_at_boot && cpu_thread_in_core(nr) != 0)
 			return 0;
 		if (smt_enabled_at_boot

commit b2b48584dfc69a5ebc70ee7976524d7122e5df5f
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue May 14 15:12:31 2013 +1000

    powerpc/powernv: Fix starting of secondary CPUs on OPALv2 and v3
    
    The current code fails to handle kexec on OPALv2. This fixes it
    and adds code to improve the situation on OPALv3 where we can
    query the CPU status from the firmware and decide what to do
    based on that.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 6a3ecca5b725..88c9459c3e07 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -71,18 +71,68 @@ int pnv_smp_kick_cpu(int nr)
 
 	BUG_ON(nr < 0 || nr >= NR_CPUS);
 
-	/* On OPAL v2 the CPU are still spinning inside OPAL itself,
-	 * get them back now
+	/*
+	 * If we already started or OPALv2 is not supported, we just
+	 * kick the CPU via the PACA
 	 */
-	if (!paca[nr].cpu_start && firmware_has_feature(FW_FEATURE_OPALv2)) {
-		pr_devel("OPAL: Starting CPU %d (HW 0x%x)...\n", nr, pcpu);
-		rc = opal_start_cpu(pcpu, start_here);
+	if (paca[nr].cpu_start || !firmware_has_feature(FW_FEATURE_OPALv2))
+		goto kick;
+
+	/*
+	 * At this point, the CPU can either be spinning on the way in
+	 * from kexec or be inside OPAL waiting to be started for the
+	 * first time. OPAL v3 allows us to query OPAL to know if it
+	 * has the CPUs, so we do that
+	 */
+	if (firmware_has_feature(FW_FEATURE_OPALv3)) {
+		uint8_t status;
+
+		rc = opal_query_cpu_status(pcpu, &status);
 		if (rc != OPAL_SUCCESS) {
-			pr_warn("OPAL Error %ld starting CPU %d\n",
+			pr_warn("OPAL Error %ld querying CPU %d state\n",
 				rc, nr);
 			return -ENODEV;
 		}
+
+		/*
+		 * Already started, just kick it, probably coming from
+		 * kexec and spinning
+		 */
+		if (status == OPAL_THREAD_STARTED)
+			goto kick;
+
+		/*
+		 * Available/inactive, let's kick it
+		 */
+		if (status == OPAL_THREAD_INACTIVE) {
+			pr_devel("OPAL: Starting CPU %d (HW 0x%x)...\n",
+				 nr, pcpu);
+			rc = opal_start_cpu(pcpu, start_here);
+			if (rc != OPAL_SUCCESS) {
+				pr_warn("OPAL Error %ld starting CPU %d\n",
+					rc, nr);
+				return -ENODEV;
+			}
+		} else {
+			/*
+			 * An unavailable CPU (or any other unknown status)
+			 * shouldn't be started. It should also
+			 * not be in the possible map but currently it can
+			 * happen
+			 */
+			pr_devel("OPAL: CPU %d (HW 0x%x) is unavailable"
+				 " (status %d)...\n", nr, pcpu, status);
+			return -ENODEV;
+		}
+	} else {
+		/*
+		 * On OPAL v2, we just kick it and hope for the best,
+		 * we must not test the error from opal_start_cpu() or
+		 * we would fail to get CPUs from kexec.
+		 */
+		opal_start_cpu(pcpu, start_here);
 	}
+ kick:
 	return smp_generic_kick_cpu(nr);
 }
 

commit 4ea9008b75cdd6781207b73f4c01b6a877a0bb60
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri May 3 17:21:00 2013 +0000

    powerpc/powerpnv: Properly handle failure starting CPUs
    
    If OPAL returns an error, propagate it upward rather than spinning
    seconds waiting for a CPU that will never show up
    
    Signed-off-by: Benjamin Herrenschmidt  <benh@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 0bdc735db16f..6a3ecca5b725 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -77,9 +77,11 @@ int pnv_smp_kick_cpu(int nr)
 	if (!paca[nr].cpu_start && firmware_has_feature(FW_FEATURE_OPALv2)) {
 		pr_devel("OPAL: Starting CPU %d (HW 0x%x)...\n", nr, pcpu);
 		rc = opal_start_cpu(pcpu, start_here);
-		if (rc != OPAL_SUCCESS)
+		if (rc != OPAL_SUCCESS) {
 			pr_warn("OPAL Error %ld starting CPU %d\n",
 				rc, nr);
+			return -ENODEV;
+		}
 	}
 	return smp_generic_kick_cpu(nr);
 }

commit cad5cef62a5a0c525d39118d2e94b6e2034d5e05
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Fri Dec 21 14:04:10 2012 -0800

    POWERPC: drivers: remove __dev* attributes.
    
    CONFIG_HOTPLUG is going away as an option.  As a result, the __dev*
    markings need to be removed.
    
    This change removes the use of __devinit, __devexit_p, __devinitdata,
    __devinitconst, and __devexit from these drivers.
    
    Based on patches originally written by Bill Pemberton, but redone by me
    in order to handle some of the coding style issues better, by hand.
    
    Cc: Bill Pemberton <wfp5p@virginia.edu>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 7698b6e13c57..0bdc735db16f 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -62,7 +62,7 @@ static int pnv_smp_cpu_bootable(unsigned int nr)
 	return 1;
 }
 
-int __devinit pnv_smp_kick_cpu(int nr)
+int pnv_smp_kick_cpu(int nr)
 {
 	unsigned int pcpu = get_hard_smp_processor_id(nr);
 	unsigned long start_here = __pa(*((unsigned long *)

commit 375f561a4131a0f501c8845a2a20f2ca1abc8f7a
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jul 26 18:51:09 2012 +0000

    powerpc/powernv: Always go into nap mode when CPU is offline
    
    The CPU hotplug code for the powernv platform currently only puts
    offline CPUs into nap mode if the powersave_nap variable is set.
    However, HV-style KVM on this platform requires secondary CPU threads
    to be offline and in nap mode.  Since we know nap mode works just
    fine on all POWER7 machines, and the only machines that support the
    powernv platform are POWER7 machines, this changes the code to
    always put offline CPUs into nap mode, regardless of powersave_nap.
    Powersave_nap still controls whether or not CPUs go into nap mode
    when idle, as before.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 3ef46254c35b..7698b6e13c57 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -106,14 +106,6 @@ static void pnv_smp_cpu_kill_self(void)
 {
 	unsigned int cpu;
 
-	/* If powersave_nap is enabled, use NAP mode, else just
-	 * spin aimlessly
-	 */
-	if (!powersave_nap) {
-		generic_mach_cpu_die();
-		return;
-	}
-
 	/* Standard hot unplug procedure */
 	local_irq_disable();
 	idle_task_exit();
@@ -128,7 +120,7 @@ static void pnv_smp_cpu_kill_self(void)
 	 */
 	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
 	while (!generic_check_cpu_restart(cpu)) {
-		power7_idle();
+		power7_nap();
 		if (!generic_check_cpu_restart(cpu)) {
 			DBG("CPU%d Unexpected exit while offline !\n", cpu);
 			/* We may be getting an IPI, so we re-enable

commit ae3a197e3d0bfe3f4bf1693723e82dc018c096f3
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:02 2012 +0100

    Disintegrate asm/system.h for PowerPC
    
    Disintegrate asm/system.h for PowerPC.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    cc: linuxppc-dev@lists.ozlabs.org

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 17210c526c52..3ef46254c35b 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -25,7 +25,6 @@
 #include <asm/machdep.h>
 #include <asm/cputable.h>
 #include <asm/firmware.h>
-#include <asm/system.h>
 #include <asm/rtas.h>
 #include <asm/vdso_datapage.h>
 #include <asm/cputhreads.h>

commit cba313da5c8ddbe6bba74c9f2b8f6d9e0bc0e723
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 5 19:35:32 2011 +0000

    powerpc/powernv: Fix problems in onlining CPUs
    
    At present, on the powernv platform, if you off-line a CPU that was
    online, and then try to on-line it again, the kernel generates a
    warning message "OPAL Error -1 starting CPU n".  Furthermore, if the
    CPU is a secondary thread that was used by KVM while it was off-line,
    the CPU fails to come online.
    
    The first problem is fixed by only calling OPAL to start the CPU the
    first time it is on-lined, as indicated by the cpu_start field of its
    PACA being zero.  The second problem is fixed by restoring the
    cpu_start field to 1 instead of 0 when using the CPU within KVM.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index e87736685243..17210c526c52 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -75,7 +75,7 @@ int __devinit pnv_smp_kick_cpu(int nr)
 	/* On OPAL v2 the CPU are still spinning inside OPAL itself,
 	 * get them back now
 	 */
-	if (firmware_has_feature(FW_FEATURE_OPALv2)) {
+	if (!paca[nr].cpu_start && firmware_has_feature(FW_FEATURE_OPALv2)) {
 		pr_devel("OPAL: Starting CPU %d (HW 0x%x)...\n", nr, pcpu);
 		rc = opal_start_cpu(pcpu, start_here);
 		if (rc != OPAL_SUCCESS)

commit 14a43e69ed257a1fadadf9fea2c05adb1686419f
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Sep 19 17:44:57 2011 +0000

    powerpc/powernv: Basic support for OPAL
    
    Add definition of OPAL interfaces along with  the wrappers to call
    into OPAL runtime and the early device-tree parsing hook to locate
    the OPAL runtime firmware.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 4f4ec3797eb6..e87736685243 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -30,6 +30,7 @@
 #include <asm/vdso_datapage.h>
 #include <asm/cputhreads.h>
 #include <asm/xics.h>
+#include <asm/opal.h>
 
 #include "powernv.h"
 
@@ -62,6 +63,28 @@ static int pnv_smp_cpu_bootable(unsigned int nr)
 	return 1;
 }
 
+int __devinit pnv_smp_kick_cpu(int nr)
+{
+	unsigned int pcpu = get_hard_smp_processor_id(nr);
+	unsigned long start_here = __pa(*((unsigned long *)
+					  generic_secondary_smp_init));
+	long rc;
+
+	BUG_ON(nr < 0 || nr >= NR_CPUS);
+
+	/* On OPAL v2 the CPU are still spinning inside OPAL itself,
+	 * get them back now
+	 */
+	if (firmware_has_feature(FW_FEATURE_OPALv2)) {
+		pr_devel("OPAL: Starting CPU %d (HW 0x%x)...\n", nr, pcpu);
+		rc = opal_start_cpu(pcpu, start_here);
+		if (rc != OPAL_SUCCESS)
+			pr_warn("OPAL Error %ld starting CPU %d\n",
+				rc, nr);
+	}
+	return smp_generic_kick_cpu(nr);
+}
+
 #ifdef CONFIG_HOTPLUG_CPU
 
 static int pnv_smp_cpu_disable(void)
@@ -127,7 +150,7 @@ static struct smp_ops_t pnv_smp_ops = {
 	.message_pass	= smp_muxed_ipi_message_pass,
 	.cause_ipi	= NULL,	/* Filled at runtime by xics_smp_probe() */
 	.probe		= xics_smp_probe,
-	.kick_cpu	= smp_generic_kick_cpu,
+	.kick_cpu	= pnv_smp_kick_cpu,
 	.setup_cpu	= pnv_smp_setup_cpu,
 	.cpu_bootable	= pnv_smp_cpu_bootable,
 #ifdef CONFIG_HOTPLUG_CPU

commit 344eb010b2e399069bac474a9fd0ba04908a2601
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Sep 19 17:44:54 2011 +0000

    powerpc/powernv: Add CPU hotplug support
    
    Unplugged CPU go into NAP mode in a loop until woken up
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
index 36c715102c58..4f4ec3797eb6 100644
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -33,7 +33,14 @@
 
 #include "powernv.h"
 
-static void __devinit pnv_smp_setup_cpu(int cpu)
+#ifdef DEBUG
+#include <asm/udbg.h>
+#define DBG(fmt...) udbg_printf(fmt)
+#else
+#define DBG(fmt...)
+#endif
+
+static void __cpuinit pnv_smp_setup_cpu(int cpu)
 {
 	if (cpu != boot_cpuid)
 		xics_setup_cpu();
@@ -55,6 +62,67 @@ static int pnv_smp_cpu_bootable(unsigned int nr)
 	return 1;
 }
 
+#ifdef CONFIG_HOTPLUG_CPU
+
+static int pnv_smp_cpu_disable(void)
+{
+	int cpu = smp_processor_id();
+
+	/* This is identical to pSeries... might consolidate by
+	 * moving migrate_irqs_away to a ppc_md with default to
+	 * the generic fixup_irqs. --BenH.
+	 */
+	set_cpu_online(cpu, false);
+	vdso_data->processorCount--;
+	if (cpu == boot_cpuid)
+		boot_cpuid = cpumask_any(cpu_online_mask);
+	xics_migrate_irqs_away();
+	return 0;
+}
+
+static void pnv_smp_cpu_kill_self(void)
+{
+	unsigned int cpu;
+
+	/* If powersave_nap is enabled, use NAP mode, else just
+	 * spin aimlessly
+	 */
+	if (!powersave_nap) {
+		generic_mach_cpu_die();
+		return;
+	}
+
+	/* Standard hot unplug procedure */
+	local_irq_disable();
+	idle_task_exit();
+	current->active_mm = NULL; /* for sanity */
+	cpu = smp_processor_id();
+	DBG("CPU%d offline\n", cpu);
+	generic_set_cpu_dead(cpu);
+	smp_wmb();
+
+	/* We don't want to take decrementer interrupts while we are offline,
+	 * so clear LPCR:PECE1. We keep PECE2 enabled.
+	 */
+	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
+	while (!generic_check_cpu_restart(cpu)) {
+		power7_idle();
+		if (!generic_check_cpu_restart(cpu)) {
+			DBG("CPU%d Unexpected exit while offline !\n", cpu);
+			/* We may be getting an IPI, so we re-enable
+			 * interrupts to process it, it will be ignored
+			 * since we aren't online (hopefully)
+			 */
+			local_irq_enable();
+			local_irq_disable();
+		}
+	}
+	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) | LPCR_PECE1);
+	DBG("CPU%d coming online...\n", cpu);
+}
+
+#endif /* CONFIG_HOTPLUG_CPU */
+
 static struct smp_ops_t pnv_smp_ops = {
 	.message_pass	= smp_muxed_ipi_message_pass,
 	.cause_ipi	= NULL,	/* Filled at runtime by xics_smp_probe() */
@@ -62,6 +130,10 @@ static struct smp_ops_t pnv_smp_ops = {
 	.kick_cpu	= smp_generic_kick_cpu,
 	.setup_cpu	= pnv_smp_setup_cpu,
 	.cpu_bootable	= pnv_smp_cpu_bootable,
+#ifdef CONFIG_HOTPLUG_CPU
+	.cpu_disable	= pnv_smp_cpu_disable,
+	.cpu_die	= generic_cpu_die,
+#endif /* CONFIG_HOTPLUG_CPU */
 };
 
 /* This is called very early during platform setup_arch */
@@ -80,4 +152,8 @@ void __init pnv_smp_init(void)
 		smp_ops->take_timebase = rtas_take_timebase;
 	}
 #endif /* CONFIG_PPC_RTAS */
+
+#ifdef CONFIG_HOTPLUG_CPU
+	ppc_md.cpu_die	= pnv_smp_cpu_kill_self;
+#endif
 }

commit 55190f88789ab62a42c3ee050090406b0bcefff8
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Sep 19 17:44:52 2011 +0000

    powerpc: Add skeleton PowerNV platform
    
    This adds a skeletton for the new Power "Non Virtualized"
    platform which will be used by machines supporting running
    without an hypervisor, for example in order to run KVM.
    
    These machines will be using a new firmware called OPAL
    for which the support will be provided by later patches.
    
    The PowerNV platform is intended to be also usable under
    the BML environment used internally for early CPU bringup
    which is why the code also supports using RTAS instead of
    OPAL in various places.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/platforms/powernv/smp.c b/arch/powerpc/platforms/powernv/smp.c
new file mode 100644
index 000000000000..36c715102c58
--- /dev/null
+++ b/arch/powerpc/platforms/powernv/smp.c
@@ -0,0 +1,83 @@
+/*
+ * SMP support for PowerNV machines.
+ *
+ * Copyright 2011 IBM Corp.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/smp.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/spinlock.h>
+#include <linux/cpu.h>
+
+#include <asm/irq.h>
+#include <asm/smp.h>
+#include <asm/paca.h>
+#include <asm/machdep.h>
+#include <asm/cputable.h>
+#include <asm/firmware.h>
+#include <asm/system.h>
+#include <asm/rtas.h>
+#include <asm/vdso_datapage.h>
+#include <asm/cputhreads.h>
+#include <asm/xics.h>
+
+#include "powernv.h"
+
+static void __devinit pnv_smp_setup_cpu(int cpu)
+{
+	if (cpu != boot_cpuid)
+		xics_setup_cpu();
+}
+
+static int pnv_smp_cpu_bootable(unsigned int nr)
+{
+	/* Special case - we inhibit secondary thread startup
+	 * during boot if the user requests it.
+	 */
+	if (system_state < SYSTEM_RUNNING && cpu_has_feature(CPU_FTR_SMT)) {
+		if (!smt_enabled_at_boot && cpu_thread_in_core(nr) != 0)
+			return 0;
+		if (smt_enabled_at_boot
+		    && cpu_thread_in_core(nr) >= smt_enabled_at_boot)
+			return 0;
+	}
+
+	return 1;
+}
+
+static struct smp_ops_t pnv_smp_ops = {
+	.message_pass	= smp_muxed_ipi_message_pass,
+	.cause_ipi	= NULL,	/* Filled at runtime by xics_smp_probe() */
+	.probe		= xics_smp_probe,
+	.kick_cpu	= smp_generic_kick_cpu,
+	.setup_cpu	= pnv_smp_setup_cpu,
+	.cpu_bootable	= pnv_smp_cpu_bootable,
+};
+
+/* This is called very early during platform setup_arch */
+void __init pnv_smp_init(void)
+{
+	smp_ops = &pnv_smp_ops;
+
+	/* XXX We don't yet have a proper entry point from HAL, for
+	 * now we rely on kexec-style entry from BML
+	 */
+
+#ifdef CONFIG_PPC_RTAS
+	/* Non-lpar has additional take/give timebase */
+	if (rtas_token("freeze-time-base") != RTAS_UNKNOWN_SERVICE) {
+		smp_ops->give_timebase = rtas_give_timebase;
+		smp_ops->take_timebase = rtas_take_timebase;
+	}
+#endif /* CONFIG_PPC_RTAS */
+}
