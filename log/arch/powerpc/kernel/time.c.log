commit abc3fce76adbdfa8f87272c784b388cd20b46049
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Apr 2 22:04:01 2020 +1000

    Revert "powerpc/64: irq_work avoid interrupt when called with hardware irqs enabled"
    
    This reverts commit ebb37cf3ffd39fdb6ec5b07111f8bb2f11d92c5f.
    
    That commit does not play well with soft-masked irq state
    manipulations in idle, interrupt replay, and possibly others due to
    tracing code sometimes using irq_work_queue (e.g., in
    trace_hardirqs_on()). That can cause PACA_IRQ_DEC to become set when
    it is not expected, and be ignored or cleared or cause warnings.
    
    The net result seems to be missing an irq_work until the next timer
    interrupt in the worst case which is usually not going to be noticed,
    however it could be a long time if the tick is disabled, which is
    against the spirit of irq_work and might cause real problems.
    
    The idea is still solid, but it would need more work. It's not really
    clear if it would be worth added complexity, so revert this for
    now (not a straight revert, but replace with a comment explaining why
    we might see interrupts happening, and gives git blame something to
    find).
    
    Fixes: ebb37cf3ffd3 ("powerpc/64: irq_work avoid interrupt when called with hardware irqs enabled")
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200402120401.1115883-1-npiggin@gmail.com

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 2d9d3a3c61d6..6fcae436ae51 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -522,35 +522,6 @@ static inline void clear_irq_work_pending(void)
 		"i" (offsetof(struct paca_struct, irq_work_pending)));
 }
 
-void arch_irq_work_raise(void)
-{
-	preempt_disable();
-	set_irq_work_pending_flag();
-	/*
-	 * Non-nmi code running with interrupts disabled will replay
-	 * irq_happened before it re-enables interrupts, so setthe
-	 * decrementer there instead of causing a hardware exception
-	 * which would immediately hit the masked interrupt handler
-	 * and have the net effect of setting the decrementer in
-	 * irq_happened.
-	 *
-	 * NMI interrupts can not check this when they return, so the
-	 * decrementer hardware exception is raised, which will fire
-	 * when interrupts are next enabled.
-	 *
-	 * BookE does not support this yet, it must audit all NMI
-	 * interrupt handlers to ensure they call nmi_enter() so this
-	 * check would be correct.
-	 */
-	if (IS_ENABLED(CONFIG_BOOKE) || !irqs_disabled() || in_nmi()) {
-		set_dec(1);
-	} else {
-		hard_irq_disable();
-		local_paca->irq_happened |= PACA_IRQ_DEC;
-	}
-	preempt_enable();
-}
-
 #else /* 32-bit */
 
 DEFINE_PER_CPU(u8, irq_work_pending);
@@ -559,16 +530,27 @@ DEFINE_PER_CPU(u8, irq_work_pending);
 #define test_irq_work_pending()		__this_cpu_read(irq_work_pending)
 #define clear_irq_work_pending()	__this_cpu_write(irq_work_pending, 0)
 
+#endif /* 32 vs 64 bit */
+
 void arch_irq_work_raise(void)
 {
+	/*
+	 * 64-bit code that uses irq soft-mask can just cause an immediate
+	 * interrupt here that gets soft masked, if this is called under
+	 * local_irq_disable(). It might be possible to prevent that happening
+	 * by noticing interrupts are disabled and setting decrementer pending
+	 * to be replayed when irqs are enabled. The problem there is that
+	 * tracing can call irq_work_raise, including in code that does low
+	 * level manipulations of irq soft-mask state (e.g., trace_hardirqs_on)
+	 * which could get tangled up if we're messing with the same state
+	 * here.
+	 */
 	preempt_disable();
 	set_irq_work_pending_flag();
 	set_dec(1);
 	preempt_enable();
 }
 
-#endif /* 32 vs 64 bit */
-
 #else  /* CONFIG_IRQ_WORK */
 
 #define test_irq_work_pending()	0

commit 60083063b755e29685902609e024ecd0c4a1a7d9
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Thu Feb 13 09:38:04 2020 +0100

    powerpc/time: Replace <linux/clk-provider.h> by <linux/of_clk.h>
    
    The PowerPC time code is not a clock provider, and just needs to call
    of_clk_init().
    
    Hence it can include <linux/of_clk.h> instead of <linux/clk-provider.h>.
    
    Remove the #ifdef protecting the of_clk_init() call, as a stub is
    available for the !CONFIG_COMMON_CLK case.
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Reviewed-by: Stephen Boyd <sboyd@kernel.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200213083804.24315-1-geert+renesas@glider.be

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index bda9cb4a0a5f..2d9d3a3c61d6 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -50,7 +50,7 @@
 #include <linux/irq.h>
 #include <linux/delay.h>
 #include <linux/irq_work.h>
-#include <linux/clk-provider.h>
+#include <linux/of_clk.h>
 #include <linux/suspend.h>
 #include <linux/sched/cputime.h>
 #include <linux/processor.h>
@@ -1149,9 +1149,7 @@ void __init time_init(void)
 	init_decrementer_clockevent();
 	tick_setup_hrtimer_broadcast();
 
-#ifdef CONFIG_COMMON_CLK
 	of_clk_init(NULL);
-#endif
 }
 
 /*

commit 2babd6ea43edacfc1577432baa187a7d212f3f4f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 26 03:35:25 2020 +1000

    powerpc/64s/exception: Avoid touching the stack in hdecrementer
    
    The hdec interrupt handler is reported to sometimes fire in Linux if
    KVM leaves it pending after a guest exists. This is harmless, so there
    is a no-op handler for it.
    
    The interrupt handler currently uses the regular kernel stack. Change
    this to avoid touching the stack entirely.
    
    This should be the last place where the regular Linux stack can be
    accessed with asynchronous interrupts (including PMI) soft-masked.
    It might be possible to take advantage of this invariant, e.g., to
    context switch the kernel stack SLB entry without clearing MSR[EE].
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200225173541.1549955-17-npiggin@gmail.com

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 1168e8b37e30..bda9cb4a0a5f 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -663,15 +663,6 @@ void timer_broadcast_interrupt(void)
 }
 #endif
 
-/*
- * Hypervisor decrementer interrupts shouldn't occur but are sometimes
- * left pending on exit from a KVM guest.  We don't need to do anything
- * to clear them, as they are edge-triggered.
- */
-void hdec_interrupt(struct pt_regs *regs)
-{
-}
-
 #ifdef CONFIG_SUSPEND
 static void generic_suspend_disable_irqs(void)
 {

commit 552263456215ada7ee8700ce022d12b0cffe4802
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Mon Dec 2 07:57:29 2019 +0000

    powerpc: Fix vDSO clock_getres()
    
    clock_getres in the vDSO library has to preserve the same behaviour
    of posix_get_hrtimer_res().
    
    In particular, posix_get_hrtimer_res() does:
        sec = 0;
        ns = hrtimer_resolution;
    and hrtimer_resolution depends on the enablement of the high
    resolution timers that can happen either at compile or at run time.
    
    Fix the powerpc vdso implementation of clock_getres keeping a copy of
    hrtimer_resolution in vdso data and using that directly.
    
    Fixes: a7f290dad32e ("[PATCH] powerpc: Merge vdso's and add vdso support to 32 bits kernel")
    Cc: stable@vger.kernel.org
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Acked-by: Shuah Khan <skhan@linuxfoundation.org>
    [chleroy: changed CLOCK_REALTIME_RES to CLOCK_HRTIMER_RES]
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/a55eca3a5e85233838c2349783bcb5164dae1d09.1575273217.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 2d13cea13954..1168e8b37e30 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -960,6 +960,7 @@ void update_vsyscall(struct timekeeper *tk)
 	vdso_data->stamp_xtime_sec = xt.tv_sec;
 	vdso_data->stamp_xtime_nsec = xt.tv_nsec;
 	vdso_data->stamp_sec_fraction = frac_sec;
+	vdso_data->hrtimer_res = hrtimer_resolution;
 	smp_wmb();
 	++(vdso_data->tb_update_count);
 }

commit ceb307474506f888e8f16dab183405ff01dffa08
Merge: 0da522107e5d b111df8447ac
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 1 14:00:59 2019 -0800

    Merge tag 'y2038-cleanups-5.5' of git://git.kernel.org:/pub/scm/linux/kernel/git/arnd/playground
    
    Pull y2038 cleanups from Arnd Bergmann:
     "y2038 syscall implementation cleanups
    
      This is a series of cleanups for the y2038 work, mostly intended for
      namespace cleaning: the kernel defines the traditional time_t, timeval
      and timespec types that often lead to y2038-unsafe code. Even though
      the unsafe usage is mostly gone from the kernel, having the types and
      associated functions around means that we can still grow new users,
      and that we may be missing conversions to safe types that actually
      matter.
    
      There are still a number of driver specific patches needed to get the
      last users of these types removed, those have been submitted to the
      respective maintainers"
    
    Link: https://lore.kernel.org/lkml/20191108210236.1296047-1-arnd@arndb.de/
    
    * tag 'y2038-cleanups-5.5' of git://git.kernel.org:/pub/scm/linux/kernel/git/arnd/playground: (26 commits)
      y2038: alarm: fix half-second cut-off
      y2038: ipc: fix x32 ABI breakage
      y2038: fix typo in powerpc vdso "LOPART"
      y2038: allow disabling time32 system calls
      y2038: itimer: change implementation to timespec64
      y2038: move itimer reset into itimer.c
      y2038: use compat_{get,set}_itimer on alpha
      y2038: itimer: compat handling to itimer.c
      y2038: time: avoid timespec usage in settimeofday()
      y2038: timerfd: Use timespec64 internally
      y2038: elfcore: Use __kernel_old_timeval for process times
      y2038: make ns_to_compat_timeval use __kernel_old_timeval
      y2038: socket: use __kernel_old_timespec instead of timespec
      y2038: socket: remove timespec reference in timestamping
      y2038: syscalls: change remaining timeval to __kernel_old_timeval
      y2038: rusage: use __kernel_old_timeval
      y2038: uapi: change __kernel_time_t to __kernel_old_time_t
      y2038: stat: avoid 'time_t' in 'struct stat'
      y2038: ipc: remove __kernel_time_t reference from headers
      y2038: vdso: powerpc: avoid timespec references
      ...

commit 7794b1d4185e2587af46435e3e2f6696dae314c7
Merge: 9dd0013824fc 2807273f5e88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 30 14:35:43 2019 -0800

    Merge tag 'powerpc-5.5-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights:
    
       - Infrastructure for secure boot on some bare metal Power9 machines.
         The firmware support is still in development, so the code here
         won't actually activate secure boot on any existing systems.
    
       - A change to xmon (our crash handler / pseudo-debugger) to restrict
         it to read-only mode when the kernel is lockdown'ed, otherwise it's
         trivial to drop into xmon and modify kernel data, such as the
         lockdown state.
    
       - Support for KASLR on 32-bit BookE machines (Freescale / NXP).
    
       - Fixes for our flush_icache_range() and __kernel_sync_dicache()
         (VDSO) to work with memory ranges >4GB.
    
       - Some reworks of the pseries CMM (Cooperative Memory Management)
         driver to make it behave more like other balloon drivers and enable
         some cleanups of generic mm code.
    
       - A series of fixes to our hardware breakpoint support to properly
         handle unaligned watchpoint addresses.
    
      Plus a bunch of other smaller improvements, fixes and cleanups.
    
      Thanks to: Alastair D'Silva, Andrew Donnellan, Aneesh Kumar K.V,
      Anthony Steinhauser, CÃ©dric Le Goater, Chris Packham, Chris Smart,
      Christophe Leroy, Christopher M. Riedl, Christoph Hellwig, Claudio
      Carvalho, Daniel Axtens, David Hildenbrand, Deb McLemore, Diana
      Craciun, Eric Richter, Geert Uytterhoeven, Greg Kroah-Hartman, Greg
      Kurz, Gustavo L. F. Walbon, Hari Bathini, Harish, Jason Yan, Krzysztof
      Kozlowski, Leonardo Bras, Mathieu Malaterre, Mauro S. M. Rodrigues,
      Michal Suchanek, Mimi Zohar, Nathan Chancellor, Nathan Lynch, Nayna
      Jain, Nick Desaulniers, Oliver O'Halloran, Qian Cai, Rasmus Villemoes,
      Ravi Bangoria, Sam Bobroff, Santosh Sivaraj, Scott Wood, Thomas Huth,
      Tyrel Datwyler, Vaibhav Jain, Valentin Longchamp, YueHaibing"
    
    * tag 'powerpc-5.5-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (144 commits)
      powerpc/fixmap: fix crash with HIGHMEM
      x86/efi: remove unused variables
      powerpc: Define arch_is_kernel_initmem_freed() for lockdep
      powerpc/prom_init: Use -ffreestanding to avoid a reference to bcmp
      powerpc: Avoid clang warnings around setjmp and longjmp
      powerpc: Don't add -mabi= flags when building with Clang
      powerpc: Fix Kconfig indentation
      powerpc/fixmap: don't clear fixmap area in paging_init()
      selftests/powerpc: spectre_v2 test must be built 64-bit
      powerpc/powernv: Disable native PCIe port management
      powerpc/kexec: Move kexec files into a dedicated subdir.
      powerpc/32: Split kexec low level code out of misc_32.S
      powerpc/sysdev: drop simple gpio
      powerpc/83xx: map IMMR with a BAT.
      powerpc/32s: automatically allocate BAT in setbat()
      powerpc/ioremap: warn on early use of ioremap()
      powerpc: Add support for GENERIC_EARLY_IOREMAP
      powerpc/fixmap: Use __fix_to_virt() instead of fix_to_virt()
      powerpc/8xx: use the fixmapped IMMR in cpm_reset()
      powerpc/8xx: add __init to cpm1 init functions
      ...

commit 176ed98c8a76ee08babf99b25b00992c2a5e7bbc
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Sun Oct 27 17:26:55 2019 +0100

    y2038: vdso: powerpc: avoid timespec references
    
    As a preparation to stop using 'struct timespec' in the kernel,
    change the powerpc vdso implementation:
    
    - split up the vdso data definition to have equivalent members
       for seconds and nanoseconds instead of an xtime structure
    
    - use timespec64 as an intermediate for the xtime update
    
    - change the asm-offsets definition to be based the appropriate
      fixed-length types
    
    This is only a temporary fix for changing the types, in order
    to actually support a 64-bit safe vdso32 version of clock_gettime(),
    the entire powerpc vdso should be replaced with the generic
    lib/vdso/ implementation. If that happens first, this patch
    becomes obsolete.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 694522308cd5..f1db1bf2dd91 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -885,7 +885,7 @@ static notrace u64 timebase_read(struct clocksource *cs)
 
 void update_vsyscall(struct timekeeper *tk)
 {
-	struct timespec xt;
+	struct timespec64 xt;
 	struct clocksource *clock = tk->tkr_mono.clock;
 	u32 mult = tk->tkr_mono.mult;
 	u32 shift = tk->tkr_mono.shift;
@@ -957,7 +957,8 @@ void update_vsyscall(struct timekeeper *tk)
 	vdso_data->tb_to_xs = new_tb_to_xs;
 	vdso_data->wtom_clock_sec = tk->wall_to_monotonic.tv_sec;
 	vdso_data->wtom_clock_nsec = tk->wall_to_monotonic.tv_nsec;
-	vdso_data->stamp_xtime = xt;
+	vdso_data->stamp_xtime_sec = xt.tv_sec;
+	vdso_data->stamp_xtime_nsec = xt.tv_nsec;
 	vdso_data->stamp_sec_fraction = frac_sec;
 	smp_wmb();
 	++(vdso_data->tb_update_count);

commit eb8e20f89093b64f48975c74ccb114e6775cee22
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sun Oct 13 21:23:51 2019 +1100

    powerpc/pseries: Mark accumulate_stolen_time() as notrace
    
    accumulate_stolen_time() is called prior to interrupt state being
    reconciled, which can trip the warning in arch_local_irq_restore():
    
      WARNING: CPU: 5 PID: 1017 at arch/powerpc/kernel/irq.c:258 .arch_local_irq_restore+0x9c/0x130
      ...
      NIP .arch_local_irq_restore+0x9c/0x130
      LR  .rb_start_commit+0x38/0x80
      Call Trace:
        .ring_buffer_lock_reserve+0xe4/0x620
        .trace_function+0x44/0x210
        .function_trace_call+0x148/0x170
        .ftrace_ops_no_ops+0x180/0x1d0
        ftrace_call+0x4/0x8
        .accumulate_stolen_time+0x1c/0xb0
        decrementer_common+0x124/0x160
    
    For now just mark it as notrace. We may change the ordering to call it
    after interrupt state has been reconciled, but that is a larger
    change.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20191024055932.27940-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 694522308cd5..968ae97382b4 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -232,7 +232,7 @@ static u64 scan_dispatch_log(u64 stop_tb)
  * Accumulate stolen time by scanning the dispatch trace log.
  * Called on entry from user mode.
  */
-void accumulate_stolen_time(void)
+void notrace accumulate_stolen_time(void)
 {
 	u64 sst, ust;
 	unsigned long save_irq_soft_mask = irq_soft_mask_return();

commit f83eeb1a01689b2691f6f56629ac9f66de8d41c2
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Thu Oct 3 18:17:44 2019 +0200

    sched/cputime: Rename vtime_account_system() to vtime_account_kernel()
    
    vtime_account_system() decides if we need to account the time to the
    system (__vtime_account_system()) or to the guest (vtime_account_guest()).
    
    So this function is a misnomer as we are on a higher level than
    "system". All we know when we call that function is that we are
    accounting kernel cputime. Whether it belongs to guest or system time
    is a lower level detail.
    
    Rename this function to vtime_account_kernel(). This will clarify things
    and avoid too many underscored vtime_account_system() versions.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wanpeng Li <wanpengli@tencent.com>
    Cc: Yauheni Kaliuta <yauheni.kaliuta@redhat.com>
    Link: https://lkml.kernel.org/r/20191003161745.28464-2-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 694522308cd5..84827da01d45 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -338,7 +338,7 @@ static unsigned long vtime_delta(struct task_struct *tsk,
 	return stime;
 }
 
-void vtime_account_system(struct task_struct *tsk)
+void vtime_account_kernel(struct task_struct *tsk)
 {
 	unsigned long stime, stime_scaled, steal_time;
 	struct cpu_accounting_data *acct = get_accounting(tsk);
@@ -366,7 +366,7 @@ void vtime_account_system(struct task_struct *tsk)
 #endif
 	}
 }
-EXPORT_SYMBOL_GPL(vtime_account_system);
+EXPORT_SYMBOL_GPL(vtime_account_kernel);
 
 void vtime_account_idle(struct task_struct *tsk)
 {
@@ -395,7 +395,7 @@ static void vtime_flush_scaled(struct task_struct *tsk,
 /*
  * Account the whole cputime accumulated in the paca
  * Must be called with interrupts disabled.
- * Assumes that vtime_account_system/idle() has been called
+ * Assumes that vtime_account_kernel/idle() has been called
  * recently (i.e. since the last entry from usermode) so that
  * get_paca()->user_time_scaled is up to date.
  */

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 325d60633dfa..694522308cd5 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * Common time routines among all ppc machines.
  *
@@ -24,11 +25,6 @@
  *
  * 1997-09-10  Updated NTP code according to technical memorandum Jan '96
  *             "A Kernel Model for Precision Timekeeping" by Dave Mills
- *
- *      This program is free software; you can redistribute it and/or
- *      modify it under the terms of the GNU General Public License
- *      as published by the Free Software Foundation; either version
- *      2 of the License, or (at your option) any later version.
  */
 
 #include <linux/errno.h>

commit de269129a48a2d590ba1d20c719e19d86e3ddb3f
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Tue Mar 5 01:12:19 2019 +0530

    powerpc/hmi: Fix kernel hang when TB is in error state.
    
    On TOD/TB errors timebase register stops/freezes until HMI error recovery
    gets TOD/TB back into running state. On successful recovery, TB starts
    running again and udelay() that relies on TB value continues to function
    properly. But in case when HMI fails to recover from TOD/TB errors, the
    TB register stay freezed. With TB not running the __delay() function
    keeps looping and never return. If __delay() is called while in panic
    path then system hangs and never reboots after panic.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 6ef32472ee1d..325d60633dfa 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -150,6 +150,8 @@ EXPORT_SYMBOL_GPL(ppc_proc_freq);
 unsigned long ppc_tb_freq;
 EXPORT_SYMBOL_GPL(ppc_tb_freq);
 
+bool tb_invalid;
+
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
 /*
  * Factor for converting from cputime_t (timebase ticks) to
@@ -459,6 +461,13 @@ void __delay(unsigned long loops)
 				diff += 1000000000;
 			spin_cpu_relax();
 		} while (diff < loops);
+	} else if (tb_invalid) {
+		/*
+		 * TB is in error state and isn't ticking anymore.
+		 * HMI handler was unable to recover from TB error.
+		 * Return immediately, so that kernel won't get stuck here.
+		 */
+		spin_cpu_relax();
 	} else {
 		start = get_tbl();
 		while (get_tbl() - start < loops)

commit 6917735e8f905da1f62ccdf62830b185524835c7
Author: Jagadeesh Pagadala <jagdsh.linux@gmail.com>
Date:   Sat Mar 23 18:20:55 2019 +0530

    powerpc: Remove duplicate headers
    
    Remove duplicate headers inclusions.
    
    Signed-off-by: Jagadeesh Pagadala <jagdsh.linux@gmail.com>
    Reviewed-by: Mukesh Ojha <mojha@codeaurora.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index bc0503ef9c9c..6ef32472ee1d 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -43,7 +43,6 @@
 #include <linux/timex.h>
 #include <linux/kernel_stat.h>
 #include <linux/time.h>
-#include <linux/clockchips.h>
 #include <linux/init.h>
 #include <linux/profile.h>
 #include <linux/cpu.h>

commit 75f8a37580b64f87c223fbd08db6b2f79129864a
Author: Brajeswar Ghosh <brajeswar.linux@gmail.com>
Date:   Mon Jan 28 21:41:36 2019 +0530

    powerpc/kernel/time: Remove duplicate header
    
    Remove linux/rtc.h which is included more than once
    
    Signed-off-by: Brajeswar Ghosh <brajeswar.linux@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 3646affae963..bc0503ef9c9c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -57,7 +57,6 @@
 #include <linux/irq_work.h>
 #include <linux/clk-provider.h>
 #include <linux/suspend.h>
-#include <linux/rtc.h>
 #include <linux/sched/cputime.h>
 #include <linux/processor.h>
 #include <asm/trace.h>

commit 51eeef9e135ac00cf706fad1a3bde775ca578462
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Aug 2 07:54:01 2018 +0000

    powerpc/time: no steal_time when CONFIG_PPC_SPLPAR is not selected
    
    If CONFIG_PPC_SPLPAR is not selected, steal_time will always
    be NUL, so accounting it is pointless
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 9289fac75af7..3646affae963 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -413,8 +413,10 @@ void vtime_flush(struct task_struct *tsk)
 	if (acct->gtime)
 		account_guest_time(tsk, cputime_to_nsecs(acct->gtime));
 
-	if (acct->steal_time)
+	if (IS_ENABLED(CONFIG_PPC_SPLPAR) && acct->steal_time) {
 		account_steal_time(cputime_to_nsecs(acct->steal_time));
+		acct->steal_time = 0;
+	}
 
 	if (acct->idle_time)
 		account_idle_time(cputime_to_nsecs(acct->idle_time));
@@ -434,7 +436,6 @@ void vtime_flush(struct task_struct *tsk)
 
 	acct->utime = 0;
 	acct->gtime = 0;
-	acct->steal_time = 0;
 	acct->idle_time = 0;
 	acct->stime = 0;
 	acct->hardirq_time = 0;

commit abcff86df2d2ec0a0ca9470fa5d2a184af18928a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Aug 2 07:53:59 2018 +0000

    powerpc/time: Only set CONFIG_ARCH_HAS_SCALED_CPUTIME on PPC64
    
    scaled cputime is only meaningfull when the processor has
    SPURR and/or PURR, which means only on PPC64.
    
    Removing it on PPC32 significantly reduces the size of
    vtime_account_system() and vtime_account_idle() on an 8xx:
    
    Before:
    00000000 l     F .text  000000a8 vtime_delta
    00000280 g     F .text  0000010c vtime_account_system
    0000038c g     F .text  00000048 vtime_account_idle
    
    After:
    (vtime_delta gets inlined inside the two functions)
    000001d8 g     F .text  000000a0 vtime_account_system
    00000278 g     F .text  00000038 vtime_account_idle
    
    In terms of performance, we also get approximatly 7% improvement on
    task switch. The following small benchmark app is run with perf stat:
    
    void *thread(void *arg)
    {
            int i;
    
            for (i = 0; i < atoi((char*)arg); i++)
                    pthread_yield();
    }
    
    int main(int argc, char **argv)
    {
            pthread_t th1, th2;
    
            pthread_create(&th1, NULL, thread, argv[1]);
            pthread_create(&th2, NULL, thread, argv[1]);
            pthread_join(th1, NULL);
            pthread_join(th2, NULL);
    
            return 0;
    }
    
    Before the patch:
    
     Performance counter stats for 'chrt -f 98 ./sched 100000' (50 runs):
    
           8228.476465      task-clock (msec)         #    0.954 CPUs utilized            ( +-  0.23% )
                200004      context-switches          #    0.024 M/sec                    ( +-  0.00% )
    
    After the patch:
    
     Performance counter stats for 'chrt -f 98 ./sched 100000' (50 runs):
    
           7649.070444      task-clock (msec)         #    0.955 CPUs utilized            ( +-  0.27% )
                200004      context-switches          #    0.026 M/sec                    ( +-  0.00% )
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index cf0d5c2834d0..9289fac75af7 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -176,7 +176,7 @@ static void calc_cputime_factors(void)
  * Read the SPURR on systems that have it, otherwise the PURR,
  * or if that doesn't exist return the timebase value passed in.
  */
-static unsigned long read_spurr(unsigned long tb)
+static inline unsigned long read_spurr(unsigned long tb)
 {
 	if (cpu_has_feature(CPU_FTR_SPURR))
 		return mfspr(SPRN_SPURR);
@@ -285,7 +285,8 @@ static inline u64 calculate_stolen_time(u64 stop_tb)
 static unsigned long vtime_delta_scaled(struct cpu_accounting_data *acct,
 					unsigned long now, unsigned long stime)
 {
-	unsigned long stime_scaled;
+	unsigned long stime_scaled = 0;
+#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
 	unsigned long nowscaled, deltascaled;
 	unsigned long utime, utime_scaled;
 
@@ -316,6 +317,7 @@ static unsigned long vtime_delta_scaled(struct cpu_accounting_data *acct,
 		}
 	}
 	acct->utime_scaled += utime_scaled;
+#endif
 
 	return stime_scaled;
 }
@@ -352,7 +354,9 @@ void vtime_account_system(struct task_struct *tsk)
 
 	if ((tsk->flags & PF_VCPU) && !irq_count()) {
 		acct->gtime += stime;
+#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
 		acct->utime_scaled += stime_scaled;
+#endif
 	} else {
 		if (hardirq_count())
 			acct->hardirq_time += stime;
@@ -361,7 +365,9 @@ void vtime_account_system(struct task_struct *tsk)
 		else
 			acct->stime += stime;
 
+#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
 		acct->stime_scaled += stime_scaled;
+#endif
 	}
 }
 EXPORT_SYMBOL_GPL(vtime_account_system);
@@ -378,6 +384,7 @@ void vtime_account_idle(struct task_struct *tsk)
 static void vtime_flush_scaled(struct task_struct *tsk,
 			       struct cpu_accounting_data *acct)
 {
+#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
 	if (acct->utime_scaled)
 		tsk->utimescaled += cputime_to_nsecs(acct->utime_scaled);
 	if (acct->stime_scaled)
@@ -386,6 +393,7 @@ static void vtime_flush_scaled(struct task_struct *tsk,
 	acct->utime_scaled = 0;
 	acct->utime_sspurr = 0;
 	acct->stime_scaled = 0;
+#endif
 }
 
 /*

commit b38a181c11d0b5e84b40732dbb06cc9d68140d60
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Aug 2 07:53:57 2018 +0000

    powerpc/time: isolate scaled cputime accounting in dedicated functions.
    
    scaled cputime is only meaningfull when the processor has
    SPURR and/or PURR, which means only on PPC64.
    
    In preparation of the following patch that will remove
    CONFIG_ARCH_HAS_SCALED_CPUTIME on PPC32, this patch moves
    all scaled cputing accounting logic into dedicated functions.
    
    This patch doesn't change any functionality. It's only code
    reorganisation.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 68e8f963d108..cf0d5c2834d0 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -282,26 +282,16 @@ static inline u64 calculate_stolen_time(u64 stop_tb)
  * Account time for a transition between system, hard irq
  * or soft irq state.
  */
-static unsigned long vtime_delta(struct task_struct *tsk,
-				 unsigned long *stime_scaled,
-				 unsigned long *steal_time)
+static unsigned long vtime_delta_scaled(struct cpu_accounting_data *acct,
+					unsigned long now, unsigned long stime)
 {
-	unsigned long now, nowscaled, deltascaled;
-	unsigned long stime;
+	unsigned long stime_scaled;
+	unsigned long nowscaled, deltascaled;
 	unsigned long utime, utime_scaled;
-	struct cpu_accounting_data *acct = get_accounting(tsk);
 
-	WARN_ON_ONCE(!irqs_disabled());
-
-	now = mftb();
 	nowscaled = read_spurr(now);
-	stime = now - acct->starttime;
-	acct->starttime = now;
 	deltascaled = nowscaled - acct->startspurr;
 	acct->startspurr = nowscaled;
-
-	*steal_time = calculate_stolen_time(now);
-
 	utime = acct->utime - acct->utime_sspurr;
 	acct->utime_sspurr = acct->utime;
 
@@ -315,18 +305,38 @@ static unsigned long vtime_delta(struct task_struct *tsk,
 	 * the user ticks get saved up in paca->user_time_scaled to be
 	 * used by account_process_tick.
 	 */
-	*stime_scaled = stime;
+	stime_scaled = stime;
 	utime_scaled = utime;
 	if (deltascaled != stime + utime) {
 		if (utime) {
-			*stime_scaled = deltascaled * stime / (stime + utime);
-			utime_scaled = deltascaled - *stime_scaled;
+			stime_scaled = deltascaled * stime / (stime + utime);
+			utime_scaled = deltascaled - stime_scaled;
 		} else {
-			*stime_scaled = deltascaled;
+			stime_scaled = deltascaled;
 		}
 	}
 	acct->utime_scaled += utime_scaled;
 
+	return stime_scaled;
+}
+
+static unsigned long vtime_delta(struct task_struct *tsk,
+				 unsigned long *stime_scaled,
+				 unsigned long *steal_time)
+{
+	unsigned long now, stime;
+	struct cpu_accounting_data *acct = get_accounting(tsk);
+
+	WARN_ON_ONCE(!irqs_disabled());
+
+	now = mftb();
+	stime = now - acct->starttime;
+	acct->starttime = now;
+
+	*stime_scaled = vtime_delta_scaled(acct, now, stime);
+
+	*steal_time = calculate_stolen_time(now);
+
 	return stime;
 }
 
@@ -365,6 +375,19 @@ void vtime_account_idle(struct task_struct *tsk)
 	acct->idle_time += stime + steal_time;
 }
 
+static void vtime_flush_scaled(struct task_struct *tsk,
+			       struct cpu_accounting_data *acct)
+{
+	if (acct->utime_scaled)
+		tsk->utimescaled += cputime_to_nsecs(acct->utime_scaled);
+	if (acct->stime_scaled)
+		tsk->stimescaled += cputime_to_nsecs(acct->stime_scaled);
+
+	acct->utime_scaled = 0;
+	acct->utime_sspurr = 0;
+	acct->stime_scaled = 0;
+}
+
 /*
  * Account the whole cputime accumulated in the paca
  * Must be called with interrupts disabled.
@@ -379,9 +402,6 @@ void vtime_flush(struct task_struct *tsk)
 	if (acct->utime)
 		account_user_time(tsk, cputime_to_nsecs(acct->utime));
 
-	if (acct->utime_scaled)
-		tsk->utimescaled += cputime_to_nsecs(acct->utime_scaled);
-
 	if (acct->gtime)
 		account_guest_time(tsk, cputime_to_nsecs(acct->gtime));
 
@@ -394,8 +414,6 @@ void vtime_flush(struct task_struct *tsk)
 	if (acct->stime)
 		account_system_index_time(tsk, cputime_to_nsecs(acct->stime),
 					  CPUTIME_SYSTEM);
-	if (acct->stime_scaled)
-		tsk->stimescaled += cputime_to_nsecs(acct->stime_scaled);
 
 	if (acct->hardirq_time)
 		account_system_index_time(tsk, cputime_to_nsecs(acct->hardirq_time),
@@ -404,14 +422,13 @@ void vtime_flush(struct task_struct *tsk)
 		account_system_index_time(tsk, cputime_to_nsecs(acct->softirq_time),
 					  CPUTIME_SOFTIRQ);
 
+	vtime_flush_scaled(tsk, acct);
+
 	acct->utime = 0;
-	acct->utime_scaled = 0;
-	acct->utime_sspurr = 0;
 	acct->gtime = 0;
 	acct->steal_time = 0;
 	acct->idle_time = 0;
 	acct->stime = 0;
-	acct->stime_scaled = 0;
 	acct->hardirq_time = 0;
 	acct->softirq_time = 0;
 }

commit b4d16ab58c41ff0125822464bdff074cebd0fe47
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Oct 17 23:39:41 2018 +1100

    powerpc/time: Fix clockevent_decrementer initalisation for PR KVM
    
    In the recent commit 8b78fdb045de ("powerpc/time: Use
    clockevents_register_device(), fixing an issue with large
    decrementer") we changed the way we initialise the decrementer
    clockevent(s).
    
    We no longer initialise the mult & shift values of
    decrementer_clockevent itself.
    
    This has the effect of breaking PR KVM, because it uses those values
    in kvmppc_emulate_dec(). The symptom is guest kernels spin forever
    mid-way through boot.
    
    For now fix it by assigning back to decrementer_clockevent the mult
    and shift values.
    
    Fixes: 8b78fdb045de ("powerpc/time: Use clockevents_register_device(), fixing an issue with large decrementer")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 40868f3ee113..68e8f963d108 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -989,6 +989,10 @@ static void register_decrementer_clockevent(int cpu)
 
 	printk_once(KERN_DEBUG "clockevent: %s mult[%x] shift[%d] cpu[%d]\n",
 		    dec->name, dec->mult, dec->shift, cpu);
+
+	/* Set values for KVM, see kvm_emulate_dec() */
+	decrementer_clockevent.mult = dec->mult;
+	decrementer_clockevent.shift = dec->shift;
 }
 
 static void enable_large_decrementer(void)

commit 817593604e5b244dc55344d298d96122457c20bd
Author: Anton Blanchard <anton@ozlabs.org>
Date:   Tue Oct 2 09:01:05 2018 +1000

    powerpc/time: Add set_state_oneshot_stopped decrementer callback
    
    If CONFIG_PPC_WATCHDOG is enabled we always cap the decrementer to
    0x7fffffff:
    
           if (IS_ENABLED(CONFIG_PPC_WATCHDOG))
                    set_dec(0x7fffffff);
            else
                    set_dec(decrementer_max);
    
    If there are no future events, we don't reprogram the decrementer
    after this and we end up with 0x7fffffff even on a large decrementer
    capable system.
    
    As suggested by Nick, add a set_state_oneshot_stopped callback
    so we program the decrementer with decrementer_max if there are
    no future events.
    
    Signed-off-by: Anton Blanchard <anton@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 6a1f0a084ca3..40868f3ee113 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -111,6 +111,7 @@ struct clock_event_device decrementer_clockevent = {
 	.rating			= 200,
 	.irq			= 0,
 	.set_next_event		= decrementer_set_next_event,
+	.set_state_oneshot_stopped = decrementer_shutdown,
 	.set_state_shutdown	= decrementer_shutdown,
 	.tick_resume		= decrementer_shutdown,
 	.features		= CLOCK_EVT_FEAT_ONESHOT |

commit 8b78fdb045de60a4eb35460092bbd3cffa925353
Author: Anton Blanchard <anton@ozlabs.org>
Date:   Tue Oct 2 09:01:04 2018 +1000

    powerpc/time: Use clockevents_register_device(), fixing an issue with large decrementer
    
    We currently cap the decrementer clockevent at 4 seconds, even on systems
    with large decrementer support. Fix this by converting the code to use
    clockevents_register_device() which calculates the upper bound based on
    the max_delta passed in.
    
    Signed-off-by: Anton Blanchard <anton@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 70f145e02487..6a1f0a084ca3 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -984,10 +984,10 @@ static void register_decrementer_clockevent(int cpu)
 	*dec = decrementer_clockevent;
 	dec->cpumask = cpumask_of(cpu);
 
+	clockevents_config_and_register(dec, ppc_tb_freq, 2, decrementer_max);
+
 	printk_once(KERN_DEBUG "clockevent: %s mult[%x] shift[%d] cpu[%d]\n",
 		    dec->name, dec->mult, dec->shift, cpu);
-
-	clockevents_register_device(dec);
 }
 
 static void enable_large_decrementer(void)
@@ -1035,18 +1035,7 @@ static void __init set_decrementer_max(void)
 
 static void __init init_decrementer_clockevent(void)
 {
-	int cpu = smp_processor_id();
-
-	clockevents_calc_mult_shift(&decrementer_clockevent, ppc_tb_freq, 4);
-
-	decrementer_clockevent.max_delta_ns =
-		clockevent_delta2ns(decrementer_max, &decrementer_clockevent);
-	decrementer_clockevent.max_delta_ticks = decrementer_max;
-	decrementer_clockevent.min_delta_ns =
-		clockevent_delta2ns(2, &decrementer_clockevent);
-	decrementer_clockevent.min_delta_ticks = 2;
-
-	register_decrementer_clockevent(cpu);
+	register_decrementer_clockevent(smp_processor_id());
 }
 
 void secondary_cpu_time_init(void)

commit 60f1d2893ee6de65cdea609c84950b133e76a769
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue May 29 16:19:14 2018 +0000

    powerpc/time: inline arch_vtime_task_switch()
    
    arch_vtime_task_switch() is a small function which is called
    only from vtime_common_task_switch(), so it is worth inlining
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 2530cf60b839..70f145e02487 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -163,12 +163,6 @@ EXPORT_SYMBOL(__cputime_usec_factor);
 void (*dtl_consumer)(struct dtl_entry *, u64);
 #endif
 
-#ifdef CONFIG_PPC64
-#define get_accounting(tsk)	(&get_paca()->accounting)
-#else
-#define get_accounting(tsk)	(&task_thread_info(tsk)->accounting)
-#endif
-
 static void calc_cputime_factors(void)
 {
 	struct div_result res;
@@ -421,21 +415,6 @@ void vtime_flush(struct task_struct *tsk)
 	acct->softirq_time = 0;
 }
 
-#ifdef CONFIG_PPC32
-/*
- * Called from the context switch with interrupts disabled, to charge all
- * accumulated times to the current process, and to prepare accounting on
- * the next process.
- */
-void arch_vtime_task_switch(struct task_struct *prev)
-{
-	struct cpu_accounting_data *acct = get_accounting(current);
-
-	acct->starttime = get_accounting(prev)->starttime;
-	acct->startspurr = get_accounting(prev)->startspurr;
-}
-#endif /* CONFIG_PPC32 */
-
 #else /* ! CONFIG_VIRT_CPU_ACCOUNTING_NATIVE */
 #define calc_cputime_factors()
 #endif

commit 34efabe418953002d1c8e0ab28634929a9ddf433
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Apr 23 10:36:42 2018 +0200

    powerpc: remove unused to_tm() helper
    
    to_tm() is now completely unused, the only reference being in the
    _dump_time() helper that is also unused. This removes both, leaving
    the rest of the powerpc RTC code y2038 safe to as far as the hardware
    supports.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index b025acb32bfe..2530cf60b839 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1159,56 +1159,6 @@ void __init time_init(void)
 #endif
 }
 
-
-#define FEBRUARY	2
-#define	STARTOFTIME	1970
-#define SECDAY		86400L
-#define SECYR		(SECDAY * 365)
-#define	leapyear(year)		((year) % 4 == 0 && \
-				 ((year) % 100 != 0 || (year) % 400 == 0))
-#define	days_in_year(a) 	(leapyear(a) ? 366 : 365)
-#define	days_in_month(a) 	(month_days[(a) - 1])
-
-static int month_days[12] = {
-	31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31
-};
-
-void to_tm(int tim, struct rtc_time * tm)
-{
-	register int    i;
-	register long   hms, day;
-
-	day = tim / SECDAY;
-	hms = tim % SECDAY;
-
-	/* Hours, minutes, seconds are easy */
-	tm->tm_hour = hms / 3600;
-	tm->tm_min = (hms % 3600) / 60;
-	tm->tm_sec = (hms % 3600) % 60;
-
-	/* Number of years in days */
-	for (i = STARTOFTIME; day >= days_in_year(i); i++)
-		day -= days_in_year(i);
-	tm->tm_year = i;
-
-	/* Number of months in days left */
-	if (leapyear(tm->tm_year))
-		days_in_month(FEBRUARY) = 29;
-	for (i = 1; day >= days_in_month(i); i++)
-		day -= days_in_month(i);
-	days_in_month(FEBRUARY) = 28;
-	tm->tm_mon = i;
-
-	/* Days are what is left over (+1) from all that. */
-	tm->tm_mday = day + 1;
-
-	/*
-	 * No-one uses the day of the week.
-	 */
-	tm->tm_wday = -1;
-}
-EXPORT_SYMBOL(to_tm);
-
 /*
  * Divide a 128-bit dividend by a 32-bit divisor, leaving a 128 bit
  * result.

commit 5235afa89a246c9d5ab35996bc38681c474c3ed7
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Apr 23 10:36:41 2018 +0200

    powerpc: use time64_t in update_persistent_clock
    
    update_persistent_clock() is deprecated because it suffers from overflow
    in 2038 on 32-bit architectures. This changes powerpc to use the
    update_persistent_clock64() replacement, and to pass down 64-bit
    timestamps consistently.
    
    This is now simpler, as we no longer have to worry about the offset
    numbers in tm_year and tm_mon that are different between the Linux
    conventions and RTAS.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index da06a4d84ae2..b025acb32bfe 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -811,16 +811,14 @@ void __init generic_calibrate_decr(void)
 	}
 }
 
-int update_persistent_clock(struct timespec now)
+int update_persistent_clock64(struct timespec64 now)
 {
 	struct rtc_time tm;
 
 	if (!ppc_md.set_rtc_time)
 		return -ENODEV;
 
-	to_tm(now.tv_sec + 1 + timezone_offset, &tm);
-	tm.tm_year -= 1900;
-	tm.tm_mon -= 1;
+	rtc_time64_to_tm(now.tv_sec + 1 + timezone_offset, &tm);
 
 	return ppc_md.set_rtc_time(&tm);
 }

commit 5bfd643583b2e2a203163fd6b617cd9027054200
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Apr 23 10:36:40 2018 +0200

    powerpc: use time64_t in read_persistent_clock
    
    Looking through the remaining users of the deprecated mktime()
    function, I found the powerpc rtc handlers, which use it in
    place of rtc_tm_to_time64().
    
    To clean this up, I'm changing over the read_persistent_clock()
    function to the read_persistent_clock64() variant, and change
    all the platform specific handlers along with it.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index ed6b2abdde15..da06a4d84ae2 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -825,7 +825,7 @@ int update_persistent_clock(struct timespec now)
 	return ppc_md.set_rtc_time(&tm);
 }
 
-static void __read_persistent_clock(struct timespec *ts)
+static void __read_persistent_clock(struct timespec64 *ts)
 {
 	struct rtc_time tm;
 	static int first = 1;
@@ -849,11 +849,10 @@ static void __read_persistent_clock(struct timespec *ts)
 	}
 	ppc_md.get_rtc_time(&tm);
 
-	ts->tv_sec = mktime(tm.tm_year+1900, tm.tm_mon+1, tm.tm_mday,
-			    tm.tm_hour, tm.tm_min, tm.tm_sec);
+	ts->tv_sec = rtc_tm_to_time64(&tm);
 }
 
-void read_persistent_clock(struct timespec *ts)
+void read_persistent_clock64(struct timespec64 *ts)
 {
 	__read_persistent_clock(ts);
 

commit e360cd37f0e9bac7b5f623132549e2d4b6417399
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:35 2018 +1000

    powerpc/time: account broadcast timer event interrupts separately
    
    These are not local timer interrupts but IPIs. It's good to be able
    to see how timer offloading is behaving, so split these out into
    their own category.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 23921f7b6e67..ed6b2abdde15 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -647,13 +647,10 @@ EXPORT_SYMBOL(timer_interrupt);
 void timer_broadcast_interrupt(void)
 {
 	u64 *next_tb = this_cpu_ptr(&decrementers_next_tb);
-	struct pt_regs *regs = get_irq_regs();
 
-	trace_timer_interrupt_entry(regs);
 	*next_tb = ~(u64)0;
 	tick_receive_broadcast();
-	__this_cpu_inc(irq_stat.timer_irqs_event);
-	trace_timer_interrupt_exit(regs);
+	__this_cpu_inc(irq_stat.broadcast_irqs_event);
 }
 #endif
 

commit bc9071133144acdbdb28cfc6ee5ce983d8fd5f81
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:33 2018 +1000

    powerpc: move timer broadcast code under GENERIC_CLOCKEVENTS_BROADCAST ifdef
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 5862a3611795..23921f7b6e67 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -643,6 +643,7 @@ void timer_interrupt(struct pt_regs *regs)
 }
 EXPORT_SYMBOL(timer_interrupt);
 
+#ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 void timer_broadcast_interrupt(void)
 {
 	u64 *next_tb = this_cpu_ptr(&decrementers_next_tb);
@@ -654,6 +655,7 @@ void timer_broadcast_interrupt(void)
 	__this_cpu_inc(irq_stat.timer_irqs_event);
 	trace_timer_interrupt_exit(regs);
 }
+#endif
 
 /*
  * Hypervisor decrementer interrupts shouldn't occur but are sometimes

commit a7cba02deceda96df8018a827e6715d6f37be7b5
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:32 2018 +1000

    powerpc: allow soft-NMI watchdog to cover timer interrupts with large decrementers
    
    Large decrementers (e.g., POWER9) can take a very long time to wrap,
    so when the timer iterrupt handler sets the decrementer to max so as
    to avoid taking another decrementer interrupt when hard enabling
    interrupts before running timers, it effectively disables the soft
    NMI coverage for timer interrupts.
    
    Fix this by using the traditional 31-bit value instead, which wraps
    after a few seconds. masked interrupt code does the same thing, and
    in normal operation neither of these paths would ever wrap even the
    31 bit value.
    
    Note: the SMP watchdog should catch timer interrupt lockups, but it
    is preferable for the local soft-NMI to catch them, mainly to avoid
    the IPI.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index ad876906f847..5862a3611795 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -578,22 +578,29 @@ void timer_interrupt(struct pt_regs *regs)
 	struct pt_regs *old_regs;
 	u64 now;
 
-	/* Ensure a positive value is written to the decrementer, or else
-	 * some CPUs will continue to take decrementer exceptions.
-	 */
-	set_dec(decrementer_max);
-
 	/* Some implementations of hotplug will get timer interrupts while
 	 * offline, just ignore these and we also need to set
 	 * decrementers_next_tb as MAX to make sure __check_irq_replay
 	 * don't replay timer interrupt when return, otherwise we'll trap
 	 * here infinitely :(
 	 */
-	if (!cpu_online(smp_processor_id())) {
+	if (unlikely(!cpu_online(smp_processor_id()))) {
 		*next_tb = ~(u64)0;
+		set_dec(decrementer_max);
 		return;
 	}
 
+	/* Ensure a positive value is written to the decrementer, or else
+	 * some CPUs will continue to take decrementer exceptions. When the
+	 * PPC_WATCHDOG (decrementer based) is configured, keep this at most
+	 * 31 bits, which is about 4 seconds on most systems, which gives
+	 * the watchdog a chance of catching timer interrupt hard lockups.
+	 */
+	if (IS_ENABLED(CONFIG_PPC_WATCHDOG))
+		set_dec(0x7fffffff);
+	else
+		set_dec(decrementer_max);
+
 	/* Conditionally hard-enable interrupts now that the DEC has been
 	 * bumped to its maximum value
 	 */

commit 3f984620f9a4fe089c0a3c951b75a460211394bb
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:31 2018 +1000

    powerpc: generic clockevents broadcast receiver call tick_receive_broadcast
    
    The broadcast tick recipient can call tick_receive_broadcast rather
    than re-running the full timer interrupt.
    
    It does not have to check for the next event time, because the sender
    already determined the timer has expired. It does not have to test
    irq_work_pending, because that's a direct decrementer interrupt and
    does not go through the clock events subsystem. And it does not have
    to read PURR because that was removed with the previous patch.
    
    This results in no code size change, but both the decrementer and
    broadcast path lengths are reduced.
    
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 1fe6a24357e7..ad876906f847 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -567,47 +567,16 @@ void arch_irq_work_raise(void)
 
 #endif /* CONFIG_IRQ_WORK */
 
-static void __timer_interrupt(void)
-{
-	struct pt_regs *regs = get_irq_regs();
-	u64 *next_tb = this_cpu_ptr(&decrementers_next_tb);
-	struct clock_event_device *evt = this_cpu_ptr(&decrementers);
-	u64 now;
-
-	trace_timer_interrupt_entry(regs);
-
-	if (test_irq_work_pending()) {
-		clear_irq_work_pending();
-		irq_work_run();
-	}
-
-	now = get_tb_or_rtc();
-	if (now >= *next_tb) {
-		*next_tb = ~(u64)0;
-		if (evt->event_handler)
-			evt->event_handler(evt);
-		__this_cpu_inc(irq_stat.timer_irqs_event);
-	} else {
-		now = *next_tb - now;
-		if (now <= decrementer_max)
-			set_dec(now);
-		/* We may have raced with new irq work */
-		if (test_irq_work_pending())
-			set_dec(1);
-		__this_cpu_inc(irq_stat.timer_irqs_others);
-	}
-
-	trace_timer_interrupt_exit(regs);
-}
-
 /*
  * timer_interrupt - gets called when the decrementer overflows,
  * with interrupts disabled.
  */
-void timer_interrupt(struct pt_regs * regs)
+void timer_interrupt(struct pt_regs *regs)
 {
-	struct pt_regs *old_regs;
+	struct clock_event_device *evt = this_cpu_ptr(&decrementers);
 	u64 *next_tb = this_cpu_ptr(&decrementers_next_tb);
+	struct pt_regs *old_regs;
+	u64 now;
 
 	/* Ensure a positive value is written to the decrementer, or else
 	 * some CPUs will continue to take decrementer exceptions.
@@ -638,13 +607,47 @@ void timer_interrupt(struct pt_regs * regs)
 
 	old_regs = set_irq_regs(regs);
 	irq_enter();
+	trace_timer_interrupt_entry(regs);
+
+	if (test_irq_work_pending()) {
+		clear_irq_work_pending();
+		irq_work_run();
+	}
+
+	now = get_tb_or_rtc();
+	if (now >= *next_tb) {
+		*next_tb = ~(u64)0;
+		if (evt->event_handler)
+			evt->event_handler(evt);
+		__this_cpu_inc(irq_stat.timer_irqs_event);
+	} else {
+		now = *next_tb - now;
+		if (now <= decrementer_max)
+			set_dec(now);
+		/* We may have raced with new irq work */
+		if (test_irq_work_pending())
+			set_dec(1);
+		__this_cpu_inc(irq_stat.timer_irqs_others);
+	}
 
-	__timer_interrupt();
+	trace_timer_interrupt_exit(regs);
 	irq_exit();
 	set_irq_regs(old_regs);
 }
 EXPORT_SYMBOL(timer_interrupt);
 
+void timer_broadcast_interrupt(void)
+{
+	u64 *next_tb = this_cpu_ptr(&decrementers_next_tb);
+	struct pt_regs *regs = get_irq_regs();
+
+	trace_timer_interrupt_entry(regs);
+	*next_tb = ~(u64)0;
+	tick_receive_broadcast();
+	__this_cpu_inc(irq_stat.timer_irqs_event);
+	trace_timer_interrupt_exit(regs);
+}
+
 /*
  * Hypervisor decrementer interrupts shouldn't occur but are sometimes
  * left pending on exit from a KVM guest.  We don't need to do anything
@@ -992,15 +995,6 @@ static int decrementer_shutdown(struct clock_event_device *dev)
 	return 0;
 }
 
-/* Interrupt handler for the timer broadcast IPI */
-void tick_broadcast_ipi_handler(void)
-{
-	u64 *next_tb = this_cpu_ptr(&decrementers_next_tb);
-
-	*next_tb = get_tb_or_rtc();
-	__timer_interrupt();
-}
-
 static void register_decrementer_clockevent(int cpu)
 {
 	struct clock_event_device *dec = &per_cpu(decrementers, cpu);

commit 3d3a6021ddcbe9c31520e4e7b65e5ce5dc58274d
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:30 2018 +1000

    powerpc/pseries: lparcfg calculate PURR on demand
    
    For SPLPAR, lparcfg provides a sum of PURR registers for all CPUs.
    Currently this is done by reading PURR in context switch and timer
    interrupt, and storing that into a per-CPU variable. These are summed
    to provide the value.
    
    This does not work with all timer schemes (e.g., NO_HZ_FULL), and it
    is sub-optimal for performance because it reads the PURR register on
    every context switch, although that's been difficult to distinguish
    from noise in the contxt_switch microbenchmark.
    
    This patch implements the sum by calling a function on each CPU, to
    read and add PURR values of each CPU.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index e7e8611e8863..1fe6a24357e7 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -597,14 +597,6 @@ static void __timer_interrupt(void)
 		__this_cpu_inc(irq_stat.timer_irqs_others);
 	}
 
-#ifdef CONFIG_PPC64
-	/* collect purr register values often, for accurate calculations */
-	if (firmware_has_feature(FW_FEATURE_SPLPAR)) {
-		struct cpu_usage *cu = this_cpu_ptr(&cpu_usage_array);
-		cu->current_tb = mfspr(SPRN_PURR);
-	}
-#endif
-
 	trace_timer_interrupt_exit(regs);
 }
 

commit ebb37cf3ffd39fdb6ec5b07111f8bb2f11d92c5f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:25 2018 +1000

    powerpc/64: irq_work avoid interrupt when called with hardware irqs enabled
    
    irq_work_raise should not cause a decrementer exception unless it is
    called from NMI context. Doing so often just results in an immediate
    masked decrementer interrupt:
    
       <...>-550    90d...    4us : update_curr_rt <-dequeue_task_rt
       <...>-550    90d...    5us : dbs_update_util_handler <-update_curr_rt
       <...>-550    90d...    6us : arch_irq_work_raise <-irq_work_queue
       <...>-550    90d...    7us : soft_nmi_interrupt <-soft_nmi_common
       <...>-550    90d...    7us : printk_nmi_enter <-soft_nmi_interrupt
       <...>-550    90d.Z.    8us : rcu_nmi_enter <-soft_nmi_interrupt
       <...>-550    90d.Z.    9us : rcu_nmi_exit <-soft_nmi_interrupt
       <...>-550    90d...    9us : printk_nmi_exit <-soft_nmi_interrupt
       <...>-550    90d...   10us : cpuacct_charge <-update_curr_rt
    
    The soft_nmi_interrupt here is the call into the watchdog, due to the
    decrementer interrupt firing with irqs soft-disabled. This is
    harmless, but sub-optimal.
    
    When it's not called from NMI context or with interrupts enabled, mark
    the decrementer pending in the irq_happened mask directly, rather than
    having the masked decrementer interupt handler do it. This will be
    replayed at the next local_irq_enable. See the comment for details.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 360e71d455cc..e7e8611e8863 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -513,6 +513,35 @@ static inline void clear_irq_work_pending(void)
 		"i" (offsetof(struct paca_struct, irq_work_pending)));
 }
 
+void arch_irq_work_raise(void)
+{
+	preempt_disable();
+	set_irq_work_pending_flag();
+	/*
+	 * Non-nmi code running with interrupts disabled will replay
+	 * irq_happened before it re-enables interrupts, so setthe
+	 * decrementer there instead of causing a hardware exception
+	 * which would immediately hit the masked interrupt handler
+	 * and have the net effect of setting the decrementer in
+	 * irq_happened.
+	 *
+	 * NMI interrupts can not check this when they return, so the
+	 * decrementer hardware exception is raised, which will fire
+	 * when interrupts are next enabled.
+	 *
+	 * BookE does not support this yet, it must audit all NMI
+	 * interrupt handlers to ensure they call nmi_enter() so this
+	 * check would be correct.
+	 */
+	if (IS_ENABLED(CONFIG_BOOKE) || !irqs_disabled() || in_nmi()) {
+		set_dec(1);
+	} else {
+		hard_irq_disable();
+		local_paca->irq_happened |= PACA_IRQ_DEC;
+	}
+	preempt_enable();
+}
+
 #else /* 32-bit */
 
 DEFINE_PER_CPU(u8, irq_work_pending);
@@ -521,8 +550,6 @@ DEFINE_PER_CPU(u8, irq_work_pending);
 #define test_irq_work_pending()		__this_cpu_read(irq_work_pending)
 #define clear_irq_work_pending()	__this_cpu_write(irq_work_pending, 0)
 
-#endif /* 32 vs 64 bit */
-
 void arch_irq_work_raise(void)
 {
 	preempt_disable();
@@ -531,6 +558,8 @@ void arch_irq_work_raise(void)
 	preempt_enable();
 }
 
+#endif /* 32 vs 64 bit */
+
 #else  /* CONFIG_IRQ_WORK */
 
 #define test_irq_work_pending()	0

commit a6201da34ff9366680e97392efd06abb9ff15014
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Mon Apr 2 13:03:37 2018 +0530

    powerpc: Fix oops due to bad access of lppaca on bare metal
    
    Commit 8e0b634b1327 ("powerpc/64s: Do not allocate lppaca if we are
    not virtualized") removed allocation of lppaca on bare metal
    platforms. But with CONFIG_PPC_SPLPAR enabled, we still access the
    lppaca on bare metal in some code paths.
    
    Fix this but adding runtime checks for SPLPAR (shared processor LPAR).
    
    Fixes: 8e0b634b1327 ("powerpc/64s: Do not allocate lppaca if we are not virtualized")
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index f7d96a68ecaa..360e71d455cc 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -266,6 +266,9 @@ void accumulate_stolen_time(void)
 
 static inline u64 calculate_stolen_time(u64 stop_tb)
 {
+	if (!firmware_has_feature(FW_FEATURE_SPLPAR))
+		return 0;
+
 	if (get_paca()->dtl_ridx != be64_to_cpu(get_lppaca()->dtl_idx))
 		return scan_dispatch_log(stop_tb);
 

commit 890ae7979758568734881ad0f382c4064e2386c3
Author: Alexandre Belloni <alexandre.belloni@bootlin.com>
Date:   Wed Feb 21 22:46:33 2018 +0100

    powerpc/time: stop validating rtc_time in .read_time
    
    The RTC core is always calling rtc_valid_tm after the read_time callback.
    It is not necessary to call it just before returning from the callback.
    
    Signed-off-by: Alexandre Belloni <alexandre.belloni@bootlin.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index a32823dcd9a4..f7d96a68ecaa 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1234,7 +1234,7 @@ void calibrate_delay(void)
 static int rtc_generic_get_time(struct device *dev, struct rtc_time *tm)
 {
 	ppc_md.get_rtc_time(tm);
-	return rtc_valid_tm(tm);
+	return 0;
 }
 
 static int rtc_generic_set_time(struct device *dev, struct rtc_time *tm)

commit 4e26bc4a4ed683c42ba45f09050575a671c6f1f4
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:50 2017 +0530

    powerpc/64: Rename soft_enabled to irq_soft_mask
    
    Rename the paca->soft_enabled to paca->irq_soft_mask as it is no
    longer used as a flag for interrupt state, but a mask.
    
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index daa6e9e35ab9..a32823dcd9a4 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -244,7 +244,7 @@ static u64 scan_dispatch_log(u64 stop_tb)
 void accumulate_stolen_time(void)
 {
 	u64 sst, ust;
-	unsigned long save_soft_enabled = soft_enabled_return();
+	unsigned long save_irq_soft_mask = irq_soft_mask_return();
 	struct cpu_accounting_data *acct = &local_paca->accounting;
 
 	/* We are called early in the exception entry, before
@@ -253,7 +253,7 @@ void accumulate_stolen_time(void)
 	 * needs to reflect that so various debug stuff doesn't
 	 * complain
 	 */
-	soft_enabled_set(IRQS_DISABLED);
+	irq_soft_mask_set(IRQS_DISABLED);
 
 	sst = scan_dispatch_log(acct->starttime_user);
 	ust = scan_dispatch_log(acct->starttime);
@@ -261,7 +261,7 @@ void accumulate_stolen_time(void)
 	acct->utime -= ust;
 	acct->steal_time += ust + sst;
 
-	soft_enabled_set(save_soft_enabled);
+	irq_soft_mask_set(save_irq_soft_mask);
 }
 
 static inline u64 calculate_stolen_time(u64 stop_tb)

commit e0b5687bed13ddbf99985d77910b9adfd429bf12
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:46 2017 +0530

    powerpc/64: Implement and use soft_enabled_return API
    
    Add a new wrapper function, soft_enabled_return(), added to return
    paca->soft_enabled value.
    
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 320b8459c74e..daa6e9e35ab9 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -244,7 +244,7 @@ static u64 scan_dispatch_log(u64 stop_tb)
 void accumulate_stolen_time(void)
 {
 	u64 sst, ust;
-	u8 save_soft_enabled = local_paca->soft_enabled;
+	unsigned long save_soft_enabled = soft_enabled_return();
 	struct cpu_accounting_data *acct = &local_paca->accounting;
 
 	/* We are called early in the exception entry, before

commit 0b63acf4a0eb8843f83954ea1bd29ccdfcbaa778
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:45 2017 +0530

    powerpc/64: Move set_soft_enabled() and rename
    
    Move set_soft_enabled() from powerpc/kernel/irq.c to asm/hw_irq.c, to
    encourage updates to paca->soft_enabled done via these access
    function. Add "memory" clobber to hint compiler since
    paca->soft_enabled memory is the target here.
    
    Renaming it as soft_enabled_set() will make namespaces works better as
    prefix than a postfix when new soft_enabled manipulation functions are
    introduced.
    
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 070092b1ba8a..320b8459c74e 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -253,7 +253,7 @@ void accumulate_stolen_time(void)
 	 * needs to reflect that so various debug stuff doesn't
 	 * complain
 	 */
-	local_paca->soft_enabled = IRQS_DISABLED;
+	soft_enabled_set(IRQS_DISABLED);
 
 	sst = scan_dispatch_log(acct->starttime_user);
 	ust = scan_dispatch_log(acct->starttime);
@@ -261,7 +261,7 @@ void accumulate_stolen_time(void)
 	acct->utime -= ust;
 	acct->steal_time += ust + sst;
 
-	local_paca->soft_enabled = save_soft_enabled;
+	soft_enabled_set(save_soft_enabled);
 }
 
 static inline u64 calculate_stolen_time(u64 stop_tb)

commit c2e480ba822718190e58849b79a76db13c3dac18
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:42 2017 +0530

    powerpc/64: Add #defines for paca->soft_enabled flags
    
    Two #defines IRQS_ENABLED and IRQS_DISABLED are added to be used when
    updating paca->soft_enabled. Replace the hardcoded values used when
    updating paca->soft_enabled with IRQ_(EN|DIS)ABLED #define. No logic
    change.
    
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index fe6f3a285455..070092b1ba8a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -253,7 +253,7 @@ void accumulate_stolen_time(void)
 	 * needs to reflect that so various debug stuff doesn't
 	 * complain
 	 */
-	local_paca->soft_enabled = 0;
+	local_paca->soft_enabled = IRQS_DISABLED;
 
 	sst = scan_dispatch_log(acct->starttime_user);
 	ust = scan_dispatch_log(acct->starttime);

commit 4e287e655e108cbbd6e3e7dcc49d591c8aa5a8a4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Jun 6 23:08:32 2017 +1000

    powerpc: use spin loop primitives in some functions
    
    Use the different spin loop primitives in some simple powerpc
    spin loops, including those which will spin as a common case.
    
    This will help to test the spin loop primitives before more
    conversions are done.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Add some includes of <linux/processor.h>]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 0cc0dad905d5..fe6f3a285455 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -59,10 +59,10 @@
 #include <linux/suspend.h>
 #include <linux/rtc.h>
 #include <linux/sched/cputime.h>
+#include <linux/processor.h>
 #include <asm/trace.h>
 
 #include <asm/io.h>
-#include <asm/processor.h>
 #include <asm/nvram.h>
 #include <asm/cache.h>
 #include <asm/machdep.h>
@@ -442,6 +442,7 @@ void __delay(unsigned long loops)
 	unsigned long start;
 	int diff;
 
+	spin_begin();
 	if (__USE_RTC()) {
 		start = get_rtcl();
 		do {
@@ -449,13 +450,14 @@ void __delay(unsigned long loops)
 			diff = get_rtcl() - start;
 			if (diff < 0)
 				diff += 1000000000;
+			spin_cpu_relax();
 		} while (diff < loops);
 	} else {
 		start = get_tbl();
 		while (get_tbl() - start < loops)
-			HMT_low();
-		HMT_medium();
+			spin_cpu_relax();
 	}
+	spin_end();
 }
 EXPORT_SYMBOL(__delay);
 

commit d4cfb11387ee29ba4626546c676fd25c7abbbbb2
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Sat May 27 18:04:52 2017 +1000

    powerpc: Convert VDSO update function to use new update_vsyscall interface
    
    This converts the powerpc VDSO time update function to use the new
    interface introduced in commit 576094b7f0aa ("time: Introduce new
    GENERIC_TIME_VSYSCALL", 2012-09-11).  Where the old interface gave
    us the time as of the last update in seconds and whole nanoseconds,
    with the new interface we get the nanoseconds part effectively in
    a binary fixed-point format with tk->tkr_mono.shift bits to the
    right of the binary point.
    
    With the old interface, the fractional nanoseconds got truncated,
    meaning that the value returned by the VDSO clock_gettime function
    would have about 1ns of jitter in it compared to the value computed
    by the generic timekeeping code in the kernel.
    
    The powerpc VDSO time functions (clock_gettime and gettimeofday)
    already work in units of 2^-32 seconds, or 0.23283 ns, because that
    makes it simple to split the result into seconds and fractional
    seconds, and represent the fractional seconds in either microseconds
    or nanoseconds.  This is good enough accuracy for now, so this patch
    avoids changing how the VDSO works or the interface in the VDSO data
    page.
    
    This patch converts the powerpc update_vsyscall_old to be called
    update_vsyscall and use the new interface.  We convert the fractional
    second to units of 2^-32 seconds without truncating to whole nanoseconds.
    (There is still a conversion to whole nanoseconds for any legacy users
    of the vdso_data/systemcfg stamp_xtime field.)
    
    In addition, this improves the accuracy of the computation of tb_to_xs
    for those systems with high-frequency timebase clocks (>= 268.5 MHz)
    by doing the right shift in two parts, one before the multiplication and
    one after, rather than doing the right shift before the multiplication.
    (We can't do all of the right shift after the multiplication unless we
    use 128-bit arithmetic.)
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 476a527b220d..0cc0dad905d5 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -841,28 +841,66 @@ static notrace u64 timebase_read(struct clocksource *cs)
 	return (u64)get_tb();
 }
 
-void update_vsyscall_old(struct timespec *wall_time, struct timespec *wtm,
-			 struct clocksource *clock, u32 mult, u64 cycle_last)
+
+void update_vsyscall(struct timekeeper *tk)
 {
+	struct timespec xt;
+	struct clocksource *clock = tk->tkr_mono.clock;
+	u32 mult = tk->tkr_mono.mult;
+	u32 shift = tk->tkr_mono.shift;
+	u64 cycle_last = tk->tkr_mono.cycle_last;
 	u64 new_tb_to_xs, new_stamp_xsec;
-	u32 frac_sec;
+	u64 frac_sec;
 
 	if (clock != &clocksource_timebase)
 		return;
 
+	xt.tv_sec = tk->xtime_sec;
+	xt.tv_nsec = (long)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
+
 	/* Make userspace gettimeofday spin until we're done. */
 	++vdso_data->tb_update_count;
 	smp_mb();
 
-	/* 19342813113834067 ~= 2^(20+64) / 1e9 */
-	new_tb_to_xs = (u64) mult * (19342813113834067ULL >> clock->shift);
-	new_stamp_xsec = (u64) wall_time->tv_nsec * XSEC_PER_SEC;
-	do_div(new_stamp_xsec, 1000000000);
-	new_stamp_xsec += (u64) wall_time->tv_sec * XSEC_PER_SEC;
+	/*
+	 * This computes ((2^20 / 1e9) * mult) >> shift as a
+	 * 0.64 fixed-point fraction.
+	 * The computation in the else clause below won't overflow
+	 * (as long as the timebase frequency is >= 1.049 MHz)
+	 * but loses precision because we lose the low bits of the constant
+	 * in the shift.  Note that 19342813113834067 ~= 2^(20+64) / 1e9.
+	 * For a shift of 24 the error is about 0.5e-9, or about 0.5ns
+	 * over a second.  (Shift values are usually 22, 23 or 24.)
+	 * For high frequency clocks such as the 512MHz timebase clock
+	 * on POWER[6789], the mult value is small (e.g. 32768000)
+	 * and so we can shift the constant by 16 initially
+	 * (295147905179 ~= 2^(20+64-16) / 1e9) and then do the
+	 * remaining shifts after the multiplication, which gives a
+	 * more accurate result (e.g. with mult = 32768000, shift = 24,
+	 * the error is only about 1.2e-12, or 0.7ns over 10 minutes).
+	 */
+	if (mult <= 62500000 && clock->shift >= 16)
+		new_tb_to_xs = ((u64) mult * 295147905179ULL) >> (clock->shift - 16);
+	else
+		new_tb_to_xs = (u64) mult * (19342813113834067ULL >> clock->shift);
+
+	/*
+	 * Compute the fractional second in units of 2^-32 seconds.
+	 * The fractional second is tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift
+	 * in nanoseconds, so multiplying that by 2^32 / 1e9 gives
+	 * it in units of 2^-32 seconds.
+	 * We assume shift <= 32 because clocks_calc_mult_shift()
+	 * generates shift values in the range 0 - 32.
+	 */
+	frac_sec = tk->tkr_mono.xtime_nsec << (32 - shift);
+	do_div(frac_sec, NSEC_PER_SEC);
 
-	BUG_ON(wall_time->tv_nsec >= NSEC_PER_SEC);
-	/* this is tv_nsec / 1e9 as a 0.32 fraction */
-	frac_sec = ((u64) wall_time->tv_nsec * 18446744073ULL) >> 32;
+	/*
+	 * Work out new stamp_xsec value for any legacy users of systemcfg.
+	 * stamp_xsec is in units of 2^-20 seconds.
+	 */
+	new_stamp_xsec = frac_sec >> 12;
+	new_stamp_xsec += tk->xtime_sec * XSEC_PER_SEC;
 
 	/*
 	 * tb_update_count is used to allow the userspace gettimeofday code
@@ -872,15 +910,13 @@ void update_vsyscall_old(struct timespec *wall_time, struct timespec *wtm,
 	 * the two values of tb_update_count match and are even then the
 	 * tb_to_xs and stamp_xsec values are consistent.  If not, then it
 	 * loops back and reads them again until this criteria is met.
-	 * We expect the caller to have done the first increment of
-	 * vdso_data->tb_update_count already.
 	 */
 	vdso_data->tb_orig_stamp = cycle_last;
 	vdso_data->stamp_xsec = new_stamp_xsec;
 	vdso_data->tb_to_xs = new_tb_to_xs;
-	vdso_data->wtom_clock_sec = wtm->tv_sec;
-	vdso_data->wtom_clock_nsec = wtm->tv_nsec;
-	vdso_data->stamp_xtime = *wall_time;
+	vdso_data->wtom_clock_sec = tk->wall_to_monotonic.tv_sec;
+	vdso_data->wtom_clock_nsec = tk->wall_to_monotonic.tv_nsec;
+	vdso_data->stamp_xtime = xt;
 	vdso_data->stamp_sec_fraction = frac_sec;
 	smp_wmb();
 	++(vdso_data->tb_update_count);

commit 6b847d795cf4ab3e574f4fcf7193fe245908a195
Author: Santosh Sivaraj <santosh@fossix.org>
Date:   Tue Jun 20 13:14:47 2017 +0530

    powerpc/time: Fix tracing in time.c
    
    Since trace_clock is in a different file and already marked with notrace,
    enable tracing in time.c by removing it from the disabled list in Makefile.
    Also annotate clocksource read functions and sched_clock with notrace.
    
    Testing: Timer and ftrace selftests run with different trace clocks.
    
    Acked-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Santosh Sivaraj <santosh@fossix.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 60714b8c9a2f..476a527b220d 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -675,7 +675,7 @@ EXPORT_SYMBOL_GPL(tb_to_ns);
  * the high 64 bits of a * b, i.e. (a * b) >> 64, where a and b
  * are 64-bit unsigned numbers.
  */
-unsigned long long sched_clock(void)
+notrace unsigned long long sched_clock(void)
 {
 	if (__USE_RTC())
 		return get_rtc();
@@ -831,12 +831,12 @@ void read_persistent_clock(struct timespec *ts)
 }
 
 /* clocksource code */
-static u64 rtc_read(struct clocksource *cs)
+static notrace u64 rtc_read(struct clocksource *cs)
 {
 	return (u64)get_rtc();
 }
 
-static u64 timebase_read(struct clocksource *cs)
+static notrace u64 timebase_read(struct clocksource *cs)
 {
 	return (u64)get_tb();
 }

commit 6e2f03e292ef46eed2b31b0a344a91d514f9cd81
Author: Ivan Mikhaylov <ivan@de.ibm.com>
Date:   Fri May 19 18:47:05 2017 +0300

    powerpc/[booke|4xx]: Don't clobber TCR[WP] when setting TCR[DIE]
    
    Prevent a kernel panic caused by unintentionally clearing TCR watchdog
    bits. At this point in the kernel boot, the watchdog may have already
    been enabled by u-boot. The original code's attempt to write to the TCR
    register results in an inadvertent clearing of the watchdog
    configuration bits, causing the 476 to reset.
    
    Signed-off-by: Ivan Mikhaylov <ivan@de.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 2b33cfaac7b8..60714b8c9a2f 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -739,12 +739,20 @@ static int __init get_freq(char *name, int cells, unsigned long *val)
 static void start_cpu_decrementer(void)
 {
 #if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
+	unsigned int tcr;
+
 	/* Clear any pending timer interrupts */
 	mtspr(SPRN_TSR, TSR_ENW | TSR_WIS | TSR_DIS | TSR_FIS);
 
-	/* Enable decrementer interrupt */
-	mtspr(SPRN_TCR, TCR_DIE);
-#endif /* defined(CONFIG_BOOKE) || defined(CONFIG_40x) */
+	tcr = mfspr(SPRN_TCR);
+	/*
+	 * The watchdog may have already been enabled by u-boot. So leave
+	 * TRC[WP] (Watchdog Period) alone.
+	 */
+	tcr &= TCR_WP_MASK;	/* Clear all bits except for TCR[WP] */
+	tcr |= TCR_DIE;		/* Enable decrementer */
+	mtspr(SPRN_TCR, tcr);
+#endif
 }
 
 void __init generic_calibrate_decr(void)

commit 115631c350bfd3b9cbc0aff75b3975e17c83d3d9
Author: Nicolai Stange <nicstange@gmail.com>
Date:   Thu Mar 30 21:53:17 2017 +0200

    powerpc/time: Set ->min_delta_ticks and ->max_delta_ticks
    
    In preparation for making the clockevents core NTP correction aware,
    all clockevent device drivers must set ->min_delta_ticks and
    ->max_delta_ticks rather than ->min_delta_ns and ->max_delta_ns: a
    clockevent device's rate is going to change dynamically and thus, the
    ratio of ns to ticks ceases to stay invariant.
    
    Make the powerpc arch's clockevent driver initialize these fields properly.
    
    This patch alone doesn't introduce any change in functionality as the
    clockevents core still looks exclusively at the (untouched) ->min_delta_ns
    and ->max_delta_ns. As soon as this has changed, a followup patch will
    purge the initialization of ->min_delta_ns and ->max_delta_ns from this
    driver.
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Oliver O'Halloran <oohall@gmail.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Signed-off-by: Nicolai Stange <nicstange@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 07b90725855e..2b33cfaac7b8 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -995,8 +995,10 @@ static void __init init_decrementer_clockevent(void)
 
 	decrementer_clockevent.max_delta_ns =
 		clockevent_delta2ns(decrementer_max, &decrementer_clockevent);
+	decrementer_clockevent.max_delta_ticks = decrementer_max;
 	decrementer_clockevent.min_delta_ns =
 		clockevent_delta2ns(2, &decrementer_clockevent);
+	decrementer_clockevent.min_delta_ticks = 2;
 
 	register_decrementer_clockevent(cpu);
 }

commit 32ef5517c298042ed58408545f475df43afe1f24
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 11:48:36 2017 +0100

    sched/headers: Prepare to move cputime functionality from <linux/sched.h> into <linux/sched/cputime.h>
    
    Introduce a trivial, mostly empty <linux/sched/cputime.h> header
    to prepare for the moving of cputime functionality out of sched.h.
    
    Update all code that relies on these facilities.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 37833c5dc274..07b90725855e 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -58,7 +58,7 @@
 #include <linux/clk-provider.h>
 #include <linux/suspend.h>
 #include <linux/rtc.h>
-#include <linux/cputime.h>
+#include <linux/sched/cputime.h>
 #include <asm/trace.h>
 
 #include <asm/io.h>

commit e601757102cfd3eeae068f53b3bc1234f3a2b2e9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/clock.h>
    
    We are going to split <linux/sched/clock.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and .c files.
    
    Create a trivial placeholder <linux/sched/clock.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index bc84a8d47b9e..37833c5dc274 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -34,6 +34,7 @@
 #include <linux/errno.h>
 #include <linux/export.h>
 #include <linux/sched.h>
+#include <linux/sched/clock.h>
 #include <linux/kernel.h>
 #include <linux/param.h>
 #include <linux/string.h>

commit b286cedd473006b33d5ae076afac509e6b2c3bf4
Merge: 522214d9be9c 9f3768e02335
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 1 10:10:16 2017 -0800

    Merge tag 'powerpc-4.11-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull more powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - an update of the disassembly code used by xmon to the latest
         versions in binutils. We've received permission from all the
         authors of the relevant binutils changes to relicense their changes
         to the relevant files from GPLv3 to GPLv2, for inclusion in Linux.
         Thanks to Peter Bergner for doing the leg work to get permission
         from everyone.
    
       - addition of the "architected" Power9 CPU table entry, allowing us
         to boot in Power9 architected mode under a hypervisor.
    
       - updates to the Power9 PMU code.
    
       - implementation of clear_bit_unlock_is_negative_byte() to optimise
         unlock_page().
    
       - Freescale updates from Scott: "Highlights include 8xx breakpoints
         and perf, t1042rdb display support, and board updates."
    
      Thanks to:
        Al Viro, Andrew Donnellan, Aneesh Kumar K.V, Balbir Singh, Douglas
        Miller, FrÃ©dÃ©ric Weisbecker, Gavin Shan, Madhavan Srinivasan,
        Michael Roth, Nathan Fontenot, Naveen N. Rao, Nicholas Piggin, Peter
        Bergner, Paul E. McKenney, Rashmica Gupta, Russell Currey, Sahil
        Mehta, Stewart Smith"
    
    * tag 'powerpc-4.11-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (48 commits)
      powerpc: Remove leftover cputime_to_nsecs call causing build error
      powerpc/mm/hash: Always clear UPRT and Host Radix bits when setting up CPU
      powerpc/optprobes: Fix TOC handling in optprobes trampoline
      powerpc/pseries: Advertise Hot Plug Event support to firmware
      cxl: fix nested locking hang during EEH hotplug
      powerpc/xmon: Dump memory in CPU endian format
      powerpc/pseries: Revert 'Auto-online hotplugged memory'
      powerpc/powernv: Make PCI non-optional
      powerpc/64: Implement clear_bit_unlock_is_negative_byte()
      powerpc/powernv: Remove unused variable in pnv_pci_sriov_disable()
      powerpc/kernel: Remove error message in pcibios_setup_phb_resources()
      powerpc/mm: Fix typo in set_pte_at()
      pci/hotplug/pnv-php: Disable MSI and PCI device properly
      pci/hotplug/pnv-php: Disable surprise hotplug capability on conflicts
      pci/hotplug/pnv-php: Remove WARN_ON() in pnv_php_put_slot()
      powerpc: Add POWER9 architected mode to cputable
      powerpc/perf: use is_kernel_addr macro in perf_get_misc_flags()
      powerpc/perf: Avoid FAB_*_MATCH checks for power9
      powerpc/perf: Add restrictions to PMC5 in power9 DD1
      powerpc/perf: Use Instruction Counter value
      ...

commit 9f3768e02335ddd6ebe1d85d5cb3a68ee6264004
Author: FrÃ©dÃ©ric Weisbecker <fweisbec@gmail.com>
Date:   Tue Feb 21 16:18:41 2017 +0100

    powerpc: Remove leftover cputime_to_nsecs call causing build error
    
    This type conversion is a leftover that got ignored during the kcpustat
    conversion to nanosecs, resulting in build breakage with config having
    CONFIG_NO_HZ_FULL=y.
    
            arch/powerpc/kernel/time.c: In function 'running_clock':
            arch/powerpc/kernel/time.c:712:2: error: implicit declaration of function 'cputime_to_nsecs' [-Werror=implicit-function-declaration]
              return local_clock() - cputime_to_nsecs(kcpustat_this_cpu->cpustat[CPUTIME_STEAL]);
    
    All we need is to remove it.
    
    Fixes: e7f340ca9c07 ("powerpc, sched/cputime: Remove unused cputime definitions")
    Reported-by: Abdul Haleem <abdhalee@linux.vnet.ibm.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index bc2e08d415fa..06bca663b4df 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -689,7 +689,7 @@ unsigned long long running_clock(void)
 	 * time and on a host which doesn't do any virtualisation TB *should* equal
 	 * VTB so it makes no difference anyway.
 	 */
-	return local_clock() - cputime_to_nsecs(kcpustat_this_cpu->cpustat[CPUTIME_STEAL]);
+	return local_clock() - kcpustat_this_cpu->cpustat[CPUTIME_STEAL];
 }
 #endif
 

commit e7f340ca9c0709508e6f590ac64f341058df241d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:48 2017 +0100

    powerpc, sched/cputime: Remove unused cputime definitions
    
    Since the core doesn't deal with cputime_t anymore, most of these APIs
    have been left unused. Lets remove these.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-33-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 01f53bfe100b..14e485525e31 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -152,20 +152,11 @@ EXPORT_SYMBOL_GPL(ppc_tb_freq);
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
 /*
- * Factors for converting from cputime_t (timebase ticks) to
- * jiffies, microseconds, seconds, and clock_t (1/USER_HZ seconds).
- * These are all stored as 0.64 fixed-point binary fractions.
+ * Factor for converting from cputime_t (timebase ticks) to
+ * microseconds. This is stored as 0.64 fixed-point binary fraction.
  */
-u64 __cputime_jiffies_factor;
-EXPORT_SYMBOL(__cputime_jiffies_factor);
 u64 __cputime_usec_factor;
 EXPORT_SYMBOL(__cputime_usec_factor);
-u64 __cputime_sec_factor;
-EXPORT_SYMBOL(__cputime_sec_factor);
-u64 __cputime_clockt_factor;
-EXPORT_SYMBOL(__cputime_clockt_factor);
-
-cputime_t cputime_one_jiffy;
 
 #ifdef CONFIG_PPC_SPLPAR
 void (*dtl_consumer)(struct dtl_entry *, u64);
@@ -181,14 +172,8 @@ static void calc_cputime_factors(void)
 {
 	struct div_result res;
 
-	div128_by_32(HZ, 0, tb_ticks_per_sec, &res);
-	__cputime_jiffies_factor = res.result_low;
 	div128_by_32(1000000, 0, tb_ticks_per_sec, &res);
 	__cputime_usec_factor = res.result_low;
-	div128_by_32(1, 0, tb_ticks_per_sec, &res);
-	__cputime_sec_factor = res.result_low;
-	div128_by_32(USER_HZ, 0, tb_ticks_per_sec, &res);
-	__cputime_clockt_factor = res.result_low;
 }
 
 /*
@@ -1053,7 +1038,6 @@ void __init time_init(void)
 	tb_ticks_per_sec = ppc_tb_freq;
 	tb_ticks_per_usec = ppc_tb_freq / 1000000;
 	calc_cputime_factors();
-	setup_cputime_one_jiffy();
 
 	/*
 	 * Compute scale factor for sched_clock.

commit fb8b049c988f1ff460b063b8a41ea9a3c79921c2
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:40 2017 +0100

    sched/cputime: Push time to account_system_time() in nsecs
    
    This is one more step toward converting cputime accounting to pure nsecs.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-25-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 739897a10fd3..01f53bfe100b 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -57,6 +57,7 @@
 #include <linux/clk-provider.h>
 #include <linux/suspend.h>
 #include <linux/rtc.h>
+#include <linux/cputime.h>
 #include <asm/trace.h>
 
 #include <asm/io.h>
@@ -72,7 +73,6 @@
 #include <asm/smp.h>
 #include <asm/vdso_datapage.h>
 #include <asm/firmware.h>
-#include <asm/cputime.h>
 #include <asm/asm-prototypes.h>
 
 /* powerpc clocksource/clockevent code */
@@ -399,7 +399,7 @@ void vtime_flush(struct task_struct *tsk)
 		tsk->utimescaled += cputime_to_nsecs(acct->utime_scaled);
 
 	if (acct->gtime)
-		account_guest_time(tsk, acct->gtime);
+		account_guest_time(tsk, cputime_to_nsecs(acct->gtime));
 
 	if (acct->steal_time)
 		account_steal_time(cputime_to_nsecs(acct->steal_time));
@@ -408,16 +408,17 @@ void vtime_flush(struct task_struct *tsk)
 		account_idle_time(cputime_to_nsecs(acct->idle_time));
 
 	if (acct->stime)
-		account_system_index_time(tsk, acct->stime, CPUTIME_SYSTEM);
-
+		account_system_index_time(tsk, cputime_to_nsecs(acct->stime),
+					  CPUTIME_SYSTEM);
 	if (acct->stime_scaled)
 		tsk->stimescaled += cputime_to_nsecs(acct->stime_scaled);
 
 	if (acct->hardirq_time)
-		account_system_index_time(tsk, acct->hardirq_time, CPUTIME_IRQ);
-
+		account_system_index_time(tsk, cputime_to_nsecs(acct->hardirq_time),
+					  CPUTIME_IRQ);
 	if (acct->softirq_time)
-		account_system_index_time(tsk, acct->softirq_time, CPUTIME_SOFTIRQ);
+		account_system_index_time(tsk, cputime_to_nsecs(acct->softirq_time),
+					  CPUTIME_SOFTIRQ);
 
 	acct->utime = 0;
 	acct->utime_scaled = 0;

commit 18b43a9bd7ae91185e398dd983fb4fffb9e81b3a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:39 2017 +0100

    sched/cputime: Push time to account_idle_time() in nsecs
    
    This is one more step toward converting cputime accounting to pure nsecs.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-24-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 53e5982edacf..739897a10fd3 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -405,7 +405,7 @@ void vtime_flush(struct task_struct *tsk)
 		account_steal_time(cputime_to_nsecs(acct->steal_time));
 
 	if (acct->idle_time)
-		account_idle_time(acct->idle_time);
+		account_idle_time(cputime_to_nsecs(acct->idle_time));
 
 	if (acct->stime)
 		account_system_index_time(tsk, acct->stime, CPUTIME_SYSTEM);

commit be9095ed4fb3cf69e9fdf64e28ff6b5bd0ec7215
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:38 2017 +0100

    sched/cputime: Push time to account_steal_time() in nsecs
    
    This is one more step toward converting cputime accounting to pure nsecs.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-23-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index c3931d816190..53e5982edacf 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -402,7 +402,7 @@ void vtime_flush(struct task_struct *tsk)
 		account_guest_time(tsk, acct->gtime);
 
 	if (acct->steal_time)
-		account_steal_time(acct->steal_time);
+		account_steal_time(cputime_to_nsecs(acct->steal_time));
 
 	if (acct->idle_time)
 		account_idle_time(acct->idle_time);

commit 23244a5c8003d4154161a8289a7d3783b0237c08
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:37 2017 +0100

    sched/cputime: Push time to account_user_time() in nsecs
    
    This is one more step toward converting cputime accounting to pure nsecs.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-22-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 3cca82e065c9..c3931d816190 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -393,7 +393,7 @@ void vtime_flush(struct task_struct *tsk)
 	struct cpu_accounting_data *acct = get_accounting(tsk);
 
 	if (acct->utime)
-		account_user_time(tsk, acct->utime);
+		account_user_time(tsk, cputime_to_nsecs(acct->utime));
 
 	if (acct->utime_scaled)
 		tsk->utimescaled += cputime_to_nsecs(acct->utime_scaled);

commit 5613fda9a503cd6137b120298902a34a1386b2c1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:23 2017 +0100

    sched/cputime: Convert task/group cputime to nsecs
    
    Now that most cputime readers use the transition API which return the
    task cputime in old style cputime_t, we can safely store the cputime in
    nsecs. This will eventually make cputime statistics less opaque and more
    granular. Back and forth convertions between cputime_t and nsecs in order
    to deal with cputime_t random granularity won't be needed anymore.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-8-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 02e97305d22b..3cca82e065c9 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -396,7 +396,7 @@ void vtime_flush(struct task_struct *tsk)
 		account_user_time(tsk, acct->utime);
 
 	if (acct->utime_scaled)
-		tsk->utimescaled += acct->utime_scaled;
+		tsk->utimescaled += cputime_to_nsecs(acct->utime_scaled);
 
 	if (acct->gtime)
 		account_guest_time(tsk, acct->gtime);
@@ -411,7 +411,7 @@ void vtime_flush(struct task_struct *tsk)
 		account_system_index_time(tsk, acct->stime, CPUTIME_SYSTEM);
 
 	if (acct->stime_scaled)
-		tsk->stimescaled += acct->stime_scaled;
+		tsk->stimescaled += cputime_to_nsecs(acct->stime_scaled);
 
 	if (acct->hardirq_time)
 		account_system_index_time(tsk, acct->hardirq_time, CPUTIME_IRQ);

commit c8d7dabf8f91fadd265e6eb87afb201d14ea299b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jan 5 18:11:50 2017 +0100

    sched/cputime: Rename vtime_account_user() to vtime_flush()
    
    CONFIG_VIRT_CPU_ACCOUNTING_NATIVE=y used to accumulate user time and
    account it on ticks and context switches only through the
    vtime_account_user() function.
    
    Now this model has been generalized on the 3 archs for all kind of
    cputime (system, irq, ...) and all the cputime flushing happens under
    vtime_account_user().
    
    So let's rename this function to better reflect its new role.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1483636310-6557-11-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 4255e6930ac1..02e97305d22b 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -382,15 +382,13 @@ void vtime_account_idle(struct task_struct *tsk)
 }
 
 /*
- * Transfer the user time accumulated in the paca
- * by the exception entry and exit code to the generic
- * process user time records.
+ * Account the whole cputime accumulated in the paca
  * Must be called with interrupts disabled.
  * Assumes that vtime_account_system/idle() has been called
  * recently (i.e. since the last entry from usermode) so that
  * get_paca()->user_time_scaled is up to date.
  */
-void vtime_account_user(struct task_struct *tsk)
+void vtime_flush(struct task_struct *tsk)
 {
 	struct cpu_accounting_data *acct = get_accounting(tsk);
 

commit a19ff1a2cc9227f82e97836a8ee3e593f622eaf9
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jan 5 18:11:47 2017 +0100

    sched/cputime, powerpc/vtime: Accumulate cputime and account only on tick/task switch
    
    Currently CONFIG_VIRT_CPU_ACCOUNTING_NATIVE=y accounts the cputime on
    any context boundary: irq entry/exit, guest entry/exit, context switch,
    etc...
    
    Calling functions such as account_system_time(), account_user_time()
    and such can be costly, especially if they are called on many fastpath
    such as twice per IRQ. Those functions do more than just accounting to
    kcpustat and task cputime. Depending on the config, some subsystems can
    perform unpleasant multiplications and divisions, among other things.
    
    So lets accumulate the cputime instead and delay the accounting on ticks
    and context switches only.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1483636310-6557-8-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 714313ebf030..4255e6930ac1 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -280,17 +280,10 @@ void accumulate_stolen_time(void)
 
 static inline u64 calculate_stolen_time(u64 stop_tb)
 {
-	u64 stolen = 0;
-	struct cpu_accounting_data *acct = &local_paca->accounting;
-
-	if (get_paca()->dtl_ridx != be64_to_cpu(get_lppaca()->dtl_idx)) {
-		stolen = scan_dispatch_log(stop_tb);
-		acct->stime -= stolen;
-	}
+	if (get_paca()->dtl_ridx != be64_to_cpu(get_lppaca()->dtl_idx))
+		return scan_dispatch_log(stop_tb);
 
-	stolen += acct->steal_time;
-	acct->steal_time = 0;
-	return stolen;
+	return 0;
 }
 
 #else /* CONFIG_PPC_SPLPAR */
@@ -306,27 +299,26 @@ static inline u64 calculate_stolen_time(u64 stop_tb)
  * or soft irq state.
  */
 static unsigned long vtime_delta(struct task_struct *tsk,
-				 unsigned long *sys_scaled,
-				 unsigned long *stolen)
+				 unsigned long *stime_scaled,
+				 unsigned long *steal_time)
 {
 	unsigned long now, nowscaled, deltascaled;
-	unsigned long udelta, delta, user_scaled;
+	unsigned long stime;
+	unsigned long utime, utime_scaled;
 	struct cpu_accounting_data *acct = get_accounting(tsk);
 
 	WARN_ON_ONCE(!irqs_disabled());
 
 	now = mftb();
 	nowscaled = read_spurr(now);
-	acct->stime += now - acct->starttime;
+	stime = now - acct->starttime;
 	acct->starttime = now;
 	deltascaled = nowscaled - acct->startspurr;
 	acct->startspurr = nowscaled;
 
-	*stolen = calculate_stolen_time(now);
+	*steal_time = calculate_stolen_time(now);
 
-	delta = acct->stime;
-	acct->stime = 0;
-	udelta = acct->utime - acct->utime_sspurr;
+	utime = acct->utime - acct->utime_sspurr;
 	acct->utime_sspurr = acct->utime;
 
 	/*
@@ -339,39 +331,54 @@ static unsigned long vtime_delta(struct task_struct *tsk,
 	 * the user ticks get saved up in paca->user_time_scaled to be
 	 * used by account_process_tick.
 	 */
-	*sys_scaled = delta;
-	user_scaled = udelta;
-	if (deltascaled != delta + udelta) {
-		if (udelta) {
-			*sys_scaled = deltascaled * delta / (delta + udelta);
-			user_scaled = deltascaled - *sys_scaled;
+	*stime_scaled = stime;
+	utime_scaled = utime;
+	if (deltascaled != stime + utime) {
+		if (utime) {
+			*stime_scaled = deltascaled * stime / (stime + utime);
+			utime_scaled = deltascaled - *stime_scaled;
 		} else {
-			*sys_scaled = deltascaled;
+			*stime_scaled = deltascaled;
 		}
 	}
-	acct->utime_scaled += user_scaled;
+	acct->utime_scaled += utime_scaled;
 
-	return delta;
+	return stime;
 }
 
 void vtime_account_system(struct task_struct *tsk)
 {
-	unsigned long delta, sys_scaled, stolen;
+	unsigned long stime, stime_scaled, steal_time;
+	struct cpu_accounting_data *acct = get_accounting(tsk);
+
+	stime = vtime_delta(tsk, &stime_scaled, &steal_time);
+
+	stime -= min(stime, steal_time);
+	acct->steal_time += steal_time;
 
-	delta = vtime_delta(tsk, &sys_scaled, &stolen);
-	account_system_time(tsk, 0, delta);
-	tsk->stimescaled += sys_scaled;
-	if (stolen)
-		account_steal_time(stolen);
+	if ((tsk->flags & PF_VCPU) && !irq_count()) {
+		acct->gtime += stime;
+		acct->utime_scaled += stime_scaled;
+	} else {
+		if (hardirq_count())
+			acct->hardirq_time += stime;
+		else if (in_serving_softirq())
+			acct->softirq_time += stime;
+		else
+			acct->stime += stime;
+
+		acct->stime_scaled += stime_scaled;
+	}
 }
 EXPORT_SYMBOL_GPL(vtime_account_system);
 
 void vtime_account_idle(struct task_struct *tsk)
 {
-	unsigned long delta, sys_scaled, stolen;
+	unsigned long stime, stime_scaled, steal_time;
+	struct cpu_accounting_data *acct = get_accounting(tsk);
 
-	delta = vtime_delta(tsk, &sys_scaled, &stolen);
-	account_idle_time(delta + stolen);
+	stime = vtime_delta(tsk, &stime_scaled, &steal_time);
+	acct->idle_time += stime + steal_time;
 }
 
 /*
@@ -385,16 +392,45 @@ void vtime_account_idle(struct task_struct *tsk)
  */
 void vtime_account_user(struct task_struct *tsk)
 {
-	cputime_t utime, utimescaled;
 	struct cpu_accounting_data *acct = get_accounting(tsk);
 
-	utime = acct->utime;
-	utimescaled = acct->utime_scaled;
+	if (acct->utime)
+		account_user_time(tsk, acct->utime);
+
+	if (acct->utime_scaled)
+		tsk->utimescaled += acct->utime_scaled;
+
+	if (acct->gtime)
+		account_guest_time(tsk, acct->gtime);
+
+	if (acct->steal_time)
+		account_steal_time(acct->steal_time);
+
+	if (acct->idle_time)
+		account_idle_time(acct->idle_time);
+
+	if (acct->stime)
+		account_system_index_time(tsk, acct->stime, CPUTIME_SYSTEM);
+
+	if (acct->stime_scaled)
+		tsk->stimescaled += acct->stime_scaled;
+
+	if (acct->hardirq_time)
+		account_system_index_time(tsk, acct->hardirq_time, CPUTIME_IRQ);
+
+	if (acct->softirq_time)
+		account_system_index_time(tsk, acct->softirq_time, CPUTIME_SOFTIRQ);
+
 	acct->utime = 0;
 	acct->utime_scaled = 0;
 	acct->utime_sspurr = 0;
-	account_user_time(tsk, utime);
-	tsk->utimescaled += utimescaled;
+	acct->gtime = 0;
+	acct->steal_time = 0;
+	acct->idle_time = 0;
+	acct->stime = 0;
+	acct->stime_scaled = 0;
+	acct->hardirq_time = 0;
+	acct->softirq_time = 0;
 }
 
 #ifdef CONFIG_PPC32
@@ -409,8 +445,6 @@ void arch_vtime_task_switch(struct task_struct *prev)
 
 	acct->starttime = get_accounting(prev)->starttime;
 	acct->startspurr = get_accounting(prev)->startspurr;
-	acct->stime = 0;
-	acct->utime = 0;
 }
 #endif /* CONFIG_PPC32 */
 

commit f828c3d0aebab130a19d36336b50afa3414fa0bc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jan 5 18:11:46 2017 +0100

    sched/cputime, powerpc: Migrate stolen_time field to the accounting structure
    
    That in order to gather all cputime accumulation to the same place.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1483636310-6557-7-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 17a2cd1c6a75..714313ebf030 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -273,7 +273,7 @@ void accumulate_stolen_time(void)
 	ust = scan_dispatch_log(acct->starttime);
 	acct->stime -= sst;
 	acct->utime -= ust;
-	local_paca->stolen_time += ust + sst;
+	acct->steal_time += ust + sst;
 
 	local_paca->soft_enabled = save_soft_enabled;
 }
@@ -288,8 +288,8 @@ static inline u64 calculate_stolen_time(u64 stop_tb)
 		acct->stime -= stolen;
 	}
 
-	stolen += get_paca()->stolen_time;
-	get_paca()->stolen_time = 0;
+	stolen += acct->steal_time;
+	acct->steal_time = 0;
 	return stolen;
 }
 

commit 8c8b73c4811f2b5e458a7418dca07d2ef85c7db1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jan 5 18:11:45 2017 +0100

    sched/cputime, powerpc: Prepare accounting structure for cputime flush on tick
    
    In order to prepare for CONFIG_VIRT_CPU_ACCOUNTING_NATIVE=y to delay
    cputime accounting to the tick, provide finegrained accumulators to
    powerpc in order to store the cputime until flushing.
    
    While at it, normalize the name of several fields according to common
    cputime naming.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1483636310-6557-6-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index ce2165089318..17a2cd1c6a75 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -271,8 +271,8 @@ void accumulate_stolen_time(void)
 
 	sst = scan_dispatch_log(acct->starttime_user);
 	ust = scan_dispatch_log(acct->starttime);
-	acct->system_time -= sst;
-	acct->user_time -= ust;
+	acct->stime -= sst;
+	acct->utime -= ust;
 	local_paca->stolen_time += ust + sst;
 
 	local_paca->soft_enabled = save_soft_enabled;
@@ -281,10 +281,11 @@ void accumulate_stolen_time(void)
 static inline u64 calculate_stolen_time(u64 stop_tb)
 {
 	u64 stolen = 0;
+	struct cpu_accounting_data *acct = &local_paca->accounting;
 
 	if (get_paca()->dtl_ridx != be64_to_cpu(get_lppaca()->dtl_idx)) {
 		stolen = scan_dispatch_log(stop_tb);
-		get_paca()->accounting.system_time -= stolen;
+		acct->stime -= stolen;
 	}
 
 	stolen += get_paca()->stolen_time;
@@ -316,17 +317,17 @@ static unsigned long vtime_delta(struct task_struct *tsk,
 
 	now = mftb();
 	nowscaled = read_spurr(now);
-	acct->system_time += now - acct->starttime;
+	acct->stime += now - acct->starttime;
 	acct->starttime = now;
 	deltascaled = nowscaled - acct->startspurr;
 	acct->startspurr = nowscaled;
 
 	*stolen = calculate_stolen_time(now);
 
-	delta = acct->system_time;
-	acct->system_time = 0;
-	udelta = acct->user_time - acct->utime_sspurr;
-	acct->utime_sspurr = acct->user_time;
+	delta = acct->stime;
+	acct->stime = 0;
+	udelta = acct->utime - acct->utime_sspurr;
+	acct->utime_sspurr = acct->utime;
 
 	/*
 	 * Because we don't read the SPURR on every kernel entry/exit,
@@ -348,7 +349,7 @@ static unsigned long vtime_delta(struct task_struct *tsk,
 			*sys_scaled = deltascaled;
 		}
 	}
-	acct->user_time_scaled += user_scaled;
+	acct->utime_scaled += user_scaled;
 
 	return delta;
 }
@@ -387,10 +388,10 @@ void vtime_account_user(struct task_struct *tsk)
 	cputime_t utime, utimescaled;
 	struct cpu_accounting_data *acct = get_accounting(tsk);
 
-	utime = acct->user_time;
-	utimescaled = acct->user_time_scaled;
-	acct->user_time = 0;
-	acct->user_time_scaled = 0;
+	utime = acct->utime;
+	utimescaled = acct->utime_scaled;
+	acct->utime = 0;
+	acct->utime_scaled = 0;
 	acct->utime_sspurr = 0;
 	account_user_time(tsk, utime);
 	tsk->utimescaled += utimescaled;
@@ -408,8 +409,8 @@ void arch_vtime_task_switch(struct task_struct *prev)
 
 	acct->starttime = get_accounting(prev)->starttime;
 	acct->startspurr = get_accounting(prev)->startspurr;
-	acct->system_time = 0;
-	acct->user_time = 0;
+	acct->stime = 0;
+	acct->utime = 0;
 }
 #endif /* CONFIG_PPC32 */
 

commit 90d08ba2b9b4be4aeca6a5b5a4b09fbcde30194d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jan 5 18:11:41 2017 +0100

    sched/cputime, powerpc32: Fix stale scaled stime on context switch
    
    On context switch with powerpc32, the cputime is accumulated in the
    thread_info struct. So the switching-in task must move forward its
    start time snapshot to the current time in order to later compute the
    delta spent in system mode.
    
    This is what we do for the normal cputime by initializing the starttime
    field to the value of the previous task's starttime which got freshly
    updated.
    
    But we are missing the update of the scaled cputime start time. As a
    result we may be accounting too much scaled cputime later.
    
    Fix this by initializing the scaled cputime the same way we do for
    normal cputime.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1483636310-6557-2-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index bc2e08d415fa..ce2165089318 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -407,6 +407,7 @@ void arch_vtime_task_switch(struct task_struct *prev)
 	struct cpu_accounting_data *acct = get_accounting(current);
 
 	acct->starttime = get_accounting(prev)->starttime;
+	acct->startspurr = get_accounting(prev)->startspurr;
 	acct->system_time = 0;
 	acct->user_time = 0;
 }

commit a5a1d1c2914b5316924c7893eb683a5420ebd3be
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:32:01 2016 +0100

    clocksource: Use a plain u64 instead of cycle_t
    
    There is no point in having an extra type for extra confusion. u64 is
    unambiguous.
    
    Conversion was done with the following coccinelle script:
    
    @rem@
    @@
    -typedef u64 cycle_t;
    
    @fix@
    typedef cycle_t;
    @@
    -cycle_t
    +u64
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 19397e2a8bf5..bc2e08d415fa 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -80,7 +80,7 @@
 #include <linux/clockchips.h>
 #include <linux/timekeeper_internal.h>
 
-static cycle_t rtc_read(struct clocksource *);
+static u64 rtc_read(struct clocksource *);
 static struct clocksource clocksource_rtc = {
 	.name         = "rtc",
 	.rating       = 400,
@@ -89,7 +89,7 @@ static struct clocksource clocksource_rtc = {
 	.read         = rtc_read,
 };
 
-static cycle_t timebase_read(struct clocksource *);
+static u64 timebase_read(struct clocksource *);
 static struct clocksource clocksource_timebase = {
 	.name         = "timebase",
 	.rating       = 400,
@@ -802,18 +802,18 @@ void read_persistent_clock(struct timespec *ts)
 }
 
 /* clocksource code */
-static cycle_t rtc_read(struct clocksource *cs)
+static u64 rtc_read(struct clocksource *cs)
 {
-	return (cycle_t)get_rtc();
+	return (u64)get_rtc();
 }
 
-static cycle_t timebase_read(struct clocksource *cs)
+static u64 timebase_read(struct clocksource *cs)
 {
-	return (cycle_t)get_tb();
+	return (u64)get_tb();
 }
 
 void update_vsyscall_old(struct timespec *wall_time, struct timespec *wtm,
-			 struct clocksource *clock, u32 mult, cycle_t cycle_last)
+			 struct clocksource *clock, u32 mult, u64 cycle_last)
 {
 	u64 new_tb_to_xs, new_stamp_xsec;
 	u32 frac_sec;

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index be9751f1cb2a..19397e2a8bf5 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -64,7 +64,7 @@
 #include <asm/nvram.h>
 #include <asm/cache.h>
 #include <asm/machdep.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/time.h>
 #include <asm/prom.h>
 #include <asm/irq.h>

commit 40565b5aedd6d0ca88b7dfd3859d709d2f6f8cf9
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Tue Nov 15 03:06:51 2016 +0100

    sched/cputime, powerpc, s390: Make scaled cputime arch specific
    
    Only s390 and powerpc have hardware facilities allowing to measure
    cputimes scaled by frequency. On all other architectures
    utimescaled/stimescaled are equal to utime/stime (however they are
    accounted separately).
    
    Remove {u,s}timescaled accounting on all architectures except
    powerpc and s390, where those values are explicitly accounted
    in the proper places.
    
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20161031162143.GB12646@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 81051986739c..be9751f1cb2a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -358,7 +358,8 @@ void vtime_account_system(struct task_struct *tsk)
 	unsigned long delta, sys_scaled, stolen;
 
 	delta = vtime_delta(tsk, &sys_scaled, &stolen);
-	account_system_time(tsk, 0, delta, sys_scaled);
+	account_system_time(tsk, 0, delta);
+	tsk->stimescaled += sys_scaled;
 	if (stolen)
 		account_steal_time(stolen);
 }
@@ -391,7 +392,8 @@ void vtime_account_user(struct task_struct *tsk)
 	acct->user_time = 0;
 	acct->user_time_scaled = 0;
 	acct->utime_sspurr = 0;
-	account_user_time(tsk, utime, utimescaled);
+	account_user_time(tsk, utime);
+	tsk->utimescaled += utimescaled;
 }
 
 #ifdef CONFIG_PPC32

commit 7008eb997bcfdd8c5274bb6672aa5d4e77994309
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Tue Nov 15 03:06:49 2016 +0100

    sched/cputime, powerpc: Remove cputime_last_delta global variable
    
    Since commit:
    
      cf9efce0ce313 ("powerpc: Account time using timebase rather than PURR")
    
    cputime_last_delta is not initialized to other value than 0, hence it's
    not used except zero check and cputime_to_scaled() just returns
    the argument.
    
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1479175612-14718-2-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index bc3f7d0d7b79..81051986739c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -164,8 +164,6 @@ u64 __cputime_sec_factor;
 EXPORT_SYMBOL(__cputime_sec_factor);
 u64 __cputime_clockt_factor;
 EXPORT_SYMBOL(__cputime_clockt_factor);
-DEFINE_PER_CPU(unsigned long, cputime_last_delta);
-DEFINE_PER_CPU(unsigned long, cputime_scaled_last_delta);
 
 cputime_t cputime_one_jiffy;
 

commit 84d69848c97faab0c25aa2667b273404d2e2a64a
Merge: d4d24d2d0a7e 590abbdd2733
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 14 14:26:58 2016 -0700

    Merge branch 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild
    
    Pull kbuild updates from Michal Marek:
    
     - EXPORT_SYMBOL for asm source by Al Viro.
    
       This does bring a regression, because genksyms no longer generates
       checksums for these symbols (CONFIG_MODVERSIONS). Nick Piggin is
       working on a patch to fix this.
    
       Plus, we are talking about functions like strcpy(), which rarely
       change prototypes.
    
     - Fixes for PPC fallout of the above by Stephen Rothwell and Nick
       Piggin
    
     - fixdep speedup by Alexey Dobriyan.
    
     - preparatory work by Nick Piggin to allow architectures to build with
       -ffunction-sections, -fdata-sections and --gc-sections
    
     - CONFIG_THIN_ARCHIVES support by Stephen Rothwell
    
     - fix for filenames with colons in the initramfs source by me.
    
    * 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild: (22 commits)
      initramfs: Escape colons in depfile
      ppc: there is no clear_pages to export
      powerpc/64: whitelist unresolved modversions CRCs
      kbuild: -ffunction-sections fix for archs with conflicting sections
      kbuild: add arch specific post-link Makefile
      kbuild: allow archs to select link dead code/data elimination
      kbuild: allow architectures to use thin archives instead of ld -r
      kbuild: Regenerate genksyms lexer
      kbuild: genksyms fix for typeof handling
      fixdep: faster CONFIG_ search
      ia64: move exports to definitions
      sparc32: debride memcpy.S a bit
      [sparc] unify 32bit and 64bit string.h
      sparc: move exports to definitions
      ppc: move exports to definitions
      arm: move exports to definitions
      s390: move exports to definitions
      m68k: move exports to definitions
      alpha: move exports to actual definitions
      x86: move exports to actual definitions
      ...

commit 0545d5436aefddff7ca417adc1a431c108403a35
Author: Daniel Axtens <dja@axtens.net>
Date:   Tue Sep 6 15:32:43 2016 +1000

    powerpc/sparse: Add more assembler prototypes
    
    Another set of things that are only called from assembler and so need
    prototypes to keep sparse happy.
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 3efbedefba6a..67859b7d1c97 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -73,6 +73,7 @@
 #include <asm/vdso_datapage.h>
 #include <asm/firmware.h>
 #include <asm/cputime.h>
+#include <asm/asm-prototypes.h>
 
 /* powerpc clocksource/clockevent code */
 

commit 9445aa1a3062a75a4d9de78026816ebc941e7b99
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Jan 13 23:33:46 2016 -0500

    ppc: move exports to definitions
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 3efbedefba6a..0ce8b76c6150 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -595,6 +595,7 @@ void timer_interrupt(struct pt_regs * regs)
 	irq_exit();
 	set_irq_regs(old_regs);
 }
+EXPORT_SYMBOL(timer_interrupt);
 
 /*
  * Hypervisor decrementer interrupts shouldn't occur but are sometimes

commit 6c84239d595dc6ffe39f0f03dae2f64ed200db95
Merge: d4c06c708123 6f367788d633
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 5 09:48:22 2016 -0400

    Merge tag 'rtc-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/abelloni/linux
    
    Pull RTC updates from Alexandre Belloni:
     "RTC for 4.8
    
      Cleanups:
       - huge cleanup of rtc-generic and char/genrtc this allowed to cleanup
         rtc-cmos, rtc-sh, rtc-m68k, rtc-powerpc and rtc-parisc
       - move mn10300 to rtc-cmos
    
      Subsystem:
       - fix wakealarms after hibernate
       - multiples fixes for rctest
       - simplify implementations of .read_alarm
    
      New drivers:
       - Maxim MAX6916
    
      Drivers:
       - ds1307: fix weekday
       - m41t80: add wakeup support
       - pcf85063: add support for PCF85063A variant
       - rv8803: extend i2c fix and other fixes
       - s35390a: fix alarm reading, this fixes instant reboot after
         shutdown for QNAP TS-41x
       - s3c: clock fixes"
    
    * tag 'rtc-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/abelloni/linux: (65 commits)
      rtc: rv8803: Clear V1F when setting the time
      rtc: rv8803: Stop the clock while setting the time
      rtc: rv8803: Always apply the IÂ²C workaround
      rtc: rv8803: Fix read day of week
      rtc: rv8803: Remove the check for valid time
      rtc: rv8803: Kconfig: Indicate rx8900 support
      rtc: asm9260: remove .owner field for driver
      rtc: at91sam9: Fix missing spin_lock_init()
      rtc: m41t80: add suspend handlers for alarm IRQ
      rtc: m41t80: make it a real error message
      rtc: pcf85063: Add support for the PCF85063A device
      rtc: pcf85063: fix year range
      rtc: hym8563: in .read_alarm set .tm_sec to 0 to signal minute accuracy
      rtc: explicitly set tm_sec = 0 for drivers with minute accurancy
      rtc: s3c: Add s3c_rtc_{enable/disable}_clk in s3c_rtc_setfreq()
      rtc: s3c: Remove unnecessary call to disable already disabled clock
      rtc: abx80x: use devm_add_action_or_reset()
      rtc: m41t80: use devm_add_action_or_reset()
      rtc: fix a typo and reduce three empty lines to one
      rtc: s35390a: improve two comments in .set_alarm
      ...

commit c223c90386bc2306510e0ceacd768a0123ff2a2f
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue May 17 08:33:46 2016 +0200

    powerpc32: provide VIRT_CPU_ACCOUNTING
    
    This patch provides VIRT_CPU_ACCOUTING to PPC32 architecture.
    PPC32 doesn't have the PACA structure, so we use the task_info
    structure to store the accounting data.
    
    In order to reuse on PPC32 the PPC64 functions, all u64 data has
    been replaced by 'unsigned long' so that it is u32 on PPC32 and
    u64 on PPC64
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 6b4d01d1ccf0..4e7759c8ca30 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -167,7 +167,15 @@ DEFINE_PER_CPU(unsigned long, cputime_scaled_last_delta);
 
 cputime_t cputime_one_jiffy;
 
+#ifdef CONFIG_PPC_SPLPAR
 void (*dtl_consumer)(struct dtl_entry *, u64);
+#endif
+
+#ifdef CONFIG_PPC64
+#define get_accounting(tsk)	(&get_paca()->accounting)
+#else
+#define get_accounting(tsk)	(&task_thread_info(tsk)->accounting)
+#endif
 
 static void calc_cputime_factors(void)
 {
@@ -187,7 +195,7 @@ static void calc_cputime_factors(void)
  * Read the SPURR on systems that have it, otherwise the PURR,
  * or if that doesn't exist return the timebase value passed in.
  */
-static u64 read_spurr(u64 tb)
+static unsigned long read_spurr(unsigned long tb)
 {
 	if (cpu_has_feature(CPU_FTR_SPURR))
 		return mfspr(SPRN_SPURR);
@@ -250,8 +258,8 @@ static u64 scan_dispatch_log(u64 stop_tb)
 void accumulate_stolen_time(void)
 {
 	u64 sst, ust;
-
 	u8 save_soft_enabled = local_paca->soft_enabled;
+	struct cpu_accounting_data *acct = &local_paca->accounting;
 
 	/* We are called early in the exception entry, before
 	 * soft/hard_enabled are sync'ed to the expected state
@@ -261,10 +269,10 @@ void accumulate_stolen_time(void)
 	 */
 	local_paca->soft_enabled = 0;
 
-	sst = scan_dispatch_log(local_paca->starttime_user);
-	ust = scan_dispatch_log(local_paca->starttime);
-	local_paca->system_time -= sst;
-	local_paca->user_time -= ust;
+	sst = scan_dispatch_log(acct->starttime_user);
+	ust = scan_dispatch_log(acct->starttime);
+	acct->system_time -= sst;
+	acct->user_time -= ust;
 	local_paca->stolen_time += ust + sst;
 
 	local_paca->soft_enabled = save_soft_enabled;
@@ -276,7 +284,7 @@ static inline u64 calculate_stolen_time(u64 stop_tb)
 
 	if (get_paca()->dtl_ridx != be64_to_cpu(get_lppaca()->dtl_idx)) {
 		stolen = scan_dispatch_log(stop_tb);
-		get_paca()->system_time -= stolen;
+		get_paca()->accounting.system_time -= stolen;
 	}
 
 	stolen += get_paca()->stolen_time;
@@ -296,27 +304,29 @@ static inline u64 calculate_stolen_time(u64 stop_tb)
  * Account time for a transition between system, hard irq
  * or soft irq state.
  */
-static u64 vtime_delta(struct task_struct *tsk,
-			u64 *sys_scaled, u64 *stolen)
+static unsigned long vtime_delta(struct task_struct *tsk,
+				 unsigned long *sys_scaled,
+				 unsigned long *stolen)
 {
-	u64 now, nowscaled, deltascaled;
-	u64 udelta, delta, user_scaled;
+	unsigned long now, nowscaled, deltascaled;
+	unsigned long udelta, delta, user_scaled;
+	struct cpu_accounting_data *acct = get_accounting(tsk);
 
 	WARN_ON_ONCE(!irqs_disabled());
 
 	now = mftb();
 	nowscaled = read_spurr(now);
-	get_paca()->system_time += now - get_paca()->starttime;
-	get_paca()->starttime = now;
-	deltascaled = nowscaled - get_paca()->startspurr;
-	get_paca()->startspurr = nowscaled;
+	acct->system_time += now - acct->starttime;
+	acct->starttime = now;
+	deltascaled = nowscaled - acct->startspurr;
+	acct->startspurr = nowscaled;
 
 	*stolen = calculate_stolen_time(now);
 
-	delta = get_paca()->system_time;
-	get_paca()->system_time = 0;
-	udelta = get_paca()->user_time - get_paca()->utime_sspurr;
-	get_paca()->utime_sspurr = get_paca()->user_time;
+	delta = acct->system_time;
+	acct->system_time = 0;
+	udelta = acct->user_time - acct->utime_sspurr;
+	acct->utime_sspurr = acct->user_time;
 
 	/*
 	 * Because we don't read the SPURR on every kernel entry/exit,
@@ -338,14 +348,14 @@ static u64 vtime_delta(struct task_struct *tsk,
 			*sys_scaled = deltascaled;
 		}
 	}
-	get_paca()->user_time_scaled += user_scaled;
+	acct->user_time_scaled += user_scaled;
 
 	return delta;
 }
 
 void vtime_account_system(struct task_struct *tsk)
 {
-	u64 delta, sys_scaled, stolen;
+	unsigned long delta, sys_scaled, stolen;
 
 	delta = vtime_delta(tsk, &sys_scaled, &stolen);
 	account_system_time(tsk, 0, delta, sys_scaled);
@@ -356,7 +366,7 @@ EXPORT_SYMBOL_GPL(vtime_account_system);
 
 void vtime_account_idle(struct task_struct *tsk)
 {
-	u64 delta, sys_scaled, stolen;
+	unsigned long delta, sys_scaled, stolen;
 
 	delta = vtime_delta(tsk, &sys_scaled, &stolen);
 	account_idle_time(delta + stolen);
@@ -374,15 +384,32 @@ void vtime_account_idle(struct task_struct *tsk)
 void vtime_account_user(struct task_struct *tsk)
 {
 	cputime_t utime, utimescaled;
+	struct cpu_accounting_data *acct = get_accounting(tsk);
 
-	utime = get_paca()->user_time;
-	utimescaled = get_paca()->user_time_scaled;
-	get_paca()->user_time = 0;
-	get_paca()->user_time_scaled = 0;
-	get_paca()->utime_sspurr = 0;
+	utime = acct->user_time;
+	utimescaled = acct->user_time_scaled;
+	acct->user_time = 0;
+	acct->user_time_scaled = 0;
+	acct->utime_sspurr = 0;
 	account_user_time(tsk, utime, utimescaled);
 }
 
+#ifdef CONFIG_PPC32
+/*
+ * Called from the context switch with interrupts disabled, to charge all
+ * accumulated times to the current process, and to prepare accounting on
+ * the next process.
+ */
+void arch_vtime_task_switch(struct task_struct *prev)
+{
+	struct cpu_accounting_data *acct = get_accounting(current);
+
+	acct->starttime = get_accounting(prev)->starttime;
+	acct->system_time = 0;
+	acct->user_time = 0;
+}
+#endif /* CONFIG_PPC32 */
+
 #else /* ! CONFIG_VIRT_CPU_ACCOUNTING_NATIVE */
 #define calc_cputime_factors()
 #endif

commit 799010244685334b34e674d354a1a71a3a6b6148
Author: Oliver O'Halloran <oohall@gmail.com>
Date:   Fri Jul 1 16:20:39 2016 +1000

    powerpc/timer: Large Decrementer support
    
    Power ISAv3 adds a large decrementer (LD) mode which increases the size
    of the decrementer register. The size of the enlarged decrementer
    register is between 32 and 64 bits with the exact size being dependent
    on the implementation. When in LD mode, reads are sign extended to 64
    bits and a decrementer exception is raised when the high bit is set (i.e
    the value goes below zero). Writes however are truncated to the physical
    register width so some care needs to be taken to ensure that the high
    bit is not set when reloading the decrementer. This patch adds support
    for using the LD inside the host kernel on processors that support it.
    
    When LD mode is supported firmware will supply the ibm,dec-bits property
    for CPU nodes to allow the kernel to determine the maximum decrementer
    value. Enabling LD mode is a hypervisor privileged operation so the kernel
    can only enable it manually when running in hypervisor mode. Guests that
    support LD mode can request it using the "ibm,client-architecture-support"
    firmware call (not implemented in this patch) or some other platform
    specific method. If this property is not supplied then the traditional
    decrementer width of 32 bit is assumed and LD mode will not be enabled.
    
    This patch was based on initial work by Jack Miller.
    
    Signed-off-by: Oliver O'Halloran <oohall@gmail.com>
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Acked-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 3ed9a5a21d77..6b4d01d1ccf0 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -96,7 +96,8 @@ static struct clocksource clocksource_timebase = {
 	.read         = timebase_read,
 };
 
-#define DECREMENTER_MAX	0x7fffffff
+#define DECREMENTER_DEFAULT_MAX 0x7FFFFFFF
+u64 decrementer_max = DECREMENTER_DEFAULT_MAX;
 
 static int decrementer_set_next_event(unsigned long evt,
 				      struct clock_event_device *dev);
@@ -504,8 +505,8 @@ static void __timer_interrupt(void)
 		__this_cpu_inc(irq_stat.timer_irqs_event);
 	} else {
 		now = *next_tb - now;
-		if (now <= DECREMENTER_MAX)
-			set_dec((int)now);
+		if (now <= decrementer_max)
+			set_dec(now);
 		/* We may have raced with new irq work */
 		if (test_irq_work_pending())
 			set_dec(1);
@@ -535,7 +536,7 @@ void timer_interrupt(struct pt_regs * regs)
 	/* Ensure a positive value is written to the decrementer, or else
 	 * some CPUs will continue to take decrementer exceptions.
 	 */
-	set_dec(DECREMENTER_MAX);
+	set_dec(decrementer_max);
 
 	/* Some implementations of hotplug will get timer interrupts while
 	 * offline, just ignore these and we also need to set
@@ -583,9 +584,9 @@ static void generic_suspend_disable_irqs(void)
 	 * with suspending.
 	 */
 
-	set_dec(DECREMENTER_MAX);
+	set_dec(decrementer_max);
 	local_irq_disable();
-	set_dec(DECREMENTER_MAX);
+	set_dec(decrementer_max);
 }
 
 static void generic_suspend_enable_irqs(void)
@@ -866,7 +867,7 @@ static int decrementer_set_next_event(unsigned long evt,
 
 static int decrementer_shutdown(struct clock_event_device *dev)
 {
-	decrementer_set_next_event(DECREMENTER_MAX, dev);
+	decrementer_set_next_event(decrementer_max, dev);
 	return 0;
 }
 
@@ -892,6 +893,49 @@ static void register_decrementer_clockevent(int cpu)
 	clockevents_register_device(dec);
 }
 
+static void enable_large_decrementer(void)
+{
+	if (!cpu_has_feature(CPU_FTR_ARCH_300))
+		return;
+
+	if (decrementer_max <= DECREMENTER_DEFAULT_MAX)
+		return;
+
+	/*
+	 * If we're running as the hypervisor we need to enable the LD manually
+	 * otherwise firmware should have done it for us.
+	 */
+	if (cpu_has_feature(CPU_FTR_HVMODE))
+		mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) | LPCR_LD);
+}
+
+static void __init set_decrementer_max(void)
+{
+	struct device_node *cpu;
+	u32 bits = 32;
+
+	/* Prior to ISAv3 the decrementer is always 32 bit */
+	if (!cpu_has_feature(CPU_FTR_ARCH_300))
+		return;
+
+	cpu = of_find_node_by_type(NULL, "cpu");
+
+	if (of_property_read_u32(cpu, "ibm,dec-bits", &bits) == 0) {
+		if (bits > 64 || bits < 32) {
+			pr_warn("time_init: firmware supplied invalid ibm,dec-bits");
+			bits = 32;
+		}
+
+		/* calculate the signed maximum given this many bits */
+		decrementer_max = (1ul << (bits - 1)) - 1;
+	}
+
+	of_node_put(cpu);
+
+	pr_info("time_init: %u bit decrementer (max: %llx)\n",
+		bits, decrementer_max);
+}
+
 static void __init init_decrementer_clockevent(void)
 {
 	int cpu = smp_processor_id();
@@ -899,7 +943,7 @@ static void __init init_decrementer_clockevent(void)
 	clockevents_calc_mult_shift(&decrementer_clockevent, ppc_tb_freq, 4);
 
 	decrementer_clockevent.max_delta_ns =
-		clockevent_delta2ns(DECREMENTER_MAX, &decrementer_clockevent);
+		clockevent_delta2ns(decrementer_max, &decrementer_clockevent);
 	decrementer_clockevent.min_delta_ns =
 		clockevent_delta2ns(2, &decrementer_clockevent);
 
@@ -908,6 +952,9 @@ static void __init init_decrementer_clockevent(void)
 
 void secondary_cpu_time_init(void)
 {
+	/* Enable and test the large decrementer for this cpu */
+	enable_large_decrementer();
+
 	/* Start the decrementer on CPUs that have manual control
 	 * such as BookE
 	 */
@@ -973,6 +1020,10 @@ void __init time_init(void)
 	vdso_data->tb_update_count = 0;
 	vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;
 
+	/* initialise and enable the large decrementer (if we have one) */
+	set_decrementer_max();
+	enable_large_decrementer();
+
 	/* Start the decrementer on CPUs that have manual control
 	 * such as BookE
 	 */

commit 169047f447e8e7a56b459e13d86e1fc06457461a
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon May 30 20:58:00 2016 +0200

    rtc: powerpc: provide rtc_class_ops directly
    
    The rtc-generic driver provides an architecture specific
    wrapper on top of the generic rtc_class_ops abstraction,
    and powerpc has another abstraction on top, which is a bit
    silly.
    
    This changes the powerpc rtc-generic device to provide its
    rtc_class_ops directly, to reduce the number of layers
    by one.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Alexandre Belloni <alexandre.belloni@free-electrons.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 3ed9a5a21d77..7a482a7f4d8d 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -56,6 +56,7 @@
 #include <linux/irq_work.h>
 #include <linux/clk-provider.h>
 #include <linux/suspend.h>
+#include <linux/rtc.h>
 #include <asm/trace.h>
 
 #include <asm/io.h>
@@ -1081,6 +1082,29 @@ void calibrate_delay(void)
 	loops_per_jiffy = tb_ticks_per_jiffy;
 }
 
+#if IS_ENABLED(CONFIG_RTC_DRV_GENERIC)
+static int rtc_generic_get_time(struct device *dev, struct rtc_time *tm)
+{
+	ppc_md.get_rtc_time(tm);
+	return rtc_valid_tm(tm);
+}
+
+static int rtc_generic_set_time(struct device *dev, struct rtc_time *tm)
+{
+	if (!ppc_md.set_rtc_time)
+		return -EOPNOTSUPP;
+
+	if (ppc_md.set_rtc_time(tm) < 0)
+		return -EOPNOTSUPP;
+
+	return 0;
+}
+
+static const struct rtc_class_ops rtc_generic_ops = {
+	.read_time = rtc_generic_get_time,
+	.set_time = rtc_generic_set_time,
+};
+
 static int __init rtc_init(void)
 {
 	struct platform_device *pdev;
@@ -1088,9 +1112,12 @@ static int __init rtc_init(void)
 	if (!ppc_md.get_rtc_time)
 		return -ENODEV;
 
-	pdev = platform_device_register_simple("rtc-generic", -1, NULL, 0);
+	pdev = platform_device_register_data(NULL, "rtc-generic", -1,
+					     &rtc_generic_ops,
+					     sizeof(rtc_generic_ops));
 
 	return PTR_ERR_OR_ZERO(pdev);
 }
 
 device_initcall(rtc_init);
+#endif

commit 7f92bc5694557dee4cefa90df27feec16c7b62da
Author: Daniel Axtens <dja@axtens.net>
Date:   Wed Jan 6 11:45:51 2016 +1100

    powerpc: sparse: Include headers for __weak symbols
    
    Sometimes when sparse warns about undefined symbols, it isn't
    because they should have 'static' added, it's because they're
    overriding __weak symbols defined elsewhere, and the header has
    been missed.
    
    Fix a few of them by adding appropriate headers.
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 81b0900a39ee..3ed9a5a21d77 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -55,6 +55,7 @@
 #include <linux/delay.h>
 #include <linux/irq_work.h>
 #include <linux/clk-provider.h>
+#include <linux/suspend.h>
 #include <asm/trace.h>
 
 #include <asm/io.h>

commit 00b912b0c88e690b1662067497182454357b18b0
Author: Daniel Axtens <dja@axtens.net>
Date:   Tue Dec 15 18:09:14 2015 +1100

    powerpc: Remove broken GregorianDay()
    
    GregorianDay() is supposed to calculate the day of the week
    (tm->tm_wday) for a given day/month/year. In that calcuation it
    indexed into an array called MonthOffset using tm->tm_mon-1. However
    tm_mon is zero-based, not one-based, so this is off-by-one. It also
    means that every January, GregoiranDay() will access element -1 of
    the MonthOffset array.
    
    It also doesn't appear to be a correct algorithm either: see in
    contrast kernel/time/timeconv.c's time_to_tm function.
    
    It's been broken forever, which suggests no-one in userland uses
    this. It looks like no-one in the kernel uses tm->tm_wday either
    (see e.g. drivers/rtc/rtc-ds1305.c:319).
    
    tm->tm_wday is conventionally set to -1 when not available in
    hardware so we can simply set it to -1 and drop the function.
    (There are over a dozen other drivers in drivers/rtc that do
    this.)
    
    Found using UBSAN.
    
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andrew Morton <akpm@linux-foundation.org> # as an example of what UBSan finds.
    Cc: Alessandro Zummo <a.zummo@towertech.it>
    Cc: Alexandre Belloni <alexandre.belloni@free-electrons.com>
    Cc: rtc-linux@googlegroups.com
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Acked-by: Alexandre Belloni <alexandre.belloni@free-electrons.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 1be1092c7204..81b0900a39ee 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1002,38 +1002,6 @@ static int month_days[12] = {
 	31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31
 };
 
-/*
- * This only works for the Gregorian calendar - i.e. after 1752 (in the UK)
- */
-void GregorianDay(struct rtc_time * tm)
-{
-	int leapsToDate;
-	int lastYear;
-	int day;
-	int MonthOffset[] = { 0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334 };
-
-	lastYear = tm->tm_year - 1;
-
-	/*
-	 * Number of leap corrections to apply up to end of last year
-	 */
-	leapsToDate = lastYear / 4 - lastYear / 100 + lastYear / 400;
-
-	/*
-	 * This year is a leap year if it is divisible by 4 except when it is
-	 * divisible by 100 unless it is divisible by 400
-	 *
-	 * e.g. 1904 was a leap year, 1900 was not, 1996 is, and 2000 was
-	 */
-	day = tm->tm_mon > 2 && leapyear(tm->tm_year);
-
-	day += lastYear*365 + leapsToDate + MonthOffset[tm->tm_mon-1] +
-		   tm->tm_mday;
-
-	tm->tm_wday = day % 7;
-}
-EXPORT_SYMBOL_GPL(GregorianDay);
-
 void to_tm(int tim, struct rtc_time * tm)
 {
 	register int    i;
@@ -1064,9 +1032,9 @@ void to_tm(int tim, struct rtc_time * tm)
 	tm->tm_mday = day + 1;
 
 	/*
-	 * Determine the day of week
+	 * No-one uses the day of the week.
 	 */
-	GregorianDay(tm);
+	tm->tm_wday = -1;
 }
 EXPORT_SYMBOL(to_tm);
 

commit 37a13e78e017c31fbb98a148b54c2552dc08d788
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Thu Jul 16 16:56:25 2015 +0530

    powerpc/time: Migrate to new 'set-state' interface
    
    Migrate powerpc driver to the new 'set-state' interface provided by
    clockevents core, the earlier 'set-mode' interface is marked obsolete
    now.
    
    This also enables us to implement callbacks for new states of clockevent
    devices, for example: ONESHOT_STOPPED.
    
    We weren't doing anything in ->set_mode(ONSHOT) and so
    set_state_oneshot() isn't implemented.
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 43922509a483..1be1092c7204 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -99,16 +99,17 @@ static struct clocksource clocksource_timebase = {
 
 static int decrementer_set_next_event(unsigned long evt,
 				      struct clock_event_device *dev);
-static void decrementer_set_mode(enum clock_event_mode mode,
-				 struct clock_event_device *dev);
+static int decrementer_shutdown(struct clock_event_device *evt);
 
 struct clock_event_device decrementer_clockevent = {
-	.name           = "decrementer",
-	.rating         = 200,
-	.irq            = 0,
-	.set_next_event = decrementer_set_next_event,
-	.set_mode       = decrementer_set_mode,
-	.features       = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_C3STOP,
+	.name			= "decrementer",
+	.rating			= 200,
+	.irq			= 0,
+	.set_next_event		= decrementer_set_next_event,
+	.set_state_shutdown	= decrementer_shutdown,
+	.tick_resume		= decrementer_shutdown,
+	.features		= CLOCK_EVT_FEAT_ONESHOT |
+				  CLOCK_EVT_FEAT_C3STOP,
 };
 EXPORT_SYMBOL(decrementer_clockevent);
 
@@ -862,11 +863,10 @@ static int decrementer_set_next_event(unsigned long evt,
 	return 0;
 }
 
-static void decrementer_set_mode(enum clock_event_mode mode,
-				 struct clock_event_device *dev)
+static int decrementer_shutdown(struct clock_event_device *dev)
 {
-	if (mode != CLOCK_EVT_MODE_ONESHOT)
-		decrementer_set_next_event(DECREMENTER_MAX, dev);
+	decrementer_set_next_event(DECREMENTER_MAX, dev);
+	return 0;
 }
 
 /* Interrupt handler for the timer broadcast IPI */

commit 8f6b9512ceadc6bd52777c299111dc642b4c65b6
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri May 1 20:05:49 2015 -0400

    powerpc: use device_initcall for registering rtc devices
    
    Currently these two RTC devices are in core platform code
    where it is not possible for them to be modular.  It will
    never be modular, so using module_init as an alias for
    __initcall can be somewhat misleading.
    
    Fix this up now, so that we can relocate module_init from
    init.h into module.h in the future.  If we don't do this, we'd
    have to add module.h to obviously non-modular code, and that
    would be a worse thing.
    
    Note that direct use of __initcall is discouraged, vs. one
    of the priority categorized subgroups.  As __initcall gets
    mapped onto device_initcall, our use of device_initcall
    directly in this change means that the runtime impact is
    zero -- they will remain at level 6 in initcall ordering.
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Acked-by: Geoff Levand <geoff@infradead.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 56f44848b044..43922509a483 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1124,4 +1124,4 @@ static int __init rtc_init(void)
 	return PTR_ERR_OR_ZERO(pdev);
 }
 
-module_init(rtc_init);
+device_initcall(rtc_init);

commit b6c295df3131c6fa25f8f29625ee0609506150ad
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:02 2015 +1100

    KVM: PPC: Book3S HV: Accumulate timing information for real-mode code
    
    This reads the timebase at various points in the real-mode guest
    entry/exit code and uses that to accumulate total, minimum and
    maximum time spent in those parts of the code.  Currently these
    times are accumulated per vcpu in 5 parts of the code:
    
    * rm_entry - time taken from the start of kvmppc_hv_entry() until
      just before entering the guest.
    * rm_intr - time from when we take a hypervisor interrupt in the
      guest until we either re-enter the guest or decide to exit to the
      host.  This includes time spent handling hcalls in real mode.
    * rm_exit - time from when we decide to exit the guest until the
      return from kvmppc_hv_entry().
    * guest - time spend in the guest
    * cede - time spent napping in real mode due to an H_CEDE hcall
      while other threads in the same vcore are active.
    
    These times are exposed in debugfs in a directory per vcpu that
    contains a file called "timings".  This file contains one line for
    each of the 5 timings above, with the name followed by a colon and
    4 numbers, which are the count (number of times the code has been
    executed), the total time, the minimum time, and the maximum time,
    all in nanoseconds.
    
    The overhead of the extra code amounts to about 30ns for an hcall that
    is handled in real mode (e.g. H_SET_DABR), which is about 25%.  Since
    production environments may not wish to incur this overhead, the new
    code is conditional on a new config symbol,
    CONFIG_KVM_BOOK3S_HV_EXIT_TIMING.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 2d7b33fab953..56f44848b044 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -608,6 +608,12 @@ void arch_suspend_enable_irqs(void)
 }
 #endif
 
+unsigned long long tb_to_ns(unsigned long long ticks)
+{
+	return mulhdu(ticks, tb_to_ns_scale) << tb_to_ns_shift;
+}
+EXPORT_SYMBOL_GPL(tb_to_ns);
+
 /*
  * Scheduler clock - returns current time in nanosec units.
  *

commit 18a8d49973667aa016e68826eeb374788b7c63b0
Merge: c189cb8ef628 ec02ace8ca0a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 21 12:30:30 2015 -0800

    Merge tag 'clk-for-linus-3.20' of git://git.linaro.org/people/mike.turquette/linux
    
    Pull clock framework updates from Mike Turquette:
     "The clock framework changes contain the usual driver additions,
      enhancements and fixes mostly for ARM32, ARM64, MIPS and Power-based
      devices.
    
      Additionally the framework core underwent a bit of surgery with two
      major changes:
    
       - The boundary between the clock core and clock providers (e.g clock
         drivers) is now more well defined with dedicated provider helper
         functions.  struct clk no longer maps 1:1 with the hardware clock
         but is a true per-user cookie which helps us tracker users of
         hardware clocks and debug bad behavior.
    
       - The addition of rate constraints for clocks.  Rate ranges are now
         supported which are analogous to the voltage ranges in the
         regulator framework.
    
      Unfortunately these changes to the core created some breakeage.  We
      think we fixed it all up but for this reason there are lots of last
      minute commits trying to undo the damage"
    
    * tag 'clk-for-linus-3.20' of git://git.linaro.org/people/mike.turquette/linux: (113 commits)
      clk: Only recalculate the rate if needed
      Revert "clk: mxs: Fix invalid 32-bit access to frac registers"
      clk: qoriq: Add support for the platform PLL
      powerpc/corenet: Enable CLK_QORIQ
      clk: Replace explicit clk assignment with __clk_hw_set_clk
      clk: Add __clk_hw_set_clk helper function
      clk: Don't dereference parent clock if is NULL
      MIPS: Alchemy: Remove bogus args from alchemy_clk_fgcs_detr
      clkdev: Always allocate a struct clk and call __clk_get() w/ CCF
      clk: shmobile: div6: Avoid division by zero in .round_rate()
      clk: mxs: Fix invalid 32-bit access to frac registers
      clk: omap: compile legacy omap3 clocks conditionally
      clkdev: Export clk_register_clkdev
      clk: Add rate constraints to clocks
      clk: remove clk-private.h
      pci: xgene: do not use clk-private.h
      arm: omap2+ remove dead clock code
      clk: Make clk API return per-user struct clk instances
      clk: tegra: Define PLLD_DSI and remove dsia(b)_mux
      clk: tegra: Add support for the Tegra132 CAR IP block
      ...

commit 4be1b29795d692d512bb67b770665d6f8ea5cb0b
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Thu Feb 12 15:01:28 2015 -0800

    powerpc: add running_clock for powerpc to prevent spurious softlockup warnings
    
    On POWER8 virtualised kernels the VTB register can be read to have a view
    of time that only increases while the guest is running.  This will prevent
    guests from seeing time jump if a guest is paused for significant amounts
    of time.
    
    On POWER7 and below virtualised kernels stolen time is subtracted from
    local_clock as a best effort approximation.  This will not eliminate
    spurious warnings in the case of a suspended guest but may reduce the
    occurance in the case of softlockups due to host over commit.
    
    Bare metal kernels should avoid reading the VTB as KVM does not restore
    sane values when not executing, the approxmation is fine as host kernels
    won't observe any stolen time.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Andrew Jones <drjones@redhat.com>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Cc: chai wen <chaiw.fnst@cn.fujitsu.com>
    Cc: Fabian Frederick <fabf@skynet.be>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: Ben Zhang <benzh@chromium.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index fa7c4f12104f..7316dd15278a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -621,6 +621,38 @@ unsigned long long sched_clock(void)
 	return mulhdu(get_tb() - boot_tb, tb_to_ns_scale) << tb_to_ns_shift;
 }
 
+
+#ifdef CONFIG_PPC_PSERIES
+
+/*
+ * Running clock - attempts to give a view of time passing for a virtualised
+ * kernels.
+ * Uses the VTB register if available otherwise a next best guess.
+ */
+unsigned long long running_clock(void)
+{
+	/*
+	 * Don't read the VTB as a host since KVM does not switch in host
+	 * timebase into the VTB when it takes a guest off the CPU, reading the
+	 * VTB would result in reading 'last switched out' guest VTB.
+	 *
+	 * Host kernels are often compiled with CONFIG_PPC_PSERIES checked, it
+	 * would be unsafe to rely only on the #ifdef above.
+	 */
+	if (firmware_has_feature(FW_FEATURE_LPAR) &&
+	    cpu_has_feature(CPU_FTR_ARCH_207S))
+		return mulhdu(get_vtb() - boot_tb, tb_to_ns_scale) << tb_to_ns_shift;
+
+	/*
+	 * This is a next best approximation without a VTB.
+	 * On a host which is running bare metal there should never be any stolen
+	 * time and on a host which doesn't do any virtualisation TB *should* equal
+	 * VTB so it makes no difference anyway.
+	 */
+	return local_clock() - cputime_to_nsecs(kcpustat_this_cpu->cpustat[CPUTIME_STEAL]);
+}
+#endif
+
 static int __init get_freq(char *name, int cells, unsigned long *val)
 {
 	struct device_node *cpu;

commit f0d373009205b53c7e14b6ac6d939ac5dcce60ca
Author: Kevin Hao <haokexin@gmail.com>
Date:   Wed Dec 3 16:53:52 2014 +0800

    powerpc: call of_clk_init() from time_init()
    
    So the boards which has COMMON_CLK enabled don't have to
    invoke this in its board specific file.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Acked-by: Scott Wood <scottwood@freescale.com>
    Acked-by: Michael Turquette <mturquette@linaro.org>
    Signed-off-by: Michael Turquette <mturquette@linaro.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index fa7c4f12104f..df9fa05b5fd3 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -54,6 +54,7 @@
 #include <linux/irq.h>
 #include <linux/delay.h>
 #include <linux/irq_work.h>
+#include <linux/clk-provider.h>
 #include <asm/trace.h>
 
 #include <asm/io.h>
@@ -943,6 +944,10 @@ void __init time_init(void)
 
 	init_decrementer_clockevent();
 	tick_setup_hrtimer_broadcast();
+
+#ifdef CONFIG_COMMON_CLK
+	of_clk_init(NULL);
+#endif
 }
 
 

commit 16b1d26e77b142546e2b9b6dc3b5aa5c44ae3b77
Author: Neelesh Gupta <neelegup@linux.vnet.ibm.com>
Date:   Tue Oct 14 14:08:36 2014 +0530

    rtc/tpo: Driver to support rtc and wakeup on PowerNV platform
    
    The patch implements the OPAL rtc driver that binds with the rtc
    driver subsystem. The driver uses the platform device infrastructure
    to probe the rtc device and register it to rtc class framework. The
    'wakeup' is supported depending upon the property 'has-tpo' present
    in the OF node. It provides a way to load the generic rtc driver in
    in the absence of an OPAL driver.
    
    The patch also moves the existing OPAL rtc get/set time interfaces to the
    new driver and exposes the necessary OPAL calls using EXPORT_SYMBOL_GPL.
    
    Test results:
    -------------
    Host:
    [root@tul169p1 ~]# ls -l /sys/class/rtc/
    total 0
    lrwxrwxrwx 1 root root 0 Oct 14 03:07 rtc0 -> ../../devices/opal-rtc/rtc/rtc0
    [root@tul169p1 ~]# cat /sys/devices/opal-rtc/rtc/rtc0/time
    08:10:07
    [root@tul169p1 ~]# echo `date '+%s' -d '+ 2 minutes'` > /sys/class/rtc/rtc0/wakealarm
    [root@tul169p1 ~]# cat /sys/class/rtc/rtc0/wakealarm
    1413274345
    [root@tul169p1 ~]#
    
    FSP:
    $ smgr mfgState
    standby
    $ rtim timeofday
    
    System time is valid: 2014/10/14 08:12:04.225115
    
    $ smgr mfgState
    ipling
    $
    
    CC: devicetree@vger.kernel.org
    CC: tglx@linutronix.de
    CC: rtc-linux@googlegroups.com
    CC: a.zummo@towertech.it
    Signed-off-by: Neelesh Gupta <neelegup@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 9f8ea617ff2c..fa7c4f12104f 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -989,6 +989,7 @@ void GregorianDay(struct rtc_time * tm)
 
 	tm->tm_wday = day % 7;
 }
+EXPORT_SYMBOL_GPL(GregorianDay);
 
 void to_tm(int tim, struct rtc_time * tm)
 {

commit 69111bac42f5ceacdd22e30947837ceb2c4493ed
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Oct 21 15:23:25 2014 -0500

    powerpc: Replace __get_cpu_var uses
    
    This still has not been merged and now powerpc is the only arch that does
    not have this change. Sorry about missing linuxppc-dev before.
    
    V2->V2
      - Fix up to work against 3.18-rc1
    
    __get_cpu_var() is used for multiple purposes in the kernel source. One of
    them is address calculation via the form &__get_cpu_var(x).  This calculates
    the address for the instance of the percpu variable of the current processor
    based on an offset.
    
    Other use cases are for storing and retrieving data from the current
    processors percpu area.  __get_cpu_var() can be used as an lvalue when
    writing data or on the right side of an assignment.
    
    __get_cpu_var() is defined as :
    
    __get_cpu_var() always only does an address determination. However, store
    and retrieve operations could use a segment prefix (or global register on
    other platforms) to avoid the address calculation.
    
    this_cpu_write() and this_cpu_read() can directly take an offset into a
    percpu area and use optimized assembly code to read and write per cpu
    variables.
    
    This patch converts __get_cpu_var into either an explicit address
    calculation using this_cpu_ptr() or into a use of this_cpu operations that
    use the offset.  Thereby address calculations are avoided and less registers
    are used when code is generated.
    
    At the end of the patch set all uses of __get_cpu_var have been removed so
    the macro is removed too.
    
    The patch set includes passes over all arches as well. Once these operations
    are used throughout then specialized macros can be defined in non -x86
    arches as well in order to optimize per cpu access by f.e.  using a global
    register that may be set to the per cpu base.
    
    Transformations done to __get_cpu_var()
    
    1. Determine the address of the percpu instance of the current processor.
    
            DEFINE_PER_CPU(int, y);
            int *x = &__get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(&y);
    
    2. Same as #1 but this time an array structure is involved.
    
            DEFINE_PER_CPU(int, y[20]);
            int *x = __get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(y);
    
    3. Retrieve the content of the current processors instance of a per cpu
    variable.
    
            DEFINE_PER_CPU(int, y);
            int x = __get_cpu_var(y)
    
       Converts to
    
            int x = __this_cpu_read(y);
    
    4. Retrieve the content of a percpu struct
    
            DEFINE_PER_CPU(struct mystruct, y);
            struct mystruct x = __get_cpu_var(y);
    
       Converts to
    
            memcpy(&x, this_cpu_ptr(&y), sizeof(x));
    
    5. Assignment to a per cpu variable
    
            DEFINE_PER_CPU(int, y)
            __get_cpu_var(y) = x;
    
       Converts to
    
            __this_cpu_write(y, x);
    
    6. Increment/Decrement etc of a per cpu variable
    
            DEFINE_PER_CPU(int, y);
            __get_cpu_var(y)++
    
       Converts to
    
            __this_cpu_inc(y)
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    CC: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    [mpe: Fix build errors caused by set/or_softirq_pending(), and rework
          assignment in __set_breakpoint() to use memcpy().]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 7505599c2593..9f8ea617ff2c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -458,9 +458,9 @@ static inline void clear_irq_work_pending(void)
 
 DEFINE_PER_CPU(u8, irq_work_pending);
 
-#define set_irq_work_pending_flag()	__get_cpu_var(irq_work_pending) = 1
-#define test_irq_work_pending()		__get_cpu_var(irq_work_pending)
-#define clear_irq_work_pending()	__get_cpu_var(irq_work_pending) = 0
+#define set_irq_work_pending_flag()	__this_cpu_write(irq_work_pending, 1)
+#define test_irq_work_pending()		__this_cpu_read(irq_work_pending)
+#define clear_irq_work_pending()	__this_cpu_write(irq_work_pending, 0)
 
 #endif /* 32 vs 64 bit */
 
@@ -482,8 +482,8 @@ void arch_irq_work_raise(void)
 static void __timer_interrupt(void)
 {
 	struct pt_regs *regs = get_irq_regs();
-	u64 *next_tb = &__get_cpu_var(decrementers_next_tb);
-	struct clock_event_device *evt = &__get_cpu_var(decrementers);
+	u64 *next_tb = this_cpu_ptr(&decrementers_next_tb);
+	struct clock_event_device *evt = this_cpu_ptr(&decrementers);
 	u64 now;
 
 	trace_timer_interrupt_entry(regs);
@@ -498,7 +498,7 @@ static void __timer_interrupt(void)
 		*next_tb = ~(u64)0;
 		if (evt->event_handler)
 			evt->event_handler(evt);
-		__get_cpu_var(irq_stat).timer_irqs_event++;
+		__this_cpu_inc(irq_stat.timer_irqs_event);
 	} else {
 		now = *next_tb - now;
 		if (now <= DECREMENTER_MAX)
@@ -506,13 +506,13 @@ static void __timer_interrupt(void)
 		/* We may have raced with new irq work */
 		if (test_irq_work_pending())
 			set_dec(1);
-		__get_cpu_var(irq_stat).timer_irqs_others++;
+		__this_cpu_inc(irq_stat.timer_irqs_others);
 	}
 
 #ifdef CONFIG_PPC64
 	/* collect purr register values often, for accurate calculations */
 	if (firmware_has_feature(FW_FEATURE_SPLPAR)) {
-		struct cpu_usage *cu = &__get_cpu_var(cpu_usage_array);
+		struct cpu_usage *cu = this_cpu_ptr(&cpu_usage_array);
 		cu->current_tb = mfspr(SPRN_PURR);
 	}
 #endif
@@ -527,7 +527,7 @@ static void __timer_interrupt(void)
 void timer_interrupt(struct pt_regs * regs)
 {
 	struct pt_regs *old_regs;
-	u64 *next_tb = &__get_cpu_var(decrementers_next_tb);
+	u64 *next_tb = this_cpu_ptr(&decrementers_next_tb);
 
 	/* Ensure a positive value is written to the decrementer, or else
 	 * some CPUs will continue to take decrementer exceptions.
@@ -813,7 +813,7 @@ static void __init clocksource_init(void)
 static int decrementer_set_next_event(unsigned long evt,
 				      struct clock_event_device *dev)
 {
-	__get_cpu_var(decrementers_next_tb) = get_tb_or_rtc() + evt;
+	__this_cpu_write(decrementers_next_tb, get_tb_or_rtc() + evt);
 	set_dec(evt);
 
 	/* We may have raced with new irq work */
@@ -833,7 +833,7 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 /* Interrupt handler for the timer broadcast IPI */
 void tick_broadcast_ipi_handler(void)
 {
-	u64 *next_tb = &__get_cpu_var(decrementers_next_tb);
+	u64 *next_tb = this_cpu_ptr(&decrementers_next_tb);
 
 	*next_tb = get_tb_or_rtc();
 	__timer_interrupt();

commit e51df2c170efaeadce4d416e1825b0830de0a795
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 20 08:55:18 2014 +1000

    powerpc: Make a bunch of things static
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 055558b953e3..7505599c2593 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -479,7 +479,7 @@ void arch_irq_work_raise(void)
 
 #endif /* CONFIG_IRQ_WORK */
 
-void __timer_interrupt(void)
+static void __timer_interrupt(void)
 {
 	struct pt_regs *regs = get_irq_regs();
 	u64 *next_tb = &__get_cpu_var(decrementers_next_tb);
@@ -643,7 +643,7 @@ static int __init get_freq(char *name, int cells, unsigned long *val)
 	return found;
 }
 
-void start_cpu_decrementer(void)
+static void start_cpu_decrementer(void)
 {
 #if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
 	/* Clear any pending timer interrupts */

commit e1802b065d189cdfa25eaf6d019c222a91618b9c
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 20 08:00:02 2014 +1000

    powerpc: Move more symbol exports next to function definitions
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 368ab374d33c..055558b953e3 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1024,6 +1024,7 @@ void to_tm(int tim, struct rtc_time * tm)
 	 */
 	GregorianDay(tm);
 }
+EXPORT_SYMBOL(to_tm);
 
 /*
  * Divide a 128-bit dividend by a 32-bit divisor, leaving a 128 bit

commit 4a0e637738f06673725792d74eed67f8779b62c7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 16 21:05:13 2014 +0000

    clocksource: Get rid of cycle_last
    
    cycle_last was added to the clocksource to support the TSC
    validation. We moved that to the core code, so we can get rid of the
    extra copy.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 9fff9cdcc519..368ab374d33c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -741,7 +741,7 @@ static cycle_t timebase_read(struct clocksource *cs)
 }
 
 void update_vsyscall_old(struct timespec *wall_time, struct timespec *wtm,
-			struct clocksource *clock, u32 mult)
+			 struct clocksource *clock, u32 mult, cycle_t cycle_last)
 {
 	u64 new_tb_to_xs, new_stamp_xsec;
 	u32 frac_sec;
@@ -774,7 +774,7 @@ void update_vsyscall_old(struct timespec *wall_time, struct timespec *wtm,
 	 * We expect the caller to have done the first increment of
 	 * vdso_data->tb_update_count already.
 	 */
-	vdso_data->tb_orig_stamp = clock->cycle_last;
+	vdso_data->tb_orig_stamp = cycle_last;
 	vdso_data->stamp_xsec = new_stamp_xsec;
 	vdso_data->tb_to_xs = new_tb_to_xs;
 	vdso_data->wtom_clock_sec = wtm->tv_sec;

commit 6e0fdf9af216887e0032c19d276889aad41cad00
Author: Paul Bolle <pebolle@tiscali.nl>
Date:   Tue May 20 22:24:58 2014 +0200

    powerpc: fix typo 'CONFIG_PMAC'
    
    Commit b0d278b7d3ae ("powerpc/perf_event: Reduce latency of calling
    perf_event_do_pending") added a check for CONFIG_PMAC were a check for
    CONFIG_PPC_PMAC was clearly intended.
    
    Fixes: b0d278b7d3ae ("powerpc/perf_event: Reduce latency of calling perf_event_do_pending")
    Signed-off-by: Paul Bolle <pebolle@tiscali.nl>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 7e711bdcc6da..9fff9cdcc519 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -551,7 +551,7 @@ void timer_interrupt(struct pt_regs * regs)
 	may_hard_irq_enable();
 
 
-#if defined(CONFIG_PPC32) && defined(CONFIG_PMAC)
+#if defined(CONFIG_PPC32) && defined(CONFIG_PPC_PMAC)
 	if (atomic_read(&ppc_n_lost_interrupts) != 0)
 		do_IRQ(regs);
 #endif

commit 8050936caf125fbe54111ba5e696b68a360556ba
Author: Anton Blanchard <anton@samba.org>
Date:   Fri May 9 17:47:12 2014 +1000

    powerpc: irq work racing with timer interrupt can result in timer interrupt hang
    
    I am seeing an issue where a CPU running perf eventually hangs.
    Traces show timer interrupts happening every 4 seconds even
    when a userspace task is running on the CPU. /proc/timer_list
    also shows pending hrtimers have not run in over an hour,
    including the scheduler.
    
    Looking closer, decrementers_next_tb is getting set to
    0xffffffffffffffff, and at that point we will never take
    a timer interrupt again.
    
    In __timer_interrupt() we set decrementers_next_tb to
    0xffffffffffffffff and rely on ->event_handler to update it:
    
            *next_tb = ~(u64)0;
            if (evt->event_handler)
                    evt->event_handler(evt);
    
    In this case ->event_handler is hrtimer_interrupt. This will eventually
    call back through the clockevents code with the next event to be
    programmed:
    
    static int decrementer_set_next_event(unsigned long evt,
                                          struct clock_event_device *dev)
    {
            /* Don't adjust the decrementer if some irq work is pending */
            if (test_irq_work_pending())
                    return 0;
            __get_cpu_var(decrementers_next_tb) = get_tb_or_rtc() + evt;
    
    If irq work came in between these two points, we will return
    before updating decrementers_next_tb and we never process a timer
    interrupt again.
    
    This looks to have been introduced by 0215f7d8c53f (powerpc: Fix races
    with irq_work). Fix it by removing the early exit and relying on
    code later on in the function to force an early decrementer:
    
           /* We may have raced with new irq work */
           if (test_irq_work_pending())
                   set_dec(1);
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: stable@vger.kernel.org # 3.14+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 122a580f7322..7e711bdcc6da 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -813,9 +813,6 @@ static void __init clocksource_init(void)
 static int decrementer_set_next_event(unsigned long evt,
 				      struct clock_event_device *dev)
 {
-	/* Don't adjust the decrementer if some irq work is pending */
-	if (test_irq_work_pending())
-		return 0;
 	__get_cpu_var(decrementers_next_tb) = get_tb_or_rtc() + evt;
 	set_dec(evt);
 

commit 0d94873011a10cea78d1bc0ba8cfc4203559d534
Author: Preeti U Murthy <preeti@linux.vnet.ibm.com>
Date:   Wed Feb 26 05:39:06 2014 +0530

    cpuidle/powernv: Add "Fast-Sleep" CPU idle state
    
    Fast sleep is one of the deep idle states on Power8 in which local timers of
    CPUs stop. On PowerPC we do not have an external clock device which can
    handle wakeup of such CPUs. Now that we have the support in the tick broadcast
    framework for archs that do not sport such a device and the low level support
    for fast sleep, enable it in the cpuidle framework on PowerNV.
    
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index df2989b0d4c0..122a580f7322 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -42,6 +42,7 @@
 #include <linux/timex.h>
 #include <linux/kernel_stat.h>
 #include <linux/time.h>
+#include <linux/clockchips.h>
 #include <linux/init.h>
 #include <linux/profile.h>
 #include <linux/cpu.h>
@@ -106,7 +107,7 @@ struct clock_event_device decrementer_clockevent = {
 	.irq            = 0,
 	.set_next_event = decrementer_set_next_event,
 	.set_mode       = decrementer_set_mode,
-	.features       = CLOCK_EVT_FEAT_ONESHOT,
+	.features       = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_C3STOP,
 };
 EXPORT_SYMBOL(decrementer_clockevent);
 
@@ -944,6 +945,7 @@ void __init time_init(void)
 	clocksource_init();
 
 	init_decrementer_clockevent();
+	tick_setup_hrtimer_broadcast();
 }
 
 

commit 1b7839559b3f1c7a09ff94904788a732063ce2de
Author: Preeti U Murthy <preeti@linux.vnet.ibm.com>
Date:   Wed Feb 26 05:38:01 2014 +0530

    powerpc: Split timer_interrupt() into timer handling and interrupt handling routines
    
    Split timer_interrupt(), which is the local timer interrupt handler on ppc
    into routines called during regular interrupt handling and __timer_interrupt(),
    which takes care of running local timers and collecting time related stats.
    
    This will enable callers interested only in running expired local timers to
    directly call into __timer_interupt(). One of the use cases of this is the
    tick broadcast IPI handling in which the sleeping CPUs need to handle the local
    timers that have expired.
    
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 3ff97dbb35be..df2989b0d4c0 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -478,6 +478,47 @@ void arch_irq_work_raise(void)
 
 #endif /* CONFIG_IRQ_WORK */
 
+void __timer_interrupt(void)
+{
+	struct pt_regs *regs = get_irq_regs();
+	u64 *next_tb = &__get_cpu_var(decrementers_next_tb);
+	struct clock_event_device *evt = &__get_cpu_var(decrementers);
+	u64 now;
+
+	trace_timer_interrupt_entry(regs);
+
+	if (test_irq_work_pending()) {
+		clear_irq_work_pending();
+		irq_work_run();
+	}
+
+	now = get_tb_or_rtc();
+	if (now >= *next_tb) {
+		*next_tb = ~(u64)0;
+		if (evt->event_handler)
+			evt->event_handler(evt);
+		__get_cpu_var(irq_stat).timer_irqs_event++;
+	} else {
+		now = *next_tb - now;
+		if (now <= DECREMENTER_MAX)
+			set_dec((int)now);
+		/* We may have raced with new irq work */
+		if (test_irq_work_pending())
+			set_dec(1);
+		__get_cpu_var(irq_stat).timer_irqs_others++;
+	}
+
+#ifdef CONFIG_PPC64
+	/* collect purr register values often, for accurate calculations */
+	if (firmware_has_feature(FW_FEATURE_SPLPAR)) {
+		struct cpu_usage *cu = &__get_cpu_var(cpu_usage_array);
+		cu->current_tb = mfspr(SPRN_PURR);
+	}
+#endif
+
+	trace_timer_interrupt_exit(regs);
+}
+
 /*
  * timer_interrupt - gets called when the decrementer overflows,
  * with interrupts disabled.
@@ -486,8 +527,6 @@ void timer_interrupt(struct pt_regs * regs)
 {
 	struct pt_regs *old_regs;
 	u64 *next_tb = &__get_cpu_var(decrementers_next_tb);
-	struct clock_event_device *evt = &__get_cpu_var(decrementers);
-	u64 now;
 
 	/* Ensure a positive value is written to the decrementer, or else
 	 * some CPUs will continue to take decrementer exceptions.
@@ -519,39 +558,7 @@ void timer_interrupt(struct pt_regs * regs)
 	old_regs = set_irq_regs(regs);
 	irq_enter();
 
-	trace_timer_interrupt_entry(regs);
-
-	if (test_irq_work_pending()) {
-		clear_irq_work_pending();
-		irq_work_run();
-	}
-
-	now = get_tb_or_rtc();
-	if (now >= *next_tb) {
-		*next_tb = ~(u64)0;
-		if (evt->event_handler)
-			evt->event_handler(evt);
-		__get_cpu_var(irq_stat).timer_irqs_event++;
-	} else {
-		now = *next_tb - now;
-		if (now <= DECREMENTER_MAX)
-			set_dec((int)now);
-		/* We may have raced with new irq work */
-		if (test_irq_work_pending())
-			set_dec(1);
-		__get_cpu_var(irq_stat).timer_irqs_others++;
-	}
-
-#ifdef CONFIG_PPC64
-	/* collect purr register values often, for accurate calculations */
-	if (firmware_has_feature(FW_FEATURE_SPLPAR)) {
-		struct cpu_usage *cu = &__get_cpu_var(cpu_usage_array);
-		cu->current_tb = mfspr(SPRN_PURR);
-	}
-#endif
-
-	trace_timer_interrupt_exit(regs);
-
+	__timer_interrupt();
 	irq_exit();
 	set_irq_regs(old_regs);
 }
@@ -828,6 +835,10 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 /* Interrupt handler for the timer broadcast IPI */
 void tick_broadcast_ipi_handler(void)
 {
+	u64 *next_tb = &__get_cpu_var(decrementers_next_tb);
+
+	*next_tb = get_tb_or_rtc();
+	__timer_interrupt();
 }
 
 static void register_decrementer_clockevent(int cpu)

commit 1b67bee129a36c22c17186cc2a9981678e9323ee
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed Feb 26 05:37:43 2014 +0530

    powerpc: Implement tick broadcast IPI as a fixed IPI message
    
    For scalability and performance reasons, we want the tick broadcast IPIs
    to be handled as efficiently as possible. Fixed IPI messages
    are one of the most efficient mechanisms available - they are faster than
    the smp_call_function mechanism because the IPI handlers are fixed and hence
    they don't involve costly operations such as adding IPI handlers to the target
    CPU's function queue, acquiring locks for synchronization etc.
    
    Luckily we have an unused IPI message slot, so use that to implement
    tick broadcast IPIs efficiently.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    [Functions renamed to tick_broadcast* and Changelog modified by
     Preeti U. Murthy<preeti@linux.vnet.ibm.com>]
    Signed-off-by: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Acked-by: Geoff Levand <geoff@infradead.org> [For the PS3 part]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index b3dab20acf34..3ff97dbb35be 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -825,6 +825,11 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 		decrementer_set_next_event(DECREMENTER_MAX, dev);
 }
 
+/* Interrupt handler for the timer broadcast IPI */
+void tick_broadcast_ipi_handler(void)
+{
+}
+
 static void register_decrementer_clockevent(int cpu)
 {
 	struct clock_event_device *dec = &per_cpu(decrementers, cpu);

commit 0215f7d8c53fb192cd4491ede0ece5cca6b5db57
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jan 14 17:11:39 2014 +1100

    powerpc: Fix races with irq_work
    
    If we set irq_work on a processor and immediately afterward, before the
    irq work has a chance to be processed, we change the decrementer value,
    we can seriously delay the handling of that irq_work.
    
    Fix it by checking in a few places for pending irq work, first before
    changing the decrementer in decrementer_set_next_event() and after
    changing it in the same function and in timer_interrupt().
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index afb1b56ef4fa..b3dab20acf34 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -536,6 +536,9 @@ void timer_interrupt(struct pt_regs * regs)
 		now = *next_tb - now;
 		if (now <= DECREMENTER_MAX)
 			set_dec((int)now);
+		/* We may have raced with new irq work */
+		if (test_irq_work_pending())
+			set_dec(1);
 		__get_cpu_var(irq_stat).timer_irqs_others++;
 	}
 
@@ -802,8 +805,16 @@ static void __init clocksource_init(void)
 static int decrementer_set_next_event(unsigned long evt,
 				      struct clock_event_device *dev)
 {
+	/* Don't adjust the decrementer if some irq work is pending */
+	if (test_irq_work_pending())
+		return 0;
 	__get_cpu_var(decrementers_next_tb) = get_tb_or_rtc() + evt;
 	set_dec(evt);
+
+	/* We may have raced with new irq work */
+	if (test_irq_work_pending())
+		set_dec(1);
+
 	return 0;
 }
 

commit c041cfa2af1ccb8d0346dc576144a1085e9b4d4b
Author: fan.du <fan.du@windriver.com>
Date:   Wed Jan 23 16:06:11 2013 +0800

    powerpc: Make irq_stat.timers_irqs counting more specific
    
    Current irq_stat.timers_irqs counting doesn't discriminate timer event handler
    and other timer interrupt(like arch_irq_work_raise). Sometimes we need to know
    exactly how much interrupts timer event handler fired, so let's be more specific
    on this.
    
    Signed-off-by: Fan Du <fan.du@windriver.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index b3b144121cc9..afb1b56ef4fa 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -510,7 +510,6 @@ void timer_interrupt(struct pt_regs * regs)
 	 */
 	may_hard_irq_enable();
 
-	__get_cpu_var(irq_stat).timer_irqs++;
 
 #if defined(CONFIG_PPC32) && defined(CONFIG_PMAC)
 	if (atomic_read(&ppc_n_lost_interrupts) != 0)
@@ -532,10 +531,12 @@ void timer_interrupt(struct pt_regs * regs)
 		*next_tb = ~(u64)0;
 		if (evt->event_handler)
 			evt->event_handler(evt);
+		__get_cpu_var(irq_stat).timer_irqs_event++;
 	} else {
 		now = *next_tb - now;
 		if (now <= DECREMENTER_MAX)
 			set_dec((int)now);
+		__get_cpu_var(irq_stat).timer_irqs_others++;
 	}
 
 #ifdef CONFIG_PPC64

commit 84b073868b9d9e754ae48b828337633d1b386482
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Nov 17 11:39:05 2013 +1100

    powerpc/pseries: Duplicate dtl entries sometimes sent to userspace
    
    When reading from the dispatch trace log (dtl) userspace interface, I
    sometimes see duplicate entries. One example:
    
    # hexdump -C dtl.out
    
    00000000  07 04 00 0c 00 00 48 44  00 00 00 00 00 00 00 00
    00000010  00 0c a0 b4 16 83 6d 68  00 00 00 00 00 00 00 00
    00000020  00 00 00 00 10 00 13 50  80 00 00 00 00 00 d0 32
    
    00000030  07 04 00 0c 00 00 48 44  00 00 00 00 00 00 00 00
    00000040  00 0c a0 b4 16 83 6d 68  00 00 00 00 00 00 00 00
    00000050  00 00 00 00 10 00 13 50  80 00 00 00 00 00 d0 32
    
    The problem is in scan_dispatch_log() where we call dtl_consumer()
    but bail out before incrementing the index.
    
    To fix this I moved dtl_consumer() after the timebase comparison.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: stable@vger.kernel.org
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 192b051df97e..b3b144121cc9 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -213,8 +213,6 @@ static u64 scan_dispatch_log(u64 stop_tb)
 	if (i == be64_to_cpu(vpa->dtl_idx))
 		return 0;
 	while (i < be64_to_cpu(vpa->dtl_idx)) {
-		if (dtl_consumer)
-			dtl_consumer(dtl, i);
 		dtb = be64_to_cpu(dtl->timebase);
 		tb_delta = be32_to_cpu(dtl->enqueue_to_dispatch_time) +
 			be32_to_cpu(dtl->ready_to_enqueue_time);
@@ -227,6 +225,8 @@ static u64 scan_dispatch_log(u64 stop_tb)
 		}
 		if (dtb > stop_tb)
 			break;
+		if (dtl_consumer)
+			dtl_consumer(dtl, i);
 		stolen += tb_delta;
 		++i;
 		++dtl;

commit 39eda2aba6be642b71f2e0ad623dcb09fd9d79cf
Merge: 2e515bf096c2 9f24b0c9ef9b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 6 10:49:42 2013 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Ben Herrenschmidt:
     "Here's the powerpc batch for this merge window.  Some of the
      highlights are:
    
       - A bunch of endian fixes ! We don't have full LE support yet in that
         release but this contains a lot of fixes all over arch/powerpc to
         use the proper accessors, call the firmware with the right endian
         mode, etc...
    
       - A few updates to our "powernv" platform (non-virtualized, the one
         to run KVM on), among other, support for bridging the P8 LPC bus
         for UARTs, support and some EEH fixes.
    
       - Some mpc51xx clock API cleanups in preparation for a clock API
         overhaul
    
       - A pile of cleanups of our old math emulation code, including better
         support for using it to emulate optional FP instructions on
         embedded chips that otherwise have a HW FPU.
    
       - Some infrastructure in selftest, for powerpc now, but could be
         generalized, initially used by some tests for our perf instruction
         counting code.
    
       - A pile of fixes for hotplug on pseries (that was seriously
         bitrotting)
    
       - The usual slew of freescale embedded updates, new boards, 64-bit
         hiberation support, e6500 core PMU support, etc..."
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (146 commits)
      powerpc: Correct FSCR bit definitions
      powerpc/xmon: Fix printing of set of CPUs in xmon
      powerpc/pseries: Move lparcfg.c to platforms/pseries
      powerpc/powernv: Return secondary CPUs to firmware on kexec
      powerpc/btext: Fix CONFIG_PPC_EARLY_DEBUG_BOOTX on ppc32
      powerpc: Cleanup handling of the DSCR bit in the FSCR register
      powerpc/pseries: Child nodes are not detached by dlpar_detach_node
      powerpc/pseries: Add mising of_node_put in delete_dt_node
      powerpc/pseries: Make dlpar_configure_connector parent node aware
      powerpc/pseries: Do all node initialization in dlpar_parse_cc_node
      powerpc/pseries: Fix parsing of initial node path in update_dt_node
      powerpc/pseries: Pack update_props_workarea to map correctly to rtas buffer header
      powerpc/pseries: Fix over writing of rtas return code in update_dt_node
      powerpc/pseries: Fix creation of loop in device node property list
      powerpc: Skip emulating & leave interrupts off for kernel program checks
      powerpc: Add more exception trampolines for hypervisor exceptions
      powerpc: Fix location and rename exception trampolines
      powerpc: Add more trap names to xmon
      powerpc/pseries: Add a warning in the case of cross-cpu VPA registration
      powerpc: Update the 00-Index in Documentation/powerpc
      ...

commit 7ffcf8ec26f4b94b95b1297131d223b121d951e5
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 7 02:01:46 2013 +1000

    powerpc: Fix little endian lppaca, slb_shadow and dtl_entry
    
    The lppaca, slb_shadow and dtl_entry hypervisor structures are
    big endian, so we have to byte swap them in little endian builds.
    
    LE KVM hosts will also need to be fixed but for now add an #error
    to remind us.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index c863aa1f524a..b2bcd34f72d2 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -210,18 +210,18 @@ static u64 scan_dispatch_log(u64 stop_tb)
 	if (!dtl)
 		return 0;
 
-	if (i == vpa->dtl_idx)
+	if (i == be64_to_cpu(vpa->dtl_idx))
 		return 0;
-	while (i < vpa->dtl_idx) {
+	while (i < be64_to_cpu(vpa->dtl_idx)) {
 		if (dtl_consumer)
 			dtl_consumer(dtl, i);
-		dtb = dtl->timebase;
-		tb_delta = dtl->enqueue_to_dispatch_time +
-			dtl->ready_to_enqueue_time;
+		dtb = be64_to_cpu(dtl->timebase);
+		tb_delta = be32_to_cpu(dtl->enqueue_to_dispatch_time) +
+			be32_to_cpu(dtl->ready_to_enqueue_time);
 		barrier();
-		if (i + N_DISPATCH_LOG < vpa->dtl_idx) {
+		if (i + N_DISPATCH_LOG < be64_to_cpu(vpa->dtl_idx)) {
 			/* buffer has overflowed */
-			i = vpa->dtl_idx - N_DISPATCH_LOG;
+			i = be64_to_cpu(vpa->dtl_idx) - N_DISPATCH_LOG;
 			dtl = local_paca->dispatch_log + (i % N_DISPATCH_LOG);
 			continue;
 		}
@@ -269,7 +269,7 @@ static inline u64 calculate_stolen_time(u64 stop_tb)
 {
 	u64 stolen = 0;
 
-	if (get_paca()->dtl_ridx != get_paca()->lppaca_ptr->dtl_idx) {
+	if (get_paca()->dtl_ridx != be64_to_cpu(get_lppaca()->dtl_idx)) {
 		stolen = scan_dispatch_log(stop_tb);
 		get_paca()->system_time -= stolen;
 	}

commit 6f7aba7bb4bd09824992cda7f77d4e8b1704a04c
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 7 02:01:34 2013 +1000

    powerpc: Add some endian annotations to time and xics code
    
    Fix a couple of sparse warnings.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 65ab9e909377..c863aa1f524a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -612,7 +612,7 @@ unsigned long long sched_clock(void)
 static int __init get_freq(char *name, int cells, unsigned long *val)
 {
 	struct device_node *cpu;
-	const unsigned int *fp;
+	const __be32 *fp;
 	int found = 0;
 
 	/* The cpu node should have timebase and clock frequency properties */

commit 8c6ffba0eddc8c110dbf444f51354ce42069abfc
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Mon Jul 15 11:20:32 2013 +0930

    PTR_RET is now PTR_ERR_OR_ZERO(): Replace most.
    
    Sweep of the simple cases.
    
    Cc: netdev@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Julia Lawall <julia.lawall@lip6.fr>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 65ab9e909377..cdcc156865ef 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1049,7 +1049,7 @@ static int __init rtc_init(void)
 
 	pdev = platform_device_register_simple("rtc-generic", -1, NULL, 0);
 
-	return PTR_RET(pdev);
+	return PTR_ERR_OR_ZERO(pdev);
 }
 
 module_init(rtc_init);

commit 061d19f279f9bebbdb1ee48bef8c25e03de32ae2
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon Jun 24 15:30:09 2013 -0400

    powerpc: Delete __cpuinit usage from all users
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    This removes all the powerpc uses of the __cpuinit macros.  There
    are no __CPUINIT users in assembly files in powerpc.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Josh Boyer <jwboyer@gmail.com>
    Cc: Matt Porter <mporter@kernel.crashing.org>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 5fc29ad7e26f..65ab9e909377 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -631,7 +631,6 @@ static int __init get_freq(char *name, int cells, unsigned long *val)
 	return found;
 }
 
-/* should become __cpuinit when secondary_cpu_time_init also is */
 void start_cpu_decrementer(void)
 {
 #if defined(CONFIG_BOOKE) || defined(CONFIG_40x)

commit 09652b00cd249a1f1a2cd1a70d31e880f24d06cc
Author: Adrian-Leonard Radu <ady8radu@gmail.com>
Date:   Sun Mar 10 03:07:00 2013 +0000

    powerpc: Use PTR_RET instead of IS_ERR/PTR_ERR
    
    Signed-off-by: Adrian-Leonard Radu <ady8radu@gmail.com>
    Acked-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index f77fa22754bc..5fc29ad7e26f 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1049,10 +1049,8 @@ static int __init rtc_init(void)
 		return -ENODEV;
 
 	pdev = platform_device_register_simple("rtc-generic", -1, NULL, 0);
-	if (IS_ERR(pdev))
-		return PTR_ERR(pdev);
 
-	return 0;
+	return PTR_RET(pdev);
 }
 
 module_init(rtc_init);

commit 266d7ad7f4fe2f44b91561f5b812115c1b3018ab
Merge: bcbd818c069b 36dfbbf136db
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 19 19:05:45 2013 -0800

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer changes from Ingo Molnar:
     "Main changes:
    
       - ntp: Add CONFIG_RTC_SYSTOHC: a generic RTC driver facility
         complementing the existing CONFIG_RTC_HCTOSYS, which uses NTP to
         keep the hardware clock updated.
    
       - posix-timers: Fix clock_adjtime to always return timex data on
         success.  This is changing the ABI, but no breakage was expected
         and found - caution is warranted nevertheless.
    
       - platform persistent clock improvements/cleanups.
    
       - clockevents: refactor timer broadcast handling to be more generic
         and less duplicated with matching architecture code (mostly ARM
         motivated.)
    
       - various fixes and cleanups"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      timers/x86/hpet: Use HPET_COUNTER to specify the hpet counter in vread_hpet()
      posix-cpu-timers: Fix nanosleep task_struct leak
      clockevents: Fix generic broadcast for FEAT_C3STOP
      time, Fix setting of hardware clock in NTP code
      hrtimer: Prevent hrtimer_enqueue_reprogram race
      clockevents: Add generic timer broadcast function
      clockevents: Add generic timer broadcast receiver
      timekeeping: Switch HAS_PERSISTENT_CLOCK to ALWAYS_USE_PERSISTENT_CLOCK
      x86/time/rtc: Don't print extended CMOS year when reading RTC
      x86: Select HAS_PERSISTENT_CLOCK on x86
      timekeeping: Add CONFIG_HAS_PERSISTENT_CLOCK option
      rtc: Skip the suspend/resume handling if persistent clock exist
      timekeeping: Add persistent_clock_exist flag
      posix-timers: Fix clock_adjtime to always return timex data on success
      Round the calculated scale factor in set_cyc2ns_scale()
      NTP: Add a CONFIG_RTC_SYSTOHC configuration
      MAINTAINERS: Update John Stultz's email
      time: create __getnstimeofday for WARNless calls

commit d652e1eb8e7b739fccbfb503a3da3e9f640fbf3d
Merge: 8f55cea410db 77852fea6e24
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 19 18:19:48 2013 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "Main changes:
    
       - scheduler side full-dynticks (user-space execution is undisturbed
         and receives no timer IRQs) preparation changes that convert the
         cputime accounting code to be full-dynticks ready, from Frederic
         Weisbecker.
    
       - Initial sched.h split-up changes, by Clark Williams
    
       - select_idle_sibling() performance improvement by Mike Galbraith:
    
            " 1 tbench pair (worst case) in a 10 core + SMT package:
    
              pre   15.22 MB/sec 1 procs
              post 252.01 MB/sec 1 procs "
    
      - sched_rr_get_interval() ABI fix/change.  We think this detail is not
        used by apps (so it's not an ABI in practice), but lets keep it
        under observation.
    
      - misc RT scheduling cleanups, optimizations"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      sched/rt: Add <linux/sched/rt.h> header to <linux/init_task.h>
      cputime: Remove irqsave from seqlock readers
      sched, powerpc: Fix sched.h split-up build failure
      cputime: Restore CPU_ACCOUNTING config defaults for PPC64
      sched/rt: Move rt specific bits into new header file
      sched/rt: Add a tuning knob to allow changing SCHED_RR timeslice
      sched: Move sched.h sysctl bits into separate header
      sched: Fix signedness bug in yield_to()
      sched: Fix select_idle_sibling() bouncing cow syndrome
      sched/rt: Further simplify pick_rt_task()
      sched/rt: Do not account zero delta_exec in update_curr_rt()
      cputime: Safely read cputime of full dynticks CPUs
      kvm: Prepare to add generic guest entry/exit callbacks
      cputime: Use accessors to read task cputime stats
      cputime: Allow dynamic switch between tick/virtual based cputime accounting
      cputime: Generic on-demand virtual cputime accounting
      cputime: Move default nsecs_to_cputime() to jiffies based cputime file
      cputime: Librarize per nsecs resolution cputime definitions
      cputime: Avoid multiplication overflow on utime scaling
      context_tracking: Export context state for generic vtime
      ...
    
    Fix up conflict in kernel/context_tracking.c due to comment additions.

commit 90889a635a9b5488624bccce3ff6b2eec68c007b
Merge: a9037430c6c7 6f16eebe1ff8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 4 11:03:03 2013 +0100

    Merge branch 'fortglx/3.9/time' of git://git.linaro.org/people/jstultz/linux into timers/core
    
    Trivial conflict in arch/x86/Kconfig
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 689dfa894c57842a05bf6dc9f97e6bb71ec5f386
Author: Tiejun Chen <tiejun.chen@windriver.com>
Date:   Tue Jan 15 17:01:19 2013 +0000

    powerpc: Max next_tb to prevent from replaying timer interrupt
    
    With lazy interrupt, we always call __check_irq_replaysome with
    decrementers_next_tb to check if we need to replay timer interrupt.
    So in hotplug case we also need to set decrementers_next_tb as MAX
    to make sure __check_irq_replay don't replay timer interrupt
    when return as we expect, otherwise we'll trap here infinitely.
    
    Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 6f6b1cccc916..127361e093f4 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -494,10 +494,15 @@ void timer_interrupt(struct pt_regs * regs)
 	set_dec(DECREMENTER_MAX);
 
 	/* Some implementations of hotplug will get timer interrupts while
-	 * offline, just ignore these
+	 * offline, just ignore these and we also need to set
+	 * decrementers_next_tb as MAX to make sure __check_irq_replay
+	 * don't replay timer interrupt when return, otherwise we'll trap
+	 * here infinitely :(
 	 */
-	if (!cpu_online(smp_processor_id()))
+	if (!cpu_online(smp_processor_id())) {
+		*next_tb = ~(u64)0;
 		return;
+	}
 
 	/* Conditionally hard-enable interrupts now that the DEC has been
 	 * bumped to its maximum value

commit c11f11fcbdb5be790c565aed46411486a7586afc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Jan 21 00:50:22 2013 +0100

    kvm: Prepare to add generic guest entry/exit callbacks
    
    Do some ground preparatory work before adding guest_enter()
    and guest_exit() context tracking callbacks. Those will
    be later used to read the guest cputime safely when we
    run in full dynticks mode.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Gleb Natapov <gleb@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 22c9b67f9983..2e04b37f67f9 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -347,6 +347,7 @@ void vtime_account_system(struct task_struct *tsk)
 	if (stolen)
 		account_steal_time(stolen);
 }
+EXPORT_SYMBOL_GPL(vtime_account_system);
 
 void vtime_account_idle(struct task_struct *tsk)
 {

commit abf917cd91cbb73952758f9741e2fa65002a48ee
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 25 07:56:04 2012 +0200

    cputime: Generic on-demand virtual cputime accounting
    
    If we want to stop the tick further idle, we need to be
    able to account the cputime without using the tick.
    
    Virtual based cputime accounting solves that problem by
    hooking into kernel/user boundaries.
    
    However implementing CONFIG_VIRT_CPU_ACCOUNTING require
    low level hooks and involves more overhead. But we already
    have a generic context tracking subsystem that is required
    for RCU needs by archs which plan to shut down the tick
    outside idle.
    
    This patch implements a generic virtual based cputime
    accounting that relies on these generic kernel/user hooks.
    
    There are some upsides of doing this:
    
    - This requires no arch code to implement CONFIG_VIRT_CPU_ACCOUNTING
    if context tracking is already built (already necessary for RCU in full
    tickless mode).
    
    - We can rely on the generic context tracking subsystem to dynamically
    (de)activate the hooks, so that we can switch anytime between virtual
    and tick based accounting. This way we don't have the overhead
    of the virtual accounting when the tick is running periodically.
    
    And one downside:
    
    - There is probably more overhead than a native virtual based cputime
    accounting. But this relies on hooks that are already set anyway.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 6f6b1cccc916..22c9b67f9983 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -143,7 +143,7 @@ EXPORT_SYMBOL_GPL(ppc_proc_freq);
 unsigned long ppc_tb_freq;
 EXPORT_SYMBOL_GPL(ppc_tb_freq);
 
-#ifdef CONFIG_VIRT_CPU_ACCOUNTING
+#ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
 /*
  * Factors for converting from cputime_t (timebase ticks) to
  * jiffies, microseconds, seconds, and clock_t (1/USER_HZ seconds).
@@ -377,7 +377,7 @@ void vtime_account_user(struct task_struct *tsk)
 	account_user_time(tsk, utime, utimescaled);
 }
 
-#else /* ! CONFIG_VIRT_CPU_ACCOUNTING */
+#else /* ! CONFIG_VIRT_CPU_ACCOUNTING_NATIVE */
 #define calc_cputime_factors()
 #endif
 

commit 023f333a99cee9b5cd3268ff87298eb01a31f78e
Author: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
Date:   Mon Dec 17 14:30:53 2012 -0700

    NTP: Add a CONFIG_RTC_SYSTOHC configuration
    
    The purpose of this option is to allow ARM/etc systems that rely on the
    class RTC subsystem to have the same kind of automatic NTP based
    synchronization that we have on PC platforms. Today ARM does not
    implement update_persistent_clock and makes extensive use of the class
    RTC system.
    
    When enabled CONFIG_RTC_SYSTOHC will provide a generic
    rtc_update_persistent_clock that stores the current time in the RTC and
    is intended complement the existing CONFIG_RTC_HCTOSYS option that loads
    the RTC at boot.
    
    Like with RTC_HCTOSYS the platform's update_persistent_clock is used
    first, if it works. Platforms with mixed class RTC and non-RTC drivers
    need to return ENODEV when class RTC should be used. Such an update for
    PPC is included in this patch.
    
    Long term, implementations of update_persistent_clock should migrate to
    proper class RTC drivers and use CONFIG_RTC_SYSTOHC instead.
    
    Tested on ARM kirkwood and PPC405
    
    Signed-off-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index ce4cb772dc78..bc844a857e05 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -667,7 +667,7 @@ int update_persistent_clock(struct timespec now)
 	struct rtc_time tm;
 
 	if (!ppc_md.set_rtc_time)
-		return 0;
+		return -ENODEV;
 
 	to_tm(now.tv_sec + 1 + timezone_offset, &tm);
 	tm.tm_year -= 1900;

commit ce73ec6db47af84d1466402781ae0872a9e7873c
Author: Shan Hai <shan.hai@windriver.com>
Date:   Thu Nov 8 15:57:49 2012 +0000

    powerpc/vdso: Remove redundant locking in update_vsyscall_tz()
    
    The locking in update_vsyscall_tz() is not only unnecessary because the vdso
    code copies the data unproteced in __kernel_gettimeofday() but also
    introduces a hard to reproduce race condition between update_vsyscall()
    and update_vsyscall_tz(), which causes user space process to loop
    forever in vdso code.
    
    The following patch removes the locking from update_vsyscall_tz().
    
    Locking is not only unnecessary because the vdso code copies the data
    unprotected in __kernel_gettimeofday() but also erroneous because updating
    the tb_update_count is not atomic and introduces a hard to reproduce race
    condition between update_vsyscall() and update_vsyscall_tz(), which further
    causes user space process to loop forever in vdso code.
    
    The below scenario describes the race condition,
    x==0    Boot CPU                        other CPU
            proc_P: x==0
                timer interrupt
                    update_vsyscall
    x==1                x++;sync            settimeofday
                                                update_vsyscall_tz
    x==2                                            x++;sync
    x==3                sync;x++
                                                    sync;x++
            proc_P: x==3 (loops until x becomes even)
    
    Because the ++ operator would be implemented as three instructions and not
    atomic on powerpc.
    
    A similar change was made for x86 in commit 6c260d58634
    ("x86: vdso: Remove bogus locking in update_vsyscall_tz")
    
    Signed-off-by: Shan Hai <shan.hai@windriver.com>
    CC: <stable@vger.kernel.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index b3b14352b05e..6f6b1cccc916 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -770,13 +770,8 @@ void update_vsyscall_old(struct timespec *wall_time, struct timespec *wtm,
 
 void update_vsyscall_tz(void)
 {
-	/* Make userspace gettimeofday spin until we're done. */
-	++vdso_data->tb_update_count;
-	smp_mb();
 	vdso_data->tz_minuteswest = sys_tz.tz_minuteswest;
 	vdso_data->tz_dsttime = sys_tz.tz_dsttime;
-	smp_mb();
-	++vdso_data->tb_update_count;
 }
 
 static void __init clocksource_init(void)

commit 1b2852b152be5150fbef7b585388ec43cf6f4415
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Nov 19 17:00:24 2012 +0100

    vtime: Warn if irqs aren't disabled on system time accounting APIs
    
    System time accounting APIs such as vtime_account_system() and
    vtime_account_idle() need to be irqsafe. Current callers include
    irq entry, exit and kvm, all of which have been checked against that
    requirement. Now it's better to grow that with an automatic check
    in case we have further callers or we missed something.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 3486cfad4a63..b3b14352b05e 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -297,6 +297,8 @@ static u64 vtime_delta(struct task_struct *tsk,
 	u64 now, nowscaled, deltascaled;
 	u64 udelta, delta, user_scaled;
 
+	WARN_ON_ONCE(!irqs_disabled());
+
 	now = mftb();
 	nowscaled = read_spurr(now);
 	get_paca()->system_time += now - get_paca()->starttime;

commit e3942ba04052364d3c6454103362cafd87456010
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 14 00:24:25 2012 +0100

    vtime: Consolidate a bit the ctx switch code
    
    On ia64 and powerpc, vtime context switch only consists
    in flushing system and user pending time, plus a few
    arch housekeeping.
    
    Consolidate that into a generic implementation. s390 is
    a special case because pending user and system time accounting
    there is hard to dissociate. So it's keeping its own implementation.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index a667aaf85846..3486cfad4a63 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -375,12 +375,6 @@ void vtime_account_user(struct task_struct *tsk)
 	account_user_time(tsk, utime, utimescaled);
 }
 
-void vtime_task_switch(struct task_struct *prev)
-{
-	vtime_account(prev);
-	vtime_account_user(prev);
-}
-
 #else /* ! CONFIG_VIRT_CPU_ACCOUNTING */
 #define calc_cputime_factors()
 #endif

commit bcebdf846522056a84ba0b0cba5f5413868c9394
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 13 23:51:06 2012 +0100

    vtime: Explicitly account pending user time on process tick
    
    All vtime implementations just flush the user time on process
    tick. Consolidate that in generic code by calling a user time
    accounting helper. This avoids an indirect call in ia64 and
    prepare to also consolidate vtime context switch code.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index ce4cb772dc78..a667aaf85846 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -355,15 +355,15 @@ void vtime_account_idle(struct task_struct *tsk)
 }
 
 /*
- * Transfer the user and system times accumulated in the paca
- * by the exception entry and exit code to the generic process
- * user and system time records.
+ * Transfer the user time accumulated in the paca
+ * by the exception entry and exit code to the generic
+ * process user time records.
  * Must be called with interrupts disabled.
- * Assumes that vtime_account() has been called recently
- * (i.e. since the last entry from usermode) so that
+ * Assumes that vtime_account_system/idle() has been called
+ * recently (i.e. since the last entry from usermode) so that
  * get_paca()->user_time_scaled is up to date.
  */
-void account_process_tick(struct task_struct *tsk, int user_tick)
+void vtime_account_user(struct task_struct *tsk)
 {
 	cputime_t utime, utimescaled;
 
@@ -378,7 +378,7 @@ void account_process_tick(struct task_struct *tsk, int user_tick)
 void vtime_task_switch(struct task_struct *prev)
 {
 	vtime_account(prev);
-	account_process_tick(prev, 0);
+	vtime_account_user(prev);
 }
 
 #else /* ! CONFIG_VIRT_CPU_ACCOUNTING */

commit fd25b4c2f226de818e1d2b71e3e681d28bcaf5ba
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 13 18:21:22 2012 +0100

    vtime: Remove the underscore prefix invasion
    
    Prepending irq-unsafe vtime APIs with underscores was actually
    a bad idea as the result is a big mess in the API namespace that
    is even waiting to be further extended. Also these helpers
    are always called from irq safe callers except kvm. Just
    provide a vtime_account_system_irqsafe() for this specific
    case so that we can remove the underscore prefix on other
    vtime functions.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 0db456f30d45..ce4cb772dc78 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -336,7 +336,7 @@ static u64 vtime_delta(struct task_struct *tsk,
 	return delta;
 }
 
-void __vtime_account_system(struct task_struct *tsk)
+void vtime_account_system(struct task_struct *tsk)
 {
 	u64 delta, sys_scaled, stolen;
 
@@ -346,7 +346,7 @@ void __vtime_account_system(struct task_struct *tsk)
 		account_steal_time(stolen);
 }
 
-void __vtime_account_idle(struct task_struct *tsk)
+void vtime_account_idle(struct task_struct *tsk)
 {
 	u64 delta, sys_scaled, stolen;
 

commit 11113334d1c5dd5355c86e531c29f1202a855c86
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Oct 24 18:05:51 2012 +0200

    vtime: Make vtime_account_system() irqsafe
    
    vtime_account_system() currently has only one caller with
    vtime_account() which is irq safe.
    
    Now we are going to call it from other places like kvm where
    irqs are not always disabled by the time we account the cputime.
    
    So let's make it irqsafe. The arch implementation part is now
    prefixed with "__".
    
    vtime_account_idle() arch implementation is prefixed accordingly
    to stay consistent.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index ce4cb772dc78..0db456f30d45 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -336,7 +336,7 @@ static u64 vtime_delta(struct task_struct *tsk,
 	return delta;
 }
 
-void vtime_account_system(struct task_struct *tsk)
+void __vtime_account_system(struct task_struct *tsk)
 {
 	u64 delta, sys_scaled, stolen;
 
@@ -346,7 +346,7 @@ void vtime_account_system(struct task_struct *tsk)
 		account_steal_time(stolen);
 }
 
-void vtime_account_idle(struct task_struct *tsk)
+void __vtime_account_idle(struct task_struct *tsk)
 {
 	u64 delta, sys_scaled, stolen;
 

commit 03d3602a833715f83ea53b9feb078b9c4c5f6c1a
Merge: 0588f1f93479 5b3900cd4094
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 12 22:17:48 2012 +0900

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer core update from Thomas Gleixner:
     - Bug fixes (one for a longstanding dead loop issue)
     - Rework of time related vsyscalls
     - Alarm timer updates
     - Jiffies updates to remove compile time dependencies
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      timekeeping: Cast raw_interval to u64 to avoid shift overflow
      timers: Fix endless looping between cascade() and internal_add_timer()
      time/jiffies: bring back unconditional LATCH definition
      time: Convert x86_64 to using new update_vsyscall
      time: Only do nanosecond rounding on GENERIC_TIME_VSYSCALL_OLD systems
      time: Introduce new GENERIC_TIME_VSYSCALL
      time: Convert CONFIG_GENERIC_TIME_VSYSCALL to CONFIG_GENERIC_TIME_VSYSCALL_OLD
      time: Move update_vsyscall definitions to timekeeper_internal.h
      time: Move timekeeper structure to timekeeper_internal.h for vsyscall changes
      jiffies: Remove compile time assumptions about CLOCK_TICK_RATE
      jiffies: Kill unused TICK_USEC_TO_NSEC
      alarmtimer: Rename alarmtimer_remove to alarmtimer_dequeue
      alarmtimer: Remove unused helpers & defines
      alarmtimer: Use hrtimer per-alarm instead of per-base
      alarmtimer: Implement minimum alarm interval for allowing suspend

commit 5f3d2f2e1a63679cf1c4a4210f2f1cc2f335bef6
Merge: 283dbd82055e d900bd736646
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 6 03:16:12 2012 +0900

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Benjamin Herrenschmidt:
     "Some highlights in addition to the usual batch of fixes:
    
       - 64TB address space support for 64-bit processes by Aneesh Kumar
    
       - Gavin Shan did a major cleanup & re-organization of our EEH support
         code (IBM fancy PCI error handling & recovery infrastructure) which
         paves the way for supporting different platform backends, along
         with some rework of the PCIe code for the PowerNV platform in order
         to remove home made resource allocations and instead use the
         generic code (which is possible after some small improvements to it
         done by Gavin).
    
       - Uprobes support by Ananth N Mavinakayanahalli
    
       - A pile of embedded updates from Freescale folks, including new SoC
         and board supports, more KVM stuff including preparing for 64-bit
         BookE KVM support, ePAPR 1.1 updates, etc..."
    
    Fixup trivial conflicts in drivers/scsi/ipr.c
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (146 commits)
      powerpc/iommu: Fix multiple issues with IOMMU pools code
      powerpc: Fix VMX fix for memcpy case
      driver/mtd:IFC NAND:Initialise internal SRAM before any write
      powerpc/fsl-pci: use 'Header Type' to identify PCIE mode
      powerpc/eeh: Don't release eeh_mutex in eeh_phb_pe_get
      powerpc: Remove tlb batching hack for nighthawk
      powerpc: Set paca->data_offset = 0 for boot cpu
      powerpc/perf: Sample only if SIAR-Valid bit is set in P7+
      powerpc/fsl-pci: fix warning when CONFIG_SWIOTLB is disabled
      powerpc/mpc85xx: Update interrupt handling for IFC controller
      powerpc/85xx: Enable USB support in p1023rds_defconfig
      powerpc/smp: Do not disable IPI interrupts during suspend
      powerpc/eeh: Fix crash on converting OF node to edev
      powerpc/eeh: Lock module while handling EEH event
      powerpc/kprobe: Don't emulate store when kprobe stwu r1
      powerpc/kprobe: Complete kprobe and migrate exception frame
      powerpc/kprobe: Introduce a new thread flag
      powerpc: Remove unused __get_user64() and __put_user64()
      powerpc/eeh: Global mutex to protect PE tree
      powerpc/eeh: Remove EEH PE for normal PCI hotplug
      ...

commit 0b981cb94bc63a2d0e5eccccdca75fe57643ffce
Merge: 4cba3335826c fdf9c356502a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 10:43:39 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "Continued quest to clean up and enhance the cputime code by Frederic
      Weisbecker, in preparation for future tickless kernel features.
    
      Other than that, smallish changes."
    
    Fix up trivial conflicts due to additions next to each other in arch/{x86/}Kconfig
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      cputime: Make finegrained irqtime accounting generally available
      cputime: Gather time/stats accounting config options into a single menu
      ia64: Reuse system and user vtime accounting functions on task switch
      ia64: Consolidate user vtime accounting
      vtime: Consolidate system/idle context detection
      cputime: Use a proper subsystem naming for vtime related APIs
      sched: cpu_power: enable ARCH_POWER
      sched/nohz: Clean up select_nohz_load_balancer()
      sched: Fix load avg vs. cpu-hotplug
      sched: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW
      sched: Fix nohz_idle_balance()
      sched: Remove useless code in yield_to()
      sched: Add time unit suffix to sched sysctl knobs
      sched/debug: Limit sd->*_idx range on sysctl
      sched: Remove AFFINE_WAKEUPS feature flag
      s390: Remove leftover account_tick_vtime() header
      cputime: Consolidate vtime handling on context switch
      sched: Move cputime code to its own file
      cputime: Generalize CONFIG_VIRT_CPU_ACCOUNTING
      tile: Remove SD_PREFER_LOCAL leftover
      ...

commit a7e1a9e3af71b45ecae2dae35851f238117b317d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Sep 8 16:14:02 2012 +0200

    vtime: Consolidate system/idle context detection
    
    Move the code that finds out to which context we account the
    cputime into generic layer.
    
    Archs that consider the whole time spent in the idle task as idle
    time (ia64, powerpc) can rely on the generic vtime_account()
    and implement vtime_account_system() and vtime_account_idle(),
    letting the generic code to decide when to call which API.
    
    Archs that have their own meaning of idle time, such as s390
    that only considers the time spent in CPU low power mode as idle
    time, can just override vtime_account().
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 39899d7ebda0..29b6d3e1ea28 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -291,13 +291,12 @@ static inline u64 calculate_stolen_time(u64 stop_tb)
  * Account time for a transition between system, hard irq
  * or soft irq state.
  */
-void vtime_account(struct task_struct *tsk)
+static u64 vtime_delta(struct task_struct *tsk,
+			u64 *sys_scaled, u64 *stolen)
 {
-	u64 now, nowscaled, delta, deltascaled;
-	unsigned long flags;
-	u64 stolen, udelta, sys_scaled, user_scaled;
+	u64 now, nowscaled, deltascaled;
+	u64 udelta, delta, user_scaled;
 
-	local_irq_save(flags);
 	now = mftb();
 	nowscaled = read_spurr(now);
 	get_paca()->system_time += now - get_paca()->starttime;
@@ -305,7 +304,7 @@ void vtime_account(struct task_struct *tsk)
 	deltascaled = nowscaled - get_paca()->startspurr;
 	get_paca()->startspurr = nowscaled;
 
-	stolen = calculate_stolen_time(now);
+	*stolen = calculate_stolen_time(now);
 
 	delta = get_paca()->system_time;
 	get_paca()->system_time = 0;
@@ -322,28 +321,38 @@ void vtime_account(struct task_struct *tsk)
 	 * the user ticks get saved up in paca->user_time_scaled to be
 	 * used by account_process_tick.
 	 */
-	sys_scaled = delta;
+	*sys_scaled = delta;
 	user_scaled = udelta;
 	if (deltascaled != delta + udelta) {
 		if (udelta) {
-			sys_scaled = deltascaled * delta / (delta + udelta);
-			user_scaled = deltascaled - sys_scaled;
+			*sys_scaled = deltascaled * delta / (delta + udelta);
+			user_scaled = deltascaled - *sys_scaled;
 		} else {
-			sys_scaled = deltascaled;
+			*sys_scaled = deltascaled;
 		}
 	}
 	get_paca()->user_time_scaled += user_scaled;
 
-	if (in_interrupt() || idle_task(smp_processor_id()) != tsk) {
-		account_system_time(tsk, 0, delta, sys_scaled);
-		if (stolen)
-			account_steal_time(stolen);
-	} else {
-		account_idle_time(delta + stolen);
-	}
-	local_irq_restore(flags);
+	return delta;
+}
+
+void vtime_account_system(struct task_struct *tsk)
+{
+	u64 delta, sys_scaled, stolen;
+
+	delta = vtime_delta(tsk, &sys_scaled, &stolen);
+	account_system_time(tsk, 0, delta, sys_scaled);
+	if (stolen)
+		account_steal_time(stolen);
+}
+
+void vtime_account_idle(struct task_struct *tsk)
+{
+	u64 delta, sys_scaled, stolen;
+
+	delta = vtime_delta(tsk, &sys_scaled, &stolen);
+	account_idle_time(delta + stolen);
 }
-EXPORT_SYMBOL_GPL(vtime_account);
 
 /*
  * Transfer the user and system times accumulated in the paca

commit bf9fae9f5e4ca8dce4708812f9ad6281e61df109
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Sep 8 15:23:11 2012 +0200

    cputime: Use a proper subsystem naming for vtime related APIs
    
    Use a naming based on vtime as a prefix for virtual based
    cputime accounting APIs:
    
    - account_system_vtime() -> vtime_account()
    - account_switch_vtime() -> vtime_task_switch()
    
    It makes it easier to allow for further declension such
    as vtime_account_system(), vtime_account_idle(), ... if we
    want to find out the context we account to from generic code.
    
    This also make it better to know on which subsystem these APIs
    refer to.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 49da7f06e643..39899d7ebda0 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -291,7 +291,7 @@ static inline u64 calculate_stolen_time(u64 stop_tb)
  * Account time for a transition between system, hard irq
  * or soft irq state.
  */
-void account_system_vtime(struct task_struct *tsk)
+void vtime_account(struct task_struct *tsk)
 {
 	u64 now, nowscaled, delta, deltascaled;
 	unsigned long flags;
@@ -343,14 +343,14 @@ void account_system_vtime(struct task_struct *tsk)
 	}
 	local_irq_restore(flags);
 }
-EXPORT_SYMBOL_GPL(account_system_vtime);
+EXPORT_SYMBOL_GPL(vtime_account);
 
 /*
  * Transfer the user and system times accumulated in the paca
  * by the exception entry and exit code to the generic process
  * user and system time records.
  * Must be called with interrupts disabled.
- * Assumes that account_system_vtime() has been called recently
+ * Assumes that vtime_account() has been called recently
  * (i.e. since the last entry from usermode) so that
  * get_paca()->user_time_scaled is up to date.
  */
@@ -366,9 +366,9 @@ void account_process_tick(struct task_struct *tsk, int user_tick)
 	account_user_time(tsk, utime, utimescaled);
 }
 
-void account_switch_vtime(struct task_struct *prev)
+void vtime_task_switch(struct task_struct *prev)
 {
-	account_system_vtime(prev);
+	vtime_account(prev);
 	account_process_tick(prev, 0);
 }
 

commit 706394211648117762edfaeffd6fc04bf3b1a75d
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Sep 4 15:34:21 2012 -0400

    time: Convert CONFIG_GENERIC_TIME_VSYSCALL to CONFIG_GENERIC_TIME_VSYSCALL_OLD
    
    To help migrate archtectures over to the new update_vsyscall method,
    redfine CONFIG_GENERIC_TIME_VSYSCALL as CONFIG_GENERIC_TIME_VSYSCALL_OLD
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 613a830d9d50..c825809b92e7 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -712,7 +712,7 @@ static cycle_t timebase_read(struct clocksource *cs)
 	return (cycle_t)get_tb();
 }
 
-void update_vsyscall(struct timespec *wall_time, struct timespec *wtm,
+void update_vsyscall_old(struct timespec *wall_time, struct timespec *wtm,
 			struct clocksource *clock, u32 mult)
 {
 	u64 new_tb_to_xs, new_stamp_xsec;

commit 189374aed657e2228ad6b39ece438c9cdafc8dae
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Sep 4 15:27:48 2012 -0400

    time: Move update_vsyscall definitions to timekeeper_internal.h
    
    Since users will need to include timekeeper_internal.h, move
    update_vsyscall definitions to timekeeper_internal.h.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index e49e93191b69..613a830d9d50 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -73,7 +73,7 @@
 /* powerpc clocksource/clockevent code */
 
 #include <linux/clockchips.h>
-#include <linux/clocksource.h>
+#include <linux/timekeeper_internal.h>
 
 static cycle_t rtc_read(struct clocksource *);
 static struct clocksource clocksource_rtc = {

commit e72bbbab278a2e6d506bd2cf380ba9bef68e8ec1
Author: Li Zhong <zhong@linux.vnet.ibm.com>
Date:   Mon Sep 10 15:37:43 2012 +0000

    powerpc/trace: Fix interrupt tracepoints vs. RCU
    
    There are a few tracepoints in the interrupt code path, which is before
    irq_enter(), or after irq_exit(), like
    trace_irq_entry()/trace_irq_exit() in do_IRQ(),
    trace_timer_interrupt_entry()/trace_timer_interrupt_exit() in
    timer_interrupt().
    
    If the interrupt is from idle(), and because tracepoint contains RCU
    read-side critical section, we could see following suspicious RCU usage
    reported:
    
    [  145.127743] ===============================
    [  145.127747] [ INFO: suspicious RCU usage. ]
    [  145.127752] 3.6.0-rc3+ #1 Not tainted
    [  145.127755] -------------------------------
    [  145.127759] /root/.workdir/linux/arch/powerpc/include/asm/trace.h:33
    suspicious rcu_dereference_check() usage!
    [  145.127765]
    [  145.127765] other info that might help us debug this:
    [  145.127765]
    [  145.127771]
    [  145.127771] RCU used illegally from idle CPU!
    [  145.127771] rcu_scheduler_active = 1, debug_locks = 0
    [  145.127777] RCU used illegally from extended quiescent state!
    [  145.127781] no locks held by swapper/0/0.
    [  145.127785]
    [  145.127785] stack backtrace:
    [  145.127789] Call Trace:
    [  145.127796] [c00000000108b530] [c000000000013c40] .show_stack
    +0x70/0x1c0 (unreliable)
    [  145.127806] [c00000000108b5e0]
    [c0000000000f59d8] .lockdep_rcu_suspicious+0x118/0x150
    [  145.127813] [c00000000108b680] [c00000000000fc58] .do_IRQ+0x498/0x500
    [  145.127820] [c00000000108b750] [c000000000003950]
    hardware_interrupt_common+0x150/0x180
    [  145.127828] --- Exception: 501 at .plpar_hcall_norets+0x84/0xd4
    [  145.127828]     LR = .check_and_cede_processor+0x38/0x70
    [  145.127836] [c00000000108bab0] [c0000000000665dc] .shared_cede_loop
    +0x5c/0x100
    [  145.127844] [c00000000108bb70] [c000000000588ab0] .cpuidle_enter
    +0x30/0x50
    [  145.127850] [c00000000108bbe0]
    [c000000000588b0c] .cpuidle_enter_state+0x3c/0xb0
    [  145.127857] [c00000000108bc60] [c000000000589730] .cpuidle_idle_call
    +0x150/0x6c0
    [  145.127863] [c00000000108bd30] [c000000000058440] .pSeries_idle
    +0x10/0x40
    [  145.127870] [c00000000108bda0] [c00000000001683c] .cpu_idle
    +0x18c/0x2d0
    [  145.127876] [c00000000108be60] [c00000000000b434] .rest_init
    +0x124/0x1b0
    [  145.127884] [c00000000108bef0] [c0000000009d0d28] .start_kernel
    +0x568/0x588
    [  145.127890] [c00000000108bf90] [c000000000009660] .start_here_common
    +0x20/0x40
    
    This is because the RCU usage in interrupt context should be used in
    area marked by rcu_irq_enter()/rcu_irq_exit(), called in
    irq_enter()/irq_exit() respectively.
    
    Move them into the irq_enter()/irq_exit() area to avoid the reporting.
    
    Signed-off-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index e49e93191b69..bd693a11d86e 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -493,8 +493,6 @@ void timer_interrupt(struct pt_regs * regs)
 	 */
 	may_hard_irq_enable();
 
-	trace_timer_interrupt_entry(regs);
-
 	__get_cpu_var(irq_stat).timer_irqs++;
 
 #if defined(CONFIG_PPC32) && defined(CONFIG_PMAC)
@@ -505,6 +503,8 @@ void timer_interrupt(struct pt_regs * regs)
 	old_regs = set_irq_regs(regs);
 	irq_enter();
 
+	trace_timer_interrupt_entry(regs);
+
 	if (test_irq_work_pending()) {
 		clear_irq_work_pending();
 		irq_work_run();
@@ -529,10 +529,10 @@ void timer_interrupt(struct pt_regs * regs)
 	}
 #endif
 
+	trace_timer_interrupt_exit(regs);
+
 	irq_exit();
 	set_irq_regs(old_regs);
-
-	trace_timer_interrupt_exit(regs);
 }
 
 /*

commit dabe859ec6360a12e71f39bf695d174e19ff2688
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jul 26 13:56:11 2012 +0000

    powerpc: Give hypervisor decrementer interrupts their own handler
    
    At the moment the handler for hypervisor decrementer interrupts is
    the same as for decrementer interrupts, i.e. timer_interrupt().
    This is bogus; if we ever do get a hypervisor decrementer interrupt
    it won't have anything to do with the next timer event.  In fact
    the only time we get hypervisor decrementer interrupts is when one
    is left pending on exit from a KVM guest.
    
    When we get a hypervisor decrementer interrupt we don't need to do
    anything special to clear it, since they are edge-triggered on the
    transition of HDEC from 0 to -1.  Thus this adds an empty handler
    function for them.  We don't need to have them masked when interrupts
    are soft-disabled, so we use STD_EXCEPTION_HV instead of
    MASKABLE_EXCEPTION_HV.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index be171ee73bf8..e49e93191b69 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -535,6 +535,15 @@ void timer_interrupt(struct pt_regs * regs)
 	trace_timer_interrupt_exit(regs);
 }
 
+/*
+ * Hypervisor decrementer interrupts shouldn't occur but are sometimes
+ * left pending on exit from a KVM guest.  We don't need to do anything
+ * to clear them, as they are edge-triggered.
+ */
+void hdec_interrupt(struct pt_regs *regs)
+{
+}
+
 #ifdef CONFIG_SUSPEND
 static void generic_suspend_disable_irqs(void)
 {

commit baa36046d09ea6dbc122c795566992318663d9eb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Jun 18 17:54:14 2012 +0200

    cputime: Consolidate vtime handling on context switch
    
    The archs that implement virtual cputime accounting all
    flush the cputime of a task when it gets descheduled
    and sometimes set up some ground initialization for the
    next task to account its cputime.
    
    These archs all put their own hooks in their context
    switch callbacks and handle the off-case themselves.
    
    Consolidate this by creating a new account_switch_vtime()
    callback called in generic code right after a context switch
    and that these archs must implement to flush the prev task
    cputime and initialize the next task cputime related state.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index be171ee73bf8..49da7f06e643 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -366,6 +366,12 @@ void account_process_tick(struct task_struct *tsk, int user_tick)
 	account_user_time(tsk, utime, utimescaled);
 }
 
+void account_switch_vtime(struct task_struct *prev)
+{
+	account_system_vtime(prev);
+	account_process_tick(prev, 0);
+}
+
 #else /* ! CONFIG_VIRT_CPU_ACCOUNTING */
 #define calc_cputime_factors()
 #endif

commit 860aed25a1f0936d4852ab936252b47cd1e630f1
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Jun 1 18:13:43 2012 +1000

    powerpc/time: Sanity check of decrementer expiration is necessary
    
    This reverts 68568add2c ("powerpc/time: Remove unnecessary sanity check
    of decrementer expiration").  We do need to check whether we have reached
    the expiration time of the next event, because we sometimes get an early
    decrementer interrupt, most notably when we set the decrementer to 1 in
    arch_irq_work_raise().  The effect of not having the sanity check is that
    if timer_interrupt() gets called early, we leave the decrementer set to
    its maximum value, which means we then don't get any more decrementer
    interrupts for about 4 seconds (or longer, depending on timebase
    frequency).  I saw these pauses as a consequence of getting a stray
    hypervisor decrementer interrupt left over from exiting a KVM guest.
    
    This isn't quite a straight revert because of changes to the surrounding
    code, but it restores the same algorithm as was previously used.
    
    Cc: stable@vger.kernel.org
    Acked-by: Anton Blanchard <anton@samba.org>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 99a995c2a3f2..be171ee73bf8 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -475,6 +475,7 @@ void timer_interrupt(struct pt_regs * regs)
 	struct pt_regs *old_regs;
 	u64 *next_tb = &__get_cpu_var(decrementers_next_tb);
 	struct clock_event_device *evt = &__get_cpu_var(decrementers);
+	u64 now;
 
 	/* Ensure a positive value is written to the decrementer, or else
 	 * some CPUs will continue to take decrementer exceptions.
@@ -509,9 +510,16 @@ void timer_interrupt(struct pt_regs * regs)
 		irq_work_run();
 	}
 
-	*next_tb = ~(u64)0;
-	if (evt->event_handler)
-		evt->event_handler(evt);
+	now = get_tb_or_rtc();
+	if (now >= *next_tb) {
+		*next_tb = ~(u64)0;
+		if (evt->event_handler)
+			evt->event_handler(evt);
+	} else {
+		now = *next_tb - now;
+		if (now <= DECREMENTER_MAX)
+			set_dec((int)now);
+	}
 
 #ifdef CONFIG_PPC64
 	/* collect purr register values often, for accurate calculations */

commit 6e35994d1f6831af1e5577e28c363c9137d7d597
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed Apr 18 06:01:19 2012 +0000

    KVM: PPC: Use clockevent multiplier and shifter for decrementer
    
    Time for which the hrtimer is started for decrementer emulation is calculated
    using tb_ticks_per_usec. While hrtimer uses the clockevent for DEC
    reprogramming (if needed) and which calculate timebase ticks using the
    multiplier and shifter mechanism implemented within clockevent layer.
    
    It was observed that this conversion (timebase->time->timebase) are not
    correct because the mechanism are not consistent.
    In our setup it adds 2% jitter.
    
    With this patch clockevent multiplier and shifter mechanism are used when
    starting hrtimer for decrementer emulation. Now the jitter is < 0.5%.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 2c42cd72d0f5..99a995c2a3f2 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -100,7 +100,7 @@ static int decrementer_set_next_event(unsigned long evt,
 static void decrementer_set_mode(enum clock_event_mode mode,
 				 struct clock_event_device *dev);
 
-static struct clock_event_device decrementer_clockevent = {
+struct clock_event_device decrementer_clockevent = {
 	.name           = "decrementer",
 	.rating         = 200,
 	.irq            = 0,
@@ -108,6 +108,7 @@ static struct clock_event_device decrementer_clockevent = {
 	.set_mode       = decrementer_set_mode,
 	.features       = CLOCK_EVT_FEAT_ONESHOT,
 };
+EXPORT_SYMBOL(decrementer_clockevent);
 
 DEFINE_PER_CPU(u64, decrementers_next_tb);
 static DEFINE_PER_CPU(struct clock_event_device, decrementers);

commit f5339277eb8d3aed37f12a27988366f68ab68930
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Thu Mar 15 18:18:00 2012 +0000

    powerpc: Remove FW_FEATURE ISERIES from arch code
    
    This is no longer selectable, so just remove all the dependent code.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index f81c81b92f0e..2c42cd72d0f5 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -17,8 +17,7 @@
  *
  * TODO (not necessarily in this file):
  * - improve precision and reproducibility of timebase frequency
- * measurement at boot time. (for iSeries, we calibrate the timebase
- * against the Titan chip's clock.)
+ * measurement at boot time.
  * - for astronomical applications: add a new function to get
  * non ambiguous timestamps even around leap seconds. This needs
  * a new timestamp format and a good name.
@@ -70,10 +69,6 @@
 #include <asm/vdso_datapage.h>
 #include <asm/firmware.h>
 #include <asm/cputime.h>
-#ifdef CONFIG_PPC_ISERIES
-#include <asm/iseries/it_lp_queue.h>
-#include <asm/iseries/hv_call_xm.h>
-#endif
 
 /* powerpc clocksource/clockevent code */
 
@@ -117,14 +112,6 @@ static struct clock_event_device decrementer_clockevent = {
 DEFINE_PER_CPU(u64, decrementers_next_tb);
 static DEFINE_PER_CPU(struct clock_event_device, decrementers);
 
-#ifdef CONFIG_PPC_ISERIES
-static unsigned long __initdata iSeries_recal_titan;
-static signed long __initdata iSeries_recal_tb;
-
-/* Forward declaration is only needed for iSereis compiles */
-static void __init clocksource_init(void);
-#endif
-
 #define XSEC_PER_SEC (1024*1024)
 
 #ifdef CONFIG_PPC64
@@ -423,74 +410,6 @@ unsigned long profile_pc(struct pt_regs *regs)
 EXPORT_SYMBOL(profile_pc);
 #endif
 
-#ifdef CONFIG_PPC_ISERIES
-
-/* 
- * This function recalibrates the timebase based on the 49-bit time-of-day
- * value in the Titan chip.  The Titan is much more accurate than the value
- * returned by the service processor for the timebase frequency.
- */
-
-static int __init iSeries_tb_recal(void)
-{
-	unsigned long titan, tb;
-
-	/* Make sure we only run on iSeries */
-	if (!firmware_has_feature(FW_FEATURE_ISERIES))
-		return -ENODEV;
-
-	tb = get_tb();
-	titan = HvCallXm_loadTod();
-	if ( iSeries_recal_titan ) {
-		unsigned long tb_ticks = tb - iSeries_recal_tb;
-		unsigned long titan_usec = (titan - iSeries_recal_titan) >> 12;
-		unsigned long new_tb_ticks_per_sec   = (tb_ticks * USEC_PER_SEC)/titan_usec;
-		unsigned long new_tb_ticks_per_jiffy =
-			DIV_ROUND_CLOSEST(new_tb_ticks_per_sec, HZ);
-		long tick_diff = new_tb_ticks_per_jiffy - tb_ticks_per_jiffy;
-		char sign = '+';		
-		/* make sure tb_ticks_per_sec and tb_ticks_per_jiffy are consistent */
-		new_tb_ticks_per_sec = new_tb_ticks_per_jiffy * HZ;
-
-		if ( tick_diff < 0 ) {
-			tick_diff = -tick_diff;
-			sign = '-';
-		}
-		if ( tick_diff ) {
-			if ( tick_diff < tb_ticks_per_jiffy/25 ) {
-				printk( "Titan recalibrate: new tb_ticks_per_jiffy = %lu (%c%ld)\n",
-						new_tb_ticks_per_jiffy, sign, tick_diff );
-				tb_ticks_per_jiffy = new_tb_ticks_per_jiffy;
-				tb_ticks_per_sec   = new_tb_ticks_per_sec;
-				calc_cputime_factors();
-				vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;
-				setup_cputime_one_jiffy();
-			}
-			else {
-				printk( "Titan recalibrate: FAILED (difference > 4 percent)\n"
-					"                   new tb_ticks_per_jiffy = %lu\n"
-					"                   old tb_ticks_per_jiffy = %lu\n",
-					new_tb_ticks_per_jiffy, tb_ticks_per_jiffy );
-			}
-		}
-	}
-	iSeries_recal_titan = titan;
-	iSeries_recal_tb = tb;
-
-	/* Called here as now we know accurate values for the timebase */
-	clocksource_init();
-	return 0;
-}
-late_initcall(iSeries_tb_recal);
-
-/* Called from platform early init */
-void __init iSeries_time_init_early(void)
-{
-	iSeries_recal_tb = get_tb();
-	iSeries_recal_titan = HvCallXm_loadTod();
-}
-#endif /* CONFIG_PPC_ISERIES */
-
 #ifdef CONFIG_IRQ_WORK
 
 /*
@@ -546,16 +465,6 @@ void arch_irq_work_raise(void)
 
 #endif /* CONFIG_IRQ_WORK */
 
-/*
- * For iSeries shared processors, we have to let the hypervisor
- * set the hardware decrementer.  We set a virtual decrementer
- * in the lppaca and call the hypervisor if the virtual
- * decrementer is less than the current value in the hardware
- * decrementer. (almost always the new decrementer value will
- * be greater than the current hardware decementer so the hypervisor
- * call will not be needed)
- */
-
 /*
  * timer_interrupt - gets called when the decrementer overflows,
  * with interrupts disabled.
@@ -599,20 +508,10 @@ void timer_interrupt(struct pt_regs * regs)
 		irq_work_run();
 	}
 
-#ifdef CONFIG_PPC_ISERIES
-	if (firmware_has_feature(FW_FEATURE_ISERIES))
-		get_lppaca()->int_dword.fields.decr_int = 0;
-#endif
-
 	*next_tb = ~(u64)0;
 	if (evt->event_handler)
 		evt->event_handler(evt);
 
-#ifdef CONFIG_PPC_ISERIES
-	if (firmware_has_feature(FW_FEATURE_ISERIES) && hvlpevent_is_pending())
-		process_hvlpevents();
-#endif
-
 #ifdef CONFIG_PPC64
 	/* collect purr register values often, for accurate calculations */
 	if (firmware_has_feature(FW_FEATURE_SPLPAR)) {
@@ -984,9 +883,8 @@ void __init time_init(void)
 	 */
 	start_cpu_decrementer();
 
-	/* Register the clocksource, if we're not running on iSeries */
-	if (!firmware_has_feature(FW_FEATURE_ISERIES))
-		clocksource_init();
+	/* Register the clocksource */
+	clocksource_init();
 
 	init_decrementer_clockevent();
 }

commit 7230c5644188cd9e3fb380cc97dde00c464a3ba7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Mar 6 18:27:59 2012 +1100

    powerpc: Rework lazy-interrupt handling
    
    The current implementation of lazy interrupts handling has some
    issues that this tries to address.
    
    We don't do the various workarounds we need to do when re-enabling
    interrupts in some cases such as when returning from an interrupt
    and thus we may still lose or get delayed decrementer or doorbell
    interrupts.
    
    The current scheme also makes it much harder to handle the external
    "edge" interrupts provided by some BookE processors when using the
    EPR facility (External Proxy) and the Freescale Hypervisor.
    
    Additionally, we tend to keep interrupts hard disabled in a number
    of cases, such as decrementer interrupts, external interrupts, or
    when a masked decrementer interrupt is pending. This is sub-optimal.
    
    This is an attempt at fixing it all in one go by reworking the way
    we do the lazy interrupt disabling from the ground up.
    
    The base idea is to replace the "hard_enabled" field with a
    "irq_happened" field in which we store a bit mask of what interrupt
    occurred while soft-disabled.
    
    When re-enabling, either via arch_local_irq_restore() or when returning
    from an interrupt, we can now decide what to do by testing bits in that
    field.
    
    We then implement replaying of the missed interrupts either by
    re-using the existing exception frame (in exception exit case) or via
    the creation of a new one from an assembly trampoline (in the
    arch_local_irq_enable case).
    
    This removes the need to play with the decrementer to try to create
    fake interrupts, among others.
    
    In addition, this adds a few refinements:
    
     - We no longer  hard disable decrementer interrupts that occur
    while soft-disabled. We now simply bump the decrementer back to max
    (on BookS) or leave it stopped (on BookE) and continue with hard interrupts
    enabled, which means that we'll potentially get better sample quality from
    performance monitor interrupts.
    
     - Timer, decrementer and doorbell interrupts now hard-enable
    shortly after removing the source of the interrupt, which means
    they no longer run entirely hard disabled. Again, this will improve
    perf sample quality.
    
     - On Book3E 64-bit, we now make the performance monitor interrupt
    act as an NMI like Book3S (the necessary C code for that to work
    appear to already be present in the FSL perf code, notably calling
    nmi_enter instead of irq_enter). (This also fixes a bug where BookE
    perfmon interrupts could clobber r14 ... oops)
    
     - We could make "masked" decrementer interrupts act as NMIs when doing
    timer-based perf sampling to improve the sample quality.
    
    Signed-off-by-yet: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    ---
    
    v2:
    
    - Add hard-enable to decrementer, timer and doorbells
    - Fix CR clobber in masked irq handling on BookE
    - Make embedded perf interrupt act as an NMI
    - Add a PACA_HAPPENED_EE_EDGE for use by FSL if they want
      to retrigger an interrupt without preventing hard-enable
    
    v3:
    
     - Fix or vs. ori bug on Book3E
     - Fix enabling of interrupts for some exceptions on Book3E
    
    v4:
    
     - Fix resend of doorbells on return from interrupt on Book3E
    
    v5:
    
     - Rebased on top of my latest series, which involves some significant
    rework of some aspects of the patch.
    
    v6:
     - 32-bit compile fix
     - more compile fixes with various .config combos
     - factor out the asm code to soft-disable interrupts
     - remove the C wrapper around preempt_schedule_irq
    
    v7:
     - Fix a bug with hard irq state tracking on native power7

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 567dd7c3ac2a..f81c81b92f0e 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -259,7 +259,6 @@ void accumulate_stolen_time(void)
 	u64 sst, ust;
 
 	u8 save_soft_enabled = local_paca->soft_enabled;
-	u8 save_hard_enabled = local_paca->hard_enabled;
 
 	/* We are called early in the exception entry, before
 	 * soft/hard_enabled are sync'ed to the expected state
@@ -268,7 +267,6 @@ void accumulate_stolen_time(void)
 	 * complain
 	 */
 	local_paca->soft_enabled = 0;
-	local_paca->hard_enabled = 0;
 
 	sst = scan_dispatch_log(local_paca->starttime_user);
 	ust = scan_dispatch_log(local_paca->starttime);
@@ -277,7 +275,6 @@ void accumulate_stolen_time(void)
 	local_paca->stolen_time += ust + sst;
 
 	local_paca->soft_enabled = save_soft_enabled;
-	local_paca->hard_enabled = save_hard_enabled;
 }
 
 static inline u64 calculate_stolen_time(u64 stop_tb)
@@ -580,6 +577,11 @@ void timer_interrupt(struct pt_regs * regs)
 	if (!cpu_online(smp_processor_id()))
 		return;
 
+	/* Conditionally hard-enable interrupts now that the DEC has been
+	 * bumped to its maximum value
+	 */
+	may_hard_irq_enable();
+
 	trace_timer_interrupt_entry(regs);
 
 	__get_cpu_var(irq_stat).timer_irqs++;

commit 9f5072d4f63f28d30d343573830ac6c85fc0deff
Author: Andreas Schwab <schwab@linux-m68k.org>
Date:   Fri Dec 9 11:35:08 2011 +0000

    powerpc: Fix wrong divisor in usecs_to_cputime
    
    Commit d57af9b (taskstats: use real microsecond granularity for CPU times)
    renamed msecs_to_cputime to usecs_to_cputime, but failed to update all
    numbers on the way.  This causes nonsensical cpu idle/iowait values to be
    displayed in /proc/stat (the only user of usecs_to_cputime so far).
    
    This also renames __cputime_msec_factor to __cputime_usec_factor, adapting
    its value and using it directly in cputime_to_usecs instead of doing two
    multiplications.
    
    Signed-off-by: Andreas Schwab <schwab@linux-m68k.org>
    Acked-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 9754743db8b9..567dd7c3ac2a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -158,13 +158,13 @@ EXPORT_SYMBOL_GPL(ppc_tb_freq);
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING
 /*
  * Factors for converting from cputime_t (timebase ticks) to
- * jiffies, milliseconds, seconds, and clock_t (1/USER_HZ seconds).
+ * jiffies, microseconds, seconds, and clock_t (1/USER_HZ seconds).
  * These are all stored as 0.64 fixed-point binary fractions.
  */
 u64 __cputime_jiffies_factor;
 EXPORT_SYMBOL(__cputime_jiffies_factor);
-u64 __cputime_msec_factor;
-EXPORT_SYMBOL(__cputime_msec_factor);
+u64 __cputime_usec_factor;
+EXPORT_SYMBOL(__cputime_usec_factor);
 u64 __cputime_sec_factor;
 EXPORT_SYMBOL(__cputime_sec_factor);
 u64 __cputime_clockt_factor;
@@ -182,8 +182,8 @@ static void calc_cputime_factors(void)
 
 	div128_by_32(HZ, 0, tb_ticks_per_sec, &res);
 	__cputime_jiffies_factor = res.result_low;
-	div128_by_32(1000, 0, tb_ticks_per_sec, &res);
-	__cputime_msec_factor = res.result_low;
+	div128_by_32(1000000, 0, tb_ticks_per_sec, &res);
+	__cputime_usec_factor = res.result_low;
 	div128_by_32(1, 0, tb_ticks_per_sec, &res);
 	__cputime_sec_factor = res.result_low;
 	div128_by_32(USER_HZ, 0, tb_ticks_per_sec, &res);

commit 7df1027542c9353bef4d027cb4ab8e99f69017b7
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Nov 23 20:07:22 2011 +0000

    powerpc/time: Optimise decrementer_check_overflow
    
    decrementer_check_overflow is called from arch_local_irq_restore so
    we want to make it as light weight as possible. As such, turn
    decrementer_check_overflow into an inline function.
    
    To avoid a circular mess of includes, separate out the two components
    of struct decrementer_clock and keep the struct clock_event_device
    part local to time.c.
    
    The fast path improves from:
    
    arch_local_irq_restore
         0:       mflr    r0
         4:       std     r0,16(r1)
         8:       stdu    r1,-112(r1)
         c:       stb     r3,578(r13)
        10:       cmpdi   cr7,r3,0
        14:       beq-    cr7,24 <.arch_local_irq_restore+0x24>
    ...
        24:       addi    r1,r1,112
        28:       ld      r0,16(r1)
        2c:       mtlr    r0
        30:       blr
    
    to:
    
    arch_local_irq_restore
        0:       std     r30,-16(r1)
        4:       ld      r30,0(r2)
        8:       stb     r3,578(r13)
        c:       cmpdi   cr7,r3,0
       10:       beq-    cr7,6c <.arch_local_irq_restore+0x6c>
    ...
       6c:       ld      r30,-16(r1)
       70:       blr
    
    Unfortunately we still setup a local TOC (due to -mminimal-toc). Yet
    another sign we should be moving to -mcmodel=medium.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index b1990b987e2c..9754743db8b9 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -114,12 +114,8 @@ static struct clock_event_device decrementer_clockevent = {
 	.features       = CLOCK_EVT_FEAT_ONESHOT,
 };
 
-struct decrementer_clock {
-	struct clock_event_device event;
-	u64 next_tb;
-};
-
-static DEFINE_PER_CPU(struct decrementer_clock, decrementers);
+DEFINE_PER_CPU(u64, decrementers_next_tb);
+static DEFINE_PER_CPU(struct clock_event_device, decrementers);
 
 #ifdef CONFIG_PPC_ISERIES
 static unsigned long __initdata iSeries_recal_titan;
@@ -570,8 +566,8 @@ void arch_irq_work_raise(void)
 void timer_interrupt(struct pt_regs * regs)
 {
 	struct pt_regs *old_regs;
-	struct decrementer_clock *decrementer =  &__get_cpu_var(decrementers);
-	struct clock_event_device *evt = &decrementer->event;
+	u64 *next_tb = &__get_cpu_var(decrementers_next_tb);
+	struct clock_event_device *evt = &__get_cpu_var(decrementers);
 
 	/* Ensure a positive value is written to the decrementer, or else
 	 * some CPUs will continue to take decrementer exceptions.
@@ -606,7 +602,7 @@ void timer_interrupt(struct pt_regs * regs)
 		get_lppaca()->int_dword.fields.decr_int = 0;
 #endif
 
-	decrementer->next_tb = ~(u64)0;
+	*next_tb = ~(u64)0;
 	if (evt->event_handler)
 		evt->event_handler(evt);
 
@@ -872,19 +868,10 @@ static void __init clocksource_init(void)
 	       clock->name, clock->mult, clock->shift);
 }
 
-void decrementer_check_overflow(void)
-{
-	u64 now = get_tb_or_rtc();
-	struct decrementer_clock *decrementer = &__get_cpu_var(decrementers);
-
-	if (now >= decrementer->next_tb)
-		set_dec(1);
-}
-
 static int decrementer_set_next_event(unsigned long evt,
 				      struct clock_event_device *dev)
 {
-	__get_cpu_var(decrementers).next_tb = get_tb_or_rtc() + evt;
+	__get_cpu_var(decrementers_next_tb) = get_tb_or_rtc() + evt;
 	set_dec(evt);
 	return 0;
 }
@@ -898,7 +885,7 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 
 static void register_decrementer_clockevent(int cpu)
 {
-	struct clock_event_device *dec = &per_cpu(decrementers, cpu).event;
+	struct clock_event_device *dec = &per_cpu(decrementers, cpu);
 
 	*dec = decrementer_clockevent;
 	dec->cpumask = cpumask_of(cpu);

commit 621692cb7efb6d0e38c62e41844a6360c6719b20
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Nov 23 20:07:21 2011 +0000

    powerpc/time: Fix some style issues
    
    Fix some formatting issues and use the DECREMENTER_MAX
    define instead of 0x7fffffff.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 2eaaa242c2e6..b1990b987e2c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -106,12 +106,12 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 				 struct clock_event_device *dev);
 
 static struct clock_event_device decrementer_clockevent = {
-       .name           = "decrementer",
-       .rating         = 200,
-       .irq            = 0,
-       .set_next_event = decrementer_set_next_event,
-       .set_mode       = decrementer_set_mode,
-       .features       = CLOCK_EVT_FEAT_ONESHOT,
+	.name           = "decrementer",
+	.rating         = 200,
+	.irq            = 0,
+	.set_next_event = decrementer_set_next_event,
+	.set_mode       = decrementer_set_mode,
+	.features       = CLOCK_EVT_FEAT_ONESHOT,
 };
 
 struct decrementer_clock {
@@ -435,7 +435,7 @@ EXPORT_SYMBOL(profile_pc);
 /* 
  * This function recalibrates the timebase based on the 49-bit time-of-day
  * value in the Titan chip.  The Titan is much more accurate than the value
- * returned by the service processor for the timebase frequency.  
+ * returned by the service processor for the timebase frequency.
  */
 
 static int __init iSeries_tb_recal(void)
@@ -636,9 +636,9 @@ static void generic_suspend_disable_irqs(void)
 	 * with suspending.
 	 */
 
-	set_dec(0x7fffffff);
+	set_dec(DECREMENTER_MAX);
 	local_irq_disable();
-	set_dec(0x7fffffff);
+	set_dec(DECREMENTER_MAX);
 }
 
 static void generic_suspend_enable_irqs(void)
@@ -982,10 +982,10 @@ void __init time_init(void)
 	boot_tb = get_tb_or_rtc();
 
 	/* If platform provided a timezone (pmac), we correct the time */
-        if (timezone_offset) {
+	if (timezone_offset) {
 		sys_tz.tz_minuteswest = -timezone_offset / 60;
 		sys_tz.tz_dsttime = 0;
-        }
+	}
 
 	vdso_data->tb_update_count = 0;
 	vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;

commit 68568add2ca70153cca3dd1858eaa0776821cf75
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Nov 23 20:07:20 2011 +0000

    powerpc/time: Remove unnecessary sanity check of decrementer expiration
    
    The clockevents code uses max_delta_ns to avoid calling a
    clockevent with too large a value.
    
    Remove the redundant version of this in the timer_interrupt
    code.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index d204b726a185..2eaaa242c2e6 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -572,7 +572,6 @@ void timer_interrupt(struct pt_regs * regs)
 	struct pt_regs *old_regs;
 	struct decrementer_clock *decrementer =  &__get_cpu_var(decrementers);
 	struct clock_event_device *evt = &decrementer->event;
-	u64 now;
 
 	/* Ensure a positive value is written to the decrementer, or else
 	 * some CPUs will continue to take decrementer exceptions.
@@ -607,16 +606,9 @@ void timer_interrupt(struct pt_regs * regs)
 		get_lppaca()->int_dword.fields.decr_int = 0;
 #endif
 
-	now = get_tb_or_rtc();
-	if (now >= decrementer->next_tb) {
-		decrementer->next_tb = ~(u64)0;
-		if (evt->event_handler)
-			evt->event_handler(evt);
-	} else {
-		now = decrementer->next_tb - now;
-		if (now <= DECREMENTER_MAX)
-			set_dec((int)now);
-	}
+	decrementer->next_tb = ~(u64)0;
+	if (evt->event_handler)
+		evt->event_handler(evt);
 
 #ifdef CONFIG_PPC_ISERIES
 	if (firmware_has_feature(FW_FEATURE_ISERIES) && hvlpevent_is_pending())

commit 11b8633ada8633991e584951d0027f2741162201
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Nov 23 20:07:19 2011 +0000

    powerpc/time: Use clocksource_register_hz
    
    Use clocksource_register_hz which calculates the shift/mult
    factors for us. Also remove the shift = 22 assumption in
    vsyscall_update - thanks to Paul Mackerras and John Stultz for
    catching that.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index fae3094c2a9a..d204b726a185 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -86,8 +86,6 @@ static struct clocksource clocksource_rtc = {
 	.rating       = 400,
 	.flags        = CLOCK_SOURCE_IS_CONTINUOUS,
 	.mask         = CLOCKSOURCE_MASK(64),
-	.shift        = 22,
-	.mult         = 0,	/* To be filled in */
 	.read         = rtc_read,
 };
 
@@ -97,8 +95,6 @@ static struct clocksource clocksource_timebase = {
 	.rating       = 400,
 	.flags        = CLOCK_SOURCE_IS_CONTINUOUS,
 	.mask         = CLOCKSOURCE_MASK(64),
-	.shift        = 22,
-	.mult         = 0,	/* To be filled in */
 	.read         = timebase_read,
 };
 
@@ -822,9 +818,8 @@ void update_vsyscall(struct timespec *wall_time, struct timespec *wtm,
 	++vdso_data->tb_update_count;
 	smp_mb();
 
-	/* XXX this assumes clock->shift == 22 */
-	/* 4611686018 ~= 2^(20+64-22) / 1e9 */
-	new_tb_to_xs = (u64) mult * 4611686018ULL;
+	/* 19342813113834067 ~= 2^(20+64) / 1e9 */
+	new_tb_to_xs = (u64) mult * (19342813113834067ULL >> clock->shift);
 	new_stamp_xsec = (u64) wall_time->tv_nsec * XSEC_PER_SEC;
 	do_div(new_stamp_xsec, 1000000000);
 	new_stamp_xsec += (u64) wall_time->tv_sec * XSEC_PER_SEC;
@@ -875,9 +870,7 @@ static void __init clocksource_init(void)
 	else
 		clock = &clocksource_timebase;
 
-	clock->mult = clocksource_hz2mult(tb_ticks_per_sec, clock->shift);
-
-	if (clocksource_register(clock)) {
+	if (clocksource_register_hz(clock, tb_ticks_per_sec)) {
 		printk(KERN_ERR "clocksource: %s is already registered\n",
 		       clock->name);
 		return;

commit d8afc6fd95496204174f19af0cb39eefee0c3e8a
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Nov 23 20:07:18 2011 +0000

    powerpc/time: Use clockevents_calc_mult_shift
    
    We can use clockevents_calc_mult_shift instead of doing all
    the work ourselves.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 5db163c96751..fae3094c2a9a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -112,8 +112,6 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 static struct clock_event_device decrementer_clockevent = {
        .name           = "decrementer",
        .rating         = 200,
-       .shift          = 0,	/* To be filled in */
-       .mult           = 0,	/* To be filled in */
        .irq            = 0,
        .set_next_event = decrementer_set_next_event,
        .set_mode       = decrementer_set_mode,
@@ -913,31 +911,6 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 		decrementer_set_next_event(DECREMENTER_MAX, dev);
 }
 
-static inline uint64_t div_sc64(unsigned long ticks, unsigned long nsec,
-				int shift)
-{
-	uint64_t tmp = ((uint64_t)ticks) << shift;
-
-	do_div(tmp, nsec);
-	return tmp;
-}
-
-static void __init setup_clockevent_multiplier(unsigned long hz)
-{
-	u64 mult, shift = 32;
-
-	while (1) {
-		mult = div_sc64(hz, NSEC_PER_SEC, shift);
-		if (mult && (mult >> 32UL) == 0UL)
-			break;
-
-		shift--;
-	}
-
-	decrementer_clockevent.shift = shift;
-	decrementer_clockevent.mult = mult;
-}
-
 static void register_decrementer_clockevent(int cpu)
 {
 	struct clock_event_device *dec = &per_cpu(decrementers, cpu).event;
@@ -955,7 +928,8 @@ static void __init init_decrementer_clockevent(void)
 {
 	int cpu = smp_processor_id();
 
-	setup_clockevent_multiplier(ppc_tb_freq);
+	clockevents_calc_mult_shift(&decrementer_clockevent, ppc_tb_freq, 4);
+
 	decrementer_clockevent.max_delta_ns =
 		clockevent_delta2ns(DECREMENTER_MAX, &decrementer_clockevent);
 	decrementer_clockevent.min_delta_ns =

commit 37fb9a0231ee43d42d069863bdfd567fca2b61af
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Nov 23 20:07:17 2011 +0000

    powerpc/time: Handle wrapping of decrementer
    
    When re-enabling interrupts we have code to handle edge sensitive
    decrementers by resetting the decrementer to 1 whenever it is negative.
    If interrupts were disabled long enough that the decrementer wrapped to
    positive we do nothing. This means interrupts can be delayed for a long
    time until it finally goes negative again.
    
    While we hope interrupts are never be disabled long enough for the
    decrementer to go positive, we have a very good test team that can
    drive any kernel into the ground. The softlockup data we get back
    from these fails could be seconds in the future, completely missing
    the cause of the lockup.
    
    We already keep track of the timebase of the next event so use that
    to work out if we should trigger a decrementer exception.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: stable@kernel.org
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 522bb1dfc353..5db163c96751 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -889,6 +889,15 @@ static void __init clocksource_init(void)
 	       clock->name, clock->mult, clock->shift);
 }
 
+void decrementer_check_overflow(void)
+{
+	u64 now = get_tb_or_rtc();
+	struct decrementer_clock *decrementer = &__get_cpu_var(decrementers);
+
+	if (now >= decrementer->next_tb)
+		set_dec(1);
+}
+
 static int decrementer_set_next_event(unsigned long evt,
 				      struct clock_event_device *dev)
 {

commit 4b16f8e2d6d64249f0ed3ca7fe2a319d0dde2719
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jul 22 18:24:23 2011 -0400

    powerpc: various straight conversions from module.h --> export.h
    
    All these files were including module.h just for the basic
    EXPORT_SYMBOL infrastructure.  We can shift them off to the
    export.h header which is a way smaller footprint and thus
    realize some compile time gains.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 03b29a6759ab..522bb1dfc353 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -33,7 +33,7 @@
  */
 
 #include <linux/errno.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>
 #include <linux/param.h>

commit 4f8b50bbbe63ae4ec6bea28a90a9a603c745ea71
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Jun 27 17:22:43 2011 +0200

    irq_work, ppc: Fix up arch hooks
    
    Commit e360adbe29 ("irq_work: Add generic hardirq context
    callbacks") fouled up the ppc bit, not properly naming the
    arch specific function that raises the 'self-IPI'.
    
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: stable@kernel.org # 37+
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-eg0aqien8p1aqvzu9dft6dtv@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index f33acfd872ad..03b29a6759ab 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -544,7 +544,7 @@ DEFINE_PER_CPU(u8, irq_work_pending);
 
 #endif /* 32 vs 64 bit */
 
-void set_irq_work_pending(void)
+void arch_irq_work_raise(void)
 {
 	preempt_disable();
 	set_irq_work_pending_flag();

commit 84ffae55af79d7b8834fd0c08d0d1ebf2c77f91e
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Apr 7 21:44:21 2011 +0000

    powerpc: Fix oops if scan_dispatch_log is called too early
    
    We currently enable interrupts before the dispatch log for the boot
    cpu is setup. If a timer interrupt comes in early enough we oops in
    scan_dispatch_log:
    
    Unable to handle kernel paging request for data at address 0x00000010
    
    ...
    
    .scan_dispatch_log+0xb0/0x170
    .account_system_vtime+0xa0/0x220
    .irq_enter+0x88/0xc0
    .do_IRQ+0x48/0x230
    
    The patch below adds a check to scan_dispatch_log to ensure the
    dispatch log has been allocated.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: <stable@kernel.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 375480c56eb9..f33acfd872ad 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -229,6 +229,9 @@ static u64 scan_dispatch_log(u64 stop_tb)
 	u64 stolen = 0;
 	u64 dtb;
 
+	if (!dtl)
+		return 0;
+
 	if (i == vpa->dtl_idx)
 		return 0;
 	while (i < vpa->dtl_idx) {

commit 963e5d3b76d657f1ebcf3561446d2ba1872bbfa2
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Mar 29 14:51:10 2011 +1100

    powerpc: Make decrementer interrupt robust against offlined CPUs
    
    With some implementations, it is possible that a timer interrupt
    occurs every few seconds on an offline CPU. In this case, just
    re-arm the decrementer and return immediately
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index aa9269600ca2..375480c56eb9 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -577,14 +577,21 @@ void timer_interrupt(struct pt_regs * regs)
 	struct clock_event_device *evt = &decrementer->event;
 	u64 now;
 
+	/* Ensure a positive value is written to the decrementer, or else
+	 * some CPUs will continue to take decrementer exceptions.
+	 */
+	set_dec(DECREMENTER_MAX);
+
+	/* Some implementations of hotplug will get timer interrupts while
+	 * offline, just ignore these
+	 */
+	if (!cpu_online(smp_processor_id()))
+		return;
+
 	trace_timer_interrupt_entry(regs);
 
 	__get_cpu_var(irq_stat).timer_irqs++;
 
-	/* Ensure a positive value is written to the decrementer, or else
-	 * some CPUs will continuue to take decrementer exceptions */
-	set_dec(DECREMENTER_MAX);
-
 #if defined(CONFIG_PPC32) && defined(CONFIG_PMAC)
 	if (atomic_read(&ppc_n_lost_interrupts) != 0)
 		do_IRQ(regs);

commit ad5d1c888e556bc00c4e86f452cad4a3a87d22c1
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Mar 20 15:28:03 2011 +0000

    powerpc: Fix accounting of softirq time when idle
    
    commit cf9efce0ce31 (powerpc: Account time using timebase rather
    than PURR) used in_irq() to detect if the time was spent in
    interrupt processing. This only catches hardirq context so if we
    are in softirq context and in the idle loop we end up accounting it
    as idle time. If we instead use in_interrupt() we catch both softirq
    and hardirq time.
    
    The issue was found when running a network intensive workload. top
    showed the following:
    
    0.0%us,  1.1%sy,  0.0%ni, 85.7%id,  0.0%wa,  9.9%hi,  3.3%si,  0.0%st
    
    85.7% idle. But this was wildly different to the perf events data.
    To confirm the suspicion I ran something to keep the core busy:
    
    # yes > /dev/null &
    
    8.2%us,  0.0%sy,  0.0%ni,  0.0%id,  0.0%wa, 10.3%hi, 81.4%si,  0.0%st
    
    We only got 8.2% of the CPU for the userspace task and softirq has
    shot up to 81.4%.
    
    With the patch below top shows the correct stats:
    
    0.0%us,  0.0%sy,  0.0%ni,  5.3%id,  0.0%wa, 13.3%hi, 81.3%si,  0.0%st
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: stable@kernel.org
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 09d31dbf43f9..aa9269600ca2 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -356,7 +356,7 @@ void account_system_vtime(struct task_struct *tsk)
 	}
 	get_paca()->user_time_scaled += user_scaled;
 
-	if (in_irq() || idle_task(smp_processor_id()) != tsk) {
+	if (in_interrupt() || idle_task(smp_processor_id()) != tsk) {
 		account_system_time(tsk, 0, delta, sys_scaled);
 		if (stolen)
 			account_steal_time(stolen);

commit b18ae08deac23187e4a22a8c94a1a473be8e8c93
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Jan 3 03:49:25 2011 +0000

    powerpc/cell: Use system_wq in cpufreq_spudemand
    
    With cmwq, there's no reason to use a separate workqueue in
    cpufreq_spudemand.  Use system_wq instead.  The work items are already
    sync canceled on stop, so it's already guaranteed that no work is
    running when spu_gov_exit() is entered.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Dave Jones <davej@redhat.com>
    Cc: cpufreq@vger.kernel.org
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 09e4dea4a85a..09d31dbf43f9 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -265,11 +265,26 @@ void accumulate_stolen_time(void)
 {
 	u64 sst, ust;
 
-	sst = scan_dispatch_log(get_paca()->starttime_user);
-	ust = scan_dispatch_log(get_paca()->starttime);
-	get_paca()->system_time -= sst;
-	get_paca()->user_time -= ust;
-	get_paca()->stolen_time += ust + sst;
+	u8 save_soft_enabled = local_paca->soft_enabled;
+	u8 save_hard_enabled = local_paca->hard_enabled;
+
+	/* We are called early in the exception entry, before
+	 * soft/hard_enabled are sync'ed to the expected state
+	 * for the exception. We are hard disabled but the PACA
+	 * needs to reflect that so various debug stuff doesn't
+	 * complain
+	 */
+	local_paca->soft_enabled = 0;
+	local_paca->hard_enabled = 0;
+
+	sst = scan_dispatch_log(local_paca->starttime_user);
+	ust = scan_dispatch_log(local_paca->starttime);
+	local_paca->system_time -= sst;
+	local_paca->user_time -= ust;
+	local_paca->stolen_time += ust + sst;
+
+	local_paca->soft_enabled = save_soft_enabled;
+	local_paca->hard_enabled = save_hard_enabled;
 }
 
 static inline u64 calculate_stolen_time(u64 stop_tb)

commit 364a1246522f99cbe58040e99af007ada31034ed
Author: Heiko Schocher <hs@denx.de>
Date:   Mon Nov 22 21:30:33 2010 +0000

    powerpc/time: printk time stamp init not correct
    
    problem:
    
    I see sometimes on my mpc5200 based board such printk timing
    information:
    
    [    0.000000] NR_IRQS:512 nr_irqs:512 16
    [    0.000000] MPC52xx PIC is up and running!
    [    0.000000] clocksource: timebase mult[79364d9] shift[22] registered
    [    0.000000] console [ttyPSC0] enabled
    [  130.300633] pid_max: default: 32768 minimum: 301
    [  130.305647] Mount-cache hash table entries: 512
    [  130.315818] NET: Registered protocol family 16
    
    reason:
    if the tbu not starts from 0 when linux boots, boot_tb
    maybe could not store the real 64 bit tbu value, because
    boot_tp is only a 32 bit unsigned long.
    
    solution:
    change boot_tb to u64
    
    [BenH: Made it u64 instead of unsigned long long]
    
    Signed-off-by: Heiko Schocher <hs@denx.de>
    cc: Wolfgang Denk <wd@denx.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 010406958d97..09e4dea4a85a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -155,7 +155,7 @@ EXPORT_SYMBOL_GPL(rtc_lock);
 
 static u64 tb_to_ns_scale __read_mostly;
 static unsigned tb_to_ns_shift __read_mostly;
-static unsigned long boot_tb __read_mostly;
+static u64 boot_tb __read_mostly;
 
 extern struct timezone sys_tz;
 static long timezone_offset;

commit d4429f608abde89e8bc1e24b43cd503feb95c496
Merge: e10117d36ef7 6a1c9dfe4186
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 21:19:54 2010 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (71 commits)
      powerpc/44x: Update ppc44x_defconfig
      powerpc/watchdog: Make default timeout for Book-E watchdog a Kconfig option
      fsl_rio: Add comments for sRIO registers.
      powerpc/fsl-booke: Add e55xx (64-bit) smp defconfig
      powerpc/fsl-booke: Add p5020 DS board support
      powerpc/fsl-booke64: Use TLB CAMs to cover linear mapping on FSL 64-bit chips
      powerpc/fsl-booke: Add support for FSL Arch v1.0 MMU in setup_page_sizes
      powerpc/fsl-booke: Add support for FSL 64-bit e5500 core
      powerpc/85xx: add cache-sram support
      powerpc/85xx: add ngPIXIS FPGA device tree node to the P1022DS board
      powerpc: Fix compile error with paca code on ppc64e
      powerpc/fsl-booke: Add p3041 DS board support
      oprofile/fsl emb: Don't set MSR[PMM] until after clearing the interrupt.
      powerpc/fsl-booke: Add PCI device ids for P2040/P3041/P5010/P5020 QoirQ chips
      powerpc/mpc8xxx_gpio: Add support for 'qoriq-gpio' controllers
      powerpc/fsl_booke: Add support to boot from core other than 0
      powerpc/p1022: Add probing for individual DMA channels
      powerpc/fsl_soc: Search all global-utilities nodes for rstccr
      powerpc: Fix invalid page flags in create TLB CAM path for PTE_64BIT
      powerpc/mpc83xx: Support for MPC8308 P1M board
      ...
    
    Fix up conflict with the generic irq_work changes in arch/powerpc/kernel/time.c

commit e360adbe29241a0194e10e20595360dd7b98a2b3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 14 14:01:34 2010 +0800

    irq_work: Add generic hardirq context callbacks
    
    Provide a mechanism that allows running code in IRQ context. It is
    most useful for NMI code that needs to interact with the rest of the
    system -- like wakeup a task to drain buffers.
    
    Perf currently has such a mechanism, so extract that and provide it as
    a generic feature, independent of perf so that others may also
    benefit.
    
    The IRQ context callback is generated through self-IPIs where
    possible, or on architectures like powerpc the decrementer (the
    built-in timer facility) is set to generate an interrupt immediately.
    
    Architectures that don't have anything like this get to do with a
    callback from the timer tick. These architectures can call
    irq_work_run() at the tail of any IRQ handlers that might enqueue such
    work (like the perf IRQ handler) to avoid undue latencies in
    processing the work.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Kyle McMartin <kyle@mcmartin.ca>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    [ various fixes ]
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    LKML-Reference: <1287036094.7768.291.camel@yhuang-dev>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 8533b3b83f5d..54888eb10c3b 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -53,7 +53,7 @@
 #include <linux/posix-timers.h>
 #include <linux/irq.h>
 #include <linux/delay.h>
-#include <linux/perf_event.h>
+#include <linux/irq_work.h>
 #include <asm/trace.h>
 
 #include <asm/io.h>
@@ -493,60 +493,60 @@ void __init iSeries_time_init_early(void)
 }
 #endif /* CONFIG_PPC_ISERIES */
 
-#ifdef CONFIG_PERF_EVENTS
+#ifdef CONFIG_IRQ_WORK
 
 /*
  * 64-bit uses a byte in the PACA, 32-bit uses a per-cpu variable...
  */
 #ifdef CONFIG_PPC64
-static inline unsigned long test_perf_event_pending(void)
+static inline unsigned long test_irq_work_pending(void)
 {
 	unsigned long x;
 
 	asm volatile("lbz %0,%1(13)"
 		: "=r" (x)
-		: "i" (offsetof(struct paca_struct, perf_event_pending)));
+		: "i" (offsetof(struct paca_struct, irq_work_pending)));
 	return x;
 }
 
-static inline void set_perf_event_pending_flag(void)
+static inline void set_irq_work_pending_flag(void)
 {
 	asm volatile("stb %0,%1(13)" : :
 		"r" (1),
-		"i" (offsetof(struct paca_struct, perf_event_pending)));
+		"i" (offsetof(struct paca_struct, irq_work_pending)));
 }
 
-static inline void clear_perf_event_pending(void)
+static inline void clear_irq_work_pending(void)
 {
 	asm volatile("stb %0,%1(13)" : :
 		"r" (0),
-		"i" (offsetof(struct paca_struct, perf_event_pending)));
+		"i" (offsetof(struct paca_struct, irq_work_pending)));
 }
 
 #else /* 32-bit */
 
-DEFINE_PER_CPU(u8, perf_event_pending);
+DEFINE_PER_CPU(u8, irq_work_pending);
 
-#define set_perf_event_pending_flag()	__get_cpu_var(perf_event_pending) = 1
-#define test_perf_event_pending()	__get_cpu_var(perf_event_pending)
-#define clear_perf_event_pending()	__get_cpu_var(perf_event_pending) = 0
+#define set_irq_work_pending_flag()	__get_cpu_var(irq_work_pending) = 1
+#define test_irq_work_pending()		__get_cpu_var(irq_work_pending)
+#define clear_irq_work_pending()	__get_cpu_var(irq_work_pending) = 0
 
 #endif /* 32 vs 64 bit */
 
-void set_perf_event_pending(void)
+void set_irq_work_pending(void)
 {
 	preempt_disable();
-	set_perf_event_pending_flag();
+	set_irq_work_pending_flag();
 	set_dec(1);
 	preempt_enable();
 }
 
-#else  /* CONFIG_PERF_EVENTS */
+#else  /* CONFIG_IRQ_WORK */
 
-#define test_perf_event_pending()	0
-#define clear_perf_event_pending()
+#define test_irq_work_pending()	0
+#define clear_irq_work_pending()
 
-#endif /* CONFIG_PERF_EVENTS */
+#endif /* CONFIG_IRQ_WORK */
 
 /*
  * For iSeries shared processors, we have to let the hypervisor
@@ -587,9 +587,9 @@ void timer_interrupt(struct pt_regs * regs)
 
 	calculate_steal_time();
 
-	if (test_perf_event_pending()) {
-		clear_perf_event_pending();
-		perf_event_do_pending();
+	if (test_irq_work_pending()) {
+		clear_irq_work_pending();
+		irq_work_run();
 	}
 
 #ifdef CONFIG_PPC_ISERIES

commit 55ec2fca3e99f83b5c674e9aba713d848392f6cc
Author: Timur Tabi <timur@freescale.com>
Date:   Mon Sep 20 11:23:41 2010 -0500

    powerpc: export ppc_proc_freq and ppc_tb_freq as GPL symbols
    
    Export the global variable 'ppc_tb_freq', so that modules (like the Book-E
    watchdog driver) can use it.  To maintain consistency, ppc_proc_freq is
    changed to a GPL-only export.  This is okay, because any module that needs
    this symbol should be an actual Linux driver, which must be GPL-licensed.
    
    Signed-off-by: Timur Tabi <timur@freescale.com>
    Acked-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index bcb738b9ff8c..644f9188d8e7 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -161,8 +161,9 @@ extern struct timezone sys_tz;
 static long timezone_offset;
 
 unsigned long ppc_proc_freq;
-EXPORT_SYMBOL(ppc_proc_freq);
+EXPORT_SYMBOL_GPL(ppc_proc_freq);
 unsigned long ppc_tb_freq;
+EXPORT_SYMBOL_GPL(ppc_tb_freq);
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING
 /*

commit 872e439a45ed4a4bd499bc55cb0dffa74027f749
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Aug 31 01:59:53 2010 +0000

    powerpc/pseries: Re-enable dispatch trace log userspace interface
    
    Since the cpu accounting code uses the hypervisor dispatch trace log
    now when CONFIG_VIRT_CPU_ACCOUNTING = y, the previous commit disabled
    access to it via files in the /sys/kernel/debug/powerpc/dtl/ directory
    in that case.  This restores those files.
    
    To do this, we now have a hook that the cpu accounting code will call
    as it processes each entry from the hypervisor dispatch trace log.
    The code in dtl.c now uses that to fill up its ring buffer, rather
    than having the hypervisor fill the ring buffer directly.
    
    This also fixes dtl_file_read() to handle overflow conditions a bit
    better and adds a spinlock to ensure that race conditions (multiple
    processes opening or reading the file concurrently) are handled
    correctly.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index fca20643c368..bcb738b9ff8c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -183,6 +183,8 @@ DEFINE_PER_CPU(unsigned long, cputime_scaled_last_delta);
 
 cputime_t cputime_one_jiffy;
 
+void (*dtl_consumer)(struct dtl_entry *, u64);
+
 static void calc_cputime_factors(void)
 {
 	struct div_result res;
@@ -218,7 +220,7 @@ static u64 read_spurr(u64 tb)
  */
 static u64 scan_dispatch_log(u64 stop_tb)
 {
-	unsigned long i = local_paca->dtl_ridx;
+	u64 i = local_paca->dtl_ridx;
 	struct dtl_entry *dtl = local_paca->dtl_curr;
 	struct dtl_entry *dtl_end = local_paca->dispatch_log_end;
 	struct lppaca *vpa = local_paca->lppaca_ptr;
@@ -229,6 +231,8 @@ static u64 scan_dispatch_log(u64 stop_tb)
 	if (i == vpa->dtl_idx)
 		return 0;
 	while (i < vpa->dtl_idx) {
+		if (dtl_consumer)
+			dtl_consumer(dtl, i);
 		dtb = dtl->timebase;
 		tb_delta = dtl->enqueue_to_dispatch_time +
 			dtl->ready_to_enqueue_time;

commit cf9efce0ce3136fa076f53e53154e98455229514
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Aug 26 19:56:43 2010 +0000

    powerpc: Account time using timebase rather than PURR
    
    Currently, when CONFIG_VIRT_CPU_ACCOUNTING is enabled, we use the
    PURR register for measuring the user and system time used by
    processes, as well as other related times such as hardirq and
    softirq times.  This turns out to be quite confusing for users
    because it means that a program will often be measured as taking
    less time when run on a multi-threaded processor (SMT2 or SMT4 mode)
    than it does when run on a single-threaded processor (ST mode), even
    though the program takes longer to finish.  The discrepancy is
    accounted for as stolen time, which is also confusing, particularly
    when there are no other partitions running.
    
    This changes the accounting to use the timebase instead, meaning that
    the reported user and system times are the actual number of real-time
    seconds that the program was executing on the processor thread,
    regardless of which SMT mode the processor is in.  Thus a program will
    generally show greater user and system times when run on a
    multi-threaded processor than on a single-threaded processor.
    
    On pSeries systems on POWER5 or later processors, we measure the
    stolen time (time when this partition wasn't running) using the
    hypervisor dispatch trace log.  We check for new entries in the
    log on every entry from user mode and on every transition from
    kernel process context to soft or hard IRQ context (i.e. when
    account_system_vtime() gets called).  So that we can correctly
    distinguish time stolen from user time and time stolen from system
    time, without having to check the log on every exit to user mode,
    we store separate timestamps for exit to user mode and entry from
    user mode.
    
    On systems that have a SPURR (POWER6 and POWER7), we read the SPURR
    in account_system_vtime() (as before), and then apportion the SPURR
    ticks since the last time we read it between scaled user time and
    scaled system time according to the relative proportions of user
    time and system time over the same interval.  This avoids having to
    read the SPURR on every kernel entry and exit.  On systems that have
    PURR but not SPURR (i.e., POWER5), we do the same using the PURR
    rather than the SPURR.
    
    This disables the DTL user interface in /sys/debug/kernel/powerpc/dtl
    for now since it conflicts with the use of the dispatch trace log
    by the time accounting code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 8533b3b83f5d..fca20643c368 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -164,8 +164,6 @@ unsigned long ppc_proc_freq;
 EXPORT_SYMBOL(ppc_proc_freq);
 unsigned long ppc_tb_freq;
 
-static DEFINE_PER_CPU(u64, last_jiffy);
-
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING
 /*
  * Factors for converting from cputime_t (timebase ticks) to
@@ -200,62 +198,151 @@ static void calc_cputime_factors(void)
 }
 
 /*
- * Read the PURR on systems that have it, otherwise the timebase.
+ * Read the SPURR on systems that have it, otherwise the PURR,
+ * or if that doesn't exist return the timebase value passed in.
  */
-static u64 read_purr(void)
+static u64 read_spurr(u64 tb)
 {
+	if (cpu_has_feature(CPU_FTR_SPURR))
+		return mfspr(SPRN_SPURR);
 	if (cpu_has_feature(CPU_FTR_PURR))
 		return mfspr(SPRN_PURR);
-	return mftb();
+	return tb;
 }
 
+#ifdef CONFIG_PPC_SPLPAR
+
 /*
- * Read the SPURR on systems that have it, otherwise the purr
+ * Scan the dispatch trace log and count up the stolen time.
+ * Should be called with interrupts disabled.
  */
-static u64 read_spurr(u64 purr)
+static u64 scan_dispatch_log(u64 stop_tb)
 {
-	/*
-	 * cpus without PURR won't have a SPURR
-	 * We already know the former when we use this, so tell gcc
-	 */
-	if (cpu_has_feature(CPU_FTR_PURR) && cpu_has_feature(CPU_FTR_SPURR))
-		return mfspr(SPRN_SPURR);
-	return purr;
+	unsigned long i = local_paca->dtl_ridx;
+	struct dtl_entry *dtl = local_paca->dtl_curr;
+	struct dtl_entry *dtl_end = local_paca->dispatch_log_end;
+	struct lppaca *vpa = local_paca->lppaca_ptr;
+	u64 tb_delta;
+	u64 stolen = 0;
+	u64 dtb;
+
+	if (i == vpa->dtl_idx)
+		return 0;
+	while (i < vpa->dtl_idx) {
+		dtb = dtl->timebase;
+		tb_delta = dtl->enqueue_to_dispatch_time +
+			dtl->ready_to_enqueue_time;
+		barrier();
+		if (i + N_DISPATCH_LOG < vpa->dtl_idx) {
+			/* buffer has overflowed */
+			i = vpa->dtl_idx - N_DISPATCH_LOG;
+			dtl = local_paca->dispatch_log + (i % N_DISPATCH_LOG);
+			continue;
+		}
+		if (dtb > stop_tb)
+			break;
+		stolen += tb_delta;
+		++i;
+		++dtl;
+		if (dtl == dtl_end)
+			dtl = local_paca->dispatch_log;
+	}
+	local_paca->dtl_ridx = i;
+	local_paca->dtl_curr = dtl;
+	return stolen;
 }
 
+/*
+ * Accumulate stolen time by scanning the dispatch trace log.
+ * Called on entry from user mode.
+ */
+void accumulate_stolen_time(void)
+{
+	u64 sst, ust;
+
+	sst = scan_dispatch_log(get_paca()->starttime_user);
+	ust = scan_dispatch_log(get_paca()->starttime);
+	get_paca()->system_time -= sst;
+	get_paca()->user_time -= ust;
+	get_paca()->stolen_time += ust + sst;
+}
+
+static inline u64 calculate_stolen_time(u64 stop_tb)
+{
+	u64 stolen = 0;
+
+	if (get_paca()->dtl_ridx != get_paca()->lppaca_ptr->dtl_idx) {
+		stolen = scan_dispatch_log(stop_tb);
+		get_paca()->system_time -= stolen;
+	}
+
+	stolen += get_paca()->stolen_time;
+	get_paca()->stolen_time = 0;
+	return stolen;
+}
+
+#else /* CONFIG_PPC_SPLPAR */
+static inline u64 calculate_stolen_time(u64 stop_tb)
+{
+	return 0;
+}
+
+#endif /* CONFIG_PPC_SPLPAR */
+
 /*
  * Account time for a transition between system, hard irq
  * or soft irq state.
  */
 void account_system_vtime(struct task_struct *tsk)
 {
-	u64 now, nowscaled, delta, deltascaled, sys_time;
+	u64 now, nowscaled, delta, deltascaled;
 	unsigned long flags;
+	u64 stolen, udelta, sys_scaled, user_scaled;
 
 	local_irq_save(flags);
-	now = read_purr();
+	now = mftb();
 	nowscaled = read_spurr(now);
-	delta = now - get_paca()->startpurr;
+	get_paca()->system_time += now - get_paca()->starttime;
+	get_paca()->starttime = now;
 	deltascaled = nowscaled - get_paca()->startspurr;
-	get_paca()->startpurr = now;
 	get_paca()->startspurr = nowscaled;
-	if (!in_interrupt()) {
-		/* deltascaled includes both user and system time.
-		 * Hence scale it based on the purr ratio to estimate
-		 * the system time */
-		sys_time = get_paca()->system_time;
-		if (get_paca()->user_time)
-			deltascaled = deltascaled * sys_time /
-			     (sys_time + get_paca()->user_time);
-		delta += sys_time;
-		get_paca()->system_time = 0;
+
+	stolen = calculate_stolen_time(now);
+
+	delta = get_paca()->system_time;
+	get_paca()->system_time = 0;
+	udelta = get_paca()->user_time - get_paca()->utime_sspurr;
+	get_paca()->utime_sspurr = get_paca()->user_time;
+
+	/*
+	 * Because we don't read the SPURR on every kernel entry/exit,
+	 * deltascaled includes both user and system SPURR ticks.
+	 * Apportion these ticks to system SPURR ticks and user
+	 * SPURR ticks in the same ratio as the system time (delta)
+	 * and user time (udelta) values obtained from the timebase
+	 * over the same interval.  The system ticks get accounted here;
+	 * the user ticks get saved up in paca->user_time_scaled to be
+	 * used by account_process_tick.
+	 */
+	sys_scaled = delta;
+	user_scaled = udelta;
+	if (deltascaled != delta + udelta) {
+		if (udelta) {
+			sys_scaled = deltascaled * delta / (delta + udelta);
+			user_scaled = deltascaled - sys_scaled;
+		} else {
+			sys_scaled = deltascaled;
+		}
+	}
+	get_paca()->user_time_scaled += user_scaled;
+
+	if (in_irq() || idle_task(smp_processor_id()) != tsk) {
+		account_system_time(tsk, 0, delta, sys_scaled);
+		if (stolen)
+			account_steal_time(stolen);
+	} else {
+		account_idle_time(delta + stolen);
 	}
-	if (in_irq() || idle_task(smp_processor_id()) != tsk)
-		account_system_time(tsk, 0, delta, deltascaled);
-	else
-		account_idle_time(delta);
-	__get_cpu_var(cputime_last_delta) = delta;
-	__get_cpu_var(cputime_scaled_last_delta) = deltascaled;
 	local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(account_system_vtime);
@@ -265,125 +352,26 @@ EXPORT_SYMBOL_GPL(account_system_vtime);
  * by the exception entry and exit code to the generic process
  * user and system time records.
  * Must be called with interrupts disabled.
+ * Assumes that account_system_vtime() has been called recently
+ * (i.e. since the last entry from usermode) so that
+ * get_paca()->user_time_scaled is up to date.
  */
 void account_process_tick(struct task_struct *tsk, int user_tick)
 {
 	cputime_t utime, utimescaled;
 
 	utime = get_paca()->user_time;
+	utimescaled = get_paca()->user_time_scaled;
 	get_paca()->user_time = 0;
-	utimescaled = cputime_to_scaled(utime);
+	get_paca()->user_time_scaled = 0;
+	get_paca()->utime_sspurr = 0;
 	account_user_time(tsk, utime, utimescaled);
 }
 
-/*
- * Stuff for accounting stolen time.
- */
-struct cpu_purr_data {
-	int	initialized;			/* thread is running */
-	u64	tb;			/* last TB value read */
-	u64	purr;			/* last PURR value read */
-	u64	spurr;			/* last SPURR value read */
-};
-
-/*
- * Each entry in the cpu_purr_data array is manipulated only by its
- * "owner" cpu -- usually in the timer interrupt but also occasionally
- * in process context for cpu online.  As long as cpus do not touch
- * each others' cpu_purr_data, disabling local interrupts is
- * sufficient to serialize accesses.
- */
-static DEFINE_PER_CPU(struct cpu_purr_data, cpu_purr_data);
-
-static void snapshot_tb_and_purr(void *data)
-{
-	unsigned long flags;
-	struct cpu_purr_data *p = &__get_cpu_var(cpu_purr_data);
-
-	local_irq_save(flags);
-	p->tb = get_tb_or_rtc();
-	p->purr = mfspr(SPRN_PURR);
-	wmb();
-	p->initialized = 1;
-	local_irq_restore(flags);
-}
-
-/*
- * Called during boot when all cpus have come up.
- */
-void snapshot_timebases(void)
-{
-	if (!cpu_has_feature(CPU_FTR_PURR))
-		return;
-	on_each_cpu(snapshot_tb_and_purr, NULL, 1);
-}
-
-/*
- * Must be called with interrupts disabled.
- */
-void calculate_steal_time(void)
-{
-	u64 tb, purr;
-	s64 stolen;
-	struct cpu_purr_data *pme;
-
-	pme = &__get_cpu_var(cpu_purr_data);
-	if (!pme->initialized)
-		return;		/* !CPU_FTR_PURR or early in early boot */
-	tb = mftb();
-	purr = mfspr(SPRN_PURR);
-	stolen = (tb - pme->tb) - (purr - pme->purr);
-	if (stolen > 0) {
-		if (idle_task(smp_processor_id()) != current)
-			account_steal_time(stolen);
-		else
-			account_idle_time(stolen);
-	}
-	pme->tb = tb;
-	pme->purr = purr;
-}
-
-#ifdef CONFIG_PPC_SPLPAR
-/*
- * Must be called before the cpu is added to the online map when
- * a cpu is being brought up at runtime.
- */
-static void snapshot_purr(void)
-{
-	struct cpu_purr_data *pme;
-	unsigned long flags;
-
-	if (!cpu_has_feature(CPU_FTR_PURR))
-		return;
-	local_irq_save(flags);
-	pme = &__get_cpu_var(cpu_purr_data);
-	pme->tb = mftb();
-	pme->purr = mfspr(SPRN_PURR);
-	pme->initialized = 1;
-	local_irq_restore(flags);
-}
-
-#endif /* CONFIG_PPC_SPLPAR */
-
 #else /* ! CONFIG_VIRT_CPU_ACCOUNTING */
 #define calc_cputime_factors()
-#define calculate_steal_time()		do { } while (0)
 #endif
 
-#if !(defined(CONFIG_VIRT_CPU_ACCOUNTING) && defined(CONFIG_PPC_SPLPAR))
-#define snapshot_purr()			do { } while (0)
-#endif
-
-/*
- * Called when a cpu comes up after the system has finished booting,
- * i.e. as a result of a hotplug cpu action.
- */
-void snapshot_timebase(void)
-{
-	__get_cpu_var(last_jiffy) = get_tb_or_rtc();
-	snapshot_purr();
-}
-
 void __delay(unsigned long loops)
 {
 	unsigned long start;
@@ -585,8 +573,6 @@ void timer_interrupt(struct pt_regs * regs)
 	old_regs = set_irq_regs(regs);
 	irq_enter();
 
-	calculate_steal_time();
-
 	if (test_perf_event_pending()) {
 		clear_perf_event_pending();
 		perf_event_do_pending();

commit b0d278b7d3ae9115939ddcea93f516308cc367e2
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Aug 10 20:38:23 2010 +0000

    powerpc/perf_event: Reduce latency of calling perf_event_do_pending
    
    Commit 0fe1ac48 ("powerpc/perf_event: Fix oops due to
    perf_event_do_pending call") moved the call to perf_event_do_pending
    in timer_interrupt() down so that it was after the irq_enter() call.
    Unfortunately this moved it after the code that checks whether it
    is time for the next decrementer clock event.  The result is that
    the call to perf_event_do_pending() won't happen until the next
    decrementer clock event is due.  This was pointed out by Milton
    Miller.
    
    This fixes it by moving the check for whether it's time for the
    next decrementer clock event down to the point where we're about
    to call the event handler, after we've called perf_event_do_pending.
    
    This has the side effect that on old pre-Core99 Powermacs where we
    use the ppc_n_lost_interrupts mechanism to replay interrupts, a
    replayed interrupt will incur a little more latency since it will
    now do the code from the irq_enter down to the irq_exit, that it
    used to skip.  However, these machines are now old and rare enough
    that this doesn't matter.  To make it clear that ppc_n_lost_interrupts
    is only used on Powermacs, and to speed up the code slightly on
    non-Powermac ppc32 machines, the code that tests ppc_n_lost_interrupts
    is now conditional on CONFIG_PMAC as well as CONFIG_PPC32.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Cc: stable@kernel.org
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index ce53dfa7130d..8533b3b83f5d 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -577,20 +577,11 @@ void timer_interrupt(struct pt_regs * regs)
 	 * some CPUs will continuue to take decrementer exceptions */
 	set_dec(DECREMENTER_MAX);
 
-#ifdef CONFIG_PPC32
+#if defined(CONFIG_PPC32) && defined(CONFIG_PMAC)
 	if (atomic_read(&ppc_n_lost_interrupts) != 0)
 		do_IRQ(regs);
 #endif
 
-	now = get_tb_or_rtc();
-	if (now < decrementer->next_tb) {
-		/* not time for this event yet */
-		now = decrementer->next_tb - now;
-		if (now <= DECREMENTER_MAX)
-			set_dec((int)now);
-		trace_timer_interrupt_exit(regs);
-		return;
-	}
 	old_regs = set_irq_regs(regs);
 	irq_enter();
 
@@ -606,8 +597,16 @@ void timer_interrupt(struct pt_regs * regs)
 		get_lppaca()->int_dword.fields.decr_int = 0;
 #endif
 
-	if (evt->event_handler)
-		evt->event_handler(evt);
+	now = get_tb_or_rtc();
+	if (now >= decrementer->next_tb) {
+		decrementer->next_tb = ~(u64)0;
+		if (evt->event_handler)
+			evt->event_handler(evt);
+	} else {
+		now = decrementer->next_tb - now;
+		if (now <= DECREMENTER_MAX)
+			set_dec((int)now);
+	}
 
 #ifdef CONFIG_PPC_ISERIES
 	if (firmware_has_feature(FW_FEATURE_ISERIES) && hvlpevent_is_pending())

commit 47916be4e28c3d6fdb97dd8fb887d1d9b3145b9d
Merge: 852db46d55e8 d75d68cfef49
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 28 21:49:22 2010 +0200

    Merge branch 'powerpc.cherry-picks' into timers/clocksource
    
    Conflicts:
            arch/powerpc/kernel/time.c
    
    Reason: The powerpc next tree contains two commits which conflict with
    the timekeeping changes:
    
    8fd63a9e powerpc: Rework VDSO gettimeofday to prevent time going backwards
    c1aa687d powerpc: Clean up obsolete code relating to decrementer and timebase
    
    John Stultz identified them and provided the conflict resolution.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit d75d68cfef4936ddf38d2694ae2f7d1f7c45db05
Author: Paul Mackerras <paulus@samba.org>
Date:   Sun Jun 20 19:04:14 2010 +0000

    powerpc: Clean up obsolete code relating to decrementer and timebase
    
    Since the decrementer and timekeeping code was moved over to using
    the generic clockevents and timekeeping infrastructure, several
    variables and functions have been obsolete and effectively unused.
    This deletes them.
    
    In particular, wakeup_decrementer() is no longer needed since the
    generic code reprograms the decrementer as part of the process of
    resuming the timekeeping code, which happens during sysdev resume.
    Thus the wakeup_decrementer calls in the suspend_enter methods for
    52xx platforms have been removed.  The call in the powermac cpu
    frequency change code has been replaced by set_dec(1), which will
    cause a timer interrupt as soon as interrupts are enabled, and the
    generic code will then reprogram the decrementer with the correct
    value.
    
    This also simplifies the generic_suspend_en/disable_irqs functions
    and makes them static since they are not referenced outside time.c.
    The preempt_enable/disable calls are removed because the generic
    code has disabled all but the boot cpu at the point where these
    functions are called, so we can't be moved to another cpu.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 5adebaf47f13..ccb8759c8532 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -149,16 +149,6 @@ unsigned long tb_ticks_per_usec = 100; /* sane default */
 EXPORT_SYMBOL(tb_ticks_per_usec);
 unsigned long tb_ticks_per_sec;
 EXPORT_SYMBOL(tb_ticks_per_sec);	/* for cputime_t conversions */
-u64 tb_to_xs;
-unsigned tb_to_us;
-
-#define TICKLEN_SCALE	NTP_SCALE_SHIFT
-static u64 last_tick_len;	/* units are ns / 2^TICKLEN_SCALE */
-static u64 ticklen_to_xs;	/* 0.64 fraction */
-
-/* If last_tick_len corresponds to about 1/HZ seconds, then
-   last_tick_len << TICKLEN_SHIFT will be about 2^63. */
-#define TICKLEN_SHIFT	(63 - 30 - TICKLEN_SCALE + SHIFT_HZ)
 
 DEFINE_SPINLOCK(rtc_lock);
 EXPORT_SYMBOL_GPL(rtc_lock);
@@ -174,7 +164,6 @@ unsigned long ppc_proc_freq;
 EXPORT_SYMBOL(ppc_proc_freq);
 unsigned long ppc_tb_freq;
 
-static u64 tb_last_jiffy __cacheline_aligned_in_smp;
 static DEFINE_PER_CPU(u64, last_jiffy);
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING
@@ -446,7 +435,6 @@ EXPORT_SYMBOL(profile_pc);
 
 static int __init iSeries_tb_recal(void)
 {
-	struct div_result divres;
 	unsigned long titan, tb;
 
 	/* Make sure we only run on iSeries */
@@ -477,10 +465,7 @@ static int __init iSeries_tb_recal(void)
 				tb_ticks_per_jiffy = new_tb_ticks_per_jiffy;
 				tb_ticks_per_sec   = new_tb_ticks_per_sec;
 				calc_cputime_factors();
-				div128_by_32( XSEC_PER_SEC, 0, tb_ticks_per_sec, &divres );
-				tb_to_xs = divres.result_low;
 				vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;
-				vdso_data->tb_to_xs = tb_to_xs;
 				setup_cputime_one_jiffy();
 			}
 			else {
@@ -643,27 +628,9 @@ void timer_interrupt(struct pt_regs * regs)
 	trace_timer_interrupt_exit(regs);
 }
 
-void wakeup_decrementer(void)
-{
-	unsigned long ticks;
-
-	/*
-	 * The timebase gets saved on sleep and restored on wakeup,
-	 * so all we need to do is to reset the decrementer.
-	 */
-	ticks = tb_ticks_since(__get_cpu_var(last_jiffy));
-	if (ticks < tb_ticks_per_jiffy)
-		ticks = tb_ticks_per_jiffy - ticks;
-	else
-		ticks = 1;
-	set_dec(ticks);
-}
-
 #ifdef CONFIG_SUSPEND
-void generic_suspend_disable_irqs(void)
+static void generic_suspend_disable_irqs(void)
 {
-	preempt_disable();
-
 	/* Disable the decrementer, so that it doesn't interfere
 	 * with suspending.
 	 */
@@ -673,12 +640,9 @@ void generic_suspend_disable_irqs(void)
 	set_dec(0x7fffffff);
 }
 
-void generic_suspend_enable_irqs(void)
+static void generic_suspend_enable_irqs(void)
 {
-	wakeup_decrementer();
-
 	local_irq_enable();
-	preempt_enable();
 }
 
 /* Overrides the weak version in kernel/power/main.c */
@@ -698,23 +662,6 @@ void arch_suspend_enable_irqs(void)
 }
 #endif
 
-#ifdef CONFIG_SMP
-void __init smp_space_timers(unsigned int max_cpus)
-{
-	int i;
-	u64 previous_tb = per_cpu(last_jiffy, boot_cpuid);
-
-	/* make sure tb > per_cpu(last_jiffy, cpu) for all cpus always */
-	previous_tb -= tb_ticks_per_jiffy;
-
-	for_each_possible_cpu(i) {
-		if (i == boot_cpuid)
-			continue;
-		per_cpu(last_jiffy, i) = previous_tb;
-	}
-}
-#endif
-
 /*
  * Scheduler clock - returns current time in nanosec units.
  *
@@ -1014,15 +961,13 @@ void secondary_cpu_time_init(void)
 /* This function is only called on the boot processor */
 void __init time_init(void)
 {
-	unsigned long flags;
 	struct div_result res;
-	u64 scale, x;
+	u64 scale;
 	unsigned shift;
 
 	if (__USE_RTC()) {
 		/* 601 processor: dec counts down by 128 every 128ns */
 		ppc_tb_freq = 1000000000;
-		tb_last_jiffy = get_rtcl();
 	} else {
 		/* Normal PowerPC with timebase register */
 		ppc_md.calibrate_decr();
@@ -1030,49 +975,14 @@ void __init time_init(void)
 		       ppc_tb_freq / 1000000, ppc_tb_freq % 1000000);
 		printk(KERN_DEBUG "time_init: processor frequency   = %lu.%.6lu MHz\n",
 		       ppc_proc_freq / 1000000, ppc_proc_freq % 1000000);
-		tb_last_jiffy = get_tb();
 	}
 
 	tb_ticks_per_jiffy = ppc_tb_freq / HZ;
 	tb_ticks_per_sec = ppc_tb_freq;
 	tb_ticks_per_usec = ppc_tb_freq / 1000000;
-	tb_to_us = mulhwu_scale_factor(ppc_tb_freq, 1000000);
 	calc_cputime_factors();
 	setup_cputime_one_jiffy();
 
-	/*
-	 * Calculate the length of each tick in ns.  It will not be
-	 * exactly 1e9/HZ unless ppc_tb_freq is divisible by HZ.
-	 * We compute 1e9 * tb_ticks_per_jiffy / ppc_tb_freq,
-	 * rounded up.
-	 */
-	x = (u64) NSEC_PER_SEC * tb_ticks_per_jiffy + ppc_tb_freq - 1;
-	do_div(x, ppc_tb_freq);
-	tick_nsec = x;
-	last_tick_len = x << TICKLEN_SCALE;
-
-	/*
-	 * Compute ticklen_to_xs, which is a factor which gets multiplied
-	 * by (last_tick_len << TICKLEN_SHIFT) to get a tb_to_xs value.
-	 * It is computed as:
-	 * ticklen_to_xs = 2^N / (tb_ticks_per_jiffy * 1e9)
-	 * where N = 64 + 20 - TICKLEN_SCALE - TICKLEN_SHIFT
-	 * which turns out to be N = 51 - SHIFT_HZ.
-	 * This gives the result as a 0.64 fixed-point fraction.
-	 * That value is reduced by an offset amounting to 1 xsec per
-	 * 2^31 timebase ticks to avoid problems with time going backwards
-	 * by 1 xsec when we do timer_recalc_offset due to losing the
-	 * fractional xsec.  That offset is equal to ppc_tb_freq/2^51
-	 * since there are 2^20 xsec in a second.
-	 */
-	div128_by_32((1ULL << 51) - ppc_tb_freq, 0,
-		     tb_ticks_per_jiffy << SHIFT_HZ, &res);
-	div128_by_32(res.result_high, res.result_low, NSEC_PER_SEC, &res);
-	ticklen_to_xs = res.result_low;
-
-	/* Compute tb_to_xs from tick_nsec */
-	tb_to_xs = mulhdu(last_tick_len << TICKLEN_SHIFT, ticklen_to_xs);
-
 	/*
 	 * Compute scale factor for sched_clock.
 	 * The calibrate_decr() function has set tb_ticks_per_sec,
@@ -1094,21 +1004,14 @@ void __init time_init(void)
 	/* Save the current timebase to pretty up CONFIG_PRINTK_TIME */
 	boot_tb = get_tb_or_rtc();
 
-	write_seqlock_irqsave(&xtime_lock, flags);
-
 	/* If platform provided a timezone (pmac), we correct the time */
         if (timezone_offset) {
 		sys_tz.tz_minuteswest = -timezone_offset / 60;
 		sys_tz.tz_dsttime = 0;
         }
 
-	vdso_data->tb_orig_stamp = tb_last_jiffy;
 	vdso_data->tb_update_count = 0;
 	vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;
-	vdso_data->stamp_xsec = (u64) xtime.tv_sec * XSEC_PER_SEC;
-	vdso_data->tb_to_xs = tb_to_xs;
-
-	write_sequnlock_irqrestore(&xtime_lock, flags);
 
 	/* Start the decrementer on CPUs that have manual control
 	 * such as BookE
@@ -1202,39 +1105,6 @@ void to_tm(int tim, struct rtc_time * tm)
 	GregorianDay(tm);
 }
 
-/* Auxiliary function to compute scaling factors */
-/* Actually the choice of a timebase running at 1/4 the of the bus
- * frequency giving resolution of a few tens of nanoseconds is quite nice.
- * It makes this computation very precise (27-28 bits typically) which
- * is optimistic considering the stability of most processor clock
- * oscillators and the precision with which the timebase frequency
- * is measured but does not harm.
- */
-unsigned mulhwu_scale_factor(unsigned inscale, unsigned outscale)
-{
-        unsigned mlt=0, tmp, err;
-        /* No concern for performance, it's done once: use a stupid
-         * but safe and compact method to find the multiplier.
-         */
-  
-        for (tmp = 1U<<31; tmp != 0; tmp >>= 1) {
-                if (mulhwu(inscale, mlt|tmp) < outscale)
-			mlt |= tmp;
-        }
-  
-        /* We might still be off by 1 for the best approximation.
-         * A side effect of this is that if outscale is too large
-         * the returned value will be zero.
-         * Many corner cases have been checked and seem to work,
-         * some might have been forgotten in the test however.
-         */
-  
-        err = inscale * (mlt+1);
-        if (err <= inscale/2)
-		mlt++;
-        return mlt;
-}
-
 /*
  * Divide a 128-bit dividend by a 32-bit divisor, leaving a 128 bit
  * result.

commit 0e469db8f70c2645acdc90981c0480a3e19d5e68
Author: Paul Mackerras <paulus@samba.org>
Date:   Sun Jun 20 19:03:08 2010 +0000

    powerpc: Rework VDSO gettimeofday to prevent time going backwards
    
    Currently it is possible for userspace to see the result of
    gettimeofday() going backwards by 1 microsecond, assuming that
    userspace is using the gettimeofday() in the VDSO.  The VDSO
    gettimeofday() algorithm computes the time in "xsecs", which are
    units of 2^-20 seconds, or approximately 0.954 microseconds,
    using the algorithm
    
            now = (timebase - tb_orig_stamp) * tb_to_xs + stamp_xsec
    
    and then converts the time in xsecs to seconds and microseconds.
    
    The kernel updates the tb_orig_stamp and stamp_xsec values every
    tick in update_vsyscall().  If the length of the tick is not an
    integer number of xsecs, then some precision is lost in converting
    the current time to xsecs.  For example, with CONFIG_HZ=1000, the
    tick is 1ms long, which is 1048.576 xsecs.  That means that
    stamp_xsec will advance by either 1048 or 1049 on each tick.
    With the right conditions, it is possible for userspace to get
    (timebase - tb_orig_stamp) * tb_to_xs being 1049 if the kernel is
    slightly late in updating the vdso_datapage, and then for stamp_xsec
    to advance by 1048 when the kernel does update it, and for userspace
    to then see (timebase - tb_orig_stamp) * tb_to_xs being zero due to
    integer truncation.  The result is that time appears to go backwards
    by 1 microsecond.
    
    To fix this we change the VDSO gettimeofday to use a new field in the
    VDSO datapage which stores the nanoseconds part of the time as a
    fractional number of seconds in a 0.32 binary fraction format.
    (Or put another way, as a 32-bit number in units of 0.23283 ns.)
    This is convenient because we can use the mulhwu instruction to
    convert it to either microseconds or nanoseconds.
    
    Since it turns out that computing the time of day using this new field
    is simpler than either using stamp_xsec (as gettimeofday does) or
    stamp_xtime.tv_nsec (as clock_gettime does), this converts both
    gettimeofday and clock_gettime to use the new field.  The existing
    __do_get_tspec function is converted to use the new field and take
    a parameter in r7 that indicates the desired resolution, 1,000,000
    for microseconds or 1,000,000,000 for nanoseconds.  The __do_get_xsec
    function is then unused and is deleted.
    
    The new algorithm is
    
            now = ((timebase - tb_orig_stamp) << 12) * tb_to_xs
                    + (stamp_xtime_seconds << 32) + stamp_sec_fraction
    
    with 'now' in units of 2^-32 seconds.  That is then converted to
    seconds and either microseconds or nanoseconds with
    
            seconds = now >> 32
            partseconds = ((now & 0xffffffff) * resolution) >> 32
    
    The 32-bit VDSO code also makes a further simplification: it ignores
    the bottom 32 bits of the tb_to_xs value, which is a 0.64 format binary
    fraction.  Doing so gets rid of 4 multiply instructions.  Assuming
    a timebase frequency of 1GHz or less and an update interval of no
    more than 10ms, the upper 32 bits of tb_to_xs will be at least
    4503599, so the error from ignoring the low 32 bits will be at most
    2.2ns, which is more than an order of magnitude less than the time
    taken to do gettimeofday or clock_gettime on our fastest processors,
    so there is no possibility of seeing inconsistent values due to this.
    
    This also moves update_gtod() down next to its only caller, and makes
    update_vsyscall use the time passed in via the wall_time argument rather
    than accessing xtime directly.  At present, wall_time always points to
    xtime, but that could change in future.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 0441bbdadbd1..5adebaf47f13 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -423,30 +423,6 @@ void udelay(unsigned long usecs)
 }
 EXPORT_SYMBOL(udelay);
 
-static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
-			       u64 new_tb_to_xs)
-{
-	/*
-	 * tb_update_count is used to allow the userspace gettimeofday code
-	 * to assure itself that it sees a consistent view of the tb_to_xs and
-	 * stamp_xsec variables.  It reads the tb_update_count, then reads
-	 * tb_to_xs and stamp_xsec and then reads tb_update_count again.  If
-	 * the two values of tb_update_count match and are even then the
-	 * tb_to_xs and stamp_xsec values are consistent.  If not, then it
-	 * loops back and reads them again until this criteria is met.
-	 * We expect the caller to have done the first increment of
-	 * vdso_data->tb_update_count already.
-	 */
-	vdso_data->tb_orig_stamp = new_tb_stamp;
-	vdso_data->stamp_xsec = new_stamp_xsec;
-	vdso_data->tb_to_xs = new_tb_to_xs;
-	vdso_data->wtom_clock_sec = wall_to_monotonic.tv_sec;
-	vdso_data->wtom_clock_nsec = wall_to_monotonic.tv_nsec;
-	vdso_data->stamp_xtime = xtime;
-	smp_wmb();
-	++(vdso_data->tb_update_count);
-}
-
 #ifdef CONFIG_SMP
 unsigned long profile_pc(struct pt_regs *regs)
 {
@@ -873,10 +849,37 @@ static cycle_t timebase_read(struct clocksource *cs)
 	return (cycle_t)get_tb();
 }
 
+static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
+			       u64 new_tb_to_xs, struct timespec *now,
+			       u32 frac_sec)
+{
+	/*
+	 * tb_update_count is used to allow the userspace gettimeofday code
+	 * to assure itself that it sees a consistent view of the tb_to_xs and
+	 * stamp_xsec variables.  It reads the tb_update_count, then reads
+	 * tb_to_xs and stamp_xsec and then reads tb_update_count again.  If
+	 * the two values of tb_update_count match and are even then the
+	 * tb_to_xs and stamp_xsec values are consistent.  If not, then it
+	 * loops back and reads them again until this criteria is met.
+	 * We expect the caller to have done the first increment of
+	 * vdso_data->tb_update_count already.
+	 */
+	vdso_data->tb_orig_stamp = new_tb_stamp;
+	vdso_data->stamp_xsec = new_stamp_xsec;
+	vdso_data->tb_to_xs = new_tb_to_xs;
+	vdso_data->wtom_clock_sec = wall_to_monotonic.tv_sec;
+	vdso_data->wtom_clock_nsec = wall_to_monotonic.tv_nsec;
+	vdso_data->stamp_xtime = *now;
+	vdso_data->stamp_sec_fraction = frac_sec;
+	smp_wmb();
+	++(vdso_data->tb_update_count);
+}
+
 void update_vsyscall(struct timespec *wall_time, struct clocksource *clock,
 		     u32 mult)
 {
 	u64 t2x, stamp_xsec;
+	u32 frac_sec;
 
 	if (clock != &clocksource_timebase)
 		return;
@@ -888,10 +891,14 @@ void update_vsyscall(struct timespec *wall_time, struct clocksource *clock,
 	/* XXX this assumes clock->shift == 22 */
 	/* 4611686018 ~= 2^(20+64-22) / 1e9 */
 	t2x = (u64) mult * 4611686018ULL;
-	stamp_xsec = (u64) xtime.tv_nsec * XSEC_PER_SEC;
+	stamp_xsec = (u64) wall_time->tv_nsec * XSEC_PER_SEC;
 	do_div(stamp_xsec, 1000000000);
-	stamp_xsec += (u64) xtime.tv_sec * XSEC_PER_SEC;
-	update_gtod(clock->cycle_last, stamp_xsec, t2x);
+	stamp_xsec += (u64) wall_time->tv_sec * XSEC_PER_SEC;
+
+	BUG_ON(wall_time->tv_nsec >= NSEC_PER_SEC);
+	/* this is tv_nsec / 1e9 as a 0.32 fraction */
+	frac_sec = ((u64) wall_time->tv_nsec * 18446744073ULL) >> 32;
+	update_gtod(clock->cycle_last, stamp_xsec, t2x, wall_time, frac_sec);
 }
 
 void update_vsyscall_tz(void)

commit 7615856ebfee52b080c22d263ca4debbd0df0ac1
Author: John Stultz <johnstul@us.ibm.com>
Date:   Tue Jul 13 17:56:23 2010 -0700

    timkeeping: Fix update_vsyscall to provide wall_to_monotonic offset
    
    update_vsyscall() did not provide the wall_to_monotoinc offset,
    so arch specific implementations tend to reference wall_to_monotonic
    directly. This limits future cleanups in the timekeeping core, so
    this patch fixes the update_vsyscall interface to provide
    wall_to_monotonic, allowing wall_to_monotonic to be made static
    as planned in Documentation/feature-removal-schedule.txt
    
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Tony Luck <tony.luck@intel.com>
    LKML-Reference: <1279068988-21864-7-git-send-email-johnstul@us.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 0711d60f40b0..e215f76bba1c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -849,8 +849,8 @@ static cycle_t timebase_read(struct clocksource *cs)
 	return (cycle_t)get_tb();
 }
 
-void update_vsyscall(struct timespec *wall_time, struct clocksource *clock,
-		     u32 mult)
+void update_vsyscall(struct timespec *wall_time, struct timespec *wtm,
+			struct clocksource *clock, u32 mult)
 {
 	u64 new_tb_to_xs, new_stamp_xsec;
 
@@ -882,8 +882,8 @@ void update_vsyscall(struct timespec *wall_time, struct clocksource *clock,
 	vdso_data->tb_orig_stamp = clock->cycle_last;
 	vdso_data->stamp_xsec = new_stamp_xsec;
 	vdso_data->tb_to_xs = new_tb_to_xs;
-	vdso_data->wtom_clock_sec = wall_to_monotonic.tv_sec;
-	vdso_data->wtom_clock_nsec = wall_to_monotonic.tv_nsec;
+	vdso_data->wtom_clock_sec = wtm->tv_sec;
+	vdso_data->wtom_clock_nsec = wtm->tv_nsec;
 	vdso_data->stamp_xtime = *wall_time;
 	smp_wmb();
 	++(vdso_data->tb_update_count);

commit 06d518e3dfb25334282c7e38b4d7a4eada215f6d
Author: John Stultz <johnstul@us.ibm.com>
Date:   Tue Jul 13 17:56:22 2010 -0700

    powerpc: Cleanup xtime usage
    
    This removes powerpc's direct xtime usage, allowing for further
    generic timeekeping cleanups
    
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Anton Blanchard <anton@samba.org>
    LKML-Reference: <1279068988-21864-6-git-send-email-johnstul@us.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 6fcd64886d1b..0711d60f40b0 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -864,9 +864,9 @@ void update_vsyscall(struct timespec *wall_time, struct clocksource *clock,
 	/* XXX this assumes clock->shift == 22 */
 	/* 4611686018 ~= 2^(20+64-22) / 1e9 */
 	new_tb_to_xs = (u64) mult * 4611686018ULL;
-	new_stamp_xsec = (u64) xtime.tv_nsec * XSEC_PER_SEC;
+	new_stamp_xsec = (u64) wall_time->tv_nsec * XSEC_PER_SEC;
 	do_div(new_stamp_xsec, 1000000000);
-	new_stamp_xsec += (u64) xtime.tv_sec * XSEC_PER_SEC;
+	new_stamp_xsec += (u64) wall_time->tv_sec * XSEC_PER_SEC;
 
 	/*
 	 * tb_update_count is used to allow the userspace gettimeofday code
@@ -884,7 +884,7 @@ void update_vsyscall(struct timespec *wall_time, struct clocksource *clock,
 	vdso_data->tb_to_xs = new_tb_to_xs;
 	vdso_data->wtom_clock_sec = wall_to_monotonic.tv_sec;
 	vdso_data->wtom_clock_nsec = wall_to_monotonic.tv_nsec;
-	vdso_data->stamp_xtime = xtime;
+	vdso_data->stamp_xtime = *wall_time;
 	smp_wmb();
 	++(vdso_data->tb_update_count);
 }
@@ -1093,7 +1093,7 @@ void __init time_init(void)
 	vdso_data->tb_orig_stamp = tb_last_jiffy;
 	vdso_data->tb_update_count = 0;
 	vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;
-	vdso_data->stamp_xsec = (u64) xtime.tv_sec * XSEC_PER_SEC;
+	vdso_data->stamp_xsec = (u64) get_seconds() * XSEC_PER_SEC;
 	vdso_data->tb_to_xs = tb_to_xs;
 
 	write_sequnlock_irqrestore(&xtime_lock, flags);

commit b0797b60d0067fe437baa97a743c7d9de98fd769
Author: John Stultz <johnstul@us.ibm.com>
Date:   Tue Jul 13 17:56:21 2010 -0700

    powerpc: Simplify update_vsyscall
    
    Currently powerpc's update_vsyscall calls an inline update_gtod.
    However, both are straightforward, and there are no other users,
    so this patch merges update_gtod into update_vsyscall.
    
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1279068988-21864-5-git-send-email-johnstul@us.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 0441bbdadbd1..6fcd64886d1b 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -423,30 +423,6 @@ void udelay(unsigned long usecs)
 }
 EXPORT_SYMBOL(udelay);
 
-static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
-			       u64 new_tb_to_xs)
-{
-	/*
-	 * tb_update_count is used to allow the userspace gettimeofday code
-	 * to assure itself that it sees a consistent view of the tb_to_xs and
-	 * stamp_xsec variables.  It reads the tb_update_count, then reads
-	 * tb_to_xs and stamp_xsec and then reads tb_update_count again.  If
-	 * the two values of tb_update_count match and are even then the
-	 * tb_to_xs and stamp_xsec values are consistent.  If not, then it
-	 * loops back and reads them again until this criteria is met.
-	 * We expect the caller to have done the first increment of
-	 * vdso_data->tb_update_count already.
-	 */
-	vdso_data->tb_orig_stamp = new_tb_stamp;
-	vdso_data->stamp_xsec = new_stamp_xsec;
-	vdso_data->tb_to_xs = new_tb_to_xs;
-	vdso_data->wtom_clock_sec = wall_to_monotonic.tv_sec;
-	vdso_data->wtom_clock_nsec = wall_to_monotonic.tv_nsec;
-	vdso_data->stamp_xtime = xtime;
-	smp_wmb();
-	++(vdso_data->tb_update_count);
-}
-
 #ifdef CONFIG_SMP
 unsigned long profile_pc(struct pt_regs *regs)
 {
@@ -876,7 +852,7 @@ static cycle_t timebase_read(struct clocksource *cs)
 void update_vsyscall(struct timespec *wall_time, struct clocksource *clock,
 		     u32 mult)
 {
-	u64 t2x, stamp_xsec;
+	u64 new_tb_to_xs, new_stamp_xsec;
 
 	if (clock != &clocksource_timebase)
 		return;
@@ -887,11 +863,30 @@ void update_vsyscall(struct timespec *wall_time, struct clocksource *clock,
 
 	/* XXX this assumes clock->shift == 22 */
 	/* 4611686018 ~= 2^(20+64-22) / 1e9 */
-	t2x = (u64) mult * 4611686018ULL;
-	stamp_xsec = (u64) xtime.tv_nsec * XSEC_PER_SEC;
-	do_div(stamp_xsec, 1000000000);
-	stamp_xsec += (u64) xtime.tv_sec * XSEC_PER_SEC;
-	update_gtod(clock->cycle_last, stamp_xsec, t2x);
+	new_tb_to_xs = (u64) mult * 4611686018ULL;
+	new_stamp_xsec = (u64) xtime.tv_nsec * XSEC_PER_SEC;
+	do_div(new_stamp_xsec, 1000000000);
+	new_stamp_xsec += (u64) xtime.tv_sec * XSEC_PER_SEC;
+
+	/*
+	 * tb_update_count is used to allow the userspace gettimeofday code
+	 * to assure itself that it sees a consistent view of the tb_to_xs and
+	 * stamp_xsec variables.  It reads the tb_update_count, then reads
+	 * tb_to_xs and stamp_xsec and then reads tb_update_count again.  If
+	 * the two values of tb_update_count match and are even then the
+	 * tb_to_xs and stamp_xsec values are consistent.  If not, then it
+	 * loops back and reads them again until this criteria is met.
+	 * We expect the caller to have done the first increment of
+	 * vdso_data->tb_update_count already.
+	 */
+	vdso_data->tb_orig_stamp = clock->cycle_last;
+	vdso_data->stamp_xsec = new_stamp_xsec;
+	vdso_data->tb_to_xs = new_tb_to_xs;
+	vdso_data->wtom_clock_sec = wall_to_monotonic.tv_sec;
+	vdso_data->wtom_clock_nsec = wall_to_monotonic.tv_nsec;
+	vdso_data->stamp_xtime = xtime;
+	smp_wmb();
+	++(vdso_data->tb_update_count);
 }
 
 void update_vsyscall_tz(void)

commit 0fe1ac48bef018bed896307cd12f6ca9b5e704ab
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Apr 13 20:46:04 2010 +0000

    powerpc/perf_event: Fix oops due to perf_event_do_pending call
    
    Anton Blanchard found that large POWER systems would occasionally
    crash in the exception exit path when profiling with perf_events.
    The symptom was that an interrupt would occur late in the exit path
    when the MSR[RI] (recoverable interrupt) bit was clear.  Interrupts
    should be hard-disabled at this point but they were enabled.  Because
    the interrupt was not recoverable the system panicked.
    
    The reason is that the exception exit path was calling
    perf_event_do_pending after hard-disabling interrupts, and
    perf_event_do_pending will re-enable interrupts.
    
    The simplest and cleanest fix for this is to use the same mechanism
    that 32-bit powerpc does, namely to cause a self-IPI by setting the
    decrementer to 1.  This means we can remove the tests in the exception
    exit path and raw_local_irq_restore.
    
    This also makes sure that the call to perf_event_do_pending from
    timer_interrupt() happens within irq_enter/irq_exit.  (Note that
    calling perf_event_do_pending from timer_interrupt does not mean that
    there is a possible 1/HZ latency; setting the decrementer to 1 ensures
    that the timer interrupt will happen immediately, i.e. within one
    timebase tick, which is a few nanoseconds or 10s of nanoseconds.)
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Cc: stable@kernel.org
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 1b16b9a3e49a..0441bbdadbd1 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -532,25 +532,60 @@ void __init iSeries_time_init_early(void)
 }
 #endif /* CONFIG_PPC_ISERIES */
 
-#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_PPC32)
-DEFINE_PER_CPU(u8, perf_event_pending);
+#ifdef CONFIG_PERF_EVENTS
 
-void set_perf_event_pending(void)
+/*
+ * 64-bit uses a byte in the PACA, 32-bit uses a per-cpu variable...
+ */
+#ifdef CONFIG_PPC64
+static inline unsigned long test_perf_event_pending(void)
 {
-	get_cpu_var(perf_event_pending) = 1;
-	set_dec(1);
-	put_cpu_var(perf_event_pending);
+	unsigned long x;
+
+	asm volatile("lbz %0,%1(13)"
+		: "=r" (x)
+		: "i" (offsetof(struct paca_struct, perf_event_pending)));
+	return x;
 }
 
+static inline void set_perf_event_pending_flag(void)
+{
+	asm volatile("stb %0,%1(13)" : :
+		"r" (1),
+		"i" (offsetof(struct paca_struct, perf_event_pending)));
+}
+
+static inline void clear_perf_event_pending(void)
+{
+	asm volatile("stb %0,%1(13)" : :
+		"r" (0),
+		"i" (offsetof(struct paca_struct, perf_event_pending)));
+}
+
+#else /* 32-bit */
+
+DEFINE_PER_CPU(u8, perf_event_pending);
+
+#define set_perf_event_pending_flag()	__get_cpu_var(perf_event_pending) = 1
 #define test_perf_event_pending()	__get_cpu_var(perf_event_pending)
 #define clear_perf_event_pending()	__get_cpu_var(perf_event_pending) = 0
 
-#else  /* CONFIG_PERF_EVENTS && CONFIG_PPC32 */
+#endif /* 32 vs 64 bit */
+
+void set_perf_event_pending(void)
+{
+	preempt_disable();
+	set_perf_event_pending_flag();
+	set_dec(1);
+	preempt_enable();
+}
+
+#else  /* CONFIG_PERF_EVENTS */
 
 #define test_perf_event_pending()	0
 #define clear_perf_event_pending()
 
-#endif /* CONFIG_PERF_EVENTS && CONFIG_PPC32 */
+#endif /* CONFIG_PERF_EVENTS */
 
 /*
  * For iSeries shared processors, we have to let the hypervisor
@@ -582,10 +617,6 @@ void timer_interrupt(struct pt_regs * regs)
 	set_dec(DECREMENTER_MAX);
 
 #ifdef CONFIG_PPC32
-	if (test_perf_event_pending()) {
-		clear_perf_event_pending();
-		perf_event_do_pending();
-	}
 	if (atomic_read(&ppc_n_lost_interrupts) != 0)
 		do_IRQ(regs);
 #endif
@@ -604,6 +635,11 @@ void timer_interrupt(struct pt_regs * regs)
 
 	calculate_steal_time();
 
+	if (test_perf_event_pending()) {
+		clear_perf_event_pending();
+		perf_event_do_pending();
+	}
+
 #ifdef CONFIG_PPC_ISERIES
 	if (firmware_has_feature(FW_FEATURE_ISERIES))
 		get_lppaca()->int_dword.fields.decr_int = 0;

commit 89713ed10815401a1bfe12e3a076b64048381b56
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Jan 31 20:34:06 2010 +0000

    powerpc: Add timer, performance monitor and machine check counts to /proc/interrupts
    
    With NO_HZ it is useful to know how often the decrementer is going off. The
    patch below adds an entry for it and also adds it into the /proc/stat
    summaries.
    
    While here, I added performance monitoring and machine check exceptions.
    I found it useful to keep an eye on the PMU exception rate
    when using the perf tool. Since it's possible to take a completely
    handled machine check on a System p box it also sounds like a good idea to
    keep a machine check summary.
    
    The event naming matches x86 to keep gratuitous differences to a minimum.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index ed1c0f58344a..1b16b9a3e49a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -575,6 +575,8 @@ void timer_interrupt(struct pt_regs * regs)
 
 	trace_timer_interrupt_entry(regs);
 
+	__get_cpu_var(irq_stat).timer_irqs++;
+
 	/* Ensure a positive value is written to the decrementer, or else
 	 * some CPUs will continuue to take decrementer exceptions */
 	set_dec(DECREMENTER_MAX);

commit b919ee827e048826786fd7e889a2b04f63382fe6
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Feb 7 19:26:29 2010 +0000

    powerpc: Only print clockevent settings once
    
    The clockevent multiplier and shift is useful information, but we
    only need to print it once.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 0a5b1081a50e..ed1c0f58344a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -935,8 +935,8 @@ static void register_decrementer_clockevent(int cpu)
 	*dec = decrementer_clockevent;
 	dec->cpumask = cpumask_of(cpu);
 
-	printk(KERN_DEBUG "clockevent: %s mult[%x] shift[%d] cpu[%d]\n",
-	       dec->name, dec->mult, dec->shift, cpu);
+	printk_once(KERN_DEBUG "clockevent: %s mult[%x] shift[%d] cpu[%d]\n",
+		    dec->name, dec->mult, dec->shift, cpu);
 
 	clockevents_register_device(dec);
 }

commit 61c03ddbdff2e080702521bc1b757854ca58c46a
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Jan 13 12:04:11 2010 +0000

    powerpc: Replace per_cpu(, smp_processor_id()) with __get_cpu_var()
    
    The cputime code has a few places that do per_cpu(, smp_processor_id()).
    Replace them with __get_cpu_var().
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 6c6093d67f30..0a5b1081a50e 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -265,8 +265,8 @@ void account_system_vtime(struct task_struct *tsk)
 		account_system_time(tsk, 0, delta, deltascaled);
 	else
 		account_idle_time(delta);
-	per_cpu(cputime_last_delta, smp_processor_id()) = delta;
-	per_cpu(cputime_scaled_last_delta, smp_processor_id()) = deltascaled;
+	__get_cpu_var(cputime_last_delta) = delta;
+	__get_cpu_var(cputime_scaled_last_delta) = deltascaled;
 	local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(account_system_vtime);

commit 3e7b484354c8f60c12119c1c5174ef354696c30d
Author: Stefan Roese <sr@denx.de>
Date:   Mon Jan 11 22:28:54 2010 +0000

    powerpc: Fix decrementer setup on 1GHz boards
    
    We noticed that recent kernels didn't boot on our 1GHz Canyonlands 460EX
    boards anymore. As it seems, patch 8d165db1 [powerpc: Improve
    decrementer accuracy] introduced this problem. The routine div_sc()
    overflows with shift = 32 resulting in this incorrect setup:
    
    time_init: decrementer frequency = 1000.000012 MHz
    time_init: processor frequency   = 1000.000012 MHz
    clocksource: timebase mult[400000] shift[22] registered
    clockevent: decrementer mult[33] shift[32] cpu[0]
    
    This patch now introduces a local div_dc64() version of this function
    so that this overflow doesn't happen anymore.
    
    Signed-off-by: Stefan Roese <sr@denx.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Detlev Zundel <dzu@denx.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 9ba2cc88591d..6c6093d67f30 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -903,12 +903,21 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 		decrementer_set_next_event(DECREMENTER_MAX, dev);
 }
 
+static inline uint64_t div_sc64(unsigned long ticks, unsigned long nsec,
+				int shift)
+{
+	uint64_t tmp = ((uint64_t)ticks) << shift;
+
+	do_div(tmp, nsec);
+	return tmp;
+}
+
 static void __init setup_clockevent_multiplier(unsigned long hz)
 {
 	u64 mult, shift = 32;
 
 	while (1) {
-		mult = div_sc(hz, NSEC_PER_SEC, shift);
+		mult = div_sc64(hz, NSEC_PER_SEC, shift);
 		if (mult && (mult >> 32UL) == 0UL)
 			break;
 

commit 09cea96caa59fabab3030c53bd698b9b568d959a
Merge: 6eb7365db6f3 e090aa80321b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 12 14:27:24 2009 -0800

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (151 commits)
      powerpc: Fix usage of 64-bit instruction in 32-bit altivec code
      MAINTAINERS: Add PowerPC patterns
      powerpc/pseries: Track previous CPPR values to correctly EOI interrupts
      powerpc/pseries: Correct pseries/dlpar.c build break without CONFIG_SMP
      powerpc: Make "intspec" pointers in irq_host->xlate() const
      powerpc/8xx: DTLB Miss cleanup
      powerpc/8xx: Remove DIRTY pte handling in DTLB Error.
      powerpc/8xx: Start using dcbX instructions in various copy routines
      powerpc/8xx: Restore _PAGE_WRITETHRU
      powerpc/8xx: Add missing Guarded setting in DTLB Error.
      powerpc/8xx: Fixup DAR from buggy dcbX instructions.
      powerpc/8xx: Tag DAR with 0x00f0 to catch buggy instructions.
      powerpc/8xx: Update TLB asm so it behaves as linux mm expects.
      powerpc/8xx: Invalidate non present TLBs
      powerpc/pseries: Serialize cpu hotplug operations during deactivate Vs deallocate
      pseries/pseries: Add code to online/offline CPUs of a DLPAR node
      powerpc: stop_this_cpu: remove the cpu from the online map.
      powerpc/pseries: Add kernel based CPU DLPAR handling
      sysfs/cpu: Add probe/release files
      powerpc/pseries: Kernel DLPAR Infrastructure
      ...

commit bcd6acd51f3d4d1ada201e9bc5c40a31d6d80c71
Merge: 11c34c7deaee 3ff6a468b45b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Dec 9 17:14:38 2009 +1100

    Merge commit 'origin/master' into next
    
    Conflicts:
            include/linux/kvm.h

commit fbf07eac7bf21c262143194181bd97c5d18b8ceb
Merge: 60d8ce2cd6c2 8629ea2eaba8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 8 19:28:09 2009 -0800

    Merge branch 'timers-for-linus-urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-for-linus-urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      hrtimer: Fix /proc/timer_list regression
      itimers: Fix racy writes to cpu_itimer fields
      timekeeping: Fix clock_gettime vsyscall time warp

commit 60d8ce2cd6c283132928c11f3fd57ff4187287e0
Merge: 849e8dea099a feae3203d711
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 8 19:27:08 2009 -0800

    Merge branch 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      timers, init: Limit the number of per cpu calibration bootup messages
      posix-cpu-timers: optimize and document timer_create callback
      clockevents: Add missing include to pacify sparse
      x86: vmiclock: Fix printk format
      x86: Fix printk format due to variable type change
      sparc: fix printk for change of variable type
      clocksource/events: Fix fallout of generic code changes
      nohz: Allow 32-bit machines to sleep for more than 2.15 seconds
      nohz: Track last do_timer() cpu
      nohz: Prevent clocksource wrapping during idle
      nohz: Type cast printk argument
      mips: Use generic mult/shift factor calculation for clocks
      clocksource: Provide a generic mult/shift factor calculation
      clockevents: Use u32 for mult and shift factors
      nohz: Introduce arch_needs_cpu
      nohz: Reuse ktime in sub-functions of tick_check_idle.
      time: Remove xtime_cache
      time: Implement logarithmic time accumulation

commit 0696b711e4be45fa104c12329f617beb29c03f78
Author: Lin Ming <ming.m.lin@intel.com>
Date:   Tue Nov 17 13:49:50 2009 +0800

    timekeeping: Fix clock_gettime vsyscall time warp
    
    Since commit 0a544198 "timekeeping: Move NTP adjusted clock multiplier
    to struct timekeeper" the clock multiplier of vsyscall is updated with
    the unmodified clock multiplier of the clock source and not with the
    NTP adjusted multiplier of the timekeeper.
    
    This causes user space observerable time warps:
    new CLOCK-warp maximum: 120 nsecs,  00000025c337c537 -> 00000025c337c4bf
    
    Add a new argument "mult" to update_vsyscall() and hand in the
    timekeeping internal NTP adjusted multiplier.
    
    Signed-off-by: Lin Ming <ming.m.lin@intel.com>
    Cc: "Zhang Yanmin" <yanmin_zhang@linux.intel.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Tony Luck <tony.luck@intel.com>
    LKML-Reference: <1258436990.17765.83.camel@minggr.sh.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index a136a11c490d..39713312fbc7 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -828,7 +828,8 @@ static cycle_t timebase_read(struct clocksource *cs)
 	return (cycle_t)get_tb();
 }
 
-void update_vsyscall(struct timespec *wall_time, struct clocksource *clock)
+void update_vsyscall(struct timespec *wall_time, struct clocksource *clock,
+		     u32 mult)
 {
 	u64 t2x, stamp_xsec;
 
@@ -841,7 +842,7 @@ void update_vsyscall(struct timespec *wall_time, struct clocksource *clock)
 
 	/* XXX this assumes clock->shift == 22 */
 	/* 4611686018 ~= 2^(20+64-22) / 1e9 */
-	t2x = (u64) clock->mult * 4611686018ULL;
+	t2x = (u64) mult * 4611686018ULL;
 	stamp_xsec = (u64) xtime.tv_nsec * XSEC_PER_SEC;
 	do_div(stamp_xsec, 1000000000);
 	stamp_xsec += (u64) xtime.tv_sec * XSEC_PER_SEC;

commit 0ffa798d947f5f5e40690cc9d38e678080a34f87
Merge: 39dc78b65103 c86e2eaded39 c5659b74f052
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Nov 15 09:51:19 2009 +0100

    Merge branches 'perf/powerpc' and 'perf/bench' into perf/core
    
    Merge reason: Both 'perf bench' and the pending PowerPC changes
                  are now ready for the next merge window.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a362c638bdf052bf424bce7645d39b101090f6ba
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Nov 14 00:26:34 2009 +0100

    clocksource/events: Fix fallout of generic code changes
    
    powerpc grew a new warning due to the type change of clockevent->mult.
    
    The architectures which use parts of the generic time keeping
    infrastructure tripped over my wrong assumption that
    clocksource_register is only used when GENERIC_TIME=y.
    
    I should have looked and also I should have known better. These
    renitent Gaul villages are racking my nerves. Some serious deprecating
    is due.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 92dc844299b6..60ceb2708948 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -905,7 +905,7 @@ static void register_decrementer_clockevent(int cpu)
 	*dec = decrementer_clockevent;
 	dec->cpumask = cpumask_of(cpu);
 
-	printk(KERN_DEBUG "clockevent: %s mult[%lx] shift[%d] cpu[%d]\n",
+	printk(KERN_DEBUG "clockevent: %s mult[%x] shift[%d] cpu[%d]\n",
 	       dec->name, dec->mult, dec->shift, cpu);
 
 	clockevents_register_device(dec);

commit 0526484aa345adbf790d1201a6f5d09be0a648a3
Merge: e0ea8b2c0677 aa021baa3295
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Nov 12 10:59:04 2009 +1100

    Merge commit 'origin/master' into next

commit 978d7eb31d44de34a7f71e04ed4158f3f854688d
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Nov 1 19:11:03 2009 +0000

    powerpc: Avoid giving out RTC dates below EPOCH
    
    Doing so causes xtime to be negative which crashes the timekeeping
    code in funny ways when doing suspend/resume
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 92dc844299b6..a136a11c490d 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -777,7 +777,7 @@ int update_persistent_clock(struct timespec now)
 	return ppc_md.set_rtc_time(&tm);
 }
 
-void read_persistent_clock(struct timespec *ts)
+static void __read_persistent_clock(struct timespec *ts)
 {
 	struct rtc_time tm;
 	static int first = 1;
@@ -800,10 +800,23 @@ void read_persistent_clock(struct timespec *ts)
 		return;
 	}
 	ppc_md.get_rtc_time(&tm);
+
 	ts->tv_sec = mktime(tm.tm_year+1900, tm.tm_mon+1, tm.tm_mday,
 			    tm.tm_hour, tm.tm_min, tm.tm_sec);
 }
 
+void read_persistent_clock(struct timespec *ts)
+{
+	__read_persistent_clock(ts);
+
+	/* Sanitize it in case real time clock is set below EPOCH */
+	if (ts->tv_sec < 0) {
+		ts->tv_sec = 0;
+		ts->tv_nsec = 0;
+	}
+		
+}
+
 /* clocksource code */
 static cycle_t rtc_read(struct clocksource *cs)
 {

commit 4ab79aa801b6b4f2e2fb508d6107cdd9320d682d
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Oct 30 05:47:19 2009 +0000

    Export symbols for KVM module
    
    We want to be able to build KVM as a module. To enable us doing so, we
    need some more exports from core Linux parts.
    
    This patch exports all functions and variables that are required for KVM.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 92dc844299b6..e05f6af64353 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -268,6 +268,7 @@ void account_system_vtime(struct task_struct *tsk)
 	per_cpu(cputime_scaled_last_delta, smp_processor_id()) = deltascaled;
 	local_irq_restore(flags);
 }
+EXPORT_SYMBOL_GPL(account_system_vtime);
 
 /*
  * Transfer the user and system times accumulated in the paca

commit 6795b85c6a4f690e61e7be31aa150d945c723fb5
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Oct 26 18:49:14 2009 +0000

    powerpc: tracing: Add powerpc tracepoints for timer entry and exit
    
    We can monitor the effectiveness of our power management of both the
    kernel and hypervisor by probing the timer interrupt. For example, on
    this box we see 10.37s timer interrupts on an idle core:
    
    <idle>-0     [010]  3900.671297: timer_interrupt_entry: pt_regs=c0000000ce1e7b10
    <idle>-0     [010]  3900.671302: timer_interrupt_exit: pt_regs=c0000000ce1e7b10
    
    <idle>-0     [010]  3911.042963: timer_interrupt_entry: pt_regs=c0000000ce1e7b10
    <idle>-0     [010]  3911.042968: timer_interrupt_exit: pt_regs=c0000000ce1e7b10
    
    <idle>-0     [010]  3921.414630: timer_interrupt_entry: pt_regs=c0000000ce1e7b10
    <idle>-0     [010]  3921.414635: timer_interrupt_exit: pt_regs=c0000000ce1e7b10
    
    Since we have a 207MHz decrementer it will go negative and fire every 10.37s
    even if Linux is completely idle.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 92dc844299b6..d6e88df4630c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -54,6 +54,7 @@
 #include <linux/irq.h>
 #include <linux/delay.h>
 #include <linux/perf_event.h>
+#include <asm/trace.h>
 
 #include <asm/io.h>
 #include <asm/processor.h>
@@ -571,6 +572,8 @@ void timer_interrupt(struct pt_regs * regs)
 	struct clock_event_device *evt = &decrementer->event;
 	u64 now;
 
+	trace_timer_interrupt_entry(regs);
+
 	/* Ensure a positive value is written to the decrementer, or else
 	 * some CPUs will continuue to take decrementer exceptions */
 	set_dec(DECREMENTER_MAX);
@@ -590,6 +593,7 @@ void timer_interrupt(struct pt_regs * regs)
 		now = decrementer->next_tb - now;
 		if (now <= DECREMENTER_MAX)
 			set_dec((int)now);
+		trace_timer_interrupt_exit(regs);
 		return;
 	}
 	old_regs = set_irq_regs(regs);
@@ -620,6 +624,8 @@ void timer_interrupt(struct pt_regs * regs)
 
 	irq_exit();
 	set_irq_regs(old_regs);
+
+	trace_timer_interrupt_exit(regs);
 }
 
 void wakeup_decrementer(void)

commit 31bbb9b58d1e8ebcf2b28c95c2250a9f8e31e397
Merge: ff830b8e5f99 3f0a525ebf4b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 23 09:46:15 2009 -0700

    Merge branch 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      itimers: Add tracepoints for itimer
      hrtimer: Add tracepoint for hrtimers
      timers: Add tracepoints for timer_list timers
      cputime: Optimize jiffies_to_cputime(1)
      itimers: Simplify arm_timer() code a bit
      itimers: Fix periodic tics precision
      itimers: Merge ITIMER_VIRT and ITIMER_PROF
    
    Trivial header file include conflicts in kernel/fork.c

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 465e498bcb33..df45a7449a66 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -53,7 +53,7 @@
 #include <linux/posix-timers.h>
 #include <linux/irq.h>
 #include <linux/delay.h>
-#include <linux/perf_counter.h>
+#include <linux/perf_event.h>
 
 #include <asm/io.h>
 #include <asm/processor.h>
@@ -527,25 +527,25 @@ void __init iSeries_time_init_early(void)
 }
 #endif /* CONFIG_PPC_ISERIES */
 
-#if defined(CONFIG_PERF_COUNTERS) && defined(CONFIG_PPC32)
-DEFINE_PER_CPU(u8, perf_counter_pending);
+#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_PPC32)
+DEFINE_PER_CPU(u8, perf_event_pending);
 
-void set_perf_counter_pending(void)
+void set_perf_event_pending(void)
 {
-	get_cpu_var(perf_counter_pending) = 1;
+	get_cpu_var(perf_event_pending) = 1;
 	set_dec(1);
-	put_cpu_var(perf_counter_pending);
+	put_cpu_var(perf_event_pending);
 }
 
-#define test_perf_counter_pending()	__get_cpu_var(perf_counter_pending)
-#define clear_perf_counter_pending()	__get_cpu_var(perf_counter_pending) = 0
+#define test_perf_event_pending()	__get_cpu_var(perf_event_pending)
+#define clear_perf_event_pending()	__get_cpu_var(perf_event_pending) = 0
 
-#else  /* CONFIG_PERF_COUNTERS && CONFIG_PPC32 */
+#else  /* CONFIG_PERF_EVENTS && CONFIG_PPC32 */
 
-#define test_perf_counter_pending()	0
-#define clear_perf_counter_pending()
+#define test_perf_event_pending()	0
+#define clear_perf_event_pending()
 
-#endif /* CONFIG_PERF_COUNTERS && CONFIG_PPC32 */
+#endif /* CONFIG_PERF_EVENTS && CONFIG_PPC32 */
 
 /*
  * For iSeries shared processors, we have to let the hypervisor
@@ -573,9 +573,9 @@ void timer_interrupt(struct pt_regs * regs)
 	set_dec(DECREMENTER_MAX);
 
 #ifdef CONFIG_PPC32
-	if (test_perf_counter_pending()) {
-		clear_perf_counter_pending();
-		perf_counter_do_pending();
+	if (test_perf_event_pending()) {
+		clear_perf_event_pending();
+		perf_event_do_pending();
 	}
 	if (atomic_read(&ppc_n_lost_interrupts) != 0)
 		do_IRQ(regs);

commit a03fdb7612874834d6847107198712d18b5242c7
Merge: 202c4675c55d 12e09337fe23
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 18 09:15:24 2009 -0700

    Merge branch 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (34 commits)
      time: Prevent 32 bit overflow with set_normalized_timespec()
      clocksource: Delay clocksource down rating to late boot
      clocksource: clocksource_select must be called with mutex locked
      clocksource: Resolve cpu hotplug dead lock with TSC unstable, fix crash
      timers: Drop a function prototype
      clocksource: Resolve cpu hotplug dead lock with TSC unstable
      timer.c: Fix S/390 comments
      timekeeping: Fix invalid getboottime() value
      timekeeping: Fix up read_persistent_clock() breakage on sh
      timekeeping: Increase granularity of read_persistent_clock(), build fix
      time: Introduce CLOCK_REALTIME_COARSE
      x86: Do not unregister PIT clocksource on PIT oneshot setup/shutdown
      clocksource: Avoid clocksource watchdog circular locking dependency
      clocksource: Protect the watchdog rating changes with clocksource_mutex
      clocksource: Call clocksource_change_rating() outside of watchdog_lock
      timekeeping: Introduce read_boot_clock
      timekeeping: Increase granularity of read_persistent_clock()
      timekeeping: Update clocksource with stop_machine
      timekeeping: Add timekeeper read_clock helper functions
      timekeeping: Move NTP adjusted clock multiplier to struct timekeeper
      ...
    
    Fix trivial conflict due to MIPS lemote -> loongson renaming.

commit f71bb0ac5e85410601b0db29d7b1635345ea61a4
Merge: 7285dd7fd375 a42548a18866
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Aug 29 10:34:18 2009 +0200

    Merge branch 'timers/posixtimers' into timers/tracing
    
    Merge reason: timer tracepoint patches depend on both branches
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 77c0a700c1c292edafa11c1e52821ce4636f81b0
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Aug 28 14:25:04 2009 +1000

    powerpc: Properly start decrementer on BookE secondary CPUs
    
    This moves the code to start the decrementer on 40x and BookE into
    a separate function which is now called from time_init() and
    secondary_time_init(), before the respective clock sources are
    registered. We also remove the 85xx specific code for doing it
    from the platform code.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index edb1edb36469..a180b4f9a4f6 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -727,6 +727,18 @@ static int __init get_freq(char *name, int cells, unsigned long *val)
 	return found;
 }
 
+/* should become __cpuinit when secondary_cpu_time_init also is */
+void start_cpu_decrementer(void)
+{
+#if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
+	/* Clear any pending timer interrupts */
+	mtspr(SPRN_TSR, TSR_ENW | TSR_WIS | TSR_DIS | TSR_FIS);
+
+	/* Enable decrementer interrupt */
+	mtspr(SPRN_TCR, TCR_DIE);
+#endif /* defined(CONFIG_BOOKE) || defined(CONFIG_40x) */
+}
+
 void __init generic_calibrate_decr(void)
 {
 	ppc_tb_freq = DEFAULT_TB_FREQ;		/* hardcoded default */
@@ -746,14 +758,6 @@ void __init generic_calibrate_decr(void)
 		printk(KERN_ERR "WARNING: Estimating processor frequency "
 				"(not found)\n");
 	}
-
-#if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
-	/* Clear any pending timer interrupts */
-	mtspr(SPRN_TSR, TSR_ENW | TSR_WIS | TSR_DIS | TSR_FIS);
-
-	/* Enable decrementer interrupt */
-	mtspr(SPRN_TCR, TCR_DIE);
-#endif
 }
 
 int update_persistent_clock(struct timespec now)
@@ -914,6 +918,11 @@ static void __init init_decrementer_clockevent(void)
 
 void secondary_cpu_time_init(void)
 {
+	/* Start the decrementer on CPUs that have manual control
+	 * such as BookE
+	 */
+	start_cpu_decrementer();
+
 	/* FIME: Should make unrelatred change to move snapshot_timebase
 	 * call here ! */
 	register_decrementer_clockevent(smp_processor_id());
@@ -1017,6 +1026,11 @@ void __init time_init(void)
 
 	write_sequnlock_irqrestore(&xtime_lock, flags);
 
+	/* Start the decrementer on CPUs that have manual control
+	 * such as BookE
+	 */
+	start_cpu_decrementer();
+
 	/* Register the clocksource, if we're not running on iSeries */
 	if (!firmware_has_feature(FW_FEATURE_ISERIES))
 		clocksource_init();

commit d90246cd8e0141332a8ab09c3c1800cc2028a686
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Sat Aug 22 22:23:13 2009 +0200

    timekeeping: Increase granularity of read_persistent_clock(), build fix
    
    Fix the following build problem on powerpc:
    
      arch/powerpc/kernel/time.c: In function 'read_persistent_clock':
      arch/powerpc/kernel/time.c:788: error: 'return' with a value, in function returning void
      arch/powerpc/kernel/time.c:791: error: 'return' with a value, in function returning void
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: dwalker@fifo99.com
    Cc: johnstul@us.ibm.com
    LKML-Reference: <20090822222313.74b9619c@skybase>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index ad63f30fe3da..a508388fb87c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -774,6 +774,7 @@ void read_persistent_clock(struct timespec *ts)
 	struct rtc_time tm;
 	static int first = 1;
 
+	ts->tv_nsec = 0;
 	/* XXX this is a litle fragile but will work okay in the short term */
 	if (first) {
 		first = 0;
@@ -781,15 +782,18 @@ void read_persistent_clock(struct timespec *ts)
 			timezone_offset = ppc_md.time_init();
 
 		/* get_boot_time() isn't guaranteed to be safe to call late */
-		if (ppc_md.get_boot_time)
-			return ppc_md.get_boot_time() -timezone_offset;
+		if (ppc_md.get_boot_time) {
+			ts->tv_sec = ppc_md.get_boot_time() - timezone_offset;
+			return;
+		}
+	}
+	if (!ppc_md.get_rtc_time) {
+		ts->tv_sec = 0;
+		return;
 	}
-	if (!ppc_md.get_rtc_time)
-		return 0;
 	ppc_md.get_rtc_time(&tm);
 	ts->tv_sec = mktime(tm.tm_year+1900, tm.tm_mon+1, tm.tm_mday,
 			    tm.tm_hour, tm.tm_min, tm.tm_sec);
-	ts->tv_nsec = 0;
 }
 
 /* clocksource code */

commit 14ea58ad797e4e9b7be755aca0fd3925d0713ede
Author: Julia Lawall <julia@diku.dk>
Date:   Sat Aug 1 22:48:27 2009 +0000

    powerpc: Use DIV_ROUND_CLOSEST in time init code
    
    The kernel.h macro DIV_ROUND_CLOSEST performs the computation (x + d/2)/d
    but is perhaps more readable.
    
    The semantic patch that makes this change is as follows:
    (http://www.emn.fr/x-info/coccinelle/)
    
    // <smpl>
    @haskernel@
    @@
    
    #include <linux/kernel.h>
    
    @depends on haskernel@
    expression x,__divisor;
    @@
    
    - (((x) + ((__divisor) / 2)) / (__divisor))
    + DIV_ROUND_CLOSEST(x,__divisor)
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index eae4511ceeac..edb1edb36469 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -479,7 +479,8 @@ static int __init iSeries_tb_recal(void)
 		unsigned long tb_ticks = tb - iSeries_recal_tb;
 		unsigned long titan_usec = (titan - iSeries_recal_titan) >> 12;
 		unsigned long new_tb_ticks_per_sec   = (tb_ticks * USEC_PER_SEC)/titan_usec;
-		unsigned long new_tb_ticks_per_jiffy = (new_tb_ticks_per_sec+(HZ/2))/HZ;
+		unsigned long new_tb_ticks_per_jiffy =
+			DIV_ROUND_CLOSEST(new_tb_ticks_per_sec, HZ);
 		long tick_diff = new_tb_ticks_per_jiffy - tb_ticks_per_jiffy;
 		char sign = '+';		
 		/* make sure tb_ticks_per_sec and tb_ticks_per_jiffy are consistent */

commit d4f587c67fc39e0030ddd718675e252e208da4d7
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Aug 14 15:47:31 2009 +0200

    timekeeping: Increase granularity of read_persistent_clock()
    
    The persistent clock of some architectures (e.g. s390) have a
    better granularity than seconds. To reduce the delta between the
    host clock and the guest clock in a virtualized system change the
    read_persistent_clock function to return a struct timespec.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: John Stultz <johnstul@us.ibm.com>
    Cc: Daniel Walker <dwalker@fifo99.com>
    LKML-Reference: <20090814134811.013873340@de.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index eae4511ceeac..ad63f30fe3da 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -769,7 +769,7 @@ int update_persistent_clock(struct timespec now)
 	return ppc_md.set_rtc_time(&tm);
 }
 
-unsigned long read_persistent_clock(void)
+void read_persistent_clock(struct timespec *ts)
 {
 	struct rtc_time tm;
 	static int first = 1;
@@ -787,8 +787,9 @@ unsigned long read_persistent_clock(void)
 	if (!ppc_md.get_rtc_time)
 		return 0;
 	ppc_md.get_rtc_time(&tm);
-	return mktime(tm.tm_year+1900, tm.tm_mon+1, tm.tm_mday,
-		      tm.tm_hour, tm.tm_min, tm.tm_sec);
+	ts->tv_sec = mktime(tm.tm_year+1900, tm.tm_mon+1, tm.tm_mday,
+			    tm.tm_hour, tm.tm_min, tm.tm_sec);
+	ts->tv_nsec = 0;
 }
 
 /* clocksource code */

commit a42548a18866e87092db93b771e6c5b060d78401
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Wed Jul 29 12:15:29 2009 +0200

    cputime: Optimize jiffies_to_cputime(1)
    
    For powerpc with CONFIG_VIRT_CPU_ACCOUNTING
    jiffies_to_cputime(1) is not compile time constant and run time
    calculations are quite expensive. To optimize we use
    precomputed value. For all other architectures is is
    preprocessor definition.
    
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    LKML-Reference: <1248862529-6063-5-git-send-email-sgruszka@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index eae4511ceeac..211d7b0cd370 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -193,6 +193,8 @@ EXPORT_SYMBOL(__cputime_clockt_factor);
 DEFINE_PER_CPU(unsigned long, cputime_last_delta);
 DEFINE_PER_CPU(unsigned long, cputime_scaled_last_delta);
 
+cputime_t cputime_one_jiffy;
+
 static void calc_cputime_factors(void)
 {
 	struct div_result res;
@@ -500,6 +502,7 @@ static int __init iSeries_tb_recal(void)
 				tb_to_xs = divres.result_low;
 				vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;
 				vdso_data->tb_to_xs = tb_to_xs;
+				setup_cputime_one_jiffy();
 			}
 			else {
 				printk( "Titan recalibrate: FAILED (difference > 4 percent)\n"
@@ -945,6 +948,7 @@ void __init time_init(void)
 	tb_ticks_per_usec = ppc_tb_freq / 1000000;
 	tb_to_us = mulhwu_scale_factor(ppc_tb_freq, 1000000);
 	calc_cputime_factors();
+	setup_cputime_one_jiffy();
 
 	/*
 	 * Calculate the length of each tick in ns.  It will not be

commit 105988c015943e77092a6568bc5fb7e386df6ccd
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 17 21:50:04 2009 +1000

    perf_counter: powerpc: Enable use of software counters on 32-bit powerpc
    
    This enables the perf_counter subsystem on 32-bit powerpc.  Since we
    don't have any support for hardware counters on 32-bit powerpc yet,
    only software counters can be used.
    
    Besides selecting HAVE_PERF_COUNTERS for 32-bit powerpc as well as
    64-bit, the main thing this does is add an implementation of
    set_perf_counter_pending().  This needs to arrange for
    perf_counter_do_pending() to be called when interrupts are enabled.
    Rather than add code to local_irq_restore as 64-bit does, the 32-bit
    set_perf_counter_pending() generates an interrupt by setting the
    decrementer to 1 so that a decrementer interrupt will become pending
    in 1 or 2 timebase ticks (if a decrementer interrupt isn't already
    pending).  When interrupts are enabled, timer_interrupt() will be
    called, and some new code in there calls perf_counter_do_pending().
    We use a per-cpu array of flags to indicate whether we need to call
    perf_counter_do_pending() or not.
    
    This introduces a couple of new Kconfig symbols: PPC_HAVE_PMU_SUPPORT,
    which is selected by processor families for which we have hardware PMU
    support (currently only PPC64), and PPC_PERF_CTRS, which enables the
    powerpc-specific perf_counter back-end.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: linuxppc-dev@ozlabs.org
    Cc: benh@kernel.crashing.org
    LKML-Reference: <19000.55404.103840.393470@cargo.ozlabs.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 15391c2ab013..eae4511ceeac 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -53,6 +53,7 @@
 #include <linux/posix-timers.h>
 #include <linux/irq.h>
 #include <linux/delay.h>
+#include <linux/perf_counter.h>
 
 #include <asm/io.h>
 #include <asm/processor.h>
@@ -525,6 +526,26 @@ void __init iSeries_time_init_early(void)
 }
 #endif /* CONFIG_PPC_ISERIES */
 
+#if defined(CONFIG_PERF_COUNTERS) && defined(CONFIG_PPC32)
+DEFINE_PER_CPU(u8, perf_counter_pending);
+
+void set_perf_counter_pending(void)
+{
+	get_cpu_var(perf_counter_pending) = 1;
+	set_dec(1);
+	put_cpu_var(perf_counter_pending);
+}
+
+#define test_perf_counter_pending()	__get_cpu_var(perf_counter_pending)
+#define clear_perf_counter_pending()	__get_cpu_var(perf_counter_pending) = 0
+
+#else  /* CONFIG_PERF_COUNTERS && CONFIG_PPC32 */
+
+#define test_perf_counter_pending()	0
+#define clear_perf_counter_pending()
+
+#endif /* CONFIG_PERF_COUNTERS && CONFIG_PPC32 */
+
 /*
  * For iSeries shared processors, we have to let the hypervisor
  * set the hardware decrementer.  We set a virtual decrementer
@@ -551,6 +572,10 @@ void timer_interrupt(struct pt_regs * regs)
 	set_dec(DECREMENTER_MAX);
 
 #ifdef CONFIG_PPC32
+	if (test_perf_counter_pending()) {
+		clear_perf_counter_pending();
+		perf_counter_do_pending();
+	}
 	if (atomic_read(&ppc_n_lost_interrupts) != 0)
 		do_IRQ(regs);
 #endif

commit 177996e6e20f15004d6757d9b859f57d181ef443
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jun 9 21:12:00 2009 +0000

    powerpc: Don't do generic calibrate_delay()
    
    Currently we are wasting time calling the generic calibrate_delay()
    function. We don't need it since our implementation of __delay() is
    based on the CPU timebase. So instead, we use our own small
    implementation that initializes loops_per_jiffy to something sensible
    to make the few users like spinlock debug be happy
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index bee1443da763..15391c2ab013 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -52,6 +52,7 @@
 #include <linux/jiffies.h>
 #include <linux/posix-timers.h>
 #include <linux/irq.h>
+#include <linux/delay.h>
 
 #include <asm/io.h>
 #include <asm/processor.h>
@@ -1143,6 +1144,15 @@ void div128_by_32(u64 dividend_high, u64 dividend_low,
 
 }
 
+/* We don't need to calibrate delay, we use the CPU timebase for that */
+void calibrate_delay(void)
+{
+	/* Some generic code (such as spinlock debug) use loops_per_jiffy
+	 * as the number of __delay(1) in a jiffy, so make it so
+	 */
+	loops_per_jiffy = tb_ticks_per_jiffy;
+}
+
 static int __init rtc_init(void)
 {
 	struct platform_device *pdev;

commit 8d165db10772f238103c3e8f955c54145e5c07f3
Author: Anton Blanchard <anton@samba.org>
Date:   Sun May 10 13:37:36 2009 +0000

    powerpc: Improve decrementer accuracy
    
    I have been looking at sources of OS jitter and notice that after a long
    NO_HZ idle period we wakeup too early:
    
    relative time (us)    event
                          timer irq exit
        999946.405        timer irq entry
             4.835        timer irq exit
            21.685        timer irq entry
             3.540          timer (tick_sched_timer) entry
    
    Here we slept for just under a second then took a timer interrupt that did
    nothing. 21.685 us later we wake up again and do the work.
    
    We set a rather low shift value of 16 for the decrementer clockevent, which I
    think is causing this issue. On this box we have a 207MHz decrementer and see:
    
    clockevent: decrementer mult[3501] shift[16] cpu[0]
    
    For calculations of large intervals this mult/shift combination could be
    off by a significant amount. I notice the sparc code has a loop that iterates
    to find a mult/shift combination that maximises the shift value while
    keeping mult under 32bit. With the patch below we get:
    
    clockevent: decrementer mult[35015c20] shift[32] cpu[15]
    
    And we no longer see the spurious wakeups.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 48571ac56fb7..bee1443da763 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -109,7 +109,7 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 static struct clock_event_device decrementer_clockevent = {
        .name           = "decrementer",
        .rating         = 200,
-       .shift          = 16,
+       .shift          = 0,	/* To be filled in */
        .mult           = 0,	/* To be filled in */
        .irq            = 0,
        .set_next_event = decrementer_set_next_event,
@@ -843,6 +843,22 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 		decrementer_set_next_event(DECREMENTER_MAX, dev);
 }
 
+static void __init setup_clockevent_multiplier(unsigned long hz)
+{
+	u64 mult, shift = 32;
+
+	while (1) {
+		mult = div_sc(hz, NSEC_PER_SEC, shift);
+		if (mult && (mult >> 32UL) == 0UL)
+			break;
+
+		shift--;
+	}
+
+	decrementer_clockevent.shift = shift;
+	decrementer_clockevent.mult = mult;
+}
+
 static void register_decrementer_clockevent(int cpu)
 {
 	struct clock_event_device *dec = &per_cpu(decrementers, cpu).event;
@@ -860,8 +876,7 @@ static void __init init_decrementer_clockevent(void)
 {
 	int cpu = smp_processor_id();
 
-	decrementer_clockevent.mult = div_sc(ppc_tb_freq, NSEC_PER_SEC,
-					     decrementer_clockevent.shift);
+	setup_clockevent_multiplier(ppc_tb_freq);
 	decrementer_clockevent.max_delta_ns =
 		clockevent_delta2ns(DECREMENTER_MAX, &decrementer_clockevent);
 	decrementer_clockevent.min_delta_ns =

commit 8e19608e8b5c001e4a66ce482edc474f05fb7355
Author: Magnus Damm <damm@igel.co.jp>
Date:   Tue Apr 21 12:24:00 2009 -0700

    clocksource: pass clocksource to read() callback
    
    Pass clocksource pointer to the read() callback for clocksources.  This
    allows us to share the callback between multiple instances.
    
    [hugh@veritas.com: fix powerpc build of clocksource pass clocksource mods]
    [akpm@linux-foundation.org: cleanup]
    Signed-off-by: Magnus Damm <damm@igel.co.jp>
    Acked-by: John Stultz <johnstul@us.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 926ea864e34f..48571ac56fb7 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -77,7 +77,7 @@
 #include <linux/clockchips.h>
 #include <linux/clocksource.h>
 
-static cycle_t rtc_read(void);
+static cycle_t rtc_read(struct clocksource *);
 static struct clocksource clocksource_rtc = {
 	.name         = "rtc",
 	.rating       = 400,
@@ -88,7 +88,7 @@ static struct clocksource clocksource_rtc = {
 	.read         = rtc_read,
 };
 
-static cycle_t timebase_read(void);
+static cycle_t timebase_read(struct clocksource *);
 static struct clocksource clocksource_timebase = {
 	.name         = "timebase",
 	.rating       = 400,
@@ -766,12 +766,12 @@ unsigned long read_persistent_clock(void)
 }
 
 /* clocksource code */
-static cycle_t rtc_read(void)
+static cycle_t rtc_read(struct clocksource *cs)
 {
 	return (cycle_t)get_rtc();
 }
 
-static cycle_t timebase_read(void)
+static cycle_t timebase_read(struct clocksource *cs)
 {
 	return (cycle_t)get_tb();
 }

commit bcd68a70cb0eee556d86d93133aa150319bd9f53
Author: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
Date:   Thu Feb 19 16:50:46 2009 +0100

    powerpc: Hook up rtc-generic, and kill rtc-ppc
    
    PowerPC has been a long time user of the generic RTC abstraction, so hook up
    rtc-generic:
      - Create the "rtc-generic" platform device if ppc_md.get_rtc_time is set,
      - Kill rtc-ppc, as rtc-generic offers the same functionality in a more
        generic way, and supports autoloading through udev.
    
    Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
    Acked-by: David Woodhouse <David.Woodhouse@intel.com>
    Acked-by: Alessandro Zummo <a.zummo@towertech.it>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Kyle McMartin <kyle@mcmartin.ca>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index c9564031a2a9..926ea864e34f 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1127,3 +1127,19 @@ void div128_by_32(u64 dividend_high, u64 dividend_low,
 	dr->result_low  = ((u64)y << 32) + z;
 
 }
+
+static int __init rtc_init(void)
+{
+	struct platform_device *pdev;
+
+	if (!ppc_md.get_rtc_time)
+		return -ENODEV;
+
+	pdev = platform_device_register_simple("rtc-generic", -1, NULL, 0);
+	if (IS_ERR(pdev))
+		return PTR_ERR(pdev);
+
+	return 0;
+}
+
+module_init(rtc_init);

commit 61420f59a589c0668f70cbe725785837c78ece90
Merge: d97106ab53f8 c742b31c03f3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 3 11:56:24 2009 -0800

    Merge branch 'cputime' of git://git390.osdl.marist.edu/pub/scm/linux-2.6
    
    * 'cputime' of git://git390.osdl.marist.edu/pub/scm/linux-2.6:
      [PATCH] fast vdso implementation for CLOCK_THREAD_CPUTIME_ID
      [PATCH] improve idle cputime accounting
      [PATCH] improve precision of idle time detection.
      [PATCH] improve precision of process accounting.
      [PATCH] idle cputime accounting
      [PATCH] fix scaled & unscaled cputime accounting

commit b840d79631c882786925303c2b0f4fefc31845ed
Merge: 597b0d21626d c3d80000e3a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 2 11:44:09 2009 -0800

    Merge branch 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (66 commits)
      x86: export vector_used_by_percpu_irq
      x86: use logical apicid in x2apic_cluster's x2apic_cpu_mask_to_apicid_and()
      sched: nominate preferred wakeup cpu, fix
      x86: fix lguest used_vectors breakage, -v2
      x86: fix warning in arch/x86/kernel/io_apic.c
      sched: fix warning in kernel/sched.c
      sched: move test_sd_parent() to an SMP section of sched.h
      sched: add SD_BALANCE_NEWIDLE at MC and CPU level for sched_mc>0
      sched: activate active load balancing in new idle cpus
      sched: bias task wakeups to preferred semi-idle packages
      sched: nominate preferred wakeup cpu
      sched: favour lower logical cpu number for sched_mc balance
      sched: framework for sched_mc/smt_power_savings=N
      sched: convert BALANCE_FOR_xx_POWER to inline functions
      x86: use possible_cpus=NUM to extend the possible cpus allowed
      x86: fix cpu_mask_to_apicid_and to include cpu_online_mask
      x86: update io_apic.c to the new cpumask code
      x86: Introduce topology_core_cpumask()/topology_thread_cpumask()
      x86: xen: use smp_call_function_many()
      x86: use work_on_cpu in x86/kernel/cpu/mcheck/mce_amd_64.c
      ...
    
    Fixed up trivial conflict in kernel/time/tick-sched.c manually

commit 79741dd35713ff4f6fd0eafd59fa94e8a4ba922d
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Wed Dec 31 15:11:38 2008 +0100

    [PATCH] idle cputime accounting
    
    The cpu time spent by the idle process actually doing something is
    currently accounted as idle time. This is plain wrong, the architectures
    that support VIRT_CPU_ACCOUNTING=y can do better: distinguish between the
    time spent doing nothing and the time spent by idle doing work. The first
    is accounted with account_idle_time and the second with account_system_time.
    The architectures that use the account_xxx_time interface directly and not
    the account_xxx_ticks interface now need to do the check for the idle
    process in their arch code. In particular to improve the system vs true
    idle time accounting the arch code needs to measure the true idle time
    instead of just testing for the idle process.
    To improve the tick based accounting as well we would need an architecture
    primitive that can tell us if the pt_regs of the interrupted context
    points to the magic instruction that halts the cpu.
    
    In addition idle time is no more added to the stime of the idle process.
    This field now contains the system time of the idle process as it should
    be. On systems without VIRT_CPU_ACCOUNTING this will always be zero as
    every tick that occurs while idle is running will be accounted as idle
    time.
    
    This patch contains the necessary common code changes to be able to
    distinguish idle system time and true idle time. The architectures with
    support for VIRT_CPU_ACCOUNTING need some changes to exploit this.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 92650ccad2e1..3be355c1cfa7 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -256,7 +256,10 @@ void account_system_vtime(struct task_struct *tsk)
 		delta += sys_time;
 		get_paca()->system_time = 0;
 	}
-	account_system_time(tsk, 0, delta, deltascaled);
+	if (in_irq() || idle_task(smp_processor_id()) != tsk)
+		account_system_time(tsk, 0, delta, deltascaled);
+	else
+		account_idle_time(delta);
 	per_cpu(cputime_last_delta, smp_processor_id()) = delta;
 	per_cpu(cputime_scaled_last_delta, smp_processor_id()) = deltascaled;
 	local_irq_restore(flags);
@@ -335,8 +338,12 @@ void calculate_steal_time(void)
 	tb = mftb();
 	purr = mfspr(SPRN_PURR);
 	stolen = (tb - pme->tb) - (purr - pme->purr);
-	if (stolen > 0)
-		account_steal_time(current, stolen);
+	if (stolen > 0) {
+		if (idle_task(smp_processor_id()) != current)
+			account_steal_time(stolen);
+		else
+			account_idle_time(stolen);
+	}
 	pme->tb = tb;
 	pme->purr = purr;
 }

commit 457533a7d3402d1d91fbc125c8bd1bd16dcd3cd4
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Wed Dec 31 15:11:37 2008 +0100

    [PATCH] fix scaled & unscaled cputime accounting
    
    The utimescaled / stimescaled fields in the task structure and the
    global cpustat should be set on all architectures. On s390 the calls
    to account_user_time_scaled and account_system_time_scaled never have
    been added. In addition system time that is accounted as guest time
    to the user time of a process is accounted to the scaled system time
    instead of the scaled user time.
    To fix the bugs and to prevent future forgetfulness this patch merges
    account_system_time_scaled into account_system_time and
    account_user_time_scaled into account_user_time.
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Michael Neuling <mikey@neuling.org>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index e1f3a5140429..92650ccad2e1 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -256,8 +256,7 @@ void account_system_vtime(struct task_struct *tsk)
 		delta += sys_time;
 		get_paca()->system_time = 0;
 	}
-	account_system_time(tsk, 0, delta);
-	account_system_time_scaled(tsk, deltascaled);
+	account_system_time(tsk, 0, delta, deltascaled);
 	per_cpu(cputime_last_delta, smp_processor_id()) = delta;
 	per_cpu(cputime_scaled_last_delta, smp_processor_id()) = deltascaled;
 	local_irq_restore(flags);
@@ -275,10 +274,8 @@ void account_process_tick(struct task_struct *tsk, int user_tick)
 
 	utime = get_paca()->user_time;
 	get_paca()->user_time = 0;
-	account_user_time(tsk, utime);
-
 	utimescaled = cputime_to_scaled(utime);
-	account_user_time_scaled(tsk, utimescaled);
+	account_user_time(tsk, utime, utimescaled);
 }
 
 /*

commit 320ab2b0b1e08e3805a3e1084a2f0eb1938d5d67
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sat Dec 13 21:20:26 2008 +1030

    cpumask: convert struct clock_event_device to cpumask pointers.
    
    Impact: change calling convention of existing clock_event APIs
    
    struct clock_event_timer's cpumask field gets changed to take pointer,
    as does the ->broadcast function.
    
    Another single-patch change.  For safety, we BUG_ON() in
    clockevents_register_device() if it's not set.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index e2ee66b5831d..6f39d35d6f55 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -869,7 +869,7 @@ static void register_decrementer_clockevent(int cpu)
 	struct clock_event_device *dec = &per_cpu(decrementers, cpu).event;
 
 	*dec = decrementer_clockevent;
-	dec->cpumask = cpumask_of_cpu(cpu);
+	dec->cpumask = cpumask_of(cpu);
 
 	printk(KERN_DEBUG "clockevent: %s mult[%lx] shift[%d] cpu[%d]\n",
 	       dec->name, dec->mult, dec->shift, cpu);

commit 3cc698789a3f44dabc5eed389b6c123caf787c0f
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Oct 28 00:18:11 2008 +0000

    powerpc: Eliminate unused do_gtod variable
    
    Since we started using the generic timekeeping code, we haven't had a
    powerpc-specific version of do_gettimeofday, and hence there is now
    nothing that reads the do_gtod variable in arch/powerpc/kernel/time.c.
    This therefore removes it and the code that sets it.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 8389c5be149c..e1f3a5140429 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -164,8 +164,6 @@ static u64 tb_to_ns_scale __read_mostly;
 static unsigned tb_to_ns_shift __read_mostly;
 static unsigned long boot_tb __read_mostly;
 
-static struct gettimeofday_struct do_gtod;
-
 extern struct timezone sys_tz;
 static long timezone_offset;
 
@@ -415,31 +413,9 @@ void udelay(unsigned long usecs)
 }
 EXPORT_SYMBOL(udelay);
 
-
-/*
- * There are two copies of tb_to_xs and stamp_xsec so that no
- * lock is needed to access and use these values in
- * do_gettimeofday.  We alternate the copies and as long as a
- * reasonable time elapses between changes, there will never
- * be inconsistent values.  ntpd has a minimum of one minute
- * between updates.
- */
 static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
 			       u64 new_tb_to_xs)
 {
-	unsigned temp_idx;
-	struct gettimeofday_vars *temp_varp;
-
-	temp_idx = (do_gtod.var_idx == 0);
-	temp_varp = &do_gtod.vars[temp_idx];
-
-	temp_varp->tb_to_xs = new_tb_to_xs;
-	temp_varp->tb_orig_stamp = new_tb_stamp;
-	temp_varp->stamp_xsec = new_stamp_xsec;
-	smp_mb();
-	do_gtod.varp = temp_varp;
-	do_gtod.var_idx = temp_idx;
-
 	/*
 	 * tb_update_count is used to allow the userspace gettimeofday code
 	 * to assure itself that it sees a consistent view of the tb_to_xs and
@@ -515,9 +491,7 @@ static int __init iSeries_tb_recal(void)
 				tb_ticks_per_sec   = new_tb_ticks_per_sec;
 				calc_cputime_factors();
 				div128_by_32( XSEC_PER_SEC, 0, tb_ticks_per_sec, &divres );
-				do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
 				tb_to_xs = divres.result_low;
-				do_gtod.varp->tb_to_xs = tb_to_xs;
 				vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;
 				vdso_data->tb_to_xs = tb_to_xs;
 			}
@@ -989,15 +963,6 @@ void __init time_init(void)
 		sys_tz.tz_dsttime = 0;
         }
 
-	do_gtod.varp = &do_gtod.vars[0];
-	do_gtod.var_idx = 0;
-	do_gtod.varp->tb_orig_stamp = tb_last_jiffy;
-	__get_cpu_var(last_jiffy) = tb_last_jiffy;
-	do_gtod.varp->stamp_xsec = (u64) xtime.tv_sec * XSEC_PER_SEC;
-	do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
-	do_gtod.varp->tb_to_xs = tb_to_xs;
-	do_gtod.tb_to_us = tb_to_us;
-
 	vdso_data->tb_orig_stamp = tb_last_jiffy;
 	vdso_data->tb_update_count = 0;
 	vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;

commit 597bc5c00b666fe123abb0af64f6e86f7ab72a90
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 27 23:56:03 2008 +0000

    powerpc: Improve resolution of VDSO clock_gettime
    
    Currently the clock_gettime implementation in the VDSO produces a
    result with microsecond resolution for the cases that are handled
    without a system call, i.e. CLOCK_REALTIME and CLOCK_MONOTONIC.  The
    nanoseconds field of the result is obtained by computing a
    microseconds value and multiplying by 1000.
    
    This changes the code in the VDSO to do the computation for
    clock_gettime with nanosecond resolution.  That means that the
    resolution of the result will ultimately depend on the timebase
    frequency.
    
    Because the timestamp in the VDSO datapage (stamp_xsec, the real time
    corresponding to the timebase count in tb_orig_stamp) is in units of
    2^-20 seconds, it doesn't have sufficient resolution for computing a
    result with nanosecond resolution.  Therefore this adds a copy of
    xtime to the VDSO datapage and updates it in update_gtod() along with
    the other time-related fields.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index e2ee66b5831d..8389c5be149c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -456,6 +456,7 @@ static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
 	vdso_data->tb_to_xs = new_tb_to_xs;
 	vdso_data->wtom_clock_sec = wall_to_monotonic.tv_sec;
 	vdso_data->wtom_clock_nsec = wall_to_monotonic.tv_nsec;
+	vdso_data->stamp_xtime = xtime;
 	smp_wmb();
 	++(vdso_data->tb_update_count);
 }

commit 84c3d4aaec3338201b449034beac41635866bddf
Merge: 43d2548bb2ef fafa3a3f1672
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Jul 16 11:07:59 2008 +1000

    Merge commit 'origin/master'
    
    Manual merge of:
    
            arch/powerpc/Kconfig
            arch/powerpc/kernel/stacktrace.c
            arch/powerpc/mm/slice.c
            arch/ppc/kernel/smp.c

commit ddb107e98b58ee280e99317cfd6efd16112678f2
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Fri Jun 27 08:03:13 2008 -0500

    powerpc/booke: don't reinitialize time base
    
    For some reason long ago I decided that we should zero out the time base
    when we calibrate the decrementer.  The problem is that this can be
    harmful in SMP systems where the firmware has already synchronized the
    time bases on the various cores.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index c73fc33aa817..eb938808ddfb 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -742,10 +742,6 @@ void __init generic_calibrate_decr(void)
 	}
 
 #if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
-	/* Set the time base to zero */
-	mtspr(SPRN_TBWL, 0);
-	mtspr(SPRN_TBWU, 0);
-
 	/* Clear any pending timer interrupts */
 	mtspr(SPRN_TSR, TSR_ENW | TSR_WIS | TSR_DIS | TSR_FIS);
 

commit 15c8b6c1aaaf1c4edd67e2f02e4d8e1bd1a51c0d
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri May 9 09:39:44 2008 +0200

    on_each_cpu(): kill unused 'retry' parameter
    
    It's not even passed on to smp_call_function() anymore, since that
    was removed. So kill it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 73401e83739a..f1a38a6c1e2d 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -322,7 +322,7 @@ void snapshot_timebases(void)
 {
 	if (!cpu_has_feature(CPU_FTR_PURR))
 		return;
-	on_each_cpu(snapshot_tb_and_purr, NULL, 0, 1);
+	on_each_cpu(snapshot_tb_and_purr, NULL, 1);
 }
 
 /*

commit 1c21a2937b1f342a8f5d580203c3396557d53b6e
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu May 8 14:27:19 2008 +1000

    [POWERPC] Fix sparse warnings in arch/powerpc/kernel
    
    Make a few things static in lparcfg.c
    Make init and exit routines static in rtas_flash.c
    Make things static in rtas_pci.c
    Make some functions static in rtas.c
    Make fops static in rtas-proc.c
    Remove unneeded extern for do_gtod in smp.c
    Make clocksource_init() static in time.c
    Make last_tick_len and ticklen_to_xs static in time.c
    Move the declaration of the pvr per-cpu into smp.h
    Make kexec_smp_down() and kexec_stack static in machine_kexec_64.c
    Don't return void in arch_teardown_msi_irqs() in msi.c
    Move declaration of GregorianDay()into asm/time.h
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 73401e83739a..c73fc33aa817 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -129,7 +129,7 @@ static unsigned long __initdata iSeries_recal_titan;
 static signed long __initdata iSeries_recal_tb;
 
 /* Forward declaration is only needed for iSereis compiles */
-void __init clocksource_init(void);
+static void __init clocksource_init(void);
 #endif
 
 #define XSEC_PER_SEC (1024*1024)
@@ -150,8 +150,8 @@ u64 tb_to_xs;
 unsigned tb_to_us;
 
 #define TICKLEN_SCALE	NTP_SCALE_SHIFT
-u64 last_tick_len;	/* units are ns / 2^TICKLEN_SCALE */
-u64 ticklen_to_xs;	/* 0.64 fraction */
+static u64 last_tick_len;	/* units are ns / 2^TICKLEN_SCALE */
+static u64 ticklen_to_xs;	/* 0.64 fraction */
 
 /* If last_tick_len corresponds to about 1/HZ seconds, then
    last_tick_len << TICKLEN_SHIFT will be about 2^63. */
@@ -164,7 +164,7 @@ static u64 tb_to_ns_scale __read_mostly;
 static unsigned tb_to_ns_shift __read_mostly;
 static unsigned long boot_tb __read_mostly;
 
-struct gettimeofday_struct do_gtod;
+static struct gettimeofday_struct do_gtod;
 
 extern struct timezone sys_tz;
 static long timezone_offset;
@@ -832,7 +832,7 @@ void update_vsyscall_tz(void)
 	++vdso_data->tb_update_count;
 }
 
-void __init clocksource_init(void)
+static void __init clocksource_init(void)
 {
 	struct clocksource *clock;
 

commit 7fc5c78409479d826341b103bdf734cb4fb02436
Author: Roman Zippel <zippel@linux-m68k.org>
Date:   Thu May 1 04:34:38 2008 -0700

    ntp: rename TICK_LENGTH_SHIFT to NTP_SCALE_SHIFT
    
    As TICK_LENGTH_SHIFT is used for more than just the tick length, the name
    isn't quite approriate anymore, so this renames it to NTP_SCALE_SHIFT.
    
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index c146af995854..73401e83739a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -149,7 +149,7 @@ EXPORT_SYMBOL(tb_ticks_per_sec);	/* for cputime_t conversions */
 u64 tb_to_xs;
 unsigned tb_to_us;
 
-#define TICKLEN_SCALE	TICK_LENGTH_SHIFT
+#define TICKLEN_SCALE	NTP_SCALE_SHIFT
 u64 last_tick_len;	/* units are ns / 2^TICKLEN_SCALE */
 u64 ticklen_to_xs;	/* 0.64 fraction */
 

commit 074b3b87941c99bc0ce35385b5817924b1ed0c23
Author: Roman Zippel <zippel@linux-m68k.org>
Date:   Thu May 1 04:34:34 2008 -0700

    ntp: increase time_freq resolution
    
    This changes time_freq to a 64bit value and makes it static (the only outside
    user had no real need to modify it).  Intermediate values were already 64bit,
    so the change isn't that big, but it saves a little in shifts by replacing
    SHIFT_NSEC with TICK_LENGTH_SHIFT.  PPM_SCALE is then used to convert between
    user space and kernel space representation.
    
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 3b26fbd6bec9..c146af995854 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1007,8 +1007,6 @@ void __init time_init(void)
 	vdso_data->stamp_xsec = (u64) xtime.tv_sec * XSEC_PER_SEC;
 	vdso_data->tb_to_xs = tb_to_xs;
 
-	time_freq = 0;
-
 	write_sequnlock_irqrestore(&xtime_lock, flags);
 
 	/* Register the clocksource, if we're not running on iSeries */

commit 06b8e878a9bc9301201cffe186eba99c4185f20a
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Feb 6 01:36:12 2008 -0800

    taskstats scaled time cleanup
    
    This moves the ability to scale cputime into generic code.  This allows us
    to fix the issue in kernel/timer.c (noticed by Balbir) where we could only
    add an unscaled value to the scaled utime/stime.
    
    This adds a cputime_to_scaled function.  As before, the POWERPC version
    does the scaling based on the last SPURR/PURR ratio calculated.  The
    generic and s390 (only other arch to implement asm/cputime.h) versions are
    both NOPs.
    
    Also moves the SPURR and PURR snapshots closer.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 5cd3db5cae41..3b26fbd6bec9 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -66,6 +66,7 @@
 #include <asm/smp.h>
 #include <asm/vdso_datapage.h>
 #include <asm/firmware.h>
+#include <asm/cputime.h>
 #ifdef CONFIG_PPC_ISERIES
 #include <asm/iseries/it_lp_queue.h>
 #include <asm/iseries/hv_call_xm.h>
@@ -189,6 +190,8 @@ u64 __cputime_sec_factor;
 EXPORT_SYMBOL(__cputime_sec_factor);
 u64 __cputime_clockt_factor;
 EXPORT_SYMBOL(__cputime_clockt_factor);
+DEFINE_PER_CPU(unsigned long, cputime_last_delta);
+DEFINE_PER_CPU(unsigned long, cputime_scaled_last_delta);
 
 static void calc_cputime_factors(void)
 {
@@ -257,8 +260,8 @@ void account_system_vtime(struct task_struct *tsk)
 	}
 	account_system_time(tsk, 0, delta);
 	account_system_time_scaled(tsk, deltascaled);
-	get_paca()->purrdelta = delta;
-	get_paca()->spurrdelta = deltascaled;
+	per_cpu(cputime_last_delta, smp_processor_id()) = delta;
+	per_cpu(cputime_scaled_last_delta, smp_processor_id()) = deltascaled;
 	local_irq_restore(flags);
 }
 
@@ -276,10 +279,7 @@ void account_process_tick(struct task_struct *tsk, int user_tick)
 	get_paca()->user_time = 0;
 	account_user_time(tsk, utime);
 
-	/* Estimate the scaled utime by scaling the real utime based
-	 * on the last spurr to purr ratio */
-	utimescaled = utime * get_paca()->spurrdelta / get_paca()->purrdelta;
-	get_paca()->spurrdelta = get_paca()->purrdelta = 0;
+	utimescaled = cputime_to_scaled(utime);
 	account_user_time_scaled(tsk, utimescaled);
 }
 

commit 7ac5dde99eb9fefdb526973c600075b7c5703a86
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Dec 13 04:35:19 2007 +1100

    [POWERPC] Implement arch disable/enable irq hooks.
    
    These hooks ensure that a decrementer interrupt is not pending when
    suspending; otherwise, problems may occur on 6xx/7xx/7xxx-based
    systems (except for powermacs, which use a separate suspend path).
    For example, with deep sleep on the 831x, a pending decrementer will
    cause a system freeze because the SoC thinks the decrementer interrupt
    would have woken the system, but the core must have interrupts
    disabled due to the setup required for deep sleep.
    
    Changed via-pmu.c to use the new ppc_md hooks, and made the arch_*
    functions call the generic_* functions unconditionally.  -- paulus
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 85cf317c9069..5cd3db5cae41 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -629,6 +629,45 @@ void wakeup_decrementer(void)
 	set_dec(ticks);
 }
 
+#ifdef CONFIG_SUSPEND
+void generic_suspend_disable_irqs(void)
+{
+	preempt_disable();
+
+	/* Disable the decrementer, so that it doesn't interfere
+	 * with suspending.
+	 */
+
+	set_dec(0x7fffffff);
+	local_irq_disable();
+	set_dec(0x7fffffff);
+}
+
+void generic_suspend_enable_irqs(void)
+{
+	wakeup_decrementer();
+
+	local_irq_enable();
+	preempt_enable();
+}
+
+/* Overrides the weak version in kernel/power/main.c */
+void arch_suspend_disable_irqs(void)
+{
+	if (ppc_md.suspend_disable_irqs)
+		ppc_md.suspend_disable_irqs();
+	generic_suspend_disable_irqs();
+}
+
+/* Overrides the weak version in kernel/power/main.c */
+void arch_suspend_enable_irqs(void)
+{
+	generic_suspend_enable_irqs();
+	if (ppc_md.suspend_enable_irqs)
+		ppc_md.suspend_enable_irqs();
+}
+#endif
+
 #ifdef CONFIG_SMP
 void __init smp_space_timers(unsigned int max_cpus)
 {

commit 53024fe250a0f044b114844a01456902bce40ade
Author: Milton Miller <miltonm@bga.com>
Date:   Fri Dec 14 15:52:20 2007 +1100

    [POWERPC] Optimize account_system_vtime
    
    We have multiple calls to has_feature being inlined, but gcc can't
    be sure that the store via get_paca() doesn't alias the path to
    cur_cpu_spec->feature.
    
    Reorder to put the calls to read_purr and read_spurr adjacent to each
    other.  To add a sense of consistency, reorder the remaining lines to
    perform parallel steps on purr and scaled purr of each line instead of
    calculating and then using one value before going on to the next.
    
    In addition, we can tell gcc that no SPURR means no PURR.  The test is
    completely hidden in the PURR case, and in the !PURR case the second test
    is eliminated resulting in the simple register copy in the out-of-line
    branch.
    
    Further, gcc sees get_paca()->system_time referenced several times and
    allocates a register to address it (shadowing r13) instead of caching its
    value.  Reading into a local varable saves the shadow of r13 and removes
    a potentially duplicate load (between the nested if and its parent).
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 398cd0c7b60f..85cf317c9069 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -219,7 +219,11 @@ static u64 read_purr(void)
  */
 static u64 read_spurr(u64 purr)
 {
-	if (cpu_has_feature(CPU_FTR_SPURR))
+	/*
+	 * cpus without PURR won't have a SPURR
+	 * We already know the former when we use this, so tell gcc
+	 */
+	if (cpu_has_feature(CPU_FTR_PURR) && cpu_has_feature(CPU_FTR_SPURR))
 		return mfspr(SPRN_SPURR);
 	return purr;
 }
@@ -230,29 +234,30 @@ static u64 read_spurr(u64 purr)
  */
 void account_system_vtime(struct task_struct *tsk)
 {
-	u64 now, nowscaled, delta, deltascaled;
+	u64 now, nowscaled, delta, deltascaled, sys_time;
 	unsigned long flags;
 
 	local_irq_save(flags);
 	now = read_purr();
-	delta = now - get_paca()->startpurr;
-	get_paca()->startpurr = now;
 	nowscaled = read_spurr(now);
+	delta = now - get_paca()->startpurr;
 	deltascaled = nowscaled - get_paca()->startspurr;
+	get_paca()->startpurr = now;
 	get_paca()->startspurr = nowscaled;
 	if (!in_interrupt()) {
 		/* deltascaled includes both user and system time.
 		 * Hence scale it based on the purr ratio to estimate
 		 * the system time */
+		sys_time = get_paca()->system_time;
 		if (get_paca()->user_time)
-			deltascaled = deltascaled * get_paca()->system_time /
-			     (get_paca()->system_time + get_paca()->user_time);
-		delta += get_paca()->system_time;
+			deltascaled = deltascaled * sys_time /
+			     (sys_time + get_paca()->user_time);
+		delta += sys_time;
 		get_paca()->system_time = 0;
 	}
 	account_system_time(tsk, 0, delta);
-	get_paca()->purrdelta = delta;
 	account_system_time_scaled(tsk, deltascaled);
+	get_paca()->purrdelta = delta;
 	get_paca()->spurrdelta = deltascaled;
 	local_irq_restore(flags);
 }

commit db3801a858e97adee0f3b4c732e6ff325d95e0a5
Author: Milton Miller <miltonm@bga.com>
Date:   Fri Dec 14 15:52:19 2007 +1100

    [POWERPC] Depend on ->initialized in calc_steal_time
    
    If CPU_FTR_PURR is not set, we will never set cpu_purr_data->initialized.
    Checking via __get_cpu_var on 64 bit avoids one dependent load compared
    to cpu_has_feature in the not-present case, and is always required when
    it is present.  The code is under CONFIG_VIRT_CPU_ACCOUNTING so 32 bit
    will not be affected.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 5cf394080590..398cd0c7b60f 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -329,11 +329,9 @@ void calculate_steal_time(void)
 	s64 stolen;
 	struct cpu_purr_data *pme;
 
-	if (!cpu_has_feature(CPU_FTR_PURR))
-		return;
 	pme = &__get_cpu_var(cpu_purr_data);
 	if (!pme->initialized)
-		return;		/* this can happen in early boot */
+		return;		/* !CPU_FTR_PURR or early in early boot */
 	tb = mftb();
 	purr = mfspr(SPRN_PURR);
 	stolen = (tb - pme->tb) - (purr - pme->purr);

commit 6e6b44e8223a01d35fceec3631be356fbdbcf004
Author: Milton Miller <miltonm@bga.com>
Date:   Fri Dec 14 15:52:15 2007 +1100

    [POWERPC] Timer interrupt: use a struct for two per_cpu varables
    
    timer_interrupt() was calculating per_cpu_offset several times, having to
    start from the toc because of potential aliasing issues.
    
    Placing both decrementer per_cpu varables in a struct and calculating
    the address once with __get_cpu_var results in better code on both 32
    and 64 bit.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 888c65726ebd..5cf394080590 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -116,8 +116,12 @@ static struct clock_event_device decrementer_clockevent = {
        .features       = CLOCK_EVT_FEAT_ONESHOT,
 };
 
-static DEFINE_PER_CPU(struct clock_event_device, decrementers);
-static DEFINE_PER_CPU(u64, decrementer_next_tb);
+struct decrementer_clock {
+	struct clock_event_device event;
+	u64 next_tb;
+};
+
+static DEFINE_PER_CPU(struct decrementer_clock, decrementers);
 
 #ifdef CONFIG_PPC_ISERIES
 static unsigned long __initdata iSeries_recal_titan;
@@ -555,8 +559,8 @@ void __init iSeries_time_init_early(void)
 void timer_interrupt(struct pt_regs * regs)
 {
 	struct pt_regs *old_regs;
-	int cpu = smp_processor_id();
-	struct clock_event_device *evt = &per_cpu(decrementers, cpu);
+	struct decrementer_clock *decrementer =  &__get_cpu_var(decrementers);
+	struct clock_event_device *evt = &decrementer->event;
 	u64 now;
 
 	/* Ensure a positive value is written to the decrementer, or else
@@ -569,9 +573,9 @@ void timer_interrupt(struct pt_regs * regs)
 #endif
 
 	now = get_tb_or_rtc();
-	if (now < per_cpu(decrementer_next_tb, cpu)) {
+	if (now < decrementer->next_tb) {
 		/* not time for this event yet */
-		now = per_cpu(decrementer_next_tb, cpu) - now;
+		now = decrementer->next_tb - now;
 		if (now <= DECREMENTER_MAX)
 			set_dec((int)now);
 		return;
@@ -810,7 +814,7 @@ void __init clocksource_init(void)
 static int decrementer_set_next_event(unsigned long evt,
 				      struct clock_event_device *dev)
 {
-	__get_cpu_var(decrementer_next_tb) = get_tb_or_rtc() + evt;
+	__get_cpu_var(decrementers).next_tb = get_tb_or_rtc() + evt;
 	set_dec(evt);
 	return 0;
 }
@@ -824,7 +828,7 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 
 static void register_decrementer_clockevent(int cpu)
 {
-	struct clock_event_device *dec = &per_cpu(decrementers, cpu);
+	struct clock_event_device *dec = &per_cpu(decrementers, cpu).event;
 
 	*dec = decrementer_clockevent;
 	dec->cpumask = cpumask_of_cpu(cpu);

commit 8b5621f183f7e56958b0a0aae689af5af2efeda7
Author: Milton Miller <miltonm@bga.com>
Date:   Fri Dec 14 15:52:10 2007 +1100

    [POWERPC] Use __get_cpu_var in time.c
    
    Use __get_cpu_var(x) instead of per_cpu(x, smp_processor_id()), as it
    is optimized on ppc64 to access the current cpu's per-cpu offset directly;
    it's local_paca.offset instead of TOC->paca[local_paca->processor_id].offset.
    
    This is the trivial portion, two functions with one use each.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index cc82fd675f91..888c65726ebd 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -327,7 +327,7 @@ void calculate_steal_time(void)
 
 	if (!cpu_has_feature(CPU_FTR_PURR))
 		return;
-	pme = &per_cpu(cpu_purr_data, smp_processor_id());
+	pme = &__get_cpu_var(cpu_purr_data);
 	if (!pme->initialized)
 		return;		/* this can happen in early boot */
 	tb = mftb();
@@ -352,7 +352,7 @@ static void snapshot_purr(void)
 	if (!cpu_has_feature(CPU_FTR_PURR))
 		return;
 	local_irq_save(flags);
-	pme = &per_cpu(cpu_purr_data, smp_processor_id());
+	pme = &__get_cpu_var(cpu_purr_data);
 	pme->tb = mftb();
 	pme->purr = mfspr(SPRN_PURR);
 	pme->initialized = 1;

commit c481887f2b74f87f714e2b76b8b834e57912bd0a
Author: Milton Miller <miltonm@bga.com>
Date:   Fri Dec 14 15:52:10 2007 +1100

    [POWERPC] init_decrementer_clockevent can be static __init
    
    as its only called from time_init, which is __init.
    
    Also remove unneeded forward declaration.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index a925a8eae121..cc82fd675f91 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -117,7 +117,6 @@ static struct clock_event_device decrementer_clockevent = {
 };
 
 static DEFINE_PER_CPU(struct clock_event_device, decrementers);
-void init_decrementer_clockevent(void);
 static DEFINE_PER_CPU(u64, decrementer_next_tb);
 
 #ifdef CONFIG_PPC_ISERIES
@@ -836,7 +835,7 @@ static void register_decrementer_clockevent(int cpu)
 	clockevents_register_device(dec);
 }
 
-void init_decrementer_clockevent(void)
+static void __init init_decrementer_clockevent(void)
 {
 	int cpu = smp_processor_id();
 

commit 2b46b5673ca67d23302c2afac045def988a3cade
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Nov 20 15:18:40 2007 +1100

    [POWERPC] Fix possible division by zero in scaled time accounting
    
    If we get no user time and no system time allocated since the last
    account_system_vtime, the system to user time ratio estimate can end
    up dividing by zero.
    
    This was causing a problem noticed by Balbir Singh.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index c0d77723ba11..a925a8eae121 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -241,8 +241,9 @@ void account_system_vtime(struct task_struct *tsk)
 		/* deltascaled includes both user and system time.
 		 * Hence scale it based on the purr ratio to estimate
 		 * the system time */
-		deltascaled = deltascaled * get_paca()->system_time /
-			(get_paca()->system_time + get_paca()->user_time);
+		if (get_paca()->user_time)
+			deltascaled = deltascaled * get_paca()->system_time /
+			     (get_paca()->system_time + get_paca()->user_time);
 		delta += get_paca()->system_time;
 		get_paca()->system_time = 0;
 	}

commit 0302f12e1c72e450d3b4569d9c03c57c9b5edac1
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Mon Nov 12 14:25:50 2007 +1100

    [POWERPC] Demote clockevent printk to KERN_DEBUG
    
    These don't need to be seen by everyone on every boot.
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 4beb6329dfb7..c0d77723ba11 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -829,7 +829,7 @@ static void register_decrementer_clockevent(int cpu)
 	*dec = decrementer_clockevent;
 	dec->cpumask = cpumask_of_cpu(cpu);
 
-	printk(KERN_INFO "clockevent: %s mult[%lx] shift[%d] cpu[%d]\n",
+	printk(KERN_DEBUG "clockevent: %s mult[%lx] shift[%d] cpu[%d]\n",
 	       dec->name, dec->mult, dec->shift, cpu);
 
 	clockevents_register_device(dec);

commit a70a93229943c177f0062490b4f8e44be4cef685
Merge: a80b824f0b63 e6fe6649b4ec
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Fri Nov 9 15:27:54 2007 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mingo/linux-2.6-sched
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mingo/linux-2.6-sched:
      sched: proper prototype for kernel/sched.c:migration_init()
      sched: avoid large irq-latencies in smp-balancing
      sched: fix copy_namespace() <-> sched_fork() dependency in do_fork
      sched: clean up the wakeup preempt check, #2
      sched: clean up the wakeup preempt check
      sched: wakeup preemption fix
      sched: remove PREEMPT_RESTRICT
      sched: turn off PREEMPT_RESTRICT
      KVM: fix !SMP build error
      x86: make nmi_cpu_busy() always defined
      x86: make ipi_handler() always defined
      sched: cleanup, use NSEC_PER_MSEC and NSEC_PER_SEC
      sched: reintroduce SMP tunings again
      sched: restore deterministic CPU accounting on powerpc
      sched: fix delay accounting regression
      sched: reintroduce the sched_min_granularity tunable
      sched: documentation: place_entity() comments
      sched: fix vslice

commit fa13a5a1f25f671d084d8884be96fc48d9b68275
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Nov 9 22:39:38 2007 +0100

    sched: restore deterministic CPU accounting on powerpc
    
    Since powerpc started using CONFIG_GENERIC_CLOCKEVENTS, the
    deterministic CPU accounting (CONFIG_VIRT_CPU_ACCOUNTING) has been
    broken on powerpc, because we end up counting user time twice: once in
    timer_interrupt() and once in update_process_times().
    
    This fixes the problem by pulling the code in update_process_times
    that updates utime and stime into a separate function called
    account_process_tick.  If CONFIG_VIRT_CPU_ACCOUNTING is not defined,
    there is a version of account_process_tick in kernel/timer.c that
    simply accounts a whole tick to either utime or stime as before.  If
    CONFIG_VIRT_CPU_ACCOUNTING is defined, then arch code gets to
    implement account_process_tick.
    
    This also lets us simplify the s390 code a bit; it means that the s390
    timer interrupt can now call update_process_times even when
    CONFIG_VIRT_CPU_ACCOUNTING is turned on, and can just implement a
    suitable account_process_tick().
    
    account_process_tick() now takes the task_struct * as an argument.
    Tested both with and without CONFIG_VIRT_CPU_ACCOUNTING.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 9eb3284deac4..a70dfb76d0a8 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -259,7 +259,7 @@ void account_system_vtime(struct task_struct *tsk)
  * user and system time records.
  * Must be called with interrupts disabled.
  */
-void account_process_vtime(struct task_struct *tsk)
+void account_process_tick(struct task_struct *tsk, int user_tick)
 {
 	cputime_t utime, utimescaled;
 
@@ -274,18 +274,6 @@ void account_process_vtime(struct task_struct *tsk)
 	account_user_time_scaled(tsk, utimescaled);
 }
 
-static void account_process_time(struct pt_regs *regs)
-{
-	int cpu = smp_processor_id();
-
-	account_process_vtime(current);
-	run_local_timers();
-	if (rcu_pending(cpu))
-		rcu_check_callbacks(cpu, user_mode(regs));
-	scheduler_tick();
- 	run_posix_cpu_timers(current);
-}
-
 /*
  * Stuff for accounting stolen time.
  */
@@ -375,7 +363,6 @@ static void snapshot_purr(void)
 
 #else /* ! CONFIG_VIRT_CPU_ACCOUNTING */
 #define calc_cputime_factors()
-#define account_process_time(regs)	update_process_times(user_mode(regs))
 #define calculate_steal_time()		do { } while (0)
 #endif
 
@@ -599,16 +586,6 @@ void timer_interrupt(struct pt_regs * regs)
 		get_lppaca()->int_dword.fields.decr_int = 0;
 #endif
 
-	/*
-	 * We cannot disable the decrementer, so in the period
-	 * between this cpu's being marked offline in cpu_online_map
-	 * and calling stop-self, it is taking timer interrupts.
-	 * Avoid calling into the scheduler rebalancing code if this
-	 * is the case.
-	 */
-	if (!cpu_is_offline(cpu))
-		account_process_time(regs);
-
 	if (evt->event_handler)
 		evt->event_handler(evt);
 	else

commit 43875cc0a54d936132010e58545269e183741fae
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Oct 31 22:25:35 2007 +1100

    [POWERPC] Fix off-by-one error in setting decrementer on Book E/4xx (v2)
    
    The decrementer in Book E and 4xx processors interrupts on the
    transition from 1 to 0, rather than on the 0 to -1 transition as on
    64-bit server and 32-bit "classic" (6xx/7xx/7xxx) processors.  At the
    moment we subtract 1 from the count of how many decrementer ticks are
    required before the next interrupt before putting it into the
    decrementer, which is correct for server/classic processors, but could
    possibly cause the interrupt to happen too early on Book E and 4xx if
    the timebase/decrementer frequency is low.
    
    This fixes the problem by making set_dec subtract 1 from the count for
    server and classic processors, instead of having the callers subtract
    1.  Since set_dec already had a bunch of ifdefs to handle different
    processor types, there is no net increase in ugliness. :)
    
    Note that calling set_dec(0) may not generate an interrupt on some
    processors.  To make sure that decrementer_set_next_event always calls
    set_dec with an interval of at least 1 tick, we set min_delta_ns of
    the decrementer_clockevent to correspond to 2 ticks (2 rather than 1
    to compensate for truncations in the conversions between ticks and
    ns).
    
    This also removes a redundant call to set the decrementer to
    0x7fffffff - it was already set to that earlier in timer_interrupt.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 9eb3284deac4..99ebcd3884d2 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -586,7 +586,7 @@ void timer_interrupt(struct pt_regs * regs)
 		/* not time for this event yet */
 		now = per_cpu(decrementer_next_tb, cpu) - now;
 		if (now <= DECREMENTER_MAX)
-			set_dec((unsigned int)now - 1);
+			set_dec((int)now);
 		return;
 	}
 	old_regs = set_irq_regs(regs);
@@ -611,8 +611,6 @@ void timer_interrupt(struct pt_regs * regs)
 
 	if (evt->event_handler)
 		evt->event_handler(evt);
-	else
-		evt->set_next_event(DECREMENTER_MAX, evt);
 
 #ifdef CONFIG_PPC_ISERIES
 	if (firmware_has_feature(FW_FEATURE_ISERIES) && hvlpevent_is_pending())
@@ -836,9 +834,6 @@ static int decrementer_set_next_event(unsigned long evt,
 				      struct clock_event_device *dev)
 {
 	__get_cpu_var(decrementer_next_tb) = get_tb_or_rtc() + evt;
-	/* The decrementer interrupts on the 0 -> -1 transition */
-	if (evt)
-		--evt;
 	set_dec(evt);
 	return 0;
 }
@@ -871,7 +866,8 @@ void init_decrementer_clockevent(void)
 					     decrementer_clockevent.shift);
 	decrementer_clockevent.max_delta_ns =
 		clockevent_delta2ns(DECREMENTER_MAX, &decrementer_clockevent);
-	decrementer_clockevent.min_delta_ns = 1000;
+	decrementer_clockevent.min_delta_ns =
+		clockevent_delta2ns(2, &decrementer_clockevent);
 
 	register_decrementer_clockevent(cpu);
 }

commit 4603ac180a824197c2262747948d0179eb076e9c
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Oct 18 03:06:37 2007 -0700

    powerpc: add scaled time accounting
    
    This adds POWERPC specific hooks for scaled time accounting.
    
    POWER6 includes a SPURR register.  The SPURR is based off the PURR register
    but is scaled based on CPU frequency and issue rates.  This gives a more
    accurate account of the instructions used per task.  The PURR and timebase
    will be constant relative to the wall clock, irrespective of the CPU
    frequency.
    
    This implementation reads the SPURR register in account_system_vtime which
    is only call called on context witch and hard and soft irq entry and exit.
    The percentage of user and system time is then estimated using the ratio of
    these accounted by the PURR.  If the SPURR is not present, the PURR read.
    
    An earlier implementation of this patch read the SPURR whenever the PURR
    was read, which included the system call entry and exit path.
    Unfortunately this showed a performance regression on lmbench runs, so was
    re-implemented.
    
    I've included the lmbench results here when run bare metal on POWER6.  1st
    column is the unpatch results.  2nd column is the results using the below
    patch and the 3rd is the % diff of these results from the base.  4th and
    5th columns are the results and % differnce from the base using the older
    patch (SPURR read in syscall entry/exit path).
    
                                  Base        Scaled-Acct     SPURR-in-syscall
                                 Result      Result  % diff    Result % diff
    Simple syscall:              0.3086      0.3086  0.0000    0.3452 11.8600
    Simple read:                 0.4591      0.4671  1.7425    0.5044 9.86713
    Simple write:                0.4364      0.4366  0.0458    0.4731 8.40971
    Simple stat:                 2.0055      2.0295  1.1967    2.0669 3.06158
    Simple fstat:                0.5962      0.5876  -1.442    0.6368 6.80979
    Simple open/close:           3.1283      3.1009  -0.875    3.2088 2.57328
    Select on 10 fd's:           0.8554      0.8457  -1.133    0.8667 1.32101
    Select on 100 fd's:          3.5292      3.6329  2.9383    3.6664 3.88756
    Select on 250 fd's:          7.9097      8.1881  3.5197    8.2242 3.97613
    Select on 500 fd's:          15.2659     15.836  3.7357    15.873 3.97814
    Select on 10 tcp fd's:       0.9576      0.9416  -1.670    0.9752 1.83792
    Select on 100 tcp fd's:      7.248       7.2254  -0.311    7.2685 0.28283
    Select on 250 tcp fd's:      17.7742     17.707  -0.375    17.749 -0.1406
    Select on 500 tcp fd's:      35.4258     35.25   -0.496    35.286 -0.3929
    Signal handler installation: 0.6131      0.6075  -0.913    0.647  5.52927
    Signal handler overhead:     2.0919      2.1078  0.7600    2.1831 4.35967
    Protection fault:            0.7345      0.7478  1.8107    0.8031 9.33968
    Pipe latency:                33.006      16.398  -50.31    33.475 1.42368
    AF_UNIX sock stream latency: 14.5093     30.910  113.03    30.715 111.692
    Process fork+exit:           219.8       222.8   1.3648    229.37 4.35623
    Process fork+execve:         876.14      873.28  -0.32     868.66 -0.8533
    Process fork+/bin/sh -c:     2830        2876.5  1.6431    2958   4.52296
    File /var/tmp/XXX write bw:  1193497     1195536 0.1708    118657 -0.5799
    Pagefaults on /var/tmp/XXX:  3.1272      3.2117  2.7020    3.2521 3.99398
    
    Also, kernel compile times show no difference with this patch applied.
    
    [pbadari@us.ibm.com: Avoid unnecessary PURR reading]
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 863a5d6d9b18..9eb3284deac4 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -211,24 +211,45 @@ static u64 read_purr(void)
 	return mftb();
 }
 
+/*
+ * Read the SPURR on systems that have it, otherwise the purr
+ */
+static u64 read_spurr(u64 purr)
+{
+	if (cpu_has_feature(CPU_FTR_SPURR))
+		return mfspr(SPRN_SPURR);
+	return purr;
+}
+
 /*
  * Account time for a transition between system, hard irq
  * or soft irq state.
  */
 void account_system_vtime(struct task_struct *tsk)
 {
-	u64 now, delta;
+	u64 now, nowscaled, delta, deltascaled;
 	unsigned long flags;
 
 	local_irq_save(flags);
 	now = read_purr();
 	delta = now - get_paca()->startpurr;
 	get_paca()->startpurr = now;
+	nowscaled = read_spurr(now);
+	deltascaled = nowscaled - get_paca()->startspurr;
+	get_paca()->startspurr = nowscaled;
 	if (!in_interrupt()) {
+		/* deltascaled includes both user and system time.
+		 * Hence scale it based on the purr ratio to estimate
+		 * the system time */
+		deltascaled = deltascaled * get_paca()->system_time /
+			(get_paca()->system_time + get_paca()->user_time);
 		delta += get_paca()->system_time;
 		get_paca()->system_time = 0;
 	}
 	account_system_time(tsk, 0, delta);
+	get_paca()->purrdelta = delta;
+	account_system_time_scaled(tsk, deltascaled);
+	get_paca()->spurrdelta = deltascaled;
 	local_irq_restore(flags);
 }
 
@@ -240,11 +261,17 @@ void account_system_vtime(struct task_struct *tsk)
  */
 void account_process_vtime(struct task_struct *tsk)
 {
-	cputime_t utime;
+	cputime_t utime, utimescaled;
 
 	utime = get_paca()->user_time;
 	get_paca()->user_time = 0;
 	account_user_time(tsk, utime);
+
+	/* Estimate the scaled utime by scaling the real utime based
+	 * on the last spurr to purr ratio */
+	utimescaled = utime * get_paca()->spurrdelta / get_paca()->purrdelta;
+	get_paca()->spurrdelta = get_paca()->purrdelta = 0;
+	account_user_time_scaled(tsk, utimescaled);
 }
 
 static void account_process_time(struct pt_regs *regs)
@@ -266,6 +293,7 @@ struct cpu_purr_data {
 	int	initialized;			/* thread is running */
 	u64	tb;			/* last TB value read */
 	u64	purr;			/* last PURR value read */
+	u64	spurr;			/* last SPURR value read */
 };
 
 /*

commit 1281c8bef8e85d3d6b114eab945b7e0445a3886d
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Oct 15 05:18:46 2007 +1000

    [POWERPC] Quieten clockevent printk
    
    The clockevent bootup message only needs to be KERN_INFO.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 9368da371f36..863a5d6d9b18 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -829,7 +829,7 @@ static void register_decrementer_clockevent(int cpu)
 	*dec = decrementer_clockevent;
 	dec->cpumask = cpumask_of_cpu(cpu);
 
-	printk(KERN_ERR "clockevent: %s mult[%lx] shift[%d] cpu[%d]\n",
+	printk(KERN_INFO "clockevent: %s mult[%lx] shift[%d] cpu[%d]\n",
 	       dec->name, dec->mult, dec->shift, cpu);
 
 	clockevents_register_device(dec);

commit cdec12aebe1b10aa58bebaa05bb697843154f7f9
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 11 21:46:45 2007 +1000

    [POWERPC] Make clockevents work on PPC601 processors
    
    In testing the new clocksource and clockevent code on a PPC601
    processor, I discovered that the clockevent multiplier value for the
    decrementer clockevent was overflowing.  Because the RTCL register in
    the 601 effectively counts at 1GHz (it doesn't actually, but it
    increases by 128 every 128ns), and the shift value was 32, that meant
    the multiplier value had to be 2^32, which won't fit in an unsigned
    long on 32-bit.  The same problem would arise on any platform where
    the timebase frequency was 1GHz or more (not that we actually have any
    such machines today).
    
    This fixes it by reducing the shift value to 16.  Doing the
    calculations with a resolution of 2^-16 nanoseconds (15 femtoseconds)
    should be quite adequate.  :)
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 64b503c82a31..9368da371f36 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -108,7 +108,7 @@ static void decrementer_set_mode(enum clock_event_mode mode,
 static struct clock_event_device decrementer_clockevent = {
        .name           = "decrementer",
        .rating         = 200,
-       .shift          = 32,
+       .shift          = 16,
        .mult           = 0,	/* To be filled in */
        .irq            = 0,
        .set_next_event = decrementer_set_next_event,

commit d968014b7280e2c447b20363e576999040ac72ef
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Oct 9 09:59:17 2007 +1000

    [POWERPC] Prevent decrementer clockevents from firing early
    
    On old powermacs, we sometimes set the decrementer to 1 in order to
    trigger a decrementer interrupt, which we use to handle an interrupt
    that was pending at the time when it was re-enabled.  This was causing
    the decrementer clock event device to call the event function for the
    next event early, which was causing problems when high-res timers were
    not enabled.
    
    This fixes the problem by recording the timebase value at which the
    next event should occur, and checking the current timebase against the
    recorded value in timer_interrupt.  If it isn't time for the next
    event, it just reprograms the decrementer and returns.
    
    This also subtracts 1 from the value stored into the decrementer,
    which is appropriate because the decrementer interrupts on the
    transition from 0 to -1, not when the decrementer reaches 0.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index d20947cf1735..64b503c82a31 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -118,6 +118,7 @@ static struct clock_event_device decrementer_clockevent = {
 
 static DEFINE_PER_CPU(struct clock_event_device, decrementers);
 void init_decrementer_clockevent(void);
+static DEFINE_PER_CPU(u64, decrementer_next_tb);
 
 #ifdef CONFIG_PPC_ISERIES
 static unsigned long __initdata iSeries_recal_titan;
@@ -541,6 +542,7 @@ void timer_interrupt(struct pt_regs * regs)
 	struct pt_regs *old_regs;
 	int cpu = smp_processor_id();
 	struct clock_event_device *evt = &per_cpu(decrementers, cpu);
+	u64 now;
 
 	/* Ensure a positive value is written to the decrementer, or else
 	 * some CPUs will continuue to take decrementer exceptions */
@@ -551,6 +553,14 @@ void timer_interrupt(struct pt_regs * regs)
 		do_IRQ(regs);
 #endif
 
+	now = get_tb_or_rtc();
+	if (now < per_cpu(decrementer_next_tb, cpu)) {
+		/* not time for this event yet */
+		now = per_cpu(decrementer_next_tb, cpu) - now;
+		if (now <= DECREMENTER_MAX)
+			set_dec((unsigned int)now - 1);
+		return;
+	}
 	old_regs = set_irq_regs(regs);
 	irq_enter();
 
@@ -797,6 +807,10 @@ void __init clocksource_init(void)
 static int decrementer_set_next_event(unsigned long evt,
 				      struct clock_event_device *dev)
 {
+	__get_cpu_var(decrementer_next_tb) = get_tb_or_rtc() + evt;
+	/* The decrementer interrupts on the 0 -> -1 transition */
+	if (evt)
+		--evt;
 	set_dec(evt);
 	return 0;
 }

commit d831d0b83f205888f4be4dee0a074ad67ef809b3
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Fri Sep 21 13:26:03 2007 +1000

    [POWERPC] Implement clockevents driver for powerpc
    
    This registers a clock event structure for the decrementer and turns
    on CONFIG_GENERIC_CLOCKEVENTS, which means that we now don't need
    most of timer_interrupt(), since the work is done in generic code.
    For secondary CPUs, their decrementer clockevent is registered when
    the CPU comes up (the generic code automatically removes the
    clockevent when the CPU goes down).
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index e71a0d8c597a..d20947cf1735 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -73,6 +73,7 @@
 
 /* powerpc clocksource/clockevent code */
 
+#include <linux/clockchips.h>
 #include <linux/clocksource.h>
 
 static cycle_t rtc_read(void);
@@ -97,6 +98,27 @@ static struct clocksource clocksource_timebase = {
 	.read         = timebase_read,
 };
 
+#define DECREMENTER_MAX	0x7fffffff
+
+static int decrementer_set_next_event(unsigned long evt,
+				      struct clock_event_device *dev);
+static void decrementer_set_mode(enum clock_event_mode mode,
+				 struct clock_event_device *dev);
+
+static struct clock_event_device decrementer_clockevent = {
+       .name           = "decrementer",
+       .rating         = 200,
+       .shift          = 32,
+       .mult           = 0,	/* To be filled in */
+       .irq            = 0,
+       .set_next_event = decrementer_set_next_event,
+       .set_mode       = decrementer_set_mode,
+       .features       = CLOCK_EVT_FEAT_ONESHOT,
+};
+
+static DEFINE_PER_CPU(struct clock_event_device, decrementers);
+void init_decrementer_clockevent(void);
+
 #ifdef CONFIG_PPC_ISERIES
 static unsigned long __initdata iSeries_recal_titan;
 static signed long __initdata iSeries_recal_tb;
@@ -517,10 +539,12 @@ void __init iSeries_time_init_early(void)
 void timer_interrupt(struct pt_regs * regs)
 {
 	struct pt_regs *old_regs;
-	int next_dec;
 	int cpu = smp_processor_id();
-	unsigned long ticks;
-	u64 tb_next_jiffy;
+	struct clock_event_device *evt = &per_cpu(decrementers, cpu);
+
+	/* Ensure a positive value is written to the decrementer, or else
+	 * some CPUs will continuue to take decrementer exceptions */
+	set_dec(DECREMENTER_MAX);
 
 #ifdef CONFIG_PPC32
 	if (atomic_read(&ppc_n_lost_interrupts) != 0)
@@ -530,7 +554,6 @@ void timer_interrupt(struct pt_regs * regs)
 	old_regs = set_irq_regs(regs);
 	irq_enter();
 
-	profile_tick(CPU_PROFILING);
 	calculate_steal_time();
 
 #ifdef CONFIG_PPC_ISERIES
@@ -538,44 +561,20 @@ void timer_interrupt(struct pt_regs * regs)
 		get_lppaca()->int_dword.fields.decr_int = 0;
 #endif
 
-	while ((ticks = tb_ticks_since(per_cpu(last_jiffy, cpu)))
-	       >= tb_ticks_per_jiffy) {
-		/* Update last_jiffy */
-		per_cpu(last_jiffy, cpu) += tb_ticks_per_jiffy;
-		/* Handle RTCL overflow on 601 */
-		if (__USE_RTC() && per_cpu(last_jiffy, cpu) >= 1000000000)
-			per_cpu(last_jiffy, cpu) -= 1000000000;
-
-		/*
-		 * We cannot disable the decrementer, so in the period
-		 * between this cpu's being marked offline in cpu_online_map
-		 * and calling stop-self, it is taking timer interrupts.
-		 * Avoid calling into the scheduler rebalancing code if this
-		 * is the case.
-		 */
-		if (!cpu_is_offline(cpu))
-			account_process_time(regs);
-
-		/*
-		 * No need to check whether cpu is offline here; boot_cpuid
-		 * should have been fixed up by now.
-		 */
-		if (cpu != boot_cpuid)
-			continue;
+	/*
+	 * We cannot disable the decrementer, so in the period
+	 * between this cpu's being marked offline in cpu_online_map
+	 * and calling stop-self, it is taking timer interrupts.
+	 * Avoid calling into the scheduler rebalancing code if this
+	 * is the case.
+	 */
+	if (!cpu_is_offline(cpu))
+		account_process_time(regs);
 
-		write_seqlock(&xtime_lock);
-		tb_next_jiffy = tb_last_jiffy + tb_ticks_per_jiffy;
-		if (__USE_RTC() && tb_next_jiffy >= 1000000000)
-			tb_next_jiffy -= 1000000000;
-		if (per_cpu(last_jiffy, cpu) >= tb_next_jiffy) {
-			tb_last_jiffy = tb_next_jiffy;
-			do_timer(1);
-		}
-		write_sequnlock(&xtime_lock);
-	}
-	
-	next_dec = tb_ticks_per_jiffy - ticks;
-	set_dec(next_dec);
+	if (evt->event_handler)
+		evt->event_handler(evt);
+	else
+		evt->set_next_event(DECREMENTER_MAX, evt);
 
 #ifdef CONFIG_PPC_ISERIES
 	if (firmware_has_feature(FW_FEATURE_ISERIES) && hvlpevent_is_pending())
@@ -795,6 +794,53 @@ void __init clocksource_init(void)
 	       clock->name, clock->mult, clock->shift);
 }
 
+static int decrementer_set_next_event(unsigned long evt,
+				      struct clock_event_device *dev)
+{
+	set_dec(evt);
+	return 0;
+}
+
+static void decrementer_set_mode(enum clock_event_mode mode,
+				 struct clock_event_device *dev)
+{
+	if (mode != CLOCK_EVT_MODE_ONESHOT)
+		decrementer_set_next_event(DECREMENTER_MAX, dev);
+}
+
+static void register_decrementer_clockevent(int cpu)
+{
+	struct clock_event_device *dec = &per_cpu(decrementers, cpu);
+
+	*dec = decrementer_clockevent;
+	dec->cpumask = cpumask_of_cpu(cpu);
+
+	printk(KERN_ERR "clockevent: %s mult[%lx] shift[%d] cpu[%d]\n",
+	       dec->name, dec->mult, dec->shift, cpu);
+
+	clockevents_register_device(dec);
+}
+
+void init_decrementer_clockevent(void)
+{
+	int cpu = smp_processor_id();
+
+	decrementer_clockevent.mult = div_sc(ppc_tb_freq, NSEC_PER_SEC,
+					     decrementer_clockevent.shift);
+	decrementer_clockevent.max_delta_ns =
+		clockevent_delta2ns(DECREMENTER_MAX, &decrementer_clockevent);
+	decrementer_clockevent.min_delta_ns = 1000;
+
+	register_decrementer_clockevent(cpu);
+}
+
+void secondary_cpu_time_init(void)
+{
+	/* FIME: Should make unrelatred change to move snapshot_timebase
+	 * call here ! */
+	register_decrementer_clockevent(smp_processor_id());
+}
+
 /* This function is only called on the boot processor */
 void __init time_init(void)
 {
@@ -908,8 +954,7 @@ void __init time_init(void)
 	if (!firmware_has_feature(FW_FEATURE_ISERIES))
 		clocksource_init();
 
-	/* Not exact, but the timer interrupt takes care of this */
-	set_dec(tb_ticks_per_jiffy);
+	init_decrementer_clockevent();
 }
 
 

commit 4a4cfe3836916e12282ceb5c4bdd799dc71af567
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Sat Sep 22 07:35:52 2007 +1000

    [POWERPC] Implement generic time of day clocksource for powerpc
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index b94e4dffba19..e71a0d8c597a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -65,17 +65,44 @@
 #include <asm/div64.h>
 #include <asm/smp.h>
 #include <asm/vdso_datapage.h>
-#ifdef CONFIG_PPC64
 #include <asm/firmware.h>
-#endif
 #ifdef CONFIG_PPC_ISERIES
 #include <asm/iseries/it_lp_queue.h>
 #include <asm/iseries/hv_call_xm.h>
 #endif
 
+/* powerpc clocksource/clockevent code */
+
+#include <linux/clocksource.h>
+
+static cycle_t rtc_read(void);
+static struct clocksource clocksource_rtc = {
+	.name         = "rtc",
+	.rating       = 400,
+	.flags        = CLOCK_SOURCE_IS_CONTINUOUS,
+	.mask         = CLOCKSOURCE_MASK(64),
+	.shift        = 22,
+	.mult         = 0,	/* To be filled in */
+	.read         = rtc_read,
+};
+
+static cycle_t timebase_read(void);
+static struct clocksource clocksource_timebase = {
+	.name         = "timebase",
+	.rating       = 400,
+	.flags        = CLOCK_SOURCE_IS_CONTINUOUS,
+	.mask         = CLOCKSOURCE_MASK(64),
+	.shift        = 22,
+	.mult         = 0,	/* To be filled in */
+	.read         = timebase_read,
+};
+
 #ifdef CONFIG_PPC_ISERIES
 static unsigned long __initdata iSeries_recal_titan;
 static signed long __initdata iSeries_recal_tb;
+
+/* Forward declaration is only needed for iSereis compiles */
+void __init clocksource_init(void);
 #endif
 
 #define XSEC_PER_SEC (1024*1024)
@@ -343,65 +370,6 @@ void udelay(unsigned long usecs)
 }
 EXPORT_SYMBOL(udelay);
 
-/*
- * This version of gettimeofday has microsecond resolution.
- */
-static inline void __do_gettimeofday(struct timeval *tv)
-{
-	unsigned long sec, usec;
-	u64 tb_ticks, xsec;
-	struct gettimeofday_vars *temp_varp;
-	u64 temp_tb_to_xs, temp_stamp_xsec;
-
-	/*
-	 * These calculations are faster (gets rid of divides)
-	 * if done in units of 1/2^20 rather than microseconds.
-	 * The conversion to microseconds at the end is done
-	 * without a divide (and in fact, without a multiply)
-	 */
-	temp_varp = do_gtod.varp;
-
-	/* Sampling the time base must be done after loading
-	 * do_gtod.varp in order to avoid racing with update_gtod.
-	 */
-	data_barrier(temp_varp);
-	tb_ticks = get_tb() - temp_varp->tb_orig_stamp;
-	temp_tb_to_xs = temp_varp->tb_to_xs;
-	temp_stamp_xsec = temp_varp->stamp_xsec;
-	xsec = temp_stamp_xsec + mulhdu(tb_ticks, temp_tb_to_xs);
-	sec = xsec / XSEC_PER_SEC;
-	usec = (unsigned long)xsec & (XSEC_PER_SEC - 1);
-	usec = SCALE_XSEC(usec, 1000000);
-
-	tv->tv_sec = sec;
-	tv->tv_usec = usec;
-}
-
-void do_gettimeofday(struct timeval *tv)
-{
-	if (__USE_RTC()) {
-		/* do this the old way */
-		unsigned long flags, seq;
-		unsigned int sec, nsec, usec;
-
-		do {
-			seq = read_seqbegin_irqsave(&xtime_lock, flags);
-			sec = xtime.tv_sec;
-			nsec = xtime.tv_nsec + tb_ticks_since(tb_last_jiffy);
-		} while (read_seqretry_irqrestore(&xtime_lock, seq, flags));
-		usec = nsec / 1000;
-		while (usec >= 1000000) {
-			usec -= 1000000;
-			++sec;
-		}
-		tv->tv_sec = sec;
-		tv->tv_usec = usec;
-		return;
-	}
-	__do_gettimeofday(tv);
-}
-
-EXPORT_SYMBOL(do_gettimeofday);
 
 /*
  * There are two copies of tb_to_xs and stamp_xsec so that no
@@ -447,56 +415,6 @@ static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
 	++(vdso_data->tb_update_count);
 }
 
-/*
- * When the timebase - tb_orig_stamp gets too big, we do a manipulation
- * between tb_orig_stamp and stamp_xsec. The goal here is to keep the
- * difference tb - tb_orig_stamp small enough to always fit inside a
- * 32 bits number. This is a requirement of our fast 32 bits userland
- * implementation in the vdso. If we "miss" a call to this function
- * (interrupt latency, CPU locked in a spinlock, ...) and we end up
- * with a too big difference, then the vdso will fallback to calling
- * the syscall
- */
-static __inline__ void timer_recalc_offset(u64 cur_tb)
-{
-	unsigned long offset;
-	u64 new_stamp_xsec;
-	u64 tlen, t2x;
-	u64 tb, xsec_old, xsec_new;
-	struct gettimeofday_vars *varp;
-
-	if (__USE_RTC())
-		return;
-	tlen = current_tick_length();
-	offset = cur_tb - do_gtod.varp->tb_orig_stamp;
-	if (tlen == last_tick_len && offset < 0x80000000u)
-		return;
-	if (tlen != last_tick_len) {
-		t2x = mulhdu(tlen << TICKLEN_SHIFT, ticklen_to_xs);
-		last_tick_len = tlen;
-	} else
-		t2x = do_gtod.varp->tb_to_xs;
-	new_stamp_xsec = (u64) xtime.tv_nsec * XSEC_PER_SEC;
-	do_div(new_stamp_xsec, 1000000000);
-	new_stamp_xsec += (u64) xtime.tv_sec * XSEC_PER_SEC;
-
-	++vdso_data->tb_update_count;
-	smp_mb();
-
-	/*
-	 * Make sure time doesn't go backwards for userspace gettimeofday.
-	 */
-	tb = get_tb();
-	varp = do_gtod.varp;
-	xsec_old = mulhdu(tb - varp->tb_orig_stamp, varp->tb_to_xs)
-		+ varp->stamp_xsec;
-	xsec_new = mulhdu(tb - cur_tb, t2x) + new_stamp_xsec;
-	if (xsec_new < xsec_old)
-		new_stamp_xsec += xsec_old - xsec_new;
-
-	update_gtod(cur_tb, new_stamp_xsec, t2x);
-}
-
 #ifdef CONFIG_SMP
 unsigned long profile_pc(struct pt_regs *regs)
 {
@@ -568,6 +486,8 @@ static int __init iSeries_tb_recal(void)
 	iSeries_recal_titan = titan;
 	iSeries_recal_tb = tb;
 
+	/* Called here as now we know accurate values for the timebase */
+	clocksource_init();
 	return 0;
 }
 late_initcall(iSeries_tb_recal);
@@ -650,7 +570,6 @@ void timer_interrupt(struct pt_regs * regs)
 		if (per_cpu(last_jiffy, cpu) >= tb_next_jiffy) {
 			tb_last_jiffy = tb_next_jiffy;
 			do_timer(1);
-			timer_recalc_offset(tb_last_jiffy);
 		}
 		write_sequnlock(&xtime_lock);
 	}
@@ -722,66 +641,6 @@ unsigned long long sched_clock(void)
 	return mulhdu(get_tb() - boot_tb, tb_to_ns_scale) << tb_to_ns_shift;
 }
 
-int do_settimeofday(struct timespec *tv)
-{
-	time_t wtm_sec, new_sec = tv->tv_sec;
-	long wtm_nsec, new_nsec = tv->tv_nsec;
-	unsigned long flags;
-	u64 new_xsec;
-	unsigned long tb_delta;
-
-	if ((unsigned long)tv->tv_nsec >= NSEC_PER_SEC)
-		return -EINVAL;
-
-	write_seqlock_irqsave(&xtime_lock, flags);
-
-	/*
-	 * Updating the RTC is not the job of this code. If the time is
-	 * stepped under NTP, the RTC will be updated after STA_UNSYNC
-	 * is cleared.  Tools like clock/hwclock either copy the RTC
-	 * to the system time, in which case there is no point in writing
-	 * to the RTC again, or write to the RTC but then they don't call
-	 * settimeofday to perform this operation.
-	 */
-
-	/* Make userspace gettimeofday spin until we're done. */
-	++vdso_data->tb_update_count;
-	smp_mb();
-
-	/*
-	 * Subtract off the number of nanoseconds since the
-	 * beginning of the last tick.
-	 */
-	tb_delta = tb_ticks_since(tb_last_jiffy);
-	tb_delta = mulhdu(tb_delta, do_gtod.varp->tb_to_xs); /* in xsec */
-	new_nsec -= SCALE_XSEC(tb_delta, 1000000000);
-
-	wtm_sec  = wall_to_monotonic.tv_sec + (xtime.tv_sec - new_sec);
-	wtm_nsec = wall_to_monotonic.tv_nsec + (xtime.tv_nsec - new_nsec);
-
- 	set_normalized_timespec(&xtime, new_sec, new_nsec);
-	set_normalized_timespec(&wall_to_monotonic, wtm_sec, wtm_nsec);
-
-	ntp_clear();
-
-	new_xsec = xtime.tv_nsec;
-	if (new_xsec != 0) {
-		new_xsec *= XSEC_PER_SEC;
-		do_div(new_xsec, NSEC_PER_SEC);
-	}
-	new_xsec += (u64)xtime.tv_sec * XSEC_PER_SEC;
-	update_gtod(tb_last_jiffy, new_xsec, do_gtod.varp->tb_to_xs);
-
-	vdso_data->tz_minuteswest = sys_tz.tz_minuteswest;
-	vdso_data->tz_dsttime = sys_tz.tz_dsttime;
-
-	write_sequnlock_irqrestore(&xtime_lock, flags);
-	clock_was_set();
-	return 0;
-}
-
-EXPORT_SYMBOL(do_settimeofday);
-
 static int __init get_freq(char *name, int cells, unsigned long *val)
 {
 	struct device_node *cpu;
@@ -873,6 +732,69 @@ unsigned long read_persistent_clock(void)
 		      tm.tm_hour, tm.tm_min, tm.tm_sec);
 }
 
+/* clocksource code */
+static cycle_t rtc_read(void)
+{
+	return (cycle_t)get_rtc();
+}
+
+static cycle_t timebase_read(void)
+{
+	return (cycle_t)get_tb();
+}
+
+void update_vsyscall(struct timespec *wall_time, struct clocksource *clock)
+{
+	u64 t2x, stamp_xsec;
+
+	if (clock != &clocksource_timebase)
+		return;
+
+	/* Make userspace gettimeofday spin until we're done. */
+	++vdso_data->tb_update_count;
+	smp_mb();
+
+	/* XXX this assumes clock->shift == 22 */
+	/* 4611686018 ~= 2^(20+64-22) / 1e9 */
+	t2x = (u64) clock->mult * 4611686018ULL;
+	stamp_xsec = (u64) xtime.tv_nsec * XSEC_PER_SEC;
+	do_div(stamp_xsec, 1000000000);
+	stamp_xsec += (u64) xtime.tv_sec * XSEC_PER_SEC;
+	update_gtod(clock->cycle_last, stamp_xsec, t2x);
+}
+
+void update_vsyscall_tz(void)
+{
+	/* Make userspace gettimeofday spin until we're done. */
+	++vdso_data->tb_update_count;
+	smp_mb();
+	vdso_data->tz_minuteswest = sys_tz.tz_minuteswest;
+	vdso_data->tz_dsttime = sys_tz.tz_dsttime;
+	smp_mb();
+	++vdso_data->tb_update_count;
+}
+
+void __init clocksource_init(void)
+{
+	struct clocksource *clock;
+
+	if (__USE_RTC())
+		clock = &clocksource_rtc;
+	else
+		clock = &clocksource_timebase;
+
+	clock->mult = clocksource_hz2mult(tb_ticks_per_sec, clock->shift);
+
+	if (clocksource_register(clock)) {
+		printk(KERN_ERR "clocksource: %s is already registered\n",
+		       clock->name);
+		return;
+	}
+
+	printk(KERN_INFO "clocksource: %s mult[%x] shift[%d] registered\n",
+	       clock->name, clock->mult, clock->shift);
+}
+
 /* This function is only called on the boot processor */
 void __init time_init(void)
 {
@@ -982,6 +904,10 @@ void __init time_init(void)
 
 	write_sequnlock_irqrestore(&xtime_lock, flags);
 
+	/* Register the clocksource, if we're not running on iSeries */
+	if (!firmware_has_feature(FW_FEATURE_ISERIES))
+		clocksource_init();
+
 	/* Not exact, but the timer interrupt takes care of this */
 	set_dec(tb_ticks_per_jiffy);
 }

commit aa3be5f32db137bc4404f32a24b36fb47d48d260
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Fri Sep 21 13:26:02 2007 +1000

    [POWERPC] Implement {read,update}_persistent_clock
    
    With these functions implemented we cooperate better with the generic
    timekeeping code.  This obsoletes the need for the timer sysdev as a bonus.
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index d95e68c0a6b3..b94e4dffba19 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -73,16 +73,11 @@
 #include <asm/iseries/hv_call_xm.h>
 #endif
 
-/* keep track of when we need to update the rtc */
-time_t last_rtc_update;
 #ifdef CONFIG_PPC_ISERIES
 static unsigned long __initdata iSeries_recal_titan;
 static signed long __initdata iSeries_recal_tb;
 #endif
 
-/* The decrementer counts down by 128 every 128ns on a 601. */
-#define DECREMENTER_COUNT_601	(1000000000 / HZ)
-
 #define XSEC_PER_SEC (1024*1024)
 
 #ifdef CONFIG_PPC64
@@ -348,39 +343,6 @@ void udelay(unsigned long usecs)
 }
 EXPORT_SYMBOL(udelay);
 
-static __inline__ void timer_check_rtc(void)
-{
-        /*
-         * update the rtc when needed, this should be performed on the
-         * right fraction of a second. Half or full second ?
-         * Full second works on mk48t59 clocks, others need testing.
-         * Note that this update is basically only used through 
-         * the adjtimex system calls. Setting the HW clock in
-         * any other way is a /dev/rtc and userland business.
-         * This is still wrong by -0.5/+1.5 jiffies because of the
-         * timer interrupt resolution and possible delay, but here we 
-         * hit a quantization limit which can only be solved by higher
-         * resolution timers and decoupling time management from timer
-         * interrupts. This is also wrong on the clocks
-         * which require being written at the half second boundary.
-         * We should have an rtc call that only sets the minutes and
-         * seconds like on Intel to avoid problems with non UTC clocks.
-         */
-        if (ppc_md.set_rtc_time && ntp_synced() &&
-	    xtime.tv_sec - last_rtc_update >= 659 &&
-	    abs((xtime.tv_nsec/1000) - (1000000-1000000/HZ)) < 500000/HZ) {
-		struct rtc_time tm;
-		to_tm(xtime.tv_sec + 1 + timezone_offset, &tm);
-		tm.tm_year -= 1900;
-		tm.tm_mon -= 1;
-		if (ppc_md.set_rtc_time(&tm) == 0)
-			last_rtc_update = xtime.tv_sec + 1;
-		else
-			/* Try again one minute later */
-			last_rtc_update += 60;
-        }
-}
-
 /*
  * This version of gettimeofday has microsecond resolution.
  */
@@ -689,7 +651,6 @@ void timer_interrupt(struct pt_regs * regs)
 			tb_last_jiffy = tb_next_jiffy;
 			do_timer(1);
 			timer_recalc_offset(tb_last_jiffy);
-			timer_check_rtc();
 		}
 		write_sequnlock(&xtime_lock);
 	}
@@ -801,11 +762,6 @@ int do_settimeofday(struct timespec *tv)
  	set_normalized_timespec(&xtime, new_sec, new_nsec);
 	set_normalized_timespec(&wall_to_monotonic, wtm_sec, wtm_nsec);
 
-	/* In case of a large backwards jump in time with NTP, we want the 
-	 * clock to be updated as soon as the PLL is again in lock.
-	 */
-	last_rtc_update = new_sec - 658;
-
 	ntp_clear();
 
 	new_xsec = xtime.tv_nsec;
@@ -881,12 +837,35 @@ void __init generic_calibrate_decr(void)
 #endif
 }
 
-unsigned long get_boot_time(void)
+int update_persistent_clock(struct timespec now)
+{
+	struct rtc_time tm;
+
+	if (!ppc_md.set_rtc_time)
+		return 0;
+
+	to_tm(now.tv_sec + 1 + timezone_offset, &tm);
+	tm.tm_year -= 1900;
+	tm.tm_mon -= 1;
+
+	return ppc_md.set_rtc_time(&tm);
+}
+
+unsigned long read_persistent_clock(void)
 {
 	struct rtc_time tm;
+	static int first = 1;
+
+	/* XXX this is a litle fragile but will work okay in the short term */
+	if (first) {
+		first = 0;
+		if (ppc_md.time_init)
+			timezone_offset = ppc_md.time_init();
 
-	if (ppc_md.get_boot_time)
-		return ppc_md.get_boot_time();
+		/* get_boot_time() isn't guaranteed to be safe to call late */
+		if (ppc_md.get_boot_time)
+			return ppc_md.get_boot_time() -timezone_offset;
+	}
 	if (!ppc_md.get_rtc_time)
 		return 0;
 	ppc_md.get_rtc_time(&tm);
@@ -898,14 +877,10 @@ unsigned long get_boot_time(void)
 void __init time_init(void)
 {
 	unsigned long flags;
-	unsigned long tm = 0;
 	struct div_result res;
 	u64 scale, x;
 	unsigned shift;
 
-        if (ppc_md.time_init != NULL)
-                timezone_offset = ppc_md.time_init();
-
 	if (__USE_RTC()) {
 		/* 601 processor: dec counts down by 128 every 128ns */
 		ppc_tb_freq = 1000000000;
@@ -980,19 +955,14 @@ void __init time_init(void)
 	/* Save the current timebase to pretty up CONFIG_PRINTK_TIME */
 	boot_tb = get_tb_or_rtc();
 
-	tm = get_boot_time();
-
 	write_seqlock_irqsave(&xtime_lock, flags);
 
 	/* If platform provided a timezone (pmac), we correct the time */
         if (timezone_offset) {
 		sys_tz.tz_minuteswest = -timezone_offset / 60;
 		sys_tz.tz_dsttime = 0;
-		tm -= timezone_offset;
         }
 
-	xtime.tv_sec = tm;
-	xtime.tv_nsec = 0;
 	do_gtod.varp = &do_gtod.vars[0];
 	do_gtod.var_idx = 0;
 	do_gtod.varp->tb_orig_stamp = tb_last_jiffy;
@@ -1010,9 +980,6 @@ void __init time_init(void)
 
 	time_freq = 0;
 
-	last_rtc_update = xtime.tv_sec;
-	set_normalized_timespec(&wall_to_monotonic,
-	                        -xtime.tv_sec, -xtime.tv_nsec);
 	write_sequnlock_irqrestore(&xtime_lock, flags);
 
 	/* Not exact, but the timer interrupt takes care of this */

commit 0ce49a3945474fc942ec37c0c0efece60f592f80
Merge: 9e4859ef5462 a88a8eff1e6e
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Sep 20 10:09:27 2007 +1000

    Merge branch 'linux-2.6'

commit c27da339698145a9383e052c1070a950d30da478
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Sep 19 14:21:56 2007 +1000

    [POWERPC] Fix timekeeping on PowerPC 601
    
    Recent changes to the timekeeping code broke support for the PowerPC 601
    processor which doesn't have the usual timebase facility but a slightly
    different thing called (yuck) the RTC.
    
    This fixes it, boot tested on an old 601 based PowerMac 7200.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 727a6699f2f4..c627cf86d1e3 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -239,7 +239,7 @@ static void snapshot_tb_and_purr(void *data)
 	struct cpu_purr_data *p = &__get_cpu_var(cpu_purr_data);
 
 	local_irq_save(flags);
-	p->tb = mftb();
+	p->tb = get_tb_or_rtc();
 	p->purr = mfspr(SPRN_PURR);
 	wmb();
 	p->initialized = 1;
@@ -317,7 +317,7 @@ static void snapshot_purr(void)
  */
 void snapshot_timebase(void)
 {
-	__get_cpu_var(last_jiffy) = get_tb();
+	__get_cpu_var(last_jiffy) = get_tb_or_rtc();
 	snapshot_purr();
 }
 
@@ -684,6 +684,8 @@ void timer_interrupt(struct pt_regs * regs)
 
 		write_seqlock(&xtime_lock);
 		tb_next_jiffy = tb_last_jiffy + tb_ticks_per_jiffy;
+		if (__USE_RTC() && tb_next_jiffy >= 1000000000)
+			tb_next_jiffy -= 1000000000;
 		if (per_cpu(last_jiffy, cpu) >= tb_next_jiffy) {
 			tb_last_jiffy = tb_next_jiffy;
 			do_timer(1);
@@ -977,7 +979,7 @@ void __init time_init(void)
 	tb_to_ns_scale = scale;
 	tb_to_ns_shift = shift;
 	/* Save the current timebase to pretty up CONFIG_PRINTK_TIME */
-	boot_tb = get_tb();
+	boot_tb = get_tb_or_rtc();
 
 	tm = get_boot_time();
 

commit aab69292e4efd38181cd300d9b83b12592643d6c
Author: Josh Boyer <jwboyer@linux.vnet.ibm.com>
Date:   Mon Aug 20 07:29:11 2007 -0500

    [POWERPC] 40x decrementer fixes
    
    Allow generic_calibrate_decr to work for 40x platforms.  Given that the hardware
    behavior is identical, this also changes the set_dec function to reload the PIT
    on 40x to match the behavior 44x currently has.
    
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index c85d9b0d9042..b5944d8e3802 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -866,7 +866,7 @@ void __init generic_calibrate_decr(void)
 				"(not found)\n");
 	}
 
-#ifdef CONFIG_BOOKE
+#if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
 	/* Set the time base to zero */
 	mtspr(SPRN_TBWL, 0);
 	mtspr(SPRN_TBWU, 0);

commit 9420dc65ff9e6b67c032286efde823aeb8684670
Author: Jesper Juhl <jesper.juhl@gmail.com>
Date:   Mon Jul 30 08:18:25 2007 +1000

    [POWERPC] Clean out a bunch of duplicate includes
    
    This removes several duplicate includes from arch/powerpc/.
    
    Signed-off-by: Jesper Juhl <jesper.juhl@gmail.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 727a6699f2f4..c85d9b0d9042 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -72,7 +72,6 @@
 #include <asm/iseries/it_lp_queue.h>
 #include <asm/iseries/hv_call_xm.h>
 #endif
-#include <asm/smp.h>
 
 /* keep track of when we need to update the rtc */
 time_t last_rtc_update;

commit 1474855d0878cced6f39f51f3c2bd7428b44cb1e
Author: Bob Nelson <rrnelson@linux.vnet.ibm.com>
Date:   Fri Jul 20 21:39:53 2007 +0200

    [CELL] oprofile: add support to OProfile for profiling CELL BE SPUs
    
    From: Maynard Johnson <mpjohn@us.ibm.com>
    
    This patch updates the existing arch/powerpc/oprofile/op_model_cell.c
    to add in the SPU profiling capabilities.  In addition, a 'cell' subdirectory
    was added to arch/powerpc/oprofile to hold Cell-specific SPU profiling code.
    Exports spu_set_profile_private_kref and spu_get_profile_private_kref which
    are used by OProfile to store private profile information in spufs data
    structures.
    
    Also incorporated several fixes from other patches (rrn).  Check pointer
    returned from kzalloc.  Eliminated unnecessary cast.  Better error
    handling and cleanup in the related area.  64-bit unsigned long parameter
    was being demoted to 32-bit unsigned int and eventually promoted back to
    unsigned long.
    
    Signed-off-by: Carl Love <carll@us.ibm.com>
    Signed-off-by: Maynard Johnson <mpjohn@us.ibm.com>
    Signed-off-by: Bob Nelson <rrnelson@us.ibm.com>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index e5df167f7824..727a6699f2f4 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -122,6 +122,7 @@ extern struct timezone sys_tz;
 static long timezone_offset;
 
 unsigned long ppc_proc_freq;
+EXPORT_SYMBOL(ppc_proc_freq);
 unsigned long ppc_tb_freq;
 
 static u64 tb_last_jiffy __cacheline_aligned_in_smp;

commit fc9069fe56d9594b7a6da0b9f9ca0eafd756260f
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Wed Jul 4 14:04:31 2007 +1000

    [POWERPC] Modify sched_clock() to make CONFIG_PRINTK_TIME more sane
    
    When booting a current kernel with CONFIG_PRINTK_TIME enabled you'll
    see messages like:
    
    [    0.000000] time_init: decrementer frequency = 188.044000 MHz
    [    0.000000] time_init: processor frequency   = 1504.352000 MHz
    [3712914.436297] Console: colour dummy device 80x25
    
    This cause by the initialisation of tb_to_ns_scale in time_init(), suddenly the
    multiplication in sched_clock() now does something :).  This patch modifies
    sched_clock() to report the offset since the machine booted so the same
    printk's now look like:
    
    [    0.000000] time_init: decrementer frequency = 188.044000 MHz
    [    0.000000] time_init: processor frequency   = 1504.352000 MHz
    [    0.000135] Console: colour dummy device 80x25
    
    Effectivly including the uptime in printk()s.
    
    This patch makes tb_to_ns_scale and tb_to_ns_shift static and
    read_mostly for good measure.
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 66d2db7495aa..e5df167f7824 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -112,8 +112,9 @@ u64 ticklen_to_xs;	/* 0.64 fraction */
 DEFINE_SPINLOCK(rtc_lock);
 EXPORT_SYMBOL_GPL(rtc_lock);
 
-u64 tb_to_ns_scale;
-unsigned tb_to_ns_shift;
+static u64 tb_to_ns_scale __read_mostly;
+static unsigned tb_to_ns_shift __read_mostly;
+static unsigned long boot_tb __read_mostly;
 
 struct gettimeofday_struct do_gtod;
 
@@ -755,7 +756,7 @@ unsigned long long sched_clock(void)
 {
 	if (__USE_RTC())
 		return get_rtc();
-	return mulhdu(get_tb(), tb_to_ns_scale) << tb_to_ns_shift;
+	return mulhdu(get_tb() - boot_tb, tb_to_ns_scale) << tb_to_ns_shift;
 }
 
 int do_settimeofday(struct timespec *tv)
@@ -974,6 +975,8 @@ void __init time_init(void)
 	}
 	tb_to_ns_scale = scale;
 	tb_to_ns_shift = shift;
+	/* Save the current timebase to pretty up CONFIG_PRINTK_TIME */
+	boot_tb = get_tb();
 
 	tm = get_boot_time();
 

commit 71712b455374a73af042fcfb5002fef5fd25ba44
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Fri Jun 22 16:54:30 2007 +1000

    [POWERPC] Move iSeries_tb_recal into its own late_initcall.
    
    Currently iSeries will recalibrate the cputime_factors in the first
    settimeofday() call.
    
    It seems the reason for doing this is to ensure a resaonable time delta after
    time_init().  On current kernels (with udev), this call is made 40-60 seconds
    into the boot process, by moving it to a late initcall it is called
    approximately 5 seconds after time_init() is called.  This is sufficient to
    recalibrate the timebase.
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    CC: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 43c687a1d76e..66d2db7495aa 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -77,9 +77,8 @@
 /* keep track of when we need to update the rtc */
 time_t last_rtc_update;
 #ifdef CONFIG_PPC_ISERIES
-unsigned long iSeries_recal_titan = 0;
-unsigned long iSeries_recal_tb = 0; 
-static unsigned long first_settimeofday = 1;
+static unsigned long __initdata iSeries_recal_titan;
+static signed long __initdata iSeries_recal_tb;
 #endif
 
 /* The decrementer counts down by 128 every 128ns on a 601. */
@@ -556,10 +555,15 @@ EXPORT_SYMBOL(profile_pc);
  * returned by the service processor for the timebase frequency.  
  */
 
-static void iSeries_tb_recal(void)
+static int __init iSeries_tb_recal(void)
 {
 	struct div_result divres;
 	unsigned long titan, tb;
+
+	/* Make sure we only run on iSeries */
+	if (!firmware_has_feature(FW_FEATURE_ISERIES))
+		return -ENODEV;
+
 	tb = get_tb();
 	titan = HvCallXm_loadTod();
 	if ( iSeries_recal_titan ) {
@@ -600,8 +604,18 @@ static void iSeries_tb_recal(void)
 	}
 	iSeries_recal_titan = titan;
 	iSeries_recal_tb = tb;
+
+	return 0;
 }
-#endif
+late_initcall(iSeries_tb_recal);
+
+/* Called from platform early init */
+void __init iSeries_time_init_early(void)
+{
+	iSeries_recal_tb = get_tb();
+	iSeries_recal_titan = HvCallXm_loadTod();
+}
+#endif /* CONFIG_PPC_ISERIES */
 
 /*
  * For iSeries shared processors, we have to let the hypervisor
@@ -765,12 +779,6 @@ int do_settimeofday(struct timespec *tv)
 	 * to the RTC again, or write to the RTC but then they don't call
 	 * settimeofday to perform this operation.
 	 */
-#ifdef CONFIG_PPC_ISERIES
-	if (firmware_has_feature(FW_FEATURE_ISERIES) && first_settimeofday) {
-		iSeries_tb_recal();
-		first_settimeofday = 0;
-	}
-#endif
 
 	/* Make userspace gettimeofday spin until we're done. */
 	++vdso_data->tb_update_count;

commit 4cefebb1b497a84d61f2fa29d497df75a84b69d4
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Jun 8 13:18:50 2007 +1000

    [POWERPC] Fix stolen time for SMT without LPAR
    
    For POWERPC, stolen time accounts for cycles lost to the hypervisor or
    PURR cycles attributed to the other SMT thread.  Hence, when a PURR is
    available, we should still calculate stolen time, irrespective of being
    virtualised.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 0a8a820672f4..43c687a1d76e 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -214,7 +214,6 @@ static void account_process_time(struct pt_regs *regs)
  	run_posix_cpu_timers(current);
 }
 
-#ifdef CONFIG_PPC_SPLPAR
 /*
  * Stuff for accounting stolen time.
  */
@@ -279,6 +278,7 @@ void calculate_steal_time(void)
 	pme->purr = purr;
 }
 
+#ifdef CONFIG_PPC_SPLPAR
 /*
  * Must be called before the cpu is added to the online map when
  * a cpu is being brought up at runtime.

commit df211c8a478e16232ebbc16e650b19163183fcb6
Author: Nathan Lynch <ntl@pobox.com>
Date:   Wed May 23 10:51:25 2007 +1000

    [POWERPC] Remove spinlock from struct cpu_purr_data
    
    cpu_purr_data is a per-cpu array used to account for stolen time on
    partitioned systems.  It used to be the case that cpus accessed each
    others' cpu_purr_data, so each entry was protected by a spinlock.
    
    However, the code was reworked ("Simplify stolen time calculation")
    with the result that each cpu accesses its own cpu_purr_data and not
    those of other cpus.  This means we can get rid of the spinlock as
    long as we're careful to disable interrupts when accessing
    cpu_purr_data in process context.
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 2c8564d54e4d..0a8a820672f4 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -222,19 +222,28 @@ struct cpu_purr_data {
 	int	initialized;			/* thread is running */
 	u64	tb;			/* last TB value read */
 	u64	purr;			/* last PURR value read */
-	spinlock_t lock;
 };
 
+/*
+ * Each entry in the cpu_purr_data array is manipulated only by its
+ * "owner" cpu -- usually in the timer interrupt but also occasionally
+ * in process context for cpu online.  As long as cpus do not touch
+ * each others' cpu_purr_data, disabling local interrupts is
+ * sufficient to serialize accesses.
+ */
 static DEFINE_PER_CPU(struct cpu_purr_data, cpu_purr_data);
 
 static void snapshot_tb_and_purr(void *data)
 {
+	unsigned long flags;
 	struct cpu_purr_data *p = &__get_cpu_var(cpu_purr_data);
 
+	local_irq_save(flags);
 	p->tb = mftb();
 	p->purr = mfspr(SPRN_PURR);
 	wmb();
 	p->initialized = 1;
+	local_irq_restore(flags);
 }
 
 /*
@@ -242,15 +251,14 @@ static void snapshot_tb_and_purr(void *data)
  */
 void snapshot_timebases(void)
 {
-	int cpu;
-
 	if (!cpu_has_feature(CPU_FTR_PURR))
 		return;
-	for_each_possible_cpu(cpu)
-		spin_lock_init(&per_cpu(cpu_purr_data, cpu).lock);
 	on_each_cpu(snapshot_tb_and_purr, NULL, 0, 1);
 }
 
+/*
+ * Must be called with interrupts disabled.
+ */
 void calculate_steal_time(void)
 {
 	u64 tb, purr;
@@ -262,7 +270,6 @@ void calculate_steal_time(void)
 	pme = &per_cpu(cpu_purr_data, smp_processor_id());
 	if (!pme->initialized)
 		return;		/* this can happen in early boot */
-	spin_lock(&pme->lock);
 	tb = mftb();
 	purr = mfspr(SPRN_PURR);
 	stolen = (tb - pme->tb) - (purr - pme->purr);
@@ -270,7 +277,6 @@ void calculate_steal_time(void)
 		account_steal_time(current, stolen);
 	pme->tb = tb;
 	pme->purr = purr;
-	spin_unlock(&pme->lock);
 }
 
 /*
@@ -284,12 +290,12 @@ static void snapshot_purr(void)
 
 	if (!cpu_has_feature(CPU_FTR_PURR))
 		return;
+	local_irq_save(flags);
 	pme = &per_cpu(cpu_purr_data, smp_processor_id());
-	spin_lock_irqsave(&pme->lock, flags);
 	pme->tb = mftb();
 	pme->purr = mfspr(SPRN_PURR);
 	pme->initialized = 1;
-	spin_unlock_irqrestore(&pme->lock, flags);
+	local_irq_restore(flags);
 }
 
 #endif /* CONFIG_PPC_SPLPAR */

commit e147ec8f18082efb700763bed4fe24f73ca50a2c
Author: will schmidt <will_schmidt@vnet.ibm.com>
Date:   Fri May 11 23:34:16 2007 +1000

    [POWERPC] Simplify smp_space_timers
    
    Greatly simplify the function smp_space_timers.
    
    The stolen time calculation (per comment within the code) doesn't need the
    half-jiffy stagger any more.  There isn't an issue with bouncing off global
    locks, so we really shouldn't need any sort of staggering at all.
    
    However, the last_jiffy value still needs to be set.   This removes the
    extra stagger logic, and just sets the values.
    
    This change should benefit applications that rely on barrier
    synchronization, and will help cut down OS jitter.
    
    Boot tested across the board (G5,power3,power4,power5,970mp blade).
    
    Signed-off-by: Will Schmidt <will_schmidt@vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 7cedef8f5f70..2c8564d54e4d 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -711,30 +711,15 @@ void wakeup_decrementer(void)
 void __init smp_space_timers(unsigned int max_cpus)
 {
 	int i;
-	unsigned long half = tb_ticks_per_jiffy / 2;
-	unsigned long offset = tb_ticks_per_jiffy / max_cpus;
 	u64 previous_tb = per_cpu(last_jiffy, boot_cpuid);
 
 	/* make sure tb > per_cpu(last_jiffy, cpu) for all cpus always */
 	previous_tb -= tb_ticks_per_jiffy;
-	/*
-	 * The stolen time calculation for POWER5 shared-processor LPAR
-	 * systems works better if the two threads' timebase interrupts
-	 * are staggered by half a jiffy with respect to each other.
-	 */
+
 	for_each_possible_cpu(i) {
 		if (i == boot_cpuid)
 			continue;
-		if (i == (boot_cpuid ^ 1))
-			per_cpu(last_jiffy, i) =
-				per_cpu(last_jiffy, boot_cpuid) - half;
-		else if (i & 1)
-			per_cpu(last_jiffy, i) =
-				per_cpu(last_jiffy, i ^ 1) + half;
-		else {
-			previous_tb += offset;
-			per_cpu(last_jiffy, i) = previous_tb;
-		}
+		per_cpu(last_jiffy, i) = previous_tb;
 	}
 }
 #endif

commit e2eb63927bfcb54232163bfec32440246fd44457
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Tue Apr 3 22:26:41 2007 +1000

    [POWERPC] Rename get_property to of_get_property: arch/powerpc
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index f6f0c6b07c4c..7cedef8f5f70 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -834,7 +834,7 @@ static int __init get_freq(char *name, int cells, unsigned long *val)
 	cpu = of_find_node_by_type(NULL, "cpu");
 
 	if (cpu) {
-		fp = get_property(cpu, name, NULL);
+		fp = of_get_property(cpu, name, NULL);
 		if (fp) {
 			found = 1;
 			*val = of_read_ulong(fp, cells);

commit 501b6d2938fd51e85279d950a6d23d515ae22c59
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Tue Nov 21 15:10:20 2006 +1100

    [POWERPC] iSeries: fix time.c for combined build
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 46a24de36fec..f6f0c6b07c4c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -631,7 +631,8 @@ void timer_interrupt(struct pt_regs * regs)
 	calculate_steal_time();
 
 #ifdef CONFIG_PPC_ISERIES
-	get_lppaca()->int_dword.fields.decr_int = 0;
+	if (firmware_has_feature(FW_FEATURE_ISERIES))
+		get_lppaca()->int_dword.fields.decr_int = 0;
 #endif
 
 	while ((ticks = tb_ticks_since(per_cpu(last_jiffy, cpu)))
@@ -674,7 +675,7 @@ void timer_interrupt(struct pt_regs * regs)
 	set_dec(next_dec);
 
 #ifdef CONFIG_PPC_ISERIES
-	if (hvlpevent_is_pending())
+	if (firmware_has_feature(FW_FEATURE_ISERIES) && hvlpevent_is_pending())
 		process_hvlpevents();
 #endif
 
@@ -774,7 +775,7 @@ int do_settimeofday(struct timespec *tv)
 	 * settimeofday to perform this operation.
 	 */
 #ifdef CONFIG_PPC_ISERIES
-	if (first_settimeofday) {
+	if (firmware_has_feature(FW_FEATURE_ISERIES) && first_settimeofday) {
 		iSeries_tb_recal();
 		first_settimeofday = 0;
 	}

commit df9c23095fc8652798c41dd860676d3dafb2f1dc
Author: Kim Phillips <kim.phillips@freescale.com>
Date:   Tue Nov 21 10:31:14 2006 -0600

    [POWERPC] Revert "[POWERPC] Add powerpc get/set_rtc_time interface to new generic rtc class"
    
    This reverts commit 7a69af63e788a324d162201a0b23df41bcf158dd.
    
    As advised by David Brownell:
    
    http://marc.theaimsgroup.com/?l=linux-kernel&m=116387226902131&w=2
    
    Signed-off-by: Kim Phillips <kim.phillips@freescale.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index a1b5e4b16151..46a24de36fec 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1014,48 +1014,6 @@ void __init time_init(void)
 	set_dec(tb_ticks_per_jiffy);
 }
 
-#ifdef CONFIG_RTC_CLASS
-static int set_rtc_class_time(struct rtc_time *tm)
-{
-	int err;
-	struct class_device *class_dev =
-		rtc_class_open(CONFIG_RTC_HCTOSYS_DEVICE);
-
-	if (class_dev == NULL)
-		return -ENODEV;
-
-	err = rtc_set_time(class_dev, tm);
-
-	rtc_class_close(class_dev);
-
-	return 0;
-}
-
-static void get_rtc_class_time(struct rtc_time *tm)
-{
-	int err;
-	struct class_device *class_dev =
-		rtc_class_open(CONFIG_RTC_HCTOSYS_DEVICE);
-
-	if (class_dev == NULL)
-		return;
-
-	err = rtc_read_time(class_dev, tm);
-
-	rtc_class_close(class_dev);
-
-	return;
-}
-
-int __init rtc_class_hookup(void)
-{
-	ppc_md.get_rtc_time = get_rtc_class_time;
-	ppc_md.set_rtc_time = set_rtc_class_time;
-
-	return 0;
-}
-#endif /* CONFIG_RTC_CLASS */
-
 
 #define FEBRUARY	2
 #define	STARTOFTIME	1970

commit cbcdb93d4443568f17e93610d240043ec5ac067b
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Tue Oct 17 23:08:35 2006 +1000

    [POWERPC] Simplify stolen time calculation
    
    In calculating stolen time, we were trying to actually account for time
    spent in the hypervisor.  We don't really have enough information to do
    that accurately, so don't try.  Instead, we now calculate stolen time as
    time that the current cpu thread is not actually dispatching instructions.
    On chips without a PURR, we cannot do this, so stolen time will always
    be zero.  On chips with a PURR, this is merely the difference between
    the elapsed PURR values and the elapsed TB values.
    
    This gives us much more sane vaules from tools such as mpstat, even if
    they are still a bit strange e.g. 2 busy threads on one cpu will both
    appear to have 50% user time and 50% stolen time while 1 busy thread on
    a cpu will look like 100% user on one of them and 100% idle on the other.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 5b59bc18dfe7..a1b5e4b16151 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -220,11 +220,8 @@ static void account_process_time(struct pt_regs *regs)
  */
 struct cpu_purr_data {
 	int	initialized;			/* thread is running */
-	u64	tb0;			/* timebase at origin time */
-	u64	purr0;			/* PURR at origin time */
 	u64	tb;			/* last TB value read */
 	u64	purr;			/* last PURR value read */
-	u64	stolen;			/* stolen time so far */
 	spinlock_t lock;
 };
 
@@ -234,10 +231,8 @@ static void snapshot_tb_and_purr(void *data)
 {
 	struct cpu_purr_data *p = &__get_cpu_var(cpu_purr_data);
 
-	p->tb0 = mftb();
-	p->purr0 = mfspr(SPRN_PURR);
-	p->tb = p->tb0;
-	p->purr = 0;
+	p->tb = mftb();
+	p->purr = mfspr(SPRN_PURR);
 	wmb();
 	p->initialized = 1;
 }
@@ -258,37 +253,24 @@ void snapshot_timebases(void)
 
 void calculate_steal_time(void)
 {
-	u64 tb, purr, t0;
+	u64 tb, purr;
 	s64 stolen;
-	struct cpu_purr_data *p0, *pme, *phim;
-	int cpu;
+	struct cpu_purr_data *pme;
 
 	if (!cpu_has_feature(CPU_FTR_PURR))
 		return;
-	cpu = smp_processor_id();
-	pme = &per_cpu(cpu_purr_data, cpu);
+	pme = &per_cpu(cpu_purr_data, smp_processor_id());
 	if (!pme->initialized)
 		return;		/* this can happen in early boot */
-	p0 = &per_cpu(cpu_purr_data, cpu & ~1);
-	phim = &per_cpu(cpu_purr_data, cpu ^ 1);
-	spin_lock(&p0->lock);
+	spin_lock(&pme->lock);
 	tb = mftb();
-	purr = mfspr(SPRN_PURR) - pme->purr0;
-	if (!phim->initialized || !cpu_online(cpu ^ 1)) {
-		stolen = (tb - pme->tb) - (purr - pme->purr);
-	} else {
-		t0 = pme->tb0;
-		if (phim->tb0 < t0)
-			t0 = phim->tb0;
-		stolen = phim->tb - t0 - phim->purr - purr - p0->stolen;
-	}
-	if (stolen > 0) {
+	purr = mfspr(SPRN_PURR);
+	stolen = (tb - pme->tb) - (purr - pme->purr);
+	if (stolen > 0)
 		account_steal_time(current, stolen);
-		p0->stolen += stolen;
-	}
 	pme->tb = tb;
 	pme->purr = purr;
-	spin_unlock(&p0->lock);
+	spin_unlock(&pme->lock);
 }
 
 /*
@@ -297,30 +279,17 @@ void calculate_steal_time(void)
  */
 static void snapshot_purr(void)
 {
-	int cpu;
-	u64 purr;
-	struct cpu_purr_data *p0, *pme, *phim;
+	struct cpu_purr_data *pme;
 	unsigned long flags;
 
 	if (!cpu_has_feature(CPU_FTR_PURR))
 		return;
-	cpu = smp_processor_id();
-	pme = &per_cpu(cpu_purr_data, cpu);
-	p0 = &per_cpu(cpu_purr_data, cpu & ~1);
-	phim = &per_cpu(cpu_purr_data, cpu ^ 1);
-	spin_lock_irqsave(&p0->lock, flags);
-	pme->tb = pme->tb0 = mftb();
-	purr = mfspr(SPRN_PURR);
-	if (!phim->initialized) {
-		pme->purr = 0;
-		pme->purr0 = purr;
-	} else {
-		/* set p->purr and p->purr0 for no change in p0->stolen */
-		pme->purr = phim->tb - phim->tb0 - phim->purr - p0->stolen;
-		pme->purr0 = purr - pme->purr;
-	}
+	pme = &per_cpu(cpu_purr_data, smp_processor_id());
+	spin_lock_irqsave(&pme->lock, flags);
+	pme->tb = mftb();
+	pme->purr = mfspr(SPRN_PURR);
 	pme->initialized = 1;
-	spin_unlock_irqrestore(&p0->lock, flags);
+	spin_unlock_irqrestore(&pme->lock, flags);
 }
 
 #endif /* CONFIG_PPC_SPLPAR */

commit 35a84c2f56e0f77ea2c5a4327b17104705f4c8c7
Author: Olaf Hering <olaf@aepfle.de>
Date:   Sat Oct 7 22:08:26 2006 +1000

    [POWERPC] Fix up after irq changes
    
    Remove struct pt_regs * from all handlers.
    Also remove the regs argument from get_irq() functions.
    Compile tested with arch/powerpc/config/* and
    arch/ppc/configs/prep_defconfig
    
    Signed-off-by: Olaf Hering <olaf@aepfle.de>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index d210d0a5006b..5b59bc18dfe7 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -706,7 +706,7 @@ void timer_interrupt(struct pt_regs * regs)
 
 #ifdef CONFIG_PPC_ISERIES
 	if (hvlpevent_is_pending())
-		process_hvlpevents(regs);
+		process_hvlpevents();
 #endif
 
 #ifdef CONFIG_PPC64

commit 7d12e780e003f93433d49ce78cfedf4b4c52adc5
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 5 14:55:46 2006 +0100

    IRQ: Maintain regs pointer globally rather than passing to IRQ handlers
    
    Maintain a per-CPU global "struct pt_regs *" variable which can be used instead
    of passing regs around manually through all ~1800 interrupt handlers in the
    Linux kernel.
    
    The regs pointer is used in few places, but it potentially costs both stack
    space and code to pass it around.  On the FRV arch, removing the regs parameter
    from all the genirq function results in a 20% speed up of the IRQ exit path
    (ie: from leaving timer_interrupt() to leaving do_IRQ()).
    
    Where appropriate, an arch may override the generic storage facility and do
    something different with the variable.  On FRV, for instance, the address is
    maintained in GR28 at all times inside the kernel as part of general exception
    handling.
    
    Having looked over the code, it appears that the parameter may be handed down
    through up to twenty or so layers of functions.  Consider a USB character
    device attached to a USB hub, attached to a USB controller that posts its
    interrupts through a cascaded auxiliary interrupt controller.  A character
    device driver may want to pass regs to the sysrq handler through the input
    layer which adds another few layers of parameter passing.
    
    I've build this code with allyesconfig for x86_64 and i386.  I've runtested the
    main part of the code on FRV and i386, though I can't test most of the drivers.
    I've also done partial conversion for powerpc and MIPS - these at least compile
    with minimal configurations.
    
    This will affect all archs.  Mostly the changes should be relatively easy.
    Take do_IRQ(), store the regs pointer at the beginning, saving the old one:
    
            struct pt_regs *old_regs = set_irq_regs(regs);
    
    And put the old one back at the end:
    
            set_irq_regs(old_regs);
    
    Don't pass regs through to generic_handle_irq() or __do_IRQ().
    
    In timer_interrupt(), this sort of change will be necessary:
    
            -       update_process_times(user_mode(regs));
            -       profile_tick(CPU_PROFILING, regs);
            +       update_process_times(user_mode(get_irq_regs()));
            +       profile_tick(CPU_PROFILING);
    
    I'd like to move update_process_times()'s use of get_irq_regs() into itself,
    except that i386, alone of the archs, uses something other than user_mode().
    
    Some notes on the interrupt handling in the drivers:
    
     (*) input_dev() is now gone entirely.  The regs pointer is no longer stored in
         the input_dev struct.
    
     (*) finish_unlinks() in drivers/usb/host/ohci-q.c needs checking.  It does
         something different depending on whether it's been supplied with a regs
         pointer or not.
    
     (*) Various IRQ handler function pointers have been moved to type
         irq_handler_t.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>
    (cherry picked from 1b16e7ac850969f38b375e511e3fa2f474a33867 commit)

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 85b9244a098c..d210d0a5006b 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -51,6 +51,7 @@
 #include <linux/rtc.h>
 #include <linux/jiffies.h>
 #include <linux/posix-timers.h>
+#include <linux/irq.h>
 
 #include <asm/io.h>
 #include <asm/processor.h>
@@ -643,6 +644,7 @@ static void iSeries_tb_recal(void)
  */
 void timer_interrupt(struct pt_regs * regs)
 {
+	struct pt_regs *old_regs;
 	int next_dec;
 	int cpu = smp_processor_id();
 	unsigned long ticks;
@@ -653,9 +655,10 @@ void timer_interrupt(struct pt_regs * regs)
 		do_IRQ(regs);
 #endif
 
+	old_regs = set_irq_regs(regs);
 	irq_enter();
 
-	profile_tick(CPU_PROFILING, regs);
+	profile_tick(CPU_PROFILING);
 	calculate_steal_time();
 
 #ifdef CONFIG_PPC_ISERIES
@@ -715,6 +718,7 @@ void timer_interrupt(struct pt_regs * regs)
 #endif
 
 	irq_exit();
+	set_irq_regs(old_regs);
 }
 
 void wakeup_decrementer(void)

commit ccaa36f73544163ef6e15eb29a620130755f6001
Merge: b4a9071af62f 5e9808235816
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Tue Oct 3 08:52:26 2006 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc: (29 commits)
      [POWERPC] Fix rheap alignment problem
      [POWERPC] Use check_legacy_ioport() for ISAPnP
      [POWERPC] Avoid NULL pointer in gpio1_interrupt
      [POWERPC] Enable generic rtc hook for the MPC8349 mITX
      [POWERPC] Add powerpc get/set_rtc_time interface to new generic rtc class
      [POWERPC] Create a "wrapper" script and use it in arch/powerpc/boot
      [POWERPC] fix spin lock nesting in hvc_iseries
      [POWERPC] EEH failure to mark pci slot as frozen.
      [POWERPC] update powerpc defconfig files after libata kconfig breakage
      [POWERPC] enable sysrq in pmac32_defconfig
      [POWERPC] UPIO_TSI cleanup
      [POWERPC] rewrite mkprep and mkbugboot in sane C
      [POWERPC] maple/pci iomem annotations
      [POWERPC] powerpc oprofile __user annotations
      [POWERPC] cell spufs iomem annotations
      [POWERPC] NULL noise removal: spufs
      [POWERPC] ppc math-emu needs -fno-builtin-fabs for math.c and fabs.c
      [POWERPC] update mpc8349_itx_defconfig and remove some debug settings
      [POWERPC] Always call cede in pseries dedicated idle loop
      [POWERPC] Fix loop logic in irq_alloc_virt()
      ...

commit 7a69af63e788a324d162201a0b23df41bcf158dd
Author: Kim Phillips <kim.phillips@freescale.com>
Date:   Tue Sep 26 17:46:37 2006 -0500

    [POWERPC] Add powerpc get/set_rtc_time interface to new generic rtc class
    
    Add powerpc get/set_rtc_time interface to new generic rtc class. This
    abstracts rtc chip specific code from the platform code for rtc-over-i2c
    platforms.  Specific RTC chip support is now configured under
    Device Drivers -> Real Time Clock. Setting time of day from the RTC
    on startup is also configurable.
    
    this time without the potentially platform breaking initcall.
    
    Signed-off-by: Kim Phillips <kim.phillips@freescale.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 7a3c3f791ade..b4ed362c457a 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -1048,6 +1048,48 @@ void __init time_init(void)
 	set_dec(tb_ticks_per_jiffy);
 }
 
+#ifdef CONFIG_RTC_CLASS
+static int set_rtc_class_time(struct rtc_time *tm)
+{
+	int err;
+	struct class_device *class_dev =
+		rtc_class_open(CONFIG_RTC_HCTOSYS_DEVICE);
+
+	if (class_dev == NULL)
+		return -ENODEV;
+
+	err = rtc_set_time(class_dev, tm);
+
+	rtc_class_close(class_dev);
+
+	return 0;
+}
+
+static void get_rtc_class_time(struct rtc_time *tm)
+{
+	int err;
+	struct class_device *class_dev =
+		rtc_class_open(CONFIG_RTC_HCTOSYS_DEVICE);
+
+	if (class_dev == NULL)
+		return;
+
+	err = rtc_read_time(class_dev, tm);
+
+	rtc_class_close(class_dev);
+
+	return;
+}
+
+int __init rtc_class_hookup(void)
+{
+	ppc_md.get_rtc_time = get_rtc_class_time;
+	ppc_md.set_rtc_time = set_rtc_class_time;
+
+	return 0;
+}
+#endif /* CONFIG_RTC_CLASS */
+
 
 #define FEBRUARY	2
 #define	STARTOFTIME	1970

commit 8ef386092d7c2891bd7acefb2a87f878f7e9a0d6
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Sat Sep 30 23:28:31 2006 -0700

    [PATCH] kill wall_jiffies
    
    With 2.6.18-rc4-mm2, now wall_jiffies will always be the same as jiffies.
    So we can kill wall_jiffies completely.
    
    This is just a cleanup and logically should not change any real behavior
    except for one thing: RTC updating code in (old) ppc and xtensa use a
    condition "jiffies - wall_jiffies == 1".  This condition is never met so I
    suppose it is just a bug.  I just remove that condition only instead of
    kill the whole "if" block.
    
    [heiko.carstens@de.ibm.com: s390 build fix and cleanup]
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Ian Molton <spyro@f2s.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Hirokazu Takata <takata.hirokazu@renesas.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Kazumoto Kojima <kkojima@rr.iij4u.or.jp>
    Cc: Richard Curnow <rc@rc0.org.uk>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Cc: Miles Bader <uclinux-v850@lsi.nec.co.jp>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 71f71da98e7d..8b278d85ca4e 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -117,8 +117,6 @@ unsigned tb_to_ns_shift;
 
 struct gettimeofday_struct do_gtod;
 
-extern unsigned long wall_jiffies;
-
 extern struct timezone sys_tz;
 static long timezone_offset;
 
@@ -816,11 +814,6 @@ int do_settimeofday(struct timespec *tv)
 	/*
 	 * Subtract off the number of nanoseconds since the
 	 * beginning of the last tick.
-	 * Note that since we don't increment jiffies_64 anywhere other
-	 * than in do_timer (since we don't have a lost tick problem),
-	 * wall_jiffies will always be the same as jiffies,
-	 * and therefore the (jiffies - wall_jiffies) computation
-	 * has been removed.
 	 */
 	tb_delta = tb_ticks_since(tb_last_jiffy);
 	tb_delta = mulhdu(tb_delta, do_gtod.varp->tb_to_xs); /* in xsec */

commit 3171a0305d62e6627a24bff35af4f997e4988a80
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Fri Sep 29 02:00:32 2006 -0700

    [PATCH] simplify update_times (avoid jiffies/jiffies_64 aliasing problem)
    
    Pass ticks to do_timer() and update_times(), and adjust x86_64 and s390
    timer interrupt handler with this change.
    
    Currently update_times() calculates ticks by "jiffies - wall_jiffies", but
    callers of do_timer() should know how many ticks to update.  Passing ticks
    get rid of this redundant calculation.  Also there are another redundancy
    pointed out by Martin Schwidefsky.
    
    This cleanup make a barrier added by
    5aee405c662ca644980c184774277fc6d0769a84 needless.  So this patch removes
    it.
    
    As a bonus, this cleanup make wall_jiffies can be removed easily, since now
    wall_jiffies is always synced with jiffies.  (This patch does not really
    remove wall_jiffies.  It would be another cleanup patch)
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Acked-by: Russell King <rmk@arm.linux.org.uk>
    Cc: Ian Molton <spyro@f2s.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Hirokazu Takata <takata.hirokazu@renesas.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Kazumoto Kojima <kkojima@rr.iij4u.or.jp>
    Cc: Richard Curnow <rc@rc0.org.uk>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Cc: Miles Bader <uclinux-v850@lsi.nec.co.jp>
    Cc: Chris Zankel <chris@zankel.net>
    Acked-by: "Luck, Tony" <tony.luck@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 7a3c3f791ade..71f71da98e7d 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -693,7 +693,7 @@ void timer_interrupt(struct pt_regs * regs)
 		tb_next_jiffy = tb_last_jiffy + tb_ticks_per_jiffy;
 		if (per_cpu(last_jiffy, cpu) >= tb_next_jiffy) {
 			tb_last_jiffy = tb_next_jiffy;
-			do_timer(regs);
+			do_timer(1);
 			timer_recalc_offset(tb_last_jiffy);
 			timer_check_rtc();
 		}

commit a4dc7ff08915a2035aa6d6decc53fa1deaa410bb
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 19 14:06:27 2006 +1000

    [POWERPC] Define of_read_ulong helper
    
    There are various places where we want to extract an unsigned long
    value from a device-tree property that can be 1 or 2 cells in length.
    This replaces some open-coded calculations, and one place where we
    assumed without checking that properties were the length we wanted,
    with a little of_read_ulong() helper.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index b9a2061cfdb7..7a3c3f791ade 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -870,9 +870,7 @@ static int __init get_freq(char *name, int cells, unsigned long *val)
 		fp = get_property(cpu, name, NULL);
 		if (fp) {
 			found = 1;
-			*val = 0;
-			while (cells--)
-				*val = (*val << 32) | *fp++;
+			*val = of_read_ulong(fp, cells);
 		}
 
 		of_node_put(cpu);

commit aa43f77939c97bf9d3580c6a5e71a5a40290e451
Merge: 2818c5dec5e2 4c15343167b5
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Aug 31 15:45:48 2006 +1000

    Merge branch 'merge'

commit eb36c2884a1a2190791afe65fd833b2d3cd4b999
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Aug 30 16:13:16 2006 +1000

    [PATCH] ppc32: fix last_jiffy time comparison
    
    This fixes a hang on ppc32.
    
    The problem was that I was comparing a 32-bit quantity with a 64-bit
    quantity, and consequently time wasn't advancing.  This makes us use a
    64-bit quantity on all platforms, which ends up simplifying the code
    since we can now get rid of the tb_last_stamp variable (which actually
    fixes another bug that Ben H and I noticed while going carefully through
    the code).
    
    This works fine on my G4 tibook.  Let me know how it goes on your
    machines.
    
    Acked-by: Olaf Hering <olaf@aepfle.de>
    Acked-by: Mikael Pettersson <mikpe@it.uu.se>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 18e59e43d2b3..a124499e65d9 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -125,15 +125,8 @@ static long timezone_offset;
 unsigned long ppc_proc_freq;
 unsigned long ppc_tb_freq;
 
-u64 tb_last_jiffy __cacheline_aligned_in_smp;
-unsigned long tb_last_stamp;
-
-/*
- * Note that on ppc32 this only stores the bottom 32 bits of
- * the timebase value, but that's enough to tell when a jiffy
- * has passed.
- */
-DEFINE_PER_CPU(unsigned long, last_jiffy);
+static u64 tb_last_jiffy __cacheline_aligned_in_smp;
+static DEFINE_PER_CPU(u64, last_jiffy);
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING
 /*
@@ -458,7 +451,7 @@ void do_gettimeofday(struct timeval *tv)
 		do {
 			seq = read_seqbegin_irqsave(&xtime_lock, flags);
 			sec = xtime.tv_sec;
-			nsec = xtime.tv_nsec + tb_ticks_since(tb_last_stamp);
+			nsec = xtime.tv_nsec + tb_ticks_since(tb_last_jiffy);
 		} while (read_seqretry_irqrestore(&xtime_lock, seq, flags));
 		usec = nsec / 1000;
 		while (usec >= 1000000) {
@@ -700,7 +693,6 @@ void timer_interrupt(struct pt_regs * regs)
 		tb_next_jiffy = tb_last_jiffy + tb_ticks_per_jiffy;
 		if (per_cpu(last_jiffy, cpu) >= tb_next_jiffy) {
 			tb_last_jiffy = tb_next_jiffy;
-			tb_last_stamp = per_cpu(last_jiffy, cpu);
 			do_timer(regs);
 			timer_recalc_offset(tb_last_jiffy);
 			timer_check_rtc();
@@ -749,7 +741,7 @@ void __init smp_space_timers(unsigned int max_cpus)
 	int i;
 	unsigned long half = tb_ticks_per_jiffy / 2;
 	unsigned long offset = tb_ticks_per_jiffy / max_cpus;
-	unsigned long previous_tb = per_cpu(last_jiffy, boot_cpuid);
+	u64 previous_tb = per_cpu(last_jiffy, boot_cpuid);
 
 	/* make sure tb > per_cpu(last_jiffy, cpu) for all cpus always */
 	previous_tb -= tb_ticks_per_jiffy;
@@ -830,7 +822,7 @@ int do_settimeofday(struct timespec *tv)
 	 * and therefore the (jiffies - wall_jiffies) computation
 	 * has been removed.
 	 */
-	tb_delta = tb_ticks_since(tb_last_stamp);
+	tb_delta = tb_ticks_since(tb_last_jiffy);
 	tb_delta = mulhdu(tb_delta, do_gtod.varp->tb_to_xs); /* in xsec */
 	new_nsec -= SCALE_XSEC(tb_delta, 1000000000);
 
@@ -950,8 +942,7 @@ void __init time_init(void)
 	if (__USE_RTC()) {
 		/* 601 processor: dec counts down by 128 every 128ns */
 		ppc_tb_freq = 1000000000;
-		tb_last_stamp = get_rtcl();
-		tb_last_jiffy = tb_last_stamp;
+		tb_last_jiffy = get_rtcl();
 	} else {
 		/* Normal PowerPC with timebase register */
 		ppc_md.calibrate_decr();
@@ -959,7 +950,7 @@ void __init time_init(void)
 		       ppc_tb_freq / 1000000, ppc_tb_freq % 1000000);
 		printk(KERN_DEBUG "time_init: processor frequency   = %lu.%.6lu MHz\n",
 		       ppc_proc_freq / 1000000, ppc_proc_freq % 1000000);
-		tb_last_stamp = tb_last_jiffy = get_tb();
+		tb_last_jiffy = get_tb();
 	}
 
 	tb_ticks_per_jiffy = ppc_tb_freq / HZ;
@@ -1036,7 +1027,7 @@ void __init time_init(void)
 	do_gtod.varp = &do_gtod.vars[0];
 	do_gtod.var_idx = 0;
 	do_gtod.varp->tb_orig_stamp = tb_last_jiffy;
-	__get_cpu_var(last_jiffy) = tb_last_stamp;
+	__get_cpu_var(last_jiffy) = tb_last_jiffy;
 	do_gtod.varp->stamp_xsec = (u64) xtime.tv_sec * XSEC_PER_SEC;
 	do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
 	do_gtod.varp->tb_to_xs = tb_to_xs;

commit ea0763a7e62f60f3e166327268a80f16ad806718
Merge: 271c511db9d3 c85c41ad73c6
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Aug 25 14:56:07 2006 +1000

    Merge branch 'merge'

commit 5db9fa9593e2ff69f2b95f9d59229dc4faaa564d
Author: Nathan Lynch <ntl@pobox.com>
Date:   Tue Aug 22 20:36:05 2006 -0500

    [POWERPC] Fix gettimeofday inaccuracies
    
    There are two problems in the powerpc gettimeofday code which can
    cause incorrect results to be returned.
    
    The first is that there is a race between do_gettimeofday and the
    timer interrupt:
    
    1. do_gettimeofday does get_tb()
    
    2. decrementer exception on boot cpu which runs timer_recalc_offset,
       which also samples the timebase and updates the do_gtod structure
       with a greater timebase value.
    
    3. do_gettimeofday calls __do_gettimeofday, which leads to the
       negative result from tb_val - temp_varp->tb_orig_stamp.
    
    The second is caused by taking the boot cpu offline, which can cause
    the value of tb_last_jiffy to be increased past the currently
    available timebase, causing the same underflow as above.
    
    [paulus@samba.org - define and use data_barrier() instead of mb().]
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 774c0a3c5019..18e59e43d2b3 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -417,7 +417,7 @@ static __inline__ void timer_check_rtc(void)
 /*
  * This version of gettimeofday has microsecond resolution.
  */
-static inline void __do_gettimeofday(struct timeval *tv, u64 tb_val)
+static inline void __do_gettimeofday(struct timeval *tv)
 {
 	unsigned long sec, usec;
 	u64 tb_ticks, xsec;
@@ -431,7 +431,12 @@ static inline void __do_gettimeofday(struct timeval *tv, u64 tb_val)
 	 * without a divide (and in fact, without a multiply)
 	 */
 	temp_varp = do_gtod.varp;
-	tb_ticks = tb_val - temp_varp->tb_orig_stamp;
+
+	/* Sampling the time base must be done after loading
+	 * do_gtod.varp in order to avoid racing with update_gtod.
+	 */
+	data_barrier(temp_varp);
+	tb_ticks = get_tb() - temp_varp->tb_orig_stamp;
 	temp_tb_to_xs = temp_varp->tb_to_xs;
 	temp_stamp_xsec = temp_varp->stamp_xsec;
 	xsec = temp_stamp_xsec + mulhdu(tb_ticks, temp_tb_to_xs);
@@ -464,7 +469,7 @@ void do_gettimeofday(struct timeval *tv)
 		tv->tv_usec = usec;
 		return;
 	}
-	__do_gettimeofday(tv, get_tb());
+	__do_gettimeofday(tv);
 }
 
 EXPORT_SYMBOL(do_gettimeofday);
@@ -650,6 +655,7 @@ void timer_interrupt(struct pt_regs * regs)
 	int next_dec;
 	int cpu = smp_processor_id();
 	unsigned long ticks;
+	u64 tb_next_jiffy;
 
 #ifdef CONFIG_PPC32
 	if (atomic_read(&ppc_n_lost_interrupts) != 0)
@@ -691,11 +697,14 @@ void timer_interrupt(struct pt_regs * regs)
 			continue;
 
 		write_seqlock(&xtime_lock);
-		tb_last_jiffy += tb_ticks_per_jiffy;
-		tb_last_stamp = per_cpu(last_jiffy, cpu);
-		do_timer(regs);
-		timer_recalc_offset(tb_last_jiffy);
-		timer_check_rtc();
+		tb_next_jiffy = tb_last_jiffy + tb_ticks_per_jiffy;
+		if (per_cpu(last_jiffy, cpu) >= tb_next_jiffy) {
+			tb_last_jiffy = tb_next_jiffy;
+			tb_last_stamp = per_cpu(last_jiffy, cpu);
+			do_timer(regs);
+			timer_recalc_offset(tb_last_jiffy);
+			timer_check_rtc();
+		}
 		write_sequnlock(&xtime_lock);
 	}
 	

commit a7f67bdf2c9f24509b8e81e0f35573b611987c80
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Wed Jul 12 15:35:54 2006 +1000

    [POWERPC] Constify & voidify get_property()
    
    Now that get_property() returns a void *, there's no need to cast its
    return value. Also, treat the return value as const, so we can
    constify get_property later.
    
    powerpc core changes.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 774c0a3c5019..8d4ccf061a4d 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -859,14 +859,14 @@ EXPORT_SYMBOL(do_settimeofday);
 static int __init get_freq(char *name, int cells, unsigned long *val)
 {
 	struct device_node *cpu;
-	unsigned int *fp;
+	const unsigned int *fp;
 	int found = 0;
 
 	/* The cpu node should have timebase and clock frequency properties */
 	cpu = of_find_node_by_type(NULL, "cpu");
 
 	if (cpu) {
-		fp = (unsigned int *)get_property(cpu, name, NULL);
+		fp = get_property(cpu, name, NULL);
 		if (fp) {
 			found = 1;
 			*val = 0;

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: JÃ¶rn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: JÃ¶rn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 7dd5dab789a1..774c0a3c5019 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -32,7 +32,6 @@
  *      2 of the License, or (at your option) any later version.
  */
 
-#include <linux/config.h>
 #include <linux/errno.h>
 #include <linux/module.h>
 #include <linux/sched.h>

commit 19923c190e0932bf0ac1e1d06a48f5c3678dd0de
Author: Roman Zippel <zippel@linux-m68k.org>
Date:   Mon Jun 26 00:25:18 2006 -0700

    [PATCH] fix and optimize clock source update
    
    This fixes the clock source updates in update_wall_time() to correctly
    track the time coming in via current_tick_length().  Optimize the fast
    paths to be as short as possible to keep the overhead low.
    
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Acked-by: John Stultz <johnstul@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 742f07a63161..7dd5dab789a1 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -102,7 +102,7 @@ EXPORT_SYMBOL(tb_ticks_per_sec);	/* for cputime_t conversions */
 u64 tb_to_xs;
 unsigned tb_to_us;
 
-#define TICKLEN_SCALE	(SHIFT_SCALE - 10)
+#define TICKLEN_SCALE	TICK_LENGTH_SHIFT
 u64 last_tick_len;	/* units are ns / 2^TICKLEN_SCALE */
 u64 ticklen_to_xs;	/* 0.64 fraction */
 
@@ -534,7 +534,7 @@ static __inline__ void timer_recalc_offset(u64 cur_tb)
 
 	if (__USE_RTC())
 		return;
-	tlen = current_tick_length(SHIFT_SCALE - 10);
+	tlen = current_tick_length();
 	offset = cur_tb - do_gtod.varp->tb_orig_stamp;
 	if (tlen == last_tick_len && offset < 0x80000000u)
 		return;

commit 260a42309b31cbc54eb4b6b85649e412bcad053f
Author: john stultz <johnstul@us.ibm.com>
Date:   Mon Jun 26 00:25:07 2006 -0700

    [PATCH] Time: Let user request precision from current_tick_length()
    
    Change the current_tick_length() function so it takes an argument which
    specifies how much precision to return in shifted nanoseconds.  This provides
    a simple way to convert between NTPs internal nanoseconds shifted by
    (SHIFT_SCALE - 10) to other shifted nanosecond units that are used by the
    clocksource abstraction.
    
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index d20907561f46..742f07a63161 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -534,7 +534,7 @@ static __inline__ void timer_recalc_offset(u64 cur_tb)
 
 	if (__USE_RTC())
 		return;
-	tlen = current_tick_length();
+	tlen = current_tick_length(SHIFT_SCALE - 10);
 	offset = cur_tb - do_gtod.varp->tb_orig_stamp;
 	if (tlen == last_tick_len && offset < 0x80000000u)
 		return;

commit 0bb474a48e5d1ceb8e4005c7664b548c9834a784
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Jun 20 18:47:26 2006 +1000

    [POWERPC] support ibm,extended-*-frequency properties
    
    Support the ibm,extended-*-frequency properties found in recent POWER5
    firmware:
    
    cpus/PowerPC,POWER5@0/clock-frequency
                     59aa5880 (1504336000)
    cpus/PowerPC,POWER5@0/ibm,extended-clock-frequency
                     00000000 59aa5880
    cpus/PowerPC,POWER5@0/timebase-frequency
                     0b354b10 (188042000)
    cpus/PowerPC,POWER5@0/ibm,extended-timebase-frequency
                     00000000 0b354b10
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 528e7f84cb67..d20907561f46 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -857,42 +857,50 @@ int do_settimeofday(struct timespec *tv)
 
 EXPORT_SYMBOL(do_settimeofday);
 
-void __init generic_calibrate_decr(void)
+static int __init get_freq(char *name, int cells, unsigned long *val)
 {
 	struct device_node *cpu;
 	unsigned int *fp;
-	int node_found;
+	int found = 0;
 
-	/*
-	 * The cpu node should have a timebase-frequency property
-	 * to tell us the rate at which the decrementer counts.
-	 */
+	/* The cpu node should have timebase and clock frequency properties */
 	cpu = of_find_node_by_type(NULL, "cpu");
 
-	ppc_tb_freq = DEFAULT_TB_FREQ;		/* hardcoded default */
-	node_found = 0;
 	if (cpu) {
-		fp = (unsigned int *)get_property(cpu, "timebase-frequency",
-						  NULL);
+		fp = (unsigned int *)get_property(cpu, name, NULL);
 		if (fp) {
-			node_found = 1;
-			ppc_tb_freq = *fp;
+			found = 1;
+			*val = 0;
+			while (cells--)
+				*val = (*val << 32) | *fp++;
 		}
+
+		of_node_put(cpu);
 	}
-	if (!node_found)
+
+	return found;
+}
+
+void __init generic_calibrate_decr(void)
+{
+	ppc_tb_freq = DEFAULT_TB_FREQ;		/* hardcoded default */
+
+	if (!get_freq("ibm,extended-timebase-frequency", 2, &ppc_tb_freq) &&
+	    !get_freq("timebase-frequency", 1, &ppc_tb_freq)) {
+
 		printk(KERN_ERR "WARNING: Estimating decrementer frequency "
 				"(not found)\n");
+	}
 
-	ppc_proc_freq = DEFAULT_PROC_FREQ;
-	node_found = 0;
-	if (cpu) {
-		fp = (unsigned int *)get_property(cpu, "clock-frequency",
-						  NULL);
-		if (fp) {
-			node_found = 1;
-			ppc_proc_freq = *fp;
-		}
+	ppc_proc_freq = DEFAULT_PROC_FREQ;	/* hardcoded default */
+
+	if (!get_freq("ibm,extended-clock-frequency", 2, &ppc_proc_freq) &&
+	    !get_freq("clock-frequency", 1, &ppc_proc_freq)) {
+
+		printk(KERN_ERR "WARNING: Estimating processor frequency "
+				"(not found)\n");
 	}
+
 #ifdef CONFIG_BOOKE
 	/* Set the time base to zero */
 	mtspr(SPRN_TBWL, 0);
@@ -904,11 +912,6 @@ void __init generic_calibrate_decr(void)
 	/* Enable decrementer interrupt */
 	mtspr(SPRN_TCR, TCR_DIE);
 #endif
-	if (!node_found)
-		printk(KERN_ERR "WARNING: Estimating processor frequency "
-				"(not found)\n");
-
-	of_node_put(cpu);
 }
 
 unsigned long get_boot_time(void)

commit 4bd174fe1cca738f53cf8bb9ac3cb327b1f516ed
Author: Olof Johansson <olof@lixom.net>
Date:   Tue Apr 18 11:25:53 2006 -0500

    [PATCH] powerpc: Remove stale iseries global
    
    Not even the iSeries maintainer seems to have access to this legendary
    piranha simulator. It adds a bit of ugliness in the common time init
    code, and if it's no longer used we might as well be done with it and
    remove the bloat.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index b861ddcf86bb..528e7f84cb67 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -76,7 +76,6 @@
 
 /* keep track of when we need to update the rtc */
 time_t last_rtc_update;
-extern int piranha_simulator;
 #ifdef CONFIG_PPC_ISERIES
 unsigned long iSeries_recal_titan = 0;
 unsigned long iSeries_recal_tb = 0; 
@@ -1010,10 +1009,7 @@ void __init time_init(void)
 	tb_to_ns_scale = scale;
 	tb_to_ns_shift = shift;
 
-#ifdef CONFIG_PPC_ISERIES
-	if (!piranha_simulator)
-#endif
-		tm = get_boot_time();
+	tm = get_boot_time();
 
 	write_seqlock_irqsave(&xtime_lock, flags);
 

commit 224ad80ac0de102d7bede8d36afbd5ef0a64019f
Author: Olof Johansson <olof@lixom.net>
Date:   Wed Apr 12 15:20:27 2006 -0500

    [PATCH] powerpc: Quiet time init output
    
    Move time_init console output to KERN_DEBUG prink level. No need to
    print it at every boot.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 24e3ad756de0..b861ddcf86bb 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -945,9 +945,9 @@ void __init time_init(void)
 	} else {
 		/* Normal PowerPC with timebase register */
 		ppc_md.calibrate_decr();
-		printk(KERN_INFO "time_init: decrementer frequency = %lu.%.6lu MHz\n",
+		printk(KERN_DEBUG "time_init: decrementer frequency = %lu.%.6lu MHz\n",
 		       ppc_tb_freq / 1000000, ppc_tb_freq % 1000000);
-		printk(KERN_INFO "time_init: processor frequency   = %lu.%.6lu MHz\n",
+		printk(KERN_DEBUG "time_init: processor frequency   = %lu.%.6lu MHz\n",
 		       ppc_proc_freq / 1000000, ppc_proc_freq % 1000000);
 		tb_last_stamp = tb_last_jiffy = get_tb();
 	}

commit 0e5519548fdc8eadc3eacb49b1908d44d347fb2b
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Mar 28 14:50:51 2006 -0800

    [PATCH] for_each_possible_cpu: powerpc
    
    for_each_cpu() actually iterates across all possible CPUs.  We've had mistakes
    in the past where people were using for_each_cpu() where they should have been
    iterating across only online or present CPUs.  This is inefficient and
    possibly buggy.
    
    We're renaming for_each_cpu() to for_each_possible_cpu() to avoid this in the
    future.
    
    This patch replaces for_each_cpu with for_each_possible_cpu.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 4a27218a086c..24e3ad756de0 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -261,7 +261,7 @@ void snapshot_timebases(void)
 
 	if (!cpu_has_feature(CPU_FTR_PURR))
 		return;
-	for_each_cpu(cpu)
+	for_each_possible_cpu(cpu)
 		spin_lock_init(&per_cpu(cpu_purr_data, cpu).lock);
 	on_each_cpu(snapshot_tb_and_purr, NULL, 0, 1);
 }
@@ -751,7 +751,7 @@ void __init smp_space_timers(unsigned int max_cpus)
 	 * systems works better if the two threads' timebase interrupts
 	 * are staggered by half a jiffy with respect to each other.
 	 */
-	for_each_cpu(i) {
+	for_each_possible_cpu(i) {
 		if (i == boot_cpuid)
 			continue;
 		if (i == (boot_cpuid ^ 1))

commit 23dd64011285010ac291f7dddf6e287bdb43a0ad
Merge: 516450179454 485ff0999041
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Mar 17 12:01:19 2006 +1100

    Merge ../linux-2.6

commit 0a45d4491d0f172e02126370f312405c5d473363
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Mar 15 13:47:15 2006 +1100

    powerpc: Fix problem with time going backwards
    
    The recent changes to keep gettimeofday in sync with xtime had the side
    effect that it was occasionally possible for the time reported by
    gettimeofday to go back by a microsecond.  There were two reasons:
    (1) when we recalculated the offsets used by gettimeofday every 2^31
    timebase ticks, we lost an accumulated fractional microsecond, and
    (2) because the update is done some time after the notional start of
    jiffy, if ntp is slowing the clock, it is possible to see time go backwards
    when the timebase factor gets reduced.
    
    This fixes it by (a) slowing the gettimeofday clock by about 1us in
    2^31 timebase ticks (a factor of less than 1 in 3.7 million), and (b)
    adjusting the timebase offsets in the rare case that the gettimeofday
    result could possibly go backwards (i.e. when ntp is slowing the clock
    and the timer interrupt is late).  In this case the adjustment will
    reduce to zero eventually because of (a).
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 2a7ddc579379..86f7e3d154d8 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -283,9 +283,9 @@ static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
 	 * the two values of tb_update_count match and are even then the
 	 * tb_to_xs and stamp_xsec values are consistent.  If not, then it
 	 * loops back and reads them again until this criteria is met.
+	 * We expect the caller to have done the first increment of
+	 * vdso_data->tb_update_count already.
 	 */
-	++(vdso_data->tb_update_count);
-	smp_wmb();
 	vdso_data->tb_orig_stamp = new_tb_stamp;
 	vdso_data->stamp_xsec = new_stamp_xsec;
 	vdso_data->tb_to_xs = new_tb_to_xs;
@@ -310,20 +310,15 @@ static __inline__ void timer_recalc_offset(u64 cur_tb)
 	unsigned long offset;
 	u64 new_stamp_xsec;
 	u64 tlen, t2x;
+	u64 tb, xsec_old, xsec_new;
+	struct gettimeofday_vars *varp;
 
 	if (__USE_RTC())
 		return;
 	tlen = current_tick_length();
 	offset = cur_tb - do_gtod.varp->tb_orig_stamp;
-	if (tlen == last_tick_len && offset < 0x80000000u) {
-		/* check that we're still in sync; if not, resync */
-		struct timeval tv;
-		__do_gettimeofday(&tv, cur_tb);
-		if (tv.tv_sec <= xtime.tv_sec &&
-		    (tv.tv_sec < xtime.tv_sec ||
-		     tv.tv_usec * 1000 <= xtime.tv_nsec))
-			return;
-	}
+	if (tlen == last_tick_len && offset < 0x80000000u)
+		return;
 	if (tlen != last_tick_len) {
 		t2x = mulhdu(tlen << TICKLEN_SHIFT, ticklen_to_xs);
 		last_tick_len = tlen;
@@ -332,6 +327,21 @@ static __inline__ void timer_recalc_offset(u64 cur_tb)
 	new_stamp_xsec = (u64) xtime.tv_nsec * XSEC_PER_SEC;
 	do_div(new_stamp_xsec, 1000000000);
 	new_stamp_xsec += (u64) xtime.tv_sec * XSEC_PER_SEC;
+
+	++vdso_data->tb_update_count;
+	smp_mb();
+
+	/*
+	 * Make sure time doesn't go backwards for userspace gettimeofday.
+	 */
+	tb = get_tb();
+	varp = do_gtod.varp;
+	xsec_old = mulhdu(tb - varp->tb_orig_stamp, varp->tb_to_xs)
+		+ varp->stamp_xsec;
+	xsec_new = mulhdu(tb - cur_tb, t2x) + new_stamp_xsec;
+	if (xsec_new < xsec_old)
+		new_stamp_xsec += xsec_old - xsec_new;
+
 	update_gtod(cur_tb, new_stamp_xsec, t2x);
 }
 
@@ -564,6 +574,10 @@ int do_settimeofday(struct timespec *tv)
 	}
 #endif
 
+	/* Make userspace gettimeofday spin until we're done. */
+	++vdso_data->tb_update_count;
+	smp_mb();
+
 	/*
 	 * Subtract off the number of nanoseconds since the
 	 * beginning of the last tick.
@@ -724,10 +738,16 @@ void __init time_init(void)
 	 * It is computed as:
 	 * ticklen_to_xs = 2^N / (tb_ticks_per_jiffy * 1e9)
 	 * where N = 64 + 20 - TICKLEN_SCALE - TICKLEN_SHIFT
-	 * so as to give the result as a 0.64 fixed-point fraction.
+	 * which turns out to be N = 51 - SHIFT_HZ.
+	 * This gives the result as a 0.64 fixed-point fraction.
+	 * That value is reduced by an offset amounting to 1 xsec per
+	 * 2^31 timebase ticks to avoid problems with time going backwards
+	 * by 1 xsec when we do timer_recalc_offset due to losing the
+	 * fractional xsec.  That offset is equal to ppc_tb_freq/2^51
+	 * since there are 2^20 xsec in a second.
 	 */
-	div128_by_32(1ULL << (64 + 20 - TICKLEN_SCALE - TICKLEN_SHIFT), 0,
-		     tb_ticks_per_jiffy, &res);
+	div128_by_32((1ULL << 51) - ppc_tb_freq, 0,
+		     tb_ticks_per_jiffy << SHIFT_HZ, &res);
 	div128_by_32(res.result_high, res.result_low, NSEC_PER_SEC, &res);
 	ticklen_to_xs = res.result_low;
 

commit 2cf82c0256b198ae28c465f2c4d7c12c836ea5ea
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Feb 27 15:41:47 2006 +1100

    powerpc: Export variables used in conversions to/from cputime_t
    
    The inline cputime_to_foo and foo_to_cputime conversion functions in
    include/asm-powerpc/cputime.h refer to 5 variables, which need to be
    exported if those functions are to be usable from modules.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 0b34db28916f..4f20a5f15d49 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -99,6 +99,7 @@ unsigned long tb_ticks_per_jiffy;
 unsigned long tb_ticks_per_usec = 100; /* sane default */
 EXPORT_SYMBOL(tb_ticks_per_usec);
 unsigned long tb_ticks_per_sec;
+EXPORT_SYMBOL(tb_ticks_per_sec);	/* for cputime_t conversions */
 u64 tb_to_xs;
 unsigned tb_to_us;
 
@@ -143,9 +144,13 @@ DEFINE_PER_CPU(unsigned long, last_jiffy);
  * These are all stored as 0.64 fixed-point binary fractions.
  */
 u64 __cputime_jiffies_factor;
+EXPORT_SYMBOL(__cputime_jiffies_factor);
 u64 __cputime_msec_factor;
+EXPORT_SYMBOL(__cputime_msec_factor);
 u64 __cputime_sec_factor;
+EXPORT_SYMBOL(__cputime_sec_factor);
 u64 __cputime_clockt_factor;
+EXPORT_SYMBOL(__cputime_clockt_factor);
 
 static void calc_cputime_factors(void)
 {

commit c6622f63db86fcbd41bf6fe05ddf2e00c1e51ced
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Feb 24 10:06:59 2006 +1100

    powerpc: Implement accurate task and CPU time accounting
    
    This implements accurate task and cpu time accounting for 64-bit
    powerpc kernels.  Instead of accounting a whole jiffy of time to a
    task on a timer interrupt because that task happened to be running at
    the time, we now account time in units of timebase ticks according to
    the actual time spent by the task in user mode and kernel mode.  We
    also count the time spent processing hardware and software interrupts
    accurately.  This is conditional on CONFIG_VIRT_CPU_ACCOUNTING.  If
    that is not set, we do tick-based approximate accounting as before.
    
    To get this accurate information, we read either the PURR (processor
    utilization of resources register) on POWER5 machines, or the timebase
    on other machines on
    
    * each entry to the kernel from usermode
    * each exit to usermode
    * transitions between process context, hard irq context and soft irq
      context in kernel mode
    * context switches.
    
    On POWER5 systems with shared-processor logical partitioning we also
    read both the PURR and the timebase at each timer interrupt and
    context switch in order to determine how much time has been taken by
    the hypervisor to run other partitions ("steal" time).  Unfortunately,
    since we need values of the PURR on both threads at the same time to
    accurately calculate the steal time, and since we can only calculate
    steal time on a per-core basis, the apportioning of the steal time
    between idle time (time which we ceded to the hypervisor in the idle
    loop) and actual stolen time is somewhat approximate at the moment.
    
    This is all based quite heavily on what s390 does, and it uses the
    generic interfaces that were added by the s390 developers,
    i.e. account_system_time(), account_user_time(), etc.
    
    This patch doesn't add any new interfaces between the kernel and
    userspace, and doesn't change the units in which time is reported to
    userspace by things such as /proc/stat, /proc/<pid>/stat, getrusage(),
    times(), etc.  Internally the various task and cpu times are stored in
    timebase units, but they are converted to USER_HZ units (1/100th of a
    second) when reported to userspace.  Some precision is therefore lost
    but there should not be any accumulating error, since the internal
    accumulation is at full precision.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 2a7ddc579379..0b34db28916f 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -51,6 +51,7 @@
 #include <linux/percpu.h>
 #include <linux/rtc.h>
 #include <linux/jiffies.h>
+#include <linux/posix-timers.h>
 
 #include <asm/io.h>
 #include <asm/processor.h>
@@ -135,6 +136,220 @@ unsigned long tb_last_stamp;
  */
 DEFINE_PER_CPU(unsigned long, last_jiffy);
 
+#ifdef CONFIG_VIRT_CPU_ACCOUNTING
+/*
+ * Factors for converting from cputime_t (timebase ticks) to
+ * jiffies, milliseconds, seconds, and clock_t (1/USER_HZ seconds).
+ * These are all stored as 0.64 fixed-point binary fractions.
+ */
+u64 __cputime_jiffies_factor;
+u64 __cputime_msec_factor;
+u64 __cputime_sec_factor;
+u64 __cputime_clockt_factor;
+
+static void calc_cputime_factors(void)
+{
+	struct div_result res;
+
+	div128_by_32(HZ, 0, tb_ticks_per_sec, &res);
+	__cputime_jiffies_factor = res.result_low;
+	div128_by_32(1000, 0, tb_ticks_per_sec, &res);
+	__cputime_msec_factor = res.result_low;
+	div128_by_32(1, 0, tb_ticks_per_sec, &res);
+	__cputime_sec_factor = res.result_low;
+	div128_by_32(USER_HZ, 0, tb_ticks_per_sec, &res);
+	__cputime_clockt_factor = res.result_low;
+}
+
+/*
+ * Read the PURR on systems that have it, otherwise the timebase.
+ */
+static u64 read_purr(void)
+{
+	if (cpu_has_feature(CPU_FTR_PURR))
+		return mfspr(SPRN_PURR);
+	return mftb();
+}
+
+/*
+ * Account time for a transition between system, hard irq
+ * or soft irq state.
+ */
+void account_system_vtime(struct task_struct *tsk)
+{
+	u64 now, delta;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	now = read_purr();
+	delta = now - get_paca()->startpurr;
+	get_paca()->startpurr = now;
+	if (!in_interrupt()) {
+		delta += get_paca()->system_time;
+		get_paca()->system_time = 0;
+	}
+	account_system_time(tsk, 0, delta);
+	local_irq_restore(flags);
+}
+
+/*
+ * Transfer the user and system times accumulated in the paca
+ * by the exception entry and exit code to the generic process
+ * user and system time records.
+ * Must be called with interrupts disabled.
+ */
+void account_process_vtime(struct task_struct *tsk)
+{
+	cputime_t utime;
+
+	utime = get_paca()->user_time;
+	get_paca()->user_time = 0;
+	account_user_time(tsk, utime);
+}
+
+static void account_process_time(struct pt_regs *regs)
+{
+	int cpu = smp_processor_id();
+
+	account_process_vtime(current);
+	run_local_timers();
+	if (rcu_pending(cpu))
+		rcu_check_callbacks(cpu, user_mode(regs));
+	scheduler_tick();
+ 	run_posix_cpu_timers(current);
+}
+
+#ifdef CONFIG_PPC_SPLPAR
+/*
+ * Stuff for accounting stolen time.
+ */
+struct cpu_purr_data {
+	int	initialized;			/* thread is running */
+	u64	tb0;			/* timebase at origin time */
+	u64	purr0;			/* PURR at origin time */
+	u64	tb;			/* last TB value read */
+	u64	purr;			/* last PURR value read */
+	u64	stolen;			/* stolen time so far */
+	spinlock_t lock;
+};
+
+static DEFINE_PER_CPU(struct cpu_purr_data, cpu_purr_data);
+
+static void snapshot_tb_and_purr(void *data)
+{
+	struct cpu_purr_data *p = &__get_cpu_var(cpu_purr_data);
+
+	p->tb0 = mftb();
+	p->purr0 = mfspr(SPRN_PURR);
+	p->tb = p->tb0;
+	p->purr = 0;
+	wmb();
+	p->initialized = 1;
+}
+
+/*
+ * Called during boot when all cpus have come up.
+ */
+void snapshot_timebases(void)
+{
+	int cpu;
+
+	if (!cpu_has_feature(CPU_FTR_PURR))
+		return;
+	for_each_cpu(cpu)
+		spin_lock_init(&per_cpu(cpu_purr_data, cpu).lock);
+	on_each_cpu(snapshot_tb_and_purr, NULL, 0, 1);
+}
+
+void calculate_steal_time(void)
+{
+	u64 tb, purr, t0;
+	s64 stolen;
+	struct cpu_purr_data *p0, *pme, *phim;
+	int cpu;
+
+	if (!cpu_has_feature(CPU_FTR_PURR))
+		return;
+	cpu = smp_processor_id();
+	pme = &per_cpu(cpu_purr_data, cpu);
+	if (!pme->initialized)
+		return;		/* this can happen in early boot */
+	p0 = &per_cpu(cpu_purr_data, cpu & ~1);
+	phim = &per_cpu(cpu_purr_data, cpu ^ 1);
+	spin_lock(&p0->lock);
+	tb = mftb();
+	purr = mfspr(SPRN_PURR) - pme->purr0;
+	if (!phim->initialized || !cpu_online(cpu ^ 1)) {
+		stolen = (tb - pme->tb) - (purr - pme->purr);
+	} else {
+		t0 = pme->tb0;
+		if (phim->tb0 < t0)
+			t0 = phim->tb0;
+		stolen = phim->tb - t0 - phim->purr - purr - p0->stolen;
+	}
+	if (stolen > 0) {
+		account_steal_time(current, stolen);
+		p0->stolen += stolen;
+	}
+	pme->tb = tb;
+	pme->purr = purr;
+	spin_unlock(&p0->lock);
+}
+
+/*
+ * Must be called before the cpu is added to the online map when
+ * a cpu is being brought up at runtime.
+ */
+static void snapshot_purr(void)
+{
+	int cpu;
+	u64 purr;
+	struct cpu_purr_data *p0, *pme, *phim;
+	unsigned long flags;
+
+	if (!cpu_has_feature(CPU_FTR_PURR))
+		return;
+	cpu = smp_processor_id();
+	pme = &per_cpu(cpu_purr_data, cpu);
+	p0 = &per_cpu(cpu_purr_data, cpu & ~1);
+	phim = &per_cpu(cpu_purr_data, cpu ^ 1);
+	spin_lock_irqsave(&p0->lock, flags);
+	pme->tb = pme->tb0 = mftb();
+	purr = mfspr(SPRN_PURR);
+	if (!phim->initialized) {
+		pme->purr = 0;
+		pme->purr0 = purr;
+	} else {
+		/* set p->purr and p->purr0 for no change in p0->stolen */
+		pme->purr = phim->tb - phim->tb0 - phim->purr - p0->stolen;
+		pme->purr0 = purr - pme->purr;
+	}
+	pme->initialized = 1;
+	spin_unlock_irqrestore(&p0->lock, flags);
+}
+
+#endif /* CONFIG_PPC_SPLPAR */
+
+#else /* ! CONFIG_VIRT_CPU_ACCOUNTING */
+#define calc_cputime_factors()
+#define account_process_time(regs)	update_process_times(user_mode(regs))
+#define calculate_steal_time()		do { } while (0)
+#endif
+
+#if !(defined(CONFIG_VIRT_CPU_ACCOUNTING) && defined(CONFIG_PPC_SPLPAR))
+#define snapshot_purr()			do { } while (0)
+#endif
+
+/*
+ * Called when a cpu comes up after the system has finished booting,
+ * i.e. as a result of a hotplug cpu action.
+ */
+void snapshot_timebase(void)
+{
+	__get_cpu_var(last_jiffy) = get_tb();
+	snapshot_purr();
+}
+
 void __delay(unsigned long loops)
 {
 	unsigned long start;
@@ -382,6 +597,7 @@ static void iSeries_tb_recal(void)
 						new_tb_ticks_per_jiffy, sign, tick_diff );
 				tb_ticks_per_jiffy = new_tb_ticks_per_jiffy;
 				tb_ticks_per_sec   = new_tb_ticks_per_sec;
+				calc_cputime_factors();
 				div128_by_32( XSEC_PER_SEC, 0, tb_ticks_per_sec, &divres );
 				do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
 				tb_to_xs = divres.result_low;
@@ -430,6 +646,7 @@ void timer_interrupt(struct pt_regs * regs)
 	irq_enter();
 
 	profile_tick(CPU_PROFILING, regs);
+	calculate_steal_time();
 
 #ifdef CONFIG_PPC_ISERIES
 	get_lppaca()->int_dword.fields.decr_int = 0;
@@ -451,7 +668,7 @@ void timer_interrupt(struct pt_regs * regs)
 		 * is the case.
 		 */
 		if (!cpu_is_offline(cpu))
-			update_process_times(user_mode(regs));
+			account_process_time(regs);
 
 		/*
 		 * No need to check whether cpu is offline here; boot_cpuid
@@ -508,13 +725,27 @@ void wakeup_decrementer(void)
 void __init smp_space_timers(unsigned int max_cpus)
 {
 	int i;
+	unsigned long half = tb_ticks_per_jiffy / 2;
 	unsigned long offset = tb_ticks_per_jiffy / max_cpus;
 	unsigned long previous_tb = per_cpu(last_jiffy, boot_cpuid);
 
 	/* make sure tb > per_cpu(last_jiffy, cpu) for all cpus always */
 	previous_tb -= tb_ticks_per_jiffy;
+	/*
+	 * The stolen time calculation for POWER5 shared-processor LPAR
+	 * systems works better if the two threads' timebase interrupts
+	 * are staggered by half a jiffy with respect to each other.
+	 */
 	for_each_cpu(i) {
-		if (i != boot_cpuid) {
+		if (i == boot_cpuid)
+			continue;
+		if (i == (boot_cpuid ^ 1))
+			per_cpu(last_jiffy, i) =
+				per_cpu(last_jiffy, boot_cpuid) - half;
+		else if (i & 1)
+			per_cpu(last_jiffy, i) =
+				per_cpu(last_jiffy, i ^ 1) + half;
+		else {
 			previous_tb += offset;
 			per_cpu(last_jiffy, i) = previous_tb;
 		}
@@ -706,6 +937,7 @@ void __init time_init(void)
 	tb_ticks_per_sec = ppc_tb_freq;
 	tb_ticks_per_usec = ppc_tb_freq / 1000000;
 	tb_to_us = mulhwu_scale_factor(ppc_tb_freq, 1000000);
+	calc_cputime_factors();
 
 	/*
 	 * Calculate the length of each tick in ns.  It will not be

commit 092b8f3488a3e50a4ab5f2f3f7c8bbf56b3144e1
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Feb 20 10:38:56 2006 +1100

    powerpc: Keep xtime and gettimeofday in sync
    
    This fixes a regression which was introduced by moving ppc32 to use
    the same sort of lockless gettimeofday as ppc64 has been using for
    some time.  This involves getting the timebase and performing some
    simple arithmetic to convert it to seconds and microseconds.  However,
    the factor and offset used there weren't being updated when NTP
    varied the tick length using adjtimex.  64-bit didn't notice the
    problem because it had a hook in the 32-bit adjtimex compat routine
    that attempted to work out what the generic timekeeping code would
    do and alter the factor and offset to match.  However, that code
    was very complex and it wasn't clear that it still matched what the
    generic code would do.
    
    Now we use the generic current_tick_length() routine that was recently
    added to check that the current tick will be as long as we expect; if
    not we recompute the factor and offset.  This keeps gettimeofday and
    xtime in sync.  In addition we check that gettimeofday hasn't got ahead
    of xtime on each timer interrupt; if it has, we resync.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 1886045a2fd8..2a7ddc579379 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -50,6 +50,7 @@
 #include <linux/security.h>
 #include <linux/percpu.h>
 #include <linux/rtc.h>
+#include <linux/jiffies.h>
 
 #include <asm/io.h>
 #include <asm/processor.h>
@@ -99,7 +100,15 @@ EXPORT_SYMBOL(tb_ticks_per_usec);
 unsigned long tb_ticks_per_sec;
 u64 tb_to_xs;
 unsigned tb_to_us;
-unsigned long processor_freq;
+
+#define TICKLEN_SCALE	(SHIFT_SCALE - 10)
+u64 last_tick_len;	/* units are ns / 2^TICKLEN_SCALE */
+u64 ticklen_to_xs;	/* 0.64 fraction */
+
+/* If last_tick_len corresponds to about 1/HZ seconds, then
+   last_tick_len << TICKLEN_SHIFT will be about 2^63. */
+#define TICKLEN_SHIFT	(63 - 30 - TICKLEN_SCALE + SHIFT_HZ)
+
 DEFINE_SPINLOCK(rtc_lock);
 EXPORT_SYMBOL_GPL(rtc_lock);
 
@@ -113,10 +122,6 @@ extern unsigned long wall_jiffies;
 extern struct timezone sys_tz;
 static long timezone_offset;
 
-void ppc_adjtimex(void);
-
-static unsigned adjusting_time = 0;
-
 unsigned long ppc_proc_freq;
 unsigned long ppc_tb_freq;
 
@@ -178,8 +183,7 @@ static __inline__ void timer_check_rtc(void)
          */
         if (ppc_md.set_rtc_time && ntp_synced() &&
 	    xtime.tv_sec - last_rtc_update >= 659 &&
-	    abs((xtime.tv_nsec/1000) - (1000000-1000000/HZ)) < 500000/HZ &&
-	    jiffies - wall_jiffies == 1) {
+	    abs((xtime.tv_nsec/1000) - (1000000-1000000/HZ)) < 500000/HZ) {
 		struct rtc_time tm;
 		to_tm(xtime.tv_sec + 1 + timezone_offset, &tm);
 		tm.tm_year -= 1900;
@@ -226,15 +230,14 @@ void do_gettimeofday(struct timeval *tv)
 	if (__USE_RTC()) {
 		/* do this the old way */
 		unsigned long flags, seq;
-		unsigned int sec, nsec, usec, lost;
+		unsigned int sec, nsec, usec;
 
 		do {
 			seq = read_seqbegin_irqsave(&xtime_lock, flags);
 			sec = xtime.tv_sec;
 			nsec = xtime.tv_nsec + tb_ticks_since(tb_last_stamp);
-			lost = jiffies - wall_jiffies;
 		} while (read_seqretry_irqrestore(&xtime_lock, seq, flags));
-		usec = nsec / 1000 + lost * (1000000 / HZ);
+		usec = nsec / 1000;
 		while (usec >= 1000000) {
 			usec -= 1000000;
 			++sec;
@@ -248,23 +251,6 @@ void do_gettimeofday(struct timeval *tv)
 
 EXPORT_SYMBOL(do_gettimeofday);
 
-/* Synchronize xtime with do_gettimeofday */ 
-
-static inline void timer_sync_xtime(unsigned long cur_tb)
-{
-#ifdef CONFIG_PPC64
-	/* why do we do this? */
-	struct timeval my_tv;
-
-	__do_gettimeofday(&my_tv, cur_tb);
-
-	if (xtime.tv_sec <= my_tv.tv_sec) {
-		xtime.tv_sec = my_tv.tv_sec;
-		xtime.tv_nsec = my_tv.tv_usec * 1000;
-	}
-#endif
-}
-
 /*
  * There are two copies of tb_to_xs and stamp_xsec so that no
  * lock is needed to access and use these values in
@@ -323,15 +309,30 @@ static __inline__ void timer_recalc_offset(u64 cur_tb)
 {
 	unsigned long offset;
 	u64 new_stamp_xsec;
+	u64 tlen, t2x;
 
 	if (__USE_RTC())
 		return;
+	tlen = current_tick_length();
 	offset = cur_tb - do_gtod.varp->tb_orig_stamp;
-	if ((offset & 0x80000000u) == 0)
-		return;
-	new_stamp_xsec = do_gtod.varp->stamp_xsec
-		+ mulhdu(offset, do_gtod.varp->tb_to_xs);
-	update_gtod(cur_tb, new_stamp_xsec, do_gtod.varp->tb_to_xs);
+	if (tlen == last_tick_len && offset < 0x80000000u) {
+		/* check that we're still in sync; if not, resync */
+		struct timeval tv;
+		__do_gettimeofday(&tv, cur_tb);
+		if (tv.tv_sec <= xtime.tv_sec &&
+		    (tv.tv_sec < xtime.tv_sec ||
+		     tv.tv_usec * 1000 <= xtime.tv_nsec))
+			return;
+	}
+	if (tlen != last_tick_len) {
+		t2x = mulhdu(tlen << TICKLEN_SHIFT, ticklen_to_xs);
+		last_tick_len = tlen;
+	} else
+		t2x = do_gtod.varp->tb_to_xs;
+	new_stamp_xsec = (u64) xtime.tv_nsec * XSEC_PER_SEC;
+	do_div(new_stamp_xsec, 1000000000);
+	new_stamp_xsec += (u64) xtime.tv_sec * XSEC_PER_SEC;
+	update_gtod(cur_tb, new_stamp_xsec, t2x);
 }
 
 #ifdef CONFIG_SMP
@@ -462,13 +463,10 @@ void timer_interrupt(struct pt_regs * regs)
 		write_seqlock(&xtime_lock);
 		tb_last_jiffy += tb_ticks_per_jiffy;
 		tb_last_stamp = per_cpu(last_jiffy, cpu);
-		timer_recalc_offset(tb_last_jiffy);
 		do_timer(regs);
-		timer_sync_xtime(tb_last_jiffy);
+		timer_recalc_offset(tb_last_jiffy);
 		timer_check_rtc();
 		write_sequnlock(&xtime_lock);
-		if (adjusting_time && (time_adjust == 0))
-			ppc_adjtimex();
 	}
 	
 	next_dec = tb_ticks_per_jiffy - ticks;
@@ -492,16 +490,18 @@ void timer_interrupt(struct pt_regs * regs)
 
 void wakeup_decrementer(void)
 {
-	int i;
+	unsigned long ticks;
 
-	set_dec(tb_ticks_per_jiffy);
 	/*
-	 * We don't expect this to be called on a machine with a 601,
-	 * so using get_tbl is fine.
+	 * The timebase gets saved on sleep and restored on wakeup,
+	 * so all we need to do is to reset the decrementer.
 	 */
-	tb_last_stamp = tb_last_jiffy = get_tb();
-	for_each_cpu(i)
-		per_cpu(last_jiffy, i) = tb_last_stamp;
+	ticks = tb_ticks_since(__get_cpu_var(last_jiffy));
+	if (ticks < tb_ticks_per_jiffy)
+		ticks = tb_ticks_per_jiffy - ticks;
+	else
+		ticks = 1;
+	set_dec(ticks);
 }
 
 #ifdef CONFIG_SMP
@@ -541,8 +541,8 @@ int do_settimeofday(struct timespec *tv)
 	time_t wtm_sec, new_sec = tv->tv_sec;
 	long wtm_nsec, new_nsec = tv->tv_nsec;
 	unsigned long flags;
-	long int tb_delta;
-	u64 new_xsec, tb_delta_xs;
+	u64 new_xsec;
+	unsigned long tb_delta;
 
 	if ((unsigned long)tv->tv_nsec >= NSEC_PER_SEC)
 		return -EINVAL;
@@ -563,9 +563,19 @@ int do_settimeofday(struct timespec *tv)
 		first_settimeofday = 0;
 	}
 #endif
+
+	/*
+	 * Subtract off the number of nanoseconds since the
+	 * beginning of the last tick.
+	 * Note that since we don't increment jiffies_64 anywhere other
+	 * than in do_timer (since we don't have a lost tick problem),
+	 * wall_jiffies will always be the same as jiffies,
+	 * and therefore the (jiffies - wall_jiffies) computation
+	 * has been removed.
+	 */
 	tb_delta = tb_ticks_since(tb_last_stamp);
-	tb_delta += (jiffies - wall_jiffies) * tb_ticks_per_jiffy;
-	tb_delta_xs = mulhdu(tb_delta, do_gtod.varp->tb_to_xs);
+	tb_delta = mulhdu(tb_delta, do_gtod.varp->tb_to_xs); /* in xsec */
+	new_nsec -= SCALE_XSEC(tb_delta, 1000000000);
 
 	wtm_sec  = wall_to_monotonic.tv_sec + (xtime.tv_sec - new_sec);
 	wtm_nsec = wall_to_monotonic.tv_nsec + (xtime.tv_nsec - new_nsec);
@@ -580,12 +590,12 @@ int do_settimeofday(struct timespec *tv)
 
 	ntp_clear();
 
-	new_xsec = 0;
-	if (new_nsec != 0) {
-		new_xsec = (u64)new_nsec * XSEC_PER_SEC;
+	new_xsec = xtime.tv_nsec;
+	if (new_xsec != 0) {
+		new_xsec *= XSEC_PER_SEC;
 		do_div(new_xsec, NSEC_PER_SEC);
 	}
-	new_xsec += (u64)new_sec * XSEC_PER_SEC - tb_delta_xs;
+	new_xsec += (u64)xtime.tv_sec * XSEC_PER_SEC;
 	update_gtod(tb_last_jiffy, new_xsec, do_gtod.varp->tb_to_xs);
 
 	vdso_data->tz_minuteswest = sys_tz.tz_minuteswest;
@@ -671,7 +681,7 @@ void __init time_init(void)
 	unsigned long flags;
 	unsigned long tm = 0;
 	struct div_result res;
-	u64 scale;
+	u64 scale, x;
 	unsigned shift;
 
         if (ppc_md.time_init != NULL)
@@ -693,11 +703,36 @@ void __init time_init(void)
 	}
 
 	tb_ticks_per_jiffy = ppc_tb_freq / HZ;
-	tb_ticks_per_sec = tb_ticks_per_jiffy * HZ;
+	tb_ticks_per_sec = ppc_tb_freq;
 	tb_ticks_per_usec = ppc_tb_freq / 1000000;
 	tb_to_us = mulhwu_scale_factor(ppc_tb_freq, 1000000);
-	div128_by_32(1024*1024, 0, tb_ticks_per_sec, &res);
-	tb_to_xs = res.result_low;
+
+	/*
+	 * Calculate the length of each tick in ns.  It will not be
+	 * exactly 1e9/HZ unless ppc_tb_freq is divisible by HZ.
+	 * We compute 1e9 * tb_ticks_per_jiffy / ppc_tb_freq,
+	 * rounded up.
+	 */
+	x = (u64) NSEC_PER_SEC * tb_ticks_per_jiffy + ppc_tb_freq - 1;
+	do_div(x, ppc_tb_freq);
+	tick_nsec = x;
+	last_tick_len = x << TICKLEN_SCALE;
+
+	/*
+	 * Compute ticklen_to_xs, which is a factor which gets multiplied
+	 * by (last_tick_len << TICKLEN_SHIFT) to get a tb_to_xs value.
+	 * It is computed as:
+	 * ticklen_to_xs = 2^N / (tb_ticks_per_jiffy * 1e9)
+	 * where N = 64 + 20 - TICKLEN_SCALE - TICKLEN_SHIFT
+	 * so as to give the result as a 0.64 fixed-point fraction.
+	 */
+	div128_by_32(1ULL << (64 + 20 - TICKLEN_SCALE - TICKLEN_SHIFT), 0,
+		     tb_ticks_per_jiffy, &res);
+	div128_by_32(res.result_high, res.result_low, NSEC_PER_SEC, &res);
+	ticklen_to_xs = res.result_low;
+
+	/* Compute tb_to_xs from tick_nsec */
+	tb_to_xs = mulhdu(last_tick_len << TICKLEN_SHIFT, ticklen_to_xs);
 
 	/*
 	 * Compute scale factor for sched_clock.
@@ -724,6 +759,14 @@ void __init time_init(void)
 		tm = get_boot_time();
 
 	write_seqlock_irqsave(&xtime_lock, flags);
+
+	/* If platform provided a timezone (pmac), we correct the time */
+        if (timezone_offset) {
+		sys_tz.tz_minuteswest = -timezone_offset / 60;
+		sys_tz.tz_dsttime = 0;
+		tm -= timezone_offset;
+        }
+
 	xtime.tv_sec = tm;
 	xtime.tv_nsec = 0;
 	do_gtod.varp = &do_gtod.vars[0];
@@ -738,18 +781,11 @@ void __init time_init(void)
 	vdso_data->tb_orig_stamp = tb_last_jiffy;
 	vdso_data->tb_update_count = 0;
 	vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;
-	vdso_data->stamp_xsec = xtime.tv_sec * XSEC_PER_SEC;
+	vdso_data->stamp_xsec = (u64) xtime.tv_sec * XSEC_PER_SEC;
 	vdso_data->tb_to_xs = tb_to_xs;
 
 	time_freq = 0;
 
-	/* If platform provided a timezone (pmac), we correct the time */
-        if (timezone_offset) {
-		sys_tz.tz_minuteswest = -timezone_offset / 60;
-		sys_tz.tz_dsttime = 0;
-		xtime.tv_sec -= timezone_offset;
-        }
-
 	last_rtc_update = xtime.tv_sec;
 	set_normalized_timespec(&wall_to_monotonic,
 	                        -xtime.tv_sec, -xtime.tv_nsec);
@@ -759,126 +795,6 @@ void __init time_init(void)
 	set_dec(tb_ticks_per_jiffy);
 }
 
-/* 
- * After adjtimex is called, adjust the conversion of tb ticks
- * to microseconds to keep do_gettimeofday synchronized 
- * with ntpd.
- *
- * Use the time_adjust, time_freq and time_offset computed by adjtimex to 
- * adjust the frequency.
- */
-
-/* #define DEBUG_PPC_ADJTIMEX 1 */
-
-void ppc_adjtimex(void)
-{
-#ifdef CONFIG_PPC64
-	unsigned long den, new_tb_ticks_per_sec, tb_ticks, old_xsec,
-		new_tb_to_xs, new_xsec, new_stamp_xsec;
-	unsigned long tb_ticks_per_sec_delta;
-	long delta_freq, ltemp;
-	struct div_result divres; 
-	unsigned long flags;
-	long singleshot_ppm = 0;
-
-	/*
-	 * Compute parts per million frequency adjustment to
-	 * accomplish the time adjustment implied by time_offset to be
-	 * applied over the elapsed time indicated by time_constant.
-	 * Use SHIFT_USEC to get it into the same units as
-	 * time_freq.
-	 */
-	if ( time_offset < 0 ) {
-		ltemp = -time_offset;
-		ltemp <<= SHIFT_USEC - SHIFT_UPDATE;
-		ltemp >>= SHIFT_KG + time_constant;
-		ltemp = -ltemp;
-	} else {
-		ltemp = time_offset;
-		ltemp <<= SHIFT_USEC - SHIFT_UPDATE;
-		ltemp >>= SHIFT_KG + time_constant;
-	}
-	
-	/* If there is a single shot time adjustment in progress */
-	if ( time_adjust ) {
-#ifdef DEBUG_PPC_ADJTIMEX
-		printk("ppc_adjtimex: ");
-		if ( adjusting_time == 0 )
-			printk("starting ");
-		printk("single shot time_adjust = %ld\n", time_adjust);
-#endif	
-	
-		adjusting_time = 1;
-		
-		/*
-		 * Compute parts per million frequency adjustment
-		 * to match time_adjust
-		 */
-		singleshot_ppm = tickadj * HZ;	
-		/*
-		 * The adjustment should be tickadj*HZ to match the code in
-		 * linux/kernel/timer.c, but experiments show that this is too
-		 * large. 3/4 of tickadj*HZ seems about right
-		 */
-		singleshot_ppm -= singleshot_ppm / 4;
-		/* Use SHIFT_USEC to get it into the same units as time_freq */
-		singleshot_ppm <<= SHIFT_USEC;
-		if ( time_adjust < 0 )
-			singleshot_ppm = -singleshot_ppm;
-	}
-	else {
-#ifdef DEBUG_PPC_ADJTIMEX
-		if ( adjusting_time )
-			printk("ppc_adjtimex: ending single shot time_adjust\n");
-#endif
-		adjusting_time = 0;
-	}
-	
-	/* Add up all of the frequency adjustments */
-	delta_freq = time_freq + ltemp + singleshot_ppm;
-	
-	/*
-	 * Compute a new value for tb_ticks_per_sec based on
-	 * the frequency adjustment
-	 */
-	den = 1000000 * (1 << (SHIFT_USEC - 8));
-	if ( delta_freq < 0 ) {
-		tb_ticks_per_sec_delta = ( tb_ticks_per_sec * ( (-delta_freq) >> (SHIFT_USEC - 8))) / den;
-		new_tb_ticks_per_sec = tb_ticks_per_sec + tb_ticks_per_sec_delta;
-	}
-	else {
-		tb_ticks_per_sec_delta = ( tb_ticks_per_sec * ( delta_freq >> (SHIFT_USEC - 8))) / den;
-		new_tb_ticks_per_sec = tb_ticks_per_sec - tb_ticks_per_sec_delta;
-	}
-	
-#ifdef DEBUG_PPC_ADJTIMEX
-	printk("ppc_adjtimex: ltemp = %ld, time_freq = %ld, singleshot_ppm = %ld\n", ltemp, time_freq, singleshot_ppm);
-	printk("ppc_adjtimex: tb_ticks_per_sec - base = %ld  new = %ld\n", tb_ticks_per_sec, new_tb_ticks_per_sec);
-#endif
-
-	/*
-	 * Compute a new value of tb_to_xs (used to convert tb to
-	 * microseconds) and a new value of stamp_xsec which is the
-	 * time (in 1/2^20 second units) corresponding to
-	 * tb_orig_stamp.  This new value of stamp_xsec compensates
-	 * for the change in frequency (implied by the new tb_to_xs)
-	 * which guarantees that the current time remains the same.
-	 */
-	write_seqlock_irqsave( &xtime_lock, flags );
-	tb_ticks = get_tb() - do_gtod.varp->tb_orig_stamp;
-	div128_by_32(1024*1024, 0, new_tb_ticks_per_sec, &divres);
-	new_tb_to_xs = divres.result_low;
-	new_xsec = mulhdu(tb_ticks, new_tb_to_xs);
-
-	old_xsec = mulhdu(tb_ticks, do_gtod.varp->tb_to_xs);
-	new_stamp_xsec = do_gtod.varp->stamp_xsec + old_xsec - new_xsec;
-
-	update_gtod(do_gtod.varp->tb_orig_stamp, new_stamp_xsec, new_tb_to_xs);
-
-	write_sequnlock_irqrestore( &xtime_lock, flags );
-#endif /* CONFIG_PPC64 */
-}
-
 
 #define FEBRUARY	2
 #define	STARTOFTIME	1970

commit d8a8188ded1251afc6a2ec8a668b0bdf038b64a1
Author: Olaf Hering <olh@suse.de>
Date:   Sat Feb 4 10:34:56 2006 +0100

    [PATCH] powerpc: remove pointer/integer confusion in generic_calibrate_decr
    
    remove pointer/integer confusion
    
    Signed-off-by: Olaf Hering <olh@suse.de>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index c4a294d657b9..1886045a2fd8 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -612,10 +612,10 @@ void __init generic_calibrate_decr(void)
 
 	ppc_tb_freq = DEFAULT_TB_FREQ;		/* hardcoded default */
 	node_found = 0;
-	if (cpu != 0) {
+	if (cpu) {
 		fp = (unsigned int *)get_property(cpu, "timebase-frequency",
 						  NULL);
-		if (fp != 0) {
+		if (fp) {
 			node_found = 1;
 			ppc_tb_freq = *fp;
 		}
@@ -626,10 +626,10 @@ void __init generic_calibrate_decr(void)
 
 	ppc_proc_freq = DEFAULT_PROC_FREQ;
 	node_found = 0;
-	if (cpu != 0) {
+	if (cpu) {
 		fp = (unsigned int *)get_property(cpu, "clock-frequency",
 						  NULL);
-		if (fp != 0) {
+		if (fp) {
 			node_found = 1;
 			ppc_proc_freq = *fp;
 		}

commit 3356bb9f7ba378a6e2709f9df95f4ea52111f4df
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Fri Jan 13 10:26:42 2006 +1100

    [PATCH] powerpc: Remove lppaca structure from the PACA
    
    At present the lppaca - the structure shared with the iSeries
    hypervisor and phyp - is contained within the PACA, our own low-level
    per-cpu structure.  This doesn't have to be so, the patch below
    removes it, making a separate array of lppaca structures.
    
    This saves approximately 500*NR_CPUS bytes of image size and kernel
    memory, because we don't need aligning gap between the Linux and
    hypervisor portions of every PACA.  On the other hand it means an
    extra level of dereference in many accesses to the lppaca.
    
    The patch also gets rid of several places where we assign the paca
    address to a local variable for no particular reason.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 56f50e91bddb..c4a294d657b9 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -431,7 +431,7 @@ void timer_interrupt(struct pt_regs * regs)
 	profile_tick(CPU_PROFILING, regs);
 
 #ifdef CONFIG_PPC_ISERIES
-	get_paca()->lppaca.int_dword.fields.decr_int = 0;
+	get_lppaca()->int_dword.fields.decr_int = 0;
 #endif
 
 	while ((ticks = tb_ticks_since(per_cpu(last_jiffy, cpu)))

commit 404849bbd2bfd62e05b36f4753f6e1af6050a824
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Thu Nov 24 16:51:31 2005 +1100

    [PATCH] powerpc: Remove some unneeded fields from the paca
    
    This patch removes several unnecessary fields from the paca:
    
    - next_jiffy_update_tb was simply unused.  Remove trivially.
    
    - The exdsi exception save area was not used.  There were plans to use
      it, but they never seem to have gone anywhere.  If they ever do, we
      can put it back.  Remove from the paca, and from asm-offsets.c
    
    - The default_decr field was used from asm, but was only ever assigned
      the value of tb_ticks_per_jiffy.  Just access tb_ticks_per_jiffy from
      asm directly instead.
    
    Built and booted on POWER5 LPAR and iSeries RS64.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index de8479769bb7..56f50e91bddb 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -699,10 +699,6 @@ void __init time_init(void)
 	div128_by_32(1024*1024, 0, tb_ticks_per_sec, &res);
 	tb_to_xs = res.result_low;
 
-#ifdef CONFIG_PPC64
-	get_paca()->default_decr = tb_ticks_per_jiffy;
-#endif
-
 	/*
 	 * Compute scale factor for sched_clock.
 	 * The calibrate_decr() function has set tb_ticks_per_sec,

commit 6defa38b3754c84cd3449447477aed81ea979407
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Nov 18 13:44:17 2005 +1100

    powerpc: Fix delay functions for 601 processors
    
    My earlier merge of delay.h introduced a timebase-based udelay for
    32-bit machines but also broke the 601, which doesn't have the
    timebase register.  This fixes it by using the 601's RTC register on
    the 601, and also moves __delay() and udelay() to be out-of-line in
    arch/powerpc/kernel/time.c.  These functions aren't really performance
    critical, after all.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 070b4b458aaf..de8479769bb7 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -130,6 +130,34 @@ unsigned long tb_last_stamp;
  */
 DEFINE_PER_CPU(unsigned long, last_jiffy);
 
+void __delay(unsigned long loops)
+{
+	unsigned long start;
+	int diff;
+
+	if (__USE_RTC()) {
+		start = get_rtcl();
+		do {
+			/* the RTCL register wraps at 1000000000 */
+			diff = get_rtcl() - start;
+			if (diff < 0)
+				diff += 1000000000;
+		} while (diff < loops);
+	} else {
+		start = get_tbl();
+		while (get_tbl() - start < loops)
+			HMT_low();
+		HMT_medium();
+	}
+}
+EXPORT_SYMBOL(__delay);
+
+void udelay(unsigned long usecs)
+{
+	__delay(tb_ticks_per_usec * usecs);
+}
+EXPORT_SYMBOL(udelay);
+
 static __inline__ void timer_check_rtc(void)
 {
         /*

commit a7f290dad32ee34d931561b7943c858fe2aae503
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Nov 11 21:15:21 2005 +1100

    [PATCH] powerpc: Merge vdso's and add vdso support to 32 bits kernel
    
    This patch moves the vdso's to arch/powerpc, adds support for the 32
    bits vdso to the 32 bits kernel, rename systemcfg (finally !), and adds
    some new (still untested) routines to both vdso's: clock_gettime() with
    support for CLOCK_REALTIME and CLOCK_MONOTONIC, clock_getres() (same
    clocks) and get_tbfreq() for glibc to retreive the timebase frequency.
    
    Tom,Steve: The implementation of get_tbfreq() I've done for 32 bits
    returns a long long (r3, r4) not a long. This is such that if we ever
    add support for >4Ghz timebases on ppc32, the userland interface won't
    have to change.
    
    I have tested gettimeofday() using some glibc patches in both ppc32 and
    ppc64 kernels using 32 bits userland (I haven't had a chance to test a
    64 bits userland yet, but the implementation didn't change and was
    tested earlier). I haven't tested yet the new functions.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 260b6ecd26a9..070b4b458aaf 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -62,8 +62,8 @@
 #include <asm/irq.h>
 #include <asm/div64.h>
 #include <asm/smp.h>
+#include <asm/vdso_datapage.h>
 #ifdef CONFIG_PPC64
-#include <asm/systemcfg.h>
 #include <asm/firmware.h>
 #endif
 #ifdef CONFIG_PPC_ISERIES
@@ -261,7 +261,6 @@ static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
 	do_gtod.varp = temp_varp;
 	do_gtod.var_idx = temp_idx;
 
-#ifdef CONFIG_PPC64
 	/*
 	 * tb_update_count is used to allow the userspace gettimeofday code
 	 * to assure itself that it sees a consistent view of the tb_to_xs and
@@ -271,14 +270,15 @@ static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
 	 * tb_to_xs and stamp_xsec values are consistent.  If not, then it
 	 * loops back and reads them again until this criteria is met.
 	 */
-	++(_systemcfg->tb_update_count);
+	++(vdso_data->tb_update_count);
 	smp_wmb();
-	_systemcfg->tb_orig_stamp = new_tb_stamp;
-	_systemcfg->stamp_xsec = new_stamp_xsec;
-	_systemcfg->tb_to_xs = new_tb_to_xs;
+	vdso_data->tb_orig_stamp = new_tb_stamp;
+	vdso_data->stamp_xsec = new_stamp_xsec;
+	vdso_data->tb_to_xs = new_tb_to_xs;
+	vdso_data->wtom_clock_sec = wall_to_monotonic.tv_sec;
+	vdso_data->wtom_clock_nsec = wall_to_monotonic.tv_nsec;
 	smp_wmb();
-	++(_systemcfg->tb_update_count);
-#endif
+	++(vdso_data->tb_update_count);
 }
 
 /*
@@ -357,9 +357,8 @@ static void iSeries_tb_recal(void)
 				do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
 				tb_to_xs = divres.result_low;
 				do_gtod.varp->tb_to_xs = tb_to_xs;
-				_systemcfg->tb_ticks_per_sec =
-					tb_ticks_per_sec;
-				_systemcfg->tb_to_xs = tb_to_xs;
+				vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;
+				vdso_data->tb_to_xs = tb_to_xs;
 			}
 			else {
 				printk( "Titan recalibrate: FAILED (difference > 4 percent)\n"
@@ -561,10 +560,8 @@ int do_settimeofday(struct timespec *tv)
 	new_xsec += (u64)new_sec * XSEC_PER_SEC - tb_delta_xs;
 	update_gtod(tb_last_jiffy, new_xsec, do_gtod.varp->tb_to_xs);
 
-#ifdef CONFIG_PPC64
-	_systemcfg->tz_minuteswest = sys_tz.tz_minuteswest;
-	_systemcfg->tz_dsttime = sys_tz.tz_dsttime;
-#endif
+	vdso_data->tz_minuteswest = sys_tz.tz_minuteswest;
+	vdso_data->tz_dsttime = sys_tz.tz_dsttime;
 
 	write_sequnlock_irqrestore(&xtime_lock, flags);
 	clock_was_set();
@@ -713,13 +710,12 @@ void __init time_init(void)
 	do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
 	do_gtod.varp->tb_to_xs = tb_to_xs;
 	do_gtod.tb_to_us = tb_to_us;
-#ifdef CONFIG_PPC64
-	_systemcfg->tb_orig_stamp = tb_last_jiffy;
-	_systemcfg->tb_update_count = 0;
-	_systemcfg->tb_ticks_per_sec = tb_ticks_per_sec;
-	_systemcfg->stamp_xsec = xtime.tv_sec * XSEC_PER_SEC;
-	_systemcfg->tb_to_xs = tb_to_xs;
-#endif
+
+	vdso_data->tb_orig_stamp = tb_last_jiffy;
+	vdso_data->tb_update_count = 0;
+	vdso_data->tb_ticks_per_sec = tb_ticks_per_sec;
+	vdso_data->stamp_xsec = xtime.tv_sec * XSEC_PER_SEC;
+	vdso_data->tb_to_xs = tb_to_xs;
 
 	time_freq = 0;
 

commit cbe62e2b4a764aa3f9a2b9c9004f8e86a1f0ade8
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Nov 10 14:28:03 2005 +1100

    powerpc: Fix SMP time initialization problem
    
    We were getting the last_jiffy per-cpu variable set ahead of the current
    timebase in smp_space_timers on SMP machines.  This caused the loop in
    timer_interrupt to loop virtually forever, since tb_ticks_since assumes
    that it will never be called with the timebase behind the last_jiffy
    value.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 386006b3d616..260b6ecd26a9 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -484,6 +484,8 @@ void __init smp_space_timers(unsigned int max_cpus)
 	unsigned long offset = tb_ticks_per_jiffy / max_cpus;
 	unsigned long previous_tb = per_cpu(last_jiffy, boot_cpuid);
 
+	/* make sure tb > per_cpu(last_jiffy, cpu) for all cpus always */
+	previous_tb -= tb_ticks_per_jiffy;
 	for_each_cpu(i) {
 		if (i != boot_cpuid) {
 			previous_tb += offset;

commit 799d6046d3fb557006e6d7c9767fdb96479b0e0a
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Nov 10 13:37:51 2005 +1100

    [PATCH] powerpc: merge code values for identifying platforms
    
    This patch merges platform codes.  systemcfg->platform is no longer used,
    systemcfg use in general is deprecated as much as possible (and renamed
    _systemcfg before it gets completely moved elsewhere in a future patch),
    _machine is now used on ppc64 along as ppc32.  Platform codes aren't gone
    yet but we are getting a step closer. A bunch of asm code in head[_64].S
    is also turned into C code.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index a6282b625b44..386006b3d616 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -271,13 +271,13 @@ static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
 	 * tb_to_xs and stamp_xsec values are consistent.  If not, then it
 	 * loops back and reads them again until this criteria is met.
 	 */
-	++(systemcfg->tb_update_count);
+	++(_systemcfg->tb_update_count);
 	smp_wmb();
-	systemcfg->tb_orig_stamp = new_tb_stamp;
-	systemcfg->stamp_xsec = new_stamp_xsec;
-	systemcfg->tb_to_xs = new_tb_to_xs;
+	_systemcfg->tb_orig_stamp = new_tb_stamp;
+	_systemcfg->stamp_xsec = new_stamp_xsec;
+	_systemcfg->tb_to_xs = new_tb_to_xs;
 	smp_wmb();
-	++(systemcfg->tb_update_count);
+	++(_systemcfg->tb_update_count);
 #endif
 }
 
@@ -357,8 +357,9 @@ static void iSeries_tb_recal(void)
 				do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
 				tb_to_xs = divres.result_low;
 				do_gtod.varp->tb_to_xs = tb_to_xs;
-				systemcfg->tb_ticks_per_sec = tb_ticks_per_sec;
-				systemcfg->tb_to_xs = tb_to_xs;
+				_systemcfg->tb_ticks_per_sec =
+					tb_ticks_per_sec;
+				_systemcfg->tb_to_xs = tb_to_xs;
 			}
 			else {
 				printk( "Titan recalibrate: FAILED (difference > 4 percent)\n"
@@ -559,8 +560,8 @@ int do_settimeofday(struct timespec *tv)
 	update_gtod(tb_last_jiffy, new_xsec, do_gtod.varp->tb_to_xs);
 
 #ifdef CONFIG_PPC64
-	systemcfg->tz_minuteswest = sys_tz.tz_minuteswest;
-	systemcfg->tz_dsttime = sys_tz.tz_dsttime;
+	_systemcfg->tz_minuteswest = sys_tz.tz_minuteswest;
+	_systemcfg->tz_dsttime = sys_tz.tz_dsttime;
 #endif
 
 	write_sequnlock_irqrestore(&xtime_lock, flags);
@@ -711,11 +712,11 @@ void __init time_init(void)
 	do_gtod.varp->tb_to_xs = tb_to_xs;
 	do_gtod.tb_to_us = tb_to_us;
 #ifdef CONFIG_PPC64
-	systemcfg->tb_orig_stamp = tb_last_jiffy;
-	systemcfg->tb_update_count = 0;
-	systemcfg->tb_ticks_per_sec = tb_ticks_per_sec;
-	systemcfg->stamp_xsec = xtime.tv_sec * XSEC_PER_SEC;
-	systemcfg->tb_to_xs = tb_to_xs;
+	_systemcfg->tb_orig_stamp = tb_last_jiffy;
+	_systemcfg->tb_update_count = 0;
+	_systemcfg->tb_ticks_per_sec = tb_ticks_per_sec;
+	_systemcfg->stamp_xsec = xtime.tv_sec * XSEC_PER_SEC;
+	_systemcfg->tb_to_xs = tb_to_xs;
 #endif
 
 	time_freq = 0;

commit 24bfb00123e82a2e70bd115277d922438813515b
Merge: c6135234550e 3f00d3e8fb96
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Nov 8 11:14:20 2005 +1100

    Merge ../linux-2.6

commit 732ee21f2894819781766a0cd88e32bdd630d11e
Author: Olof Johansson <olof@lixom.net>
Date:   Mon Nov 7 00:57:55 2005 -0800

    [PATCH] POWERPC/PPC64: Fix CONFIG_SMP=n build for ppc64
    
    Two CONFIG_SMP=n build fixes due to missing <asm/smp.h> includes.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 6996a593dcb3..b1c89bc4bf90 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -69,6 +69,7 @@
 #include <asm/iseries/it_lp_queue.h>
 #include <asm/iseries/hv_call_xm.h>
 #endif
+#include <asm/smp.h>
 
 /* keep track of when we need to update the rtc */
 time_t last_rtc_update;

commit 2249ca9d60d3a8a1f6f223f0f0a0283fcb7ce33e
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Nov 7 13:18:13 2005 +1100

    powerpc: Various UP build fixes
    
    Mostly this involves adding #include <asm/smp.h>, since that defines
    things like boot_cpuid[_phys] and [gs]et_hard_smp_processor_id, which
    are SMP-related but still needed on UP.  This incorporates fixes
    posted by Olof Johansson and Heikki Lindholm.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 6996a593dcb3..1c7ea5e06311 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -61,6 +61,7 @@
 #include <asm/prom.h>
 #include <asm/irq.h>
 #include <asm/div64.h>
+#include <asm/smp.h>
 #ifdef CONFIG_PPC64
 #include <asm/systemcfg.h>
 #include <asm/firmware.h>
@@ -118,10 +119,6 @@ static unsigned adjusting_time = 0;
 unsigned long ppc_proc_freq;
 unsigned long ppc_tb_freq;
 
-#ifdef CONFIG_PPC32	/* XXX for now */
-#define boot_cpuid	0
-#endif
-
 u64 tb_last_jiffy __cacheline_aligned_in_smp;
 unsigned long tb_last_stamp;
 

commit 8875ccfb7a6bd69d95a4e889ab36adda06c30d9e
Author: Kelly Daly <kelly@au.ibm.com>
Date:   Wed Nov 2 14:13:34 2005 +1100

    merge filename and modify references to iseries/it_lp_queue.h
    
    Signed-off-by: Kelly Daly <kelly@au.ibm.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 348746c55ebd..6996a593dcb3 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -66,7 +66,7 @@
 #include <asm/firmware.h>
 #endif
 #ifdef CONFIG_PPC_ISERIES
-#include <asm/iSeries/ItLpQueue.h>
+#include <asm/iseries/it_lp_queue.h>
 #include <asm/iseries/hv_call_xm.h>
 #endif
 

commit 8021b8a77690d13ffb93eeadb6728e924d700bd5
Author: Kelly Daly <kelly@au.ibm.com>
Date:   Wed Nov 2 11:41:12 2005 +1100

    merge filename and modify references to iseries/hv_call_xm.h
    
    Signed-off-by: Kelly Daly <kelly@au.ibm.com>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 23436b6c1881..348746c55ebd 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -67,7 +67,7 @@
 #endif
 #ifdef CONFIG_PPC_ISERIES
 #include <asm/iSeries/ItLpQueue.h>
-#include <asm/iSeries/HvCallXm.h>
+#include <asm/iseries/hv_call_xm.h>
 #endif
 
 /* keep track of when we need to update the rtc */

commit 734d6524800b6a8362666e893a5f3f29b9ef0be9
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 31 13:57:01 2005 +1100

    powerpc: apply recent changes to merged code
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index ed5c38fb146c..23436b6c1881 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -70,10 +70,6 @@
 #include <asm/iSeries/HvCallXm.h>
 #endif
 
-u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES;
-
-EXPORT_SYMBOL(jiffies_64);
-
 /* keep track of when we need to update the rtc */
 time_t last_rtc_update;
 extern int piranha_simulator;

commit 5f6b5b973a125de0dbe236ce659a495787c81ff0
Author: Paul Mackerras <paulus@samba.org>
Date:   Sun Oct 30 22:55:52 2005 +1100

    powerpc: Fix time setting bug on 32-bit
    
    This fixes a bug where settimeofday would set the wrong parameters
    in do_gtod, resulting in gettimeofday returning a value about 4
    hours after the correct time.  The bug was that we divided a
    negative 64-bit value with do_div, which treated it as unsigned
    and gave us a result that was approximately 1.8e10 too large
    (since the divisor was 1e9).
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 9f264c2f02c3..ed5c38fb146c 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -518,7 +518,7 @@ int do_settimeofday(struct timespec *tv)
 	long wtm_nsec, new_nsec = tv->tv_nsec;
 	unsigned long flags;
 	long int tb_delta;
-	u64 new_xsec;
+	u64 new_xsec, tb_delta_xs;
 
 	if ((unsigned long)tv->tv_nsec >= NSEC_PER_SEC)
 		return -EINVAL;
@@ -541,8 +541,7 @@ int do_settimeofday(struct timespec *tv)
 #endif
 	tb_delta = tb_ticks_since(tb_last_stamp);
 	tb_delta += (jiffies - wall_jiffies) * tb_ticks_per_jiffy;
-
-	new_nsec -= 1000 * mulhwu(tb_to_us, tb_delta);
+	tb_delta_xs = mulhdu(tb_delta, do_gtod.varp->tb_to_xs);
 
 	wtm_sec  = wall_to_monotonic.tv_sec + (xtime.tv_sec - new_sec);
 	wtm_nsec = wall_to_monotonic.tv_nsec + (xtime.tv_nsec - new_nsec);
@@ -557,9 +556,12 @@ int do_settimeofday(struct timespec *tv)
 
 	ntp_clear();
 
-	new_xsec = (u64)new_nsec * XSEC_PER_SEC;
-	do_div(new_xsec, NSEC_PER_SEC);
-	new_xsec += (u64)new_sec * XSEC_PER_SEC;
+	new_xsec = 0;
+	if (new_nsec != 0) {
+		new_xsec = (u64)new_nsec * XSEC_PER_SEC;
+		do_div(new_xsec, NSEC_PER_SEC);
+	}
+	new_xsec += (u64)new_sec * XSEC_PER_SEC - tb_delta_xs;
 	update_gtod(tb_last_jiffy, new_xsec, do_gtod.varp->tb_to_xs);
 
 #ifdef CONFIG_PPC64

commit 0fd6f717948083d480f38e97f62cc116faf0e534
Author: Kumar Gala <galak@freescale.com>
Date:   Tue Oct 25 23:02:59 2005 -0500

    [PATCH] powerpc: Add support for Book-E timer config to generic_calibrate_decr
    
    We need to initialize some control SPRS for timers on Book-E before
    we start taking decrementer interrupts.
    
    Signed-off-by: Kumar K. Gala <kumar.gala@freescale.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 82f2cabb4f47..9f264c2f02c3 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -610,6 +610,17 @@ void __init generic_calibrate_decr(void)
 			ppc_proc_freq = *fp;
 		}
 	}
+#ifdef CONFIG_BOOKE
+	/* Set the time base to zero */
+	mtspr(SPRN_TBWL, 0);
+	mtspr(SPRN_TBWU, 0);
+
+	/* Clear any pending timer interrupts */
+	mtspr(SPRN_TSR, TSR_ENW | TSR_WIS | TSR_DIS | TSR_FIS);
+
+	/* Enable decrementer interrupt */
+	mtspr(SPRN_TCR, TCR_DIE);
+#endif
 	if (!node_found)
 		printk(KERN_ERR "WARNING: Estimating processor frequency "
 				"(not found)\n");

commit d2e615125762459fd2bb9f0c91881509a0b9c31b
Author: Kumar Gala <galak@freescale.com>
Date:   Thu Oct 20 11:43:33 2005 -0500

    [PATCH] powerpc: Make sure we have an RTC before trying to adjust it
    
    Its valid for ppc_md.set_rtc_time to be NULL.  We need to check
    that its non-NULL before trying to update the RTC.
    
    Signed-off-by: Kumar K. Gala <kumar.gala@freescale.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index ad501d62aa6e..82f2cabb4f47 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -154,7 +154,7 @@ static __inline__ void timer_check_rtc(void)
          * We should have an rtc call that only sets the minutes and
          * seconds like on Intel to avoid problems with non UTC clocks.
          */
-        if (ntp_synced() &&
+        if (ppc_md.set_rtc_time && ntp_synced() &&
 	    xtime.tv_sec - last_rtc_update >= 659 &&
 	    abs((xtime.tv_nsec/1000) - (1000000-1000000/HZ)) < 500000/HZ &&
 	    jiffies - wall_jiffies == 1) {

commit 96c44507601d64f29b8ccc867637292e326c7019
Author: Paul Mackerras <paulus@samba.org>
Date:   Sun Oct 23 17:14:56 2005 +1000

    powerpc: Fix time code for 601 processors
    
    The 601 doesn't have the timebase register; instead it has an RTCL
    register that counts nanoseconds and wraps at 1000000000, and an
    RTCU register that counts seconds.  This makes the necessary changes
    for the merged time code to use the RTCL/U registers when the kernel
    is running on a 601.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index b635c7de6698..ad501d62aa6e 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -126,6 +126,16 @@ unsigned long ppc_tb_freq;
 #define boot_cpuid	0
 #endif
 
+u64 tb_last_jiffy __cacheline_aligned_in_smp;
+unsigned long tb_last_stamp;
+
+/*
+ * Note that on ppc32 this only stores the bottom 32 bits of
+ * the timebase value, but that's enough to tell when a jiffy
+ * has passed.
+ */
+DEFINE_PER_CPU(unsigned long, last_jiffy);
+
 static __inline__ void timer_check_rtc(void)
 {
         /*
@@ -191,6 +201,26 @@ static inline void __do_gettimeofday(struct timeval *tv, u64 tb_val)
 
 void do_gettimeofday(struct timeval *tv)
 {
+	if (__USE_RTC()) {
+		/* do this the old way */
+		unsigned long flags, seq;
+		unsigned int sec, nsec, usec, lost;
+
+		do {
+			seq = read_seqbegin_irqsave(&xtime_lock, flags);
+			sec = xtime.tv_sec;
+			nsec = xtime.tv_nsec + tb_ticks_since(tb_last_stamp);
+			lost = jiffies - wall_jiffies;
+		} while (read_seqretry_irqrestore(&xtime_lock, seq, flags));
+		usec = nsec / 1000 + lost * (1000000 / HZ);
+		while (usec >= 1000000) {
+			usec -= 1000000;
+			++sec;
+		}
+		tv->tv_sec = sec;
+		tv->tv_usec = usec;
+		return;
+	}
 	__do_gettimeofday(tv, get_tb());
 }
 
@@ -272,6 +302,8 @@ static __inline__ void timer_recalc_offset(u64 cur_tb)
 	unsigned long offset;
 	u64 new_stamp_xsec;
 
+	if (__USE_RTC())
+		return;
 	offset = cur_tb - do_gtod.varp->tb_orig_stamp;
 	if ((offset & 0x80000000u) == 0)
 		return;
@@ -357,15 +389,6 @@ static void iSeries_tb_recal(void)
  * call will not be needed)
  */
 
-u64 tb_last_stamp __cacheline_aligned_in_smp;
-
-/*
- * Note that on ppc32 this only stores the bottom 32 bits of
- * the timebase value, but that's enough to tell when a jiffy
- * has passed.
- */
-DEFINE_PER_CPU(unsigned long, last_jiffy);
-
 /*
  * timer_interrupt - gets called when the decrementer overflows,
  * with interrupts disabled.
@@ -415,10 +438,11 @@ void timer_interrupt(struct pt_regs * regs)
 			continue;
 
 		write_seqlock(&xtime_lock);
-		tb_last_stamp += tb_ticks_per_jiffy;
-		timer_recalc_offset(tb_last_stamp);
+		tb_last_jiffy += tb_ticks_per_jiffy;
+		tb_last_stamp = per_cpu(last_jiffy, cpu);
+		timer_recalc_offset(tb_last_jiffy);
 		do_timer(regs);
-		timer_sync_xtime(tb_last_stamp);
+		timer_sync_xtime(tb_last_jiffy);
 		timer_check_rtc();
 		write_sequnlock(&xtime_lock);
 		if (adjusting_time && (time_adjust == 0))
@@ -453,7 +477,7 @@ void wakeup_decrementer(void)
 	 * We don't expect this to be called on a machine with a 601,
 	 * so using get_tbl is fine.
 	 */
-	tb_last_stamp = get_tb();
+	tb_last_stamp = tb_last_jiffy = get_tb();
 	for_each_cpu(i)
 		per_cpu(last_jiffy, i) = tb_last_stamp;
 }
@@ -483,6 +507,8 @@ void __init smp_space_timers(unsigned int max_cpus)
  */
 unsigned long long sched_clock(void)
 {
+	if (__USE_RTC())
+		return get_rtc();
 	return mulhdu(get_tb(), tb_to_ns_scale) << tb_to_ns_shift;
 }
 
@@ -534,7 +560,7 @@ int do_settimeofday(struct timespec *tv)
 	new_xsec = (u64)new_nsec * XSEC_PER_SEC;
 	do_div(new_xsec, NSEC_PER_SEC);
 	new_xsec += (u64)new_sec * XSEC_PER_SEC;
-	update_gtod(tb_last_stamp, new_xsec, do_gtod.varp->tb_to_xs);
+	update_gtod(tb_last_jiffy, new_xsec, do_gtod.varp->tb_to_xs);
 
 #ifdef CONFIG_PPC64
 	systemcfg->tz_minuteswest = sys_tz.tz_minuteswest;
@@ -616,12 +642,20 @@ void __init time_init(void)
         if (ppc_md.time_init != NULL)
                 timezone_offset = ppc_md.time_init();
 
-	ppc_md.calibrate_decr();
-
-	printk(KERN_INFO "time_init: decrementer frequency = %lu.%.6lu MHz\n",
-	       ppc_tb_freq / 1000000, ppc_tb_freq % 1000000);
-	printk(KERN_INFO "time_init: processor frequency   = %lu.%.6lu MHz\n",
-	       ppc_proc_freq / 1000000, ppc_proc_freq % 1000000);
+	if (__USE_RTC()) {
+		/* 601 processor: dec counts down by 128 every 128ns */
+		ppc_tb_freq = 1000000000;
+		tb_last_stamp = get_rtcl();
+		tb_last_jiffy = tb_last_stamp;
+	} else {
+		/* Normal PowerPC with timebase register */
+		ppc_md.calibrate_decr();
+		printk(KERN_INFO "time_init: decrementer frequency = %lu.%.6lu MHz\n",
+		       ppc_tb_freq / 1000000, ppc_tb_freq % 1000000);
+		printk(KERN_INFO "time_init: processor frequency   = %lu.%.6lu MHz\n",
+		       ppc_proc_freq / 1000000, ppc_proc_freq % 1000000);
+		tb_last_stamp = tb_last_jiffy = get_tb();
+	}
 
 	tb_ticks_per_jiffy = ppc_tb_freq / HZ;
 	tb_ticks_per_sec = tb_ticks_per_jiffy * HZ;
@@ -661,17 +695,16 @@ void __init time_init(void)
 	write_seqlock_irqsave(&xtime_lock, flags);
 	xtime.tv_sec = tm;
 	xtime.tv_nsec = 0;
-	tb_last_stamp = get_tb();
 	do_gtod.varp = &do_gtod.vars[0];
 	do_gtod.var_idx = 0;
-	do_gtod.varp->tb_orig_stamp = tb_last_stamp;
+	do_gtod.varp->tb_orig_stamp = tb_last_jiffy;
 	__get_cpu_var(last_jiffy) = tb_last_stamp;
 	do_gtod.varp->stamp_xsec = (u64) xtime.tv_sec * XSEC_PER_SEC;
 	do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
 	do_gtod.varp->tb_to_xs = tb_to_xs;
 	do_gtod.tb_to_us = tb_to_us;
 #ifdef CONFIG_PPC64
-	systemcfg->tb_orig_stamp = tb_last_stamp;
+	systemcfg->tb_orig_stamp = tb_last_jiffy;
 	systemcfg->tb_update_count = 0;
 	systemcfg->tb_ticks_per_sec = tb_ticks_per_sec;
 	systemcfg->stamp_xsec = xtime.tv_sec * XSEC_PER_SEC;

commit a5b518ed314bfd25ea5e433ce09f8b27080023db
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Oct 22 14:55:23 2005 +1000

    ppc64/powerpc: Fix time initialization on SMP systems
    
    This moves smp_space_timers from arch/ppc64/kernel/smp.c to
    arch/powerpc/kernel/time.c and makes it initialize last_jiffy[]
    instead of paca[].next_jiffy_update_tb, since last_jiffy[] is
    now what the time code uses.  It also declares smp_space_timers
    in include/asm-powerpc/time.h and gets rid of an ifdef in
    div128_by_32.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 3e722370113b..b635c7de6698 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -458,7 +458,7 @@ void wakeup_decrementer(void)
 		per_cpu(last_jiffy, i) = tb_last_stamp;
 }
 
-#ifdef CONFIG_SMPxxx
+#ifdef CONFIG_SMP
 void __init smp_space_timers(unsigned int max_cpus)
 {
 	int i;
@@ -948,16 +948,6 @@ void div128_by_32(u64 dividend_high, u64 dividend_low,
 	w = a / divisor;
 	ra = ((u64)(a - (w * divisor)) << 32) + b;
 
-#ifdef CONFIG_PPC64
-	x = ra / divisor;
-	rb = ((ra - (x * divisor)) << 32) + c;
-
-	y = rb / divisor;
-	rc = ((rb - (y * divisor)) << 32) + d;
-
-	z = rc / divisor;
-#else
-	/* for 32-bit, use do_div from div64.h */
 	rb = ((u64) do_div(ra, divisor) << 32) + c;
 	x = ra;
 
@@ -966,10 +956,8 @@ void div128_by_32(u64 dividend_high, u64 dividend_low,
 
 	do_div(rc, divisor);
 	z = rc;
-#endif
 
 	dr->result_high = ((u64)w << 32) + x;
 	dr->result_low  = ((u64)y << 32) + z;
 
 }
-

commit 5d14a18d59b661356409e5a1f624236155a209ba
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 20 22:33:06 2005 +1000

    powerpc: Fix some bugs in the new merged time code
    
    I had the sense of the test for when to use the old 601-style RTC
    registers inverted.  pmac_calibrate_decr and via_calibrate_decr
    weren't setting ppc_tb_freq, on which all the further calculations
    depended.  Lastly, update_gtod was losing the top 32 bits of
    the new tb_to_xs value.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 7d406b73a855..3e722370113b 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -222,7 +222,7 @@ static inline void timer_sync_xtime(unsigned long cur_tb)
  * between updates.
  */
 static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
-			       unsigned int new_tb_to_xs)
+			       u64 new_tb_to_xs)
 {
 	unsigned temp_idx;
 	struct gettimeofday_vars *temp_varp;

commit 374e99d450a0c44dc30041fa83eccfd0890330c7
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 20 21:04:51 2005 +1000

    powerpc: Move some calculations from xxx_calibrate_decr to time_init
    
    Previously the individual xxx_calibrate_decr functions would each
    print the timebase and cpu frequency and calculate several values
    such as tb_to_us and tb_to_xs.  This moves those printks and
    calculations into time_init just after the call to the platform's
    calibrate_decr function.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index d1608473075f..7d406b73a855 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -548,11 +548,9 @@ int do_settimeofday(struct timespec *tv)
 
 EXPORT_SYMBOL(do_settimeofday);
 
-#if defined(CONFIG_PPC_PSERIES) || defined(CONFIG_PPC_MAPLE) || defined(CONFIG_PPC_BPA) || defined(CONFIG_PPC_ISERIES)
 void __init generic_calibrate_decr(void)
 {
 	struct device_node *cpu;
-	struct div_result divres;
 	unsigned int *fp;
 	int node_found;
 
@@ -591,20 +589,7 @@ void __init generic_calibrate_decr(void)
 				"(not found)\n");
 
 	of_node_put(cpu);
-
-	printk(KERN_INFO "time_init: decrementer frequency = %lu.%.6lu MHz\n",
-	       ppc_tb_freq/1000000, ppc_tb_freq%1000000);
-	printk(KERN_INFO "time_init: processor frequency   = %lu.%.6lu MHz\n",
-	       ppc_proc_freq/1000000, ppc_proc_freq%1000000);
-
-	tb_ticks_per_jiffy = ppc_tb_freq / HZ;
-	tb_ticks_per_sec = tb_ticks_per_jiffy * HZ;
-	tb_ticks_per_usec = ppc_tb_freq / 1000000;
-	tb_to_us = mulhwu_scale_factor(ppc_tb_freq, 1000000);
-	div128_by_32(1024*1024, 0, tb_ticks_per_sec, &divres);
-	tb_to_xs = divres.result_low;
 }
-#endif
 
 unsigned long get_boot_time(void)
 {
@@ -633,6 +618,18 @@ void __init time_init(void)
 
 	ppc_md.calibrate_decr();
 
+	printk(KERN_INFO "time_init: decrementer frequency = %lu.%.6lu MHz\n",
+	       ppc_tb_freq / 1000000, ppc_tb_freq % 1000000);
+	printk(KERN_INFO "time_init: processor frequency   = %lu.%.6lu MHz\n",
+	       ppc_proc_freq / 1000000, ppc_proc_freq % 1000000);
+
+	tb_ticks_per_jiffy = ppc_tb_freq / HZ;
+	tb_ticks_per_sec = tb_ticks_per_jiffy * HZ;
+	tb_ticks_per_usec = ppc_tb_freq / 1000000;
+	tb_to_us = mulhwu_scale_factor(ppc_tb_freq, 1000000);
+	div128_by_32(1024*1024, 0, tb_ticks_per_sec, &res);
+	tb_to_xs = res.result_low;
+
 #ifdef CONFIG_PPC64
 	get_paca()->default_decr = tb_ticks_per_jiffy;
 #endif

commit f2783c15007468c14972e2617db51e9affc7fad9
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 20 09:23:26 2005 +1000

    powerpc: Merge time.c and asm/time.h.
    
    We now use the merged time.c for both 32-bit and 64-bit compilation
    with ARCH=powerpc, and for ARCH=ppc64, but not for ARCH=ppc32.
    This removes setup_default_decr (folds its function into time_init)
    and moves wakeup_decrementer into time.c.  This also makes an
    asm-powerpc/rtc.h.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
new file mode 100644
index 000000000000..d1608473075f
--- /dev/null
+++ b/arch/powerpc/kernel/time.c
@@ -0,0 +1,978 @@
+/*
+ * Common time routines among all ppc machines.
+ *
+ * Written by Cort Dougan (cort@cs.nmt.edu) to merge
+ * Paul Mackerras' version and mine for PReP and Pmac.
+ * MPC8xx/MBX changes by Dan Malek (dmalek@jlc.net).
+ * Converted for 64-bit by Mike Corrigan (mikejc@us.ibm.com)
+ *
+ * First round of bugfixes by Gabriel Paubert (paubert@iram.es)
+ * to make clock more stable (2.4.0-test5). The only thing
+ * that this code assumes is that the timebases have been synchronized
+ * by firmware on SMP and are never stopped (never do sleep
+ * on SMP then, nap and doze are OK).
+ * 
+ * Speeded up do_gettimeofday by getting rid of references to
+ * xtime (which required locks for consistency). (mikejc@us.ibm.com)
+ *
+ * TODO (not necessarily in this file):
+ * - improve precision and reproducibility of timebase frequency
+ * measurement at boot time. (for iSeries, we calibrate the timebase
+ * against the Titan chip's clock.)
+ * - for astronomical applications: add a new function to get
+ * non ambiguous timestamps even around leap seconds. This needs
+ * a new timestamp format and a good name.
+ *
+ * 1997-09-10  Updated NTP code according to technical memorandum Jan '96
+ *             "A Kernel Model for Precision Timekeeping" by Dave Mills
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/config.h>
+#include <linux/errno.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/param.h>
+#include <linux/string.h>
+#include <linux/mm.h>
+#include <linux/interrupt.h>
+#include <linux/timex.h>
+#include <linux/kernel_stat.h>
+#include <linux/time.h>
+#include <linux/init.h>
+#include <linux/profile.h>
+#include <linux/cpu.h>
+#include <linux/security.h>
+#include <linux/percpu.h>
+#include <linux/rtc.h>
+
+#include <asm/io.h>
+#include <asm/processor.h>
+#include <asm/nvram.h>
+#include <asm/cache.h>
+#include <asm/machdep.h>
+#include <asm/uaccess.h>
+#include <asm/time.h>
+#include <asm/prom.h>
+#include <asm/irq.h>
+#include <asm/div64.h>
+#ifdef CONFIG_PPC64
+#include <asm/systemcfg.h>
+#include <asm/firmware.h>
+#endif
+#ifdef CONFIG_PPC_ISERIES
+#include <asm/iSeries/ItLpQueue.h>
+#include <asm/iSeries/HvCallXm.h>
+#endif
+
+u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES;
+
+EXPORT_SYMBOL(jiffies_64);
+
+/* keep track of when we need to update the rtc */
+time_t last_rtc_update;
+extern int piranha_simulator;
+#ifdef CONFIG_PPC_ISERIES
+unsigned long iSeries_recal_titan = 0;
+unsigned long iSeries_recal_tb = 0; 
+static unsigned long first_settimeofday = 1;
+#endif
+
+/* The decrementer counts down by 128 every 128ns on a 601. */
+#define DECREMENTER_COUNT_601	(1000000000 / HZ)
+
+#define XSEC_PER_SEC (1024*1024)
+
+#ifdef CONFIG_PPC64
+#define SCALE_XSEC(xsec, max)	(((xsec) * max) / XSEC_PER_SEC)
+#else
+/* compute ((xsec << 12) * max) >> 32 */
+#define SCALE_XSEC(xsec, max)	mulhwu((xsec) << 12, max)
+#endif
+
+unsigned long tb_ticks_per_jiffy;
+unsigned long tb_ticks_per_usec = 100; /* sane default */
+EXPORT_SYMBOL(tb_ticks_per_usec);
+unsigned long tb_ticks_per_sec;
+u64 tb_to_xs;
+unsigned tb_to_us;
+unsigned long processor_freq;
+DEFINE_SPINLOCK(rtc_lock);
+EXPORT_SYMBOL_GPL(rtc_lock);
+
+u64 tb_to_ns_scale;
+unsigned tb_to_ns_shift;
+
+struct gettimeofday_struct do_gtod;
+
+extern unsigned long wall_jiffies;
+
+extern struct timezone sys_tz;
+static long timezone_offset;
+
+void ppc_adjtimex(void);
+
+static unsigned adjusting_time = 0;
+
+unsigned long ppc_proc_freq;
+unsigned long ppc_tb_freq;
+
+#ifdef CONFIG_PPC32	/* XXX for now */
+#define boot_cpuid	0
+#endif
+
+static __inline__ void timer_check_rtc(void)
+{
+        /*
+         * update the rtc when needed, this should be performed on the
+         * right fraction of a second. Half or full second ?
+         * Full second works on mk48t59 clocks, others need testing.
+         * Note that this update is basically only used through 
+         * the adjtimex system calls. Setting the HW clock in
+         * any other way is a /dev/rtc and userland business.
+         * This is still wrong by -0.5/+1.5 jiffies because of the
+         * timer interrupt resolution and possible delay, but here we 
+         * hit a quantization limit which can only be solved by higher
+         * resolution timers and decoupling time management from timer
+         * interrupts. This is also wrong on the clocks
+         * which require being written at the half second boundary.
+         * We should have an rtc call that only sets the minutes and
+         * seconds like on Intel to avoid problems with non UTC clocks.
+         */
+        if (ntp_synced() &&
+	    xtime.tv_sec - last_rtc_update >= 659 &&
+	    abs((xtime.tv_nsec/1000) - (1000000-1000000/HZ)) < 500000/HZ &&
+	    jiffies - wall_jiffies == 1) {
+		struct rtc_time tm;
+		to_tm(xtime.tv_sec + 1 + timezone_offset, &tm);
+		tm.tm_year -= 1900;
+		tm.tm_mon -= 1;
+		if (ppc_md.set_rtc_time(&tm) == 0)
+			last_rtc_update = xtime.tv_sec + 1;
+		else
+			/* Try again one minute later */
+			last_rtc_update += 60;
+        }
+}
+
+/*
+ * This version of gettimeofday has microsecond resolution.
+ */
+static inline void __do_gettimeofday(struct timeval *tv, u64 tb_val)
+{
+	unsigned long sec, usec;
+	u64 tb_ticks, xsec;
+	struct gettimeofday_vars *temp_varp;
+	u64 temp_tb_to_xs, temp_stamp_xsec;
+
+	/*
+	 * These calculations are faster (gets rid of divides)
+	 * if done in units of 1/2^20 rather than microseconds.
+	 * The conversion to microseconds at the end is done
+	 * without a divide (and in fact, without a multiply)
+	 */
+	temp_varp = do_gtod.varp;
+	tb_ticks = tb_val - temp_varp->tb_orig_stamp;
+	temp_tb_to_xs = temp_varp->tb_to_xs;
+	temp_stamp_xsec = temp_varp->stamp_xsec;
+	xsec = temp_stamp_xsec + mulhdu(tb_ticks, temp_tb_to_xs);
+	sec = xsec / XSEC_PER_SEC;
+	usec = (unsigned long)xsec & (XSEC_PER_SEC - 1);
+	usec = SCALE_XSEC(usec, 1000000);
+
+	tv->tv_sec = sec;
+	tv->tv_usec = usec;
+}
+
+void do_gettimeofday(struct timeval *tv)
+{
+	__do_gettimeofday(tv, get_tb());
+}
+
+EXPORT_SYMBOL(do_gettimeofday);
+
+/* Synchronize xtime with do_gettimeofday */ 
+
+static inline void timer_sync_xtime(unsigned long cur_tb)
+{
+#ifdef CONFIG_PPC64
+	/* why do we do this? */
+	struct timeval my_tv;
+
+	__do_gettimeofday(&my_tv, cur_tb);
+
+	if (xtime.tv_sec <= my_tv.tv_sec) {
+		xtime.tv_sec = my_tv.tv_sec;
+		xtime.tv_nsec = my_tv.tv_usec * 1000;
+	}
+#endif
+}
+
+/*
+ * There are two copies of tb_to_xs and stamp_xsec so that no
+ * lock is needed to access and use these values in
+ * do_gettimeofday.  We alternate the copies and as long as a
+ * reasonable time elapses between changes, there will never
+ * be inconsistent values.  ntpd has a minimum of one minute
+ * between updates.
+ */
+static inline void update_gtod(u64 new_tb_stamp, u64 new_stamp_xsec,
+			       unsigned int new_tb_to_xs)
+{
+	unsigned temp_idx;
+	struct gettimeofday_vars *temp_varp;
+
+	temp_idx = (do_gtod.var_idx == 0);
+	temp_varp = &do_gtod.vars[temp_idx];
+
+	temp_varp->tb_to_xs = new_tb_to_xs;
+	temp_varp->tb_orig_stamp = new_tb_stamp;
+	temp_varp->stamp_xsec = new_stamp_xsec;
+	smp_mb();
+	do_gtod.varp = temp_varp;
+	do_gtod.var_idx = temp_idx;
+
+#ifdef CONFIG_PPC64
+	/*
+	 * tb_update_count is used to allow the userspace gettimeofday code
+	 * to assure itself that it sees a consistent view of the tb_to_xs and
+	 * stamp_xsec variables.  It reads the tb_update_count, then reads
+	 * tb_to_xs and stamp_xsec and then reads tb_update_count again.  If
+	 * the two values of tb_update_count match and are even then the
+	 * tb_to_xs and stamp_xsec values are consistent.  If not, then it
+	 * loops back and reads them again until this criteria is met.
+	 */
+	++(systemcfg->tb_update_count);
+	smp_wmb();
+	systemcfg->tb_orig_stamp = new_tb_stamp;
+	systemcfg->stamp_xsec = new_stamp_xsec;
+	systemcfg->tb_to_xs = new_tb_to_xs;
+	smp_wmb();
+	++(systemcfg->tb_update_count);
+#endif
+}
+
+/*
+ * When the timebase - tb_orig_stamp gets too big, we do a manipulation
+ * between tb_orig_stamp and stamp_xsec. The goal here is to keep the
+ * difference tb - tb_orig_stamp small enough to always fit inside a
+ * 32 bits number. This is a requirement of our fast 32 bits userland
+ * implementation in the vdso. If we "miss" a call to this function
+ * (interrupt latency, CPU locked in a spinlock, ...) and we end up
+ * with a too big difference, then the vdso will fallback to calling
+ * the syscall
+ */
+static __inline__ void timer_recalc_offset(u64 cur_tb)
+{
+	unsigned long offset;
+	u64 new_stamp_xsec;
+
+	offset = cur_tb - do_gtod.varp->tb_orig_stamp;
+	if ((offset & 0x80000000u) == 0)
+		return;
+	new_stamp_xsec = do_gtod.varp->stamp_xsec
+		+ mulhdu(offset, do_gtod.varp->tb_to_xs);
+	update_gtod(cur_tb, new_stamp_xsec, do_gtod.varp->tb_to_xs);
+}
+
+#ifdef CONFIG_SMP
+unsigned long profile_pc(struct pt_regs *regs)
+{
+	unsigned long pc = instruction_pointer(regs);
+
+	if (in_lock_functions(pc))
+		return regs->link;
+
+	return pc;
+}
+EXPORT_SYMBOL(profile_pc);
+#endif
+
+#ifdef CONFIG_PPC_ISERIES
+
+/* 
+ * This function recalibrates the timebase based on the 49-bit time-of-day
+ * value in the Titan chip.  The Titan is much more accurate than the value
+ * returned by the service processor for the timebase frequency.  
+ */
+
+static void iSeries_tb_recal(void)
+{
+	struct div_result divres;
+	unsigned long titan, tb;
+	tb = get_tb();
+	titan = HvCallXm_loadTod();
+	if ( iSeries_recal_titan ) {
+		unsigned long tb_ticks = tb - iSeries_recal_tb;
+		unsigned long titan_usec = (titan - iSeries_recal_titan) >> 12;
+		unsigned long new_tb_ticks_per_sec   = (tb_ticks * USEC_PER_SEC)/titan_usec;
+		unsigned long new_tb_ticks_per_jiffy = (new_tb_ticks_per_sec+(HZ/2))/HZ;
+		long tick_diff = new_tb_ticks_per_jiffy - tb_ticks_per_jiffy;
+		char sign = '+';		
+		/* make sure tb_ticks_per_sec and tb_ticks_per_jiffy are consistent */
+		new_tb_ticks_per_sec = new_tb_ticks_per_jiffy * HZ;
+
+		if ( tick_diff < 0 ) {
+			tick_diff = -tick_diff;
+			sign = '-';
+		}
+		if ( tick_diff ) {
+			if ( tick_diff < tb_ticks_per_jiffy/25 ) {
+				printk( "Titan recalibrate: new tb_ticks_per_jiffy = %lu (%c%ld)\n",
+						new_tb_ticks_per_jiffy, sign, tick_diff );
+				tb_ticks_per_jiffy = new_tb_ticks_per_jiffy;
+				tb_ticks_per_sec   = new_tb_ticks_per_sec;
+				div128_by_32( XSEC_PER_SEC, 0, tb_ticks_per_sec, &divres );
+				do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
+				tb_to_xs = divres.result_low;
+				do_gtod.varp->tb_to_xs = tb_to_xs;
+				systemcfg->tb_ticks_per_sec = tb_ticks_per_sec;
+				systemcfg->tb_to_xs = tb_to_xs;
+			}
+			else {
+				printk( "Titan recalibrate: FAILED (difference > 4 percent)\n"
+					"                   new tb_ticks_per_jiffy = %lu\n"
+					"                   old tb_ticks_per_jiffy = %lu\n",
+					new_tb_ticks_per_jiffy, tb_ticks_per_jiffy );
+			}
+		}
+	}
+	iSeries_recal_titan = titan;
+	iSeries_recal_tb = tb;
+}
+#endif
+
+/*
+ * For iSeries shared processors, we have to let the hypervisor
+ * set the hardware decrementer.  We set a virtual decrementer
+ * in the lppaca and call the hypervisor if the virtual
+ * decrementer is less than the current value in the hardware
+ * decrementer. (almost always the new decrementer value will
+ * be greater than the current hardware decementer so the hypervisor
+ * call will not be needed)
+ */
+
+u64 tb_last_stamp __cacheline_aligned_in_smp;
+
+/*
+ * Note that on ppc32 this only stores the bottom 32 bits of
+ * the timebase value, but that's enough to tell when a jiffy
+ * has passed.
+ */
+DEFINE_PER_CPU(unsigned long, last_jiffy);
+
+/*
+ * timer_interrupt - gets called when the decrementer overflows,
+ * with interrupts disabled.
+ */
+void timer_interrupt(struct pt_regs * regs)
+{
+	int next_dec;
+	int cpu = smp_processor_id();
+	unsigned long ticks;
+
+#ifdef CONFIG_PPC32
+	if (atomic_read(&ppc_n_lost_interrupts) != 0)
+		do_IRQ(regs);
+#endif
+
+	irq_enter();
+
+	profile_tick(CPU_PROFILING, regs);
+
+#ifdef CONFIG_PPC_ISERIES
+	get_paca()->lppaca.int_dword.fields.decr_int = 0;
+#endif
+
+	while ((ticks = tb_ticks_since(per_cpu(last_jiffy, cpu)))
+	       >= tb_ticks_per_jiffy) {
+		/* Update last_jiffy */
+		per_cpu(last_jiffy, cpu) += tb_ticks_per_jiffy;
+		/* Handle RTCL overflow on 601 */
+		if (__USE_RTC() && per_cpu(last_jiffy, cpu) >= 1000000000)
+			per_cpu(last_jiffy, cpu) -= 1000000000;
+
+		/*
+		 * We cannot disable the decrementer, so in the period
+		 * between this cpu's being marked offline in cpu_online_map
+		 * and calling stop-self, it is taking timer interrupts.
+		 * Avoid calling into the scheduler rebalancing code if this
+		 * is the case.
+		 */
+		if (!cpu_is_offline(cpu))
+			update_process_times(user_mode(regs));
+
+		/*
+		 * No need to check whether cpu is offline here; boot_cpuid
+		 * should have been fixed up by now.
+		 */
+		if (cpu != boot_cpuid)
+			continue;
+
+		write_seqlock(&xtime_lock);
+		tb_last_stamp += tb_ticks_per_jiffy;
+		timer_recalc_offset(tb_last_stamp);
+		do_timer(regs);
+		timer_sync_xtime(tb_last_stamp);
+		timer_check_rtc();
+		write_sequnlock(&xtime_lock);
+		if (adjusting_time && (time_adjust == 0))
+			ppc_adjtimex();
+	}
+	
+	next_dec = tb_ticks_per_jiffy - ticks;
+	set_dec(next_dec);
+
+#ifdef CONFIG_PPC_ISERIES
+	if (hvlpevent_is_pending())
+		process_hvlpevents(regs);
+#endif
+
+#ifdef CONFIG_PPC64
+	/* collect purr register values often, for accurate calculations */
+	if (firmware_has_feature(FW_FEATURE_SPLPAR)) {
+		struct cpu_usage *cu = &__get_cpu_var(cpu_usage_array);
+		cu->current_tb = mfspr(SPRN_PURR);
+	}
+#endif
+
+	irq_exit();
+}
+
+void wakeup_decrementer(void)
+{
+	int i;
+
+	set_dec(tb_ticks_per_jiffy);
+	/*
+	 * We don't expect this to be called on a machine with a 601,
+	 * so using get_tbl is fine.
+	 */
+	tb_last_stamp = get_tb();
+	for_each_cpu(i)
+		per_cpu(last_jiffy, i) = tb_last_stamp;
+}
+
+#ifdef CONFIG_SMPxxx
+void __init smp_space_timers(unsigned int max_cpus)
+{
+	int i;
+	unsigned long offset = tb_ticks_per_jiffy / max_cpus;
+	unsigned long previous_tb = per_cpu(last_jiffy, boot_cpuid);
+
+	for_each_cpu(i) {
+		if (i != boot_cpuid) {
+			previous_tb += offset;
+			per_cpu(last_jiffy, i) = previous_tb;
+		}
+	}
+}
+#endif
+
+/*
+ * Scheduler clock - returns current time in nanosec units.
+ *
+ * Note: mulhdu(a, b) (multiply high double unsigned) returns
+ * the high 64 bits of a * b, i.e. (a * b) >> 64, where a and b
+ * are 64-bit unsigned numbers.
+ */
+unsigned long long sched_clock(void)
+{
+	return mulhdu(get_tb(), tb_to_ns_scale) << tb_to_ns_shift;
+}
+
+int do_settimeofday(struct timespec *tv)
+{
+	time_t wtm_sec, new_sec = tv->tv_sec;
+	long wtm_nsec, new_nsec = tv->tv_nsec;
+	unsigned long flags;
+	long int tb_delta;
+	u64 new_xsec;
+
+	if ((unsigned long)tv->tv_nsec >= NSEC_PER_SEC)
+		return -EINVAL;
+
+	write_seqlock_irqsave(&xtime_lock, flags);
+
+	/*
+	 * Updating the RTC is not the job of this code. If the time is
+	 * stepped under NTP, the RTC will be updated after STA_UNSYNC
+	 * is cleared.  Tools like clock/hwclock either copy the RTC
+	 * to the system time, in which case there is no point in writing
+	 * to the RTC again, or write to the RTC but then they don't call
+	 * settimeofday to perform this operation.
+	 */
+#ifdef CONFIG_PPC_ISERIES
+	if (first_settimeofday) {
+		iSeries_tb_recal();
+		first_settimeofday = 0;
+	}
+#endif
+	tb_delta = tb_ticks_since(tb_last_stamp);
+	tb_delta += (jiffies - wall_jiffies) * tb_ticks_per_jiffy;
+
+	new_nsec -= 1000 * mulhwu(tb_to_us, tb_delta);
+
+	wtm_sec  = wall_to_monotonic.tv_sec + (xtime.tv_sec - new_sec);
+	wtm_nsec = wall_to_monotonic.tv_nsec + (xtime.tv_nsec - new_nsec);
+
+ 	set_normalized_timespec(&xtime, new_sec, new_nsec);
+	set_normalized_timespec(&wall_to_monotonic, wtm_sec, wtm_nsec);
+
+	/* In case of a large backwards jump in time with NTP, we want the 
+	 * clock to be updated as soon as the PLL is again in lock.
+	 */
+	last_rtc_update = new_sec - 658;
+
+	ntp_clear();
+
+	new_xsec = (u64)new_nsec * XSEC_PER_SEC;
+	do_div(new_xsec, NSEC_PER_SEC);
+	new_xsec += (u64)new_sec * XSEC_PER_SEC;
+	update_gtod(tb_last_stamp, new_xsec, do_gtod.varp->tb_to_xs);
+
+#ifdef CONFIG_PPC64
+	systemcfg->tz_minuteswest = sys_tz.tz_minuteswest;
+	systemcfg->tz_dsttime = sys_tz.tz_dsttime;
+#endif
+
+	write_sequnlock_irqrestore(&xtime_lock, flags);
+	clock_was_set();
+	return 0;
+}
+
+EXPORT_SYMBOL(do_settimeofday);
+
+#if defined(CONFIG_PPC_PSERIES) || defined(CONFIG_PPC_MAPLE) || defined(CONFIG_PPC_BPA) || defined(CONFIG_PPC_ISERIES)
+void __init generic_calibrate_decr(void)
+{
+	struct device_node *cpu;
+	struct div_result divres;
+	unsigned int *fp;
+	int node_found;
+
+	/*
+	 * The cpu node should have a timebase-frequency property
+	 * to tell us the rate at which the decrementer counts.
+	 */
+	cpu = of_find_node_by_type(NULL, "cpu");
+
+	ppc_tb_freq = DEFAULT_TB_FREQ;		/* hardcoded default */
+	node_found = 0;
+	if (cpu != 0) {
+		fp = (unsigned int *)get_property(cpu, "timebase-frequency",
+						  NULL);
+		if (fp != 0) {
+			node_found = 1;
+			ppc_tb_freq = *fp;
+		}
+	}
+	if (!node_found)
+		printk(KERN_ERR "WARNING: Estimating decrementer frequency "
+				"(not found)\n");
+
+	ppc_proc_freq = DEFAULT_PROC_FREQ;
+	node_found = 0;
+	if (cpu != 0) {
+		fp = (unsigned int *)get_property(cpu, "clock-frequency",
+						  NULL);
+		if (fp != 0) {
+			node_found = 1;
+			ppc_proc_freq = *fp;
+		}
+	}
+	if (!node_found)
+		printk(KERN_ERR "WARNING: Estimating processor frequency "
+				"(not found)\n");
+
+	of_node_put(cpu);
+
+	printk(KERN_INFO "time_init: decrementer frequency = %lu.%.6lu MHz\n",
+	       ppc_tb_freq/1000000, ppc_tb_freq%1000000);
+	printk(KERN_INFO "time_init: processor frequency   = %lu.%.6lu MHz\n",
+	       ppc_proc_freq/1000000, ppc_proc_freq%1000000);
+
+	tb_ticks_per_jiffy = ppc_tb_freq / HZ;
+	tb_ticks_per_sec = tb_ticks_per_jiffy * HZ;
+	tb_ticks_per_usec = ppc_tb_freq / 1000000;
+	tb_to_us = mulhwu_scale_factor(ppc_tb_freq, 1000000);
+	div128_by_32(1024*1024, 0, tb_ticks_per_sec, &divres);
+	tb_to_xs = divres.result_low;
+}
+#endif
+
+unsigned long get_boot_time(void)
+{
+	struct rtc_time tm;
+
+	if (ppc_md.get_boot_time)
+		return ppc_md.get_boot_time();
+	if (!ppc_md.get_rtc_time)
+		return 0;
+	ppc_md.get_rtc_time(&tm);
+	return mktime(tm.tm_year+1900, tm.tm_mon+1, tm.tm_mday,
+		      tm.tm_hour, tm.tm_min, tm.tm_sec);
+}
+
+/* This function is only called on the boot processor */
+void __init time_init(void)
+{
+	unsigned long flags;
+	unsigned long tm = 0;
+	struct div_result res;
+	u64 scale;
+	unsigned shift;
+
+        if (ppc_md.time_init != NULL)
+                timezone_offset = ppc_md.time_init();
+
+	ppc_md.calibrate_decr();
+
+#ifdef CONFIG_PPC64
+	get_paca()->default_decr = tb_ticks_per_jiffy;
+#endif
+
+	/*
+	 * Compute scale factor for sched_clock.
+	 * The calibrate_decr() function has set tb_ticks_per_sec,
+	 * which is the timebase frequency.
+	 * We compute 1e9 * 2^64 / tb_ticks_per_sec and interpret
+	 * the 128-bit result as a 64.64 fixed-point number.
+	 * We then shift that number right until it is less than 1.0,
+	 * giving us the scale factor and shift count to use in
+	 * sched_clock().
+	 */
+	div128_by_32(1000000000, 0, tb_ticks_per_sec, &res);
+	scale = res.result_low;
+	for (shift = 0; res.result_high != 0; ++shift) {
+		scale = (scale >> 1) | (res.result_high << 63);
+		res.result_high >>= 1;
+	}
+	tb_to_ns_scale = scale;
+	tb_to_ns_shift = shift;
+
+#ifdef CONFIG_PPC_ISERIES
+	if (!piranha_simulator)
+#endif
+		tm = get_boot_time();
+
+	write_seqlock_irqsave(&xtime_lock, flags);
+	xtime.tv_sec = tm;
+	xtime.tv_nsec = 0;
+	tb_last_stamp = get_tb();
+	do_gtod.varp = &do_gtod.vars[0];
+	do_gtod.var_idx = 0;
+	do_gtod.varp->tb_orig_stamp = tb_last_stamp;
+	__get_cpu_var(last_jiffy) = tb_last_stamp;
+	do_gtod.varp->stamp_xsec = (u64) xtime.tv_sec * XSEC_PER_SEC;
+	do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
+	do_gtod.varp->tb_to_xs = tb_to_xs;
+	do_gtod.tb_to_us = tb_to_us;
+#ifdef CONFIG_PPC64
+	systemcfg->tb_orig_stamp = tb_last_stamp;
+	systemcfg->tb_update_count = 0;
+	systemcfg->tb_ticks_per_sec = tb_ticks_per_sec;
+	systemcfg->stamp_xsec = xtime.tv_sec * XSEC_PER_SEC;
+	systemcfg->tb_to_xs = tb_to_xs;
+#endif
+
+	time_freq = 0;
+
+	/* If platform provided a timezone (pmac), we correct the time */
+        if (timezone_offset) {
+		sys_tz.tz_minuteswest = -timezone_offset / 60;
+		sys_tz.tz_dsttime = 0;
+		xtime.tv_sec -= timezone_offset;
+        }
+
+	last_rtc_update = xtime.tv_sec;
+	set_normalized_timespec(&wall_to_monotonic,
+	                        -xtime.tv_sec, -xtime.tv_nsec);
+	write_sequnlock_irqrestore(&xtime_lock, flags);
+
+	/* Not exact, but the timer interrupt takes care of this */
+	set_dec(tb_ticks_per_jiffy);
+}
+
+/* 
+ * After adjtimex is called, adjust the conversion of tb ticks
+ * to microseconds to keep do_gettimeofday synchronized 
+ * with ntpd.
+ *
+ * Use the time_adjust, time_freq and time_offset computed by adjtimex to 
+ * adjust the frequency.
+ */
+
+/* #define DEBUG_PPC_ADJTIMEX 1 */
+
+void ppc_adjtimex(void)
+{
+#ifdef CONFIG_PPC64
+	unsigned long den, new_tb_ticks_per_sec, tb_ticks, old_xsec,
+		new_tb_to_xs, new_xsec, new_stamp_xsec;
+	unsigned long tb_ticks_per_sec_delta;
+	long delta_freq, ltemp;
+	struct div_result divres; 
+	unsigned long flags;
+	long singleshot_ppm = 0;
+
+	/*
+	 * Compute parts per million frequency adjustment to
+	 * accomplish the time adjustment implied by time_offset to be
+	 * applied over the elapsed time indicated by time_constant.
+	 * Use SHIFT_USEC to get it into the same units as
+	 * time_freq.
+	 */
+	if ( time_offset < 0 ) {
+		ltemp = -time_offset;
+		ltemp <<= SHIFT_USEC - SHIFT_UPDATE;
+		ltemp >>= SHIFT_KG + time_constant;
+		ltemp = -ltemp;
+	} else {
+		ltemp = time_offset;
+		ltemp <<= SHIFT_USEC - SHIFT_UPDATE;
+		ltemp >>= SHIFT_KG + time_constant;
+	}
+	
+	/* If there is a single shot time adjustment in progress */
+	if ( time_adjust ) {
+#ifdef DEBUG_PPC_ADJTIMEX
+		printk("ppc_adjtimex: ");
+		if ( adjusting_time == 0 )
+			printk("starting ");
+		printk("single shot time_adjust = %ld\n", time_adjust);
+#endif	
+	
+		adjusting_time = 1;
+		
+		/*
+		 * Compute parts per million frequency adjustment
+		 * to match time_adjust
+		 */
+		singleshot_ppm = tickadj * HZ;	
+		/*
+		 * The adjustment should be tickadj*HZ to match the code in
+		 * linux/kernel/timer.c, but experiments show that this is too
+		 * large. 3/4 of tickadj*HZ seems about right
+		 */
+		singleshot_ppm -= singleshot_ppm / 4;
+		/* Use SHIFT_USEC to get it into the same units as time_freq */
+		singleshot_ppm <<= SHIFT_USEC;
+		if ( time_adjust < 0 )
+			singleshot_ppm = -singleshot_ppm;
+	}
+	else {
+#ifdef DEBUG_PPC_ADJTIMEX
+		if ( adjusting_time )
+			printk("ppc_adjtimex: ending single shot time_adjust\n");
+#endif
+		adjusting_time = 0;
+	}
+	
+	/* Add up all of the frequency adjustments */
+	delta_freq = time_freq + ltemp + singleshot_ppm;
+	
+	/*
+	 * Compute a new value for tb_ticks_per_sec based on
+	 * the frequency adjustment
+	 */
+	den = 1000000 * (1 << (SHIFT_USEC - 8));
+	if ( delta_freq < 0 ) {
+		tb_ticks_per_sec_delta = ( tb_ticks_per_sec * ( (-delta_freq) >> (SHIFT_USEC - 8))) / den;
+		new_tb_ticks_per_sec = tb_ticks_per_sec + tb_ticks_per_sec_delta;
+	}
+	else {
+		tb_ticks_per_sec_delta = ( tb_ticks_per_sec * ( delta_freq >> (SHIFT_USEC - 8))) / den;
+		new_tb_ticks_per_sec = tb_ticks_per_sec - tb_ticks_per_sec_delta;
+	}
+	
+#ifdef DEBUG_PPC_ADJTIMEX
+	printk("ppc_adjtimex: ltemp = %ld, time_freq = %ld, singleshot_ppm = %ld\n", ltemp, time_freq, singleshot_ppm);
+	printk("ppc_adjtimex: tb_ticks_per_sec - base = %ld  new = %ld\n", tb_ticks_per_sec, new_tb_ticks_per_sec);
+#endif
+
+	/*
+	 * Compute a new value of tb_to_xs (used to convert tb to
+	 * microseconds) and a new value of stamp_xsec which is the
+	 * time (in 1/2^20 second units) corresponding to
+	 * tb_orig_stamp.  This new value of stamp_xsec compensates
+	 * for the change in frequency (implied by the new tb_to_xs)
+	 * which guarantees that the current time remains the same.
+	 */
+	write_seqlock_irqsave( &xtime_lock, flags );
+	tb_ticks = get_tb() - do_gtod.varp->tb_orig_stamp;
+	div128_by_32(1024*1024, 0, new_tb_ticks_per_sec, &divres);
+	new_tb_to_xs = divres.result_low;
+	new_xsec = mulhdu(tb_ticks, new_tb_to_xs);
+
+	old_xsec = mulhdu(tb_ticks, do_gtod.varp->tb_to_xs);
+	new_stamp_xsec = do_gtod.varp->stamp_xsec + old_xsec - new_xsec;
+
+	update_gtod(do_gtod.varp->tb_orig_stamp, new_stamp_xsec, new_tb_to_xs);
+
+	write_sequnlock_irqrestore( &xtime_lock, flags );
+#endif /* CONFIG_PPC64 */
+}
+
+
+#define FEBRUARY	2
+#define	STARTOFTIME	1970
+#define SECDAY		86400L
+#define SECYR		(SECDAY * 365)
+#define	leapyear(year)		((year) % 4 == 0 && \
+				 ((year) % 100 != 0 || (year) % 400 == 0))
+#define	days_in_year(a) 	(leapyear(a) ? 366 : 365)
+#define	days_in_month(a) 	(month_days[(a) - 1])
+
+static int month_days[12] = {
+	31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31
+};
+
+/*
+ * This only works for the Gregorian calendar - i.e. after 1752 (in the UK)
+ */
+void GregorianDay(struct rtc_time * tm)
+{
+	int leapsToDate;
+	int lastYear;
+	int day;
+	int MonthOffset[] = { 0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334 };
+
+	lastYear = tm->tm_year - 1;
+
+	/*
+	 * Number of leap corrections to apply up to end of last year
+	 */
+	leapsToDate = lastYear / 4 - lastYear / 100 + lastYear / 400;
+
+	/*
+	 * This year is a leap year if it is divisible by 4 except when it is
+	 * divisible by 100 unless it is divisible by 400
+	 *
+	 * e.g. 1904 was a leap year, 1900 was not, 1996 is, and 2000 was
+	 */
+	day = tm->tm_mon > 2 && leapyear(tm->tm_year);
+
+	day += lastYear*365 + leapsToDate + MonthOffset[tm->tm_mon-1] +
+		   tm->tm_mday;
+
+	tm->tm_wday = day % 7;
+}
+
+void to_tm(int tim, struct rtc_time * tm)
+{
+	register int    i;
+	register long   hms, day;
+
+	day = tim / SECDAY;
+	hms = tim % SECDAY;
+
+	/* Hours, minutes, seconds are easy */
+	tm->tm_hour = hms / 3600;
+	tm->tm_min = (hms % 3600) / 60;
+	tm->tm_sec = (hms % 3600) % 60;
+
+	/* Number of years in days */
+	for (i = STARTOFTIME; day >= days_in_year(i); i++)
+		day -= days_in_year(i);
+	tm->tm_year = i;
+
+	/* Number of months in days left */
+	if (leapyear(tm->tm_year))
+		days_in_month(FEBRUARY) = 29;
+	for (i = 1; day >= days_in_month(i); i++)
+		day -= days_in_month(i);
+	days_in_month(FEBRUARY) = 28;
+	tm->tm_mon = i;
+
+	/* Days are what is left over (+1) from all that. */
+	tm->tm_mday = day + 1;
+
+	/*
+	 * Determine the day of week
+	 */
+	GregorianDay(tm);
+}
+
+/* Auxiliary function to compute scaling factors */
+/* Actually the choice of a timebase running at 1/4 the of the bus
+ * frequency giving resolution of a few tens of nanoseconds is quite nice.
+ * It makes this computation very precise (27-28 bits typically) which
+ * is optimistic considering the stability of most processor clock
+ * oscillators and the precision with which the timebase frequency
+ * is measured but does not harm.
+ */
+unsigned mulhwu_scale_factor(unsigned inscale, unsigned outscale)
+{
+        unsigned mlt=0, tmp, err;
+        /* No concern for performance, it's done once: use a stupid
+         * but safe and compact method to find the multiplier.
+         */
+  
+        for (tmp = 1U<<31; tmp != 0; tmp >>= 1) {
+                if (mulhwu(inscale, mlt|tmp) < outscale)
+			mlt |= tmp;
+        }
+  
+        /* We might still be off by 1 for the best approximation.
+         * A side effect of this is that if outscale is too large
+         * the returned value will be zero.
+         * Many corner cases have been checked and seem to work,
+         * some might have been forgotten in the test however.
+         */
+  
+        err = inscale * (mlt+1);
+        if (err <= inscale/2)
+		mlt++;
+        return mlt;
+}
+
+/*
+ * Divide a 128-bit dividend by a 32-bit divisor, leaving a 128 bit
+ * result.
+ */
+void div128_by_32(u64 dividend_high, u64 dividend_low,
+		  unsigned divisor, struct div_result *dr)
+{
+	unsigned long a, b, c, d;
+	unsigned long w, x, y, z;
+	u64 ra, rb, rc;
+
+	a = dividend_high >> 32;
+	b = dividend_high & 0xffffffff;
+	c = dividend_low >> 32;
+	d = dividend_low & 0xffffffff;
+
+	w = a / divisor;
+	ra = ((u64)(a - (w * divisor)) << 32) + b;
+
+#ifdef CONFIG_PPC64
+	x = ra / divisor;
+	rb = ((ra - (x * divisor)) << 32) + c;
+
+	y = rb / divisor;
+	rc = ((rb - (y * divisor)) << 32) + d;
+
+	z = rc / divisor;
+#else
+	/* for 32-bit, use do_div from div64.h */
+	rb = ((u64) do_div(ra, divisor) << 32) + c;
+	x = ra;
+
+	rc = ((u64) do_div(rb, divisor) << 32) + d;
+	y = rb;
+
+	do_div(rc, divisor);
+	z = rc;
+#endif
+
+	dr->result_high = ((u64)w << 32) + x;
+	dr->result_low  = ((u64)y << 32) + z;
+
+}
+
