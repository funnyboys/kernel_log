commit 9cb8f069deeed708bf19486d5893e297dc467ae0
Author: Dmitry Safonov <dima@arista.com>
Date:   Mon Jun 8 21:32:29 2020 -0700

    kernel: rename show_stack_loglvl() => show_stack()
    
    Now the last users of show_stack() got converted to use an explicit log
    level, show_stack_loglvl() can drop it's redundant suffix and become once
    again well known show_stack().
    
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20200418201944.482088-51-dima@arista.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index c477b8585a29..b6440657ef92 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -260,7 +260,7 @@ static void raise_backtrace_ipi(cpumask_t *mask)
 			pr_cont(" current pointer corrupt? (%px)\n", p->__current);
 
 		pr_warn("Back trace of paca->saved_r1 (0x%016llx) (possibly stale):\n", p->saved_r1);
-		show_stack(p->__current, (unsigned long *)p->saved_r1);
+		show_stack(p->__current, (unsigned long *)p->saved_r1, KERN_WARNING);
 	}
 }
 

commit 3d13e839e801e081bdece0127c2affa33d0f77cf
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Feb 20 22:51:37 2020 +1100

    powerpc: Rename current_stack_pointer() to current_stack_frame()
    
    current_stack_pointer(), which was called __get_SP(), used to just
    return the value in r1.
    
    But that caused problems in some cases, so it was turned into a
    function in commit bfe9a2cfe91a ("powerpc: Reimplement __get_SP() as a
    function not a define").
    
    Because it's a function in a separate compilation unit to all its
    callers, it has the effect of causing a stack frame to be created, and
    then returns the address of that frame. This is good in some cases
    like those described in the above commit, but in other cases it's
    overkill, we just need to know what stack page we're on.
    
    On some other arches current_stack_pointer is just a register global
    giving the stack pointer, and we'd like to do that too. So rename our
    current_stack_pointer() to current_stack_frame() to make that
    possible.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Link: https://lore.kernel.org/r/20200220115141.2707-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index e2a46cfed5fd..c477b8585a29 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -57,7 +57,7 @@ void save_stack_trace(struct stack_trace *trace)
 {
 	unsigned long sp;
 
-	sp = current_stack_pointer();
+	sp = current_stack_frame();
 
 	save_context_stack(trace, sp, current, 1);
 }
@@ -71,7 +71,7 @@ void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 		return;
 
 	if (tsk == current)
-		sp = current_stack_pointer();
+		sp = current_stack_frame();
 	else
 		sp = tsk->thread.ksp;
 
@@ -131,7 +131,7 @@ static int __save_stack_trace_tsk_reliable(struct task_struct *tsk,
 	}
 
 	if (tsk == current)
-		sp = current_stack_pointer();
+		sp = current_stack_frame();
 	else
 		sp = tsk->thread.ksp;
 

commit 370011a27028d6f05e598ed6211a0ca2dc0213f7
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Thu Sep 5 23:50:29 2019 +0530

    powerpc/ftrace: Enable HAVE_FUNCTION_GRAPH_RET_ADDR_PTR
    
    This associates entries in the ftrace_ret_stack with corresponding stack
    frames, enabling more robust stack unwinding. Also update the only user
    of ftrace_graph_ret_addr() to pass the stack pointer.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/0224f2d0971b069c678e2ff678cfc2cd1e114cfe.1567707399.git.naveen.n.rao@linux.vnet.ibm.com

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 1e2276963f6d..e2a46cfed5fd 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -182,7 +182,7 @@ static int __save_stack_trace_tsk_reliable(struct task_struct *tsk,
 		 * FIXME: IMHO these tests do not belong in
 		 * arch-dependent code, they are generic.
 		 */
-		ip = ftrace_graph_ret_addr(tsk, &graph_idx, ip, NULL);
+		ip = ftrace_graph_ret_addr(tsk, &graph_idx, ip, stack);
 #ifdef CONFIG_KPROBES
 		/*
 		 * Mark stacktraces with kretprobed functions on them

commit 39070a96a1c2c502b2f77972ba8c2eba3ca6cd3a
Author: Joe Lawrence <joe.lawrence@redhat.com>
Date:   Fri Mar 1 14:17:21 2019 -0500

    powerpc: Remove export of save_stack_trace_tsk_reliable()
    
    As tglx points out, there are no in-tree module users of
    save_stack_trace_tsk_reliable() and its x86 counterpart is not
    exported, so remove the powerpc symbol export.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Joe Lawrence <joe.lawrence@redhat.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index f958f3bcba04..1e2276963f6d 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -220,7 +220,6 @@ int save_stack_trace_tsk_reliable(struct task_struct *tsk,
 
 	return ret;
 }
-EXPORT_SYMBOL_GPL(save_stack_trace_tsk_reliable);
 #endif /* CONFIG_HAVE_RELIABLE_STACKTRACE */
 
 #if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_NMI_IPI)

commit 018cce33c5e62dda265df8ae0ddf7f3a3357ad1f
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:08:52 2019 +0000

    powerpc: prep stack walkers for THREAD_INFO_IN_TASK
    
    [text copied from commit 9bbd4c56b0b6
    ("arm64: prep stack walkers for THREAD_INFO_IN_TASK")]
    
    When CONFIG_THREAD_INFO_IN_TASK is selected, task stacks may be freed
    before a task is destroyed. To account for this, the stacks are
    refcounted, and when manipulating the stack of another task, it is
    necessary to get/put the stack to ensure it isn't freed and/or re-used
    while we do so.
    
    This patch reworks the powerpc stack walking code to account for this.
    When CONFIG_THREAD_INFO_IN_TASK is not selected these perform no
    refcounting, and this should only be a structural change that does not
    affect behaviour.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Move try_get_task_stack() below tsk == NULL check in show_stack()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index cf31ce6c1f53..f958f3bcba04 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -67,12 +67,17 @@ void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 {
 	unsigned long sp;
 
+	if (!try_get_task_stack(tsk))
+		return;
+
 	if (tsk == current)
 		sp = current_stack_pointer();
 	else
 		sp = tsk->thread.ksp;
 
 	save_context_stack(trace, sp, tsk, 0);
+
+	put_task_stack(tsk);
 }
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
 
@@ -90,9 +95,8 @@ EXPORT_SYMBOL_GPL(save_stack_trace_regs);
  *
  * If the task is not 'current', the caller *must* ensure the task is inactive.
  */
-int
-save_stack_trace_tsk_reliable(struct task_struct *tsk,
-				struct stack_trace *trace)
+static int __save_stack_trace_tsk_reliable(struct task_struct *tsk,
+					   struct stack_trace *trace)
 {
 	unsigned long sp;
 	unsigned long newsp;
@@ -197,6 +201,25 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 	}
 	return 0;
 }
+
+int save_stack_trace_tsk_reliable(struct task_struct *tsk,
+				  struct stack_trace *trace)
+{
+	int ret;
+
+	/*
+	 * If the task doesn't have a stack (e.g., a zombie), the stack is
+	 * "reliably" empty.
+	 */
+	if (!try_get_task_stack(tsk))
+		return 0;
+
+	ret = __save_stack_trace_tsk_reliable(tsk, trace);
+
+	put_task_stack(tsk);
+
+	return ret;
+}
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk_reliable);
 #endif /* CONFIG_HAVE_RELIABLE_STACKTRACE */
 

commit 3de27dcf8121c2a710ab93dce23e0f5901c29783
Author: Joe Lawrence <joe.lawrence@redhat.com>
Date:   Tue Jan 22 10:57:24 2019 -0500

    powerpc/livepatch: return -ERRNO values in save_stack_trace_tsk_reliable()
    
    To match its x86 counterpart, save_stack_trace_tsk_reliable() should
    return -EINVAL in cases that it is currently returning 1.  No caller is
    currently differentiating non-zero error codes, but let's keep the
    arch-specific implementations consistent.
    
    Signed-off-by: Joe Lawrence <joe.lawrence@redhat.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 28c3c25755d7..cf31ce6c1f53 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -133,7 +133,7 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 
 	if (sp < stack_page + sizeof(struct thread_struct) ||
 	    sp > stack_end - STACK_FRAME_MIN_SIZE) {
-		return 1;
+		return -EINVAL;
 	}
 
 	for (firstframe = true; sp != stack_end;
@@ -143,16 +143,16 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 
 		/* sanity check: ABI requires SP to be aligned 16 bytes. */
 		if (sp & 0xF)
-			return 1;
+			return -EINVAL;
 
 		newsp = stack[0];
 		/* Stack grows downwards; unwinder may only go up. */
 		if (newsp <= sp)
-			return 1;
+			return -EINVAL;
 
 		if (newsp != stack_end &&
 		    newsp > stack_end - STACK_FRAME_MIN_SIZE) {
-			return 1; /* invalid backlink, too far up. */
+			return -EINVAL; /* invalid backlink, too far up. */
 		}
 
 		/*
@@ -166,13 +166,13 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 		/* Mark stacktraces with exception frames as unreliable. */
 		if (sp <= stack_end - STACK_INT_FRAME_SIZE &&
 		    stack[STACK_FRAME_MARKER] == STACK_FRAME_REGS_MARKER) {
-			return 1;
+			return -EINVAL;
 		}
 
 		/* Examine the saved LR: it must point into kernel code. */
 		ip = stack[STACK_FRAME_LR_SAVE];
 		if (!__kernel_text_address(ip))
-			return 1;
+			return -EINVAL;
 
 		/*
 		 * FIXME: IMHO these tests do not belong in
@@ -185,7 +185,7 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 		 * as unreliable.
 		 */
 		if (ip == (unsigned long)kretprobe_trampoline)
-			return 1;
+			return -EINVAL;
 #endif
 
 		if (trace->nr_entries >= trace->max_entries)

commit 29a77bbb0cf2cea41fa46f8fa176f6cb1e3182c4
Author: Joe Lawrence <joe.lawrence@redhat.com>
Date:   Tue Jan 22 10:57:23 2019 -0500

    powerpc/livepatch: small cleanups in save_stack_trace_tsk_reliable()
    
    Mostly cosmetic changes:
    
    - Group common stack pointer code at the top
    - Simplify the first frame logic
    - Code stackframe iteration into for...loop construct
    - Check for trace->nr_entries overflow before adding any into the array
    
    Suggested-by: Nicolai Stange <nstange@suse.de>
    Signed-off-by: Joe Lawrence <joe.lawrence@redhat.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 06688f4d557b..28c3c25755d7 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -95,20 +95,11 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 				struct stack_trace *trace)
 {
 	unsigned long sp;
+	unsigned long newsp;
 	unsigned long stack_page = (unsigned long)task_stack_page(tsk);
 	unsigned long stack_end;
 	int graph_idx = 0;
-
-	/*
-	 * The last frame (unwinding first) may not yet have saved
-	 * its LR onto the stack.
-	 */
-	int firstframe = 1;
-
-	if (tsk == current)
-		sp = current_stack_pointer();
-	else
-		sp = tsk->thread.ksp;
+	bool firstframe;
 
 	stack_end = stack_page + THREAD_SIZE;
 	if (!is_idle_task(tsk)) {
@@ -135,14 +126,20 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 		stack_end -= STACK_FRAME_OVERHEAD;
 	}
 
+	if (tsk == current)
+		sp = current_stack_pointer();
+	else
+		sp = tsk->thread.ksp;
+
 	if (sp < stack_page + sizeof(struct thread_struct) ||
 	    sp > stack_end - STACK_FRAME_MIN_SIZE) {
 		return 1;
 	}
 
-	for (;;) {
+	for (firstframe = true; sp != stack_end;
+	     firstframe = false, sp = newsp) {
 		unsigned long *stack = (unsigned long *) sp;
-		unsigned long newsp, ip;
+		unsigned long ip;
 
 		/* sanity check: ABI requires SP to be aligned 16 bytes. */
 		if (sp & 0xF)
@@ -163,10 +160,8 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 		 * rest of the frame may be uninitialized, continue to
 		 * the next.
 		 */
-		if (firstframe) {
-			firstframe = 0;
-			goto next;
-		}
+		if (firstframe)
+			continue;
 
 		/* Mark stacktraces with exception frames as unreliable. */
 		if (sp <= stack_end - STACK_INT_FRAME_SIZE &&
@@ -193,19 +188,12 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 			return 1;
 #endif
 
+		if (trace->nr_entries >= trace->max_entries)
+			return -E2BIG;
 		if (!trace->skip)
 			trace->entries[trace->nr_entries++] = ip;
 		else
 			trace->skip--;
-
-next:
-		if (newsp == stack_end)
-			break;
-
-		if (trace->nr_entries >= trace->max_entries)
-			return -E2BIG;
-
-		sp = newsp;
 	}
 	return 0;
 }

commit 18be37603de81674e41a0b0282326a0debc1696e
Author: Joe Lawrence <joe.lawrence@redhat.com>
Date:   Tue Jan 22 10:57:22 2019 -0500

    powerpc/livepatch: relax reliable stack tracer checks for first-frame
    
    The bottom-most stack frame (the first to be unwound) may be largely
    uninitialized, for the "Power Architecture 64-Bit ELF V2 ABI" only
    requires its backchain pointer to be set.
    
    The reliable stack tracer should be careful when verifying this frame:
    skip checks on STACK_FRAME_LR_SAVE and STACK_FRAME_MARKER offsets that
    may contain uninitialized residual data.
    
    Fixes: df78d3f61480 ("powerpc/livepatch: Implement reliable stack tracing for the consistency model")
    Signed-off-by: Joe Lawrence <joe.lawrence@redhat.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index e2c50b55138f..06688f4d557b 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -84,6 +84,12 @@ save_stack_trace_regs(struct pt_regs *regs, struct stack_trace *trace)
 EXPORT_SYMBOL_GPL(save_stack_trace_regs);
 
 #ifdef CONFIG_HAVE_RELIABLE_STACKTRACE
+/*
+ * This function returns an error if it detects any unreliable features of the
+ * stack.  Otherwise it guarantees that the stack trace is reliable.
+ *
+ * If the task is not 'current', the caller *must* ensure the task is inactive.
+ */
 int
 save_stack_trace_tsk_reliable(struct task_struct *tsk,
 				struct stack_trace *trace)
@@ -142,12 +148,6 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 		if (sp & 0xF)
 			return 1;
 
-		/* Mark stacktraces with exception frames as unreliable. */
-		if (sp <= stack_end - STACK_INT_FRAME_SIZE &&
-		    stack[STACK_FRAME_MARKER] == STACK_FRAME_REGS_MARKER) {
-			return 1;
-		}
-
 		newsp = stack[0];
 		/* Stack grows downwards; unwinder may only go up. */
 		if (newsp <= sp)
@@ -158,11 +158,26 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 			return 1; /* invalid backlink, too far up. */
 		}
 
+		/*
+		 * We can only trust the bottom frame's backlink, the
+		 * rest of the frame may be uninitialized, continue to
+		 * the next.
+		 */
+		if (firstframe) {
+			firstframe = 0;
+			goto next;
+		}
+
+		/* Mark stacktraces with exception frames as unreliable. */
+		if (sp <= stack_end - STACK_INT_FRAME_SIZE &&
+		    stack[STACK_FRAME_MARKER] == STACK_FRAME_REGS_MARKER) {
+			return 1;
+		}
+
 		/* Examine the saved LR: it must point into kernel code. */
 		ip = stack[STACK_FRAME_LR_SAVE];
-		if (!firstframe && !__kernel_text_address(ip))
+		if (!__kernel_text_address(ip))
 			return 1;
-		firstframe = 0;
 
 		/*
 		 * FIXME: IMHO these tests do not belong in
@@ -183,6 +198,7 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 		else
 			trace->skip--;
 
+next:
 		if (newsp == stack_end)
 			break;
 

commit e08ecba17b72aeb01859601bc242a5bc48620109
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jun 19 21:51:55 2018 +1000

    powerpc/64s: Fix build failures with CONFIG_NMI_IPI=n
    
    I broke the build when CONFIG_NMI_IPI=n with my recent commit to add
    arch_trigger_cpumask_backtrace(), eg:
    
      stacktrace.c:(.text+0x1b0): undefined reference to `.smp_send_safe_nmi_ipi'
    
    We should rework the CONFIG symbols here in future to avoid these
    double barrelled ifdefs but for now they fix the build.
    
    Fixes: 5cc05910f26e ("powerpc/64s: Wire up arch_trigger_cpumask_backtrace()")
    Reported-by: Christophe LEROY <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 07e97f289c52..e2c50b55138f 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -196,7 +196,7 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk_reliable);
 #endif /* CONFIG_HAVE_RELIABLE_STACKTRACE */
 
-#ifdef CONFIG_PPC_BOOK3S_64
+#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_NMI_IPI)
 static void handle_backtrace_ipi(struct pt_regs *regs)
 {
 	nmi_cpu_backtrace(regs);
@@ -242,4 +242,4 @@ void arch_trigger_cpumask_backtrace(const cpumask_t *mask, bool exclude_self)
 {
 	nmi_trigger_cpumask_backtrace(mask, exclude_self, raise_backtrace_ipi);
 }
-#endif /* CONFIG_PPC64 */
+#endif /* defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_NMI_IPI) */

commit 7af76c5f23abc7afedf449e7d2960f463cbc4097
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed May 2 23:07:29 2018 +1000

    powerpc/stacktrace: Update copyright
    
    This now has new code in it written by Nick and I, and switch to a
    SPDX tag.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index b4f134e8bbd9..07e97f289c52 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -1,13 +1,11 @@
+// SPDX-License-Identifier: GPL-2.0
+
 /*
- * Stack trace utility
+ * Stack trace utility functions etc.
  *
  * Copyright 2008 Christoph Hellwig, IBM Corp.
  * Copyright 2018 SUSE Linux GmbH
- *
- *      This program is free software; you can redistribute it and/or
- *      modify it under the terms of the GNU General Public License
- *      as published by the Free Software Foundation; either version
- *      2 of the License, or (at your option) any later version.
+ * Copyright 2018 Nick Piggin, Michael Ellerman, IBM Corp.
  */
 
 #include <linux/export.h>

commit 5cc05910f26e6fd6da15f052f86f6150e4b91664
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed May 2 23:07:28 2018 +1000

    powerpc/64s: Wire up arch_trigger_cpumask_backtrace()
    
    This allows eg. the RCU stall detector, or the soft/hardlockup
    detectors to trigger a backtrace on all CPUs.
    
    We implement this by sending a "safe" NMI, which will actually only
    send an IPI. Unfortunately the generic code prints "NMI", so that's a
    little confusing but we can probably live with it.
    
    If one of the CPUs doesn't respond to the IPI, we then print some info
    from it's paca and do a backtrace based on its saved_r1.
    
    Example output:
    
      INFO: rcu_sched detected stalls on CPUs/tasks:
            2-...0: (0 ticks this GP) idle=1be/1/4611686018427387904 softirq=1055/1055 fqs=25735
            (detected by 4, t=58847 jiffies, g=58, c=57, q=1258)
      Sending NMI from CPU 4 to CPUs 2:
      CPU 2 didn't respond to backtrace IPI, inspecting paca.
      irq_soft_mask: 0x01 in_mce: 0 in_nmi: 0 current: 3623 (bash)
      Back trace of paca->saved_r1 (0xc0000000e1c83ba0) (possibly stale):
      Call Trace:
      [c0000000e1c83ba0] [0000000000000014] 0x14 (unreliable)
      [c0000000e1c83bc0] [c000000000765798] lkdtm_do_action+0x48/0x80
      [c0000000e1c83bf0] [c000000000765a40] direct_entry+0x110/0x1b0
      [c0000000e1c83c90] [c00000000058e650] full_proxy_write+0x90/0xe0
      [c0000000e1c83ce0] [c0000000003aae3c] __vfs_write+0x6c/0x1f0
      [c0000000e1c83d80] [c0000000003ab214] vfs_write+0xd4/0x240
      [c0000000e1c83dd0] [c0000000003ab5cc] ksys_write+0x6c/0x110
      [c0000000e1c83e30] [c00000000000b860] system_call+0x58/0x6c
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 8dd6ba0c7d35..b4f134e8bbd9 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -13,6 +13,7 @@
 #include <linux/export.h>
 #include <linux/kallsyms.h>
 #include <linux/module.h>
+#include <linux/nmi.h>
 #include <linux/sched.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/task_stack.h>
@@ -22,6 +23,8 @@
 #include <linux/ftrace.h>
 #include <asm/kprobes.h>
 
+#include <asm/paca.h>
+
 /*
  * Save stack-backtrace addresses into a stack_trace buffer.
  */
@@ -194,3 +197,51 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 }
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk_reliable);
 #endif /* CONFIG_HAVE_RELIABLE_STACKTRACE */
+
+#ifdef CONFIG_PPC_BOOK3S_64
+static void handle_backtrace_ipi(struct pt_regs *regs)
+{
+	nmi_cpu_backtrace(regs);
+}
+
+static void raise_backtrace_ipi(cpumask_t *mask)
+{
+	unsigned int cpu;
+
+	for_each_cpu(cpu, mask) {
+		if (cpu == smp_processor_id())
+			handle_backtrace_ipi(NULL);
+		else
+			smp_send_safe_nmi_ipi(cpu, handle_backtrace_ipi, 5 * USEC_PER_SEC);
+	}
+
+	for_each_cpu(cpu, mask) {
+		struct paca_struct *p = paca_ptrs[cpu];
+
+		cpumask_clear_cpu(cpu, mask);
+
+		pr_warn("CPU %d didn't respond to backtrace IPI, inspecting paca.\n", cpu);
+		if (!virt_addr_valid(p)) {
+			pr_warn("paca pointer appears corrupt? (%px)\n", p);
+			continue;
+		}
+
+		pr_warn("irq_soft_mask: 0x%02x in_mce: %d in_nmi: %d",
+			p->irq_soft_mask, p->in_mce, p->in_nmi);
+
+		if (virt_addr_valid(p->__current))
+			pr_cont(" current: %d (%s)\n", p->__current->pid,
+				p->__current->comm);
+		else
+			pr_cont(" current pointer corrupt? (%px)\n", p->__current);
+
+		pr_warn("Back trace of paca->saved_r1 (0x%016llx) (possibly stale):\n", p->saved_r1);
+		show_stack(p->__current, (unsigned long *)p->saved_r1);
+	}
+}
+
+void arch_trigger_cpumask_backtrace(const cpumask_t *mask, bool exclude_self)
+{
+	nmi_trigger_cpumask_backtrace(mask, exclude_self, raise_backtrace_ipi);
+}
+#endif /* CONFIG_PPC64 */

commit 5e3f0d15ae5f95bdde8d092a0884d2defe27d448
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Tue May 22 14:38:20 2018 +0530

    powerpc/livepatch: Fix build error with kprobes disabled.
    
    arch/powerpc/kernel/stacktrace.c: In function ‘save_stack_trace_tsk_reliable’:
    arch/powerpc/kernel/stacktrace.c:176:28: error: ‘kretprobe_trampoline’ undeclared
       if (ip == (unsigned long)kretprobe_trampoline)
                                ^~~~~~~~~~~~~~~~~~~~
    
    Fixes: df78d3f61480 ("powerpc/livepatch: Implement reliable stack tracing for the consistency model")
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 26a50603177c..8dd6ba0c7d35 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -168,13 +168,14 @@ save_stack_trace_tsk_reliable(struct task_struct *tsk,
 		 * arch-dependent code, they are generic.
 		 */
 		ip = ftrace_graph_ret_addr(tsk, &graph_idx, ip, NULL);
-
+#ifdef CONFIG_KPROBES
 		/*
 		 * Mark stacktraces with kretprobed functions on them
 		 * as unreliable.
 		 */
 		if (ip == (unsigned long)kretprobe_trampoline)
 			return 1;
+#endif
 
 		if (!trace->skip)
 			trace->entries[trace->nr_entries++] = ip;

commit df78d3f6148092d33a9a24c7a9cfac3d0220b484
Author: Torsten Duwe <duwe@lst.de>
Date:   Fri May 4 14:38:34 2018 +0200

    powerpc/livepatch: Implement reliable stack tracing for the consistency model
    
    The "Power Architecture 64-Bit ELF V2 ABI" says in section 2.3.2.3:
    
    [...] There are several rules that must be adhered to in order to ensure
    reliable and consistent call chain backtracing:
    
    * Before a function calls any other function, it shall establish its
      own stack frame, whose size shall be a multiple of 16 bytes.
    
     – In instances where a function’s prologue creates a stack frame, the
       back-chain word of the stack frame shall be updated atomically with
       the value of the stack pointer (r1) when a back chain is implemented.
       (This must be supported as default by all ELF V2 ABI-compliant
       environments.)
    [...]
     – The function shall save the link register that contains its return
       address in the LR save doubleword of its caller’s stack frame before
       calling another function.
    
    To me this sounds like the equivalent of HAVE_RELIABLE_STACKTRACE.
    This patch may be unneccessarily limited to ppc64le, but OTOH the only
    user of this flag so far is livepatching, which is only implemented on
    PPCs with 64-LE, a.k.a. ELF ABI v2.
    
    Feel free to add other ppc variants, but so far only ppc64le got tested.
    
    This change also implements save_stack_trace_tsk_reliable() for ppc64le
    that checks for the above conditions, where possible.
    
    Signed-off-by: Torsten Duwe <duwe@suse.de>
    Signed-off-by: Nicolai Stange <nstange@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index d534ed901538..26a50603177c 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -2,7 +2,7 @@
  * Stack trace utility
  *
  * Copyright 2008 Christoph Hellwig, IBM Corp.
- *
+ * Copyright 2018 SUSE Linux GmbH
  *
  *      This program is free software; you can redistribute it and/or
  *      modify it under the terms of the GNU General Public License
@@ -11,11 +11,16 @@
  */
 
 #include <linux/export.h>
+#include <linux/kallsyms.h>
+#include <linux/module.h>
 #include <linux/sched.h>
 #include <linux/sched/debug.h>
+#include <linux/sched/task_stack.h>
 #include <linux/stacktrace.h>
 #include <asm/ptrace.h>
 #include <asm/processor.h>
+#include <linux/ftrace.h>
+#include <asm/kprobes.h>
 
 /*
  * Save stack-backtrace addresses into a stack_trace buffer.
@@ -76,3 +81,115 @@ save_stack_trace_regs(struct pt_regs *regs, struct stack_trace *trace)
 	save_context_stack(trace, regs->gpr[1], current, 0);
 }
 EXPORT_SYMBOL_GPL(save_stack_trace_regs);
+
+#ifdef CONFIG_HAVE_RELIABLE_STACKTRACE
+int
+save_stack_trace_tsk_reliable(struct task_struct *tsk,
+				struct stack_trace *trace)
+{
+	unsigned long sp;
+	unsigned long stack_page = (unsigned long)task_stack_page(tsk);
+	unsigned long stack_end;
+	int graph_idx = 0;
+
+	/*
+	 * The last frame (unwinding first) may not yet have saved
+	 * its LR onto the stack.
+	 */
+	int firstframe = 1;
+
+	if (tsk == current)
+		sp = current_stack_pointer();
+	else
+		sp = tsk->thread.ksp;
+
+	stack_end = stack_page + THREAD_SIZE;
+	if (!is_idle_task(tsk)) {
+		/*
+		 * For user tasks, this is the SP value loaded on
+		 * kernel entry, see "PACAKSAVE(r13)" in _switch() and
+		 * system_call_common()/EXCEPTION_PROLOG_COMMON().
+		 *
+		 * Likewise for non-swapper kernel threads,
+		 * this also happens to be the top of the stack
+		 * as setup by copy_thread().
+		 *
+		 * Note that stack backlinks are not properly setup by
+		 * copy_thread() and thus, a forked task() will have
+		 * an unreliable stack trace until it's been
+		 * _switch()'ed to for the first time.
+		 */
+		stack_end -= STACK_FRAME_OVERHEAD + sizeof(struct pt_regs);
+	} else {
+		/*
+		 * idle tasks have a custom stack layout,
+		 * c.f. cpu_idle_thread_init().
+		 */
+		stack_end -= STACK_FRAME_OVERHEAD;
+	}
+
+	if (sp < stack_page + sizeof(struct thread_struct) ||
+	    sp > stack_end - STACK_FRAME_MIN_SIZE) {
+		return 1;
+	}
+
+	for (;;) {
+		unsigned long *stack = (unsigned long *) sp;
+		unsigned long newsp, ip;
+
+		/* sanity check: ABI requires SP to be aligned 16 bytes. */
+		if (sp & 0xF)
+			return 1;
+
+		/* Mark stacktraces with exception frames as unreliable. */
+		if (sp <= stack_end - STACK_INT_FRAME_SIZE &&
+		    stack[STACK_FRAME_MARKER] == STACK_FRAME_REGS_MARKER) {
+			return 1;
+		}
+
+		newsp = stack[0];
+		/* Stack grows downwards; unwinder may only go up. */
+		if (newsp <= sp)
+			return 1;
+
+		if (newsp != stack_end &&
+		    newsp > stack_end - STACK_FRAME_MIN_SIZE) {
+			return 1; /* invalid backlink, too far up. */
+		}
+
+		/* Examine the saved LR: it must point into kernel code. */
+		ip = stack[STACK_FRAME_LR_SAVE];
+		if (!firstframe && !__kernel_text_address(ip))
+			return 1;
+		firstframe = 0;
+
+		/*
+		 * FIXME: IMHO these tests do not belong in
+		 * arch-dependent code, they are generic.
+		 */
+		ip = ftrace_graph_ret_addr(tsk, &graph_idx, ip, NULL);
+
+		/*
+		 * Mark stacktraces with kretprobed functions on them
+		 * as unreliable.
+		 */
+		if (ip == (unsigned long)kretprobe_trampoline)
+			return 1;
+
+		if (!trace->skip)
+			trace->entries[trace->nr_entries++] = ip;
+		else
+			trace->skip--;
+
+		if (newsp == stack_end)
+			break;
+
+		if (trace->nr_entries >= trace->max_entries)
+			return -E2BIG;
+
+		sp = newsp;
+	}
+	return 0;
+}
+EXPORT_SYMBOL_GPL(save_stack_trace_tsk_reliable);
+#endif /* CONFIG_HAVE_RELIABLE_STACKTRACE */

commit 4f9b514b765a3057341f3236c94877d9413babc7
Author: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
Date:   Mon Mar 27 16:32:33 2017 -0300

    powerpc: Make /proc/self/stack always print the current stack
    
    For the current task, the kernel stack would only tell the last time the
    process was rescheduled, if ever. Use the current stack pointer for the
    current task.
    
    Otherwise, every once in a while, the stacktrace printed when reading
    /proc/self/stack would look like the process is running in userspace,
    while it's not, which some may consider as a bug.
    
    This is also consistent with some other architectures, like x86 and arm,
    at least.
    
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 66711958493c..d534ed901538 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -59,7 +59,14 @@ EXPORT_SYMBOL_GPL(save_stack_trace);
 
 void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 {
-	save_context_stack(trace, tsk->thread.ksp, tsk, 0);
+	unsigned long sp;
+
+	if (tsk == current)
+		sp = current_stack_pointer();
+	else
+		sp = tsk->thread.ksp;
+
+	save_context_stack(trace, sp, tsk, 0);
 }
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
 

commit b17b01533b719e9949e437abf66436a875739b40
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/debug.h>
    
    We are going to split <linux/sched/debug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/debug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 4f24606afc3f..66711958493c 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -12,6 +12,7 @@
 
 #include <linux/export.h>
 #include <linux/sched.h>
+#include <linux/sched/debug.h>
 #include <linux/stacktrace.h>
 #include <asm/ptrace.h>
 #include <asm/processor.h>

commit 35de3b1aa16842214e0cd7c6036daf4619294314
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Dec 8 13:50:56 2015 -0500

    powerpc: Implement save_stack_trace_regs() to enable kprobe stack tracing
    
    It has come to my attention that kprobe event stack tracing does not
    work on powerpc. You can see with the following:
    
      # cd /sys/kernel/debug/tracing
      # echo stacktrace > trace_options
      # echo 'p kfree' > kprobe_events
      # echo 1 > events/kprobes/enable
    
    Will print the following warning:
      save_stack_trace_regs() not implemented yet.
    
    Although save_stack_trace() (which normal event stack traces use) is
    implemented, save_stack_trace_regs() which kprobe events use is not.
    This is a cheap attempt to implement that function.
    
    Note, This may have issues if a task tries to get a stack trace from
    another task with its regs, because it just passes in "current" to
    save_context_stack(). But this does solve the issue with stack tracing
    kprobe events.
    
    Reported-by: Chunyu Hu <chuhu@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index ea43a347a104..4f24606afc3f 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -61,3 +61,10 @@ void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 	save_context_stack(trace, tsk->thread.ksp, tsk, 0);
 }
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
+
+void
+save_stack_trace_regs(struct pt_regs *regs, struct stack_trace *trace)
+{
+	save_context_stack(trace, regs->gpr[1], current, 0);
+}
+EXPORT_SYMBOL_GPL(save_stack_trace_regs);

commit acf620ecf56cfc4edaffaf158250e128539cdd26
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Oct 13 19:41:39 2014 +1100

    powerpc: Rename __get_SP() to current_stack_pointer()
    
    Michael points out that __get_SP() is a pretty horrible
    function name. Let's give it a better name.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 7f65baec29f6..ea43a347a104 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -50,7 +50,7 @@ void save_stack_trace(struct stack_trace *trace)
 {
 	unsigned long sp;
 
-	sp = __get_SP();
+	sp = current_stack_pointer();
 
 	save_context_stack(trace, sp, current, 1);
 }

commit bfe9a2cfe91a1c920f152ce5fd0a9ad74b3daf12
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Oct 13 19:41:38 2014 +1100

    powerpc: Reimplement __get_SP() as a function not a define
    
    Li Zhong points out an issue with our current __get_SP()
    implementation. If ftrace function tracing is enabled (ie -pg
    profiling using _mcount) we spill a stack frame on 64bit all the
    time.
    
    If a function calls __get_SP() and later calls a function that is
    tail call optimised, we will pop the stack frame and the value
    returned by __get_SP() is no longer valid. An example from Li can
    be found in save_stack_trace -> save_context_stack:
    
    c0000000000432c0 <.save_stack_trace>:
    c0000000000432c0:       mflr    r0
    c0000000000432c4:       std     r0,16(r1)
    c0000000000432c8:       stdu    r1,-128(r1) <-- stack frame for _mcount
    c0000000000432cc:       std     r3,112(r1)
    c0000000000432d0:       bl      <._mcount>
    c0000000000432d4:       nop
    
    c0000000000432d8:       mr      r4,r1 <-- __get_SP()
    
    c0000000000432dc:       ld      r5,632(r13)
    c0000000000432e0:       ld      r3,112(r1)
    c0000000000432e4:       li      r6,1
    
    c0000000000432e8:       addi    r1,r1,128 <-- pop stack frame
    
    c0000000000432ec:       ld      r0,16(r1)
    c0000000000432f0:       mtlr    r0
    c0000000000432f4:       b       <.save_context_stack> <-- tail call optimized
    
    save_context_stack ends up with a stack pointer below the current
    one, and it is likely to be scribbled over.
    
    Fix this by making __get_SP() a function which returns the
    callers stack frame. Also replace inline assembly which grabs
    the stack pointer in save_stack_trace and show_stack with
    __get_SP().
    
    This also fixes an issue with perf_arch_fetch_caller_regs().
    It currently unwinds the stack once, which will skip a
    valid stack frame on a leaf function. With the __get_SP() fixes
    in this patch, we never need to unwind the stack frame to get
    to the first interesting frame.
    
    We have to export __get_SP() because perf_arch_fetch_caller_regs()
    (which is used in modules) calls it from a header file.
    
    Reported-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 3d30ef1038e5..7f65baec29f6 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -50,7 +50,7 @@ void save_stack_trace(struct stack_trace *trace)
 {
 	unsigned long sp;
 
-	asm("mr %0,1" : "=r" (sp));
+	sp = __get_SP();
 
 	save_context_stack(trace, sp, current, 1);
 }

commit 4b16f8e2d6d64249f0ed3ca7fe2a319d0dde2719
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jul 22 18:24:23 2011 -0400

    powerpc: various straight conversions from module.h --> export.h
    
    All these files were including module.h just for the basic
    EXPORT_SYMBOL infrastructure.  We can shift them off to the
    export.h header which is a way smaller footprint and thus
    realize some compile time gains.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index b0dbb1daa4df..3d30ef1038e5 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -10,7 +10,7 @@
  *      2 of the License, or (at your option) any later version.
  */
 
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/sched.h>
 #include <linux/stacktrace.h>
 #include <asm/ptrace.h>

commit d3b060231b2e1eb7e7e9680ff93326a4ae576720
Author: Huang Weiyi <weiyi.huang@gmail.com>
Date:   Thu Jul 24 00:44:51 2008 +1000

    powerpc: Removed duplicated include in stacktrace.c
    
    Removed duplicated include file <linux/module.h> in
    arch/powerpc/kernel/stacktrace.c.
    
    Signed-off-by: Huang Weiyi <weiyi.huang@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index f2589645870a..b0dbb1daa4df 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -13,7 +13,6 @@
 #include <linux/module.h>
 #include <linux/sched.h>
 #include <linux/stacktrace.h>
-#include <linux/module.h>
 #include <asm/ptrace.h>
 #include <asm/processor.h>
 

commit 6fdc9f5076e0f6018e1d9250c6673f812b556d90
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Jul 17 08:12:25 2008 +1000

    powerpc: Fix support for latencytop
    
    We need to pass the kernel stack pointer instead of the user space
    stack pointer in save_stack_trace_tsk().
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 071bee3ec749..f2589645870a 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -59,6 +59,6 @@ EXPORT_SYMBOL_GPL(save_stack_trace);
 
 void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 {
-	save_context_stack(trace, tsk->thread.regs->gpr[1], tsk, 0);
+	save_context_stack(trace, tsk->thread.ksp, tsk, 0);
 }
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk);

commit 84c3d4aaec3338201b449034beac41635866bddf
Merge: 43d2548bb2ef fafa3a3f1672
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Jul 16 11:07:59 2008 +1000

    Merge commit 'origin/master'
    
    Manual merge of:
    
            arch/powerpc/Kconfig
            arch/powerpc/kernel/stacktrace.c
            arch/powerpc/mm/slice.c
            arch/ppc/kernel/smp.c

commit 01f4b8b8b8db09b88be7df7e51192e4e678b69d3
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jul 11 00:08:18 2008 +1000

    powerpc: support for latencytop
    
    Implement save_stack_trace_tsk on powerpc, so that we can run with
    latencytop.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 962944038430..6a4fb003fa54 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -10,33 +10,34 @@
  *      2 of the License, or (at your option) any later version.
  */
 
+#include <linux/module.h>
 #include <linux/sched.h>
 #include <linux/stacktrace.h>
 #include <asm/ptrace.h>
+#include <asm/processor.h>
 
 /*
  * Save stack-backtrace addresses into a stack_trace buffer.
  */
-void save_stack_trace(struct stack_trace *trace)
+static void save_context_stack(struct stack_trace *trace, unsigned long sp,
+			struct task_struct *tsk, int savesched)
 {
-	unsigned long sp;
-
-	asm("mr %0,1" : "=r" (sp));
-
 	for (;;) {
 		unsigned long *stack = (unsigned long *) sp;
 		unsigned long newsp, ip;
 
-		if (!validate_sp(sp, current, STACK_FRAME_OVERHEAD))
+		if (!validate_sp(sp, tsk, STACK_FRAME_OVERHEAD))
 			return;
 
 		newsp = stack[0];
 		ip = stack[STACK_FRAME_LR_SAVE];
 
-		if (!trace->skip)
-			trace->entries[trace->nr_entries++] = ip;
-		else
-			trace->skip--;
+		if (savesched || !in_sched_functions(ip)) {
+			if (!trace->skip)
+				trace->entries[trace->nr_entries++] = ip;
+			else
+				trace->skip--;
+		}
 
 		if (trace->nr_entries >= trace->max_entries)
 			return;
@@ -44,3 +45,19 @@ void save_stack_trace(struct stack_trace *trace)
 		sp = newsp;
 	}
 }
+
+void save_stack_trace(struct stack_trace *trace)
+{
+	unsigned long sp;
+
+	asm("mr %0,1" : "=r" (sp));
+
+	save_context_stack(trace, sp, current, 1);
+}
+EXPORT_SYMBOL_GPL(save_stack_trace);
+
+void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
+{
+	save_context_stack(trace, tsk->thread.regs->gpr[1], tsk, 0);
+}
+EXPORT_SYMBOL_GPL(save_stack_trace_tsk);

commit 7798ed0f57b4d137e660fbf5be1e1528e40f89ac
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Mon Jul 14 19:55:03 2008 +1000

    generic-ipi: powerpc/generic-ipi tree build failure
    
    Today's linux-next build (powerpc allmodconfig) failed like this:
    
    ERROR: ".save_stack_trace" [tests/backtracetest.ko] undefined!
    
    But save_stack_trace is exported in arch/powerpc/kernel/stacktrace.c
    
    I couldn't figure it out until I noticed these earlier warnings:
    
    arch/powerpc/kernel/stacktrace.c:47: warning: data definition has no type or storage class
    arch/powerpc/kernel/stacktrace.c:47: warning: type defaults to 'int' in declaration of 'EXPORT_SYMBOL_GPL'
    arch/powerpc/kernel/stacktrace.c:47: warning: parameter names (without types) in function declaration
    
    I applied the patch below.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: <linuxppc-dev@ozlabs.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 9861f17258da..3cf0d94ba340 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -12,6 +12,7 @@
 
 #include <linux/sched.h>
 #include <linux/stacktrace.h>
+#include <linux/module.h>
 #include <asm/ptrace.h>
 
 /*

commit 7b4c9505f2fd82b117dd015b561f723b9a5dab79
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jul 3 09:17:55 2008 +0200

    stacktrace: export save_stack_trace[_tsk]
    
    Andrew Morton reported this against linux-next:
    
    ERROR: ".save_stack_trace" [tests/backtracetest.ko] undefined!
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index 962944038430..9861f17258da 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -44,3 +44,4 @@ void save_stack_trace(struct stack_trace *trace)
 		sp = newsp;
 	}
 }
+EXPORT_SYMBOL_GPL(save_stack_trace);

commit 885aa35c9669ce7919d203036a87a7e1a4ebd25f
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 24 00:32:29 2008 +1000

    [POWERPC] Fix new warnings arising from stacktrace patch
    
    Remove the inclusion of asm-offsets.h from stacktrace.c.  It isn't
    supposed to be included in C code and it causes problems with multiple
    definitions of things.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
index e3638eeaaae7..962944038430 100644
--- a/arch/powerpc/kernel/stacktrace.c
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -13,7 +13,6 @@
 #include <linux/sched.h>
 #include <linux/stacktrace.h>
 #include <asm/ptrace.h>
-#include <asm/asm-offsets.h>
 
 /*
  * Save stack-backtrace addresses into a stack_trace buffer.

commit fd3e0bbc6052ca9747a5332b382584ece83aab6d
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 17 14:35:00 2008 +1000

    [POWERPC] Stacktrace support for lockdep
    
    This adds stacktrace support for powerpc, which will be needed for
    lockdep.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/stacktrace.c b/arch/powerpc/kernel/stacktrace.c
new file mode 100644
index 000000000000..e3638eeaaae7
--- /dev/null
+++ b/arch/powerpc/kernel/stacktrace.c
@@ -0,0 +1,47 @@
+/*
+ * Stack trace utility
+ *
+ * Copyright 2008 Christoph Hellwig, IBM Corp.
+ *
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/sched.h>
+#include <linux/stacktrace.h>
+#include <asm/ptrace.h>
+#include <asm/asm-offsets.h>
+
+/*
+ * Save stack-backtrace addresses into a stack_trace buffer.
+ */
+void save_stack_trace(struct stack_trace *trace)
+{
+	unsigned long sp;
+
+	asm("mr %0,1" : "=r" (sp));
+
+	for (;;) {
+		unsigned long *stack = (unsigned long *) sp;
+		unsigned long newsp, ip;
+
+		if (!validate_sp(sp, current, STACK_FRAME_OVERHEAD))
+			return;
+
+		newsp = stack[0];
+		ip = stack[STACK_FRAME_LR_SAVE];
+
+		if (!trace->skip)
+			trace->entries[trace->nr_entries++] = ip;
+		else
+			trace->skip--;
+
+		if (trace->nr_entries >= trace->max_entries)
+			return;
+
+		sp = newsp;
+	}
+}
