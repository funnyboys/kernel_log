commit abd106fb437ad1cd8c8df8ccabd0fa941ef6342a
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri May 8 14:34:05 2020 +1000

    powerpc/64s: machine check do not trace real-mode handler
    
    Rather than notrace annotations throughout a significant part of the
    machine check code across kernel/ pseries/ and powernv/ which can
    easily be broken and is infrequently tested, use paca->ftrace_enabled
    to blanket-disable tracing of the real-mode non-maskable handler.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Acked-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Link: https://lore.kernel.org/r/20200508043408.886394-14-npiggin@gmail.com

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index be7e3f92a7b5..fd90c0eda229 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -16,6 +16,7 @@
 #include <linux/export.h>
 #include <linux/irq_work.h>
 #include <linux/extable.h>
+#include <linux/ftrace.h>
 
 #include <asm/machdep.h>
 #include <asm/mce.h>
@@ -571,10 +572,14 @@ EXPORT_SYMBOL_GPL(machine_check_print_event_info);
  *
  * regs->nip and regs->msr contains srr0 and ssr1.
  */
-long machine_check_early(struct pt_regs *regs)
+long notrace machine_check_early(struct pt_regs *regs)
 {
 	long handled = 0;
 	bool nested = in_nmi();
+	u8 ftrace_enabled = this_cpu_get_ftrace_enabled();
+
+	this_cpu_set_ftrace_enabled(0);
+
 	if (!nested)
 		nmi_enter();
 
@@ -589,6 +594,8 @@ long machine_check_early(struct pt_regs *regs)
 	if (!nested)
 		nmi_exit();
 
+	this_cpu_set_ftrace_enabled(ftrace_enabled);
+
 	return handled;
 }
 

commit 116ac378bb3ff844df333e7609e7604651a0db9d
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri May 8 14:34:03 2020 +1000

    powerpc/64s: machine check interrupt update NMI accounting
    
    machine_check_early() is taken as an NMI, so nmi_enter() is used
    there. machine_check_exception() is no longer taken as an NMI (it's
    invoked via irq_work in the case a machine check hits in kernel mode),
    so remove the nmi_enter() from that case.
    
    In NMI context, hash faults don't try to refill the hash table, which
    can lead to crashes accessing non-pinned kernel pages. System reset
    still has this potential problem.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Drop change in show_regs() which breaks Book3E]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200508043408.886394-12-npiggin@gmail.com

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 8077b5fb18a7..be7e3f92a7b5 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -574,6 +574,9 @@ EXPORT_SYMBOL_GPL(machine_check_print_event_info);
 long machine_check_early(struct pt_regs *regs)
 {
 	long handled = 0;
+	bool nested = in_nmi();
+	if (!nested)
+		nmi_enter();
 
 	hv_nmi_check_nonrecoverable(regs);
 
@@ -582,6 +585,10 @@ long machine_check_early(struct pt_regs *regs)
 	 */
 	if (ppc_md.machine_check_early)
 		handled = ppc_md.machine_check_early(regs);
+
+	if (!nested)
+		nmi_exit();
+
 	return handled;
 }
 

commit efbc4303b255bb80ab1283794b36dd5fe1fb0ec3
Author: Ganesh Goudar <ganeshgr@linux.ibm.com>
Date:   Fri Mar 27 00:19:16 2020 +0530

    powerpc/pseries: Handle UE event for memcpy_mcsafe
    
    memcpy_mcsafe has been implemented for power machines which is used
    by pmem infrastructure, so that an UE encountered during memcpy from
    pmem devices would not result in panic instead a right error code
    is returned. The implementation expects machine check handler to ignore
    the event and set nip to continue the execution from fixup code.
    
    Appropriate changes are already made to powernv machine check handler,
    make similar changes to pseries machine check handler to ignore the
    the event and set nip to continue execution at the fixup entry if we
    hit UE at an instruction with a fixup entry.
    
    while we are at it, have a common function which searches the exception
    table entry and updates nip with fixup address, and any future common
    changes can be made in this function that are valid for both architectures.
    
    powernv changes are made by
    commit 895e3dceeb97 ("powerpc/mce: Handle UE event for memcpy_mcsafe")
    
    Reviewed-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Reviewed-by: Santosh S <santosh@fossix.org>
    Signed-off-by: Ganesh Goudar <ganeshgr@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200326184916.31172-1-ganeshgr@linux.ibm.com

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 34c1001e9e8b..8077b5fb18a7 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -15,6 +15,7 @@
 #include <linux/percpu.h>
 #include <linux/export.h>
 #include <linux/irq_work.h>
+#include <linux/extable.h>
 
 #include <asm/machdep.h>
 #include <asm/mce.h>
@@ -251,6 +252,19 @@ void machine_check_queue_event(void)
 	/* Queue irq work to process this event later. */
 	irq_work_queue(&mce_event_process_work);
 }
+
+void mce_common_process_ue(struct pt_regs *regs,
+			   struct mce_error_info *mce_err)
+{
+	const struct exception_table_entry *entry;
+
+	entry = search_kernel_exception_table(regs->nip);
+	if (entry) {
+		mce_err->ignore_event = true;
+		regs->nip = extable_fixup(entry);
+	}
+}
+
 /*
  * process pending MCE event from the mce event queue. This function will be
  * called during syscall exit.

commit 9ca766f9891d23743b4e1a7b1cafdc63723cd6a7
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Aug 2 20:56:34 2019 +1000

    powerpc/64s/pseries: machine check convert to use common event code
    
    The common machine_check_event data structures and queues are mostly
    platform independent, with powernv decoding SRR1/DSISR/etc., into
    machine_check_event objects.
    
    This patch converts pseries to use this infrastructure by decoding
    fwnmi/rtas data into machine_check_event objects.
    
    This allows queueing to be used by a subsequent change to delay the
    virtual mode handling of machine checks that occur in kernel space
    where it is unsafe to switch immediately to virtual mode, similarly
    to powernv.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Fix implicit fallthrough warnings in mce_handle_error()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190802105709.27696-10-npiggin@gmail.com

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 04280a5871fc..34c1001e9e8b 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -325,7 +325,7 @@ static void machine_check_process_queued_event(struct irq_work *work)
 void machine_check_print_event_info(struct machine_check_event *evt,
 				    bool user_mode, bool in_guest)
 {
-	const char *level, *sevstr, *subtype, *err_type;
+	const char *level, *sevstr, *subtype, *err_type, *initiator;
 	uint64_t ea = 0, pa = 0;
 	int n = 0;
 	char dar_str[50];
@@ -410,6 +410,28 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 		break;
 	}
 
+	switch(evt->initiator) {
+	case MCE_INITIATOR_CPU:
+		initiator = "CPU";
+		break;
+	case MCE_INITIATOR_PCI:
+		initiator = "PCI";
+		break;
+	case MCE_INITIATOR_ISA:
+		initiator = "ISA";
+		break;
+	case MCE_INITIATOR_MEMORY:
+		initiator = "Memory";
+		break;
+	case MCE_INITIATOR_POWERMGM:
+		initiator = "Power Management";
+		break;
+	case MCE_INITIATOR_UNKNOWN:
+	default:
+		initiator = "Unknown";
+		break;
+	}
+
 	switch (evt->error_type) {
 	case MCE_ERROR_TYPE_UE:
 		err_type = "UE";
@@ -476,6 +498,14 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 		if (evt->u.link_error.effective_address_provided)
 			ea = evt->u.link_error.effective_address;
 		break;
+	case MCE_ERROR_TYPE_DCACHE:
+		err_type = "D-Cache";
+		subtype = "Unknown";
+		break;
+	case MCE_ERROR_TYPE_ICACHE:
+		err_type = "I-Cache";
+		subtype = "Unknown";
+		break;
 	default:
 	case MCE_ERROR_TYPE_UNKNOWN:
 		err_type = "Unknown";
@@ -508,6 +538,8 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 			level, evt->cpu, evt->srr0, (void *)evt->srr0, pa_str);
 	}
 
+	printk("%sMCE: CPU%d: Initiator %s\n", level, evt->cpu, initiator);
+
 	subtype = evt->error_class < ARRAY_SIZE(mc_error_class) ?
 		mc_error_class[evt->error_class] : "Unknown";
 	printk("%sMCE: CPU%d: %s\n", level, evt->cpu, subtype);

commit 7290f3b3d3e66b54720f23079ffc60e0b7bbb0cc
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Aug 2 20:56:33 2019 +1000

    powerpc/64s/powernv: machine check dump SLB contents
    
    Re-use the code introduced in pseries to save and dump the contents
    of the SLB in the case of an SLB involved machine check exception.
    
    This patch also avoids allocating the SLB save array on pseries radix.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190802105709.27696-9-npiggin@gmail.com

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index ec4b3e1087be..04280a5871fc 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -511,6 +511,12 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 	subtype = evt->error_class < ARRAY_SIZE(mc_error_class) ?
 		mc_error_class[evt->error_class] : "Unknown";
 	printk("%sMCE: CPU%d: %s\n", level, evt->cpu, subtype);
+
+#ifdef CONFIG_PPC_BOOK3S_64
+	/* Display faulty slb contents for SLB errors. */
+	if (evt->error_type == MCE_ERROR_TYPE_SLB)
+		slb_dump_contents(local_paca->mce_faulty_slbs);
+#endif
 }
 EXPORT_SYMBOL_GPL(machine_check_print_event_info);
 

commit 895e3dceeb97855dc9990136cbb80a842fe581aa
Author: Balbir Singh <bsingharora@gmail.com>
Date:   Tue Aug 20 13:43:50 2019 +0530

    powerpc/mce: Handle UE event for memcpy_mcsafe
    
    If we take a UE on one of the instructions with a fixup entry, set nip
    to continue execution at the fixup entry. Stop processing the event
    further or print it.
    
    Co-developed-by: Reza Arbab <arbab@linux.ibm.com>
    Signed-off-by: Reza Arbab <arbab@linux.ibm.com>
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Reviewed-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Santosh Sivaraj <santosh@fossix.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190820081352.8641-6-santosh@fossix.org

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index a3b122a685a5..ec4b3e1087be 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -149,6 +149,7 @@ void save_mce_event(struct pt_regs *regs, long handled,
 		if (phys_addr != ULONG_MAX) {
 			mce->u.ue_error.physical_address_provided = true;
 			mce->u.ue_error.physical_address = phys_addr;
+			mce->u.ue_error.ignore_event = mce_err->ignore_event;
 			machine_check_ue_event(mce);
 		}
 	}
@@ -266,8 +267,17 @@ static void machine_process_ue_event(struct work_struct *work)
 		/*
 		 * This should probably queued elsewhere, but
 		 * oh! well
+		 *
+		 * Don't report this machine check because the caller has a
+		 * asked us to ignore the event, it has a fixup handler which
+		 * will do the appropriate error handling and reporting.
 		 */
 		if (evt->error_type == MCE_ERROR_TYPE_UE) {
+			if (evt->u.ue_error.ignore_event) {
+				__this_cpu_dec(mce_ue_count);
+				continue;
+			}
+
 			if (evt->u.ue_error.physical_address_provided) {
 				unsigned long pfn;
 
@@ -301,6 +311,12 @@ static void machine_check_process_queued_event(struct irq_work *work)
 	while (__this_cpu_read(mce_queue_count) > 0) {
 		index = __this_cpu_read(mce_queue_count) - 1;
 		evt = this_cpu_ptr(&mce_event_queue[index]);
+
+		if (evt->error_type == MCE_ERROR_TYPE_UE &&
+		    evt->u.ue_error.ignore_event) {
+			__this_cpu_dec(mce_queue_count);
+			continue;
+		}
 		machine_check_print_event_info(evt, false, false);
 		__this_cpu_dec(mce_queue_count);
 	}

commit 1a1715f516fd7fcfedffee5978ec4c07c5164470
Author: Reza Arbab <arbab@linux.ibm.com>
Date:   Tue Aug 20 13:43:48 2019 +0530

    powerpc/mce: Make machine_check_ue_event() static
    
    The function doesn't get used outside this file, so make it static.
    
    Signed-off-by: Reza Arbab <arbab@linux.ibm.com>
    Signed-off-by: Santosh Sivaraj <santosh@fossix.org>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190820081352.8641-4-santosh@fossix.org

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index cff31d4a501f..a3b122a685a5 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -34,7 +34,7 @@ static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT],
 
 static void machine_check_process_queued_event(struct irq_work *work);
 static void machine_check_ue_irq_work(struct irq_work *work);
-void machine_check_ue_event(struct machine_check_event *evt);
+static void machine_check_ue_event(struct machine_check_event *evt);
 static void machine_process_ue_event(struct work_struct *work);
 
 static struct irq_work mce_event_process_work = {
@@ -212,7 +212,7 @@ static void machine_check_ue_irq_work(struct irq_work *work)
 /*
  * Queue up the MCE event which then can be handled later.
  */
-void machine_check_ue_event(struct machine_check_event *evt)
+static void machine_check_ue_event(struct machine_check_event *evt)
 {
 	int index;
 

commit b5bda6263cad9a927e1a4edb7493d542da0c1410
Author: Santosh Sivaraj <santosh@fossix.org>
Date:   Tue Aug 20 13:43:46 2019 +0530

    powerpc/mce: Schedule work from irq_work
    
    schedule_work() cannot be called from MCE exception context as MCE can
    interrupt even in interrupt disabled context.
    
    Fixes: 733e4a4c4467 ("powerpc/mce: hookup memory_failure for UE errors")
    Cc: stable@vger.kernel.org # v4.15+
    Reviewed-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Santosh Sivaraj <santosh@fossix.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190820081352.8641-2-santosh@fossix.org

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index b18df633eae9..cff31d4a501f 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -33,6 +33,7 @@ static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT],
 					mce_ue_event_queue);
 
 static void machine_check_process_queued_event(struct irq_work *work);
+static void machine_check_ue_irq_work(struct irq_work *work);
 void machine_check_ue_event(struct machine_check_event *evt);
 static void machine_process_ue_event(struct work_struct *work);
 
@@ -40,6 +41,10 @@ static struct irq_work mce_event_process_work = {
         .func = machine_check_process_queued_event,
 };
 
+static struct irq_work mce_ue_event_irq_work = {
+	.func = machine_check_ue_irq_work,
+};
+
 DECLARE_WORK(mce_ue_event_work, machine_process_ue_event);
 
 static void mce_set_error_info(struct machine_check_event *mce,
@@ -199,6 +204,10 @@ void release_mce_event(void)
 	get_mce_event(NULL, true);
 }
 
+static void machine_check_ue_irq_work(struct irq_work *work)
+{
+	schedule_work(&mce_ue_event_work);
+}
 
 /*
  * Queue up the MCE event which then can be handled later.
@@ -216,7 +225,7 @@ void machine_check_ue_event(struct machine_check_event *evt)
 	memcpy(this_cpu_ptr(&mce_ue_event_queue[index]), evt, sizeof(*evt));
 
 	/* Queue work to process this event later. */
-	schedule_work(&mce_ue_event_work);
+	irq_work_queue(&mce_ue_event_irq_work);
 }
 
 /*

commit 1a59d1b8e05ea6ab45f7e18897de1ef0e6bc3da6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:05 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 156
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version this program is distributed in the
      hope that it will be useful but without any warranty without even
      the implied warranty of merchantability or fitness for a particular
      purpose see the gnu general public license for more details you
      should have received a copy of the gnu general public license along
      with this program if not write to the free software foundation inc
      59 temple place suite 330 boston ma 02111 1307 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 1334 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070033.113240726@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 4581377cfc98..b18df633eae9 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -1,20 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * Machine check exception handling.
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
- *
  * Copyright 2013 IBM Corporation
  * Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
  */

commit 50dbabe06a6e1c35980154ea1fac2ed6ad28652b
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Mon Apr 29 23:46:02 2019 +0530

    powerpc/powernv/mce: Print additional information about MCE error.
    
    Print more information about MCE error whether it is an hardware or
    software error.
    
    Some of the MCE errors can be easily categorized as hardware or
    software errors e.g. UEs are due to hardware error, where as error
    triggered due to invalid usage of tlbie is a pure software bug. But
    not all the MCE errors can be easily categorize into either software
    or hardware. There are errors like multihit errors which are usually
    result of a software bug, but in some rare cases a hardware failure
    can cause a multihit error. In past, we have seen case where after
    replacing faulty chip, multihit errors stopped occurring. Same with
    parity errors, which are usually due to faulty hardware but there are
    chances where multihit can also cause an parity error. Such errors are
    difficult to determine what really caused it. Hence this patch
    classifies MCE errors into following four categorize:
    
      1. Hardware error:
            UE and Link timeout failure errors.
      2. Probable hardware error (some chance of software cause)
            SLB/ERAT/TLB Parity errors.
      3. Software error
            Invalid tlbie form.
      4. Probable software error (some chance of hardware cause)
            SLB/ERAT/TLB Multihit errors.
    
    Sample output:
    
      MCE: CPU80: machine check (Warning) Guest SLB Multihit DAR: 000001001b6e0320 [Recovered]
      MCE: CPU80: PID: 24765 Comm: qemu-system-ppc Guest NIP: [00007fffa309dc60]
      MCE: CPU80: Probable Software error (some chance of hardware cause)
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 71d245a387ab..4581377cfc98 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -123,6 +123,7 @@ void save_mce_event(struct pt_regs *regs, long handled,
 	mce->initiator = mce_err->initiator;
 	mce->severity = mce_err->severity;
 	mce->sync_error = mce_err->sync_error;
+	mce->error_class = mce_err->error_class;
 
 	/*
 	 * Populate the mce error_type and type-specific error_type.
@@ -363,6 +364,13 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 		"Store (timeout)",
 		"Page table walk Load/Store (timeout)",
 	};
+	static const char *mc_error_class[] = {
+		"Unknown",
+		"Hardware error",
+		"Probable Hardware error (some chance of software cause)",
+		"Software error",
+		"Probable Software error (some chance of hardware cause)",
+	};
 
 	/* Print things out */
 	if (evt->version != MCE_V1) {
@@ -487,6 +495,10 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 		printk("%sMCE: CPU%d: NIP: [%016llx] %pS%s\n",
 			level, evt->cpu, evt->srr0, (void *)evt->srr0, pa_str);
 	}
+
+	subtype = evt->error_class < ARRAY_SIZE(mc_error_class) ?
+		mc_error_class[evt->error_class] : "Unknown";
+	printk("%sMCE: CPU%d: %s\n", level, evt->cpu, subtype);
 }
 EXPORT_SYMBOL_GPL(machine_check_print_event_info);
 

commit cda6618d060b5e8afc93e691d4bcd987f3dd4393
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Mon Apr 29 23:45:55 2019 +0530

    powerpc/powernv/mce: Print correct severity for MCE error.
    
    Currently all machine check errors are printed as severe errors which
    isn't correct. Print soft errors as warning instead of severe errors.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 25a8b20cbbdc..71d245a387ab 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -122,6 +122,7 @@ void save_mce_event(struct pt_regs *regs, long handled,
 
 	mce->initiator = mce_err->initiator;
 	mce->severity = mce_err->severity;
+	mce->sync_error = mce_err->sync_error;
 
 	/*
 	 * Populate the mce error_type and type-specific error_type.
@@ -376,9 +377,9 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 		break;
 	case MCE_SEV_WARNING:
 		level = KERN_WARNING;
-		sevstr = "";
+		sevstr = "Warning";
 		break;
-	case MCE_SEV_ERROR_SYNC:
+	case MCE_SEV_SEVERE:
 		level = KERN_ERR;
 		sevstr = "Severe";
 		break;

commit d6e8a150850601277039a548ffcdddd1bfe3e365
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Mon Apr 29 23:45:48 2019 +0530

    powerpc/powernv/mce: Reduce MCE console logs to lesser lines.
    
    Also add cpu number while displaying MCE log. This will help cleaner
    logs when MCE hits on multiple cpus simultaneously.
    
    Before the changes the MCE output was:
    
      Severe Machine check interrupt [Recovered]
        NIP [d00000000ba80280]: insert_slb_entry.constprop.0+0x278/0x2c0 [mcetest_slb]
        Initiator: CPU
        Error type: SLB [Multihit]
          Effective address: d00000000ba80280
    
    After this patch series changes the MCE output will be:
    
      MCE: CPU80: machine check (Warning) Host SLB Multihit [Recovered]
      MCE: CPU80: NIP: [d00000000b550280] insert_slb_entry.constprop.0+0x278/0x2c0 [mcetest_slb]
      MCE: CPU80: Probable software error (some chance of hardware cause)
    
    UE in host application:
    
      MCE: CPU48: machine check (Severe) Host UE Load/Store DAR: 00007fffc6079a80 paddr: 0000000f8e260000 [Not recovered]
      MCE: CPU48: PID: 4584 Comm: find NIP: [0000000010023368]
      MCE: CPU48: Hardware error
    
    and for MCE in Guest:
    
      MCE: CPU80: machine check (Warning) Guest SLB Multihit DAR: 000001001b6e0320 [Recovered]
      MCE: CPU80: PID: 24765 Comm: qemu-system-ppc Guest NIP: [00007fffa309dc60]
      MCE: CPU80: Probable software error (some chance of hardware cause)
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index b5fec1f9751a..25a8b20cbbdc 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -112,6 +112,7 @@ void save_mce_event(struct pt_regs *regs, long handled,
 	mce->srr1 = regs->msr;
 	mce->gpr3 = regs->gpr[3];
 	mce->in_use = 1;
+	mce->cpu = get_paca()->paca_index;
 
 	/* Mark it recovered if we have handled it and MSR(RI=1). */
 	if (handled && (regs->msr & MSR_RI))
@@ -310,7 +311,11 @@ static void machine_check_process_queued_event(struct irq_work *work)
 void machine_check_print_event_info(struct machine_check_event *evt,
 				    bool user_mode, bool in_guest)
 {
-	const char *level, *sevstr, *subtype;
+	const char *level, *sevstr, *subtype, *err_type;
+	uint64_t ea = 0, pa = 0;
+	int n = 0;
+	char dar_str[50];
+	char pa_str[50];
 	static const char *mc_ue_types[] = {
 		"Indeterminate",
 		"Instruction fetch",
@@ -384,101 +389,103 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 		break;
 	}
 
-	printk("%s%s Machine check interrupt [%s]\n", level, sevstr,
-	       evt->disposition == MCE_DISPOSITION_RECOVERED ?
-	       "Recovered" : "Not recovered");
-
-	if (in_guest) {
-		printk("%s  Guest NIP: %016llx\n", level, evt->srr0);
-	} else if (user_mode) {
-		printk("%s  NIP: [%016llx] PID: %d Comm: %s\n", level,
-			evt->srr0, current->pid, current->comm);
-	} else {
-		printk("%s  NIP [%016llx]: %pS\n", level, evt->srr0,
-		       (void *)evt->srr0);
-	}
-
-	printk("%s  Initiator: %s\n", level,
-	       evt->initiator == MCE_INITIATOR_CPU ? "CPU" : "Unknown");
 	switch (evt->error_type) {
 	case MCE_ERROR_TYPE_UE:
+		err_type = "UE";
 		subtype = evt->u.ue_error.ue_error_type <
 			ARRAY_SIZE(mc_ue_types) ?
 			mc_ue_types[evt->u.ue_error.ue_error_type]
 			: "Unknown";
-		printk("%s  Error type: UE [%s]\n", level, subtype);
 		if (evt->u.ue_error.effective_address_provided)
-			printk("%s    Effective address: %016llx\n",
-			       level, evt->u.ue_error.effective_address);
+			ea = evt->u.ue_error.effective_address;
 		if (evt->u.ue_error.physical_address_provided)
-			printk("%s    Physical address:  %016llx\n",
-			       level, evt->u.ue_error.physical_address);
+			pa = evt->u.ue_error.physical_address;
 		break;
 	case MCE_ERROR_TYPE_SLB:
+		err_type = "SLB";
 		subtype = evt->u.slb_error.slb_error_type <
 			ARRAY_SIZE(mc_slb_types) ?
 			mc_slb_types[evt->u.slb_error.slb_error_type]
 			: "Unknown";
-		printk("%s  Error type: SLB [%s]\n", level, subtype);
 		if (evt->u.slb_error.effective_address_provided)
-			printk("%s    Effective address: %016llx\n",
-			       level, evt->u.slb_error.effective_address);
+			ea = evt->u.slb_error.effective_address;
 		break;
 	case MCE_ERROR_TYPE_ERAT:
+		err_type = "ERAT";
 		subtype = evt->u.erat_error.erat_error_type <
 			ARRAY_SIZE(mc_erat_types) ?
 			mc_erat_types[evt->u.erat_error.erat_error_type]
 			: "Unknown";
-		printk("%s  Error type: ERAT [%s]\n", level, subtype);
 		if (evt->u.erat_error.effective_address_provided)
-			printk("%s    Effective address: %016llx\n",
-			       level, evt->u.erat_error.effective_address);
+			ea = evt->u.erat_error.effective_address;
 		break;
 	case MCE_ERROR_TYPE_TLB:
+		err_type = "TLB";
 		subtype = evt->u.tlb_error.tlb_error_type <
 			ARRAY_SIZE(mc_tlb_types) ?
 			mc_tlb_types[evt->u.tlb_error.tlb_error_type]
 			: "Unknown";
-		printk("%s  Error type: TLB [%s]\n", level, subtype);
 		if (evt->u.tlb_error.effective_address_provided)
-			printk("%s    Effective address: %016llx\n",
-			       level, evt->u.tlb_error.effective_address);
+			ea = evt->u.tlb_error.effective_address;
 		break;
 	case MCE_ERROR_TYPE_USER:
+		err_type = "User";
 		subtype = evt->u.user_error.user_error_type <
 			ARRAY_SIZE(mc_user_types) ?
 			mc_user_types[evt->u.user_error.user_error_type]
 			: "Unknown";
-		printk("%s  Error type: User [%s]\n", level, subtype);
 		if (evt->u.user_error.effective_address_provided)
-			printk("%s    Effective address: %016llx\n",
-			       level, evt->u.user_error.effective_address);
+			ea = evt->u.user_error.effective_address;
 		break;
 	case MCE_ERROR_TYPE_RA:
+		err_type = "Real address";
 		subtype = evt->u.ra_error.ra_error_type <
 			ARRAY_SIZE(mc_ra_types) ?
 			mc_ra_types[evt->u.ra_error.ra_error_type]
 			: "Unknown";
-		printk("%s  Error type: Real address [%s]\n", level, subtype);
 		if (evt->u.ra_error.effective_address_provided)
-			printk("%s    Effective address: %016llx\n",
-			       level, evt->u.ra_error.effective_address);
+			ea = evt->u.ra_error.effective_address;
 		break;
 	case MCE_ERROR_TYPE_LINK:
+		err_type = "Link";
 		subtype = evt->u.link_error.link_error_type <
 			ARRAY_SIZE(mc_link_types) ?
 			mc_link_types[evt->u.link_error.link_error_type]
 			: "Unknown";
-		printk("%s  Error type: Link [%s]\n", level, subtype);
 		if (evt->u.link_error.effective_address_provided)
-			printk("%s    Effective address: %016llx\n",
-			       level, evt->u.link_error.effective_address);
+			ea = evt->u.link_error.effective_address;
 		break;
 	default:
 	case MCE_ERROR_TYPE_UNKNOWN:
-		printk("%s  Error type: Unknown\n", level);
+		err_type = "Unknown";
+		subtype = "";
 		break;
 	}
+
+	dar_str[0] = pa_str[0] = '\0';
+	if (ea && evt->srr0 != ea) {
+		/* Load/Store address */
+		n = sprintf(dar_str, "DAR: %016llx ", ea);
+		if (pa)
+			sprintf(dar_str + n, "paddr: %016llx ", pa);
+	} else if (pa) {
+		sprintf(pa_str, " paddr: %016llx", pa);
+	}
+
+	printk("%sMCE: CPU%d: machine check (%s) %s %s %s %s[%s]\n",
+		level, evt->cpu, sevstr, in_guest ? "Guest" : "Host",
+		err_type, subtype, dar_str,
+		evt->disposition == MCE_DISPOSITION_RECOVERED ?
+		"Recovered" : "Not recovered");
+
+	if (in_guest || user_mode) {
+		printk("%sMCE: CPU%d: PID: %d Comm: %s %sNIP: [%016llx]%s\n",
+			level, evt->cpu, current->pid, current->comm,
+			in_guest ? "Guest " : "", evt->srr0, pa_str);
+	} else {
+		printk("%sMCE: CPU%d: NIP: [%016llx] %pS%s\n",
+			level, evt->cpu, evt->srr0, (void *)evt->srr0, pa_str);
+	}
 }
 EXPORT_SYMBOL_GPL(machine_check_print_event_info);
 

commit ccd477028a202993b9ddca5d2404fdaca3b7a55c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Feb 26 18:51:07 2019 +1000

    powerpc/64s: Fix HV NMI vs HV interrupt recoverability test
    
    HV interrupts that use HSRR registers do not enter with MSR[RI] clear,
    but their entry code is not recoverable vs NMI, due to shared use of
    HSPRG1 as a scratch register to save r13.
    
    This means that a system reset or machine check that hits in HSRR
    interrupt entry can cause r13 to be silently corrupted.
    
    Fix this by marking NMIs non-recoverable if they land in HV interrupt
    ranges.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index d501b48f287e..b5fec1f9751a 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -31,6 +31,7 @@
 
 #include <asm/machdep.h>
 #include <asm/mce.h>
+#include <asm/nmi.h>
 
 static DEFINE_PER_CPU(int, mce_nest_count);
 static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT], mce_event);
@@ -490,6 +491,8 @@ long machine_check_early(struct pt_regs *regs)
 {
 	long handled = 0;
 
+	hv_nmi_check_nonrecoverable(regs);
+
 	/*
 	 * See if platform is capable of handling machine check.
 	 */

commit c05772018491e5294f55d63b239ab0d532e96616
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Feb 21 13:40:20 2019 +1100

    powerpc/64s: Better printing of machine check info for guest MCEs
    
    This adds an "in_guest" parameter to machine_check_print_event_info()
    so that we can avoid trying to translate guest NIP values into
    symbolic form using the host kernel's symbol table.
    
    Reviewed-by: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Reviewed-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index bd933a75f0bc..d501b48f287e 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -301,13 +301,13 @@ static void machine_check_process_queued_event(struct irq_work *work)
 	while (__this_cpu_read(mce_queue_count) > 0) {
 		index = __this_cpu_read(mce_queue_count) - 1;
 		evt = this_cpu_ptr(&mce_event_queue[index]);
-		machine_check_print_event_info(evt, false);
+		machine_check_print_event_info(evt, false, false);
 		__this_cpu_dec(mce_queue_count);
 	}
 }
 
 void machine_check_print_event_info(struct machine_check_event *evt,
-				    bool user_mode)
+				    bool user_mode, bool in_guest)
 {
 	const char *level, *sevstr, *subtype;
 	static const char *mc_ue_types[] = {
@@ -387,7 +387,9 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 	       evt->disposition == MCE_DISPOSITION_RECOVERED ?
 	       "Recovered" : "Not recovered");
 
-	if (user_mode) {
+	if (in_guest) {
+		printk("%s  Guest NIP: %016llx\n", level, evt->srr0);
+	} else if (user_mode) {
 		printk("%s  NIP: [%016llx] PID: %d Comm: %s\n", level,
 			evt->srr0, current->pid, current->comm);
 	} else {

commit a43c1590426c44a5c6bbaf51b70a36a5c6d86914
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Tue Sep 11 19:57:00 2018 +0530

    powerpc/pseries: Flush SLB contents on SLB MCE errors.
    
    On pseries, as of today system crashes if we get a machine check
    exceptions due to SLB errors. These are soft errors and can be fixed
    by flushing the SLBs so the kernel can continue to function instead of
    system crash. We do this in real mode before turning on MMU. Otherwise
    we would run into nested machine checks. This patch now fetches the
    rtas error log in real mode and flushes the SLBs on SLB/ERAT errors.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michal Suchanek <msuchanek@suse.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index efdd16a79075..bd933a75f0bc 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -488,10 +488,11 @@ long machine_check_early(struct pt_regs *regs)
 {
 	long handled = 0;
 
-	__this_cpu_inc(irq_stat.mce_exceptions);
-
-	if (cur_cpu_spec && cur_cpu_spec->machine_check_early)
-		handled = cur_cpu_spec->machine_check_early(regs);
+	/*
+	 * See if platform is capable of handling machine check.
+	 */
+	if (ppc_md.machine_check_early)
+		handled = ppc_md.machine_check_early(regs);
 	return handled;
 }
 

commit 03f51d4efa2287cc628bb20b0c032036d2a9e66a
Merge: 367b0df173b0 015eb1b89e95
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 2 10:01:04 2018 -0800

    Merge tag 'powerpc-4.16-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights:
    
       - Enable support for memory protection keys aka "pkeys" on Power7/8/9
         when using the hash table MMU.
    
       - Extend our interrupt soft masking to support masking PMU interrupts
         as well as "normal" interrupts, and then use that to implement
         local_t for a ~4x speedup vs the current atomics-based
         implementation.
    
       - A new driver "ocxl" for "Open Coherent Accelerator Processor
         Interface (OpenCAPI)" devices.
    
       - Support for new device tree properties on PowerVM to describe
         hotpluggable memory and devices.
    
       - Add support for CLOCK_{REALTIME/MONOTONIC}_COARSE to the 64-bit
         VDSO.
    
       - Freescale updates from Scott: fixes for CPM GPIO and an FSL PCI
         erratum workaround, plus a minor cleanup patch.
    
      As well as quite a lot of other changes all over the place, and small
      fixes and cleanups as always.
    
      Thanks to: Alan Modra, Alastair D'Silva, Alexey Kardashevskiy,
      Alistair Popple, Andreas Schwab, Andrew Donnellan, Aneesh Kumar K.V,
      Anju T Sudhakar, Anshuman Khandual, Anton Blanchard, Arnd Bergmann,
      Balbir Singh, Benjamin Herrenschmidt, Bhaktipriya Shridhar, Bryant G.
      Ly, Cédric Le Goater, Christophe Leroy, Christophe Lombard, Cyril Bur,
      David Gibson, Desnes A. Nunes do Rosario, Dmitry Torokhov, Frederic
      Barrat, Geert Uytterhoeven, Guilherme G. Piccoli, Gustavo A. R. Silva,
      Gustavo Romero, Ivan Mikhaylov, Joakim Tjernlund, Joe Perches, Josh
      Poimboeuf, Juan J. Alvarez, Julia Cartwright, Kamalesh Babulal,
      Madhavan Srinivasan, Mahesh Salgaonkar, Mathieu Malaterre, Michael
      Bringmann, Michael Hanselmann, Michael Neuling, Nathan Fontenot,
      Naveen N. Rao, Nicholas Piggin, Paul Mackerras, Philippe Bergheaud,
      Ram Pai, Russell Currey, Santosh Sivaraj, Scott Wood, Seth Forshee,
      Simon Guo, Stewart Smith, Sukadev Bhattiprolu, Thiago Jung Bauermann,
      Vaibhav Jain, Vasyl Gomonovych"
    
    * tag 'powerpc-4.16-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (199 commits)
      powerpc/mm/radix: Fix build error when RADIX_MMU=n
      macintosh/ams-input: Use true and false for boolean values
      macintosh: change some data types from int to bool
      powerpc/watchdog: Print the NIP in soft_nmi_interrupt()
      powerpc/watchdog: regs can't be null in soft_nmi_interrupt()
      powerpc/watchdog: Tweak watchdog printks
      powerpc/cell: Remove axonram driver
      rtc-opal: Fix handling of firmware error codes, prevent busy loops
      powerpc/mpc52xx_gpt: make use of raw_spinlock variants
      macintosh/adb: Properly mark continued kernel messages
      powerpc/pseries: Fix cpu hotplug crash with memoryless nodes
      powerpc/numa: Ensure nodes initialized for hotplug
      powerpc/numa: Use ibm,max-associativity-domains to discover possible nodes
      powerpc/kernel: Block interrupts when updating TIDR
      powerpc/powernv/idoa: Remove unnecessary pcidev from pci_dn
      powerpc/mm/nohash: do not flush the entire mm when range is a single page
      powerpc/pseries: Add Initialization of VF Bars
      powerpc/pseries/pci: Associate PEs to VFs in configure SR-IOV
      powerpc/eeh: Add EEH notify resume sysfs
      powerpc/eeh: Add EEH operations to notify resume
      ...

commit d4173023e63cb85ec02eda02d1789bf078719f00
Merge: 0aebc6a440b9 c0f45555b824
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 14:18:52 2018 -0800

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull siginfo cleanups from Eric Biederman:
     "Long ago when 2.4 was just a testing release copy_siginfo_to_user was
      made to copy individual fields to userspace, possibly for efficiency
      and to ensure initialized values were not copied to userspace.
    
      Unfortunately the design was complex, it's assumptions unstated, and
      humans are fallible and so while it worked much of the time that
      design failed to ensure unitialized memory is not copied to userspace.
    
      This set of changes is part of a new design to clean up siginfo and
      simplify things, and hopefully make the siginfo handling robust enough
      that a simple inspection of the code can be made to ensure we don't
      copy any unitializied fields to userspace.
    
      The design is to unify struct siginfo and struct compat_siginfo into a
      single definition that is shared between all architectures so that
      anyone adding to the set of information shared with struct siginfo can
      see the whole picture. Hopefully ensuring all future si_code
      assignments are arch independent.
    
      The design is to unify copy_siginfo_to_user32 and
      copy_siginfo_from_user32 so that those function are complete and cope
      with all of the different cases documented in signinfo_layout. I don't
      think there was a single implementation of either of those functions
      that was complete and correct before my changes unified them.
    
      The design is to introduce a series of helpers including
      force_siginfo_fault that take the values that are needed in struct
      siginfo and build the siginfo structure for their callers. Ensuring
      struct siginfo is built correctly.
    
      The remaining work for 4.17 (unless someone thinks it is post -rc1
      material) is to push usage of those helpers down into the
      architectures so that architecture specific code will not need to deal
      with the fiddly work of intializing struct siginfo, and then when
      struct siginfo is guaranteed to be fully initialized change copy
      siginfo_to_user into a simple wrapper around copy_to_user.
    
      Further there is work in progress on the issues that have been
      documented requires arch specific knowledge to sort out.
    
      The changes below fix or at least document all of the issues that have
      been found with siginfo generation. Then proceed to unify struct
      siginfo the 32 bit helpers that copy siginfo to and from userspace,
      and generally clean up anything that is not arch specific with regards
      to siginfo generation.
    
      It is a lot but with the unification you can of siginfo you can
      already see the code reduction in the kernel"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (45 commits)
      signal/memory-failure: Use force_sig_mceerr and send_sig_mceerr
      mm/memory_failure: Remove unused trapno from memory_failure
      signal/ptrace: Add force_sig_ptrace_errno_trap and use it where needed
      signal/powerpc: Remove unnecessary signal_code parameter of do_send_trap
      signal: Helpers for faults with specialized siginfo layouts
      signal: Add send_sig_fault and force_sig_fault
      signal: Replace memset(info,...) with clear_siginfo for clarity
      signal: Don't use structure initializers for struct siginfo
      signal/arm64: Better isolate the COMPAT_TASK portion of ptrace_hbptriggered
      ptrace: Use copy_siginfo in setsiginfo and getsiginfo
      signal: Unify and correct copy_siginfo_to_user32
      signal: Remove the code to clear siginfo before calling copy_siginfo_from_user32
      signal: Unify and correct copy_siginfo_from_user32
      signal/blackfin: Remove pointless UID16_SIGINFO_COMPAT_NEEDED
      signal/blackfin: Move the blackfin specific si_codes to asm-generic/siginfo.h
      signal/tile: Move the tile specific si_codes to asm-generic/siginfo.h
      signal/frv: Move the frv specific si_codes to asm-generic/siginfo.h
      signal/ia64: Move the ia64 specific si_codes to asm-generic/siginfo.h
      signal/powerpc: Remove redefinition of NSIGTRAP on powerpc
      signal: Move addr_lsb into the _sigfault union for clarity
      ...

commit d075745d893c78730e4a3b7a60fca23c2f764081
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Jan 17 20:51:13 2018 +1100

    KVM: PPC: Book3S HV: Improve handling of debug-trigger HMIs on POWER9
    
    Hypervisor maintenance interrupts (HMIs) are generated by various
    causes, signalled by bits in the hypervisor maintenance exception
    register (HMER).  In most cases calling OPAL to handle the interrupt
    is the correct thing to do, but the "debug trigger" HMIs signalled by
    PPC bit 17 (bit 46) of HMER are used to invoke software workarounds
    for hardware bugs, and OPAL does not have any code to handle this
    cause.  The debug trigger HMI is used in POWER9 DD2.0 and DD2.1 chips
    to work around a hardware bug in executing vector load instructions to
    cache inhibited memory.  In POWER9 DD2.2 chips, it is generated when
    conditions are detected relating to threads being in TM (transactional
    memory) suspended mode when the core SMT configuration needs to be
    reconfigured.
    
    The kernel currently has code to detect the vector CI load condition,
    but only when the HMI occurs in the host, not when it occurs in a
    guest.  If a HMI occurs in the guest, it is always passed to OPAL, and
    then we always re-sync the timebase, because the HMI cause might have
    been a timebase error, for which OPAL would re-sync the timebase, thus
    removing the timebase offset which KVM applied for the guest.  Since
    we don't know what OPAL did, we don't know whether to subtract the
    timebase offset from the timebase, so instead we re-sync the timebase.
    
    This adds code to determine explicitly what the cause of a debug
    trigger HMI will be.  This is based on a new device-tree property
    under the CPU nodes called ibm,hmi-special-triggers, if it is
    present, or otherwise based on the PVR (processor version register).
    The handling of debug trigger HMIs is pulled out into a separate
    function which can be called from the KVM guest exit code.  If this
    function handles and clears the HMI, and no other HMI causes remain,
    then we skip calling OPAL and we proceed to subtract the guest
    timebase offset from the timebase.
    
    The overall handling for HMIs that occur in the host (i.e. not in a
    KVM guest) is largely unchanged, except that we now don't set the flag
    for the vector CI load workaround on DD2.2 processors.
    
    This also removes a BUG_ON in the KVM code.  BUG_ON is generally not
    useful in KVM guest entry/exit code since it is difficult to handle
    the resulting trap gracefully.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 742e4658c5dc..d2fecaec4fec 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -495,37 +495,123 @@ long machine_check_early(struct pt_regs *regs)
 	return handled;
 }
 
-long hmi_exception_realmode(struct pt_regs *regs)
+/* Possible meanings for HMER_DEBUG_TRIG bit being set on POWER9 */
+static enum {
+	DTRIG_UNKNOWN,
+	DTRIG_VECTOR_CI,	/* need to emulate vector CI load instr */
+	DTRIG_SUSPEND_ESCAPE,	/* need to escape from TM suspend mode */
+} hmer_debug_trig_function;
+
+static int init_debug_trig_function(void)
 {
-	__this_cpu_inc(irq_stat.hmi_exceptions);
-
-#ifdef CONFIG_PPC_BOOK3S_64
-	/* Workaround for P9 vector CI loads (see p9_hmi_special_emu) */
-	if (pvr_version_is(PVR_POWER9)) {
-		unsigned long hmer = mfspr(SPRN_HMER);
-
-		/* Do we have the debug bit set */
-		if (hmer & PPC_BIT(17)) {
-			hmer &= ~PPC_BIT(17);
-			mtspr(SPRN_HMER, hmer);
-
-			/*
-			 * Now to avoid problems with soft-disable we
-			 * only do the emulation if we are coming from
-			 * user space
-			 */
-			if (user_mode(regs))
-				local_paca->hmi_p9_special_emu = 1;
-
-			/*
-			 * Don't bother going to OPAL if that's the
-			 * only relevant bit.
-			 */
-			if (!(hmer & mfspr(SPRN_HMEER)))
-				return local_paca->hmi_p9_special_emu;
+	int pvr;
+	struct device_node *cpun;
+	struct property *prop = NULL;
+	const char *str;
+
+	/* First look in the device tree */
+	preempt_disable();
+	cpun = of_get_cpu_node(smp_processor_id(), NULL);
+	if (cpun) {
+		of_property_for_each_string(cpun, "ibm,hmi-special-triggers",
+					    prop, str) {
+			if (strcmp(str, "bit17-vector-ci-load") == 0)
+				hmer_debug_trig_function = DTRIG_VECTOR_CI;
+			else if (strcmp(str, "bit17-tm-suspend-escape") == 0)
+				hmer_debug_trig_function = DTRIG_SUSPEND_ESCAPE;
 		}
+		of_node_put(cpun);
+	}
+	preempt_enable();
+
+	/* If we found the property, don't look at PVR */
+	if (prop)
+		goto out;
+
+	pvr = mfspr(SPRN_PVR);
+	/* Check for POWER9 Nimbus (scale-out) */
+	if ((PVR_VER(pvr) == PVR_POWER9) && (pvr & 0xe000) == 0) {
+		/* DD2.2 and later */
+		if ((pvr & 0xfff) >= 0x202)
+			hmer_debug_trig_function = DTRIG_SUSPEND_ESCAPE;
+		/* DD2.0 and DD2.1 - used for vector CI load emulation */
+		else if ((pvr & 0xfff) >= 0x200)
+			hmer_debug_trig_function = DTRIG_VECTOR_CI;
+	}
+
+ out:
+	switch (hmer_debug_trig_function) {
+	case DTRIG_VECTOR_CI:
+		pr_debug("HMI debug trigger used for vector CI load\n");
+		break;
+	case DTRIG_SUSPEND_ESCAPE:
+		pr_debug("HMI debug trigger used for TM suspend escape\n");
+		break;
+	default:
+		break;
 	}
-#endif /* CONFIG_PPC_BOOK3S_64 */
+	return 0;
+}
+__initcall(init_debug_trig_function);
+
+/*
+ * Handle HMIs that occur as a result of a debug trigger.
+ * Return values:
+ * -1 means this is not a HMI cause that we know about
+ *  0 means no further handling is required
+ *  1 means further handling is required
+ */
+long hmi_handle_debugtrig(struct pt_regs *regs)
+{
+	unsigned long hmer = mfspr(SPRN_HMER);
+	long ret = 0;
+
+	/* HMER_DEBUG_TRIG bit is used for various workarounds on P9 */
+	if (!((hmer & HMER_DEBUG_TRIG)
+	      && hmer_debug_trig_function != DTRIG_UNKNOWN))
+		return -1;
+		
+	hmer &= ~HMER_DEBUG_TRIG;
+	/* HMER is a write-AND register */
+	mtspr(SPRN_HMER, ~HMER_DEBUG_TRIG);
+
+	switch (hmer_debug_trig_function) {
+	case DTRIG_VECTOR_CI:
+		/*
+		 * Now to avoid problems with soft-disable we
+		 * only do the emulation if we are coming from
+		 * host user space
+		 */
+		if (regs && user_mode(regs))
+			ret = local_paca->hmi_p9_special_emu = 1;
+
+		break;
+
+	default:
+		break;
+	}
+
+	/*
+	 * See if any other HMI causes remain to be handled
+	 */
+	if (hmer & mfspr(SPRN_HMEER))
+		return -1;
+
+	return ret;
+}
+
+/*
+ * Return values:
+ */
+long hmi_exception_realmode(struct pt_regs *regs)
+{	
+	int ret;
+
+	__this_cpu_inc(irq_stat.hmi_exceptions);
+
+	ret = hmi_handle_debugtrig(regs);
+	if (ret >= 0)
+		return ret;
 
 	wait_for_subcore_guest_exit();
 

commit 733e4a4c4467033918f4e64f797b03efe209d82c
Author: Balbir Singh <bsingharora@gmail.com>
Date:   Fri Sep 29 14:26:55 2017 +1000

    powerpc/mce: hookup memory_failure for UE errors
    
    If we are in user space and hit a UE error, we now have the
    basic infrastructure to walk the page tables and find out
    the effective address that was accessed, since the DAR
    is not valid.
    
    We use a work_queue content to hookup the bad pfn, any
    other context causes problems, since memory_failure itself
    can call into schedule() via lru_drain_ bits.
    
    We could probably poison the struct page to avoid a race
    between detection and taking corrective action.
    
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 1af7c559edbc..742e4658c5dc 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -39,11 +39,21 @@ static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT], mce_event);
 static DEFINE_PER_CPU(int, mce_queue_count);
 static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT], mce_event_queue);
 
+/* Queue for delayed MCE UE events. */
+static DEFINE_PER_CPU(int, mce_ue_count);
+static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT],
+					mce_ue_event_queue);
+
 static void machine_check_process_queued_event(struct irq_work *work);
+void machine_check_ue_event(struct machine_check_event *evt);
+static void machine_process_ue_event(struct work_struct *work);
+
 static struct irq_work mce_event_process_work = {
         .func = machine_check_process_queued_event,
 };
 
+DECLARE_WORK(mce_ue_event_work, machine_process_ue_event);
+
 static void mce_set_error_info(struct machine_check_event *mce,
 			       struct mce_error_info *mce_err)
 {
@@ -143,6 +153,7 @@ void save_mce_event(struct pt_regs *regs, long handled,
 		if (phys_addr != ULONG_MAX) {
 			mce->u.ue_error.physical_address_provided = true;
 			mce->u.ue_error.physical_address = phys_addr;
+			machine_check_ue_event(mce);
 		}
 	}
 	return;
@@ -197,6 +208,26 @@ void release_mce_event(void)
 	get_mce_event(NULL, true);
 }
 
+
+/*
+ * Queue up the MCE event which then can be handled later.
+ */
+void machine_check_ue_event(struct machine_check_event *evt)
+{
+	int index;
+
+	index = __this_cpu_inc_return(mce_ue_count) - 1;
+	/* If queue is full, just return for now. */
+	if (index >= MAX_MC_EVT) {
+		__this_cpu_dec(mce_ue_count);
+		return;
+	}
+	memcpy(this_cpu_ptr(&mce_ue_event_queue[index]), evt, sizeof(*evt));
+
+	/* Queue work to process this event later. */
+	schedule_work(&mce_ue_event_work);
+}
+
 /*
  * Queue up the MCE event which then can be handled later.
  */
@@ -219,7 +250,39 @@ void machine_check_queue_event(void)
 	/* Queue irq work to process this event later. */
 	irq_work_queue(&mce_event_process_work);
 }
-
+/*
+ * process pending MCE event from the mce event queue. This function will be
+ * called during syscall exit.
+ */
+static void machine_process_ue_event(struct work_struct *work)
+{
+	int index;
+	struct machine_check_event *evt;
+
+	while (__this_cpu_read(mce_ue_count) > 0) {
+		index = __this_cpu_read(mce_ue_count) - 1;
+		evt = this_cpu_ptr(&mce_ue_event_queue[index]);
+#ifdef CONFIG_MEMORY_FAILURE
+		/*
+		 * This should probably queued elsewhere, but
+		 * oh! well
+		 */
+		if (evt->error_type == MCE_ERROR_TYPE_UE) {
+			if (evt->u.ue_error.physical_address_provided) {
+				unsigned long pfn;
+
+				pfn = evt->u.ue_error.physical_address >>
+					PAGE_SHIFT;
+				memory_failure(pfn, SIGBUS, 0);
+			} else
+				pr_warn("Failed to identify bad address from "
+					"where the uncorrectable error (UE) "
+					"was generated\n");
+		}
+#endif
+		__this_cpu_dec(mce_ue_count);
+	}
+}
 /*
  * process pending MCE event from the mce event queue. This function will be
  * called during syscall exit.
@@ -227,6 +290,7 @@ void machine_check_queue_event(void)
 static void machine_check_process_queued_event(struct irq_work *work)
 {
 	int index;
+	struct machine_check_event *evt;
 
 	add_taint(TAINT_MACHINE_CHECK, LOCKDEP_NOW_UNRELIABLE);
 
@@ -236,8 +300,8 @@ static void machine_check_process_queued_event(struct irq_work *work)
 	 */
 	while (__this_cpu_read(mce_queue_count) > 0) {
 		index = __this_cpu_read(mce_queue_count) - 1;
-		machine_check_print_event_info(
-				this_cpu_ptr(&mce_event_queue[index]), false);
+		evt = this_cpu_ptr(&mce_event_queue[index]);
+		machine_check_print_event_info(evt, false);
 		__this_cpu_dec(mce_queue_count);
 	}
 }

commit ba41e1e1ccb9771ce41a3b8e2121f95486e76ac9
Author: Balbir Singh <bsingharora@gmail.com>
Date:   Fri Sep 29 14:26:53 2017 +1000

    powerpc/mce: Hookup derror (load/store) UE errors
    
    Extract physical_address for UE errors by walking the page
    tables for the mm and address at the NIP, to extract the
    instruction. Then use the instruction to find the effective
    address via analyse_instr().
    
    We might have page table walking races, but we expect them to
    be rare, the physical address extraction is best effort. The idea
    is to then hook up this infrastructure to memory failure eventually.
    
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index d1775bdade72..1af7c559edbc 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -82,7 +82,7 @@ static void mce_set_error_info(struct machine_check_event *mce,
  */
 void save_mce_event(struct pt_regs *regs, long handled,
 		    struct mce_error_info *mce_err,
-		    uint64_t nip, uint64_t addr)
+		    uint64_t nip, uint64_t addr, uint64_t phys_addr)
 {
 	int index = __this_cpu_inc_return(mce_nest_count) - 1;
 	struct machine_check_event *mce = this_cpu_ptr(&mce_event[index]);
@@ -140,6 +140,10 @@ void save_mce_event(struct pt_regs *regs, long handled,
 	} else if (mce->error_type == MCE_ERROR_TYPE_UE) {
 		mce->u.ue_error.effective_address_provided = true;
 		mce->u.ue_error.effective_address = addr;
+		if (phys_addr != ULONG_MAX) {
+			mce->u.ue_error.physical_address_provided = true;
+			mce->u.ue_error.physical_address = phys_addr;
+		}
 	}
 	return;
 }

commit 81b61fa7a065e639d097b09c303e3212e4264fac
Author: Balbir Singh <bsingharora@gmail.com>
Date:   Fri Sep 29 14:26:52 2017 +1000

    powerpc/mce: Align the print of physical address better
    
    Use the same alignment as Effective address.
    
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 6757720b1979..d1775bdade72 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -340,7 +340,7 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 			printk("%s    Effective address: %016llx\n",
 			       level, evt->u.ue_error.effective_address);
 		if (evt->u.ue_error.physical_address_provided)
-			printk("%s      Physical address: %016llx\n",
+			printk("%s    Physical address:  %016llx\n",
 			       level, evt->u.ue_error.physical_address);
 		break;
 	case MCE_ERROR_TYPE_SLB:

commit 73e341eb6bea01fde706d10d7edba97f154d9c5f
Author: Balbir Singh <bsingharora@gmail.com>
Date:   Fri Sep 29 14:26:51 2017 +1000

    powerpc/mce: Remove unused function get_mce_fault_addr()
    
    There are no users of get_mce_fault_addr() since commit
    1363875bdb63 ("powerpc/64s: fix handling of non-synchronous machine
    checks") removed the last usage.
    
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index f588951b171d..6757720b1979 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -411,45 +411,6 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 }
 EXPORT_SYMBOL_GPL(machine_check_print_event_info);
 
-uint64_t get_mce_fault_addr(struct machine_check_event *evt)
-{
-	switch (evt->error_type) {
-	case MCE_ERROR_TYPE_UE:
-		if (evt->u.ue_error.effective_address_provided)
-			return evt->u.ue_error.effective_address;
-		break;
-	case MCE_ERROR_TYPE_SLB:
-		if (evt->u.slb_error.effective_address_provided)
-			return evt->u.slb_error.effective_address;
-		break;
-	case MCE_ERROR_TYPE_ERAT:
-		if (evt->u.erat_error.effective_address_provided)
-			return evt->u.erat_error.effective_address;
-		break;
-	case MCE_ERROR_TYPE_TLB:
-		if (evt->u.tlb_error.effective_address_provided)
-			return evt->u.tlb_error.effective_address;
-		break;
-	case MCE_ERROR_TYPE_USER:
-		if (evt->u.user_error.effective_address_provided)
-			return evt->u.user_error.effective_address;
-		break;
-	case MCE_ERROR_TYPE_RA:
-		if (evt->u.ra_error.effective_address_provided)
-			return evt->u.ra_error.effective_address;
-		break;
-	case MCE_ERROR_TYPE_LINK:
-		if (evt->u.link_error.effective_address_provided)
-			return evt->u.link_error.effective_address;
-		break;
-	default:
-	case MCE_ERROR_TYPE_UNKNOWN:
-		break;
-	}
-	return 0;
-}
-EXPORT_SYMBOL(get_mce_fault_addr);
-
 /*
  * This function is called in real mode. Strictly no printk's please.
  *

commit 5080332c2c893118dbc18755f35c8b0131cf0fc4
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Sep 15 15:25:48 2017 +1000

    powerpc/64s: Add workaround for P9 vector CI load issue
    
    POWER9 DD2.1 and earlier has an issue where some cache inhibited
    vector load will return bad data. The workaround is two part, one
    firmware/microcode part triggers HMI interrupts when hitting such
    loads, the other part is this patch which then emulates the
    instructions in Linux.
    
    The affected instructions are limited to lxvd2x, lxvw4x, lxvb16x and
    lxvh8x.
    
    When an instruction triggers the HMI, all threads in the core will be
    sent to the HMI handler, not just the one running the vector load.
    
    In general, these spurious HMIs are detected by the emulation code and
    we just return back to the running process. Unfortunately, if a
    spurious interrupt occurs on a vector load that's to normal memory we
    have no way to detect that it's spurious (unless we walk the page
    tables, which is very expensive). In this case we emulate the load but
    we need do so using a vector load itself to ensure 128bit atomicity is
    preserved.
    
    Some additional debugfs emulated instruction counters are added also.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [mpe: Switch CONFIG_PPC_BOOK3S_64 to CONFIG_VSX to unbreak the build]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 9b2ea7e71c06..f588951b171d 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -470,6 +470,34 @@ long hmi_exception_realmode(struct pt_regs *regs)
 {
 	__this_cpu_inc(irq_stat.hmi_exceptions);
 
+#ifdef CONFIG_PPC_BOOK3S_64
+	/* Workaround for P9 vector CI loads (see p9_hmi_special_emu) */
+	if (pvr_version_is(PVR_POWER9)) {
+		unsigned long hmer = mfspr(SPRN_HMER);
+
+		/* Do we have the debug bit set */
+		if (hmer & PPC_BIT(17)) {
+			hmer &= ~PPC_BIT(17);
+			mtspr(SPRN_HMER, hmer);
+
+			/*
+			 * Now to avoid problems with soft-disable we
+			 * only do the emulation if we are coming from
+			 * user space
+			 */
+			if (user_mode(regs))
+				local_paca->hmi_p9_special_emu = 1;
+
+			/*
+			 * Don't bother going to OPAL if that's the
+			 * only relevant bit.
+			 */
+			if (!(hmer & mfspr(SPRN_HMEER)))
+				return local_paca->hmi_p9_special_emu;
+		}
+	}
+#endif /* CONFIG_PPC_BOOK3S_64 */
+
 	wait_for_subcore_guest_exit();
 
 	if (ppc_md.hmi_exception_early)
@@ -477,5 +505,5 @@ long hmi_exception_realmode(struct pt_regs *regs)
 
 	wait_for_tb_resync();
 
-	return 0;
+	return 1;
 }

commit ccd3cd361341b71ae2afa596f6b470fcb32a916e
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Aug 8 16:39:24 2017 +1000

    powerpc/mce: Move 64-bit machine check code into mce.c
    
    We already have mce.c which is built for 64bit and contains other parts
    of the machine check code, so move these bits in there too.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index e0e131e662ed..9b2ea7e71c06 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -22,11 +22,14 @@
 #undef DEBUG
 #define pr_fmt(fmt) "mce: " fmt
 
+#include <linux/hardirq.h>
 #include <linux/types.h>
 #include <linux/ptrace.h>
 #include <linux/percpu.h>
 #include <linux/export.h>
 #include <linux/irq_work.h>
+
+#include <asm/machdep.h>
 #include <asm/mce.h>
 
 static DEFINE_PER_CPU(int, mce_nest_count);
@@ -446,3 +449,33 @@ uint64_t get_mce_fault_addr(struct machine_check_event *evt)
 	return 0;
 }
 EXPORT_SYMBOL(get_mce_fault_addr);
+
+/*
+ * This function is called in real mode. Strictly no printk's please.
+ *
+ * regs->nip and regs->msr contains srr0 and ssr1.
+ */
+long machine_check_early(struct pt_regs *regs)
+{
+	long handled = 0;
+
+	__this_cpu_inc(irq_stat.mce_exceptions);
+
+	if (cur_cpu_spec && cur_cpu_spec->machine_check_early)
+		handled = cur_cpu_spec->machine_check_early(regs);
+	return handled;
+}
+
+long hmi_exception_realmode(struct pt_regs *regs)
+{
+	__this_cpu_inc(irq_stat.hmi_exceptions);
+
+	wait_for_subcore_guest_exit();
+
+	if (ppc_md.hmi_exception_early)
+		ppc_md.hmi_exception_early(regs);
+
+	wait_for_tb_resync();
+
+	return 0;
+}

commit d691b7e7d1b5186eae62fd32adee65d3316bfdf6
Merge: b59eea554f57 1e0fc9d1eb2b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 7 13:55:45 2017 -0700

    Merge tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for STRICT_KERNEL_RWX on 64-bit server CPUs.
    
       - Platform support for FSP2 (476fpe) board
    
       - Enable ZONE_DEVICE on 64-bit server CPUs.
    
       - Generic & powerpc spin loop primitives to optimise busy waiting
    
       - Convert VDSO update function to use new update_vsyscall() interface
    
       - Optimisations to hypercall/syscall/context-switch paths
    
       - Improvements to the CPU idle code on Power8 and Power9.
    
      As well as many other fixes and improvements.
    
      Thanks to: Akshay Adiga, Andrew Donnellan, Andrew Jeffery, Anshuman
      Khandual, Anton Blanchard, Balbir Singh, Benjamin Herrenschmidt,
      Christophe Leroy, Christophe Lombard, Colin Ian King, Dan Carpenter,
      Gautham R. Shenoy, Hari Bathini, Ian Munsie, Ivan Mikhaylov, Javier
      Martinez Canillas, Madhavan Srinivasan, Masahiro Yamada, Matt Brown,
      Michael Neuling, Michal Suchanek, Murilo Opsfelder Araujo, Naveen N.
      Rao, Nicholas Piggin, Oliver O'Halloran, Paul Mackerras, Pavel Machek,
      Russell Currey, Santosh Sivaraj, Stephen Rothwell, Thiago Jung
      Bauermann, Yang Li"
    
    * tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (158 commits)
      powerpc/Kconfig: Enable STRICT_KERNEL_RWX for some configs
      powerpc/mm/radix: Implement STRICT_RWX/mark_rodata_ro() for Radix
      powerpc/mm/hash: Implement mark_rodata_ro() for hash
      powerpc/vmlinux.lds: Align __init_begin to 16M
      powerpc/lib/code-patching: Use alternate map for patch_instruction()
      powerpc/xmon: Add patch_instruction() support for xmon
      powerpc/kprobes/optprobes: Use patch_instruction()
      powerpc/kprobes: Move kprobes over to patch_instruction()
      powerpc/mm/radix: Fix execute permissions for interrupt_vectors
      powerpc/pseries: Fix passing of pp0 in updatepp() and updateboltedpp()
      powerpc/64s: Blacklist rtas entry/exit from kprobes
      powerpc/64s: Blacklist functions invoked on a trap
      powerpc/64s: Un-blacklist system_call() from kprobes
      powerpc/64s: Move system_call() symbol to just after setting MSR_EE
      powerpc/64s: Blacklist system_call() and system_call_common() from kprobes
      powerpc/64s: Convert .L__replay_interrupt_return to a local label
      powerpc64/elfv1: Only dereference function descriptor for non-text symbols
      cxl: Export library to support IBM XSL
      powerpc/dts: Use #include "..." to include local DT
      powerpc/perf/hv-24x7: Aggregate result elements on POWER9 SMT8
      ...

commit 8aa586c6880442c7b288bea28d6eba039411915a
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Thu May 11 16:33:12 2017 +0530

    powerpc/book3s: EXPORT_SYMBOL_GPL machine_check_print_event_info
    
    It will be used in arch/powerpc/kvm/book3s_hv.c KVM module.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 5f9eada3519b..a9bfa49f3698 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -405,6 +405,7 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 		break;
 	}
 }
+EXPORT_SYMBOL_GPL(machine_check_print_event_info);
 
 uint64_t get_mce_fault_addr(struct machine_check_event *evt)
 {

commit 90df4bfb4d9e00a1ab6885900b808bef2b62a21c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Mon May 29 16:26:44 2017 +1000

    powerpc/64s: Machine check handle ifetch from foreign real address for POWER9
    
    The i-side 0111b machine check, which is "Instruction Fetch to foreign
    address space", was missed by 7b9f71f974 ("powerpc/64s: POWER9 machine
    check handler").
    
        The POWER9 processor core considers host real addresses with a
        nonzero value in RA(8:12) as foreign address space, accessible only
        by the copy and paste instructions. The copy and paste instruction
        pair can be used to invoke the Nest accelerators via the Virtual
        Accelerator Switchboard (VAS).
    
    It is an error for any regular load/store or ifetch to go to a foreign
    addresses. When relocation is on, this causes an MMU exception. When
    relocation is off, a machine check exception. It is possible to trigger
    this machine check by branching to a foreign address with MSR[IR]=0.
    
    Fixes: 7b9f71f974a1 ("powerpc/64s: POWER9 machine check handler")
    Reported-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 5f9eada3519b..92f185875694 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -268,6 +268,7 @@ void machine_check_print_event_info(struct machine_check_event *evt,
 	static const char *mc_ra_types[] = {
 		"Indeterminate",
 		"Instruction fetch (bad)",
+		"Instruction fetch (foreign)",
 		"Page table walk ifetch (bad)",
 		"Page table walk ifetch (foreign)",
 		"Load (bad)",

commit d93b0ac01a9ce276ec39644be47001873d3d183c
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Tue Apr 18 22:08:17 2017 +0530

    powerpc/book3s/mce: Move add_taint() later in virtual mode
    
    machine_check_early() gets called in real mode. The very first time when
    add_taint() is called, it prints a warning which ends up calling opal
    call (that uses OPAL_CALL wrapper) for writing it to console. If we get a
    very first machine check while we are in opal we are doomed. OPAL_CALL
    overwrites the PACASAVEDMSR in r13 and in this case when we are done with
    MCE handling the original opal call will use this new MSR on it's way
    back to opal_return. This usually leads to unexpected behaviour or the
    kernel to panic. Instead move the add_taint() call later in the virtual
    mode where it is safe to call.
    
    This is broken with current FW level. We got lucky so far for not getting
    very first MCE hit while in OPAL. But easily reproducible on Mambo.
    
    Fixes: 27ea2c420cad ("powerpc: Set the correct kernel taint on machine check errors.")
    Cc: stable@vger.kernel.org # v4.2+
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 16eb0b508761..5f9eada3519b 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -221,6 +221,8 @@ static void machine_check_process_queued_event(struct irq_work *work)
 {
 	int index;
 
+	add_taint(TAINT_MACHINE_CHECK, LOCKDEP_NOW_UNRELIABLE);
+
 	/*
 	 * For now just print it to console.
 	 * TODO: log this error event to FSP or nvram.

commit 63f44d65143976af9b28cf70ebe23f2c8b38677a
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Apr 3 15:29:34 2017 +1000

    powerpc/book3s: Print task info if we take a machine check in user mode
    
    For an MCE (Machine Check Exception) that hits while in user mode
    MSR(PR=1), print the task info to the console MCE error log. This may
    help to identify an application that triggered the MCE.
    
    After this patch the MCE console looks like:
    
      Severe Machine check interrupt [Recovered]
        NIP: [0000000010039778] PID: 762 Comm: ebizzy
        Initiator: CPU
        Error type: SLB [Multihit]
          Effective address: 0000000010039778
    
      Severe Machine check interrupt [Not recovered]
        NIP: [0000000010039778] PID: 763 Comm: ebizzy
        Initiator: CPU
        Error type: UE [Page table walk ifetch]
          Effective address: 0000000010039778
      ebizzy[763]: unhandled signal 7 at 0000000010039778 nip 0000000010039778 lr 0000000010001b44 code 30004
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index b960f00f4908..16eb0b508761 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -228,12 +228,13 @@ static void machine_check_process_queued_event(struct irq_work *work)
 	while (__this_cpu_read(mce_queue_count) > 0) {
 		index = __this_cpu_read(mce_queue_count) - 1;
 		machine_check_print_event_info(
-				this_cpu_ptr(&mce_event_queue[index]));
+				this_cpu_ptr(&mce_event_queue[index]), false);
 		__this_cpu_dec(mce_queue_count);
 	}
 }
 
-void machine_check_print_event_info(struct machine_check_event *evt)
+void machine_check_print_event_info(struct machine_check_event *evt,
+				    bool user_mode)
 {
 	const char *level, *sevstr, *subtype;
 	static const char *mc_ue_types[] = {
@@ -311,7 +312,15 @@ void machine_check_print_event_info(struct machine_check_event *evt)
 	printk("%s%s Machine check interrupt [%s]\n", level, sevstr,
 	       evt->disposition == MCE_DISPOSITION_RECOVERED ?
 	       "Recovered" : "Not recovered");
-	printk("%s  NIP [%016llx]: %pS\n", level, evt->srr0, (void *)evt->srr0);
+
+	if (user_mode) {
+		printk("%s  NIP: [%016llx] PID: %d Comm: %s\n", level,
+			evt->srr0, current->pid, current->comm);
+	} else {
+		printk("%s  NIP [%016llx]: %pS\n", level, evt->srr0,
+		       (void *)evt->srr0);
+	}
+
 	printk("%s  Initiator: %s\n", level,
 	       evt->initiator == MCE_INITIATOR_CPU ? "CPU" : "Unknown");
 	switch (evt->error_type) {

commit 5b1d6fc2d4d927852214f2a7e2a8eb1bdce3b574
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Tue Mar 28 19:15:04 2017 +0530

    powerpc/book3s: Print the kernel function name in machine check
    
    For D-side errors we print the load/store address that caused the
    machine check as 'Effective address'. But the instruction that may have
    caused the machine check can also be helpful, so in addition to printing
    the NIP, also print the kernel function name as well.
    
    After this patch the MCE console log would look like:
    
      Severe Machine check interrupt [Recovered]
        NIP [d00000001bc70194]: init_module+0x194/0x2b0 [bork_kernel]
        Initiator: CPU
        Error type: SLB [Parity]
          Effective address: d000000026de0000
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 399aeafb7dd4..b960f00f4908 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -311,7 +311,7 @@ void machine_check_print_event_info(struct machine_check_event *evt)
 	printk("%s%s Machine check interrupt [%s]\n", level, sevstr,
 	       evt->disposition == MCE_DISPOSITION_RECOVERED ?
 	       "Recovered" : "Not recovered");
-	printk("%s  NIP: %016llx\n", level, evt->srr0);
+	printk("%s  NIP [%016llx]: %pS\n", level, evt->srr0, (void *)evt->srr0);
 	printk("%s  Initiator: %s\n", level,
 	       evt->initiator == MCE_INITIATOR_CPU ? "CPU" : "Unknown");
 	switch (evt->error_type) {

commit fc84427b7e1471b0a1220c56072f574618335edc
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Mar 14 22:36:43 2017 +1000

    powerpc/64s: Machine check print NIP
    
    Print the faulting address of the machine check that may help with
    debugging. The effective address reported can be a target memory address
    rather than the faulting instruction address.
    
    Fix up a dangling bracket while here.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index a1475e6aef3a..399aeafb7dd4 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -310,7 +310,8 @@ void machine_check_print_event_info(struct machine_check_event *evt)
 
 	printk("%s%s Machine check interrupt [%s]\n", level, sevstr,
 	       evt->disposition == MCE_DISPOSITION_RECOVERED ?
-	       "Recovered" : "[Not recovered");
+	       "Recovered" : "Not recovered");
+	printk("%s  NIP: %016llx\n", level, evt->srr0);
 	printk("%s  Initiator: %s\n", level,
 	       evt->initiator == MCE_INITIATOR_CPU ? "CPU" : "Unknown");
 	switch (evt->error_type) {

commit 7b9f71f974a12740e79e918cfd58c2fce0b5b580
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Feb 28 12:00:48 2017 +1000

    powerpc/64s: POWER9 machine check handler
    
    Add POWER9 machine check handler. There are several new types of errors
    added, so logging messages for those are also added.
    
    This doesn't attempt to reuse any of the P7/8 defines or functions,
    because that becomes too complex. The better option in future is to use
    a table driven approach.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 949507277436..a1475e6aef3a 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -58,6 +58,15 @@ static void mce_set_error_info(struct machine_check_event *mce,
 	case MCE_ERROR_TYPE_TLB:
 		mce->u.tlb_error.tlb_error_type = mce_err->u.tlb_error_type;
 		break;
+	case MCE_ERROR_TYPE_USER:
+		mce->u.user_error.user_error_type = mce_err->u.user_error_type;
+		break;
+	case MCE_ERROR_TYPE_RA:
+		mce->u.ra_error.ra_error_type = mce_err->u.ra_error_type;
+		break;
+	case MCE_ERROR_TYPE_LINK:
+		mce->u.link_error.link_error_type = mce_err->u.link_error_type;
+		break;
 	case MCE_ERROR_TYPE_UNKNOWN:
 	default:
 		break;
@@ -116,6 +125,15 @@ void save_mce_event(struct pt_regs *regs, long handled,
 	} else if (mce->error_type == MCE_ERROR_TYPE_ERAT) {
 		mce->u.erat_error.effective_address_provided = true;
 		mce->u.erat_error.effective_address = addr;
+	} else if (mce->error_type == MCE_ERROR_TYPE_USER) {
+		mce->u.user_error.effective_address_provided = true;
+		mce->u.user_error.effective_address = addr;
+	} else if (mce->error_type == MCE_ERROR_TYPE_RA) {
+		mce->u.ra_error.effective_address_provided = true;
+		mce->u.ra_error.effective_address = addr;
+	} else if (mce->error_type == MCE_ERROR_TYPE_LINK) {
+		mce->u.link_error.effective_address_provided = true;
+		mce->u.link_error.effective_address = addr;
 	} else if (mce->error_type == MCE_ERROR_TYPE_UE) {
 		mce->u.ue_error.effective_address_provided = true;
 		mce->u.ue_error.effective_address = addr;
@@ -240,6 +258,29 @@ void machine_check_print_event_info(struct machine_check_event *evt)
 		"Parity",
 		"Multihit",
 	};
+	static const char *mc_user_types[] = {
+		"Indeterminate",
+		"tlbie(l) invalid",
+	};
+	static const char *mc_ra_types[] = {
+		"Indeterminate",
+		"Instruction fetch (bad)",
+		"Page table walk ifetch (bad)",
+		"Page table walk ifetch (foreign)",
+		"Load (bad)",
+		"Store (bad)",
+		"Page table walk Load/Store (bad)",
+		"Page table walk Load/Store (foreign)",
+		"Load/Store (foreign)",
+	};
+	static const char *mc_link_types[] = {
+		"Indeterminate",
+		"Instruction fetch (timeout)",
+		"Page table walk ifetch (timeout)",
+		"Load (timeout)",
+		"Store (timeout)",
+		"Page table walk Load/Store (timeout)",
+	};
 
 	/* Print things out */
 	if (evt->version != MCE_V1) {
@@ -316,6 +357,36 @@ void machine_check_print_event_info(struct machine_check_event *evt)
 			printk("%s    Effective address: %016llx\n",
 			       level, evt->u.tlb_error.effective_address);
 		break;
+	case MCE_ERROR_TYPE_USER:
+		subtype = evt->u.user_error.user_error_type <
+			ARRAY_SIZE(mc_user_types) ?
+			mc_user_types[evt->u.user_error.user_error_type]
+			: "Unknown";
+		printk("%s  Error type: User [%s]\n", level, subtype);
+		if (evt->u.user_error.effective_address_provided)
+			printk("%s    Effective address: %016llx\n",
+			       level, evt->u.user_error.effective_address);
+		break;
+	case MCE_ERROR_TYPE_RA:
+		subtype = evt->u.ra_error.ra_error_type <
+			ARRAY_SIZE(mc_ra_types) ?
+			mc_ra_types[evt->u.ra_error.ra_error_type]
+			: "Unknown";
+		printk("%s  Error type: Real address [%s]\n", level, subtype);
+		if (evt->u.ra_error.effective_address_provided)
+			printk("%s    Effective address: %016llx\n",
+			       level, evt->u.ra_error.effective_address);
+		break;
+	case MCE_ERROR_TYPE_LINK:
+		subtype = evt->u.link_error.link_error_type <
+			ARRAY_SIZE(mc_link_types) ?
+			mc_link_types[evt->u.link_error.link_error_type]
+			: "Unknown";
+		printk("%s  Error type: Link [%s]\n", level, subtype);
+		if (evt->u.link_error.effective_address_provided)
+			printk("%s    Effective address: %016llx\n",
+			       level, evt->u.link_error.effective_address);
+		break;
 	default:
 	case MCE_ERROR_TYPE_UNKNOWN:
 		printk("%s  Error type: Unknown\n", level);
@@ -342,6 +413,18 @@ uint64_t get_mce_fault_addr(struct machine_check_event *evt)
 		if (evt->u.tlb_error.effective_address_provided)
 			return evt->u.tlb_error.effective_address;
 		break;
+	case MCE_ERROR_TYPE_USER:
+		if (evt->u.user_error.effective_address_provided)
+			return evt->u.user_error.effective_address;
+		break;
+	case MCE_ERROR_TYPE_RA:
+		if (evt->u.ra_error.effective_address_provided)
+			return evt->u.ra_error.effective_address;
+		break;
+	case MCE_ERROR_TYPE_LINK:
+		if (evt->u.link_error.effective_address_provided)
+			return evt->u.link_error.effective_address;
+		break;
 	default:
 	case MCE_ERROR_TYPE_UNKNOWN:
 		break;

commit c1bbf387d6191e6e18f3adc4db45b922822c2ba4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Feb 28 12:00:47 2017 +1000

    powerpc/64s: allow machine check handler to set severity and initiator
    
    Currently severity and initiator are always set to MCE_SEV_ERROR_SYNC and
    MCE_INITIATOR_CPU in the core mce code. Allow them to be set by the
    machine specific mce handlers.
    
    No functional change for existing handlers.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index c6923ff45131..949507277436 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -90,13 +90,14 @@ void save_mce_event(struct pt_regs *regs, long handled,
 	mce->gpr3 = regs->gpr[3];
 	mce->in_use = 1;
 
-	mce->initiator = MCE_INITIATOR_CPU;
 	/* Mark it recovered if we have handled it and MSR(RI=1). */
 	if (handled && (regs->msr & MSR_RI))
 		mce->disposition = MCE_DISPOSITION_RECOVERED;
 	else
 		mce->disposition = MCE_DISPOSITION_NOT_RECOVERED;
-	mce->severity = MCE_SEV_ERROR_SYNC;
+
+	mce->initiator = mce_err->initiator;
+	mce->severity = mce_err->severity;
 
 	/*
 	 * Populate the mce error_type and type-specific error_type.

commit d6d56ec738dcac4ac7716ccb7a08a4637de47955
Author: Tobias Klauser <tklauser@distanz.ch>
Date:   Thu Nov 17 17:19:21 2016 +0100

    powerpc/mce: Remove unused but set variable
    
    Remove the unused but set variable srr1 in save_mce_event() to
    fix the following GCC warning when building with 'W=1':
    
      arch/powerpc/kernel/mce.c:75:11: warning: variable 'srr1' set but not used
    
    It has never been used.
    
    Signed-off-by: Tobias Klauser <tklauser@distanz.ch>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 5e7ece0fda9f..c6923ff45131 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -72,7 +72,6 @@ void save_mce_event(struct pt_regs *regs, long handled,
 		    struct mce_error_info *mce_err,
 		    uint64_t nip, uint64_t addr)
 {
-	uint64_t srr1;
 	int index = __this_cpu_inc_return(mce_nest_count) - 1;
 	struct machine_check_event *mce = this_cpu_ptr(&mce_event[index]);
 
@@ -99,8 +98,6 @@ void save_mce_event(struct pt_regs *regs, long handled,
 		mce->disposition = MCE_DISPOSITION_NOT_RECOVERED;
 	mce->severity = MCE_SEV_ERROR_SYNC;
 
-	srr1 = regs->msr;
-
 	/*
 	 * Populate the mce error_type and type-specific error_type.
 	 */

commit c74dd88e77d3ecbc9e55c78796d82c9aa21cabad
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Tue Aug 9 10:39:13 2016 +0530

    powerpc/book3s: Fix MCE console messages for unrecoverable MCE.
    
    When machine check occurs with MSR(RI=0), it means MC interrupt is
    unrecoverable and kernel goes down to panic path. But the console
    message still shows it as recovered. This patch fixes the MCE console
    messages.
    
    Fixes: 36df96f8acaf ("powerpc/book3s: Decode and save machine check event.")
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index ef267fd9dd22..5e7ece0fda9f 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -92,7 +92,8 @@ void save_mce_event(struct pt_regs *regs, long handled,
 	mce->in_use = 1;
 
 	mce->initiator = MCE_INITIATOR_CPU;
-	if (handled)
+	/* Mark it recovered if we have handled it and MSR(RI=1). */
+	if (handled && (regs->msr & MSR_RI))
 		mce->disposition = MCE_DISPOSITION_RECOVERED;
 	else
 		mce->disposition = MCE_DISPOSITION_NOT_RECOVERED;

commit c04a5880299eab3da8c10547db96ea9cdffd44a6
Merge: a1c28b75a958 138a076496e6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 20 10:12:41 2016 -0700

    Merge tag 'powerpc-4.7-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights:
       - Support for Power ISA 3.0 (Power9) Radix Tree MMU from Aneesh Kumar K.V
       - Live patching support for ppc64le (also merged via livepatching.git)
    
      Various cleanups & minor fixes from:
       - Aaro Koskinen, Alexey Kardashevskiy, Andrew Donnellan, Aneesh Kumar K.V,
         Chris Smart, Daniel Axtens, Frederic Barrat, Gavin Shan, Ian Munsie,
         Lennart Sorensen, Madhavan Srinivasan, Mahesh Salgaonkar, Markus Elfring,
         Michael Ellerman, Oliver O'Halloran, Paul Gortmaker, Paul Mackerras,
         Rashmica Gupta, Russell Currey, Suraj Jitindar Singh, Thiago Jung
         Bauermann, Valentin Rothberg, Vipin K Parashar.
    
      General:
       - Update LMB associativity index during DLPAR add/remove from Nathan
         Fontenot
       - Fix branching to OOL handlers in relocatable kernel from Hari Bathini
       - Add support for userspace Power9 copy/paste from Chris Smart
       - Always use STRICT_MM_TYPECHECKS from Michael Ellerman
       - Add mask of possible MMU features from Michael Ellerman
    
      PCI:
       - Enable pass through of NVLink to guests from Alexey Kardashevskiy
       - Cleanups in preparation for powernv PCI hotplug from Gavin Shan
       - Don't report error in eeh_pe_reset_and_recover() from Gavin Shan
       - Restore initial state in eeh_pe_reset_and_recover() from Gavin Shan
       - Revert "powerpc/eeh: Fix crash in eeh_add_device_early() on Cell"
         from Guilherme G Piccoli
       - Remove the dependency on EEH struct in DDW mechanism from Guilherme
         G Piccoli
    
      selftests:
       - Test cp_abort during context switch from Chris Smart
       - Add several tests for transactional memory support from Rashmica
         Gupta
    
      perf:
       - Add support for sampling interrupt register state from Anju T
       - Add support for unwinding perf-stackdump from Chandan Kumar
    
      cxl:
       - Configure the PSL for two CAPI ports on POWER8NVL from Philippe
         Bergheaud
       - Allow initialization on timebase sync failures from Frederic Barrat
       - Increase timeout for detection of AFU mmio hang from Frederic
         Barrat
       - Handle num_of_processes larger than can fit in the SPA from Ian
         Munsie
       - Ensure PSL interrupt is configured for contexts with no AFU IRQs
         from Ian Munsie
       - Add kernel API to allow a context to operate with relocate disabled
         from Ian Munsie
       - Check periodically the coherent platform function's state from
         Christophe Lombard
    
      Freescale:
       - Updates from Scott: "Contains 86xx fixes, minor device tree fixes,
         an erratum workaround, and a kconfig dependency fix."
    
    * tag 'powerpc-4.7-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (192 commits)
      powerpc/86xx: Fix PCI interrupt map definition
      powerpc/86xx: Move pci1 definition to the include file
      powerpc/fsl: Fix build of the dtb embedded kernel images
      powerpc/fsl: Fix rcpm compatible string
      powerpc/fsl: Remove FSL_SOC dependency from FSL_LBC
      powerpc/fsl-pci: Add a workaround for PCI 5 errata
      powerpc/fsl: Fix SPI compatible on t208xrdb and t1040rdb
      powerpc/powernv/npu: Add PE to PHB's list
      powerpc/powernv: Fix insufficient memory allocation
      powerpc/iommu: Remove the dependency on EEH struct in DDW mechanism
      Revert "powerpc/eeh: Fix crash in eeh_add_device_early() on Cell"
      powerpc/eeh: Drop unnecessary label in eeh_pe_change_owner()
      powerpc/eeh: Ignore handlers in eeh_pe_reset_and_recover()
      powerpc/eeh: Restore initial state in eeh_pe_reset_and_recover()
      powerpc/eeh: Don't report error in eeh_pe_reset_and_recover()
      Revert "powerpc/powernv: Exclude root bus in pnv_pci_reset_secondary_bus()"
      powerpc/powernv/npu: Enable NVLink pass through
      powerpc/powernv/npu: Rework TCE Kill handling
      powerpc/powernv/npu: Add set/unset window helpers
      powerpc/powernv/ioda2: Export debug helper pe_level_printk()
      ...

commit c01e01597cbd0cf0571c2b05bf9e2245afb4478d
Author: Masanari Iida <standby24x7@gmail.com>
Date:   Wed Apr 20 00:27:33 2016 +0900

    treewide: Fix typos in printk
    
    This patch fix spelling typos in printk from various part
    of the codes.
    
    Signed-off-by: Masanari Iida <standby24x7@gmail.com>
    Acked-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index b2eb4686bd8f..671fd5122406 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -284,7 +284,7 @@ void machine_check_print_event_info(struct machine_check_event *evt)
 			printk("%s    Effective address: %016llx\n",
 			       level, evt->u.ue_error.effective_address);
 		if (evt->u.ue_error.physical_address_provided)
-			printk("%s      Physial address: %016llx\n",
+			printk("%s      Physical address: %016llx\n",
 			       level, evt->u.ue_error.physical_address);
 		break;
 	case MCE_ERROR_TYPE_SLB:

commit 635218c785bef355bc8266a1fdb28f38cdca365d
Author: Daniel Axtens <dja@axtens.net>
Date:   Wed Jan 6 11:45:50 2016 +1100

    powerpc: sparse: static-ify some things
    
    As sparse suggests, these should be made static.
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Reviewed-by: Stewart Smith <stewart@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index b2eb4686bd8f..35138225af6e 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -37,7 +37,7 @@ static DEFINE_PER_CPU(int, mce_queue_count);
 static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT], mce_event_queue);
 
 static void machine_check_process_queued_event(struct irq_work *work);
-struct irq_work mce_event_process_work = {
+static struct irq_work mce_event_process_work = {
         .func = machine_check_process_queued_event,
 };
 

commit ffb2d78eca08a1451137583d4e435aecfd6af809
Author: Daniel Axtens <dja@axtens.net>
Date:   Tue May 12 13:23:59 2015 +1000

    powerpc/mce: fix off by one errors in mce event handling
    
    Before 69111bac42f5 ("powerpc: Replace __get_cpu_var uses"), in
    save_mce_event, index got the value of mce_nest_count, and
    mce_nest_count was incremented *after* index was set.
    
    However, that patch changed the behaviour so that mce_nest count was
    incremented *before* setting index.
    
    This causes an off-by-one error, as get_mce_event sets index as
    mce_nest_count - 1 before reading mce_event.  Thus get_mce_event reads
    bogus data, causing warnings like
    "Machine Check Exception, Unknown event version 0 !"
    and breaking MCEs handling.
    
    Restore the old behaviour and unbreak MCE handling by subtracting one
    from the newly incremented value.
    
    The same broken change occured in machine_check_queue_event (which set
    a queue read by machine_check_process_queued_event).  Fix that too,
    unbreaking printing of MCE information.
    
    Fixes: 69111bac42f5 ("powerpc: Replace __get_cpu_var uses")
    CC: stable@vger.kernel.org
    CC: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    CC: Christoph Lameter <cl@linux.com>
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 15c99b649b04..b2eb4686bd8f 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -73,7 +73,7 @@ void save_mce_event(struct pt_regs *regs, long handled,
 		    uint64_t nip, uint64_t addr)
 {
 	uint64_t srr1;
-	int index = __this_cpu_inc_return(mce_nest_count);
+	int index = __this_cpu_inc_return(mce_nest_count) - 1;
 	struct machine_check_event *mce = this_cpu_ptr(&mce_event[index]);
 
 	/*
@@ -184,7 +184,7 @@ void machine_check_queue_event(void)
 	if (!get_mce_event(&evt, MCE_EVENT_RELEASE))
 		return;
 
-	index = __this_cpu_inc_return(mce_queue_count);
+	index = __this_cpu_inc_return(mce_queue_count) - 1;
 	/* If queue is full, just return for now. */
 	if (index >= MAX_MC_EVT) {
 		__this_cpu_dec(mce_queue_count);

commit 69111bac42f5ceacdd22e30947837ceb2c4493ed
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Oct 21 15:23:25 2014 -0500

    powerpc: Replace __get_cpu_var uses
    
    This still has not been merged and now powerpc is the only arch that does
    not have this change. Sorry about missing linuxppc-dev before.
    
    V2->V2
      - Fix up to work against 3.18-rc1
    
    __get_cpu_var() is used for multiple purposes in the kernel source. One of
    them is address calculation via the form &__get_cpu_var(x).  This calculates
    the address for the instance of the percpu variable of the current processor
    based on an offset.
    
    Other use cases are for storing and retrieving data from the current
    processors percpu area.  __get_cpu_var() can be used as an lvalue when
    writing data or on the right side of an assignment.
    
    __get_cpu_var() is defined as :
    
    __get_cpu_var() always only does an address determination. However, store
    and retrieve operations could use a segment prefix (or global register on
    other platforms) to avoid the address calculation.
    
    this_cpu_write() and this_cpu_read() can directly take an offset into a
    percpu area and use optimized assembly code to read and write per cpu
    variables.
    
    This patch converts __get_cpu_var into either an explicit address
    calculation using this_cpu_ptr() or into a use of this_cpu operations that
    use the offset.  Thereby address calculations are avoided and less registers
    are used when code is generated.
    
    At the end of the patch set all uses of __get_cpu_var have been removed so
    the macro is removed too.
    
    The patch set includes passes over all arches as well. Once these operations
    are used throughout then specialized macros can be defined in non -x86
    arches as well in order to optimize per cpu access by f.e.  using a global
    register that may be set to the per cpu base.
    
    Transformations done to __get_cpu_var()
    
    1. Determine the address of the percpu instance of the current processor.
    
            DEFINE_PER_CPU(int, y);
            int *x = &__get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(&y);
    
    2. Same as #1 but this time an array structure is involved.
    
            DEFINE_PER_CPU(int, y[20]);
            int *x = __get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(y);
    
    3. Retrieve the content of the current processors instance of a per cpu
    variable.
    
            DEFINE_PER_CPU(int, y);
            int x = __get_cpu_var(y)
    
       Converts to
    
            int x = __this_cpu_read(y);
    
    4. Retrieve the content of a percpu struct
    
            DEFINE_PER_CPU(struct mystruct, y);
            struct mystruct x = __get_cpu_var(y);
    
       Converts to
    
            memcpy(&x, this_cpu_ptr(&y), sizeof(x));
    
    5. Assignment to a per cpu variable
    
            DEFINE_PER_CPU(int, y)
            __get_cpu_var(y) = x;
    
       Converts to
    
            __this_cpu_write(y, x);
    
    6. Increment/Decrement etc of a per cpu variable
    
            DEFINE_PER_CPU(int, y);
            __get_cpu_var(y)++
    
       Converts to
    
            __this_cpu_inc(y)
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    CC: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    [mpe: Fix build errors caused by set/or_softirq_pending(), and rework
          assignment in __set_breakpoint() to use memcpy().]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index a7fd4cb78b78..15c99b649b04 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -73,8 +73,8 @@ void save_mce_event(struct pt_regs *regs, long handled,
 		    uint64_t nip, uint64_t addr)
 {
 	uint64_t srr1;
-	int index = __get_cpu_var(mce_nest_count)++;
-	struct machine_check_event *mce = &__get_cpu_var(mce_event[index]);
+	int index = __this_cpu_inc_return(mce_nest_count);
+	struct machine_check_event *mce = this_cpu_ptr(&mce_event[index]);
 
 	/*
 	 * Return if we don't have enough space to log mce event.
@@ -143,7 +143,7 @@ void save_mce_event(struct pt_regs *regs, long handled,
  */
 int get_mce_event(struct machine_check_event *mce, bool release)
 {
-	int index = __get_cpu_var(mce_nest_count) - 1;
+	int index = __this_cpu_read(mce_nest_count) - 1;
 	struct machine_check_event *mc_evt;
 	int ret = 0;
 
@@ -153,7 +153,7 @@ int get_mce_event(struct machine_check_event *mce, bool release)
 
 	/* Check if we have MCE info to process. */
 	if (index < MAX_MC_EVT) {
-		mc_evt = &__get_cpu_var(mce_event[index]);
+		mc_evt = this_cpu_ptr(&mce_event[index]);
 		/* Copy the event structure and release the original */
 		if (mce)
 			*mce = *mc_evt;
@@ -163,7 +163,7 @@ int get_mce_event(struct machine_check_event *mce, bool release)
 	}
 	/* Decrement the count to free the slot. */
 	if (release)
-		__get_cpu_var(mce_nest_count)--;
+		__this_cpu_dec(mce_nest_count);
 
 	return ret;
 }
@@ -184,13 +184,13 @@ void machine_check_queue_event(void)
 	if (!get_mce_event(&evt, MCE_EVENT_RELEASE))
 		return;
 
-	index = __get_cpu_var(mce_queue_count)++;
+	index = __this_cpu_inc_return(mce_queue_count);
 	/* If queue is full, just return for now. */
 	if (index >= MAX_MC_EVT) {
-		__get_cpu_var(mce_queue_count)--;
+		__this_cpu_dec(mce_queue_count);
 		return;
 	}
-	__get_cpu_var(mce_event_queue[index]) = evt;
+	memcpy(this_cpu_ptr(&mce_event_queue[index]), &evt, sizeof(evt));
 
 	/* Queue irq work to process this event later. */
 	irq_work_queue(&mce_event_process_work);
@@ -208,11 +208,11 @@ static void machine_check_process_queued_event(struct irq_work *work)
 	 * For now just print it to console.
 	 * TODO: log this error event to FSP or nvram.
 	 */
-	while (__get_cpu_var(mce_queue_count) > 0) {
-		index = __get_cpu_var(mce_queue_count) - 1;
+	while (__this_cpu_read(mce_queue_count) > 0) {
+		index = __this_cpu_read(mce_queue_count) - 1;
 		machine_check_print_event_info(
-				&__get_cpu_var(mce_event_queue[index]));
-		__get_cpu_var(mce_queue_count)--;
+				this_cpu_ptr(&mce_event_queue[index]));
+		__this_cpu_dec(mce_queue_count);
 	}
 }
 

commit 55672ecfa21f23616541c50e0e687f14f9ecf165
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Mon Dec 16 10:46:24 2013 +0530

    powerpc/book3s: Recover from MC in sapphire on SCOM read via MMIO.
    
    Detect and recover from machine check when inside opal on a special
    scom load instructions. On specific SCOM read via MMIO we may get a machine
    check exception with SRR0 pointing inside opal. To recover from MC
    in this scenario, get a recovery instruction address and return to it from
    MC.
    
    OPAL will export the machine check recoverable ranges through
    device tree node mcheck-recoverable-ranges under ibm,opal:
    
    # hexdump /proc/device-tree/ibm,opal/mcheck-recoverable-ranges
    0000000 0000 0000 3000 2804 0000 000c 0000 0000
    0000010 3000 2814 0000 0000 3000 27f0 0000 000c
    0000020 0000 0000 3000 2814 xxxx xxxx xxxx xxxx
    0000030 llll llll yyyy yyyy yyyy yyyy
    ...
    ...
    #
    
    where:
            xxxx xxxx xxxx xxxx = Starting instruction address
            llll llll           = Length of the address range.
            yyyy yyyy yyyy yyyy = recovery address
    
    Each recoverable address range entry is (start address, len,
    recovery address), 2 cells each for start and recovery address, 1 cell for
    len, totalling 5 cells per entry. During kernel boot time, build up the
    recovery table with the list of recovery ranges from device-tree node which
    will be used during machine check exception to recover from MMIO SCOM UE.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index cadef7e64e42..a7fd4cb78b78 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -70,7 +70,7 @@ static void mce_set_error_info(struct machine_check_event *mce,
  */
 void save_mce_event(struct pt_regs *regs, long handled,
 		    struct mce_error_info *mce_err,
-		    uint64_t addr)
+		    uint64_t nip, uint64_t addr)
 {
 	uint64_t srr1;
 	int index = __get_cpu_var(mce_nest_count)++;
@@ -86,7 +86,7 @@ void save_mce_event(struct pt_regs *regs, long handled,
 
 	/* Populate generic machine check info */
 	mce->version = MCE_V1;
-	mce->srr0 = regs->nip;
+	mce->srr0 = nip;
 	mce->srr1 = regs->msr;
 	mce->gpr3 = regs->gpr[3];
 	mce->in_use = 1;

commit 30c826358d10c1d6f8147de3310b97488daec830
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Tue Jan 14 15:45:09 2014 +0530

    Move precessing of MCE queued event out from syscall exit path.
    
    Huge Dickins reported an issue that b5ff4211a829
    "powerpc/book3s: Queue up and process delayed MCE events" breaks the
    PowerMac G5 boot. This patch fixes it by moving the mce even processing
    away from syscall exit, which was wrong to do that in first place, and
    using irq work framework to delay processing of mce event.
    
    Reported-by: Hugh Dickins <hughd@google.com
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index c0c52ec1fca7..cadef7e64e42 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -26,6 +26,7 @@
 #include <linux/ptrace.h>
 #include <linux/percpu.h>
 #include <linux/export.h>
+#include <linux/irq_work.h>
 #include <asm/mce.h>
 
 static DEFINE_PER_CPU(int, mce_nest_count);
@@ -35,6 +36,11 @@ static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT], mce_event);
 static DEFINE_PER_CPU(int, mce_queue_count);
 static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT], mce_event_queue);
 
+static void machine_check_process_queued_event(struct irq_work *work);
+struct irq_work mce_event_process_work = {
+        .func = machine_check_process_queued_event,
+};
+
 static void mce_set_error_info(struct machine_check_event *mce,
 			       struct mce_error_info *mce_err)
 {
@@ -185,17 +191,19 @@ void machine_check_queue_event(void)
 		return;
 	}
 	__get_cpu_var(mce_event_queue[index]) = evt;
+
+	/* Queue irq work to process this event later. */
+	irq_work_queue(&mce_event_process_work);
 }
 
 /*
  * process pending MCE event from the mce event queue. This function will be
  * called during syscall exit.
  */
-void machine_check_process_queued_event(void)
+static void machine_check_process_queued_event(struct irq_work *work)
 {
 	int index;
 
-	preempt_disable();
 	/*
 	 * For now just print it to console.
 	 * TODO: log this error event to FSP or nvram.
@@ -206,7 +214,6 @@ void machine_check_process_queued_event(void)
 				&__get_cpu_var(mce_event_queue[index]));
 		__get_cpu_var(mce_queue_count)--;
 	}
-	preempt_enable();
 }
 
 void machine_check_print_event_info(struct machine_check_event *evt)

commit b63a0ffe35de7e5f9b907bbc2c783e702f7e15af
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Wed Oct 30 20:06:13 2013 +0530

    powerpc/powernv: Machine check exception handling.
    
    Add basic error handling in machine check exception handler.
    
    - If MSR_RI isn't set, we can not recover.
    - Check if disposition set to OpalMCE_DISPOSITION_RECOVERED.
    - Check if address at fault is inside kernel address space, if not then send
      SIGBUS to process if we hit exception when in userspace.
    - If address at fault is not provided then and if we get a synchronous machine
      check while in userspace then kill the task.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index 1c6d15701c56..c0c52ec1fca7 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -316,3 +316,30 @@ void machine_check_print_event_info(struct machine_check_event *evt)
 		break;
 	}
 }
+
+uint64_t get_mce_fault_addr(struct machine_check_event *evt)
+{
+	switch (evt->error_type) {
+	case MCE_ERROR_TYPE_UE:
+		if (evt->u.ue_error.effective_address_provided)
+			return evt->u.ue_error.effective_address;
+		break;
+	case MCE_ERROR_TYPE_SLB:
+		if (evt->u.slb_error.effective_address_provided)
+			return evt->u.slb_error.effective_address;
+		break;
+	case MCE_ERROR_TYPE_ERAT:
+		if (evt->u.erat_error.effective_address_provided)
+			return evt->u.erat_error.effective_address;
+		break;
+	case MCE_ERROR_TYPE_TLB:
+		if (evt->u.tlb_error.effective_address_provided)
+			return evt->u.tlb_error.effective_address;
+		break;
+	default:
+	case MCE_ERROR_TYPE_UNKNOWN:
+		break;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(get_mce_fault_addr);

commit b5ff4211a8294be2ddbaf963fa3666fa042292a8
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Wed Oct 30 20:05:49 2013 +0530

    powerpc/book3s: Queue up and process delayed MCE events.
    
    When machine check real mode handler can not continue into host kernel
    in V mode, it returns from the interrupt and we loose MCE event which
    never gets logged. In such a situation queue up the MCE event so that
    we can log it later when we get back into host kernel with r1 pointing to
    kernel stack e.g. during syscall exit.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
index aeecdf1ba897..1c6d15701c56 100644
--- a/arch/powerpc/kernel/mce.c
+++ b/arch/powerpc/kernel/mce.c
@@ -31,6 +31,10 @@
 static DEFINE_PER_CPU(int, mce_nest_count);
 static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT], mce_event);
 
+/* Queue for delayed MCE events. */
+static DEFINE_PER_CPU(int, mce_queue_count);
+static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT], mce_event_queue);
+
 static void mce_set_error_info(struct machine_check_event *mce,
 			       struct mce_error_info *mce_err)
 {
@@ -162,3 +166,153 @@ void release_mce_event(void)
 {
 	get_mce_event(NULL, true);
 }
+
+/*
+ * Queue up the MCE event which then can be handled later.
+ */
+void machine_check_queue_event(void)
+{
+	int index;
+	struct machine_check_event evt;
+
+	if (!get_mce_event(&evt, MCE_EVENT_RELEASE))
+		return;
+
+	index = __get_cpu_var(mce_queue_count)++;
+	/* If queue is full, just return for now. */
+	if (index >= MAX_MC_EVT) {
+		__get_cpu_var(mce_queue_count)--;
+		return;
+	}
+	__get_cpu_var(mce_event_queue[index]) = evt;
+}
+
+/*
+ * process pending MCE event from the mce event queue. This function will be
+ * called during syscall exit.
+ */
+void machine_check_process_queued_event(void)
+{
+	int index;
+
+	preempt_disable();
+	/*
+	 * For now just print it to console.
+	 * TODO: log this error event to FSP or nvram.
+	 */
+	while (__get_cpu_var(mce_queue_count) > 0) {
+		index = __get_cpu_var(mce_queue_count) - 1;
+		machine_check_print_event_info(
+				&__get_cpu_var(mce_event_queue[index]));
+		__get_cpu_var(mce_queue_count)--;
+	}
+	preempt_enable();
+}
+
+void machine_check_print_event_info(struct machine_check_event *evt)
+{
+	const char *level, *sevstr, *subtype;
+	static const char *mc_ue_types[] = {
+		"Indeterminate",
+		"Instruction fetch",
+		"Page table walk ifetch",
+		"Load/Store",
+		"Page table walk Load/Store",
+	};
+	static const char *mc_slb_types[] = {
+		"Indeterminate",
+		"Parity",
+		"Multihit",
+	};
+	static const char *mc_erat_types[] = {
+		"Indeterminate",
+		"Parity",
+		"Multihit",
+	};
+	static const char *mc_tlb_types[] = {
+		"Indeterminate",
+		"Parity",
+		"Multihit",
+	};
+
+	/* Print things out */
+	if (evt->version != MCE_V1) {
+		pr_err("Machine Check Exception, Unknown event version %d !\n",
+		       evt->version);
+		return;
+	}
+	switch (evt->severity) {
+	case MCE_SEV_NO_ERROR:
+		level = KERN_INFO;
+		sevstr = "Harmless";
+		break;
+	case MCE_SEV_WARNING:
+		level = KERN_WARNING;
+		sevstr = "";
+		break;
+	case MCE_SEV_ERROR_SYNC:
+		level = KERN_ERR;
+		sevstr = "Severe";
+		break;
+	case MCE_SEV_FATAL:
+	default:
+		level = KERN_ERR;
+		sevstr = "Fatal";
+		break;
+	}
+
+	printk("%s%s Machine check interrupt [%s]\n", level, sevstr,
+	       evt->disposition == MCE_DISPOSITION_RECOVERED ?
+	       "Recovered" : "[Not recovered");
+	printk("%s  Initiator: %s\n", level,
+	       evt->initiator == MCE_INITIATOR_CPU ? "CPU" : "Unknown");
+	switch (evt->error_type) {
+	case MCE_ERROR_TYPE_UE:
+		subtype = evt->u.ue_error.ue_error_type <
+			ARRAY_SIZE(mc_ue_types) ?
+			mc_ue_types[evt->u.ue_error.ue_error_type]
+			: "Unknown";
+		printk("%s  Error type: UE [%s]\n", level, subtype);
+		if (evt->u.ue_error.effective_address_provided)
+			printk("%s    Effective address: %016llx\n",
+			       level, evt->u.ue_error.effective_address);
+		if (evt->u.ue_error.physical_address_provided)
+			printk("%s      Physial address: %016llx\n",
+			       level, evt->u.ue_error.physical_address);
+		break;
+	case MCE_ERROR_TYPE_SLB:
+		subtype = evt->u.slb_error.slb_error_type <
+			ARRAY_SIZE(mc_slb_types) ?
+			mc_slb_types[evt->u.slb_error.slb_error_type]
+			: "Unknown";
+		printk("%s  Error type: SLB [%s]\n", level, subtype);
+		if (evt->u.slb_error.effective_address_provided)
+			printk("%s    Effective address: %016llx\n",
+			       level, evt->u.slb_error.effective_address);
+		break;
+	case MCE_ERROR_TYPE_ERAT:
+		subtype = evt->u.erat_error.erat_error_type <
+			ARRAY_SIZE(mc_erat_types) ?
+			mc_erat_types[evt->u.erat_error.erat_error_type]
+			: "Unknown";
+		printk("%s  Error type: ERAT [%s]\n", level, subtype);
+		if (evt->u.erat_error.effective_address_provided)
+			printk("%s    Effective address: %016llx\n",
+			       level, evt->u.erat_error.effective_address);
+		break;
+	case MCE_ERROR_TYPE_TLB:
+		subtype = evt->u.tlb_error.tlb_error_type <
+			ARRAY_SIZE(mc_tlb_types) ?
+			mc_tlb_types[evt->u.tlb_error.tlb_error_type]
+			: "Unknown";
+		printk("%s  Error type: TLB [%s]\n", level, subtype);
+		if (evt->u.tlb_error.effective_address_provided)
+			printk("%s    Effective address: %016llx\n",
+			       level, evt->u.tlb_error.effective_address);
+		break;
+	default:
+	case MCE_ERROR_TYPE_UNKNOWN:
+		printk("%s  Error type: Unknown\n", level);
+		break;
+	}
+}

commit 36df96f8acaf51992177645eb2d781f766ce97dc
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Wed Oct 30 20:05:40 2013 +0530

    powerpc/book3s: Decode and save machine check event.
    
    Now that we handle machine check in linux, the MCE decoding should also
    take place in linux host. This info is crucial to log before we go down
    in case we can not handle the machine check errors. This patch decodes
    and populates a machine check event which contain high level meaning full
    MCE information.
    
    We do this in real mode C code with ME bit on. The MCE information is still
    available on emergency stack (in pt_regs structure format). Even if we take
    another exception at this point the MCE early handler will allocate a new
    stack frame on top of current one. So when we return back here we still have
    our MCE information safe on current stack.
    
    We use per cpu buffer to save high level MCE information. Each per cpu buffer
    is an array of machine check event structure indexed by per cpu counter
    mce_nest_count. The mce_nest_count is incremented every time we enter
    machine check early handler in real mode to get the current free slot
    (index = mce_nest_count - 1). The mce_nest_count is decremented once the
    MCE info is consumed by virtual mode machine exception handler.
    
    This patch provides save_mce_event(), get_mce_event() and release_mce_event()
    generic routines that can be used by machine check handlers to populate and
    retrieve the event. The routine release_mce_event() will free the event slot so
    that it can be reused. Caller can invoke get_mce_event() with a release flag
    either to release the event slot immediately OR keep it so that it can be
    fetched again. The event slot can be also released anytime by invoking
    release_mce_event().
    
    This patch also updates kvm code to invoke get_mce_event to retrieve generic
    mce event rather than paca->opal_mce_evt.
    
    The KVM code always calls get_mce_event() with release flags set to false so
    that event is available for linus host machine
    
    If machine check occurs while we are in guest, KVM tries to handle the error.
    If KVM is able to handle MC error successfully, it enters the guest and
    delivers the machine check to guest. If KVM is not able to handle MC error, it
    exists the guest and passes the control to linux host machine check handler
    which then logs MC event and decides how to handle it in linux host. In failure
    case, KVM needs to make sure that the MC event is available for linux host to
    consume. Hence KVM always calls get_mce_event() with release flags set to false
    and later it invokes release_mce_event() only if it succeeds to handle error.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/mce.c b/arch/powerpc/kernel/mce.c
new file mode 100644
index 000000000000..aeecdf1ba897
--- /dev/null
+++ b/arch/powerpc/kernel/mce.c
@@ -0,0 +1,164 @@
+/*
+ * Machine check exception handling.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright 2013 IBM Corporation
+ * Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
+ */
+
+#undef DEBUG
+#define pr_fmt(fmt) "mce: " fmt
+
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/percpu.h>
+#include <linux/export.h>
+#include <asm/mce.h>
+
+static DEFINE_PER_CPU(int, mce_nest_count);
+static DEFINE_PER_CPU(struct machine_check_event[MAX_MC_EVT], mce_event);
+
+static void mce_set_error_info(struct machine_check_event *mce,
+			       struct mce_error_info *mce_err)
+{
+	mce->error_type = mce_err->error_type;
+	switch (mce_err->error_type) {
+	case MCE_ERROR_TYPE_UE:
+		mce->u.ue_error.ue_error_type = mce_err->u.ue_error_type;
+		break;
+	case MCE_ERROR_TYPE_SLB:
+		mce->u.slb_error.slb_error_type = mce_err->u.slb_error_type;
+		break;
+	case MCE_ERROR_TYPE_ERAT:
+		mce->u.erat_error.erat_error_type = mce_err->u.erat_error_type;
+		break;
+	case MCE_ERROR_TYPE_TLB:
+		mce->u.tlb_error.tlb_error_type = mce_err->u.tlb_error_type;
+		break;
+	case MCE_ERROR_TYPE_UNKNOWN:
+	default:
+		break;
+	}
+}
+
+/*
+ * Decode and save high level MCE information into per cpu buffer which
+ * is an array of machine_check_event structure.
+ */
+void save_mce_event(struct pt_regs *regs, long handled,
+		    struct mce_error_info *mce_err,
+		    uint64_t addr)
+{
+	uint64_t srr1;
+	int index = __get_cpu_var(mce_nest_count)++;
+	struct machine_check_event *mce = &__get_cpu_var(mce_event[index]);
+
+	/*
+	 * Return if we don't have enough space to log mce event.
+	 * mce_nest_count may go beyond MAX_MC_EVT but that's ok,
+	 * the check below will stop buffer overrun.
+	 */
+	if (index >= MAX_MC_EVT)
+		return;
+
+	/* Populate generic machine check info */
+	mce->version = MCE_V1;
+	mce->srr0 = regs->nip;
+	mce->srr1 = regs->msr;
+	mce->gpr3 = regs->gpr[3];
+	mce->in_use = 1;
+
+	mce->initiator = MCE_INITIATOR_CPU;
+	if (handled)
+		mce->disposition = MCE_DISPOSITION_RECOVERED;
+	else
+		mce->disposition = MCE_DISPOSITION_NOT_RECOVERED;
+	mce->severity = MCE_SEV_ERROR_SYNC;
+
+	srr1 = regs->msr;
+
+	/*
+	 * Populate the mce error_type and type-specific error_type.
+	 */
+	mce_set_error_info(mce, mce_err);
+
+	if (!addr)
+		return;
+
+	if (mce->error_type == MCE_ERROR_TYPE_TLB) {
+		mce->u.tlb_error.effective_address_provided = true;
+		mce->u.tlb_error.effective_address = addr;
+	} else if (mce->error_type == MCE_ERROR_TYPE_SLB) {
+		mce->u.slb_error.effective_address_provided = true;
+		mce->u.slb_error.effective_address = addr;
+	} else if (mce->error_type == MCE_ERROR_TYPE_ERAT) {
+		mce->u.erat_error.effective_address_provided = true;
+		mce->u.erat_error.effective_address = addr;
+	} else if (mce->error_type == MCE_ERROR_TYPE_UE) {
+		mce->u.ue_error.effective_address_provided = true;
+		mce->u.ue_error.effective_address = addr;
+	}
+	return;
+}
+
+/*
+ * get_mce_event:
+ *	mce	Pointer to machine_check_event structure to be filled.
+ *	release Flag to indicate whether to free the event slot or not.
+ *		0 <= do not release the mce event. Caller will invoke
+ *		     release_mce_event() once event has been consumed.
+ *		1 <= release the slot.
+ *
+ *	return	1 = success
+ *		0 = failure
+ *
+ * get_mce_event() will be called by platform specific machine check
+ * handle routine and in KVM.
+ * When we call get_mce_event(), we are still in interrupt context and
+ * preemption will not be scheduled until ret_from_expect() routine
+ * is called.
+ */
+int get_mce_event(struct machine_check_event *mce, bool release)
+{
+	int index = __get_cpu_var(mce_nest_count) - 1;
+	struct machine_check_event *mc_evt;
+	int ret = 0;
+
+	/* Sanity check */
+	if (index < 0)
+		return ret;
+
+	/* Check if we have MCE info to process. */
+	if (index < MAX_MC_EVT) {
+		mc_evt = &__get_cpu_var(mce_event[index]);
+		/* Copy the event structure and release the original */
+		if (mce)
+			*mce = *mc_evt;
+		if (release)
+			mc_evt->in_use = 0;
+		ret = 1;
+	}
+	/* Decrement the count to free the slot. */
+	if (release)
+		__get_cpu_var(mce_nest_count)--;
+
+	return ret;
+}
+
+void release_mce_event(void)
+{
+	get_mce_event(NULL, true);
+}
