commit 65fddcfca8ad14778f71a57672fd01e8112d30fa
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:42 2020 -0700

    mm: reorder includes after introduction of linux/pgtable.h
    
    The replacement of <asm/pgrable.h> with <linux/pgtable.h> made the include
    of the latter in the middle of asm includes.  Fix this up with the aid of
    the below script and manual adjustments here and there.
    
            import sys
            import re
    
            if len(sys.argv) is not 3:
                print "USAGE: %s <file> <header>" % (sys.argv[0])
                sys.exit(1)
    
            hdr_to_move="#include <linux/%s>" % sys.argv[2]
            moved = False
            in_hdrs = False
    
            with open(sys.argv[1], "r") as f:
                lines = f.readlines()
                for _line in lines:
                    line = _line.rstrip('
    ')
                    if line == hdr_to_move:
                        continue
                    if line.startswith("#include <linux/"):
                        in_hdrs = True
                    elif not moved and in_hdrs:
                        moved = True
                        print hdr_to_move
                    print line
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 076d20c949ae..0ba1ed77dc68 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -30,13 +30,13 @@
 #include <linux/lockdep.h>
 #include <linux/memory.h>
 #include <linux/nmi.h>
+#include <linux/pgtable.h>
 
 #include <asm/debugfs.h>
 #include <asm/io.h>
 #include <asm/kdump.h>
 #include <asm/prom.h>
 #include <asm/processor.h>
-#include <linux/pgtable.h>
 #include <asm/smp.h>
 #include <asm/elf.h>
 #include <asm/machdep.h>

commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index bb47555d48a2..076d20c949ae 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -36,7 +36,7 @@
 #include <asm/kdump.h>
 #include <asm/prom.h>
 #include <asm/processor.h>
-#include <asm/pgtable.h>
+#include <linux/pgtable.h>
 #include <asm/smp.h>
 #include <asm/elf.h>
 #include <asm/machdep.h>

commit baddc87d6824cda18037881352fe97382fdb0867
Merge: bb5f33c06940 595d153dd102
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue May 26 22:56:03 2020 +1000

    Merge branch 'fixes' into next
    
    Merge our fixes branch from this cycle. It contains several important
    fixes we need in next for testing purposes, and also some that will
    conflict with upcoming changes.

commit d2cbbd45d433b96e41711a293e59cff259143694
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri May 8 14:34:01 2020 +1000

    powerpc/pseries: Limit machine check stack to 4GB
    
    This allows rtas_args to be put on the machine check stack, which
    avoids a lot of complications with re-entrancy deadlocks.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Mahesh Salgaonkar <mahesh@linux.ibm.com>
    Link: https://lore.kernel.org/r/20200508043408.886394-10-npiggin@gmail.com

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 438a9befce41..defe05b6b7a9 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -709,7 +709,7 @@ void __init exc_lvl_early_init(void)
  */
 void __init emergency_stack_init(void)
 {
-	u64 limit;
+	u64 limit, mce_limit;
 	unsigned int i;
 
 	/*
@@ -726,7 +726,16 @@ void __init emergency_stack_init(void)
 	 * initialized in kernel/irq.c. These are initialized here in order
 	 * to have emergency stacks available as early as possible.
 	 */
-	limit = min(ppc64_bolted_size(), ppc64_rma_size);
+	limit = mce_limit = min(ppc64_bolted_size(), ppc64_rma_size);
+
+	/*
+	 * Machine check on pseries calls rtas, but can't use the static
+	 * rtas_args due to a machine check hitting while the lock is held.
+	 * rtas args have to be under 4GB, so the machine check stack is
+	 * limited to 4GB so args can be put on stack.
+	 */
+	if (firmware_has_feature(FW_FEATURE_LPAR) && mce_limit > SZ_4G)
+		mce_limit = SZ_4G;
 
 	for_each_possible_cpu(i) {
 		paca_ptrs[i]->emergency_sp = alloc_stack(limit, i) + THREAD_SIZE;
@@ -736,7 +745,7 @@ void __init emergency_stack_init(void)
 		paca_ptrs[i]->nmi_emergency_sp = alloc_stack(limit, i) + THREAD_SIZE;
 
 		/* emergency stack for machine check exception handling. */
-		paca_ptrs[i]->mc_emergency_sp = alloc_stack(limit, i) + THREAD_SIZE;
+		paca_ptrs[i]->mc_emergency_sp = alloc_stack(mce_limit, i) + THREAD_SIZE;
 #endif
 	}
 }

commit 94c0b013c98583614e1ad911e8795ca36da34a85
Author: Chris Packham <chris.packham@alliedtelesis.co.nz>
Date:   Fri Apr 17 10:19:08 2020 +1200

    powerpc/setup_64: Set cache-line-size based on cache-block-size
    
    If {i,d}-cache-block-size is set and {i,d}-cache-line-size is not, use
    the block-size value for both. Per the devicetree spec cache-line-size
    is only needed if it differs from the block size.
    
    Originally the code would fallback from block size to line size. An
    error message was printed if both properties were missing.
    
    Later the code was refactored to use clearer names and logic but it
    inadvertently made line size a required property, meaning on systems
    without a line size property we fall back to the default from the
    cputable.
    
    On powernv (OPAL) platforms, since the introduction of device tree CPU
    features (5a61ef74f269 ("powerpc/64s: Support new device tree binding
    for discovering CPU features")), that has led to the wrong value being
    used, as the fallback value is incorrect for Power8/Power9 CPUs.
    
    The incorrect values flow through to the VDSO and also to the sysconf
    values, SC_LEVEL1_ICACHE_LINESIZE etc.
    
    Fixes: bd067f83b084 ("powerpc/64: Fix naming of cache block vs. cache line")
    Cc: stable@vger.kernel.org # v4.11+
    Signed-off-by: Chris Packham <chris.packham@alliedtelesis.co.nz>
    Reported-by: Qian Cai <cai@lca.pw>
    [mpe: Add even more detail to change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200416221908.7886-1-chris.packham@alliedtelesis.co.nz

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 438a9befce41..8105010b0e76 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -534,6 +534,8 @@ static bool __init parse_cache_info(struct device_node *np,
 	lsizep = of_get_property(np, propnames[3], NULL);
 	if (bsizep == NULL)
 		bsizep = lsizep;
+	if (lsizep == NULL)
+		lsizep = bsizep;
 	if (lsizep != NULL)
 		lsize = be32_to_cpu(*lsizep);
 	if (bsizep != NULL)

commit 7053f80d96967d8e72e9f2a724bbfc3906ce2b07
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Mar 20 14:21:16 2020 +1100

    powerpc/64: Prevent stack protection in early boot
    
    The previous commit reduced the amount of code that is run before we
    setup a paca. However there are still a few remaining functions that
    run with no paca, or worse, with an arbitrary value in r13 that will
    be used as a paca pointer.
    
    In particular the stack protector canary is stored in the paca, so if
    stack protector is activated for any of these functions we will read
    the stack canary from wherever r13 points. If r13 happens to point
    outside of memory we will get a machine check / checkstop.
    
    For example if we modify initialise_paca() to trigger stack
    protection, and then boot in the mambo simulator with r13 poisoned in
    skiboot before calling the kernel:
    
      DEBUG: 19952232: (19952232): INSTRUCTION: PC=0xC0000000191FC1E8: [0x3C4C006D]: addis   r2,r12,0x6D [fetch]
      DEBUG: 19952236: (19952236): INSTRUCTION: PC=0xC00000001807EAD8: [0x7D8802A6]: mflr    r12 [fetch]
      FATAL ERROR: 19952276: (19952276): Check Stop for 0:0: Machine Check with ME bit of MSR off
      DEBUG: 19952276: (19952276): INSTRUCTION: PC=0xC0000000191FCA7C: [0xE90D0CF8]: ld      r8,0xCF8(r13) [Instruction Failed]
      INFO: 19952276: (19952277): ** Execution stopped: Mambo Error, Machine Check Stop,  **
      systemsim % bt
      pc:                             0xC0000000191FCA7C      initialise_paca+0x54
      lr:                             0xC0000000191FC22C      early_setup+0x44
      stack:0x00000000198CBED0        0x0     +0x0
      stack:0x00000000198CBF00        0xC0000000191FC22C      early_setup+0x44
      stack:0x00000000198CBF90        0x1801C968      +0x1801C968
    
    So annotate the relevant functions to ensure stack protection is never
    enabled for them.
    
    Fixes: 06ec27aea9fc ("powerpc/64: add stack protector support")
    Cc: stable@vger.kernel.org # v4.20+
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200320032116.1024773-2-mpe@ellerman.id.au

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 17886d147dd0..438a9befce41 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -279,7 +279,7 @@ void __init record_spr_defaults(void)
  * device-tree is not accessible via normal means at this point.
  */
 
-void __init early_setup(unsigned long dt_ptr)
+void __init __nostackprotector early_setup(unsigned long dt_ptr)
 {
 	static __initdata struct paca_struct boot_paca;
 

commit d4a8e98621543d5798421eed177978bf2b3cdd11
Author: Daniel Axtens <dja@axtens.net>
Date:   Fri Mar 20 14:21:15 2020 +1100

    powerpc/64: Setup a paca before parsing device tree etc.
    
    Currently we set up the paca after parsing the device tree for CPU
    features. Prior to that, r13 contains random data, which means there
    is random data in r13 while we're running the generic dt parsing code.
    
    This random data varies depending on whether we boot through a vmlinux
    or a zImage: for the vmlinux case it's usually around zero, but for
    zImages we see random values like 912a72603d420015.
    
    This is poor practice, and can also lead to difficult-to-debug
    crashes. For example, when kcov is enabled, the kcov instrumentation
    attempts to read preempt_count out of the current task, which goes via
    the paca. This then crashes in the zImage case.
    
    Similarly stack protector can cause crashes if r13 is bogus, by
    reading from the stack canary in the paca.
    
    To resolve this:
    
     - move the paca setup to before the CPU feature parsing.
    
     - because we no longer have access to CPU feature flags in paca
     setup, change the HV feature test in the paca setup path to consider
     the actual value of the MSR rather than the CPU feature.
    
    Translations get switched on once we leave early_setup, so I think
    we'd already catch any other cases where the paca or task aren't set
    up.
    
    Boot tested on a P9 guest and host.
    
    Fixes: fb0b0a73b223 ("powerpc: Enable kcov")
    Fixes: 06ec27aea9fc ("powerpc/64: add stack protector support")
    Cc: stable@vger.kernel.org # v4.20+
    Reviewed-by: Andrew Donnellan <ajd@linux.ibm.com>
    Suggested-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    [mpe: Reword comments & change log a bit to mention stack protector]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200320032116.1024773-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index e05e6dd67ae6..17886d147dd0 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -285,18 +285,36 @@ void __init early_setup(unsigned long dt_ptr)
 
 	/* -------- printk is _NOT_ safe to use here ! ------- */
 
-	/* Try new device tree based feature discovery ... */
-	if (!dt_cpu_ftrs_init(__va(dt_ptr)))
-		/* Otherwise use the old style CPU table */
-		identify_cpu(0, mfspr(SPRN_PVR));
-
-	/* Assume we're on cpu 0 for now. Don't write to the paca yet! */
+	/*
+	 * Assume we're on cpu 0 for now.
+	 *
+	 * We need to load a PACA very early for a few reasons.
+	 *
+	 * The stack protector canary is stored in the paca, so as soon as we
+	 * call any stack protected code we need r13 pointing somewhere valid.
+	 *
+	 * If we are using kcov it will call in_task() in its instrumentation,
+	 * which relies on the current task from the PACA.
+	 *
+	 * dt_cpu_ftrs_init() calls into generic OF/fdt code, as well as
+	 * printk(), which can trigger both stack protector and kcov.
+	 *
+	 * percpu variables and spin locks also use the paca.
+	 *
+	 * So set up a temporary paca. It will be replaced below once we know
+	 * what CPU we are on.
+	 */
 	initialise_paca(&boot_paca, 0);
 	setup_paca(&boot_paca);
 	fixup_boot_paca();
 
 	/* -------- printk is now safe to use ------- */
 
+	/* Try new device tree based feature discovery ... */
+	if (!dt_cpu_ftrs_init(__va(dt_ptr)))
+		/* Otherwise use the old style CPU table */
+		identify_cpu(0, mfspr(SPRN_PVR));
+
 	/* Enable early debugging if any specified (see udbg.h) */
 	udbg_early_init();
 

commit 63289e7d3a645a4291ec43c1d526dd4a811da1a0
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Dec 21 08:32:28 2019 +0000

    powerpc: align stack to 2 * THREAD_SIZE with VMAP_STACK
    
    In order to ease stack overflow detection, align
    stack to 2 * THREAD_SIZE when using VMAP_STACK.
    This allows overflow detection using a single bit check.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/60e9ae86b7d2cdcf21468787076d345663648f46.1576916812.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6104917a282d..e05e6dd67ae6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -633,7 +633,7 @@ static void *__init alloc_stack(unsigned long limit, int cpu)
 
 	BUILD_BUG_ON(STACK_INT_FRAME_SIZE % 16);
 
-	ptr = memblock_alloc_try_nid(THREAD_SIZE, THREAD_SIZE,
+	ptr = memblock_alloc_try_nid(THREAD_SIZE, THREAD_ALIGN,
 				     MEMBLOCK_LOW_LIMIT, limit,
 				     early_cpu_to_node(cpu));
 	if (!ptr)

commit 265c3491c4bc8d40587996d6ee2f447a7ccfb4f3
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Sep 12 13:49:43 2019 +0000

    powerpc: Add support for GENERIC_EARLY_IOREMAP
    
    Add support for GENERIC_EARLY_IOREMAP.
    
    Let's define 16 slots of 256Kbytes each for early ioremap.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/412c7eaa6a373d8f82a3c3ee01e6a65a1a6589de.1568295907.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index d2af4c228970..6104917a282d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -65,6 +65,7 @@
 #include <asm/hw_irq.h>
 #include <asm/feature-fixups.h>
 #include <asm/kup.h>
+#include <asm/early_ioremap.h>
 
 #include "setup.h"
 
@@ -332,6 +333,8 @@ void __init early_setup(unsigned long dt_ptr)
 	apply_feature_fixups();
 	setup_feature_keys();
 
+	early_ioremap_setup();
+
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu();
 

commit 3b9176e9a874a848afa7eb2f6943639eb18b7a17
Author: Qian Cai <cai@lca.pw>
Date:   Mon Jul 15 14:32:32 2019 -0400

    powerpc/setup_64: fix -Wempty-body warnings
    
    At the beginning of setup_64.c, it has,
    
      #ifdef DEBUG
      #define DBG(fmt...) udbg_printf(fmt)
      #else
      #define DBG(fmt...)
      #endif
    
    where DBG() could be compiled away, and generate warnings,
    
      arch/powerpc/kernel/setup_64.c: In function 'initialize_cache_info':
      arch/powerpc/kernel/setup_64.c:579:49: warning: suggest braces around
      empty body in an 'if' statement [-Wempty-body]
          DBG("Argh, can't find dcache properties !\n");
                                                     ^
      arch/powerpc/kernel/setup_64.c:582:49: warning: suggest braces around
      empty body in an 'if' statement [-Wempty-body]
          DBG("Argh, can't find icache properties !\n");
    
    Fix it by using the suggestions from Michael:
    
      "Neither of those sites should use DBG(), that's not really early
      boot code, they should just use pr_warn().
    
      And the other uses of DBG() in initialize_cache_info() should just
      be removed.
    
      In smp_release_cpus() the entry/exit DBG's should just be removed,
      and the spinning_secondaries line should just be pr_debug().
    
      That would just leave the two calls in early_setup(). If we taught
      udbg_printf() to return early when udbg_putc is NULL, then we could
      just call udbg_printf() unconditionally and get rid of the DBG macro
      entirely."
    
    Suggested-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Qian Cai <cai@lca.pw>
    [mpe: Split udbg change out into previous patch]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/1563215552-8166-1-git-send-email-cai@lca.pw

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 44b4c432a273..d2af4c228970 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -68,12 +68,6 @@
 
 #include "setup.h"
 
-#ifdef DEBUG
-#define DBG(fmt...) udbg_printf(fmt)
-#else
-#define DBG(fmt...)
-#endif
-
 int spinning_secondaries;
 u64 ppc64_pft_size;
 
@@ -305,7 +299,7 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Enable early debugging if any specified (see udbg.h) */
 	udbg_early_init();
 
- 	DBG(" -> early_setup(), dt_ptr: 0x%lx\n", dt_ptr);
+	udbg_printf(" -> %s(), dt_ptr: 0x%lx\n", __func__, dt_ptr);
 
 	/*
 	 * Do early initialization using the flattened device
@@ -362,11 +356,11 @@ void __init early_setup(unsigned long dt_ptr)
 	 */
 	this_cpu_enable_ftrace();
 
-	DBG(" <- early_setup()\n");
+	udbg_printf(" <- %s()\n", __func__);
 
 #ifdef CONFIG_PPC_EARLY_DEBUG_BOOTX
 	/*
-	 * This needs to be done *last* (after the above DBG() even)
+	 * This needs to be done *last* (after the above udbg_printf() even)
 	 *
 	 * Right after we return from this function, we turn on the MMU
 	 * which means the real-mode access trick that btext does will
@@ -436,8 +430,6 @@ void smp_release_cpus(void)
 	if (!use_spinloop())
 		return;
 
-	DBG(" -> smp_release_cpus()\n");
-
 	/* All secondary cpus are spinning on a common spinloop, release them
 	 * all now so they can start to spin on their individual paca
 	 * spinloops. For non SMP kernels, the secondary cpus never get out
@@ -456,9 +448,7 @@ void smp_release_cpus(void)
 			break;
 		udelay(1);
 	}
-	DBG("spinning_secondaries = %d\n", spinning_secondaries);
-
-	DBG(" <- smp_release_cpus()\n");
+	pr_debug("spinning_secondaries = %d\n", spinning_secondaries);
 }
 #endif /* CONFIG_SMP || CONFIG_KEXEC_CORE */
 
@@ -551,8 +541,6 @@ void __init initialize_cache_info(void)
 	struct device_node *cpu = NULL, *l2, *l3 = NULL;
 	u32 pvr;
 
-	DBG(" -> initialize_cache_info()\n");
-
 	/*
 	 * All shipping POWER8 machines have a firmware bug that
 	 * puts incorrect information in the device-tree. This will
@@ -576,10 +564,10 @@ void __init initialize_cache_info(void)
 	 */
 	if (cpu) {
 		if (!parse_cache_info(cpu, false, &ppc64_caches.l1d))
-			DBG("Argh, can't find dcache properties !\n");
+			pr_warn("Argh, can't find dcache properties !\n");
 
 		if (!parse_cache_info(cpu, true, &ppc64_caches.l1i))
-			DBG("Argh, can't find icache properties !\n");
+			pr_warn("Argh, can't find icache properties !\n");
 
 		/*
 		 * Try to find the L2 and L3 if any. Assume they are
@@ -604,8 +592,6 @@ void __init initialize_cache_info(void)
 
 	cur_cpu_spec->dcache_bsize = dcache_bsize;
 	cur_cpu_spec->icache_bsize = icache_bsize;
-
-	DBG(" <- initialize_cache_info()\n");
 }
 
 /*

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index a400854a5036..44b4c432a273 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -1,13 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * 
  * Common boot and setup code.
  *
  * Copyright (C) 2001 PPC64 Team, IBM Corp
- *
- *      This program is free software; you can redistribute it and/or
- *      modify it under the terms of the GNU General Public License
- *      as published by the Free Software Foundation; either version
- *      2 of the License, or (at your option) any later version.
  */
 
 #include <linux/export.h>

commit b970afcfcabd63cd3832e95db096439c177c3592
Merge: 8ea5b2abd07e 8150a153c013
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 10 05:29:27 2019 -0700

    Merge tag 'powerpc-5.2-1' of ssh://gitolite.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Slightly delayed due to the issue with printk() calling
      probe_kernel_read() interacting with our new user access prevention
      stuff, but all fixed now.
    
      The only out-of-area changes are the addition of a cpuhp_state, small
      additions to Documentation and MAINTAINERS updates.
    
      Highlights:
    
       - Support for Kernel Userspace Access/Execution Prevention (like
         SMAP/SMEP/PAN/PXN) on some 64-bit and 32-bit CPUs. This prevents
         the kernel from accidentally accessing userspace outside
         copy_to/from_user(), or ever executing userspace.
    
       - KASAN support on 32-bit.
    
       - Rework of where we map the kernel, vmalloc, etc. on 64-bit hash to
         use the same address ranges we use with the Radix MMU.
    
       - A rewrite into C of large parts of our idle handling code for
         64-bit Book3S (ie. power8 & power9).
    
       - A fast path entry for syscalls on 32-bit CPUs, for a 12-17% speedup
         in the null_syscall benchmark.
    
       - On 64-bit bare metal we have support for recovering from errors
         with the time base (our clocksource), however if that fails
         currently we hang in __delay() and never crash. We now have support
         for detecting that case and short circuiting __delay() so we at
         least panic() and reboot.
    
       - Add support for optionally enabling the DAWR on Power9, which had
         to be disabled by default due to a hardware erratum. This has the
         effect of enabling hardware breakpoints for GDB, the downside is a
         badly behaved program could crash the machine by pointing the DAWR
         at cache inhibited memory. This is opt-in obviously.
    
       - xmon, our crash handler, gets support for a read only mode where
         operations that could change memory or otherwise disturb the system
         are disabled.
    
      Plus many clean-ups, reworks and minor fixes etc.
    
      Thanks to: Christophe Leroy, Akshay Adiga, Alastair D'Silva, Alexey
      Kardashevskiy, Andrew Donnellan, Aneesh Kumar K.V, Anju T Sudhakar,
      Anton Blanchard, Ben Hutchings, Bo YU, Breno Leitao, Cédric Le Goater,
      Christopher M. Riedl, Christoph Hellwig, Colin Ian King, David Gibson,
      Ganesh Goudar, Gautham R. Shenoy, George Spelvin, Greg Kroah-Hartman,
      Greg Kurz, Horia Geantă, Jagadeesh Pagadala, Joel Stanley, Joe
      Perches, Julia Lawall, Laurentiu Tudor, Laurent Vivier, Lukas Bulwahn,
      Madhavan Srinivasan, Mahesh Salgaonkar, Mathieu Malaterre, Michael
      Neuling, Mukesh Ojha, Nathan Fontenot, Nathan Lynch, Nicholas Piggin,
      Nick Desaulniers, Oliver O'Halloran, Peng Hao, Qian Cai, Ravi
      Bangoria, Rick Lindsley, Russell Currey, Sachin Sant, Stewart Smith,
      Sukadev Bhattiprolu, Thomas Huth, Tobin C. Harding, Tyrel Datwyler,
      Valentin Schneider, Wei Yongjun, Wen Yang, YueHaibing"
    
    * tag 'powerpc-5.2-1' of ssh://gitolite.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (205 commits)
      powerpc/64s: Use early_mmu_has_feature() in set_kuap()
      powerpc/book3s/64: check for NULL pointer in pgd_alloc()
      powerpc/mm: Fix hugetlb page initialization
      ocxl: Fix return value check in afu_ioctl()
      powerpc/mm: fix section mismatch for setup_kup()
      powerpc/mm: fix redundant inclusion of pgtable-frag.o in Makefile
      powerpc/mm: Fix makefile for KASAN
      powerpc/kasan: add missing/lost Makefile
      selftests/powerpc: Add a signal fuzzer selftest
      powerpc/booke64: set RI in default MSR
      ocxl: Provide global MMIO accessors for external drivers
      ocxl: move event_fd handling to frontend
      ocxl: afu_irq only deals with IRQ IDs, not offsets
      ocxl: Allow external drivers to use OpenCAPI contexts
      ocxl: Create a clear delineation between ocxl backend & frontend
      ocxl: Don't pass pci_dev around
      ocxl: Split pci.c
      ocxl: Remove some unused exported symbols
      ocxl: Remove superfluous 'extern' from headers
      ocxl: read_pasid never returns an error, so make it void
      ...

commit 0a499fc5c37e6db096969a83534fd98a2bf2b36c
Merge: e50c5d2e725e 0336e04a6520
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 13:01:16 2019 -0700

    Merge branch 'core-speculation-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull speculation mitigation update from Ingo Molnar:
     "This adds the "mitigations=" bootline option, which offers a
      cross-arch set of options that will work on x86, PowerPC and s390 that
      will map to the arch specific option internally"
    
    * 'core-speculation-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      s390/speculation: Support 'mitigations=' cmdline option
      powerpc/speculation: Support 'mitigations=' cmdline option
      x86/speculation: Support 'mitigations=' cmdline option
      cpu/speculation: Add 'mitigations=' cmdline option

commit b28c97505eb1a5265e367c398c3406be6ce5e313
Author: Russell Currey <ruscur@russell.cc>
Date:   Thu Apr 18 16:51:21 2019 +1000

    powerpc/64: Setup KUP on secondary CPUs
    
    Some platforms (i.e. Radix MMU) need per-CPU initialisation for KUP.
    
    Any platforms that only want to do KUP initialisation once
    globally can just check to see if they're running on the boot CPU, or
    check if whatever setup they need has already been performed.
    
    Note that this is only for 64-bit.
    
    Signed-off-by: Russell Currey <ruscur@russell.cc>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6179c4200339..684e34493bf5 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -390,6 +390,9 @@ void early_setup_secondary(void)
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu_secondary();
 
+	/* Perform any KUP setup that is per-cpu */
+	setup_kup();
+
 	/*
 	 * At this point, we can let interrupts switch to virtual mode
 	 * (the MMU has been setup), so adjust the MSR in the PACA to

commit 69795cabe4cfe5122438d50010ad5310c113a013
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Apr 18 16:51:18 2019 +1000

    powerpc: Add framework for Kernel Userspace Protection
    
    This patch adds a skeleton for Kernel Userspace Protection
    functionnalities like Kernel Userspace Access Protection and Kernel
    Userspace Execution Prevention
    
    The subsequent implementation of KUAP for radix makes use of a MMU
    feature in order to patch out assembly when KUAP is disabled or
    unsupported. This won't work unless there's an entry point for KUP
    support before the feature magic happens, so for PPC64 setup_kup() is
    called early in setup.
    
    On PPC32, feature_fixup() is done too early to allow the same.
    
    Suggested-by: Russell Currey <ruscur@russell.cc>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ba404dd9ce1d..6179c4200339 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -68,6 +68,7 @@
 #include <asm/cputhreads.h>
 #include <asm/hw_irq.h>
 #include <asm/feature-fixups.h>
+#include <asm/kup.h>
 
 #include "setup.h"
 
@@ -331,6 +332,12 @@ void __init early_setup(unsigned long dt_ptr)
 	 */
 	configure_exceptions();
 
+	/*
+	 * Configure Kernel Userspace Protection. This needs to happen before
+	 * feature fixups for platforms that implement this using features.
+	 */
+	setup_kup();
+
 	/* Apply all the dynamic patching */
 	apply_feature_fixups();
 	setup_feature_keys();

commit 782e69efb3dfed6e8360bc612e8c7827a901a8f9
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Apr 12 15:39:30 2019 -0500

    powerpc/speculation: Support 'mitigations=' cmdline option
    
    Configure powerpc CPU runtime speculation bug mitigations in accordance
    with the 'mitigations=' cmdline option.  This affects Meltdown, Spectre
    v1, Spectre v2, and Speculative Store Bypass.
    
    The default behavior is unchanged.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Jiri Kosina <jkosina@suse.cz> (on x86)
    Reviewed-by: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: linux-s390@vger.kernel.org
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-arch@vger.kernel.org
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Tyler Hicks <tyhicks@canonical.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Steven Price <steven.price@arm.com>
    Cc: Phil Auld <pauld@redhat.com>
    Link: https://lkml.kernel.org/r/245a606e1a42a558a310220312d9b6adb9159df6.1555085500.git.jpoimboe@redhat.com

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 236c1151a3a7..c7ec27ba8926 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -958,7 +958,7 @@ void setup_rfi_flush(enum l1d_flush_type types, bool enable)
 
 	enabled_flush_types = types;
 
-	if (!no_rfi_flush)
+	if (!no_rfi_flush && !cpu_mitigations_off())
 		rfi_flush_enable(enable);
 }
 

commit 8a7f97b902f4fb0d94b355b6b3f1fbd7154cafb9
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Mar 11 23:30:31 2019 -0700

    treewide: add checks for the return value of memblock_alloc*()
    
    Add check for the return value of memblock_alloc*() functions and call
    panic() in case of error.  The panic message repeats the one used by
    panicing memblock allocators with adjustment of parameters to include
    only relevant ones.
    
    The replacement was mostly automated with semantic patches like the one
    below with manual massaging of format strings.
    
      @@
      expression ptr, size, align;
      @@
      ptr = memblock_alloc(size, align);
      + if (!ptr)
      +     panic("%s: Failed to allocate %lu bytes align=0x%lx\n", __func__, size, align);
    
    [anders.roxell@linaro.org: use '%pa' with 'phys_addr_t' type]
      Link: http://lkml.kernel.org/r/20190131161046.21886-1-anders.roxell@linaro.org
    [rppt@linux.ibm.com: fix format strings for panics after memblock_alloc]
      Link: http://lkml.kernel.org/r/1548950940-15145-1-git-send-email-rppt@linux.ibm.com
    [rppt@linux.ibm.com: don't panic if the allocation in sparse_buffer_init fails]
      Link: http://lkml.kernel.org/r/20190131074018.GD28876@rapoport-lnx
    [akpm@linux-foundation.org: fix xtensa printk warning]
    Link: http://lkml.kernel.org/r/1548057848-15136-20-git-send-email-rppt@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Anders Roxell <anders.roxell@linaro.org>
    Reviewed-by: Guo Ren <ren_guo@c-sky.com>                [c-sky]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>    [s390]
    Reviewed-by: Juergen Gross <jgross@suse.com>            [Xen]
    Reviewed-by: Geert Uytterhoeven <geert@linux-m68k.org>  [m68k]
    Acked-by: Max Filippov <jcmvbkbc@gmail.com>             [xtensa]
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Dennis Zhou <dennis@kernel.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Rob Herring <robh+dt@kernel.org>
    Cc: Rob Herring <robh@kernel.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ff0aac42bb33..ba404dd9ce1d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -905,6 +905,10 @@ static void __ref init_fallback_flush(void)
 	l1d_flush_fallback_area = memblock_alloc_try_nid(l1d_size * 2,
 						l1d_size, MEMBLOCK_LOW_LIMIT,
 						limit, NUMA_NO_NODE);
+	if (!l1d_flush_fallback_area)
+		panic("%s: Failed to allocate %llu bytes align=0x%llx max_addr=%pa\n",
+		      __func__, l1d_size * 2, l1d_size, &limit);
+
 
 	for_each_possible_cpu(cpu) {
 		struct paca_struct *paca = paca_ptrs[cpu];

commit b5dd0c658c31b469ccff1b637e5124851e7a4a1c
Merge: 610cd4eadec4 fe0436e10c88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 7 19:25:37 2019 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge more updates from Andrew Morton:
    
     - some of the rest of MM
    
     - various misc things
    
     - dynamic-debug updates
    
     - checkpatch
    
     - some epoll speedups
    
     - autofs
    
     - rapidio
    
     - lib/, lib/lzo/ updates
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (83 commits)
      samples/mic/mpssd/mpssd.h: remove duplicate header
      kernel/fork.c: remove duplicated include
      include/linux/relay.h: fix percpu annotation in struct rchan
      arch/nios2/mm/fault.c: remove duplicate include
      unicore32: stop printing the virtual memory layout
      MAINTAINERS: fix GTA02 entry and mark as orphan
      mm: create the new vm_fault_t type
      arm, s390, unicore32: remove oneliner wrappers for memblock_alloc()
      arch: simplify several early memory allocations
      openrisc: simplify pte_alloc_one_kernel()
      sh: prefer memblock APIs returning virtual address
      microblaze: prefer memblock API returning virtual address
      powerpc: prefer memblock APIs returning virtual address
      lib/lzo: separate lzo-rle from lzo
      lib/lzo: implement run-length encoding
      lib/lzo: fast 8-byte copy on arm64
      lib/lzo: 64-bit CTZ on arm64
      lib/lzo: tidy-up ifdefs
      ipc/sem.c: replace kvmalloc/memset with kvzalloc and use struct_size
      ipc: annotate implicit fall through
      ...

commit f806714f7048715cc18f16ebe26a761e09b2f210
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Thu Mar 7 16:30:48 2019 -0800

    powerpc: prefer memblock APIs returning virtual address
    
    Patch series "memblock: simplify several early memory allocation", v4.
    
    These patches simplify some of the early memory allocations by replacing
    usage of older memblock APIs with newer and shinier ones.
    
    Quite a few places in the arch/ code allocated memory using a memblock
    API that returns a physical address of the allocated area, then
    converted this physical address to a virtual one and then used memset(0)
    to clear the allocated range.
    
    More recent memblock APIs do all the three steps in one call and their
    usage simplifies the code.
    
    It's important to note that regardless of API used, the core allocation
    is nearly identical for any set of memblock allocators: first it tries
    to find a free memory with all the constraints specified by the caller
    and then falls back to the allocation with some or all constraints
    disabled.
    
    The first three patches perform the conversion of call sites that have
    exact requirements for the node and the possible memory range.
    
    The fourth patch is a bit one-off as it simplifies openrisc's
    implementation of pte_alloc_one_kernel(), and not only the memblock
    usage.
    
    The fifth patch takes care of simpler cases when the allocation can be
    satisfied with a simple call to memblock_alloc().
    
    The sixth patch removes one-liner wrappers for memblock_alloc on arm and
    unicore32, as suggested by Christoph.
    
    This patch (of 6):
    
    There are a several places that allocate memory using memblock APIs that
    return a physical address, convert the returned address to the virtual
    address and frequently also memset(0) the allocated range.
    
    Update these places to use memblock allocators already returning a
    virtual address.  Use memblock functions that clear the allocated memory
    instead of calling memset(0) where appropriate.
    
    The calls to memblock_alloc_base() that were not followed by memset(0)
    are replaced with memblock_alloc_try_nid_raw().  Since the latter does
    not panic() when the allocation fails, the appropriate panic() calls are
    added to the call sites.
    
    Link: http://lkml.kernel.org/r/1546248566-14910-2-git-send-email-rppt@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stefan Kristiansson <stefan.kristiansson@saunalahti.fi>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Michal Simek <michal.simek@xilinx.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 236c1151a3a7..5de413ae3cd6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -933,8 +933,9 @@ static void __ref init_fallback_flush(void)
 	 * hardware prefetch runoff. We don't have a recipe for load patterns to
 	 * reliably avoid the prefetcher.
 	 */
-	l1d_flush_fallback_area = __va(memblock_alloc_base(l1d_size * 2, l1d_size, limit));
-	memset(l1d_flush_fallback_area, 0, l1d_size * 2);
+	l1d_flush_fallback_area = memblock_alloc_try_nid(l1d_size * 2,
+						l1d_size, MEMBLOCK_LOW_LIMIT,
+						limit, NUMA_NO_NODE);
 
 	for_each_possible_cpu(cpu) {
 		struct paca_struct *paca = paca_ptrs[cpu];

commit d608898abc749424e26aa0e451d39e33cf3f4adc
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Jan 12 09:55:53 2019 +0000

    powerpc: clean stack pointers naming
    
    Some stack pointers used to also be thread_info pointers
    and were called tp. Now that they are only stack pointers,
    rename them sp.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 2db1c5f7d141..daa361fc6a24 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -716,19 +716,14 @@ void __init emergency_stack_init(void)
 	limit = min(ppc64_bolted_size(), ppc64_rma_size);
 
 	for_each_possible_cpu(i) {
-		void *ti;
-
-		ti = alloc_stack(limit, i);
-		paca_ptrs[i]->emergency_sp = ti + THREAD_SIZE;
+		paca_ptrs[i]->emergency_sp = alloc_stack(limit, i) + THREAD_SIZE;
 
 #ifdef CONFIG_PPC_BOOK3S_64
 		/* emergency stack for NMI exception handling. */
-		ti = alloc_stack(limit, i);
-		paca_ptrs[i]->nmi_emergency_sp = ti + THREAD_SIZE;
+		paca_ptrs[i]->nmi_emergency_sp = alloc_stack(limit, i) + THREAD_SIZE;
 
 		/* emergency stack for machine check exception handling. */
-		ti = alloc_stack(limit, i);
-		paca_ptrs[i]->mc_emergency_sp = ti + THREAD_SIZE;
+		paca_ptrs[i]->mc_emergency_sp = alloc_stack(limit, i) + THREAD_SIZE;
 #endif
 	}
 }

commit a7916a1de526162d73e894b6d3ebd895d4302078
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:09:00 2019 +0000

    powerpc: regain entire stack space
    
    thread_info is not anymore in the stack, so the entire stack
    can now be used.
    
    There is also no risk anymore of corrupting task_cpu(p) with a
    stack overflow so the patch removes the test.
    
    When doing this, an explicit test for NULL stack pointer is
    needed in validate_sp() as it is not anymore implicitely covered
    by the sizeof(thread_info) gap.
    
    In the meantime, with the previous patch all pointers to the stacks
    are not anymore pointers to thread_info so this patch changes them
    to void*
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 0912948a8ea6..2db1c5f7d141 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -716,19 +716,19 @@ void __init emergency_stack_init(void)
 	limit = min(ppc64_bolted_size(), ppc64_rma_size);
 
 	for_each_possible_cpu(i) {
-		struct thread_info *ti;
+		void *ti;
 
 		ti = alloc_stack(limit, i);
-		paca_ptrs[i]->emergency_sp = (void *)ti + THREAD_SIZE;
+		paca_ptrs[i]->emergency_sp = ti + THREAD_SIZE;
 
 #ifdef CONFIG_PPC_BOOK3S_64
 		/* emergency stack for NMI exception handling. */
 		ti = alloc_stack(limit, i);
-		paca_ptrs[i]->nmi_emergency_sp = (void *)ti + THREAD_SIZE;
+		paca_ptrs[i]->nmi_emergency_sp = ti + THREAD_SIZE;
 
 		/* emergency stack for machine check exception handling. */
 		ti = alloc_stack(limit, i);
-		paca_ptrs[i]->mc_emergency_sp = (void *)ti + THREAD_SIZE;
+		paca_ptrs[i]->mc_emergency_sp = ti + THREAD_SIZE;
 #endif
 	}
 }

commit ed1cd6deb013a11959d17a94e35ce159197632da
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:08:58 2019 +0000

    powerpc: Activate CONFIG_THREAD_INFO_IN_TASK
    
    This patch activates CONFIG_THREAD_INFO_IN_TASK which
    moves the thread_info into task_struct.
    
    Moving thread_info into task_struct has the following advantages:
      - It protects thread_info from corruption in the case of stack
        overflows.
      - Its address is harder to determine if stack addresses are leaked,
        making a number of attacks more difficult.
    
    This has the following consequences:
      - thread_info is now located at the beginning of task_struct.
      - The 'cpu' field is now in task_struct, and only exists when
        CONFIG_SMP is active.
      - thread_info doesn't have anymore the 'task' field.
    
    This patch:
      - Removes all recopy of thread_info struct when the stack changes.
      - Changes the CURRENT_THREAD_INFO() macro to point to current.
      - Selects CONFIG_THREAD_INFO_IN_TASK.
      - Modifies raw_smp_processor_id() to get ->cpu from current without
        including linux/sched.h to avoid circular inclusion and without
        including asm/asm-offsets.h to avoid symbol names duplication
        between ASM constants and C constants.
      - Modifies klp_init_thread_info() to take a task_struct pointer
        argument.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Add task_stack.h to livepatch.h to fix build fails]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 080dd515d587..0912948a8ea6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -689,24 +689,6 @@ void __init exc_lvl_early_init(void)
 }
 #endif
 
-/*
- * Emergency stacks are used for a range of things, from asynchronous
- * NMIs (system reset, machine check) to synchronous, process context.
- * We set preempt_count to zero, even though that isn't necessarily correct. To
- * get the right value we'd need to copy it from the previous thread_info, but
- * doing that might fault causing more problems.
- * TODO: what to do with accounting?
- */
-static void emerg_stack_init_thread_info(struct thread_info *ti, int cpu)
-{
-	ti->task = NULL;
-	ti->cpu = cpu;
-	ti->preempt_count = 0;
-	ti->local_flags = 0;
-	ti->flags = 0;
-	klp_init_thread_info(ti);
-}
-
 /*
  * Stack space used when we detect a bad kernel stack pointer, and
  * early in SMP boots before relocation is enabled. Exclusive emergency
@@ -737,18 +719,15 @@ void __init emergency_stack_init(void)
 		struct thread_info *ti;
 
 		ti = alloc_stack(limit, i);
-		emerg_stack_init_thread_info(ti, i);
 		paca_ptrs[i]->emergency_sp = (void *)ti + THREAD_SIZE;
 
 #ifdef CONFIG_PPC_BOOK3S_64
 		/* emergency stack for NMI exception handling. */
 		ti = alloc_stack(limit, i);
-		emerg_stack_init_thread_info(ti, i);
 		paca_ptrs[i]->nmi_emergency_sp = (void *)ti + THREAD_SIZE;
 
 		/* emergency stack for machine check exception handling. */
 		ti = alloc_stack(limit, i);
-		emerg_stack_init_thread_info(ti, i);
 		paca_ptrs[i]->mc_emergency_sp = (void *)ti + THREAD_SIZE;
 #endif
 	}

commit c8e409a33cf8df5060064a70df3e1350841371e1
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:08:44 2019 +0000

    powerpc/irq: use memblock functions returning virtual address
    
    Since only the virtual address of allocated blocks is used,
    lets use functions returning directly virtual address.
    
    Those functions have the advantage of also zeroing the block.
    
    Suggested-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Acked-by: Mike Rapoport <rppt@linux.ibm.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 236c1151a3a7..080dd515d587 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -634,19 +634,17 @@ __init u64 ppc64_bolted_size(void)
 
 static void *__init alloc_stack(unsigned long limit, int cpu)
 {
-	unsigned long pa;
+	void *ptr;
 
 	BUILD_BUG_ON(STACK_INT_FRAME_SIZE % 16);
 
-	pa = memblock_alloc_base_nid(THREAD_SIZE, THREAD_SIZE, limit,
-					early_cpu_to_node(cpu), MEMBLOCK_NONE);
-	if (!pa) {
-		pa = memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit);
-		if (!pa)
-			panic("cannot allocate stacks");
-	}
+	ptr = memblock_alloc_try_nid(THREAD_SIZE, THREAD_SIZE,
+				     MEMBLOCK_LOW_LIMIT, limit,
+				     early_cpu_to_node(cpu));
+	if (!ptr)
+		panic("cannot allocate stacks");
 
-	return __va(pa);
+	return ptr;
 }
 
 void __init irqstack_early_init(void)
@@ -739,20 +737,17 @@ void __init emergency_stack_init(void)
 		struct thread_info *ti;
 
 		ti = alloc_stack(limit, i);
-		memset(ti, 0, THREAD_SIZE);
 		emerg_stack_init_thread_info(ti, i);
 		paca_ptrs[i]->emergency_sp = (void *)ti + THREAD_SIZE;
 
 #ifdef CONFIG_PPC_BOOK3S_64
 		/* emergency stack for NMI exception handling. */
 		ti = alloc_stack(limit, i);
-		memset(ti, 0, THREAD_SIZE);
 		emerg_stack_init_thread_info(ti, i);
 		paca_ptrs[i]->nmi_emergency_sp = (void *)ti + THREAD_SIZE;
 
 		/* emergency stack for machine check exception handling. */
 		ti = alloc_stack(limit, i);
-		memset(ti, 0, THREAD_SIZE);
 		emerg_stack_init_thread_info(ti, i);
 		paca_ptrs[i]->mc_emergency_sp = (void *)ti + THREAD_SIZE;
 #endif

commit 66f93c5a02d5ba6ef17fef459143961382593212
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Nov 15 12:34:27 2018 +1000

    powerpc/64: Fix kernel stack 16-byte alignment
    
    Commit 4c2de74cc869 ("powerpc/64: Interrupts save PPR on stack rather
    than thread_struct") changed sizeof(struct pt_regs) % 16 from 0 to 8,
    which causes the interrupt frame allocation on kernel entry to put the
    kernel stack out of alignment.
    
    Quadword (16-byte) alignment for the stack is required by both the
    64-bit v1 ABI (v1.9 § 3.2.2) and the 64-bit v2 ABI (v1.1 § 2.2.2.1).
    
    Add a pad field to fix alignment, and add a BUILD_BUG_ON to catch this
    in future.
    
    Fixes: 4c2de74cc869 ("powerpc/64: Interrupts save PPR on stack rather than thread_struct")
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 2a51e4cc8246..236c1151a3a7 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -636,6 +636,8 @@ static void *__init alloc_stack(unsigned long limit, int cpu)
 {
 	unsigned long pa;
 
+	BUILD_BUG_ON(STACK_INT_FRAME_SIZE % 16);
+
 	pa = memblock_alloc_base_nid(THREAD_SIZE, THREAD_SIZE, limit,
 					early_cpu_to_node(cpu), MEMBLOCK_NONE);
 	if (!pa) {

commit 57c8a661d95dff48dd9c2f2496139082bbaf241a
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:49 2018 -0700

    mm: remove include/linux/bootmem.h
    
    Move remaining definitions and declarations from include/linux/bootmem.h
    into include/linux/memblock.h and remove the redundant header.
    
    The includes were replaced with the semantic patch below and then
    semi-automated removal of duplicated '#include <linux/memblock.h>
    
    @@
    @@
    - #include <linux/bootmem.h>
    + #include <linux/memblock.h>
    
    [sfr@canb.auug.org.au: dma-direct: fix up for the removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181002185342.133d1680@canb.auug.org.au
    [sfr@canb.auug.org.au: powerpc: fix up for removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181005161406.73ef8727@canb.auug.org.au
    [sfr@canb.auug.org.au: x86/kaslr, ACPI/NUMA: fix for linux/bootmem.h removal]
      Link: http://lkml.kernel.org/r/20181008190341.5e396491@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1536927045-23536-30-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 9216c3a7fcfc..2a51e4cc8246 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -29,10 +29,9 @@
 #include <linux/unistd.h>
 #include <linux/serial.h>
 #include <linux/serial_8250.h>
-#include <linux/bootmem.h>
+#include <linux/memblock.h>
 #include <linux/pci.h>
 #include <linux/lockdep.h>
-#include <linux/memblock.h>
 #include <linux/memory.h>
 #include <linux/nmi.h>
 

commit 97ad1087efffed26cb00e310a927f9603332dfcb
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:44 2018 -0700

    memblock: replace BOOTMEM_ALLOC_* with MEMBLOCK variants
    
    Drop BOOTMEM_ALLOC_ACCESSIBLE and BOOTMEM_ALLOC_ANYWHERE in favor of
    identical MEMBLOCK definitions.
    
    Link: http://lkml.kernel.org/r/1536927045-23536-29-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f90ab3ea9af3..9216c3a7fcfc 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -764,7 +764,7 @@ void __init emergency_stack_init(void)
 static void * __init pcpu_fc_alloc(unsigned int cpu, size_t size, size_t align)
 {
 	return memblock_alloc_try_nid(size, align, __pa(MAX_DMA_ADDRESS),
-				      BOOTMEM_ALLOC_ACCESSIBLE,
+				      MEMBLOCK_ALLOC_ACCESSIBLE,
 				      early_cpu_to_node(cpu));
 
 }

commit 2013288f723887837d2f1cebef5fcf663b2319de
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:21 2018 -0700

    memblock: replace free_bootmem{_node} with memblock_free
    
    The free_bootmem and free_bootmem_node are merely wrappers for
    memblock_free. Replace their usage with a call to memblock_free using the
    following semantic patch:
    
    @@
    expression e1, e2, e3;
    @@
    (
    - free_bootmem(e1, e2)
    + memblock_free(e1, e2)
    |
    - free_bootmem_node(e1, e2, e3)
    + memblock_free(e2, e3)
    )
    
    Link: http://lkml.kernel.org/r/1536927045-23536-24-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 26d7c49a157b..f90ab3ea9af3 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -771,7 +771,7 @@ static void * __init pcpu_fc_alloc(unsigned int cpu, size_t size, size_t align)
 
 static void __init pcpu_fc_free(void *ptr, size_t size)
 {
-	free_bootmem(__pa(ptr), size);
+	memblock_free(__pa(ptr), size);
 }
 
 static int pcpu_cpu_distance(unsigned int from, unsigned int to)

commit ccfa2a0f2e8581a68715aaad5ad0fb56daf7db43
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:08:45 2018 -0700

    memblock: replace __alloc_bootmem_node with appropriate memblock_ API
    
    Use memblock_alloc_try_nid whenever goal (i.e. minimal address is
    specified) and memblock_alloc_node otherwise.
    
    Link: http://lkml.kernel.org/r/1536927045-23536-17-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index faf00222b324..26d7c49a157b 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -763,8 +763,10 @@ void __init emergency_stack_init(void)
 
 static void * __init pcpu_fc_alloc(unsigned int cpu, size_t size, size_t align)
 {
-	return __alloc_bootmem_node(NODE_DATA(early_cpu_to_node(cpu)), size, align,
-				    __pa(MAX_DMA_ADDRESS));
+	return memblock_alloc_try_nid(size, align, __pa(MAX_DMA_ADDRESS),
+				      BOOTMEM_ALLOC_ACCESSIBLE,
+				      early_cpu_to_node(cpu));
+
 }
 
 static void __init pcpu_fc_free(void *ptr, size_t size)

commit dd9a8c5a87395b6f05552c3b44e42fdc95760552
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Sep 11 13:07:56 2018 +1000

    powerpc/tm: Fix HFSCR bit for no suspend case
    
    Currently on P9N DD2.1 we end up taking infinite TM facility
    unavailable exceptions on the first TM usage by userspace.
    
    In the special case of TM no suspend (P9N DD2.1), Linux is told TM is
    off via CPU dt-ftrs but told to (partially) use it via
    OPAL_REINIT_CPUS_TM_SUSPEND_DISABLED. So HFSCR[TM] will be off from
    dt-ftrs but we need to turn it on for the no suspend case.
    
    This patch fixes this by enabling HFSCR TM in this case.
    
    Cc: stable@vger.kernel.org # 4.15+
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6a501b25dd85..faf00222b324 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -243,13 +243,19 @@ static void cpu_ready_for_interrupts(void)
 	}
 
 	/*
-	 * Fixup HFSCR:TM based on CPU features. The bit is set by our
-	 * early asm init because at that point we haven't updated our
-	 * CPU features from firmware and device-tree. Here we have,
-	 * so let's do it.
+	 * Set HFSCR:TM based on CPU features:
+	 * In the special case of TM no suspend (P9N DD2.1), Linux is
+	 * told TM is off via the dt-ftrs but told to (partially) use
+	 * it via OPAL_REINIT_CPUS_TM_SUSPEND_DISABLED. So HFSCR[TM]
+	 * will be off from dt-ftrs but we need to turn it on for the
+	 * no suspend case.
 	 */
-	if (cpu_has_feature(CPU_FTR_HVMODE) && !cpu_has_feature(CPU_FTR_TM_COMP))
-		mtspr(SPRN_HFSCR, mfspr(SPRN_HFSCR) & ~HFSCR_TM);
+	if (cpu_has_feature(CPU_FTR_HVMODE)) {
+		if (cpu_has_feature(CPU_FTR_TM_COMP))
+			mtspr(SPRN_HFSCR, mfspr(SPRN_HFSCR) | HFSCR_TM);
+		else
+			mtspr(SPRN_HFSCR, mfspr(SPRN_HFSCR) & ~HFSCR_TM);
+	}
 
 	/* Set IR and DR in PACA MSR */
 	get_paca()->kernel_msr = MSR_KERNEL;

commit 2c86cd188f8a5631f3d75a1dea14d22df85189b4
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jul 5 16:25:01 2018 +0000

    powerpc: clean inclusions of asm/feature-fixups.h
    
    files not using feature fixup don't need asm/feature-fixups.h
    files using feature fixup need asm/feature-fixups.h
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 225bc5f91049..6a501b25dd85 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -68,6 +68,7 @@
 #include <asm/opal.h>
 #include <asm/cputhreads.h>
 #include <asm/hw_irq.h>
+#include <asm/feature-fixups.h>
 
 #include "setup.h"
 

commit 8c1aef6a682f87a059f10ab606cc1e2cdd663d5a
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 19 14:35:52 2018 +1000

    powerpc/64: hard disable irqs in panic_smp_self_stop
    
    Similarly to commit 855bfe0de1 ("powerpc: hard disable irqs in
    smp_send_stop loop"), irqs should be hard disabled by
    panic_smp_self_stop.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 7a7ce8ad455e..225bc5f91049 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -387,6 +387,14 @@ void early_setup_secondary(void)
 
 #endif /* CONFIG_SMP */
 
+void panic_smp_self_stop(void)
+{
+	hard_irq_disable();
+	spin_begin();
+	while (1)
+		spin_cpu_relax();
+}
+
 #if defined(CONFIG_SMP) || defined(CONFIG_KEXEC_CORE)
 static bool use_spinloop(void)
 {

commit d103978636c27fce216bbc8bb289981047b71bd4
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Thu Apr 19 12:34:03 2018 +0530

    powerpc64/ftrace: Delay enabling ftrace on secondary cpus
    
    On the boot cpu, though we enable paca->ftrace_enabled in early_setup()
    (via cpu_ready_for_interrupts()), we don't start tracing until much
    later since ftrace is not initialized yet and since we only support
    DYNAMIC_FTRACE on powerpc. However, it is possible that ftrace has been
    initialized by the time some of the secondary cpus start up. In this
    case, we will try to trace some of the early boot code which can cause
    problems.
    
    To address this, move setting paca->ftrace_enabled from
    cpu_ready_for_interrupts() to early_setup() for the boot cpu, and towards
    the end of start_secondary() for secondary cpus.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 313136006d1c..7a7ce8ad455e 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -252,9 +252,6 @@ static void cpu_ready_for_interrupts(void)
 
 	/* Set IR and DR in PACA MSR */
 	get_paca()->kernel_msr = MSR_KERNEL;
-
-	/* We are now ok to enable ftrace */
-	get_paca()->ftrace_enabled = 1;
 }
 
 unsigned long spr_default_dscr = 0;
@@ -349,6 +346,13 @@ void __init early_setup(unsigned long dt_ptr)
 	 */
 	cpu_ready_for_interrupts();
 
+	/*
+	 * We enable ftrace here, but since we only support DYNAMIC_FTRACE, it
+	 * will only actually get enabled on the boot cpu much later once
+	 * ftrace itself has been initialized.
+	 */
+	this_cpu_enable_ftrace();
+
 	DBG(" <- early_setup()\n");
 
 #ifdef CONFIG_PPC_EARLY_DEBUG_BOOTX

commit ea678ac627e01daf5b4f1da24bf1d0c500e10898
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Thu Apr 19 12:34:00 2018 +0530

    powerpc64/ftrace: Add a field in paca to disable ftrace in unsafe code paths
    
    We have some C code that we call into from real mode where we cannot
    take any exceptions. Though the C functions themselves are mostly safe,
    if these functions are traced, there is a possibility that we may take
    an exception. For instance, in certain conditions, the ftrace code uses
    WARN(), which uses a 'trap' to do its job.
    
    For such scenarios, introduce a new field in paca 'ftrace_enabled',
    which is checked on ftrace entry before continuing. This field can then
    be set to zero to disable/pause ftrace, and set to a non-zero value to
    resume ftrace.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index b78f142a4148..313136006d1c 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -252,6 +252,9 @@ static void cpu_ready_for_interrupts(void)
 
 	/* Set IR and DR in PACA MSR */
 	get_paca()->kernel_msr = MSR_KERNEL;
+
+	/* We are now ok to enable ftrace */
+	get_paca()->ftrace_enabled = 1;
 }
 
 unsigned long spr_default_dscr = 0;

commit 9dfbf78e4114fcaf4ef61c49885c3ab5bad40d0b
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Thu Jan 18 00:33:36 2018 +0530

    powerpc/64s: Default l1d_size to 64K in RFI fallback flush
    
    If there is no d-cache-size property in the device tree, l1d_size could
    be zero. We don't actually expect that to happen, it's only been seen
    on mambo (simulator) in some configurations.
    
    A zero-size l1d_size leads to the loop in the asm wrapping around to
    2^64-1, and then walking off the end of the fallback area and
    eventually causing a page fault which is fatal.
    
    Just default to 64K which is correct on some CPUs, and sane enough to
    not cause a crash on others.
    
    Fixes: aa8a5e0062ac9 ('powerpc/64s: Add support for RFI flush of L1-D cache')
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    [mpe: Rewrite comment and change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 44c30dd38067..b78f142a4148 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -890,6 +890,17 @@ static void __ref init_fallback_flush(void)
 		return;
 
 	l1d_size = ppc64_caches.l1d.size;
+
+	/*
+	 * If there is no d-cache-size property in the device tree, l1d_size
+	 * could be zero. That leads to the loop in the asm wrapping around to
+	 * 2^64-1, and then walking off the end of the fallback area and
+	 * eventually causing a page fault which is fatal. Just default to
+	 * something vaguely sane.
+	 */
+	if (!l1d_size)
+		l1d_size = (64 * 1024);
+
 	limit = min(ppc64_bolted_size(), ppc64_rma_size);
 
 	/*

commit 501a78cbc17c329fabf8e9750a1e9ab810c88a0e
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Apr 5 22:49:13 2018 +1000

    powerpc/64s: Fix section mismatch warnings from setup_rfi_flush()
    
    The recent LPM changes to setup_rfi_flush() are causing some section
    mismatch warnings because we removed the __init annotation on
    setup_rfi_flush():
    
      The function setup_rfi_flush() references
      the function __init ppc64_bolted_size().
      the function __init memblock_alloc_base().
    
    The references are actually in init_fallback_flush(), but that is
    inlined into setup_rfi_flush().
    
    These references are safe because:
     - only pseries calls setup_rfi_flush() at runtime
     - pseries always passes L1D_FLUSH_FALLBACK at boot
     - so the fallback flush area will always be allocated
     - so the check in init_fallback_flush() will always return early:
       /* Only allocate the fallback flush area once (at boot time). */
       if (l1d_flush_fallback_area)
            return;
    
     - and therefore we won't actually call the freed init routines.
    
    We should rework the code to make it safer by default rather than
    relying on the above, but for now as a quick-fix just add a __ref
    annotation to squash the warning.
    
    Fixes: abf110f3e1ce ("powerpc/rfi-flush: Make it possible to call setup_rfi_flush() again")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 66f2b6299c40..44c30dd38067 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -880,7 +880,7 @@ void rfi_flush_enable(bool enable)
 	rfi_flush = enable;
 }
 
-static void init_fallback_flush(void)
+static void __ref init_fallback_flush(void)
 {
 	u64 l1d_size, limit;
 	int cpu;

commit f437c51748fa1dd423a878c870ad203843a51c8d
Merge: 872a100a49c3 29ab6c4708a5
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Mar 31 00:11:24 2018 +1100

    Merge branch 'topic/paca' into next
    
    Bring in yet another series that touches KVM code, and might need to
    be merged into the kvm-ppc branch to resolve conflicts.
    
    This required some changes in pnv_power9_force_smt4_catch/release()
    due to the paca array becomming an array of pointers.

commit f3865f9a7112590f0cae02dce05ec3c3a09ff405
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:21 2018 +1000

    powerpc/64: Allocate per-cpu stacks node-local if possible
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 02fa358982e6..16ea71fa1ead 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -611,6 +611,21 @@ __init u64 ppc64_bolted_size(void)
 #endif
 }
 
+static void *__init alloc_stack(unsigned long limit, int cpu)
+{
+	unsigned long pa;
+
+	pa = memblock_alloc_base_nid(THREAD_SIZE, THREAD_SIZE, limit,
+					early_cpu_to_node(cpu), MEMBLOCK_NONE);
+	if (!pa) {
+		pa = memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit);
+		if (!pa)
+			panic("cannot allocate stacks");
+	}
+
+	return __va(pa);
+}
+
 void __init irqstack_early_init(void)
 {
 	u64 limit = ppc64_bolted_size();
@@ -622,12 +637,8 @@ void __init irqstack_early_init(void)
 	 * accessed in realmode.
 	 */
 	for_each_possible_cpu(i) {
-		softirq_ctx[i] = (struct thread_info *)
-			__va(memblock_alloc_base(THREAD_SIZE,
-					    THREAD_SIZE, limit));
-		hardirq_ctx[i] = (struct thread_info *)
-			__va(memblock_alloc_base(THREAD_SIZE,
-					    THREAD_SIZE, limit));
+		softirq_ctx[i] = alloc_stack(limit, i);
+		hardirq_ctx[i] = alloc_stack(limit, i);
 	}
 }
 
@@ -635,20 +646,21 @@ void __init irqstack_early_init(void)
 void __init exc_lvl_early_init(void)
 {
 	unsigned int i;
-	unsigned long sp;
 
 	for_each_possible_cpu(i) {
-		sp = memblock_alloc(THREAD_SIZE, THREAD_SIZE);
-		critirq_ctx[i] = (struct thread_info *)__va(sp);
-		paca_ptrs[i]->crit_kstack = __va(sp + THREAD_SIZE);
+		void *sp;
 
-		sp = memblock_alloc(THREAD_SIZE, THREAD_SIZE);
-		dbgirq_ctx[i] = (struct thread_info *)__va(sp);
-		paca_ptrs[i]->dbg_kstack = __va(sp + THREAD_SIZE);
+		sp = alloc_stack(ULONG_MAX, i);
+		critirq_ctx[i] = sp;
+		paca_ptrs[i]->crit_kstack = sp + THREAD_SIZE;
 
-		sp = memblock_alloc(THREAD_SIZE, THREAD_SIZE);
-		mcheckirq_ctx[i] = (struct thread_info *)__va(sp);
-		paca_ptrs[i]->mc_kstack = __va(sp + THREAD_SIZE);
+		sp = alloc_stack(ULONG_MAX, i);
+		dbgirq_ctx[i] = sp;
+		paca_ptrs[i]->dbg_kstack = sp + THREAD_SIZE;
+
+		sp = alloc_stack(ULONG_MAX, i);
+		mcheckirq_ctx[i] = sp;
+		paca_ptrs[i]->mc_kstack = sp + THREAD_SIZE;
 	}
 
 	if (cpu_has_feature(CPU_FTR_DEBUG_LVL_EXC))
@@ -702,20 +714,21 @@ void __init emergency_stack_init(void)
 
 	for_each_possible_cpu(i) {
 		struct thread_info *ti;
-		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
+
+		ti = alloc_stack(limit, i);
 		memset(ti, 0, THREAD_SIZE);
 		emerg_stack_init_thread_info(ti, i);
 		paca_ptrs[i]->emergency_sp = (void *)ti + THREAD_SIZE;
 
 #ifdef CONFIG_PPC_BOOK3S_64
 		/* emergency stack for NMI exception handling. */
-		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
+		ti = alloc_stack(limit, i);
 		memset(ti, 0, THREAD_SIZE);
 		emerg_stack_init_thread_info(ti, i);
 		paca_ptrs[i]->nmi_emergency_sp = (void *)ti + THREAD_SIZE;
 
 		/* emergency stack for machine check exception handling. */
-		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
+		ti = alloc_stack(limit, i);
 		memset(ti, 0, THREAD_SIZE);
 		emerg_stack_init_thread_info(ti, i);
 		paca_ptrs[i]->mc_emergency_sp = (void *)ti + THREAD_SIZE;

commit 4890aea65ae7b5d424b5020e8be193b08a545990
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:20 2018 +1000

    powerpc/64: Allocate pacas per node
    
    Per-node allocations are possible on 64s with radix that does
    not have the bolted SLB limitation.
    
    Hash would be able to do the same if all CPUs had the bottom of
    their node-local memory bolted as well. This is left as an
    exercise for the reader.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Add dummy definition of boot_cpuid for !SMP]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index dde34d35d1e7..02fa358982e6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -312,6 +312,10 @@ void __init early_setup(unsigned long dt_ptr)
 	early_init_devtree(__va(dt_ptr));
 
 	/* Now we know the logical id of our boot cpu, setup the paca. */
+	if (boot_cpuid != 0) {
+		/* Poison paca_ptrs[0] again if it's not the boot cpu */
+		memset(&paca_ptrs[0], 0x88, sizeof(paca_ptrs[0]));
+	}
 	setup_paca(paca_ptrs[boot_cpuid]);
 	fixup_boot_paca();
 

commit c0abd0c745bdabe027a8f013a866f385fba717b1
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:17 2018 +1000

    powerpc/64: move default SPR recording
    
    Move this into the early setup code, and don't iterate over CPU masks.
    We don't want to call into sysfs so early from setup, and a future patch
    won't initialize CPU masks by the time this is called.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Fold in incremental fix from Nick for DSCR handling]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 3ce12af4906f..dde34d35d1e7 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -254,6 +254,14 @@ static void cpu_ready_for_interrupts(void)
 	get_paca()->kernel_msr = MSR_KERNEL;
 }
 
+unsigned long spr_default_dscr = 0;
+
+void __init record_spr_defaults(void)
+{
+	if (early_cpu_has_feature(CPU_FTR_DSCR))
+		spr_default_dscr = mfspr(SPRN_DSCR);
+}
+
 /*
  * Early initialization entry point. This is called by head.S
  * with MMU translation disabled. We rely on the "feature" of

commit d2e60075a3d4422dc54b919f3b125d8066b839d4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:12 2018 +1000

    powerpc/64: Use array of paca pointers and allocate pacas individually
    
    Change the paca array into an array of pointers to pacas. Allocate
    pacas individually.
    
    This allows flexibility in where the PACAs are allocated. Future work
    will allocate them node-local. Platforms that don't have address limits
    on PACAs would be able to defer PACA allocations until later in boot
    rather than allocate all possible ones up-front then freeing unused.
    
    This is slightly more overhead (one additional indirection) for cross
    CPU paca references, but those aren't too common.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index c388cc3357fa..3ce12af4906f 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -110,7 +110,7 @@ void __init setup_tlb_core_data(void)
 		if (cpu_first_thread_sibling(boot_cpuid) == first)
 			first = boot_cpuid;
 
-		paca[cpu].tcd_ptr = &paca[first].tcd;
+		paca_ptrs[cpu]->tcd_ptr = &paca_ptrs[first]->tcd;
 
 		/*
 		 * If we have threads, we need either tlbsrx.
@@ -304,7 +304,7 @@ void __init early_setup(unsigned long dt_ptr)
 	early_init_devtree(__va(dt_ptr));
 
 	/* Now we know the logical id of our boot cpu, setup the paca. */
-	setup_paca(&paca[boot_cpuid]);
+	setup_paca(paca_ptrs[boot_cpuid]);
 	fixup_boot_paca();
 
 	/*
@@ -628,15 +628,15 @@ void __init exc_lvl_early_init(void)
 	for_each_possible_cpu(i) {
 		sp = memblock_alloc(THREAD_SIZE, THREAD_SIZE);
 		critirq_ctx[i] = (struct thread_info *)__va(sp);
-		paca[i].crit_kstack = __va(sp + THREAD_SIZE);
+		paca_ptrs[i]->crit_kstack = __va(sp + THREAD_SIZE);
 
 		sp = memblock_alloc(THREAD_SIZE, THREAD_SIZE);
 		dbgirq_ctx[i] = (struct thread_info *)__va(sp);
-		paca[i].dbg_kstack = __va(sp + THREAD_SIZE);
+		paca_ptrs[i]->dbg_kstack = __va(sp + THREAD_SIZE);
 
 		sp = memblock_alloc(THREAD_SIZE, THREAD_SIZE);
 		mcheckirq_ctx[i] = (struct thread_info *)__va(sp);
-		paca[i].mc_kstack = __va(sp + THREAD_SIZE);
+		paca_ptrs[i]->mc_kstack = __va(sp + THREAD_SIZE);
 	}
 
 	if (cpu_has_feature(CPU_FTR_DEBUG_LVL_EXC))
@@ -693,20 +693,20 @@ void __init emergency_stack_init(void)
 		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
 		memset(ti, 0, THREAD_SIZE);
 		emerg_stack_init_thread_info(ti, i);
-		paca[i].emergency_sp = (void *)ti + THREAD_SIZE;
+		paca_ptrs[i]->emergency_sp = (void *)ti + THREAD_SIZE;
 
 #ifdef CONFIG_PPC_BOOK3S_64
 		/* emergency stack for NMI exception handling. */
 		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
 		memset(ti, 0, THREAD_SIZE);
 		emerg_stack_init_thread_info(ti, i);
-		paca[i].nmi_emergency_sp = (void *)ti + THREAD_SIZE;
+		paca_ptrs[i]->nmi_emergency_sp = (void *)ti + THREAD_SIZE;
 
 		/* emergency stack for machine check exception handling. */
 		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
 		memset(ti, 0, THREAD_SIZE);
 		emerg_stack_init_thread_info(ti, i);
-		paca[i].mc_emergency_sp = (void *)ti + THREAD_SIZE;
+		paca_ptrs[i]->mc_emergency_sp = (void *)ti + THREAD_SIZE;
 #endif
 	}
 }
@@ -762,7 +762,7 @@ void __init setup_per_cpu_areas(void)
 	delta = (unsigned long)pcpu_base_addr - (unsigned long)__per_cpu_start;
 	for_each_possible_cpu(cpu) {
                 __per_cpu_offset[cpu] = delta + pcpu_unit_offsets[cpu];
-		paca[cpu].data_offset = __per_cpu_offset[cpu];
+		paca_ptrs[cpu]->data_offset = __per_cpu_offset[cpu];
 	}
 }
 #endif
@@ -875,8 +875,9 @@ static void init_fallback_flush(void)
 	memset(l1d_flush_fallback_area, 0, l1d_size * 2);
 
 	for_each_possible_cpu(cpu) {
-		paca[cpu].rfi_flush_fallback_area = l1d_flush_fallback_area;
-		paca[cpu].l1d_flush_size = l1d_size;
+		struct paca_struct *paca = paca_ptrs[cpu];
+		paca->rfi_flush_fallback_area = l1d_flush_fallback_area;
+		paca->l1d_flush_size = l1d_size;
 	}
 }
 

commit 8ad33041563a10b34988800c682ada14b2612533
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Mar 27 23:01:48 2018 +1100

    powerpc/64s: Move cpu_show_meltdown()
    
    This landed in setup_64.c for no good reason other than we had nowhere
    else to put it. Now that we have a security-related file, that is a
    better place for it so move it.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4ec4a27b36a9..7f7621668613 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -934,12 +934,4 @@ static __init int rfi_flush_debugfs_init(void)
 }
 device_initcall(rfi_flush_debugfs_init);
 #endif
-
-ssize_t cpu_show_meltdown(struct device *dev, struct device_attribute *attr, char *buf)
-{
-	if (rfi_flush)
-		return sprintf(buf, "Mitigation: RFI Flush\n");
-
-	return sprintf(buf, "Vulnerable\n");
-}
 #endif /* CONFIG_PPC_BOOK3S_64 */

commit 0063d61ccfc011f379a31acaeba6de7c926fed2c
Author: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
Date:   Wed Mar 14 19:40:41 2018 -0300

    powerpc/rfi-flush: Differentiate enabled and patched flush types
    
    Currently the rfi-flush messages print 'Using <type> flush' for all
    enabled_flush_types, but that is not necessarily true -- as now the
    fallback flush is always enabled on pseries, but the fixup function
    overwrites its nop/branch slot with other flush types, if available.
    
    So, replace the 'Using <type> flush' messages with '<type> flush is
    available'.
    
    Also, print the patched flush types in the fixup function, so users
    can know what is (not) being used (e.g., the slower, fallback flush,
    or no flush type at all if flush is disabled via the debugfs switch).
    
    Suggested-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index d60e2f7eff1b..4ec4a27b36a9 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -884,15 +884,15 @@ static void init_fallback_flush(void)
 void setup_rfi_flush(enum l1d_flush_type types, bool enable)
 {
 	if (types & L1D_FLUSH_FALLBACK) {
-		pr_info("rfi-flush: Using fallback displacement flush\n");
+		pr_info("rfi-flush: fallback displacement flush available\n");
 		init_fallback_flush();
 	}
 
 	if (types & L1D_FLUSH_ORI)
-		pr_info("rfi-flush: Using ori type flush\n");
+		pr_info("rfi-flush: ori type flush available\n");
 
 	if (types & L1D_FLUSH_MTTRIG)
-		pr_info("rfi-flush: Using mttrig type flush\n");
+		pr_info("rfi-flush: mttrig type flush available\n");
 
 	enabled_flush_types = types;
 

commit abf110f3e1cea40f5ea15e85f5d67c39c14568a7
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Mar 14 19:40:39 2018 -0300

    powerpc/rfi-flush: Make it possible to call setup_rfi_flush() again
    
    For PowerVM migration we want to be able to call setup_rfi_flush()
    again after we've migrated the partition.
    
    To support that we need to check that we're not trying to allocate the
    fallback flush area after memblock has gone away (i.e., boot-time only).
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 3efc01a570e8..d60e2f7eff1b 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -860,6 +860,10 @@ static void init_fallback_flush(void)
 	u64 l1d_size, limit;
 	int cpu;
 
+	/* Only allocate the fallback flush area once (at boot time). */
+	if (l1d_flush_fallback_area)
+		return;
+
 	l1d_size = ppc64_caches.l1d.size;
 	limit = min(ppc64_bolted_size(), ppc64_rma_size);
 
@@ -877,7 +881,7 @@ static void init_fallback_flush(void)
 	}
 }
 
-void __init setup_rfi_flush(enum l1d_flush_type types, bool enable)
+void setup_rfi_flush(enum l1d_flush_type types, bool enable)
 {
 	if (types & L1D_FLUSH_FALLBACK) {
 		pr_info("rfi-flush: Using fallback displacement flush\n");

commit 1e2a9fc7496955faacbbed49461d611b704a7505
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Mar 14 19:40:38 2018 -0300

    powerpc/rfi-flush: Move the logic to avoid a redo into the debugfs code
    
    rfi_flush_enable() includes a check to see if we're already
    enabled (or disabled), and in that case does nothing.
    
    But that means calling setup_rfi_flush() a 2nd time doesn't actually
    work, which is a bit confusing.
    
    Move that check into the debugfs code, where it really belongs.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index c388cc3357fa..3efc01a570e8 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -846,9 +846,6 @@ static void do_nothing(void *unused)
 
 void rfi_flush_enable(bool enable)
 {
-	if (rfi_flush == enable)
-		return;
-
 	if (enable) {
 		do_rfi_flush_fixups(enabled_flush_types);
 		on_each_cpu(do_nothing, NULL, 1);
@@ -902,13 +899,19 @@ void __init setup_rfi_flush(enum l1d_flush_type types, bool enable)
 #ifdef CONFIG_DEBUG_FS
 static int rfi_flush_set(void *data, u64 val)
 {
+	bool enable;
+
 	if (val == 1)
-		rfi_flush_enable(true);
+		enable = true;
 	else if (val == 0)
-		rfi_flush_enable(false);
+		enable = false;
 	else
 		return -EINVAL;
 
+	/* Only do anything if we're changing state */
+	if (enable != rfi_flush)
+		rfi_flush_enable(enable);
+
 	return 0;
 }
 

commit bdcb1aefc5b3f7d0f1dc8b02673602bca2ff7a4b
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Jan 17 23:58:18 2018 +1000

    powerpc/64s: Improve RFI L1-D cache flush fallback
    
    The fallback RFI flush is used when firmware does not provide a way
    to flush the cache. It's a "displacement flush" that evicts useful
    data by displacing it with an uninteresting buffer.
    
    The flush has to take care to work with implementation specific cache
    replacment policies, so the recipe has been in flux. The initial
    slow but conservative approach is to touch all lines of a congruence
    class, with dependencies between each load. It has since been
    determined that a linear pattern of loads without dependencies is
    sufficient, and is significantly faster.
    
    Measuring the speed of a null syscall with RFI fallback flush enabled
    gives the relative improvement:
    
    P8 - 1.83x
    P9 - 1.75x
    
    The flush also becomes simpler and more adaptable to different cache
    geometries.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index d1fa0e91f526..c388cc3357fa 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -875,19 +875,8 @@ static void init_fallback_flush(void)
 	memset(l1d_flush_fallback_area, 0, l1d_size * 2);
 
 	for_each_possible_cpu(cpu) {
-		/*
-		 * The fallback flush is currently coded for 8-way
-		 * associativity. Different associativity is possible, but it
-		 * will be treated as 8-way and may not evict the lines as
-		 * effectively.
-		 *
-		 * 128 byte lines are mandatory.
-		 */
-		u64 c = l1d_size / 8;
-
 		paca[cpu].rfi_flush_fallback_area = l1d_flush_fallback_area;
-		paca[cpu].l1d_flush_congruence = c;
-		paca[cpu].l1d_flush_sets = c / 128;
+		paca[cpu].l1d_flush_size = l1d_size;
 	}
 }
 

commit ebf0b6a8b1e445d2be66087732aafcda12ab9f59
Merge: 5400fc229e60 1b689a95ce74
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sun Jan 21 23:21:14 2018 +1100

    Merge branch 'fixes' into next
    
    Merge our fixes branch from the 4.15 cycle.
    
    Unusually the fixes branch saw some significant features merged,
    notably the RFI flush patches, so we want the code in next to be
    tested against that, to avoid any surprises when the two are merged.
    
    There's also some other work on the panic handling that was reverted
    in fixes and we now want to do properly in next, which would conflict.
    
    And we also fix a few other minor merge conflicts.

commit 4e26bc4a4ed683c42ba45f09050575a671c6f1f4
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:50 2017 +0530

    powerpc/64: Rename soft_enabled to irq_soft_mask
    
    Rename the paca->soft_enabled to paca->irq_soft_mask as it is no
    longer used as a flag for interrupt state, but a mask.
    
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f6bedebda90a..896dacef2f2d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -189,7 +189,7 @@ static void __init fixup_boot_paca(void)
 	/* Allow percpu accesses to work until we setup percpu data */
 	get_paca()->data_offset = 0;
 	/* Mark interrupts disabled in PACA */
-	soft_enabled_set(IRQS_DISABLED);
+	irq_soft_mask_set(IRQS_DISABLED);
 }
 
 static void __init configure_exceptions(void)
@@ -352,7 +352,7 @@ void __init early_setup(unsigned long dt_ptr)
 void early_setup_secondary(void)
 {
 	/* Mark interrupts disabled in PACA */
-	soft_enabled_set(IRQS_DISABLED);
+	irq_soft_mask_set(IRQS_DISABLED);
 
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu_secondary();

commit 0b63acf4a0eb8843f83954ea1bd29ccdfcbaa778
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:45 2017 +0530

    powerpc/64: Move set_soft_enabled() and rename
    
    Move set_soft_enabled() from powerpc/kernel/irq.c to asm/hw_irq.c, to
    encourage updates to paca->soft_enabled done via these access
    function. Add "memory" clobber to hint compiler since
    paca->soft_enabled memory is the target here.
    
    Renaming it as soft_enabled_set() will make namespaces works better as
    prefix than a postfix when new soft_enabled manipulation functions are
    introduced.
    
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 2fd4e167ef09..f6bedebda90a 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -189,7 +189,7 @@ static void __init fixup_boot_paca(void)
 	/* Allow percpu accesses to work until we setup percpu data */
 	get_paca()->data_offset = 0;
 	/* Mark interrupts disabled in PACA */
-	get_paca()->soft_enabled = IRQS_DISABLED;
+	soft_enabled_set(IRQS_DISABLED);
 }
 
 static void __init configure_exceptions(void)
@@ -352,7 +352,7 @@ void __init early_setup(unsigned long dt_ptr)
 void early_setup_secondary(void)
 {
 	/* Mark interrupts disabled in PACA */
-	get_paca()->soft_enabled = 0;
+	soft_enabled_set(IRQS_DISABLED);
 
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu_secondary();

commit c2e480ba822718190e58849b79a76db13c3dac18
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:42 2017 +0530

    powerpc/64: Add #defines for paca->soft_enabled flags
    
    Two #defines IRQS_ENABLED and IRQS_DISABLED are added to be used when
    updating paca->soft_enabled. Replace the hardcoded values used when
    updating paca->soft_enabled with IRQ_(EN|DIS)ABLED #define. No logic
    change.
    
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 8f285d6a3db1..2fd4e167ef09 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -66,6 +66,7 @@
 #include <asm/livepatch.h>
 #include <asm/opal.h>
 #include <asm/cputhreads.h>
+#include <asm/hw_irq.h>
 
 #include "setup.h"
 
@@ -187,6 +188,8 @@ static void __init fixup_boot_paca(void)
 	get_paca()->cpu_start = 1;
 	/* Allow percpu accesses to work until we setup percpu data */
 	get_paca()->data_offset = 0;
+	/* Mark interrupts disabled in PACA */
+	get_paca()->soft_enabled = IRQS_DISABLED;
 }
 
 static void __init configure_exceptions(void)

commit 1af19331a3a18296a918802dbe032a13328e264d
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Dec 22 21:17:13 2017 +1000

    powerpc/64s: Relax PACA address limitations
    
    Book3S PACA memory allocation is restricted by the RMA limit and also
    must not take SLB faults when accessed in virtual mode. Currently a
    fixed 256MB limit is used for this, which is imprecise and sub-optimal.
    
    Update the paca allocation limits to use use the ppc64_rma_size for RMA
    limit, and share the safe_stack_limit() that is currently used for stack
    allocations that must not take virtual mode faults.
    
    The safe_stack_limit() name is changed to ppc64_bolted_size() to match
    ppc64_rma_size and some comments are updated. We also need to use
    early_mmu_has_feature() because we are now calling this function prior
    to the jump label patching that enables mmu_has_feature().
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Change mmu_has_feature() to early_mmu_has_feature()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index d3124c302146..8f285d6a3db1 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -565,25 +565,31 @@ void __init initialize_cache_info(void)
 	DBG(" <- initialize_cache_info()\n");
 }
 
-/* This returns the limit below which memory accesses to the linear
- * mapping are guarnateed not to cause a TLB or SLB miss. This is
- * used to allocate interrupt or emergency stacks for which our
- * exception entry path doesn't deal with being interrupted.
+/*
+ * This returns the limit below which memory accesses to the linear
+ * mapping are guarnateed not to cause an architectural exception (e.g.,
+ * TLB or SLB miss fault).
+ *
+ * This is used to allocate PACAs and various interrupt stacks that
+ * that are accessed early in interrupt handlers that must not cause
+ * re-entrant interrupts.
  */
-static __init u64 safe_stack_limit(void)
+__init u64 ppc64_bolted_size(void)
 {
 #ifdef CONFIG_PPC_BOOK3E
 	/* Freescale BookE bolts the entire linear mapping */
-	if (mmu_has_feature(MMU_FTR_TYPE_FSL_E))
+	/* XXX: BookE ppc64_rma_limit setup seems to disagree? */
+	if (early_mmu_has_feature(MMU_FTR_TYPE_FSL_E))
 		return linear_map_top;
 	/* Other BookE, we assume the first GB is bolted */
 	return 1ul << 30;
 #else
+	/* BookS radix, does not take faults on linear mapping */
 	if (early_radix_enabled())
 		return ULONG_MAX;
 
-	/* BookS, the first segment is bolted */
-	if (mmu_has_feature(MMU_FTR_1T_SEGMENT))
+	/* BookS hash, the first segment is bolted */
+	if (early_mmu_has_feature(MMU_FTR_1T_SEGMENT))
 		return 1UL << SID_SHIFT_1T;
 	return 1UL << SID_SHIFT;
 #endif
@@ -591,7 +597,7 @@ static __init u64 safe_stack_limit(void)
 
 void __init irqstack_early_init(void)
 {
-	u64 limit = safe_stack_limit();
+	u64 limit = ppc64_bolted_size();
 	unsigned int i;
 
 	/*
@@ -676,7 +682,7 @@ void __init emergency_stack_init(void)
 	 * initialized in kernel/irq.c. These are initialized here in order
 	 * to have emergency stacks available as early as possible.
 	 */
-	limit = min(safe_stack_limit(), ppc64_rma_size);
+	limit = min(ppc64_bolted_size(), ppc64_rma_size);
 
 	for_each_possible_cpu(i) {
 		struct thread_info *ti;

commit 236003e6b5443c45c18e613d2b0d776a9f87540e
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jan 16 22:17:18 2018 +1100

    powerpc/64s: Allow control of RFI flush via debugfs
    
    Expose the state of the RFI flush (enabled/disabled) via debugfs, and
    allow it to be enabled/disabled at runtime.
    
    eg: $ cat /sys/kernel/debug/powerpc/rfi_flush
        1
        $ echo 0 > /sys/kernel/debug/powerpc/rfi_flush
        $ cat /sys/kernel/debug/powerpc/rfi_flush
        0
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 624d2a62d05d..e67413f4a8f0 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -38,6 +38,7 @@
 #include <linux/memory.h>
 #include <linux/nmi.h>
 
+#include <asm/debugfs.h>
 #include <asm/io.h>
 #include <asm/kdump.h>
 #include <asm/prom.h>
@@ -902,6 +903,35 @@ void __init setup_rfi_flush(enum l1d_flush_type types, bool enable)
 		rfi_flush_enable(enable);
 }
 
+#ifdef CONFIG_DEBUG_FS
+static int rfi_flush_set(void *data, u64 val)
+{
+	if (val == 1)
+		rfi_flush_enable(true);
+	else if (val == 0)
+		rfi_flush_enable(false);
+	else
+		return -EINVAL;
+
+	return 0;
+}
+
+static int rfi_flush_get(void *data, u64 *val)
+{
+	*val = rfi_flush ? 1 : 0;
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(fops_rfi_flush, rfi_flush_get, rfi_flush_set, "%llu\n");
+
+static __init int rfi_flush_debugfs_init(void)
+{
+	debugfs_create_file("rfi_flush", 0600, powerpc_debugfs_root, NULL, &fops_rfi_flush);
+	return 0;
+}
+device_initcall(rfi_flush_debugfs_init);
+#endif
+
 ssize_t cpu_show_meltdown(struct device *dev, struct device_attribute *attr, char *buf)
 {
 	if (rfi_flush)

commit fd6e440f20b1a4304553775fc55938848ff617c9
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jan 16 21:20:05 2018 +1100

    powerpc/64s: Wire up cpu_show_meltdown()
    
    The recent commit 87590ce6e373 ("sysfs/cpu: Add vulnerability folder")
    added a generic folder and set of files for reporting information on
    CPU vulnerabilities. One of those was for meltdown:
    
      /sys/devices/system/cpu/vulnerabilities/meltdown
    
    This commit wires up that file for 64-bit Book3S powerpc.
    
    For now we default to "Vulnerable" unless the RFI flush is enabled.
    That may not actually be true on all hardware, further patches will
    refine the reporting based on the CPU/platform etc. But for now we
    default to being pessimists.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 491be4179ddd..624d2a62d05d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -901,4 +901,12 @@ void __init setup_rfi_flush(enum l1d_flush_type types, bool enable)
 	if (!no_rfi_flush)
 		rfi_flush_enable(enable);
 }
+
+ssize_t cpu_show_meltdown(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	if (rfi_flush)
+		return sprintf(buf, "Mitigation: RFI Flush\n");
+
+	return sprintf(buf, "Vulnerable\n");
+}
 #endif /* CONFIG_PPC_BOOK3S_64 */

commit bc9c9304a45480797e13a8e1df96ffcf44fb62fe
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jan 10 03:07:15 2018 +1100

    powerpc/64s: Support disabling RFI flush with no_rfi_flush and nopti
    
    Because there may be some performance overhead of the RFI flush, add
    kernel command line options to disable it.
    
    We add a sensibly named 'no_rfi_flush' option, but we also hijack the
    x86 option 'nopti'. The RFI flush is not the same as KPTI, but if we
    see 'nopti' we can guess that the user is trying to avoid any overhead
    of Meltdown mitigations, and it means we don't have to educate every
    one about a different command line option.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 96163f4c3673..491be4179ddd 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -805,8 +805,29 @@ early_initcall(disable_hardlockup_detector);
 #ifdef CONFIG_PPC_BOOK3S_64
 static enum l1d_flush_type enabled_flush_types;
 static void *l1d_flush_fallback_area;
+static bool no_rfi_flush;
 bool rfi_flush;
 
+static int __init handle_no_rfi_flush(char *p)
+{
+	pr_info("rfi-flush: disabled on command line.");
+	no_rfi_flush = true;
+	return 0;
+}
+early_param("no_rfi_flush", handle_no_rfi_flush);
+
+/*
+ * The RFI flush is not KPTI, but because users will see doco that says to use
+ * nopti we hijack that option here to also disable the RFI flush.
+ */
+static int __init handle_no_pti(char *p)
+{
+	pr_info("rfi-flush: disabling due to 'nopti' on command line.\n");
+	handle_no_rfi_flush(NULL);
+	return 0;
+}
+early_param("nopti", handle_no_pti);
+
 static void do_nothing(void *unused)
 {
 	/*
@@ -877,6 +898,7 @@ void __init setup_rfi_flush(enum l1d_flush_type types, bool enable)
 
 	enabled_flush_types = types;
 
-	rfi_flush_enable(enable);
+	if (!no_rfi_flush)
+		rfi_flush_enable(enable);
 }
 #endif /* CONFIG_PPC_BOOK3S_64 */

commit aa8a5e0062ac940f7659394f4817c948dc8c0667
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jan 10 03:07:15 2018 +1100

    powerpc/64s: Add support for RFI flush of L1-D cache
    
    On some CPUs we can prevent the Meltdown vulnerability by flushing the
    L1-D cache on exit from kernel to user mode, and from hypervisor to
    guest.
    
    This is known to be the case on at least Power7, Power8 and Power9. At
    this time we do not know the status of the vulnerability on other CPUs
    such as the 970 (Apple G5), pasemi CPUs (AmigaOne X1000) or Freescale
    CPUs. As more information comes to light we can enable this, or other
    mechanisms on those CPUs.
    
    The vulnerability occurs when the load of an architecturally
    inaccessible memory region (eg. userspace load of kernel memory) is
    speculatively executed to the point where its result can influence the
    address of a subsequent speculatively executed load.
    
    In order for that to happen, the first load must hit in the L1,
    because before the load is sent to the L2 the permission check is
    performed. Therefore if no kernel addresses hit in the L1 the
    vulnerability can not occur. We can ensure that is the case by
    flushing the L1 whenever we return to userspace. Similarly for
    hypervisor vs guest.
    
    In order to flush the L1-D cache on exit, we add a section of nops at
    each (h)rfi location that returns to a lower privileged context, and
    patch that with some sequence. Newer firmwares are able to advertise
    to us that there is a special nop instruction that flushes the L1-D.
    If we do not see that advertised, we fall back to doing a displacement
    flush in software.
    
    For guest kernels we support migration between some CPU versions, and
    different CPUs may use different flush instructions. So that we are
    prepared to migrate to a machine with a different flush instruction
    activated, we may have to patch more than one flush instruction at
    boot if the hypervisor tells us to.
    
    In the end this patch is mostly the work of Nicholas Piggin and
    Michael Ellerman. However a cast of thousands contributed to analysis
    of the issue, earlier versions of the patch, back ports testing etc.
    Many thanks to all of them.
    
    Tested-by: Jon Masters <jcm@redhat.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 8956a9856604..96163f4c3673 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -801,3 +801,82 @@ static int __init disable_hardlockup_detector(void)
 	return 0;
 }
 early_initcall(disable_hardlockup_detector);
+
+#ifdef CONFIG_PPC_BOOK3S_64
+static enum l1d_flush_type enabled_flush_types;
+static void *l1d_flush_fallback_area;
+bool rfi_flush;
+
+static void do_nothing(void *unused)
+{
+	/*
+	 * We don't need to do the flush explicitly, just enter+exit kernel is
+	 * sufficient, the RFI exit handlers will do the right thing.
+	 */
+}
+
+void rfi_flush_enable(bool enable)
+{
+	if (rfi_flush == enable)
+		return;
+
+	if (enable) {
+		do_rfi_flush_fixups(enabled_flush_types);
+		on_each_cpu(do_nothing, NULL, 1);
+	} else
+		do_rfi_flush_fixups(L1D_FLUSH_NONE);
+
+	rfi_flush = enable;
+}
+
+static void init_fallback_flush(void)
+{
+	u64 l1d_size, limit;
+	int cpu;
+
+	l1d_size = ppc64_caches.l1d.size;
+	limit = min(safe_stack_limit(), ppc64_rma_size);
+
+	/*
+	 * Align to L1d size, and size it at 2x L1d size, to catch possible
+	 * hardware prefetch runoff. We don't have a recipe for load patterns to
+	 * reliably avoid the prefetcher.
+	 */
+	l1d_flush_fallback_area = __va(memblock_alloc_base(l1d_size * 2, l1d_size, limit));
+	memset(l1d_flush_fallback_area, 0, l1d_size * 2);
+
+	for_each_possible_cpu(cpu) {
+		/*
+		 * The fallback flush is currently coded for 8-way
+		 * associativity. Different associativity is possible, but it
+		 * will be treated as 8-way and may not evict the lines as
+		 * effectively.
+		 *
+		 * 128 byte lines are mandatory.
+		 */
+		u64 c = l1d_size / 8;
+
+		paca[cpu].rfi_flush_fallback_area = l1d_flush_fallback_area;
+		paca[cpu].l1d_flush_congruence = c;
+		paca[cpu].l1d_flush_sets = c / 128;
+	}
+}
+
+void __init setup_rfi_flush(enum l1d_flush_type types, bool enable)
+{
+	if (types & L1D_FLUSH_FALLBACK) {
+		pr_info("rfi-flush: Using fallback displacement flush\n");
+		init_fallback_flush();
+	}
+
+	if (types & L1D_FLUSH_ORI)
+		pr_info("rfi-flush: Using ori type flush\n");
+
+	if (types & L1D_FLUSH_MTTRIG)
+		pr_info("rfi-flush: Using mttrig type flush\n");
+
+	enabled_flush_types = types;
+
+	rfi_flush_enable(enable);
+}
+#endif /* CONFIG_PPC_BOOK3S_64 */

commit 10de741fd322c31f6c4f07869f4c6f0fa64b5bfe
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Nov 22 17:01:05 2017 +1100

    powerpc: Remove DEBUG define in 64-bit early setup code
    
    This statement causes some not very useful messages to always
    be printed on the serial port at boot, even on quiet boots.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 8956a9856604..d3124c302146 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -10,8 +10,6 @@
  *      2 of the License, or (at your option) any later version.
  */
 
-#define DEBUG
-
 #include <linux/export.h>
 #include <linux/string.h>
 #include <linux/sched.h>

commit 1696d0fb7fcd18160c9cc92a3f2b2d68e6923dd8
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Oct 24 21:44:44 2017 +1000

    powerpc/64: Set DSCR default initially from SPR
    
    Take the DSCR value set by firmware as the dscr_default value,
    rather than zero.
    
    POWER9 recommends DSCR default to a non-zero value.
    
    Signed-off-by: From: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Make record_spr_defaults() __init]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 87b4fe75fd45..8956a9856604 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -69,6 +69,8 @@
 #include <asm/opal.h>
 #include <asm/cputhreads.h>
 
+#include "setup.h"
+
 #ifdef DEBUG
 #define DBG(fmt...) udbg_printf(fmt)
 #else
@@ -316,6 +318,13 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu();
 
+	/*
+	 * After firmware and early platform setup code has set things up,
+	 * we note the SPR values for configurable control/performance
+	 * registers, and use those as initial defaults.
+	 */
+	record_spr_defaults();
+
 	/*
 	 * At this point, we can let interrupts switch to virtual mode
 	 * (the MMU has been setup), so adjust the MSR in the PACA to

commit 339a3293f4e493a6c40f71e4faab0c8389174313
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Mon Oct 23 18:05:07 2017 +1000

    powerpc/powernv: Avoid waiting for secondary hold spinloop with OPAL
    
    OPAL boot does not insert secondaries at 0x60 to wait at the secondary
    hold spinloop. Instead they are started later, and inserted at
    generic_secondary_smp_init(), which is after the secondary hold
    spinloop.
    
    Avoid waiting on this spinloop when booting with OPAL firmware. This
    wait always times out that case.
    
    This saves 100ms boot time on powernv, and 10s of seconds of real time
    when booting on the simulator in SMP.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index b89c6aac48c9..87b4fe75fd45 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -360,8 +360,16 @@ void early_setup_secondary(void)
 #if defined(CONFIG_SMP) || defined(CONFIG_KEXEC_CORE)
 static bool use_spinloop(void)
 {
-	if (!IS_ENABLED(CONFIG_PPC_BOOK3E))
+	if (IS_ENABLED(CONFIG_PPC_BOOK3S)) {
+		/*
+		 * See comments in head_64.S -- not all platforms insert
+		 * secondaries at __secondary_hold and wait at the spin
+		 * loop.
+		 */
+		if (firmware_has_feature(FW_FEATURE_OPAL))
+			return false;
 		return true;
+	}
 
 	/*
 	 * When book3e boots from kexec, the ePAPR spin table does

commit 70412c55d419e971785094e9f7880fdbcd690520
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Mon Aug 28 14:27:19 2017 +1000

    powerpc/64: Fix watchdog configuration regressions
    
    This fixes a couple more bits of fallout from the new hard lockup watchdog
    patch.
    
    It restores the required hw_nmi_get_sample_period() function for the
    perf watchdog, and removes some function declarations on 64e that are only
    defined for 64s. This fixes the 64e build when the hardlockup detector is
    enabled.
    
    It restores the default behaviour of disabling the perf watchdog, and also
    fixes disabling the 64s watchdog when running as a guest.
    
    Fixes: 2104180a53 ("powerpc/64s: implement arch-specific hardlockup watchdog")
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 7393bac3c7f4..b89c6aac48c9 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -756,3 +756,31 @@ unsigned long memory_block_size_bytes(void)
 struct ppc_pci_io ppc_pci_io;
 EXPORT_SYMBOL(ppc_pci_io);
 #endif
+
+#ifdef CONFIG_HARDLOCKUP_DETECTOR_PERF
+u64 hw_nmi_get_sample_period(int watchdog_thresh)
+{
+	return ppc_proc_freq * watchdog_thresh;
+}
+#endif
+
+/*
+ * The perf based hardlockup detector breaks PMU event based branches, so
+ * disable it by default. Book3S has a soft-nmi hardlockup detector based
+ * on the decrementer interrupt, so it does not suffer from this problem.
+ *
+ * It is likely to get false positives in VM guests, so disable it there
+ * by default too.
+ */
+static int __init disable_hardlockup_detector(void)
+{
+#ifdef CONFIG_HARDLOCKUP_DETECTOR_PERF
+	hardlockup_detector_disable();
+#else
+	if (firmware_has_feature(FW_FEATURE_LPAR))
+		hardlockup_detector_disable();
+#endif
+
+	return 0;
+}
+early_initcall(disable_hardlockup_detector);

commit d55071905ee1719094c66dd3c40e2a9ef5c65eaf
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sun Aug 13 11:33:41 2017 +1000

    powerpc/64s/radix: Remove bolted-SLB address limit for per-cpu stacks
    
    Radix MMU does not take SLB or TLB interrupts when accessing kernel
    linear address. Remove this restriction for radix mode.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index af23d4b576ec..7393bac3c7f4 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -564,6 +564,9 @@ static __init u64 safe_stack_limit(void)
 	/* Other BookE, we assume the first GB is bolted */
 	return 1ul << 30;
 #else
+	if (early_radix_enabled())
+		return ULONG_MAX;
+
 	/* BookS, the first segment is bolted */
 	if (mmu_has_feature(MMU_FTR_1T_SEGMENT))
 		return 1UL << SID_SHIFT_1T;
@@ -578,7 +581,8 @@ void __init irqstack_early_init(void)
 
 	/*
 	 * Interrupt stacks must be in the first segment since we
-	 * cannot afford to take SLB misses on them.
+	 * cannot afford to take SLB misses on them. They are not
+	 * accessed in realmode.
 	 */
 	for_each_possible_cpu(i) {
 		softirq_ctx[i] = (struct thread_info *)
@@ -649,8 +653,9 @@ void __init emergency_stack_init(void)
 	 * aligned.
 	 *
 	 * Since we use these as temporary stacks during secondary CPU
-	 * bringup, we need to get at them in real mode. This means they
-	 * must also be within the RMO region.
+	 * bringup, machine check, system reset, and HMI, we need to get
+	 * at them in real mode. This means they must also be within the RMO
+	 * region.
 	 *
 	 * The IRQ stacks allocated elsewhere in this file are zeroed and
 	 * initialized in kernel/irq.c. These are initialized here in order

commit 2104180a53698df5aec35aed5f840a26ade0551d
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Jul 12 14:35:52 2017 -0700

    powerpc/64s: implement arch-specific hardlockup watchdog
    
    Implement an arch-speicfic watchdog rather than use the perf-based
    hardlockup detector.
    
    The new watchdog takes the soft-NMI directly, rather than going through
    perf.  Perf interrupts are to be made maskable in future, so that would
    prevent the perf detector from working in those regions.
    
    Additionally, implement a SMP based detector where all CPUs watch one
    another by pinging a shared cpumask.  This is because powerpc Book3S
    does not have a true periodic local NMI, but some platforms do implement
    a true NMI IPI.
    
    If a CPU is stuck with interrupts hard disabled, the soft-NMI watchdog
    does not work, but the SMP watchdog will.  Even on platforms without a
    true NMI IPI to get a good trace from the stuck CPU, other CPUs will
    notice the lockup sufficiently to report it and panic.
    
    [npiggin@gmail.com: honor watchdog disable at boot/hotplug]
      Link: http://lkml.kernel.org/r/20170621001346.5bb337c9@roar.ozlabs.ibm.com
    [npiggin@gmail.com: fix false positive warning at CPU unplug]
      Link: http://lkml.kernel.org/r/20170630080740.20766-1-npiggin@gmail.com
    [akpm@linux-foundation.org: coding-style fixes]
    Link: http://lkml.kernel.org/r/20170616065715.18390-6-npiggin@gmail.com
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Tested-by: Babu Moger <babu.moger@oracle.com>   [sparc]
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 074a075a9cdb..af23d4b576ec 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -751,22 +751,3 @@ unsigned long memory_block_size_bytes(void)
 struct ppc_pci_io ppc_pci_io;
 EXPORT_SYMBOL(ppc_pci_io);
 #endif
-
-#ifdef CONFIG_HARDLOCKUP_DETECTOR_PERF
-u64 hw_nmi_get_sample_period(int watchdog_thresh)
-{
-	return ppc_proc_freq * watchdog_thresh;
-}
-
-/*
- * The hardlockup detector breaks PMU event based branches and is likely
- * to get false positives in KVM guests, so disable it by default.
- */
-static int __init disable_hardlockup_detector(void)
-{
-	hardlockup_detector_disable();
-
-	return 0;
-}
-early_initcall(disable_hardlockup_detector);
-#endif

commit 05a4a95279311c3a4633b4277a5d21cfd616c6c7
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Jul 12 14:35:46 2017 -0700

    kernel/watchdog: split up config options
    
    Split SOFTLOCKUP_DETECTOR from LOCKUP_DETECTOR, and split
    HARDLOCKUP_DETECTOR_PERF from HARDLOCKUP_DETECTOR.
    
    LOCKUP_DETECTOR implies the general boot, sysctl, and programming
    interfaces for the lockup detectors.
    
    An architecture that wants to use a hard lockup detector must define
    HAVE_HARDLOCKUP_DETECTOR_PERF or HAVE_HARDLOCKUP_DETECTOR_ARCH.
    
    Alternatively an arch can define HAVE_NMI_WATCHDOG, which provides the
    minimum arch_touch_nmi_watchdog, and it otherwise does its own thing and
    does not implement the LOCKUP_DETECTOR interfaces.
    
    sparc is unusual in that it has started to implement some of the
    interfaces, but not fully yet.  It should probably be converted to a full
    HAVE_HARDLOCKUP_DETECTOR_ARCH.
    
    [npiggin@gmail.com: fix]
      Link: http://lkml.kernel.org/r/20170617223522.66c0ad88@roar.ozlabs.ibm.com
    Link: http://lkml.kernel.org/r/20170616065715.18390-4-npiggin@gmail.com
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Reviewed-by: Babu Moger <babu.moger@oracle.com>
    Tested-by: Babu Moger <babu.moger@oracle.com>   [sparc]
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4640f6d64f8b..074a075a9cdb 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -752,7 +752,7 @@ struct ppc_pci_io ppc_pci_io;
 EXPORT_SYMBOL(ppc_pci_io);
 #endif
 
-#ifdef CONFIG_HARDLOCKUP_DETECTOR
+#ifdef CONFIG_HARDLOCKUP_DETECTOR_PERF
 u64 hw_nmi_get_sample_period(int watchdog_thresh)
 {
 	return ppc_proc_freq * watchdog_thresh;

commit 34f19ff1b5a0d11e46df479623d6936460105c9f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Jun 21 15:58:29 2017 +1000

    powerpc/64: Initialise thread_info for emergency stacks
    
    Emergency stacks have their thread_info mostly uninitialised, which in
    particular means garbage preempt_count values.
    
    Emergency stack code runs with interrupts disabled entirely, and is
    used very rarely, so this has been unnoticed so far. It was found by a
    proposed new powerpc watchdog that takes a soft-NMI directly from the
    masked_interrupt handler and using the emergency stack. That crashed
    at BUG_ON(in_nmi()) in nmi_enter(). preempt_count()s were found to be
    garbage.
    
    To fix this, zero the entire THREAD_SIZE allocation, and initialize
    the thread_info.
    
    Cc: stable@vger.kernel.org
    Reported-by: Abdul Haleem <abdhalee@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Move it all into setup_64.c, use a function not a macro. Fix
          crashes on Cell by setting preempt_count to 0 not HARDIRQ_OFFSET]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index a8c1f99e9607..4640f6d64f8b 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -615,6 +615,24 @@ void __init exc_lvl_early_init(void)
 }
 #endif
 
+/*
+ * Emergency stacks are used for a range of things, from asynchronous
+ * NMIs (system reset, machine check) to synchronous, process context.
+ * We set preempt_count to zero, even though that isn't necessarily correct. To
+ * get the right value we'd need to copy it from the previous thread_info, but
+ * doing that might fault causing more problems.
+ * TODO: what to do with accounting?
+ */
+static void emerg_stack_init_thread_info(struct thread_info *ti, int cpu)
+{
+	ti->task = NULL;
+	ti->cpu = cpu;
+	ti->preempt_count = 0;
+	ti->local_flags = 0;
+	ti->flags = 0;
+	klp_init_thread_info(ti);
+}
+
 /*
  * Stack space used when we detect a bad kernel stack pointer, and
  * early in SMP boots before relocation is enabled. Exclusive emergency
@@ -633,24 +651,31 @@ void __init emergency_stack_init(void)
 	 * Since we use these as temporary stacks during secondary CPU
 	 * bringup, we need to get at them in real mode. This means they
 	 * must also be within the RMO region.
+	 *
+	 * The IRQ stacks allocated elsewhere in this file are zeroed and
+	 * initialized in kernel/irq.c. These are initialized here in order
+	 * to have emergency stacks available as early as possible.
 	 */
 	limit = min(safe_stack_limit(), ppc64_rma_size);
 
 	for_each_possible_cpu(i) {
 		struct thread_info *ti;
 		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
-		klp_init_thread_info(ti);
+		memset(ti, 0, THREAD_SIZE);
+		emerg_stack_init_thread_info(ti, i);
 		paca[i].emergency_sp = (void *)ti + THREAD_SIZE;
 
 #ifdef CONFIG_PPC_BOOK3S_64
 		/* emergency stack for NMI exception handling. */
 		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
-		klp_init_thread_info(ti);
+		memset(ti, 0, THREAD_SIZE);
+		emerg_stack_init_thread_info(ti, i);
 		paca[i].nmi_emergency_sp = (void *)ti + THREAD_SIZE;
 
 		/* emergency stack for machine check exception handling. */
 		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
-		klp_init_thread_info(ti);
+		memset(ti, 0, THREAD_SIZE);
+		emerg_stack_init_thread_info(ti, i);
 		paca[i].mc_emergency_sp = (void *)ti + THREAD_SIZE;
 #endif
 	}

commit ba4a648f12f4cd0a8003dd229b6ca8a53348ee4b
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jun 6 20:23:57 2017 +1000

    powerpc/numa: Fix percpu allocations to be NUMA aware
    
    In commit 8c272261194d ("powerpc/numa: Enable USE_PERCPU_NUMA_NODE_ID"), we
    switched to the generic implementation of cpu_to_node(), which uses a percpu
    variable to hold the NUMA node for each CPU.
    
    Unfortunately we neglected to notice that we use cpu_to_node() in the allocation
    of our percpu areas, leading to a chicken and egg problem. In practice what
    happens is when we are setting up the percpu areas, cpu_to_node() reports that
    all CPUs are on node 0, so we allocate all percpu areas on node 0.
    
    This is visible in the dmesg output, as all pcpu allocs being in group 0:
    
      pcpu-alloc: [0] 00 01 02 03 [0] 04 05 06 07
      pcpu-alloc: [0] 08 09 10 11 [0] 12 13 14 15
      pcpu-alloc: [0] 16 17 18 19 [0] 20 21 22 23
      pcpu-alloc: [0] 24 25 26 27 [0] 28 29 30 31
      pcpu-alloc: [0] 32 33 34 35 [0] 36 37 38 39
      pcpu-alloc: [0] 40 41 42 43 [0] 44 45 46 47
    
    To fix it we need an early_cpu_to_node() which can run prior to percpu being
    setup. We already have the numa_cpu_lookup_table we can use, so just plumb it
    in. With the patch dmesg output shows two groups, 0 and 1:
    
      pcpu-alloc: [0] 00 01 02 03 [0] 04 05 06 07
      pcpu-alloc: [0] 08 09 10 11 [0] 12 13 14 15
      pcpu-alloc: [0] 16 17 18 19 [0] 20 21 22 23
      pcpu-alloc: [1] 24 25 26 27 [1] 28 29 30 31
      pcpu-alloc: [1] 32 33 34 35 [1] 36 37 38 39
      pcpu-alloc: [1] 40 41 42 43 [1] 44 45 46 47
    
    We can also check the data_offset in the paca of various CPUs, with the fix we
    see:
    
      CPU 0:  data_offset = 0x0ffe8b0000
      CPU 24: data_offset = 0x1ffe5b0000
    
    And we can see from dmesg that CPU 24 has an allocation on node 1:
    
      node   0: [mem 0x0000000000000000-0x0000000fffffffff]
      node   1: [mem 0x0000001000000000-0x0000001fffffffff]
    
    Cc: stable@vger.kernel.org # v3.16+
    Fixes: 8c272261194d ("powerpc/numa: Enable USE_PERCPU_NUMA_NODE_ID")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f35ff9dea4fb..a8c1f99e9607 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -661,7 +661,7 @@ void __init emergency_stack_init(void)
 
 static void * __init pcpu_fc_alloc(unsigned int cpu, size_t size, size_t align)
 {
-	return __alloc_bootmem_node(NODE_DATA(cpu_to_node(cpu)), size, align,
+	return __alloc_bootmem_node(NODE_DATA(early_cpu_to_node(cpu)), size, align,
 				    __pa(MAX_DMA_ADDRESS));
 }
 
@@ -672,7 +672,7 @@ static void __init pcpu_fc_free(void *ptr, size_t size)
 
 static int pcpu_cpu_distance(unsigned int from, unsigned int to)
 {
-	if (cpu_to_node(from) == cpu_to_node(to))
+	if (early_cpu_to_node(from) == early_cpu_to_node(to))
 		return LOCAL_DISTANCE;
 	else
 		return REMOTE_DISTANCE;

commit dc2a24816637ff6c60f08c4245aba01c6e9b6a79
Merge: ac3c4aa248c5 5a61ef74f269
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 12 10:04:09 2017 -0700

    Merge tag 'powerpc-4.12-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull more powerpc updates from Michael Ellerman:
     "The change to the Linux page table geometry was delayed for more
      testing with 16G pages, and there's the new CPU features stuff which
      just needed one more polish before going in. Plus a few changes from
      Scott which came in a bit late. And then various fixes, mostly minor.
    
      Summary highlights:
    
       - rework the Linux page table geometry to lower memory usage on
         64-bit Book3S (IBM chips) using the Hash MMU.
    
       - support for a new device tree binding for discovering CPU features
         on future firmwares.
    
       - Freescale updates from Scott:
          "Includes a fix for a powerpc/next mm regression on 64e, a fix for
           a kernel hang on 64e when using a debugger inside a relocated
           kernel, a qman fix, and misc qe improvements."
    
      Thanks to: Christophe Leroy, Gavin Shan, Horia Geantă, LiuHailong,
      Nicholas Piggin, Roy Pledge, Scott Wood, Valentin Longchamp"
    
    * tag 'powerpc-4.12-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:
      powerpc/64s: Support new device tree binding for discovering CPU features
      powerpc: Don't print cpu_spec->cpu_name if it's NULL
      of/fdt: introduce of_scan_flat_dt_subnodes and of_get_flat_dt_phandle
      powerpc/64s: Fix unnecessary machine check handler relocation branch
      powerpc/mm/book3s/64: Rework page table geometry for lower memory usage
      powerpc: Fix distclean with Makefile.postlink
      powerpc/64e: Don't place the stack beyond TASK_SIZE
      powerpc/powernv: Block PCI config access on BCM5718 during EEH recovery
      powerpc/8xx: Adding support of IRQ in MPC8xx GPIO
      soc/fsl/qbman: Disable IRQs for deferred QBMan work
      soc/fsl/qe: add EXPORT_SYMBOL for the 2 qe_tdm functions
      soc/fsl/qe: only apply QE_General4 workaround on affected SoCs
      soc/fsl/qe: round brg_freq to 1kHz granularity
      soc/fsl/qe: get rid of immrbar_virt_to_phys()
      net: ethernet: ucc_geth: fix MEM_PART_MURAM mode
      powerpc/64e: Fix hang when debugging programs with relocated kernel

commit 5a61ef74f269f2573f48fa53607a8911216c3326
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue May 9 13:16:52 2017 +1000

    powerpc/64s: Support new device tree binding for discovering CPU features
    
    The ibm,powerpc-cpu-features device tree binding describes CPU features with
    ASCII names and extensible compatibility, privilege, and enablement metadata
    that allows improved flexibility and compatibility with new hardware.
    
    The interface is described in detail in ibm,powerpc-cpu-features.txt in this
    patch.
    
    Currently this code is not enabled by default, and there are no released
    firmwares that provide the binding.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 0f7b15860a06..1bf8978ec8da 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -49,6 +49,7 @@
 #include <asm/paca.h>
 #include <asm/time.h>
 #include <asm/cputable.h>
+#include <asm/dt_cpu_ftrs.h>
 #include <asm/sections.h>
 #include <asm/btext.h>
 #include <asm/nvram.h>
@@ -265,8 +266,10 @@ void __init early_setup(unsigned long dt_ptr)
 
 	/* -------- printk is _NOT_ safe to use here ! ------- */
 
-	/* Identify CPU type */
-	identify_cpu(0, mfspr(SPRN_PVR));
+	/* Try new device tree based feature discovery ... */
+	if (!dt_cpu_ftrs_init(__va(dt_ptr)))
+		/* Otherwise use the old style CPU table */
+		identify_cpu(0, mfspr(SPRN_PVR));
 
 	/* Assume we're on cpu 0 for now. Don't write to the paca yet! */
 	initialise_paca(&boot_paca, 0);
@@ -532,6 +535,9 @@ void __init initialize_cache_info(void)
 	dcache_bsize = ppc64_caches.l1d.block_size;
 	icache_bsize = ppc64_caches.l1i.block_size;
 
+	cur_cpu_spec->dcache_bsize = dcache_bsize;
+	cur_cpu_spec->icache_bsize = icache_bsize;
+
 	DBG(" <- initialize_cache_info()\n");
 }
 

commit 7246f60068840847bdcf595be5f0b5ca632736e0
Merge: e579dde654fc 700b7eadd562
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 5 11:36:44 2017 -0700

    Merge tag 'powerpc-4.12-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Larger virtual address space on 64-bit server CPUs. By default we
         use a 128TB virtual address space, but a process can request access
         to the full 512TB by passing a hint to mmap().
    
       - Support for the new Power9 "XIVE" interrupt controller.
    
       - TLB flushing optimisations for the radix MMU on Power9.
    
       - Support for CAPI cards on Power9, using the "Coherent Accelerator
         Interface Architecture 2.0".
    
       - The ability to configure the mmap randomisation limits at build and
         runtime.
    
       - Several small fixes and cleanups to the kprobes code, as well as
         support for KPROBES_ON_FTRACE.
    
       - Major improvements to handling of system reset interrupts,
         correctly treating them as NMIs, giving them a dedicated stack and
         using a new hypervisor call to trigger them, all of which should
         aid debugging and robustness.
    
       - Many fixes and other minor enhancements.
    
      Thanks to: Alastair D'Silva, Alexey Kardashevskiy, Alistair Popple,
      Andrew Donnellan, Aneesh Kumar K.V, Anshuman Khandual, Anton
      Blanchard, Balbir Singh, Ben Hutchings, Benjamin Herrenschmidt,
      Bhupesh Sharma, Chris Packham, Christian Zigotzky, Christophe Leroy,
      Christophe Lombard, Daniel Axtens, David Gibson, Gautham R. Shenoy,
      Gavin Shan, Geert Uytterhoeven, Guilherme G. Piccoli, Hamish Martin,
      Hari Bathini, Kees Cook, Laurent Dufour, Madhavan Srinivasan, Mahesh J
      Salgaonkar, Mahesh Salgaonkar, Masami Hiramatsu, Matt Brown, Matthew
      R. Ochs, Michael Neuling, Naveen N. Rao, Nicholas Piggin, Oliver
      O'Halloran, Pan Xinhui, Paul Mackerras, Rashmica Gupta, Russell
      Currey, Sukadev Bhattiprolu, Thadeu Lima de Souza Cascardo, Tobin C.
      Harding, Tyrel Datwyler, Uma Krishnan, Vaibhav Jain, Vipin K Parashar,
      Yang Shi"
    
    * tag 'powerpc-4.12-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (214 commits)
      powerpc/64s: Power9 has no LPCR[VRMASD] field so don't set it
      powerpc/powernv: Fix TCE kill on NVLink2
      powerpc/mm/radix: Drop support for CPUs without lockless tlbie
      powerpc/book3s/mce: Move add_taint() later in virtual mode
      powerpc/sysfs: Move #ifdef CONFIG_HOTPLUG_CPU out of the function body
      powerpc/smp: Document irq enable/disable after migrating IRQs
      powerpc/mpc52xx: Don't select user-visible RTAS_PROC
      powerpc/powernv: Document cxl dependency on special case in pnv_eeh_reset()
      powerpc/eeh: Clean up and document event handling functions
      powerpc/eeh: Avoid use after free in eeh_handle_special_event()
      cxl: Mask slice error interrupts after first occurrence
      cxl: Route eeh events to all drivers in cxl_pci_error_detected()
      cxl: Force context lock during EEH flow
      powerpc/64: Allow CONFIG_RELOCATABLE if COMPILE_TEST
      powerpc/xmon: Teach xmon oops about radix vectors
      powerpc/mm/hash: Fix off-by-one in comment about kernel contexts ids
      powerpc/pseries: Enable VFIO
      powerpc/powernv: Fix iommu table size calculation hook for small tables
      powerpc/powernv: Check kzalloc() return value in pnv_pci_table_alloc
      powerpc: Add arch/powerpc/tools directory
      ...

commit b1ee8a3de5790777f325416ad97340428d8ae25f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Dec 20 04:30:06 2016 +1000

    powerpc/64s: Dedicated system reset interrupt stack
    
    The system reset interrupt is used for crash/debug situations, so it is
    desirable to have as little impact on the normal state of the system as
    possible.
    
    Currently it uses the current kernel stack to process the exception.
    This stores into the stack which may be involved with the crash. The
    stack pointer may be corrupted, or it may have overflowed.
    
    Avoid or minimise these problems by creating a dedicated NMI stack for
    the system reset interrupt to use.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 729e990a019d..0f7b15860a06 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -628,6 +628,11 @@ void __init emergency_stack_init(void)
 		paca[i].emergency_sp = (void *)ti + THREAD_SIZE;
 
 #ifdef CONFIG_PPC_BOOK3S_64
+		/* emergency stack for NMI exception handling. */
+		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
+		klp_init_thread_info(ti);
+		paca[i].nmi_emergency_sp = (void *)ti + THREAD_SIZE;
+
 		/* emergency stack for machine check exception handling. */
 		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
 		klp_init_thread_info(ti);

commit 7ed23e1bae8bf7e37fd555066550a00b95a3a98b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Mar 20 17:49:03 2017 +1100

    powerpc: Disable HFSCR[TM] if TM is not supported
    
    On Power8 & Power9 the early CPU inititialisation in __init_HFSCR()
    turns on HFSCR[TM] (Hypervisor Facility Status and Control Register
    [Transactional Memory]), but that doesn't take into account that TM
    might be disabled by CPU features, or disabled by the kernel being built
    with CONFIG_PPC_TRANSACTIONAL_MEM=n.
    
    So later in boot, when we have setup the CPU features, clear HSCR[TM] if
    the TM CPU feature has been disabled. We use CPU_FTR_TM_COMP to account
    for the CONFIG_PPC_TRANSACTIONAL_MEM=n case.
    
    Without this a KVM guest might try use TM, even if told not to, and
    cause an oops in the host kernel. Typically the oops is seen in
    __kvmppc_vcore_entry() and may or may not be fatal to the host, but is
    always bad news.
    
    In practice all shipping CPU revisions do support TM, and all host
    kernels we are aware of build with TM support enabled, so no one should
    actually be able to hit this in the wild.
    
    Fixes: 2a3563b023e5 ("powerpc: Setup in HFSCR for POWER8")
    Cc: stable@vger.kernel.org # v3.10+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Tested-by: Sam Bobroff <sam.bobroff@au1.ibm.com>
    [mpe: Rewrite change log with input from Sam, add Fixes/stable]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 9cfaa8b69b5f..f997154dfc41 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -236,6 +236,15 @@ static void cpu_ready_for_interrupts(void)
 		mtspr(SPRN_LPCR, lpcr | LPCR_AIL_3);
 	}
 
+	/*
+	 * Fixup HFSCR:TM based on CPU features. The bit is set by our
+	 * early asm init because at that point we haven't updated our
+	 * CPU features from firmware and device-tree. Here we have,
+	 * so let's do it.
+	 */
+	if (cpu_has_feature(CPU_FTR_HVMODE) && !cpu_has_feature(CPU_FTR_TM_COMP))
+		mtspr(SPRN_HFSCR, mfspr(SPRN_HFSCR) & ~HFSCR_TM);
+
 	/* Set IR and DR in PACA MSR */
 	get_paca()->kernel_msr = MSR_KERNEL;
 }

commit 5511a45fc134f0784c403ef3488e2c07cd15bf14
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Mar 21 16:24:38 2017 +1100

    powerpc/64: Don't use early_cpu_has_feature() in cpu_ready_for_interrupts()
    
    cpu_ready_for_interrupts() is called after feature patching, so there's
    no need to use early_cpu_has_feature().
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 9cfaa8b69b5f..729e990a019d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -230,8 +230,8 @@ static void cpu_ready_for_interrupts(void)
 	 * If we are not in hypervisor mode the job is done once for
 	 * the whole partition in configure_exceptions().
 	 */
-	if (early_cpu_has_feature(CPU_FTR_HVMODE) &&
-	    early_cpu_has_feature(CPU_FTR_ARCH_207S)) {
+	if (cpu_has_feature(CPU_FTR_HVMODE) &&
+	    cpu_has_feature(CPU_FTR_ARCH_207S)) {
 		unsigned long lpcr = mfspr(SPRN_LPCR);
 		mtspr(SPRN_LPCR, lpcr | LPCR_AIL_3);
 	}

commit 6ba422c75facb1b1e0e206c464ee121b8073f7e0
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Mar 5 10:54:34 2017 +1100

    powerpc/64: Avoid panic during boot due to divide by zero in init_cache_info()
    
    I see a panic in early boot when building with a recent gcc toolchain.
    The issue is a divide by zero, which is undefined. Older toolchains
    let us get away with it:
    
    int foo(int a) { return a / 0; }
    
    foo:
            li 9,0
            divw 3,3,9
            extsw 3,3
            blr
    
    But newer ones catch it:
    
    foo:
            trap
    
    Add a check to avoid the divide by zero.
    
    Fixes: e2827fe5c156 ("powerpc/64: Clean up ppc64_caches using a struct per cache")
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index adf2084f214b..9cfaa8b69b5f 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -408,7 +408,10 @@ static void init_cache_info(struct ppc_cache_info *info, u32 size, u32 lsize,
 	info->line_size = lsize;
 	info->block_size = bsize;
 	info->log_block_size = __ilog2(bsize);
-	info->blocks_per_page = PAGE_SIZE / bsize;
+	if (bsize)
+		info->blocks_per_page = PAGE_SIZE / bsize;
+	else
+		info->blocks_per_page = 0;
 
 	if (sets == 0)
 		info->assoc = 0xffff;

commit 0d2b5cdc76ff7eef9d727c2e802213ec15d5e94f
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Feb 15 20:24:25 2017 +1100

    powerpc/64e: Fix bogus usage of WARN_ONCE()
    
    WARN_ONCE() takes a condition and a format string. We were passing a
    constant string as the condition, and the function name as the format
    string. It would work, but the message would be just the function name.
    
    Fix it by just using WARN_ONCE() directly instead of if (x) WARN_ONCE().
    
    Noticed-by: Geliang Tang <geliangtang@163.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index b9855f1b290a..adf2084f214b 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -113,14 +113,12 @@ void __init setup_tlb_core_data(void)
 		 * If we have threads, we need either tlbsrx.
 		 * or e6500 tablewalk mode, or else TLB handlers
 		 * will be racy and could produce duplicate entries.
+		 * Should we panic instead?
 		 */
-		if (smt_enabled_at_boot >= 2 &&
-		    !mmu_has_feature(MMU_FTR_USE_TLBRSRV) &&
-		    book3e_htw_mode != PPC_HTW_E6500) {
-			/* Should we panic instead? */
-			WARN_ONCE("%s: unsupported MMU configuration -- expect problems\n",
-				  __func__);
-		}
+		WARN_ONCE(smt_enabled_at_boot >= 2 &&
+			  !mmu_has_feature(MMU_FTR_USE_TLBRSRV) &&
+			  book3e_htw_mode != PPC_HTW_E6500,
+			  "%s: unsupported MMU configuration\n", __func__);
 	}
 }
 #endif

commit 98a5f361b8625c6f4841d6ba013bbf0e80d08147
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Feb 3 17:20:07 2017 +1100

    powerpc: Add new cache geometry aux vectors
    
    This adds AUX vectors for the L1I,D, L2 and L3 cache levels
    providing for each cache level the size of the cache in bytes
    and the geometry (line size and number of ways).
    
    We chose to not use the existing alpha/sh definition which
    packs all the information in a single entry per cache level as
    it is too restricted to represent some of the geometries used
    on POWER.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 364fbffd7e83..b9855f1b290a 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -411,6 +411,11 @@ static void init_cache_info(struct ppc_cache_info *info, u32 size, u32 lsize,
 	info->block_size = bsize;
 	info->log_block_size = __ilog2(bsize);
 	info->blocks_per_page = PAGE_SIZE / bsize;
+
+	if (sets == 0)
+		info->assoc = 0xffff;
+	else
+		info->assoc = size / (sets * lsize);
 }
 
 static bool __init parse_cache_info(struct device_node *np,

commit 608b42140e966a65cabc68d997875065f3e63c2f
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:49 2017 -0600

    powerpc/64: Hard code cache geometry on POWER8
    
    All shipping firmware versions have it wrong in the device-tree
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index db18f7b68a1d..364fbffd7e83 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -472,11 +472,27 @@ static bool __init parse_cache_info(struct device_node *np,
 
 void __init initialize_cache_info(void)
 {
-	struct device_node *cpu, *l2, *l3 = NULL;
+	struct device_node *cpu = NULL, *l2, *l3 = NULL;
+	u32 pvr;
 
 	DBG(" -> initialize_cache_info()\n");
 
-	cpu = of_find_node_by_type(NULL, "cpu");
+	/*
+	 * All shipping POWER8 machines have a firmware bug that
+	 * puts incorrect information in the device-tree. This will
+	 * be (hopefully) fixed for future chips but for now hard
+	 * code the values if we are running on one of these
+	 */
+	pvr = PVR_VER(mfspr(SPRN_PVR));
+	if (pvr == PVR_POWER8 || pvr == PVR_POWER8E ||
+	    pvr == PVR_POWER8NVL) {
+						/* size    lsize   blk  sets */
+		init_cache_info(&ppc64_caches.l1i, 0x8000,   128,  128, 32);
+		init_cache_info(&ppc64_caches.l1d, 0x10000,  128,  128, 64);
+		init_cache_info(&ppc64_caches.l2,  0x80000,  128,  0,   512);
+		init_cache_info(&ppc64_caches.l3,  0x800000, 128,  0,   8192);
+	} else
+		cpu = of_find_node_by_type(NULL, "cpu");
 
 	/*
 	 * We're assuming *all* of the CPUs have the same

commit 65e01f386fcddb3460be78fc886856889f80ecc7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:48 2017 -0600

    powerpc/64: Add L2 and L3 cache shape info
    
    Retrieved from device-tree when available
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index b87dcb2968d9..db18f7b68a1d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -472,22 +472,38 @@ static bool __init parse_cache_info(struct device_node *np,
 
 void __init initialize_cache_info(void)
 {
-	struct device_node *np;
+	struct device_node *cpu, *l2, *l3 = NULL;
 
 	DBG(" -> initialize_cache_info()\n");
 
-	np  = of_find_node_by_type(NULL, "cpu");
+	cpu = of_find_node_by_type(NULL, "cpu");
 
 	/*
 	 * We're assuming *all* of the CPUs have the same
 	 * d-cache and i-cache sizes... -Peter
 	 */
-	if (np) {
-		if (!parse_cache_info(np, false, &ppc64_caches.l1d))
+	if (cpu) {
+		if (!parse_cache_info(cpu, false, &ppc64_caches.l1d))
 			DBG("Argh, can't find dcache properties !\n");
 
-		if (!parse_cache_info(np, true, &ppc64_caches.l1i))
+		if (!parse_cache_info(cpu, true, &ppc64_caches.l1i))
 			DBG("Argh, can't find icache properties !\n");
+
+		/*
+		 * Try to find the L2 and L3 if any. Assume they are
+		 * unified and use the D-side properties.
+		 */
+		l2 = of_find_next_cache_node(cpu);
+		of_node_put(cpu);
+		if (l2) {
+			parse_cache_info(l2, false, &ppc64_caches.l2);
+			l3 = of_find_next_cache_node(l2);
+			of_node_put(l2);
+		}
+		if (l3) {
+			parse_cache_info(l3, false, &ppc64_caches.l3);
+			of_node_put(l3);
+		}
 	}
 
 	/* For use by binfmt_elf */

commit e2827fe5c1566f66a922dd7493cbe4522c50580a
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:47 2017 -0600

    powerpc/64: Clean up ppc64_caches using a struct per cache
    
    We have two set of identical struct members for the I and D sides
    and mostly identical bunches of code to parse the device-tree to
    populate them. Instead make a ppc_cache_info structure with one
    copy for I and one for D
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 75c9a8641ba1..b87dcb2968d9 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -78,10 +78,14 @@ int spinning_secondaries;
 u64 ppc64_pft_size;
 
 struct ppc64_caches ppc64_caches = {
-	.dblock_size = 0x40,
-	.log_dblock_size = 6,
-	.iblock_size = 0x40,
-	.log_iblock_size = 6
+	.l1d = {
+		.block_size = 0x40,
+		.log_block_size = 6,
+	},
+	.l1i = {
+		.block_size = 0x40,
+		.log_block_size = 6
+	},
 };
 EXPORT_SYMBOL_GPL(ppc64_caches);
 
@@ -397,105 +401,98 @@ void smp_release_cpus(void)
  * cache informations about the CPU that will be used by cache flush
  * routines and/or provided to userland
  */
+
+static void init_cache_info(struct ppc_cache_info *info, u32 size, u32 lsize,
+			    u32 bsize, u32 sets)
+{
+	info->size = size;
+	info->sets = sets;
+	info->line_size = lsize;
+	info->block_size = bsize;
+	info->log_block_size = __ilog2(bsize);
+	info->blocks_per_page = PAGE_SIZE / bsize;
+}
+
+static bool __init parse_cache_info(struct device_node *np,
+				    bool icache,
+				    struct ppc_cache_info *info)
+{
+	static const char *ipropnames[] __initdata = {
+		"i-cache-size",
+		"i-cache-sets",
+		"i-cache-block-size",
+		"i-cache-line-size",
+	};
+	static const char *dpropnames[] __initdata = {
+		"d-cache-size",
+		"d-cache-sets",
+		"d-cache-block-size",
+		"d-cache-line-size",
+	};
+	const char **propnames = icache ? ipropnames : dpropnames;
+	const __be32 *sizep, *lsizep, *bsizep, *setsp;
+	u32 size, lsize, bsize, sets;
+	bool success = true;
+
+	size = 0;
+	sets = -1u;
+	lsize = bsize = cur_cpu_spec->dcache_bsize;
+	sizep = of_get_property(np, propnames[0], NULL);
+	if (sizep != NULL)
+		size = be32_to_cpu(*sizep);
+	setsp = of_get_property(np, propnames[1], NULL);
+	if (setsp != NULL)
+		sets = be32_to_cpu(*setsp);
+	bsizep = of_get_property(np, propnames[2], NULL);
+	lsizep = of_get_property(np, propnames[3], NULL);
+	if (bsizep == NULL)
+		bsizep = lsizep;
+	if (lsizep != NULL)
+		lsize = be32_to_cpu(*lsizep);
+	if (bsizep != NULL)
+		bsize = be32_to_cpu(*bsizep);
+	if (sizep == NULL || bsizep == NULL || lsizep == NULL)
+		success = false;
+
+	/*
+	 * OF is weird .. it represents fully associative caches
+	 * as "1 way" which doesn't make much sense and doesn't
+	 * leave room for direct mapped. We'll assume that 0
+	 * in OF means direct mapped for that reason.
+	 */
+	if (sets == 1)
+		sets = 0;
+	else if (sets == 0)
+		sets = 1;
+
+	init_cache_info(info, size, lsize, bsize, sets);
+
+	return success;
+}
+
 void __init initialize_cache_info(void)
 {
 	struct device_node *np;
-	unsigned long num_cpus = 0;
 
 	DBG(" -> initialize_cache_info()\n");
 
-	for_each_node_by_type(np, "cpu") {
-		num_cpus += 1;
+	np  = of_find_node_by_type(NULL, "cpu");
 
-		/*
-		 * We're assuming *all* of the CPUs have the same
-		 * d-cache and i-cache sizes... -Peter
-		 */
-		if (num_cpus == 1) {
-			const __be32 *sizep, *lsizep, *bsizep, *setsp;
-			u32 size, lsize, bsize, sets;
-
-			size = 0;
-			sets = -1u;
-			lsize = bsize = cur_cpu_spec->dcache_bsize;
-			sizep = of_get_property(np, "d-cache-size", NULL);
-			if (sizep != NULL)
-				size = be32_to_cpu(*sizep);
-			setsp = of_get_property(np, "d-cache-sets", NULL);
-			if (setsp != NULL)
-				sets = be32_to_cpu(*setsp);
-			bsizep = of_get_property(np, "d-cache-block-size",
-						 NULL);
-			lsizep = of_get_property(np, "d-cache-line-size",
-						 NULL);
-			if (bsizep == NULL)
-				bsizep = lsizep;
-			if (lsizep != NULL)
-				lsize = be32_to_cpu(*lsizep);
-			if (bsizep != NULL)
-				bsize = be32_to_cpu(*bsizep);
-			if (sizep == NULL || bsizep == NULL || lsizep == NULL)
-				DBG("Argh, can't find dcache properties ! "
-				    "sizep: %p, bsizep: %p, lsizep: %p\n",
-				    sizep, bsizep, lsizep);
-
-			/*
-			 * OF is weird .. it represents fully associative caches
-			 * as "1 way" which doesn't make much sense and doesn't
-			 * leave room for direct mapped. We'll assume that 0
-			 * in OF means direct mapped for that reason.
-			 */
-			if (sets == 1)
-				sets = 0;
-			else if (sets == 0)
-				sets = 1;
-			ppc64_caches.dsize = size;
-			ppc64_caches.dsets = sets;
-			ppc64_caches.dline_size = lsize;
-			ppc64_caches.dblock_size = bsize;
-			ppc64_caches.log_dblock_size = __ilog2(bsize);
-			ppc64_caches.dblocks_per_page = PAGE_SIZE / bsize;
-
-			size = 0;
-			sets = -1u;
-			lsize = bsize = cur_cpu_spec->icache_bsize;
-			sizep = of_get_property(np, "i-cache-size", NULL);
-			if (sizep != NULL)
-				size = be32_to_cpu(*sizep);
-			setsp = of_get_property(np, "i-cache-sets", NULL);
-			if (setsp != NULL)
-				sets = be32_to_cpu(*setsp);
-			bsizep = of_get_property(np, "i-cache-block-size",
-						 NULL);
-			lsizep = of_get_property(np, "i-cache-line-size",
-						 NULL);
-			if (bsizep == NULL)
-				bsizep = lsizep;
-			if (lsizep != NULL)
-				lsize = be32_to_cpu(*lsizep);
-			if (bsizep != NULL)
-				bsize = be32_to_cpu(*bsizep);
-			if (sizep == NULL || bsizep == NULL || lsizep == NULL)
-				DBG("Argh, can't find icache properties ! "
-				    "sizep: %p, bsizep: %p, lsizep: %p\n",
-				    sizep, bsizep, lsizep);
-
-			if (sets == 1)
-				sets = 0;
-			else if (sets == 0)
-				sets = 1;
-			ppc64_caches.isize = size;
-			ppc64_caches.isets = sets;
-			ppc64_caches.iline_size = lsize;
-			ppc64_caches.iblock_size = bsize;
-			ppc64_caches.log_iblock_size = __ilog2(bsize);
-			ppc64_caches.iblocks_per_page = PAGE_SIZE / bsize;
-		}
+	/*
+	 * We're assuming *all* of the CPUs have the same
+	 * d-cache and i-cache sizes... -Peter
+	 */
+	if (np) {
+		if (!parse_cache_info(np, false, &ppc64_caches.l1d))
+			DBG("Argh, can't find dcache properties !\n");
+
+		if (!parse_cache_info(np, true, &ppc64_caches.l1i))
+			DBG("Argh, can't find icache properties !\n");
 	}
 
 	/* For use by binfmt_elf */
-	dcache_bsize = ppc64_caches.dblock_size;
-	icache_bsize = ppc64_caches.iblock_size;
+	dcache_bsize = ppc64_caches.l1d.block_size;
+	icache_bsize = ppc64_caches.l1i.block_size;
 
 	DBG(" <- initialize_cache_info()\n");
 }

commit 5d451a87e5ebbde18c2b48284778f29d308816c2
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:46 2017 -0600

    powerpc/64: Retrieve number of L1 cache sets from device-tree
    
    It will be used to calculate the associativity
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 08cccb2501e7..75c9a8641ba1 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -412,14 +412,18 @@ void __init initialize_cache_info(void)
 		 * d-cache and i-cache sizes... -Peter
 		 */
 		if (num_cpus == 1) {
-			const __be32 *sizep, *lsizep, *bsizep;
-			u32 size, lsize, bsize;
+			const __be32 *sizep, *lsizep, *bsizep, *setsp;
+			u32 size, lsize, bsize, sets;
 
 			size = 0;
+			sets = -1u;
 			lsize = bsize = cur_cpu_spec->dcache_bsize;
 			sizep = of_get_property(np, "d-cache-size", NULL);
 			if (sizep != NULL)
 				size = be32_to_cpu(*sizep);
+			setsp = of_get_property(np, "d-cache-sets", NULL);
+			if (setsp != NULL)
+				sets = be32_to_cpu(*setsp);
 			bsizep = of_get_property(np, "d-cache-block-size",
 						 NULL);
 			lsizep = of_get_property(np, "d-cache-line-size",
@@ -435,17 +439,32 @@ void __init initialize_cache_info(void)
 				    "sizep: %p, bsizep: %p, lsizep: %p\n",
 				    sizep, bsizep, lsizep);
 
+			/*
+			 * OF is weird .. it represents fully associative caches
+			 * as "1 way" which doesn't make much sense and doesn't
+			 * leave room for direct mapped. We'll assume that 0
+			 * in OF means direct mapped for that reason.
+			 */
+			if (sets == 1)
+				sets = 0;
+			else if (sets == 0)
+				sets = 1;
 			ppc64_caches.dsize = size;
+			ppc64_caches.dsets = sets;
 			ppc64_caches.dline_size = lsize;
 			ppc64_caches.dblock_size = bsize;
 			ppc64_caches.log_dblock_size = __ilog2(bsize);
 			ppc64_caches.dblocks_per_page = PAGE_SIZE / bsize;
 
 			size = 0;
+			sets = -1u;
 			lsize = bsize = cur_cpu_spec->icache_bsize;
 			sizep = of_get_property(np, "i-cache-size", NULL);
 			if (sizep != NULL)
 				size = be32_to_cpu(*sizep);
+			setsp = of_get_property(np, "i-cache-sets", NULL);
+			if (setsp != NULL)
+				sets = be32_to_cpu(*setsp);
 			bsizep = of_get_property(np, "i-cache-block-size",
 						 NULL);
 			lsizep = of_get_property(np, "i-cache-line-size",
@@ -461,7 +480,12 @@ void __init initialize_cache_info(void)
 				    "sizep: %p, bsizep: %p, lsizep: %p\n",
 				    sizep, bsizep, lsizep);
 
+			if (sets == 1)
+				sets = 0;
+			else if (sets == 0)
+				sets = 1;
 			ppc64_caches.isize = size;
+			ppc64_caches.isets = sets;
 			ppc64_caches.iline_size = lsize;
 			ppc64_caches.iblock_size = bsize;
 			ppc64_caches.log_iblock_size = __ilog2(bsize);

commit bd067f83b0840e798328d14133ce4542d3bf9e71
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:45 2017 -0600

    powerpc/64: Fix naming of cache block vs. cache line
    
    In a number of places we called "cache line size" what is actually
    the cache block size, which in the powerpc architecture, means the
    effective size to use with cache management instructions (it can
    be different from the actual cache line size).
    
    We fix the naming across the board and properly retrieve both
    pieces of information when available in the device-tree.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ae84d345c13c..08cccb2501e7 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -78,10 +78,10 @@ int spinning_secondaries;
 u64 ppc64_pft_size;
 
 struct ppc64_caches ppc64_caches = {
-	.dline_size = 0x40,
-	.log_dline_size = 6,
-	.iline_size = 0x40,
-	.log_iline_size = 6
+	.dblock_size = 0x40,
+	.log_dblock_size = 6,
+	.iblock_size = 0x40,
+	.log_iblock_size = 6
 };
 EXPORT_SYMBOL_GPL(ppc64_caches);
 
@@ -412,59 +412,66 @@ void __init initialize_cache_info(void)
 		 * d-cache and i-cache sizes... -Peter
 		 */
 		if (num_cpus == 1) {
-			const __be32 *sizep, *lsizep;
-			u32 size, lsize;
+			const __be32 *sizep, *lsizep, *bsizep;
+			u32 size, lsize, bsize;
 
 			size = 0;
-			lsize = cur_cpu_spec->dcache_bsize;
+			lsize = bsize = cur_cpu_spec->dcache_bsize;
 			sizep = of_get_property(np, "d-cache-size", NULL);
 			if (sizep != NULL)
 				size = be32_to_cpu(*sizep);
-			lsizep = of_get_property(np, "d-cache-block-size",
+			bsizep = of_get_property(np, "d-cache-block-size",
 						 NULL);
-			/* fallback if block size missing */
-			if (lsizep == NULL)
-				lsizep = of_get_property(np,
-							 "d-cache-line-size",
-							 NULL);
+			lsizep = of_get_property(np, "d-cache-line-size",
+						 NULL);
+			if (bsizep == NULL)
+				bsizep = lsizep;
 			if (lsizep != NULL)
 				lsize = be32_to_cpu(*lsizep);
-			if (sizep == NULL || lsizep == NULL)
+			if (bsizep != NULL)
+				bsize = be32_to_cpu(*bsizep);
+			if (sizep == NULL || bsizep == NULL || lsizep == NULL)
 				DBG("Argh, can't find dcache properties ! "
-				    "sizep: %p, lsizep: %p\n", sizep, lsizep);
+				    "sizep: %p, bsizep: %p, lsizep: %p\n",
+				    sizep, bsizep, lsizep);
 
 			ppc64_caches.dsize = size;
 			ppc64_caches.dline_size = lsize;
-			ppc64_caches.log_dline_size = __ilog2(lsize);
-			ppc64_caches.dlines_per_page = PAGE_SIZE / lsize;
+			ppc64_caches.dblock_size = bsize;
+			ppc64_caches.log_dblock_size = __ilog2(bsize);
+			ppc64_caches.dblocks_per_page = PAGE_SIZE / bsize;
 
 			size = 0;
-			lsize = cur_cpu_spec->icache_bsize;
+			lsize = bsize = cur_cpu_spec->icache_bsize;
 			sizep = of_get_property(np, "i-cache-size", NULL);
 			if (sizep != NULL)
 				size = be32_to_cpu(*sizep);
-			lsizep = of_get_property(np, "i-cache-block-size",
+			bsizep = of_get_property(np, "i-cache-block-size",
+						 NULL);
+			lsizep = of_get_property(np, "i-cache-line-size",
 						 NULL);
-			if (lsizep == NULL)
-				lsizep = of_get_property(np,
-							 "i-cache-line-size",
-							 NULL);
+			if (bsizep == NULL)
+				bsizep = lsizep;
 			if (lsizep != NULL)
 				lsize = be32_to_cpu(*lsizep);
-			if (sizep == NULL || lsizep == NULL)
+			if (bsizep != NULL)
+				bsize = be32_to_cpu(*bsizep);
+			if (sizep == NULL || bsizep == NULL || lsizep == NULL)
 				DBG("Argh, can't find icache properties ! "
-				    "sizep: %p, lsizep: %p\n", sizep, lsizep);
+				    "sizep: %p, bsizep: %p, lsizep: %p\n",
+				    sizep, bsizep, lsizep);
 
 			ppc64_caches.isize = size;
 			ppc64_caches.iline_size = lsize;
-			ppc64_caches.log_iline_size = __ilog2(lsize);
-			ppc64_caches.ilines_per_page = PAGE_SIZE / lsize;
+			ppc64_caches.iblock_size = bsize;
+			ppc64_caches.log_iblock_size = __ilog2(bsize);
+			ppc64_caches.iblocks_per_page = PAGE_SIZE / bsize;
 		}
 	}
 
 	/* For use by binfmt_elf */
-	dcache_bsize = ppc64_caches.dline_size;
-	icache_bsize = ppc64_caches.iline_size;
+	dcache_bsize = ppc64_caches.dblock_size;
+	icache_bsize = ppc64_caches.iblock_size;
 
 	DBG(" <- initialize_cache_info()\n");
 }

commit f9e473f1aa7597affff87bc6a599cf0aa389f0c1
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:44 2017 -0600

    powerpc: Remove obsolete comment about patching instructions
    
    We don't patch instructions based on the cache lines or block
    sizes these days.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ed3362bc9a2a..ae84d345c13c 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -77,9 +77,6 @@
 int spinning_secondaries;
 u64 ppc64_pft_size;
 
-/* Pick defaults since we might want to patch instructions
- * before we've read this from the device tree.
- */
 struct ppc64_caches ppc64_caches = {
 	.dline_size = 0x40,
 	.log_dline_size = 6,

commit 33ec723cac63529e5d0efa1125c893d2049c023d
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:43 2017 -0600

    powerpc: Move {d,i,u}cache_bsize definitions to a common place
    
    The variables are defined twice in setup_32.c and setup_64.c, do it
    once in setup-common.c instead
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6824157e4d2e..ed3362bc9a2a 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -88,14 +88,6 @@ struct ppc64_caches ppc64_caches = {
 };
 EXPORT_SYMBOL_GPL(ppc64_caches);
 
-/*
- * These are used in binfmt_elf.c to put aux entries on the stack
- * for each elf executable being started.
- */
-int dcache_bsize;
-int icache_bsize;
-int ucache_bsize;
-
 #if defined(CONFIG_PPC_BOOK3E) && defined(CONFIG_SMP)
 void __init setup_tlb_core_data(void)
 {

commit de399813b521ea7e38bbfb5e5b620b5e202e5783
Merge: 57ca04ab4401 c6f6634721c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 16 09:26:42 2016 -0800

    Merge tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for the kexec_file_load() syscall, which is a prereq for
         secure and trusted boot.
    
       - Prevent kernel execution of userspace on P9 Radix (similar to
         SMEP/PXN).
    
       - Sort the exception tables at build time, to save time at boot, and
         store them as relative offsets to save space in the kernel image &
         memory.
    
       - Allow building the kernel with thin archives, which should allow us
         to build an allyesconfig once some other fixes land.
    
       - Build fixes to allow us to correctly rebuild when changing the
         kernel endian from big to little or vice versa.
    
       - Plumbing so that we can avoid doing a full mm TLB flush on P9
         Radix.
    
       - Initial stack protector support (-fstack-protector).
    
       - Support for dumping the radix (aka. Linux) and hash page tables via
         debugfs.
    
       - Fix an oops in cxl coredump generation when cxl_get_fd() is used.
    
       - Freescale updates from Scott: "Highlights include 8xx hugepage
         support, qbman fixes/cleanup, device tree updates, and some misc
         cleanup."
    
       - Many and varied fixes and minor enhancements as always.
    
      Thanks to:
        Alexey Kardashevskiy, Andrew Donnellan, Aneesh Kumar K.V, Anshuman
        Khandual, Anton Blanchard, Balbir Singh, Bartlomiej Zolnierkiewicz,
        Christophe Jaillet, Christophe Leroy, Denis Kirjanov, Elimar
        Riesebieter, Frederic Barrat, Gautham R. Shenoy, Geliang Tang, Geoff
        Levand, Jack Miller, Johan Hovold, Lars-Peter Clausen, Libin,
        Madhavan Srinivasan, Michael Neuling, Nathan Fontenot, Naveen N.
        Rao, Nicholas Piggin, Pan Xinhui, Peter Senna Tschudin, Rashmica
        Gupta, Rui Teng, Russell Currey, Scott Wood, Simon Guo, Suraj
        Jitindar Singh, Thiago Jung Bauermann, Tobias Klauser, Vaibhav Jain"
    
    [ And thanks to Michael, who took time off from a new baby to get this
      pull request done.   - Linus ]
    
    * tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (174 commits)
      powerpc/fsl/dts: add FMan node for t1042d4rdb
      powerpc/fsl/dts: add sg_2500_aqr105_phy4 alias on t1024rdb
      powerpc/fsl/dts: add QMan and BMan nodes on t1024
      powerpc/fsl/dts: add QMan and BMan nodes on t1023
      soc/fsl/qman: test: use DEFINE_SPINLOCK()
      powerpc/fsl-lbc: use DEFINE_SPINLOCK()
      powerpc/8xx: Implement support of hugepages
      powerpc: get hugetlbpage handling more generic
      powerpc: port 64 bits pgtable_cache to 32 bits
      powerpc/boot: Request no dynamic linker for boot wrapper
      soc/fsl/bman: Use resource_size instead of computation
      soc/fsl/qe: use builtin_platform_driver
      powerpc/fsl_pmc: use builtin_platform_driver
      powerpc/83xx/suspend: use builtin_platform_driver
      powerpc/ftrace: Fix the comments for ftrace_modify_code
      powerpc/perf: macros for power9 format encoding
      powerpc/perf: power9 raw event format encoding
      powerpc/perf: update attribute_group data structure
      powerpc/perf: factor out the event format field
      powerpc/mm/iommu, vfio/spapr: Put pages on VFIO container shutdown
      ...

commit da6658859b9c734fee36570f3a7d51764c6c3838
Author: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
Date:   Tue Nov 29 23:45:50 2016 +1100

    powerpc: Change places using CONFIG_KEXEC to use CONFIG_KEXEC_CORE instead.
    
    Commit 2965faa5e03d ("kexec: split kexec_load syscall from kexec core
    code") introduced CONFIG_KEXEC_CORE so that CONFIG_KEXEC means whether
    the kexec_load system call should be compiled-in and CONFIG_KEXEC_FILE
    means whether the kexec_file_load system call should be compiled-in.
    These options can be set independently from each other.
    
    Since until now powerpc only supported kexec_load, CONFIG_KEXEC and
    CONFIG_KEXEC_CORE were synonyms. That is not the case anymore, so we
    need to make a distinction. Almost all places where CONFIG_KEXEC was
    being used should be using CONFIG_KEXEC_CORE instead, since
    kexec_file_load also needs that code compiled in.
    
    Signed-off-by: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 7ac8e6eaab5b..c3e129080c31 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -346,7 +346,7 @@ void early_setup_secondary(void)
 
 #endif /* CONFIG_SMP */
 
-#if defined(CONFIG_SMP) || defined(CONFIG_KEXEC)
+#if defined(CONFIG_SMP) || defined(CONFIG_KEXEC_CORE)
 static bool use_spinloop(void)
 {
 	if (!IS_ENABLED(CONFIG_PPC_BOOK3E))
@@ -391,7 +391,7 @@ void smp_release_cpus(void)
 
 	DBG(" <- smp_release_cpus()\n");
 }
-#endif /* CONFIG_SMP || CONFIG_KEXEC */
+#endif /* CONFIG_SMP || CONFIG_KEXEC_CORE */
 
 /*
  * Initialize some remaining members of the ppc64_caches and systemcfg

commit c0a36013639b06760f7c2c21a8387eac855432e1
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Nov 15 15:28:33 2016 +1100

    powerpc/64: Fix setting of AIL in hypervisor mode
    
    Commit d3cbff1b5 "powerpc: Put exception configuration in a common place"
    broke the setting of the AIL bit (which enables taking exceptions with
    the MMU still on) on all processors, moving it incorrectly to a function
    called only on the boot CPU. This was correct for the guest case but
    not when running in hypervisor mode.
    
    This fixes it by partially reverting that commit, putting the setting
    back in cpu_ready_for_interrupts()
    
    Fixes: d3cbff1b5a90 ("powerpc: Put exception configuration in a common place")
    Cc: stable@vger.kernel.org # v4.8+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 7ac8e6eaab5b..8d586cff8a41 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -226,17 +226,25 @@ static void __init configure_exceptions(void)
 		if (firmware_has_feature(FW_FEATURE_OPAL))
 			opal_configure_cores();
 
-		/* Enable AIL if supported, and we are in hypervisor mode */
-		if (early_cpu_has_feature(CPU_FTR_HVMODE) &&
-		    early_cpu_has_feature(CPU_FTR_ARCH_207S)) {
-			unsigned long lpcr = mfspr(SPRN_LPCR);
-			mtspr(SPRN_LPCR, lpcr | LPCR_AIL_3);
-		}
+		/* AIL on native is done in cpu_ready_for_interrupts() */
 	}
 }
 
 static void cpu_ready_for_interrupts(void)
 {
+	/*
+	 * Enable AIL if supported, and we are in hypervisor mode. This
+	 * is called once for every processor.
+	 *
+	 * If we are not in hypervisor mode the job is done once for
+	 * the whole partition in configure_exceptions().
+	 */
+	if (early_cpu_has_feature(CPU_FTR_HVMODE) &&
+	    early_cpu_has_feature(CPU_FTR_ARCH_207S)) {
+		unsigned long lpcr = mfspr(SPRN_LPCR);
+		mtspr(SPRN_LPCR, lpcr | LPCR_AIL_3);
+	}
+
 	/* Set IR and DR in PACA MSR */
 	get_paca()->kernel_msr = MSR_KERNEL;
 }

commit 97f6e0cc35026a2a09147a6da636d901525e1969
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Aug 10 17:27:34 2016 +1000

    powerpc/32: Fix crash during static key init
    
    We cannot do those initializations from apply_feature_fixups() as
    this function runs in a very restricted environment on 32-bit where
    the kernel isn't running at its linked address and the PTRRELOC()
    macro must be used for any global accesss.
    
    Instead, split them into a separtate steup_feature_keys() function
    which is called in a more suitable spot on ppc32.
    
    Fixes: 309b315b6ec6 ("powerpc: Call jump_label_init() in apply_feature_fixups()")
    Reported-and-tested-by: Christian Kujau <lists@nerdbynature.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index eafb9a79e011..7ac8e6eaab5b 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -300,6 +300,7 @@ void __init early_setup(unsigned long dt_ptr)
 
 	/* Apply all the dynamic patching */
 	apply_feature_fixups();
+	setup_feature_keys();
 
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu();

commit b8f1b4f8606b40b478072fb551f79e2984d44fad
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Sat Jul 23 14:42:35 2016 +0530

    powerpc/mm: Convert early cpu/mmu feature check to use the new helpers
    
    This switches early feature checks to use the non static key variant of
    the function. In later patches we will be switching cpu_has_feature()
    and mmu_has_feature() to use static keys and we can use them only after
    static key/jump label is initialized. Any check for feature before jump
    label init should be done using this new helper.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 984696136f96..eafb9a79e011 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -227,8 +227,8 @@ static void __init configure_exceptions(void)
 			opal_configure_cores();
 
 		/* Enable AIL if supported, and we are in hypervisor mode */
-		if (cpu_has_feature(CPU_FTR_HVMODE) &&
-		    cpu_has_feature(CPU_FTR_ARCH_207S)) {
+		if (early_cpu_has_feature(CPU_FTR_HVMODE) &&
+		    early_cpu_has_feature(CPU_FTR_ARCH_207S)) {
 			unsigned long lpcr = mfspr(SPRN_LPCR);
 			mtspr(SPRN_LPCR, lpcr | LPCR_AIL_3);
 		}

commit 9e8066f398396d26010d9c6c3c2538ff25461ef8
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jul 26 21:55:48 2016 +1000

    powerpc/64: Do feature patching before MMU init
    
    Up until now we needed to do the MMU init before feature patching,
    because part of the MMU init was scanning the device tree and setting
    and/or clearing some MMU feature bits.
    
    Now that we have split that MMU feature modification out into routines
    called from early_init_devtree() (called earlier) we can now do feature
    patching before calling MMU init.
    
    The advantage of this is it means the remainder of the MMU init runs
    with the final set of features which will apply for the rest of the life
    of the system. This means we don't have to special case anything called
    from MMU init to deal with a changing set of feature bits.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index d8216aed22b7..984696136f96 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -298,12 +298,12 @@ void __init early_setup(unsigned long dt_ptr)
 	 */
 	configure_exceptions();
 
-	/* Initialize the hash table or TLB handling */
-	early_init_mmu();
-
 	/* Apply all the dynamic patching */
 	apply_feature_fixups();
 
+	/* Initialize the hash table or TLB handling */
+	early_init_mmu();
+
 	/*
 	 * At this point, we can let interrupts switch to virtual mode
 	 * (the MMU has been setup), so adjust the MSR in the PACA to

commit b1923caa6e641f3d0a93b5d045aef67ded5aef67
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:07:51 2016 +1000

    powerpc: Merge 32-bit and 64-bit setup_arch()
    
    There is little enough differences now.
    
    mpe: Add a/p/k/setup.h to contain the prototypes and empty versions of
    functions we need, rather than using weak functions. Add a few other
    empty versions to avoid as many #ifdefs as possible in the code.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index fba96ada3012..d8216aed22b7 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -35,7 +35,6 @@
 #include <linux/pci.h>
 #include <linux/lockdep.h>
 #include <linux/memblock.h>
-#include <linux/hugetlb.h>
 #include <linux/memory.h>
 #include <linux/nmi.h>
 
@@ -64,12 +63,10 @@
 #include <asm/xmon.h>
 #include <asm/udbg.h>
 #include <asm/kexec.h>
-#include <asm/mmu_context.h>
 #include <asm/code-patching.h>
-#include <asm/kvm_ppc.h>
-#include <asm/hugetlb.h>
 #include <asm/livepatch.h>
 #include <asm/opal.h>
+#include <asm/cputhreads.h>
 
 #ifdef DEBUG
 #define DBG(fmt...) udbg_printf(fmt)
@@ -100,7 +97,7 @@ int icache_bsize;
 int ucache_bsize;
 
 #if defined(CONFIG_PPC_BOOK3E) && defined(CONFIG_SMP)
-static void setup_tlb_core_data(void)
+void __init setup_tlb_core_data(void)
 {
 	int cpu;
 
@@ -133,10 +130,6 @@ static void setup_tlb_core_data(void)
 		}
 	}
 }
-#else
-static void setup_tlb_core_data(void)
-{
-}
 #endif
 
 #ifdef CONFIG_SMP
@@ -144,7 +137,7 @@ static void setup_tlb_core_data(void)
 static char *smt_enabled_cmdline;
 
 /* Look for ibm,smt-enabled OF option */
-static void check_smt_enabled(void)
+void __init check_smt_enabled(void)
 {
 	struct device_node *dn;
 	const char *smt_option;
@@ -193,8 +186,6 @@ static int __init early_smt_enabled(char *p)
 }
 early_param("smt-enabled", early_smt_enabled);
 
-#else
-#define check_smt_enabled()
 #endif /* CONFIG_SMP */
 
 /** Fix up paca fields required for the boot cpu */
@@ -408,7 +399,7 @@ void smp_release_cpus(void)
  * cache informations about the CPU that will be used by cache flush
  * routines and/or provided to userland
  */
-static void __init initialize_cache_info(void)
+void __init initialize_cache_info(void)
 {
 	struct device_node *np;
 	unsigned long num_cpus = 0;
@@ -480,38 +471,6 @@ static void __init initialize_cache_info(void)
 	DBG(" <- initialize_cache_info()\n");
 }
 
-static __init void print_system_info(void)
-{
-	pr_info("-----------------------------------------------------\n");
-	pr_info("ppc64_pft_size    = 0x%llx\n", ppc64_pft_size);
-	pr_info("phys_mem_size     = 0x%llx\n", memblock_phys_mem_size());
-
-	if (ppc64_caches.dline_size != 0x80)
-		pr_info("dcache_line_size  = 0x%x\n", ppc64_caches.dline_size);
-	if (ppc64_caches.iline_size != 0x80)
-		pr_info("icache_line_size  = 0x%x\n", ppc64_caches.iline_size);
-
-	pr_info("cpu_features      = 0x%016lx\n", cur_cpu_spec->cpu_features);
-	pr_info("  possible        = 0x%016lx\n", CPU_FTRS_POSSIBLE);
-	pr_info("  always          = 0x%016lx\n", CPU_FTRS_ALWAYS);
-	pr_info("cpu_user_features = 0x%08x 0x%08x\n", cur_cpu_spec->cpu_user_features,
-		cur_cpu_spec->cpu_user_features2);
-	pr_info("mmu_features      = 0x%08x\n", cur_cpu_spec->mmu_features);
-	pr_info("firmware_features = 0x%016lx\n", powerpc_firmware_features);
-
-#ifdef CONFIG_PPC_STD_MMU_64
-	if (htab_address)
-		pr_info("htab_address      = 0x%p\n", htab_address);
-
-	pr_info("htab_hash_mask    = 0x%lx\n", htab_hash_mask);
-#endif
-
-	if (PHYSICAL_START > 0)
-		pr_info("physical_start    = 0x%llx\n",
-		       (unsigned long long)PHYSICAL_START);
-	pr_info("-----------------------------------------------------\n");
-}
-
 /* This returns the limit below which memory accesses to the linear
  * mapping are guarnateed not to cause a TLB or SLB miss. This is
  * used to allocate interrupt or emergency stacks for which our
@@ -533,7 +492,7 @@ static __init u64 safe_stack_limit(void)
 #endif
 }
 
-static void __init irqstack_early_init(void)
+void __init irqstack_early_init(void)
 {
 	u64 limit = safe_stack_limit();
 	unsigned int i;
@@ -553,7 +512,7 @@ static void __init irqstack_early_init(void)
 }
 
 #ifdef CONFIG_PPC_BOOK3E
-static void __init exc_lvl_early_init(void)
+void __init exc_lvl_early_init(void)
 {
 	unsigned int i;
 	unsigned long sp;
@@ -575,8 +534,6 @@ static void __init exc_lvl_early_init(void)
 	if (cpu_has_feature(CPU_FTR_DEBUG_LVL_EXC))
 		patch_exception(0x040, exc_debug_debug_book3e);
 }
-#else
-#define exc_lvl_early_init()
 #endif
 
 /*
@@ -584,7 +541,7 @@ static void __init exc_lvl_early_init(void)
  * early in SMP boots before relocation is enabled. Exclusive emergency
  * stack for machine checks.
  */
-static void __init emergency_stack_init(void)
+void __init emergency_stack_init(void)
 {
 	u64 limit;
 	unsigned int i;
@@ -615,124 +572,6 @@ static void __init emergency_stack_init(void)
 	}
 }
 
-/*
- * Called into from start_kernel this initializes memblock, which is used
- * to manage page allocation until mem_init is called.
- */
-void __init setup_arch(char **cmdline_p)
-{
-	*cmdline_p = boot_command_line;
-
-	/*
-	 * Unflatten the device-tree passed by prom_init or kexec
-	 */
-	unflatten_device_tree();
-
-	/*
-	 * Fill the ppc64_caches & systemcfg structures with informations
-	 * retrieved from the device-tree.
-	 */
-	initialize_cache_info();
-
-#ifdef CONFIG_PPC_RTAS
-	/*
-	 * Initialize RTAS if available
-	 */
-	rtas_initialize();
-#endif /* CONFIG_PPC_RTAS */
-
-	/*
-	 * Check if we have an initrd provided via the device-tree
-	 */
-	check_for_initrd();
-
-	/* Probe the machine type */
-	probe_machine();
-
-	setup_panic();
-
-	/*
-	 * We can discover serial ports now since the above did setup the
-	 * hash table management for us, thus ioremap works. We do that early
-	 * so that further code can be debugged
-	 */
-	find_legacy_serial_ports();
-
-	/*
-	 * Register early console
-	 */
-	register_early_udbg_console();
-
-	smp_setup_cpu_maps();
-
-	/*
-	 * Initialize xmon
-	 */
-	xmon_setup();
-
-	check_smt_enabled();
-	setup_tlb_core_data();
-
-	/*
-	 * Freescale Book3e parts spin in a loop provided by firmware,
-	 * so smp_release_cpus() does nothing for them
-	 */
-#if defined(CONFIG_SMP)
-	/*
-	 * Release secondary cpus out of their spinloops at 0x60 now that
-	 * we can map physical -> logical CPU ids
-	 */
-	smp_release_cpus();
-#endif
-
-	/* Print various info about the machine that has been gathered so far. */
-	print_system_info();
-
-	/* Reserve large chunks of memory for use by CMA for KVM */
-	kvm_cma_reserve();
-
-	/*
-	 * Reserve any gigantic pages requested on the command line.
-	 * memblock needs to have been initialized by the time this is
-	 * called since this will reserve memory.
-	 */
-	reserve_hugetlb_gpages();
-
-	klp_init_thread_info(&init_thread_info);
-
-	init_mm.start_code = (unsigned long)_stext;
-	init_mm.end_code = (unsigned long) _etext;
-	init_mm.end_data = (unsigned long) _edata;
-	init_mm.brk = klimit;
-#ifdef CONFIG_PPC_64K_PAGES
-	init_mm.context.pte_frag = NULL;
-#endif
-#ifdef CONFIG_SPAPR_TCE_IOMMU
-	mm_iommu_init(&init_mm.context);
-#endif
-	irqstack_early_init();
-	exc_lvl_early_init();
-	emergency_stack_init();
-
-	initmem_init();
-
-#ifdef CONFIG_DUMMY_CONSOLE
-	conswitchp = &dummy_con;
-#endif
-	if (ppc_md.setup_arch)
-		ppc_md.setup_arch();
-
-	paging_init();
-
-	/* Initialize the MMU context management stuff */
-	mmu_context_init();
-
-	/* Interrupt code needs to be 64K-aligned */
-	if ((unsigned long)_stext & 0xffff)
-		panic("Kernelbase not 64K-aligned (0x%lx)!\n",
-		      (unsigned long)_stext);
-}
-
 #ifdef CONFIG_SMP
 #define PCPU_DYN_SIZE		()
 

commit 009776baa18448b223be73ac74912fef7e17b9e2
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:07:50 2016 +1000

    powerpc/64: Make a few boot functions __init
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f55c25dff02c..fba96ada3012 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -198,7 +198,7 @@ early_param("smt-enabled", early_smt_enabled);
 #endif /* CONFIG_SMP */
 
 /** Fix up paca fields required for the boot cpu */
-static void fixup_boot_paca(void)
+static void __init fixup_boot_paca(void)
 {
 	/* The boot cpu is started */
 	get_paca()->cpu_start = 1;
@@ -206,7 +206,7 @@ static void fixup_boot_paca(void)
 	get_paca()->data_offset = 0;
 }
 
-static void configure_exceptions(void)
+static void __init configure_exceptions(void)
 {
 	/*
 	 * Setup the trampolines from the lowmem exception vectors
@@ -517,7 +517,7 @@ static __init void print_system_info(void)
  * used to allocate interrupt or emergency stacks for which our
  * exception entry path doesn't deal with being interrupted.
  */
-static u64 safe_stack_limit(void)
+static __init u64 safe_stack_limit(void)
 {
 #ifdef CONFIG_PPC_BOOK3E
 	/* Freescale BookE bolts the entire linear mapping */

commit f7b9ebb79e90b19bf6a2cb805a536258437fc3fa
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:04:12 2016 +1000

    powerpc: Re-order setup_panic()
    
    Do it right after probe_machine() since it's about testing ppc_md,
    and put the test in the common code.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 3fd59bca06f7..f55c25dff02c 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -649,6 +649,8 @@ void __init setup_arch(char **cmdline_p)
 	/* Probe the machine type */
 	probe_machine();
 
+	setup_panic();
+
 	/*
 	 * We can discover serial ports now since the above did setup the
 	 * hash table management for us, thus ioremap works. We do that early
@@ -696,9 +698,6 @@ void __init setup_arch(char **cmdline_p)
 	 */
 	reserve_hugetlb_gpages();
 
-	if (ppc_md.panic)
-		setup_panic();
-
 	klp_init_thread_info(&init_thread_info);
 
 	init_mm.start_code = (unsigned long)_stext;

commit e39afba3aa11f7088ddc00d37ab34a85d960a76e
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:04:11 2016 +1000

    powerpc: Re-order the call to smp_setup_cpu_maps()
    
    It makes more sense to do it before intializing xmon() as xmon might
    use the info in there. We do want to register the console early
    though in case we want some functioning printk's in the cpu map setup.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 61c3e6c42262..3fd59bca06f7 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -661,12 +661,13 @@ void __init setup_arch(char **cmdline_p)
 	 */
 	register_early_udbg_console();
 
+	smp_setup_cpu_maps();
+
 	/*
 	 * Initialize xmon
 	 */
 	xmon_setup();
 
-	smp_setup_cpu_maps();
 	check_smt_enabled();
 	setup_tlb_core_data();
 

commit fa745a129cae93ca5d871ebac2a8f6c27ae3fbf2
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:04:09 2016 +1000

    powerpc/64: Move the content of setup_system() to setup_arch()
    
    And kill setup_system().
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index aca215d77fe3..61c3e6c42262 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -512,78 +512,6 @@ static __init void print_system_info(void)
 	pr_info("-----------------------------------------------------\n");
 }
 
-/*
- * Do some initial setup of the system.  The parameters are those which 
- * were passed in from the bootloader.
- */
-void __init setup_system(void)
-{
-	DBG(" -> setup_system()\n");
-
-	/*
-	 * Unflatten the device-tree passed by prom_init or kexec
-	 */
-	unflatten_device_tree();
-
-	/*
-	 * Fill the ppc64_caches & systemcfg structures with informations
- 	 * retrieved from the device-tree.
-	 */
-	initialize_cache_info();
-
-#ifdef CONFIG_PPC_RTAS
-	/*
-	 * Initialize RTAS if available
-	 */
-	rtas_initialize();
-#endif /* CONFIG_PPC_RTAS */
-
-	/*
-	 * Check if we have an initrd provided via the device-tree
-	 */
-	check_for_initrd();
-
-	/* Probe the machine type */
-	probe_machine();
-
- 	/*
-	 * We can discover serial ports now since the above did setup the
-	 * hash table management for us, thus ioremap works. We do that early
-	 * so that further code can be debugged
-	 */
-	find_legacy_serial_ports();
-
-	/*
-	 * Register early console
-	 */
-	register_early_udbg_console();
-
-	/*
-	 * Initialize xmon
-	 */
-	xmon_setup();
-
-	smp_setup_cpu_maps();
-	check_smt_enabled();
-	setup_tlb_core_data();
-
-	/*
-	 * Freescale Book3e parts spin in a loop provided by firmware,
-	 * so smp_release_cpus() does nothing for them
-	 */
-#if defined(CONFIG_SMP)
-	/* Release secondary cpus out of their spinloops at 0x60 now that
-	 * we can map physical -> logical CPU ids
-	 */
-	smp_release_cpus();
-#endif
-
-	/* Print various info about the machine that has been gathered so far. */
-	print_system_info();
-
-	DBG(" <- setup_system()\n");
-}
-
 /* This returns the limit below which memory accesses to the linear
  * mapping are guarnateed not to cause a TLB or SLB miss. This is
  * used to allocate interrupt or emergency stacks for which our
@@ -695,6 +623,68 @@ void __init setup_arch(char **cmdline_p)
 {
 	*cmdline_p = boot_command_line;
 
+	/*
+	 * Unflatten the device-tree passed by prom_init or kexec
+	 */
+	unflatten_device_tree();
+
+	/*
+	 * Fill the ppc64_caches & systemcfg structures with informations
+	 * retrieved from the device-tree.
+	 */
+	initialize_cache_info();
+
+#ifdef CONFIG_PPC_RTAS
+	/*
+	 * Initialize RTAS if available
+	 */
+	rtas_initialize();
+#endif /* CONFIG_PPC_RTAS */
+
+	/*
+	 * Check if we have an initrd provided via the device-tree
+	 */
+	check_for_initrd();
+
+	/* Probe the machine type */
+	probe_machine();
+
+	/*
+	 * We can discover serial ports now since the above did setup the
+	 * hash table management for us, thus ioremap works. We do that early
+	 * so that further code can be debugged
+	 */
+	find_legacy_serial_ports();
+
+	/*
+	 * Register early console
+	 */
+	register_early_udbg_console();
+
+	/*
+	 * Initialize xmon
+	 */
+	xmon_setup();
+
+	smp_setup_cpu_maps();
+	check_smt_enabled();
+	setup_tlb_core_data();
+
+	/*
+	 * Freescale Book3e parts spin in a loop provided by firmware,
+	 * so smp_release_cpus() does nothing for them
+	 */
+#if defined(CONFIG_SMP)
+	/*
+	 * Release secondary cpus out of their spinloops at 0x60 now that
+	 * we can map physical -> logical CPU ids
+	 */
+	smp_release_cpus();
+#endif
+
+	/* Print various info about the machine that has been gathered so far. */
+	print_system_info();
+
 	/* Reserve large chunks of memory for use by CMA for KVM */
 	kvm_cma_reserve();
 

commit 9df549afeab4ea968b6d83cf9d7a1e3c577a9846
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:04:08 2016 +1000

    powerpc/64: Move setting of {i,d}cache_bsize to initialize_cache_info()
    
    Also remove the completely osbolete comment. We *do* look in the
    device-tree.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 2395a88b1142..aca215d77fe3 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -473,6 +473,10 @@ static void __init initialize_cache_info(void)
 		}
 	}
 
+	/* For use by binfmt_elf */
+	dcache_bsize = ppc64_caches.dline_size;
+	icache_bsize = ppc64_caches.iline_size;
+
 	DBG(" <- initialize_cache_info()\n");
 }
 
@@ -691,15 +695,6 @@ void __init setup_arch(char **cmdline_p)
 {
 	*cmdline_p = boot_command_line;
 
-	/*
-	 * Set cache line size based on type of cpu as a default.
-	 * Systems with OF can look in the properties on the cpu node(s)
-	 * for a possibly more accurate value.
-	 */
-	dcache_bsize = ppc64_caches.dline_size;
-	icache_bsize = ppc64_caches.iline_size;
-
-
 	/* Reserve large chunks of memory for use by CMA for KVM */
 	kvm_cma_reserve();
 

commit bf1b61fb574bfe13ab71347389a2ab16f673d24f
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:04:07 2016 +1000

    powerpc/64: Move the boot time info banner to a separate function
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 8b9768a97387..2395a88b1142 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -476,6 +476,37 @@ static void __init initialize_cache_info(void)
 	DBG(" <- initialize_cache_info()\n");
 }
 
+static __init void print_system_info(void)
+{
+	pr_info("-----------------------------------------------------\n");
+	pr_info("ppc64_pft_size    = 0x%llx\n", ppc64_pft_size);
+	pr_info("phys_mem_size     = 0x%llx\n", memblock_phys_mem_size());
+
+	if (ppc64_caches.dline_size != 0x80)
+		pr_info("dcache_line_size  = 0x%x\n", ppc64_caches.dline_size);
+	if (ppc64_caches.iline_size != 0x80)
+		pr_info("icache_line_size  = 0x%x\n", ppc64_caches.iline_size);
+
+	pr_info("cpu_features      = 0x%016lx\n", cur_cpu_spec->cpu_features);
+	pr_info("  possible        = 0x%016lx\n", CPU_FTRS_POSSIBLE);
+	pr_info("  always          = 0x%016lx\n", CPU_FTRS_ALWAYS);
+	pr_info("cpu_user_features = 0x%08x 0x%08x\n", cur_cpu_spec->cpu_user_features,
+		cur_cpu_spec->cpu_user_features2);
+	pr_info("mmu_features      = 0x%08x\n", cur_cpu_spec->mmu_features);
+	pr_info("firmware_features = 0x%016lx\n", powerpc_firmware_features);
+
+#ifdef CONFIG_PPC_STD_MMU_64
+	if (htab_address)
+		pr_info("htab_address      = 0x%p\n", htab_address);
+
+	pr_info("htab_hash_mask    = 0x%lx\n", htab_hash_mask);
+#endif
+
+	if (PHYSICAL_START > 0)
+		pr_info("physical_start    = 0x%llx\n",
+		       (unsigned long long)PHYSICAL_START);
+	pr_info("-----------------------------------------------------\n");
+}
 
 /*
  * Do some initial setup of the system.  The parameters are those which 
@@ -543,37 +574,8 @@ void __init setup_system(void)
 	smp_release_cpus();
 #endif
 
-	pr_info("Starting Linux %s %s\n", init_utsname()->machine,
-		 init_utsname()->version);
-
-	pr_info("-----------------------------------------------------\n");
-	pr_info("ppc64_pft_size    = 0x%llx\n", ppc64_pft_size);
-	pr_info("phys_mem_size     = 0x%llx\n", memblock_phys_mem_size());
-
-	if (ppc64_caches.dline_size != 0x80)
-		pr_info("dcache_line_size  = 0x%x\n", ppc64_caches.dline_size);
-	if (ppc64_caches.iline_size != 0x80)
-		pr_info("icache_line_size  = 0x%x\n", ppc64_caches.iline_size);
-
-	pr_info("cpu_features      = 0x%016lx\n", cur_cpu_spec->cpu_features);
-	pr_info("  possible        = 0x%016lx\n", CPU_FTRS_POSSIBLE);
-	pr_info("  always          = 0x%016lx\n", CPU_FTRS_ALWAYS);
-	pr_info("cpu_user_features = 0x%08x 0x%08x\n", cur_cpu_spec->cpu_user_features,
-		cur_cpu_spec->cpu_user_features2);
-	pr_info("mmu_features      = 0x%08x\n", cur_cpu_spec->mmu_features);
-	pr_info("firmware_features = 0x%016lx\n", powerpc_firmware_features);
-
-#ifdef CONFIG_PPC_STD_MMU_64
-	if (htab_address)
-		pr_info("htab_address      = 0x%p\n", htab_address);
-
-	pr_info("htab_hash_mask    = 0x%lx\n", htab_hash_mask);
-#endif
-
-	if (PHYSICAL_START > 0)
-		pr_info("physical_start    = 0x%llx\n",
-		       (unsigned long long)PHYSICAL_START);
-	pr_info("-----------------------------------------------------\n");
+	/* Print various info about the machine that has been gathered so far. */
+	print_system_info();
 
 	DBG(" <- setup_system()\n");
 }

commit f2d576948d6cec16e4aae201d738c4f22039a551
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:04:06 2016 +1000

    powerpc: Get rid of ppc_md.init_early()
    
    It is now called right after platform probe, so the probe function
    can just do the job.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 883d527899a7..8b9768a97387 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -511,14 +511,6 @@ void __init setup_system(void)
 	/* Probe the machine type */
 	probe_machine();
 
-	/*
-	 * Do some platform specific early initializations, that includes
-	 * setting up the hash table pointers. It also sets up some interrupt-mapping
-	 * related options that will be used by finish_device_tree()
-	 */
-	if (ppc_md.init_early)
-		ppc_md.init_early();
-
  	/*
 	 * We can discover serial ports now since the above did setup the
 	 * hash table management for us, thus ioremap works. We do that early

commit 406b0b6ae3fcd5c7946a68a9e43b470c79d292a2
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:04:00 2016 +1000

    powerpc/64: Move 64-bit probe_machine() to later in the boot process
    
    We no long need the machine type that early, so we can move probe_machine()
    to after the device-tree has been expanded. This will allow further
    consolidation.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4ffd090633de..883d527899a7 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -301,9 +301,6 @@ void __init early_setup(unsigned long dt_ptr)
 	setup_paca(&paca[boot_cpuid]);
 	fixup_boot_paca();
 
-	/* Probe the machine type */
-	probe_machine();
-
 	/*
 	 * Configure exception handlers. This include setting up trampolines
 	 * if needed, setting exception endian mode, etc...
@@ -511,6 +508,9 @@ void __init setup_system(void)
 	 */
 	check_for_initrd();
 
+	/* Probe the machine type */
+	probe_machine();
+
 	/*
 	 * Do some platform specific early initializations, that includes
 	 * setting up the hash table pointers. It also sets up some interrupt-mapping

commit d3cbff1b5a90afe6cb201aa2187c9609e21f92ad
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:03:49 2016 +1000

    powerpc: Put exception configuration in a common place
    
    The various calls to establish exception endianness and AIL are
    now done from a single point using already established CPU and FW
    feature bits to decide what to do.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 155dbcce8ef8..4ffd090633de 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -69,6 +69,7 @@
 #include <asm/kvm_ppc.h>
 #include <asm/hugetlb.h>
 #include <asm/livepatch.h>
+#include <asm/opal.h>
 
 #ifdef DEBUG
 #define DBG(fmt...) udbg_printf(fmt)
@@ -205,23 +206,50 @@ static void fixup_boot_paca(void)
 	get_paca()->data_offset = 0;
 }
 
-static void cpu_ready_for_interrupts(void)
+static void configure_exceptions(void)
 {
-	/* Set IR and DR in PACA MSR */
-	get_paca()->kernel_msr = MSR_KERNEL;
-
 	/*
-	 * Enable AIL if supported, and we are in hypervisor mode. If we are
-	 * not in hypervisor mode, we enable relocation-on interrupts later
-	 * in pSeries_setup_arch() using the H_SET_MODE hcall.
+	 * Setup the trampolines from the lowmem exception vectors
+	 * to the kdump kernel when not using a relocatable kernel.
 	 */
-	if (cpu_has_feature(CPU_FTR_HVMODE) &&
-	    cpu_has_feature(CPU_FTR_ARCH_207S)) {
-		unsigned long lpcr = mfspr(SPRN_LPCR);
-		mtspr(SPRN_LPCR, lpcr | LPCR_AIL_3);
+	setup_kdump_trampoline();
+
+	/* Under a PAPR hypervisor, we need hypercalls */
+	if (firmware_has_feature(FW_FEATURE_SET_MODE)) {
+		/* Enable AIL if possible */
+		pseries_enable_reloc_on_exc();
+
+		/*
+		 * Tell the hypervisor that we want our exceptions to
+		 * be taken in little endian mode.
+		 *
+		 * We don't call this for big endian as our calling convention
+		 * makes us always enter in BE, and the call may fail under
+		 * some circumstances with kdump.
+		 */
+#ifdef __LITTLE_ENDIAN__
+		pseries_little_endian_exceptions();
+#endif
+	} else {
+		/* Set endian mode using OPAL */
+		if (firmware_has_feature(FW_FEATURE_OPAL))
+			opal_configure_cores();
+
+		/* Enable AIL if supported, and we are in hypervisor mode */
+		if (cpu_has_feature(CPU_FTR_HVMODE) &&
+		    cpu_has_feature(CPU_FTR_ARCH_207S)) {
+			unsigned long lpcr = mfspr(SPRN_LPCR);
+			mtspr(SPRN_LPCR, lpcr | LPCR_AIL_3);
+		}
 	}
 }
 
+static void cpu_ready_for_interrupts(void)
+{
+	/* Set IR and DR in PACA MSR */
+	get_paca()->kernel_msr = MSR_KERNEL;
+}
+
 /*
  * Early initialization entry point. This is called by head.S
  * with MMU translation disabled. We rely on the "feature" of
@@ -277,10 +305,10 @@ void __init early_setup(unsigned long dt_ptr)
 	probe_machine();
 
 	/*
-	 * Setup the trampolines from the lowmem exception vectors
-	 * to the kdump kernel when not using a relocatable kernel.
+	 * Configure exception handlers. This include setting up trampolines
+	 * if needed, setting exception endian mode, etc...
 	 */
-	setup_kdump_trampoline();
+	configure_exceptions();
 
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu();

commit de4cf3de594f96f5a27f0e2346dd211beb126f88
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:03:43 2016 +1000

    powerpc: Move 64-bit memory reserves to setup_arch()
    
    There is really no need to do them that early, early_setup() runs
    before MMU is on, we should do the strict minimum there to get the
    MMU going.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 0a6d5f70cbd4..155dbcce8ef8 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -295,16 +295,6 @@ void __init early_setup(unsigned long dt_ptr)
 	 */
 	cpu_ready_for_interrupts();
 
-	/* Reserve large chunks of memory for use by CMA for KVM */
-	kvm_cma_reserve();
-
-	/*
-	 * Reserve any gigantic pages requested on the command line.
-	 * memblock needs to have been initialized by the time this is
-	 * called since this will reserve memory.
-	 */
-	reserve_hugetlb_gpages();
-
 	DBG(" <- early_setup()\n");
 
 #ifdef CONFIG_PPC_EARLY_DEBUG_BOOTX
@@ -687,6 +677,17 @@ void __init setup_arch(char **cmdline_p)
 	dcache_bsize = ppc64_caches.dline_size;
 	icache_bsize = ppc64_caches.iline_size;
 
+
+	/* Reserve large chunks of memory for use by CMA for KVM */
+	kvm_cma_reserve();
+
+	/*
+	 * Reserve any gigantic pages requested on the command line.
+	 * memblock needs to have been initialized by the time this is
+	 * called since this will reserve memory.
+	 */
+	reserve_hugetlb_gpages();
+
 	if (ppc_md.panic)
 		setup_panic();
 
@@ -711,7 +712,6 @@ void __init setup_arch(char **cmdline_p)
 #ifdef CONFIG_DUMMY_CONSOLE
 	conswitchp = &dummy_con;
 #endif
-
 	if (ppc_md.setup_arch)
 		ppc_md.setup_arch();
 

commit c4bd6cb87c9e28a7d9f4a97db5a06cc538eb5e48
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:03:42 2016 +1000

    powerpc: Move 64-bit feature fixup earlier
    
    Make it part of early_setup() as we really want the feature fixups
    to be applied before we turn on the MMU since they can have an impact
    on the various assembly path related to MMU management and interrupts.
    
    This makes 64-bit match what 32-bit does.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 373ef9d692f6..0a6d5f70cbd4 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -285,6 +285,9 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu();
 
+	/* Apply all the dynamic patching */
+	apply_feature_fixups();
+
 	/*
 	 * At this point, we can let interrupts switch to virtual mode
 	 * (the MMU has been setup), so adjust the MSR in the PACA to
@@ -467,8 +470,6 @@ void __init setup_system(void)
 {
 	DBG(" -> setup_system()\n");
 
-	apply_feature_fixups();
-
 	/*
 	 * Unflatten the device-tree passed by prom_init or kexec
 	 */

commit 9402c684613163888714df0955fa1f17142b08bf
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:03:41 2016 +1000

    powerpc: Factor do_feature_fixup calls
    
    32 and 64-bit do a similar set of calls early on, we move it all to
    a single common function to make the boot code more readable.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 521846c904ca..373ef9d692f6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -467,18 +467,7 @@ void __init setup_system(void)
 {
 	DBG(" -> setup_system()\n");
 
-	/* Apply the CPUs-specific and firmware specific fixups to kernel
-	 * text (nop out sections not relevant to this CPU or this firmware)
-	 */
-	do_feature_fixups(cur_cpu_spec->cpu_features,
-			  &__start___ftr_fixup, &__stop___ftr_fixup);
-	do_feature_fixups(cur_cpu_spec->mmu_features,
-			  &__start___mmu_ftr_fixup, &__stop___mmu_ftr_fixup);
-	do_feature_fixups(powerpc_firmware_features,
-			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
-	do_lwsync_fixups(cur_cpu_spec->cpu_features,
-			 &__start___lwsync_fixup, &__stop___lwsync_fixup);
-	do_final_fixups();
+	apply_feature_fixups();
 
 	/*
 	 * Unflatten the device-tree passed by prom_init or kexec

commit da6a97bf12d57e341029b3624ed112175ecff514
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:03:44 2016 +1000

    powerpc: Move epapr_paravirt_early_init() to early_init_devtree()
    
    The function is called by both 32-bit and 64-bit early setup right
    after early_init_devtree(). All it does is run yet another early
    DT parser which is precisely what early_init_devtree() is about,
    so move it in there.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 98f72c6d0ebc..521846c904ca 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -68,7 +68,6 @@
 #include <asm/code-patching.h>
 #include <asm/kvm_ppc.h>
 #include <asm/hugetlb.h>
-#include <asm/epapr_hcalls.h>
 #include <asm/livepatch.h>
 
 #ifdef DEBUG
@@ -270,8 +269,6 @@ void __init early_setup(unsigned long dt_ptr)
 	 */
 	early_init_devtree(__va(dt_ptr));
 
-	epapr_paravirt_early_init();
-
 	/* Now we know the logical id of our boot cpu, setup the paca. */
 	setup_paca(&paca[boot_cpuid]);
 	fixup_boot_paca();

commit 63c254a501049f70c53aea602525c6912362079e
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:03:46 2016 +1000

    powerpc: Add comment explaining the purpose of setup_kdump_trampoline()
    
    Anything in early_setup() needs to be justified to be there, in
    this case, we need the trampolines before we can take exceptions
    and thus before we turn on the MMU.
    
    Also remove a pretty meaningless and misplaced debug message
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [mpe: Fix comment formatting]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 5530bb55a78b..98f72c6d0ebc 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -279,10 +279,12 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Probe the machine type */
 	probe_machine();
 
+	/*
+	 * Setup the trampolines from the lowmem exception vectors
+	 * to the kdump kernel when not using a relocatable kernel.
+	 */
 	setup_kdump_trampoline();
 
-	DBG("Found, Initializing memory management...\n");
-
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu();
 

commit 103b7827d977ea34c982e6a9d2f960f731f7ee76
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Fri Mar 4 10:31:48 2016 +0530

    powerpc: Fix misleading comment in early_setup_secondary()
    
    Current comment in the early_setup_secondary() for paca->soft_enabled
    update is misleading. Comment should say to Mark interrupts "disabled"
    instead of "enabled". Fix the typo.
    
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 96d4a2b23d0f..5530bb55a78b 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -321,7 +321,7 @@ void __init early_setup(unsigned long dt_ptr)
 #ifdef CONFIG_SMP
 void early_setup_secondary(void)
 {
-	/* Mark interrupts enabled in PACA */
+	/* Mark interrupts disabled in PACA */
 	get_paca()->soft_enabled = 0;
 
 	/* Initialize the hash table or TLB handling */

commit 4d4fb97a62105c07dcccd350c391a65f576726c4
Merge: 61bf12d3304d 85baa095497f
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Fri Apr 15 11:31:51 2016 +0200

    Merge branch 'topic/livepatch' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux into for-4.7/livepatching-ppc64le
    
    Pull livepatching support for ppc64 architecture from Michael Ellerman.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

commit 5d31a96e6c0187f2c5d7004e005fd094a1277e9e
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Mar 24 22:04:04 2016 +1100

    powerpc/livepatch: Add livepatch stack to struct thread_info
    
    In order to support live patching we need to maintain an alternate
    stack of TOC & LR values. We use the base of the stack for this, and
    store the "live patch stack pointer" in struct thread_info.
    
    Unlike the other fields of thread_info, we can not statically initialise
    that value, so it must be done at run time.
    
    This patch just adds the code to support that, it is not enabled until
    the next patch which actually adds live patch support.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Balbir Singh <bsingharora@gmail.com>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 5c03a6a9b054..e37b92ebb315 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -69,6 +69,7 @@
 #include <asm/kvm_ppc.h>
 #include <asm/hugetlb.h>
 #include <asm/epapr_hcalls.h>
+#include <asm/livepatch.h>
 
 #ifdef DEBUG
 #define DBG(fmt...) udbg_printf(fmt)
@@ -670,16 +671,16 @@ static void __init emergency_stack_init(void)
 	limit = min(safe_stack_limit(), ppc64_rma_size);
 
 	for_each_possible_cpu(i) {
-		unsigned long sp;
-		sp  = memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit);
-		sp += THREAD_SIZE;
-		paca[i].emergency_sp = __va(sp);
+		struct thread_info *ti;
+		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
+		klp_init_thread_info(ti);
+		paca[i].emergency_sp = (void *)ti + THREAD_SIZE;
 
 #ifdef CONFIG_PPC_BOOK3S_64
 		/* emergency stack for machine check exception handling. */
-		sp  = memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit);
-		sp += THREAD_SIZE;
-		paca[i].mc_emergency_sp = __va(sp);
+		ti = __va(memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit));
+		klp_init_thread_info(ti);
+		paca[i].mc_emergency_sp = (void *)ti + THREAD_SIZE;
 #endif
 	}
 }
@@ -703,6 +704,8 @@ void __init setup_arch(char **cmdline_p)
 	if (ppc_md.panic)
 		setup_panic();
 
+	klp_init_thread_info(&init_thread_info);
+
 	init_mm.start_code = (unsigned long)_stext;
 	init_mm.end_code = (unsigned long) _etext;
 	init_mm.end_data = (unsigned long) _edata;

commit 06bea3dbfe6a4c333c4333362c46bdf4d9e43504
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Thu Feb 4 11:29:36 2016 -0800

    locking/lockdep: Eliminate lockdep_init()
    
    Lockdep is initialized at compile time now.  Get rid of lockdep_init().
    
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Krinkin <krinkin.m.u@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Cc: mm-commits@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 5c03a6a9b054..f98be8383a39 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -255,9 +255,6 @@ void __init early_setup(unsigned long dt_ptr)
 	setup_paca(&boot_paca);
 	fixup_boot_paca();
 
-	/* Initialize lockdep early or else spinlocks will blow */
-	lockdep_init();
-
 	/* -------- printk is now safe to use ------- */
 
 	/* Enable early debugging if any specified (see udbg.h) */

commit 567cf94dc7801f6602a73b55f04cb096a3c351fb
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Oct 6 22:48:19 2015 -0500

    powerpc/book3e-64/kexec: Enable SMP release
    
    The SMP release mechanism for FSL book3e is different from when booting
    with normal hardware.  In theory we could simulate the normal spin
    table mechanism, but not at the addresses U-Boot put in the device tree
    -- so there'd need to be even more communication between the kernel and
    kexec to set that up.  Instead, kexec-tools will set a boolean property
    linux,booted-from-kexec in the /chosen node.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Cc: devicetree@vger.kernel.org

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 505ec2c698e0..5c03a6a9b054 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -340,11 +340,26 @@ void early_setup_secondary(void)
 #endif /* CONFIG_SMP */
 
 #if defined(CONFIG_SMP) || defined(CONFIG_KEXEC)
+static bool use_spinloop(void)
+{
+	if (!IS_ENABLED(CONFIG_PPC_BOOK3E))
+		return true;
+
+	/*
+	 * When book3e boots from kexec, the ePAPR spin table does
+	 * not get used.
+	 */
+	return of_property_read_bool(of_chosen, "linux,booted-from-kexec");
+}
+
 void smp_release_cpus(void)
 {
 	unsigned long *ptr;
 	int i;
 
+	if (!use_spinloop())
+		return;
+
 	DBG(" -> smp_release_cpus()\n");
 
 	/* All secondary cpus are spinning on a common spinloop, release them
@@ -524,7 +539,7 @@ void __init setup_system(void)
 	 * Freescale Book3e parts spin in a loop provided by firmware,
 	 * so smp_release_cpus() does nothing for them
 	 */
-#if defined(CONFIG_SMP) && !defined(CONFIG_PPC_FSL_BOOK3E)
+#if defined(CONFIG_SMP)
 	/* Release secondary cpus out of their spinloops at 0x60 now that
 	 * we can map physical -> logical CPU ids
 	 */

commit d9e1831a420267a7ced708bb259d65b0a3c0344d
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Oct 6 22:48:09 2015 -0500

    powerpc/85xx: Load all early TLB entries at once
    
    Use an AS=1 trampoline TLB entry to allow all normal TLB1 entries to
    be loaded at once.  This avoids the need to keep the translation that
    code is executing from in the same TLB entry in the final TLB
    configuration as during early boot, which in turn is helpful for
    relocatable kernels (e.g. kdump) where the kernel is not running from
    what would be the first TLB entry.
    
    On e6500, we limit map_mem_in_cams() to the primary hwthread of a
    core (the boot cpu is always considered primary, as a kdump kernel
    can be entered on any cpu).  Each TLB only needs to be set up once,
    and when we do, we don't want another thread to be running when we
    create a temporary trampoline TLB1 entry.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index bdcbb716f4d6..505ec2c698e0 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -108,6 +108,14 @@ static void setup_tlb_core_data(void)
 	for_each_possible_cpu(cpu) {
 		int first = cpu_first_thread_sibling(cpu);
 
+		/*
+		 * If we boot via kdump on a non-primary thread,
+		 * make sure we point at the thread that actually
+		 * set up this TLB.
+		 */
+		if (cpu_first_thread_sibling(boot_cpuid) == first)
+			first = boot_cpuid;
+
 		paca[cpu].tcd_ptr = &paca[first].tcd;
 
 		/*

commit 15b244a88e1b2895605be4300b40b575345bcf50
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Fri Jun 5 16:35:24 2015 +1000

    powerpc/mmu: Add userspace-to-physical addresses translation cache
    
    We are adding support for DMA memory pre-registration to be used in
    conjunction with VFIO. The idea is that the userspace which is going to
    run a guest may want to pre-register a user space memory region so
    it all gets pinned once and never goes away. Having this done,
    a hypervisor will not have to pin/unpin pages on every DMA map/unmap
    request. This is going to help with multiple pinning of the same memory.
    
    Another use of it is in-kernel real mode (mmu off) acceleration of
    DMA requests where real time translation of guest physical to host
    physical addresses is non-trivial and may fail as linux ptes may be
    temporarily invalid. Also, having cached host physical addresses
    (compared to just pinning at the start and then walking the page table
    again on every H_PUT_TCE), we can be sure that the addresses which we put
    into TCE table are the ones we already pinned.
    
    This adds a list of memory regions to mm_context_t. Each region consists
    of a header and a list of physical addresses. This adds API to:
    1. register/unregister memory regions;
    2. do final cleanup (which puts all pre-registered pages);
    3. do userspace to physical address translation;
    4. manage usage counters; multiple registration of the same memory
    is allowed (once per container).
    
    This implements 2 counters per registered memory region:
    - @mapped: incremented on every DMA mapping; decremented on unmapping;
    initialized to 1 when a region is just registered; once it becomes zero,
    no more mappings allowe;
    - @used: incremented on every "register" ioctl; decremented on
    "unregister"; unregistration is allowed for DMA mapped regions unless
    it is the very last reference. For the very last reference this checks
    that the region is still mapped and returns -EBUSY so the userspace
    gets to know that memory is still pinned and unregistration needs to
    be retried; @used remains 1.
    
    Host physical addresses are stored in vmalloc'ed array. In order to
    access these in the real mode (mmu off), there is a real_vmalloc_addr()
    helper. In-kernel acceleration patchset will move it from KVM to MMU code.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 9fe3dcdbfca7..bdcbb716f4d6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -686,6 +686,9 @@ void __init setup_arch(char **cmdline_p)
 	init_mm.brk = klimit;
 #ifdef CONFIG_PPC_64K_PAGES
 	init_mm.context.pte_frag = NULL;
+#endif
+#ifdef CONFIG_SPAPR_TCE_IOMMU
+	mm_iommu_init(&init_mm.context);
 #endif
 	irqstack_early_init();
 	exc_lvl_early_init();

commit 5c0aebf6e101e9ea6ddea7aaba13582c89206333
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Apr 15 12:17:07 2015 +1000

    powerpc: Show utsname->machine in boot-up banner
    
    Currently we print "Starting Linux PPC64" at boot. But we don't mention
    anywhere whether the kernel is big or little endian.
    
    If we print the utsname->machine value instead we get either "ppc64" or
    "ppc64le" which is much more informative, eg:
    
      Starting Linux ppc64le #1 SMP Wed Apr 15 12:12:20 AEST 2015
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index c69671c03c3b..9fe3dcdbfca7 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -523,7 +523,8 @@ void __init setup_system(void)
 	smp_release_cpus();
 #endif
 
-	pr_info("Starting Linux PPC64 %s\n", init_utsname()->version);
+	pr_info("Starting Linux %s %s\n", init_utsname()->machine,
+		 init_utsname()->version);
 
 	pr_info("-----------------------------------------------------\n");
 	pr_info("ppc64_pft_size    = 0x%llx\n", ppc64_pft_size);

commit d19d5efd8c8840aa4f38a6dfbfe500d8cc27de46
Merge: 34c9a0ffc75a 2fe0753d4940
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 16 13:53:32 2015 -0500

    Merge tag 'powerpc-4.1-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux
    
    Pull powerpc updates from Michael Ellerman:
    
     - Numerous minor fixes, cleanups etc.
    
     - More EEH work from Gavin to remove its dependency on device_nodes.
    
     - Memory hotplug implemented entirely in the kernel from Nathan
       Fontenot.
    
     - Removal of redundant CONFIG_PPC_OF by Kevin Hao.
    
     - Rewrite of VPHN parsing logic & tests from Greg Kurz.
    
     - A fix from Nish Aravamudan to reduce memory usage by clamping
       nodes_possible_map.
    
     - Support for pstore on powernv from Hari Bathini.
    
     - Removal of old powerpc specific byte swap routines by David Gibson.
    
     - Fix from Vasant Hegde to prevent the flash driver telling you it was
       flashing your firmware when it wasn't.
    
     - Patch from Ben Herrenschmidt to add an OPAL heartbeat driver.
    
     - Fix for an oops causing get/put_cpu_var() imbalance in perf by Jan
       Stancek.
    
     - Some fixes for migration from Tyrel Datwyler.
    
     - A new syscall to switch the cpu endian by Michael Ellerman.
    
     - Large series from Wei Yang to implement SRIOV, reviewed and acked by
       Bjorn.
    
     - A fix for the OPAL sensor driver from Cédric Le Goater.
    
     - Fixes to get STRICT_MM_TYPECHECKS building again by Michael Ellerman.
    
     - Large series from Daniel Axtens to make our PCI hooks per PHB rather
       than per machine.
    
     - Small patch from Sam Bobroff to explicitly abort non-suspended
       transactions on syscalls, plus a test to exercise it.
    
     - Numerous reworks and fixes for the 24x7 PMU from Sukadev Bhattiprolu.
    
     - Small patch to enable the hard lockup detector from Anton Blanchard.
    
     - Fix from Dave Olson for missing L2 cache information on some CPUs.
    
     - Some fixes from Michael Ellerman to get Cell machines booting again.
    
     - Freescale updates from Scott: Highlights include BMan device tree
       nodes, an MSI erratum workaround, a couple minor performance
       improvements, config updates, and misc fixes/cleanup.
    
    * tag 'powerpc-4.1-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux: (196 commits)
      powerpc/powermac: Fix build error seen with powermac smp builds
      powerpc/pseries: Fix compile of memory hotplug without CONFIG_MEMORY_HOTREMOVE
      powerpc: Remove PPC32 code from pseries specific find_and_init_phbs()
      powerpc/cell: Fix iommu breakage caused by controller_ops change
      powerpc/eeh: Fix crash in eeh_add_device_early() on Cell
      powerpc/perf: Cap 64bit userspace backtraces to PERF_MAX_STACK_DEPTH
      powerpc/perf/hv-24x7: Fail 24x7 initcall if create_events_from_catalog() fails
      powerpc/pseries: Correct memory hotplug locking
      powerpc: Fix missing L2 cache size in /sys/devices/system/cpu
      powerpc: Add ppc64 hard lockup detector support
      oprofile: Disable oprofile NMI timer on ppc64
      powerpc/perf/hv-24x7: Add missing put_cpu_var()
      powerpc/perf/hv-24x7: Break up single_24x7_request
      powerpc/perf/hv-24x7: Define update_event_count()
      powerpc/perf/hv-24x7: Whitespace cleanup
      powerpc/perf/hv-24x7: Define add_event_to_24x7_request()
      powerpc/perf/hv-24x7: Rename hv_24x7_event_update
      powerpc/perf/hv-24x7: Move debug prints to separate function
      powerpc/perf/hv-24x7: Drop event_24x7_request()
      powerpc/perf/hv-24x7: Use pr_devel() to log message
      ...
    
    Conflicts:
            tools/testing/selftests/powerpc/Makefile
            tools/testing/selftests/powerpc/tm/Makefile

commit c54b2bf1b5e99760d53ea0376e96a046f93df6ae
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Apr 9 12:52:56 2015 +1000

    powerpc: Add ppc64 hard lockup detector support
    
    The hard lockup detector uses a PMU event as a periodic NMI to
    detect if we are stuck (where stuck means no timer interrupts have
    occurred).
    
    Ben's rework of the ppc64 soft disable code has made ppc64 PMU
    exceptions a partial NMI. They can get disabled if an external
    interrupt comes in, but otherwise PMU interrupts will fire in
    interrupt disabled regions.
    
    We disable the hard lockup detector by default for a few reasons:
    
    - It breaks userspace event based branches on POWER8.
    - It is likely to produce false positives on KVM guests.
    - Since PMCs can only count to 2^31, counting cycles means we might
      take multiple PMU exceptions per second per hardware thread even
      if our hard lockup timeout is 10 seconds.
    
    It can be enabled via a boot option, or via procfs.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 49f553bbb360..7551e5692597 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -37,6 +37,7 @@
 #include <linux/memblock.h>
 #include <linux/hugetlb.h>
 #include <linux/memory.h>
+#include <linux/nmi.h>
 
 #include <asm/io.h>
 #include <asm/kdump.h>
@@ -779,3 +780,22 @@ unsigned long memory_block_size_bytes(void)
 struct ppc_pci_io ppc_pci_io;
 EXPORT_SYMBOL(ppc_pci_io);
 #endif
+
+#ifdef CONFIG_HARDLOCKUP_DETECTOR
+u64 hw_nmi_get_sample_period(int watchdog_thresh)
+{
+	return ppc_proc_freq * watchdog_thresh;
+}
+
+/*
+ * The hardlockup detector breaks PMU event based branches and is likely
+ * to get false positives in KVM guests, so disable it by default.
+ */
+static int __init disable_hardlockup_detector(void)
+{
+	watchdog_enable_hardlockup_detector(false);
+
+	return 0;
+}
+early_initcall(disable_hardlockup_detector);
+#endif

commit e39f223fc93580c86ccf6b3422033e349f57f0dd
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Nov 18 16:47:35 2014 +1100

    powerpc: Remove more traces of bootmem
    
    Although we are now selecting NO_BOOTMEM, we still have some traces of
    bootmem lying around. That is because even with NO_BOOTMEM there is
    still a shim that converts bootmem calls into memblock calls, but
    ultimately we want to remove all traces of bootmem.
    
    Most of the patch is conversions from alloc_bootmem() to
    memblock_virt_alloc(). In general a call such as:
    
      p = (struct foo *)alloc_bootmem(x);
    
    Becomes:
    
      p = memblock_virt_alloc(x, 0);
    
    We don't need the cast because memblock_virt_alloc() returns a void *.
    The alignment value of zero tells memblock to use the default alignment,
    which is SMP_CACHE_BYTES, the same value alloc_bootmem() uses.
    
    We remove a number of NULL checks on the result of
    memblock_virt_alloc(). That is because memblock_virt_alloc() will panic
    if it can't allocate, in exactly the same way as alloc_bootmem(), so the
    NULL checks are and always have been redundant.
    
    The memory returned by memblock_virt_alloc() is already zeroed, so we
    remove several memsets of the result of memblock_virt_alloc().
    
    Finally we convert a few uses of __alloc_bootmem(x, y, MAX_DMA_ADDRESS)
    to just plain memblock_virt_alloc(). We don't use memblock_alloc_base()
    because MAX_DMA_ADDRESS is ~0ul on powerpc, so limiting the allocation
    to that is pointless, 16XB ought to be enough for anyone.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6e5310ddf8c7..49f553bbb360 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -660,7 +660,7 @@ static void __init emergency_stack_init(void)
 }
 
 /*
- * Called into from start_kernel this initializes bootmem, which is used
+ * Called into from start_kernel this initializes memblock, which is used
  * to manage page allocation until mem_init is called.
  */
 void __init setup_arch(char **cmdline_p)

commit 21098b9e07476deb3f40acd7e51cffbffb4ef865
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Sep 17 22:15:36 2014 +1000

    powerpc: Move sparse_init() into initmem_init
    
    We did part of sparse initialisation in setup_arch and part in
    initmem_init. Put them together.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Tested-by: Emil Medve <Emilian.Medve@Freescale.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 91d9cfaffe6c..6e5310ddf8c7 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -690,7 +690,6 @@ void __init setup_arch(char **cmdline_p)
 	emergency_stack_init();
 
 	initmem_init();
-	sparse_init();
 
 #ifdef CONFIG_DUMMY_CONSOLE
 	conswitchp = &dummy_con;

commit 10239733ee8617bac3f1c1769af43a88ed979324
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Sep 17 22:15:33 2014 +1000

    powerpc: Remove bootmem allocator
    
    At the moment we transition from the memblock alloctor to the bootmem
    allocator. Gitting rid of the bootmem allocator removes a bunch of
    complicated code (most of which I owe the dubious honour of being
    responsible for writing).
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Tested-by: Emil Medve <Emilian.Medve@Freescale.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 615aa904b216..91d9cfaffe6c 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -689,8 +689,7 @@ void __init setup_arch(char **cmdline_p)
 	exc_lvl_early_init();
 	emergency_stack_init();
 
-	/* set up the bootmem stuff with available memory */
-	do_init_bootmem();
+	initmem_init();
 	sparse_init();
 
 #ifdef CONFIG_DUMMY_CONSOLE

commit 64ff91ff85b56321e65b476e335955af9bed2c66
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Oct 14 12:24:35 2014 +1100

    powerpc: Remove ppc64_boot_msg
    
    ppc64_boot_msg is meant to be a boot debug aid, but
    is only used in one spot. Get rid of it, and save
    ourseleves a couple of lines in the kernel log
    buffer.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4f3cfe1b6a33..615aa904b216 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -665,8 +665,6 @@ static void __init emergency_stack_init(void)
  */
 void __init setup_arch(char **cmdline_p)
 {
-	ppc64_boot_msg(0x12, "Setup Arch");
-
 	*cmdline_p = boot_command_line;
 
 	/*
@@ -711,33 +709,6 @@ void __init setup_arch(char **cmdline_p)
 	if ((unsigned long)_stext & 0xffff)
 		panic("Kernelbase not 64K-aligned (0x%lx)!\n",
 		      (unsigned long)_stext);
-
-	ppc64_boot_msg(0x15, "Setup Done");
-}
-
-
-/* ToDo: do something useful if ppc_md is not yet setup. */
-#define PPC64_LINUX_FUNCTION 0x0f000000
-#define PPC64_IPL_MESSAGE 0xc0000000
-#define PPC64_TERM_MESSAGE 0xb0000000
-
-static void ppc64_do_msg(unsigned int src, const char *msg)
-{
-	if (ppc_md.progress) {
-		char buf[128];
-
-		sprintf(buf, "%08X\n", src);
-		ppc_md.progress(buf, 0);
-		snprintf(buf, 128, "%s", msg);
-		ppc_md.progress(buf, 0);
-	}
-}
-
-/* Print a boot progress message. */
-void ppc64_boot_msg(unsigned int src, const char *msg)
-{
-	ppc64_do_msg(PPC64_LINUX_FUNCTION|PPC64_IPL_MESSAGE|src, msg);
-	printk("[boot]%04x %s\n", src, msg);
 }
 
 #ifdef CONFIG_SMP

commit 2c186e05a5c6dc8fcfb1e8bf6901ad1598c40db6
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Oct 13 20:21:22 2014 +1100

    powerpc: Add printk levels to setup_system output
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index cd07d79ad21c..4f3cfe1b6a33 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -522,36 +522,36 @@ void __init setup_system(void)
 	smp_release_cpus();
 #endif
 
-	printk("Starting Linux PPC64 %s\n", init_utsname()->version);
+	pr_info("Starting Linux PPC64 %s\n", init_utsname()->version);
 
-	printk("-----------------------------------------------------\n");
-	printk("ppc64_pft_size    = 0x%llx\n", ppc64_pft_size);
-	printk("phys_mem_size     = 0x%llx\n", memblock_phys_mem_size());
+	pr_info("-----------------------------------------------------\n");
+	pr_info("ppc64_pft_size    = 0x%llx\n", ppc64_pft_size);
+	pr_info("phys_mem_size     = 0x%llx\n", memblock_phys_mem_size());
 
 	if (ppc64_caches.dline_size != 0x80)
-		printk("dcache_line_size  = 0x%x\n", ppc64_caches.dline_size);
+		pr_info("dcache_line_size  = 0x%x\n", ppc64_caches.dline_size);
 	if (ppc64_caches.iline_size != 0x80)
-		printk("icache_line_size  = 0x%x\n", ppc64_caches.iline_size);
+		pr_info("icache_line_size  = 0x%x\n", ppc64_caches.iline_size);
 
-	printk("cpu_features      = 0x%016lx\n", cur_cpu_spec->cpu_features);
-	printk("  possible        = 0x%016lx\n", CPU_FTRS_POSSIBLE);
-	printk("  always          = 0x%016lx\n", CPU_FTRS_ALWAYS);
-	printk("cpu_user_features = 0x%08x 0x%08x\n", cur_cpu_spec->cpu_user_features,
+	pr_info("cpu_features      = 0x%016lx\n", cur_cpu_spec->cpu_features);
+	pr_info("  possible        = 0x%016lx\n", CPU_FTRS_POSSIBLE);
+	pr_info("  always          = 0x%016lx\n", CPU_FTRS_ALWAYS);
+	pr_info("cpu_user_features = 0x%08x 0x%08x\n", cur_cpu_spec->cpu_user_features,
 		cur_cpu_spec->cpu_user_features2);
-	printk("mmu_features      = 0x%08x\n", cur_cpu_spec->mmu_features);
-	printk("firmware_features = 0x%016lx\n", powerpc_firmware_features);
+	pr_info("mmu_features      = 0x%08x\n", cur_cpu_spec->mmu_features);
+	pr_info("firmware_features = 0x%016lx\n", powerpc_firmware_features);
 
 #ifdef CONFIG_PPC_STD_MMU_64
 	if (htab_address)
-		printk("htab_address      = 0x%p\n", htab_address);
+		pr_info("htab_address      = 0x%p\n", htab_address);
 
-	printk("htab_hash_mask    = 0x%lx\n", htab_hash_mask);
+	pr_info("htab_hash_mask    = 0x%lx\n", htab_hash_mask);
 #endif
 
 	if (PHYSICAL_START > 0)
-		printk("physical_start    = 0x%llx\n",
+		pr_info("physical_start    = 0x%llx\n",
 		       (unsigned long long)PHYSICAL_START);
-	printk("-----------------------------------------------------\n");
+	pr_info("-----------------------------------------------------\n");
 
 	DBG(" <- setup_system()\n");
 }

commit 3e47d1474c2b4099f0fadd12a6553fdb2e8feaae
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Sep 17 14:39:36 2014 +1000

    powerpc: Remove powerpc specific cmd_line
    
    There is no need for yet another copy of the command line, just
    use boot_command_line like everyone else.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 2ef9893a06bd..cd07d79ad21c 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -667,7 +667,7 @@ void __init setup_arch(char **cmdline_p)
 {
 	ppc64_boot_msg(0x12, "Setup Arch");
 
-	*cmdline_p = cmd_line;
+	*cmdline_p = boot_command_line;
 
 	/*
 	 * Set cache line size based on type of cpu as a default.

commit 87d99c0e2c2f9d1386d8e284a5fbc13e96adbe25
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Aug 6 19:08:22 2014 +1000

    powerpc/ppc64: Print CPU/MMU/FW features at boot
    
    "Helps debug funky firmware issues".
    
    After:
      Starting Linux PPC64 #108 SMP Wed Aug 6 19:04:51 EST 2014
      -----------------------------------------------------
      ppc64_pft_size    = 0x1a
      phys_mem_size     = 0x200000000
      cpu_features      = 0x17fc7a6c18500249
        possible        = 0x1fffffff18700649
        always          = 0x0000000000000040
      cpu_user_features = 0xdc0065c2 0xee000000
      mmu_features      = 0x5a000001
      firmware_features = 0x00000001405a440b
      htab_hash_mask    = 0x7ffff
      -----------------------------------------------------
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 32f4c320b9dc..2ef9893a06bd 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -533,6 +533,14 @@ void __init setup_system(void)
 	if (ppc64_caches.iline_size != 0x80)
 		printk("icache_line_size  = 0x%x\n", ppc64_caches.iline_size);
 
+	printk("cpu_features      = 0x%016lx\n", cur_cpu_spec->cpu_features);
+	printk("  possible        = 0x%016lx\n", CPU_FTRS_POSSIBLE);
+	printk("  always          = 0x%016lx\n", CPU_FTRS_ALWAYS);
+	printk("cpu_user_features = 0x%08x 0x%08x\n", cur_cpu_spec->cpu_user_features,
+		cur_cpu_spec->cpu_user_features2);
+	printk("mmu_features      = 0x%08x\n", cur_cpu_spec->mmu_features);
+	printk("firmware_features = 0x%016lx\n", powerpc_firmware_features);
+
 #ifdef CONFIG_PPC_STD_MMU_64
 	if (htab_address)
 		printk("htab_address      = 0x%p\n", htab_address);

commit bdce97e94b95db409264d5ae6badd0db7628681c
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Aug 6 19:08:21 2014 +1000

    powerpc/ppc64: Clean up the boot-time settings display
    
    At boot we display a bunch of low level settings which can be useful to
    know, and can help to spot bugs when things are fundamentally
    misconfigured.
    
    At the moment they are very widely spaced, so that we can accommodate
    the line:
    
      ppc64_caches.dcache_line_size = 0xYY
    
    But we only print that line when the cache line size is not 128, ie.
    almost never, so it just makes the display look odd usually.
    
    The ppc64_caches prefix is redundant so remove it, which means we can
    align things a bit closer for the common case. While we're there
    replace the last use of camelCase (physicalMemorySize), and use
    phys_mem_size.
    
    Before:
      Starting Linux PPC64 #104 SMP Wed Aug 6 18:41:34 EST 2014
      -----------------------------------------------------
      ppc64_pft_size                = 0x1a
      physicalMemorySize            = 0x200000000
      ppc64_caches.dcache_line_size = 0xf0
      ppc64_caches.icache_line_size = 0xf0
      htab_address                  = 0xdeadbeef
      htab_hash_mask                = 0x7ffff
      physical_start                = 0xf000bar
      -----------------------------------------------------
    
    After:
      Starting Linux PPC64 #103 SMP Wed Aug 6 18:38:04 EST 2014
      -----------------------------------------------------
      ppc64_pft_size    = 0x1a
      phys_mem_size     = 0x200000000
      dcache_line_size  = 0xf0
      icache_line_size  = 0xf0
      htab_address      = 0xdeadbeef
      htab_hash_mask    = 0x7ffff
      physical_start    = 0xf000bar
      -----------------------------------------------------
    
    This patch is final, no bike shedding ;)
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 75d62d63fe68..32f4c320b9dc 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -525,21 +525,23 @@ void __init setup_system(void)
 	printk("Starting Linux PPC64 %s\n", init_utsname()->version);
 
 	printk("-----------------------------------------------------\n");
-	printk("ppc64_pft_size                = 0x%llx\n", ppc64_pft_size);
-	printk("physicalMemorySize            = 0x%llx\n", memblock_phys_mem_size());
+	printk("ppc64_pft_size    = 0x%llx\n", ppc64_pft_size);
+	printk("phys_mem_size     = 0x%llx\n", memblock_phys_mem_size());
+
 	if (ppc64_caches.dline_size != 0x80)
-		printk("ppc64_caches.dcache_line_size = 0x%x\n",
-		       ppc64_caches.dline_size);
+		printk("dcache_line_size  = 0x%x\n", ppc64_caches.dline_size);
 	if (ppc64_caches.iline_size != 0x80)
-		printk("ppc64_caches.icache_line_size = 0x%x\n",
-		       ppc64_caches.iline_size);
+		printk("icache_line_size  = 0x%x\n", ppc64_caches.iline_size);
+
 #ifdef CONFIG_PPC_STD_MMU_64
 	if (htab_address)
-		printk("htab_address                  = 0x%p\n", htab_address);
-	printk("htab_hash_mask                = 0x%lx\n", htab_hash_mask);
-#endif /* CONFIG_PPC_STD_MMU_64 */
+		printk("htab_address      = 0x%p\n", htab_address);
+
+	printk("htab_hash_mask    = 0x%lx\n", htab_hash_mask);
+#endif
+
 	if (PHYSICAL_START > 0)
-		printk("physical_start                = 0x%llx\n",
+		printk("physical_start    = 0x%llx\n",
 		       (unsigned long long)PHYSICAL_START);
 	printk("-----------------------------------------------------\n");
 

commit 1618bd53e6f43918f90ca04a4fcaf664b0a78749
Author: Daniel Walter <dwalter@google.com>
Date:   Fri Aug 8 14:24:01 2014 -0700

    arch/powerpc: replace obsolete strict_strto* calls
    
    Replace strict_strto calls with more appropriate kstrto calls
    
    Signed-off-by: Daniel Walter <dwalter@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index d0225572faa1..75d62d63fe68 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -149,13 +149,13 @@ static void check_smt_enabled(void)
 		else if (!strcmp(smt_enabled_cmdline, "off"))
 			smt_enabled_at_boot = 0;
 		else {
-			long smt;
+			int smt;
 			int rc;
 
-			rc = strict_strtol(smt_enabled_cmdline, 10, &smt);
+			rc = kstrtoint(smt_enabled_cmdline, 10, &smt);
 			if (!rc)
 				smt_enabled_at_boot =
-					min(threads_per_core, (int)smt);
+					min(threads_per_core, smt);
 		}
 	} else {
 		dn = of_find_node_by_path("/options");

commit 9287b95ec9ded0a4458094ebd967502263d80112
Merge: ea668936b708 78eb9094ca08
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Aug 5 14:13:41 2014 +1000

    Merge remote-tracking branch 'scott/next' into next
    
    Scott writes:
    
    Highlights include e6500 hardware threading support, an e6500 TLB erratum
    workaround, corenet error reporting, support for a new board, and some
    minor fixes.

commit e16c8765533a155ebd3d7c36fc80440a03bbf46a
Author: Andy Fleming <afleming@freescale.com>
Date:   Thu Dec 8 01:20:27 2011 -0600

    powerpc/e6500: Add support for hardware threads
    
    The general idea is that each core will release all of its
    threads into the secondary thread startup code, which will
    eventually wait in the secondary core holding area, for the
    appropriate bit in the PACA to be set. The kick_cpu function
    pointer will set that bit in the PACA, and thus "release"
    the core/thread to boot. We also need to do a few things that
    U-Boot normally does for CPUs (like enable branch prediction).
    
    Signed-off-by: Andy Fleming <afleming@freescale.com>
    [scottwood@freescale.com: various changes, including only enabling
     threads if Linux wants to kick them]
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ee082d771178..6d06947e7d21 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -507,7 +507,11 @@ void __init setup_system(void)
 	check_smt_enabled();
 	setup_tlb_core_data();
 
-#ifdef CONFIG_SMP
+	/*
+	 * Freescale Book3e parts spin in a loop provided by firmware,
+	 * so smp_release_cpus() does nothing for them
+	 */
+#if defined(CONFIG_SMP) && !defined(CONFIG_PPC_FSL_BOOK3E)
 	/* Release secondary cpus out of their spinloops at 0x60 now that
 	 * we can map physical -> logical CPU ids
 	 */

commit 633440f18f5795aa28d990b92dd108486911bfd5
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Jul 17 15:29:45 2014 +1000

    powerpc: Document how we set AIL on guest kernels
    
    I spent ten minutes scratching my head, trying to work out where we
    enabled relocation on interrupts for guest kernels. Expand the doco to
    make it clear.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 694339043b56..75f94ae20e9e 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -201,7 +201,11 @@ static void cpu_ready_for_interrupts(void)
 	/* Set IR and DR in PACA MSR */
 	get_paca()->kernel_msr = MSR_KERNEL;
 
-	/* Enable AIL if supported */
+	/*
+	 * Enable AIL if supported, and we are in hypervisor mode. If we are
+	 * not in hypervisor mode, we enable relocation-on interrupts later
+	 * in pSeries_setup_arch() using the H_SET_MODE hcall.
+	 */
 	if (cpu_has_feature(CPU_FTR_HVMODE) &&
 	    cpu_has_feature(CPU_FTR_ARCH_207S)) {
 		unsigned long lpcr = mfspr(SPRN_LPCR);

commit 376af5947c0e441ccbf98f0212d4ffbf171528f6
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Jul 10 12:29:19 2014 +1000

    powerpc: Remove STAB code
    
    Old cpus didn't have a Segment Lookaside Buffer (SLB), instead they had
    a Segment Table (STAB). Now that we've dropped support for those cpus,
    we can remove the STAB support entirely.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ee082d771178..694339043b56 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -673,9 +673,6 @@ void __init setup_arch(char **cmdline_p)
 	exc_lvl_early_init();
 	emergency_stack_init();
 
-#ifdef CONFIG_PPC_STD_MMU_64
-	stabs_alloc();
-#endif
 	/* set up the bootmem stuff with available memory */
 	do_init_bootmem();
 	sparse_init();

commit a5d862576a64cb3e0c22dc9cc2170e4d750714f9
Author: Anton Blanchard <anton@au1.ibm.com>
Date:   Wed Jun 4 17:50:47 2014 +1000

    powerpc: Allow ppc_md platform hook to override memory_block_size_bytes
    
    The pseries platform code unconditionally overrides
    memory_block_size_bytes regardless of the running platform.
    
    Create a ppc_md hook that so each platform can choose to
    do what it wants.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 90b532ace0d5..ee082d771178 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -36,6 +36,7 @@
 #include <linux/lockdep.h>
 #include <linux/memblock.h>
 #include <linux/hugetlb.h>
+#include <linux/memory.h>
 
 #include <asm/io.h>
 #include <asm/kdump.h>
@@ -780,6 +781,15 @@ void __init setup_per_cpu_areas(void)
 }
 #endif
 
+#ifdef CONFIG_MEMORY_HOTPLUG_SPARSE
+unsigned long memory_block_size_bytes(void)
+{
+	if (ppc_md.memory_block_size)
+		return ppc_md.memory_block_size();
+
+	return MIN_MEMORY_BLOCK_SIZE;
+}
+#endif
 
 #if defined(CONFIG_PPC_INDIRECT_PIO) || defined(CONFIG_PPC_INDIRECT_MMIO)
 struct ppc_pci_io ppc_pci_io;

commit 2751b628c97e66e61f482935ca59148751972941
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Mar 11 11:54:06 2014 +1100

    powerpc: Fix SMP issues with ppc64le ABIv2
    
    There is no need to put a function descriptor in
    __secondary_hold_spinloop. Use ppc_function_entry to get the
    instruction address and put it in __secondary_hold_spinloop instead.
    
    Also fix an issue where we assumed cur_cpu_spec held a function
    descriptor.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index fbe24377eda3..90b532ace0d5 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -341,7 +341,7 @@ void smp_release_cpus(void)
 
 	ptr  = (unsigned long *)((unsigned long)&__secondary_hold_spinloop
 			- PHYSICAL_START);
-	*ptr = __pa(generic_secondary_smp_init);
+	*ptr = ppc_function_entry(generic_secondary_smp_init);
 
 	/* And wait a bit for them to catch up */
 	for (i = 0; i < 100000; i++) {

commit 18aa0da33e18cb2037932f7ad5c7d51f22e012f5
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Apr 11 16:43:35 2014 +1000

    powerpc: Don't try to set LPCR unless we're in hypervisor mode
    
    Commit 8f619b5429d9 ("powerpc/ppc64: Do not turn AIL (reloc-on
    interrupts) too early") added code to set the AIL bit in the LPCR
    without checking whether the kernel is running in hypervisor mode.  The
    result is that when the kernel is running as a guest (i.e., under
    PowerKVM or PowerVM), the processor takes a privileged instruction
    interrupt at that point, causing a panic.  The visible result is that
    the kernel hangs after printing "returning from prom_init".
    
    This fixes it by checking for hypervisor mode being available before
    setting LPCR.  If we are not in hypervisor mode, we enable relocation-on
    interrupts later in pSeries_setup_arch using the H_SET_MODE hcall.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 3d7a50a08f5e..fbe24377eda3 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -201,7 +201,8 @@ static void cpu_ready_for_interrupts(void)
 	get_paca()->kernel_msr = MSR_KERNEL;
 
 	/* Enable AIL if supported */
-	if (cpu_has_feature(CPU_FTR_ARCH_207S)) {
+	if (cpu_has_feature(CPU_FTR_HVMODE) &&
+	    cpu_has_feature(CPU_FTR_ARCH_207S)) {
 		unsigned long lpcr = mfspr(SPRN_LPCR);
 		mtspr(SPRN_LPCR, lpcr | LPCR_AIL_3);
 	}

commit 8f619b5429d9d852df09b85d9e41459859e04951
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Mar 28 13:36:30 2014 +1100

    powerpc/ppc64: Do not turn AIL (reloc-on interrupts) too early
    
    Turn them on at the same time as we allow MSR_IR/DR in the paca
    kernel MSR, ie, after the MMU has been setup enough to be able
    to handle relocated access to the linear mapping.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 1d33e817ab2d..3d7a50a08f5e 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -195,6 +195,18 @@ static void fixup_boot_paca(void)
 	get_paca()->data_offset = 0;
 }
 
+static void cpu_ready_for_interrupts(void)
+{
+	/* Set IR and DR in PACA MSR */
+	get_paca()->kernel_msr = MSR_KERNEL;
+
+	/* Enable AIL if supported */
+	if (cpu_has_feature(CPU_FTR_ARCH_207S)) {
+		unsigned long lpcr = mfspr(SPRN_LPCR);
+		mtspr(SPRN_LPCR, lpcr | LPCR_AIL_3);
+	}
+}
+
 /*
  * Early initialization entry point. This is called by head.S
  * with MMU translation disabled. We rely on the "feature" of
@@ -264,9 +276,9 @@ void __init early_setup(unsigned long dt_ptr)
 	/*
 	 * At this point, we can let interrupts switch to virtual mode
 	 * (the MMU has been setup), so adjust the MSR in the PACA to
-	 * have IR and DR set.
+	 * have IR and DR set and enable AIL if it exists
 	 */
-	get_paca()->kernel_msr = MSR_KERNEL;
+	cpu_ready_for_interrupts();
 
 	/* Reserve large chunks of memory for use by CMA for KVM */
 	kvm_cma_reserve();
@@ -307,7 +319,7 @@ void early_setup_secondary(void)
 	 * (the MMU has been setup), so adjust the MSR in the PACA to
 	 * have IR and DR set.
 	 */
-	get_paca()->kernel_msr = MSR_KERNEL;
+	cpu_ready_for_interrupts();
 }
 
 #endif /* CONFIG_SMP */

commit a944a9c40b81a71609692c4909bb57e1d01f4bb7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Mar 28 13:36:29 2014 +1100

    powerpc/ppc64: Gracefully handle early interrupts
    
    If we take an interrupt such as a trap caused by a BUG_ON before the
    MMU has been setup, the interrupt handlers try to enable virutal mode
    and cause a recursive crash, making the original problem very hard
    to debug.
    
    This fixes it by adjusting the "kernel_msr" value in the PACA so that
    it only has MSR_IR and MSR_DR (translation for instruction and data)
    set after the MMU has been initialized for the processor.
    
    We may still not have a console yet but at least we don't get into
    a recursive fault (and early debug console or memory dump via JTAG
    of the kernel buffer *will* give us the proper error).
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index d8aabbdc6483..1d33e817ab2d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -261,6 +261,14 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu();
 
+	/*
+	 * At this point, we can let interrupts switch to virtual mode
+	 * (the MMU has been setup), so adjust the MSR in the PACA to
+	 * have IR and DR set.
+	 */
+	get_paca()->kernel_msr = MSR_KERNEL;
+
+	/* Reserve large chunks of memory for use by CMA for KVM */
 	kvm_cma_reserve();
 
 	/*
@@ -293,6 +301,13 @@ void early_setup_secondary(void)
 
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu_secondary();
+
+	/*
+	 * At this point, we can let interrupts switch to virtual mode
+	 * (the MMU has been setup), so adjust the MSR in the PACA to
+	 * have IR and DR set.
+	 */
+	get_paca()->kernel_msr = MSR_KERNEL;
 }
 
 #endif /* CONFIG_SMP */

commit 36ae37e3436b0c7731ae15a03d9215ff24bef9f2
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Mar 28 13:36:27 2014 +1100

    powerpc: Make boot_cpuid common between 32 and 64-bit
    
    Move the definition to setup-common.c and set the init value
    to -1 on both 32 and 64-bit (it was 0 on 64-bit).
    
    Additionally add a check to prom.c to garantee that the init
    value has been udpated after the DT scan.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4933909cc5c0..d8aabbdc6483 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -74,7 +74,6 @@
 #define DBG(fmt...)
 #endif
 
-int boot_cpuid = 0;
 int spinning_secondaries;
 u64 ppc64_pft_size;
 

commit 82d86de25b9c99db546e17c6f7ebf9a691da557e
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Mar 7 14:48:35 2014 -0600

    powerpc/e6500: Make TLB lock recursive
    
    Once special level interrupts are supported, we may take nested TLB
    misses -- so allow the same thread to acquire the lock recursively.
    
    The lock will not be effective against the nested TLB miss handler
    trying to write the same entry as the interrupted TLB miss handler, but
    that's also a problem on non-threaded CPUs that lack TLB write
    conditional.  This will be addressed in the patch that enables crit/mc
    support by invalidating the TLB on return from level exceptions.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index da9c42f53bb1..4933909cc5c0 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -102,6 +102,8 @@ static void setup_tlb_core_data(void)
 {
 	int cpu;
 
+	BUILD_BUG_ON(offsetof(struct tlb_core_data, lock) != 0);
+
 	for_each_possible_cpu(cpu) {
 		int first = cpu_first_thread_sibling(cpu);
 

commit 160c73243350304c09e69cbcc4fa34ff89868b68
Author: Tiejun Chen <tiejun.chen@windriver.com>
Date:   Wed Oct 23 17:31:21 2013 +0800

    powerpc/book3e: initialize crit/mc/dbg kernel stack pointers
    
    We already allocated critical/machine/debug check exceptions, but
    we also should initialize those associated kernel stack pointers
    for use by special exceptions in the PACA.
    
    Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f5f11a7d30e5..da9c42f53bb1 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -552,14 +552,20 @@ static void __init irqstack_early_init(void)
 static void __init exc_lvl_early_init(void)
 {
 	unsigned int i;
+	unsigned long sp;
 
 	for_each_possible_cpu(i) {
-		critirq_ctx[i] = (struct thread_info *)
-			__va(memblock_alloc(THREAD_SIZE, THREAD_SIZE));
-		dbgirq_ctx[i] = (struct thread_info *)
-			__va(memblock_alloc(THREAD_SIZE, THREAD_SIZE));
-		mcheckirq_ctx[i] = (struct thread_info *)
-			__va(memblock_alloc(THREAD_SIZE, THREAD_SIZE));
+		sp = memblock_alloc(THREAD_SIZE, THREAD_SIZE);
+		critirq_ctx[i] = (struct thread_info *)__va(sp);
+		paca[i].crit_kstack = __va(sp + THREAD_SIZE);
+
+		sp = memblock_alloc(THREAD_SIZE, THREAD_SIZE);
+		dbgirq_ctx[i] = (struct thread_info *)__va(sp);
+		paca[i].dbg_kstack = __va(sp + THREAD_SIZE);
+
+		sp = memblock_alloc(THREAD_SIZE, THREAD_SIZE);
+		mcheckirq_ctx[i] = (struct thread_info *)__va(sp);
+		paca[i].mc_kstack = __va(sp + THREAD_SIZE);
 	}
 
 	if (cpu_has_feature(CPU_FTR_DEBUG_LVL_EXC))

commit 1b17366d695c8ab03f98d0155357e97a427e1dce
Merge: d12de1ef5eba 7179ba52889b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 27 21:11:26 2014 -0800

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Ben Herrenschmidt:
     "So here's my next branch for powerpc.  A bit late as I was on vacation
      last week.  It's mostly the same stuff that was in next already, I
      just added two patches today which are the wiring up of lockref for
      powerpc, which for some reason fell through the cracks last time and
      is trivial.
    
      The highlights are, in addition to a bunch of bug fixes:
    
       - Reworked Machine Check handling on kernels running without a
         hypervisor (or acting as a hypervisor).  Provides hooks to handle
         some errors in real mode such as TLB errors, handle SLB errors,
         etc...
    
       - Support for retrieving memory error information from the service
         processor on IBM servers running without a hypervisor and routing
         them to the memory poison infrastructure.
    
       - _PAGE_NUMA support on server processors
    
       - 32-bit BookE relocatable kernel support
    
       - FSL e6500 hardware tablewalk support
    
       - A bunch of new/revived board support
    
       - FSL e6500 deeper idle states and altivec powerdown support
    
      You'll notice a generic mm change here, it has been acked by the
      relevant authorities and is a pre-req for our _PAGE_NUMA support"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (121 commits)
      powerpc: Implement arch_spin_is_locked() using arch_spin_value_unlocked()
      powerpc: Add support for the optimised lockref implementation
      powerpc/powernv: Call OPAL sync before kexec'ing
      powerpc/eeh: Escalate error on non-existing PE
      powerpc/eeh: Handle multiple EEH errors
      powerpc: Fix transactional FP/VMX/VSX unavailable handlers
      powerpc: Don't corrupt transactional state when using FP/VMX in kernel
      powerpc: Reclaim two unused thread_info flag bits
      powerpc: Fix races with irq_work
      Move precessing of MCE queued event out from syscall exit path.
      pseries/cpuidle: Remove redundant call to ppc64_runlatch_off() in cpu idle routines
      powerpc: Make add_system_ram_resources() __init
      powerpc: add SATA_MV to ppc64_defconfig
      powerpc/powernv: Increase candidate fw image size
      powerpc: Add debug checks to catch invalid cpu-to-node mappings
      powerpc: Fix the setup of CPU-to-Node mappings during CPU online
      powerpc/iommu: Don't detach device without IOMMU group
      powerpc/eeh: Hotplug improvement
      powerpc/eeh: Call opal_pci_reinit() on powernv for restoring config space
      powerpc/eeh: Add restore_config operation
      ...

commit 28efc35fe68dacbddc4b12c2fa8f2df1593a4ad3
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Oct 11 19:22:38 2013 -0500

    powerpc/e6500: TLB miss handler with hardware tablewalk support
    
    There are a few things that make the existing hw tablewalk handlers
    unsuitable for e6500:
    
     - Indirect entries go in TLB1 (though the resulting direct entries go in
       TLB0).
    
     - It has threads, but no "tlbsrx." -- so we need a spinlock and
       a normal "tlbsx".  Because we need this lock, hardware tablewalk
       is mandatory on e6500 unless we want to add spinlock+tlbsx to
       the normal bolted TLB miss handler.
    
     - TLB1 has no HES (nor next-victim hint) so we need software round robin
       (TODO: integrate this round robin data with hugetlb/KVM)
    
     - The existing tablewalk handlers map half of a page table at a time,
       because IBM hardware has a fixed 1MiB indirect page size.  e6500
       has variable size indirect entries, with a minimum of 2MiB.
       So we can't do the half-page indirect mapping, and even if we
       could it would be less efficient than mapping the full page.
    
     - Like on e5500, the linear mapping is bolted, so we don't need the
       overhead of supporting nested tlb misses.
    
    Note that hardware tablewalk does not work in rev1 of e6500.
    We do not expect to support e6500 rev1 in mainline Linux.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Cc: Mihai Caraman <mihai.caraman@freescale.com>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 2232aff66059..1ce9b87d7df8 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -97,6 +97,36 @@ int dcache_bsize;
 int icache_bsize;
 int ucache_bsize;
 
+#if defined(CONFIG_PPC_BOOK3E) && defined(CONFIG_SMP)
+static void setup_tlb_core_data(void)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		int first = cpu_first_thread_sibling(cpu);
+
+		paca[cpu].tcd_ptr = &paca[first].tcd;
+
+		/*
+		 * If we have threads, we need either tlbsrx.
+		 * or e6500 tablewalk mode, or else TLB handlers
+		 * will be racy and could produce duplicate entries.
+		 */
+		if (smt_enabled_at_boot >= 2 &&
+		    !mmu_has_feature(MMU_FTR_USE_TLBRSRV) &&
+		    book3e_htw_mode != PPC_HTW_E6500) {
+			/* Should we panic instead? */
+			WARN_ONCE("%s: unsupported MMU configuration -- expect problems\n",
+				  __func__);
+		}
+	}
+}
+#else
+static void setup_tlb_core_data(void)
+{
+}
+#endif
+
 #ifdef CONFIG_SMP
 
 static char *smt_enabled_cmdline;
@@ -445,6 +475,7 @@ void __init setup_system(void)
 
 	smp_setup_cpu_maps();
 	check_smt_enabled();
+	setup_tlb_core_data();
 
 #ifdef CONFIG_SMP
 	/* Release secondary cpus out of their spinloops at 0x60 now that

commit 729b0f715371ce1e7636b4958fc45d6882442456
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Wed Oct 30 20:04:00 2013 +0530

    powerpc/book3s: Introduce exclusive emergency stack for machine check exception.
    
    This patch introduces exclusive emergency stack for machine check exception.
    We use emergency stack to handle machine check exception so that we can save
    MCE information (srr1, srr0, dar and dsisr) before turning on ME bit and be
    ready for re-entrancy. This helps us to prevent clobbering of MCE information
    in case of nested machine checks.
    
    The reason for using emergency stack over normal kernel stack is that the
    machine check might occur in the middle of setting up a stack frame which may
    result into improper use of kernel stack.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 5760b9bea936..2232aff66059 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -540,7 +540,8 @@ static void __init exc_lvl_early_init(void)
 
 /*
  * Stack space used when we detect a bad kernel stack pointer, and
- * early in SMP boots before relocation is enabled.
+ * early in SMP boots before relocation is enabled. Exclusive emergency
+ * stack for machine checks.
  */
 static void __init emergency_stack_init(void)
 {
@@ -563,6 +564,13 @@ static void __init emergency_stack_init(void)
 		sp  = memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit);
 		sp += THREAD_SIZE;
 		paca[i].emergency_sp = __va(sp);
+
+#ifdef CONFIG_PPC_BOOK3S_64
+		/* emergency stack for machine check exception handling. */
+		sp  = memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit);
+		sp += THREAD_SIZE;
+		paca[i].mc_emergency_sp = __va(sp);
+#endif
 	}
 }
 

commit 565c2f249a0cb549d419f4c92fb8642b404d42b5
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sun May 12 07:26:23 2013 +0800

    powerpc: Use patch_exception to update the debug exception handler
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4085aaa9478f..5760b9bea936 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -520,9 +520,6 @@ static void __init irqstack_early_init(void)
 #ifdef CONFIG_PPC_BOOK3E
 static void __init exc_lvl_early_init(void)
 {
-	extern unsigned int interrupt_base_book3e;
-	extern unsigned int exc_debug_debug_book3e;
-
 	unsigned int i;
 
 	for_each_possible_cpu(i) {
@@ -535,8 +532,7 @@ static void __init exc_lvl_early_init(void)
 	}
 
 	if (cpu_has_feature(CPU_FTR_DEBUG_LVL_EXC))
-		patch_branch(&interrupt_base_book3e + (0x040 / 4) + 1,
-			     (unsigned long)&exc_debug_debug_book3e, 0);
+		patch_exception(0x040, exc_debug_debug_book3e);
 }
 #else
 #define exc_lvl_early_init()

commit b71d47c14fba6270c0b5a0d56639bf042017025b
Author: Jason Baron <jbaron@akamai.com>
Date:   Mon Nov 25 23:23:11 2013 +0000

    powerpc: Clean up panic_timeout usage
    
    Default CONFIG_PANIC_TIMEOUT to 180 seconds on powerpc. The
    pSeries continue to set the timeout to 10 seconds at run-time.
    
    Thus, there's a small window where we don't have the correct
    value on pSeries, but if this is only run-time discoverable we
    don't have a better option. In any case, if the user changes the
    default setting of 180 seconds, we honor that user setting.
    
    Signed-off-by: Jason Baron <jbaron@akamai.com>
    Cc: benh@kernel.crashing.org
    Cc: paulus@samba.org
    Cc: ralf@linux-mips.org
    Cc: mpe@ellerman.id.au
    Cc: felipe.contreras@gmail.com
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/705bbe0f70fb20759151642ba0176a6414ec9f7a.1385418410.git.jbaron@akamai.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4085aaa9478f..856dd4e99bfe 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -588,9 +588,6 @@ void __init setup_arch(char **cmdline_p)
 	dcache_bsize = ppc64_caches.dline_size;
 	icache_bsize = ppc64_caches.iline_size;
 
-	/* reboot on panic */
-	panic_timeout = 180;
-
 	if (ppc_md.panic)
 		setup_panic();
 

commit b88c4767d9e2290aaa22b8b3702ad72af0ebd113
Author: Robert Jennings <rcj@linux.vnet.ibm.com>
Date:   Mon Oct 28 09:20:51 2013 -0500

    powerpc: Move local setup.h declarations to arch includes
    
    Move the few declarations from arch/powerpc/kernel/setup.h
    into arch/powerpc/include/asm/setup.h.  This resolves a
    sparse warning for arch/powerpc/mm/numa.c which defines
    do_init_bootmem() but can't include the setup.h header
    in the prior path.
    
    Resolves:
    arch/powerpc/mm/numa.c:998:13:
            warning: symbol 'do_init_bootmem' was not declared.
                     Should it be static?
    
    Signed-off-by: Robert C Jennings <rcj@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 278ca93e1f28..4085aaa9478f 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -68,8 +68,6 @@
 #include <asm/hugetlb.h>
 #include <asm/epapr_hcalls.h>
 
-#include "setup.h"
-
 #ifdef DEBUG
 #define DBG(fmt...) udbg_printf(fmt)
 #else

commit 39eda2aba6be642b71f2e0ad623dcb09fd9d79cf
Merge: 2e515bf096c2 9f24b0c9ef9b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 6 10:49:42 2013 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Ben Herrenschmidt:
     "Here's the powerpc batch for this merge window.  Some of the
      highlights are:
    
       - A bunch of endian fixes ! We don't have full LE support yet in that
         release but this contains a lot of fixes all over arch/powerpc to
         use the proper accessors, call the firmware with the right endian
         mode, etc...
    
       - A few updates to our "powernv" platform (non-virtualized, the one
         to run KVM on), among other, support for bridging the P8 LPC bus
         for UARTs, support and some EEH fixes.
    
       - Some mpc51xx clock API cleanups in preparation for a clock API
         overhaul
    
       - A pile of cleanups of our old math emulation code, including better
         support for using it to emulate optional FP instructions on
         embedded chips that otherwise have a HW FPU.
    
       - Some infrastructure in selftest, for powerpc now, but could be
         generalized, initially used by some tests for our perf instruction
         counting code.
    
       - A pile of fixes for hotplug on pseries (that was seriously
         bitrotting)
    
       - The usual slew of freescale embedded updates, new boards, 64-bit
         hiberation support, e6500 core PMU support, etc..."
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (146 commits)
      powerpc: Correct FSCR bit definitions
      powerpc/xmon: Fix printing of set of CPUs in xmon
      powerpc/pseries: Move lparcfg.c to platforms/pseries
      powerpc/powernv: Return secondary CPUs to firmware on kexec
      powerpc/btext: Fix CONFIG_PPC_EARLY_DEBUG_BOOTX on ppc32
      powerpc: Cleanup handling of the DSCR bit in the FSCR register
      powerpc/pseries: Child nodes are not detached by dlpar_detach_node
      powerpc/pseries: Add mising of_node_put in delete_dt_node
      powerpc/pseries: Make dlpar_configure_connector parent node aware
      powerpc/pseries: Do all node initialization in dlpar_parse_cc_node
      powerpc/pseries: Fix parsing of initial node path in update_dt_node
      powerpc/pseries: Pack update_props_workarea to map correctly to rtas buffer header
      powerpc/pseries: Fix over writing of rtas return code in update_dt_node
      powerpc/pseries: Fix creation of loop in device node property list
      powerpc: Skip emulating & leave interrupts off for kernel program checks
      powerpc: Add more exception trampolines for hypervisor exceptions
      powerpc: Fix location and rename exception trampolines
      powerpc: Add more trap names to xmon
      powerpc/pseries: Add a warning in the case of cross-cpu VPA registration
      powerpc: Update the 00-Index in Documentation/powerpc
      ...

commit bf550fc93d9855872a95e69e4002256110d89858
Merge: 7e48c101e0c5 cc2df20c7c4c
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Aug 29 00:41:59 2013 +0200

    Merge remote-tracking branch 'origin/next' into kvm-ppc-next
    
    Conflicts:
            mm/Kconfig
    
    CMA DMA split and ZSWAP introduction were conflicting, fix up manually.

commit 7946d5a513e510269d6e0126597f8667c886d0d7
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 7 02:01:30 2013 +1000

    powerpc: Make cache info device tree accesses endian safe
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 9d5bae118d7e..45f2d1fac670 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -320,14 +320,14 @@ static void __init initialize_cache_info(void)
 		 * d-cache and i-cache sizes... -Peter
 		 */
 		if (num_cpus == 1) {
-			const u32 *sizep, *lsizep;
+			const __be32 *sizep, *lsizep;
 			u32 size, lsize;
 
 			size = 0;
 			lsize = cur_cpu_spec->dcache_bsize;
 			sizep = of_get_property(np, "d-cache-size", NULL);
 			if (sizep != NULL)
-				size = *sizep;
+				size = be32_to_cpu(*sizep);
 			lsizep = of_get_property(np, "d-cache-block-size",
 						 NULL);
 			/* fallback if block size missing */
@@ -336,7 +336,7 @@ static void __init initialize_cache_info(void)
 							 "d-cache-line-size",
 							 NULL);
 			if (lsizep != NULL)
-				lsize = *lsizep;
+				lsize = be32_to_cpu(*lsizep);
 			if (sizep == NULL || lsizep == NULL)
 				DBG("Argh, can't find dcache properties ! "
 				    "sizep: %p, lsizep: %p\n", sizep, lsizep);
@@ -350,7 +350,7 @@ static void __init initialize_cache_info(void)
 			lsize = cur_cpu_spec->icache_bsize;
 			sizep = of_get_property(np, "i-cache-size", NULL);
 			if (sizep != NULL)
-				size = *sizep;
+				size = be32_to_cpu(*sizep);
 			lsizep = of_get_property(np, "i-cache-block-size",
 						 NULL);
 			if (lsizep == NULL)
@@ -358,7 +358,7 @@ static void __init initialize_cache_info(void)
 							 "i-cache-line-size",
 							 NULL);
 			if (lsizep != NULL)
-				lsize = *lsizep;
+				lsize = be32_to_cpu(*lsizep);
 			if (sizep == NULL || lsizep == NULL)
 				DBG("Argh, can't find icache properties ! "
 				    "sizep: %p, lsizep: %p\n", sizep, lsizep);

commit ecd73cc5c9e137559f4625b347f20cf9ed0de3d5
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Jul 15 13:03:08 2013 +1000

    powerpc: Better split CONFIG_PPC_INDIRECT_PIO and CONFIG_PPC_INDIRECT_MMIO
    
    Remove the generic PPC_INDIRECT_IO and ensure we only add overhead
    to the right accessors. IE. If only CONFIG_PPC_INDIRECT_PIO is set,
    we don't add overhead to all MMIO accessors.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 0c0450dd4d31..9d5bae118d7e 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -716,8 +716,7 @@ void __init setup_per_cpu_areas(void)
 #endif
 
 
-#ifdef CONFIG_PPC_INDIRECT_IO
+#if defined(CONFIG_PPC_INDIRECT_PIO) || defined(CONFIG_PPC_INDIRECT_MMIO)
 struct ppc_pci_io ppc_pci_io;
 EXPORT_SYMBOL(ppc_pci_io);
-#endif /* CONFIG_PPC_INDIRECT_IO */
-
+#endif

commit 7191b615759ec10cab9eea43be5ecc42cda82364
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jul 25 12:12:32 2013 +1000

    powerpc/pmac: Early debug output on screen on 64-bit macs
    
    We have a bunch of CONFIG_PPC_EARLY_DEBUG_* options that are intended
    for bringup/debug only. They hard wire a machine specific udbg backend
    very early on (before we even probe the platform), and use whatever
    tricks are available on each machine/cpu to be able to get some kind
    of output out there early on.
    
    So far, on powermac with no serial ports, we have CONFIG_PPC_EARLY_DEBUG_BOOTX
    to use the low-level btext engine on the screen, but it doesn't do much, at
    least on 64-bit. It only really gets enabled after the platform has been
    probed and the MMU enabled.
    
    This adds a way to enable it much earlier. From prom_init.c (while still
    running with Open Firmware), we grab the screen details and set things up
    using the physical address of the frame buffer.
    
    Then btext itself uses the "rm_ci" feature of the 970 processor (Real
    Mode Cache Inhibited) to access it while in real mode.
    
    We need to do a little bit of reorg of the btext code to inline things
    better, in order to limit how much we touch memory while in this mode as
    the consequences might be ... interesting.
    
    This successfully allowed me to debug problems early on with the G5
    (related to gold being broken vs. ppc64 kernels).
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 79ba9b77fe72..0c0450dd4d31 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -10,7 +10,7 @@
  *      2 of the License, or (at your option) any later version.
  */
 
-#undef DEBUG
+#define DEBUG
 
 #include <linux/export.h>
 #include <linux/string.h>
@@ -240,6 +240,18 @@ void __init early_setup(unsigned long dt_ptr)
 	reserve_hugetlb_gpages();
 
 	DBG(" <- early_setup()\n");
+
+#ifdef CONFIG_PPC_EARLY_DEBUG_BOOTX
+	/*
+	 * This needs to be done *last* (after the above DBG() even)
+	 *
+	 * Right after we return from this function, we turn on the MMU
+	 * which means the real-mode access trick that btext does will
+	 * no longer work, it needs to switch to using a real MMU
+	 * mapping. This call will ensure that it does
+	 */
+	btext_map();
+#endif /* CONFIG_PPC_EARLY_DEBUG_BOOTX */
 }
 
 #ifdef CONFIG_SMP

commit b0d436c739b0d4afcdfe2e97d4d1ee41ea2db62e
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 7 02:01:24 2013 +1000

    powerpc: Fix a number of sparse warnings
    
    Address some of the trivial sparse warnings in arch/powerpc.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f03770e0fc8d..79ba9b77fe72 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -325,7 +325,7 @@ static void __init initialize_cache_info(void)
 							 NULL);
 			if (lsizep != NULL)
 				lsize = *lsizep;
-			if (sizep == 0 || lsizep == 0)
+			if (sizep == NULL || lsizep == NULL)
 				DBG("Argh, can't find dcache properties ! "
 				    "sizep: %p, lsizep: %p\n", sizep, lsizep);
 
@@ -347,7 +347,7 @@ static void __init initialize_cache_info(void)
 							 NULL);
 			if (lsizep != NULL)
 				lsize = *lsizep;
-			if (sizep == 0 || lsizep == 0)
+			if (sizep == NULL || lsizep == NULL)
 				DBG("Argh, can't find icache properties ! "
 				    "sizep: %p, lsizep: %p\n", sizep, lsizep);
 

commit 4e21b94c9c644c43223878f4c848e852743e789c
Author: Laurentiu TUDOR <Laurentiu.Tudor@freescale.com>
Date:   Wed Jul 3 17:13:15 2013 +0300

    powerpc/85xx: Move ePAPR paravirt initialization earlier
    
    At console init, when the kernel tries to flush the log buffer
    the ePAPR byte-channel based console write fails silently,
    losing the buffered messages.
    This happens because The ePAPR para-virtualization init isn't
    done early enough so that the hcall instruction to be set,
    causing the byte-channel write hcall to be a nop.
    To fix, change the ePAPR para-virt init to use early device
    tree functions and move it in early init.
    
    Signed-off-by: Laurentiu Tudor <Laurentiu.Tudor@freescale.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 389fb8077cc9..f03770e0fc8d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -66,6 +66,7 @@
 #include <asm/code-patching.h>
 #include <asm/kvm_ppc.h>
 #include <asm/hugetlb.h>
+#include <asm/epapr_hcalls.h>
 
 #include "setup.h"
 
@@ -215,6 +216,8 @@ void __init early_setup(unsigned long dt_ptr)
 	 */
 	early_init_devtree(__va(dt_ptr));
 
+	epapr_paravirt_early_init();
+
 	/* Now we know the logical id of our boot cpu, setup the paca. */
 	setup_paca(&paca[boot_cpuid]);
 	fixup_boot_paca();

commit 6c45b810989d1c04194499d666f695d3f811965f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 2 11:15:17 2013 +0530

    powerpc/kvm: Contiguous memory allocator based RMA allocation
    
    Older version of power architecture use Real Mode Offset register and Real Mode Limit
    Selector for mapping guest Real Mode Area. The guest RMA should be physically
    contigous since we use the range when address translation is not enabled.
    
    This patch switch RMA allocation code to use contigous memory allocator. The patch
    also remove the the linear allocator which not used any more
    
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ee28d1f4b853..8a022f5de0ef 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -611,8 +611,6 @@ void __init setup_arch(char **cmdline_p)
 	/* Initialize the MMU context management stuff */
 	mmu_context_init();
 
-	kvm_linear_init();
-
 	/* Interrupt code needs to be 64K-aligned */
 	if ((unsigned long)_stext & 0xffff)
 		panic("Kernelbase not 64K-aligned (0x%lx)!\n",

commit fa61a4e376d2129690c82dfb05b31705a67d6e0b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 2 11:15:16 2013 +0530

    powerpc/kvm: Contiguous memory allocator based hash page table allocation
    
    Powerpc architecture uses a hash based page table mechanism for mapping virtual
    addresses to physical address. The architecture require this hash page table to
    be physically contiguous. With KVM on Powerpc currently we use early reservation
    mechanism for allocating guest hash page table. This implies that we need to
    reserve a big memory region to ensure we can create large number of guest
    simultaneously with KVM on Power. Another disadvantage is that the reserved memory
    is not available to rest of the subsystems and and that implies we limit the total
    available memory in the host.
    
    This patch series switch the guest hash page table allocation to use
    contiguous memory allocator.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index e379d3fd1694..ee28d1f4b853 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -229,6 +229,8 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu();
 
+	kvm_cma_reserve();
+
 	/*
 	 * Reserve any gigantic pages requested on the command line.
 	 * memblock needs to have been initialized by the time this is

commit 8246aca7058f3f2c2ae503081777965cd8df7b90
Author: Chen Gang <gang.chen@asianux.com>
Date:   Wed Mar 20 14:30:12 2013 +0800

    powerpc/smp: Section mismatch from smp_release_cpus to __initdata spinning_secondaries
    
    the smp_release_cpus is a normal funciton and called in normal environments,
      but it calls the __initdata spinning_secondaries.
      need modify spinning_secondaries to match smp_release_cpus.
    
    the related warning:
      (the linker report boot_paca.33377, but it should be spinning_secondaries)
    
    -----------------------------------------------------------------------------
    
    WARNING: arch/powerpc/kernel/built-in.o(.text+0x23176): Section mismatch in reference from the function .smp_release_cpus() to the variable .init.data:boot_paca.33377
    The function .smp_release_cpus() references
    the variable __initdata boot_paca.33377.
    This is often because .smp_release_cpus lacks a __initdata
    annotation or the annotation of boot_paca.33377 is wrong.
    
    WARNING: arch/powerpc/kernel/built-in.o(.text+0x231fe): Section mismatch in reference from the function .smp_release_cpus() to the variable .init.data:boot_paca.33377
    The function .smp_release_cpus() references
    the variable __initdata boot_paca.33377.
    This is often because .smp_release_cpus lacks a __initdata
    annotation or the annotation of boot_paca.33377 is wrong.
    
    -----------------------------------------------------------------------------
    
    Signed-off-by: Chen Gang <gang.chen@asianux.com>
    CC: <stable@vger.kernel.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index e379d3fd1694..389fb8077cc9 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -76,7 +76,7 @@
 #endif
 
 int boot_cpuid = 0;
-int __initdata spinning_secondaries;
+int spinning_secondaries;
 u64 ppc64_pft_size;
 
 /* Pick defaults since we might want to patch instructions

commit 5c1f6ee9a31cbdac90bbb8ae1ba4475031ac74b4
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Sun Apr 28 09:37:33 2013 +0000

    powerpc: Reduce PTE table memory wastage
    
    We allocate one page for the last level of linux page table. With THP and
    large page size of 16MB, that would mean we are wasting large part
    of that page. To map 16MB area, we only need a PTE space of 2K with 64K
    page size. This patch reduce the space wastage by sharing the page
    allocated for the last level of linux page table with multiple pmd
    entries. We call these smaller chunks PTE page fragments and allocated
    page, PTE page.
    
    In order to support systems which doesn't have 64K HPTE support, we also
    add another 2K to PTE page fragment. The second half of the PTE fragments
    is used for storing slot and secondary bit information of an HPTE. With this
    we now have a 4K PTE fragment.
    
    We use a simple approach to share the PTE page. On allocation, we bump the
    PTE page refcount to 16 and share the PTE page with the next 16 pte alloc
    request. This should help in the node locality of the PTE page fragment,
    assuming that the immediate pte alloc request will mostly come from the
    same NUMA node. We don't try to reuse the freed PTE page fragment. Hence
    we could be waisting some space.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 75fbaceb5c87..e379d3fd1694 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -583,7 +583,9 @@ void __init setup_arch(char **cmdline_p)
 	init_mm.end_code = (unsigned long) _etext;
 	init_mm.end_data = (unsigned long) _edata;
 	init_mm.brk = klimit;
-	
+#ifdef CONFIG_PPC_64K_PAGES
+	init_mm.context.pte_frag = NULL;
+#endif
 	irqstack_early_init();
 	exc_lvl_early_init();
 	emergency_stack_init();

commit 25e138149c19fa0680147b825be475f5fd57f155
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Feb 12 14:44:50 2013 +0000

    powerpc: Apply early paca fixups to boot_paca and the boot cpu's paca
    
    In commit 466921c we added a hack to set the paca data_offset to zero so
    that per-cpu accesses would work on the boot cpu prior to per-cpu areas
    being setup. This fixed a problem with lockdep touching per-cpu areas
    very early in boot.
    
    However if we combine CONFIG_LOCK_STAT=y with any of the PPC_EARLY_DEBUG
    options, we can hit the same problem in udbg_early_init(). To avoid that
    we need to set the data_offset of the boot_paca also. So factor out the
    fixup logic and call it for both the boot_paca, and "the paca of the
    boot cpu".
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Tested-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f2514e13062b..75fbaceb5c87 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -156,6 +156,15 @@ early_param("smt-enabled", early_smt_enabled);
 #define check_smt_enabled()
 #endif /* CONFIG_SMP */
 
+/** Fix up paca fields required for the boot cpu */
+static void fixup_boot_paca(void)
+{
+	/* The boot cpu is started */
+	get_paca()->cpu_start = 1;
+	/* Allow percpu accesses to work until we setup percpu data */
+	get_paca()->data_offset = 0;
+}
+
 /*
  * Early initialization entry point. This is called by head.S
  * with MMU translation disabled. We rely on the "feature" of
@@ -187,6 +196,7 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Assume we're on cpu 0 for now. Don't write to the paca yet! */
 	initialise_paca(&boot_paca, 0);
 	setup_paca(&boot_paca);
+	fixup_boot_paca();
 
 	/* Initialize lockdep early or else spinlocks will blow */
 	lockdep_init();
@@ -207,11 +217,7 @@ void __init early_setup(unsigned long dt_ptr)
 
 	/* Now we know the logical id of our boot cpu, setup the paca. */
 	setup_paca(&paca[boot_cpuid]);
-
-	/* Fix up paca fields required for the boot cpu */
-	get_paca()->cpu_start = 1;
-	/* Allow percpu accesses to "work" until we setup percpu data */
-	get_paca()->data_offset = 0;
+	fixup_boot_paca();
 
 	/* Probe the machine type */
 	probe_machine();

commit 6a7e406419d8b176efbc5be41a82299025ad1b43
Author: Geoff Levand <geoff@infradead.org>
Date:   Wed Feb 13 17:03:16 2013 +0000

    powerpc: Move boot_paca into early_setup
    
    The powerpc boot_paca symbol is now only used within the
    early_setup() routine, so move it from its global definition
    into early_setup().
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6da881b35dac..f2514e13062b 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -177,6 +177,8 @@ early_param("smt-enabled", early_smt_enabled);
 
 void __init early_setup(unsigned long dt_ptr)
 {
+	static __initdata struct paca_struct boot_paca;
+
 	/* -------- printk is _NOT_ safe to use here ! ------- */
 
 	/* Identify CPU type */

commit 61e2390ede3cea186cc01f5f3d0c9eb570c42c40
Author: Michael Neuling <mikey@neuling.org>
Date:   Mon Nov 5 17:10:35 2012 +1100

    powerpc: Make load_hander handle upto 64k offset
    
    If we change load_hander() to use an ori instead of addi, we can load handlers
    upto 64k away provided we are still 64k aligned.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index efb6a41b3131..6da881b35dac 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -601,6 +601,11 @@ void __init setup_arch(char **cmdline_p)
 
 	kvm_linear_init();
 
+	/* Interrupt code needs to be 64K-aligned */
+	if ((unsigned long)_stext & 0xffff)
+		panic("Kernelbase not 64K-aligned (0x%lx)!\n",
+		      (unsigned long)_stext);
+
 	ppc64_boot_msg(0x15, "Setup Done");
 }
 

commit 466921c5a4669f4315528a25f9afd66601ce2c04
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Sep 20 22:07:58 2012 +0000

    powerpc: Set paca->data_offset = 0 for boot cpu
    
    In commit 407821a we assigned a poison value to the paca->data_offset.
    
    Unfortunately with CONFIG_LOCK_STAT=y lockdep will read & write to percpu
    data very early in boot, prior to us initialising the percpu areas,
    leading to a crash.
    
    We have been getting away with this because the data_offset was previously
    set to zero. This causes lockdep to read & write to the initial copy of
    the percpu variables, which are discarded later in boot.
    
    Although that is "fishy", it does work, and for lock statistics it is no
    big deal to discard the counts from early boot.
    
    So set the paca->data_offset = 0 for the boot cpu paca only.
    
    Reported-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 389bd4f0cdb1..efb6a41b3131 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -208,6 +208,8 @@ void __init early_setup(unsigned long dt_ptr)
 
 	/* Fix up paca fields required for the boot cpu */
 	get_paca()->cpu_start = 1;
+	/* Allow percpu accesses to "work" until we setup percpu data */
+	get_paca()->data_offset = 0;
 
 	/* Probe the machine type */
 	probe_machine();

commit 0195c00244dc2e9f522475868fa278c473ba7339
Merge: f21ce8f8447c 141124c02059
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 15:58:21 2012 -0700

    Merge tag 'split-asm_system_h-for-linus-20120328' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-asm_system
    
    Pull "Disintegrate and delete asm/system.h" from David Howells:
     "Here are a bunch of patches to disintegrate asm/system.h into a set of
      separate bits to relieve the problem of circular inclusion
      dependencies.
    
      I've built all the working defconfigs from all the arches that I can
      and made sure that they don't break.
    
      The reason for these patches is that I recently encountered a circular
      dependency problem that came about when I produced some patches to
      optimise get_order() by rewriting it to use ilog2().
    
      This uses bitops - and on the SH arch asm/bitops.h drags in
      asm-generic/get_order.h by a circuituous route involving asm/system.h.
    
      The main difficulty seems to be asm/system.h.  It holds a number of
      low level bits with no/few dependencies that are commonly used (eg.
      memory barriers) and a number of bits with more dependencies that
      aren't used in many places (eg.  switch_to()).
    
      These patches break asm/system.h up into the following core pieces:
    
        (1) asm/barrier.h
    
            Move memory barriers here.  This already done for MIPS and Alpha.
    
        (2) asm/switch_to.h
    
            Move switch_to() and related stuff here.
    
        (3) asm/exec.h
    
            Move arch_align_stack() here.  Other process execution related bits
            could perhaps go here from asm/processor.h.
    
        (4) asm/cmpxchg.h
    
            Move xchg() and cmpxchg() here as they're full word atomic ops and
            frequently used by atomic_xchg() and atomic_cmpxchg().
    
        (5) asm/bug.h
    
            Move die() and related bits.
    
        (6) asm/auxvec.h
    
            Move AT_VECTOR_SIZE_ARCH here.
    
      Other arch headers are created as needed on a per-arch basis."
    
    Fixed up some conflicts from other header file cleanups and moving code
    around that has happened in the meantime, so David's testing is somewhat
    weakened by that.  We'll find out anything that got broken and fix it..
    
    * tag 'split-asm_system_h-for-linus-20120328' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-asm_system: (38 commits)
      Delete all instances of asm/system.h
      Remove all #inclusions of asm/system.h
      Add #includes needed to permit the removal of asm/system.h
      Move all declarations of free_initmem() to linux/mm.h
      Disintegrate asm/system.h for OpenRISC
      Split arch_align_stack() out from asm-generic/system.h
      Split the switch_to() wrapper out of asm-generic/system.h
      Move the asm-generic/system.h xchg() implementation to asm-generic/cmpxchg.h
      Create asm-generic/barrier.h
      Make asm-generic/cmpxchg.h #include asm-generic/cmpxchg-local.h
      Disintegrate asm/system.h for Xtensa
      Disintegrate asm/system.h for Unicore32 [based on ver #3, changed by gxt]
      Disintegrate asm/system.h for Tile
      Disintegrate asm/system.h for Sparc
      Disintegrate asm/system.h for SH
      Disintegrate asm/system.h for Score
      Disintegrate asm/system.h for S390
      Disintegrate asm/system.h for PowerPC
      Disintegrate asm/system.h for PA-RISC
      Disintegrate asm/system.h for MN10300
      ...

commit ae3a197e3d0bfe3f4bf1693723e82dc018c096f3
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:02 2012 +0100

    Disintegrate asm/system.h for PowerPC
    
    Disintegrate asm/system.h for PowerPC.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    cc: linuxppc-dev@lists.ozlabs.org

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4cb8f1e9d044..6d639b2d24c6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -52,7 +52,6 @@
 #include <asm/btext.h>
 #include <asm/nvram.h>
 #include <asm/setup.h>
-#include <asm/system.h>
 #include <asm/rtas.h>
 #include <asm/iommu.h>
 #include <asm/serial.h>

commit b4e706111d501991c59d2af23a299ab52a06b03d
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jan 16 16:50:10 2012 +0100

    KVM: PPC: Convert RMA allocation into generic code
    
    We have code to allocate big chunks of linear memory on bootup for later use.
    This code is currently used for RMA allocation, but can be useful beyond that
    extent.
    
    Make it generic so we can reuse it for other stuff later.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4cb8f1e9d044..4721b0c8d7b7 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -598,7 +598,7 @@ void __init setup_arch(char **cmdline_p)
 	/* Initialize the MMU context management stuff */
 	mmu_context_init();
 
-	kvm_rma_init();
+	kvm_linear_init();
 
 	ppc64_boot_msg(0x15, "Setup Done");
 }

commit a6146888be0aa80ea41c99178d7d2e08efc776b5
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Mon Oct 10 10:50:43 2011 +0000

    powerpc: Add gpages reservation code for 64-bit FSL BOOKE
    
    For 64-bit FSL_BOOKE implementations, gigantic pages need to be
    reserved at boot time by the memblock code based on the command line.
    This adds the call that handles the reservation, and fixes some code
    comments.
    
    It also removes the previous pr_err when reserve_hugetlb_gpages
    is called on a system without hugetlb enabled - the way the code is
    structured, the call is unconditional and the resulting error message
    spurious and confusing.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index fb9bb46e7e88..4cb8f1e9d044 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -35,6 +35,8 @@
 #include <linux/pci.h>
 #include <linux/lockdep.h>
 #include <linux/memblock.h>
+#include <linux/hugetlb.h>
+
 #include <asm/io.h>
 #include <asm/kdump.h>
 #include <asm/prom.h>
@@ -64,6 +66,7 @@
 #include <asm/mmu_context.h>
 #include <asm/code-patching.h>
 #include <asm/kvm_ppc.h>
+#include <asm/hugetlb.h>
 
 #include "setup.h"
 
@@ -217,6 +220,13 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Initialize the hash table or TLB handling */
 	early_init_mmu();
 
+	/*
+	 * Reserve any gigantic pages requested on the command line.
+	 * memblock needs to have been initialized by the time this is
+	 * called since this will reserve memory.
+	 */
+	reserve_hugetlb_gpages();
+
 	DBG(" <- early_setup()\n");
 }
 

commit d715e433b7ad19c02fc4becf0d5e9a59f97925de
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Nov 14 12:54:47 2011 +0000

    powerpc: Copy down exception vectors after feature fixups
    
    kdump fails because we try to execute an HV only instruction. Feature
    fixups are being applied after we copy the exception vectors down to 0
    so they miss out on any updates.
    
    We have always had this issue but it only became critical in v3.0
    when we added CFAR support (breaks POWER5) and v3.1 when we added
    POWERNV (breaks everyone).
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: <stable@kernel.org> [v3.0+]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 1a9dea80a69b..fb9bb46e7e88 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -359,6 +359,7 @@ void __init setup_system(void)
 			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
 	do_lwsync_fixups(cur_cpu_spec->cpu_features,
 			 &__start___lwsync_fixup, &__stop___lwsync_fixup);
+	do_final_fixups();
 
 	/*
 	 * Unflatten the device-tree passed by prom_init or kexec

commit 32aaeffbd4a7457bf2f7448b33b5946ff2a960eb
Merge: 208bca086040 67b84999b1a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 19:44:47 2011 -0800

    Merge branch 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    * 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux: (230 commits)
      Revert "tracing: Include module.h in define_trace.h"
      irq: don't put module.h into irq.h for tracking irqgen modules.
      bluetooth: macroize two small inlines to avoid module.h
      ip_vs.h: fix implicit use of module_get/module_put from module.h
      nf_conntrack.h: fix up fallout from implicit moduleparam.h presence
      include: replace linux/module.h with "struct module" wherever possible
      include: convert various register fcns to macros to avoid include chaining
      crypto.h: remove unused crypto_tfm_alg_modname() inline
      uwb.h: fix implicit use of asm/page.h for PAGE_SIZE
      pm_runtime.h: explicitly requires notifier.h
      linux/dmaengine.h: fix implicit use of bitmap.h and asm/page.h
      miscdevice.h: fix up implicit use of lists and types
      stop_machine.h: fix implicit use of smp.h for smp_processor_id
      of: fix implicit use of errno.h in include/linux/of.h
      of_platform.h: delete needless include <linux/module.h>
      acpi: remove module.h include from platform/aclinux.h
      miscdevice.h: delete unnecessary inclusion of module.h
      device_cgroup.h: delete needless include <linux/module.h>
      net: sch_generic remove redundant use of <linux/module.h>
      net: inet_timewait_sock doesnt need <linux/module.h>
      ...
    
    Fix up trivial conflicts (other header files, and  removal of the ab3550 mfd driver) in
     - drivers/media/dvb/frontends/dibx000_common.c
     - drivers/media/video/{mt9m111.c,ov6650.c}
     - drivers/mfd/ab3550-core.c
     - include/linux/dmaengine.h

commit 4b16f8e2d6d64249f0ed3ca7fe2a319d0dde2719
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jul 22 18:24:23 2011 -0400

    powerpc: various straight conversions from module.h --> export.h
    
    All these files were including module.h just for the basic
    EXPORT_SYMBOL infrastructure.  We can shift them off to the
    export.h header which is a way smaller footprint and thus
    realize some compile time gains.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index aebef1320ed7..dc6655594e9d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -12,7 +12,7 @@
 
 #undef DEBUG
 
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/string.h>
 #include <linux/sched.h>
 #include <linux/init.h>

commit dfbe93a222e74b6f96ad84eff2b04a0f864fac65
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 10 20:44:23 2011 +0000

    powerpc: Coding style cleanups
    
    While converting code to use for_each_node_by_type I noticed a
    number of coding style issues.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index eade1fd8ee2e..d4168c93098f 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -281,11 +281,11 @@ static void __init initialize_cache_info(void)
 	for_each_node_by_type(np, "cpu") {
 		num_cpus += 1;
 
-		/* We're assuming *all* of the CPUs have the same
+		/*
+		 * We're assuming *all* of the CPUs have the same
 		 * d-cache and i-cache sizes... -Peter
 		 */
-
-		if ( num_cpus == 1 ) {
+		if (num_cpus == 1) {
 			const u32 *sizep, *lsizep;
 			u32 size, lsize;
 
@@ -294,10 +294,13 @@ static void __init initialize_cache_info(void)
 			sizep = of_get_property(np, "d-cache-size", NULL);
 			if (sizep != NULL)
 				size = *sizep;
-			lsizep = of_get_property(np, "d-cache-block-size", NULL);
+			lsizep = of_get_property(np, "d-cache-block-size",
+						 NULL);
 			/* fallback if block size missing */
 			if (lsizep == NULL)
-				lsizep = of_get_property(np, "d-cache-line-size", NULL);
+				lsizep = of_get_property(np,
+							 "d-cache-line-size",
+							 NULL);
 			if (lsizep != NULL)
 				lsize = *lsizep;
 			if (sizep == 0 || lsizep == 0)
@@ -314,9 +317,12 @@ static void __init initialize_cache_info(void)
 			sizep = of_get_property(np, "i-cache-size", NULL);
 			if (sizep != NULL)
 				size = *sizep;
-			lsizep = of_get_property(np, "i-cache-block-size", NULL);
+			lsizep = of_get_property(np, "i-cache-block-size",
+						 NULL);
 			if (lsizep == NULL)
-				lsizep = of_get_property(np, "i-cache-line-size", NULL);
+				lsizep = of_get_property(np,
+							 "i-cache-line-size",
+							 NULL);
 			if (lsizep != NULL)
 				lsize = *lsizep;
 			if (sizep == 0 || lsizep == 0)

commit 94db7c5e14f44b943febe54e089d077cd983d284
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 10 20:44:22 2011 +0000

    powerpc: Use for_each_node_by_type instead of open coding it
    
    Use for_each_node_by_type instead of open coding it.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index aebef1320ed7..eade1fd8ee2e 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -278,7 +278,7 @@ static void __init initialize_cache_info(void)
 
 	DBG(" -> initialize_cache_info()\n");
 
-	for (np = NULL; (np = of_find_node_by_type(np, "cpu"));) {
+	for_each_node_by_type(np, "cpu") {
 		num_cpus += 1;
 
 		/* We're assuming *all* of the CPUs have the same

commit 184475029a724b6b900d88fc3a5f462a6107d5af
Merge: 3b76eefe0f97 f1f4ee01c0d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 22:59:39 2011 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (99 commits)
      drivers/virt: add missing linux/interrupt.h to fsl_hypervisor.c
      powerpc/85xx: fix mpic configuration in CAMP mode
      powerpc: Copy back TIF flags on return from softirq stack
      powerpc/64: Make server perfmon only built on ppc64 server devices
      powerpc/pseries: Fix hvc_vio.c build due to recent changes
      powerpc: Exporting boot_cpuid_phys
      powerpc: Add CFAR to oops output
      hvc_console: Add kdb support
      powerpc/pseries: Fix hvterm_raw_get_chars to accept < 16 chars, fixing xmon
      powerpc/irq: Quieten irq mapping printks
      powerpc: Enable lockup and hung task detectors in pseries and ppc64 defeconfigs
      powerpc: Add mpt2sas driver to pseries and ppc64 defconfig
      powerpc: Disable IRQs off tracer in ppc64 defconfig
      powerpc: Sync pseries and ppc64 defconfigs
      powerpc/pseries/hvconsole: Fix dropped console output
      hvc_console: Improve tty/console put_chars handling
      powerpc/kdump: Fix timeout in crash_kexec_wait_realmode
      powerpc/mm: Fix output of total_ram.
      powerpc/cpufreq: Add cpufreq driver for Momentum Maple boards
      powerpc: Correct annotations of pmu registration functions
      ...
    
    Fix up trivial Kconfig/Makefile conflicts in arch/powerpc, drivers, and
    drivers/cpufreq

commit aa04b4cc5be64b4fb9ef4e0fdf2418e2f4737fb2
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:25:44 2011 +0000

    KVM: PPC: Allocate RMAs (Real Mode Areas) at boot for use by guests
    
    This adds infrastructure which will be needed to allow book3s_hv KVM to
    run on older POWER processors, including PPC970, which don't support
    the Virtual Real Mode Area (VRMA) facility, but only the Real Mode
    Offset (RMO) facility.  These processors require a physically
    contiguous, aligned area of memory for each guest.  When the guest does
    an access in real mode (MMU off), the address is compared against a
    limit value, and if it is lower, the address is ORed with an offset
    value (from the Real Mode Offset Register (RMOR)) and the result becomes
    the real address for the access.  The size of the RMA has to be one of
    a set of supported values, which usually includes 64MB, 128MB, 256MB
    and some larger powers of 2.
    
    Since we are unlikely to be able to allocate 64MB or more of physically
    contiguous memory after the kernel has been running for a while, we
    allocate a pool of RMAs at boot time using the bootmem allocator.  The
    size and number of the RMAs can be set using the kvm_rma_size=xx and
    kvm_rma_count=xx kernel command line options.
    
    KVM exports a new capability, KVM_CAP_PPC_RMA, to signal the availability
    of the pool of preallocated RMAs.  The capability value is 1 if the
    processor can use an RMA but doesn't require one (because it supports
    the VRMA facility), or 2 if the processor requires an RMA for each guest.
    
    This adds a new ioctl, KVM_ALLOCATE_RMA, which allocates an RMA from the
    pool and returns a file descriptor which can be used to map the RMA.  It
    also returns the size of the RMA in the argument structure.
    
    Having an RMA means we will get multiple KMV_SET_USER_MEMORY_REGION
    ioctl calls from userspace.  To cope with this, we now preallocate the
    kvm->arch.ram_pginfo array when the VM is created with a size sufficient
    for up to 64GB of guest memory.  Subsequently we will get rid of this
    array and use memory associated with each memslot instead.
    
    This moves most of the code that translates the user addresses into
    host pfns (page frame numbers) out of kvmppc_prepare_vrma up one level
    to kvmppc_core_prepare_memory_region.  Also, instead of having to look
    up the VMA for each page in order to check the page size, we now check
    that the pages we get are compound pages of 16MB.  However, if we are
    adding memory that is mapped to an RMA, we don't bother with calling
    get_user_pages_fast and instead just offset from the base pfn for the
    RMA.
    
    Typically the RMA gets added after vcpus are created, which makes it
    inconvenient to have the LPCR (logical partition control register) value
    in the vcpu->arch struct, since the LPCR controls whether the processor
    uses RMA or VRMA for the guest.  This moves the LPCR value into the
    kvm->arch struct and arranges for the MER (mediated external request)
    bit, which is the only bit that varies between vcpus, to be set in
    assembly code when going into the guest if there is a pending external
    interrupt request.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index a88bf2713d41..532054f24ecb 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -63,6 +63,7 @@
 #include <asm/kexec.h>
 #include <asm/mmu_context.h>
 #include <asm/code-patching.h>
+#include <asm/kvm_ppc.h>
 
 #include "setup.h"
 
@@ -580,6 +581,8 @@ void __init setup_arch(char **cmdline_p)
 	/* Initialize the MMU context management stuff */
 	mmu_context_init();
 
+	kvm_rma_init();
+
 	ppc64_boot_msg(0x15, "Setup Done");
 }
 

commit 7ac87abb8166b99584149fcfb2efef5773a078e9
Author: Matt Evans <matt@ozlabs.org>
Date:   Wed May 25 18:09:12 2011 +0000

    powerpc: Fix early boot accounting of CPUs
    
    smp_release_cpus() waits for all cpus (including the bootcpu) due to an
    off-by-one count on boot_cpu_count (which is all CPUs).  This patch replaces
    that with spinning_secondaries (which is all secondary CPUs).
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index a88bf2713d41..05769190e7f1 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -73,7 +73,7 @@
 #endif
 
 int boot_cpuid = 0;
-int __initdata boot_cpu_count;
+int __initdata spinning_secondaries;
 u64 ppc64_pft_size;
 
 /* Pick defaults since we might want to patch instructions
@@ -253,11 +253,11 @@ void smp_release_cpus(void)
 	for (i = 0; i < 100000; i++) {
 		mb();
 		HMT_low();
-		if (boot_cpu_count == 0)
+		if (spinning_secondaries == 0)
 			break;
 		udelay(1);
 	}
-	DBG("boot_cpu_count = %d\n", boot_cpu_count);
+	DBG("spinning_secondaries = %d\n", spinning_secondaries);
 
 	DBG(" <- smp_release_cpus()\n");
 }

commit d36b4c4f3cc6caae6d4a12d9f995513e4c3acdd5
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Apr 6 00:18:48 2011 -0500

    powerpc/fsl-booke64: Add support for Debug Level exception handler
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index c2ec0a12e14f..a88bf2713d41 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -62,6 +62,7 @@
 #include <asm/udbg.h>
 #include <asm/kexec.h>
 #include <asm/mmu_context.h>
+#include <asm/code-patching.h>
 
 #include "setup.h"
 
@@ -477,6 +478,9 @@ static void __init irqstack_early_init(void)
 #ifdef CONFIG_PPC_BOOK3E
 static void __init exc_lvl_early_init(void)
 {
+	extern unsigned int interrupt_base_book3e;
+	extern unsigned int exc_debug_debug_book3e;
+
 	unsigned int i;
 
 	for_each_possible_cpu(i) {
@@ -487,6 +491,10 @@ static void __init exc_lvl_early_init(void)
 		mcheckirq_ctx[i] = (struct thread_info *)
 			__va(memblock_alloc(THREAD_SIZE, THREAD_SIZE));
 	}
+
+	if (cpu_has_feature(CPU_FTR_DEBUG_LVL_EXC))
+		patch_branch(&interrupt_base_book3e + (0x040 / 4) + 1,
+			     (unsigned long)&exc_debug_debug_book3e, 0);
 }
 #else
 #define exc_lvl_early_init()

commit 40bd587a88fcd425f489f3d9f0be7daa84014141
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue May 3 14:07:01 2011 +0000

    powerpc: Rename slb0_limit() to safe_stack_limit() and add Book3E support
    
    slb0_limit() wasn't a very descriptive name. This changes it along with
    a comment explaining what it's used for, and provides a 64-bit BookE
    implementation.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 959c63cf62e4..c2ec0a12e14f 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -434,17 +434,30 @@ void __init setup_system(void)
 	DBG(" <- setup_system()\n");
 }
 
-static u64 slb0_limit(void)
+/* This returns the limit below which memory accesses to the linear
+ * mapping are guarnateed not to cause a TLB or SLB miss. This is
+ * used to allocate interrupt or emergency stacks for which our
+ * exception entry path doesn't deal with being interrupted.
+ */
+static u64 safe_stack_limit(void)
 {
-	if (mmu_has_feature(MMU_FTR_1T_SEGMENT)) {
+#ifdef CONFIG_PPC_BOOK3E
+	/* Freescale BookE bolts the entire linear mapping */
+	if (mmu_has_feature(MMU_FTR_TYPE_FSL_E))
+		return linear_map_top;
+	/* Other BookE, we assume the first GB is bolted */
+	return 1ul << 30;
+#else
+	/* BookS, the first segment is bolted */
+	if (mmu_has_feature(MMU_FTR_1T_SEGMENT))
 		return 1UL << SID_SHIFT_1T;
-	}
 	return 1UL << SID_SHIFT;
+#endif
 }
 
 static void __init irqstack_early_init(void)
 {
-	u64 limit = slb0_limit();
+	u64 limit = safe_stack_limit();
 	unsigned int i;
 
 	/*
@@ -497,7 +510,7 @@ static void __init emergency_stack_init(void)
 	 * bringup, we need to get at them in real mode. This means they
 	 * must also be within the RMO region.
 	 */
-	limit = min(slb0_limit(), ppc64_rma_size);
+	limit = min(safe_stack_limit(), ppc64_rma_size);
 
 	for_each_possible_cpu(i) {
 		unsigned long sp;

commit 44ae3ab3358e962039c36ad4ae461ae9fb29596c
Author: Matt Evans <matt@ozlabs.org>
Date:   Wed Apr 6 19:48:50 2011 +0000

    powerpc: Free up some CPU feature bits by moving out MMU-related features
    
    Some of the 64bit PPC CPU features are MMU-related, so this patch moves
    them to MMU_FTR_ bits.  All cpu_has_feature()-style tests are moved to
    mmu_has_feature(), and seven feature bits are freed as a result.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 91a5cc5f0d02..959c63cf62e4 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -436,7 +436,7 @@ void __init setup_system(void)
 
 static u64 slb0_limit(void)
 {
-	if (cpu_has_feature(CPU_FTR_1T_SEGMENT)) {
+	if (mmu_has_feature(MMU_FTR_1T_SEGMENT)) {
 		return 1UL << SID_SHIFT_1T;
 	}
 	return 1UL << SID_SHIFT;

commit 9d07bc841c9779b4d7902e417f4e509996ce805d
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Mar 16 14:54:35 2011 +1100

    powerpc: Properly handshake CPUs going out of boot spin loop
    
    We need to wait a bit for them to have done their CPU setup
    or we might end up with translation and EE on with different
    LPCR values between threads
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 5a0401fcaebd..91a5cc5f0d02 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -72,6 +72,7 @@
 #endif
 
 int boot_cpuid = 0;
+int __initdata boot_cpu_count;
 u64 ppc64_pft_size;
 
 /* Pick defaults since we might want to patch instructions
@@ -233,6 +234,7 @@ void early_setup_secondary(void)
 void smp_release_cpus(void)
 {
 	unsigned long *ptr;
+	int i;
 
 	DBG(" -> smp_release_cpus()\n");
 
@@ -245,7 +247,16 @@ void smp_release_cpus(void)
 	ptr  = (unsigned long *)((unsigned long)&__secondary_hold_spinloop
 			- PHYSICAL_START);
 	*ptr = __pa(generic_secondary_smp_init);
-	mb();
+
+	/* And wait a bit for them to catch up */
+	for (i = 0; i < 100000; i++) {
+		mb();
+		HMT_low();
+		if (boot_cpu_count == 0)
+			break;
+		udelay(1);
+	}
+	DBG("boot_cpu_count = %d\n", boot_cpu_count);
 
 	DBG(" <- smp_release_cpus()\n");
 }

commit 8f4da26e9bf89f54b68d5cc3f3596f45e5f43911
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Dec 8 00:55:03 2010 +0000

    powerpc: Fix incorrect comment about interrupt stack allocation
    
    We now allow interrupt stacks anywhere in the first segment which can be
    256M or 1TB. Fix the comment.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ce6f61c6f871..5a0401fcaebd 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -437,8 +437,8 @@ static void __init irqstack_early_init(void)
 	unsigned int i;
 
 	/*
-	 * interrupt stacks must be under 256MB, we cannot afford to take
-	 * SLB misses on them.
+	 * Interrupt stacks must be in the first segment since we
+	 * cannot afford to take SLB misses on them.
 	 */
 	for_each_possible_cpu(i) {
 		softirq_ctx[i] = (struct thread_info *)

commit 0f6b77ca12bea571e0a97b0588f62aa5f6012d61
Author: Alessio Igor Bogani <abogani@texware.it>
Date:   Tue Nov 16 07:55:16 2010 +0000

    powerpc: Update a BKL related comment
    
    The commit 5e3d20a remove bkl from startup code so setup_arch() it isn't called
    with bkl held anymore. Update the comment on top of that function.
    Fix also a typo.
    
    This work was supported by a hardware donation from the CE Linux Forum.
    
    Signed-off-by: Alessio Igor Bogani <abogani@texware.it>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 2a178b0ebcdf..ce6f61c6f871 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -497,9 +497,8 @@ static void __init emergency_stack_init(void)
 }
 
 /*
- * Called into from start_kernel, after lock_kernel has been called.
- * Initializes bootmem, which is unsed to manage page allocation until
- * mem_init is called.
+ * Called into from start_kernel this initializes bootmem, which is used
+ * to manage page allocation until mem_init is called.
  */
 void __init setup_arch(char **cmdline_p)
 {

commit daab7fc734a53fdeaf844b7c03053118ad1769da
Merge: 774ea0bcb27f 2bfc96a127bc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Aug 31 09:45:21 2010 +0200

    Merge commit 'v2.6.36-rc3' into x86/memblock
    
    Conflicts:
            arch/x86/kernel/trampoline.c
            mm/memblock.c
    
    Merge reason: Resolve the conflicts, update to latest upstream.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 954e6da54b2f3a5e2634312db800bc1395c509ee
Author: Nathan Fontenot <nfont@austin.ibm.com>
Date:   Thu Aug 5 07:42:11 2010 +0000

    powerpc: Correct smt_enabled=X boot option for > 2 threads per core
    
    The 'smt_enabled=X' boot option does not handle values of X > 2.
    For Power 7 processors with smt modes of 0,1,2,3, and 4 this does
    not work.  This patch allows the smt_enabled option to be set to
    any value limited to a max equal to the number of threads per
    core.
    
    Signed-off-by: Nathan Fontenot <nfont@austin.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 1bee4b68fa45..e72690ec9b87 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -95,7 +95,7 @@ int ucache_bsize;
 
 #ifdef CONFIG_SMP
 
-static int smt_enabled_cmdline;
+static char *smt_enabled_cmdline;
 
 /* Look for ibm,smt-enabled OF option */
 static void check_smt_enabled(void)
@@ -103,37 +103,46 @@ static void check_smt_enabled(void)
 	struct device_node *dn;
 	const char *smt_option;
 
-	/* Allow the command line to overrule the OF option */
-	if (smt_enabled_cmdline)
-		return;
-
-	dn = of_find_node_by_path("/options");
-
-	if (dn) {
-		smt_option = of_get_property(dn, "ibm,smt-enabled", NULL);
+	/* Default to enabling all threads */
+	smt_enabled_at_boot = threads_per_core;
 
-                if (smt_option) {
-			if (!strcmp(smt_option, "on"))
-				smt_enabled_at_boot = 1;
-			else if (!strcmp(smt_option, "off"))
-				smt_enabled_at_boot = 0;
-                }
-        }
+	/* Allow the command line to overrule the OF option */
+	if (smt_enabled_cmdline) {
+		if (!strcmp(smt_enabled_cmdline, "on"))
+			smt_enabled_at_boot = threads_per_core;
+		else if (!strcmp(smt_enabled_cmdline, "off"))
+			smt_enabled_at_boot = 0;
+		else {
+			long smt;
+			int rc;
+
+			rc = strict_strtol(smt_enabled_cmdline, 10, &smt);
+			if (!rc)
+				smt_enabled_at_boot =
+					min(threads_per_core, (int)smt);
+		}
+	} else {
+		dn = of_find_node_by_path("/options");
+		if (dn) {
+			smt_option = of_get_property(dn, "ibm,smt-enabled",
+						     NULL);
+
+			if (smt_option) {
+				if (!strcmp(smt_option, "on"))
+					smt_enabled_at_boot = threads_per_core;
+				else if (!strcmp(smt_option, "off"))
+					smt_enabled_at_boot = 0;
+			}
+
+			of_node_put(dn);
+		}
+	}
 }
 
 /* Look for smt-enabled= cmdline option */
 static int __init early_smt_enabled(char *p)
 {
-	smt_enabled_cmdline = 1;
-
-	if (!p)
-		return 0;
-
-	if (!strcmp(p, "on") || !strcmp(p, "1"))
-		smt_enabled_at_boot = 1;
-	else if (!strcmp(p, "off") || !strcmp(p, "0"))
-		smt_enabled_at_boot = 0;
-
+	smt_enabled_cmdline = p;
 	return 0;
 }
 early_param("smt-enabled", early_smt_enabled);
@@ -380,8 +389,8 @@ void __init setup_system(void)
 	 */
 	xmon_setup();
 
-	check_smt_enabled();
 	smp_setup_cpu_maps();
+	check_smt_enabled();
 
 #ifdef CONFIG_SMP
 	/* Release secondary cpus out of their spinloops at 0x60 now that

commit cd3db0c4ca3d237e7ad20f7107216e575705d2b0
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 6 15:39:02 2010 -0700

    memblock: Remove rmo_size, burry it in arch/powerpc where it belongs
    
    The RMA (RMO is a misnomer) is a concept specific to ppc64 (in fact
    server ppc64 though I hijack it on embedded ppc64 for similar purposes)
    and represents the area of memory that can be accessed in real mode
    (aka with MMU off), or on embedded, from the exception vectors (which
    is bolted in the TLB) which pretty much boils down to the same thing.
    
    We take that out of the generic MEMBLOCK data structure and move it into
    arch/powerpc where it belongs, renaming it to "RMA" while at it.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index d135f93cb0f6..4360944b60f0 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -487,7 +487,7 @@ static void __init emergency_stack_init(void)
 	 * bringup, we need to get at them in real mode. This means they
 	 * must also be within the RMO region.
 	 */
-	limit = min(slb0_limit(), memblock.rmo_size);
+	limit = min(slb0_limit(), ppc64_rma_size);
 
 	for_each_possible_cpu(i) {
 		unsigned long sp;

commit 412a4ac5e9cf7fdeb6af562c25547a9b9da7674f
Merge: e8e5c2155b00 0c2daaafcdec
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Aug 4 10:26:03 2010 +1000

    Merge commit 'gcl/next' into next

commit fc53b4202e61c7e9008c241933ae282aab8a6082
Author: Matt Evans <matt@ozlabs.org>
Date:   Wed Jul 7 21:55:37 2010 +0000

    powerpc/kexec: Switch to a static PACA on the way out
    
    With dynamic PACAs, the kexecing CPU's PACA won't lie within the kernel
    static data and there is a chance that something may stomp it when preparing
    to kexec.  This patch switches this final CPU to a static PACA just before
    we pull the switch.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index c352f322dbdd..96e662c1d46b 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -142,16 +142,6 @@ early_param("smt-enabled", early_smt_enabled);
 #define check_smt_enabled()
 #endif /* CONFIG_SMP */
 
-/* Put the paca pointer into r13 and SPRG_PACA */
-static void __init setup_paca(struct paca_struct *new_paca)
-{
-	local_paca = new_paca;
-	mtspr(SPRN_SPRG_PACA, local_paca);
-#ifdef CONFIG_PPC_BOOK3E
-	mtspr(SPRN_SPRG_TLB_EXFRAME, local_paca->extlb);
-#endif
-}
-
 /*
  * Early initialization entry point. This is called by head.S
  * with MMU translation disabled. We rely on the "feature" of

commit 95f72d1ed41a66f1c1c29c24d479de81a0bea36f
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Jul 12 14:36:09 2010 +1000

    lmb: rename to memblock
    
    via following scripts
    
          FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
          sed -i \
            -e 's/lmb/memblock/g' \
            -e 's/LMB/MEMBLOCK/g' \
            $FILES
    
          for N in $(find . -name lmb.[ch]); do
            M=$(echo $N | sed 's/lmb/memblock/g')
            mv $N $M
          done
    
    and remove some wrong change like lmbench and dlmb etc.
    
    also move memblock.c from lib/ to mm/
    
    Suggested-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 643dcac40fcb..d135f93cb0f6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -34,7 +34,7 @@
 #include <linux/bootmem.h>
 #include <linux/pci.h>
 #include <linux/lockdep.h>
-#include <linux/lmb.h>
+#include <linux/memblock.h>
 #include <asm/io.h>
 #include <asm/kdump.h>
 #include <asm/prom.h>
@@ -158,7 +158,7 @@ static void __init setup_paca(struct paca_struct *new_paca)
  * the CPU that ignores the top 2 bits of the address in real
  * mode so we can access kernel globals normally provided we
  * only toy with things in the RMO region. From here, we do
- * some early parsing of the device-tree to setup out LMB
+ * some early parsing of the device-tree to setup out MEMBLOCK
  * data structures, and allocate & initialize the hash table
  * and segment tables so we can start running with translation
  * enabled.
@@ -404,7 +404,7 @@ void __init setup_system(void)
 
 	printk("-----------------------------------------------------\n");
 	printk("ppc64_pft_size                = 0x%llx\n", ppc64_pft_size);
-	printk("physicalMemorySize            = 0x%llx\n", lmb_phys_mem_size());
+	printk("physicalMemorySize            = 0x%llx\n", memblock_phys_mem_size());
 	if (ppc64_caches.dline_size != 0x80)
 		printk("ppc64_caches.dcache_line_size = 0x%x\n",
 		       ppc64_caches.dline_size);
@@ -443,10 +443,10 @@ static void __init irqstack_early_init(void)
 	 */
 	for_each_possible_cpu(i) {
 		softirq_ctx[i] = (struct thread_info *)
-			__va(lmb_alloc_base(THREAD_SIZE,
+			__va(memblock_alloc_base(THREAD_SIZE,
 					    THREAD_SIZE, limit));
 		hardirq_ctx[i] = (struct thread_info *)
-			__va(lmb_alloc_base(THREAD_SIZE,
+			__va(memblock_alloc_base(THREAD_SIZE,
 					    THREAD_SIZE, limit));
 	}
 }
@@ -458,11 +458,11 @@ static void __init exc_lvl_early_init(void)
 
 	for_each_possible_cpu(i) {
 		critirq_ctx[i] = (struct thread_info *)
-			__va(lmb_alloc(THREAD_SIZE, THREAD_SIZE));
+			__va(memblock_alloc(THREAD_SIZE, THREAD_SIZE));
 		dbgirq_ctx[i] = (struct thread_info *)
-			__va(lmb_alloc(THREAD_SIZE, THREAD_SIZE));
+			__va(memblock_alloc(THREAD_SIZE, THREAD_SIZE));
 		mcheckirq_ctx[i] = (struct thread_info *)
-			__va(lmb_alloc(THREAD_SIZE, THREAD_SIZE));
+			__va(memblock_alloc(THREAD_SIZE, THREAD_SIZE));
 	}
 }
 #else
@@ -487,11 +487,11 @@ static void __init emergency_stack_init(void)
 	 * bringup, we need to get at them in real mode. This means they
 	 * must also be within the RMO region.
 	 */
-	limit = min(slb0_limit(), lmb.rmo_size);
+	limit = min(slb0_limit(), memblock.rmo_size);
 
 	for_each_possible_cpu(i) {
 		unsigned long sp;
-		sp  = lmb_alloc_base(THREAD_SIZE, THREAD_SIZE, limit);
+		sp  = memblock_alloc_base(THREAD_SIZE, THREAD_SIZE, limit);
 		sp += THREAD_SIZE;
 		paca[i].emergency_sp = __va(sp);
 	}

commit ae01f84b93b274e2f215bdf6d0b46435679b5f9a
Author: Anton Blanchard <anton@samba.org>
Date:   Mon May 31 18:45:11 2010 +0000

    powerpc: Optimise per cpu accesses on 64bit
    
    Now we dynamically allocate the paca array, it takes an extra load
    whenever we want to access another cpu's paca. One place we do that a lot
    is per cpu variables. A simple example:
    
    DEFINE_PER_CPU(unsigned long, vara);
    unsigned long test4(int cpu)
    {
            return per_cpu(vara, cpu);
    }
    
    This takes 4 loads, 5 if you include the actual load of the per cpu variable:
    
        ld r11,-32760(r30)  # load address of paca pointer
        ld r9,-32768(r30)   # load link address of percpu variable
        sldi r3,r29,9       # get offset into paca (each entry is 512 bytes)
        ld r0,0(r11)        # load paca pointer
        add r3,r0,r3        # paca + offset
        ld r11,64(r3)       # load paca[cpu].data_offset
    
        ldx r3,r9,r11       # load per cpu variable
    
    If we remove the ppc64 specific per_cpu_offset(), we get the generic one
    which indexes into a statically allocated array. This removes one load and
    one add:
    
        ld r11,-32760(r30)  # load address of __per_cpu_offset
        ld r9,-32768(r30)   # load link address of percpu variable
        sldi r3,r29,3       # get offset into __per_cpu_offset (each entry 8 bytes)
        ldx r11,r11,r3      # load __per_cpu_offset[cpu]
    
        ldx r3,r9,r11       # load per cpu variable
    
    Having all the offsets in one array also helps when iterating over a per cpu
    variable across a number of cpus, such as in the scheduler. Before we would
    need to load one paca cacheline when calculating each per cpu offset. Now we
    have 16 (128 / sizeof(long)) per cpu offsets in each cacheline.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 643dcac40fcb..c352f322dbdd 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -600,6 +600,9 @@ static int pcpu_cpu_distance(unsigned int from, unsigned int to)
 		return REMOTE_DISTANCE;
 }
 
+unsigned long __per_cpu_offset[NR_CPUS] __read_mostly;
+EXPORT_SYMBOL(__per_cpu_offset);
+
 void __init setup_per_cpu_areas(void)
 {
 	const size_t dyn_size = PERCPU_MODULE_RESERVE + PERCPU_DYNAMIC_RESERVE;
@@ -624,8 +627,10 @@ void __init setup_per_cpu_areas(void)
 		panic("cannot initialize percpu area (err=%d)", rc);
 
 	delta = (unsigned long)pcpu_base_addr - (unsigned long)__per_cpu_start;
-	for_each_possible_cpu(cpu)
-		paca[cpu].data_offset = delta + pcpu_unit_offsets[cpu];
+	for_each_possible_cpu(cpu) {
+                __per_cpu_offset[cpu] = delta + pcpu_unit_offsets[cpu];
+		paca[cpu].data_offset = __per_cpu_offset[cpu];
+	}
 }
 #endif
 

commit f1ba9a5b2ab7d3f5a910d93371c4f22b636b7683
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 2 22:24:26 2010 +0000

    powerpc: Unconditionally enabled irq stacks
    
    Irq stacks provide an essential protection from stack overflows through
    external interrupts, at the cost of two additionals stacks per CPU.
    
    Enable them unconditionally to simplify the kernel build and prevent
    people from accidentally disabling them.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f3fb5a79de52..643dcac40fcb 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -432,7 +432,6 @@ static u64 slb0_limit(void)
 	return 1UL << SID_SHIFT;
 }
 
-#ifdef CONFIG_IRQSTACKS
 static void __init irqstack_early_init(void)
 {
 	u64 limit = slb0_limit();
@@ -451,9 +450,6 @@ static void __init irqstack_early_init(void)
 					    THREAD_SIZE, limit));
 	}
 }
-#else
-#define irqstack_early_init()
-#endif
 
 #ifdef CONFIG_PPC_BOOK3E
 static void __init exc_lvl_early_init(void)

commit 095c7965f4dc870ed2b65143b1e2610de653416c
Author: Anton Blanchard <anton@samba.org>
Date:   Mon May 10 18:59:18 2010 +0000

    powerpc: Use more accurate limit for first segment memory allocations
    
    Author: Milton Miller <miltonm@bga.com>
    
    On large machines we are running out of room below 256MB. In some cases we
    only need to ensure the allocation is in the first segment, which may be
    256MB or 1TB.
    
    Add slb0_limit and use it to specify the upper limit for the irqstack and
    emergency stacks.
    
    On a large ppc64 box, this fixes a panic at boot when the crashkernel=
    option is specified (previously we would run out of memory below 256MB).
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index cea66987a6ea..f3fb5a79de52 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -424,9 +424,18 @@ void __init setup_system(void)
 	DBG(" <- setup_system()\n");
 }
 
+static u64 slb0_limit(void)
+{
+	if (cpu_has_feature(CPU_FTR_1T_SEGMENT)) {
+		return 1UL << SID_SHIFT_1T;
+	}
+	return 1UL << SID_SHIFT;
+}
+
 #ifdef CONFIG_IRQSTACKS
 static void __init irqstack_early_init(void)
 {
+	u64 limit = slb0_limit();
 	unsigned int i;
 
 	/*
@@ -436,10 +445,10 @@ static void __init irqstack_early_init(void)
 	for_each_possible_cpu(i) {
 		softirq_ctx[i] = (struct thread_info *)
 			__va(lmb_alloc_base(THREAD_SIZE,
-					    THREAD_SIZE, 0x10000000));
+					    THREAD_SIZE, limit));
 		hardirq_ctx[i] = (struct thread_info *)
 			__va(lmb_alloc_base(THREAD_SIZE,
-					    THREAD_SIZE, 0x10000000));
+					    THREAD_SIZE, limit));
 	}
 }
 #else
@@ -470,7 +479,7 @@ static void __init exc_lvl_early_init(void)
  */
 static void __init emergency_stack_init(void)
 {
-	unsigned long limit;
+	u64 limit;
 	unsigned int i;
 
 	/*
@@ -482,7 +491,7 @@ static void __init emergency_stack_init(void)
 	 * bringup, we need to get at them in real mode. This means they
 	 * must also be within the RMO region.
 	 */
-	limit = min(0x10000000ULL, lmb.rmo_size);
+	limit = min(slb0_limit(), lmb.rmo_size);
 
 	for_each_possible_cpu(i) {
 		unsigned long sp;

commit abb17f9c3a92c5acf30e749efdf0419b7f50a5b8
Author: Milton Miller <miltonm@bga.com>
Date:   Wed May 19 02:56:29 2010 +0000

    powerpc: Use common cpu_die (fixes SMP+SUSPEND build)
    
    Configuring a powerpc 32 bit kernel for both SMP and SUSPEND turns on
    CPU_HOTPLUG to enable disable_nonboot_cpus to be called by the common
    suspend code.  Previously the definition of cpu_die for ppc32 was in
    the powermac platform code, causing it to be undefined if that platform
    as not selected.
    
    arch/powerpc/kernel/built-in.o: In function 'cpu_idle':
    arch/powerpc/kernel/idle.c:98: undefined reference to 'cpu_die'
    
    Move the code from setup_64 to smp.c and rename the power mac
    versions to their specific names.
    
    Note that this does not setup the cpu_die pointers in either
    smp_ops (request a given cpu die) or ppc_md (make this cpu die),
    for other platforms but there are generic versions in smp.c.
    
    Reported-by: Matt Sealey <matt@genesi-usa.com>
    Reported-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Anton Vorontsov <avorontsov@mvista.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 914389158a9b..cea66987a6ea 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -573,12 +573,6 @@ void ppc64_boot_msg(unsigned int src, const char *msg)
 	printk("[boot]%04x %s\n", src, msg);
 }
 
-void cpu_die(void)
-{
-	if (ppc_md.cpu_die)
-		ppc_md.cpu_die();
-}
-
 #ifdef CONFIG_SMP
 #define PCPU_DYN_SIZE		()
 

commit a93272969c6b1d59883fcbb04845420bd72c9a20
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Mar 16 13:16:25 2010 +0000

    powerpc: Fix swiotlb to respect the boot option
    
    powerpc initializes swiotlb before parsing the kernel boot options so
    swiotlb options (e.g. specifying the swiotlb buffer size) are ignored.
    
    Any time before freeing bootmem works for swiotlb so this patch moves
    powerpc's swiotlb initialization after parsing the kernel boot
    options, mem_init (as x86 does).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Tested-by: Becky Bruce <beckyb@kernel.crashing.org>
    Tested-by: Albert Herranz <albert_herranz@yahoo.es>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 63547394048c..914389158a9b 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -61,7 +61,6 @@
 #include <asm/xmon.h>
 #include <asm/udbg.h>
 #include <asm/kexec.h>
-#include <asm/swiotlb.h>
 #include <asm/mmu_context.h>
 
 #include "setup.h"
@@ -541,11 +540,6 @@ void __init setup_arch(char **cmdline_p)
 	if (ppc_md.setup_arch)
 		ppc_md.setup_arch();
 
-#ifdef CONFIG_SWIOTLB
-	if (ppc_swiotlb_enable)
-		swiotlb_init(1);
-#endif
-
 	paging_init();
 
 	/* Initialize the MMU context management stuff */

commit 1426d5a3bd07589534286375998c0c8c6fdc5260
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Jan 28 13:23:22 2010 +0000

    powerpc: Dynamically allocate pacas
    
    On 64-bit kernels we currently have a 512 byte struct paca_struct for
    each cpu (usually just called "the paca"). Currently they are statically
    allocated, which means a kernel built for a large number of cpus will
    waste a lot of space if it's booted on a machine with few cpus.
    
    We can avoid that by only allocating the number of pacas we need at
    boot. However this is complicated by the fact that we need to access
    the paca before we know how many cpus there are in the system.
    
    The solution is to dynamically allocate enough space for NR_CPUS pacas,
    but then later in boot when we know how many cpus we have, we free any
    unused pacas.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6568406b2a30..63547394048c 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -144,9 +144,9 @@ early_param("smt-enabled", early_smt_enabled);
 #endif /* CONFIG_SMP */
 
 /* Put the paca pointer into r13 and SPRG_PACA */
-void __init setup_paca(int cpu)
+static void __init setup_paca(struct paca_struct *new_paca)
 {
-	local_paca = &paca[cpu];
+	local_paca = new_paca;
 	mtspr(SPRN_SPRG_PACA, local_paca);
 #ifdef CONFIG_PPC_BOOK3E
 	mtspr(SPRN_SPRG_TLB_EXFRAME, local_paca->extlb);
@@ -176,14 +176,12 @@ void __init early_setup(unsigned long dt_ptr)
 {
 	/* -------- printk is _NOT_ safe to use here ! ------- */
 
-	/* Fill in any unititialised pacas */
-	initialise_pacas();
-
 	/* Identify CPU type */
 	identify_cpu(0, mfspr(SPRN_PVR));
 
 	/* Assume we're on cpu 0 for now. Don't write to the paca yet! */
-	setup_paca(0);
+	initialise_paca(&boot_paca, 0);
+	setup_paca(&boot_paca);
 
 	/* Initialize lockdep early or else spinlocks will blow */
 	lockdep_init();
@@ -203,7 +201,7 @@ void __init early_setup(unsigned long dt_ptr)
 	early_init_devtree(__va(dt_ptr));
 
 	/* Now we know the logical id of our boot cpu, setup the paca. */
-	setup_paca(boot_cpuid);
+	setup_paca(&paca[boot_cpuid]);
 
 	/* Fix up paca fields required for the boot cpu */
 	get_paca()->cpu_start = 1;

commit bcd6acd51f3d4d1ada201e9bc5c40a31d6d80c71
Merge: 11c34c7deaee 3ff6a468b45b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Dec 9 17:14:38 2009 +1100

    Merge commit 'origin/master' into next
    
    Conflicts:
            include/linux/kvm.h

commit ad32e8cb86e7894aac51c8963eaa9f36bb8a4e14
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Nov 10 19:46:19 2009 +0900

    swiotlb: Defer swiotlb init printing, export swiotlb_print_info()
    
    This enables us to avoid printing swiotlb memory info when we
    initialize swiotlb. After swiotlb initialization, we could find
    that we don't need swiotlb.
    
    This patch removes the code to print swiotlb memory info in
    swiotlb_init() and exports the function to do that.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: chrisw@sous-sol.org
    Cc: dwmw2@infradead.org
    Cc: joerg.roedel@amd.com
    Cc: muli@il.ibm.com
    Cc: tony.luck@intel.com
    Cc: benh@kernel.crashing.org
    LKML-Reference: <1257849980-22640-9-git-send-email-fujita.tomonori@lab.ntt.co.jp>
    [ -v2: merge up conflict ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 04f638d82fb3..df2c9e932b37 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -550,7 +550,7 @@ void __init setup_arch(char **cmdline_p)
 
 #ifdef CONFIG_SWIOTLB
 	if (ppc_swiotlb_enable)
-		swiotlb_init();
+		swiotlb_init(1);
 #endif
 
 	paging_init();

commit cd015707176820b86d07b5dffdecfefdd539a497
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Oct 13 19:45:03 2009 +0000

    powerpc: Enable sparse irq_descs on powerpc
    
    Defining CONFIG_SPARSE_IRQ enables generic code that gets rid of the
    static irq_desc array, and replaces it with an array of pointers to
    irq_descs.
    
    It also allows node local allocation of irq_descs, however we
    currently don't have the information available to do that, so we just
    allocate them on all on node 0.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 04f638d82fb3..fd785f7a279b 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -356,11 +356,6 @@ void __init setup_system(void)
 	 */
 	initialize_cache_info();
 
-	/*
-	 * Initialize irq remapping subsystem
-	 */
-	irq_early_init();
-
 #ifdef CONFIG_PPC_RTAS
 	/*
 	 * Initialize RTAS if available

commit ce7a35c73a308c62f9f0ca9f0821ebe0dc553008
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Fri Oct 16 07:05:17 2009 +0000

    powerpc: Fix compile errors found by new ppc64e_defconfig
    
    Fix the following 3 issues:
    
    arch/powerpc/kernel/process.c: In function 'arch_randomize_brk':
    arch/powerpc/kernel/process.c:1183: error: 'mmu_highuser_ssize' undeclared (first use in this function)
    arch/powerpc/kernel/process.c:1183: error: (Each undeclared identifier is reported only once
    arch/powerpc/kernel/process.c:1183: error: for each function it appears in.)
    arch/powerpc/kernel/process.c:1183: error: 'MMU_SEGSIZE_1T' undeclared (first use in this function)
    
    In file included from arch/powerpc/kernel/setup_64.c:60:
    arch/powerpc/include/asm/mmu-hash64.h:132: error: redefinition of 'struct mmu_psize_def'
    arch/powerpc/include/asm/mmu-hash64.h:159: error: expected identifier or '(' before numeric constant
    arch/powerpc/include/asm/mmu-hash64.h:396: error: conflicting types for 'mm_context_t'
    arch/powerpc/include/asm/mmu-book3e.h:184: error: previous declaration of 'mm_context_t' was here
    
    cc1: warnings being treated as errors
    arch/powerpc/kernel/pci_64.c: In function 'pcibios_unmap_io_space':
    arch/powerpc/kernel/pci_64.c:100: error: unused variable 'res'
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 797ea95aae2e..04f638d82fb3 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -57,7 +57,6 @@
 #include <asm/cache.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
-#include <asm/mmu-hash64.h>
 #include <asm/firmware.h>
 #include <asm/xmon.h>
 #include <asm/udbg.h>

commit 723e9db7a46e328527cc3da2b478b831184fe828
Merge: ada3fa150572 d331d8305cba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 15 09:51:09 2009 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (134 commits)
      powerpc/nvram: Enable use Generic NVRAM driver for different size chips
      powerpc/iseries: Fix oops reading from /proc/iSeries/mf/*/cmdline
      powerpc/ps3: Workaround for flash memory I/O error
      powerpc/booke: Don't set DABR on 64-bit BookE, use DAC1 instead
      powerpc/perf_counters: Reduce stack usage of power_check_constraints
      powerpc: Fix bug where perf_counters breaks oprofile
      powerpc/85xx: Fix SMP compile error and allow NULL for smp_ops
      powerpc/irq: Improve nanodoc
      powerpc: Fix some late PowerMac G5 with PCIe ATI graphics
      powerpc/fsl-booke: Use HW PTE format if CONFIG_PTE_64BIT
      powerpc/book3e: Add missing page sizes
      powerpc/pseries: Fix to handle slb resize across migration
      powerpc/powermac: Thermal control turns system off too eagerly
      powerpc/pci: Merge ppc32 and ppc64 versions of phb_scan()
      powerpc/405ex: support cuImage via included dtb
      powerpc/405ex: provide necessary fixup function to support cuImage
      powerpc/40x: Add support for the ESTeem 195E (PPC405EP) SBC
      powerpc/44x: Add Eiger AMCC (AppliedMicro) PPC460SX evaluation board support.
      powerpc/44x: Update Arches defconfig
      powerpc/44x: Update Arches dts
      ...
    
    Fix up conflicts in drivers/char/agp/uninorth-agp.c

commit 2d27cfd3286966c04d4192a9db5a6c7ea60eebf1
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jul 23 23:15:59 2009 +0000

    powerpc: Remaining 64-bit Book3E support
    
    This contains all the bits that didn't fit in previous patches :-) This
    includes the actual exception handlers assembly, the changes to the
    kernel entry, other misc bits and wiring it all up in Kconfig.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 65aced7b833a..87df51720641 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -454,6 +454,24 @@ static void __init irqstack_early_init(void)
 #define irqstack_early_init()
 #endif
 
+#ifdef CONFIG_PPC_BOOK3E
+static void __init exc_lvl_early_init(void)
+{
+	unsigned int i;
+
+	for_each_possible_cpu(i) {
+		critirq_ctx[i] = (struct thread_info *)
+			__va(lmb_alloc(THREAD_SIZE, THREAD_SIZE));
+		dbgirq_ctx[i] = (struct thread_info *)
+			__va(lmb_alloc(THREAD_SIZE, THREAD_SIZE));
+		mcheckirq_ctx[i] = (struct thread_info *)
+			__va(lmb_alloc(THREAD_SIZE, THREAD_SIZE));
+	}
+}
+#else
+#define exc_lvl_early_init()
+#endif
+
 /*
  * Stack space used when we detect a bad kernel stack pointer, and
  * early in SMP boots before relocation is enabled.
@@ -513,6 +531,7 @@ void __init setup_arch(char **cmdline_p)
 	init_mm.brk = klimit;
 	
 	irqstack_early_init();
+	exc_lvl_early_init();
 	emergency_stack_init();
 
 #ifdef CONFIG_PPC_STD_MMU_64

commit 25d21ad6e799cccd097b9df2a2fefe19a7e1dfcf
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jul 23 23:15:47 2009 +0000

    powerpc: Add TLB management code for 64-bit Book3E
    
    This adds the TLB miss handler assembly, the low level TLB flush routines
    along with the necessary hook for dealing with our virtual page tables
    or indirect TLB entries that need to be flushes when PTE pages are freed.
    
    There is currently no support for hugetlbfs
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index a6b6c4c9ae41..65aced7b833a 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -62,6 +62,7 @@
 #include <asm/udbg.h>
 #include <asm/kexec.h>
 #include <asm/swiotlb.h>
+#include <asm/mmu_context.h>
 
 #include "setup.h"
 
@@ -147,6 +148,9 @@ void __init setup_paca(int cpu)
 {
 	local_paca = &paca[cpu];
 	mtspr(SPRN_SPRG_PACA, local_paca);
+#ifdef CONFIG_PPC_BOOK3E
+	mtspr(SPRN_SPRG_TLB_EXFRAME, local_paca->extlb);
+#endif
 }
 
 /*

commit cf54dc7cd4f9aab55cd3e1794b0b74c3c88cd1a0
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jul 23 23:15:28 2009 +0000

    powerpc: Move definitions of secondary CPU spinloop to header file
    
    Those definitions are currently declared extern in the .c file where
    they are used, move them to a header file instead.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 325dc5b2e626..a6b6c4c9ae41 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -230,9 +230,6 @@ void early_setup_secondary(void)
 #endif /* CONFIG_SMP */
 
 #if defined(CONFIG_SMP) || defined(CONFIG_KEXEC)
-extern unsigned long __secondary_hold_spinloop;
-extern void generic_secondary_smp_init(void);
-
 void smp_release_cpus(void)
 {
 	unsigned long *ptr;

commit 6f0ef0f505af1ce6e9756087a9d4cc3778bae8c6
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jul 23 23:15:26 2009 +0000

    powerpc/mm: Call mmu_context_init() from ppc64
    
    Our 64-bit hash context handling has no init function, but 64-bit Book3E
    will use the common mmu_context_nohash.c code which does, so define an
    empty inline mmu_context_init() for 64-bit server and call it from
    our 64-bit setup_arch()
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 91b89b8d63d8..325dc5b2e626 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -534,6 +534,10 @@ void __init setup_arch(char **cmdline_p)
 #endif
 
 	paging_init();
+
+	/* Initialize the MMU context management stuff */
+	mmu_context_init();
+
 	ppc64_boot_msg(0x15, "Setup Done");
 }
 

commit ee43eb788b3a06425fffb912677e2e1c8b00dd3b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 14 20:52:54 2009 +0000

    powerpc: Use names rather than numbers for SPRGs (v2)
    
    The kernel uses SPRG registers for various purposes, typically in
    low level assembly code as scratch registers or to hold per-cpu
    global infos such as the PACA or the current thread_info pointer.
    
    We want to be able to easily shuffle the usage of those registers
    as some implementations have specific constraints realted to some
    of them, for example, some have userspace readable aliases, etc..
    and the current choice isn't always the best.
    
    This patch should not change any code generation, and replaces the
    usage of SPRN_SPRGn everywhere in the kernel with a named replacement
    and adds documentation next to the definition of the names as to
    what those are used for on each processor family.
    
    The only parts that still use the original numbers are bits of KVM
    or suspend/resume code that just blindly needs to save/restore all
    the SPRGs.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 1f6816003ebe..91b89b8d63d8 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -142,11 +142,11 @@ early_param("smt-enabled", early_smt_enabled);
 #define check_smt_enabled()
 #endif /* CONFIG_SMP */
 
-/* Put the paca pointer into r13 and SPRG3 */
+/* Put the paca pointer into r13 and SPRG_PACA */
 void __init setup_paca(int cpu)
 {
 	local_paca = &paca[cpu];
-	mtspr(SPRN_SPRG3, local_paca);
+	mtspr(SPRN_SPRG_PACA, local_paca);
 }
 
 /*

commit c2a7e818019f20a5cf7fb26a6eb59e212e6c0cd8
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 14 15:00:53 2009 +0900

    powerpc64: convert to dynamic percpu allocator
    
    Now that percpu allows arbitrary embedding of the first chunk,
    powerpc64 can easily be converted to dynamic percpu allocator.
    Convert it.  powerpc supports several large page sizes.  Cap atom_size
    at 1M.  There isn't much to gain by going above that anyway.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 1f6816003ebe..aa6e4500635f 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -57,6 +57,7 @@
 #include <asm/cache.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
+#include <asm/mmu-hash64.h>
 #include <asm/firmware.h>
 #include <asm/xmon.h>
 #include <asm/udbg.h>
@@ -569,25 +570,53 @@ void cpu_die(void)
 }
 
 #ifdef CONFIG_SMP
-void __init setup_per_cpu_areas(void)
+#define PCPU_DYN_SIZE		()
+
+static void * __init pcpu_fc_alloc(unsigned int cpu, size_t size, size_t align)
 {
-	int i;
-	unsigned long size;
-	char *ptr;
-
-	/* Copy section for each CPU (we discard the original) */
-	size = ALIGN(__per_cpu_end - __per_cpu_start, PAGE_SIZE);
-#ifdef CONFIG_MODULES
-	if (size < PERCPU_ENOUGH_ROOM)
-		size = PERCPU_ENOUGH_ROOM;
-#endif
+	return __alloc_bootmem_node(NODE_DATA(cpu_to_node(cpu)), size, align,
+				    __pa(MAX_DMA_ADDRESS));
+}
 
-	for_each_possible_cpu(i) {
-		ptr = alloc_bootmem_pages_node(NODE_DATA(cpu_to_node(i)), size);
+static void __init pcpu_fc_free(void *ptr, size_t size)
+{
+	free_bootmem(__pa(ptr), size);
+}
 
-		paca[i].data_offset = ptr - __per_cpu_start;
-		memcpy(ptr, __per_cpu_start, __per_cpu_end - __per_cpu_start);
-	}
+static int pcpu_cpu_distance(unsigned int from, unsigned int to)
+{
+	if (cpu_to_node(from) == cpu_to_node(to))
+		return LOCAL_DISTANCE;
+	else
+		return REMOTE_DISTANCE;
+}
+
+void __init setup_per_cpu_areas(void)
+{
+	const size_t dyn_size = PERCPU_MODULE_RESERVE + PERCPU_DYNAMIC_RESERVE;
+	size_t atom_size;
+	unsigned long delta;
+	unsigned int cpu;
+	int rc;
+
+	/*
+	 * Linear mapping is one of 4K, 1M and 16M.  For 4K, no need
+	 * to group units.  For larger mappings, use 1M atom which
+	 * should be large enough to contain a number of units.
+	 */
+	if (mmu_linear_psize == MMU_PAGE_4K)
+		atom_size = PAGE_SIZE;
+	else
+		atom_size = 1 << 20;
+
+	rc = pcpu_embed_first_chunk(0, dyn_size, atom_size, pcpu_cpu_distance,
+				    pcpu_fc_alloc, pcpu_fc_free);
+	if (rc < 0)
+		panic("cannot initialize percpu area (err=%d)", rc);
+
+	delta = (unsigned long)pcpu_base_addr - (unsigned long)__per_cpu_start;
+	for_each_possible_cpu(cpu)
+		paca[cpu].data_offset = delta + pcpu_unit_offsets[cpu];
 }
 #endif
 

commit e468455e5845f83950d1271a6cd0425b9c7290ab
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed Jun 10 19:05:00 2009 +0000

    powerpc: Fix warning in setup_64.c when CONFIG_RELOCATABLE=y
    
    When CONFIG_RELOCATABLE is enabled, PHYSICAL_START is actually a
    variable of type phys_addr_t. That means to print it we need to
    cast to unsigned long long and use llx.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f46548e66045..1f6816003ebe 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -424,8 +424,8 @@ void __init setup_system(void)
 	printk("htab_hash_mask                = 0x%lx\n", htab_hash_mask);
 #endif /* CONFIG_PPC_STD_MMU_64 */
 	if (PHYSICAL_START > 0)
-		printk("physical_start                = 0x%lx\n",
-		       PHYSICAL_START);
+		printk("physical_start                = 0x%llx\n",
+		       (unsigned long long)PHYSICAL_START);
 	printk("-----------------------------------------------------\n");
 
 	DBG(" <- setup_system()\n");

commit ec3cf2ece22a8ede7478bf38e2a818986322662b
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Thu May 14 12:42:28 2009 +0000

    powerpc: Add support for swiotlb on 32-bit
    
    This patch includes the basic infrastructure to use swiotlb
    bounce buffering on 32-bit powerpc.  It is not yet enabled on
    any platforms.  Probably the most interesting bit is the
    addition of addr_needs_map to dma_ops - we need this as
    a dma_op because the decision of whether or not an addr
    can be mapped by a device is device-specific.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 42221055f0c4..f46548e66045 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -61,6 +61,7 @@
 #include <asm/xmon.h>
 #include <asm/udbg.h>
 #include <asm/kexec.h>
+#include <asm/swiotlb.h>
 
 #include "setup.h"
 
@@ -527,6 +528,11 @@ void __init setup_arch(char **cmdline_p)
 	if (ppc_md.setup_arch)
 		ppc_md.setup_arch();
 
+#ifdef CONFIG_SWIOTLB
+	if (ppc_swiotlb_enable)
+		swiotlb_init();
+#endif
+
 	paging_init();
 	ppc64_boot_msg(0x15, "Setup Done");
 }

commit 944916858a430a0627e483657d4cfa2cd2dfb4f7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jun 2 21:17:45 2009 +0000

    powerpc: Shield code specific to 64-bit server processors
    
    This is a random collection of added ifdef's around portions of
    code that only mak sense on server processors. Using either
    CONFIG_PPC_STD_MMU_64 or CONFIG_PPC_BOOK3S as seems appropriate.
    
    This is meant to make the future merging of Book3E 64-bit support
    easier.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index c410c606955d..42221055f0c4 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -417,9 +417,11 @@ void __init setup_system(void)
 	if (ppc64_caches.iline_size != 0x80)
 		printk("ppc64_caches.icache_line_size = 0x%x\n",
 		       ppc64_caches.iline_size);
+#ifdef CONFIG_PPC_STD_MMU_64
 	if (htab_address)
 		printk("htab_address                  = 0x%p\n", htab_address);
 	printk("htab_hash_mask                = 0x%lx\n", htab_hash_mask);
+#endif /* CONFIG_PPC_STD_MMU_64 */
 	if (PHYSICAL_START > 0)
 		printk("physical_start                = 0x%lx\n",
 		       PHYSICAL_START);
@@ -511,8 +513,9 @@ void __init setup_arch(char **cmdline_p)
 	irqstack_early_init();
 	emergency_stack_init();
 
+#ifdef CONFIG_PPC_STD_MMU_64
 	stabs_alloc();
-
+#endif
 	/* set up the bootmem stuff with available memory */
 	do_init_bootmem();
 	sparse_init();

commit 757c74d298dc8438760b8dea275c4c6e0ac8a77f
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Mar 19 19:34:16 2009 +0000

    powerpc/mm: Introduce early_init_mmu() on 64-bit
    
    This moves some MMU related init code out of setup_64.c into hash_utils_64.c
    and calls it early_init_mmu() and early_init_mmu_secondary(). This will
    make it easier to plug in a new MMU type.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 73e16e298e28..c410c606955d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -202,8 +202,6 @@ void __init early_setup(unsigned long dt_ptr)
 
 	/* Fix up paca fields required for the boot cpu */
 	get_paca()->cpu_start = 1;
-	get_paca()->stab_real = __pa((u64)&initial_stab);
-	get_paca()->stab_addr = (u64)&initial_stab;
 
 	/* Probe the machine type */
 	probe_machine();
@@ -212,20 +210,8 @@ void __init early_setup(unsigned long dt_ptr)
 
 	DBG("Found, Initializing memory management...\n");
 
-	/*
-	 * Initialize the MMU Hash table and create the linear mapping
-	 * of memory. Has to be done before stab/slb initialization as
-	 * this is currently where the page size encoding is obtained
-	 */
-	htab_initialize();
-
-	/*
-	 * Initialize stab / SLB management except on iSeries
-	 */
-	if (cpu_has_feature(CPU_FTR_SLB))
-		slb_initialize();
-	else if (!firmware_has_feature(FW_FEATURE_ISERIES))
-		stab_initialize(get_paca()->stab_real);
+	/* Initialize the hash table or TLB handling */
+	early_init_mmu();
 
 	DBG(" <- early_setup()\n");
 }
@@ -233,22 +219,11 @@ void __init early_setup(unsigned long dt_ptr)
 #ifdef CONFIG_SMP
 void early_setup_secondary(void)
 {
-	struct paca_struct *lpaca = get_paca();
-
 	/* Mark interrupts enabled in PACA */
-	lpaca->soft_enabled = 0;
+	get_paca()->soft_enabled = 0;
 
-	/* Initialize hash table for that CPU */
-	htab_initialize_secondary();
-
-	/* Initialize STAB/SLB. We use a virtual address as it works
-	 * in real mode on pSeries and we want a virutal address on
-	 * iSeries anyway
-	 */
-	if (cpu_has_feature(CPU_FTR_SLB))
-		slb_initialize();
-	else
-		stab_initialize(lpaca->stab_addr);
+	/* Initialize the hash table or TLB handling */
+	early_init_mmu_secondary();
 }
 
 #endif /* CONFIG_SMP */

commit 33642d31d19c967b9739253912cdd48885509805
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed Jan 14 20:43:15 2009 +0000

    powerpc: Remove unused ppc64_terminate_msg()
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 2d34196bba8c..73e16e298e28 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -578,13 +578,6 @@ void ppc64_boot_msg(unsigned int src, const char *msg)
 	printk("[boot]%04x %s\n", src, msg);
 }
 
-/* Print a termination message (print only -- does not stop the kernel) */
-void ppc64_terminate_msg(unsigned int src, const char *msg)
-{
-	ppc64_do_msg(PPC64_LINUX_FUNCTION|PPC64_TERM_MESSAGE|src, msg);
-	printk("[terminate]%04x %s\n", src, msg);
-}
-
 void cpu_die(void)
 {
 	if (ppc_md.cpu_die)

commit fe333321e2a71f706b794d55b6a3dcb5ab240f65
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 6 14:26:03 2009 +0000

    powerpc: Change u64/s64 to a long long integer type
    
    Convert arch/powerpc/ over to long long based u64:
    
     -#ifdef __powerpc64__
     -# include <asm-generic/int-l64.h>
     -#else
     -# include <asm-generic/int-ll64.h>
     -#endif
     +#include <asm-generic/int-ll64.h>
    
    This will avoid reoccuring spurious warnings in core kernel code that
    comes when people test on their own hardware. (i.e. x86 in ~98% of the
    cases) This is what x86 uses and it generally helps keep 64-bit code
    32-bit clean too.
    
    [Adjusted to not impact user mode (from paulus) - sfr]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index d8bd2161e738..2d34196bba8c 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -434,8 +434,8 @@ void __init setup_system(void)
 	printk("Starting Linux PPC64 %s\n", init_utsname()->version);
 
 	printk("-----------------------------------------------------\n");
-	printk("ppc64_pft_size                = 0x%lx\n", ppc64_pft_size);
-	printk("physicalMemorySize            = 0x%lx\n", lmb_phys_mem_size());
+	printk("ppc64_pft_size                = 0x%llx\n", ppc64_pft_size);
+	printk("physicalMemorySize            = 0x%llx\n", lmb_phys_mem_size());
 	if (ppc64_caches.dline_size != 0x80)
 		printk("ppc64_caches.dcache_line_size = 0x%x\n",
 		       ppc64_caches.dline_size);
@@ -493,7 +493,7 @@ static void __init emergency_stack_init(void)
 	 * bringup, we need to get at them in real mode. This means they
 	 * must also be within the RMO region.
 	 */
-	limit = min(0x10000000UL, lmb.rmo_size);
+	limit = min(0x10000000ULL, lmb.rmo_size);
 
 	for_each_possible_cpu(i) {
 		unsigned long sp;

commit 7c03d653cd257793dc40520c94e229b5fd0578e7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Dec 18 19:13:32 2008 +0000

    powerpc/mm: Introduce MMU features
    
    We're soon running out of CPU features and I need to add some new
    ones for various MMU related bits, so this patch separates the MMU
    features from the CPU features.  I moved over the 32-bit MMU related
    ones, added base features for MMU type families, but didn't move
    over any 64-bit only feature yet.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ce48f5c5c542..d8bd2161e738 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -361,6 +361,8 @@ void __init setup_system(void)
 	 */
 	do_feature_fixups(cur_cpu_spec->cpu_features,
 			  &__start___ftr_fixup, &__stop___ftr_fixup);
+	do_feature_fixups(cur_cpu_spec->mmu_features,
+			  &__start___mmu_ftr_fixup, &__stop___mmu_ftr_fixup);
 	do_feature_fixups(powerpc_firmware_features,
 			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
 	do_lwsync_fixups(cur_cpu_spec->cpu_features,

commit 6b82b3e4b54b2fce2ca11976c535012b836b2016
Author: Anton Vorontsov <avorontsov@ru.mvista.com>
Date:   Tue Dec 9 09:47:29 2008 +0000

    powerpc: Remove `have_of' global variable
    
    The `have_of' variable is a relic from the arch/ppc time, it isn't
    useful nowadays.
    
    Signed-off-by: Anton Vorontsov <avorontsov@ru.mvista.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 93c875ae985a..ce48f5c5c542 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -70,7 +70,6 @@
 #define DBG(fmt...)
 #endif
 
-int have_of = 1;
 int boot_cpuid = 0;
 u64 ppc64_pft_size;
 

commit 786b32f892dc341b607445bdef29d8e41a840925
Author: Julia Lawall <julia@diku.dk>
Date:   Sun Nov 23 00:48:56 2008 +0000

    powerpc: Eliminate NULL test and memset after alloc_bootmem
    
    As noted by Akinobu Mita in commit b1fceac2 ("x86: remove unnecessary
    memset and NULL check after alloc_bootmem()"), alloc_bootmem and
    related functions never return NULL and always return a zeroed region
    of memory.  Thus a NULL test or memset after calls to these functions
    is unnecessary.
    
    This was fixed using the following semantic patch.
    (http://www.emn.fr/x-info/coccinelle/)
    
    // <smpl>
    @@
    expression E;
    statement S;
    @@
    
    E = \(alloc_bootmem\|alloc_bootmem_low\|alloc_bootmem_pages\|alloc_bootmem_low_pages\|alloc_bootmem_node\|alloc_bootmem_low_pages_node\|alloc_bootmem_pages_node\)(...)
    ... when != E
    (
    - BUG_ON (E == NULL);
    |
    - if (E == NULL) S
    )
    
    @@
    expression E,E1;
    @@
    
    E = \(alloc_bootmem\|alloc_bootmem_low\|alloc_bootmem_pages\|alloc_bootmem_low_pages\|alloc_bootmem_node\|alloc_bootmem_low_pages_node\|alloc_bootmem_pages_node\)(...)
    ... when != E
    - memset(E,0,E1);
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 169d74cef157..93c875ae985a 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -606,8 +606,6 @@ void __init setup_per_cpu_areas(void)
 
 	for_each_possible_cpu(i) {
 		ptr = alloc_bootmem_pages_node(NODE_DATA(cpu_to_node(i)), size);
-		if (!ptr)
-			panic("Cannot allocate cpu data for CPU %d\n", i);
 
 		paca[i].data_offset = ptr - __per_cpu_start;
 		memcpy(ptr, __per_cpu_start, __per_cpu_end - __per_cpu_start);

commit b160544cccb403310cf38ddb3ebc156ea454848a
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Oct 22 19:39:49 2008 +0000

    powerpc: Fix compiler warning for the relocatable kernel
    
    Fixes this warning:
     arch/powerpc/kernel/setup_64.c:447:5: warning: "kernstart_addr" is not defined
    
    which arises because PHYSICAL_START is no longer a constant when
    CONFIG_RELOCATABLE=y.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 843c0af210d0..169d74cef157 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -444,9 +444,9 @@ void __init setup_system(void)
 	if (htab_address)
 		printk("htab_address                  = 0x%p\n", htab_address);
 	printk("htab_hash_mask                = 0x%lx\n", htab_hash_mask);
-#if PHYSICAL_START > 0
-	printk("physical_start                = 0x%lx\n", PHYSICAL_START);
-#endif
+	if (PHYSICAL_START > 0)
+		printk("physical_start                = 0x%lx\n",
+		       PHYSICAL_START);
 	printk("-----------------------------------------------------\n");
 
 	DBG(" <- setup_system()\n");

commit 1f6a93e4c35e75d547b51f56ba8139ab1a91628c
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Aug 30 11:40:24 2008 +1000

    powerpc: Make it possible to move the interrupt handlers away from the kernel
    
    This changes the way that the exception prologs transfer control to
    the handlers in 64-bit kernels with the aim of making it possible to
    have the prologs separate from the main body of the kernel.  Now,
    instead of computing the address of the handler by taking the top
    32 bits of the paca address (to get the 0xc0000000........ part) and
    ORing in something in the bottom 16 bits, we get the base address of
    the kernel by doing a load from the paca and add an offset.
    
    This also replaces an mfmsr and an ori to compute the MSR value for
    the handler with a load from the paca.  That makes it unnecessary to
    have a separate version of EXCEPTION_PROLOG_PSERIES that forces 64-bit
    mode.
    
    We can no longer use a direct branches in the exception prolog code,
    which means that the SLB miss handlers can't branch directly to
    .slb_miss_realmode any more.  Instead we have to compute the address
    and do an indirect branch.  This is conditional on CONFIG_RELOCATABLE;
    for non-relocatable kernels we use a direct branch as before.  (A later
    change will allow CONFIG_RELOCATABLE to be set on 64-bit powerpc.)
    
    Since the secondary CPUs on pSeries start execution in the first 0x100
    bytes of real memory and then have to get to wherever the kernel is,
    we can't use a direct branch to get there.  Instead this changes
    __secondary_hold_spinloop from a flag to a function pointer.  When it
    is set to a non-NULL value, the secondary CPUs jump to the function
    pointed to by that value.
    
    Finally this eliminates one code difference between 32-bit and 64-bit
    by making __secondary_hold be the text address of the secondary CPU
    spinloop rather than a function descriptor for it.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 8b25f51f03bf..843c0af210d0 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -255,9 +255,11 @@ void early_setup_secondary(void)
 #endif /* CONFIG_SMP */
 
 #if defined(CONFIG_SMP) || defined(CONFIG_KEXEC)
+extern unsigned long __secondary_hold_spinloop;
+extern void generic_secondary_smp_init(void);
+
 void smp_release_cpus(void)
 {
-	extern unsigned long __secondary_hold_spinloop;
 	unsigned long *ptr;
 
 	DBG(" -> smp_release_cpus()\n");
@@ -266,12 +268,11 @@ void smp_release_cpus(void)
 	 * all now so they can start to spin on their individual paca
 	 * spinloops. For non SMP kernels, the secondary cpus never get out
 	 * of the common spinloop.
-	 * This is useless but harmless on iSeries, secondaries are already
-	 * waiting on their paca spinloops. */
+	 */
 
 	ptr  = (unsigned long *)((unsigned long)&__secondary_hold_spinloop
 			- PHYSICAL_START);
-	*ptr = 1;
+	*ptr = __pa(generic_secondary_smp_init);
 	mb();
 
 	DBG(" <- smp_release_cpus()\n");

commit e2075f79a99b45a6cc10de021c93f07212098a84
Author: Nathan Lynch <ntl@pobox.com>
Date:   Sun Jul 27 15:24:52 2008 +1000

    powerpc: Update cpu_sibling_maps dynamically
    
    Rather doing one initialization pass over all the per-cpu
    cpu_sibling_maps at boot, update the maps at cpu online/offline time.
    
    This is a behavior change -- the thread_siblings attribute now
    reflects only online siblings, whereas it would display offline
    siblings before.  The new behavior matches that of x86, and is
    arguably more useful.
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 04d8de9f0fc6..8b25f51f03bf 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -611,9 +611,6 @@ void __init setup_per_cpu_areas(void)
 		paca[i].data_offset = ptr - __per_cpu_start;
 		memcpy(ptr, __per_cpu_start, __per_cpu_end - __per_cpu_start);
 	}
-
-	/* Now that per_cpu is setup, initialize cpu_sibling_map */
-	smp_setup_cpu_sibling_map();
 }
 #endif
 

commit 2d1b2027626d5151fff8ef7c06ca8e7876a1a510
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Jul 2 01:16:40 2008 +1000

    powerpc: Fixup lwsync at runtime
    
    To allow for a single kernel image on e500 v1/v2/mc we need to fixup lwsync
    at runtime.  On e500v1/v2 lwsync causes an illop so we need to patch up
    the code.  We default to 'sync' since that is always safe and if the cpu
    is capable we will replace 'sync' with 'lwsync'.
    
    We introduce CPU_FTR_LWSYNC as a way to determine at runtime if this is
    needed.  This flag could be moved elsewhere since we dont really use it
    for the normal CPU_FTR purpose.
    
    Finally we only store the relative offset in the fixup section to keep it
    as small as possible rather than using a full fixup_entry.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 098fd96a394a..04d8de9f0fc6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -363,6 +363,8 @@ void __init setup_system(void)
 			  &__start___ftr_fixup, &__stop___ftr_fixup);
 	do_feature_fixups(powerpc_firmware_features,
 			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
+	do_lwsync_fixups(cur_cpu_spec->cpu_features,
+			 &__start___lwsync_fixup, &__stop___lwsync_fixup);
 
 	/*
 	 * Unflatten the device-tree passed by prom_init or kexec

commit f2fd25131b5a9c802faa1de1e9b5f1b06d16eec3
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed May 7 10:25:34 2008 +1000

    [POWERPC] Initialize lockdep earlier
    
    This moves lockdep_init() to before udbg_early_init() as the later
    can call things that acquire spinlocks etc...  This also makes printk
    safer to use earlier.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f2cd82eaf49d..098fd96a394a 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -181,14 +181,14 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Assume we're on cpu 0 for now. Don't write to the paca yet! */
 	setup_paca(0);
 
-	/* Enable early debugging if any specified (see udbg.h) */
-	udbg_early_init();
-
 	/* Initialize lockdep early or else spinlocks will blow */
 	lockdep_init();
 
 	/* -------- printk is now safe to use ------- */
 
+	/* Enable early debugging if any specified (see udbg.h) */
+	udbg_early_init();
+
  	DBG(" -> early_setup(), dt_ptr: 0x%lx\n", dt_ptr);
 
 	/*

commit 24d9649574fbe591fdfa6b00893d4096f513e539
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed May 7 10:00:56 2008 +1000

    [POWERPC] Document when printk is useable
    
    When debugging early boot problems, it's common to sprinkle printk's
    all over the place.  However, on 64-bit powerpc, this can lead to
    memory corruption if done too early due to the PACA pointer and
    lockdep core not being initialized.
    
    This adds some comments to early_setup() that document when it is
    safe to do so in order to save time for whoever has to debug that
    stuff next.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 25e3fd8606ab..f2cd82eaf49d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -170,6 +170,8 @@ void __init setup_paca(int cpu)
 
 void __init early_setup(unsigned long dt_ptr)
 {
+	/* -------- printk is _NOT_ safe to use here ! ------- */
+
 	/* Fill in any unititialised pacas */
 	initialise_pacas();
 
@@ -185,6 +187,8 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Initialize lockdep early or else spinlocks will blow */
 	lockdep_init();
 
+	/* -------- printk is now safe to use ------- */
+
  	DBG(" -> early_setup(), dt_ptr: 0x%lx\n", dt_ptr);
 
 	/*

commit 3243d87441bf7f97c5c9f7dd46b35f5783ec6740
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed Apr 30 13:21:45 2008 +1000

    [POWERPC] Make emergency stack safe for current_thread_info() use
    
    The current_thread_info() macro, used by preempt_count(), assumes the
    base address and size of the stack are THREAD_SIZE aligned.
    
    The emergency stack currently isn't either of these things, which
    could potentially cause problems anytime we're running on the
    emergency stack.  That includes when we detect a bad kernel stack
    pointer, and also during early_setup_secondary().
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index dff6308d1b5e..25e3fd8606ab 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -487,9 +487,12 @@ static void __init emergency_stack_init(void)
 	 */
 	limit = min(0x10000000UL, lmb.rmo_size);
 
-	for_each_possible_cpu(i)
-		paca[i].emergency_sp =
-		__va(lmb_alloc_base(HW_PAGE_SIZE, 128, limit)) + HW_PAGE_SIZE;
+	for_each_possible_cpu(i) {
+		unsigned long sp;
+		sp  = lmb_alloc_base(THREAD_SIZE, THREAD_SIZE, limit);
+		sp += THREAD_SIZE;
+		paca[i].emergency_sp = __va(sp);
+	}
 }
 
 /*

commit 90035fe378c7459ba19c43c63d5f878284224ce4
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Thu Apr 24 13:43:49 2008 +1000

    [POWERPC] Raise the upper limit of NR_CPUS and move the pacas into the BSS
    
    This adds the required functionality to fill in all pacas at runtime.
    
    With NR_CPUS=1024
    text    data     bss     dec     hex filename
     137 1704032       0 1704169  1a00e9 arch/powerpc/kernel/paca.o :Before
     121 1179744  524288 1704153  1a00d9 arch/powerpc/kernel/paca.o :After
    
    Also remove unneeded #includes from arch/powerpc/kernel/paca.c
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 153a48dc8f40..dff6308d1b5e 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -170,6 +170,9 @@ void __init setup_paca(int cpu)
 
 void __init early_setup(unsigned long dt_ptr)
 {
+	/* Fill in any unititialised pacas */
+	initialise_pacas();
+
 	/* Identify CPU type */
 	identify_cpu(0, mfspr(SPRN_PVR));
 

commit 37dd2badcfcec35f5e21a0926968d77a404f03c3
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Tue Apr 22 04:22:34 2008 +1000

    [POWERPC] 85xx: Add support for relocatable kernel (and booting at non-zero)
    
    Added support to allow an 85xx kernel to be run from a non-zero physical
    address (useful for cooperative asymmetric multiprocessing situations and
    kdump).  The support can be configured at compile time by setting
    CONFIG_PAGE_OFFSET, CONFIG_KERNEL_START, and CONFIG_PHYSICAL_START as
    desired.
    
    Alternatively, the kernel build can set CONFIG_RELOCATABLE.  Setting this
    config option causes the kernel to determine at runtime the physical
    addresses of CONFIG_PAGE_OFFSET and CONFIG_KERNEL_START.  If
    CONFIG_RELOCATABLE is set, then CONFIG_PHYSICAL_START has no meaning.
    However, CONFIG_PHYSICAL_START will always be used to set the LOAD program
    header physical address field in the resulting ELF image.
    
    Currently we are limited to running at a physical address that is a
    multiple of 256M.  This is due to how we map TLBs to cover
    lowmem.  This should be fixed to allow 64M or maybe even 16M alignment
    in the future.  It is considered an error to try and run a kernel at a
    non-aligned physical address.
    
    All the magic for this support is accomplished by proper initialization
    of the kernel memory subsystem and use of ARCH_PFN_OFFSET.
    
    The use of ARCH_PFN_OFFSET only affects normal memory and not IO mappings.
    ioremap uses map_page and isn't affected by ARCH_PFN_OFFSET.
    
    /dev/mem continues to allow access to any physical address in the system
    regardless of how CONFIG_PHYSICAL_START is set.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 31ada9fdfc5c..153a48dc8f40 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -435,7 +435,7 @@ void __init setup_system(void)
 		printk("htab_address                  = 0x%p\n", htab_address);
 	printk("htab_hash_mask                = 0x%lx\n", htab_hash_mask);
 #if PHYSICAL_START > 0
-	printk("physical_start                = 0x%x\n", PHYSICAL_START);
+	printk("physical_start                = 0x%lx\n", PHYSICAL_START);
 #endif
 	printk("-----------------------------------------------------\n");
 

commit 945feb174b14e7098cc7ecf0cf4768d35bc52f9c
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Apr 17 14:35:01 2008 +1000

    [POWERPC] irqtrace support for 64-bit powerpc
    
    This adds the low level irq tracing hooks to the powerpc architecture
    needed to enable full lockdep functionality.
    
    This is partly based on Johannes Berg's initial version.  I removed
    the asm trampoline that isn't needed (thus improving performance) and
    modified all sorts of bits and pieces, reworking most of the assembly,
    etc...
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 0205d408d2ed..31ada9fdfc5c 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -33,6 +33,7 @@
 #include <linux/serial_8250.h>
 #include <linux/bootmem.h>
 #include <linux/pci.h>
+#include <linux/lockdep.h>
 #include <linux/lmb.h>
 #include <asm/io.h>
 #include <asm/kdump.h>
@@ -178,6 +179,9 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Enable early debugging if any specified (see udbg.h) */
 	udbg_early_init();
 
+	/* Initialize lockdep early or else spinlocks will blow */
+	lockdep_init();
+
  	DBG(" -> early_setup(), dt_ptr: 0x%lx\n", dt_ptr);
 
 	/*

commit 4846c5deb9776a7306d0f656ade7505278ac39ba
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Apr 16 05:52:26 2008 +1000

    [POWERPC] Clean up some linker and symbol usage
    
    * PAGE_OFFSET is not always the start of code, use _stext instead.
    * grab PAGE_SIZE and KERNELBASE from asm/page.h like ppc64 does.  Makes the
      code a bit more common and provide a single place to manipulate the
      defines for things like kdump.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 2c2d8315193c..0205d408d2ed 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -510,7 +510,7 @@ void __init setup_arch(char **cmdline_p)
 	if (ppc_md.panic)
 		setup_panic();
 
-	init_mm.start_code = PAGE_OFFSET;
+	init_mm.start_code = (unsigned long)_stext;
 	init_mm.end_code = (unsigned long) _etext;
 	init_mm.end_data = (unsigned long) _edata;
 	init_mm.brk = klimit;

commit d9b2b2a277219d4812311d995054ce4f95067725
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Feb 13 16:56:49 2008 -0800

    [LIB]: Make PowerPC LMB code generic so sparc64 can use it too.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 3b1529c103ef..2c2d8315193c 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -33,6 +33,7 @@
 #include <linux/serial_8250.h>
 #include <linux/bootmem.h>
 #include <linux/pci.h>
+#include <linux/lmb.h>
 #include <asm/io.h>
 #include <asm/kdump.h>
 #include <asm/prom.h>
@@ -55,7 +56,6 @@
 #include <asm/cache.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
-#include <asm/lmb.h>
 #include <asm/firmware.h>
 #include <asm/xmon.h>
 #include <asm/udbg.h>

commit 20474abda6bb11396434593daf2f52679cf62edf
Author: Benjamin Herrenschmidt <benh@au1.ibm.com>
Date:   Sun Oct 28 08:49:28 2007 +1100

    [POWERPC] Fix cache line vs. block size confusion
    
    We had an historical confusion in the kernel between cache line
    and cache block size. The former is an implementation detail of
    the L1 cache which can be useful for performance optimisations,
    the later is the actual size on which the cache control
    instructions operate, which can be different.
    
    For some reason, we had a weird hack reading the right property
    on powermac and the wrong one on any other 64 bits (32 bits is
    unaffected as it only uses the cputable for cache block size
    infos at this stage).
    
    This fixes the booting-without-of.txt documentation to mention
    the right properties, and fixes the 64 bits initialization code
    to look for the block size first, with a fallback to the line
    size if the property is missing.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ede77dbbd4df..3b1529c103ef 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -291,23 +291,16 @@ static void __init initialize_cache_info(void)
 		if ( num_cpus == 1 ) {
 			const u32 *sizep, *lsizep;
 			u32 size, lsize;
-			const char *dc, *ic;
-
-			/* Then read cache informations */
-			if (machine_is(powermac)) {
-				dc = "d-cache-block-size";
-				ic = "i-cache-block-size";
-			} else {
-				dc = "d-cache-line-size";
-				ic = "i-cache-line-size";
-			}
 
 			size = 0;
 			lsize = cur_cpu_spec->dcache_bsize;
 			sizep = of_get_property(np, "d-cache-size", NULL);
 			if (sizep != NULL)
 				size = *sizep;
-			lsizep = of_get_property(np, dc, NULL);
+			lsizep = of_get_property(np, "d-cache-block-size", NULL);
+			/* fallback if block size missing */
+			if (lsizep == NULL)
+				lsizep = of_get_property(np, "d-cache-line-size", NULL);
 			if (lsizep != NULL)
 				lsize = *lsizep;
 			if (sizep == 0 || lsizep == 0)
@@ -324,7 +317,9 @@ static void __init initialize_cache_info(void)
 			sizep = of_get_property(np, "i-cache-size", NULL);
 			if (sizep != NULL)
 				size = *sizep;
-			lsizep = of_get_property(np, ic, NULL);
+			lsizep = of_get_property(np, "i-cache-block-size", NULL);
+			if (lsizep == NULL)
+				lsizep = of_get_property(np, "i-cache-line-size", NULL);
 			if (lsizep != NULL)
 				lsize = *lsizep;
 			if (sizep == 0 || lsizep == 0)

commit 9697add0f88b439d4f5f25556785beeaf6b836b9
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Oct 15 05:33:17 2007 +1000

    [POWERPC] Quieten cache information at boot
    
    After 6 years the ppc64 kernel still thinks its important to tell me my
    cache line size is 0x80 bytes. I think most people who care know that by
    now. The rest probably cant even understand the hex output.
    
    Since we might have misconfigured firmware or cpus that have a linesize
    that isnt 128 bytes, I still print it out for those cases. If people
    would prefer to remove it completely, lets do it.
    
    Also for lpar remove the htab_address printout since its not used.
    
    Anton
    ppc64 boot log usability expert
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 0e014550b83f..ede77dbbd4df 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -426,11 +426,14 @@ void __init setup_system(void)
 	printk("-----------------------------------------------------\n");
 	printk("ppc64_pft_size                = 0x%lx\n", ppc64_pft_size);
 	printk("physicalMemorySize            = 0x%lx\n", lmb_phys_mem_size());
-	printk("ppc64_caches.dcache_line_size = 0x%x\n",
-	       ppc64_caches.dline_size);
-	printk("ppc64_caches.icache_line_size = 0x%x\n",
-	       ppc64_caches.iline_size);
-	printk("htab_address                  = 0x%p\n", htab_address);
+	if (ppc64_caches.dline_size != 0x80)
+		printk("ppc64_caches.dcache_line_size = 0x%x\n",
+		       ppc64_caches.dline_size);
+	if (ppc64_caches.iline_size != 0x80)
+		printk("ppc64_caches.icache_line_size = 0x%x\n",
+		       ppc64_caches.iline_size);
+	if (htab_address)
+		printk("htab_address                  = 0x%p\n", htab_address);
 	printk("htab_hash_mask                = 0x%lx\n", htab_hash_mask);
 #if PHYSICAL_START > 0
 	printk("physical_start                = 0x%x\n", PHYSICAL_START);

commit d5a7430ddcdb598261d70f7eb1bf450b5be52085
Author: Mike Travis <travis@sgi.com>
Date:   Tue Oct 16 01:24:05 2007 -0700

    Convert cpu_sibling_map to be a per cpu variable
    
    Convert cpu_sibling_map from a static array sized by NR_CPUS to a per_cpu
    variable.  This saves sizeof(cpumask_t) * NR unused cpus.  Access is mostly
    from startup and CPU HOTPLUG functions.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: "Siddha, Suresh B" <suresh.b.siddha@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 008ab6823b02..0e014550b83f 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -597,6 +597,9 @@ void __init setup_per_cpu_areas(void)
 		paca[i].data_offset = ptr - __per_cpu_start;
 		memcpy(ptr, __per_cpu_start, __per_cpu_end - __per_cpu_start);
 	}
+
+	/* Now that per_cpu is setup, initialize cpu_sibling_map */
+	smp_setup_cpu_sibling_map();
 }
 #endif
 

commit 38db7e740ade7f07f6315e3a3b1172d7e456b793
Author: Grant Likely <grant.likely@secretlab.ca>
Date:   Thu Oct 11 04:48:18 2007 +1000

    [POWERPC] Only call ppc_md.setup_arch() if it is provided
    
    This allows platforms which don't have anything to do at setup_arch time
    (like a bunch of the 4xx platforms) to eliminate an empty setup_arch hook.
    
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 3089eaed3256..008ab6823b02 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -530,7 +530,8 @@ void __init setup_arch(char **cmdline_p)
 	conswitchp = &dummy_con;
 #endif
 
-	ppc_md.setup_arch();
+	if (ppc_md.setup_arch)
+		ppc_md.setup_arch();
 
 	paging_init();
 	ppc64_boot_msg(0x15, "Setup Done");

commit 3c607ce2a3213f33b8b6b854b5f7db876021e466
Author: Linas Vepstas <linas@austin.ibm.com>
Date:   Fri Sep 7 03:47:29 2007 +1000

    [POWERPC] setup_64.c and prom.c comment cleanup
    
    Grammatical corrections to comments.
    
    Signed-off-by: Linas Vepstas <linas@austin.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6018178708a5..3089eaed3256 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -181,9 +181,9 @@ void __init early_setup(unsigned long dt_ptr)
  	DBG(" -> early_setup(), dt_ptr: 0x%lx\n", dt_ptr);
 
 	/*
-	 * Do early initializations using the flattened device
-	 * tree, like retreiving the physical memory map or
-	 * calculating/retreiving the hash table size
+	 * Do early initialization using the flattened device
+	 * tree, such as retrieving the physical memory map or
+	 * calculating/retrieving the hash table size.
 	 */
 	early_init_devtree(__va(dt_ptr));
 

commit 826ea8f22cf612d534f33c492c98f7895043bfd1
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Wed Jul 18 16:17:48 2007 +1000

    Revert "[POWERPC] Do firmware feature fixups after features are initialised"
    
    This reverts commit 5a26f6bbb767d7ad23311a1e81cfdd2bebefb855.
    
    The original patch causes boot failures when built with ppc64_defconfig.  The
    quickest fix is to revert it while alterates are investigated.
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index bc43bba05cf8..6018178708a5 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -350,11 +350,13 @@ void __init setup_system(void)
 {
 	DBG(" -> setup_system()\n");
 
-	/* Apply CPUs-specific fixups to kernel text (nop out sections
-	 * not relevant to this CPU)
+	/* Apply the CPUs-specific and firmware specific fixups to kernel
+	 * text (nop out sections not relevant to this CPU or this firmware)
 	 */
 	do_feature_fixups(cur_cpu_spec->cpu_features,
 			  &__start___ftr_fixup, &__stop___ftr_fixup);
+	do_feature_fixups(powerpc_firmware_features,
+			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
 
 	/*
 	 * Unflatten the device-tree passed by prom_init or kexec
@@ -392,12 +394,6 @@ void __init setup_system(void)
 	if (ppc_md.init_early)
 		ppc_md.init_early();
 
-	/* Apply firmware specific fixups to kernel text (nop out
-	 * sections not relevant to this firmware)
-	 */
-	do_feature_fixups(powerpc_firmware_features,
-			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
-
  	/*
 	 * We can discover serial ports now since the above did setup the
 	 * hash table management for us, thus ioremap works. We do that early

commit 5a26f6bbb767d7ad23311a1e81cfdd2bebefb855
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Jun 8 14:00:35 2007 +1000

    [POWERPC] Do firmware feature fixups after features are initialised
    
    On pSeries the firmware features are not setup until ppc_md.init_early,
    so we can't do the firmware feature sections fixups till after this.
    
    Currently firmware feature sections is only used on iSeries which inits
    the firmware features much earlier.  This is a bug in waiting on
    pSeries.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6018178708a5..bc43bba05cf8 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -350,13 +350,11 @@ void __init setup_system(void)
 {
 	DBG(" -> setup_system()\n");
 
-	/* Apply the CPUs-specific and firmware specific fixups to kernel
-	 * text (nop out sections not relevant to this CPU or this firmware)
+	/* Apply CPUs-specific fixups to kernel text (nop out sections
+	 * not relevant to this CPU)
 	 */
 	do_feature_fixups(cur_cpu_spec->cpu_features,
 			  &__start___ftr_fixup, &__stop___ftr_fixup);
-	do_feature_fixups(powerpc_firmware_features,
-			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
 
 	/*
 	 * Unflatten the device-tree passed by prom_init or kexec
@@ -394,6 +392,12 @@ void __init setup_system(void)
 	if (ppc_md.init_early)
 		ppc_md.init_early();
 
+	/* Apply firmware specific fixups to kernel text (nop out
+	 * sections not relevant to this firmware)
+	 */
+	do_feature_fixups(powerpc_firmware_features,
+			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
+
  	/*
 	 * We can discover serial ports now since the above did setup the
 	 * hash table management for us, thus ioremap works. We do that early

commit b6e3590f8145c77b8fcef3247e2412335221412f
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed May 2 19:27:12 2007 +0200

    [PATCH] x86: Allow percpu variables to be page-aligned
    
    Let's allow page-alignment in general for per-cpu data (wanted by Xen, and
    Ingo suggested KVM as well).
    
    Because larger alignments can use more room, we increase the max per-cpu
    memory to 64k rather than 32k: it's getting a little tight.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 22083ce3cc30..6018178708a5 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -582,14 +582,14 @@ void __init setup_per_cpu_areas(void)
 	char *ptr;
 
 	/* Copy section for each CPU (we discard the original) */
-	size = ALIGN(__per_cpu_end - __per_cpu_start, SMP_CACHE_BYTES);
+	size = ALIGN(__per_cpu_end - __per_cpu_start, PAGE_SIZE);
 #ifdef CONFIG_MODULES
 	if (size < PERCPU_ENOUGH_ROOM)
 		size = PERCPU_ENOUGH_ROOM;
 #endif
 
 	for_each_possible_cpu(i) {
-		ptr = alloc_bootmem_node(NODE_DATA(cpu_to_node(i)), size);
+		ptr = alloc_bootmem_pages_node(NODE_DATA(cpu_to_node(i)), size);
 		if (!ptr)
 			panic("Cannot allocate cpu data for CPU %d\n", i);
 

commit e2eb63927bfcb54232163bfec32440246fd44457
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Tue Apr 3 22:26:41 2007 +1000

    [POWERPC] Rename get_property to of_get_property: arch/powerpc
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 3c0c7f435844..22083ce3cc30 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -109,7 +109,7 @@ static void check_smt_enabled(void)
 	dn = of_find_node_by_path("/options");
 
 	if (dn) {
-		smt_option = get_property(dn, "ibm,smt-enabled", NULL);
+		smt_option = of_get_property(dn, "ibm,smt-enabled", NULL);
 
                 if (smt_option) {
 			if (!strcmp(smt_option, "on"))
@@ -304,10 +304,10 @@ static void __init initialize_cache_info(void)
 
 			size = 0;
 			lsize = cur_cpu_spec->dcache_bsize;
-			sizep = get_property(np, "d-cache-size", NULL);
+			sizep = of_get_property(np, "d-cache-size", NULL);
 			if (sizep != NULL)
 				size = *sizep;
-			lsizep = get_property(np, dc, NULL);
+			lsizep = of_get_property(np, dc, NULL);
 			if (lsizep != NULL)
 				lsize = *lsizep;
 			if (sizep == 0 || lsizep == 0)
@@ -321,10 +321,10 @@ static void __init initialize_cache_info(void)
 
 			size = 0;
 			lsize = cur_cpu_spec->icache_bsize;
-			sizep = get_property(np, "i-cache-size", NULL);
+			sizep = of_get_property(np, "i-cache-size", NULL);
 			if (sizep != NULL)
 				size = *sizep;
-			lsizep = get_property(np, ic, NULL);
+			lsizep = of_get_property(np, ic, NULL);
 			if (lsizep != NULL)
 				lsize = *lsizep;
 			if (sizep == 0 || lsizep == 0)

commit 8545cd201134860b1eb72578419f5cbd4c0789c0
Author: Olaf Hering <olaf@aepfle.de>
Date:   Fri Mar 23 01:11:59 2007 +0100

    [POWERPC] Remove unused inclusion of linux/ide.h
    
    Remove unneeded inclusion of linux/ide.h
    It does not compile with CONFIG_BLOCK=n.
    
    Remove asm/ide.h from ksyms file, it gets included earlier via
    linux/ide.h.
    
    Compile tested with all defconfig files.
    
    Signed-off-by: Olaf Hering <olaf@aepfle.de>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 3733de30e84d..3c0c7f435844 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -20,7 +20,6 @@
 #include <linux/reboot.h>
 #include <linux/delay.h>
 #include <linux/initrd.h>
-#include <linux/ide.h>
 #include <linux/seq_file.h>
 #include <linux/ioport.h>
 #include <linux/console.h>

commit 974a76f51355d22f4f63d83d6bb1ccecd019ec58
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Nov 10 20:38:53 2006 +1100

    [POWERPC] Distinguish POWER6 partition modes and tell userspace
    
    This adds code to look at the properties firmware puts in the device
    tree to determine what compatibility mode the partition is in on
    POWER6 machines, and set the ELF aux vector AT_HWCAP and AT_PLATFORM
    entries appropriately.
    
    Specifically, we look at the cpu-version property in the cpu node(s).
    If that contains a "logical" PVR value (of the form 0x0f00000x), we
    call identify_cpu again with this PVR value.  A value of 0x0f000001
    indicates the partition is in POWER5+ compatibility mode, and a value
    of 0x0f000002 indicates "POWER6 architected" mode, with various
    extensions disabled.  We also look for various other properties:
    ibm,dfp, ibm,purr and ibm,spurr.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f602a53b1b79..3733de30e84d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -171,7 +171,7 @@ void __init setup_paca(int cpu)
 void __init early_setup(unsigned long dt_ptr)
 {
 	/* Identify CPU type */
-	identify_cpu(0);
+	identify_cpu(0, mfspr(SPRN_PVR));
 
 	/* Assume we're on cpu 0 for now. Don't write to the paca yet! */
 	setup_paca(0);

commit 4cb3cee03d558fd457cb58f56c80a2a09a66110c
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sat Nov 11 17:25:10 2006 +1100

    [POWERPC] Allow hooking of PCI MMIO & PIO accessors on 64 bits
    
    This patch reworks the way iSeries hooks on PCI IO operations (both MMIO
    and PIO) and provides a generic way for other platforms to do so (we
    have need to do that for various other platforms).
    
    While reworking the IO ops, I ended up doing some spring cleaning in
    io.h and eeh.h which I might want to split into 2 or 3 patches (among
    others, eeh.h had a lot of useless stuff in it).
    
    A side effect is that EEH for PIO should work now (it used to pass IO
    ports down to the eeh address check functions which is bogus).
    
    Also, new are MMIO "repeat" ops, which other archs like ARM already had,
    and that we have too now: readsb, readsw, readsl, writesb, writesw,
    writesl.
    
    In the long run, I might also make EEH use the hooks instead
    of wrapping at the toplevel, which would make things even cleaner and
    relegate EEH completely in platforms/iseries, but we have to measure the
    performance impact there (though it's really only on MMIO reads)
    
    Since I also need to hook on ioremap, I shuffled the functions a bit
    there. I introduced ioremap_flags() to use by drivers who want to pass
    explicit flags to ioremap (and it can be hooked). The old __ioremap() is
    still there as a low level and cannot be hooked, thus drivers who use it
    should migrate unless they know they want the low level version.
    
    The patch "arch provides generic iomap missing accessors" (should be
    number 4 in this series) is a pre-requisite to provide full iomap
    API support with this patch.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f7ad64acf47e..f602a53b1b79 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -599,3 +599,10 @@ void __init setup_per_cpu_areas(void)
 	}
 }
 #endif
+
+
+#ifdef CONFIG_PPC_INDIRECT_IO
+struct ppc_pci_io ppc_pci_io;
+EXPORT_SYMBOL(ppc_pci_io);
+#endif /* CONFIG_PPC_INDIRECT_IO */
+

commit 12d04eef927bf61328af2c7cbe756c96f98ac3bf
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sat Nov 11 17:25:02 2006 +1100

    [POWERPC] Refactor 64 bits DMA operations
    
    This patch completely refactors DMA operations for 64 bits powerpc. 32 bits
    is untouched for now.
    
    We use the new dev_archdata structure to add the dma operations pointer
    and associated data to struct device. While at it, we also add the OF node
    pointer and numa node. In the future, we might want to look into merging
    that with pci_dn as well.
    
    The old vio, pci-iommu and pci-direct DMA ops are gone. They are now replaced
    by a set of generic iommu and direct DMA ops (non PCI specific) that can be
    used by bus types. The toplevel implementation is now inline.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index b0f1c82df994..f7ad64acf47e 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -33,6 +33,7 @@
 #include <linux/serial.h>
 #include <linux/serial_8250.h>
 #include <linux/bootmem.h>
+#include <linux/pci.h>
 #include <asm/io.h>
 #include <asm/kdump.h>
 #include <asm/prom.h>

commit 79acbb3ff2d8095b692e1502b9eb2ccec348de26
Merge: 19a79859e168 2b5f6dcce5bf
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 4 15:59:07 2006 +1100

    Merge branch 'linux-2.6' into for-linus

commit 57744ea95edd340d7140852ce86c743df2cd588c
Author: Geoff Levand <geoffrey.levand@am.sony.com>
Date:   Fri Nov 10 12:01:02 2006 -0800

    [PATCH] Check for null init_early routine
    
    Add a check for a null ppc_md.init_early to allow platforms that
    don't require an init_early routine to just set this member to null.
    
    Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ae6b67ce54e7..2fef772edfc1 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -380,7 +380,8 @@ void __init setup_system(void)
 	 * setting up the hash table pointers. It also sets up some interrupt-mapping
 	 * related options that will be used by finish_device_tree()
 	 */
-	ppc_md.init_early();
+	if (ppc_md.init_early)
+		ppc_md.init_early();
 
  	/*
 	 * We can discover serial ports now since the above did setup the

commit fd6e7d2d6a0231ebfa08e1f9a323497ea548da7d
Author: s.hauer@pengutronix.de <s.hauer@pengutronix.de>
Date:   Thu Nov 2 13:56:10 2006 +0100

    [PATCH] Clean up usage of boot_dev
    
    dev_t boot_dev is declared in arch/powerpc/kernel/setup_32.c
    and in arch/powerpc/kernel/setup_64.c but not used in these files.
    It is only used in arch/powerpc/platforms/powermac/setup.c, so make
    it static in this file.
    
    Signed-off-by: Sascha Hauer <s.hauer@pengutronix.de>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index b1b0cda3f748..ae6b67ce54e7 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -71,7 +71,6 @@
 
 int have_of = 1;
 int boot_cpuid = 0;
-dev_t boot_dev;
 u64 ppc64_pft_size;
 
 /* Pick defaults since we might want to patch instructions

commit 0909c8c2d547e45ca50e2492b08ec93a37b35237
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Oct 20 11:47:18 2006 +1000

    [POWERPC] Support feature fixups in vdso's
    
    This patch reworks the feature fixup mecanism so vdso's can be fixed up.
    The main issue was that the construct:
    
            .long   label  (or .llong on 64 bits)
    
    will not work in the case of a shared library like the vdso. It will
    generate an empty placeholder in the fixup table along with a reloc,
    which is not something we can deal with in the vdso.
    
    The idea here (thanks Alan Modra !) is to instead use something like:
    
    1:
            .long   label - 1b
    
    That is, the feature fixup tables no longer contain addresses of bits of
    code to patch, but offsets of such code from the fixup table entry
    itself. That is properly resolved by ld when building the .so's. I've
    modified the fixup mecanism generically to use that method for the rest
    of the kernel as well.
    
    Another trick is that the 32 bits vDSO included in the 64 bits kernel
    need to have a table in the 64 bits format. However, gas does not
    support 32 bits code with a statement of the form:
    
            .llong  label - 1b  (Or even just .llong label)
    
    That is, it cannot emit the right fixup/relocation for the linker to use
    to assign a 32 bits address to an .llong field. Thus, in the specific
    case of the 32 bits vdso built as part of the 64 bits kernel, we are
    using a modified macro that generates:
    
            .long   0xffffffff
            .llong  label - 1b
    
    Note that is assumes that the value is negative which is enforced by
    the .lds (those offsets are always negative as the .text is always
    before the fixup table and gas doesn't support emiting the reloc the
    other way around).
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 1969b5686eee..16278968dab6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -354,9 +354,9 @@ void __init setup_system(void)
 	/* Apply the CPUs-specific and firmware specific fixups to kernel
 	 * text (nop out sections not relevant to this CPU or this firmware)
 	 */
-	do_feature_fixups(0, cur_cpu_spec->cpu_features,
+	do_feature_fixups(cur_cpu_spec->cpu_features,
 			  &__start___ftr_fixup, &__stop___ftr_fixup);
-	do_feature_fixups(0, powerpc_firmware_features,
+	do_feature_fixups(powerpc_firmware_features,
 			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
 
 	/*

commit 42c4aaadb737e0e672b3fb86b2c41ff59f0fb8bc
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Oct 24 16:42:40 2006 +1000

    [POWERPC] Consolidate feature fixup code
    
    There are currently two versions of the functions for applying the
    feature fixups, one for CPU features and one for firmware features. In
    addition, they are both in assembly and with separate implementations
    for 32 and 64 bits. identify_cpu() is also implemented in assembly and
    separately for 32 and 64 bits.
    
    This patch replaces them with a pair of C functions. The call sites are
    slightly moved on ppc64 as well to be called from C instead of from
    assembly, though it's a very small change, and thus shouldn't cause any
    problem.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4b2e32eab9dc..1969b5686eee 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -170,6 +170,9 @@ void __init setup_paca(int cpu)
 
 void __init early_setup(unsigned long dt_ptr)
 {
+	/* Identify CPU type */
+	identify_cpu(0);
+
 	/* Assume we're on cpu 0 for now. Don't write to the paca yet! */
 	setup_paca(0);
 
@@ -348,6 +351,14 @@ void __init setup_system(void)
 {
 	DBG(" -> setup_system()\n");
 
+	/* Apply the CPUs-specific and firmware specific fixups to kernel
+	 * text (nop out sections not relevant to this CPU or this firmware)
+	 */
+	do_feature_fixups(0, cur_cpu_spec->cpu_features,
+			  &__start___ftr_fixup, &__stop___ftr_fixup);
+	do_feature_fixups(0, powerpc_firmware_features,
+			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
+
 	/*
 	 * Unflatten the device-tree passed by prom_init or kexec
 	 */

commit d04c56f73c30a5e593202ecfcf25ed43d42363a2
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Oct 4 16:47:49 2006 +1000

    [POWERPC] Lazy interrupt disabling for 64-bit machines
    
    This implements a lazy strategy for disabling interrupts.  This means
    that local_irq_disable() et al. just clear the 'interrupts are
    enabled' flag in the paca.  If an interrupt comes along, the interrupt
    entry code notices that interrupts are supposed to be disabled, and
    clears the EE bit in SRR1, clears the 'interrupts are hard-enabled'
    flag in the paca, and returns.  This means that interrupts only
    actually get disabled in the processor when an interrupt comes along.
    
    When interrupts are enabled by local_irq_enable() et al., the code
    sets the interrupts-enabled flag in the paca, and then checks whether
    interrupts got hard-disabled.  If so, it also sets the EE bit in the
    MSR to hard-enable the interrupts.
    
    This has the potential to improve performance, and also makes it
    easier to make a kernel that can boot on iSeries and on other 64-bit
    machines, since this lazy-disable strategy is very similar to the
    soft-disable strategy that iSeries already uses.
    
    This version renames paca->proc_enabled to paca->soft_enabled, and
    changes a couple of soft-disables in the kexec code to hard-disables,
    which should fix the crash that Michael Ellerman saw.  This doesn't
    yet use a reserved CR field for the soft_enabled and hard_enabled
    flags.  This applies on top of Stephen Rothwell's patches to make it
    possible to build a combined iSeries/other kernel.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4b2e32eab9dc..b1b0cda3f748 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -223,8 +223,8 @@ void early_setup_secondary(void)
 {
 	struct paca_struct *lpaca = get_paca();
 
-	/* Mark enabled in PACA */
-	lpaca->proc_enabled = 0;
+	/* Mark interrupts enabled in PACA */
+	lpaca->soft_enabled = 0;
 
 	/* Initialize hash table for that CPU */
 	htab_initialize_secondary();

commit 476792839467c08ddeedd8b44a7423d415b68259
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Oct 3 14:12:08 2006 +1000

    [POWERPC] Fix xmon=off and cleanup xmon initialisation
    
    My patch to make the early xmon logic work with earlier early param
    parsing (480f6f35a149802a94ad5c1a2673ed6ec8d2c158) breaks xmon=off.
    
    No one does this obviously as xmon rocks, but it should really work
    as documented.
    
    While fixing that it struck me that we could move the xmon param
    handling into xmon.c, and also consolidate the
    xmon_init()/do_early_xmon logic into xmon_setup(). This means
    xmon=early drops into xmon a little earlier on 32-bit, but it
    seems to work just fine.
    
    Tested on PSERIES and CLASSIC32.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index cda2dbe70a76..4b2e32eab9dc 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -390,19 +390,15 @@ void __init setup_system(void)
 	 */
 	find_legacy_serial_ports();
 
-	/*
-	 * Initialize xmon
-	 */
-#ifdef CONFIG_XMON_DEFAULT
-	xmon_init(1);
-#endif
 	/*
 	 * Register early console
 	 */
 	register_early_udbg_console();
 
-	if (do_early_xmon)
-		debugger(NULL);
+	/*
+	 * Initialize xmon
+	 */
+	xmon_setup();
 
 	check_smt_enabled();
 	smp_setup_cpu_maps();

commit 96b644bdec977b97a45133e5b4466ba47a7a5e65
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:13 2006 -0700

    [PATCH] namespaces: utsname: use init_utsname when appropriate
    
    In some places, particularly drivers and __init code, the init utsns is the
    appropriate one to use.  This patch replaces those with a the init_utsname
    helper.
    
    Changes: Removed several uses of init_utsname().  Hope I picked all the
            right ones in net/ipv4/ipconfig.c.  These are now changed to
            utsname() (the per-process namespace utsname) in the previous
            patch (2/7)
    
    [akpm@osdl.org: CIFS fix]
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 962ad5ebc767..cda2dbe70a76 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -414,7 +414,7 @@ void __init setup_system(void)
 	smp_release_cpus();
 #endif
 
-	printk("Starting Linux PPC64 %s\n", system_utsname.version);
+	printk("Starting Linux PPC64 %s\n", init_utsname()->version);
 
 	printk("-----------------------------------------------------\n");
 	printk("ppc64_pft_size                = 0x%lx\n", ppc64_pft_size);

commit fb48388337182013bce811b9c336e8e64b0c858b
Author: Olaf Hering <olaf@aepfle.de>
Date:   Sat Sep 30 23:27:51 2006 -0700

    [PATCH] remove SYSRQ_KEY and related defines from ppc/sh/h8300
    
    Remove unused global SYSRQ_KEY from ppc and powerpc
    Remove unused define SYSRQ_KEY from sh/sh64 and h8300
    Remove unused pckbd_sysrq_xlate and kbd_sysrq_xlate usage
    
    Signed-off-by: Olaf Hering <olaf@aepfle.de>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Kazumoto Kojima <kkojima@rr.iij4u.or.jp>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 00d6b8addd78..962ad5ebc767 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -93,11 +93,6 @@ int dcache_bsize;
 int icache_bsize;
 int ucache_bsize;
 
-#ifdef CONFIG_MAGIC_SYSRQ
-unsigned long SYSRQ_KEY;
-#endif /* CONFIG_MAGIC_SYSRQ */
-
-
 #ifdef CONFIG_SMP
 
 static int smt_enabled_cmdline;

commit 5a2fe38d2844ba2f2dd8f4946d795e09d8f7e095
Author: Olof Johansson <olof@lixom.net>
Date:   Wed Sep 6 14:34:41 2006 -0500

    [POWERPC] powerpc: Reduce default cacheline size to 64 bytes
    
    Reduce default cacheline size on 64-bit powerpc from 128 bytes to 64.
    This is the architected minimum. In most cases we'll still end up using
    cache line information from the device tree, but defaults are used during
    early boot and doing a few dcbst/icbi's too many there won't do any harm.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 77efe19ccd2c..00d6b8addd78 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -78,10 +78,10 @@ u64 ppc64_pft_size;
  * before we've read this from the device tree.
  */
 struct ppc64_caches ppc64_caches = {
-	.dline_size = 0x80,
-	.log_dline_size = 7,
-	.iline_size = 0x80,
-	.log_iline_size = 7
+	.dline_size = 0x40,
+	.log_dline_size = 6,
+	.iline_size = 0x40,
+	.log_iline_size = 6
 };
 EXPORT_SYMBOL_GPL(ppc64_caches);
 

commit a7f67bdf2c9f24509b8e81e0f35573b611987c80
Author: Jeremy Kerr <jk@ozlabs.org>
Date:   Wed Jul 12 15:35:54 2006 +1000

    [POWERPC] Constify & voidify get_property()
    
    Now that get_property() returns a void *, there's no need to cast its
    return value. Also, treat the return value as const, so we can
    constify get_property later.
    
    powerpc core changes.
    
    Signed-off-by: Jeremy Kerr <jk@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index e2447aef3a8f..77efe19ccd2c 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -106,7 +106,7 @@ static int smt_enabled_cmdline;
 static void check_smt_enabled(void)
 {
 	struct device_node *dn;
-	char *smt_option;
+	const char *smt_option;
 
 	/* Allow the command line to overrule the OF option */
 	if (smt_enabled_cmdline)
@@ -115,7 +115,7 @@ static void check_smt_enabled(void)
 	dn = of_find_node_by_path("/options");
 
 	if (dn) {
-		smt_option = (char *)get_property(dn, "ibm,smt-enabled", NULL);
+		smt_option = get_property(dn, "ibm,smt-enabled", NULL);
 
                 if (smt_option) {
 			if (!strcmp(smt_option, "on"))
@@ -292,7 +292,7 @@ static void __init initialize_cache_info(void)
 		 */
 
 		if ( num_cpus == 1 ) {
-			u32 *sizep, *lsizep;
+			const u32 *sizep, *lsizep;
 			u32 size, lsize;
 			const char *dc, *ic;
 
@@ -307,10 +307,10 @@ static void __init initialize_cache_info(void)
 
 			size = 0;
 			lsize = cur_cpu_spec->dcache_bsize;
-			sizep = (u32 *)get_property(np, "d-cache-size", NULL);
+			sizep = get_property(np, "d-cache-size", NULL);
 			if (sizep != NULL)
 				size = *sizep;
-			lsizep = (u32 *) get_property(np, dc, NULL);
+			lsizep = get_property(np, dc, NULL);
 			if (lsizep != NULL)
 				lsize = *lsizep;
 			if (sizep == 0 || lsizep == 0)
@@ -324,10 +324,10 @@ static void __init initialize_cache_info(void)
 
 			size = 0;
 			lsize = cur_cpu_spec->icache_bsize;
-			sizep = (u32 *)get_property(np, "i-cache-size", NULL);
+			sizep = get_property(np, "i-cache-size", NULL);
 			if (sizep != NULL)
 				size = *sizep;
-			lsizep = (u32 *)get_property(np, ic, NULL);
+			lsizep = get_property(np, ic, NULL);
 			if (lsizep != NULL)
 				lsize = *lsizep;
 			if (sizep == 0 || lsizep == 0)

commit 06a36db1d712242a00cb30aaebdd088b4be28082
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Jul 13 17:52:17 2006 +1000

    [POWERPC] iseries: Move ItLpNaca into platforms/iseries
    
    Move ItLpNaca into platforms/iseries now that it's not used elsewhere.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index fd1785e4c9bb..e2447aef3a8f 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -56,7 +56,6 @@
 #include <asm/page.h>
 #include <asm/mmu.h>
 #include <asm/lmb.h>
-#include <asm/iseries/it_lp_naca.h>
 #include <asm/firmware.h>
 #include <asm/xmon.h>
 #include <asm/udbg.h>

commit 0ebfff1491ef85d41ddf9c633834838be144f69f
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Jul 3 21:36:01 2006 +1000

    [POWERPC] Add new interrupt mapping core and change platforms to use it
    
    This adds the new irq remapper core and removes the old one.  Because
    there are some fundamental conflicts with the old code, like the value
    of NO_IRQ which I'm now setting to 0 (as per discussions with Linus),
    etc..., this commit also changes the relevant platform and driver code
    over to use the new remapper (so as not to cause difficulties later
    in bisecting).
    
    This patch removes the old pre-parsing of the open firmware interrupt
    tree along with all the bogus assumptions it made to try to renumber
    interrupts according to the platform. This is all to be handled by the
    new code now.
    
    For the pSeries XICS interrupt controller, a single remapper host is
    created for the whole machine regardless of how many interrupt
    presentation and source controllers are found, and it's set to match
    any device node that isn't a 8259.  That works fine on pSeries and
    avoids having to deal with some of the complexities of split source
    controllers vs. presentation controllers in the pSeries device trees.
    
    The powerpc i8259 PIC driver now always requests the legacy interrupt
    range. It also has the feature of being able to match any device node
    (including NULL) if passed no device node as an input. That will help
    porting over platforms with broken device-trees like Pegasos who don't
    have a proper interrupt tree.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ac7276c40685..fd1785e4c9bb 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -361,12 +361,15 @@ void __init setup_system(void)
 
 	/*
 	 * Fill the ppc64_caches & systemcfg structures with informations
-	 * retrieved from the device-tree. Need to be called before
-	 * finish_device_tree() since the later requires some of the
-	 * informations filled up here to properly parse the interrupt tree.
+ 	 * retrieved from the device-tree.
 	 */
 	initialize_cache_info();
 
+	/*
+	 * Initialize irq remapping subsystem
+	 */
+	irq_early_init();
+
 #ifdef CONFIG_PPC_RTAS
 	/*
 	 * Initialize RTAS if available
@@ -393,12 +396,6 @@ void __init setup_system(void)
 	 */
 	find_legacy_serial_ports();
 
-	/*
-	 * "Finish" the device-tree, that is do the actual parsing of
-	 * some of the properties like the interrupt map
-	 */
-	finish_device_tree();
-
 	/*
 	 * Initialize xmon
 	 */
@@ -427,8 +424,6 @@ void __init setup_system(void)
 
 	printk("-----------------------------------------------------\n");
 	printk("ppc64_pft_size                = 0x%lx\n", ppc64_pft_size);
-	printk("ppc64_interrupt_controller    = 0x%ld\n",
-	       ppc64_interrupt_controller);
 	printk("physicalMemorySize            = 0x%lx\n", lmb_phys_mem_size());
 	printk("ppc64_caches.dcache_line_size = 0x%x\n",
 	       ppc64_caches.dline_size);

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jörn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jörn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 175539c9afa0..ac7276c40685 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -12,7 +12,6 @@
 
 #undef DEBUG
 
-#include <linux/config.h>
 #include <linux/module.h>
 #include <linux/string.h>
 #include <linux/sched.h>

commit 33dbcf72f607f5da791402e161feaf1ccf5d5be4
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed Jun 28 13:18:53 2006 +1000

    [POWERPC] Make sure smp_processor_id works very early in boot
    
    There's a small period early in boot where we don't know which cpu we're
    running on. That's ok, except that it means we have no paca, or more
    correctly that our paca pointer points somewhere random.
    
    So that we can safely call things like smp_processor_id(), we need a paca,
    so just assume we're on cpu 0. No code should _write_ to the paca before
    we've set the correct one up.
    
    We setup the proper paca after we've scanned the flat device tree in
    early_setup(), so there's no need to do it again in start_here_common.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index a2fb2e627aad..175539c9afa0 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -177,6 +177,9 @@ void __init setup_paca(int cpu)
 
 void __init early_setup(unsigned long dt_ptr)
 {
+	/* Assume we're on cpu 0 for now. Don't write to the paca yet! */
+	setup_paca(0);
+
 	/* Enable early debugging if any specified (see udbg.h) */
 	udbg_early_init();
 

commit 4ba99b97dadd35b9ce1438b2bc7c992a4a14a8b1
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Jun 23 18:20:09 2006 +1000

    [POWERPC] Setup the boot cpu's paca pointer in C rather than asm
    
    There's no need to set the boot cpu paca in asm, so do it in C so us
    mere mortals can understand it.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index cc26530145db..a2fb2e627aad 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -149,6 +149,13 @@ early_param("smt-enabled", early_smt_enabled);
 #define check_smt_enabled()
 #endif /* CONFIG_SMP */
 
+/* Put the paca pointer into r13 and SPRG3 */
+void __init setup_paca(int cpu)
+{
+	local_paca = &paca[cpu];
+	mtspr(SPRN_SPRG3, local_paca);
+}
+
 /*
  * Early initialization entry point. This is called by head.S
  * with MMU translation disabled. We rely on the "feature" of
@@ -183,7 +190,7 @@ void __init early_setup(unsigned long dt_ptr)
 	early_init_devtree(__va(dt_ptr));
 
 	/* Now we know the logical id of our boot cpu, setup the paca. */
-	setup_boot_paca();
+	setup_paca(boot_cpuid);
 
 	/* Fix up paca fields required for the boot cpu */
 	get_paca()->cpu_start = 1;

commit aa98c50dcb5d5b85d2a4c26d54fa1e3c31c11e4b
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Jun 23 18:17:32 2006 +1000

    [POWERPC] Make kexec_setup() a regular initcall
    
    There's no reason kexec_setup() needs to be called explicitly from
    setup_system(), it can just be a regular initcall.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6f213a9e36bf..cc26530145db 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -350,10 +350,6 @@ void __init setup_system(void)
 	 */
 	unflatten_device_tree();
 
-#ifdef CONFIG_KEXEC
-	kexec_setup();	/* requires unflattened device tree. */
-#endif
-
 	/*
 	 * Fill the ppc64_caches & systemcfg structures with informations
 	 * retrieved from the device-tree. Need to be called before

commit 7d0daae4ae1a3e80d78b83cddf414a3b98a962f4
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Jun 23 18:16:38 2006 +1000

    [POWERPC] powerpc: Initialise ppc_md htab pointers earlier
    
    Initialise the ppc_md htab callbacks earlier, in the probe routines. This
    allows us to call htab_finish_init() from htab_initialize(), and makes it
    private to hash_utils_64.c. Move htab_finish_init() and make_bl() above
    htab_initialize() to avoid forward declarations.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 78f3a5fd43f6..6f213a9e36bf 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -358,11 +358,7 @@ void __init setup_system(void)
 	 * Fill the ppc64_caches & systemcfg structures with informations
 	 * retrieved from the device-tree. Need to be called before
 	 * finish_device_tree() since the later requires some of the
-	 * informations filled up here to properly parse the interrupt
-	 * tree.
-	 * It also sets up the cache line sizes which allows to call
-	 * routines like flush_icache_range (used by the hash init
-	 * later on).
+	 * informations filled up here to properly parse the interrupt tree.
 	 */
 	initialize_cache_info();
 

commit 473104134b35ce1c3ca77b738c561d6c215adc1b
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed May 17 18:00:49 2006 +1000

    [PATCH] powerpc: Kdump header cleanup
    
    We need to know the base address of the kdump kernel even when we're not a
    kdump kernel, so add a #define for it. Move the logic that sets the kdump
    kernelbase into kdump.h instead of page.h.
    
    Rename kdump_setup() to setup_kdump_trampoline() to make it clearer what it's
    doing, and add an empty definition for the !CRASH_DUMP case to avoid a
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 59773d9044ba..78f3a5fd43f6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -193,9 +193,7 @@ void __init early_setup(unsigned long dt_ptr)
 	/* Probe the machine type */
 	probe_machine();
 
-#ifdef CONFIG_CRASH_DUMP
-	kdump_setup();
-#endif
+	setup_kdump_trampoline();
 
 	DBG("Found, Initializing memory management...\n");
 

commit 2babf5c2ec2f2d5de3e38d20f7df7fd815fd10c9
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed May 17 18:00:46 2006 +1000

    [PATCH] powerpc: Unify mem= handling
    
    We currently do mem= handling in three seperate places. And as benh pointed out
    I wrote two of them. Now that we parse command line parameters earlier we can
    clean this mess up.
    
    Moving the parsing out of prom_init means the device tree might be allocated
    above the memory limit. If that happens we'd have to move it. As it happens
    we already have logic to do that for kdump, so just genericise it.
    
    This also means we might have reserved regions above the memory limit, if we
    do the bootmem allocator will blow up, so we have to modify
    lmb_enforce_memory_limit() to truncate the reserves as well.
    
    Tested on P5 LPAR, iSeries, F50, 44p. Tested moving device tree on P5 and
    44p and F50.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6224624c3d38..59773d9044ba 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -347,9 +347,6 @@ void __init setup_system(void)
 {
 	DBG(" -> setup_system()\n");
 
-#ifdef CONFIG_KEXEC
-	kdump_move_device_tree();
-#endif
 	/*
 	 * Unflatten the device-tree passed by prom_init or kexec
 	 */

commit 846f77b08c8301682ded5ce127c56397327a60d0
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed May 17 18:00:45 2006 +1000

    [PATCH] powerpc: Parse early parameters earlier
    
    Currently we have call parse_early_param() earliyish, but not really very
    early. In particular, it's not early enough to do things like mem=x or
    crashkernel=blah, which is annoying.
    
    So do it earlier. I've checked all the early param handlers, and none of them
    look like they should have any trouble with this. I haven't tested the
    booke_wdt ones though.
    
    On 32-bit we were doing the CONFIG_CMDLINE logic twice, so don't.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index a1923d917b30..6224624c3d38 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -414,11 +414,6 @@ void __init setup_system(void)
 	 */
 	register_early_udbg_console();
 
-	/* Save unparsed command line copy for /proc/cmdline */
-	strlcpy(saved_command_line, cmd_line, COMMAND_LINE_SIZE);
-
-	parse_early_param();
-
 	if (do_early_xmon)
 		debugger(NULL);
 

commit 480f6f35a149802a94ad5c1a2673ed6ec8d2c158
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed May 17 18:00:41 2006 +1000

    [PATCH] powerpc: Make early xmon logic immune to location of early parsing
    
    Currently early_xmon() calls directly into debugger() if xmon=early is passed.
    This ties the invocation of early xmon to the location of parse_early_param(),
    which might change.
    
    Tested on P5 LPAR and F50.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index ab6ea37a77aa..a1923d917b30 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -419,6 +419,9 @@ void __init setup_system(void)
 
 	parse_early_param();
 
+	if (do_early_xmon)
+		debugger(NULL);
+
 	check_smt_enabled();
 	smp_setup_cpu_maps();
 

commit 7e990266c845d7f712c96013891aaf74baef198f
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Fri May 5 00:02:08 2006 -0500

    powerpc: provide ppc_md.panic() for both ppc32 & ppc64
    
    Allow boards to provide a panic callback on ppc32.  Moved the code to sets
    this up into setup-common.c so its shared between ppc32 & ppc64.  Also moved
    do_init_bootmem prototype into setup.h.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4467c49903b6..ab6ea37a77aa 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -100,12 +100,6 @@ unsigned long SYSRQ_KEY;
 #endif /* CONFIG_MAGIC_SYSRQ */
 
 
-static int ppc64_panic_event(struct notifier_block *, unsigned long, void *);
-static struct notifier_block ppc64_panic_block = {
-	.notifier_call = ppc64_panic_event,
-	.priority = INT_MIN /* may not return; must be done last */
-};
-
 #ifdef CONFIG_SMP
 
 static int smt_enabled_cmdline;
@@ -456,13 +450,6 @@ void __init setup_system(void)
 	DBG(" <- setup_system()\n");
 }
 
-static int ppc64_panic_event(struct notifier_block *this,
-                             unsigned long event, void *ptr)
-{
-	ppc_md.panic((char *)ptr);  /* May not return */
-	return NOTIFY_DONE;
-}
-
 #ifdef CONFIG_IRQSTACKS
 static void __init irqstack_early_init(void)
 {
@@ -517,8 +504,6 @@ static void __init emergency_stack_init(void)
  */
 void __init setup_arch(char **cmdline_p)
 {
-	extern void do_init_bootmem(void);
-
 	ppc64_boot_msg(0x12, "Setup Arch");
 
 	*cmdline_p = cmd_line;
@@ -535,8 +520,7 @@ void __init setup_arch(char **cmdline_p)
 	panic_timeout = 180;
 
 	if (ppc_md.panic)
-		atomic_notifier_chain_register(&panic_notifier_list,
-				&ppc64_panic_block);
+		setup_panic();
 
 	init_mm.start_code = PAGE_OFFSET;
 	init_mm.end_code = (unsigned long) _etext;

commit 1269277a5e7c6d7ae1852e648a8bcdb78035e9fa
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Mon Apr 24 23:22:17 2006 +0100

    [PATCH] powerpc: Use check_legacy_ioport() on ppc32 too.
    
    Some people report that we die on some Macs when we are expecting to
    catch machine checks after poking at some random I/O address. I'd seen
    it happen on my dual G4 with serial ports until we fixed those to use
    OF, but now other users are reporting it with i8042.
    
    This expands the use of check_legacy_ioport() to avoid that situation
    even on 32-bit kernels.
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 13e91c4d70a8..4467c49903b6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -594,14 +594,6 @@ void ppc64_terminate_msg(unsigned int src, const char *msg)
 	printk("[terminate]%04x %s\n", src, msg);
 }
 
-int check_legacy_ioport(unsigned long base_port)
-{
-	if (ppc_md.check_legacy_ioport == NULL)
-		return 0;
-	return ppc_md.check_legacy_ioport(base_port);
-}
-EXPORT_SYMBOL(check_legacy_ioport);
-
 void cpu_die(void)
 {
 	if (ppc_md.cpu_die)

commit 856d08ec46c5ecf3df13827c492fb6998fdc8322
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Sun Apr 2 02:45:04 2006 +1000

    [PATCH] powerpc: iSeries needs slb_initialize to be called
    
    Since the powerpc 64k pages patch went in, systems that have SLBs
    (like Power4 iSeries) needed to have slb_initialize called to set up
    some variables for the SLB miss handler.  This was not being called
    on the boot processor on iSeries, so on single cpu iSeries machines,
    we would get apparent memory curruption as soon as we entered user mode.
    
    This patch fixes that by calling slb_initialize on the boot cpu if the
    processor has an SLB.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 59aa92cd6fa4..13e91c4d70a8 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -215,12 +215,10 @@ void __init early_setup(unsigned long dt_ptr)
 	/*
 	 * Initialize stab / SLB management except on iSeries
 	 */
-	if (!firmware_has_feature(FW_FEATURE_ISERIES)) {
-		if (cpu_has_feature(CPU_FTR_SLB))
-			slb_initialize();
-		else
-			stab_initialize(get_paca()->stab_real);
-	}
+	if (cpu_has_feature(CPU_FTR_SLB))
+		slb_initialize();
+	else if (!firmware_has_feature(FW_FEATURE_ISERIES))
+		stab_initialize(get_paca()->stab_real);
 
 	DBG(" <- early_setup()\n");
 }

commit 0e5519548fdc8eadc3eacb49b1908d44d347fb2b
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Mar 28 14:50:51 2006 -0800

    [PATCH] for_each_possible_cpu: powerpc
    
    for_each_cpu() actually iterates across all possible CPUs.  We've had mistakes
    in the past where people were using for_each_cpu() where they should have been
    iterating across only online or present CPUs.  This is inefficient and
    possibly buggy.
    
    We're renaming for_each_cpu() to for_each_possible_cpu() to avoid this in the
    future.
    
    This patch replaces for_each_cpu with for_each_possible_cpu.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 05b152299396..59aa92cd6fa4 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -474,7 +474,7 @@ static void __init irqstack_early_init(void)
 	 * interrupt stacks must be under 256MB, we cannot afford to take
 	 * SLB misses on them.
 	 */
-	for_each_cpu(i) {
+	for_each_possible_cpu(i) {
 		softirq_ctx[i] = (struct thread_info *)
 			__va(lmb_alloc_base(THREAD_SIZE,
 					    THREAD_SIZE, 0x10000000));
@@ -507,7 +507,7 @@ static void __init emergency_stack_init(void)
 	 */
 	limit = min(0x10000000UL, lmb.rmo_size);
 
-	for_each_cpu(i)
+	for_each_possible_cpu(i)
 		paca[i].emergency_sp =
 		__va(lmb_alloc_base(HW_PAGE_SIZE, 128, limit)) + HW_PAGE_SIZE;
 }
@@ -624,7 +624,7 @@ void __init setup_per_cpu_areas(void)
 		size = PERCPU_ENOUGH_ROOM;
 #endif
 
-	for_each_cpu(i) {
+	for_each_possible_cpu(i) {
 		ptr = alloc_bootmem_node(NODE_DATA(cpu_to_node(i)), size);
 		if (!ptr)
 			panic("Cannot allocate cpu data for CPU %d\n", i);

commit bac30d1a78d0f11c613968fc8b351a91ed465386
Merge: e8222502ee61 ca9ba4471c12
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Mar 29 13:24:50 2006 +1100

    Merge ../linux-2.6

commit e8222502ee6157e2713da9e0792c21f4ad458d50
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Mar 28 23:15:54 2006 +1100

    [PATCH] powerpc: Kill _machine and hard-coded platform numbers
    
    This removes statically assigned platform numbers and reworks the
    powerpc platform probe code to use a better mechanism.  With this,
    board support files can simply declare a new machine type with a
    macro, and implement a probe() function that uses the flattened
    device-tree to detect if they apply for a given machine.
    
    We now have a machine_is() macro that replaces the comparisons of
    _machine with the various PLATFORM_* constants.  This commit also
    changes various drivers to use the new macro instead of looking at
    _machine.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 5b63a861ef43..6aea1fb74b69 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -95,11 +95,6 @@ int dcache_bsize;
 int icache_bsize;
 int ucache_bsize;
 
-/* The main machine-dep calls structure
- */
-struct machdep_calls ppc_md;
-EXPORT_SYMBOL(ppc_md);
-
 #ifdef CONFIG_MAGIC_SYSRQ
 unsigned long SYSRQ_KEY;
 #endif /* CONFIG_MAGIC_SYSRQ */
@@ -160,32 +155,6 @@ early_param("smt-enabled", early_smt_enabled);
 #define check_smt_enabled()
 #endif /* CONFIG_SMP */
 
-extern struct machdep_calls pSeries_md;
-extern struct machdep_calls pmac_md;
-extern struct machdep_calls maple_md;
-extern struct machdep_calls cell_md;
-extern struct machdep_calls iseries_md;
-
-/* Ultimately, stuff them in an elf section like initcalls... */
-static struct machdep_calls __initdata *machines[] = {
-#ifdef CONFIG_PPC_PSERIES
-	&pSeries_md,
-#endif /* CONFIG_PPC_PSERIES */
-#ifdef CONFIG_PPC_PMAC
-	&pmac_md,
-#endif /* CONFIG_PPC_PMAC */
-#ifdef CONFIG_PPC_MAPLE
-	&maple_md,
-#endif /* CONFIG_PPC_MAPLE */
-#ifdef CONFIG_PPC_CELL
-	&cell_md,
-#endif
-#ifdef CONFIG_PPC_ISERIES
-	&iseries_md,
-#endif
-	NULL
-};
-
 /*
  * Early initialization entry point. This is called by head.S
  * with MMU translation disabled. We rely on the "feature" of
@@ -207,12 +176,10 @@ static struct machdep_calls __initdata *machines[] = {
 
 void __init early_setup(unsigned long dt_ptr)
 {
-	static struct machdep_calls **mach;
-
 	/* Enable early debugging if any specified (see udbg.h) */
 	udbg_early_init();
 
-	DBG(" -> early_setup()\n");
+ 	DBG(" -> early_setup(), dt_ptr: 0x%lx\n", dt_ptr);
 
 	/*
 	 * Do early initializations using the flattened device
@@ -229,22 +196,8 @@ void __init early_setup(unsigned long dt_ptr)
 	get_paca()->stab_real = __pa((u64)&initial_stab);
 	get_paca()->stab_addr = (u64)&initial_stab;
 
-	/*
-	 * Iterate all ppc_md structures until we find the proper
-	 * one for the current machine type
-	 */
-	DBG("Probing machine type for platform %x...\n", _machine);
-
-	for (mach = machines; *mach; mach++) {
-		if ((*mach)->probe(_machine))
-			break;
-	}
-	/* What can we do if we didn't find ? */
-	if (*mach == NULL) {
-		DBG("No suitable machine found !\n");
-		for (;;);
-	}
-	ppc_md = **mach;
+	/* Probe the machine type */
+	probe_machine();
 
 #ifdef CONFIG_CRASH_DUMP
 	kdump_setup();
@@ -346,7 +299,7 @@ static void __init initialize_cache_info(void)
 			const char *dc, *ic;
 
 			/* Then read cache informations */
-			if (_machine == PLATFORM_POWERMAC) {
+			if (machine_is(powermac)) {
 				dc = "d-cache-block-size";
 				ic = "i-cache-block-size";
 			} else {
@@ -490,7 +443,6 @@ void __init setup_system(void)
 	printk("ppc64_pft_size                = 0x%lx\n", ppc64_pft_size);
 	printk("ppc64_interrupt_controller    = 0x%ld\n",
 	       ppc64_interrupt_controller);
-	printk("platform                      = 0x%x\n", _machine);
 	printk("physicalMemorySize            = 0x%lx\n", lmb_phys_mem_size());
 	printk("ppc64_caches.dcache_line_size = 0x%x\n",
 	       ppc64_caches.dline_size);

commit e041c683412d5bf44dc2b109053e3b837b71742d
Author: Alan Stern <stern@rowland.harvard.edu>
Date:   Mon Mar 27 01:16:30 2006 -0800

    [PATCH] Notifier chain update: API changes
    
    The kernel's implementation of notifier chains is unsafe.  There is no
    protection against entries being added to or removed from a chain while the
    chain is in use.  The issues were discussed in this thread:
    
        http://marc.theaimsgroup.com/?l=linux-kernel&m=113018709002036&w=2
    
    We noticed that notifier chains in the kernel fall into two basic usage
    classes:
    
            "Blocking" chains are always called from a process context
            and the callout routines are allowed to sleep;
    
            "Atomic" chains can be called from an atomic context and
            the callout routines are not allowed to sleep.
    
    We decided to codify this distinction and make it part of the API.  Therefore
    this set of patches introduces three new, parallel APIs: one for blocking
    notifiers, one for atomic notifiers, and one for "raw" notifiers (which is
    really just the old API under a new name).  New kinds of data structures are
    used for the heads of the chains, and new routines are defined for
    registration, unregistration, and calling a chain.  The three APIs are
    explained in include/linux/notifier.h and their implementation is in
    kernel/sys.c.
    
    With atomic and blocking chains, the implementation guarantees that the chain
    links will not be corrupted and that chain callers will not get messed up by
    entries being added or removed.  For raw chains the implementation provides no
    guarantees at all; users of this API must provide their own protections.  (The
    idea was that situations may come up where the assumptions of the atomic and
    blocking APIs are not appropriate, so it should be possible for users to
    handle these things in their own way.)
    
    There are some limitations, which should not be too hard to live with.  For
    atomic/blocking chains, registration and unregistration must always be done in
    a process context since the chain is protected by a mutex/rwsem.  Also, a
    callout routine for a non-raw chain must not try to register or unregister
    entries on its own chain.  (This did happen in a couple of places and the code
    had to be changed to avoid it.)
    
    Since atomic chains may be called from within an NMI handler, they cannot use
    spinlocks for synchronization.  Instead we use RCU.  The overhead falls almost
    entirely in the unregister routine, which is okay since unregistration is much
    less frequent that calling a chain.
    
    Here is the list of chains that we adjusted and their classifications.  None
    of them use the raw API, so for the moment it is only a placeholder.
    
      ATOMIC CHAINS
      -------------
    arch/i386/kernel/traps.c:               i386die_chain
    arch/ia64/kernel/traps.c:               ia64die_chain
    arch/powerpc/kernel/traps.c:            powerpc_die_chain
    arch/sparc64/kernel/traps.c:            sparc64die_chain
    arch/x86_64/kernel/traps.c:             die_chain
    drivers/char/ipmi/ipmi_si_intf.c:       xaction_notifier_list
    kernel/panic.c:                         panic_notifier_list
    kernel/profile.c:                       task_free_notifier
    net/bluetooth/hci_core.c:               hci_notifier
    net/ipv4/netfilter/ip_conntrack_core.c: ip_conntrack_chain
    net/ipv4/netfilter/ip_conntrack_core.c: ip_conntrack_expect_chain
    net/ipv6/addrconf.c:                    inet6addr_chain
    net/netfilter/nf_conntrack_core.c:      nf_conntrack_chain
    net/netfilter/nf_conntrack_core.c:      nf_conntrack_expect_chain
    net/netlink/af_netlink.c:               netlink_chain
    
      BLOCKING CHAINS
      ---------------
    arch/powerpc/platforms/pseries/reconfig.c:      pSeries_reconfig_chain
    arch/s390/kernel/process.c:             idle_chain
    arch/x86_64/kernel/process.c            idle_notifier
    drivers/base/memory.c:                  memory_chain
    drivers/cpufreq/cpufreq.c               cpufreq_policy_notifier_list
    drivers/cpufreq/cpufreq.c               cpufreq_transition_notifier_list
    drivers/macintosh/adb.c:                adb_client_list
    drivers/macintosh/via-pmu.c             sleep_notifier_list
    drivers/macintosh/via-pmu68k.c          sleep_notifier_list
    drivers/macintosh/windfarm_core.c       wf_client_list
    drivers/usb/core/notify.c               usb_notifier_list
    drivers/video/fbmem.c                   fb_notifier_list
    kernel/cpu.c                            cpu_chain
    kernel/module.c                         module_notify_list
    kernel/profile.c                        munmap_notifier
    kernel/profile.c                        task_exit_notifier
    kernel/sys.c                            reboot_notifier_list
    net/core/dev.c                          netdev_chain
    net/decnet/dn_dev.c:                    dnaddr_chain
    net/ipv4/devinet.c:                     inetaddr_chain
    
    It's possible that some of these classifications are wrong.  If they are,
    please let us know or submit a patch to fix them.  Note that any chain that
    gets called very frequently should be atomic, because the rwsem read-locking
    used for blocking chains is very likely to incur cache misses on SMP systems.
    (However, if the chain's callout routines may sleep then the chain cannot be
    atomic.)
    
    The patch set was written by Alan Stern and Chandra Seetharaman, incorporating
    material written by Keith Owens and suggestions from Paul McKenney and Andrew
    Morton.
    
    [jes@sgi.com: restructure the notifier chain initialization macros]
    Signed-off-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Signed-off-by: Jes Sorensen <jes@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 2f3fdad35594..e20c1fae3423 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -579,7 +579,8 @@ void __init setup_arch(char **cmdline_p)
 	panic_timeout = 180;
 
 	if (ppc_md.panic)
-		notifier_chain_register(&panic_notifier_list, &ppc64_panic_block);
+		atomic_notifier_chain_register(&panic_notifier_list,
+				&ppc64_panic_block);
 
 	init_mm.start_code = PAGE_OFFSET;
 	init_mm.end_code = (unsigned long) _etext;

commit a0652fc9a28c3ef8cd59264bfcb089c44d1b0e06
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Mar 27 15:03:03 2006 +1100

    powerpc: Unify the 32 and 64 bit idle loops
    
    This unifies the 32-bit (ARCH=ppc and ARCH=powerpc) and 64-bit idle
    loops.  It brings over the concept of having a ppc_md.power_save
    function from 32-bit to ARCH=powerpc, which lets us get rid of
    native_idle().  With this we will also be able to simplify the idle
    handling for pSeries and cell.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6c9b093c23a5..5b63a861ef43 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -607,12 +607,6 @@ void __init setup_arch(char **cmdline_p)
 
 	ppc_md.setup_arch();
 
-	/* Use the default idle loop if the platform hasn't provided one. */
-	if (NULL == ppc_md.idle_loop) {
-		ppc_md.idle_loop = default_idle;
-		printk(KERN_INFO "Using default idle loop\n");
-	}
-
 	paging_init();
 	ppc64_boot_msg(0x15, "Setup Done");
 }

commit 4df20460a3ff0d60280738b094945c56cb5567a5
Author: Anton Blanchard <anton@samba.org>
Date:   Sat Mar 25 17:25:17 2006 +1100

    [PATCH] powerpc: Allow non zero boot cpuids
    
    We currently have a hack to flip the boot cpu and its secondary thread
    to logical cpuid 0 and 1. This means the logical - physical mapping will
    differ depending on which cpu is boot cpu. This is most apparent on
    kexec, where we might kexec on any cpu and therefore change the mapping
    from boot to boot.
    
    The patch below does a first pass early on to work out the logical cpuid
    of the boot thread. We then fix up some paca structures to match.
    
    Ive also removed the boot_cpuid_phys variable for ppc64, to be
    consistent we use get_hard_smp_processor_id(boot_cpuid) everywhere.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 2f3fdad35594..6c9b093c23a5 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -73,7 +73,6 @@
 
 int have_of = 1;
 int boot_cpuid = 0;
-int boot_cpuid_phys = 0;
 dev_t boot_dev;
 u64 ppc64_pft_size;
 
@@ -208,7 +207,6 @@ static struct machdep_calls __initdata *machines[] = {
 
 void __init early_setup(unsigned long dt_ptr)
 {
-	struct paca_struct *lpaca = get_paca();
 	static struct machdep_calls **mach;
 
 	/* Enable early debugging if any specified (see udbg.h) */
@@ -223,6 +221,14 @@ void __init early_setup(unsigned long dt_ptr)
 	 */
 	early_init_devtree(__va(dt_ptr));
 
+	/* Now we know the logical id of our boot cpu, setup the paca. */
+	setup_boot_paca();
+
+	/* Fix up paca fields required for the boot cpu */
+	get_paca()->cpu_start = 1;
+	get_paca()->stab_real = __pa((u64)&initial_stab);
+	get_paca()->stab_addr = (u64)&initial_stab;
+
 	/*
 	 * Iterate all ppc_md structures until we find the proper
 	 * one for the current machine type
@@ -260,7 +266,7 @@ void __init early_setup(unsigned long dt_ptr)
 		if (cpu_has_feature(CPU_FTR_SLB))
 			slb_initialize();
 		else
-			stab_initialize(lpaca->stab_real);
+			stab_initialize(get_paca()->stab_real);
 	}
 
 	DBG(" <- early_setup()\n");

commit f8642ebee8e46d054d83828a4048fba4ebcd8f68
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Mar 21 20:46:11 2006 +1100

    [PATCH] powerpc: Remove calculation of io hole
    
    In mm_init_ppc64() we calculate the location of the "IO hole", but then
    no one ever looks at the value. So don't bother.
    
    That's actually all mm_init_ppc64() does, so get rid of it too.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index f96c49b03ba0..2f3fdad35594 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -497,8 +497,6 @@ void __init setup_system(void)
 #endif
 	printk("-----------------------------------------------------\n");
 
-	mm_init_ppc64();
-
 	DBG(" <- setup_system()\n");
 }
 

commit f018b36f3e1f21318066de8d01740d30e38b03d5
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Feb 16 14:13:50 2006 +1100

    [PATCH] powerpc: Don't start secondary CPUs in a UP && KEXEC kernel
    
    Because smp_release_cpus() is built for SMP || KEXEC, it's not safe to
    unconditionally call it from setup_system(). On a UP && KEXEC kernel we'll
    start up the secondary CPUs which will then go beserk and we die.
    
    Simple fix is to conditionally call smp_release_cpus() in setup_system(). With
    that in place we don't need the dummy definition of smp_release_cpus() because
    all call sites are #ifdef'ed either SMP or KEXEC.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index a717dff695ef..f96c49b03ba0 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -311,8 +311,6 @@ void smp_release_cpus(void)
 
 	DBG(" <- smp_release_cpus()\n");
 }
-#else
-#define smp_release_cpus()
 #endif /* CONFIG_SMP || CONFIG_KEXEC */
 
 /*
@@ -473,10 +471,12 @@ void __init setup_system(void)
 	check_smt_enabled();
 	smp_setup_cpu_maps();
 
+#ifdef CONFIG_SMP
 	/* Release secondary cpus out of their spinloops at 0x60 now that
 	 * we can map physical -> logical CPU ids
 	 */
 	smp_release_cpus();
+#endif
 
 	printk("Starting Linux PPC64 %s\n", system_utsname.version);
 

commit b68239ee746760bd99a68692f4c97a28f08a5d01
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Feb 3 19:05:47 2006 +1100

    [PATCH] powerpc: Don't overwrite flat device tree with kdump kernel
    
    It's possible for prom_init to allocate the flat device tree inside the
    kdump crash kernel region. If this happens, when we load the kdump kernel we
    overwrite the flattened device tree, which is bad.
    
    We could make prom_init try and avoid allocating inside the crash kernel
    region, but then we run into issues if the crash kernel region uses all the
    space inside the RMO. The easiest solution is to move the flat device tree
    once we're running in the kernel.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index e29b275e09e0..a717dff695ef 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -398,6 +398,9 @@ void __init setup_system(void)
 {
 	DBG(" -> setup_system()\n");
 
+#ifdef CONFIG_KEXEC
+	kdump_move_device_tree();
+#endif
 	/*
 	 * Unflatten the device-tree passed by prom_init or kexec
 	 */

commit 7a0268fa1a3613f2c526a9b3058701b277f6abe1
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Jan 11 13:16:44 2006 +1100

    [PATCH] powerpc/64: per cpu data optimisations
    
    The current ppc64 per cpu data implementation is quite slow. eg:
    
            lhz 11,18(13)           /* smp_processor_id() */
            ld 9,.LC63-.LCTOC1(30)  /* per_cpu__variable_name */
            ld 8,.LC61-.LCTOC1(30)  /* __per_cpu_offset */
            sldi 11,11,3            /* form index into __per_cpu_offset */
            mr 10,9
            ldx 9,11,8              /* __per_cpu_offset[smp_processor_id()] */
            ldx 0,10,9              /* load per cpu data */
    
    5 loads for something that is supposed to be fast, pretty awful. One
    reason for the large number of loads is that we have to synthesize 2
    64bit constants (per_cpu__variable_name and __per_cpu_offset).
    
    By putting __per_cpu_offset into the paca we can avoid the 2 loads
    associated with it:
    
            ld 11,56(13)            /* paca->data_offset */
            ld 9,.LC59-.LCTOC1(30)  /* per_cpu__variable_name */
            ldx 0,9,11              /* load per cpu data
    
    Longer term we can should be able to do even better than 3 loads.
    If per_cpu__variable_name wasnt a 64bit constant and paca->data_offset
    was in a register we could cut it down to one load. A suggestion from
    Rusty is to use gcc's __thread extension here. In order to do this we
    would need to free up r13 (the __thread register and where the paca
    currently is). So far Ive had a few unsuccessful attempts at doing that :)
    
    The patch also allocates per cpu memory node local on NUMA machines.
    This patch from Rusty has been sitting in my queue _forever_ but stalled
    when I hit the compiler bug. Sorry about that.
    
    Finally I also only allocate per cpu data for possible cpus, which comes
    straight out of the x86-64 port. On a pseries kernel (with NR_CPUS == 128)
    and 4 possible cpus we see some nice gains:
    
                 total       used       free     shared    buffers cached
    Mem:       4012228     212860    3799368          0          0 162424
    
                 total       used       free     shared    buffers cached
    Mem:       4016200     212984    3803216          0          0 162424
    
    A saving of 3.75MB. Quite nice for smaller machines. Note: we now have
    to be careful of per cpu users that touch data for !possible cpus.
    
    At this stage it might be worth making the NUMA and possible cpu
    optimisations generic, but per cpu init is done so early we have to be
    careful that all architectures have their possible map setup correctly.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 0420418f317a..e29b275e09e0 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -33,6 +33,7 @@
 #include <linux/unistd.h>
 #include <linux/serial.h>
 #include <linux/serial_8250.h>
+#include <linux/bootmem.h>
 #include <asm/io.h>
 #include <asm/kdump.h>
 #include <asm/prom.h>
@@ -654,3 +655,28 @@ void cpu_die(void)
 	if (ppc_md.cpu_die)
 		ppc_md.cpu_die();
 }
+
+#ifdef CONFIG_SMP
+void __init setup_per_cpu_areas(void)
+{
+	int i;
+	unsigned long size;
+	char *ptr;
+
+	/* Copy section for each CPU (we discard the original) */
+	size = ALIGN(__per_cpu_end - __per_cpu_start, SMP_CACHE_BYTES);
+#ifdef CONFIG_MODULES
+	if (size < PERCPU_ENOUGH_ROOM)
+		size = PERCPU_ENOUGH_ROOM;
+#endif
+
+	for_each_cpu(i) {
+		ptr = alloc_bootmem_node(NODE_DATA(cpu_to_node(i)), size);
+		if (!ptr)
+			panic("Cannot allocate cpu data for CPU %d\n", i);
+
+		paca[i].data_offset = ptr - __per_cpu_start;
+		memcpy(ptr, __per_cpu_start, __per_cpu_end - __per_cpu_start);
+	}
+}
+#endif

commit 296167ae1799815b9ed2d135a847436502f2ee91
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed Jan 11 11:54:09 2006 +1100

    [PATCH] powerpc: Make early debugging configurable via Kconfig
    
    This patch adds Kconfig entries to control the early debugging options,
    currently in setup_64.c.
    
    Doing this via Kconfig rather than #defines means you can have one source tree,
    which is buildable for multiple platforms - and you can enable the correct
    early debug option for each platform via .config.
    
    I made udbg_early_init() a static inline because otherwise GCC is to daft to
    optimise it away when debugging is off.
    
    Now that we have udbg_init_rtas() we can make call_rtas_display_status* static.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 177d8249078d..0420418f317a 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -70,37 +70,6 @@
 #define DBG(fmt...)
 #endif
 
-/*
- * Here are some early debugging facilities. You can enable one
- * but your kernel will not boot on anything else if you do so
- */
-
-/* For use on LPAR machines that support an HVC console on vterm 0 */
-extern void udbg_init_debug_lpar(void);
-
-/* This one is for use on Apple G5 machines */
-extern void udbg_init_pmac_realmode(void);
-
-/* That's RTAS panel debug */
-extern void call_rtas_display_status_delay(unsigned char c);
-
-/* Here's maple real mode debug */
-extern void udbg_init_maple_realmode(void);
-
-/* For iSeries - hit Ctrl-x Ctrl-x to see the output */
-extern void udbg_init_iseries(void);
-
-#define EARLY_DEBUG_INIT() do {} while(0)
-
-#if 0
-#define EARLY_DEBUG_INIT() udbg_init_debug_lpar()
-#define EARLY_DEBUG_INIT() udbg_init_iseries()
-#define EARLY_DEBUG_INIT() udbg_init_maple_realmode()
-#define EARLY_DEBUG_INIT() udbg_init_pmac_realmode()
-#define EARLY_DEBUG_INIT()						\
-	do { udbg_putc = call_rtas_display_status_delay; } while(0)
-#endif
-
 int have_of = 1;
 int boot_cpuid = 0;
 int boot_cpuid_phys = 0;
@@ -241,11 +210,8 @@ void __init early_setup(unsigned long dt_ptr)
 	struct paca_struct *lpaca = get_paca();
 	static struct machdep_calls **mach;
 
-	/*
-	 * Enable early debugging if any specified (see top of
-	 * this file)
-	 */
-	EARLY_DEBUG_INIT();
+	/* Enable early debugging if any specified (see udbg.h) */
+	udbg_early_init();
 
 	DBG(" -> early_setup()\n");
 

commit bf6a7112bda99aadd6675526423a96be6b356a3d
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed Jan 11 11:54:08 2006 +1100

    [PATCH] powerpc: Early debugging support for iSeries
    
    Connect iSeries up to the standard early debugging infrastructure.
    
    To actually use this you need to enable the iSeries early debugging
    in setup_64.c. Then after the messages are logged hit Ctrl-x Ctrl-x on
    your console to dump the Hypervisor console buffer.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index c4b76961d6de..177d8249078d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -75,22 +75,26 @@
  * but your kernel will not boot on anything else if you do so
  */
 
-/* This one is for use on LPAR machines that support an HVC console
- * on vterm 0
- */
+/* For use on LPAR machines that support an HVC console on vterm 0 */
 extern void udbg_init_debug_lpar(void);
-/* This one is for use on Apple G5 machines
- */
+
+/* This one is for use on Apple G5 machines */
 extern void udbg_init_pmac_realmode(void);
+
 /* That's RTAS panel debug */
 extern void call_rtas_display_status_delay(unsigned char c);
+
 /* Here's maple real mode debug */
 extern void udbg_init_maple_realmode(void);
 
+/* For iSeries - hit Ctrl-x Ctrl-x to see the output */
+extern void udbg_init_iseries(void);
+
 #define EARLY_DEBUG_INIT() do {} while(0)
 
 #if 0
 #define EARLY_DEBUG_INIT() udbg_init_debug_lpar()
+#define EARLY_DEBUG_INIT() udbg_init_iseries()
 #define EARLY_DEBUG_INIT() udbg_init_maple_realmode()
 #define EARLY_DEBUG_INIT() udbg_init_pmac_realmode()
 #define EARLY_DEBUG_INIT()						\

commit 13b8a272297b29870d5bf5f8db7a381dd9e82382
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Jan 10 16:19:05 2006 +1100

    powerpc: Introduce a new config symbol to control 16550 early debug code
    
    The previous change by Kumar Gala in this area led to legacy_serial.c
    and udbg_16550.c being built as modules when CONFIG_SERIAL_8250=m.
    Fix this by introducing a new symbol, CONFIG_PPC_UDBG_16550, to
    control whether these files get built, and arrange for it to be selected
    for those platforms that need it.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 81567e931260..c4b76961d6de 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -472,9 +472,7 @@ void __init setup_system(void)
 	 * hash table management for us, thus ioremap works. We do that early
 	 * so that further code can be debugged
 	 */
-#ifdef CONFIG_SERIAL_8250
 	find_legacy_serial_ports();
-#endif
 
 	/*
 	 * "Finish" the device-tree, that is do the actual parsing of

commit 943ffb587cfdf3b2adfe52a6db08573f4ecf3284
Author: Adrian Bunk <bunk@stusta.de>
Date:   Tue Jan 10 00:10:13 2006 +0100

    spelling: s/retreive/retrieve/
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 98e9f0595dd8..81567e931260 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -438,7 +438,7 @@ void __init setup_system(void)
 
 	/*
 	 * Fill the ppc64_caches & systemcfg structures with informations
-	 * retreived from the device-tree. Need to be called before
+	 * retrieved from the device-tree. Need to be called before
 	 * finish_device_tree() since the later requires some of the
 	 * informations filled up here to properly parse the interrupt
 	 * tree.

commit 79e7bac0d6ad56d62e2364313b5e5e5950c7385d
Author: Kumar Gala <galak@gate.crashing.org>
Date:   Wed Dec 21 09:27:13 2005 -0600

    [PATCH] powerpc: Call find_legacy_serial_ports() if we enable CONFIG_SERIAL_8250
    
    In setup_arch and setup_system call find_legacy_serial_ports() if we
    build in support for 8250 serial ports instead of basing it on PPC_MULTIPLATFORM.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 419e0b974b96..98e9f0595dd8 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -472,7 +472,7 @@ void __init setup_system(void)
 	 * hash table management for us, thus ioremap works. We do that early
 	 * so that further code can be debugged
 	 */
-#ifdef CONFIG_PPC_MULTIPLATFORM
+#ifdef CONFIG_SERIAL_8250
 	find_legacy_serial_ports();
 #endif
 

commit 758438a7b8da593c9116e95cc7fdff6e9e0b0c40
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Mon Dec 5 15:49:00 2005 -0600

    [PATCH] powerpc: Fixups for kernel linked at 32 MB
    
    There's a few places where we need to fix things up for the kernel to work
    if it's linked at 32MB:
    
     - platforms/powermac/smp.c
       To start secondary cpus on pmac we patch the reset vector, which is fine.
       Except if we're above 32MB we don't have enough bits for an absolute branch,
       it needs to relative.
     - kernel/head_64.s
        - A few branches in the cpu hold code need to load the full target address
          and do a bctr.
        - after_prom_start needs to load PHYSICAL_START as the dest address, not 0.
        - The exception prolog needs to load the low word of the target adddress,
          not just the low halfword.
        - Fixup handling of the initial stab address.
     - kernel/setup_64.c
       smp_release_cpus() needs to write 1 to the spinloop flag near 0, not 32 MB.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index e67120e34652..419e0b974b96 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -322,6 +322,7 @@ void early_setup_secondary(void)
 void smp_release_cpus(void)
 {
 	extern unsigned long __secondary_hold_spinloop;
+	unsigned long *ptr;
 
 	DBG(" -> smp_release_cpus()\n");
 
@@ -332,7 +333,9 @@ void smp_release_cpus(void)
 	 * This is useless but harmless on iSeries, secondaries are already
 	 * waiting on their paca spinloops. */
 
-	__secondary_hold_spinloop = 1;
+	ptr  = (unsigned long *)((unsigned long)&__secondary_hold_spinloop
+			- PHYSICAL_START);
+	*ptr = 1;
 	mb();
 
 	DBG(" <- smp_release_cpus()\n");

commit 0cc4746cadda16826a1b3214c042a2f75445b71c
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Sun Dec 4 18:39:37 2005 +1100

    [PATCH] powerpc: Reroute interrupts from 0 + offset to PHYSICAL_START + offset
    
    Regardless of where the kernel's linked we always get interrupts at low
    addresses. This patch creates a trampoline in the first 3 pages of memory,
    where interrupts land, and patches those addresses to jump into the real
    kernel code at PHYSICAL_START.
    
    We also need to reserve the trampoline code and a bit more in prom.c
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6509dd7c2f8f..e67120e34652 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -34,6 +34,7 @@
 #include <linux/serial.h>
 #include <linux/serial_8250.h>
 #include <asm/io.h>
+#include <asm/kdump.h>
 #include <asm/prom.h>
 #include <asm/processor.h>
 #include <asm/pgtable.h>
@@ -268,6 +269,10 @@ void __init early_setup(unsigned long dt_ptr)
 	}
 	ppc_md = **mach;
 
+#ifdef CONFIG_CRASH_DUMP
+	kdump_setup();
+#endif
+
 	DBG("Found, Initializing memory management...\n");
 
 	/*

commit 398ab1fcb960ea0800f40a9c36355855e3e23389
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Sun Dec 4 18:39:23 2005 +1100

    [PATCH] powerpc: Add CONFIG_CRASH_DUMP
    
    This patch adds a Kconfig variable, CONFIG_CRASH_DUMP, which configures the
    built kernel for use as a Kdump kernel.
    
    Currently "all" this involves is changing the value of KERNELBASE to 32 MB.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 65603e9af984..6509dd7c2f8f 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -512,6 +512,9 @@ void __init setup_system(void)
 	       ppc64_caches.iline_size);
 	printk("htab_address                  = 0x%p\n", htab_address);
 	printk("htab_hash_mask                = 0x%lx\n", htab_hash_mask);
+#if PHYSICAL_START > 0
+	printk("physical_start                = 0x%x\n", PHYSICAL_START);
+#endif
 	printk("-----------------------------------------------------\n");
 
 	mm_init_ppc64();

commit 51d3082fe6e55aecfa17113dbe98077c749f724c
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Nov 23 17:57:25 2005 +1100

    [PATCH] powerpc: Unify udbg (#2)
    
    This patch unifies udbg for both ppc32 and ppc64 when building the
    merged achitecture. xmon now has a single "back end". The powermac udbg
    stuff gets enriched with some ADB capabilities and btext output. In
    addition, the early_init callback is now called on ppc32 as well,
    approx. in the same order as ppc64 regarding device-tree manipulations.
    The init sequences of ppc32 and ppc64 are getting closer, I'll unify
    them in a later patch.
    
    For now, you can force udbg to the scc using "sccdbg" or to btext using
    "btextdbg" on powermacs. I'll implement a cleaner way of forcing udbg
    output to something else than the autodetected OF output device in a
    later patch.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 0fc442ad1d26..65603e9af984 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -474,10 +474,6 @@ void __init setup_system(void)
 	 */
 	finish_device_tree();
 
-#ifdef CONFIG_BOOTX_TEXT
-	init_boot_display();
-#endif
-
 	/*
 	 * Initialize xmon
 	 */

commit 463ce0e103f419f51b1769111e73fe8bb305d0ec
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Nov 23 17:56:06 2005 +1100

    [PATCH] powerpc: serial port discovery (#2)
    
    This moves the discovery of legacy serial ports to a separate file,
    makes it common to ppc32 and ppc64, and reworks it to use the new OF
    address translators to get to the ports early. This new version can also
    detect some PCI serial cards using legacy chips and will probably match
    those discovered port with the default console choice.
    
    Only ppc64 gets udbg still yet, unifying udbg isn't finished yet.
    
    It also adds some speed-probing code to udbg so that the default console
    can come up at the same speed it was set to by the firmware.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index e3fb78397dc6..0fc442ad1d26 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -459,6 +459,15 @@ void __init setup_system(void)
 	 */
 	ppc_md.init_early();
 
+ 	/*
+	 * We can discover serial ports now since the above did setup the
+	 * hash table management for us, thus ioremap works. We do that early
+	 * so that further code can be debugged
+	 */
+#ifdef CONFIG_PPC_MULTIPLATFORM
+	find_legacy_serial_ports();
+#endif
+
 	/*
 	 * "Finish" the device-tree, that is do the actual parsing of
 	 * some of the properties like the interrupt map
@@ -657,187 +666,6 @@ void ppc64_terminate_msg(unsigned int src, const char *msg)
 	printk("[terminate]%04x %s\n", src, msg);
 }
 
-#ifndef CONFIG_PPC_ISERIES
-/*
- * This function can be used by platforms to "find" legacy serial ports.
- * It works for "serial" nodes under an "isa" node, and will try to
- * respect the "ibm,aix-loc" property if any. It works with up to 8
- * ports.
- */
-
-#define MAX_LEGACY_SERIAL_PORTS	8
-static struct plat_serial8250_port serial_ports[MAX_LEGACY_SERIAL_PORTS+1];
-static unsigned int old_serial_count;
-
-void __init generic_find_legacy_serial_ports(u64 *physport,
-		unsigned int *default_speed)
-{
-	struct device_node *np;
-	u32 *sizeprop;
-
-	struct isa_reg_property {
-		u32 space;
-		u32 address;
-		u32 size;
-	};
-	struct pci_reg_property {
-		struct pci_address addr;
-		u32 size_hi;
-		u32 size_lo;
-	};                                                                        
-
-	DBG(" -> generic_find_legacy_serial_port()\n");
-
-	*physport = 0;
-	if (default_speed)
-		*default_speed = 0;
-
-	np = of_find_node_by_path("/");
-	if (!np)
-		return;
-
-	/* First fill our array */
-	for (np = NULL; (np = of_find_node_by_type(np, "serial"));) {
-		struct device_node *isa, *pci;
-		struct isa_reg_property *reg;
-		unsigned long phys_size, addr_size, io_base;
-		u32 *rangesp;
-		u32 *interrupts, *clk, *spd;
-		char *typep;
-		int index, rlen, rentsize;
-
-		/* Ok, first check if it's under an "isa" parent */
-		isa = of_get_parent(np);
-		if (!isa || strcmp(isa->name, "isa")) {
-			DBG("%s: no isa parent found\n", np->full_name);
-			continue;
-		}
-		
-		/* Now look for an "ibm,aix-loc" property that gives us ordering
-		 * if any...
-		 */
-	 	typep = (char *)get_property(np, "ibm,aix-loc", NULL);
-
-		/* Get the ISA port number */
-		reg = (struct isa_reg_property *)get_property(np, "reg", NULL);	
-		if (reg == NULL)
-			goto next_port;
-		/* We assume the interrupt number isn't translated ... */
-		interrupts = (u32 *)get_property(np, "interrupts", NULL);
-		/* get clock freq. if present */
-		clk = (u32 *)get_property(np, "clock-frequency", NULL);
-		/* get default speed if present */
-		spd = (u32 *)get_property(np, "current-speed", NULL);
-		/* Default to locate at end of array */
-		index = old_serial_count; /* end of the array by default */
-
-		/* If we have a location index, then use it */
-		if (typep && *typep == 'S') {
-			index = simple_strtol(typep+1, NULL, 0) - 1;
-			/* if index is out of range, use end of array instead */
-			if (index >= MAX_LEGACY_SERIAL_PORTS)
-				index = old_serial_count;
-			/* if our index is still out of range, that mean that
-			 * array is full, we could scan for a free slot but that
-			 * make little sense to bother, just skip the port
-			 */
-			if (index >= MAX_LEGACY_SERIAL_PORTS)
-				goto next_port;
-			if (index >= old_serial_count)
-				old_serial_count = index + 1;
-			/* Check if there is a port who already claimed our slot */
-			if (serial_ports[index].iobase != 0) {
-				/* if we still have some room, move it, else override */
-				if (old_serial_count < MAX_LEGACY_SERIAL_PORTS) {
-					DBG("Moved legacy port %d -> %d\n", index,
-					    old_serial_count);
-					serial_ports[old_serial_count++] =
-						serial_ports[index];
-				} else {
-					DBG("Replacing legacy port %d\n", index);
-				}
-			}
-		}
-		if (index >= MAX_LEGACY_SERIAL_PORTS)
-			goto next_port;
-		if (index >= old_serial_count)
-			old_serial_count = index + 1;
-
-		/* Now fill the entry */
-		memset(&serial_ports[index], 0, sizeof(struct plat_serial8250_port));
-		serial_ports[index].uartclk = clk ? *clk : BASE_BAUD * 16;
-		serial_ports[index].iobase = reg->address;
-		serial_ports[index].irq = interrupts ? interrupts[0] : 0;
-		serial_ports[index].flags = ASYNC_BOOT_AUTOCONF;
-
-		DBG("Added legacy port, index: %d, port: %x, irq: %d, clk: %d\n",
-		    index,
-		    serial_ports[index].iobase,
-		    serial_ports[index].irq,
-		    serial_ports[index].uartclk);
-
-		/* Get phys address of IO reg for port 1 */
-		if (index != 0)
-			goto next_port;
-
-		pci = of_get_parent(isa);
-		if (!pci) {
-			DBG("%s: no pci parent found\n", np->full_name);
-			goto next_port;
-		}
-
-		rangesp = (u32 *)get_property(pci, "ranges", &rlen);
-		if (rangesp == NULL) {
-			of_node_put(pci);
-			goto next_port;
-		}
-		rlen /= 4;
-
-		/* we need the #size-cells of the PCI bridge node itself */
-		phys_size = 1;
-		sizeprop = (u32 *)get_property(pci, "#size-cells", NULL);
-		if (sizeprop != NULL)
-			phys_size = *sizeprop;
-		/* we need the parent #addr-cells */
-		addr_size = prom_n_addr_cells(pci);
-		rentsize = 3 + addr_size + phys_size;
-		io_base = 0;
-		for (;rlen >= rentsize; rlen -= rentsize,rangesp += rentsize) {
-			if (((rangesp[0] >> 24) & 0x3) != 1)
-				continue; /* not IO space */
-			io_base = rangesp[3];
-			if (addr_size == 2)
-				io_base = (io_base << 32) | rangesp[4];
-		}
-		if (io_base != 0) {
-			*physport = io_base + reg->address;
-			if (default_speed && spd)
-				*default_speed = *spd;
-		}
-		of_node_put(pci);
-	next_port:
-		of_node_put(isa);
-	}
-
-	DBG(" <- generic_find_legacy_serial_port()\n");
-}
-
-static struct platform_device serial_device = {
-	.name	= "serial8250",
-	.id	= PLAT8250_DEV_PLATFORM,
-	.dev	= {
-		.platform_data = serial_ports,
-	},
-};
-
-static int __init serial_dev_init(void)
-{
-	return platform_device_register(&serial_device);
-}
-arch_initcall(serial_dev_init);
-
-#endif /* CONFIG_PPC_ISERIES */
-
 int check_legacy_ioport(unsigned long base_port)
 {
 	if (ppc_md.check_legacy_ioport == NULL)

commit dabcafd3f363bacd6b89f537af27dc79128e4806
Author: Olof Johansson <olof@lixom.net>
Date:   Thu Dec 8 19:40:17 2005 -0600

    [PATCH] powerpc: Set cache info defaults
    
    Cache info is setup by walking the device tree in initialize_cache_info().
    However, icache_flush_range might be called before that, in
    slb_initialize()->patch_slb_encoding, which modifies the load immediate
    instructions used with SLB fault code.
    
    Not only that, but depending on memory layout, we might take SLB faults
    during unflatten_device_tree. So that fault will load an SLB entry that
    might not contain the right LLP flags for the segment.
    
    Either we can walk the flattened device tree to setup cache info, or
    we can pick the known defaults that are known to work. Doing it in the
    flattened device tree is hairier since we need to know the machine type
    to know what property to look for, etc, etc.
    
    For now, it's just easier to go with the defaults. Worst thing that
    happens from it is that we might waste a few cycles doing too small
    dcbst/icbi increments.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 608fee7c7e20..e3fb78397dc6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -102,7 +102,15 @@ int boot_cpuid_phys = 0;
 dev_t boot_dev;
 u64 ppc64_pft_size;
 
-struct ppc64_caches ppc64_caches;
+/* Pick defaults since we might want to patch instructions
+ * before we've read this from the device tree.
+ */
+struct ppc64_caches ppc64_caches = {
+	.dline_size = 0x80,
+	.log_dline_size = 7,
+	.iline_size = 0x80,
+	.log_iline_size = 7
+};
 EXPORT_SYMBOL_GPL(ppc64_caches);
 
 /*

commit 593e537b93193d1696809817533ce5ad510445b1
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Sat Nov 12 00:06:06 2005 +1100

    [PATCH] powerpc: Export htab start/end via device tree
    
    The userspace kexec-tools need to know the location of the htab on non-lpar
    machines, as well as the end of the kernel. Export via the device tree.
    
    NB. This patch has been updated to use "linux,x" property names. You may
    need to update your kexec-tools to match.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index fdbd9f9122f2..608fee7c7e20 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -59,6 +59,7 @@
 #include <asm/firmware.h>
 #include <asm/xmon.h>
 #include <asm/udbg.h>
+#include <asm/kexec.h>
 
 #include "setup.h"
 
@@ -415,6 +416,10 @@ void __init setup_system(void)
 	 */
 	unflatten_device_tree();
 
+#ifdef CONFIG_KEXEC
+	kexec_setup();	/* requires unflattened device tree. */
+#endif
+
 	/*
 	 * Fill the ppc64_caches & systemcfg structures with informations
 	 * retreived from the device-tree. Need to be called before

commit a7f290dad32ee34d931561b7943c858fe2aae503
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Nov 11 21:15:21 2005 +1100

    [PATCH] powerpc: Merge vdso's and add vdso support to 32 bits kernel
    
    This patch moves the vdso's to arch/powerpc, adds support for the 32
    bits vdso to the 32 bits kernel, rename systemcfg (finally !), and adds
    some new (still untested) routines to both vdso's: clock_gettime() with
    support for CLOCK_REALTIME and CLOCK_MONOTONIC, clock_getres() (same
    clocks) and get_tbfreq() for glibc to retreive the timebase frequency.
    
    Tom,Steve: The implementation of get_tbfreq() I've done for 32 bits
    returns a long long (r3, r4) not a long. This is such that if we ever
    add support for >4Ghz timebases on ppc32, the userland interface won't
    have to change.
    
    I have tested gettimeofday() using some glibc patches in both ppc32 and
    ppc64 kernels using 32 bits userland (I haven't had a chance to test a
    64 bits userland yet, but the implementation didn't change and was
    tested earlier). I haven't tested yet the new functions.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6791668213e7..fdbd9f9122f2 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -57,7 +57,6 @@
 #include <asm/lmb.h>
 #include <asm/iseries/it_lp_naca.h>
 #include <asm/firmware.h>
-#include <asm/systemcfg.h>
 #include <asm/xmon.h>
 #include <asm/udbg.h>
 
@@ -375,9 +374,8 @@ static void __init initialize_cache_info(void)
 				DBG("Argh, can't find dcache properties ! "
 				    "sizep: %p, lsizep: %p\n", sizep, lsizep);
 
-			_systemcfg->dcache_size = ppc64_caches.dsize = size;
-			_systemcfg->dcache_line_size =
-				ppc64_caches.dline_size = lsize;
+			ppc64_caches.dsize = size;
+			ppc64_caches.dline_size = lsize;
 			ppc64_caches.log_dline_size = __ilog2(lsize);
 			ppc64_caches.dlines_per_page = PAGE_SIZE / lsize;
 
@@ -393,22 +391,13 @@ static void __init initialize_cache_info(void)
 				DBG("Argh, can't find icache properties ! "
 				    "sizep: %p, lsizep: %p\n", sizep, lsizep);
 
-			_systemcfg->icache_size = ppc64_caches.isize = size;
-			_systemcfg->icache_line_size =
-				ppc64_caches.iline_size = lsize;
+			ppc64_caches.isize = size;
+			ppc64_caches.iline_size = lsize;
 			ppc64_caches.log_iline_size = __ilog2(lsize);
 			ppc64_caches.ilines_per_page = PAGE_SIZE / lsize;
 		}
 	}
 
-	/* Add an eye catcher and the systemcfg layout version number */
-	strcpy(_systemcfg->eye_catcher, "SYSTEMCFG:PPC64");
-	_systemcfg->version.major = SYSTEMCFG_MAJOR;
-	_systemcfg->version.minor = SYSTEMCFG_MINOR;
-	_systemcfg->processor = mfspr(SPRN_PVR);
-	_systemcfg->platform = _machine;
-	_systemcfg->physicalMemorySize = lmb_phys_mem_size();
-
 	DBG(" <- initialize_cache_info()\n");
 }
 
@@ -495,15 +484,14 @@ void __init setup_system(void)
 
 	printk("-----------------------------------------------------\n");
 	printk("ppc64_pft_size                = 0x%lx\n", ppc64_pft_size);
-	printk("ppc64_interrupt_controller    = 0x%ld\n", ppc64_interrupt_controller);
-	printk("systemcfg                     = 0x%p\n", _systemcfg);
-	printk("systemcfg->platform           = 0x%x\n", _systemcfg->platform);
-	printk("systemcfg->processorCount     = 0x%lx\n", _systemcfg->processorCount);
-	printk("systemcfg->physicalMemorySize = 0x%lx\n", _systemcfg->physicalMemorySize);
+	printk("ppc64_interrupt_controller    = 0x%ld\n",
+	       ppc64_interrupt_controller);
+	printk("platform                      = 0x%x\n", _machine);
+	printk("physicalMemorySize            = 0x%lx\n", lmb_phys_mem_size());
 	printk("ppc64_caches.dcache_line_size = 0x%x\n",
-			ppc64_caches.dline_size);
+	       ppc64_caches.dline_size);
 	printk("ppc64_caches.icache_line_size = 0x%x\n",
-			ppc64_caches.iline_size);
+	       ppc64_caches.iline_size);
 	printk("htab_address                  = 0x%p\n", htab_address);
 	printk("htab_hash_mask                = 0x%lx\n", htab_hash_mask);
 	printk("-----------------------------------------------------\n");
@@ -567,33 +555,6 @@ static void __init emergency_stack_init(void)
 		__va(lmb_alloc_base(HW_PAGE_SIZE, 128, limit)) + HW_PAGE_SIZE;
 }
 
-/*
- * Called from setup_arch to initialize the bitmap of available
- * syscalls in the systemcfg page
- */
-void __init setup_syscall_map(void)
-{
-	unsigned int i, count64 = 0, count32 = 0;
-	extern unsigned long *sys_call_table;
-	extern unsigned long sys_ni_syscall;
-
-
-	for (i = 0; i < __NR_syscalls; i++) {
-		if (sys_call_table[i*2] != sys_ni_syscall) {
-			count64++;
-			_systemcfg->syscall_map_64[i >> 5] |=
-				0x80000000UL >> (i & 0x1f);
-		}
-		if (sys_call_table[i*2+1] != sys_ni_syscall) {
-			count32++;
-			_systemcfg->syscall_map_32[i >> 5] |=
-				0x80000000UL >> (i & 0x1f);
-		}
-	}
-	printk(KERN_INFO "Syscall map setup, %d 32-bit and %d 64-bit syscalls\n",
-	       count32, count64);
-}
-
 /*
  * Called into from start_kernel, after lock_kernel has been called.
  * Initializes bootmem, which is unsed to manage page allocation until
@@ -635,9 +596,6 @@ void __init setup_arch(char **cmdline_p)
 	do_init_bootmem();
 	sparse_init();
 
-	/* initialize the syscall map in systemcfg */
-	setup_syscall_map();
-
 #ifdef CONFIG_DUMMY_CONSOLE
 	conswitchp = &dummy_con;
 #endif

commit 49b09853df1a303876b82a6480efb2f7b45ef041
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Nov 10 15:53:40 2005 +1100

    powerpc: Move some extern declarations from C code into headers
    
    This also make klimit have the same type on 32-bit as on 64-bit,
    namely unsigned long, and defines and initializes it in one place.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index be607b877a55..6791668213e7 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -96,14 +96,6 @@ extern void udbg_init_maple_realmode(void);
 	do { udbg_putc = call_rtas_display_status_delay; } while(0)
 #endif
 
-/* extern void *stab; */
-extern unsigned long klimit;
-
-extern void mm_init_ppc64(void);
-extern void early_init_devtree(void *flat_dt);
-extern void unflatten_device_tree(void);
-extern void check_for_initrd(void);
-
 int have_of = 1;
 int boot_cpuid = 0;
 int boot_cpuid_phys = 0;

commit 799d6046d3fb557006e6d7c9767fdb96479b0e0a
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Nov 10 13:37:51 2005 +1100

    [PATCH] powerpc: merge code values for identifying platforms
    
    This patch merges platform codes.  systemcfg->platform is no longer used,
    systemcfg use in general is deprecated as much as possible (and renamed
    _systemcfg before it gets completely moved elsewhere in a future patch),
    _machine is now used on ppc64 along as ppc32.  Platform codes aren't gone
    yet but we are getting a step closer. A bunch of asm code in head[_64].S
    is also turned into C code.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index fe39aac4f24d..be607b877a55 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -100,10 +100,9 @@ extern void udbg_init_maple_realmode(void);
 extern unsigned long klimit;
 
 extern void mm_init_ppc64(void);
-extern void stab_initialize(unsigned long stab);
-extern void htab_initialize(void);
 extern void early_init_devtree(void *flat_dt);
 extern void unflatten_device_tree(void);
+extern void check_for_initrd(void);
 
 int have_of = 1;
 int boot_cpuid = 0;
@@ -256,11 +255,10 @@ void __init early_setup(unsigned long dt_ptr)
 	 * Iterate all ppc_md structures until we find the proper
 	 * one for the current machine type
 	 */
-	DBG("Probing machine type for platform %x...\n",
-	    systemcfg->platform);
+	DBG("Probing machine type for platform %x...\n", _machine);
 
 	for (mach = machines; *mach; mach++) {
-		if ((*mach)->probe(systemcfg->platform))
+		if ((*mach)->probe(_machine))
 			break;
 	}
 	/* What can we do if we didn't find ? */
@@ -292,6 +290,28 @@ void __init early_setup(unsigned long dt_ptr)
 	DBG(" <- early_setup()\n");
 }
 
+#ifdef CONFIG_SMP
+void early_setup_secondary(void)
+{
+	struct paca_struct *lpaca = get_paca();
+
+	/* Mark enabled in PACA */
+	lpaca->proc_enabled = 0;
+
+	/* Initialize hash table for that CPU */
+	htab_initialize_secondary();
+
+	/* Initialize STAB/SLB. We use a virtual address as it works
+	 * in real mode on pSeries and we want a virutal address on
+	 * iSeries anyway
+	 */
+	if (cpu_has_feature(CPU_FTR_SLB))
+		slb_initialize();
+	else
+		stab_initialize(lpaca->stab_addr);
+}
+
+#endif /* CONFIG_SMP */
 
 #if defined(CONFIG_SMP) || defined(CONFIG_KEXEC)
 void smp_release_cpus(void)
@@ -317,7 +337,8 @@ void smp_release_cpus(void)
 #endif /* CONFIG_SMP || CONFIG_KEXEC */
 
 /*
- * Initialize some remaining members of the ppc64_caches and systemcfg structures
+ * Initialize some remaining members of the ppc64_caches and systemcfg
+ * structures
  * (at least until we get rid of them completely). This is mostly some
  * cache informations about the CPU that will be used by cache flush
  * routines and/or provided to userland
@@ -342,7 +363,7 @@ static void __init initialize_cache_info(void)
 			const char *dc, *ic;
 
 			/* Then read cache informations */
-			if (systemcfg->platform == PLATFORM_POWERMAC) {
+			if (_machine == PLATFORM_POWERMAC) {
 				dc = "d-cache-block-size";
 				ic = "i-cache-block-size";
 			} else {
@@ -362,8 +383,8 @@ static void __init initialize_cache_info(void)
 				DBG("Argh, can't find dcache properties ! "
 				    "sizep: %p, lsizep: %p\n", sizep, lsizep);
 
-			systemcfg->dcache_size = ppc64_caches.dsize = size;
-			systemcfg->dcache_line_size =
+			_systemcfg->dcache_size = ppc64_caches.dsize = size;
+			_systemcfg->dcache_line_size =
 				ppc64_caches.dline_size = lsize;
 			ppc64_caches.log_dline_size = __ilog2(lsize);
 			ppc64_caches.dlines_per_page = PAGE_SIZE / lsize;
@@ -380,8 +401,8 @@ static void __init initialize_cache_info(void)
 				DBG("Argh, can't find icache properties ! "
 				    "sizep: %p, lsizep: %p\n", sizep, lsizep);
 
-			systemcfg->icache_size = ppc64_caches.isize = size;
-			systemcfg->icache_line_size =
+			_systemcfg->icache_size = ppc64_caches.isize = size;
+			_systemcfg->icache_line_size =
 				ppc64_caches.iline_size = lsize;
 			ppc64_caches.log_iline_size = __ilog2(lsize);
 			ppc64_caches.ilines_per_page = PAGE_SIZE / lsize;
@@ -389,10 +410,12 @@ static void __init initialize_cache_info(void)
 	}
 
 	/* Add an eye catcher and the systemcfg layout version number */
-	strcpy(systemcfg->eye_catcher, "SYSTEMCFG:PPC64");
-	systemcfg->version.major = SYSTEMCFG_MAJOR;
-	systemcfg->version.minor = SYSTEMCFG_MINOR;
-	systemcfg->processor = mfspr(SPRN_PVR);
+	strcpy(_systemcfg->eye_catcher, "SYSTEMCFG:PPC64");
+	_systemcfg->version.major = SYSTEMCFG_MAJOR;
+	_systemcfg->version.minor = SYSTEMCFG_MINOR;
+	_systemcfg->processor = mfspr(SPRN_PVR);
+	_systemcfg->platform = _machine;
+	_systemcfg->physicalMemorySize = lmb_phys_mem_size();
 
 	DBG(" <- initialize_cache_info()\n");
 }
@@ -481,10 +504,10 @@ void __init setup_system(void)
 	printk("-----------------------------------------------------\n");
 	printk("ppc64_pft_size                = 0x%lx\n", ppc64_pft_size);
 	printk("ppc64_interrupt_controller    = 0x%ld\n", ppc64_interrupt_controller);
-	printk("systemcfg                     = 0x%p\n", systemcfg);
-	printk("systemcfg->platform           = 0x%x\n", systemcfg->platform);
-	printk("systemcfg->processorCount     = 0x%lx\n", systemcfg->processorCount);
-	printk("systemcfg->physicalMemorySize = 0x%lx\n", systemcfg->physicalMemorySize);
+	printk("systemcfg                     = 0x%p\n", _systemcfg);
+	printk("systemcfg->platform           = 0x%x\n", _systemcfg->platform);
+	printk("systemcfg->processorCount     = 0x%lx\n", _systemcfg->processorCount);
+	printk("systemcfg->physicalMemorySize = 0x%lx\n", _systemcfg->physicalMemorySize);
 	printk("ppc64_caches.dcache_line_size = 0x%x\n",
 			ppc64_caches.dline_size);
 	printk("ppc64_caches.icache_line_size = 0x%x\n",
@@ -566,12 +589,12 @@ void __init setup_syscall_map(void)
 	for (i = 0; i < __NR_syscalls; i++) {
 		if (sys_call_table[i*2] != sys_ni_syscall) {
 			count64++;
-			systemcfg->syscall_map_64[i >> 5] |=
+			_systemcfg->syscall_map_64[i >> 5] |=
 				0x80000000UL >> (i & 0x1f);
 		}
 		if (sys_call_table[i*2+1] != sys_ni_syscall) {
 			count32++;
-			systemcfg->syscall_map_32[i >> 5] |=
+			_systemcfg->syscall_map_32[i >> 5] |=
 				0x80000000UL >> (i & 0x1f);
 		}
 	}

commit 66ba135c5a398df5c3a4b43d84d9df80cbc87c61
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Wed Nov 9 11:01:06 2005 +1100

    powerpc: create kernel/setup.h
    
    for functions defined by setup-common.c and used in setup_xx.c
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 54c606f680b3..fe39aac4f24d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -61,6 +61,8 @@
 #include <asm/xmon.h>
 #include <asm/udbg.h>
 
+#include "setup.h"
+
 #ifdef DEBUG
 #define DBG(fmt...) udbg_printf(fmt)
 #else

commit fca5dcd4835ed09bb1a48a355344aff7a25c76e0
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Nov 8 22:55:08 2005 +1100

    powerpc: Simplify and clean up the xmon terminal I/O
    
    This factors out the common bits of arch/powerpc/xmon/start_*.c into
    a new nonstdio.c, and removes some stuff that was supposed to make
    xmon's I/O routines somewhat stdio-like but was never used.
    
    It also makes the parsing of the xmon= command line option common,
    so that ppc32 can now use xmon={off,on,early} also.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 0471e843b6c5..54c606f680b3 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -858,26 +858,6 @@ int check_legacy_ioport(unsigned long base_port)
 }
 EXPORT_SYMBOL(check_legacy_ioport);
 
-#ifdef CONFIG_XMON
-static int __init early_xmon(char *p)
-{
-	/* ensure xmon is enabled */
-	if (p) {
-		if (strncmp(p, "on", 2) == 0)
-			xmon_init(1);
-		if (strncmp(p, "off", 3) == 0)
-			xmon_init(0);
-		if (strncmp(p, "early", 5) != 0)
-			return 0;
-	}
-	xmon_init(1);
-	debugger(NULL);
-
-	return 0;
-}
-early_param("xmon", early_xmon);
-#endif
-
 void cpu_die(void)
 {
 	if (ppc_md.cpu_die)

commit a82765b6eee3d1267ded3320ca67b39fe1844599
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Nov 2 22:34:20 2005 +0000

    [PATCH] powerpc: Fix ppc32 initrd
    
    OK, the Fedora ppc32 and ppc64 kernels should both be arch/powerpc by
    tomorrow. They're booting on G5, POWER5, and my powerbook. I'll test
    pmac SMP and Pegasos later -- but pmac smp is known broken in arch/ppc
    anyway, and I'll live with a potential Pegasos regression for now; it
    wasn't supported officially in FC4 either.
    
    I needed to fix ppc32 initrd -- we were never setting initrd_start.
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 785fd9d7b386..0471e843b6c5 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -395,43 +395,6 @@ static void __init initialize_cache_info(void)
 	DBG(" <- initialize_cache_info()\n");
 }
 
-static void __init check_for_initrd(void)
-{
-#ifdef CONFIG_BLK_DEV_INITRD
-	u64 *prop;
-
-	DBG(" -> check_for_initrd()\n");
-
-	if (of_chosen) {
-		prop = (u64 *)get_property(of_chosen,
-				"linux,initrd-start", NULL);
-		if (prop != NULL) {
-			initrd_start = (unsigned long)__va(*prop);
-			prop = (u64 *)get_property(of_chosen,
-					"linux,initrd-end", NULL);
-			if (prop != NULL) {
-				initrd_end = (unsigned long)__va(*prop);
-				initrd_below_start_ok = 1;
-			} else
-				initrd_start = 0;
-		}
-	}
-
-	/* If we were passed an initrd, set the ROOT_DEV properly if the values
-	 * look sensible. If not, clear initrd reference.
-	 */
-	if (initrd_start >= KERNELBASE && initrd_end >= KERNELBASE &&
-	    initrd_end > initrd_start)
-		ROOT_DEV = Root_RAM0;
-	else
-		initrd_start = initrd_end = 0;
-
-	if (initrd_start)
-		printk("Found initrd at 0x%lx:0x%lx\n", initrd_start, initrd_end);
-
-	DBG(" <- check_for_initrd()\n");
-#endif /* CONFIG_BLK_DEV_INITRD */
-}
 
 /*
  * Do some initial setup of the system.  The parameters are those which 

commit c6135234550ed89a6fd0e8cb229633967e41d649
Merge: 76032de898f3 0b154bb7d0cc
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Nov 7 14:42:09 2005 +1100

    Merge ../linux-2.6

commit dcad47fc423ac9f4934579af814fa2dad5c8081b
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Mon Nov 7 09:49:43 2005 +1100

    [PATCH] powerpc: Kill ppcdebug
    
    The ancient ppcdebug/PPCDBG mechanism is now only used in two places.
    First, in the hash setup code, one of the bits allows the size of the
    hash table to be reduced by a factor of 8 - which would be better
    accomplished with a command line option for that purpose.  The other
    was a bunch of bus walking related messages in the iSeries code, which
    would seem to be insufficient reason to keep the mechanism.
    
    This patch removes the last traces of this mechanism.
    
    Built and booted on iSeries and pSeries POWER5 LPAR (ARCH=powerpc).
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6b52cce872be..5f8154f95f96 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -41,7 +41,6 @@
 #include <asm/elf.h>
 #include <asm/machdep.h>
 #include <asm/paca.h>
-#include <asm/ppcdebug.h>
 #include <asm/time.h>
 #include <asm/cputable.h>
 #include <asm/sections.h>
@@ -60,6 +59,7 @@
 #include <asm/firmware.h>
 #include <asm/systemcfg.h>
 #include <asm/xmon.h>
+#include <asm/udbg.h>
 
 #ifdef DEBUG
 #define DBG(fmt...) udbg_printf(fmt)
@@ -243,12 +243,6 @@ void __init early_setup(unsigned long dt_ptr)
 
 	DBG(" -> early_setup()\n");
 
-	/*
-	 * Fill the default DBG level (do we want to keep
-	 * that old mecanism around forever ?)
-	 */
-	ppcdbg_initialize();
-
 	/*
 	 * Do early initializations using the flattened device
 	 * tree, like retreiving the physical memory map or
@@ -516,7 +510,6 @@ void __init setup_system(void)
 
 	printk("-----------------------------------------------------\n");
 	printk("ppc64_pft_size                = 0x%lx\n", ppc64_pft_size);
-	printk("ppc64_debug_switch            = 0x%lx\n", ppc64_debug_switch);
 	printk("ppc64_interrupt_controller    = 0x%ld\n", ppc64_interrupt_controller);
 	printk("systemcfg                     = 0x%p\n", systemcfg);
 	printk("systemcfg->platform           = 0x%x\n", systemcfg->platform);

commit 3c726f8dee6f55e96475574e9f645327e461884c
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Nov 7 11:06:55 2005 +1100

    [PATCH] ppc64: support 64k pages
    
    Adds a new CONFIG_PPC_64K_PAGES which, when enabled, changes the kernel
    base page size to 64K.  The resulting kernel still boots on any
    hardware.  On current machines with 4K pages support only, the kernel
    will maintain 16 "subpages" for each 64K page transparently.
    
    Note that while real 64K capable HW has been tested, the current patch
    will not enable it yet as such hardware is not released yet, and I'm
    still verifying with the firmware architects the proper to get the
    information from the newer hypervisors.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6b52cce872be..b0994050024f 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -277,16 +277,21 @@ void __init early_setup(unsigned long dt_ptr)
 	DBG("Found, Initializing memory management...\n");
 
 	/*
-	 * Initialize stab / SLB management
+	 * Initialize the MMU Hash table and create the linear mapping
+	 * of memory. Has to be done before stab/slb initialization as
+	 * this is currently where the page size encoding is obtained
 	 */
-	if (!firmware_has_feature(FW_FEATURE_ISERIES))
-		stab_initialize(lpaca->stab_real);
+	htab_initialize();
 
 	/*
-	 * Initialize the MMU Hash table and create the linear mapping
-	 * of memory
+	 * Initialize stab / SLB management except on iSeries
 	 */
-	htab_initialize();
+	if (!firmware_has_feature(FW_FEATURE_ISERIES)) {
+		if (cpu_has_feature(CPU_FTR_SLB))
+			slb_initialize();
+		else
+			stab_initialize(lpaca->stab_real);
+	}
 
 	DBG(" <- early_setup()\n");
 }
@@ -552,10 +557,12 @@ static void __init irqstack_early_init(void)
 	 * SLB misses on them.
 	 */
 	for_each_cpu(i) {
-		softirq_ctx[i] = (struct thread_info *)__va(lmb_alloc_base(THREAD_SIZE,
-					THREAD_SIZE, 0x10000000));
-		hardirq_ctx[i] = (struct thread_info *)__va(lmb_alloc_base(THREAD_SIZE,
-					THREAD_SIZE, 0x10000000));
+		softirq_ctx[i] = (struct thread_info *)
+			__va(lmb_alloc_base(THREAD_SIZE,
+					    THREAD_SIZE, 0x10000000));
+		hardirq_ctx[i] = (struct thread_info *)
+			__va(lmb_alloc_base(THREAD_SIZE,
+					    THREAD_SIZE, 0x10000000));
 	}
 }
 #else
@@ -583,8 +590,8 @@ static void __init emergency_stack_init(void)
 	limit = min(0x10000000UL, lmb.rmo_size);
 
 	for_each_cpu(i)
-		paca[i].emergency_sp = __va(lmb_alloc_base(PAGE_SIZE, 128,
-						limit)) + PAGE_SIZE;
+		paca[i].emergency_sp =
+		__va(lmb_alloc_base(HW_PAGE_SIZE, 128, limit)) + HW_PAGE_SIZE;
 }
 
 /*

commit 5ad570786158e327a1c5d32dd3d66f26d8de6340
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Nov 5 10:33:55 2005 +1100

    powerpc: Merge smp.c and smp.h
    
    This also moves setup_cpu_maps to setup-common.c (calling it
    smp_setup_cpu_maps) and uses it on both 32-bit and 64-bit.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 275d86ddd612..6b52cce872be 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -181,114 +181,8 @@ static int __init early_smt_enabled(char *p)
 }
 early_param("smt-enabled", early_smt_enabled);
 
-/**
- * setup_cpu_maps - initialize the following cpu maps:
- *                  cpu_possible_map
- *                  cpu_present_map
- *                  cpu_sibling_map
- *
- * Having the possible map set up early allows us to restrict allocations
- * of things like irqstacks to num_possible_cpus() rather than NR_CPUS.
- *
- * We do not initialize the online map here; cpus set their own bits in
- * cpu_online_map as they come up.
- *
- * This function is valid only for Open Firmware systems.  finish_device_tree
- * must be called before using this.
- *
- * While we're here, we may as well set the "physical" cpu ids in the paca.
- */
-static void __init setup_cpu_maps(void)
-{
-	struct device_node *dn = NULL;
-	int cpu = 0;
-	int swap_cpuid = 0;
-
-	check_smt_enabled();
-
-	while ((dn = of_find_node_by_type(dn, "cpu")) && cpu < NR_CPUS) {
-		u32 *intserv;
-		int j, len = sizeof(u32), nthreads;
-
-		intserv = (u32 *)get_property(dn, "ibm,ppc-interrupt-server#s",
-					      &len);
-		if (!intserv)
-			intserv = (u32 *)get_property(dn, "reg", NULL);
-
-		nthreads = len / sizeof(u32);
-
-		for (j = 0; j < nthreads && cpu < NR_CPUS; j++) {
-			cpu_set(cpu, cpu_present_map);
-			set_hard_smp_processor_id(cpu, intserv[j]);
-
-			if (intserv[j] == boot_cpuid_phys)
-				swap_cpuid = cpu;
-			cpu_set(cpu, cpu_possible_map);
-			cpu++;
-		}
-	}
-
-	/* Swap CPU id 0 with boot_cpuid_phys, so we can always assume that
-	 * boot cpu is logical 0.
-	 */
-	if (boot_cpuid_phys != get_hard_smp_processor_id(0)) {
-		u32 tmp;
-		tmp = get_hard_smp_processor_id(0);
-		set_hard_smp_processor_id(0, boot_cpuid_phys);
-		set_hard_smp_processor_id(swap_cpuid, tmp);
-	}
-
-	/*
-	 * On pSeries LPAR, we need to know how many cpus
-	 * could possibly be added to this partition.
-	 */
-	if (systemcfg->platform == PLATFORM_PSERIES_LPAR &&
-				(dn = of_find_node_by_path("/rtas"))) {
-		int num_addr_cell, num_size_cell, maxcpus;
-		unsigned int *ireg;
-
-		num_addr_cell = prom_n_addr_cells(dn);
-		num_size_cell = prom_n_size_cells(dn);
-
-		ireg = (unsigned int *)
-			get_property(dn, "ibm,lrdr-capacity", NULL);
-
-		if (!ireg)
-			goto out;
-
-		maxcpus = ireg[num_addr_cell + num_size_cell];
-
-		/* Double maxcpus for processors which have SMT capability */
-		if (cpu_has_feature(CPU_FTR_SMT))
-			maxcpus *= 2;
-
-		if (maxcpus > NR_CPUS) {
-			printk(KERN_WARNING
-			       "Partition configured for %d cpus, "
-			       "operating system maximum is %d.\n",
-			       maxcpus, NR_CPUS);
-			maxcpus = NR_CPUS;
-		} else
-			printk(KERN_INFO "Partition configured for %d cpus.\n",
-			       maxcpus);
-
-		for (cpu = 0; cpu < maxcpus; cpu++)
-			cpu_set(cpu, cpu_possible_map);
-	out:
-		of_node_put(dn);
-	}
-
-	/*
-	 * Do the sibling map; assume only two threads per processor.
-	 */
-	for_each_cpu(cpu) {
-		cpu_set(cpu, cpu_sibling_map[cpu]);
-		if (cpu_has_feature(CPU_FTR_SMT))
-			cpu_set(cpu ^ 0x1, cpu_sibling_map[cpu]);
-	}
-
-	systemcfg->processorCount = num_present_cpus();
-}
+#else
+#define check_smt_enabled()
 #endif /* CONFIG_SMP */
 
 extern struct machdep_calls pSeries_md;
@@ -417,6 +311,8 @@ void smp_release_cpus(void)
 
 	DBG(" <- smp_release_cpus()\n");
 }
+#else
+#define smp_release_cpus()
 #endif /* CONFIG_SMP || CONFIG_KEXEC */
 
 /*
@@ -608,17 +504,13 @@ void __init setup_system(void)
 
 	parse_early_param();
 
-#ifdef CONFIG_SMP
-	/*
-	 * iSeries has already initialized the cpu maps at this point.
-	 */
-	setup_cpu_maps();
+	check_smt_enabled();
+	smp_setup_cpu_maps();
 
 	/* Release secondary cpus out of their spinloops at 0x60 now that
 	 * we can map physical -> logical CPU ids
 	 */
 	smp_release_cpus();
-#endif
 
 	printk("Starting Linux PPC64 %s\n", system_utsname.version);
 

commit b8f510219edc719d4c305918e16edc578bcfc16f
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Nov 4 12:09:42 2005 +1100

    powerpc: Implement smp_release_cpus() in C not asm
    
    There's no reason for smp_release_cpus() to be asm, and most people can make
    more sense of C code. Add an extern declaration to smp.h and remove the custom
    one in machine_kexec.c
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index d4a3c5dd1a21..275d86ddd612 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -103,8 +103,6 @@ extern void htab_initialize(void);
 extern void early_init_devtree(void *flat_dt);
 extern void unflatten_device_tree(void);
 
-extern void smp_release_cpus(void);
-
 int have_of = 1;
 int boot_cpuid = 0;
 int boot_cpuid_phys = 0;
@@ -400,6 +398,27 @@ void __init early_setup(unsigned long dt_ptr)
 }
 
 
+#if defined(CONFIG_SMP) || defined(CONFIG_KEXEC)
+void smp_release_cpus(void)
+{
+	extern unsigned long __secondary_hold_spinloop;
+
+	DBG(" -> smp_release_cpus()\n");
+
+	/* All secondary cpus are spinning on a common spinloop, release them
+	 * all now so they can start to spin on their individual paca
+	 * spinloops. For non SMP kernels, the secondary cpus never get out
+	 * of the common spinloop.
+	 * This is useless but harmless on iSeries, secondaries are already
+	 * waiting on their paca spinloops. */
+
+	__secondary_hold_spinloop = 1;
+	mb();
+
+	DBG(" <- smp_release_cpus()\n");
+}
+#endif /* CONFIG_SMP || CONFIG_KEXEC */
+
 /*
  * Initialize some remaining members of the ppc64_caches and systemcfg structures
  * (at least until we get rid of them completely). This is mostly some

commit aaf8a7a2949481482200686c7bd3852e5be2e093
Merge: 104dd65fef37 ecb3ca2783d6
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Wed Nov 2 16:06:03 2005 +1100

    Merge iSeries include file move

commit f218aab5cf74672a368933965f5bb612dac3c349
Author: Kelly Daly <kelly@au.ibm.com>
Date:   Wed Nov 2 13:51:41 2005 +1100

    merge filename and modify references to iseries/it_lp_naca.h
    
    Signed-off-by: Kelly Daly <kelly@au.ibm.com>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 40c48100bf1b..079867e18145 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -56,7 +56,7 @@
 #include <asm/page.h>
 #include <asm/mmu.h>
 #include <asm/lmb.h>
-#include <asm/iSeries/ItLpNaca.h>
+#include <asm/iseries/it_lp_naca.h>
 #include <asm/firmware.h>
 #include <asm/systemcfg.h>
 #include <asm/xmon.h>

commit f3f66f599db131ea57dc567ffd931d269dbc690e
Author: Arnd Bergmann <arndb@de.ibm.com>
Date:   Mon Oct 31 20:08:37 2005 -0500

    [PATCH] powerpc: Rename BPA to Cell
    
    The official name for BPA is now CBEA (Cell Broadband
    Engine Architecture). This patch renames all occurences
    of the term BPA to 'Cell' for easier recognition.
    
    Signed-off-by: Arnd Bergmann <arndb@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 19530ce9cd27..70ead7d0d12e 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -296,7 +296,7 @@ static void __init setup_cpu_maps(void)
 extern struct machdep_calls pSeries_md;
 extern struct machdep_calls pmac_md;
 extern struct machdep_calls maple_md;
-extern struct machdep_calls bpa_md;
+extern struct machdep_calls cell_md;
 extern struct machdep_calls iseries_md;
 
 /* Ultimately, stuff them in an elf section like initcalls... */
@@ -310,8 +310,8 @@ static struct machdep_calls __initdata *machines[] = {
 #ifdef CONFIG_PPC_MAPLE
 	&maple_md,
 #endif /* CONFIG_PPC_MAPLE */
-#ifdef CONFIG_PPC_BPA
-	&bpa_md,
+#ifdef CONFIG_PPC_CELL
+	&cell_md,
 #endif
 #ifdef CONFIG_PPC_ISERIES
 	&iseries_md,

commit bec7c458b372251617e0fdc6bf8ce4df06bab430
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Tue Nov 1 11:45:19 2005 +1100

    powerpc: make mem= work on iSeries again
    
    By parsing the command line earlier, we can add the mem= value to the
    flattened device tree and let the generic code sort out the memory limit
    for us.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 40c48100bf1b..19530ce9cd27 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -631,23 +631,6 @@ static int ppc64_panic_event(struct notifier_block *this,
 	return NOTIFY_DONE;
 }
 
-#ifdef CONFIG_PPC_ISERIES
-/*
- * On iSeries we just parse the mem=X option from the command line.
- * On pSeries it's a bit more complicated, see prom_init_mem()
- */
-static int __init early_parsemem(char *p)
-{
-	if (!p)
-		return 0;
-
-	memory_limit = ALIGN(memparse(p, &p), PAGE_SIZE);
-
-	return 0;
-}
-early_param("mem", early_parsemem);
-#endif /* CONFIG_PPC_ISERIES */
-
 #ifdef CONFIG_IRQSTACKS
 static void __init irqstack_early_init(void)
 {

commit cf00a8d18b9a1c2d55b2728e89125c234e821db5
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 31 13:07:02 2005 +1100

    powerpc: Fix bug arising from having multiple memory_limit variables
    
    We had a static memory_limit in prom.c, and then another one defined
    in setup_64.c and used in numa.c, which resulted in the kernel crashing
    when mem=xxx was given on the command line.  This puts the declaration
    in system.h and the definition in mem.c.  This also moves the
    definition of tce_alloc_start/end out of setup_64.c.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 681537f8ea10..40c48100bf1b 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -631,15 +631,6 @@ static int ppc64_panic_event(struct notifier_block *this,
 	return NOTIFY_DONE;
 }
 
-/*
- * These three variables are used to save values passed to us by prom_init()
- * via the device tree. The TCE variables are needed because with a memory_limit
- * in force we may need to explicitly map the TCE are at the top of RAM.
- */
-unsigned long memory_limit;
-unsigned long tce_alloc_start;
-unsigned long tce_alloc_end;
-
 #ifdef CONFIG_PPC_ISERIES
 /*
  * On iSeries we just parse the mem=X option from the command line.

commit f78541dcec327b0c46b150ee7d727f3db80275c4
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Oct 28 22:53:37 2005 +1000

    powerpc: Merge xmon
    
    The merged version follows the ppc64 version pretty closely mostly,
    and in fact ARCH=ppc64 now uses the arch/powerpc/xmon version.
    The main difference for ppc64 is that the 'p' command to call
    show_state (which was always pretty dodgy) has been replaced by
    the ppc32 'p' command, which calls a given procedure (so in fact
    the old 'p' command behaviour can be achieved with 'p $show_state').
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 950e6f0fea98..681537f8ea10 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -59,6 +59,7 @@
 #include <asm/iSeries/ItLpNaca.h>
 #include <asm/firmware.h>
 #include <asm/systemcfg.h>
+#include <asm/xmon.h>
 
 #ifdef DEBUG
 #define DBG(fmt...) udbg_printf(fmt)

commit 640768eef245f1578e75e02c17d277a1496a535b
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Oct 28 12:51:45 2005 +1000

    ppc64: use the merged syscall table
    
    This allows us to also use entry_64.S from the merged tree and reverts
    the setup_64.c part of fda262b8978d0089758ef9444508434c74113a61.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index a8f7ff5ab1a4..950e6f0fea98 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -701,17 +701,6 @@ static void __init emergency_stack_init(void)
 						limit)) + PAGE_SIZE;
 }
 
-extern unsigned long *sys_call_table;
-extern unsigned long sys_ni_syscall;
-#ifdef CONFIG_PPC_MERGE
-#define SYS_CALL_ENTRY64(i)	sys_call_table[(i) * 2]
-#define SYS_CALL_ENTRY32(i)	sys_call_table[(i) * 2 + 1]
-#else
-extern unsigned long *sys_call_table32;
-#define SYS_CALL_ENTRY64(i)	sys_call_table[(i)]
-#define SYS_CALL_ENTRY32(i)	sys_call_table32[(i)]
-#endif
-
 /*
  * Called from setup_arch to initialize the bitmap of available
  * syscalls in the systemcfg page
@@ -719,14 +708,17 @@ extern unsigned long *sys_call_table32;
 void __init setup_syscall_map(void)
 {
 	unsigned int i, count64 = 0, count32 = 0;
+	extern unsigned long *sys_call_table;
+	extern unsigned long sys_ni_syscall;
+
 
 	for (i = 0; i < __NR_syscalls; i++) {
-		if (SYS_CALL_ENTRY64(i) != sys_ni_syscall) {
+		if (sys_call_table[i*2] != sys_ni_syscall) {
 			count64++;
 			systemcfg->syscall_map_64[i >> 5] |=
 				0x80000000UL >> (i & 0x1f);
 		}
-		if (SYS_CALL_ENTRY32(i) != sys_ni_syscall) {
+		if (sys_call_table[i*2+1] != sys_ni_syscall) {
 			count32++;
 			systemcfg->syscall_map_32[i >> 5] |=
 				0x80000000UL >> (i & 0x1f);

commit e37bc5df8e96c72f27ec3579499726b656e4e641
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Mon Oct 24 11:41:33 2005 +1000

    [PATCH] powerpc: Purge bootinfo.h
    
    With ARCH=powerpc we assume the presence of a device tree, so we don't
    require any support for the old bi_recs method of passing boot
    parameters.  Likewise, we've never needed it for ppc64, but we still
    had an include/asm-ppc64/bootinfo.h from which nothing was used.  This
    patch removes that file, and all references to it in arch/ppc64 and
    arch/powerpc.  A related, unused variable 'boot_mem_size' is also
    removed from setup_32.c.  The bootinfo stuff remains in ARCH=ppc for
    the time being.
    
    Built and booted on Power5 (ARCH=ppc64 and ARCH=powerpc), built for
    32-bit powermac (ARCH=powerpc and ARCH=ppc).
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 721adeea601d..a8f7ff5ab1a4 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -37,7 +37,6 @@
 #include <asm/prom.h>
 #include <asm/processor.h>
 #include <asm/pgtable.h>
-#include <asm/bootinfo.h>
 #include <asm/smp.h>
 #include <asm/elf.h>
 #include <asm/machdep.h>

commit fda262b8978d0089758ef9444508434c74113a61
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 27 20:20:42 2005 +1000

    [PATCH] ppc64: remove arch/ppc64/kernel/setup.c
    
    and use setup_64.c from the merged tree instead.  The only difference
    between them was the code to set up the syscall maps.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 9cdf294e76f6..721adeea601d 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -702,6 +702,17 @@ static void __init emergency_stack_init(void)
 						limit)) + PAGE_SIZE;
 }
 
+extern unsigned long *sys_call_table;
+extern unsigned long sys_ni_syscall;
+#ifdef CONFIG_PPC_MERGE
+#define SYS_CALL_ENTRY64(i)	sys_call_table[(i) * 2]
+#define SYS_CALL_ENTRY32(i)	sys_call_table[(i) * 2 + 1]
+#else
+extern unsigned long *sys_call_table32;
+#define SYS_CALL_ENTRY64(i)	sys_call_table[(i)]
+#define SYS_CALL_ENTRY32(i)	sys_call_table32[(i)]
+#endif
+
 /*
  * Called from setup_arch to initialize the bitmap of available
  * syscalls in the systemcfg page
@@ -709,17 +720,14 @@ static void __init emergency_stack_init(void)
 void __init setup_syscall_map(void)
 {
 	unsigned int i, count64 = 0, count32 = 0;
-	extern unsigned long *sys_call_table;
-	extern unsigned long sys_ni_syscall;
-
 
 	for (i = 0; i < __NR_syscalls; i++) {
-		if (sys_call_table[i*2] != sys_ni_syscall) {
+		if (SYS_CALL_ENTRY64(i) != sys_ni_syscall) {
 			count64++;
 			systemcfg->syscall_map_64[i >> 5] |=
 				0x80000000UL >> (i & 0x1f);
 		}
-		if (sys_call_table[i*2+1] != sys_ni_syscall) {
+		if (SYS_CALL_ENTRY32(i) != sys_ni_syscall) {
 			count32++;
 			systemcfg->syscall_map_32[i >> 5] |=
 				0x80000000UL >> (i & 0x1f);

commit cb4ab974ae0bff3f49086090a1a50373c5edc8f4
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 27 11:08:31 2005 +1000

    powerpc: Remove common stuff from setup_64.c
    
    This should have been in commit 03501dab035ab7da5e1373f5e130cfd6346d3f21
    but got missed by accident.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 0312422881ae..9cdf294e76f6 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -138,23 +138,6 @@ static struct notifier_block ppc64_panic_block = {
 	.priority = INT_MIN /* may not return; must be done last */
 };
 
-/*
- * Perhaps we can put the pmac screen_info[] here
- * on pmac as well so we don't need the ifdef's.
- * Until we get multiple-console support in here
- * that is.  -- Cort
- * Maybe tie it to serial consoles, since this is really what
- * these processors use on existing boards.  -- Dan
- */ 
-struct screen_info screen_info = {
-	.orig_x = 0,
-	.orig_y = 25,
-	.orig_video_cols = 80,
-	.orig_video_lines = 25,
-	.orig_video_isVGA = 1,
-	.orig_video_points = 16
-};
-
 #ifdef CONFIG_SMP
 
 static int smt_enabled_cmdline;
@@ -641,51 +624,6 @@ void __init setup_system(void)
 	DBG(" <- setup_system()\n");
 }
 
-/* also used by kexec */
-void machine_shutdown(void)
-{
-	if (ppc_md.nvram_sync)
-		ppc_md.nvram_sync();
-}
-
-void machine_restart(char *cmd)
-{
-	machine_shutdown();
-	ppc_md.restart(cmd);
-#ifdef CONFIG_SMP
-	smp_send_stop();
-#endif
-	printk(KERN_EMERG "System Halted, OK to turn off power\n");
-	local_irq_disable();
-	while (1) ;
-}
-
-void machine_power_off(void)
-{
-	machine_shutdown();
-	ppc_md.power_off();
-#ifdef CONFIG_SMP
-	smp_send_stop();
-#endif
-	printk(KERN_EMERG "System Halted, OK to turn off power\n");
-	local_irq_disable();
-	while (1) ;
-}
-/* Used by the G5 thermal driver */
-EXPORT_SYMBOL_GPL(machine_power_off);
-
-void machine_halt(void)
-{
-	machine_shutdown();
-	ppc_md.halt();
-#ifdef CONFIG_SMP
-	smp_send_stop();
-#endif
-	printk(KERN_EMERG "System Halted, OK to turn off power\n");
-	local_irq_disable();
-	while (1) ;
-}
-
 static int ppc64_panic_event(struct notifier_block *this,
                              unsigned long event, void *ptr)
 {
@@ -693,90 +631,6 @@ static int ppc64_panic_event(struct notifier_block *this,
 	return NOTIFY_DONE;
 }
 
-
-#ifdef CONFIG_SMP
-DEFINE_PER_CPU(unsigned int, pvr);
-#endif
-
-static int show_cpuinfo(struct seq_file *m, void *v)
-{
-	unsigned long cpu_id = (unsigned long)v - 1;
-	unsigned int pvr;
-	unsigned short maj;
-	unsigned short min;
-
-	if (cpu_id == NR_CPUS) {
-		seq_printf(m, "timebase\t: %lu\n", ppc_tb_freq);
-
-		if (ppc_md.show_cpuinfo != NULL)
-			ppc_md.show_cpuinfo(m);
-
-		return 0;
-	}
-
-	/* We only show online cpus: disable preempt (overzealous, I
-	 * knew) to prevent cpu going down. */
-	preempt_disable();
-	if (!cpu_online(cpu_id)) {
-		preempt_enable();
-		return 0;
-	}
-
-#ifdef CONFIG_SMP
-	pvr = per_cpu(pvr, cpu_id);
-#else
-	pvr = mfspr(SPRN_PVR);
-#endif
-	maj = (pvr >> 8) & 0xFF;
-	min = pvr & 0xFF;
-
-	seq_printf(m, "processor\t: %lu\n", cpu_id);
-	seq_printf(m, "cpu\t\t: ");
-
-	if (cur_cpu_spec->pvr_mask)
-		seq_printf(m, "%s", cur_cpu_spec->cpu_name);
-	else
-		seq_printf(m, "unknown (%08x)", pvr);
-
-#ifdef CONFIG_ALTIVEC
-	if (cpu_has_feature(CPU_FTR_ALTIVEC))
-		seq_printf(m, ", altivec supported");
-#endif /* CONFIG_ALTIVEC */
-
-	seq_printf(m, "\n");
-
-	/*
-	 * Assume here that all clock rates are the same in a
-	 * smp system.  -- Cort
-	 */
-	seq_printf(m, "clock\t\t: %lu.%06luMHz\n", ppc_proc_freq / 1000000,
-		   ppc_proc_freq % 1000000);
-
-	seq_printf(m, "revision\t: %hd.%hd\n\n", maj, min);
-
-	preempt_enable();
-	return 0;
-}
-
-static void *c_start(struct seq_file *m, loff_t *pos)
-{
-	return *pos <= NR_CPUS ? (void *)((*pos)+1) : NULL;
-}
-static void *c_next(struct seq_file *m, void *v, loff_t *pos)
-{
-	++*pos;
-	return c_start(m, pos);
-}
-static void c_stop(struct seq_file *m, void *v)
-{
-}
-struct seq_operations cpuinfo_op = {
-	.start =c_start,
-	.next =	c_next,
-	.stop =	c_stop,
-	.show =	show_cpuinfo,
-};
-
 /*
  * These three variables are used to save values passed to us by prom_init()
  * via the device tree. The TCE variables are needed because with a memory_limit
@@ -803,130 +657,6 @@ static int __init early_parsemem(char *p)
 early_param("mem", early_parsemem);
 #endif /* CONFIG_PPC_ISERIES */
 
-#ifdef CONFIG_PPC_MULTIPLATFORM
-static int __init set_preferred_console(void)
-{
-	struct device_node *prom_stdout = NULL;
-	char *name;
-	u32 *spd;
-	int offset = 0;
-
-	DBG(" -> set_preferred_console()\n");
-
-	/* The user has requested a console so this is already set up. */
-	if (strstr(saved_command_line, "console=")) {
-		DBG(" console was specified !\n");
-		return -EBUSY;
-	}
-
-	if (!of_chosen) {
-		DBG(" of_chosen is NULL !\n");
-		return -ENODEV;
-	}
-	/* We are getting a weird phandle from OF ... */
-	/* ... So use the full path instead */
-	name = (char *)get_property(of_chosen, "linux,stdout-path", NULL);
-	if (name == NULL) {
-		DBG(" no linux,stdout-path !\n");
-		return -ENODEV;
-	}
-	prom_stdout = of_find_node_by_path(name);
-	if (!prom_stdout) {
-		DBG(" can't find stdout package %s !\n", name);
-		return -ENODEV;
-	}	
-	DBG("stdout is %s\n", prom_stdout->full_name);
-
-	name = (char *)get_property(prom_stdout, "name", NULL);
-	if (!name) {
-		DBG(" stdout package has no name !\n");
-		goto not_found;
-	}
-	spd = (u32 *)get_property(prom_stdout, "current-speed", NULL);
-
-	if (0)
-		;
-#ifdef CONFIG_SERIAL_8250_CONSOLE
-	else if (strcmp(name, "serial") == 0) {
-		int i;
-		u32 *reg = (u32 *)get_property(prom_stdout, "reg", &i);
-		if (i > 8) {
-			switch (reg[1]) {
-				case 0x3f8:
-					offset = 0;
-					break;
-				case 0x2f8:
-					offset = 1;
-					break;
-				case 0x898:
-					offset = 2;
-					break;
-				case 0x890:
-					offset = 3;
-					break;
-				default:
-					/* We dont recognise the serial port */
-					goto not_found;
-			}
-		}
-	}
-#endif /* CONFIG_SERIAL_8250_CONSOLE */
-#ifdef CONFIG_PPC_PSERIES
-	else if (strcmp(name, "vty") == 0) {
- 		u32 *reg = (u32 *)get_property(prom_stdout, "reg", NULL);
- 		char *compat = (char *)get_property(prom_stdout, "compatible", NULL);
-
- 		if (reg && compat && (strcmp(compat, "hvterm-protocol") == 0)) {
- 			/* Host Virtual Serial Interface */
- 			int offset;
- 			switch (reg[0]) {
- 				case 0x30000000:
- 					offset = 0;
- 					break;
- 				case 0x30000001:
- 					offset = 1;
- 					break;
- 				default:
-					goto not_found;
- 			}
-			of_node_put(prom_stdout);
-			DBG("Found hvsi console at offset %d\n", offset);
- 			return add_preferred_console("hvsi", offset, NULL);
- 		} else {
- 			/* pSeries LPAR virtual console */
-			of_node_put(prom_stdout);
-			DBG("Found hvc console\n");
- 			return add_preferred_console("hvc", 0, NULL);
- 		}
-	}
-#endif /* CONFIG_PPC_PSERIES */
-#ifdef CONFIG_SERIAL_PMACZILOG_CONSOLE
-	else if (strcmp(name, "ch-a") == 0)
-		offset = 0;
-	else if (strcmp(name, "ch-b") == 0)
-		offset = 1;
-#endif /* CONFIG_SERIAL_PMACZILOG_CONSOLE */
-	else
-		goto not_found;
-	of_node_put(prom_stdout);
-
-	DBG("Found serial console at ttyS%d\n", offset);
-
-	if (spd) {
-		static char __initdata opt[16];
-		sprintf(opt, "%d", *spd);
-		return add_preferred_console("ttyS", offset, opt);
-	} else
-		return add_preferred_console("ttyS", offset, NULL);
-
- not_found:
-	DBG("No preferred console found !\n");
-	of_node_put(prom_stdout);
-	return -ENODEV;
-}
-console_initcall(set_preferred_console);
-#endif /* CONFIG_PPC_MULTIPLATFORM */
-
 #ifdef CONFIG_IRQSTACKS
 static void __init irqstack_early_init(void)
 {

commit 0458060c1c59c5378d8fb5daabe18cf4681c35cd
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 20 21:00:20 2005 +1000

    ppc64: Move init_boot_text call and conswitchp init into setup_arch
    
    This way they get done in one place for all platforms, and it is
    more consistent with what ppc32 does.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 97ffdcf09c03..0312422881ae 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -586,6 +586,10 @@ void __init setup_system(void)
 	 */
 	finish_device_tree();
 
+#ifdef CONFIG_BOOTX_TEXT
+	init_boot_display();
+#endif
+
 	/*
 	 * Initialize xmon
 	 */
@@ -1039,6 +1043,10 @@ void __init setup_arch(char **cmdline_p)
 	/* initialize the syscall map in systemcfg */
 	setup_syscall_map();
 
+#ifdef CONFIG_DUMMY_CONSOLE
+	conswitchp = &dummy_con;
+#endif
+
 	ppc_md.setup_arch();
 
 	/* Use the default idle loop if the platform hasn't provided one. */

commit d8699e65c6bc0a81b5e679ca5b135bfe3c3fb483
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 20 17:02:01 2005 +1000

    ppc64: Change ppc_md.get_cpuinfo to ppc_md.show_cpuinfo
    
    ... for consistency with ppc32; also add in ppc32's show_percpuinfo
    function.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 6f29614c3581..97ffdcf09c03 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -704,8 +704,8 @@ static int show_cpuinfo(struct seq_file *m, void *v)
 	if (cpu_id == NR_CPUS) {
 		seq_printf(m, "timebase\t: %lu\n", ppc_tb_freq);
 
-		if (ppc_md.get_cpuinfo != NULL)
-			ppc_md.get_cpuinfo(m);
+		if (ppc_md.show_cpuinfo != NULL)
+			ppc_md.show_cpuinfo(m);
 
 		return 0;
 	}

commit f2783c15007468c14972e2617db51e9affc7fad9
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 20 09:23:26 2005 +1000

    powerpc: Merge time.c and asm/time.h.
    
    We now use the merged time.c for both 32-bit and 64-bit compilation
    with ARCH=powerpc, and for ARCH=ppc64, but not for ARCH=ppc32.
    This removes setup_default_decr (folds its function into time_init)
    and moves wakeup_decrementer into time.c.  This also makes an
    asm-powerpc/rtc.h.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 4fcf67575cbb..6f29614c3581 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -1083,15 +1083,6 @@ void ppc64_terminate_msg(unsigned int src, const char *msg)
 	printk("[terminate]%04x %s\n", src, msg);
 }
 
-/* This should only be called on processor 0 during calibrate decr */
-void __init setup_default_decr(void)
-{
-	struct paca_struct *lpaca = get_paca();
-
-	lpaca->default_decr = tb_ticks_per_jiffy;
-	lpaca->next_jiffy_update_tb = get_tb() + tb_ticks_per_jiffy;
-}
-
 #ifndef CONFIG_PPC_ISERIES
 /*
  * This function can be used by platforms to "find" legacy serial ports.

commit cc5aa206d2c853929ce67d8f5ebb57cd1c7fd413
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Oct 11 17:35:20 2005 +1000

    powerpc: Remove debug messages from setup_64.c
    
    A bunch of printks were left in arch/powerpc/kernel/setup_64.c from
    when I was chasing a bug.  This removes them.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
index 212b00823f82..4fcf67575cbb 100644
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@ -567,14 +567,11 @@ void __init setup_system(void)
 	 */
 	rtas_initialize();
 #endif /* CONFIG_PPC_RTAS */
-	printk("%s:%d rtas.dev=%p (@ %p)\n", __FILE__, __LINE__, rtas.dev,
-	       &rtas.dev);
 
 	/*
 	 * Check if we have an initrd provided via the device-tree
 	 */
 	check_for_initrd();
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 
 	/*
 	 * Do some platform specific early initializations, that includes
@@ -582,14 +579,12 @@ void __init setup_system(void)
 	 * related options that will be used by finish_device_tree()
 	 */
 	ppc_md.init_early();
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 
 	/*
 	 * "Finish" the device-tree, that is do the actual parsing of
 	 * some of the properties like the interrupt map
 	 */
 	finish_device_tree();
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 
 	/*
 	 * Initialize xmon
@@ -597,31 +592,26 @@ void __init setup_system(void)
 #ifdef CONFIG_XMON_DEFAULT
 	xmon_init(1);
 #endif
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 	/*
 	 * Register early console
 	 */
 	register_early_udbg_console();
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 
 	/* Save unparsed command line copy for /proc/cmdline */
 	strlcpy(saved_command_line, cmd_line, COMMAND_LINE_SIZE);
 
 	parse_early_param();
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 
 #ifdef CONFIG_SMP
 	/*
 	 * iSeries has already initialized the cpu maps at this point.
 	 */
 	setup_cpu_maps();
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 
 	/* Release secondary cpus out of their spinloops at 0x60 now that
 	 * we can map physical -> logical CPU ids
 	 */
 	smp_release_cpus();
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 #endif
 
 	printk("Starting Linux PPC64 %s\n", system_utsname.version);
@@ -641,10 +631,8 @@ void __init setup_system(void)
 	printk("htab_address                  = 0x%p\n", htab_address);
 	printk("htab_hash_mask                = 0x%lx\n", htab_hash_mask);
 	printk("-----------------------------------------------------\n");
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 
 	mm_init_ppc64();
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 
 	DBG(" <- setup_system()\n");
 }
@@ -1016,7 +1004,6 @@ void __init setup_arch(char **cmdline_p)
 {
 	extern void do_init_bootmem(void);
 
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 	ppc64_boot_msg(0x12, "Setup Arch");
 
 	*cmdline_p = cmd_line;
@@ -1031,7 +1018,6 @@ void __init setup_arch(char **cmdline_p)
 
 	/* reboot on panic */
 	panic_timeout = 180;
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 
 	if (ppc_md.panic)
 		notifier_chain_register(&panic_notifier_list, &ppc64_panic_block);
@@ -1044,14 +1030,12 @@ void __init setup_arch(char **cmdline_p)
 	irqstack_early_init();
 	emergency_stack_init();
 
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 	stabs_alloc();
 
 	/* set up the bootmem stuff with available memory */
 	do_init_bootmem();
 	sparse_init();
 
-	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
 	/* initialize the syscall map in systemcfg */
 	setup_syscall_map();
 

commit 40ef8cbc6d360e564573eb19582249c35d8ba330
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 10 22:50:37 2005 +1000

    powerpc: Get 64-bit configs to compile with ARCH=powerpc
    
    This is a bunch of mostly small fixes that are needed to get
    ARCH=powerpc to compile for 64-bit.  This adds setup_64.c from
    arch/ppc64/kernel/setup.c and locks.c from arch/ppc64/lib/locks.c.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c
new file mode 100644
index 000000000000..212b00823f82
--- /dev/null
+++ b/arch/powerpc/kernel/setup_64.c
@@ -0,0 +1,1324 @@
+/*
+ * 
+ * Common boot and setup code.
+ *
+ * Copyright (C) 2001 PPC64 Team, IBM Corp
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#undef DEBUG
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/sched.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/reboot.h>
+#include <linux/delay.h>
+#include <linux/initrd.h>
+#include <linux/ide.h>
+#include <linux/seq_file.h>
+#include <linux/ioport.h>
+#include <linux/console.h>
+#include <linux/utsname.h>
+#include <linux/tty.h>
+#include <linux/root_dev.h>
+#include <linux/notifier.h>
+#include <linux/cpu.h>
+#include <linux/unistd.h>
+#include <linux/serial.h>
+#include <linux/serial_8250.h>
+#include <asm/io.h>
+#include <asm/prom.h>
+#include <asm/processor.h>
+#include <asm/pgtable.h>
+#include <asm/bootinfo.h>
+#include <asm/smp.h>
+#include <asm/elf.h>
+#include <asm/machdep.h>
+#include <asm/paca.h>
+#include <asm/ppcdebug.h>
+#include <asm/time.h>
+#include <asm/cputable.h>
+#include <asm/sections.h>
+#include <asm/btext.h>
+#include <asm/nvram.h>
+#include <asm/setup.h>
+#include <asm/system.h>
+#include <asm/rtas.h>
+#include <asm/iommu.h>
+#include <asm/serial.h>
+#include <asm/cache.h>
+#include <asm/page.h>
+#include <asm/mmu.h>
+#include <asm/lmb.h>
+#include <asm/iSeries/ItLpNaca.h>
+#include <asm/firmware.h>
+#include <asm/systemcfg.h>
+
+#ifdef DEBUG
+#define DBG(fmt...) udbg_printf(fmt)
+#else
+#define DBG(fmt...)
+#endif
+
+/*
+ * Here are some early debugging facilities. You can enable one
+ * but your kernel will not boot on anything else if you do so
+ */
+
+/* This one is for use on LPAR machines that support an HVC console
+ * on vterm 0
+ */
+extern void udbg_init_debug_lpar(void);
+/* This one is for use on Apple G5 machines
+ */
+extern void udbg_init_pmac_realmode(void);
+/* That's RTAS panel debug */
+extern void call_rtas_display_status_delay(unsigned char c);
+/* Here's maple real mode debug */
+extern void udbg_init_maple_realmode(void);
+
+#define EARLY_DEBUG_INIT() do {} while(0)
+
+#if 0
+#define EARLY_DEBUG_INIT() udbg_init_debug_lpar()
+#define EARLY_DEBUG_INIT() udbg_init_maple_realmode()
+#define EARLY_DEBUG_INIT() udbg_init_pmac_realmode()
+#define EARLY_DEBUG_INIT()						\
+	do { udbg_putc = call_rtas_display_status_delay; } while(0)
+#endif
+
+/* extern void *stab; */
+extern unsigned long klimit;
+
+extern void mm_init_ppc64(void);
+extern void stab_initialize(unsigned long stab);
+extern void htab_initialize(void);
+extern void early_init_devtree(void *flat_dt);
+extern void unflatten_device_tree(void);
+
+extern void smp_release_cpus(void);
+
+int have_of = 1;
+int boot_cpuid = 0;
+int boot_cpuid_phys = 0;
+dev_t boot_dev;
+u64 ppc64_pft_size;
+
+struct ppc64_caches ppc64_caches;
+EXPORT_SYMBOL_GPL(ppc64_caches);
+
+/*
+ * These are used in binfmt_elf.c to put aux entries on the stack
+ * for each elf executable being started.
+ */
+int dcache_bsize;
+int icache_bsize;
+int ucache_bsize;
+
+/* The main machine-dep calls structure
+ */
+struct machdep_calls ppc_md;
+EXPORT_SYMBOL(ppc_md);
+
+#ifdef CONFIG_MAGIC_SYSRQ
+unsigned long SYSRQ_KEY;
+#endif /* CONFIG_MAGIC_SYSRQ */
+
+
+static int ppc64_panic_event(struct notifier_block *, unsigned long, void *);
+static struct notifier_block ppc64_panic_block = {
+	.notifier_call = ppc64_panic_event,
+	.priority = INT_MIN /* may not return; must be done last */
+};
+
+/*
+ * Perhaps we can put the pmac screen_info[] here
+ * on pmac as well so we don't need the ifdef's.
+ * Until we get multiple-console support in here
+ * that is.  -- Cort
+ * Maybe tie it to serial consoles, since this is really what
+ * these processors use on existing boards.  -- Dan
+ */ 
+struct screen_info screen_info = {
+	.orig_x = 0,
+	.orig_y = 25,
+	.orig_video_cols = 80,
+	.orig_video_lines = 25,
+	.orig_video_isVGA = 1,
+	.orig_video_points = 16
+};
+
+#ifdef CONFIG_SMP
+
+static int smt_enabled_cmdline;
+
+/* Look for ibm,smt-enabled OF option */
+static void check_smt_enabled(void)
+{
+	struct device_node *dn;
+	char *smt_option;
+
+	/* Allow the command line to overrule the OF option */
+	if (smt_enabled_cmdline)
+		return;
+
+	dn = of_find_node_by_path("/options");
+
+	if (dn) {
+		smt_option = (char *)get_property(dn, "ibm,smt-enabled", NULL);
+
+                if (smt_option) {
+			if (!strcmp(smt_option, "on"))
+				smt_enabled_at_boot = 1;
+			else if (!strcmp(smt_option, "off"))
+				smt_enabled_at_boot = 0;
+                }
+        }
+}
+
+/* Look for smt-enabled= cmdline option */
+static int __init early_smt_enabled(char *p)
+{
+	smt_enabled_cmdline = 1;
+
+	if (!p)
+		return 0;
+
+	if (!strcmp(p, "on") || !strcmp(p, "1"))
+		smt_enabled_at_boot = 1;
+	else if (!strcmp(p, "off") || !strcmp(p, "0"))
+		smt_enabled_at_boot = 0;
+
+	return 0;
+}
+early_param("smt-enabled", early_smt_enabled);
+
+/**
+ * setup_cpu_maps - initialize the following cpu maps:
+ *                  cpu_possible_map
+ *                  cpu_present_map
+ *                  cpu_sibling_map
+ *
+ * Having the possible map set up early allows us to restrict allocations
+ * of things like irqstacks to num_possible_cpus() rather than NR_CPUS.
+ *
+ * We do not initialize the online map here; cpus set their own bits in
+ * cpu_online_map as they come up.
+ *
+ * This function is valid only for Open Firmware systems.  finish_device_tree
+ * must be called before using this.
+ *
+ * While we're here, we may as well set the "physical" cpu ids in the paca.
+ */
+static void __init setup_cpu_maps(void)
+{
+	struct device_node *dn = NULL;
+	int cpu = 0;
+	int swap_cpuid = 0;
+
+	check_smt_enabled();
+
+	while ((dn = of_find_node_by_type(dn, "cpu")) && cpu < NR_CPUS) {
+		u32 *intserv;
+		int j, len = sizeof(u32), nthreads;
+
+		intserv = (u32 *)get_property(dn, "ibm,ppc-interrupt-server#s",
+					      &len);
+		if (!intserv)
+			intserv = (u32 *)get_property(dn, "reg", NULL);
+
+		nthreads = len / sizeof(u32);
+
+		for (j = 0; j < nthreads && cpu < NR_CPUS; j++) {
+			cpu_set(cpu, cpu_present_map);
+			set_hard_smp_processor_id(cpu, intserv[j]);
+
+			if (intserv[j] == boot_cpuid_phys)
+				swap_cpuid = cpu;
+			cpu_set(cpu, cpu_possible_map);
+			cpu++;
+		}
+	}
+
+	/* Swap CPU id 0 with boot_cpuid_phys, so we can always assume that
+	 * boot cpu is logical 0.
+	 */
+	if (boot_cpuid_phys != get_hard_smp_processor_id(0)) {
+		u32 tmp;
+		tmp = get_hard_smp_processor_id(0);
+		set_hard_smp_processor_id(0, boot_cpuid_phys);
+		set_hard_smp_processor_id(swap_cpuid, tmp);
+	}
+
+	/*
+	 * On pSeries LPAR, we need to know how many cpus
+	 * could possibly be added to this partition.
+	 */
+	if (systemcfg->platform == PLATFORM_PSERIES_LPAR &&
+				(dn = of_find_node_by_path("/rtas"))) {
+		int num_addr_cell, num_size_cell, maxcpus;
+		unsigned int *ireg;
+
+		num_addr_cell = prom_n_addr_cells(dn);
+		num_size_cell = prom_n_size_cells(dn);
+
+		ireg = (unsigned int *)
+			get_property(dn, "ibm,lrdr-capacity", NULL);
+
+		if (!ireg)
+			goto out;
+
+		maxcpus = ireg[num_addr_cell + num_size_cell];
+
+		/* Double maxcpus for processors which have SMT capability */
+		if (cpu_has_feature(CPU_FTR_SMT))
+			maxcpus *= 2;
+
+		if (maxcpus > NR_CPUS) {
+			printk(KERN_WARNING
+			       "Partition configured for %d cpus, "
+			       "operating system maximum is %d.\n",
+			       maxcpus, NR_CPUS);
+			maxcpus = NR_CPUS;
+		} else
+			printk(KERN_INFO "Partition configured for %d cpus.\n",
+			       maxcpus);
+
+		for (cpu = 0; cpu < maxcpus; cpu++)
+			cpu_set(cpu, cpu_possible_map);
+	out:
+		of_node_put(dn);
+	}
+
+	/*
+	 * Do the sibling map; assume only two threads per processor.
+	 */
+	for_each_cpu(cpu) {
+		cpu_set(cpu, cpu_sibling_map[cpu]);
+		if (cpu_has_feature(CPU_FTR_SMT))
+			cpu_set(cpu ^ 0x1, cpu_sibling_map[cpu]);
+	}
+
+	systemcfg->processorCount = num_present_cpus();
+}
+#endif /* CONFIG_SMP */
+
+extern struct machdep_calls pSeries_md;
+extern struct machdep_calls pmac_md;
+extern struct machdep_calls maple_md;
+extern struct machdep_calls bpa_md;
+extern struct machdep_calls iseries_md;
+
+/* Ultimately, stuff them in an elf section like initcalls... */
+static struct machdep_calls __initdata *machines[] = {
+#ifdef CONFIG_PPC_PSERIES
+	&pSeries_md,
+#endif /* CONFIG_PPC_PSERIES */
+#ifdef CONFIG_PPC_PMAC
+	&pmac_md,
+#endif /* CONFIG_PPC_PMAC */
+#ifdef CONFIG_PPC_MAPLE
+	&maple_md,
+#endif /* CONFIG_PPC_MAPLE */
+#ifdef CONFIG_PPC_BPA
+	&bpa_md,
+#endif
+#ifdef CONFIG_PPC_ISERIES
+	&iseries_md,
+#endif
+	NULL
+};
+
+/*
+ * Early initialization entry point. This is called by head.S
+ * with MMU translation disabled. We rely on the "feature" of
+ * the CPU that ignores the top 2 bits of the address in real
+ * mode so we can access kernel globals normally provided we
+ * only toy with things in the RMO region. From here, we do
+ * some early parsing of the device-tree to setup out LMB
+ * data structures, and allocate & initialize the hash table
+ * and segment tables so we can start running with translation
+ * enabled.
+ *
+ * It is this function which will call the probe() callback of
+ * the various platform types and copy the matching one to the
+ * global ppc_md structure. Your platform can eventually do
+ * some very early initializations from the probe() routine, but
+ * this is not recommended, be very careful as, for example, the
+ * device-tree is not accessible via normal means at this point.
+ */
+
+void __init early_setup(unsigned long dt_ptr)
+{
+	struct paca_struct *lpaca = get_paca();
+	static struct machdep_calls **mach;
+
+	/*
+	 * Enable early debugging if any specified (see top of
+	 * this file)
+	 */
+	EARLY_DEBUG_INIT();
+
+	DBG(" -> early_setup()\n");
+
+	/*
+	 * Fill the default DBG level (do we want to keep
+	 * that old mecanism around forever ?)
+	 */
+	ppcdbg_initialize();
+
+	/*
+	 * Do early initializations using the flattened device
+	 * tree, like retreiving the physical memory map or
+	 * calculating/retreiving the hash table size
+	 */
+	early_init_devtree(__va(dt_ptr));
+
+	/*
+	 * Iterate all ppc_md structures until we find the proper
+	 * one for the current machine type
+	 */
+	DBG("Probing machine type for platform %x...\n",
+	    systemcfg->platform);
+
+	for (mach = machines; *mach; mach++) {
+		if ((*mach)->probe(systemcfg->platform))
+			break;
+	}
+	/* What can we do if we didn't find ? */
+	if (*mach == NULL) {
+		DBG("No suitable machine found !\n");
+		for (;;);
+	}
+	ppc_md = **mach;
+
+	DBG("Found, Initializing memory management...\n");
+
+	/*
+	 * Initialize stab / SLB management
+	 */
+	if (!firmware_has_feature(FW_FEATURE_ISERIES))
+		stab_initialize(lpaca->stab_real);
+
+	/*
+	 * Initialize the MMU Hash table and create the linear mapping
+	 * of memory
+	 */
+	htab_initialize();
+
+	DBG(" <- early_setup()\n");
+}
+
+
+/*
+ * Initialize some remaining members of the ppc64_caches and systemcfg structures
+ * (at least until we get rid of them completely). This is mostly some
+ * cache informations about the CPU that will be used by cache flush
+ * routines and/or provided to userland
+ */
+static void __init initialize_cache_info(void)
+{
+	struct device_node *np;
+	unsigned long num_cpus = 0;
+
+	DBG(" -> initialize_cache_info()\n");
+
+	for (np = NULL; (np = of_find_node_by_type(np, "cpu"));) {
+		num_cpus += 1;
+
+		/* We're assuming *all* of the CPUs have the same
+		 * d-cache and i-cache sizes... -Peter
+		 */
+
+		if ( num_cpus == 1 ) {
+			u32 *sizep, *lsizep;
+			u32 size, lsize;
+			const char *dc, *ic;
+
+			/* Then read cache informations */
+			if (systemcfg->platform == PLATFORM_POWERMAC) {
+				dc = "d-cache-block-size";
+				ic = "i-cache-block-size";
+			} else {
+				dc = "d-cache-line-size";
+				ic = "i-cache-line-size";
+			}
+
+			size = 0;
+			lsize = cur_cpu_spec->dcache_bsize;
+			sizep = (u32 *)get_property(np, "d-cache-size", NULL);
+			if (sizep != NULL)
+				size = *sizep;
+			lsizep = (u32 *) get_property(np, dc, NULL);
+			if (lsizep != NULL)
+				lsize = *lsizep;
+			if (sizep == 0 || lsizep == 0)
+				DBG("Argh, can't find dcache properties ! "
+				    "sizep: %p, lsizep: %p\n", sizep, lsizep);
+
+			systemcfg->dcache_size = ppc64_caches.dsize = size;
+			systemcfg->dcache_line_size =
+				ppc64_caches.dline_size = lsize;
+			ppc64_caches.log_dline_size = __ilog2(lsize);
+			ppc64_caches.dlines_per_page = PAGE_SIZE / lsize;
+
+			size = 0;
+			lsize = cur_cpu_spec->icache_bsize;
+			sizep = (u32 *)get_property(np, "i-cache-size", NULL);
+			if (sizep != NULL)
+				size = *sizep;
+			lsizep = (u32 *)get_property(np, ic, NULL);
+			if (lsizep != NULL)
+				lsize = *lsizep;
+			if (sizep == 0 || lsizep == 0)
+				DBG("Argh, can't find icache properties ! "
+				    "sizep: %p, lsizep: %p\n", sizep, lsizep);
+
+			systemcfg->icache_size = ppc64_caches.isize = size;
+			systemcfg->icache_line_size =
+				ppc64_caches.iline_size = lsize;
+			ppc64_caches.log_iline_size = __ilog2(lsize);
+			ppc64_caches.ilines_per_page = PAGE_SIZE / lsize;
+		}
+	}
+
+	/* Add an eye catcher and the systemcfg layout version number */
+	strcpy(systemcfg->eye_catcher, "SYSTEMCFG:PPC64");
+	systemcfg->version.major = SYSTEMCFG_MAJOR;
+	systemcfg->version.minor = SYSTEMCFG_MINOR;
+	systemcfg->processor = mfspr(SPRN_PVR);
+
+	DBG(" <- initialize_cache_info()\n");
+}
+
+static void __init check_for_initrd(void)
+{
+#ifdef CONFIG_BLK_DEV_INITRD
+	u64 *prop;
+
+	DBG(" -> check_for_initrd()\n");
+
+	if (of_chosen) {
+		prop = (u64 *)get_property(of_chosen,
+				"linux,initrd-start", NULL);
+		if (prop != NULL) {
+			initrd_start = (unsigned long)__va(*prop);
+			prop = (u64 *)get_property(of_chosen,
+					"linux,initrd-end", NULL);
+			if (prop != NULL) {
+				initrd_end = (unsigned long)__va(*prop);
+				initrd_below_start_ok = 1;
+			} else
+				initrd_start = 0;
+		}
+	}
+
+	/* If we were passed an initrd, set the ROOT_DEV properly if the values
+	 * look sensible. If not, clear initrd reference.
+	 */
+	if (initrd_start >= KERNELBASE && initrd_end >= KERNELBASE &&
+	    initrd_end > initrd_start)
+		ROOT_DEV = Root_RAM0;
+	else
+		initrd_start = initrd_end = 0;
+
+	if (initrd_start)
+		printk("Found initrd at 0x%lx:0x%lx\n", initrd_start, initrd_end);
+
+	DBG(" <- check_for_initrd()\n");
+#endif /* CONFIG_BLK_DEV_INITRD */
+}
+
+/*
+ * Do some initial setup of the system.  The parameters are those which 
+ * were passed in from the bootloader.
+ */
+void __init setup_system(void)
+{
+	DBG(" -> setup_system()\n");
+
+	/*
+	 * Unflatten the device-tree passed by prom_init or kexec
+	 */
+	unflatten_device_tree();
+
+	/*
+	 * Fill the ppc64_caches & systemcfg structures with informations
+	 * retreived from the device-tree. Need to be called before
+	 * finish_device_tree() since the later requires some of the
+	 * informations filled up here to properly parse the interrupt
+	 * tree.
+	 * It also sets up the cache line sizes which allows to call
+	 * routines like flush_icache_range (used by the hash init
+	 * later on).
+	 */
+	initialize_cache_info();
+
+#ifdef CONFIG_PPC_RTAS
+	/*
+	 * Initialize RTAS if available
+	 */
+	rtas_initialize();
+#endif /* CONFIG_PPC_RTAS */
+	printk("%s:%d rtas.dev=%p (@ %p)\n", __FILE__, __LINE__, rtas.dev,
+	       &rtas.dev);
+
+	/*
+	 * Check if we have an initrd provided via the device-tree
+	 */
+	check_for_initrd();
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+
+	/*
+	 * Do some platform specific early initializations, that includes
+	 * setting up the hash table pointers. It also sets up some interrupt-mapping
+	 * related options that will be used by finish_device_tree()
+	 */
+	ppc_md.init_early();
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+
+	/*
+	 * "Finish" the device-tree, that is do the actual parsing of
+	 * some of the properties like the interrupt map
+	 */
+	finish_device_tree();
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+
+	/*
+	 * Initialize xmon
+	 */
+#ifdef CONFIG_XMON_DEFAULT
+	xmon_init(1);
+#endif
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+	/*
+	 * Register early console
+	 */
+	register_early_udbg_console();
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+
+	/* Save unparsed command line copy for /proc/cmdline */
+	strlcpy(saved_command_line, cmd_line, COMMAND_LINE_SIZE);
+
+	parse_early_param();
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+
+#ifdef CONFIG_SMP
+	/*
+	 * iSeries has already initialized the cpu maps at this point.
+	 */
+	setup_cpu_maps();
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+
+	/* Release secondary cpus out of their spinloops at 0x60 now that
+	 * we can map physical -> logical CPU ids
+	 */
+	smp_release_cpus();
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+#endif
+
+	printk("Starting Linux PPC64 %s\n", system_utsname.version);
+
+	printk("-----------------------------------------------------\n");
+	printk("ppc64_pft_size                = 0x%lx\n", ppc64_pft_size);
+	printk("ppc64_debug_switch            = 0x%lx\n", ppc64_debug_switch);
+	printk("ppc64_interrupt_controller    = 0x%ld\n", ppc64_interrupt_controller);
+	printk("systemcfg                     = 0x%p\n", systemcfg);
+	printk("systemcfg->platform           = 0x%x\n", systemcfg->platform);
+	printk("systemcfg->processorCount     = 0x%lx\n", systemcfg->processorCount);
+	printk("systemcfg->physicalMemorySize = 0x%lx\n", systemcfg->physicalMemorySize);
+	printk("ppc64_caches.dcache_line_size = 0x%x\n",
+			ppc64_caches.dline_size);
+	printk("ppc64_caches.icache_line_size = 0x%x\n",
+			ppc64_caches.iline_size);
+	printk("htab_address                  = 0x%p\n", htab_address);
+	printk("htab_hash_mask                = 0x%lx\n", htab_hash_mask);
+	printk("-----------------------------------------------------\n");
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+
+	mm_init_ppc64();
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+
+	DBG(" <- setup_system()\n");
+}
+
+/* also used by kexec */
+void machine_shutdown(void)
+{
+	if (ppc_md.nvram_sync)
+		ppc_md.nvram_sync();
+}
+
+void machine_restart(char *cmd)
+{
+	machine_shutdown();
+	ppc_md.restart(cmd);
+#ifdef CONFIG_SMP
+	smp_send_stop();
+#endif
+	printk(KERN_EMERG "System Halted, OK to turn off power\n");
+	local_irq_disable();
+	while (1) ;
+}
+
+void machine_power_off(void)
+{
+	machine_shutdown();
+	ppc_md.power_off();
+#ifdef CONFIG_SMP
+	smp_send_stop();
+#endif
+	printk(KERN_EMERG "System Halted, OK to turn off power\n");
+	local_irq_disable();
+	while (1) ;
+}
+/* Used by the G5 thermal driver */
+EXPORT_SYMBOL_GPL(machine_power_off);
+
+void machine_halt(void)
+{
+	machine_shutdown();
+	ppc_md.halt();
+#ifdef CONFIG_SMP
+	smp_send_stop();
+#endif
+	printk(KERN_EMERG "System Halted, OK to turn off power\n");
+	local_irq_disable();
+	while (1) ;
+}
+
+static int ppc64_panic_event(struct notifier_block *this,
+                             unsigned long event, void *ptr)
+{
+	ppc_md.panic((char *)ptr);  /* May not return */
+	return NOTIFY_DONE;
+}
+
+
+#ifdef CONFIG_SMP
+DEFINE_PER_CPU(unsigned int, pvr);
+#endif
+
+static int show_cpuinfo(struct seq_file *m, void *v)
+{
+	unsigned long cpu_id = (unsigned long)v - 1;
+	unsigned int pvr;
+	unsigned short maj;
+	unsigned short min;
+
+	if (cpu_id == NR_CPUS) {
+		seq_printf(m, "timebase\t: %lu\n", ppc_tb_freq);
+
+		if (ppc_md.get_cpuinfo != NULL)
+			ppc_md.get_cpuinfo(m);
+
+		return 0;
+	}
+
+	/* We only show online cpus: disable preempt (overzealous, I
+	 * knew) to prevent cpu going down. */
+	preempt_disable();
+	if (!cpu_online(cpu_id)) {
+		preempt_enable();
+		return 0;
+	}
+
+#ifdef CONFIG_SMP
+	pvr = per_cpu(pvr, cpu_id);
+#else
+	pvr = mfspr(SPRN_PVR);
+#endif
+	maj = (pvr >> 8) & 0xFF;
+	min = pvr & 0xFF;
+
+	seq_printf(m, "processor\t: %lu\n", cpu_id);
+	seq_printf(m, "cpu\t\t: ");
+
+	if (cur_cpu_spec->pvr_mask)
+		seq_printf(m, "%s", cur_cpu_spec->cpu_name);
+	else
+		seq_printf(m, "unknown (%08x)", pvr);
+
+#ifdef CONFIG_ALTIVEC
+	if (cpu_has_feature(CPU_FTR_ALTIVEC))
+		seq_printf(m, ", altivec supported");
+#endif /* CONFIG_ALTIVEC */
+
+	seq_printf(m, "\n");
+
+	/*
+	 * Assume here that all clock rates are the same in a
+	 * smp system.  -- Cort
+	 */
+	seq_printf(m, "clock\t\t: %lu.%06luMHz\n", ppc_proc_freq / 1000000,
+		   ppc_proc_freq % 1000000);
+
+	seq_printf(m, "revision\t: %hd.%hd\n\n", maj, min);
+
+	preempt_enable();
+	return 0;
+}
+
+static void *c_start(struct seq_file *m, loff_t *pos)
+{
+	return *pos <= NR_CPUS ? (void *)((*pos)+1) : NULL;
+}
+static void *c_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	++*pos;
+	return c_start(m, pos);
+}
+static void c_stop(struct seq_file *m, void *v)
+{
+}
+struct seq_operations cpuinfo_op = {
+	.start =c_start,
+	.next =	c_next,
+	.stop =	c_stop,
+	.show =	show_cpuinfo,
+};
+
+/*
+ * These three variables are used to save values passed to us by prom_init()
+ * via the device tree. The TCE variables are needed because with a memory_limit
+ * in force we may need to explicitly map the TCE are at the top of RAM.
+ */
+unsigned long memory_limit;
+unsigned long tce_alloc_start;
+unsigned long tce_alloc_end;
+
+#ifdef CONFIG_PPC_ISERIES
+/*
+ * On iSeries we just parse the mem=X option from the command line.
+ * On pSeries it's a bit more complicated, see prom_init_mem()
+ */
+static int __init early_parsemem(char *p)
+{
+	if (!p)
+		return 0;
+
+	memory_limit = ALIGN(memparse(p, &p), PAGE_SIZE);
+
+	return 0;
+}
+early_param("mem", early_parsemem);
+#endif /* CONFIG_PPC_ISERIES */
+
+#ifdef CONFIG_PPC_MULTIPLATFORM
+static int __init set_preferred_console(void)
+{
+	struct device_node *prom_stdout = NULL;
+	char *name;
+	u32 *spd;
+	int offset = 0;
+
+	DBG(" -> set_preferred_console()\n");
+
+	/* The user has requested a console so this is already set up. */
+	if (strstr(saved_command_line, "console=")) {
+		DBG(" console was specified !\n");
+		return -EBUSY;
+	}
+
+	if (!of_chosen) {
+		DBG(" of_chosen is NULL !\n");
+		return -ENODEV;
+	}
+	/* We are getting a weird phandle from OF ... */
+	/* ... So use the full path instead */
+	name = (char *)get_property(of_chosen, "linux,stdout-path", NULL);
+	if (name == NULL) {
+		DBG(" no linux,stdout-path !\n");
+		return -ENODEV;
+	}
+	prom_stdout = of_find_node_by_path(name);
+	if (!prom_stdout) {
+		DBG(" can't find stdout package %s !\n", name);
+		return -ENODEV;
+	}	
+	DBG("stdout is %s\n", prom_stdout->full_name);
+
+	name = (char *)get_property(prom_stdout, "name", NULL);
+	if (!name) {
+		DBG(" stdout package has no name !\n");
+		goto not_found;
+	}
+	spd = (u32 *)get_property(prom_stdout, "current-speed", NULL);
+
+	if (0)
+		;
+#ifdef CONFIG_SERIAL_8250_CONSOLE
+	else if (strcmp(name, "serial") == 0) {
+		int i;
+		u32 *reg = (u32 *)get_property(prom_stdout, "reg", &i);
+		if (i > 8) {
+			switch (reg[1]) {
+				case 0x3f8:
+					offset = 0;
+					break;
+				case 0x2f8:
+					offset = 1;
+					break;
+				case 0x898:
+					offset = 2;
+					break;
+				case 0x890:
+					offset = 3;
+					break;
+				default:
+					/* We dont recognise the serial port */
+					goto not_found;
+			}
+		}
+	}
+#endif /* CONFIG_SERIAL_8250_CONSOLE */
+#ifdef CONFIG_PPC_PSERIES
+	else if (strcmp(name, "vty") == 0) {
+ 		u32 *reg = (u32 *)get_property(prom_stdout, "reg", NULL);
+ 		char *compat = (char *)get_property(prom_stdout, "compatible", NULL);
+
+ 		if (reg && compat && (strcmp(compat, "hvterm-protocol") == 0)) {
+ 			/* Host Virtual Serial Interface */
+ 			int offset;
+ 			switch (reg[0]) {
+ 				case 0x30000000:
+ 					offset = 0;
+ 					break;
+ 				case 0x30000001:
+ 					offset = 1;
+ 					break;
+ 				default:
+					goto not_found;
+ 			}
+			of_node_put(prom_stdout);
+			DBG("Found hvsi console at offset %d\n", offset);
+ 			return add_preferred_console("hvsi", offset, NULL);
+ 		} else {
+ 			/* pSeries LPAR virtual console */
+			of_node_put(prom_stdout);
+			DBG("Found hvc console\n");
+ 			return add_preferred_console("hvc", 0, NULL);
+ 		}
+	}
+#endif /* CONFIG_PPC_PSERIES */
+#ifdef CONFIG_SERIAL_PMACZILOG_CONSOLE
+	else if (strcmp(name, "ch-a") == 0)
+		offset = 0;
+	else if (strcmp(name, "ch-b") == 0)
+		offset = 1;
+#endif /* CONFIG_SERIAL_PMACZILOG_CONSOLE */
+	else
+		goto not_found;
+	of_node_put(prom_stdout);
+
+	DBG("Found serial console at ttyS%d\n", offset);
+
+	if (spd) {
+		static char __initdata opt[16];
+		sprintf(opt, "%d", *spd);
+		return add_preferred_console("ttyS", offset, opt);
+	} else
+		return add_preferred_console("ttyS", offset, NULL);
+
+ not_found:
+	DBG("No preferred console found !\n");
+	of_node_put(prom_stdout);
+	return -ENODEV;
+}
+console_initcall(set_preferred_console);
+#endif /* CONFIG_PPC_MULTIPLATFORM */
+
+#ifdef CONFIG_IRQSTACKS
+static void __init irqstack_early_init(void)
+{
+	unsigned int i;
+
+	/*
+	 * interrupt stacks must be under 256MB, we cannot afford to take
+	 * SLB misses on them.
+	 */
+	for_each_cpu(i) {
+		softirq_ctx[i] = (struct thread_info *)__va(lmb_alloc_base(THREAD_SIZE,
+					THREAD_SIZE, 0x10000000));
+		hardirq_ctx[i] = (struct thread_info *)__va(lmb_alloc_base(THREAD_SIZE,
+					THREAD_SIZE, 0x10000000));
+	}
+}
+#else
+#define irqstack_early_init()
+#endif
+
+/*
+ * Stack space used when we detect a bad kernel stack pointer, and
+ * early in SMP boots before relocation is enabled.
+ */
+static void __init emergency_stack_init(void)
+{
+	unsigned long limit;
+	unsigned int i;
+
+	/*
+	 * Emergency stacks must be under 256MB, we cannot afford to take
+	 * SLB misses on them. The ABI also requires them to be 128-byte
+	 * aligned.
+	 *
+	 * Since we use these as temporary stacks during secondary CPU
+	 * bringup, we need to get at them in real mode. This means they
+	 * must also be within the RMO region.
+	 */
+	limit = min(0x10000000UL, lmb.rmo_size);
+
+	for_each_cpu(i)
+		paca[i].emergency_sp = __va(lmb_alloc_base(PAGE_SIZE, 128,
+						limit)) + PAGE_SIZE;
+}
+
+/*
+ * Called from setup_arch to initialize the bitmap of available
+ * syscalls in the systemcfg page
+ */
+void __init setup_syscall_map(void)
+{
+	unsigned int i, count64 = 0, count32 = 0;
+	extern unsigned long *sys_call_table;
+	extern unsigned long sys_ni_syscall;
+
+
+	for (i = 0; i < __NR_syscalls; i++) {
+		if (sys_call_table[i*2] != sys_ni_syscall) {
+			count64++;
+			systemcfg->syscall_map_64[i >> 5] |=
+				0x80000000UL >> (i & 0x1f);
+		}
+		if (sys_call_table[i*2+1] != sys_ni_syscall) {
+			count32++;
+			systemcfg->syscall_map_32[i >> 5] |=
+				0x80000000UL >> (i & 0x1f);
+		}
+	}
+	printk(KERN_INFO "Syscall map setup, %d 32-bit and %d 64-bit syscalls\n",
+	       count32, count64);
+}
+
+/*
+ * Called into from start_kernel, after lock_kernel has been called.
+ * Initializes bootmem, which is unsed to manage page allocation until
+ * mem_init is called.
+ */
+void __init setup_arch(char **cmdline_p)
+{
+	extern void do_init_bootmem(void);
+
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+	ppc64_boot_msg(0x12, "Setup Arch");
+
+	*cmdline_p = cmd_line;
+
+	/*
+	 * Set cache line size based on type of cpu as a default.
+	 * Systems with OF can look in the properties on the cpu node(s)
+	 * for a possibly more accurate value.
+	 */
+	dcache_bsize = ppc64_caches.dline_size;
+	icache_bsize = ppc64_caches.iline_size;
+
+	/* reboot on panic */
+	panic_timeout = 180;
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+
+	if (ppc_md.panic)
+		notifier_chain_register(&panic_notifier_list, &ppc64_panic_block);
+
+	init_mm.start_code = PAGE_OFFSET;
+	init_mm.end_code = (unsigned long) _etext;
+	init_mm.end_data = (unsigned long) _edata;
+	init_mm.brk = klimit;
+	
+	irqstack_early_init();
+	emergency_stack_init();
+
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+	stabs_alloc();
+
+	/* set up the bootmem stuff with available memory */
+	do_init_bootmem();
+	sparse_init();
+
+	printk("%s:%d rtas.dev=%p\n", __FILE__, __LINE__, rtas.dev);
+	/* initialize the syscall map in systemcfg */
+	setup_syscall_map();
+
+	ppc_md.setup_arch();
+
+	/* Use the default idle loop if the platform hasn't provided one. */
+	if (NULL == ppc_md.idle_loop) {
+		ppc_md.idle_loop = default_idle;
+		printk(KERN_INFO "Using default idle loop\n");
+	}
+
+	paging_init();
+	ppc64_boot_msg(0x15, "Setup Done");
+}
+
+
+/* ToDo: do something useful if ppc_md is not yet setup. */
+#define PPC64_LINUX_FUNCTION 0x0f000000
+#define PPC64_IPL_MESSAGE 0xc0000000
+#define PPC64_TERM_MESSAGE 0xb0000000
+
+static void ppc64_do_msg(unsigned int src, const char *msg)
+{
+	if (ppc_md.progress) {
+		char buf[128];
+
+		sprintf(buf, "%08X\n", src);
+		ppc_md.progress(buf, 0);
+		snprintf(buf, 128, "%s", msg);
+		ppc_md.progress(buf, 0);
+	}
+}
+
+/* Print a boot progress message. */
+void ppc64_boot_msg(unsigned int src, const char *msg)
+{
+	ppc64_do_msg(PPC64_LINUX_FUNCTION|PPC64_IPL_MESSAGE|src, msg);
+	printk("[boot]%04x %s\n", src, msg);
+}
+
+/* Print a termination message (print only -- does not stop the kernel) */
+void ppc64_terminate_msg(unsigned int src, const char *msg)
+{
+	ppc64_do_msg(PPC64_LINUX_FUNCTION|PPC64_TERM_MESSAGE|src, msg);
+	printk("[terminate]%04x %s\n", src, msg);
+}
+
+/* This should only be called on processor 0 during calibrate decr */
+void __init setup_default_decr(void)
+{
+	struct paca_struct *lpaca = get_paca();
+
+	lpaca->default_decr = tb_ticks_per_jiffy;
+	lpaca->next_jiffy_update_tb = get_tb() + tb_ticks_per_jiffy;
+}
+
+#ifndef CONFIG_PPC_ISERIES
+/*
+ * This function can be used by platforms to "find" legacy serial ports.
+ * It works for "serial" nodes under an "isa" node, and will try to
+ * respect the "ibm,aix-loc" property if any. It works with up to 8
+ * ports.
+ */
+
+#define MAX_LEGACY_SERIAL_PORTS	8
+static struct plat_serial8250_port serial_ports[MAX_LEGACY_SERIAL_PORTS+1];
+static unsigned int old_serial_count;
+
+void __init generic_find_legacy_serial_ports(u64 *physport,
+		unsigned int *default_speed)
+{
+	struct device_node *np;
+	u32 *sizeprop;
+
+	struct isa_reg_property {
+		u32 space;
+		u32 address;
+		u32 size;
+	};
+	struct pci_reg_property {
+		struct pci_address addr;
+		u32 size_hi;
+		u32 size_lo;
+	};                                                                        
+
+	DBG(" -> generic_find_legacy_serial_port()\n");
+
+	*physport = 0;
+	if (default_speed)
+		*default_speed = 0;
+
+	np = of_find_node_by_path("/");
+	if (!np)
+		return;
+
+	/* First fill our array */
+	for (np = NULL; (np = of_find_node_by_type(np, "serial"));) {
+		struct device_node *isa, *pci;
+		struct isa_reg_property *reg;
+		unsigned long phys_size, addr_size, io_base;
+		u32 *rangesp;
+		u32 *interrupts, *clk, *spd;
+		char *typep;
+		int index, rlen, rentsize;
+
+		/* Ok, first check if it's under an "isa" parent */
+		isa = of_get_parent(np);
+		if (!isa || strcmp(isa->name, "isa")) {
+			DBG("%s: no isa parent found\n", np->full_name);
+			continue;
+		}
+		
+		/* Now look for an "ibm,aix-loc" property that gives us ordering
+		 * if any...
+		 */
+	 	typep = (char *)get_property(np, "ibm,aix-loc", NULL);
+
+		/* Get the ISA port number */
+		reg = (struct isa_reg_property *)get_property(np, "reg", NULL);	
+		if (reg == NULL)
+			goto next_port;
+		/* We assume the interrupt number isn't translated ... */
+		interrupts = (u32 *)get_property(np, "interrupts", NULL);
+		/* get clock freq. if present */
+		clk = (u32 *)get_property(np, "clock-frequency", NULL);
+		/* get default speed if present */
+		spd = (u32 *)get_property(np, "current-speed", NULL);
+		/* Default to locate at end of array */
+		index = old_serial_count; /* end of the array by default */
+
+		/* If we have a location index, then use it */
+		if (typep && *typep == 'S') {
+			index = simple_strtol(typep+1, NULL, 0) - 1;
+			/* if index is out of range, use end of array instead */
+			if (index >= MAX_LEGACY_SERIAL_PORTS)
+				index = old_serial_count;
+			/* if our index is still out of range, that mean that
+			 * array is full, we could scan for a free slot but that
+			 * make little sense to bother, just skip the port
+			 */
+			if (index >= MAX_LEGACY_SERIAL_PORTS)
+				goto next_port;
+			if (index >= old_serial_count)
+				old_serial_count = index + 1;
+			/* Check if there is a port who already claimed our slot */
+			if (serial_ports[index].iobase != 0) {
+				/* if we still have some room, move it, else override */
+				if (old_serial_count < MAX_LEGACY_SERIAL_PORTS) {
+					DBG("Moved legacy port %d -> %d\n", index,
+					    old_serial_count);
+					serial_ports[old_serial_count++] =
+						serial_ports[index];
+				} else {
+					DBG("Replacing legacy port %d\n", index);
+				}
+			}
+		}
+		if (index >= MAX_LEGACY_SERIAL_PORTS)
+			goto next_port;
+		if (index >= old_serial_count)
+			old_serial_count = index + 1;
+
+		/* Now fill the entry */
+		memset(&serial_ports[index], 0, sizeof(struct plat_serial8250_port));
+		serial_ports[index].uartclk = clk ? *clk : BASE_BAUD * 16;
+		serial_ports[index].iobase = reg->address;
+		serial_ports[index].irq = interrupts ? interrupts[0] : 0;
+		serial_ports[index].flags = ASYNC_BOOT_AUTOCONF;
+
+		DBG("Added legacy port, index: %d, port: %x, irq: %d, clk: %d\n",
+		    index,
+		    serial_ports[index].iobase,
+		    serial_ports[index].irq,
+		    serial_ports[index].uartclk);
+
+		/* Get phys address of IO reg for port 1 */
+		if (index != 0)
+			goto next_port;
+
+		pci = of_get_parent(isa);
+		if (!pci) {
+			DBG("%s: no pci parent found\n", np->full_name);
+			goto next_port;
+		}
+
+		rangesp = (u32 *)get_property(pci, "ranges", &rlen);
+		if (rangesp == NULL) {
+			of_node_put(pci);
+			goto next_port;
+		}
+		rlen /= 4;
+
+		/* we need the #size-cells of the PCI bridge node itself */
+		phys_size = 1;
+		sizeprop = (u32 *)get_property(pci, "#size-cells", NULL);
+		if (sizeprop != NULL)
+			phys_size = *sizeprop;
+		/* we need the parent #addr-cells */
+		addr_size = prom_n_addr_cells(pci);
+		rentsize = 3 + addr_size + phys_size;
+		io_base = 0;
+		for (;rlen >= rentsize; rlen -= rentsize,rangesp += rentsize) {
+			if (((rangesp[0] >> 24) & 0x3) != 1)
+				continue; /* not IO space */
+			io_base = rangesp[3];
+			if (addr_size == 2)
+				io_base = (io_base << 32) | rangesp[4];
+		}
+		if (io_base != 0) {
+			*physport = io_base + reg->address;
+			if (default_speed && spd)
+				*default_speed = *spd;
+		}
+		of_node_put(pci);
+	next_port:
+		of_node_put(isa);
+	}
+
+	DBG(" <- generic_find_legacy_serial_port()\n");
+}
+
+static struct platform_device serial_device = {
+	.name	= "serial8250",
+	.id	= PLAT8250_DEV_PLATFORM,
+	.dev	= {
+		.platform_data = serial_ports,
+	},
+};
+
+static int __init serial_dev_init(void)
+{
+	return platform_device_register(&serial_device);
+}
+arch_initcall(serial_dev_init);
+
+#endif /* CONFIG_PPC_ISERIES */
+
+int check_legacy_ioport(unsigned long base_port)
+{
+	if (ppc_md.check_legacy_ioport == NULL)
+		return 0;
+	return ppc_md.check_legacy_ioport(base_port);
+}
+EXPORT_SYMBOL(check_legacy_ioport);
+
+#ifdef CONFIG_XMON
+static int __init early_xmon(char *p)
+{
+	/* ensure xmon is enabled */
+	if (p) {
+		if (strncmp(p, "on", 2) == 0)
+			xmon_init(1);
+		if (strncmp(p, "off", 3) == 0)
+			xmon_init(0);
+		if (strncmp(p, "early", 5) != 0)
+			return 0;
+	}
+	xmon_init(1);
+	debugger(NULL);
+
+	return 0;
+}
+early_param("xmon", early_xmon);
+#endif
+
+void cpu_die(void)
+{
+	if (ppc_md.cpu_die)
+		ppc_md.cpu_die();
+}
