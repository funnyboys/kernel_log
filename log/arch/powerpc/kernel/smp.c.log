commit 65fddcfca8ad14778f71a57672fd01e8112d30fa
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:42 2020 -0700

    mm: reorder includes after introduction of linux/pgtable.h
    
    The replacement of <asm/pgrable.h> with <linux/pgtable.h> made the include
    of the latter in the middle of asm includes.  Fix this up with the aid of
    the below script and manual adjustments here and there.
    
            import sys
            import re
    
            if len(sys.argv) is not 3:
                print "USAGE: %s <file> <header>" % (sys.argv[0])
                sys.exit(1)
    
            hdr_to_move="#include <linux/%s>" % sys.argv[2]
            moved = False
            in_hdrs = False
    
            with open(sys.argv[1], "r") as f:
                lines = f.readlines()
                for _line in lines:
                    line = _line.rstrip('
    ')
                    if line == hdr_to_move:
                        continue
                    if line.startswith("#include <linux/"):
                        in_hdrs = True
                    elif not moved and in_hdrs:
                        moved = True
                        print hdr_to_move
                    print line
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 2146908ffb4c..73199470c265 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -33,6 +33,7 @@
 #include <linux/processor.h>
 #include <linux/random.h>
 #include <linux/stackprotector.h>
+#include <linux/pgtable.h>
 
 #include <asm/ptrace.h>
 #include <linux/atomic.h>
@@ -41,7 +42,6 @@
 #include <asm/kvm_ppc.h>
 #include <asm/dbell.h>
 #include <asm/page.h>
-#include <linux/pgtable.h>
 #include <asm/prom.h>
 #include <asm/smp.h>
 #include <asm/time.h>

commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index c820c95162ff..2146908ffb4c 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -41,7 +41,7 @@
 #include <asm/kvm_ppc.h>
 #include <asm/dbell.h>
 #include <asm/page.h>
-#include <asm/pgtable.h>
+#include <linux/pgtable.h>
 #include <asm/prom.h>
 #include <asm/smp.h>
 #include <asm/time.h>

commit 82a7cebdd95cffa55449d6c1d97cc9b743a66056
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri May 29 09:07:31 2020 +1000

    powerpc: Fix misleading small cores print
    
    Currently when we boot on a big core system, we get this print:
      [    0.040500] Using small cores at SMT level
    
    This is misleading as we've actually detected big cores.
    
    This patch clears up the print to say we've detect big cores but are
    using small cores for scheduling.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200528230731.1235752-1-mikey@neuling.org

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 6d2a3a3666f0..c820c95162ff 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -1383,7 +1383,7 @@ void __init smp_cpus_done(unsigned int max_cpus)
 
 #ifdef CONFIG_SCHED_SMT
 	if (has_big_cores) {
-		pr_info("Using small cores at SMT level\n");
+		pr_info("Big cores detected but using small core scheduling\n");
 		power9_topology[0].mask = smallcore_smt_mask;
 		powerpc_topology[0].mask = smallcore_smt_mask;
 	}

commit c72e8da06250390bb7759399a32fa0ab6f84e6d1
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Mar 13 22:20:20 2020 +1100

    powerpc/smp: Use IS_ENABLED() to avoid #ifdef
    
    We can avoid the #ifdef by using IS_ENABLED() in the existing
    condition check.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200313112020.28235-2-mpe@ellerman.id.au

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index aae61a3b3201..6d2a3a3666f0 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -1189,18 +1189,17 @@ int get_physical_package_id(int cpu)
 {
 	int pkg_id = cpu_to_chip_id(cpu);
 
-#ifdef CONFIG_PPC_SPLPAR
 	/*
 	 * If the platform is PowerNV or Guest on KVM, ibm,chip-id is
 	 * defined. Hence we would return the chip-id as the result of
 	 * get_physical_package_id.
 	 */
-	if (pkg_id == -1 && firmware_has_feature(FW_FEATURE_LPAR)) {
+	if (pkg_id == -1 && firmware_has_feature(FW_FEATURE_LPAR) &&
+	    IS_ENABLED(CONFIG_PPC_SPLPAR)) {
 		struct device_node *np = of_get_cpu_node(cpu, NULL);
 		pkg_id = of_node_to_nid(np);
 		of_node_put(np);
 	}
-#endif /* CONFIG_PPC_SPLPAR */
 
 	return pkg_id;
 }

commit 4b4d181d63518334070a877ba789211bde77da9e
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Mar 13 22:20:19 2020 +1100

    powerpc/smp: Drop superfluous NULL check
    
    We don't need the NULL check of np, the result is the same because the
    OF helpers cope with NULL, of_node_to_nid(NULL) == NUMA_NO_NODE (-1).
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200313112020.28235-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 37c12e3bab9e..aae61a3b3201 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -1197,11 +1197,8 @@ int get_physical_package_id(int cpu)
 	 */
 	if (pkg_id == -1 && firmware_has_feature(FW_FEATURE_LPAR)) {
 		struct device_node *np = of_get_cpu_node(cpu, NULL);
-
-		if (np) {
-			pkg_id = of_node_to_nid(np);
-			of_node_put(np);
-		}
+		pkg_id = of_node_to_nid(np);
+		of_node_put(np);
 	}
 #endif /* CONFIG_PPC_SPLPAR */
 

commit 247257b03b04398ca07da4bce3d17bee25d623cb
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Wed Jan 29 19:23:01 2020 +0530

    powerpc/numa: Remove late request for home node associativity
    
    With commit ("powerpc/numa: Early request for home node associativity"),
    commit 2ea626306810 ("powerpc/topology: Get topology for shared
    processors at boot") which was requesting home node associativity
    becomes redundant.
    
    Hence remove the late request for home node associativity.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Reported-by: Abdul Haleem <abdhalee@linux.vnet.ibm.com>
    Reviewed-by: Nathan Lynch <nathanl@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200129135301.24739-6-srikar@linux.vnet.ibm.com

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index f68cde82bdf3..37c12e3bab9e 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -1383,11 +1383,6 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	if (smp_ops && smp_ops->bringup_done)
 		smp_ops->bringup_done();
 
-	/*
-	 * On a shared LPAR, associativity needs to be requested.
-	 * Hence, get numa topology before dumping cpu topology
-	 */
-	shared_proc_topology_init();
 	dump_numa_cpu_topology();
 
 #ifdef CONFIG_SCHED_SMT

commit a05f0e5be4e81e4977d3f92aaf7688ee0cb7d5db
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Wed Jan 29 19:21:21 2020 +0530

    powerpc/smp: Use nid as fallback for package_id
    
    package_id is to match cores that are part of the same chip. On
    PowerNV machines, package_id defaults to chip_id. However ibm,chip_id
    property is not present in device-tree of PowerVM LPARs. Hence lscpu
    output shows one core per socket and multiple cores.
    
    To overcome this, use nid as the package_id on PowerVM LPARs.
    
    Before the patch:
    
      Architecture:        ppc64le
      Byte Order:          Little Endian
      CPU(s):              128
      On-line CPU(s) list: 0-127
      Thread(s) per core:  8
      Core(s) per socket:  1                     <----------------------
      Socket(s):           16                    <----------------------
      NUMA node(s):        2
      Model:               2.2 (pvr 004e 0202)
      Model name:          POWER9 (architected), altivec supported
      Hypervisor vendor:   pHyp
      Virtualization type: para
      L1d cache:           32K
      L1i cache:           32K
      L2 cache:            512K
      L3 cache:            10240K
      NUMA node0 CPU(s):   0-63
      NUMA node1 CPU(s):   64-127
      #
      # cat /sys/devices/system/cpu/cpu0/topology/physical_package_id
      -1
    
    After the patch:
    
      Architecture:        ppc64le
      Byte Order:          Little Endian
      CPU(s):              128
      On-line CPU(s) list: 0-127
      Thread(s) per core:  8                     <---------------------
      Core(s) per socket:  8                     <---------------------
      Socket(s):           2
      NUMA node(s):        2
      Model:               2.2 (pvr 004e 0202)
      Model name:          POWER9 (architected), altivec supported
      Hypervisor vendor:   pHyp
      Virtualization type: para
      L1d cache:           32K
      L1i cache:           32K
      L2 cache:            512K
      L3 cache:            10240K
      NUMA node0 CPU(s):   0-63
      NUMA node1 CPU(s):   64-127
      #
      # cat /sys/devices/system/cpu/cpu0/topology/physical_package_id
      0
    
    Now lscpu output is more in line with the system configuration.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    [mpe: Use pkg_id instead of ppid, tweak change log and comment]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200129135121.24617-1-srikar@linux.vnet.ibm.com

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index ea6adbf6a221..f68cde82bdf3 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -1185,10 +1185,34 @@ static inline void add_cpu_to_smallcore_masks(int cpu)
 	}
 }
 
+int get_physical_package_id(int cpu)
+{
+	int pkg_id = cpu_to_chip_id(cpu);
+
+#ifdef CONFIG_PPC_SPLPAR
+	/*
+	 * If the platform is PowerNV or Guest on KVM, ibm,chip-id is
+	 * defined. Hence we would return the chip-id as the result of
+	 * get_physical_package_id.
+	 */
+	if (pkg_id == -1 && firmware_has_feature(FW_FEATURE_LPAR)) {
+		struct device_node *np = of_get_cpu_node(cpu, NULL);
+
+		if (np) {
+			pkg_id = of_node_to_nid(np);
+			of_node_put(np);
+		}
+	}
+#endif /* CONFIG_PPC_SPLPAR */
+
+	return pkg_id;
+}
+EXPORT_SYMBOL_GPL(get_physical_package_id);
+
 static void add_cpu_to_masks(int cpu)
 {
 	int first_thread = cpu_first_thread_sibling(cpu);
-	int chipid = cpu_to_chip_id(cpu);
+	int pkg_id = get_physical_package_id(cpu);
 	int i;
 
 	/*
@@ -1217,11 +1241,11 @@ static void add_cpu_to_masks(int cpu)
 	for_each_cpu(i, cpu_l2_cache_mask(cpu))
 		set_cpus_related(cpu, i, cpu_core_mask);
 
-	if (chipid == -1)
+	if (pkg_id == -1)
 		return;
 
 	for_each_cpu(i, cpu_online_mask)
-		if (cpu_to_chip_id(i) == chipid)
+		if (get_physical_package_id(i) == pkg_id)
 			set_cpus_related(cpu, i, cpu_core_mask);
 }
 

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e784342bdaa1..ea6adbf6a221 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * SMP support for ppc.
  *
@@ -8,11 +9,6 @@
  *
  * PowerPC-64 Support added by Dave Engebretsen, Peter Bergner, and
  * Mike Corrigan {engebret|bergner|mikec}@us.ibm.com
- *
- *      This program is free software; you can redistribute it and/or
- *      modify it under the terms of the GNU General Public License
- *      as published by the Free Software Foundation; either version
- *      2 of the License, or (at your option) any later version.
  */
 
 #undef DEBUG

commit 7c19c2e5f9c18e364a306253065474e5f6ad960c
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:09:02 2019 +0000

    powerpc: 'current_set' is now a table of task_struct pointers
    
    The table of pointers 'current_set' has been used for retrieving
    the stack and current. They used to be thread_info pointers as
    they were pointing to the stack and current was taken from the
    'task' field of the thread_info.
    
    Now, the pointers of 'current_set' table are now both pointers
    to task_struct and pointers to thread_info.
    
    As they are used to get current, and the stack pointer is
    retrieved from current's stack field, this patch changes
    their type to task_struct, and renames secondary_ti to
    secondary_current.
    
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 96c25a89e877..e784342bdaa1 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -76,7 +76,7 @@
 static DEFINE_PER_CPU(int, cpu_state) = { 0 };
 #endif
 
-struct thread_info *secondary_ti;
+struct task_struct *secondary_current;
 bool has_big_cores;
 
 DEFINE_PER_CPU(cpumask_var_t, cpu_sibling_map);
@@ -631,7 +631,7 @@ void smp_send_stop(void)
 }
 #endif /* CONFIG_NMI_IPI */
 
-struct thread_info *current_set[NR_CPUS];
+struct task_struct *current_set[NR_CPUS];
 
 static void smp_store_cpu_info(int id)
 {
@@ -896,7 +896,7 @@ void smp_prepare_boot_cpu(void)
 	paca_ptrs[boot_cpuid]->__current = current;
 #endif
 	set_numa_node(numa_cpu_lookup_table[boot_cpuid]);
-	current_set[boot_cpuid] = task_thread_info(current);
+	current_set[boot_cpuid] = current;
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
@@ -981,15 +981,13 @@ static bool secondaries_inhibited(void)
 
 static void cpu_idle_thread_init(unsigned int cpu, struct task_struct *idle)
 {
-	struct thread_info *ti = task_thread_info(idle);
-
 #ifdef CONFIG_PPC64
 	paca_ptrs[cpu]->__current = idle;
 	paca_ptrs[cpu]->kstack = (unsigned long)task_stack_page(idle) +
 				 THREAD_SIZE - STACK_FRAME_OVERHEAD;
 #endif
 	idle->cpu = cpu;
-	secondary_ti = current_set[cpu] = ti;
+	secondary_current = current_set[cpu] = idle;
 }
 
 int __cpu_up(unsigned int cpu, struct task_struct *tidle)

commit ed1cd6deb013a11959d17a94e35ce159197632da
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:08:58 2019 +0000

    powerpc: Activate CONFIG_THREAD_INFO_IN_TASK
    
    This patch activates CONFIG_THREAD_INFO_IN_TASK which
    moves the thread_info into task_struct.
    
    Moving thread_info into task_struct has the following advantages:
      - It protects thread_info from corruption in the case of stack
        overflows.
      - Its address is harder to determine if stack addresses are leaked,
        making a number of attacks more difficult.
    
    This has the following consequences:
      - thread_info is now located at the beginning of task_struct.
      - The 'cpu' field is now in task_struct, and only exists when
        CONFIG_SMP is active.
      - thread_info doesn't have anymore the 'task' field.
    
    This patch:
      - Removes all recopy of thread_info struct when the stack changes.
      - Changes the CURRENT_THREAD_INFO() macro to point to current.
      - Selects CONFIG_THREAD_INFO_IN_TASK.
      - Modifies raw_smp_processor_id() to get ->cpu from current without
        including linux/sched.h to avoid circular inclusion and without
        including asm/asm-offsets.h to avoid symbol names duplication
        between ASM constants and C constants.
      - Modifies klp_init_thread_info() to take a task_struct pointer
        argument.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Add task_stack.h to livepatch.h to fix build fails]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 829ef5411b50..96c25a89e877 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -988,7 +988,7 @@ static void cpu_idle_thread_init(unsigned int cpu, struct task_struct *idle)
 	paca_ptrs[cpu]->kstack = (unsigned long)task_stack_page(idle) +
 				 THREAD_SIZE - STACK_FRAME_OVERHEAD;
 #endif
-	ti->cpu = cpu;
+	idle->cpu = cpu;
 	secondary_ti = current_set[cpu] = ti;
 }
 

commit 678c668a7732f3a11c25ae86d5737a019667e3c5
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 17 23:26:56 2019 +1100

    powerpc/64: Use task_stack_page() to initialise paca->kstack
    
    Rather than using the thread info use task_stack_page() to initialise
    paca->kstack, that way it will work with THREAD_INFO_IN_TASK.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Split out of larger patch]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 5366d9e7bed4..829ef5411b50 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -20,6 +20,7 @@
 #include <linux/kernel.h>
 #include <linux/export.h>
 #include <linux/sched/mm.h>
+#include <linux/sched/task_stack.h>
 #include <linux/sched/topology.h>
 #include <linux/smp.h>
 #include <linux/interrupt.h>
@@ -984,7 +985,8 @@ static void cpu_idle_thread_init(unsigned int cpu, struct task_struct *idle)
 
 #ifdef CONFIG_PPC64
 	paca_ptrs[cpu]->__current = idle;
-	paca_ptrs[cpu]->kstack = (unsigned long)ti + THREAD_SIZE - STACK_FRAME_OVERHEAD;
+	paca_ptrs[cpu]->kstack = (unsigned long)task_stack_page(idle) +
+				 THREAD_SIZE - STACK_FRAME_OVERHEAD;
 #endif
 	ti->cpu = cpu;
 	secondary_ti = current_set[cpu] = ti;

commit 6fe243fe5157076f3b8d88a02f064b41a4b7eec2
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Mon Nov 26 12:01:07 2018 +1000

    powerpc/smp: Make __smp_send_nmi_ipi() static
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 6e521a3f67ca..5366d9e7bed4 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -462,7 +462,8 @@ static void do_smp_send_nmi_ipi(int cpu, bool safe)
  * - delay_us > 0 is the delay before giving up waiting for targets to
  *   begin executing the handler, == 0 specifies indefinite delay.
  */
-int __smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us, bool safe)
+static int __smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *),
+				u64 delay_us, bool safe)
 {
 	unsigned long flags;
 	int me = raw_smp_processor_id();

commit 88b9a3d1425a436e95c41f09986fdae2daee437a
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Mon Nov 26 12:01:06 2018 +1000

    powerpc/smp: Fix NMI IPI xmon timeout
    
    The xmon debugger IPI handler waits in the callback function while
    xmon is still active. This means they don't complete the IPI, and the
    initiator always times out waiting for them.
    
    Things manage to work after the timeout because there is some fallback
    logic to keep NMI IPI state sane in case of the timeout, but this is a
    bit ugly.
    
    This patch changes NMI IPI back to half-asynchronous (i.e., wait for
    everyone to call in, do not wait for IPI function to complete), but
    the complexity is avoided by going one step further and allowing new
    IPIs to be issued before the IPI functions to all complete.
    
    If synchronization against that is required, it is left up to the
    caller, but current callers don't require that. In fact with the
    timeout handling, callers must be able to cope with this already.
    
    Fixes: 5b73151fff63 ("powerpc: NMI IPI make NMI IPIs fully sychronous")
    Cc: stable@vger.kernel.org # v4.19+
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 137196a4248b..6e521a3f67ca 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -358,13 +358,12 @@ void arch_send_call_function_ipi_mask(const struct cpumask *mask)
  * NMI IPIs may not be recoverable, so should not be used as ongoing part of
  * a running system. They can be used for crash, debug, halt/reboot, etc.
  *
- * NMI IPIs are globally single threaded. No more than one in progress at
- * any time.
- *
  * The IPI call waits with interrupts disabled until all targets enter the
- * NMI handler, then the call returns.
+ * NMI handler, then returns. Subsequent IPIs can be issued before targets
+ * have returned from their handlers, so there is no guarantee about
+ * concurrency or re-entrancy.
  *
- * No new NMI can be initiated until targets exit the handler.
+ * A new NMI can be issued before all targets exit the handler.
  *
  * The IPI call may time out without all targets entering the NMI handler.
  * In that case, there is some logic to recover (and ignore subsequent
@@ -375,7 +374,7 @@ void arch_send_call_function_ipi_mask(const struct cpumask *mask)
 
 static atomic_t __nmi_ipi_lock = ATOMIC_INIT(0);
 static struct cpumask nmi_ipi_pending_mask;
-static int nmi_ipi_busy_count = 0;
+static bool nmi_ipi_busy = false;
 static void (*nmi_ipi_function)(struct pt_regs *) = NULL;
 
 static void nmi_ipi_lock_start(unsigned long *flags)
@@ -414,7 +413,7 @@ static void nmi_ipi_unlock_end(unsigned long *flags)
  */
 int smp_handle_nmi_ipi(struct pt_regs *regs)
 {
-	void (*fn)(struct pt_regs *);
+	void (*fn)(struct pt_regs *) = NULL;
 	unsigned long flags;
 	int me = raw_smp_processor_id();
 	int ret = 0;
@@ -425,29 +424,17 @@ int smp_handle_nmi_ipi(struct pt_regs *regs)
 	 * because the caller may have timed out.
 	 */
 	nmi_ipi_lock_start(&flags);
-	if (!nmi_ipi_busy_count)
-		goto out;
-	if (!cpumask_test_cpu(me, &nmi_ipi_pending_mask))
-		goto out;
-
-	fn = nmi_ipi_function;
-	if (!fn)
-		goto out;
-
-	cpumask_clear_cpu(me, &nmi_ipi_pending_mask);
-	nmi_ipi_busy_count++;
-	nmi_ipi_unlock();
-
-	ret = 1;
-
-	fn(regs);
-
-	nmi_ipi_lock();
-	if (nmi_ipi_busy_count > 1) /* Can race with caller time-out */
-		nmi_ipi_busy_count--;
-out:
+	if (cpumask_test_cpu(me, &nmi_ipi_pending_mask)) {
+		cpumask_clear_cpu(me, &nmi_ipi_pending_mask);
+		fn = READ_ONCE(nmi_ipi_function);
+		WARN_ON_ONCE(!fn);
+		ret = 1;
+	}
 	nmi_ipi_unlock_end(&flags);
 
+	if (fn)
+		fn(regs);
+
 	return ret;
 }
 
@@ -473,7 +460,7 @@ static void do_smp_send_nmi_ipi(int cpu, bool safe)
  * - cpu is the target CPU (must not be this CPU), or NMI_IPI_ALL_OTHERS.
  * - fn is the target callback function.
  * - delay_us > 0 is the delay before giving up waiting for targets to
- *   complete executing the handler, == 0 specifies indefinite delay.
+ *   begin executing the handler, == 0 specifies indefinite delay.
  */
 int __smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us, bool safe)
 {
@@ -487,31 +474,33 @@ int __smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us, bool
 	if (unlikely(!smp_ops))
 		return 0;
 
-	/* Take the nmi_ipi_busy count/lock with interrupts hard disabled */
 	nmi_ipi_lock_start(&flags);
-	while (nmi_ipi_busy_count) {
+	while (nmi_ipi_busy) {
 		nmi_ipi_unlock_end(&flags);
-		spin_until_cond(nmi_ipi_busy_count == 0);
+		spin_until_cond(!nmi_ipi_busy);
 		nmi_ipi_lock_start(&flags);
 	}
-
+	nmi_ipi_busy = true;
 	nmi_ipi_function = fn;
 
+	WARN_ON_ONCE(!cpumask_empty(&nmi_ipi_pending_mask));
+
 	if (cpu < 0) {
 		/* ALL_OTHERS */
 		cpumask_copy(&nmi_ipi_pending_mask, cpu_online_mask);
 		cpumask_clear_cpu(me, &nmi_ipi_pending_mask);
 	} else {
-		/* cpumask starts clear */
 		cpumask_set_cpu(cpu, &nmi_ipi_pending_mask);
 	}
-	nmi_ipi_busy_count++;
+
 	nmi_ipi_unlock();
 
+	/* Interrupts remain hard disabled */
+
 	do_smp_send_nmi_ipi(cpu, safe);
 
 	nmi_ipi_lock();
-	/* nmi_ipi_busy_count is held here, so unlock/lock is okay */
+	/* nmi_ipi_busy is set here, so unlock/lock is okay */
 	while (!cpumask_empty(&nmi_ipi_pending_mask)) {
 		nmi_ipi_unlock();
 		udelay(1);
@@ -519,34 +508,19 @@ int __smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us, bool
 		if (delay_us) {
 			delay_us--;
 			if (!delay_us)
-				goto timeout;
+				break;
 		}
 	}
 
-	while (nmi_ipi_busy_count > 1) {
-		nmi_ipi_unlock();
-		udelay(1);
-		nmi_ipi_lock();
-		if (delay_us) {
-			delay_us--;
-			if (!delay_us)
-				goto timeout;
-		}
-	}
-
-timeout:
 	if (!cpumask_empty(&nmi_ipi_pending_mask)) {
 		/* Timeout waiting for CPUs to call smp_handle_nmi_ipi */
 		ret = 0;
 		cpumask_clear(&nmi_ipi_pending_mask);
 	}
-	if (nmi_ipi_busy_count > 1) {
-		/* Timeout waiting for CPUs to execute fn */
-		ret = 0;
-		nmi_ipi_busy_count = 1;
-	}
 
-	nmi_ipi_busy_count--;
+	nmi_ipi_function = NULL;
+	nmi_ipi_busy = false;
+
 	nmi_ipi_unlock_end(&flags);
 
 	return ret;
@@ -614,17 +588,8 @@ void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
 static void nmi_stop_this_cpu(struct pt_regs *regs)
 {
 	/*
-	 * This is a special case because it never returns, so the NMI IPI
-	 * handling would never mark it as done, which makes any later
-	 * smp_send_nmi_ipi() call spin forever. Mark it done now.
-	 *
 	 * IRQs are already hard disabled by the smp_handle_nmi_ipi.
 	 */
-	nmi_ipi_lock();
-	if (nmi_ipi_busy_count > 1)
-		nmi_ipi_busy_count--;
-	nmi_ipi_unlock();
-
 	spin_begin();
 	while (1)
 		spin_cpu_relax();

commit 1b5fc84aba170bdfe3533396ca9662ceea1609b7
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Mon Nov 26 12:01:05 2018 +1000

    powerpc/smp: Fix NMI IPI timeout
    
    The NMI IPI timeout logic is broken, if __smp_send_nmi_ipi() times out
    on the first condition, delay_us will be zero which will send it into
    the second spin loop with no timeout so it will spin forever.
    
    Fixes: 5b73151fff63 ("powerpc: NMI IPI make NMI IPIs fully sychronous")
    Cc: stable@vger.kernel.org # v4.19+
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 3f15edf25a0d..137196a4248b 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -519,7 +519,7 @@ int __smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us, bool
 		if (delay_us) {
 			delay_us--;
 			if (!delay_us)
-				break;
+				goto timeout;
 		}
 	}
 
@@ -530,10 +530,11 @@ int __smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us, bool
 		if (delay_us) {
 			delay_us--;
 			if (!delay_us)
-				break;
+				goto timeout;
 		}
 	}
 
+timeout:
 	if (!cpumask_empty(&nmi_ipi_pending_mask)) {
 		/* Timeout waiting for CPUs to call smp_handle_nmi_ipi */
 		ret = 0;

commit b6aeddea74b08518289fc86545297cf18a0b53a7
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Oct 19 16:19:10 2018 +1100

    powerpc: Fix stack protector crashes on CPU hotplug
    
    Recently in commit 7241d26e8175 ("powerpc/64: properly initialise
    the stackprotector canary on SMP.") we fixed a crash with stack
    protector on SMP by initialising the stack canary in
    cpu_idle_thread_init().
    
    But this can also causes crashes, when a CPU comes back online after
    being offline:
    
      Kernel panic - not syncing: stack-protector: Kernel stack is corrupted in: pnv_smp_cpu_kill_self+0x2a0/0x2b0
      CPU: 1 PID: 0 Comm: swapper/1 Not tainted 4.19.0-rc3-gcc-7.3.1-00168-g4ffe713b7587 #94
      Call Trace:
        dump_stack+0xb0/0xf4 (unreliable)
        panic+0x144/0x328
        __stack_chk_fail+0x2c/0x30
        pnv_smp_cpu_kill_self+0x2a0/0x2b0
        cpu_die+0x48/0x70
        arch_cpu_idle_dead+0x20/0x40
        do_idle+0x274/0x390
        cpu_startup_entry+0x38/0x50
        start_secondary+0x5e4/0x600
        start_secondary_prolog+0x10/0x14
    
    Looking at the stack we see that the canary value in the stack frame
    doesn't match the canary in the task/paca. That is because we have
    reinitialised the task/paca value, but then the CPU coming online has
    returned into a function using the old canary value. That causes the
    comparison to fail.
    
    Instead we can call boot_init_stack_canary() from start_secondary()
    which never returns. This is essentially what the generic code does in
    cpu_startup_entry() under #ifdef X86, we should make that non-x86
    specific in a future patch.
    
    Fixes: 7241d26e8175 ("powerpc/64: properly initialise the stackprotector canary on SMP.")
    Reported-by: Joel Stanley <joel@jms.id.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8e3a5da24d59..3f15edf25a0d 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -35,6 +35,7 @@
 #include <linux/profile.h>
 #include <linux/processor.h>
 #include <linux/random.h>
+#include <linux/stackprotector.h>
 
 #include <asm/ptrace.h>
 #include <linux/atomic.h>
@@ -1014,16 +1015,9 @@ static void cpu_idle_thread_init(unsigned int cpu, struct task_struct *idle)
 {
 	struct thread_info *ti = task_thread_info(idle);
 
-#ifdef CONFIG_STACKPROTECTOR
-	idle->stack_canary = get_random_canary();
-#endif
-
 #ifdef CONFIG_PPC64
 	paca_ptrs[cpu]->__current = idle;
 	paca_ptrs[cpu]->kstack = (unsigned long)ti + THREAD_SIZE - STACK_FRAME_OVERHEAD;
-#ifdef CONFIG_STACKPROTECTOR
-	paca_ptrs[cpu]->canary = idle->stack_canary;
-#endif
 #endif
 	ti->cpu = cpu;
 	secondary_ti = current_set[cpu] = ti;
@@ -1316,6 +1310,8 @@ void start_secondary(void *unused)
 	notify_cpu_starting(cpu);
 	set_cpu_online(cpu, true);
 
+	boot_init_stack_canary();
+
 	local_irq_enable();
 
 	/* We can enable ftrace for secondary cpus now */

commit 8e8a31d7fd54d68fc9c6c1e69f52ccdaf43b01ea
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Thu Oct 11 11:03:02 2018 +0530

    powerpc: Use cpu_smallcore_sibling_mask at SMT level on bigcores
    
    POWER9 SMT8 cores consist of two groups of threads, where threads in
    each group shares L1-cache. The scheduler is not aware of this
    distinction as the current sched-domain hierarchy has all the threads
    of the core defined at the SMT domain.
    
            SMT  [Thread siblings of the SMT8 core]
            DIE  [CPUs in the same die]
            NUMA [All the CPUs in the system]
    
    Due to this, we can observe run-to-run variance when we run a
    multi-threaded benchmark bound to a single core based on how the
    scheduler spreads the software threads across the two groups in the
    core.
    
    We fix this in this patch by defining each group of threads which
    share L1-cache to be the SMT level. The group of threads in the SMT8
    core is defined to be the CACHE level. The sched-domain hierarchy
    after this patch will be :
    
            SMT     [Thread siblings in the core that share L1 cache]
            CACHE   [Thread siblings that are in the SMT8 core]
            DIE     [CPUs in the same die]
            NUMA    [All the CPUs in the system]
    
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8d245ff059c9..8e3a5da24d59 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -1274,6 +1274,7 @@ static bool shared_caches;
 void start_secondary(void *unused)
 {
 	unsigned int cpu = smp_processor_id();
+	struct cpumask *(*sibling_mask)(int) = cpu_sibling_mask;
 
 	mmgrab(&init_mm);
 	current->active_mm = &init_mm;
@@ -1299,11 +1300,13 @@ void start_secondary(void *unused)
 	/* Update topology CPU masks */
 	add_cpu_to_masks(cpu);
 
+	if (has_big_cores)
+		sibling_mask = cpu_smallcore_mask;
 	/*
 	 * Check for any shared caches. Note that this must be done on a
 	 * per-core basis because one core in the pair might be disabled.
 	 */
-	if (!cpumask_equal(cpu_l2_cache_mask(cpu), cpu_sibling_mask(cpu)))
+	if (!cpumask_equal(cpu_l2_cache_mask(cpu), sibling_mask(cpu)))
 		shared_caches = true;
 
 	set_numa_node(numa_cpu_lookup_table[cpu]);
@@ -1370,6 +1373,13 @@ static const struct cpumask *shared_cache_mask(int cpu)
 	return cpu_l2_cache_mask(cpu);
 }
 
+#ifdef CONFIG_SCHED_SMT
+static const struct cpumask *smallcore_smt_mask(int cpu)
+{
+	return cpu_smallcore_mask(cpu);
+}
+#endif
+
 static struct sched_domain_topology_level power9_topology[] = {
 #ifdef CONFIG_SCHED_SMT
 	{ cpu_smt_mask, powerpc_smt_flags, SD_INIT_NAME(SMT) },
@@ -1397,6 +1407,13 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	shared_proc_topology_init();
 	dump_numa_cpu_topology();
 
+#ifdef CONFIG_SCHED_SMT
+	if (has_big_cores) {
+		pr_info("Using small cores at SMT level\n");
+		power9_topology[0].mask = smallcore_smt_mask;
+		powerpc_topology[0].mask = smallcore_smt_mask;
+	}
+#endif
 	/*
 	 * If any CPU detects that it's sharing a cache with another CPU then
 	 * use the deeper topology that is aware of this sharing.

commit 425752c63b6f3fed7b5a9cba2b8101a92cf36995
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Thu Oct 11 11:03:01 2018 +0530

    powerpc: Detect the presence of big-cores via "ibm, thread-groups"
    
    On IBM POWER9, the device tree exposes a property array identifed by
    "ibm,thread-groups" which will indicate which groups of threads share
    a particular set of resources.
    
    As of today we only have one form of grouping identifying the group of
    threads in the core that share the L1 cache, translation cache and
    instruction data flow.
    
    This patch adds helper functions to parse the contents of
    "ibm,thread-groups" and populate a per-cpu variable to cache
    information about siblings of each CPU that share the L1, traslation
    cache and instruction data-flow.
    
    It also defines a new global variable named "has_big_cores" which
    indicates if the cores on this configuration have multiple groups of
    threads that share L1 cache.
    
    For each online CPU, it maintains a cpu_smallcore_mask, which
    indicates the online siblings which share the L1-cache with it.
    
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e774d3bf3a03..8d245ff059c9 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -75,14 +75,32 @@ static DEFINE_PER_CPU(int, cpu_state) = { 0 };
 #endif
 
 struct thread_info *secondary_ti;
+bool has_big_cores;
 
 DEFINE_PER_CPU(cpumask_var_t, cpu_sibling_map);
+DEFINE_PER_CPU(cpumask_var_t, cpu_smallcore_map);
 DEFINE_PER_CPU(cpumask_var_t, cpu_l2_cache_map);
 DEFINE_PER_CPU(cpumask_var_t, cpu_core_map);
 
 EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
 EXPORT_PER_CPU_SYMBOL(cpu_l2_cache_map);
 EXPORT_PER_CPU_SYMBOL(cpu_core_map);
+EXPORT_SYMBOL_GPL(has_big_cores);
+
+#define MAX_THREAD_LIST_SIZE	8
+#define THREAD_GROUP_SHARE_L1   1
+struct thread_groups {
+	unsigned int property;
+	unsigned int nr_groups;
+	unsigned int threads_per_group;
+	unsigned int thread_list[MAX_THREAD_LIST_SIZE];
+};
+
+/*
+ * On big-cores system, cpu_l1_cache_map for each CPU corresponds to
+ * the set its siblings that share the L1-cache.
+ */
+DEFINE_PER_CPU(cpumask_var_t, cpu_l1_cache_map);
 
 /* SMP operations for this machine */
 struct smp_ops_t *smp_ops;
@@ -675,6 +693,185 @@ static void set_cpus_unrelated(int i, int j,
 }
 #endif
 
+/*
+ * parse_thread_groups: Parses the "ibm,thread-groups" device tree
+ *                      property for the CPU device node @dn and stores
+ *                      the parsed output in the thread_groups
+ *                      structure @tg if the ibm,thread-groups[0]
+ *                      matches @property.
+ *
+ * @dn: The device node of the CPU device.
+ * @tg: Pointer to a thread group structure into which the parsed
+ *      output of "ibm,thread-groups" is stored.
+ * @property: The property of the thread-group that the caller is
+ *            interested in.
+ *
+ * ibm,thread-groups[0..N-1] array defines which group of threads in
+ * the CPU-device node can be grouped together based on the property.
+ *
+ * ibm,thread-groups[0] tells us the property based on which the
+ * threads are being grouped together. If this value is 1, it implies
+ * that the threads in the same group share L1, translation cache.
+ *
+ * ibm,thread-groups[1] tells us how many such thread groups exist.
+ *
+ * ibm,thread-groups[2] tells us the number of threads in each such
+ * group.
+ *
+ * ibm,thread-groups[3..N-1] is the list of threads identified by
+ * "ibm,ppc-interrupt-server#s" arranged as per their membership in
+ * the grouping.
+ *
+ * Example: If ibm,thread-groups = [1,2,4,5,6,7,8,9,10,11,12] it
+ * implies that there are 2 groups of 4 threads each, where each group
+ * of threads share L1, translation cache.
+ *
+ * The "ibm,ppc-interrupt-server#s" of the first group is {5,6,7,8}
+ * and the "ibm,ppc-interrupt-server#s" of the second group is {9, 10,
+ * 11, 12} structure
+ *
+ * Returns 0 on success, -EINVAL if the property does not exist,
+ * -ENODATA if property does not have a value, and -EOVERFLOW if the
+ * property data isn't large enough.
+ */
+static int parse_thread_groups(struct device_node *dn,
+			       struct thread_groups *tg,
+			       unsigned int property)
+{
+	int i;
+	u32 thread_group_array[3 + MAX_THREAD_LIST_SIZE];
+	u32 *thread_list;
+	size_t total_threads;
+	int ret;
+
+	ret = of_property_read_u32_array(dn, "ibm,thread-groups",
+					 thread_group_array, 3);
+	if (ret)
+		return ret;
+
+	tg->property = thread_group_array[0];
+	tg->nr_groups = thread_group_array[1];
+	tg->threads_per_group = thread_group_array[2];
+	if (tg->property != property ||
+	    tg->nr_groups < 1 ||
+	    tg->threads_per_group < 1)
+		return -ENODATA;
+
+	total_threads = tg->nr_groups * tg->threads_per_group;
+
+	ret = of_property_read_u32_array(dn, "ibm,thread-groups",
+					 thread_group_array,
+					 3 + total_threads);
+	if (ret)
+		return ret;
+
+	thread_list = &thread_group_array[3];
+
+	for (i = 0 ; i < total_threads; i++)
+		tg->thread_list[i] = thread_list[i];
+
+	return 0;
+}
+
+/*
+ * get_cpu_thread_group_start : Searches the thread group in tg->thread_list
+ *                              that @cpu belongs to.
+ *
+ * @cpu : The logical CPU whose thread group is being searched.
+ * @tg : The thread-group structure of the CPU node which @cpu belongs
+ *       to.
+ *
+ * Returns the index to tg->thread_list that points to the the start
+ * of the thread_group that @cpu belongs to.
+ *
+ * Returns -1 if cpu doesn't belong to any of the groups pointed to by
+ * tg->thread_list.
+ */
+static int get_cpu_thread_group_start(int cpu, struct thread_groups *tg)
+{
+	int hw_cpu_id = get_hard_smp_processor_id(cpu);
+	int i, j;
+
+	for (i = 0; i < tg->nr_groups; i++) {
+		int group_start = i * tg->threads_per_group;
+
+		for (j = 0; j < tg->threads_per_group; j++) {
+			int idx = group_start + j;
+
+			if (tg->thread_list[idx] == hw_cpu_id)
+				return group_start;
+		}
+	}
+
+	return -1;
+}
+
+static int init_cpu_l1_cache_map(int cpu)
+
+{
+	struct device_node *dn = of_get_cpu_node(cpu, NULL);
+	struct thread_groups tg = {.property = 0,
+				   .nr_groups = 0,
+				   .threads_per_group = 0};
+	int first_thread = cpu_first_thread_sibling(cpu);
+	int i, cpu_group_start = -1, err = 0;
+
+	if (!dn)
+		return -ENODATA;
+
+	err = parse_thread_groups(dn, &tg, THREAD_GROUP_SHARE_L1);
+	if (err)
+		goto out;
+
+	zalloc_cpumask_var_node(&per_cpu(cpu_l1_cache_map, cpu),
+				GFP_KERNEL,
+				cpu_to_node(cpu));
+
+	cpu_group_start = get_cpu_thread_group_start(cpu, &tg);
+
+	if (unlikely(cpu_group_start == -1)) {
+		WARN_ON_ONCE(1);
+		err = -ENODATA;
+		goto out;
+	}
+
+	for (i = first_thread; i < first_thread + threads_per_core; i++) {
+		int i_group_start = get_cpu_thread_group_start(i, &tg);
+
+		if (unlikely(i_group_start == -1)) {
+			WARN_ON_ONCE(1);
+			err = -ENODATA;
+			goto out;
+		}
+
+		if (i_group_start == cpu_group_start)
+			cpumask_set_cpu(i, per_cpu(cpu_l1_cache_map, cpu));
+	}
+
+out:
+	of_node_put(dn);
+	return err;
+}
+
+static int init_big_cores(void)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		int err = init_cpu_l1_cache_map(cpu);
+
+		if (err)
+			return err;
+
+		zalloc_cpumask_var_node(&per_cpu(cpu_smallcore_map, cpu),
+					GFP_KERNEL,
+					cpu_to_node(cpu));
+	}
+
+	has_big_cores = true;
+	return 0;
+}
+
 void __init smp_prepare_cpus(unsigned int max_cpus)
 {
 	unsigned int cpu;
@@ -713,6 +910,12 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	cpumask_set_cpu(boot_cpuid, cpu_l2_cache_mask(boot_cpuid));
 	cpumask_set_cpu(boot_cpuid, cpu_core_mask(boot_cpuid));
 
+	init_big_cores();
+	if (has_big_cores) {
+		cpumask_set_cpu(boot_cpuid,
+				cpu_smallcore_mask(boot_cpuid));
+	}
+
 	if (smp_ops && smp_ops->probe)
 		smp_ops->probe();
 }
@@ -1003,10 +1206,28 @@ static void remove_cpu_from_masks(int cpu)
 		set_cpus_unrelated(cpu, i, cpu_core_mask);
 		set_cpus_unrelated(cpu, i, cpu_l2_cache_mask);
 		set_cpus_unrelated(cpu, i, cpu_sibling_mask);
+		if (has_big_cores)
+			set_cpus_unrelated(cpu, i, cpu_smallcore_mask);
 	}
 }
 #endif
 
+static inline void add_cpu_to_smallcore_masks(int cpu)
+{
+	struct cpumask *this_l1_cache_map = per_cpu(cpu_l1_cache_map, cpu);
+	int i, first_thread = cpu_first_thread_sibling(cpu);
+
+	if (!has_big_cores)
+		return;
+
+	cpumask_set_cpu(cpu, cpu_smallcore_mask(cpu));
+
+	for (i = first_thread; i < first_thread + threads_per_core; i++) {
+		if (cpu_online(i) && cpumask_test_cpu(i, this_l1_cache_map))
+			set_cpus_related(i, cpu, cpu_smallcore_mask);
+	}
+}
+
 static void add_cpu_to_masks(int cpu)
 {
 	int first_thread = cpu_first_thread_sibling(cpu);
@@ -1023,6 +1244,7 @@ static void add_cpu_to_masks(int cpu)
 		if (cpu_online(i))
 			set_cpus_related(i, cpu, cpu_sibling_mask);
 
+	add_cpu_to_smallcore_masks(cpu);
 	/*
 	 * Copy the thread sibling mask into the cache sibling mask
 	 * and mark any CPUs that share an L2 with this CPU.

commit 7241d26e8175e95290a6549a470c330dbfc63442
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Oct 13 09:45:12 2018 +0000

    powerpc/64: properly initialise the stackprotector canary on SMP.
    
    commit 06ec27aea9fc ("powerpc/64: add stack protector support")
    doesn't initialise the stack canary on SMP secondary CPU's paca,
    leading to the following false positive report from the
    stack protector.
    
    smp: Bringing up secondary CPUs ...
    Kernel panic - not syncing: stack-protector: Kernel stack is corrupted in: __schedule+0x978/0xa80
    CPU: 1 PID: 0 Comm: swapper/1 Not tainted 4.19.0-rc7-next-20181010-autotest-autotest #1
    Call Trace:
    [c000001fed5b3bf0] [c000000000a0ef3c] dump_stack+0xb0/0xf4 (unreliable)
    [c000001fed5b3c30] [c0000000000f9d68] panic+0x140/0x308
    [c000001fed5b3cc0] [c0000000000f9844] __stack_chk_fail+0x24/0x30
    [c000001fed5b3d20] [c000000000a2c3a8] __schedule+0x978/0xa80
    [c000001fed5b3e00] [c000000000a2c9b4] schedule_idle+0x34/0x60
    [c000001fed5b3e30] [c00000000013d344] do_idle+0x224/0x3d0
    [c000001fed5b3ec0] [c00000000013d6e0] cpu_startup_entry+0x30/0x50
    [c000001fed5b3ef0] [c000000000047f34] start_secondary+0x4d4/0x520
    [c000001fed5b3f90] [c00000000000b370] start_secondary_prolog+0x10/0x14
    
    This patch properly initialises the stack_canary of the secondary
    idle tasks.
    
    Reported-by: Abdul Haleem <abdhalee@linux.vnet.ibm.com>
    Fixes: 06ec27aea9fc ("powerpc/64: add stack protector support")
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 61c1fadbc644..e774d3bf3a03 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -34,6 +34,7 @@
 #include <linux/topology.h>
 #include <linux/profile.h>
 #include <linux/processor.h>
+#include <linux/random.h>
 
 #include <asm/ptrace.h>
 #include <linux/atomic.h>
@@ -810,9 +811,16 @@ static void cpu_idle_thread_init(unsigned int cpu, struct task_struct *idle)
 {
 	struct thread_info *ti = task_thread_info(idle);
 
+#ifdef CONFIG_STACKPROTECTOR
+	idle->stack_canary = get_random_canary();
+#endif
+
 #ifdef CONFIG_PPC64
 	paca_ptrs[cpu]->__current = idle;
 	paca_ptrs[cpu]->kstack = (unsigned long)ti + THREAD_SIZE - STACK_FRAME_OVERHEAD;
+#ifdef CONFIG_STACKPROTECTOR
+	paca_ptrs[cpu]->canary = idle->stack_canary;
+#endif
 #endif
 	ti->cpu = cpu;
 	secondary_ti = current_set[cpu] = ti;

commit 2ea62630681027c455117aa471ea3ab8bb099ead
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Fri Aug 17 20:24:39 2018 +0530

    powerpc/topology: Get topology for shared processors at boot
    
    On a shared LPAR, Phyp will not update the CPU associativity at boot
    time. Just after the boot system does recognize itself as a shared
    LPAR and trigger a request for correct CPU associativity. But by then
    the scheduler would have already created/destroyed its sched domains.
    
    This causes
      - Broken load balance across Nodes causing islands of cores.
      - Performance degradation esp if the system is lightly loaded
      - dmesg to wrongly report all CPUs to be in Node 0.
      - Messages in dmesg saying borken topology.
      - With commit 051f3ca02e46 ("sched/topology: Introduce NUMA identity
        node sched domain"), can cause rcu stalls at boot up.
    
    The sched_domains_numa_masks table which is used to generate cpumasks
    is only created at boot time just before creating sched domains and
    never updated. Hence, its better to get the topology correct before
    the sched domains are created.
    
    For example on 64 core Power 8 shared LPAR, dmesg reports
    
      Brought up 512 CPUs
      Node 0 CPUs: 0-511
      Node 1 CPUs:
      Node 2 CPUs:
      Node 3 CPUs:
      Node 4 CPUs:
      Node 5 CPUs:
      Node 6 CPUs:
      Node 7 CPUs:
      Node 8 CPUs:
      Node 9 CPUs:
      Node 10 CPUs:
      Node 11 CPUs:
      ...
      BUG: arch topology borken
           the DIE domain not a subset of the NUMA domain
      BUG: arch topology borken
           the DIE domain not a subset of the NUMA domain
    
    numactl/lscpu output will still be correct with cores spreading across
    all nodes:
    
      Socket(s):             64
      NUMA node(s):          12
      Model:                 2.0 (pvr 004d 0200)
      Model name:            POWER8 (architected), altivec supported
      Hypervisor vendor:     pHyp
      Virtualization type:   para
      L1d cache:             64K
      L1i cache:             32K
      NUMA node0 CPU(s): 0-7,32-39,64-71,96-103,176-183,272-279,368-375,464-471
      NUMA node1 CPU(s): 8-15,40-47,72-79,104-111,184-191,280-287,376-383,472-479
      NUMA node2 CPU(s): 16-23,48-55,80-87,112-119,192-199,288-295,384-391,480-487
      NUMA node3 CPU(s): 24-31,56-63,88-95,120-127,200-207,296-303,392-399,488-495
      NUMA node4 CPU(s):     208-215,304-311,400-407,496-503
      NUMA node5 CPU(s):     168-175,264-271,360-367,456-463
      NUMA node6 CPU(s):     128-135,224-231,320-327,416-423
      NUMA node7 CPU(s):     136-143,232-239,328-335,424-431
      NUMA node8 CPU(s):     216-223,312-319,408-415,504-511
      NUMA node9 CPU(s):     144-151,240-247,336-343,432-439
      NUMA node10 CPU(s):    152-159,248-255,344-351,440-447
      NUMA node11 CPU(s):    160-167,256-263,352-359,448-455
    
    Currently on this LPAR, the scheduler detects 2 levels of Numa and
    created numa sched domains for all CPUs, but it finds a single DIE
    domain consisting of all CPUs. Hence it deletes all numa sched
    domains.
    
    To address this, detect the shared processor and update topology soon
    after CPUs are setup so that correct topology is updated just before
    scheduler creates sched domain.
    
    With the fix, dmesg reports:
    
      numa: Node 0 CPUs: 0-7 32-39 64-71 96-103 176-183 272-279 368-375 464-471
      numa: Node 1 CPUs: 8-15 40-47 72-79 104-111 184-191 280-287 376-383 472-479
      numa: Node 2 CPUs: 16-23 48-55 80-87 112-119 192-199 288-295 384-391 480-487
      numa: Node 3 CPUs: 24-31 56-63 88-95 120-127 200-207 296-303 392-399 488-495
      numa: Node 4 CPUs: 208-215 304-311 400-407 496-503
      numa: Node 5 CPUs: 168-175 264-271 360-367 456-463
      numa: Node 6 CPUs: 128-135 224-231 320-327 416-423
      numa: Node 7 CPUs: 136-143 232-239 328-335 424-431
      numa: Node 8 CPUs: 216-223 312-319 408-415 504-511
      numa: Node 9 CPUs: 144-151 240-247 336-343 432-439
      numa: Node 10 CPUs: 152-159 248-255 344-351 440-447
      numa: Node 11 CPUs: 160-167 256-263 352-359 448-455
    
    and lscpu also reports:
    
      Socket(s):             64
      NUMA node(s):          12
      Model:                 2.0 (pvr 004d 0200)
      Model name:            POWER8 (architected), altivec supported
      Hypervisor vendor:     pHyp
      Virtualization type:   para
      L1d cache:             64K
      L1i cache:             32K
      NUMA node0 CPU(s): 0-7,32-39,64-71,96-103,176-183,272-279,368-375,464-471
      NUMA node1 CPU(s): 8-15,40-47,72-79,104-111,184-191,280-287,376-383,472-479
      NUMA node2 CPU(s): 16-23,48-55,80-87,112-119,192-199,288-295,384-391,480-487
      NUMA node3 CPU(s): 24-31,56-63,88-95,120-127,200-207,296-303,392-399,488-495
      NUMA node4 CPU(s):     208-215,304-311,400-407,496-503
      NUMA node5 CPU(s):     168-175,264-271,360-367,456-463
      NUMA node6 CPU(s):     128-135,224-231,320-327,416-423
      NUMA node7 CPU(s):     136-143,232-239,328-335,424-431
      NUMA node8 CPU(s):     216-223,312-319,408-415,504-511
      NUMA node9 CPU(s):     144-151,240-247,336-343,432-439
      NUMA node10 CPU(s):    152-159,248-255,344-351,440-447
      NUMA node11 CPU(s):    160-167,256-263,352-359,448-455
    
    Reported-by: Manjunatha H R <manjuhr1@in.ibm.com>
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    [mpe: Trim / format change log]
    Tested-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index b19d832ef386..61c1fadbc644 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -1160,6 +1160,11 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	if (smp_ops && smp_ops->bringup_done)
 		smp_ops->bringup_done();
 
+	/*
+	 * On a shared LPAR, associativity needs to be requested.
+	 * Hence, get numa topology before dumping cpu topology
+	 */
+	shared_proc_topology_init();
 	dump_numa_cpu_topology();
 
 	/*

commit 5b73151fff63fb019db8171cb81c6c978533844b
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Apr 25 15:17:59 2018 +1000

    powerpc: NMI IPI make NMI IPIs fully sychronous
    
    There is an asynchronous aspect to smp_send_nmi_ipi. The caller waits
    for all CPUs to call in to the handler, but it does not wait for
    completion of the handler. This is a needless complication, so remove
    it and always wait synchronously.
    
    The synchronous wait allows the caller to easily time out and clear
    the wait for completion (zero nmi_ipi_busy_count) in the case of badly
    behaved handlers. This would have prevented the recent smp_send_stop
    NMI IPI bug from causing the system to hang.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 4794d6b4f4d2..b19d832ef386 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -423,7 +423,8 @@ int smp_handle_nmi_ipi(struct pt_regs *regs)
 	fn(regs);
 
 	nmi_ipi_lock();
-	nmi_ipi_busy_count--;
+	if (nmi_ipi_busy_count > 1) /* Can race with caller time-out */
+		nmi_ipi_busy_count--;
 out:
 	nmi_ipi_unlock_end(&flags);
 
@@ -448,29 +449,11 @@ static void do_smp_send_nmi_ipi(int cpu, bool safe)
 	}
 }
 
-void smp_flush_nmi_ipi(u64 delay_us)
-{
-	unsigned long flags;
-
-	nmi_ipi_lock_start(&flags);
-	while (nmi_ipi_busy_count) {
-		nmi_ipi_unlock_end(&flags);
-		udelay(1);
-		if (delay_us) {
-			delay_us--;
-			if (!delay_us)
-				return;
-		}
-		nmi_ipi_lock_start(&flags);
-	}
-	nmi_ipi_unlock_end(&flags);
-}
-
 /*
  * - cpu is the target CPU (must not be this CPU), or NMI_IPI_ALL_OTHERS.
  * - fn is the target callback function.
  * - delay_us > 0 is the delay before giving up waiting for targets to
- *   enter the handler, == 0 specifies indefinite delay.
+ *   complete executing the handler, == 0 specifies indefinite delay.
  */
 int __smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us, bool safe)
 {
@@ -507,8 +490,23 @@ int __smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us, bool
 
 	do_smp_send_nmi_ipi(cpu, safe);
 
+	nmi_ipi_lock();
+	/* nmi_ipi_busy_count is held here, so unlock/lock is okay */
 	while (!cpumask_empty(&nmi_ipi_pending_mask)) {
+		nmi_ipi_unlock();
 		udelay(1);
+		nmi_ipi_lock();
+		if (delay_us) {
+			delay_us--;
+			if (!delay_us)
+				break;
+		}
+	}
+
+	while (nmi_ipi_busy_count > 1) {
+		nmi_ipi_unlock();
+		udelay(1);
+		nmi_ipi_lock();
 		if (delay_us) {
 			delay_us--;
 			if (!delay_us)
@@ -516,12 +514,17 @@ int __smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us, bool
 		}
 	}
 
-	nmi_ipi_lock();
 	if (!cpumask_empty(&nmi_ipi_pending_mask)) {
-		/* Could not gather all CPUs */
+		/* Timeout waiting for CPUs to call smp_handle_nmi_ipi */
 		ret = 0;
 		cpumask_clear(&nmi_ipi_pending_mask);
 	}
+	if (nmi_ipi_busy_count > 1) {
+		/* Timeout waiting for CPUs to execute fn */
+		ret = 0;
+		nmi_ipi_busy_count = 1;
+	}
+
 	nmi_ipi_busy_count--;
 	nmi_ipi_unlock_end(&flags);
 
@@ -597,7 +600,8 @@ static void nmi_stop_this_cpu(struct pt_regs *regs)
 	 * IRQs are already hard disabled by the smp_handle_nmi_ipi.
 	 */
 	nmi_ipi_lock();
-	nmi_ipi_busy_count--;
+	if (nmi_ipi_busy_count > 1)
+		nmi_ipi_busy_count--;
 	nmi_ipi_unlock();
 
 	spin_begin();

commit de6e5d38417e6cdb005843db420a2974993d36ff
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 19 14:35:53 2018 +1000

    powerpc: smp_send_stop do not offline stopped CPUs
    
    Marking CPUs stopped by smp_send_stop as offline can cause warnings
    due to cross-CPU wakeups. This trace was noticed on a busy system
    running a sysrq+c crash test, after the injected crash:
    
    WARNING: CPU: 51 PID: 1546 at kernel/sched/core.c:1179 set_task_cpu+0x22c/0x240
    CPU: 51 PID: 1546 Comm: kworker/u352:1 Tainted: G      D
    Workqueue: mlx5e mlx5e_update_stats_work [mlx5_core]
    [...]
    NIP [c00000000017c21c] set_task_cpu+0x22c/0x240
    LR [c00000000017d580] try_to_wake_up+0x230/0x720
    Call Trace:
    [c000000001017700] runqueues+0x0/0xb00 (unreliable)
    [c00000000017d580] try_to_wake_up+0x230/0x720
    [c00000000015a214] insert_work+0x104/0x140
    [c00000000015adb0] __queue_work+0x230/0x690
    [c000003fc5007910] [c00000000015b26c] queue_work_on+0x5c/0x90
    [c0080000135fc8f8] mlx5_cmd_exec+0x538/0xcb0 [mlx5_core]
    [c008000013608fd0] mlx5_core_access_reg+0x140/0x1d0 [mlx5_core]
    [c00800001362777c] mlx5e_update_pport_counters.constprop.59+0x6c/0x90 [mlx5_core]
    [c008000013628868] mlx5e_update_ndo_stats+0x28/0x90 [mlx5_core]
    [c008000013625558] mlx5e_update_stats_work+0x68/0xb0 [mlx5_core]
    [c00000000015bcec] process_one_work+0x1bc/0x5f0
    [c00000000015ecac] worker_thread+0xac/0x6b0
    [c000000000168338] kthread+0x168/0x1b0
    [c00000000000b628] ret_from_kernel_thread+0x5c/0xb4
    
    This happens because firstly the CPU is not really offline in the
    usual sense, processes and interrupts have not been migrated away.
    Secondly smp_send_stop does not happen atomically on all CPUs, so
    one CPU can have marked itself offline, while another CPU is still
    running processes or interrupts which can affect the first CPU.
    
    Fix this by just not marking the CPU as offline. It's more like
    frozen in time, so offline does not really reflect its state properly
    anyway. There should be nothing in the crash/panic path that walks
    online CPUs and synchronously waits for them, so this change should
    not introduce new hangs.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 5eadfffabe35..4794d6b4f4d2 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -600,9 +600,6 @@ static void nmi_stop_this_cpu(struct pt_regs *regs)
 	nmi_ipi_busy_count--;
 	nmi_ipi_unlock();
 
-	/* Remove this CPU */
-	set_cpu_online(smp_processor_id(), false);
-
 	spin_begin();
 	while (1)
 		spin_cpu_relax();
@@ -617,9 +614,6 @@ void smp_send_stop(void)
 
 static void stop_this_cpu(void *dummy)
 {
-	/* Remove this CPU */
-	set_cpu_online(smp_processor_id(), false);
-
 	hard_irq_disable();
 	spin_begin();
 	while (1)

commit 6ba55716a24f5f399ad4d37685e4bb721f8e6dd5
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed May 2 23:07:27 2018 +1000

    powerpc/nmi: Add an API for sending "safe" NMIs
    
    Currently the options we have for sending NMIs are not necessarily
    safe, that is they can potentially interrupt a CPU in a
    non-recoverable region of code, meaning the kernel must then panic().
    
    But we'd like to use smp_send_nmi_ipi() to do cross-CPU calls in
    situations where we don't want to risk a panic(), because it doesn't
    have the requirement that interrupts must be enabled like
    smp_call_function().
    
    So add an API for the caller to indicate that it wants to use the NMI
    infrastructure, but doesn't want to do anything "unsafe".
    
    Currently that is implemented by not actually calling cause_nmi_ipi(),
    instead falling back to an IPI. In future we can pass the safe
    parameter down to cause_nmi_ipi() and the individual backends can
    potentially take it into account before deciding what to do.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index b009a562c76b..5eadfffabe35 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -430,9 +430,9 @@ int smp_handle_nmi_ipi(struct pt_regs *regs)
 	return ret;
 }
 
-static void do_smp_send_nmi_ipi(int cpu)
+static void do_smp_send_nmi_ipi(int cpu, bool safe)
 {
-	if (smp_ops->cause_nmi_ipi && smp_ops->cause_nmi_ipi(cpu))
+	if (!safe && smp_ops->cause_nmi_ipi && smp_ops->cause_nmi_ipi(cpu))
 		return;
 
 	if (cpu >= 0) {
@@ -472,7 +472,7 @@ void smp_flush_nmi_ipi(u64 delay_us)
  * - delay_us > 0 is the delay before giving up waiting for targets to
  *   enter the handler, == 0 specifies indefinite delay.
  */
-int smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us)
+int __smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us, bool safe)
 {
 	unsigned long flags;
 	int me = raw_smp_processor_id();
@@ -505,7 +505,7 @@ int smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us)
 	nmi_ipi_busy_count++;
 	nmi_ipi_unlock();
 
-	do_smp_send_nmi_ipi(cpu);
+	do_smp_send_nmi_ipi(cpu, safe);
 
 	while (!cpumask_empty(&nmi_ipi_pending_mask)) {
 		udelay(1);
@@ -527,6 +527,16 @@ int smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us)
 
 	return ret;
 }
+
+int smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us)
+{
+	return __smp_send_nmi_ipi(cpu, fn, delay_us, false);
+}
+
+int smp_send_safe_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us)
+{
+	return __smp_send_nmi_ipi(cpu, fn, delay_us, true);
+}
 #endif /* CONFIG_NMI_IPI */
 
 #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
@@ -570,7 +580,7 @@ void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
 			 * entire NMI dance and waiting for
 			 * cpus to clear pending mask, etc.
 			 */
-			do_smp_send_nmi_ipi(cpu);
+			do_smp_send_nmi_ipi(cpu, false);
 		}
 	}
 }

commit 21bfd6a8e9999f40f9eae09ca6ba33e7f75f0be4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:34 2018 +1000

    powerpc: move a stray NMI IPI case under NMI_IPI ifdef
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 6f5e3a6e259c..b009a562c76b 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -194,7 +194,9 @@ const char *smp_ipi_name[] = {
 #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 	[PPC_MSG_TICK_BROADCAST] = "ipi tick-broadcast",
 #endif
+#ifdef CONFIG_NMI_IPI
 	[PPC_MSG_NMI_IPI] = "nmi ipi",
+#endif
 };
 
 /* optional function to request ipi, for controllers with >= 4 ipis */

commit bc9071133144acdbdb28cfc6ee5ce983d8fd5f81
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:33 2018 +1000

    powerpc: move timer broadcast code under GENERIC_CLOCKEVENTS_BROADCAST ifdef
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index f66eec89c14c..6f5e3a6e259c 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -156,11 +156,13 @@ static irqreturn_t reschedule_action(int irq, void *data)
 	return IRQ_HANDLED;
 }
 
+#ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 static irqreturn_t tick_broadcast_ipi_action(int irq, void *data)
 {
 	timer_broadcast_interrupt();
 	return IRQ_HANDLED;
 }
+#endif
 
 #ifdef CONFIG_NMI_IPI
 static irqreturn_t nmi_ipi_action(int irq, void *data)
@@ -173,7 +175,9 @@ static irqreturn_t nmi_ipi_action(int irq, void *data)
 static irq_handler_t smp_ipi_action[] = {
 	[PPC_MSG_CALL_FUNCTION] =  call_function_action,
 	[PPC_MSG_RESCHEDULE] = reschedule_action,
+#ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 	[PPC_MSG_TICK_BROADCAST] = tick_broadcast_ipi_action,
+#endif
 #ifdef CONFIG_NMI_IPI
 	[PPC_MSG_NMI_IPI] = nmi_ipi_action,
 #endif
@@ -187,7 +191,9 @@ static irq_handler_t smp_ipi_action[] = {
 const char *smp_ipi_name[] = {
 	[PPC_MSG_CALL_FUNCTION] =  "ipi call function",
 	[PPC_MSG_RESCHEDULE] = "ipi reschedule",
+#ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 	[PPC_MSG_TICK_BROADCAST] = "ipi tick-broadcast",
+#endif
 	[PPC_MSG_NMI_IPI] = "nmi ipi",
 };
 
@@ -278,8 +284,10 @@ irqreturn_t smp_ipi_demux_relaxed(void)
 			generic_smp_call_function_interrupt();
 		if (all & IPI_MESSAGE(PPC_MSG_RESCHEDULE))
 			scheduler_ipi();
+#ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 		if (all & IPI_MESSAGE(PPC_MSG_TICK_BROADCAST))
 			timer_broadcast_interrupt();
+#endif
 #ifdef CONFIG_NMI_IPI
 		if (all & IPI_MESSAGE(PPC_MSG_NMI_IPI))
 			nmi_ipi_action(0, NULL);

commit 3f984620f9a4fe089c0a3c951b75a460211394bb
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:31 2018 +1000

    powerpc: generic clockevents broadcast receiver call tick_receive_broadcast
    
    The broadcast tick recipient can call tick_receive_broadcast rather
    than re-running the full timer interrupt.
    
    It does not have to check for the next event time, because the sender
    already determined the timer has expired. It does not have to test
    irq_work_pending, because that's a direct decrementer interrupt and
    does not go through the clock events subsystem. And it does not have
    to read PURR because that was removed with the previous patch.
    
    This results in no code size change, but both the decrementer and
    broadcast path lengths are reduced.
    
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index c96f8fbc1942..f66eec89c14c 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -158,7 +158,7 @@ static irqreturn_t reschedule_action(int irq, void *data)
 
 static irqreturn_t tick_broadcast_ipi_action(int irq, void *data)
 {
-	tick_broadcast_ipi_handler();
+	timer_broadcast_interrupt();
 	return IRQ_HANDLED;
 }
 
@@ -279,7 +279,7 @@ irqreturn_t smp_ipi_demux_relaxed(void)
 		if (all & IPI_MESSAGE(PPC_MSG_RESCHEDULE))
 			scheduler_ipi();
 		if (all & IPI_MESSAGE(PPC_MSG_TICK_BROADCAST))
-			tick_broadcast_ipi_handler();
+			timer_broadcast_interrupt();
 #ifdef CONFIG_NMI_IPI
 		if (all & IPI_MESSAGE(PPC_MSG_NMI_IPI))
 			nmi_ipi_action(0, NULL);

commit 424ef0160f439feb2a1a6e796a281e2bfa7b6997
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Thu Apr 19 12:34:04 2018 +0530

    powerpc64/ftrace: Disable ftrace during hotplug
    
    Disable ftrace when a cpu is about to go offline. When the cpu is woken
    up, ftrace will get enabled in start_secondary().
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 9e711cdbe384..c96f8fbc1942 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -1166,6 +1166,8 @@ int __cpu_disable(void)
 	if (!smp_ops->cpu_disable)
 		return -ENOSYS;
 
+	this_cpu_disable_ftrace();
+
 	err = smp_ops->cpu_disable();
 	if (err)
 		return err;
@@ -1184,6 +1186,12 @@ void __cpu_die(unsigned int cpu)
 
 void cpu_die(void)
 {
+	/*
+	 * Disable on the down path. This will be re-enabled by
+	 * start_secondary() via start_secondary_resume() below
+	 */
+	this_cpu_disable_ftrace();
+
 	if (ppc_md.cpu_die)
 		ppc_md.cpu_die();
 

commit d103978636c27fce216bbc8bb289981047b71bd4
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Thu Apr 19 12:34:03 2018 +0530

    powerpc64/ftrace: Delay enabling ftrace on secondary cpus
    
    On the boot cpu, though we enable paca->ftrace_enabled in early_setup()
    (via cpu_ready_for_interrupts()), we don't start tracing until much
    later since ftrace is not initialized yet and since we only support
    DYNAMIC_FTRACE on powerpc. However, it is possible that ftrace has been
    initialized by the time some of the secondary cpus start up. In this
    case, we will try to trace some of the early boot code which can cause
    problems.
    
    To address this, move setting paca->ftrace_enabled from
    cpu_ready_for_interrupts() to early_setup() for the boot cpu, and towards
    the end of start_secondary() for secondary cpus.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 9ca7148b5881..9e711cdbe384 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -59,6 +59,7 @@
 #include <asm/kexec.h>
 #include <asm/asm-prototypes.h>
 #include <asm/cpu_has_feature.h>
+#include <asm/ftrace.h>
 
 #ifdef DEBUG
 #include <asm/udbg.h>
@@ -1066,6 +1067,9 @@ void start_secondary(void *unused)
 
 	local_irq_enable();
 
+	/* We can enable ftrace for secondary cpus now */
+	this_cpu_enable_ftrace();
+
 	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
 
 	BUG();

commit 6029755eed95e5c90f763188c87ae3ff41e48e5c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Apr 27 11:51:59 2018 +1000

    powerpc: Fix deadlock with multiple calls to smp_send_stop
    
    smp_send_stop can lock up the IPI path for any subsequent calls,
    because the receiving CPUs spin in their handler function. This
    started becoming a problem with the addition of an smp_send_stop
    call in the reboot path, because panics can reboot after doing
    their own smp_send_stop.
    
    The NMI IPI variant was fixed with ac61c11566 ("powerpc: Fix
    smp_send_stop NMI IPI handling"), which leaves the smp_call_function
    variant.
    
    This is fixed by having smp_send_stop only ever do the
    smp_call_function once. This is a bit less robust than the NMI IPI
    fix, because any other call to smp_call_function after smp_send_stop
    could deadlock, but that has always been the case, and it was not
    been a problem before.
    
    Fixes: f2748bdfe1573 ("powerpc/powernv: Always stop secondaries before reboot/shutdown")
    Reported-by: Abdul Haleem <abdhalee@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 3582f30b60b7..9ca7148b5881 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -565,17 +565,6 @@ void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
 }
 #endif
 
-static void stop_this_cpu(void *dummy)
-{
-	/* Remove this CPU */
-	set_cpu_online(smp_processor_id(), false);
-
-	hard_irq_disable();
-	spin_begin();
-	while (1)
-		spin_cpu_relax();
-}
-
 #ifdef CONFIG_NMI_IPI
 static void nmi_stop_this_cpu(struct pt_regs *regs)
 {
@@ -583,23 +572,57 @@ static void nmi_stop_this_cpu(struct pt_regs *regs)
 	 * This is a special case because it never returns, so the NMI IPI
 	 * handling would never mark it as done, which makes any later
 	 * smp_send_nmi_ipi() call spin forever. Mark it done now.
+	 *
+	 * IRQs are already hard disabled by the smp_handle_nmi_ipi.
 	 */
 	nmi_ipi_lock();
 	nmi_ipi_busy_count--;
 	nmi_ipi_unlock();
 
-	stop_this_cpu(NULL);
+	/* Remove this CPU */
+	set_cpu_online(smp_processor_id(), false);
+
+	spin_begin();
+	while (1)
+		spin_cpu_relax();
 }
-#endif
 
 void smp_send_stop(void)
 {
-#ifdef CONFIG_NMI_IPI
 	smp_send_nmi_ipi(NMI_IPI_ALL_OTHERS, nmi_stop_this_cpu, 1000000);
-#else
+}
+
+#else /* CONFIG_NMI_IPI */
+
+static void stop_this_cpu(void *dummy)
+{
+	/* Remove this CPU */
+	set_cpu_online(smp_processor_id(), false);
+
+	hard_irq_disable();
+	spin_begin();
+	while (1)
+		spin_cpu_relax();
+}
+
+void smp_send_stop(void)
+{
+	static bool stopped = false;
+
+	/*
+	 * Prevent waiting on csd lock from a previous smp_send_stop.
+	 * This is racy, but in general callers try to do the right
+	 * thing and only fire off one smp_send_stop (e.g., see
+	 * kernel/panic.c)
+	 */
+	if (stopped)
+		return;
+
+	stopped = true;
+
 	smp_call_function(stop_this_cpu, NULL, 0);
-#endif
 }
+#endif /* CONFIG_NMI_IPI */
 
 struct thread_info *current_set[NR_CPUS];
 

commit ac61c1156623455c46701654abd8c99720bceea1
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Apr 25 12:17:53 2018 +1000

    powerpc: Fix smp_send_stop NMI IPI handling
    
    The NMI IPI handler for a receiving CPU increments nmi_ipi_busy_count
    over the handler function call, which causes later smp_send_nmi_ipi()
    callers to spin until the call is finished.
    
    The stop_this_cpu() function never returns, so the busy count is never
    decremeted, which can cause the system to hang in some cases. For
    example panic() will call smp_send_stop() early on which calls
    stop_this_cpu() on other CPUs, then later in the reboot path,
    pnv_restart() will call smp_send_stop() again, which hangs.
    
    Fix this by adding a special case to the stop_this_cpu() handler to
    decrement the busy count, because it will never return.
    
    Now that the NMI/non-NMI versions of stop_this_cpu() are different,
    split them out into separate functions rather than doing #ifdef tricks
    to share the body between the two functions.
    
    Fixes: 6bed3237624e3 ("powerpc: use NMI IPI for smp_send_stop")
    Reported-by: Abdul Haleem <abdhalee@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Split out the functions, tweak change log a bit]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e16ec7b3b427..3582f30b60b7 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -565,11 +565,7 @@ void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
 }
 #endif
 
-#ifdef CONFIG_NMI_IPI
-static void stop_this_cpu(struct pt_regs *regs)
-#else
 static void stop_this_cpu(void *dummy)
-#endif
 {
 	/* Remove this CPU */
 	set_cpu_online(smp_processor_id(), false);
@@ -580,10 +576,26 @@ static void stop_this_cpu(void *dummy)
 		spin_cpu_relax();
 }
 
+#ifdef CONFIG_NMI_IPI
+static void nmi_stop_this_cpu(struct pt_regs *regs)
+{
+	/*
+	 * This is a special case because it never returns, so the NMI IPI
+	 * handling would never mark it as done, which makes any later
+	 * smp_send_nmi_ipi() call spin forever. Mark it done now.
+	 */
+	nmi_ipi_lock();
+	nmi_ipi_busy_count--;
+	nmi_ipi_unlock();
+
+	stop_this_cpu(NULL);
+}
+#endif
+
 void smp_send_stop(void)
 {
 #ifdef CONFIG_NMI_IPI
-	smp_send_nmi_ipi(NMI_IPI_ALL_OTHERS, stop_this_cpu, 1000000);
+	smp_send_nmi_ipi(NMI_IPI_ALL_OTHERS, nmi_stop_this_cpu, 1000000);
 #else
 	smp_call_function(stop_this_cpu, NULL, 0);
 #endif

commit 855bfe0de1a05a01f89975ea8ba9f5521fb0f567
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sun Apr 1 20:36:14 2018 +1000

    powerpc: hard disable irqs in smp_send_stop loop
    
    The hard lockup watchdog can fire under local_irq_disable
    on platforms with irq soft masking.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index db88660bf6bd..e16ec7b3b427 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -574,9 +574,10 @@ static void stop_this_cpu(void *dummy)
 	/* Remove this CPU */
 	set_cpu_online(smp_processor_id(), false);
 
-	local_irq_disable();
+	hard_irq_disable();
+	spin_begin();
 	while (1)
-		;
+		spin_cpu_relax();
 }
 
 void smp_send_stop(void)

commit 6bed3237624e3faad1592543952907cd01a42c83
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sun Apr 1 20:36:13 2018 +1000

    powerpc: use NMI IPI for smp_send_stop
    
    Use the NMI IPI rather than smp_call_function for smp_send_stop.
    Have stopped CPUs hard disable interrupts rather than just soft
    disable.
    
    This function is used in crash/panic/shutdown paths to bring other
    CPUs down as quickly and reliably as possible, and minimizing their
    potential to cause trouble.
    
    Avoiding the Linux smp_call_function infrastructure and (if supported)
    using true NMI IPIs makes this more robust.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index cfc08b099c49..db88660bf6bd 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -565,7 +565,11 @@ void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
 }
 #endif
 
+#ifdef CONFIG_NMI_IPI
+static void stop_this_cpu(struct pt_regs *regs)
+#else
 static void stop_this_cpu(void *dummy)
+#endif
 {
 	/* Remove this CPU */
 	set_cpu_online(smp_processor_id(), false);
@@ -577,7 +581,11 @@ static void stop_this_cpu(void *dummy)
 
 void smp_send_stop(void)
 {
+#ifdef CONFIG_NMI_IPI
+	smp_send_nmi_ipi(NMI_IPI_ALL_OTHERS, stop_this_cpu, 1000000);
+#else
 	smp_call_function(stop_this_cpu, NULL, 0);
+#endif
 }
 
 struct thread_info *current_set[NR_CPUS];

commit d2e60075a3d4422dc54b919f3b125d8066b839d4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:12 2018 +1000

    powerpc/64: Use array of paca pointers and allocate pacas individually
    
    Change the paca array into an array of pointers to pacas. Allocate
    pacas individually.
    
    This allows flexibility in where the PACAs are allocated. Future work
    will allocate them node-local. Platforms that don't have address limits
    on PACAs would be able to defer PACA allocations until later in boot
    rather than allocate all possible ones up-front then freeing unused.
    
    This is slightly more overhead (one additional indirection) for cross
    CPU paca references, but those aren't too common.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index bbe7634b3a43..cfc08b099c49 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -123,8 +123,8 @@ int smp_generic_kick_cpu(int nr)
 	 * cpu_start field to become non-zero After we set cpu_start,
 	 * the processor will continue on to secondary_start
 	 */
-	if (!paca[nr].cpu_start) {
-		paca[nr].cpu_start = 1;
+	if (!paca_ptrs[nr]->cpu_start) {
+		paca_ptrs[nr]->cpu_start = 1;
 		smp_mb();
 		return 0;
 	}
@@ -657,7 +657,7 @@ void smp_prepare_boot_cpu(void)
 {
 	BUG_ON(smp_processor_id() != boot_cpuid);
 #ifdef CONFIG_PPC64
-	paca[boot_cpuid].__current = current;
+	paca_ptrs[boot_cpuid]->__current = current;
 #endif
 	set_numa_node(numa_cpu_lookup_table[boot_cpuid]);
 	current_set[boot_cpuid] = task_thread_info(current);
@@ -748,8 +748,8 @@ static void cpu_idle_thread_init(unsigned int cpu, struct task_struct *idle)
 	struct thread_info *ti = task_thread_info(idle);
 
 #ifdef CONFIG_PPC64
-	paca[cpu].__current = idle;
-	paca[cpu].kstack = (unsigned long)ti + THREAD_SIZE - STACK_FRAME_OVERHEAD;
+	paca_ptrs[cpu]->__current = idle;
+	paca_ptrs[cpu]->kstack = (unsigned long)ti + THREAD_SIZE - STACK_FRAME_OVERHEAD;
 #endif
 	ti->cpu = cpu;
 	secondary_ti = current_set[cpu] = ti;

commit 4145f358644b970fcff293c09fdcc7939e8527d2
Author: Balbir Singh <bsingharora@gmail.com>
Date:   Fri Dec 15 19:14:55 2017 +1100

    powernv/kdump: Fix cases where the kdump kernel can get HMI's
    
    Certain HMI's such as malfunction error propagate through
    all threads/core on the system. If a thread was offline
    prior to us crashing the system and jumping to the kdump
    kernel, bad things happen when it wakes up due to an HMI
    in the kdump kernel.
    
    There are several possible ways to solve this problem
    
    1. Put the offline cores in a state such that they are
    not woken up for machine check and HMI errors. This
    does not work, since we might need to wake up offline
    threads to handle TB errors
    2. Ignore HMI errors, setup HMEER to mask HMI errors,
    but this still leads the window open for any MCEs
    and masking them for the duration of the dump might
    be a concern
    3. Wake up offline CPUs, as in send them to
    crash_ipi_callback (not wake them up as in mark them
    online as seen by the hotplug). kexec does a
    wake_online_cpus() call, this patch does something
    similar, but instead sends an IPI and forces them to
    crash_ipi_callback()
    
    This patch takes approach #3.
    
    Care is taken to enable this only for powenv platforms
    via crash_wake_offline (a global value set at setup
    time). The crash code sends out IPI's to all CPU's
    which then move to crash_ipi_callback and kexec_smp_wait().
    
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e0a4c1f82e25..bbe7634b3a43 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -543,7 +543,25 @@ void smp_send_debugger_break(void)
 #ifdef CONFIG_KEXEC_CORE
 void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
 {
+	int cpu;
+
 	smp_send_nmi_ipi(NMI_IPI_ALL_OTHERS, crash_ipi_callback, 1000000);
+	if (kdump_in_progress() && crash_wake_offline) {
+		for_each_present_cpu(cpu) {
+			if (cpu_online(cpu))
+				continue;
+			/*
+			 * crash_ipi_callback will wait for
+			 * all cpus, including offline CPUs.
+			 * We don't care about nmi_ipi_function.
+			 * Offline cpus will jump straight into
+			 * crash_ipi_callback, we can skip the
+			 * entire NMI dance and waiting for
+			 * cpus to clear pending mask, etc.
+			 */
+			do_smp_send_nmi_ipi(cpu);
+		}
+	}
 }
 #endif
 

commit 96d91431d6915073c539c8bdd439b4c863148fc1
Author: Oliver O'Halloran <oohall@gmail.com>
Date:   Thu Jun 29 17:12:56 2017 +1000

    powerpc/smp: Add Power9 scheduler topology
    
    In previous generations of Power processors each core had a private L2
    cache. The Power 9 processor has a slightly different design where the
    L2 cache is shared among pairs of cores rather than being completely
    private.
    
    Making the scheduler aware of this cache sharing allows the scheduler to
    make better migration decisions. For example, if two CPU heavy tasks
    share a core then one task can be migrated to the paired core to improve
    throughput. Under the existing three level topology the task could be
    migrated to any core on the same chip, while with the new topology it
    would be preferentially migrated to the paired core so it remains
    cache-hot.
    
    Signed-off-by: Oliver O'Halloran <oohall@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index b42c868e1ac1..e0a4c1f82e25 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -957,6 +957,8 @@ static void add_cpu_to_masks(int cpu)
 			set_cpus_related(cpu, i, cpu_core_mask);
 }
 
+static bool shared_caches;
+
 /* Activate a secondary processor. */
 void start_secondary(void *unused)
 {
@@ -986,6 +988,13 @@ void start_secondary(void *unused)
 	/* Update topology CPU masks */
 	add_cpu_to_masks(cpu);
 
+	/*
+	 * Check for any shared caches. Note that this must be done on a
+	 * per-core basis because one core in the pair might be disabled.
+	 */
+	if (!cpumask_equal(cpu_l2_cache_mask(cpu), cpu_sibling_mask(cpu)))
+		shared_caches = true;
+
 	set_numa_node(numa_cpu_lookup_table[cpu]);
 	set_numa_mem(local_memory_node(numa_cpu_lookup_table[cpu]));
 
@@ -1027,6 +1036,35 @@ static struct sched_domain_topology_level powerpc_topology[] = {
 	{ NULL, },
 };
 
+/*
+ * P9 has a slightly odd architecture where pairs of cores share an L2 cache.
+ * This topology makes it *much* cheaper to migrate tasks between adjacent cores
+ * since the migrated task remains cache hot. We want to take advantage of this
+ * at the scheduler level so an extra topology level is required.
+ */
+static int powerpc_shared_cache_flags(void)
+{
+	return SD_SHARE_PKG_RESOURCES;
+}
+
+/*
+ * We can't just pass cpu_l2_cache_mask() directly because
+ * returns a non-const pointer and the compiler barfs on that.
+ */
+static const struct cpumask *shared_cache_mask(int cpu)
+{
+	return cpu_l2_cache_mask(cpu);
+}
+
+static struct sched_domain_topology_level power9_topology[] = {
+#ifdef CONFIG_SCHED_SMT
+	{ cpu_smt_mask, powerpc_smt_flags, SD_INIT_NAME(SMT) },
+#endif
+	{ shared_cache_mask, powerpc_shared_cache_flags, SD_INIT_NAME(CACHE) },
+	{ cpu_cpu_mask, SD_INIT_NAME(DIE) },
+	{ NULL, },
+};
+
 void __init smp_cpus_done(unsigned int max_cpus)
 {
 	/*
@@ -1040,7 +1078,17 @@ void __init smp_cpus_done(unsigned int max_cpus)
 
 	dump_numa_cpu_topology();
 
-	set_sched_topology(powerpc_topology);
+	/*
+	 * If any CPU detects that it's sharing a cache with another CPU then
+	 * use the deeper topology that is aware of this sharing.
+	 */
+	if (shared_caches) {
+		pr_info("Using shared cache scheduler topology\n");
+		set_sched_topology(power9_topology);
+	} else {
+		pr_info("Using standard scheduler topology\n");
+		set_sched_topology(powerpc_topology);
+	}
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit 2a636a56d2d39676fe85190dec102c7440e24977
Author: Oliver O'Halloran <oohall@gmail.com>
Date:   Thu Jun 29 17:12:55 2017 +1000

    powerpc/smp: Add cpu_l2_cache_map
    
    We want to add an extra level to the CPU scheduler topology to account
    for cores which share a cache. To do this we need to build a cpumask
    for each CPU that indicates which CPUs share this cache to use as an
    input to the scheduler.
    
    Signed-off-by: Oliver O'Halloran <oohall@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index a29c23bd9f2e..b42c868e1ac1 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -75,9 +75,11 @@ static DEFINE_PER_CPU(int, cpu_state) = { 0 };
 struct thread_info *secondary_ti;
 
 DEFINE_PER_CPU(cpumask_var_t, cpu_sibling_map);
+DEFINE_PER_CPU(cpumask_var_t, cpu_l2_cache_map);
 DEFINE_PER_CPU(cpumask_var_t, cpu_core_map);
 
 EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
+EXPORT_PER_CPU_SYMBOL(cpu_l2_cache_map);
 EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 
 /* SMP operations for this machine */
@@ -610,6 +612,8 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	for_each_possible_cpu(cpu) {
 		zalloc_cpumask_var_node(&per_cpu(cpu_sibling_map, cpu),
 					GFP_KERNEL, cpu_to_node(cpu));
+		zalloc_cpumask_var_node(&per_cpu(cpu_l2_cache_map, cpu),
+					GFP_KERNEL, cpu_to_node(cpu));
 		zalloc_cpumask_var_node(&per_cpu(cpu_core_map, cpu),
 					GFP_KERNEL, cpu_to_node(cpu));
 		/*
@@ -624,6 +628,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 
 	/* Init the cpumasks so the boot CPU is related to itself */
 	cpumask_set_cpu(boot_cpuid, cpu_sibling_mask(boot_cpuid));
+	cpumask_set_cpu(boot_cpuid, cpu_l2_cache_mask(boot_cpuid));
 	cpumask_set_cpu(boot_cpuid, cpu_core_mask(boot_cpuid));
 
 	if (smp_ops && smp_ops->probe)
@@ -907,6 +912,7 @@ static void remove_cpu_from_masks(int cpu)
 	/* NB: cpu_core_mask is a superset of the others */
 	for_each_cpu(i, cpu_core_mask(cpu)) {
 		set_cpus_unrelated(cpu, i, cpu_core_mask);
+		set_cpus_unrelated(cpu, i, cpu_l2_cache_mask);
 		set_cpus_unrelated(cpu, i, cpu_sibling_mask);
 	}
 }
@@ -929,17 +935,22 @@ static void add_cpu_to_masks(int cpu)
 			set_cpus_related(i, cpu, cpu_sibling_mask);
 
 	/*
-	 * Copy the thread sibling into core sibling mask, and
-	 * add CPUs that share a chip or an L2 to the core sibling
-	 * mask.
+	 * Copy the thread sibling mask into the cache sibling mask
+	 * and mark any CPUs that share an L2 with this CPU.
 	 */
 	for_each_cpu(i, cpu_sibling_mask(cpu))
+		set_cpus_related(cpu, i, cpu_l2_cache_mask);
+	update_mask_by_l2(cpu, cpu_l2_cache_mask);
+
+	/*
+	 * Copy the cache sibling mask into core sibling mask and mark
+	 * any CPUs on the same chip as this CPU.
+	 */
+	for_each_cpu(i, cpu_l2_cache_mask(cpu))
 		set_cpus_related(cpu, i, cpu_core_mask);
 
-	if (chipid == -1) {
-		update_mask_by_l2(cpu, cpu_core_mask);
+	if (chipid == -1)
 		return;
-	}
 
 	for_each_cpu(i, cpu_online_mask)
 		if (cpu_to_chip_id(i) == chipid)

commit df52f6714071c49a1fb1f541d4c4ff929bd4594d
Author: Oliver O'Halloran <oohall@gmail.com>
Date:   Thu Jun 29 17:12:54 2017 +1000

    powerpc/smp: Rework CPU topology construction
    
    The CPU scheduler topology is constructed from a number of per-cpu
    cpumasks which describe which sets of logical CPUs are related in some
    fashion. Current code that handles constructing these masks when CPUs
    are hot(un)plugged can be simplified a bit by exploiting the fact that
    the scheduler requires higher levels of the toplogy (e.g package level
    groupings) to be supersets of the lower levels (e.g.  threas in a core).
    This patch reworks the cpumask construction to be simpler and easier to
    extend with extra topology levels.
    
    Signed-off-by: Oliver O'Halloran <oohall@gmail.com>
    [mpe: Fix CONFIG_HOTPLUG_CPU=n build]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index bf31f0874949..a29c23bd9f2e 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -571,6 +571,26 @@ static void smp_store_cpu_info(int id)
 #endif
 }
 
+/*
+ * Relationships between CPUs are maintained in a set of per-cpu cpumasks so
+ * rather than just passing around the cpumask we pass around a function that
+ * returns the that cpumask for the given CPU.
+ */
+static void set_cpus_related(int i, int j, struct cpumask *(*get_cpumask)(int))
+{
+	cpumask_set_cpu(i, get_cpumask(j));
+	cpumask_set_cpu(j, get_cpumask(i));
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+static void set_cpus_unrelated(int i, int j,
+		struct cpumask *(*get_cpumask)(int))
+{
+	cpumask_clear_cpu(i, get_cpumask(j));
+	cpumask_clear_cpu(j, get_cpumask(i));
+}
+#endif
+
 void __init smp_prepare_cpus(unsigned int max_cpus)
 {
 	unsigned int cpu;
@@ -602,6 +622,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 		}
 	}
 
+	/* Init the cpumasks so the boot CPU is related to itself */
 	cpumask_set_cpu(boot_cpuid, cpu_sibling_mask(boot_cpuid));
 	cpumask_set_cpu(boot_cpuid, cpu_core_mask(boot_cpuid));
 
@@ -828,24 +849,6 @@ int cpu_first_thread_of_core(int core)
 }
 EXPORT_SYMBOL_GPL(cpu_first_thread_of_core);
 
-static void traverse_siblings_chip_id(int cpu, bool add, int chipid)
-{
-	const struct cpumask *mask = add ? cpu_online_mask : cpu_present_mask;
-	int i;
-
-	for_each_cpu(i, mask) {
-		if (cpu_to_chip_id(i) == chipid) {
-			if (add) {
-				cpumask_set_cpu(cpu, cpu_core_mask(i));
-				cpumask_set_cpu(i, cpu_core_mask(cpu));
-			} else {
-				cpumask_clear_cpu(cpu, cpu_core_mask(i));
-				cpumask_clear_cpu(i, cpu_core_mask(cpu));
-			}
-		}
-	}
-}
-
 /* Must be called when no change can occur to cpu_present_mask,
  * i.e. during cpu online or offline.
  */
@@ -868,46 +871,85 @@ static struct device_node *cpu_to_l2cache(int cpu)
 	return cache;
 }
 
-static void traverse_core_siblings(int cpu, bool add)
+static bool update_mask_by_l2(int cpu, struct cpumask *(*mask_fn)(int))
 {
 	struct device_node *l2_cache, *np;
-	const struct cpumask *mask;
-	int chip_id;
 	int i;
 
-	/* threads that share a chip-id are considered siblings */
-	chip_id = cpu_to_chip_id(cpu);
-
-	if (chip_id >= 0) {
-		traverse_siblings_chip_id(cpu, add, chip_id);
-		return;
-	}
-
 	l2_cache = cpu_to_l2cache(cpu);
-	mask = add ? cpu_online_mask : cpu_present_mask;
-	for_each_cpu(i, mask) {
+	if (!l2_cache)
+		return false;
+
+	for_each_cpu(i, cpu_online_mask) {
+		/*
+		 * when updating the marks the current CPU has not been marked
+		 * online, but we need to update the cache masks
+		 */
 		np = cpu_to_l2cache(i);
 		if (!np)
 			continue;
-		if (np == l2_cache) {
-			if (add) {
-				cpumask_set_cpu(cpu, cpu_core_mask(i));
-				cpumask_set_cpu(i, cpu_core_mask(cpu));
-			} else {
-				cpumask_clear_cpu(cpu, cpu_core_mask(i));
-				cpumask_clear_cpu(i, cpu_core_mask(cpu));
-			}
-		}
+
+		if (np == l2_cache)
+			set_cpus_related(cpu, i, mask_fn);
+
 		of_node_put(np);
 	}
 	of_node_put(l2_cache);
+
+	return true;
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+static void remove_cpu_from_masks(int cpu)
+{
+	int i;
+
+	/* NB: cpu_core_mask is a superset of the others */
+	for_each_cpu(i, cpu_core_mask(cpu)) {
+		set_cpus_unrelated(cpu, i, cpu_core_mask);
+		set_cpus_unrelated(cpu, i, cpu_sibling_mask);
+	}
+}
+#endif
+
+static void add_cpu_to_masks(int cpu)
+{
+	int first_thread = cpu_first_thread_sibling(cpu);
+	int chipid = cpu_to_chip_id(cpu);
+	int i;
+
+	/*
+	 * This CPU will not be in the online mask yet so we need to manually
+	 * add it to it's own thread sibling mask.
+	 */
+	cpumask_set_cpu(cpu, cpu_sibling_mask(cpu));
+
+	for (i = first_thread; i < first_thread + threads_per_core; i++)
+		if (cpu_online(i))
+			set_cpus_related(i, cpu, cpu_sibling_mask);
+
+	/*
+	 * Copy the thread sibling into core sibling mask, and
+	 * add CPUs that share a chip or an L2 to the core sibling
+	 * mask.
+	 */
+	for_each_cpu(i, cpu_sibling_mask(cpu))
+		set_cpus_related(cpu, i, cpu_core_mask);
+
+	if (chipid == -1) {
+		update_mask_by_l2(cpu, cpu_core_mask);
+		return;
+	}
+
+	for_each_cpu(i, cpu_online_mask)
+		if (cpu_to_chip_id(i) == chipid)
+			set_cpus_related(cpu, i, cpu_core_mask);
 }
 
 /* Activate a secondary processor. */
 void start_secondary(void *unused)
 {
 	unsigned int cpu = smp_processor_id();
-	int i, base;
 
 	mmgrab(&init_mm);
 	current->active_mm = &init_mm;
@@ -930,22 +972,8 @@ void start_secondary(void *unused)
 
 	vdso_getcpu_init();
 #endif
-	/* Update sibling maps */
-	base = cpu_first_thread_sibling(cpu);
-	for (i = 0; i < threads_per_core; i++) {
-		if (cpu_is_offline(base + i) && (cpu != base + i))
-			continue;
-		cpumask_set_cpu(cpu, cpu_sibling_mask(base + i));
-		cpumask_set_cpu(base + i, cpu_sibling_mask(cpu));
-
-		/* cpu_core_map should be a superset of
-		 * cpu_sibling_map even if we don't have cache
-		 * information, so update the former here, too.
-		 */
-		cpumask_set_cpu(cpu, cpu_core_mask(base + i));
-		cpumask_set_cpu(base + i, cpu_core_mask(cpu));
-	}
-	traverse_core_siblings(cpu, true);
+	/* Update topology CPU masks */
+	add_cpu_to_masks(cpu);
 
 	set_numa_node(numa_cpu_lookup_table[cpu]);
 	set_numa_mem(local_memory_node(numa_cpu_lookup_table[cpu]));
@@ -1008,7 +1036,6 @@ void __init smp_cpus_done(unsigned int max_cpus)
 int __cpu_disable(void)
 {
 	int cpu = smp_processor_id();
-	int base, i;
 	int err;
 
 	if (!smp_ops->cpu_disable)
@@ -1019,14 +1046,7 @@ int __cpu_disable(void)
 		return err;
 
 	/* Update sibling maps */
-	base = cpu_first_thread_sibling(cpu);
-	for (i = 0; i < threads_per_core && base + i < nr_cpu_ids; i++) {
-		cpumask_clear_cpu(cpu, cpu_sibling_mask(base + i));
-		cpumask_clear_cpu(base + i, cpu_sibling_mask(cpu));
-		cpumask_clear_cpu(cpu, cpu_core_mask(base + i));
-		cpumask_clear_cpu(base + i, cpu_core_mask(cpu));
-	}
-	traverse_core_siblings(cpu, false);
+	remove_cpu_from_masks(cpu);
 
 	return 0;
 }

commit e3d8b67e2c60dcd35661d34df249f20d20463f0c
Author: Oliver O'Halloran <oohall@gmail.com>
Date:   Thu Jun 29 17:12:53 2017 +1000

    powerpc/smp: Use cpu_to_chip_id() to find core siblings
    
    When building the CPU scheduler topology the kernel uses the ibm,chipid
    property from the devicetree to group logical CPUs. Currently the DT
    search for this property is open-coded in smp.c and this functionality
    is a duplication of what's in cpu_to_chip_id() already. This patch
    removes the existing search in favor of that.
    
    It's worth mentioning that the semantics of the search are different
    in cpu_to_chip_id(). When there is no ibm,chipid in the CPUs node it
    will also search /cpus and / for the property, but this should not
    effect the output topology.
    
    Signed-off-by: Oliver O'Halloran <oohall@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8d3320562c70..bf31f0874949 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -830,19 +830,11 @@ EXPORT_SYMBOL_GPL(cpu_first_thread_of_core);
 
 static void traverse_siblings_chip_id(int cpu, bool add, int chipid)
 {
-	const struct cpumask *mask;
-	struct device_node *np;
-	int i, plen;
-	const __be32 *prop;
+	const struct cpumask *mask = add ? cpu_online_mask : cpu_present_mask;
+	int i;
 
-	mask = add ? cpu_online_mask : cpu_present_mask;
 	for_each_cpu(i, mask) {
-		np = of_get_cpu_node(i, NULL);
-		if (!np)
-			continue;
-		prop = of_get_property(np, "ibm,chip-id", &plen);
-		if (prop && plen == sizeof(int) &&
-		    of_read_number(prop, 1) == chipid) {
+		if (cpu_to_chip_id(i) == chipid) {
 			if (add) {
 				cpumask_set_cpu(cpu, cpu_core_mask(i));
 				cpumask_set_cpu(i, cpu_core_mask(cpu));
@@ -851,7 +843,6 @@ static void traverse_siblings_chip_id(int cpu, bool add, int chipid)
 				cpumask_clear_cpu(i, cpu_core_mask(cpu));
 			}
 		}
-		of_node_put(np);
 	}
 }
 
@@ -881,21 +872,15 @@ static void traverse_core_siblings(int cpu, bool add)
 {
 	struct device_node *l2_cache, *np;
 	const struct cpumask *mask;
-	int i, chip, plen;
-	const __be32 *prop;
+	int chip_id;
+	int i;
 
-	/* First see if we have ibm,chip-id properties in cpu nodes */
-	np = of_get_cpu_node(cpu, NULL);
-	if (np) {
-		chip = -1;
-		prop = of_get_property(np, "ibm,chip-id", &plen);
-		if (prop && plen == sizeof(int))
-			chip = of_read_number(prop, 1);
-		of_node_put(np);
-		if (chip >= 0) {
-			traverse_siblings_chip_id(cpu, add, chip);
-			return;
-		}
+	/* threads that share a chip-id are considered siblings */
+	chip_id = cpu_to_chip_id(cpu);
+
+	if (chip_id >= 0) {
+		traverse_siblings_chip_id(cpu, add, chip_id);
+		return;
 	}
 
 	l2_cache = cpu_to_l2cache(cpu);

commit 0459ddfdb31e7d812b555a2530ecbacdf96961a6
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Aug 9 22:41:21 2017 +1000

    powerpc: NMI IPI improve lock primitive
    
    When the NMI IPI lock is contended, spin at low SMT priority, using
    loads only, and with interrupts enabled (where possible). This
    improves behaviour under high contention (e.g., a system crash when
    a number of CPUs are trying to enter the debugger).
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index cf0e1245b8cc..8d3320562c70 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -351,7 +351,7 @@ static void nmi_ipi_lock_start(unsigned long *flags)
 	hard_irq_disable();
 	while (atomic_cmpxchg(&__nmi_ipi_lock, 0, 1) == 1) {
 		raw_local_irq_restore(*flags);
-		cpu_relax();
+		spin_until_cond(atomic_read(&__nmi_ipi_lock) == 0);
 		raw_local_irq_save(*flags);
 		hard_irq_disable();
 	}
@@ -360,7 +360,7 @@ static void nmi_ipi_lock_start(unsigned long *flags)
 static void nmi_ipi_lock(void)
 {
 	while (atomic_cmpxchg(&__nmi_ipi_lock, 0, 1) == 1)
-		cpu_relax();
+		spin_until_cond(atomic_read(&__nmi_ipi_lock) == 0);
 }
 
 static void nmi_ipi_unlock(void)
@@ -475,7 +475,7 @@ int smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us)
 	nmi_ipi_lock_start(&flags);
 	while (nmi_ipi_busy_count) {
 		nmi_ipi_unlock_end(&flags);
-		cpu_relax();
+		spin_until_cond(nmi_ipi_busy_count == 0);
 		nmi_ipi_lock_start(&flags);
 	}
 

commit bb272221e9db79f13d454e1f3fb6b05013be985e
Merge: 253fd51e2f53 5771a8c08880
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Jul 31 20:20:29 2017 +1000

    Merge tag 'v4.13-rc1' into fixes
    
    The fixes branch is based off a random pre-rc1 commit, because we had
    some fixes that needed to go in before rc1 was released.
    
    However we now need to fix some code that went in after that point, but
    before rc1, so merge rc1 to get that code into fixes so we can fix it!

commit 7b7622bb95eb587cbaa79608e47b832a82a262b1
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Jul 27 23:23:37 2017 +1000

    powerpc/smp: Call smp_ops->setup_cpu() directly on the boot CPU
    
    In smp_cpus_done() we need to call smp_ops->setup_cpu() for the boot
    CPU, which means it has to run *on* the boot CPU.
    
    In the past we ensured it ran on the boot CPU by changing the CPU
    affinity mask of current directly. That was removed in commit
    6d11b87d55eb ("powerpc/smp: Replace open coded task affinity logic"),
    and replaced with a work queue call.
    
    Unfortunately using a work queue leads to a lockdep warning, now that
    the CPU hotplug lock is a regular semaphore:
    
      ======================================================
      WARNING: possible circular locking dependency detected
      ...
      kworker/0:1/971 is trying to acquire lock:
       (cpu_hotplug_lock.rw_sem){++++++}, at: [<c000000000100974>] apply_workqueue_attrs+0x34/0xa0
    
      but task is already holding lock:
       ((&wfc.work)){+.+.+.}, at: [<c0000000000fdb2c>] process_one_work+0x25c/0x800
      ...
           CPU0                    CPU1
           ----                    ----
      lock((&wfc.work));
                                   lock(cpu_hotplug_lock.rw_sem);
                                   lock((&wfc.work));
      lock(cpu_hotplug_lock.rw_sem);
    
    Although the deadlock can't happen in practice, because
    smp_cpus_done() only runs in early boot before CPU hotplug is allowed,
    lockdep can't tell that.
    
    Luckily in commit 8fb12156b8db ("init: Pin init task to the boot CPU,
    initially") tglx changed the generic code to pin init to the boot CPU
    to begin with. The unpinning of init from the boot CPU happens in
    sched_init_smp(), which is called after smp_cpus_done().
    
    So smp_cpus_done() is always called on the boot CPU, which means we
    don't need the work queue call at all - and the lockdep warning goes
    away.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index c6b8bace1766..b0ea6d4d4853 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -985,21 +985,13 @@ static struct sched_domain_topology_level powerpc_topology[] = {
 	{ NULL, },
 };
 
-static __init long smp_setup_cpu_workfn(void *data __always_unused)
-{
-	smp_ops->setup_cpu(boot_cpuid);
-	return 0;
-}
-
 void __init smp_cpus_done(unsigned int max_cpus)
 {
 	/*
-	 * We want the setup_cpu() here to be called on the boot CPU, but
-	 * init might run on any CPU, so make sure it's invoked on the boot
-	 * CPU.
+	 * We are running pinned to the boot CPU, see rest_init().
 	 */
 	if (smp_ops && smp_ops->setup_cpu)
-		work_on_cpu_safe(boot_cpuid, smp_setup_cpu_workfn, NULL);
+		smp_ops->setup_cpu(boot_cpuid);
 
 	if (smp_ops && smp_ops->bringup_done)
 		smp_ops->bringup_done();

commit 2104180a53698df5aec35aed5f840a26ade0551d
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Jul 12 14:35:52 2017 -0700

    powerpc/64s: implement arch-specific hardlockup watchdog
    
    Implement an arch-speicfic watchdog rather than use the perf-based
    hardlockup detector.
    
    The new watchdog takes the soft-NMI directly, rather than going through
    perf.  Perf interrupts are to be made maskable in future, so that would
    prevent the perf detector from working in those regions.
    
    Additionally, implement a SMP based detector where all CPUs watch one
    another by pinging a shared cpumask.  This is because powerpc Book3S
    does not have a true periodic local NMI, but some platforms do implement
    a true NMI IPI.
    
    If a CPU is stuck with interrupts hard disabled, the soft-NMI watchdog
    does not work, but the SMP watchdog will.  Even on platforms without a
    true NMI IPI to get a good trace from the stuck CPU, other CPUs will
    notice the lockup sufficiently to report it and panic.
    
    [npiggin@gmail.com: honor watchdog disable at boot/hotplug]
      Link: http://lkml.kernel.org/r/20170621001346.5bb337c9@roar.ozlabs.ibm.com
    [npiggin@gmail.com: fix false positive warning at CPU unplug]
      Link: http://lkml.kernel.org/r/20170630080740.20766-1-npiggin@gmail.com
    [akpm@linux-foundation.org: coding-style fixes]
    Link: http://lkml.kernel.org/r/20170616065715.18390-6-npiggin@gmail.com
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Tested-by: Babu Moger <babu.moger@oracle.com>   [sparc]
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index c6b8bace1766..997c88d54acf 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -435,13 +435,31 @@ static void do_smp_send_nmi_ipi(int cpu)
 	}
 }
 
+void smp_flush_nmi_ipi(u64 delay_us)
+{
+	unsigned long flags;
+
+	nmi_ipi_lock_start(&flags);
+	while (nmi_ipi_busy_count) {
+		nmi_ipi_unlock_end(&flags);
+		udelay(1);
+		if (delay_us) {
+			delay_us--;
+			if (!delay_us)
+				return;
+		}
+		nmi_ipi_lock_start(&flags);
+	}
+	nmi_ipi_unlock_end(&flags);
+}
+
 /*
  * - cpu is the target CPU (must not be this CPU), or NMI_IPI_ALL_OTHERS.
  * - fn is the target callback function.
  * - delay_us > 0 is the delay before giving up waiting for targets to
  *   enter the handler, == 0 specifies indefinite delay.
  */
-static int smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us)
+int smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us)
 {
 	unsigned long flags;
 	int me = raw_smp_processor_id();

commit d691b7e7d1b5186eae62fd32adee65d3316bfdf6
Merge: b59eea554f57 1e0fc9d1eb2b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 7 13:55:45 2017 -0700

    Merge tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for STRICT_KERNEL_RWX on 64-bit server CPUs.
    
       - Platform support for FSP2 (476fpe) board
    
       - Enable ZONE_DEVICE on 64-bit server CPUs.
    
       - Generic & powerpc spin loop primitives to optimise busy waiting
    
       - Convert VDSO update function to use new update_vsyscall() interface
    
       - Optimisations to hypercall/syscall/context-switch paths
    
       - Improvements to the CPU idle code on Power8 and Power9.
    
      As well as many other fixes and improvements.
    
      Thanks to: Akshay Adiga, Andrew Donnellan, Andrew Jeffery, Anshuman
      Khandual, Anton Blanchard, Balbir Singh, Benjamin Herrenschmidt,
      Christophe Leroy, Christophe Lombard, Colin Ian King, Dan Carpenter,
      Gautham R. Shenoy, Hari Bathini, Ian Munsie, Ivan Mikhaylov, Javier
      Martinez Canillas, Madhavan Srinivasan, Masahiro Yamada, Matt Brown,
      Michael Neuling, Michal Suchanek, Murilo Opsfelder Araujo, Naveen N.
      Rao, Nicholas Piggin, Oliver O'Halloran, Paul Mackerras, Pavel Machek,
      Russell Currey, Santosh Sivaraj, Stephen Rothwell, Thiago Jung
      Bauermann, Yang Li"
    
    * tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (158 commits)
      powerpc/Kconfig: Enable STRICT_KERNEL_RWX for some configs
      powerpc/mm/radix: Implement STRICT_RWX/mark_rodata_ro() for Radix
      powerpc/mm/hash: Implement mark_rodata_ro() for hash
      powerpc/vmlinux.lds: Align __init_begin to 16M
      powerpc/lib/code-patching: Use alternate map for patch_instruction()
      powerpc/xmon: Add patch_instruction() support for xmon
      powerpc/kprobes/optprobes: Use patch_instruction()
      powerpc/kprobes: Move kprobes over to patch_instruction()
      powerpc/mm/radix: Fix execute permissions for interrupt_vectors
      powerpc/pseries: Fix passing of pp0 in updatepp() and updateboltedpp()
      powerpc/64s: Blacklist rtas entry/exit from kprobes
      powerpc/64s: Blacklist functions invoked on a trap
      powerpc/64s: Un-blacklist system_call() from kprobes
      powerpc/64s: Move system_call() symbol to just after setting MSR_EE
      powerpc/64s: Blacklist system_call() and system_call_common() from kprobes
      powerpc/64s: Convert .L__replay_interrupt_return to a local label
      powerpc64/elfv1: Only dereference function descriptor for non-text symbols
      cxl: Export library to support IBM XSL
      powerpc/dts: Use #include "..." to include local DT
      powerpc/perf/hv-24x7: Aggregate result elements on POWER9 SMT8
      ...

commit 4e287e655e108cbbd6e3e7dcc49d591c8aa5a8a4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Jun 6 23:08:32 2017 +1000

    powerpc: use spin loop primitives in some functions
    
    Use the different spin loop primitives in some simple powerpc
    spin loops, including those which will spin as a common case.
    
    This will help to test the spin loop primitives before more
    conversions are done.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Add some includes of <linux/processor.h>]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 418019728efa..a975ddf77200 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -33,6 +33,7 @@
 #include <linux/notifier.h>
 #include <linux/topology.h>
 #include <linux/profile.h>
+#include <linux/processor.h>
 
 #include <asm/ptrace.h>
 #include <linux/atomic.h>
@@ -767,8 +768,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 		smp_ops->give_timebase();
 
 	/* Wait until cpu puts itself in the online & active maps */
-	while (!cpu_online(cpu))
-		cpu_relax();
+	spin_until_cond(cpu_online(cpu));
 
 	return 0;
 }

commit c642af9c41f09296997519499d16ff30e700816a
Author: Santosh Sivaraj <santosh@fossix.org>
Date:   Tue Jun 27 12:30:06 2017 +0530

    powerpc/smp: Convert NR_CPUS to nr_cpu_ids
    
    nr_cpu_ids can be limited by nr_cpus boot parameter, whereas NR_CPUS is a
    compile time constant, which shouldn't be compared against during cpu kick.
    
    Signed-off-by: Santosh Sivaraj <santosh@fossix.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 05bf5836107c..418019728efa 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -112,7 +112,7 @@ int smp_generic_cpu_bootable(unsigned int nr)
 #ifdef CONFIG_PPC64
 int smp_generic_kick_cpu(int nr)
 {
-	if (nr < 0 || nr >= NR_CPUS)
+	if (nr < 0 || nr >= nr_cpu_ids)
 		return -EINVAL;
 
 	/*

commit f8d0d5dc641cd405ad40cb2498b04df9716baee6
Author: Santosh Sivaraj <santosh@fossix.org>
Date:   Tue Jun 27 12:30:05 2017 +0530

    powerpc/smp: Do not BUG_ON if invalid CPU during kick
    
    During secondary start, we do not need to BUG_ON if an invalid CPU number
    is passed. We already print an error if secondary cannot be started, so
    just return an error instead.
    
    Signed-off-by: Santosh Sivaraj <santosh@fossix.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index df2a41647d8e..05bf5836107c 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -112,7 +112,8 @@ int smp_generic_cpu_bootable(unsigned int nr)
 #ifdef CONFIG_PPC64
 int smp_generic_kick_cpu(int nr)
 {
-	BUG_ON(nr < 0 || nr >= NR_CPUS);
+	if (nr < 0 || nr >= NR_CPUS)
+		return -EINVAL;
 
 	/*
 	 * The processor is currently spinning, waiting for the

commit a8fcfc1917681ba1ccc23a429543a67aad8bfd00
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 16 20:42:37 2017 +0200

    powerpc: Adjust system_state check
    
    To enable smp_processor_id() and might_sleep() debug checks earlier, it's
    required to add system states between SYSTEM_BOOTING and SYSTEM_RUNNING.
    
    Adjust the system_state check in smp_generic_cpu_bootable() to handle the
    extra states.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/20170516184735.359536998@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index df2a41647d8e..1069f74fca47 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -97,7 +97,7 @@ int smp_generic_cpu_bootable(unsigned int nr)
 	/* Special case - we inhibit secondary thread startup
 	 * during boot if the user requests it.
 	 */
-	if (system_state == SYSTEM_BOOTING && cpu_has_feature(CPU_FTR_SMT)) {
+	if (system_state < SYSTEM_RUNNING && cpu_has_feature(CPU_FTR_SMT)) {
 		if (!smt_enabled_at_boot && cpu_thread_in_core(nr) != 0)
 			return 0;
 		if (smt_enabled_at_boot

commit 7246f60068840847bdcf595be5f0b5ca632736e0
Merge: e579dde654fc 700b7eadd562
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 5 11:36:44 2017 -0700

    Merge tag 'powerpc-4.12-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Larger virtual address space on 64-bit server CPUs. By default we
         use a 128TB virtual address space, but a process can request access
         to the full 512TB by passing a hint to mmap().
    
       - Support for the new Power9 "XIVE" interrupt controller.
    
       - TLB flushing optimisations for the radix MMU on Power9.
    
       - Support for CAPI cards on Power9, using the "Coherent Accelerator
         Interface Architecture 2.0".
    
       - The ability to configure the mmap randomisation limits at build and
         runtime.
    
       - Several small fixes and cleanups to the kprobes code, as well as
         support for KPROBES_ON_FTRACE.
    
       - Major improvements to handling of system reset interrupts,
         correctly treating them as NMIs, giving them a dedicated stack and
         using a new hypervisor call to trigger them, all of which should
         aid debugging and robustness.
    
       - Many fixes and other minor enhancements.
    
      Thanks to: Alastair D'Silva, Alexey Kardashevskiy, Alistair Popple,
      Andrew Donnellan, Aneesh Kumar K.V, Anshuman Khandual, Anton
      Blanchard, Balbir Singh, Ben Hutchings, Benjamin Herrenschmidt,
      Bhupesh Sharma, Chris Packham, Christian Zigotzky, Christophe Leroy,
      Christophe Lombard, Daniel Axtens, David Gibson, Gautham R. Shenoy,
      Gavin Shan, Geert Uytterhoeven, Guilherme G. Piccoli, Hamish Martin,
      Hari Bathini, Kees Cook, Laurent Dufour, Madhavan Srinivasan, Mahesh J
      Salgaonkar, Mahesh Salgaonkar, Masami Hiramatsu, Matt Brown, Matthew
      R. Ochs, Michael Neuling, Naveen N. Rao, Nicholas Piggin, Oliver
      O'Halloran, Pan Xinhui, Paul Mackerras, Rashmica Gupta, Russell
      Currey, Sukadev Bhattiprolu, Thadeu Lima de Souza Cascardo, Tobin C.
      Harding, Tyrel Datwyler, Uma Krishnan, Vaibhav Jain, Vipin K Parashar,
      Yang Shi"
    
    * tag 'powerpc-4.12-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (214 commits)
      powerpc/64s: Power9 has no LPCR[VRMASD] field so don't set it
      powerpc/powernv: Fix TCE kill on NVLink2
      powerpc/mm/radix: Drop support for CPUs without lockless tlbie
      powerpc/book3s/mce: Move add_taint() later in virtual mode
      powerpc/sysfs: Move #ifdef CONFIG_HOTPLUG_CPU out of the function body
      powerpc/smp: Document irq enable/disable after migrating IRQs
      powerpc/mpc52xx: Don't select user-visible RTAS_PROC
      powerpc/powernv: Document cxl dependency on special case in pnv_eeh_reset()
      powerpc/eeh: Clean up and document event handling functions
      powerpc/eeh: Avoid use after free in eeh_handle_special_event()
      cxl: Mask slice error interrupts after first occurrence
      cxl: Route eeh events to all drivers in cxl_pci_error_detected()
      cxl: Force context lock during EEH flow
      powerpc/64: Allow CONFIG_RELOCATABLE if COMPILE_TEST
      powerpc/xmon: Teach xmon oops about radix vectors
      powerpc/mm/hash: Fix off-by-one in comment about kernel contexts ids
      powerpc/pseries: Enable VFIO
      powerpc/powernv: Fix iommu table size calculation hook for small tables
      powerpc/powernv: Check kzalloc() return value in pnv_pci_table_alloc
      powerpc: Add arch/powerpc/tools directory
      ...

commit 687b8f24f14db842c8c3f8cb8b24c9a29b691db8
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Feb 15 20:49:54 2017 +1100

    powerpc/smp: Document irq enable/disable after migrating IRQs
    
    This code was until recently completely undocumented and even now the comment is
    not very verbose.
    
    We've already had one patch sent to remove the IRQ enable/disable because it's
    "paradoxical and unnecessary". So document it thoroughly to save anyone else
    from puzzling over it.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index b37973f11ce0..1eef9e73e6a3 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -615,7 +615,14 @@ int generic_cpu_disable(void)
 	/* Update affinity of all IRQs previously aimed at this CPU */
 	irq_migrate_all_off_this_cpu();
 
-	/* Give the CPU time to drain in-flight ones */
+	/*
+	 * Depending on the details of the interrupt controller, it's possible
+	 * that one of the interrupts we just migrated away from this CPU is
+	 * actually already pending on this CPU. If we leave it in that state
+	 * the interrupt will never be EOI'ed, and will never fire again. So
+	 * temporarily enable interrupts here, to allow any pending interrupt to
+	 * be received (and EOI'ed), before we take this CPU offline.
+	 */
 	local_irq_enable();
 	mdelay(1);
 	local_irq_disable();

commit c64af6458e2e2ddf86aff559837d3925fbf9cbb5
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Dec 20 04:30:09 2016 +1000

    powerpc: Add struct smp_ops_t.cause_nmi_ipi operation
    
    Have the NMI IPI code use this op when the platform defines it.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 7461b195b29e..b37973f11ce0 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -417,6 +417,9 @@ int smp_handle_nmi_ipi(struct pt_regs *regs)
 
 static void do_smp_send_nmi_ipi(int cpu)
 {
+	if (smp_ops->cause_nmi_ipi && smp_ops->cause_nmi_ipi(cpu))
+		return;
+
 	if (cpu >= 0) {
 		do_message_pass(cpu, PPC_MSG_NMI_IPI);
 	} else {

commit ddd703ca06ede1b2d01ed1b0cb8d4315ab808099
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Dec 20 04:30:08 2016 +1000

    powerpc: Add NMI IPI infrastructure
    
    Add a simple NMI IPI system that handles concurrency and reentrancy.
    
    The platform does not have to implement a true non-maskable interrupt,
    the default is to simply use the debugger break IPI message. This has
    now been co-opted for a general IPI message, and users (debugger and
    crash) have been reimplemented on top of the NMI system.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Incorporate incremental fixes from Nick]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 2eca1e491e2b..7461b195b29e 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -87,8 +87,6 @@ volatile unsigned int cpu_callin_map[NR_CPUS];
 
 int smt_enabled_at_boot = 1;
 
-static void (*crash_ipi_function_ptr)(struct pt_regs *) = NULL;
-
 /*
  * Returns 1 if the specified cpu should be brought up during boot.
  * Used to inhibit booting threads if they've been disabled or
@@ -159,32 +157,33 @@ static irqreturn_t tick_broadcast_ipi_action(int irq, void *data)
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t debug_ipi_action(int irq, void *data)
+#ifdef CONFIG_NMI_IPI
+static irqreturn_t nmi_ipi_action(int irq, void *data)
 {
-	if (crash_ipi_function_ptr) {
-		crash_ipi_function_ptr(get_irq_regs());
-		return IRQ_HANDLED;
-	}
-
-#ifdef CONFIG_DEBUGGER
-	debugger_ipi(get_irq_regs());
-#endif /* CONFIG_DEBUGGER */
-
+	smp_handle_nmi_ipi(get_irq_regs());
 	return IRQ_HANDLED;
 }
+#endif
 
 static irq_handler_t smp_ipi_action[] = {
 	[PPC_MSG_CALL_FUNCTION] =  call_function_action,
 	[PPC_MSG_RESCHEDULE] = reschedule_action,
 	[PPC_MSG_TICK_BROADCAST] = tick_broadcast_ipi_action,
-	[PPC_MSG_DEBUGGER_BREAK] = debug_ipi_action,
+#ifdef CONFIG_NMI_IPI
+	[PPC_MSG_NMI_IPI] = nmi_ipi_action,
+#endif
 };
 
+/*
+ * The NMI IPI is a fallback and not truly non-maskable. It is simpler
+ * than going through the call function infrastructure, and strongly
+ * serialized, so it is more appropriate for debugging.
+ */
 const char *smp_ipi_name[] = {
 	[PPC_MSG_CALL_FUNCTION] =  "ipi call function",
 	[PPC_MSG_RESCHEDULE] = "ipi reschedule",
 	[PPC_MSG_TICK_BROADCAST] = "ipi tick-broadcast",
-	[PPC_MSG_DEBUGGER_BREAK] = "ipi debugger",
+	[PPC_MSG_NMI_IPI] = "nmi ipi",
 };
 
 /* optional function to request ipi, for controllers with >= 4 ipis */
@@ -192,14 +191,13 @@ int smp_request_message_ipi(int virq, int msg)
 {
 	int err;
 
-	if (msg < 0 || msg > PPC_MSG_DEBUGGER_BREAK) {
+	if (msg < 0 || msg > PPC_MSG_NMI_IPI)
 		return -EINVAL;
-	}
-#if !defined(CONFIG_DEBUGGER) && !defined(CONFIG_KEXEC_CORE)
-	if (msg == PPC_MSG_DEBUGGER_BREAK) {
+#ifndef CONFIG_NMI_IPI
+	if (msg == PPC_MSG_NMI_IPI)
 		return 1;
-	}
 #endif
+
 	err = request_irq(virq, smp_ipi_action[msg],
 			  IRQF_PERCPU | IRQF_NO_THREAD | IRQF_NO_SUSPEND,
 			  smp_ipi_name[msg], NULL);
@@ -277,8 +275,10 @@ irqreturn_t smp_ipi_demux_relaxed(void)
 			scheduler_ipi();
 		if (all & IPI_MESSAGE(PPC_MSG_TICK_BROADCAST))
 			tick_broadcast_ipi_handler();
-		if (all & IPI_MESSAGE(PPC_MSG_DEBUGGER_BREAK))
-			debug_ipi_action(0, NULL);
+#ifdef CONFIG_NMI_IPI
+		if (all & IPI_MESSAGE(PPC_MSG_NMI_IPI))
+			nmi_ipi_action(0, NULL);
+#endif
 	} while (info->messages);
 
 	return IRQ_HANDLED;
@@ -315,6 +315,184 @@ void arch_send_call_function_ipi_mask(const struct cpumask *mask)
 		do_message_pass(cpu, PPC_MSG_CALL_FUNCTION);
 }
 
+#ifdef CONFIG_NMI_IPI
+
+/*
+ * "NMI IPI" system.
+ *
+ * NMI IPIs may not be recoverable, so should not be used as ongoing part of
+ * a running system. They can be used for crash, debug, halt/reboot, etc.
+ *
+ * NMI IPIs are globally single threaded. No more than one in progress at
+ * any time.
+ *
+ * The IPI call waits with interrupts disabled until all targets enter the
+ * NMI handler, then the call returns.
+ *
+ * No new NMI can be initiated until targets exit the handler.
+ *
+ * The IPI call may time out without all targets entering the NMI handler.
+ * In that case, there is some logic to recover (and ignore subsequent
+ * NMI interrupts that may eventually be raised), but the platform interrupt
+ * handler may not be able to distinguish this from other exception causes,
+ * which may cause a crash.
+ */
+
+static atomic_t __nmi_ipi_lock = ATOMIC_INIT(0);
+static struct cpumask nmi_ipi_pending_mask;
+static int nmi_ipi_busy_count = 0;
+static void (*nmi_ipi_function)(struct pt_regs *) = NULL;
+
+static void nmi_ipi_lock_start(unsigned long *flags)
+{
+	raw_local_irq_save(*flags);
+	hard_irq_disable();
+	while (atomic_cmpxchg(&__nmi_ipi_lock, 0, 1) == 1) {
+		raw_local_irq_restore(*flags);
+		cpu_relax();
+		raw_local_irq_save(*flags);
+		hard_irq_disable();
+	}
+}
+
+static void nmi_ipi_lock(void)
+{
+	while (atomic_cmpxchg(&__nmi_ipi_lock, 0, 1) == 1)
+		cpu_relax();
+}
+
+static void nmi_ipi_unlock(void)
+{
+	smp_mb();
+	WARN_ON(atomic_read(&__nmi_ipi_lock) != 1);
+	atomic_set(&__nmi_ipi_lock, 0);
+}
+
+static void nmi_ipi_unlock_end(unsigned long *flags)
+{
+	nmi_ipi_unlock();
+	raw_local_irq_restore(*flags);
+}
+
+/*
+ * Platform NMI handler calls this to ack
+ */
+int smp_handle_nmi_ipi(struct pt_regs *regs)
+{
+	void (*fn)(struct pt_regs *);
+	unsigned long flags;
+	int me = raw_smp_processor_id();
+	int ret = 0;
+
+	/*
+	 * Unexpected NMIs are possible here because the interrupt may not
+	 * be able to distinguish NMI IPIs from other types of NMIs, or
+	 * because the caller may have timed out.
+	 */
+	nmi_ipi_lock_start(&flags);
+	if (!nmi_ipi_busy_count)
+		goto out;
+	if (!cpumask_test_cpu(me, &nmi_ipi_pending_mask))
+		goto out;
+
+	fn = nmi_ipi_function;
+	if (!fn)
+		goto out;
+
+	cpumask_clear_cpu(me, &nmi_ipi_pending_mask);
+	nmi_ipi_busy_count++;
+	nmi_ipi_unlock();
+
+	ret = 1;
+
+	fn(regs);
+
+	nmi_ipi_lock();
+	nmi_ipi_busy_count--;
+out:
+	nmi_ipi_unlock_end(&flags);
+
+	return ret;
+}
+
+static void do_smp_send_nmi_ipi(int cpu)
+{
+	if (cpu >= 0) {
+		do_message_pass(cpu, PPC_MSG_NMI_IPI);
+	} else {
+		int c;
+
+		for_each_online_cpu(c) {
+			if (c == raw_smp_processor_id())
+				continue;
+			do_message_pass(c, PPC_MSG_NMI_IPI);
+		}
+	}
+}
+
+/*
+ * - cpu is the target CPU (must not be this CPU), or NMI_IPI_ALL_OTHERS.
+ * - fn is the target callback function.
+ * - delay_us > 0 is the delay before giving up waiting for targets to
+ *   enter the handler, == 0 specifies indefinite delay.
+ */
+static int smp_send_nmi_ipi(int cpu, void (*fn)(struct pt_regs *), u64 delay_us)
+{
+	unsigned long flags;
+	int me = raw_smp_processor_id();
+	int ret = 1;
+
+	BUG_ON(cpu == me);
+	BUG_ON(cpu < 0 && cpu != NMI_IPI_ALL_OTHERS);
+
+	if (unlikely(!smp_ops))
+		return 0;
+
+	/* Take the nmi_ipi_busy count/lock with interrupts hard disabled */
+	nmi_ipi_lock_start(&flags);
+	while (nmi_ipi_busy_count) {
+		nmi_ipi_unlock_end(&flags);
+		cpu_relax();
+		nmi_ipi_lock_start(&flags);
+	}
+
+	nmi_ipi_function = fn;
+
+	if (cpu < 0) {
+		/* ALL_OTHERS */
+		cpumask_copy(&nmi_ipi_pending_mask, cpu_online_mask);
+		cpumask_clear_cpu(me, &nmi_ipi_pending_mask);
+	} else {
+		/* cpumask starts clear */
+		cpumask_set_cpu(cpu, &nmi_ipi_pending_mask);
+	}
+	nmi_ipi_busy_count++;
+	nmi_ipi_unlock();
+
+	do_smp_send_nmi_ipi(cpu);
+
+	while (!cpumask_empty(&nmi_ipi_pending_mask)) {
+		udelay(1);
+		if (delay_us) {
+			delay_us--;
+			if (!delay_us)
+				break;
+		}
+	}
+
+	nmi_ipi_lock();
+	if (!cpumask_empty(&nmi_ipi_pending_mask)) {
+		/* Could not gather all CPUs */
+		ret = 0;
+		cpumask_clear(&nmi_ipi_pending_mask);
+	}
+	nmi_ipi_busy_count--;
+	nmi_ipi_unlock_end(&flags);
+
+	return ret;
+}
+#endif /* CONFIG_NMI_IPI */
+
 #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 void tick_broadcast(const struct cpumask *mask)
 {
@@ -325,29 +503,22 @@ void tick_broadcast(const struct cpumask *mask)
 }
 #endif
 
-#if defined(CONFIG_DEBUGGER) || defined(CONFIG_KEXEC_CORE)
-void smp_send_debugger_break(void)
+#ifdef CONFIG_DEBUGGER
+void debugger_ipi_callback(struct pt_regs *regs)
 {
-	int cpu;
-	int me = raw_smp_processor_id();
-
-	if (unlikely(!smp_ops))
-		return;
+	debugger_ipi(regs);
+}
 
-	for_each_online_cpu(cpu)
-		if (cpu != me)
-			do_message_pass(cpu, PPC_MSG_DEBUGGER_BREAK);
+void smp_send_debugger_break(void)
+{
+	smp_send_nmi_ipi(NMI_IPI_ALL_OTHERS, debugger_ipi_callback, 1000000);
 }
 #endif
 
 #ifdef CONFIG_KEXEC_CORE
 void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
 {
-	crash_ipi_function_ptr = crash_ipi_callback;
-	if (crash_ipi_callback) {
-		mb();
-		smp_send_debugger_break();
-	}
+	smp_send_nmi_ipi(NMI_IPI_ALL_OTHERS, crash_ipi_callback, 1000000);
 }
 #endif
 

commit 6d11b87d55eb75007a3721c2de5938f5bbf607fb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:31 2017 +0200

    powerpc/smp: Replace open coded task affinity logic
    
    Init task invokes smp_ops->setup_cpu() from smp_cpus_done(). Init task can
    run on any online CPU at this point, but the setup_cpu() callback requires
    to be invoked on the boot CPU. This is achieved by temporarily setting the
    affinity of the calling user space thread to the requested CPU and reset it
    to the original affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    That's actually not a problem in this context as neither CPU hotplug nor
    affinity settings can happen, but the access to task_struct::cpus_allowed
    is about to restricted.
    
    Replace it with a call to work_on_cpu_safe() which achieves the same result.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.518053336@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 46f89e66a273..d68ed1f004a3 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -787,24 +787,21 @@ static struct sched_domain_topology_level powerpc_topology[] = {
 	{ NULL, },
 };
 
-void __init smp_cpus_done(unsigned int max_cpus)
+static __init long smp_setup_cpu_workfn(void *data __always_unused)
 {
-	cpumask_var_t old_mask;
+	smp_ops->setup_cpu(boot_cpuid);
+	return 0;
+}
 
-	/* We want the setup_cpu() here to be called from CPU 0, but our
-	 * init thread may have been "borrowed" by another CPU in the meantime
-	 * se we pin us down to CPU 0 for a short while
+void __init smp_cpus_done(unsigned int max_cpus)
+{
+	/*
+	 * We want the setup_cpu() here to be called on the boot CPU, but
+	 * init might run on any CPU, so make sure it's invoked on the boot
+	 * CPU.
 	 */
-	alloc_cpumask_var(&old_mask, GFP_NOWAIT);
-	cpumask_copy(old_mask, &current->cpus_allowed);
-	set_cpus_allowed_ptr(current, cpumask_of(boot_cpuid));
-	
 	if (smp_ops && smp_ops->setup_cpu)
-		smp_ops->setup_cpu(boot_cpuid);
-
-	set_cpus_allowed_ptr(current, old_mask);
-
-	free_cpumask_var(old_mask);
+		work_on_cpu_safe(boot_cpuid, smp_setup_cpu_workfn, NULL);
 
 	if (smp_ops && smp_ops->bringup_done)
 		smp_ops->bringup_done();
@@ -812,7 +809,6 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	dump_numa_cpu_topology();
 
 	set_sched_topology(powerpc_topology);
-
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit b87ac0218355a83abb899a0022bb2e5252879fc0
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Apr 13 20:16:22 2017 +1000

    powerpc: Introduce msgsnd/doorbell barrier primitives
    
    POWER9 changes requirements and adds new instructions for
    synchronization.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 17de938d41c5..2eca1e491e2b 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -245,12 +245,18 @@ void smp_muxed_ipi_message_pass(int cpu, int msg)
 #endif
 
 irqreturn_t smp_ipi_demux(void)
+{
+	mb();	/* order any irq clear */
+
+	return smp_ipi_demux_relaxed();
+}
+
+/* sync-free variant. Callers should ensure synchronization */
+irqreturn_t smp_ipi_demux_relaxed(void)
 {
 	struct cpu_messages *info;
 	unsigned long all;
 
-	mb();	/* order any irq clear */
-
 	info = this_cpu_ptr(&ipi_message);
 	do {
 		all = xchg(&info->messages, 0);

commit b866cc2199d6a6cdcefe4acfe4cfca3ac3c6d38e
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Apr 13 20:16:21 2017 +1000

    powerpc: Change the doorbell IPI calling convention
    
    Change the doorbell callers to know about their msgsnd addressing,
    rather than have them set a per-cpu target data tag at boot that gets
    sent to the cause_ipi functions. The data is only used for doorbell IPI
    functions, no other IPI types, so it makes sense to keep that detail
    local to doorbell.
    
    Have the platform code understand doorbell IPIs, rather than the
    interrupt controller code understand them. Platform code can look at
    capabilities it has available and decide which to use.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 6e61cdb89194..17de938d41c5 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -39,6 +39,7 @@
 #include <asm/irq.h>
 #include <asm/hw_irq.h>
 #include <asm/kvm_ppc.h>
+#include <asm/dbell.h>
 #include <asm/page.h>
 #include <asm/pgtable.h>
 #include <asm/prom.h>
@@ -211,17 +212,9 @@ int smp_request_message_ipi(int virq, int msg)
 #ifdef CONFIG_PPC_SMP_MUXED_IPI
 struct cpu_messages {
 	long messages;			/* current messages */
-	unsigned long data;		/* data for cause ipi */
 };
 static DEFINE_PER_CPU_SHARED_ALIGNED(struct cpu_messages, ipi_message);
 
-void smp_muxed_ipi_set_data(int cpu, unsigned long data)
-{
-	struct cpu_messages *info = &per_cpu(ipi_message, cpu);
-
-	info->data = data;
-}
-
 void smp_muxed_ipi_set_message(int cpu, int msg)
 {
 	struct cpu_messages *info = &per_cpu(ipi_message, cpu);
@@ -236,14 +229,13 @@ void smp_muxed_ipi_set_message(int cpu, int msg)
 
 void smp_muxed_ipi_message_pass(int cpu, int msg)
 {
-	struct cpu_messages *info = &per_cpu(ipi_message, cpu);
-
 	smp_muxed_ipi_set_message(cpu, msg);
+
 	/*
 	 * cause_ipi functions are required to include a full barrier
 	 * before doing whatever causes the IPI.
 	 */
-	smp_ops->cause_ipi(cpu, info->data);
+	smp_ops->cause_ipi(cpu);
 }
 
 #ifdef __BIG_ENDIAN__
@@ -254,11 +246,12 @@ void smp_muxed_ipi_message_pass(int cpu, int msg)
 
 irqreturn_t smp_ipi_demux(void)
 {
-	struct cpu_messages *info = this_cpu_ptr(&ipi_message);
+	struct cpu_messages *info;
 	unsigned long all;
 
 	mb();	/* order any irq clear */
 
+	info = this_cpu_ptr(&ipi_message);
 	do {
 		all = xchg(&info->messages, 0);
 #if defined(CONFIG_KVM_XICS) && defined(CONFIG_KVM_BOOK3S_HV_POSSIBLE)

commit a978e13965a40ac07163643cc3fa0ddb0d354198
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:49 2017 +1000

    powerpc/smp: Remove migrate_irq() custom implementation
    
    Some powerpc platforms use this to move IRQs away from a CPU being
    unplugged. This function has several bugs such as not taking the right
    locks or failing to NULL check pointers.
    
    There's a new generic function doing exactly the same thing without all
    the bugs, so let's use it instead.
    
    mpe: The obvious place for the select of GENERIC_IRQ_MIGRATION is on
    HOTPLUG_CPU, but that doesn't work. On some configs PM_SLEEP_SMP will
    select HOTPLUG_CPU even though its dependencies are not met, which means
    the select of GENERIC_IRQ_MIGRATION doesn't happen. That leads to the
    build breaking. Fix it by moving the select of GENERIC_IRQ_MIGRATION to
    SMP.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index b12f5f0a408f..6e61cdb89194 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -439,7 +439,14 @@ int generic_cpu_disable(void)
 #ifdef CONFIG_PPC64
 	vdso_data->processorCount--;
 #endif
-	migrate_irqs();
+	/* Update affinity of all IRQs previously aimed at this CPU */
+	irq_migrate_all_off_this_cpu();
+
+	/* Give the CPU time to drain in-flight ones */
+	local_irq_enable();
+	mdelay(1);
+	local_irq_disable();
+
 	return 0;
 }
 

commit 14d4ae5c4cb89c05262fe41cb7a26f6ba949d8df
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:48 2017 +1000

    powerpc: Add optional smp_ops->prepare_cpu SMP callback
    
    Some platforms (will) need to perform allocations before bringing
    a new CPU online. Doing it from smp_ops->setup_cpu is the wrong
    thing to do:
    
     - It has no useful failure path (too late)
     - Calling any allocator will enable interrupts prematurely
       causing problems with large decrementer among others
    
    Instead, add a new callback that is called from __cpu_up (so from
    the context trying to online the new CPU) at a point where we
    can safely allocate and handle failures.
    
    This will be used by XIVE support.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 46f89e66a273..b12f5f0a408f 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -521,6 +521,16 @@ int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 
 	cpu_idle_thread_init(cpu, tidle);
 
+	/*
+	 * The platform might need to allocate resources prior to bringing
+	 * up the CPU
+	 */
+	if (smp_ops->prepare_cpu) {
+		rc = smp_ops->prepare_cpu(cpu);
+		if (rc)
+			return rc;
+	}
+
 	/* Make sure callin-map entry is 0 (can be leftover a CPU
 	 * hotplug
 	 */

commit 68e21be2916b359fd8afb536c1911dc014cfd03e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 19:08:20 2017 +0100

    sched/headers: Move task->mm handling methods to <linux/sched/mm.h>
    
    Move the following task->mm helper APIs into a new header file,
    <linux/sched/mm.h>, to further reduce the size and complexity
    of <linux/sched.h>.
    
    Here are how the APIs are used in various kernel files:
    
      # mm_alloc():
      arch/arm/mach-rpc/ecard.c
      fs/exec.c
      include/linux/sched/mm.h
      kernel/fork.c
    
      # __mmdrop():
      arch/arc/include/asm/mmu_context.h
      include/linux/sched/mm.h
      kernel/fork.c
    
      # mmdrop():
      arch/arm/mach-rpc/ecard.c
      arch/m68k/sun3/mmu_emu.c
      arch/x86/mm/tlb.c
      drivers/gpu/drm/amd/amdkfd/kfd_process.c
      drivers/gpu/drm/i915/i915_gem_userptr.c
      drivers/infiniband/hw/hfi1/file_ops.c
      drivers/vfio/vfio_iommu_spapr_tce.c
      fs/exec.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      fs/proc/task_nommu.c
      fs/userfaultfd.c
      include/linux/mmu_notifier.h
      include/linux/sched/mm.h
      kernel/fork.c
      kernel/futex.c
      kernel/sched/core.c
      mm/khugepaged.c
      mm/ksm.c
      mm/mmu_context.c
      mm/mmu_notifier.c
      mm/oom_kill.c
      virt/kvm/kvm_main.c
    
      # mmdrop_async_fn():
      include/linux/sched/mm.h
    
      # mmdrop_async():
      include/linux/sched/mm.h
      kernel/fork.c
    
      # mmget_not_zero():
      fs/userfaultfd.c
      include/linux/sched/mm.h
      mm/oom_kill.c
    
      # mmput():
      arch/arc/include/asm/mmu_context.h
      arch/arc/kernel/troubleshoot.c
      arch/frv/mm/mmu-context.c
      arch/powerpc/platforms/cell/spufs/context.c
      arch/sparc/include/asm/mmu_context_32.h
      drivers/android/binder.c
      drivers/gpu/drm/etnaviv/etnaviv_gem.c
      drivers/gpu/drm/i915/i915_gem_userptr.c
      drivers/infiniband/core/umem.c
      drivers/infiniband/core/umem_odp.c
      drivers/infiniband/core/uverbs_main.c
      drivers/infiniband/hw/mlx4/main.c
      drivers/infiniband/hw/mlx5/main.c
      drivers/infiniband/hw/usnic/usnic_uiom.c
      drivers/iommu/amd_iommu_v2.c
      drivers/iommu/intel-svm.c
      drivers/lguest/lguest_user.c
      drivers/misc/cxl/fault.c
      drivers/misc/mic/scif/scif_rma.c
      drivers/oprofile/buffer_sync.c
      drivers/vfio/vfio_iommu_type1.c
      drivers/vhost/vhost.c
      drivers/xen/gntdev.c
      fs/exec.c
      fs/proc/array.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      fs/proc/task_nommu.c
      fs/userfaultfd.c
      include/linux/sched/mm.h
      kernel/cpuset.c
      kernel/events/core.c
      kernel/events/uprobes.c
      kernel/exit.c
      kernel/fork.c
      kernel/ptrace.c
      kernel/sys.c
      kernel/trace/trace_output.c
      kernel/tsacct.c
      mm/memcontrol.c
      mm/memory.c
      mm/mempolicy.c
      mm/migrate.c
      mm/mmu_notifier.c
      mm/nommu.c
      mm/oom_kill.c
      mm/process_vm_access.c
      mm/rmap.c
      mm/swapfile.c
      mm/util.c
      virt/kvm/async_pf.c
    
      # mmput_async():
      include/linux/sched/mm.h
      kernel/fork.c
      mm/oom_kill.c
    
      # get_task_mm():
      arch/arc/kernel/troubleshoot.c
      arch/powerpc/platforms/cell/spufs/context.c
      drivers/android/binder.c
      drivers/gpu/drm/etnaviv/etnaviv_gem.c
      drivers/infiniband/core/umem.c
      drivers/infiniband/core/umem_odp.c
      drivers/infiniband/hw/mlx4/main.c
      drivers/infiniband/hw/mlx5/main.c
      drivers/infiniband/hw/usnic/usnic_uiom.c
      drivers/iommu/amd_iommu_v2.c
      drivers/iommu/intel-svm.c
      drivers/lguest/lguest_user.c
      drivers/misc/cxl/fault.c
      drivers/misc/mic/scif/scif_rma.c
      drivers/oprofile/buffer_sync.c
      drivers/vfio/vfio_iommu_type1.c
      drivers/vhost/vhost.c
      drivers/xen/gntdev.c
      fs/proc/array.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      include/linux/sched/mm.h
      kernel/cpuset.c
      kernel/events/core.c
      kernel/exit.c
      kernel/fork.c
      kernel/ptrace.c
      kernel/sys.c
      kernel/trace/trace_output.c
      kernel/tsacct.c
      mm/memcontrol.c
      mm/memory.c
      mm/mempolicy.c
      mm/migrate.c
      mm/mmu_notifier.c
      mm/nommu.c
      mm/util.c
    
      # mm_access():
      fs/proc/base.c
      include/linux/sched/mm.h
      kernel/fork.c
      mm/process_vm_access.c
    
      # mm_release():
      arch/arc/include/asm/mmu_context.h
      fs/exec.c
      include/linux/sched/mm.h
      include/uapi/linux/sched.h
      kernel/exit.c
      kernel/fork.c
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index fce17789c675..46f89e66a273 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -19,7 +19,7 @@
 
 #include <linux/kernel.h>
 #include <linux/export.h>
-#include <linux/sched.h>
+#include <linux/sched/mm.h>
 #include <linux/sched/topology.h>
 #include <linux/smp.h>
 #include <linux/interrupt.h>

commit 105ab3d8ce7269887d24d224054677125e18037c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/topology.h>
    
    We are going to split <linux/sched/topology.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/topology.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 21fdf02583fe..fce17789c675 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -20,6 +20,7 @@
 #include <linux/kernel.h>
 #include <linux/export.h>
 #include <linux/sched.h>
+#include <linux/sched/topology.h>
 #include <linux/smp.h>
 #include <linux/interrupt.h>
 #include <linux/delay.h>

commit 0c98d344fe5c27f6e4bce42ac503e9e9a51c7d1d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 15:38:10 2017 +0100

    sched/core: Remove the tsk_cpus_allowed() wrapper
    
    So the original intention of tsk_cpus_allowed() was to 'future-proof'
    the field - but it's pretty ineffectual at that, because half of
    the code uses ->cpus_allowed directly ...
    
    Also, the wrapper makes the code longer than the original expression!
    
    So just get rid of it. This also shrinks <linux/sched.h> a bit.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 573fb3a461b5..21fdf02583fe 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -795,7 +795,7 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	 * se we pin us down to CPU 0 for a short while
 	 */
 	alloc_cpumask_var(&old_mask, GFP_NOWAIT);
-	cpumask_copy(old_mask, tsk_cpus_allowed(current));
+	cpumask_copy(old_mask, &current->cpus_allowed);
 	set_cpus_allowed_ptr(current, cpumask_of(boot_cpuid));
 	
 	if (smp_ops && smp_ops->setup_cpu)

commit f1f1007644ffc8051a4c11427d58b1967ae7b75a
Author: Vegard Nossum <vegard.nossum@oracle.com>
Date:   Mon Feb 27 14:30:07 2017 -0800

    mm: add new mmgrab() helper
    
    Apart from adding the helper function itself, the rest of the kernel is
    converted mechanically using:
    
      git grep -l 'atomic_inc.*mm_count' | xargs sed -i 's/atomic_inc(&\(.*\)->mm_count);/mmgrab\(\1\);/'
      git grep -l 'atomic_inc.*mm_count' | xargs sed -i 's/atomic_inc(&\(.*\)\.mm_count);/mmgrab\(\&\1\);/'
    
    This is needed for a later patch that hooks into the helper, but might
    be a worthwhile cleanup on its own.
    
    (Michal Hocko provided most of the kerneldoc comment.)
    
    Link: http://lkml.kernel.org/r/20161218123229.22952-1-vegard.nossum@oracle.com
    Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 893bd7f79be6..573fb3a461b5 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -707,7 +707,7 @@ void start_secondary(void *unused)
 	unsigned int cpu = smp_processor_id();
 	int i, base;
 
-	atomic_inc(&init_mm.mm_count);
+	mmgrab(&init_mm);
 	current->active_mm = &init_mm;
 
 	smp_store_cpu_info(cpu);

commit da6658859b9c734fee36570f3a7d51764c6c3838
Author: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
Date:   Tue Nov 29 23:45:50 2016 +1100

    powerpc: Change places using CONFIG_KEXEC to use CONFIG_KEXEC_CORE instead.
    
    Commit 2965faa5e03d ("kexec: split kexec_load syscall from kexec core
    code") introduced CONFIG_KEXEC_CORE so that CONFIG_KEXEC means whether
    the kexec_load system call should be compiled-in and CONFIG_KEXEC_FILE
    means whether the kexec_file_load system call should be compiled-in.
    These options can be set independently from each other.
    
    Since until now powerpc only supported kexec_load, CONFIG_KEXEC and
    CONFIG_KEXEC_CORE were synonyms. That is not the case anymore, so we
    need to make a distinction. Almost all places where CONFIG_KEXEC was
    being used should be using CONFIG_KEXEC_CORE instead, since
    kexec_file_load also needs that code compiled in.
    
    Signed-off-by: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 9c6f3fd58059..893bd7f79be6 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -193,7 +193,7 @@ int smp_request_message_ipi(int virq, int msg)
 	if (msg < 0 || msg > PPC_MSG_DEBUGGER_BREAK) {
 		return -EINVAL;
 	}
-#if !defined(CONFIG_DEBUGGER) && !defined(CONFIG_KEXEC)
+#if !defined(CONFIG_DEBUGGER) && !defined(CONFIG_KEXEC_CORE)
 	if (msg == PPC_MSG_DEBUGGER_BREAK) {
 		return 1;
 	}
@@ -325,7 +325,7 @@ void tick_broadcast(const struct cpumask *mask)
 }
 #endif
 
-#if defined(CONFIG_DEBUGGER) || defined(CONFIG_KEXEC)
+#if defined(CONFIG_DEBUGGER) || defined(CONFIG_KEXEC_CORE)
 void smp_send_debugger_break(void)
 {
 	int cpu;
@@ -340,7 +340,7 @@ void smp_send_debugger_break(void)
 }
 #endif
 
-#ifdef CONFIG_KEXEC
+#ifdef CONFIG_KEXEC_CORE
 void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
 {
 	crash_ipi_function_ptr = crash_ipi_callback;

commit 19ab58d19ef6291bab07c8e7110f3198c23432c1
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Tue Aug 16 19:50:12 2016 +0800

    powerpc, hotplug: Avoid to touch non-existent cpumasks.
    
    We observed a kernel oops when running a PPC guest with config NR_CPUS=4
    and qemu option "-smp cores=1,threads=8":
    
    [   30.634781] Unable to handle kernel paging request for data at
    address 0xc00000014192eb17
    [   30.636173] Faulting instruction address: 0xc00000000003e5cc
    [   30.637069] Oops: Kernel access of bad area, sig: 11 [#1]
    [   30.637877] SMP NR_CPUS=4 NUMA pSeries
    [   30.638471] Modules linked in:
    [   30.638949] CPU: 3 PID: 27 Comm: migration/3 Not tainted
    4.7.0-07963-g9714b26 #1
    [   30.640059] task: c00000001e29c600 task.stack: c00000001e2a8000
    [   30.640956] NIP: c00000000003e5cc LR: c00000000003e550 CTR:
    0000000000000000
    [   30.642001] REGS: c00000001e2ab8e0 TRAP: 0300   Not tainted
    (4.7.0-07963-g9714b26)
    [   30.643139] MSR: 8000000102803033 <SF,VEC,VSX,FP,ME,IR,DR,RI,LE,TM[E]>  CR: 22004084  XER: 00000000
    [   30.644583] CFAR: c000000000009e98 DAR: c00000014192eb17 DSISR: 40000000 SOFTE: 0
    GPR00: c00000000140a6b8 c00000001e2abb60 c0000000016dd300 0000000000000003
    GPR04: 0000000000000000 0000000000000004 c0000000016e5920 0000000000000008
    GPR08: 0000000000000004 c00000014192eb17 0000000000000000 0000000000000020
    GPR12: c00000000140a6c0 c00000000ffffc00 c0000000000d3ea8 c00000001e005680
    GPR16: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
    GPR20: 0000000000000000 c00000001e6b3a00 0000000000000000 0000000000000001
    GPR24: c00000001ff85138 c00000001ff85130 000000001eb6f000 0000000000000001
    GPR28: 0000000000000000 c0000000017014e0 0000000000000000 0000000000000018
    [   30.653882] NIP [c00000000003e5cc] __cpu_disable+0xcc/0x190
    [   30.654713] LR [c00000000003e550] __cpu_disable+0x50/0x190
    [   30.655528] Call Trace:
    [   30.655893] [c00000001e2abb60] [c00000000003e550] __cpu_disable+0x50/0x190 (unreliable)
    [   30.657280] [c00000001e2abbb0] [c0000000000aca0c] take_cpu_down+0x5c/0x100
    [   30.658365] [c00000001e2abc10] [c000000000163918] multi_cpu_stop+0x1a8/0x1e0
    [   30.659617] [c00000001e2abc60] [c000000000163cc0] cpu_stopper_thread+0xf0/0x1d0
    [   30.660737] [c00000001e2abd20] [c0000000000d8d70] smpboot_thread_fn+0x290/0x2a0
    [   30.661879] [c00000001e2abd80] [c0000000000d3fa8] kthread+0x108/0x130
    [   30.662876] [c00000001e2abe30] [c000000000009968] ret_from_kernel_thread+0x5c/0x74
    [   30.664017] Instruction dump:
    [   30.664477] 7bde1f24 38a00000 787f1f24 3b600001 39890008 7d204b78 7d05e214 7d0b07b4
    [   30.665642] 796b1f24 7d26582a 7d204a14 7d29f214 <7d4048a8> 7d4a3878 7d4049ad 40c2fff4
    [   30.666854] ---[ end trace 32643b7195717741 ]---
    
    The reason of this is that in __cpu_disable(), when we try to set the
    cpu_sibling_mask or cpu_core_mask of the sibling CPUs of the disabled
    one, we don't check whether the current configuration employs those
    sibling CPUs(hw threads). And if a CPU is not employed by a
    configuration, the percpu structures cpu_{sibling,core}_mask are not
    allocated, therefore accessing those cpumasks will result in problems as
    above.
    
    This patch fixes this problem by adding an addition check on whether the
    id is no less than nr_cpu_ids in the sibling CPU iteration code.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 25a39052bf6b..9c6f3fd58059 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -830,7 +830,7 @@ int __cpu_disable(void)
 
 	/* Update sibling maps */
 	base = cpu_first_thread_sibling(cpu);
-	for (i = 0; i < threads_per_core; i++) {
+	for (i = 0; i < threads_per_core && base + i < nr_cpu_ids; i++) {
 		cpumask_clear_cpu(cpu, cpu_sibling_mask(base + i));
 		cpumask_clear_cpu(base + i, cpu_sibling_mask(cpu));
 		cpumask_clear_cpu(cpu, cpu_core_mask(base + i));

commit b92a226e528423b8d249dd09bb450d53361fbfcb
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sat Jul 23 14:42:40 2016 +0530

    powerpc: Move cpu_has_feature() to a separate file
    
    We plan to use jump label for cpu_has_feature(). In order to implement
    this we need to include the linux/jump_label.h in asm/cputable.h.
    
    Unfortunately if we do that it leads to an include loop. The root of the
    problem seems to be that reg.h needs cputable.h (for CPU_FTRs), and then
    cputable.h via jump_label.h eventually pulls in hw_irq.h which needs
    reg.h (for MSR_EE).
    
    So move cpu_has_feature() to a separate file on its own.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [mpe: Rename to cpu_has_feature.h and flesh out change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 5a1f015ea9f3..25a39052bf6b 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -55,6 +55,7 @@
 #include <asm/debug.h>
 #include <asm/kexec.h>
 #include <asm/asm-prototypes.h>
+#include <asm/cpu_has_feature.h>
 
 #ifdef DEBUG
 #include <asm/udbg.h>

commit f8ab481066e7246e4b272233aa0b6948f5069f41
Author: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
Date:   Thu Jun 2 08:45:14 2016 -0300

    powerpc: export cpu_to_core_id()
    
    Export cpu_to_core_id(). This will be used by the lpfc driver.
    
    This enables topology_core_id() from <linux/topology.h> (defined
    to cpu_to_core_id() in arch/powerpc/include/asm/topology.h) to be
    used by (non-builtin) modules.
    
    That is arch-neutral, already used by eg, drivers/base/topology.c,
    but it is builtin (obj-y in Makefile) thus didn't need the export.
    
    Since the module uses topology_core_id() and this is defined to
    cpu_to_core_id(), it needs the export, otherwise:
    
        ERROR: "cpu_to_core_id" [drivers/scsi/lpfc/lpfc.ko] undefined!
    
    Tested on next-20160601.
    
    Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 1b55c7864291..5a1f015ea9f3 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -595,6 +595,7 @@ int cpu_to_core_id(int cpu)
 	of_node_put(np);
 	return id;
 }
+EXPORT_SYMBOL_GPL(cpu_to_core_id);
 
 /* Helper routines for cpu to core mapping */
 int cpu_core_index_of_thread(int cpu)

commit 665e87ffe1c400c525c3a4cd6fcb5db75972fadd
Author: Daniel Axtens <dja@axtens.net>
Date:   Wed May 18 11:16:51 2016 +1000

    powerpc/sparse: Include headers containing prototypes
    
    Sometimes headers that provide prototypes for functions are
    accidentally omitted from the files that define the functions.
    
    Fix a couple of times that occurs.
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index f1adc3c4f4ca..1b55c7864291 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -31,6 +31,7 @@
 #include <linux/cpu.h>
 #include <linux/notifier.h>
 #include <linux/topology.h>
+#include <linux/profile.h>
 
 #include <asm/ptrace.h>
 #include <linux/atomic.h>

commit 42f5b4cacd783faf05e3ff8bf85e8be31f3dfa9d
Author: Daniel Axtens <dja@axtens.net>
Date:   Wed May 18 11:16:50 2016 +1000

    powerpc: Introduce asm-prototypes.h
    
    Sparse picked up a number of functions that are implemented in C and
    then only referred to in asm code.
    
    This introduces asm-prototypes.h, which provides a place for
    prototypes of these functions.
    
    This silences some sparse warnings.
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    [mpe: Add include guards, clean up copyright & GPL text]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 55c924b65f71..f1adc3c4f4ca 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -53,6 +53,7 @@
 #include <asm/vdso.h>
 #include <asm/debug.h>
 #include <asm/kexec.h>
+#include <asm/asm-prototypes.h>
 
 #ifdef DEBUG
 #include <asm/udbg.h>

commit e9d867a67fd03ccc07248ca4e9c2f74fed494d5b
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Thu Mar 10 12:54:08 2016 +0100

    sched: Allow per-cpu kernel threads to run on online && !active
    
    In order to enable symmetric hotplug, we must mirror the online &&
    !active state of cpu-down on the cpu-up side.
    
    However, to retain sanity, limit this state to per-cpu kthreads.
    
    Aside from the change to set_cpus_allowed_ptr(), which allow moving
    the per-cpu kthreads on, the other critical piece is the cpu selection
    for pinned tasks in select_task_rq(). This avoids dropping into
    select_fallback_rq().
    
    select_fallback_rq() cannot be allowed to select !active cpus because
    its used to migrate user tasks away. And we do not want to move user
    tasks onto cpus that are in transition.
    
    Requested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160301152303.GV6356@twins.programming.kicks-ass.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8cac1eb41466..55c924b65f71 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -565,7 +565,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 		smp_ops->give_timebase();
 
 	/* Wait until cpu puts itself in the online & active maps */
-	while (!cpu_online(cpu) || !cpu_active(cpu))
+	while (!cpu_online(cpu))
 		cpu_relax();
 
 	return 0;

commit d5e2d00898bdfed9586472679760fc81a2ca2d02
Merge: 31e182363b39 6e669f085d59
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 19 15:38:41 2016 -0700

    Merge tag 'powerpc-4.6-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "This was delayed a day or two by some build-breakage on old toolchains
      which we've now fixed.
    
      There's two PCI commits both acked by Bjorn.
    
      There's one commit to mm/hugepage.c which is (co)authored by Kirill.
    
      Highlights:
       - Restructure Linux PTE on Book3S/64 to Radix format from Paul
         Mackerras
       - Book3s 64 MMU cleanup in preparation for Radix MMU from Aneesh
         Kumar K.V
       - Add POWER9 cputable entry from Michael Neuling
       - FPU/Altivec/VSX save/restore optimisations from Cyril Bur
       - Add support for new ftrace ABI on ppc64le from Torsten Duwe
    
      Various cleanups & minor fixes from:
       - Adam Buchbinder, Andrew Donnellan, Balbir Singh, Christophe Leroy,
         Cyril Bur, Luis Henriques, Madhavan Srinivasan, Pan Xinhui, Russell
         Currey, Sukadev Bhattiprolu, Suraj Jitindar Singh.
    
      General:
       - atomics: Allow architectures to define their own __atomic_op_*
         helpers from Boqun Feng
       - Implement atomic{, 64}_*_return_* variants and acquire/release/
         relaxed variants for (cmp)xchg from Boqun Feng
       - Add powernv_defconfig from Jeremy Kerr
       - Fix BUG_ON() reporting in real mode from Balbir Singh
       - Add xmon command to dump OPAL msglog from Andrew Donnellan
       - Add xmon command to dump process/task similar to ps(1) from Douglas
         Miller
       - Clean up memory hotplug failure paths from David Gibson
    
      pci/eeh:
       - Redesign SR-IOV on PowerNV to give absolute isolation between VFs
         from Wei Yang.
       - EEH Support for SRIOV VFs from Wei Yang and Gavin Shan.
       - PCI/IOV: Rename and export virtfn_{add, remove} from Wei Yang
       - PCI: Add pcibios_bus_add_device() weak function from Wei Yang
       - MAINTAINERS: Update EEH details and maintainership from Russell
         Currey
    
      cxl:
       - Support added to the CXL driver for running on both bare-metal and
         hypervisor systems, from Christophe Lombard and Frederic Barrat.
       - Ignore probes for virtual afu pci devices from Vaibhav Jain
    
      perf:
       - Export Power8 generic and cache events to sysfs from Sukadev
         Bhattiprolu
       - hv-24x7: Fix usage with chip events, display change in counter
         values, display domain indices in sysfs, eliminate domain suffix in
         event names, from Sukadev Bhattiprolu
    
      Freescale:
       - Updates from Scott: "Highlights include 8xx optimizations, 32-bit
         checksum optimizations, 86xx consolidation, e5500/e6500 cpu
         hotplug, more fman and other dt bits, and minor fixes/cleanup"
    
    * tag 'powerpc-4.6-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (179 commits)
      powerpc: Fix unrecoverable SLB miss during restore_math()
      powerpc/8xx: Fix do_mtspr_cpu6() build on older compilers
      powerpc/rcpm: Fix build break when SMP=n
      powerpc/book3e-64: Use hardcoded mttmr opcode
      powerpc/fsl/dts: Add "jedec,spi-nor" flash compatible
      powerpc/T104xRDB: add tdm riser card node to device tree
      powerpc32: PAGE_EXEC required for inittext
      powerpc/mpc85xx: Add pcsphy nodes to FManV3 device tree
      powerpc/mpc85xx: Add MDIO bus muxing support to the board device tree(s)
      powerpc/86xx: Introduce and use common dtsi
      powerpc/86xx: Update device tree
      powerpc/86xx: Move dts files to fsl directory
      powerpc/86xx: Switch to kconfig fragments approach
      powerpc/86xx: Update defconfigs
      powerpc/86xx: Consolidate common platform code
      powerpc32: Remove one insn in mulhdu
      powerpc32: small optimisation in flush_icache_range()
      powerpc: Simplify test in __dma_sync()
      powerpc32: move xxxxx_dcache_range() functions inline
      powerpc32: Remove clear_pages() and define clear_page() inline
      ...

commit 10dc3747661bea9215417b659449bb7b8ed3df2c
Merge: 047486d8e7c2 f958ee745f70
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 16 09:55:35 2016 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "One of the largest releases for KVM...  Hardly any generic
      changes, but lots of architecture-specific updates.
    
      ARM:
       - VHE support so that we can run the kernel at EL2 on ARMv8.1 systems
       - PMU support for guests
       - 32bit world switch rewritten in C
       - various optimizations to the vgic save/restore code.
    
      PPC:
       - enabled KVM-VFIO integration ("VFIO device")
       - optimizations to speed up IPIs between vcpus
       - in-kernel handling of IOMMU hypercalls
       - support for dynamic DMA windows (DDW).
    
      s390:
       - provide the floating point registers via sync regs;
       - separated instruction vs.  data accesses
       - dirty log improvements for huge guests
       - bugfixes and documentation improvements.
    
      x86:
       - Hyper-V VMBus hypercall userspace exit
       - alternative implementation of lowest-priority interrupts using
         vector hashing (for better VT-d posted interrupt support)
       - fixed guest debugging with nested virtualizations
       - improved interrupt tracking in the in-kernel IOAPIC
       - generic infrastructure for tracking writes to guest
         memory - currently its only use is to speedup the legacy shadow
         paging (pre-EPT) case, but in the future it will be used for
         virtual GPUs as well
       - much cleanup (LAPIC, kvmclock, MMU, PIT), including ubsan fixes"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (217 commits)
      KVM: x86: remove eager_fpu field of struct kvm_vcpu_arch
      KVM: x86: disable MPX if host did not enable MPX XSAVE features
      arm64: KVM: vgic-v3: Only wipe LRs on vcpu exit
      arm64: KVM: vgic-v3: Reset LRs at boot time
      arm64: KVM: vgic-v3: Do not save an LR known to be empty
      arm64: KVM: vgic-v3: Save maintenance interrupt state only if required
      arm64: KVM: vgic-v3: Avoid accessing ICH registers
      KVM: arm/arm64: vgic-v2: Make GICD_SGIR quicker to hit
      KVM: arm/arm64: vgic-v2: Only wipe LRs on vcpu exit
      KVM: arm/arm64: vgic-v2: Reset LRs at boot time
      KVM: arm/arm64: vgic-v2: Do not save an LR known to be empty
      KVM: arm/arm64: vgic-v2: Move GICH_ELRSR saving to its own function
      KVM: arm/arm64: vgic-v2: Save maintenance interrupt state only if required
      KVM: arm/arm64: vgic-v2: Avoid accessing GICH registers
      KVM: s390: allocate only one DMA page per VM
      KVM: s390: enable STFLE interpretation only if enabled for the guest
      KVM: s390: wake up when the VCPU cpu timer expires
      KVM: s390: step the VCPU timer while in enabled wait
      KVM: s390: protect VCPU cpu timer with a seqcount
      KVM: s390: step VCPU cpu timer during kvm_run ioctl
      ...

commit 2f4f1f815bc6d03ea42d4f67dd1e284525e7524e
Author: chenhui zhao <chenhui.zhao@freescale.com>
Date:   Fri Nov 20 17:14:01 2015 +0800

    powerpc/mpc85xx: Add hotplug support on E5500 and E500MC cores
    
    Freescale E500MC and E5500 core-based platforms, like P4080, T1040,
    support disabling/enabling CPU dynamically.
    This patch adds this feature on those platforms.
    
    Signed-off-by: Chenhui Zhao <chenhui.zhao@freescale.com>
    Signed-off-by: Tang Yuantian <Yuantian.Tang@feescale.com>
    [scottwood: removed unused pr_fmt]
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index ec9ec2058d2d..8575d044775d 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -427,7 +427,7 @@ void generic_cpu_die(unsigned int cpu)
 
 	for (i = 0; i < 100; i++) {
 		smp_rmb();
-		if (per_cpu(cpu_state, cpu) == CPU_DEAD)
+		if (is_cpu_dead(cpu))
 			return;
 		msleep(100);
 	}
@@ -454,6 +454,11 @@ int generic_check_cpu_restart(unsigned int cpu)
 	return per_cpu(cpu_state, cpu) == CPU_UP_PREPARE;
 }
 
+int is_cpu_dead(unsigned int cpu)
+{
+	return per_cpu(cpu_state, cpu) == CPU_DEAD;
+}
+
 static bool secondaries_inhibited(void)
 {
 	return kvm_hv_mode_active();

commit fc6d73d67436e7784758a831227bd019547a3f73
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 26 18:43:40 2016 +0000

    arch/hotplug: Call into idle with a proper state
    
    Let the non boot cpus call into idle with the corresponding hotplug state, so
    the hotplug core can handle the further bringup. That's a first step to
    convert the boot side of the hotplugged cpus to do all the synchronization
    with the other side through the state machine. For now it'll only start the
    hotplug thread and kick the full bringup of the cpu.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: "Srivatsa S. Bhat" <srivatsa@mit.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/20160226182341.614102639@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index ec9ec2058d2d..cc13d4c83291 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -727,7 +727,7 @@ void start_secondary(void *unused)
 
 	local_irq_enable();
 
-	cpu_startup_entry(CPUHP_ONLINE);
+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
 
 	BUG();
 }

commit e17769eb8c897101e2c6df62ec397e450b6e53b4
Author: Suresh E. Warrier <warrier@linux.vnet.ibm.com>
Date:   Mon Dec 21 16:22:51 2015 -0600

    KVM: PPC: Book3S HV: Send IPI to host core to wake VCPU
    
    This patch adds support to real-mode KVM to search for a core
    running in the host partition and send it an IPI message with
    VCPU to be woken. This avoids having to switch to the host
    partition to complete an H_IPI hypercall when the VCPU which
    is the target of the the H_IPI is not loaded (is not running
    in the guest).
    
    The patch also includes the support in the IPI handler running
    in the host to do the wakeup by calling kvmppc_xics_ipi_action
    for the PPC_MSG_RM_HOST_ACTION message.
    
    When a guest is being destroyed, we need to ensure that there
    are no pending IPIs waiting to wake up a VCPU before we free
    the VCPUs of the guest. This is accomplished by:
    - Forces a PPC_MSG_CALL_FUNCTION IPI to be completed by all CPUs
      before freeing any VCPUs in kvm_arch_destroy_vm().
    - Any PPC_MSG_RM_HOST_ACTION messages must be executed first
      before any other PPC_MSG_CALL_FUNCTION messages.
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e222efcf6aef..cb8be5dc118a 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -257,6 +257,17 @@ irqreturn_t smp_ipi_demux(void)
 
 	do {
 		all = xchg(&info->messages, 0);
+#if defined(CONFIG_KVM_XICS) && defined(CONFIG_KVM_BOOK3S_HV_POSSIBLE)
+		/*
+		 * Must check for PPC_MSG_RM_HOST_ACTION messages
+		 * before PPC_MSG_CALL_FUNCTION messages because when
+		 * a VM is destroyed, we call kick_all_cpus_sync()
+		 * to ensure that any pending PPC_MSG_RM_HOST_ACTION
+		 * messages have completed before we free any VCPUs.
+		 */
+		if (all & IPI_MESSAGE(PPC_MSG_RM_HOST_ACTION))
+			kvmppc_xics_ipi_action();
+#endif
 		if (all & IPI_MESSAGE(PPC_MSG_CALL_FUNCTION))
 			generic_smp_call_function_interrupt();
 		if (all & IPI_MESSAGE(PPC_MSG_RESCHEDULE))

commit 31639c77e0a7f9f742c813ae697f337b44981ed2
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Thu Dec 17 14:59:04 2015 -0600

    powerpc/smp: Add smp_muxed_ipi_set_message
    
    smp_muxed_ipi_message_pass() invokes smp_ops->cause_ipi, which
    uses an ioremapped address to access registers on the XICS
    interrupt controller to cause the IPI. Because of this real
    mode callers cannot call smp_muxed_ipi_message_pass() for IPI
    messaging.
    
    This patch creates a separate function smp_muxed_ipi_set_message
    just to set the IPI message without the cause_ipi routine.
    After calling this function to set the IPI message, real
    mode callers must cause the IPI by writing to the XICS registers
    directly.
    
    As part of this, we also change smp_muxed_ipi_message_pass
    to call smp_muxed_ipi_set_message to set the message instead
    of doing it directly inside the routine.
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index a53a13047330..e222efcf6aef 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -218,7 +218,7 @@ void smp_muxed_ipi_set_data(int cpu, unsigned long data)
 	info->data = data;
 }
 
-void smp_muxed_ipi_message_pass(int cpu, int msg)
+void smp_muxed_ipi_set_message(int cpu, int msg)
 {
 	struct cpu_messages *info = &per_cpu(ipi_message, cpu);
 	char *message = (char *)&info->messages;
@@ -228,6 +228,13 @@ void smp_muxed_ipi_message_pass(int cpu, int msg)
 	 */
 	smp_mb();
 	message[msg] = 1;
+}
+
+void smp_muxed_ipi_message_pass(int cpu, int msg)
+{
+	struct cpu_messages *info = &per_cpu(ipi_message, cpu);
+
+	smp_muxed_ipi_set_message(cpu, msg);
 	/*
 	 * cause_ipi functions are required to include a full barrier
 	 * before doing whatever causes the IPI.

commit bd7f561f76563f0b21701628874d8adc863b0c25
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Thu Dec 17 14:59:03 2015 -0600

    powerpc/smp: Support more IPI messages
    
    This patch increases the number of demuxed messages for a
    controller with a single ipi to 8 for 64-bit systems.
    
    This is required because we want to use the IPI mechanism
    to send messages from a CPU running in KVM real mode in a
    guest to a CPU in the host to take some action. Currently,
    we only support 4 messages and all 4 are already taken.
    
    Define a fifth message PPC_MSG_RM_HOST_ACTION for this
    purpose.
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index ec9ec2058d2d..a53a13047330 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -206,7 +206,7 @@ int smp_request_message_ipi(int virq, int msg)
 
 #ifdef CONFIG_PPC_SMP_MUXED_IPI
 struct cpu_messages {
-	int messages;			/* current messages */
+	long messages;			/* current messages */
 	unsigned long data;		/* data for cause ipi */
 };
 static DEFINE_PER_CPU_SHARED_ALIGNED(struct cpu_messages, ipi_message);
@@ -236,15 +236,15 @@ void smp_muxed_ipi_message_pass(int cpu, int msg)
 }
 
 #ifdef __BIG_ENDIAN__
-#define IPI_MESSAGE(A) (1 << (24 - 8 * (A)))
+#define IPI_MESSAGE(A) (1uL << ((BITS_PER_LONG - 8) - 8 * (A)))
 #else
-#define IPI_MESSAGE(A) (1 << (8 * (A)))
+#define IPI_MESSAGE(A) (1uL << (8 * (A)))
 #endif
 
 irqreturn_t smp_ipi_demux(void)
 {
 	struct cpu_messages *info = this_cpu_ptr(&ipi_message);
-	unsigned int all;
+	unsigned long all;
 
 	mb();	/* order any irq clear */
 

commit 875ebe940d77a41682c367ad799b4f39f128d3fa
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Feb 24 17:58:02 2015 +1100

    powerpc/smp: Wait until secondaries are active & online
    
    Anton has a busy ppc64le KVM box where guests sometimes hit the infamous
    "kernel BUG at kernel/smpboot.c:134!" issue during boot:
    
      BUG_ON(td->cpu != smp_processor_id());
    
    Basically a per CPU hotplug thread scheduled on the wrong CPU. The oops
    output confirms it:
    
      CPU: 0
      Comm: watchdog/130
    
    The problem is that we aren't ensuring the CPU active bit is set for the
    secondary before allowing the master to continue on. The master unparks
    the secondary CPU's kthreads and the scheduler looks for a CPU to run
    on. It calls select_task_rq() and realises the suggested CPU is not in
    the cpus_allowed mask. It then ends up in select_fallback_rq(), and
    since the active bit isnt't set we choose some other CPU to run on.
    
    This seems to have been introduced by 6acbfb96976f "sched: Fix hotplug
    vs. set_cpus_allowed_ptr()", which changed from setting active before
    online to setting active after online. However that was in turn fixing a
    bug where other code assumed an active CPU was also online, so we can't
    just revert that fix.
    
    The simplest fix is just to spin waiting for both active & online to be
    set. We already have a barrier prior to set_cpu_online() (which also
    sets active), to ensure all other setup is completed before online &
    active are set.
    
    Fixes: 6acbfb96976f ("sched: Fix hotplug vs. set_cpus_allowed_ptr()")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 6e19afa35a15..ec9ec2058d2d 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -541,8 +541,8 @@ int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 	if (smp_ops->give_timebase)
 		smp_ops->give_timebase();
 
-	/* Wait until cpu puts itself in the online map */
-	while (!cpu_online(cpu))
+	/* Wait until cpu puts itself in the online & active maps */
+	while (!cpu_online(cpu) || !cpu_active(cpu))
 		cpu_relax();
 
 	return 0;

commit d3f180ea1a44aecba1b0dab2a253428e77f906bf
Merge: 6b00f7efb530 a6130ed253a9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 11 18:15:38 2015 -0800

    Merge tag 'powerpc-3.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux
    
    Pull powerpc updates from Michael Ellerman:
    
     - Update of all defconfigs
    
     - Addition of a bunch of config options to modernise our defconfigs
    
     - Some PS3 updates from Geoff
    
     - Optimised memcmp for 64 bit from Anton
    
     - Fix for kprobes that allows 'perf probe' to work from Naveen
    
     - Several cxl updates from Ian & Ryan
    
     - Expanded support for the '24x7' PMU from Cody & Sukadev
    
     - Freescale updates from Scott:
        "Highlights include 8xx optimizations, some more work on datapath
         device tree content, e300 machine check support, t1040 corenet
         error reporting, and various cleanups and fixes"
    
    * tag 'powerpc-3.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux: (102 commits)
      cxl: Add missing return statement after handling AFU errror
      cxl: Fail AFU initialisation if an invalid configuration record is found
      cxl: Export optional AFU configuration record in sysfs
      powerpc/mm: Warn on flushing tlb page in kernel context
      powerpc/powernv: Add OPAL soft-poweroff routine
      powerpc/perf/hv-24x7: Document sysfs event description entries
      powerpc/perf/hv-gpci: add the remaining gpci requests
      powerpc/perf/{hv-gpci, hv-common}: generate requests with counters annotated
      powerpc/perf/hv-24x7: parse catalog and populate sysfs with events
      perf: define EVENT_DEFINE_RANGE_FORMAT_LITE helper
      perf: add PMU_EVENT_ATTR_STRING() helper
      perf: provide sysfs_show for struct perf_pmu_events_attr
      powerpc/kernel: Avoid initializing device-tree pointer twice
      powerpc: Remove old compile time disabled syscall tracing code
      powerpc/kernel: Make syscall_exit a local label
      cxl: Fix device_node reference counting
      powerpc/mm: bail out early when flushing TLB page
      powerpc: defconfigs: add MTD_SPI_NOR (new dependency for M25P80)
      perf/powerpc: reset event hw state when adding it to the PMU
      powerpc/qe: Use strlcpy()
      ...

commit 8aa989b8fba1428b50a1be771c01285f1de0227b
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jan 27 16:48:03 2015 +1100

    powerpc: Remove some unused functions
    
    Remove slice_set_psize() which is not used.
    
    It was added in 3a8247cc2c85 "powerpc: Only demote individual slices
    rather than whole process" but was never used.
    
    Remove vsx_assist_exception() which is not used.
    
    It was added in ce48b2100785 "powerpc: Add VSX context save/restore,
    ptrace and signal support" but was never used.
    
    Remove generic_mach_cpu_die() which is not used.
    
    Its last caller was removed in 375f561a4131 "powerpc/powernv: Always go
    into nap mode when CPU is offline".
    
    Remove mpc7448_hpc2_power_off() and mpc7448_hpc2_halt() which are
    unused.
    
    These were introduced in c5d56332fd6c "[POWERPC] Add general support for
    mpc7448hpc2 (Taiga) platform" but were never used.
    
    This was partially found by using a static code analysis program called
    cppcheck.
    
    Signed-off-by: Rickard Strandqvist <rickard_strandqvist@spectrumdigital.se>
    [mpe: Update changelog with details on when/why they are unused]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8ec017cb4446..1cc4bdce19f3 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -434,20 +434,6 @@ void generic_cpu_die(unsigned int cpu)
 	printk(KERN_ERR "CPU%d didn't die...\n", cpu);
 }
 
-void generic_mach_cpu_die(void)
-{
-	unsigned int cpu;
-
-	local_irq_disable();
-	idle_task_exit();
-	cpu = smp_processor_id();
-	printk(KERN_DEBUG "CPU%d offline\n", cpu);
-	__this_cpu_write(cpu_state, CPU_DEAD);
-	smp_wmb();
-	while (__this_cpu_read(cpu_state) != CPU_UP_PREPARE)
-		cpu_relax();
-}
-
 void generic_set_cpu_dead(unsigned int cpu)
 {
 	per_cpu(cpu_state, cpu) = CPU_DEAD;

commit 1be6f10f6f9caade3a053938cb80a2eed237e262
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Dec 29 15:47:05 2014 +1100

    Revert "powerpc: Secondary CPUs must set cpu_callin_map after setting active and online"
    
    This reverts commit 7c5c92ed56d932b2c19c3f8aea86369509407d33.
    
    Although this did fix the bug it was aimed at, it also broke secondary
    startup on platforms that use give/take_timebase(). Unfortunately we
    didn't detect that while it was in next.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8ec017cb4446..8b2d2dc8ef10 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -700,6 +700,7 @@ void start_secondary(void *unused)
 	smp_store_cpu_info(cpu);
 	set_dec(tb_ticks_per_jiffy);
 	preempt_disable();
+	cpu_callin_map[cpu] = 1;
 
 	if (smp_ops->setup_cpu)
 		smp_ops->setup_cpu(cpu);
@@ -738,14 +739,6 @@ void start_secondary(void *unused)
 	notify_cpu_starting(cpu);
 	set_cpu_online(cpu, true);
 
-	/*
-	 * CPU must be marked active and online before we signal back to the
-	 * master, because the scheduler needs to see the cpu_online and
-	 * cpu_active bits set.
-	 */
-	smp_wmb();
-	cpu_callin_map[cpu] = 1;
-
 	local_irq_enable();
 
 	cpu_startup_entry(CPUHP_ONLINE);

commit 7c5c92ed56d932b2c19c3f8aea86369509407d33
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Dec 9 10:58:19 2014 +1100

    powerpc: Secondary CPUs must set cpu_callin_map after setting active and online
    
    I have a busy ppc64le KVM box where guests sometimes hit the infamous
    "kernel BUG at kernel/smpboot.c:134!" issue during boot:
    
      BUG_ON(td->cpu != smp_processor_id());
    
    Basically a per CPU hotplug thread scheduled on the wrong CPU. The oops
    output confirms it:
    
      CPU: 0
      Comm: watchdog/130
    
    The problem is that we aren't ensuring the CPU active and online bits are set
    before allowing the master to continue on. The master unparks the secondary
    CPUs kthreads and the scheduler looks for a CPU to run on. It calls
    select_task_rq and realises the suggested CPU is not in the cpus_allowed
    mask. It then ends up in select_fallback_rq, and since the active and
    online bits aren't set we choose some other CPU to run on.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8b2d2dc8ef10..8ec017cb4446 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -700,7 +700,6 @@ void start_secondary(void *unused)
 	smp_store_cpu_info(cpu);
 	set_dec(tb_ticks_per_jiffy);
 	preempt_disable();
-	cpu_callin_map[cpu] = 1;
 
 	if (smp_ops->setup_cpu)
 		smp_ops->setup_cpu(cpu);
@@ -739,6 +738,14 @@ void start_secondary(void *unused)
 	notify_cpu_starting(cpu);
 	set_cpu_online(cpu, true);
 
+	/*
+	 * CPU must be marked active and online before we signal back to the
+	 * master, because the scheduler needs to see the cpu_online and
+	 * cpu_active bits set.
+	 */
+	smp_wmb();
+	cpu_callin_map[cpu] = 1;
+
 	local_irq_enable();
 
 	cpu_startup_entry(CPUHP_ONLINE);

commit 69111bac42f5ceacdd22e30947837ceb2c4493ed
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Oct 21 15:23:25 2014 -0500

    powerpc: Replace __get_cpu_var uses
    
    This still has not been merged and now powerpc is the only arch that does
    not have this change. Sorry about missing linuxppc-dev before.
    
    V2->V2
      - Fix up to work against 3.18-rc1
    
    __get_cpu_var() is used for multiple purposes in the kernel source. One of
    them is address calculation via the form &__get_cpu_var(x).  This calculates
    the address for the instance of the percpu variable of the current processor
    based on an offset.
    
    Other use cases are for storing and retrieving data from the current
    processors percpu area.  __get_cpu_var() can be used as an lvalue when
    writing data or on the right side of an assignment.
    
    __get_cpu_var() is defined as :
    
    __get_cpu_var() always only does an address determination. However, store
    and retrieve operations could use a segment prefix (or global register on
    other platforms) to avoid the address calculation.
    
    this_cpu_write() and this_cpu_read() can directly take an offset into a
    percpu area and use optimized assembly code to read and write per cpu
    variables.
    
    This patch converts __get_cpu_var into either an explicit address
    calculation using this_cpu_ptr() or into a use of this_cpu operations that
    use the offset.  Thereby address calculations are avoided and less registers
    are used when code is generated.
    
    At the end of the patch set all uses of __get_cpu_var have been removed so
    the macro is removed too.
    
    The patch set includes passes over all arches as well. Once these operations
    are used throughout then specialized macros can be defined in non -x86
    arches as well in order to optimize per cpu access by f.e.  using a global
    register that may be set to the per cpu base.
    
    Transformations done to __get_cpu_var()
    
    1. Determine the address of the percpu instance of the current processor.
    
            DEFINE_PER_CPU(int, y);
            int *x = &__get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(&y);
    
    2. Same as #1 but this time an array structure is involved.
    
            DEFINE_PER_CPU(int, y[20]);
            int *x = __get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(y);
    
    3. Retrieve the content of the current processors instance of a per cpu
    variable.
    
            DEFINE_PER_CPU(int, y);
            int x = __get_cpu_var(y)
    
       Converts to
    
            int x = __this_cpu_read(y);
    
    4. Retrieve the content of a percpu struct
    
            DEFINE_PER_CPU(struct mystruct, y);
            struct mystruct x = __get_cpu_var(y);
    
       Converts to
    
            memcpy(&x, this_cpu_ptr(&y), sizeof(x));
    
    5. Assignment to a per cpu variable
    
            DEFINE_PER_CPU(int, y)
            __get_cpu_var(y) = x;
    
       Converts to
    
            __this_cpu_write(y, x);
    
    6. Increment/Decrement etc of a per cpu variable
    
            DEFINE_PER_CPU(int, y);
            __get_cpu_var(y)++
    
       Converts to
    
            __this_cpu_inc(y)
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    CC: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    [mpe: Fix build errors caused by set/or_softirq_pending(), and rework
          assignment in __set_breakpoint() to use memcpy().]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 71e186d5f331..8b2d2dc8ef10 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -243,7 +243,7 @@ void smp_muxed_ipi_message_pass(int cpu, int msg)
 
 irqreturn_t smp_ipi_demux(void)
 {
-	struct cpu_messages *info = &__get_cpu_var(ipi_message);
+	struct cpu_messages *info = this_cpu_ptr(&ipi_message);
 	unsigned int all;
 
 	mb();	/* order any irq clear */
@@ -442,9 +442,9 @@ void generic_mach_cpu_die(void)
 	idle_task_exit();
 	cpu = smp_processor_id();
 	printk(KERN_DEBUG "CPU%d offline\n", cpu);
-	__get_cpu_var(cpu_state) = CPU_DEAD;
+	__this_cpu_write(cpu_state, CPU_DEAD);
 	smp_wmb();
-	while (__get_cpu_var(cpu_state) != CPU_UP_PREPARE)
+	while (__this_cpu_read(cpu_state) != CPU_UP_PREPARE)
 		cpu_relax();
 }
 

commit bc3c4327c92b9ceb9a6356ec64d1b2ab2dc851f9
Author: Li Zhong <zhong@linux.vnet.ibm.com>
Date:   Wed Aug 27 17:34:00 2014 +0800

    powerpc: Only set numa node information for present cpus at boottime
    
    As Nish suggested, it makes more sense to init the numa node informatiion
    for present cpus at boottime, which could also avoid WARN_ON(1) in
    numa_setup_cpu().
    
    With this change, we also need to change the smp_prepare_cpus() to set up
    numa information only on present cpus.
    
    For those possible, but not present cpus, their numa information
    will be set up after they are started, as the original code did before commit
    2fabf084b6ad.
    
    Cc: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Cc: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Acked-by: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Tested-by: Cyril Bur <cyril.bur@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 4866d5dbd420..71e186d5f331 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -380,8 +380,11 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 		/*
 		 * numa_node_id() works after this.
 		 */
-		set_cpu_numa_node(cpu, numa_cpu_lookup_table[cpu]);
-		set_cpu_numa_mem(cpu, local_memory_node(numa_cpu_lookup_table[cpu]));
+		if (cpu_present(cpu)) {
+			set_cpu_numa_node(cpu, numa_cpu_lookup_table[cpu]);
+			set_cpu_numa_mem(cpu,
+				local_memory_node(numa_cpu_lookup_table[cpu]));
+		}
 	}
 
 	cpumask_set_cpu(boot_cpuid, cpu_sibling_mask(boot_cpuid));
@@ -729,6 +732,9 @@ void start_secondary(void *unused)
 	}
 	traverse_core_siblings(cpu, true);
 
+	set_numa_node(numa_cpu_lookup_table[cpu]);
+	set_numa_mem(local_memory_node(numa_cpu_lookup_table[cpu]));
+
 	smp_wmb();
 	notify_cpu_starting(cpu);
 	set_cpu_online(cpu, true);

commit 1217d34b531c76362217057ca70a8ce8950574e0
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 20 08:55:19 2014 +1000

    powerpc: Ensure global functions include their prototype
    
    Fix a number of places where global functions were not including
    their prototype. This ensures the prototype and the function match.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index a0738af4aba6..4866d5dbd420 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -52,6 +52,7 @@
 #endif
 #include <asm/vdso.h>
 #include <asm/debug.h>
+#include <asm/kexec.h>
 
 #ifdef DEBUG
 #include <asm/udbg.h>

commit 2fabf084b6ad6337675d700b159a6091023544f2
Author: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
Date:   Thu Jul 17 16:15:12 2014 -0700

    powerpc: reorder per-cpu NUMA information's initialization
    
    There is an issue currently where NUMA information is used on powerpc
    (and possibly ia64) before it has been read from the device-tree, which
    leads to large slab consumption with CONFIG_SLUB and memoryless nodes.
    
    NUMA powerpc non-boot CPU's cpu_to_node/cpu_to_mem is only accurate
    after start_secondary(), similar to ia64, which is invoked via
    smp_init().
    
    Commit 6ee0578b4daae ("workqueue: mark init_workqueues() as
    early_initcall()") made init_workqueues() be invoked via
    do_pre_smp_initcalls(), which is obviously before the secondary
    processors are online.
    
    Additionally, the following commits changed init_workqueues() to use
    cpu_to_node to determine the node to use for kthread_create_on_node:
    
    bce903809ab3f ("workqueue: add wq_numa_tbl_len and
    wq_numa_possible_cpumask[]")
    f3f90ad469342 ("workqueue: determine NUMA node of workers accourding to
    the allowed cpumask")
    
    Therefore, when init_workqueues() runs, it sees all CPUs as being on
    Node 0. On LPARs or KVM guests where Node 0 is memoryless, this leads to
    a high number of slab deactivations
    (http://www.spinics.net/lists/linux-mm/msg67489.html).
    
    Fix this by initializing the powerpc-specific CPU<->node/local memory
    node mapping as early as possible, which on powerpc is
    do_init_bootmem(). Currently that function initializes the mapping for
    the boot CPU, but we extend it to setup the mapping for all possible
    CPUs. Then, in smp_prepare_cpus(), we can correspondingly set the
    per-cpu values for all possible CPUs. That ensures that before the
    early_initcalls run (and really as early as possible), the per-cpu NUMA
    mapping is accurate.
    
    While testing memoryless nodes on PowerKVM guests with a fix to the
    workqueue logic to use cpu_to_mem() instead of cpu_to_node(), with a
    guest topology of:
    
    available: 2 nodes (0-1)
    node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49
    node 0 size: 0 MB
    node 0 free: 0 MB
    node 1 cpus: 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99
    node 1 size: 16336 MB
    node 1 free: 15329 MB
    node distances:
    node   0   1
      0:  10  40
      1:  40  10
    
    the slab consumption decreases from
    
    Slab:             932416 kB
    SUnreclaim:       902336 kB
    
    to
    
    Slab:             395264 kB
    SUnreclaim:       359424 kB
    
    And we a corresponding increase in the slab efficiency from
    
    slab                                   mem     objs    slabs
                                          used   active   active
    ------------------------------------------------------------
    kmalloc-16384                       337 MB   11.28%  100.00%
    task_struct                         288 MB    9.93%  100.00%
    
    to
    
    slab                                   mem     objs    slabs
                                          used   active   active
    ------------------------------------------------------------
    kmalloc-16384                        37 MB  100.00%  100.00%
    task_struct                          31 MB  100.00%  100.00%
    
    Powerpc didn't support memoryless nodes until recently (64bb80d87f01
    "powerpc/numa: Enable CONFIG_HAVE_MEMORYLESS_NODES" and 8c272261194d
    "powerpc/numa: Enable USE_PERCPU_NUMA_NODE_ID"). Those commits also
    helped improve memory consumption with these kind of environments.
    
    Signed-off-by: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 1007fb802e6b..a0738af4aba6 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -376,6 +376,11 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 					GFP_KERNEL, cpu_to_node(cpu));
 		zalloc_cpumask_var_node(&per_cpu(cpu_core_map, cpu),
 					GFP_KERNEL, cpu_to_node(cpu));
+		/*
+		 * numa_node_id() works after this.
+		 */
+		set_cpu_numa_node(cpu, numa_cpu_lookup_table[cpu]);
+		set_cpu_numa_mem(cpu, local_memory_node(numa_cpu_lookup_table[cpu]));
 	}
 
 	cpumask_set_cpu(boot_cpuid, cpu_sibling_mask(boot_cpuid));
@@ -723,12 +728,6 @@ void start_secondary(void *unused)
 	}
 	traverse_core_siblings(cpu, true);
 
-	/*
-	 * numa_node_id() works after this.
-	 */
-	set_numa_node(numa_cpu_lookup_table[cpu]);
-	set_numa_mem(local_memory_node(numa_cpu_lookup_table[cpu]));
-
 	smp_wmb();
 	notify_cpu_starting(cpu);
 	set_cpu_online(cpu, true);

commit b6220ad66bcd4a50737eb3c08e9466aa44f3bc98
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Tue Jun 24 18:05:29 2014 -0700

    sched: Fix compiler warnings
    
    Commit 143e1e28cb (sched: Rework sched_domain topology definition)
    introduced a number of functions with a return value of 'const int'.
    gcc doesn't know what to do with that and, if the kernel is compiled
    with W=1, complains with the following warnings whenever sched.h
    is included.
    
      include/linux/sched.h:875:25: warning: type qualifiers ignored on function return type
      include/linux/sched.h:882:25: warning: type qualifiers ignored on function return type
      include/linux/sched.h:889:25: warning: type qualifiers ignored on function return type
      include/linux/sched.h:1002:21: warning: type qualifiers ignored on function return type
    
    Commits fb2aa855 (sched, ARM: Create a dedicated scheduler topology table)
    and 607b45e9a (sched, powerpc: Create a dedicated topology table) introduce
    the same warning in the arm and powerpc code.
    
    Drop 'const' from the function declarations to fix the problem.
    
    The fix for all three patches has to be applied together to avoid
    compilation failures for the affected architectures.
    
    Acked-by: Vincent Guittot <vincent.guittot@linaro.org>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1403658329-13196-1-git-send-email-linux@roeck-us.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 51a3ff78838a..1007fb802e6b 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -747,7 +747,7 @@ int setup_profiling_timer(unsigned int multiplier)
 
 #ifdef CONFIG_SCHED_SMT
 /* cpumask of CPUs with asymetric SMT dependancy */
-static const int powerpc_smt_flags(void)
+static int powerpc_smt_flags(void)
 {
 	int flags = SD_SHARE_CPUCAPACITY | SD_SHARE_PKG_RESOURCES;
 

commit b2e09f633a3994ee97fa6bc734b533d9c8e6ea0f
Merge: 3737a1276163 535560d841b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 12 19:42:15 2014 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull more scheduler updates from Ingo Molnar:
     "Second round of scheduler changes:
       - try-to-wakeup and IPI reduction speedups, from Andy Lutomirski
       - continued power scheduling cleanups and refactorings, from Nicolas
         Pitre
       - misc fixes and enhancements"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/deadline: Delete extraneous extern for to_ratio()
      sched/idle: Optimize try-to-wake-up IPI
      sched/idle: Simplify wake_up_idle_cpu()
      sched/idle: Clear polling before descheduling the idle thread
      sched, trace: Add a tracepoint for IPI-less remote wakeups
      cpuidle: Set polling in poll_idle
      sched: Remove redundant assignment to "rt_rq" in update_curr_rt(...)
      sched: Rename capacity related flags
      sched: Final power vs. capacity cleanups
      sched: Remove remaining dubious usage of "power"
      sched: Let 'struct sched_group_power' care about CPU capacity
      sched/fair: Disambiguate existing/remaining "capacity" usage
      sched/fair: Change "has_capacity" to "has_free_capacity"
      sched/fair: Remove "power" from 'struct numa_stats'
      sched: Fix signedness bug in yield_to()
      sched/fair: Use time_after() in record_wakee()
      sched/balancing: Reduce the rate of needless idle load balancing
      sched/fair: Fix unlocked reads of some cfs_b->quota/period

commit c5aec4c76af1a2d89ee2f2d4d5463b2ad2d85de5
Merge: 2937f5efa575 0c0a3e5a100b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 10 18:54:22 2014 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Ben Herrenschmidt:
     "Here is the bulk of the powerpc changes for this merge window.  It got
      a bit delayed in part because I wasn't paying attention, and in part
      because I discovered I had a core PCI change without a PCI maintainer
      ack in it.  Bjorn eventually agreed it was ok to merge it though we'll
      probably improve it later and I didn't want to rebase to add his ack.
    
      There is going to be a bit more next week, essentially fixes that I
      still want to sort through and test.
    
      The biggest item this time is the support to build the ppc64 LE kernel
      with our new v2 ABI.  We previously supported v2 userspace but the
      kernel itself was a tougher nut to crack.  This is now sorted mostly
      thanks to Anton and Rusty.
    
      We also have a fairly big series from Cedric that add support for
      64-bit LE zImage boot wrapper.  This was made harder by the fact that
      traditionally our zImage wrapper was always 32-bit, but our new LE
      toolchains don't really support 32-bit anymore (it's somewhat there
      but not really "supported") so we didn't want to rely on it.  This
      meant more churn that just endian fixes.
    
      This brings some more LE bits as well, such as the ability to run in
      LE mode without a hypervisor (ie. under OPAL firmware) by doing the
      right OPAL call to reinitialize the CPU to take HV interrupts in the
      right mode and the usual pile of endian fixes.
    
      There's another series from Gavin adding EEH improvements (one day we
      *will* have a release with less than 20 EEH patches, I promise!).
    
      Another highlight is the support for the "Split core" functionality on
      P8 by Michael.  This allows a P8 core to be split into "sub cores" of
      4 threads which allows the subcores to run different guests under KVM
      (the HW still doesn't support a partition per thread).
    
      And then the usual misc bits and fixes ..."
    
    [ Further delayed by gmail deciding that BenH is a dirty spammer.
      Google knows.  ]
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (155 commits)
      powerpc/powernv: Add missing include to LPC code
      selftests/powerpc: Test the THP bug we fixed in the previous commit
      powerpc/mm: Check paca psize is up to date for huge mappings
      powerpc/powernv: Pass buffer size to OPAL validate flash call
      powerpc/pseries: hcall functions are exported to modules, need _GLOBAL_TOC()
      powerpc: Exported functions __clear_user and copy_page use r2 so need _GLOBAL_TOC()
      powerpc/powernv: Set memory_block_size_bytes to 256MB
      powerpc: Allow ppc_md platform hook to override memory_block_size_bytes
      powerpc/powernv: Fix endian issues in memory error handling code
      powerpc/eeh: Skip eeh sysfs when eeh is disabled
      powerpc: 64bit sendfile is capped at 2GB
      powerpc/powernv: Provide debugfs access to the LPC bus via OPAL
      powerpc/serial: Use saner flags when creating legacy ports
      powerpc: Add cpu family documentation
      powerpc/xmon: Fix up xmon format strings
      powerpc/powernv: Add calls to support little endian host
      powerpc: Document sysfs DSCR interface
      powerpc: Fix regression of per-CPU DSCR setting
      powerpc: Split __SYSFS_SPRSETUP macro
      arch: powerpc/fadump: Cleaning up inconsistent NULL checks
      ...

commit 5d4dfddd4f02b028d6ddaaa04d75d3b0cad1c9ae
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue May 27 13:50:41 2014 -0400

    sched: Rename capacity related flags
    
    It is better not to think about compute capacity as being equivalent
    to "CPU power".  The upcoming "power aware" scheduler work may create
    confusion with the notion of energy consumption if "power" is used too
    liberally.
    
    Let's rename the following feature flags since they do relate to capacity:
    
            SD_SHARE_CPUPOWER  -> SD_SHARE_CPUCAPACITY
            ARCH_POWER         -> ARCH_CAPACITY
            NONTASK_POWER      -> NONTASK_CAPACITY
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: linaro-kernel@lists.linaro.org
    Cc: Andy Fleming <afleming@freescale.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Grant Likely <grant.likely@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Rob Herring <robh+dt@kernel.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vasant Hegde <hegdevasant@linux.vnet.ibm.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: devicetree@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/n/tip-e93lpnxb87owfievqatey6b5@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 10ffffef0414..c51d16379cba 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -770,7 +770,7 @@ int setup_profiling_timer(unsigned int multiplier)
 /* cpumask of CPUs with asymetric SMT dependancy */
 static const int powerpc_smt_flags(void)
 {
-	int flags = SD_SHARE_CPUPOWER | SD_SHARE_PKG_RESOURCES;
+	int flags = SD_SHARE_CPUCAPACITY | SD_SHARE_PKG_RESOURCES;
 
 	if (cpu_has_feature(CPU_FTR_ASYM_SMT)) {
 		printk_once(KERN_INFO "Enabling Asymmetric SMT scheduling\n");

commit 6f5e40a3001d2497a134386a173e3ec3fdf2ad0b
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri May 23 18:15:28 2014 +1000

    powerpc: Check cpu_thread_in_subcore() in __cpu_up()
    
    To support split core we need to change the check in __cpu_up() that
    determines if a cpu is allowed to come online.
    
    Currently we refuse to online cpus which are not the primary thread
    within their core.
    
    On POWER8 with split core support this check needs to instead refuse to
    online cpus which are not the primary thread within their *sub* core.
    
    On POWER7 and other systems that do not support split core,
    threads_per_subcore == threads_per_core and so the check is equivalent.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 5cdd9eb3b24c..6af946e9a984 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -490,7 +490,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 	 * Don't allow secondary threads to come online if inhibited
 	 */
 	if (threads_per_core > 1 && secondaries_inhibited() &&
-	    cpu % threads_per_core != 0)
+	    cpu_thread_in_subcore(cpu))
 		return -EBUSY;
 
 	if (smp_ops == NULL ||

commit 441c19c8a290f5f1e1b263691641124c84232b6e
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri May 23 18:15:25 2014 +1000

    powerpc/kvm/book3s_hv: Rework the secondary inhibit code
    
    As part of the support for split core on POWER8, we want to be able to
    block splitting of the core while KVM VMs are active.
    
    The logic to do that would be exactly the same as the code we currently
    have for inhibiting onlining of secondaries.
    
    Instead of adding an identical mechanism to block split core, rework the
    secondary inhibit code to be a "HV KVM is active" check. We can then use
    that in both the cpu hotplug code and the upcoming split core code.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Acked-by: Alexander Graf <agraf@suse.de>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 4863ea14f270..5cdd9eb3b24c 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -36,6 +36,7 @@
 #include <linux/atomic.h>
 #include <asm/irq.h>
 #include <asm/hw_irq.h>
+#include <asm/kvm_ppc.h>
 #include <asm/page.h>
 #include <asm/pgtable.h>
 #include <asm/prom.h>
@@ -458,38 +459,9 @@ int generic_check_cpu_restart(unsigned int cpu)
 	return per_cpu(cpu_state, cpu) == CPU_UP_PREPARE;
 }
 
-static atomic_t secondary_inhibit_count;
-
-/*
- * Don't allow secondary CPU threads to come online
- */
-void inhibit_secondary_onlining(void)
-{
-	/*
-	 * This makes secondary_inhibit_count stable during cpu
-	 * online/offline operations.
-	 */
-	get_online_cpus();
-
-	atomic_inc(&secondary_inhibit_count);
-	put_online_cpus();
-}
-EXPORT_SYMBOL_GPL(inhibit_secondary_onlining);
-
-/*
- * Allow secondary CPU threads to come online again
- */
-void uninhibit_secondary_onlining(void)
-{
-	get_online_cpus();
-	atomic_dec(&secondary_inhibit_count);
-	put_online_cpus();
-}
-EXPORT_SYMBOL_GPL(uninhibit_secondary_onlining);
-
-static int secondaries_inhibited(void)
+static bool secondaries_inhibited(void)
 {
-	return atomic_read(&secondary_inhibit_count);
+	return kvm_hv_mode_active();
 }
 
 #else /* HOTPLUG_CPU */

commit 64bb80d87f01ec01c76863b61b457e0904387f2f
Author: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
Date:   Fri May 16 16:41:20 2014 -0700

    powerpc/numa: Enable CONFIG_HAVE_MEMORYLESS_NODES
    
    Based off fd1197f1 for ia64, enable CONFIG_HAVE_MEMORYLESS_NODES if
    NUMA. Initialize the local memory node in start_secondary.
    
    With this commit and the preceding to enable
    CONFIG_USER_PERCPU_NUMA_NODE_ID, which is a prerequisite, in a PowerKVM
    guest with the following topology:
    
    numactl --hardware
    available: 3 nodes (0-2)
    node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22
    23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46
    47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70
    71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94
    95 96 97 98 99
    node 0 size: 1998 MB
    node 0 free: 521 MB
    node 1 cpus: 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114
    115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132
    133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150
    151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168
    169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186
    187 188 189 190 191 192 193 194 195 196 197 198 199
    node 1 size: 0 MB
    node 1 free: 0 MB
    node 2 cpus:
    node 2 size: 2039 MB
    node 2 free: 1739 MB
    node distances:
    node   0   1   2
      0:  10  40  40
      1:  40  10  40
      2:  40  40  10
    
    the unreclaimable slab is reduced by close to 130M:
    
    Before:
            Slab:             418176 kB
            SReclaimable:      26624 kB
            SUnreclaim:       391552 kB
    
    After:
            Slab:             298944 kB
            SReclaimable:      31744 kB
            SUnreclaim:       267200 kB
    
    Signed-off-by: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index d7252adea759..4863ea14f270 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -755,6 +755,7 @@ void start_secondary(void *unused)
 	 * numa_node_id() works after this.
 	 */
 	set_numa_node(numa_cpu_lookup_table[cpu]);
+	set_numa_mem(local_memory_node(numa_cpu_lookup_table[cpu]));
 
 	smp_wmb();
 	notify_cpu_starting(cpu);

commit 8c272261194dfda11cc046fbe808e052f6f284eb
Author: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
Date:   Mon May 19 11:14:23 2014 -0700

    powerpc/numa: Enable USE_PERCPU_NUMA_NODE_ID
    
    Based off 3bccd996 for ia64, convert powerpc to use the generic per-CPU
    topology tracking, specifically:
    
        initialize per cpu numa_node entry in start_secondary
        remove the powerpc cpu_to_node()
        define CONFIG_USE_PERCPU_NUMA_NODE_ID if NUMA
    
    Signed-off-by: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e2a4232c5871..d7252adea759 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -390,6 +390,7 @@ void smp_prepare_boot_cpu(void)
 #ifdef CONFIG_PPC64
 	paca[boot_cpuid].__current = current;
 #endif
+	set_numa_node(numa_cpu_lookup_table[boot_cpuid]);
 	current_set[boot_cpuid] = task_thread_info(current);
 }
 
@@ -750,6 +751,11 @@ void start_secondary(void *unused)
 	}
 	traverse_core_siblings(cpu, true);
 
+	/*
+	 * numa_node_id() works after this.
+	 */
+	set_numa_node(numa_cpu_lookup_table[cpu]);
+
 	smp_wmb();
 	notify_cpu_starting(cpu);
 	set_cpu_online(cpu, true);

commit 607b45e9a216e89a63351556e488eea06be0ff48
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Fri Apr 11 11:44:39 2014 +0200

    sched, powerpc: Create a dedicated topology table
    
    Create a dedicated topology table for handling asymetric feature of powerpc.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Reviewed-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Fleming <afleming@freescale.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Grant Likely <grant.likely@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Cc: Rob Herring <robh+dt@kernel.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vasant Hegde <hegdevasant@linux.vnet.ibm.com>
    Cc: tony.luck@intel.com
    Cc: fenghua.yu@intel.com
    Cc: schwidefsky@de.ibm.com
    Cc: cmetcalf@tilera.com
    Cc: dietmar.eggemann@arm.com
    Cc: devicetree@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/1397209481-28542-4-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e2a4232c5871..10ffffef0414 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -766,6 +766,28 @@ int setup_profiling_timer(unsigned int multiplier)
 	return 0;
 }
 
+#ifdef CONFIG_SCHED_SMT
+/* cpumask of CPUs with asymetric SMT dependancy */
+static const int powerpc_smt_flags(void)
+{
+	int flags = SD_SHARE_CPUPOWER | SD_SHARE_PKG_RESOURCES;
+
+	if (cpu_has_feature(CPU_FTR_ASYM_SMT)) {
+		printk_once(KERN_INFO "Enabling Asymmetric SMT scheduling\n");
+		flags |= SD_ASYM_PACKING;
+	}
+	return flags;
+}
+#endif
+
+static struct sched_domain_topology_level powerpc_topology[] = {
+#ifdef CONFIG_SCHED_SMT
+	{ cpu_smt_mask, powerpc_smt_flags, SD_INIT_NAME(SMT) },
+#endif
+	{ cpu_cpu_mask, SD_INIT_NAME(DIE) },
+	{ NULL, },
+};
+
 void __init smp_cpus_done(unsigned int max_cpus)
 {
 	cpumask_var_t old_mask;
@@ -790,15 +812,8 @@ void __init smp_cpus_done(unsigned int max_cpus)
 
 	dump_numa_cpu_topology();
 
-}
+	set_sched_topology(powerpc_topology);
 
-int arch_sd_sibling_asym_packing(void)
-{
-	if (cpu_has_feature(CPU_FTR_ASYM_SMT)) {
-		printk_once(KERN_INFO "Enabling Asymmetric SMT scheduling\n");
-		return SD_ASYM_PACKING;
-	}
-	return 0;
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit 1b67bee129a36c22c17186cc2a9981678e9323ee
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed Feb 26 05:37:43 2014 +0530

    powerpc: Implement tick broadcast IPI as a fixed IPI message
    
    For scalability and performance reasons, we want the tick broadcast IPIs
    to be handled as efficiently as possible. Fixed IPI messages
    are one of the most efficient mechanisms available - they are faster than
    the smp_call_function mechanism because the IPI handlers are fixed and hence
    they don't involve costly operations such as adding IPI handlers to the target
    CPU's function queue, acquiring locks for synchronization etc.
    
    Luckily we have an unused IPI message slot, so use that to implement
    tick broadcast IPIs efficiently.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    [Functions renamed to tick_broadcast* and Changelog modified by
     Preeti U. Murthy<preeti@linux.vnet.ibm.com>]
    Signed-off-by: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Acked-by: Geoff Levand <geoff@infradead.org> [For the PS3 part]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index ee7d76bfcb4c..e2a4232c5871 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -35,6 +35,7 @@
 #include <asm/ptrace.h>
 #include <linux/atomic.h>
 #include <asm/irq.h>
+#include <asm/hw_irq.h>
 #include <asm/page.h>
 #include <asm/pgtable.h>
 #include <asm/prom.h>
@@ -145,9 +146,9 @@ static irqreturn_t reschedule_action(int irq, void *data)
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t unused_action(int irq, void *data)
+static irqreturn_t tick_broadcast_ipi_action(int irq, void *data)
 {
-	/* This slot is unused and hence available for use, if needed */
+	tick_broadcast_ipi_handler();
 	return IRQ_HANDLED;
 }
 
@@ -168,14 +169,14 @@ static irqreturn_t debug_ipi_action(int irq, void *data)
 static irq_handler_t smp_ipi_action[] = {
 	[PPC_MSG_CALL_FUNCTION] =  call_function_action,
 	[PPC_MSG_RESCHEDULE] = reschedule_action,
-	[PPC_MSG_UNUSED] = unused_action,
+	[PPC_MSG_TICK_BROADCAST] = tick_broadcast_ipi_action,
 	[PPC_MSG_DEBUGGER_BREAK] = debug_ipi_action,
 };
 
 const char *smp_ipi_name[] = {
 	[PPC_MSG_CALL_FUNCTION] =  "ipi call function",
 	[PPC_MSG_RESCHEDULE] = "ipi reschedule",
-	[PPC_MSG_UNUSED] = "ipi unused",
+	[PPC_MSG_TICK_BROADCAST] = "ipi tick-broadcast",
 	[PPC_MSG_DEBUGGER_BREAK] = "ipi debugger",
 };
 
@@ -251,6 +252,8 @@ irqreturn_t smp_ipi_demux(void)
 			generic_smp_call_function_interrupt();
 		if (all & IPI_MESSAGE(PPC_MSG_RESCHEDULE))
 			scheduler_ipi();
+		if (all & IPI_MESSAGE(PPC_MSG_TICK_BROADCAST))
+			tick_broadcast_ipi_handler();
 		if (all & IPI_MESSAGE(PPC_MSG_DEBUGGER_BREAK))
 			debug_ipi_action(0, NULL);
 	} while (info->messages);
@@ -289,6 +292,16 @@ void arch_send_call_function_ipi_mask(const struct cpumask *mask)
 		do_message_pass(cpu, PPC_MSG_CALL_FUNCTION);
 }
 
+#ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
+void tick_broadcast(const struct cpumask *mask)
+{
+	unsigned int cpu;
+
+	for_each_cpu(cpu, mask)
+		do_message_pass(cpu, PPC_MSG_TICK_BROADCAST);
+}
+#endif
+
 #if defined(CONFIG_DEBUGGER) || defined(CONFIG_KEXEC)
 void smp_send_debugger_break(void)
 {

commit 402d9a1e02f7215628f13b7c80ff3e98c3a0cadc
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed Feb 26 05:37:29 2014 +0530

    powerpc: Free up the slot of PPC_MSG_CALL_FUNC_SINGLE IPI message
    
    The IPI handlers for both PPC_MSG_CALL_FUNC and PPC_MSG_CALL_FUNC_SINGLE map
    to a common implementation - generic_smp_call_function_single_interrupt(). So,
    we can consolidate them and save one of the IPI message slots, (which are
    precious on powerpc, since only 4 of those slots are available).
    
    So, implement the functionality of PPC_MSG_CALL_FUNC_SINGLE using
    PPC_MSG_CALL_FUNC itself and release its IPI message slot, so that it can be
    used for something else in the future, if desired.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Acked-by: Geoff Levand <geoff@infradead.org> [For the PS3 part]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index ac2621af3154..ee7d76bfcb4c 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -145,9 +145,9 @@ static irqreturn_t reschedule_action(int irq, void *data)
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t call_function_single_action(int irq, void *data)
+static irqreturn_t unused_action(int irq, void *data)
 {
-	generic_smp_call_function_single_interrupt();
+	/* This slot is unused and hence available for use, if needed */
 	return IRQ_HANDLED;
 }
 
@@ -168,14 +168,14 @@ static irqreturn_t debug_ipi_action(int irq, void *data)
 static irq_handler_t smp_ipi_action[] = {
 	[PPC_MSG_CALL_FUNCTION] =  call_function_action,
 	[PPC_MSG_RESCHEDULE] = reschedule_action,
-	[PPC_MSG_CALL_FUNC_SINGLE] = call_function_single_action,
+	[PPC_MSG_UNUSED] = unused_action,
 	[PPC_MSG_DEBUGGER_BREAK] = debug_ipi_action,
 };
 
 const char *smp_ipi_name[] = {
 	[PPC_MSG_CALL_FUNCTION] =  "ipi call function",
 	[PPC_MSG_RESCHEDULE] = "ipi reschedule",
-	[PPC_MSG_CALL_FUNC_SINGLE] = "ipi call function single",
+	[PPC_MSG_UNUSED] = "ipi unused",
 	[PPC_MSG_DEBUGGER_BREAK] = "ipi debugger",
 };
 
@@ -251,8 +251,6 @@ irqreturn_t smp_ipi_demux(void)
 			generic_smp_call_function_interrupt();
 		if (all & IPI_MESSAGE(PPC_MSG_RESCHEDULE))
 			scheduler_ipi();
-		if (all & IPI_MESSAGE(PPC_MSG_CALL_FUNC_SINGLE))
-			generic_smp_call_function_single_interrupt();
 		if (all & IPI_MESSAGE(PPC_MSG_DEBUGGER_BREAK))
 			debug_ipi_action(0, NULL);
 	} while (info->messages);
@@ -280,7 +278,7 @@ EXPORT_SYMBOL_GPL(smp_send_reschedule);
 
 void arch_send_call_function_single_ipi(int cpu)
 {
-	do_message_pass(cpu, PPC_MSG_CALL_FUNC_SINGLE);
+	do_message_pass(cpu, PPC_MSG_CALL_FUNCTION);
 }
 
 void arch_send_call_function_ipi_mask(const struct cpumask *mask)

commit dece8ada993e1764a115bdff0f1effffaa5fc8dc
Merge: a68c33f3592e f991db1cf1bd
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Dec 30 15:19:31 2013 +1100

    Merge branch 'merge' into next
    
    Merge a pile of fixes that went into the "merge" branch (3.13-rc's) such
    as Anton Little Endian fixes.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit f8a1883a833bbad8e6b5ec4f0918b7797e652d65
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Dec 12 15:59:36 2013 +1100

    powerpc: Fix topology core_id endian issue on LE builds
    
    cpu_to_core_id() is missing a byteswap:
    
    cat /sys/devices/system/cpu/cpu63/topology/core_id
    201326592
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index a3b64f3bf9a2..c1cf4a1522d9 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -580,7 +580,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 int cpu_to_core_id(int cpu)
 {
 	struct device_node *np;
-	const int *reg;
+	const __be32 *reg;
 	int id = -1;
 
 	np = of_get_cpu_node(cpu, NULL);
@@ -591,7 +591,7 @@ int cpu_to_core_id(int cpu)
 	if (!reg)
 		goto out;
 
-	id = *reg;
+	id = be32_to_cpup(reg);
 out:
 	of_node_put(np);
 	return id;

commit dfee0efe3ec8d4099c69e8234e4e4306619b9ba6
Author: Chen Gang <gang.chen@asianux.com>
Date:   Mon Jul 22 14:40:20 2013 +0800

    powerpc: kernel: remove useless code which related with 'max_cpus'
    
    Since not need 'max_cpus' after the related commit, the related code
    are useless too, need be removed.
    
    The related commit:
    
      c1aa687 powerpc: Clean up obsolete code relating to decrementer and timebase
    
    The related warning:
    
      arch/powerpc/kernel/smp.c:323:43: warning: parameter ‘max_cpus’ set but not used [-Wunused-but-set-parameter]
    
    Signed-off-by: Chen Gang <gang.chen@asianux.com>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index a3b64f3bf9a2..19d654b0150a 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -369,13 +369,8 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	cpumask_set_cpu(boot_cpuid, cpu_sibling_mask(boot_cpuid));
 	cpumask_set_cpu(boot_cpuid, cpu_core_mask(boot_cpuid));
 
-	if (smp_ops)
-		if (smp_ops->probe)
-			max_cpus = smp_ops->probe();
-		else
-			max_cpus = NR_CPUS;
-	else
-		max_cpus = 1;
+	if (smp_ops && smp_ops->probe)
+		smp_ops->probe();
 }
 
 void smp_prepare_boot_cpu(void)

commit 3eb906c6b6c123513718e7742a96a4189f900382
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Nov 20 11:05:01 2013 +1100

    powerpc: Make cpu_to_chip_id() available when SMP=n
    
    Up until now we have only used cpu_to_chip_id() in the topology code,
    which is only used on SMP builds. However my recent commit a4da0d5
    "Implement arch_get_random_long/int() for powernv" added a usage when
    SMP=n, breaking the build.
    
    Move cpu_to_chip_id() into prom.c so it is available for SMP=n builds.
    
    We would move the extern to prom.h, but that breaks the include in
    topology.h. Instead we leave it in smp.h, but move it out of the
    CONFIG_SMP #ifdef. We also need to include asm/smp.h in rng.c, because
    the linux version skips asm/smp.h on UP. What a mess.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 930cd8af3503..a3b64f3bf9a2 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -597,22 +597,6 @@ int cpu_to_core_id(int cpu)
 	return id;
 }
 
-/* Return the value of the chip-id property corresponding
- * to the given logical cpu.
- */
-int cpu_to_chip_id(int cpu)
-{
-	struct device_node *np;
-
-	np = of_get_cpu_node(cpu, NULL);
-	if (!np)
-		return -1;
-
-	of_node_put(np);
-	return of_get_ibm_chip_id(np);
-}
-EXPORT_SYMBOL(cpu_to_chip_id);
-
 /* Helper routines for cpu to core mapping */
 int cpu_core_index_of_thread(int cpu)
 {

commit 6dedcca610c6d6189b4a54d32118d1654adb73d2
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Wed Sep 25 15:08:27 2013 -0600

    hotplug, powerpc, x86: Remove cpu_hotplug_driver_lock()
    
    cpu_hotplug_driver_lock() serializes CPU online/offline operations
    when ARCH_CPU_PROBE_RELEASE is set.  This lock interface is no longer
    necessary with the following reason:
    
     - lock_device_hotplug() now protects CPU online/offline operations,
       including the probe & release interfaces enabled by
       ARCH_CPU_PROBE_RELEASE.  The use of cpu_hotplug_driver_lock() is
       redundant.
     - cpu_hotplug_driver_lock() is only valid when ARCH_CPU_PROBE_RELEASE
       is defined, which is misleading and is only enabled on powerpc.
    
    This patch removes the cpu_hotplug_driver_lock() interface.  As
    a result, ARCH_CPU_PROBE_RELEASE only enables / disables the cpu
    probe & release interface as intended.  There is no functional change
    in this patch.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8e59abc237d7..930cd8af3503 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -844,18 +844,6 @@ void __cpu_die(unsigned int cpu)
 		smp_ops->cpu_die(cpu);
 }
 
-static DEFINE_MUTEX(powerpc_cpu_hotplug_driver_mutex);
-
-void cpu_hotplug_driver_lock()
-{
-	mutex_lock(&powerpc_cpu_hotplug_driver_mutex);
-}
-
-void cpu_hotplug_driver_unlock()
-{
-	mutex_unlock(&powerpc_cpu_hotplug_driver_mutex);
-}
-
 void cpu_die(void)
 {
 	if (ppc_md.cpu_die)

commit 256588fda10f2a712631f8a4e72641a66adebdb8
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Mon Sep 9 18:37:56 2013 -0700

    powerpc: Export cpu_to_chip_id() to fix build error
    
    powerpc allmodconfig build fails with:
    
    ERROR: ".cpu_to_chip_id" [drivers/block/mtip32xx/mtip32xx.ko] undefined!
    
    The problem was introduced with commit 15863ff3b (powerpc: Make chip-id
    information available to userspace).
    
    Export the missing symbol.
    
    Cc: Vasant Hegde <hegdevasant@linux.vnet.ibm.com>
    Cc: Shivaprasad G Bhat <sbhat@linux.vnet.ibm.com>
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 442d8e23f8f4..8e59abc237d7 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -611,6 +611,7 @@ int cpu_to_chip_id(int cpu)
 	of_node_put(np);
 	return of_get_ibm_chip_id(np);
 }
+EXPORT_SYMBOL(cpu_to_chip_id);
 
 /* Helper routines for cpu to core mapping */
 int cpu_core_index_of_thread(int cpu)

commit 0654de1cd744a86e19e0a5afb75e3c23571093e6
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 7 02:01:48 2013 +1000

    powerpc: Little endian SMP IPI demux
    
    Add little endian support for demuxing SMP IPIs
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index d9d8bb0f4454..442d8e23f8f4 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -232,6 +232,12 @@ void smp_muxed_ipi_message_pass(int cpu, int msg)
 	smp_ops->cause_ipi(cpu, info->data);
 }
 
+#ifdef __BIG_ENDIAN__
+#define IPI_MESSAGE(A) (1 << (24 - 8 * (A)))
+#else
+#define IPI_MESSAGE(A) (1 << (8 * (A)))
+#endif
+
 irqreturn_t smp_ipi_demux(void)
 {
 	struct cpu_messages *info = &__get_cpu_var(ipi_message);
@@ -241,19 +247,14 @@ irqreturn_t smp_ipi_demux(void)
 
 	do {
 		all = xchg(&info->messages, 0);
-
-#ifdef __BIG_ENDIAN
-		if (all & (1 << (24 - 8 * PPC_MSG_CALL_FUNCTION)))
+		if (all & IPI_MESSAGE(PPC_MSG_CALL_FUNCTION))
 			generic_smp_call_function_interrupt();
-		if (all & (1 << (24 - 8 * PPC_MSG_RESCHEDULE)))
+		if (all & IPI_MESSAGE(PPC_MSG_RESCHEDULE))
 			scheduler_ipi();
-		if (all & (1 << (24 - 8 * PPC_MSG_CALL_FUNC_SINGLE)))
+		if (all & IPI_MESSAGE(PPC_MSG_CALL_FUNC_SINGLE))
 			generic_smp_call_function_single_interrupt();
-		if (all & (1 << (24 - 8 * PPC_MSG_DEBUGGER_BREAK)))
+		if (all & IPI_MESSAGE(PPC_MSG_DEBUGGER_BREAK))
 			debug_ipi_action(0, NULL);
-#else
-#error Unsupported ENDIAN
-#endif
 	} while (info->messages);
 
 	return IRQ_HANDLED;

commit 15863ff3b8dae4cacd831ce10aa34992e9ababb0
Author: Vasant Hegde <hegdevasant@linux.vnet.ibm.com>
Date:   Mon Aug 12 17:35:57 2013 +0530

    powerpc: Make chip-id information available to userspace
    
    So far "/sys/devices/system/cpu/cpuX/topology/physical_package_id"
    was always default (-1) on ppc64 architecture.
    
    Now, some systems have an ibm,chip-id property in the cpu nodes in
    the device tree. On these systems, we now use this information to
    display physical_package_id.
    
    Signed-off-by: Vasant Hegde <hegdevasant@linux.vnet.ibm.com>
    Signed-off-by: Shivaprasad G Bhat <sbhat@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 009900dd8f25..d9d8bb0f4454 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -596,6 +596,21 @@ int cpu_to_core_id(int cpu)
 	return id;
 }
 
+/* Return the value of the chip-id property corresponding
+ * to the given logical cpu.
+ */
+int cpu_to_chip_id(int cpu)
+{
+	struct device_node *np;
+
+	np = of_get_cpu_node(cpu, NULL);
+	if (!np)
+		return -1;
+
+	of_node_put(np);
+	return of_get_ibm_chip_id(np);
+}
+
 /* Helper routines for cpu to core mapping */
 int cpu_core_index_of_thread(int cpu)
 {

commit 256f2d4b463d3030ebc8d2b54f427543814a2bdc
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Aug 12 16:29:33 2013 +1000

    powerpc: Use ibm, chip-id property to compute cpu_core_mask if available
    
    Some systems have an ibm,chip-id property in the cpu nodes in the
    device tree.  On these systems, we now use that to compute the
    cpu_core_mask (i.e. the set of core siblings) rather than looking
    at cache properties.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Tested-by: Vasant Hegde <hegdevasant@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 0cc69d5deac9..009900dd8f25 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -609,6 +609,33 @@ int cpu_first_thread_of_core(int core)
 }
 EXPORT_SYMBOL_GPL(cpu_first_thread_of_core);
 
+static void traverse_siblings_chip_id(int cpu, bool add, int chipid)
+{
+	const struct cpumask *mask;
+	struct device_node *np;
+	int i, plen;
+	const __be32 *prop;
+
+	mask = add ? cpu_online_mask : cpu_present_mask;
+	for_each_cpu(i, mask) {
+		np = of_get_cpu_node(i, NULL);
+		if (!np)
+			continue;
+		prop = of_get_property(np, "ibm,chip-id", &plen);
+		if (prop && plen == sizeof(int) &&
+		    of_read_number(prop, 1) == chipid) {
+			if (add) {
+				cpumask_set_cpu(cpu, cpu_core_mask(i));
+				cpumask_set_cpu(i, cpu_core_mask(cpu));
+			} else {
+				cpumask_clear_cpu(cpu, cpu_core_mask(i));
+				cpumask_clear_cpu(i, cpu_core_mask(cpu));
+			}
+		}
+		of_node_put(np);
+	}
+}
+
 /* Must be called when no change can occur to cpu_present_mask,
  * i.e. during cpu online or offline.
  */
@@ -633,14 +660,29 @@ static struct device_node *cpu_to_l2cache(int cpu)
 
 static void traverse_core_siblings(int cpu, bool add)
 {
-	struct device_node *l2_cache;
+	struct device_node *l2_cache, *np;
 	const struct cpumask *mask;
-	int i;
+	int i, chip, plen;
+	const __be32 *prop;
+
+	/* First see if we have ibm,chip-id properties in cpu nodes */
+	np = of_get_cpu_node(cpu, NULL);
+	if (np) {
+		chip = -1;
+		prop = of_get_property(np, "ibm,chip-id", &plen);
+		if (prop && plen == sizeof(int))
+			chip = of_read_number(prop, 1);
+		of_node_put(np);
+		if (chip >= 0) {
+			traverse_siblings_chip_id(cpu, add, chip);
+			return;
+		}
+	}
 
 	l2_cache = cpu_to_l2cache(cpu);
 	mask = add ? cpu_online_mask : cpu_present_mask;
 	for_each_cpu(i, mask) {
-		struct device_node *np = cpu_to_l2cache(i);
+		np = cpu_to_l2cache(i);
 		if (!np)
 			continue;
 		if (np == l2_cache) {

commit a8a5356cd511db229aeaad636dc0f83d8c4d0a15
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Aug 12 16:28:47 2013 +1000

    powerpc: Pull out cpu_core_mask updates into a separate function
    
    This factors out the details of updating cpu_core_mask into a separate
    function, to make it easier to change how the mask is calculated later.
    This makes no functional change.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e69211293789..0cc69d5deac9 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -631,11 +631,36 @@ static struct device_node *cpu_to_l2cache(int cpu)
 	return cache;
 }
 
+static void traverse_core_siblings(int cpu, bool add)
+{
+	struct device_node *l2_cache;
+	const struct cpumask *mask;
+	int i;
+
+	l2_cache = cpu_to_l2cache(cpu);
+	mask = add ? cpu_online_mask : cpu_present_mask;
+	for_each_cpu(i, mask) {
+		struct device_node *np = cpu_to_l2cache(i);
+		if (!np)
+			continue;
+		if (np == l2_cache) {
+			if (add) {
+				cpumask_set_cpu(cpu, cpu_core_mask(i));
+				cpumask_set_cpu(i, cpu_core_mask(cpu));
+			} else {
+				cpumask_clear_cpu(cpu, cpu_core_mask(i));
+				cpumask_clear_cpu(i, cpu_core_mask(cpu));
+			}
+		}
+		of_node_put(np);
+	}
+	of_node_put(l2_cache);
+}
+
 /* Activate a secondary processor. */
 void start_secondary(void *unused)
 {
 	unsigned int cpu = smp_processor_id();
-	struct device_node *l2_cache;
 	int i, base;
 
 	atomic_inc(&init_mm.mm_count);
@@ -674,18 +699,7 @@ void start_secondary(void *unused)
 		cpumask_set_cpu(cpu, cpu_core_mask(base + i));
 		cpumask_set_cpu(base + i, cpu_core_mask(cpu));
 	}
-	l2_cache = cpu_to_l2cache(cpu);
-	for_each_online_cpu(i) {
-		struct device_node *np = cpu_to_l2cache(i);
-		if (!np)
-			continue;
-		if (np == l2_cache) {
-			cpumask_set_cpu(cpu, cpu_core_mask(i));
-			cpumask_set_cpu(i, cpu_core_mask(cpu));
-		}
-		of_node_put(np);
-	}
-	of_node_put(l2_cache);
+	traverse_core_siblings(cpu, true);
 
 	smp_wmb();
 	notify_cpu_starting(cpu);
@@ -741,7 +755,6 @@ int arch_sd_sibling_asym_packing(void)
 #ifdef CONFIG_HOTPLUG_CPU
 int __cpu_disable(void)
 {
-	struct device_node *l2_cache;
 	int cpu = smp_processor_id();
 	int base, i;
 	int err;
@@ -761,20 +774,7 @@ int __cpu_disable(void)
 		cpumask_clear_cpu(cpu, cpu_core_mask(base + i));
 		cpumask_clear_cpu(base + i, cpu_core_mask(cpu));
 	}
-
-	l2_cache = cpu_to_l2cache(cpu);
-	for_each_present_cpu(i) {
-		struct device_node *np = cpu_to_l2cache(i);
-		if (!np)
-			continue;
-		if (np == l2_cache) {
-			cpumask_clear_cpu(cpu, cpu_core_mask(i));
-			cpumask_clear_cpu(i, cpu_core_mask(cpu));
-		}
-		of_node_put(np);
-	}
-	of_node_put(l2_cache);
-
+	traverse_core_siblings(cpu, false);
 
 	return 0;
 }

commit 3cd852502316d42e3e75859e92d9f0a952bb55a2
Author: Andy Fleming <afleming@freescale.com>
Date:   Mon Aug 5 14:58:34 2013 -0500

    powerpc: Add smp_generic_cpu_bootable
    
    Cell and PSeries both implemented their own versions of a
    cpu_bootable smp_op which do the same thing (well, the PSeries
    one has support for more than 2 threads). Copy the PSeries one
    to generic code, and rename it smp_generic_cpu_bootable.
    
    Signed-off-by: Andy Fleming <afleming@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 9882240020c0..e69211293789 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -81,6 +81,28 @@ int smt_enabled_at_boot = 1;
 
 static void (*crash_ipi_function_ptr)(struct pt_regs *) = NULL;
 
+/*
+ * Returns 1 if the specified cpu should be brought up during boot.
+ * Used to inhibit booting threads if they've been disabled or
+ * limited on the command line
+ */
+int smp_generic_cpu_bootable(unsigned int nr)
+{
+	/* Special case - we inhibit secondary thread startup
+	 * during boot if the user requests it.
+	 */
+	if (system_state == SYSTEM_BOOTING && cpu_has_feature(CPU_FTR_SMT)) {
+		if (!smt_enabled_at_boot && cpu_thread_in_core(nr) != 0)
+			return 0;
+		if (smt_enabled_at_boot
+		    && cpu_thread_in_core(nr) >= smt_enabled_at_boot)
+			return 0;
+	}
+
+	return 1;
+}
+
+
 #ifdef CONFIG_PPC64
 int smp_generic_kick_cpu(int nr)
 {

commit b0d436c739b0d4afcdfe2e97d4d1ee41ea2db62e
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 7 02:01:24 2013 +1000

    powerpc: Fix a number of sparse warnings
    
    Address some of the trivial sparse warnings in arch/powerpc.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 38b0ba65a735..9882240020c0 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -172,7 +172,7 @@ int smp_request_message_ipi(int virq, int msg)
 #endif
 	err = request_irq(virq, smp_ipi_action[msg],
 			  IRQF_PERCPU | IRQF_NO_THREAD | IRQF_NO_SUSPEND,
-			  smp_ipi_name[msg], 0);
+			  smp_ipi_name[msg], NULL);
 	WARN(err < 0, "unable to request_irq %d for %s (rc %d)\n",
 		virq, smp_ipi_name[msg], err);
 

commit cce606feb425093c8371089d392e336d186e125b
Author: Li Zhong <zhong@linux.vnet.ibm.com>
Date:   Thu May 16 18:20:26 2013 +0800

    powerpc: Set cpu sibling mask before online cpu
    
    It seems following race is possible:
    
            cpu0                                    cpux
    smp_init->cpu_up->_cpu_up
            __cpu_up
                    kick_cpu(1)
    -------------------------------------------------------------------------
                    waiting online                  ...
                    ...                             notify CPU_STARTING
                                                            set cpux active
                                                    set cpux online
    -------------------------------------------------------------------------
                    finish waiting online
                    ...
    sched_init_smp
            init_sched_domains(cpu_active_mask)
                    build_sched_domains
                                                    set cpux sibling info
    -------------------------------------------------------------------------
    
    Execution of cpu0 and cpux could be concurrent between two separator
    lines.
    
    So if the cpux sibling information was set too late (normally
    impossible, but could be triggered by adding some delay in
    start_secondary, after setting cpu online), build_sched_domains()
    running on cpu0 might see cpux active, with an empty sibling mask, then
    cause some bad address accessing like following:
    
    [    0.099855] Unable to handle kernel paging request for data at address 0xc00000038518078f
    [    0.099868] Faulting instruction address: 0xc0000000000b7a64
    [    0.099883] Oops: Kernel access of bad area, sig: 11 [#1]
    [    0.099895] PREEMPT SMP NR_CPUS=16 DEBUG_PAGEALLOC NUMA pSeries
    [    0.099922] Modules linked in:
    [    0.099940] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 3.10.0-rc1-00120-gb973425-dirty #16
    [    0.099956] task: c0000001fed80000 ti: c0000001fed7c000 task.ti: c0000001fed7c000
    [    0.099971] NIP: c0000000000b7a64 LR: c0000000000b7a40 CTR: c0000000000b4934
    [    0.099985] REGS: c0000001fed7f760 TRAP: 0300   Not tainted  (3.10.0-rc1-00120-gb973425-dirty)
    [    0.099997] MSR: 8000000000009032 <SF,EE,ME,IR,DR,RI>  CR: 24272828  XER: 20000003
    [    0.100045] SOFTE: 1
    [    0.100053] CFAR: c000000000445ee8
    [    0.100064] DAR: c00000038518078f, DSISR: 40000000
    [    0.100073]
    GPR00: 0000000000000080 c0000001fed7f9e0 c000000000c84d48 0000000000000010
    GPR04: 0000000000000010 0000000000000000 c0000001fc55e090 0000000000000000
    GPR08: ffffffffffffffff c000000000b80b30 c000000000c962d8 00000003845ffc5f
    GPR12: 0000000000000000 c00000000f33d000 c00000000000b9e4 0000000000000000
    GPR16: 0000000000000000 0000000000000000 0000000000000001 0000000000000000
    GPR20: c000000000ccf750 0000000000000000 c000000000c94d48 c0000001fc504000
    GPR24: c0000001fc504000 c0000001fecef848 c000000000c94d48 c000000000ccf000
    GPR28: c0000001fc522090 0000000000000010 c0000001fecef848 c0000001fed7fae0
    [    0.100293] NIP [c0000000000b7a64] .get_group+0x84/0xc4
    [    0.100307] LR [c0000000000b7a40] .get_group+0x60/0xc4
    [    0.100318] Call Trace:
    [    0.100332] [c0000001fed7f9e0] [c0000000000dbce4] .lock_is_held+0xa8/0xd0 (unreliable)
    [    0.100354] [c0000001fed7fa70] [c0000000000bf62c] .build_sched_domains+0x728/0xd14
    [    0.100375] [c0000001fed7fbe0] [c000000000af67bc] .sched_init_smp+0x4fc/0x654
    [    0.100394] [c0000001fed7fce0] [c000000000adce24] .kernel_init_freeable+0x17c/0x30c
    [    0.100413] [c0000001fed7fdb0] [c00000000000ba08] .kernel_init+0x24/0x12c
    [    0.100431] [c0000001fed7fe30] [c000000000009f74] .ret_from_kernel_thread+0x5c/0x68
    [    0.100445] Instruction dump:
    [    0.100456] 38800010 38a00000 4838e3f5 60000000 7c6307b4 2fbf0000 419e0040 3d220001
    [    0.100496] 78601f24 39491590 e93e0008 7d6a002a <7d69582a> f97f0000 7d4a002a e93e0010
    [    0.100559] ---[ end trace 31fd0ba7d8756001 ]---
    
    This patch tries to move the sibling maps updating before
    notify_cpu_starting() and cpu online, and a write barrier there to make
    sure sibling maps are updated before active and online mask.
    
    Signed-off-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 85398c71665c..38b0ba65a735 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -637,12 +637,10 @@ void start_secondary(void *unused)
 
 	vdso_getcpu_init();
 #endif
-	notify_cpu_starting(cpu);
-	set_cpu_online(cpu, true);
 	/* Update sibling maps */
 	base = cpu_first_thread_sibling(cpu);
 	for (i = 0; i < threads_per_core; i++) {
-		if (cpu_is_offline(base + i))
+		if (cpu_is_offline(base + i) && (cpu != base + i))
 			continue;
 		cpumask_set_cpu(cpu, cpu_sibling_mask(base + i));
 		cpumask_set_cpu(base + i, cpu_sibling_mask(cpu));
@@ -667,6 +665,10 @@ void start_secondary(void *unused)
 	}
 	of_node_put(l2_cache);
 
+	smp_wmb();
+	notify_cpu_starting(cpu);
+	set_cpu_online(cpu, true);
+
 	local_irq_enable();
 
 	cpu_startup_entry(CPUHP_ONLINE);

commit 061d19f279f9bebbdb1ee48bef8c25e03de32ae2
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon Jun 24 15:30:09 2013 -0400

    powerpc: Delete __cpuinit usage from all users
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    This removes all the powerpc uses of the __cpuinit macros.  There
    are no __CPUINIT users in assembly files in powerpc.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Josh Boyer <jwboyer@gmail.com>
    Cc: Matt Porter <mporter@kernel.crashing.org>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index ee7ac5e6e28a..85398c71665c 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -480,7 +480,7 @@ static void cpu_idle_thread_init(unsigned int cpu, struct task_struct *idle)
 	secondary_ti = current_set[cpu] = ti;
 }
 
-int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *tidle)
+int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
 	int rc, c;
 
@@ -610,7 +610,7 @@ static struct device_node *cpu_to_l2cache(int cpu)
 }
 
 /* Activate a secondary processor. */
-__cpuinit void start_secondary(void *unused)
+void start_secondary(void *unused)
 {
 	unsigned int cpu = smp_processor_id();
 	struct device_node *l2_cache;

commit 799fef06123f86ff69cf754f996219e6ad1678f8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 22:49:56 2013 +0100

    powerpc: Use generic idle loop
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Link: http://lkml.kernel.org/r/20130321215235.026838003@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 76bd9da8cb71..ee7ac5e6e28a 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -669,7 +669,7 @@ __cpuinit void start_secondary(void *unused)
 
 	local_irq_enable();
 
-	cpu_idle();
+	cpu_startup_entry(CPUHP_ONLINE);
 
 	BUG();
 }

commit 174ea471c395d70ee0c839ad59ca49f3b702de58
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Tue Feb 5 05:07:06 2013 +0000

    powerpc: fix ics_rtas_init and start_secondary section mismatch
    
    It seems, we're fine with just annotating the two functions.
    Thus, this fixes the following build warnings on ppc64:
    
    WARNING: arch/powerpc/sysdev/xics/built-in.o(.text+0x1664):
    The function .ics_rtas_init() references
    the function __init .xics_register_ics().
    This is often because .ics_rtas_init lacks a __init
    annotation or the annotation of .xics_register_ics is wrong.
    
    WARNING: arch/powerpc/sysdev/built-in.o(.text+0x6044):
    The function .ics_rtas_init() references
    the function __init .xics_register_ics().
    This is often because .ics_rtas_init lacks a __init
    annotation or the annotation of .xics_register_ics is wrong.
    
    WARNING: arch/powerpc/kernel/built-in.o(.text+0x2db30):
    The function .start_secondary() references
    the function __cpuinit .vdso_getcpu_init().
    This is often because .start_secondary lacks a __cpuinit
    annotation or the annotation of .vdso_getcpu_init is wrong.
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 793401e65088..76bd9da8cb71 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -610,7 +610,7 @@ static struct device_node *cpu_to_l2cache(int cpu)
 }
 
 /* Activate a secondary processor. */
-void start_secondary(void *unused)
+__cpuinit void start_secondary(void *unused)
 {
 	unsigned int cpu = smp_processor_id();
 	struct device_node *l2_cache;

commit cad5cef62a5a0c525d39118d2e94b6e2034d5e05
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Fri Dec 21 14:04:10 2012 -0800

    POWERPC: drivers: remove __dev* attributes.
    
    CONFIG_HOTPLUG is going away as an option.  As a result, the __dev*
    markings need to be removed.
    
    This change removes the use of __devinit, __devexit_p, __devinitdata,
    __devinitconst, and __devexit from these drivers.
    
    Based on patches originally written by Bill Pemberton, but redone by me
    in order to handle some of the coding style issues better, by hand.
    
    Cc: Bill Pemberton <wfp5p@virginia.edu>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e5b133ebd8a5..793401e65088 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -82,7 +82,7 @@ int smt_enabled_at_boot = 1;
 static void (*crash_ipi_function_ptr)(struct pt_regs *) = NULL;
 
 #ifdef CONFIG_PPC64
-int __devinit smp_generic_kick_cpu(int nr)
+int smp_generic_kick_cpu(int nr)
 {
 	BUG_ON(nr < 0 || nr >= NR_CPUS);
 
@@ -311,7 +311,7 @@ void smp_send_stop(void)
 
 struct thread_info *current_set[NR_CPUS];
 
-static void __devinit smp_store_cpu_info(int id)
+static void smp_store_cpu_info(int id)
 {
 	per_cpu(cpu_pvr, id) = mfspr(SPRN_PVR);
 #ifdef CONFIG_PPC_FSL_BOOK3E
@@ -355,7 +355,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 		max_cpus = 1;
 }
 
-void __devinit smp_prepare_boot_cpu(void)
+void smp_prepare_boot_cpu(void)
 {
 	BUG_ON(smp_processor_id() != boot_cpuid);
 #ifdef CONFIG_PPC64
@@ -610,7 +610,7 @@ static struct device_node *cpu_to_l2cache(int cpu)
 }
 
 /* Activate a secondary processor. */
-void __devinit start_secondary(void *unused)
+void start_secondary(void *unused)
 {
 	unsigned int cpu = smp_processor_id();
 	struct device_node *l2_cache;

commit 0588000eac9ba4178cebade437da3b28e8fad48f
Merge: 8b5869ad85f7 81c52c56e2b4
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Oct 31 13:36:18 2012 +0100

    Merge commit 'origin/queue' into for-queue
    
    Conflicts:
            arch/powerpc/include/asm/Kbuild
            arch/powerpc/include/uapi/asm/Kbuild

commit 512691d4907d7cf4b8d05c6f8572d1fa60ccec20
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 15 01:15:41 2012 +0000

    KVM: PPC: Book3S HV: Allow KVM guests to stop secondary threads coming online
    
    When a Book3S HV KVM guest is running, we need the host to be in
    single-thread mode, that is, all of the cores (or at least all of
    the cores where the KVM guest could run) to be running only one
    active hardware thread.  This is because of the hardware restriction
    in POWER processors that all of the hardware threads in the core
    must be in the same logical partition.  Complying with this restriction
    is much easier if, from the host kernel's point of view, only one
    hardware thread is active.
    
    This adds two hooks in the SMP hotplug code to allow the KVM code to
    make sure that secondary threads (i.e. hardware threads other than
    thread 0) cannot come online while any KVM guest exists.  The KVM
    code still has to check that any core where it runs a guest has the
    secondary threads offline, but having done that check it can now be
    sure that they will not come online while the guest is running.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8d4214afc21d..c4f420c5fc1b 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -417,6 +417,45 @@ int generic_check_cpu_restart(unsigned int cpu)
 {
 	return per_cpu(cpu_state, cpu) == CPU_UP_PREPARE;
 }
+
+static atomic_t secondary_inhibit_count;
+
+/*
+ * Don't allow secondary CPU threads to come online
+ */
+void inhibit_secondary_onlining(void)
+{
+	/*
+	 * This makes secondary_inhibit_count stable during cpu
+	 * online/offline operations.
+	 */
+	get_online_cpus();
+
+	atomic_inc(&secondary_inhibit_count);
+	put_online_cpus();
+}
+EXPORT_SYMBOL_GPL(inhibit_secondary_onlining);
+
+/*
+ * Allow secondary CPU threads to come online again
+ */
+void uninhibit_secondary_onlining(void)
+{
+	get_online_cpus();
+	atomic_dec(&secondary_inhibit_count);
+	put_online_cpus();
+}
+EXPORT_SYMBOL_GPL(uninhibit_secondary_onlining);
+
+static int secondaries_inhibited(void)
+{
+	return atomic_read(&secondary_inhibit_count);
+}
+
+#else /* HOTPLUG_CPU */
+
+#define secondaries_inhibited()		0
+
 #endif
 
 static void cpu_idle_thread_init(unsigned int cpu, struct task_struct *idle)
@@ -435,6 +474,13 @@ int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
 	int rc, c;
 
+	/*
+	 * Don't allow secondary threads to come online if inhibited
+	 */
+	if (threads_per_core > 1 && secondaries_inhibited() &&
+	    cpu % threads_per_core != 0)
+		return -EBUSY;
+
 	if (smp_ops == NULL ||
 	    (smp_ops->cpu_bootable && !smp_ops->cpu_bootable(cpu)))
 		return -EINVAL;

commit e6651de9cc701ab4829b3a11a7ace85a79405d32
Author: Zhao Chenhui <chenhui.zhao@freescale.com>
Date:   Fri Jul 20 20:47:01 2012 +0800

    powerpc/smp: Do not disable IPI interrupts during suspend
    
    During suspend, all interrupts including IPI will be disabled. In this case,
    the suspend process will hang in SMP. To prevent this, pass the flag
    IRQF_NO_SUSPEND when requesting IPI irq.
    
    Signed-off-by: Zhao Chenhui <chenhui.zhao@freescale.com>
    Signed-off-by: Li Yang <leoli@freescale.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index a51ed205016e..2b952b5386fd 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -171,7 +171,7 @@ int smp_request_message_ipi(int virq, int msg)
 	}
 #endif
 	err = request_irq(virq, smp_ipi_action[msg],
-			  IRQF_PERCPU | IRQF_NO_THREAD,
+			  IRQF_PERCPU | IRQF_NO_THREAD | IRQF_NO_SUSPEND,
 			  smp_ipi_name[msg], 0);
 	WARN(err < 0, "unable to request_irq %d for %s (rc %d)\n",
 		virq, smp_ipi_name[msg], err);

commit ae5cab476342bc7311945cf89d5cbd8d57f4a5a9
Author: Zhao Chenhui <chenhui.zhao@freescale.com>
Date:   Fri Jul 20 20:42:34 2012 +0800

    powerpc/smp: add generic_set_cpu_up() to set cpu_state as CPU_UP_PREPARE
    
    In the case of cpu hotplug, the cpu_state should be set to CPU_UP_PREPARE
    when kicking cpu.  Otherwise, the cpu_state is always CPU_DEAD after
    calling generic_set_cpu_dead(), which makes the delay in generic_cpu_die()
    not happen.
    
    Signed-off-by: Zhao Chenhui <chenhui.zhao@freescale.com>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8d4214afc21d..a51ed205016e 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -102,7 +102,7 @@ int __devinit smp_generic_kick_cpu(int nr)
 	 * Ok it's not there, so it might be soft-unplugged, let's
 	 * try to bring it back
 	 */
-	per_cpu(cpu_state, nr) = CPU_UP_PREPARE;
+	generic_set_cpu_up(nr);
 	smp_wmb();
 	smp_send_reschedule(nr);
 #endif /* CONFIG_HOTPLUG_CPU */
@@ -413,6 +413,16 @@ void generic_set_cpu_dead(unsigned int cpu)
 	per_cpu(cpu_state, cpu) = CPU_DEAD;
 }
 
+/*
+ * The cpu_state should be set to CPU_UP_PREPARE in kick_cpu(), otherwise
+ * the cpu_state is always CPU_DEAD after calling generic_set_cpu_dead(),
+ * which makes the delay in generic_cpu_die() not happen.
+ */
+void generic_set_cpu_up(unsigned int cpu)
+{
+	per_cpu(cpu_state, cpu) = CPU_UP_PREPARE;
+}
+
 int generic_check_cpu_restart(unsigned int cpu)
 {
 	return per_cpu(cpu_state, cpu) == CPU_UP_PREPARE;

commit 9fb1b36ca1234e64a5d1cc573175303395e3354d
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 4 18:33:08 2012 +0000

    powerpc: Make sure IPI handlers see data written by IPI senders
    
    We have been observing hangs, both of KVM guest vcpu tasks and more
    generally, where a process that is woken doesn't properly wake up and
    continue to run, but instead sticks in TASK_WAKING state.  This
    happens because the update of rq->wake_list in ttwu_queue_remote()
    is not ordered with the update of ipi_message in
    smp_muxed_ipi_message_pass(), and the reading of rq->wake_list in
    scheduler_ipi() is not ordered with the reading of ipi_message in
    smp_ipi_demux().  Thus it is possible for the IPI receiver not to see
    the updated rq->wake_list and therefore conclude that there is nothing
    for it to do.
    
    In order to make sure that anything done before smp_send_reschedule()
    is ordered before anything done in the resulting call to scheduler_ipi(),
    this adds barriers in smp_muxed_message_pass() and smp_ipi_demux().
    The barrier in smp_muxed_message_pass() is a full barrier to ensure that
    there is a full ordering between the smp_send_reschedule() caller and
    scheduler_ipi().  In smp_ipi_demux(), we use xchg() rather than
    xchg_local() because xchg() includes release and acquire barriers.
    Using xchg() rather than xchg_local() makes sense given that
    ipi_message is not just accessed locally.
    
    This moves the barrier between setting the message and calling the
    cause_ipi() function into the individual cause_ipi implementations.
    Most of them -- those that used outb, out_8 or similar -- already had
    a full barrier because out_8 etc. include a sync before the MMIO
    store.  This adds an explicit barrier in the two remaining cases.
    
    These changes made no measurable difference to the speed of IPIs as
    measured using a simple ping-pong latency test across two CPUs on
    different cores of a POWER7 machine.
    
    The analysis of the reason why processes were not waking up properly
    is due to Milton Miller.
    
    Cc: stable@vger.kernel.org # v3.0+
    Reported-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 0321007086f7..8d4214afc21d 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -198,8 +198,15 @@ void smp_muxed_ipi_message_pass(int cpu, int msg)
 	struct cpu_messages *info = &per_cpu(ipi_message, cpu);
 	char *message = (char *)&info->messages;
 
+	/*
+	 * Order previous accesses before accesses in the IPI handler.
+	 */
+	smp_mb();
 	message[msg] = 1;
-	mb();
+	/*
+	 * cause_ipi functions are required to include a full barrier
+	 * before doing whatever causes the IPI.
+	 */
 	smp_ops->cause_ipi(cpu, info->data);
 }
 
@@ -211,7 +218,7 @@ irqreturn_t smp_ipi_demux(void)
 	mb();	/* order any irq clear */
 
 	do {
-		all = xchg_local(&info->messages, 0);
+		all = xchg(&info->messages, 0);
 
 #ifdef __BIG_ENDIAN
 		if (all & (1 << (24 - 8 * PPC_MSG_CALL_FUNCTION)))

commit 18ad51dd342a7eb09dbcd059d0b451b616d4dafc
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Jul 4 20:37:11 2012 +0000

    powerpc: Add VDSO version of getcpu
    
    We have a request for a fast method of getting CPU and NUMA node IDs
    from userspace. This patch implements a getcpu VDSO function,
    similar to x86.
    
    Ben suggested we use SPRG3 which is userspace readable. SPRG3 can be
    modified by a KVM guest, so we save the SPRG3 value in the paca and
    restore it when transitioning from the guest to the host.
    
    I have a glibc patch that implements sched_getcpu on top of this.
    Testing on a POWER7:
    
    baseline: 538 cycles
    vdso:      30 cycles
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e1417c42155c..0321007086f7 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -48,6 +48,7 @@
 #ifdef CONFIG_PPC64
 #include <asm/paca.h>
 #endif
+#include <asm/vdso.h>
 #include <asm/debug.h>
 
 #ifdef DEBUG
@@ -570,6 +571,8 @@ void __devinit start_secondary(void *unused)
 #ifdef CONFIG_PPC64
 	if (system_state == SYSTEM_RUNNING)
 		vdso_data->processorCount++;
+
+	vdso_getcpu_init();
 #endif
 	notify_cpu_starting(cpu);
 	set_cpu_online(cpu, true);

commit e250d4bca6cb91471e0757179a152c0943ecce4a
Author: Yong Zhang <yong.zhang@windriver.com>
Date:   Mon May 28 21:16:04 2012 +0000

    powerpc/smp: remove call to ipi_call_lock()/ipi_call_unlock()
    
    1) call_function.lock used in smp_call_function_many() is just to protect
       call_function.queue and &data->refs, cpu_online_mask is outside of the
       lock. And it's not necessary to protect cpu_online_mask,
       because data->cpumask is pre-calculate and even if a cpu is brougt up
       when calling arch_send_call_function_ipi_mask(), it's harmless because
       validation test in generic_smp_call_function_interrupt() will take care
       of it.
    
    2) For cpu down issue, stop_machine() will guarantee that no concurrent
       smp_call_fuction() is processing.
    
    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e4cb34322de4..e1417c42155c 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -571,7 +571,6 @@ void __devinit start_secondary(void *unused)
 	if (system_state == SYSTEM_RUNNING)
 		vdso_data->processorCount++;
 #endif
-	ipi_call_lock();
 	notify_cpu_starting(cpu);
 	set_cpu_online(cpu, true);
 	/* Update sibling maps */
@@ -601,7 +600,6 @@ void __devinit start_secondary(void *unused)
 		of_node_put(np);
 	}
 	of_node_put(l2_cache);
-	ipi_call_unlock();
 
 	local_irq_enable();
 

commit 17e32eacc3543c25a4377bb7ce54026e38db7d20
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:48 2012 +0000

    powerpc: Use generic idle thread allocation
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Link: http://lkml.kernel.org/r/20120420124557.311212868@linutronix.de

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index d38030fb3471..e4cb34322de4 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -57,27 +57,9 @@
 #define DBG(fmt...)
 #endif
 
-
-/* Store all idle threads, this can be reused instead of creating
-* a new thread. Also avoids complicated thread destroy functionality
-* for idle threads.
-*/
 #ifdef CONFIG_HOTPLUG_CPU
-/*
- * Needed only for CONFIG_HOTPLUG_CPU because __cpuinitdata is
- * removed after init for !CONFIG_HOTPLUG_CPU.
- */
-static DEFINE_PER_CPU(struct task_struct *, idle_thread_array);
-#define get_idle_for_cpu(x)      (per_cpu(idle_thread_array, x))
-#define set_idle_for_cpu(x, p)   (per_cpu(idle_thread_array, x) = (p))
-
 /* State of each CPU during hotplug phases */
 static DEFINE_PER_CPU(int, cpu_state) = { 0 };
-
-#else
-static struct task_struct *idle_thread_array[NR_CPUS] __cpuinitdata ;
-#define get_idle_for_cpu(x)      (idle_thread_array[(x)])
-#define set_idle_for_cpu(x, p)   (idle_thread_array[(x)] = (p))
 #endif
 
 struct thread_info *secondary_ti;
@@ -429,57 +411,16 @@ int generic_check_cpu_restart(unsigned int cpu)
 }
 #endif
 
-struct create_idle {
-	struct work_struct work;
-	struct task_struct *idle;
-	struct completion done;
-	int cpu;
-};
-
-static void __cpuinit do_fork_idle(struct work_struct *work)
+static void cpu_idle_thread_init(unsigned int cpu, struct task_struct *idle)
 {
-	struct create_idle *c_idle =
-		container_of(work, struct create_idle, work);
-
-	c_idle->idle = fork_idle(c_idle->cpu);
-	complete(&c_idle->done);
-}
-
-static int __cpuinit create_idle(unsigned int cpu)
-{
-	struct thread_info *ti;
-	struct create_idle c_idle = {
-		.cpu	= cpu,
-		.done	= COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
-	};
-	INIT_WORK_ONSTACK(&c_idle.work, do_fork_idle);
-
-	c_idle.idle = get_idle_for_cpu(cpu);
-
-	/* We can't use kernel_thread since we must avoid to
-	 * reschedule the child. We use a workqueue because
-	 * we want to fork from a kernel thread, not whatever
-	 * userspace process happens to be trying to online us.
-	 */
-	if (!c_idle.idle) {
-		schedule_work(&c_idle.work);
-		wait_for_completion(&c_idle.done);
-	} else
-		init_idle(c_idle.idle, cpu);
-	if (IS_ERR(c_idle.idle)) {		
-		pr_err("Failed fork for CPU %u: %li", cpu, PTR_ERR(c_idle.idle));
-		return PTR_ERR(c_idle.idle);
-	}
-	ti = task_thread_info(c_idle.idle);
+	struct thread_info *ti = task_thread_info(idle);
 
 #ifdef CONFIG_PPC64
-	paca[cpu].__current = c_idle.idle;
+	paca[cpu].__current = idle;
 	paca[cpu].kstack = (unsigned long)ti + THREAD_SIZE - STACK_FRAME_OVERHEAD;
 #endif
 	ti->cpu = cpu;
-	current_set[cpu] = ti;
-
-	return 0;
+	secondary_ti = current_set[cpu] = ti;
 }
 
 int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *tidle)
@@ -490,12 +431,7 @@ int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *tidle)
 	    (smp_ops->cpu_bootable && !smp_ops->cpu_bootable(cpu)))
 		return -EINVAL;
 
-	/* Make sure we have an idle thread */
-	rc = create_idle(cpu);
-	if (rc)
-		return rc;
-
-	secondary_ti = current_set[cpu];
+	cpu_idle_thread_init(cpu, tidle);
 
 	/* Make sure callin-map entry is 0 (can be leftover a CPU
 	 * hotplug

commit 8239c25f47d2b318156993b15f33900a86ea5e17
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:42 2012 +0000

    smp: Add task_struct argument to __cpu_up()
    
    Preparatory patch to make the idle thread allocation for secondary
    cpus generic.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20120420124556.964170564@linutronix.de

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index d9f94410fd7f..d38030fb3471 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -482,7 +482,7 @@ static int __cpuinit create_idle(unsigned int cpu)
 	return 0;
 }
 
-int __cpuinit __cpu_up(unsigned int cpu)
+int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
 	int rc, c;
 

commit ae3a197e3d0bfe3f4bf1693723e82dc018c096f3
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:02 2012 +0100

    Disintegrate asm/system.h for PowerPC
    
    Disintegrate asm/system.h for PowerPC.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    cc: linuxppc-dev@lists.ozlabs.org

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 46695febc09f..d9f94410fd7f 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -43,12 +43,12 @@
 #include <asm/machdep.h>
 #include <asm/cputhreads.h>
 #include <asm/cputable.h>
-#include <asm/system.h>
 #include <asm/mpic.h>
 #include <asm/vdso_datapage.h>
 #ifdef CONFIG_PPC64
 #include <asm/paca.h>
 #endif
+#include <asm/debug.h>
 
 #ifdef DEBUG
 #include <asm/udbg.h>

commit 7affca3537d74365128e477b40c529d6f2fe86c8
Merge: 356b95424cfb ff4b8a57f0aa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 7 12:03:30 2012 -0800

    Merge branch 'driver-core-next' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core
    
    * 'driver-core-next' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core: (73 commits)
      arm: fix up some samsung merge sysdev conversion problems
      firmware: Fix an oops on reading fw_priv->fw in sysfs loading file
      Drivers:hv: Fix a bug in vmbus_driver_unregister()
      driver core: remove __must_check from device_create_file
      debugfs: add missing #ifdef HAS_IOMEM
      arm: time.h: remove device.h #include
      driver-core: remove sysdev.h usage.
      clockevents: remove sysdev.h
      arm: convert sysdev_class to a regular subsystem
      arm: leds: convert sysdev_class to a regular subsystem
      kobject: remove kset_find_obj_hinted()
      m86k: gpio - convert sysdev_class to a regular subsystem
      mips: txx9_sram - convert sysdev_class to a regular subsystem
      mips: 7segled - convert sysdev_class to a regular subsystem
      sh: dma - convert sysdev_class to a regular subsystem
      sh: intc - convert sysdev_class to a regular subsystem
      power: suspend - convert sysdev_class to a regular subsystem
      power: qe_ic - convert sysdev_class to a regular subsystem
      power: cmm - convert sysdev_class to a regular subsystem
      s390: time - convert sysdev_class to a regular subsystem
      ...
    
    Fix up conflicts with 'struct sysdev' removal from various platform
    drivers that got changed:
     - arch/arm/mach-exynos/cpu.c
     - arch/arm/mach-exynos/irq-eint.c
     - arch/arm/mach-s3c64xx/common.c
     - arch/arm/mach-s3c64xx/cpu.c
     - arch/arm/mach-s5p64x0/cpu.c
     - arch/arm/mach-s5pv210/common.c
     - arch/arm/plat-samsung/include/plat/cpu.h
     - arch/powerpc/kernel/sysfs.c
    and fix up cpu_is_hotpluggable() as per Greg in include/linux/cpu.h

commit ff4b8a57f0aaa2882d444ca44b2b9b333d22a4df
Merge: 805a6af8dba5 ea04018e6bc5
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Fri Jan 6 11:42:52 2012 -0800

    Merge branch 'driver-core-next' into Linux 3.2
    
    This resolves the conflict in the arch/arm/mach-s3c64xx/s3c6400.c file,
    and it fixes the build error in the arch/x86/kernel/microcode_core.c
    file, that the merge did not catch.
    
    The microcode_core.c patch was provided by Stephen Rothwell
    <sfr@canb.auug.org.au> who was invaluable in the merge issues involved
    with the large sysdev removal process in the driver-core tree.
    
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 8a25a2fd126c621f44f3aeaef80d51f00fc11639
Author: Kay Sievers <kay.sievers@vrfy.org>
Date:   Wed Dec 21 14:29:42 2011 -0800

    cpu: convert 'cpu' and 'machinecheck' sysdev_class to a regular subsystem
    
    This moves the 'cpu sysdev_class' over to a regular 'cpu' subsystem
    and converts the devices to regular devices. The sysdev drivers are
    implemented as subsystem interfaces now.
    
    After all sysdev classes are ported to regular driver core entities, the
    sysdev implementation will be entirely removed from the kernel.
    
    Userspace relies on events and generic sysfs subsystem infrastructure
    from sysdev devices, which are made available with this conversion.
    
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Borislav Petkov <bp@amd64.org>
    Cc: Tigran Aivazian <tigran@aivazian.fsnet.co.uk>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Zhang Rui <rui.zhang@intel.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: "Srivatsa S. Bhat" <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 25ddbfc7dd36..da08240353fa 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -27,7 +27,7 @@
 #include <linux/spinlock.h>
 #include <linux/cache.h>
 #include <linux/err.h>
-#include <linux/sysdev.h>
+#include <linux/device.h>
 #include <linux/cpu.h>
 #include <linux/notifier.h>
 #include <linux/topology.h>

commit 3b5e16d7ad0cf4b03a0650092a6fdcb27b536a82
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 5 02:30:50 2011 +0000

    powerpc: Mark IPI interrupts IRQF_NO_THREAD
    
    IPI handlers cannot be threaded. Remove the obsolete IRQF_DISABLED
    flag (see commit e58aa3d2) while at it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 6df70907d60a..f0abe92f63f2 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -187,7 +187,8 @@ int smp_request_message_ipi(int virq, int msg)
 		return 1;
 	}
 #endif
-	err = request_irq(virq, smp_ipi_action[msg], IRQF_PERCPU,
+	err = request_irq(virq, smp_ipi_action[msg],
+			  IRQF_PERCPU | IRQF_NO_THREAD,
 			  smp_ipi_name[msg], 0);
 	WARN(err < 0, "unable to request_irq %d for %s (rc %d)\n",
 		virq, smp_ipi_name[msg], err);

commit a3a9f3b47d12b5f6dfc9c7ed9d7b193d77812195
Author: Yong Zhang <yong.zhang0@gmail.com>
Date:   Fri Oct 21 23:56:27 2011 +0000

    powerpc/irq: Remove IRQF_DISABLED
    
    Since commit [e58aa3d2: genirq: Run irq handlers with interrupts disabled],
    We run all interrupt handlers with interrupts disabled
    and we even check and yell when an interrupt handler
    returns with interrupts enabled (see commit [b738a50a:
    genirq: Warn when handler enables interrupts]).
    
    So now this flag is a NOOP and can be removed.
    
    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 25ddbfc7dd36..6df70907d60a 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -187,7 +187,7 @@ int smp_request_message_ipi(int virq, int msg)
 		return 1;
 	}
 #endif
-	err = request_irq(virq, smp_ipi_action[msg], IRQF_DISABLED|IRQF_PERCPU,
+	err = request_irq(virq, smp_ipi_action[msg], IRQF_PERCPU,
 			  smp_ipi_name[msg], 0);
 	WARN(err < 0, "unable to request_irq %d for %s (rc %d)\n",
 		virq, smp_ipi_name[msg], err);

commit 32aaeffbd4a7457bf2f7448b33b5946ff2a960eb
Merge: 208bca086040 67b84999b1a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 19:44:47 2011 -0800

    Merge branch 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    * 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux: (230 commits)
      Revert "tracing: Include module.h in define_trace.h"
      irq: don't put module.h into irq.h for tracking irqgen modules.
      bluetooth: macroize two small inlines to avoid module.h
      ip_vs.h: fix implicit use of module_get/module_put from module.h
      nf_conntrack.h: fix up fallout from implicit moduleparam.h presence
      include: replace linux/module.h with "struct module" wherever possible
      include: convert various register fcns to macros to avoid include chaining
      crypto.h: remove unused crypto_tfm_alg_modname() inline
      uwb.h: fix implicit use of asm/page.h for PAGE_SIZE
      pm_runtime.h: explicitly requires notifier.h
      linux/dmaengine.h: fix implicit use of bitmap.h and asm/page.h
      miscdevice.h: fix up implicit use of lists and types
      stop_machine.h: fix implicit use of smp.h for smp_processor_id
      of: fix implicit use of errno.h in include/linux/of.h
      of_platform.h: delete needless include <linux/module.h>
      acpi: remove module.h include from platform/aclinux.h
      miscdevice.h: delete unnecessary inclusion of module.h
      device_cgroup.h: delete needless include <linux/module.h>
      net: sch_generic remove redundant use of <linux/module.h>
      net: inet_timewait_sock doesnt need <linux/module.h>
      ...
    
    Fix up trivial conflicts (other header files, and  removal of the ab3550 mfd driver) in
     - drivers/media/dvb/frontends/dibx000_common.c
     - drivers/media/video/{mt9m111.c,ov6650.c}
     - drivers/mfd/ab3550-core.c
     - include/linux/dmaengine.h

commit 4b16f8e2d6d64249f0ed3ca7fe2a319d0dde2719
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jul 22 18:24:23 2011 -0400

    powerpc: various straight conversions from module.h --> export.h
    
    All these files were including module.h just for the basic
    EXPORT_SYMBOL infrastructure.  We can shift them off to the
    export.h header which is a way smaller footprint and thus
    realize some compile time gains.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 7bf2187dfd99..37f4c9862310 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -18,7 +18,7 @@
 #undef DEBUG
 
 #include <linux/kernel.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/sched.h>
 #include <linux/smp.h>
 #include <linux/interrupt.h>

commit fb82b83970a32263698e54a8779d2ce88cd3b060
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Sep 19 17:44:49 2011 +0000

    powerpc/smp: More generic support for "soft hotplug"
    
    This adds more generic support for doing CPU hotplug with a simple
    idle loop and no actual reset of the processors. The generic
    smp_generic_kick_cpu() does the hotplug bringup trick if the PACA
    shows that the CPU has already been started at boot and we provide
    an accessor for the CPU state.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 7bf2187dfd99..af7e7722ecae 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -70,6 +70,10 @@
 static DEFINE_PER_CPU(struct task_struct *, idle_thread_array);
 #define get_idle_for_cpu(x)      (per_cpu(idle_thread_array, x))
 #define set_idle_for_cpu(x, p)   (per_cpu(idle_thread_array, x) = (p))
+
+/* State of each CPU during hotplug phases */
+static DEFINE_PER_CPU(int, cpu_state) = { 0 };
+
 #else
 static struct task_struct *idle_thread_array[NR_CPUS] __cpuinitdata ;
 #define get_idle_for_cpu(x)      (idle_thread_array[(x)])
@@ -104,12 +108,25 @@ int __devinit smp_generic_kick_cpu(int nr)
 	 * cpu_start field to become non-zero After we set cpu_start,
 	 * the processor will continue on to secondary_start
 	 */
-	paca[nr].cpu_start = 1;
-	smp_mb();
+	if (!paca[nr].cpu_start) {
+		paca[nr].cpu_start = 1;
+		smp_mb();
+		return 0;
+	}
+
+#ifdef CONFIG_HOTPLUG_CPU
+	/*
+	 * Ok it's not there, so it might be soft-unplugged, let's
+	 * try to bring it back
+	 */
+	per_cpu(cpu_state, nr) = CPU_UP_PREPARE;
+	smp_wmb();
+	smp_send_reschedule(nr);
+#endif /* CONFIG_HOTPLUG_CPU */
 
 	return 0;
 }
-#endif
+#endif /* CONFIG_PPC64 */
 
 static irqreturn_t call_function_action(int irq, void *data)
 {
@@ -357,8 +374,6 @@ void __devinit smp_prepare_boot_cpu(void)
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
-/* State of each CPU during hotplug phases */
-static DEFINE_PER_CPU(int, cpu_state) = { 0 };
 
 int generic_cpu_disable(void)
 {
@@ -406,6 +421,11 @@ void generic_set_cpu_dead(unsigned int cpu)
 {
 	per_cpu(cpu_state, cpu) = CPU_DEAD;
 }
+
+int generic_check_cpu_restart(unsigned int cpu)
+{
+	return per_cpu(cpu_state, cpu) == CPU_UP_PREPARE;
+}
 #endif
 
 struct create_idle {

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index f932f8a0cf0c..7bf2187dfd99 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -33,7 +33,7 @@
 #include <linux/topology.h>
 
 #include <asm/ptrace.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #include <asm/irq.h>
 #include <asm/page.h>
 #include <asm/pgtable.h>

commit 184475029a724b6b900d88fc3a5f462a6107d5af
Merge: 3b76eefe0f97 f1f4ee01c0d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 22:59:39 2011 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (99 commits)
      drivers/virt: add missing linux/interrupt.h to fsl_hypervisor.c
      powerpc/85xx: fix mpic configuration in CAMP mode
      powerpc: Copy back TIF flags on return from softirq stack
      powerpc/64: Make server perfmon only built on ppc64 server devices
      powerpc/pseries: Fix hvc_vio.c build due to recent changes
      powerpc: Exporting boot_cpuid_phys
      powerpc: Add CFAR to oops output
      hvc_console: Add kdb support
      powerpc/pseries: Fix hvterm_raw_get_chars to accept < 16 chars, fixing xmon
      powerpc/irq: Quieten irq mapping printks
      powerpc: Enable lockup and hung task detectors in pseries and ppc64 defeconfigs
      powerpc: Add mpt2sas driver to pseries and ppc64 defconfig
      powerpc: Disable IRQs off tracer in ppc64 defconfig
      powerpc: Sync pseries and ppc64 defconfigs
      powerpc/pseries/hvconsole: Fix dropped console output
      hvc_console: Improve tty/console put_chars handling
      powerpc/kdump: Fix timeout in crash_kexec_wait_realmode
      powerpc/mm: Fix output of total_ram.
      powerpc/cpufreq: Add cpufreq driver for Momentum Maple boards
      powerpc: Correct annotations of pmu registration functions
      ...
    
    Fix up trivial Kconfig/Makefile conflicts in arch/powerpc, drivers, and
    drivers/cpufreq

commit de56a948b9182fbcf92cb8212f114de096c2d574
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:21:34 2011 +0000

    KVM: PPC: Add support for Book3S processors in hypervisor mode
    
    This adds support for KVM running on 64-bit Book 3S processors,
    specifically POWER7, in hypervisor mode.  Using hypervisor mode means
    that the guest can use the processor's supervisor mode.  That means
    that the guest can execute privileged instructions and access privileged
    registers itself without trapping to the host.  This gives excellent
    performance, but does mean that KVM cannot emulate a processor
    architecture other than the one that the hardware implements.
    
    This code assumes that the guest is running paravirtualized using the
    PAPR (Power Architecture Platform Requirements) interface, which is the
    interface that IBM's PowerVM hypervisor uses.  That means that existing
    Linux distributions that run on IBM pSeries machines will also run
    under KVM without modification.  In order to communicate the PAPR
    hypercalls to qemu, this adds a new KVM_EXIT_PAPR_HCALL exit code
    to include/linux/kvm.h.
    
    Currently the choice between book3s_hv support and book3s_pr support
    (i.e. the existing code, which runs the guest in user mode) has to be
    made at kernel configuration time, so a given kernel binary can only
    do one or the other.
    
    This new book3s_hv code doesn't support MMIO emulation at present.
    Since we are running paravirtualized guests, this isn't a serious
    restriction.
    
    With the guest running in supervisor mode, most exceptions go straight
    to the guest.  We will never get data or instruction storage or segment
    interrupts, alignment interrupts, decrementer interrupts, program
    interrupts, single-step interrupts, etc., coming to the hypervisor from
    the guest.  Therefore this introduces a new KVMTEST_NONHV macro for the
    exception entry path so that we don't have to do the KVM test on entry
    to those exception handlers.
    
    We do however get hypervisor decrementer, hypervisor data storage,
    hypervisor instruction storage, and hypervisor emulation assist
    interrupts, so we have to handle those.
    
    In hypervisor mode, real-mode accesses can access all of RAM, not just
    a limited amount.  Therefore we put all the guest state in the vcpu.arch
    and use the shadow_vcpu in the PACA only for temporary scratch space.
    We allocate the vcpu with kzalloc rather than vzalloc, and we don't use
    anything in the kvmppc_vcpu_book3s struct, so we don't allocate it.
    We don't have a shared page with the guest, but we still need a
    kvm_vcpu_arch_shared struct to store the values of various registers,
    so we include one in the vcpu_arch struct.
    
    The POWER7 processor has a restriction that all threads in a core have
    to be in the same partition.  MMU-on kernel code counts as a partition
    (partition 0), so we have to do a partition switch on every entry to and
    exit from the guest.  At present we require the host and guest to run
    in single-thread mode because of this hardware restriction.
    
    This code allocates a hashed page table for the guest and initializes
    it with HPTEs for the guest's Virtual Real Memory Area (VRMA).  We
    require that the guest memory is allocated using 16MB huge pages, in
    order to simplify the low-level memory management.  This also means that
    we can get away without tracking paging activity in the host for now,
    since huge pages can't be paged or swapped.
    
    This also adds a few new exports needed by the book3s_hv code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8ebc6700b98d..09a85a9045d6 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -243,6 +243,7 @@ void smp_send_reschedule(int cpu)
 	if (likely(smp_ops))
 		smp_ops->message_pass(cpu, PPC_MSG_RESCHEDULE);
 }
+EXPORT_SYMBOL_GPL(smp_send_reschedule);
 
 void arch_send_call_function_single_ipi(int cpu)
 {

commit 3160b09796129abc9523ea3cd1633b0faba64a02
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Tue Jun 28 14:54:47 2011 -0500

    powerpc: Create next_tlbcam_idx percpu variable for FSL_BOOKE
    
    This is used to round-robin TLBCAM entries.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 6c8e739a12da..567a1746ed74 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -305,6 +305,10 @@ struct thread_info *current_set[NR_CPUS];
 static void __devinit smp_store_cpu_info(int id)
 {
 	per_cpu(cpu_pvr, id) = mfspr(SPRN_PVR);
+#ifdef CONFIG_PPC_FSL_BOOK3E
+	per_cpu(next_tlbcam_idx, id)
+		= (mfspr(SPRN_TLB1CFG) & TLBnCFG_N_ENTRY) - 1;
+#endif
 }
 
 void __init smp_prepare_cpus(unsigned int max_cpus)

commit 3d97a619acbb2c8a7a9a7da08c2d3041dfdd241f
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Jun 22 11:19:49 2011 +0000

    powerpc/book3e-64: Reraise doorbell when masked by soft-irq-disable
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 2975f64cf310..6c8e739a12da 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -202,14 +202,6 @@ void smp_muxed_ipi_message_pass(int cpu, int msg)
 	smp_ops->cause_ipi(cpu, info->data);
 }
 
-void smp_muxed_ipi_resend(void)
-{
-	struct cpu_messages *info = &__get_cpu_var(ipi_message);
-
-	if (info->messages)
-		smp_ops->cause_ipi(smp_processor_id(), info->data);
-}
-
 irqreturn_t smp_ipi_demux(void)
 {
 	struct cpu_messages *info = &__get_cpu_var(ipi_message);

commit 9ca980dce523760ce04a798470d36fd5aa596b78
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed May 25 23:34:12 2011 +0000

    powerpc: Avoid extra indirect function call in sending IPIs
    
    On many platforms (including pSeries), smp_ops->message_pass is always
    smp_muxed_ipi_message_pass.  This changes arch/powerpc/kernel/smp.c so
    that if smp_ops->message_pass is NULL, it calls smp_muxed_ipi_message_pass
    directly.
    
    This means that a platform doesn't need to set both .message_pass and
    .cause_ipi, only one of them.  It is a slight performance improvement
    in that it gets rid of an indirect function call at the expense of a
    predictable conditional branch.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8ebc6700b98d..2975f64cf310 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -238,15 +238,25 @@ irqreturn_t smp_ipi_demux(void)
 }
 #endif /* CONFIG_PPC_SMP_MUXED_IPI */
 
+static inline void do_message_pass(int cpu, int msg)
+{
+	if (smp_ops->message_pass)
+		smp_ops->message_pass(cpu, msg);
+#ifdef CONFIG_PPC_SMP_MUXED_IPI
+	else
+		smp_muxed_ipi_message_pass(cpu, msg);
+#endif
+}
+
 void smp_send_reschedule(int cpu)
 {
 	if (likely(smp_ops))
-		smp_ops->message_pass(cpu, PPC_MSG_RESCHEDULE);
+		do_message_pass(cpu, PPC_MSG_RESCHEDULE);
 }
 
 void arch_send_call_function_single_ipi(int cpu)
 {
-	smp_ops->message_pass(cpu, PPC_MSG_CALL_FUNC_SINGLE);
+	do_message_pass(cpu, PPC_MSG_CALL_FUNC_SINGLE);
 }
 
 void arch_send_call_function_ipi_mask(const struct cpumask *mask)
@@ -254,7 +264,7 @@ void arch_send_call_function_ipi_mask(const struct cpumask *mask)
 	unsigned int cpu;
 
 	for_each_cpu(cpu, mask)
-		smp_ops->message_pass(cpu, PPC_MSG_CALL_FUNCTION);
+		do_message_pass(cpu, PPC_MSG_CALL_FUNCTION);
 }
 
 #if defined(CONFIG_DEBUGGER) || defined(CONFIG_KEXEC)
@@ -268,7 +278,7 @@ void smp_send_debugger_break(void)
 
 	for_each_online_cpu(cpu)
 		if (cpu != me)
-			smp_ops->message_pass(cpu, PPC_MSG_DEBUGGER_BREAK);
+			do_message_pass(cpu, PPC_MSG_DEBUGGER_BREAK);
 }
 #endif
 

commit 7ef71d753ea0286bfeb4251b9ba592716ebdd9e8
Author: Milton Miller <miltonm@bga.com>
Date:   Tue May 24 20:34:18 2011 +0000

    powerpc/cell: Use common smp ipi actions
    
    The cell iic interrupt controller has enough software caused interrupts
    to use a unique interrupt for each of the 4 messages powerpc uses.
    This means each interrupt gets its own irq action/data combination.
    
    Use the seperate, optimized, arch common ipi action functions
    registered via the helper smp_request_message_ipi instead passing the
    message as action data to a single action that then demultipexes to
    the required acton via a switch statement.
    
    smp_request_message_ipi will register the action as IRQF_PER_CPU
    and IRQF_DISABLED, and WARN if the allocation fails for some reason,
    so no need to print on that failure.  It will return positive if
    the message will not be used by the kernel, in which case we can
    free the virq.
    
    In addition to elimiating inefficient code, this also corrects the
    error that a kernel built with kexec but without a debugger would
    not register the ipi for kdump to notify the other cpus of a crash.
    
    This also restores the debugger action to be static to kernel/smp.c.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 4a6f2ec7e761..8ebc6700b98d 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -129,7 +129,7 @@ static irqreturn_t call_function_single_action(int irq, void *data)
 	return IRQ_HANDLED;
 }
 
-irqreturn_t debug_ipi_action(int irq, void *data)
+static irqreturn_t debug_ipi_action(int irq, void *data)
 {
 	if (crash_ipi_function_ptr) {
 		crash_ipi_function_ptr(get_irq_regs());

commit 880102e78547c1db158a17e36cf0cdd98e7ad710
Merge: 3d07f0e83d43 39ab05c8e0b5
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri May 20 15:36:52 2011 +1000

    Merge remote branch 'origin/master' into merge
    
    Manual merge of arch/powerpc/kernel/smp.c and add missing scheduler_ipi()
    call to arch/powerpc/platforms/cell/interrupt.c
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 4c8440666b995f20604852b35dcfdbcc1d5931f1
Merge: 751e1f5099f1 c560bbceaf6b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu May 19 17:00:06 2011 +1000

    Merge branch 'merge' into next

commit 714542721b4a53a3ebbdd5f0619ac0f66e7df610
Author: Milton Miller <miltonm@bga.com>
Date:   Tue May 10 19:29:46 2011 +0000

    powerpc: Use bytes instead of bitops in smp ipi multiplexing
    
    Since there are only 4 messages, we can replace the atomic bit set
    (which uses atomic load reserve and store conditional sequence) with
    a byte stores to seperate bytes.  We still have to perform a load
    reserve and store conditional sequence to avoid loosing messages on
    reception but we can do that with a single call to xchg.
    
    The do {} while and __BIG_ENDIAN specific mask testing was chosen by
    looking at the generated asm code.  On gcc-4.4, the bit masking becomes
    a simple bit mask and test of the register returned from xchg without
    storing and loading the value to the stack like attempts with a union
    of bytes and an int (or worse, loading single bit constants from the
    constant pool into non-voliatle registers that had to be preseved on
    the stack).  The do {} while avoids an unconditional branch to the
    end of the loop to test the entry / repeat condition of a while loop
    and instead optimises for the expected single iteration of the loop.
    
    We have a full mb() at the beginning to cover ordering between send,
    ipi, and receive so we can use xchg_local and forgo the further
    acquire and release barriers of xchg.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index d76f7d7929be..a8909aa50642 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -180,7 +180,7 @@ int smp_request_message_ipi(int virq, int msg)
 
 #ifdef CONFIG_PPC_SMP_MUXED_IPI
 struct cpu_messages {
-	unsigned long messages;		/* current messages bits */
+	int messages;			/* current messages */
 	unsigned long data;		/* data for cause ipi */
 };
 static DEFINE_PER_CPU_SHARED_ALIGNED(struct cpu_messages, ipi_message);
@@ -195,9 +195,9 @@ void smp_muxed_ipi_set_data(int cpu, unsigned long data)
 void smp_muxed_ipi_message_pass(int cpu, int msg)
 {
 	struct cpu_messages *info = &per_cpu(ipi_message, cpu);
-	unsigned long *tgt = &info->messages;
+	char *message = (char *)&info->messages;
 
-	set_bit(msg, tgt);
+	message[msg] = 1;
 	mb();
 	smp_ops->cause_ipi(cpu, info->data);
 }
@@ -205,30 +205,35 @@ void smp_muxed_ipi_message_pass(int cpu, int msg)
 void smp_muxed_ipi_resend(void)
 {
 	struct cpu_messages *info = &__get_cpu_var(ipi_message);
-	unsigned long *tgt = &info->messages;
 
-	if (*tgt)
+	if (info->messages)
 		smp_ops->cause_ipi(smp_processor_id(), info->data);
 }
 
 irqreturn_t smp_ipi_demux(void)
 {
 	struct cpu_messages *info = &__get_cpu_var(ipi_message);
-	unsigned long *tgt = &info->messages;
+	unsigned int all;
 
 	mb();	/* order any irq clear */
-	while (*tgt) {
-		if (test_and_clear_bit(PPC_MSG_CALL_FUNCTION, tgt))
+
+	do {
+		all = xchg_local(&info->messages, 0);
+
+#ifdef __BIG_ENDIAN
+		if (all & (1 << (24 - 8 * PPC_MSG_CALL_FUNCTION)))
 			generic_smp_call_function_interrupt();
-		if (test_and_clear_bit(PPC_MSG_RESCHEDULE, tgt))
+		if (all & (1 << (24 - 8 * PPC_MSG_RESCHEDULE)))
 			reschedule_action(0, NULL); /* upcoming sched hook */
-		if (test_and_clear_bit(PPC_MSG_CALL_FUNC_SINGLE, tgt))
+		if (all & (1 << (24 - 8 * PPC_MSG_CALL_FUNC_SINGLE)))
 			generic_smp_call_function_single_interrupt();
-#if defined(CONFIG_DEBUGGER) || defined(CONFIG_KEXEC)
-		if (test_and_clear_bit(PPC_MSG_DEBUGGER_BREAK, tgt))
+		if (all & (1 << (24 - 8 * PPC_MSG_DEBUGGER_BREAK)))
 			debug_ipi_action(0, NULL);
+#else
+#error Unsupported ENDIAN
 #endif
-	}
+	} while (info->messages);
+
 	return IRQ_HANDLED;
 }
 #endif /* CONFIG_PPC_SMP_MUXED_IPI */

commit 1ece355b6825b7c61d1dc39a5c6cf49dc746e193
Author: Milton Miller <miltonm@bga.com>
Date:   Tue May 10 19:29:42 2011 +0000

    powerpc: Add kconfig for muxed smp ipi support
    
    Compile the new smp ipi mux and demux code only if a platform
    will make use of it.  The new config is selected as required.
    
    The new cause_ipi smp op is only available conditionally to point out
    configs where the select is required; this makes setting the op an
    immediate fail instead of a deferred unresolved symbol at link.
    
    This also creates a new config for power surge powermac upgrade support
    that can be disabled in expert mode but is default on.
    
    I also removed the depends / default y on CONFIG_XICS since it is selected
    by PSERIES.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index fa8e8700064b..d76f7d7929be 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -178,6 +178,7 @@ int smp_request_message_ipi(int virq, int msg)
 	return err;
 }
 
+#ifdef CONFIG_PPC_SMP_MUXED_IPI
 struct cpu_messages {
 	unsigned long messages;		/* current messages bits */
 	unsigned long data;		/* data for cause ipi */
@@ -230,6 +231,7 @@ irqreturn_t smp_ipi_demux(void)
 	}
 	return IRQ_HANDLED;
 }
+#endif /* CONFIG_PPC_SMP_MUXED_IPI */
 
 void smp_send_reschedule(int cpu)
 {

commit 23d72bfd8f9f24aa9efafed3586a99f5669c23d7
Author: Milton Miller <miltonm@bga.com>
Date:   Tue May 10 19:29:39 2011 +0000

    powerpc: Consolidate ipi message mux and demux
    
    Consolidate the mux and demux of ipi messages into smp.c and call
    a new smp_ops callback to actually trigger the ipi.
    
    The powerpc architecture code is optimised for having 4 distinct
    ipi triggers, which are mapped to 4 distinct messages (ipi many, ipi
    single, scheduler ipi, and enter debugger).  However, several interrupt
    controllers only provide a single software triggered interrupt that
    can be delivered to each cpu.  To resolve this limitation, each smp_ops
    implementation created a per-cpu variable that is manipulated with atomic
    bitops.  Since these lines will be contended they are optimialy marked as
    shared_aligned and take a full cache line for each cpu.  Distro kernels
    may have 2 or 3 of these in their config, each taking per-cpu space
    even though at most one will be in use.
    
    This consolidation removes smp_message_recv and replaces the single call
    actions cases with direct calls from the common message recognition loop.
    The complicated debugger ipi case with its muxed crash handling code is
    moved to debug_ipi_action which is now called from the demux code (instead
    of the multi-message action calling smp_message_recv).
    
    I put a call to reschedule_action to increase the likelyhood of correctly
    merging the anticipated scheduler_ipi() hook coming from the scheduler
    tree; that single required call can be inlined later.
    
    The actual message decode is a copy of the old pseries xics code with its
    memory barriers and cache line spacing, augmented with a per-cpu unsigned
    long based on the book-e doorbell code.  The optional data is set via a
    callback from the implementation and is passed to the new cause-ipi hook
    along with the logical cpu number.  While currently only the doorbell
    implemntation uses this data it should be almost zero cost to retrieve and
    pass it -- it adds a single register load for the argument from the same
    cache line to which we just completed a store and the register is dead
    on return from the call.  I extended the data element from unsigned int
    to unsigned long in case some other code wanted to associate a pointer.
    
    The doorbell check_self is replaced by a call to smp_muxed_ipi_resend,
    conditioned on the CPU_DBELL feature.  The ifdef guard could be relaxed
    to CONFIG_SMP but I left it with BOOKE for now.
    
    Also, the doorbell interrupt vector for book-e was not calling irq_enter
    and irq_exit, which throws off cpu accounting and causes code to not
    realize it is running in interrupt context.  Add the missing calls.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index b74411446922..fa8e8700064b 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -111,35 +111,6 @@ int __devinit smp_generic_kick_cpu(int nr)
 }
 #endif
 
-void smp_message_recv(int msg)
-{
-	switch(msg) {
-	case PPC_MSG_CALL_FUNCTION:
-		generic_smp_call_function_interrupt();
-		break;
-	case PPC_MSG_RESCHEDULE:
-		/* we notice need_resched on exit */
-		break;
-	case PPC_MSG_CALL_FUNC_SINGLE:
-		generic_smp_call_function_single_interrupt();
-		break;
-	case PPC_MSG_DEBUGGER_BREAK:
-		if (crash_ipi_function_ptr) {
-			crash_ipi_function_ptr(get_irq_regs());
-			break;
-		}
-#ifdef CONFIG_DEBUGGER
-		debugger_ipi(get_irq_regs());
-		break;
-#endif /* CONFIG_DEBUGGER */
-		/* FALLTHROUGH */
-	default:
-		printk("SMP %d: smp_message_recv(): unknown msg %d\n",
-		       smp_processor_id(), msg);
-		break;
-	}
-}
-
 static irqreturn_t call_function_action(int irq, void *data)
 {
 	generic_smp_call_function_interrupt();
@@ -158,9 +129,17 @@ static irqreturn_t call_function_single_action(int irq, void *data)
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t debug_ipi_action(int irq, void *data)
+irqreturn_t debug_ipi_action(int irq, void *data)
 {
-	smp_message_recv(PPC_MSG_DEBUGGER_BREAK);
+	if (crash_ipi_function_ptr) {
+		crash_ipi_function_ptr(get_irq_regs());
+		return IRQ_HANDLED;
+	}
+
+#ifdef CONFIG_DEBUGGER
+	debugger_ipi(get_irq_regs());
+#endif /* CONFIG_DEBUGGER */
+
 	return IRQ_HANDLED;
 }
 
@@ -199,6 +178,59 @@ int smp_request_message_ipi(int virq, int msg)
 	return err;
 }
 
+struct cpu_messages {
+	unsigned long messages;		/* current messages bits */
+	unsigned long data;		/* data for cause ipi */
+};
+static DEFINE_PER_CPU_SHARED_ALIGNED(struct cpu_messages, ipi_message);
+
+void smp_muxed_ipi_set_data(int cpu, unsigned long data)
+{
+	struct cpu_messages *info = &per_cpu(ipi_message, cpu);
+
+	info->data = data;
+}
+
+void smp_muxed_ipi_message_pass(int cpu, int msg)
+{
+	struct cpu_messages *info = &per_cpu(ipi_message, cpu);
+	unsigned long *tgt = &info->messages;
+
+	set_bit(msg, tgt);
+	mb();
+	smp_ops->cause_ipi(cpu, info->data);
+}
+
+void smp_muxed_ipi_resend(void)
+{
+	struct cpu_messages *info = &__get_cpu_var(ipi_message);
+	unsigned long *tgt = &info->messages;
+
+	if (*tgt)
+		smp_ops->cause_ipi(smp_processor_id(), info->data);
+}
+
+irqreturn_t smp_ipi_demux(void)
+{
+	struct cpu_messages *info = &__get_cpu_var(ipi_message);
+	unsigned long *tgt = &info->messages;
+
+	mb();	/* order any irq clear */
+	while (*tgt) {
+		if (test_and_clear_bit(PPC_MSG_CALL_FUNCTION, tgt))
+			generic_smp_call_function_interrupt();
+		if (test_and_clear_bit(PPC_MSG_RESCHEDULE, tgt))
+			reschedule_action(0, NULL); /* upcoming sched hook */
+		if (test_and_clear_bit(PPC_MSG_CALL_FUNC_SINGLE, tgt))
+			generic_smp_call_function_single_interrupt();
+#if defined(CONFIG_DEBUGGER) || defined(CONFIG_KEXEC)
+		if (test_and_clear_bit(PPC_MSG_DEBUGGER_BREAK, tgt))
+			debug_ipi_action(0, NULL);
+#endif
+	}
+	return IRQ_HANDLED;
+}
+
 void smp_send_reschedule(int cpu)
 {
 	if (likely(smp_ops))

commit e04763713286b1e00e1c2a33fe2741caf9470f2b
Author: Milton Miller <miltonm@bga.com>
Date:   Tue May 10 19:29:06 2011 +0000

    powerpc: Remove call sites of MSG_ALL_BUT_SELF
    
    The only user of MSG_ALL_BUT_SELF in the whole kernel tree is powerpc,
    and it only uses it to start the debugger. Both debuggers always call
    smp_send_debugger_break with MSG_ALL_BUT_SELF, and only mpic can do
    anything more optimal than a loop over all online cpus, but all message
    passing implementations have to code for this special delivery target.
    
    Convert smp_send_debugger_break to take void and loop calling the smp_ops
    message_pass function for each of the other cpus in the online cpumask.
    
    Use raw_smp_processor_id() because we are either entering the debugger
    or trying to start kdump and the additional warning it not useful were
    it to trigger.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 87517ab6d365..b74411446922 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -218,11 +218,18 @@ void arch_send_call_function_ipi_mask(const struct cpumask *mask)
 		smp_ops->message_pass(cpu, PPC_MSG_CALL_FUNCTION);
 }
 
-#ifdef CONFIG_DEBUGGER
-void smp_send_debugger_break(int cpu)
+#if defined(CONFIG_DEBUGGER) || defined(CONFIG_KEXEC)
+void smp_send_debugger_break(void)
 {
-	if (likely(smp_ops))
-		smp_ops->message_pass(cpu, PPC_MSG_DEBUGGER_BREAK);
+	int cpu;
+	int me = raw_smp_processor_id();
+
+	if (unlikely(!smp_ops))
+		return;
+
+	for_each_online_cpu(cpu)
+		if (cpu != me)
+			smp_ops->message_pass(cpu, PPC_MSG_DEBUGGER_BREAK);
 }
 #endif
 
@@ -230,9 +237,9 @@ void smp_send_debugger_break(int cpu)
 void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
 {
 	crash_ipi_function_ptr = crash_ipi_callback;
-	if (crash_ipi_callback && smp_ops) {
+	if (crash_ipi_callback) {
 		mb();
-		smp_ops->message_pass(MSG_ALL_BUT_SELF, PPC_MSG_DEBUGGER_BREAK);
+		smp_send_debugger_break();
 	}
 }
 #endif

commit c560bbceaf6b06e52f1ef20131b76a3fdc0a2c19
Author: kerstin jonsson <kerstin.jonsson@ericsson.com>
Date:   Tue May 17 23:57:11 2011 +0000

    powerpc/4xx: Fix regression in SMP on 476
    
    commit c56e58537d504706954a06570b4034c04e5b7500 breaks SMP support in PPC_47x chip.
     secondary_ti must be set to current thread info before callin kick_cpu or else
     start_secondary_47x will jump into void when trying to return to c-code.
     In the current setup secondary_ti is initialized before the CPU idle task is started
     and only the boot core will start. I am not sure this is the correct solution, but it
     makes SMP possible in my chip.
     Note! The HOTPLUG support probably need some fixing to, There is no trampoline code
     available in head_44x.S - start_secondary_resume?
    
    Signed-off-by: Kerstin Jonsson <kerstin.jonsson@ericsson.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index cbdbb14be4b0..f2dcab7aadc8 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -410,8 +410,6 @@ int __cpuinit __cpu_up(unsigned int cpu)
 {
 	int rc, c;
 
-	secondary_ti = current_set[cpu];
-
 	if (smp_ops == NULL ||
 	    (smp_ops->cpu_bootable && !smp_ops->cpu_bootable(cpu)))
 		return -EINVAL;
@@ -421,6 +419,8 @@ int __cpuinit __cpu_up(unsigned int cpu)
 	if (rc)
 		return rc;
 
+	secondary_ti = current_set[cpu];
+
 	/* Make sure callin-map entry is 0 (can be leftover a CPU
 	 * hotplug
 	 */

commit 104699c0ab473535793b5fea156adaf309afd29b
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Thu Apr 28 05:07:23 2011 +0000

    powerpc: Convert old cpumask API into new one
    
    Adapt new API.
    
    Almost change is trivial. Most important change is the below line
    because we plan to change task->cpus_allowed implementation.
    
    -       ctx->cpus_allowed = current->cpus_allowed;
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index b6083f4f39b1..87517ab6d365 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -513,7 +513,7 @@ int cpu_first_thread_of_core(int core)
 }
 EXPORT_SYMBOL_GPL(cpu_first_thread_of_core);
 
-/* Must be called when no change can occur to cpu_present_map,
+/* Must be called when no change can occur to cpu_present_mask,
  * i.e. during cpu online or offline.
  */
 static struct device_node *cpu_to_l2cache(int cpu)
@@ -614,7 +614,7 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	 * se we pin us down to CPU 0 for a short while
 	 */
 	alloc_cpumask_var(&old_mask, GFP_NOWAIT);
-	cpumask_copy(old_mask, &current->cpus_allowed);
+	cpumask_copy(old_mask, tsk_cpus_allowed(current));
 	set_cpus_allowed_ptr(current, cpumask_of(boot_cpuid));
 	
 	if (smp_ops && smp_ops->setup_cpu)

commit de300974761d92f71cb583730ac9e1d4eb1b7156
Author: Michael Ellerman <michael@ozlabs.org>
Date:   Mon Apr 11 21:46:19 2011 +0000

    powerpc/smp: smp_ops->kick_cpu() should be able to fail
    
    When we start a cpu we use smp_ops->kick_cpu(), which currently
    returns void, it should be able to fail. Convert it to return
    int, and update all uses.
    
    Convert all the current error cases to return -ENOENT, which is
    what would eventually be returned by __cpu_up() currently when
    it doesn't detect the cpu as coming up in time.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index cbdbb14be4b0..b6083f4f39b1 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -95,7 +95,7 @@ int smt_enabled_at_boot = 1;
 static void (*crash_ipi_function_ptr)(struct pt_regs *) = NULL;
 
 #ifdef CONFIG_PPC64
-void __devinit smp_generic_kick_cpu(int nr)
+int __devinit smp_generic_kick_cpu(int nr)
 {
 	BUG_ON(nr < 0 || nr >= NR_CPUS);
 
@@ -106,6 +106,8 @@ void __devinit smp_generic_kick_cpu(int nr)
 	 */
 	paca[nr].cpu_start = 1;
 	smp_mb();
+
+	return 0;
 }
 #endif
 
@@ -434,7 +436,11 @@ int __cpuinit __cpu_up(unsigned int cpu)
 
 	/* wake up cpus */
 	DBG("smp: kicking cpu %d\n", cpu);
-	smp_ops->kick_cpu(cpu);
+	rc = smp_ops->kick_cpu(cpu);
+	if (rc) {
+		pr_err("smp: failed starting cpu %d (rc %d)\n", cpu, rc);
+		return rc;
+	}
 
 	/*
 	 * wait to see if the cpu made a callin (is actually up).

commit 184748cc50b2dceb8287f9fb657eda48ff8fcfe7
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 5 17:23:39 2011 +0200

    sched: Provide scheduler_ipi() callback in response to smp_send_reschedule()
    
    For future rework of try_to_wake_up() we'd like to push part of that
    function onto the CPU the task is actually going to run on.
    
    In order to do so we need a generic callback from the existing scheduler IPI.
    
    This patch introduces such a generic callback: scheduler_ipi() and
    implements it as a NOP.
    
    BenH notes: PowerPC might use this IPI on offline CPUs under rare conditions!
    
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Acked-by: Chris Metcalf <cmetcalf@tilera.com>
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110405152728.744338123@chello.nl

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index cbdbb14be4b0..9f9c204bef69 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -116,7 +116,7 @@ void smp_message_recv(int msg)
 		generic_smp_call_function_interrupt();
 		break;
 	case PPC_MSG_RESCHEDULE:
-		/* we notice need_resched on exit */
+		scheduler_ipi();
 		break;
 	case PPC_MSG_CALL_FUNC_SINGLE:
 		generic_smp_call_function_single_interrupt();
@@ -146,7 +146,7 @@ static irqreturn_t call_function_action(int irq, void *data)
 
 static irqreturn_t reschedule_action(int irq, void *data)
 {
-	/* we just need the return path side effect of checking need_resched */
+	scheduler_ipi();
 	return IRQ_HANDLED;
 }
 

commit aeeafbfa7a5692c68d306043878aa2dd785e5230
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Mar 8 14:49:33 2011 +1100

    powerpc/smp: Increase vdso_data->processorCount, not just decrease it
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 54faff91b805..cbdbb14be4b0 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -551,6 +551,10 @@ void __devinit start_secondary(void *unused)
 
 	secondary_cpu_time_init();
 
+#ifdef CONFIG_PPC64
+	if (system_state == SYSTEM_RUNNING)
+		vdso_data->processorCount++;
+#endif
 	ipi_call_lock();
 	notify_cpu_starting(cpu);
 	set_cpu_online(cpu, true);

commit c56e58537d504706954a06570b4034c04e5b7500
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Mar 8 14:40:04 2011 +1100

    powerpc/smp: Create idle threads on demand and properly reset them
    
    Instead of creating idle threads at boot for all possible CPUs, we
    create them on demand, like x86 or ARM, and we properly call init_idle
    to re-initialize an idle thread when a CPU was unplugged and is now
    re-plugged.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index d7f8cc18ae05..54faff91b805 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -57,6 +57,25 @@
 #define DBG(fmt...)
 #endif
 
+
+/* Store all idle threads, this can be reused instead of creating
+* a new thread. Also avoids complicated thread destroy functionality
+* for idle threads.
+*/
+#ifdef CONFIG_HOTPLUG_CPU
+/*
+ * Needed only for CONFIG_HOTPLUG_CPU because __cpuinitdata is
+ * removed after init for !CONFIG_HOTPLUG_CPU.
+ */
+static DEFINE_PER_CPU(struct task_struct *, idle_thread_array);
+#define get_idle_for_cpu(x)      (per_cpu(idle_thread_array, x))
+#define set_idle_for_cpu(x, p)   (per_cpu(idle_thread_array, x) = (p))
+#else
+static struct task_struct *idle_thread_array[NR_CPUS] __cpuinitdata ;
+#define get_idle_for_cpu(x)      (idle_thread_array[(x)])
+#define set_idle_for_cpu(x, p)   (idle_thread_array[(x)] = (p))
+#endif
+
 struct thread_info *secondary_ti;
 
 DEFINE_PER_CPU(cpumask_var_t, cpu_sibling_map);
@@ -238,23 +257,6 @@ static void __devinit smp_store_cpu_info(int id)
 	per_cpu(cpu_pvr, id) = mfspr(SPRN_PVR);
 }
 
-static void __init smp_create_idle(unsigned int cpu)
-{
-	struct task_struct *p;
-
-	/* create a process for the processor */
-	p = fork_idle(cpu);
-	if (IS_ERR(p))
-		panic("failed fork for CPU %u: %li", cpu, PTR_ERR(p));
-#ifdef CONFIG_PPC64
-	paca[cpu].__current = p;
-	paca[cpu].kstack = (unsigned long) task_thread_info(p)
-		+ THREAD_SIZE - STACK_FRAME_OVERHEAD;
-#endif
-	current_set[cpu] = task_thread_info(p);
-	task_thread_info(p)->cpu = cpu;
-}
-
 void __init smp_prepare_cpus(unsigned int max_cpus)
 {
 	unsigned int cpu;
@@ -288,10 +290,6 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 			max_cpus = NR_CPUS;
 	else
 		max_cpus = 1;
-
-	for_each_possible_cpu(cpu)
-		if (cpu != boot_cpuid)
-			smp_create_idle(cpu);
 }
 
 void __devinit smp_prepare_boot_cpu(void)
@@ -355,9 +353,62 @@ void generic_set_cpu_dead(unsigned int cpu)
 }
 #endif
 
+struct create_idle {
+	struct work_struct work;
+	struct task_struct *idle;
+	struct completion done;
+	int cpu;
+};
+
+static void __cpuinit do_fork_idle(struct work_struct *work)
+{
+	struct create_idle *c_idle =
+		container_of(work, struct create_idle, work);
+
+	c_idle->idle = fork_idle(c_idle->cpu);
+	complete(&c_idle->done);
+}
+
+static int __cpuinit create_idle(unsigned int cpu)
+{
+	struct thread_info *ti;
+	struct create_idle c_idle = {
+		.cpu	= cpu,
+		.done	= COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
+	};
+	INIT_WORK_ONSTACK(&c_idle.work, do_fork_idle);
+
+	c_idle.idle = get_idle_for_cpu(cpu);
+
+	/* We can't use kernel_thread since we must avoid to
+	 * reschedule the child. We use a workqueue because
+	 * we want to fork from a kernel thread, not whatever
+	 * userspace process happens to be trying to online us.
+	 */
+	if (!c_idle.idle) {
+		schedule_work(&c_idle.work);
+		wait_for_completion(&c_idle.done);
+	} else
+		init_idle(c_idle.idle, cpu);
+	if (IS_ERR(c_idle.idle)) {		
+		pr_err("Failed fork for CPU %u: %li", cpu, PTR_ERR(c_idle.idle));
+		return PTR_ERR(c_idle.idle);
+	}
+	ti = task_thread_info(c_idle.idle);
+
+#ifdef CONFIG_PPC64
+	paca[cpu].__current = c_idle.idle;
+	paca[cpu].kstack = (unsigned long)ti + THREAD_SIZE - STACK_FRAME_OVERHEAD;
+#endif
+	ti->cpu = cpu;
+	current_set[cpu] = ti;
+
+	return 0;
+}
+
 int __cpuinit __cpu_up(unsigned int cpu)
 {
-	int c;
+	int rc, c;
 
 	secondary_ti = current_set[cpu];
 
@@ -365,6 +416,11 @@ int __cpuinit __cpu_up(unsigned int cpu)
 	    (smp_ops->cpu_bootable && !smp_ops->cpu_bootable(cpu)))
 		return -EINVAL;
 
+	/* Make sure we have an idle thread */
+	rc = create_idle(cpu);
+	if (rc)
+		return rc;
+
 	/* Make sure callin-map entry is 0 (can be leftover a CPU
 	 * hotplug
 	 */

commit 105765f451d3ff007bb4ae3761e825686d9615db
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Apr 1 09:23:37 2011 +1100

    powerpc/smp: Don't expose per-cpu "cpu_state" array
    
    Instead, keep it static, expose an accessor and use that from
    the PowerMac code. Avoids easy namespace collisions and will
    make it easier to consolidate with other implementations.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index df3739713edd..d7f8cc18ae05 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -305,7 +305,7 @@ void __devinit smp_prepare_boot_cpu(void)
 
 #ifdef CONFIG_HOTPLUG_CPU
 /* State of each CPU during hotplug phases */
-DEFINE_PER_CPU(int, cpu_state) = { 0 };
+static DEFINE_PER_CPU(int, cpu_state) = { 0 };
 
 int generic_cpu_disable(void)
 {
@@ -348,6 +348,11 @@ void generic_mach_cpu_die(void)
 	while (__get_cpu_var(cpu_state) != CPU_UP_PREPARE)
 		cpu_relax();
 }
+
+void generic_set_cpu_dead(unsigned int cpu)
+{
+	per_cpu(cpu_state, cpu) = CPU_DEAD;
+}
 #endif
 
 int __cpuinit __cpu_up(unsigned int cpu)

commit d72944457bb7d5c4be43aa1b741cb93c69484c20
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Mar 8 13:50:37 2011 +1100

    powerpc/smp: Add a smp_ops->bringup_up() done callback
    
    This allows us to stop abusing smp_ops->setup_cpu() for cleanup
    tasks that have to take place after the initial boot time CPU
    bringup.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index f6cc5c19c6ac..df3739713edd 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -553,7 +553,11 @@ void __init smp_cpus_done(unsigned int max_cpus)
 
 	free_cpumask_var(old_mask);
 
+	if (smp_ops && smp_ops->bringup_done)
+		smp_ops->bringup_done();
+
 	dump_numa_cpu_topology();
+
 }
 
 int arch_sd_sibling_asym_packing(void)

commit 1c91cc570576dfd0f288d664c095d64d11aaace4
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Feb 11 13:05:17 2011 +1100

    powerpc/pmac/smp: Rename fixup_irqs() to migrate_irqs() and use it on ppc32
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index be7d7282341c..f6cc5c19c6ac 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -317,8 +317,8 @@ int generic_cpu_disable(void)
 	set_cpu_online(cpu, false);
 #ifdef CONFIG_PPC64
 	vdso_data->processorCount--;
-	fixup_irqs(cpu_online_mask);
 #endif
+	migrate_irqs();
 	return 0;
 }
 

commit 7a53a4fe707a93a33f6c5d42173bf213cb6ff71d
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Feb 11 12:49:01 2011 +1100

    powerpc/smp: Remove unused smp_ops->cpu_enable()
    
    Remove the last remnants of cpu_enable(), everybody uses the normal
    __cpu_up() path now
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 19d0c2576282..be7d7282341c 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -350,21 +350,11 @@ void generic_mach_cpu_die(void)
 }
 #endif
 
-static int __devinit cpu_enable(unsigned int cpu)
-{
-	if (smp_ops && smp_ops->cpu_enable)
-		return smp_ops->cpu_enable(cpu);
-
-	return -ENOSYS;
-}
-
 int __cpuinit __cpu_up(unsigned int cpu)
 {
 	int c;
 
 	secondary_ti = current_set[cpu];
-	if (!cpu_enable(cpu))
-		return 0;
 
 	if (smp_ops == NULL ||
 	    (smp_ops->cpu_bootable && !smp_ops->cpu_bootable(cpu)))

commit b527d07114fdab83f39040c69b4b0a4b1b232c16
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Feb 11 12:46:41 2011 +1100

    powerpc/smp: Remove unused generic_cpu_enable()
    
    Nobody uses it, besides we should always use the normal __cpu_up
    path anyways
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 3c0fab5e1e16..19d0c2576282 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -322,28 +322,6 @@ int generic_cpu_disable(void)
 	return 0;
 }
 
-int generic_cpu_enable(unsigned int cpu)
-{
-	/* Do the normal bootup if we haven't
-	 * already bootstrapped. */
-	if (system_state != SYSTEM_RUNNING)
-		return -ENOSYS;
-
-	/* get the target out of it's holding state */
-	per_cpu(cpu_state, cpu) = CPU_UP_PREPARE;
-	smp_wmb();
-
-	while (!cpu_online(cpu))
-		cpu_relax();
-
-#ifdef CONFIG_PPC64
-	fixup_irqs(cpu_online_mask);
-	/* counter the irq disable in fixup_irqs */
-	local_irq_enable();
-#endif
-	return 0;
-}
-
 void generic_cpu_die(unsigned int cpu)
 {
 	int i;

commit 4fcb8833af3355065bd8bffcd338eabc6f3a38a0
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Feb 10 18:46:50 2011 +1100

    powerpc/smp: Fix generic_mach_cpu_die()
    
    This is used by some "soft" hotplug implementations. I needs to
    call idle_task_exit() when the CPU is going away, and we remove
    the now no-longer needed set_cpu_online() and local_irq_enable()
    which are handled by the return to start_secondary
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 1c9956c43801..3c0fab5e1e16 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -362,14 +362,13 @@ void generic_mach_cpu_die(void)
 	unsigned int cpu;
 
 	local_irq_disable();
+	idle_task_exit();
 	cpu = smp_processor_id();
 	printk(KERN_DEBUG "CPU%d offline\n", cpu);
 	__get_cpu_var(cpu_state) = CPU_DEAD;
 	smp_wmb();
 	while (__get_cpu_var(cpu_state) != CPU_UP_PREPARE)
 		cpu_relax();
-	set_cpu_online(cpu, true);
-	local_irq_enable();
 }
 #endif
 

commit fa3f82c8bb7acbe049ea71f258b3ae0a33d9d40b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Feb 10 18:45:24 2011 +1100

    powerpc/smp: soft-replugged CPUs must go back to start_secondary
    
    Various thing are torn down when a CPU is hot-unplugged. That CPU
    is expected to go back to start_secondary when re-plugged to re
    initialize everything, such as clock sources, maps, ...
    
    Some implementations just return from cpu_die() callback
    in the idle loop when the CPU is "re-plugged". This is not enough.
    
    We fix it using a little asm trampoline which resets the stack
    and calls back into start_secondary as if we were all fresh from
    boot. The trampoline already existed on ppc64, but we add it for
    ppc32
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 981360509172..1c9956c43801 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -502,7 +502,7 @@ static struct device_node *cpu_to_l2cache(int cpu)
 }
 
 /* Activate a secondary processor. */
-int __devinit start_secondary(void *unused)
+void __devinit start_secondary(void *unused)
 {
 	unsigned int cpu = smp_processor_id();
 	struct device_node *l2_cache;
@@ -558,7 +558,8 @@ int __devinit start_secondary(void *unused)
 	local_irq_enable();
 
 	cpu_idle();
-	return 0;
+
+	BUG();
 }
 
 int setup_profiling_timer(unsigned int multiplier)
@@ -660,5 +661,9 @@ void cpu_die(void)
 {
 	if (ppc_md.cpu_die)
 		ppc_md.cpu_die();
+
+	/* If we return, we re-enter start_secondary */
+	start_secondary_resume();
 }
+
 #endif

commit 99d86705253dcf728dbbec4d694a6764b6edb70c
Author: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
Date:   Wed Oct 6 08:36:59 2010 +0000

    powerpc: Cleanup APIs for cpu/thread/core mappings
    
    These APIs take logical cpu number as input
    Change cpu_first_thread_in_core() to cpu_first_thread_sibling()
    Change cpu_last_thread_in_core() to cpu_last_thread_sibling()
    
    These APIs convert core number (index) to logical cpu/thread numbers
    Add cpu_first_thread_of_core(int core)
    Changed cpu_thread_to_core() to cpu_core_index_of_thread(int cpu)
    
    The goal is to make 'threads_per_core' accessible to the
    pseries_energy module.  Instead of making an API to read
    threads_per_core, this is a higher level wrapper function to
    convert from logical cpu number to core number.
    
    The current APIs cpu_first_thread_in_core() and
    cpu_last_thread_in_core() returns logical CPU number while
    cpu_thread_to_core() returns core number or index which is
    not a logical CPU number.  The new APIs are now clearly named to
    distinguish 'core number' versus first and last 'logical cpu
    number' in that core.
    
    The new APIs cpu_{first,last}_thread_sibling() work on
    logical cpu numbers.  While cpu_first_thread_of_core() and
    cpu_core_index_of_thread() work on core index.
    
    Example usage:  (4 threads per core system)
    
    cpu_first_thread_sibling(5) = 4
    cpu_last_thread_sibling(5) = 7
    cpu_core_index_of_thread(5) = 1
    cpu_first_thread_of_core(1) = 4
    
    cpu_core_index_of_thread() is used in cpu_to_drc_index() in the
    module and cpu_first_thread_of_core() is used in
    drc_index_to_cpu() in the module.
    
    Make API changes to few callers.  Export symbols for use in modules.
    
    Signed-off-by: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 68034bbf2e4f..981360509172 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -466,7 +466,20 @@ int cpu_to_core_id(int cpu)
 	return id;
 }
 
-/* Must be called when no change can occur to cpu_present_mask,
+/* Helper routines for cpu to core mapping */
+int cpu_core_index_of_thread(int cpu)
+{
+	return cpu >> threads_shift;
+}
+EXPORT_SYMBOL_GPL(cpu_core_index_of_thread);
+
+int cpu_first_thread_of_core(int core)
+{
+	return core << threads_shift;
+}
+EXPORT_SYMBOL_GPL(cpu_first_thread_of_core);
+
+/* Must be called when no change can occur to cpu_present_map,
  * i.e. during cpu online or offline.
  */
 static struct device_node *cpu_to_l2cache(int cpu)
@@ -514,7 +527,7 @@ int __devinit start_secondary(void *unused)
 	notify_cpu_starting(cpu);
 	set_cpu_online(cpu, true);
 	/* Update sibling maps */
-	base = cpu_first_thread_in_core(cpu);
+	base = cpu_first_thread_sibling(cpu);
 	for (i = 0; i < threads_per_core; i++) {
 		if (cpu_is_offline(base + i))
 			continue;
@@ -600,7 +613,7 @@ int __cpu_disable(void)
 		return err;
 
 	/* Update sibling maps */
-	base = cpu_first_thread_in_core(cpu);
+	base = cpu_first_thread_sibling(cpu);
 	for (i = 0; i < threads_per_core; i++) {
 		cpumask_clear_cpu(cpu, cpu_sibling_mask(base + i));
 		cpumask_clear_cpu(base + i, cpu_sibling_mask(cpu));

commit cf9efce0ce3136fa076f53e53154e98455229514
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Aug 26 19:56:43 2010 +0000

    powerpc: Account time using timebase rather than PURR
    
    Currently, when CONFIG_VIRT_CPU_ACCOUNTING is enabled, we use the
    PURR register for measuring the user and system time used by
    processes, as well as other related times such as hardirq and
    softirq times.  This turns out to be quite confusing for users
    because it means that a program will often be measured as taking
    less time when run on a multi-threaded processor (SMT2 or SMT4 mode)
    than it does when run on a single-threaded processor (ST mode), even
    though the program takes longer to finish.  The discrepancy is
    accounted for as stolen time, which is also confusing, particularly
    when there are no other partitions running.
    
    This changes the accounting to use the timebase instead, meaning that
    the reported user and system times are the actual number of real-time
    seconds that the program was executing on the processor thread,
    regardless of which SMT mode the processor is in.  Thus a program will
    generally show greater user and system times when run on a
    multi-threaded processor than on a single-threaded processor.
    
    On pSeries systems on POWER5 or later processors, we measure the
    stolen time (time when this partition wasn't running) using the
    hypervisor dispatch trace log.  We check for new entries in the
    log on every entry from user mode and on every transition from
    kernel process context to soft or hard IRQ context (i.e. when
    account_system_vtime() gets called).  So that we can correctly
    distinguish time stolen from user time and time stolen from system
    time, without having to check the log on every exit to user mode,
    we store separate timestamps for exit to user mode and entry from
    user mode.
    
    On systems that have a SPURR (POWER6 and POWER7), we read the SPURR
    in account_system_vtime() (as before), and then apportion the SPURR
    ticks since the last time we read it between scaled user time and
    scaled system time according to the relative proportions of user
    time and system time over the same interval.  This avoids having to
    read the SPURR on every kernel entry and exit.  On systems that have
    PURR but not SPURR (i.e., POWER5), we do the same using the PURR
    rather than the SPURR.
    
    This disables the DTL user interface in /sys/debug/kernel/powerpc/dtl
    for now since it conflicts with the use of the dispatch trace log
    by the time accounting code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 9019f0f1bb5e..68034bbf2e4f 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -508,9 +508,6 @@ int __devinit start_secondary(void *unused)
 	if (smp_ops->take_timebase)
 		smp_ops->take_timebase();
 
-	if (system_state > SYSTEM_BOOTING)
-		snapshot_timebase();
-
 	secondary_cpu_time_init();
 
 	ipi_call_lock();
@@ -575,8 +572,6 @@ void __init smp_cpus_done(unsigned int max_cpus)
 
 	free_cpumask_var(old_mask);
 
-	snapshot_timebases();
-
 	dump_numa_cpu_topology();
 }
 

commit e1f0ece113fe028593b6869fe191a991322c5d85
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Aug 10 20:02:05 2010 +0000

    powerpc: Move arch_sd_sibling_asym_packing() to smp.c
    
    Simple cleanup by moving arch_sd_sibling_asym_packing from process.c to
    smp.c to save an #ifdef CONFIG_SMP
    
    No functionality change.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 0008bc58e826..9019f0f1bb5e 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -580,6 +580,15 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	dump_numa_cpu_topology();
 }
 
+int arch_sd_sibling_asym_packing(void)
+{
+	if (cpu_has_feature(CPU_FTR_ASYM_SMT)) {
+		printk_once(KERN_INFO "Enabling Asymmetric SMT scheduling\n");
+		return SD_ASYM_PACKING;
+	}
+	return 0;
+}
+
 #ifdef CONFIG_HOTPLUG_CPU
 int __cpu_disable(void)
 {

commit 6685a477494ceb063c10300891e48895bb1843c9
Author: Signed-off-by: Darren Hart <dvhltc@us.ibm.com>
Date:   Wed Aug 4 18:28:34 2010 +0000

    powerpc: Silence __cpu_up() under normal operation
    
    During CPU offline/online tests __cpu_up would flood the logs with
    the following message:
    
    Processor 0 found.
    
    This provides no useful information to the user as there is no context
    provided, and since the operation was a success (to this point) it is expected
    that the CPU will come back online, providing all the feedback necessary.
    
    Change the "Processor found" message to DBG() similar to other such messages in
    the same function. Also, add an appropriate log level for the "Processor is
    stuck" message.
    
    Signed-off-by: Darren Hart <dvhltc@us.ibm.com>
    Acked-by: Will Schmidt <will_schmidt@vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Nathan Fontenot <nfont@austin.ibm.com>
    Cc: Robert Jennings <rcj@linux.vnet.ibm.com>
    Cc: Brian King <brking@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index a61b3ddd7bb3..0008bc58e826 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -427,11 +427,11 @@ int __cpuinit __cpu_up(unsigned int cpu)
 #endif
 
 	if (!cpu_callin_map[cpu]) {
-		printk("Processor %u is stuck.\n", cpu);
+		printk(KERN_ERR "Processor %u is stuck.\n", cpu);
 		return -ENOENT;
 	}
 
-	printk("Processor %u found.\n", cpu);
+	DBG("Processor %u found.\n", cpu);
 
 	if (smp_ops->give_timebase)
 		smp_ops->give_timebase();

commit d77cb21b578a5428482bc0fd187f7c0518a0b32a
Author: Tiejun Chen <tiejun.chen@windriver.com>
Date:   Thu Jul 15 20:17:11 2010 +0000

    powerpc/smp: remove the incorrect decrementer initial codes for AP
    
    We already defined start_cpu_decrementer() to invoke decrementer for AP as
    the following path:
    
    start_secondary() -> secondary_cpu_time_init() -> start_cpu_decrementer()
    
    So remove these incorrect codes introduced from commit:
    e7f75ad0 powerpc/47x: Base ppc476 support
    
    And actually we really should not enable decrementer before calling set_dec().
    
    Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8764daad309b..a61b3ddd7bb3 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -499,14 +499,6 @@ int __devinit start_secondary(void *unused)
 	current->active_mm = &init_mm;
 
 	smp_store_cpu_info(cpu);
-
-#if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
-	/* Clear any pending timer interrupts */
-	mtspr(SPRN_TSR, TSR_ENW | TSR_WIS | TSR_DIS | TSR_FIS);
-
-	/* Enable decrementer interrupt */
-	mtspr(SPRN_TCR, TCR_DIE);
-#endif
 	set_dec(tb_ticks_per_jiffy);
 	preempt_disable();
 	cpu_callin_map[cpu] = 1;

commit c1aa687d499a8bce55cb8cf962f0b72c0f933f14
Author: Paul Mackerras <paulus@samba.org>
Date:   Sun Jun 20 19:04:14 2010 +0000

    powerpc: Clean up obsolete code relating to decrementer and timebase
    
    Since the decrementer and timekeeping code was moved over to using
    the generic clockevents and timekeeping infrastructure, several
    variables and functions have been obsolete and effectively unused.
    This deletes them.
    
    In particular, wakeup_decrementer() is no longer needed since the
    generic code reprograms the decrementer as part of the process of
    resuming the timekeeping code, which happens during sysdev resume.
    Thus the wakeup_decrementer calls in the suspend_enter methods for
    52xx platforms have been removed.  The call in the powermac cpu
    frequency change code has been replaced by set_dec(1), which will
    cause a timer interrupt as soon as interrupts are enabled, and the
    generic code will then reprogram the decrementer with the correct
    value.
    
    This also simplifies the generic_suspend_en/disable_irqs functions
    and makes them static since they are not referenced outside time.c.
    The preempt_enable/disable calls are removed because the generic
    code has disabled all but the boot cpu at the point where these
    functions are called, so we can't be moved to another cpu.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 5c196d1086d9..8764daad309b 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -288,8 +288,6 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 			max_cpus = NR_CPUS;
 	else
 		max_cpus = 1;
- 
-	smp_space_timers(max_cpus);
 
 	for_each_possible_cpu(cpu)
 		if (cpu != boot_cpuid)

commit abb17f9c3a92c5acf30e749efdf0419b7f50a5b8
Author: Milton Miller <miltonm@bga.com>
Date:   Wed May 19 02:56:29 2010 +0000

    powerpc: Use common cpu_die (fixes SMP+SUSPEND build)
    
    Configuring a powerpc 32 bit kernel for both SMP and SUSPEND turns on
    CPU_HOTPLUG to enable disable_nonboot_cpus to be called by the common
    suspend code.  Previously the definition of cpu_die for ppc32 was in
    the powermac platform code, causing it to be undefined if that platform
    as not selected.
    
    arch/powerpc/kernel/built-in.o: In function 'cpu_idle':
    arch/powerpc/kernel/idle.c:98: undefined reference to 'cpu_die'
    
    Move the code from setup_64 to smp.c and rename the power mac
    versions to their specific names.
    
    Note that this does not setup the cpu_die pointers in either
    smp_ops (request a given cpu die) or ppc_md (make this cpu die),
    for other platforms but there are generic versions in smp.c.
    
    Reported-by: Matt Sealey <matt@genesi-usa.com>
    Reported-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Anton Vorontsov <avorontsov@mvista.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index bf366167d369..5c196d1086d9 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -648,4 +648,10 @@ void cpu_hotplug_driver_unlock()
 {
 	mutex_unlock(&powerpc_cpu_hotplug_driver_mutex);
 }
+
+void cpu_die(void)
+{
+	if (ppc_md.cpu_die)
+		ppc_md.cpu_die();
+}
 #endif

commit 828a69869ba266cabb486a6b59ea8643d56b33ce
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Apr 26 15:32:44 2010 +0000

    powerpc/cpumask: Update some comments
    
    Since the *_map cpumask variants are deprecated, change the comments to
    instead refer to *_mask.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 39babb1e2ce1..bf366167d369 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -468,7 +468,7 @@ int cpu_to_core_id(int cpu)
 	return id;
 }
 
-/* Must be called when no change can occur to cpu_present_map,
+/* Must be called when no change can occur to cpu_present_mask,
  * i.e. during cpu online or offline.
  */
 static struct device_node *cpu_to_l2cache(int cpu)

commit cc1ba8ea6dde3f049b2b365d8fdc13976aee25cb
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Apr 26 15:32:41 2010 +0000

    powerpc/cpumask: Dynamically allocate cpu_sibling_map and cpu_core_map cpumasks
    
    Dynamically allocate cpu_sibling_map and cpu_core_map cpumasks.
    
    We don't need to set_cpu_online() the boot cpu in smp_prepare_boot_cpu,
    init/main.c does it for us.
    
    We also postpone setting of the boot cpu in cpu_sibling_map and cpu_core_map
    until when the memory allocator is available (smp_prepare_cpus), similar
    to x86.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 62e82c25c583..39babb1e2ce1 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -59,8 +59,8 @@
 
 struct thread_info *secondary_ti;
 
-DEFINE_PER_CPU(cpumask_t, cpu_sibling_map) = CPU_MASK_NONE;
-DEFINE_PER_CPU(cpumask_t, cpu_core_map) = CPU_MASK_NONE;
+DEFINE_PER_CPU(cpumask_var_t, cpu_sibling_map);
+DEFINE_PER_CPU(cpumask_var_t, cpu_core_map);
 
 EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
 EXPORT_PER_CPU_SYMBOL(cpu_core_map);
@@ -271,6 +271,16 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	smp_store_cpu_info(boot_cpuid);
 	cpu_callin_map[boot_cpuid] = 1;
 
+	for_each_possible_cpu(cpu) {
+		zalloc_cpumask_var_node(&per_cpu(cpu_sibling_map, cpu),
+					GFP_KERNEL, cpu_to_node(cpu));
+		zalloc_cpumask_var_node(&per_cpu(cpu_core_map, cpu),
+					GFP_KERNEL, cpu_to_node(cpu));
+	}
+
+	cpumask_set_cpu(boot_cpuid, cpu_sibling_mask(boot_cpuid));
+	cpumask_set_cpu(boot_cpuid, cpu_core_mask(boot_cpuid));
+
 	if (smp_ops)
 		if (smp_ops->probe)
 			max_cpus = smp_ops->probe();
@@ -289,10 +299,6 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 void __devinit smp_prepare_boot_cpu(void)
 {
 	BUG_ON(smp_processor_id() != boot_cpuid);
-
-	set_cpu_online(boot_cpuid, true);
-	cpu_set(boot_cpuid, per_cpu(cpu_sibling_map, boot_cpuid));
-	cpu_set(boot_cpuid, per_cpu(cpu_core_map, boot_cpuid));
 #ifdef CONFIG_PPC64
 	paca[boot_cpuid].__current = current;
 #endif
@@ -525,15 +531,15 @@ int __devinit start_secondary(void *unused)
 	for (i = 0; i < threads_per_core; i++) {
 		if (cpu_is_offline(base + i))
 			continue;
-		cpu_set(cpu, per_cpu(cpu_sibling_map, base + i));
-		cpu_set(base + i, per_cpu(cpu_sibling_map, cpu));
+		cpumask_set_cpu(cpu, cpu_sibling_mask(base + i));
+		cpumask_set_cpu(base + i, cpu_sibling_mask(cpu));
 
 		/* cpu_core_map should be a superset of
 		 * cpu_sibling_map even if we don't have cache
 		 * information, so update the former here, too.
 		 */
-		cpu_set(cpu, per_cpu(cpu_core_map, base +i));
-		cpu_set(base + i, per_cpu(cpu_core_map, cpu));
+		cpumask_set_cpu(cpu, cpu_core_mask(base + i));
+		cpumask_set_cpu(base + i, cpu_core_mask(cpu));
 	}
 	l2_cache = cpu_to_l2cache(cpu);
 	for_each_online_cpu(i) {
@@ -541,8 +547,8 @@ int __devinit start_secondary(void *unused)
 		if (!np)
 			continue;
 		if (np == l2_cache) {
-			cpu_set(cpu, per_cpu(cpu_core_map, i));
-			cpu_set(i, per_cpu(cpu_core_map, cpu));
+			cpumask_set_cpu(cpu, cpu_core_mask(i));
+			cpumask_set_cpu(i, cpu_core_mask(cpu));
 		}
 		of_node_put(np);
 	}
@@ -602,10 +608,10 @@ int __cpu_disable(void)
 	/* Update sibling maps */
 	base = cpu_first_thread_in_core(cpu);
 	for (i = 0; i < threads_per_core; i++) {
-		cpu_clear(cpu, per_cpu(cpu_sibling_map, base + i));
-		cpu_clear(base + i, per_cpu(cpu_sibling_map, cpu));
-		cpu_clear(cpu, per_cpu(cpu_core_map, base +i));
-		cpu_clear(base + i, per_cpu(cpu_core_map, cpu));
+		cpumask_clear_cpu(cpu, cpu_sibling_mask(base + i));
+		cpumask_clear_cpu(base + i, cpu_sibling_mask(cpu));
+		cpumask_clear_cpu(cpu, cpu_core_mask(base + i));
+		cpumask_clear_cpu(base + i, cpu_core_mask(cpu));
 	}
 
 	l2_cache = cpu_to_l2cache(cpu);
@@ -614,8 +620,8 @@ int __cpu_disable(void)
 		if (!np)
 			continue;
 		if (np == l2_cache) {
-			cpu_clear(cpu, per_cpu(cpu_core_map, i));
-			cpu_clear(i, per_cpu(cpu_core_map, cpu));
+			cpumask_clear_cpu(cpu, cpu_core_mask(i));
+			cpumask_clear_cpu(i, cpu_core_mask(cpu));
 		}
 		of_node_put(np);
 	}

commit b6decb707952c678d110699abb5ed86d45ca6927
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Apr 26 15:32:35 2010 +0000

    powerpc/cpumask: Convert fixup_irqs to new cpumask API
    
    Use new cpumask_* functions, and dynamically allocate cpumask in fixup_irqs.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 17523a0abf66..62e82c25c583 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -313,7 +313,7 @@ int generic_cpu_disable(void)
 	set_cpu_online(cpu, false);
 #ifdef CONFIG_PPC64
 	vdso_data->processorCount--;
-	fixup_irqs(cpu_online_map);
+	fixup_irqs(cpu_online_mask);
 #endif
 	return 0;
 }
@@ -333,7 +333,7 @@ int generic_cpu_enable(unsigned int cpu)
 		cpu_relax();
 
 #ifdef CONFIG_PPC64
-	fixup_irqs(cpu_online_map);
+	fixup_irqs(cpu_online_mask);
 	/* counter the irq disable in fixup_irqs */
 	local_irq_enable();
 #endif

commit bfb9126defa80cbed6d91ed9685b238b0d7e81c4
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Apr 26 15:32:34 2010 +0000

    powerpc/cpumask: Convert smp_cpus_done to new cpumask API
    
    Use the new cpumask_* functions and dynamically allocate the cpumask in
    smp_cpus_done.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 3fe4de2b685e..17523a0abf66 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -562,19 +562,22 @@ int setup_profiling_timer(unsigned int multiplier)
 
 void __init smp_cpus_done(unsigned int max_cpus)
 {
-	cpumask_t old_mask;
+	cpumask_var_t old_mask;
 
 	/* We want the setup_cpu() here to be called from CPU 0, but our
 	 * init thread may have been "borrowed" by another CPU in the meantime
 	 * se we pin us down to CPU 0 for a short while
 	 */
-	old_mask = current->cpus_allowed;
+	alloc_cpumask_var(&old_mask, GFP_NOWAIT);
+	cpumask_copy(old_mask, &current->cpus_allowed);
 	set_cpus_allowed_ptr(current, cpumask_of(boot_cpuid));
 	
 	if (smp_ops && smp_ops->setup_cpu)
 		smp_ops->setup_cpu(boot_cpuid);
 
-	set_cpus_allowed_ptr(current, &old_mask);
+	set_cpus_allowed_ptr(current, old_mask);
+
+	free_cpumask_var(old_mask);
 
 	snapshot_timebases();
 

commit e7f75ad01d590243904c2d95ab47e6b2e9ef6dad
Author: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
Date:   Fri Mar 5 10:43:12 2010 +0000

    powerpc/47x: Base ppc476 support
    
    This patch adds the base support for the 476 processor.  The code was
    primarily written by Ben Herrenschmidt and Torez Smith, but I've been
    maintaining it for a while.
    
    The goal is to have a single binary that will run on 44x and 47x, but
    we still have some details to work out.  The biggest is that the L1 cache
    line size differs on the two platforms, but it's currently a compile-time
    option.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Torez Smith  <lnxtorez@linux.vnet.ibm.com>
    Signed-off-by: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e36f94f7411a..3fe4de2b685e 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -495,6 +495,14 @@ int __devinit start_secondary(void *unused)
 	current->active_mm = &init_mm;
 
 	smp_store_cpu_info(cpu);
+
+#if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
+	/* Clear any pending timer interrupts */
+	mtspr(SPRN_TSR, TSR_ENW | TSR_WIS | TSR_DIS | TSR_FIS);
+
+	/* Enable decrementer interrupt */
+	mtspr(SPRN_TCR, TCR_DIE);
+#endif
 	set_dec(tb_ticks_per_jiffy);
 	preempt_disable();
 	cpu_callin_map[cpu] = 1;

commit 21dbeb91a24d867af0e98ba155bfa80d2906344f
Author: Julia Lawall <julia@diku.dk>
Date:   Fri Mar 26 12:03:29 2010 +0000

    powerpc: Use set_cpus_allowed_ptr
    
    Use set_cpus_allowed_ptr rather than set_cpus_allowed.
    
    The semantic patch that makes this change is as follows:
    (http://coccinelle.lip6.fr/)
    
    // <smpl>
    @@
    expression E1,E2;
    @@
    
    - set_cpus_allowed(E1, cpumask_of_cpu(E2))
    + set_cpus_allowed_ptr(E1, cpumask_of(E2))
    
    @@
    expression E;
    identifier I;
    @@
    
    - set_cpus_allowed(E, I)
    + set_cpus_allowed_ptr(E, &I)
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index c2ee14498077..e36f94f7411a 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -561,12 +561,12 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	 * se we pin us down to CPU 0 for a short while
 	 */
 	old_mask = current->cpus_allowed;
-	set_cpus_allowed(current, cpumask_of_cpu(boot_cpuid));
+	set_cpus_allowed_ptr(current, cpumask_of(boot_cpuid));
 	
 	if (smp_ops && smp_ops->setup_cpu)
 		smp_ops->setup_cpu(boot_cpuid);
 
-	set_cpus_allowed(current, old_mask);
+	set_cpus_allowed_ptr(current, &old_mask);
 
 	snapshot_timebases();
 

commit d0174c721900ff807b95ac4ab23191ffcb7511c7
Author: Nathan Fontenot <nfont@austin.ibm.com>
Date:   Thu Jan 14 09:52:44 2010 +0000

    powerpc: Move cpu hotplug driver lock from pseries to powerpc
    
    Move the defintion and lock helper routines for the cpu hotplug driver
    lock from pseries to powerpc code to avoid build breaks for platforms
    other than pseries that use cpu hotplug.
    
    Signed-off-by: Nathan Fontenot <nfont@austin.ibm.com>
    Acked-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index a521fb8a40ee..c2ee14498077 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -619,4 +619,16 @@ void __cpu_die(unsigned int cpu)
 	if (smp_ops->cpu_die)
 		smp_ops->cpu_die(cpu);
 }
+
+static DEFINE_MUTEX(powerpc_cpu_hotplug_driver_mutex);
+
+void cpu_hotplug_driver_lock()
+{
+	mutex_lock(&powerpc_cpu_hotplug_driver_mutex);
+}
+
+void cpu_hotplug_driver_unlock()
+{
+	mutex_unlock(&powerpc_cpu_hotplug_driver_mutex);
+}
 #endif

commit d0316554d3586cbea60592a41391b5def2553d6f
Merge: fb0bbb92d42d 51e99be00ce2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 14 09:58:24 2009 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (34 commits)
      m68k: rename global variable vmalloc_end to m68k_vmalloc_end
      percpu: add missing per_cpu_ptr_to_phys() definition for UP
      percpu: Fix kdump failure if booted with percpu_alloc=page
      percpu: make misc percpu symbols unique
      percpu: make percpu symbols in ia64 unique
      percpu: make percpu symbols in powerpc unique
      percpu: make percpu symbols in x86 unique
      percpu: make percpu symbols in xen unique
      percpu: make percpu symbols in cpufreq unique
      percpu: make percpu symbols in oprofile unique
      percpu: make percpu symbols in tracer unique
      percpu: make percpu symbols under kernel/ and mm/ unique
      percpu: remove some sparse warnings
      percpu: make alloc_percpu() handle array types
      vmalloc: fix use of non-existent percpu variable in put_cpu_var()
      this_cpu: Use this_cpu_xx in trace_functions_graph.c
      this_cpu: Use this_cpu_xx for ftrace
      this_cpu: Use this_cpu_xx in nmi handling
      this_cpu: Use this_cpu operations in RCU
      this_cpu: Use this_cpu ops for VM statistics
      ...
    
    Fix up trivial (famous last words) global per-cpu naming conflicts in
            arch/x86/kvm/svm.c
            mm/slab.c

commit 8389b37dffdc695b4fb363ebe0ed9748bb3b48d0
Author: Valentine Barshak <vbarshak@ru.mvista.com>
Date:   Wed Nov 25 11:48:52 2009 +0000

    powerpc: stop_this_cpu: remove the cpu from the online map.
    
    Remove the CPU from the online map to prevent smp_call_function
    from sending messages to a stopped CPU.
    
    Signed-off-by: Valentine Barshak <vbarshak@ru.mvista.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 9b86a74d2815..97196eefef3e 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -218,6 +218,9 @@ void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
 
 static void stop_this_cpu(void *dummy)
 {
+	/* Remove this CPU */
+	set_cpu_online(smp_processor_id(), false);
+
 	local_irq_disable();
 	while (1)
 		;

commit 6b7487fc6517736a6e32ccc0f8b46109c1b998ec
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 29 22:34:14 2009 +0900

    percpu: make percpu symbols in powerpc unique
    
    This patch updates percpu related symbols in powerpc such that percpu
    symbols are unique and don't clash with local symbols.  This serves
    two purposes of decreasing the possibility of global percpu symbol
    collision and allowing dropping per_cpu__ prefix from percpu symbols.
    
    * arch/powerpc/kernel/perf_callchain.c: s/callchain/cpu_perf_callchain/
    
    * arch/powerpc/kernel/setup-common.c: s/pvr/cpu_pvr/
    
    * arch/powerpc/platforms/pseries/dtl.c: s/dtl/cpu_dtl/
    
    * arch/powerpc/platforms/cell/interrupt.c: s/iic/cpu_iic/
    
    Partly based on Rusty Russell's "alloc_percpu: rename percpu vars
    which cause name clashes" patch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: linuxppc-dev@ozlabs.org

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 9b86a74d2815..2ebb48410976 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -232,7 +232,7 @@ struct thread_info *current_set[NR_CPUS];
 
 static void __devinit smp_store_cpu_info(int id)
 {
-	per_cpu(pvr, id) = mfspr(SPRN_PVR);
+	per_cpu(cpu_pvr, id) = mfspr(SPRN_PVR);
 }
 
 static void __init smp_create_idle(unsigned int cpu)

commit ea0f1cab6ed43121ff6f24c1bb02e88a8d11a2d6
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Sep 24 09:34:48 2009 -0600

    cpumask: Use accessors for cpu_*_mask: powerpc
    
    Use the accessors rather than frobbing bits directly (the new versions
    are const).
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 7f68ceb3bdb8..9b86a74d2815 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -287,7 +287,7 @@ void __devinit smp_prepare_boot_cpu(void)
 {
 	BUG_ON(smp_processor_id() != boot_cpuid);
 
-	cpu_set(boot_cpuid, cpu_online_map);
+	set_cpu_online(boot_cpuid, true);
 	cpu_set(boot_cpuid, per_cpu(cpu_sibling_map, boot_cpuid));
 	cpu_set(boot_cpuid, per_cpu(cpu_core_map, boot_cpuid));
 #ifdef CONFIG_PPC64
@@ -307,7 +307,7 @@ int generic_cpu_disable(void)
 	if (cpu == boot_cpuid)
 		return -EBUSY;
 
-	cpu_clear(cpu, cpu_online_map);
+	set_cpu_online(cpu, false);
 #ifdef CONFIG_PPC64
 	vdso_data->processorCount--;
 	fixup_irqs(cpu_online_map);
@@ -361,7 +361,7 @@ void generic_mach_cpu_die(void)
 	smp_wmb();
 	while (__get_cpu_var(cpu_state) != CPU_UP_PREPARE)
 		cpu_relax();
-	cpu_set(cpu, cpu_online_map);
+	set_cpu_online(cpu, true);
 	local_irq_enable();
 }
 #endif
@@ -508,7 +508,7 @@ int __devinit start_secondary(void *unused)
 
 	ipi_call_lock();
 	notify_cpu_starting(cpu);
-	cpu_set(cpu, cpu_online_map);
+	set_cpu_online(cpu, true);
 	/* Update sibling maps */
 	base = cpu_first_thread_in_core(cpu);
 	for (i = 0; i < threads_per_core; i++) {

commit f063ea02fba5782099b6730d5733ee44638df8f9
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Sep 24 09:34:45 2009 -0600

    cpumask: arch_send_call_function_ipi_mask: powerpc
    
    We're weaning the core code off handing cpumask's around on-stack.
    This introduces arch_send_call_function_ipi_mask(), and by defining
    it, the old arch_send_call_function_ipi is defined by the core code.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index d387b3937ccc..7f68ceb3bdb8 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -189,11 +189,11 @@ void arch_send_call_function_single_ipi(int cpu)
 	smp_ops->message_pass(cpu, PPC_MSG_CALL_FUNC_SINGLE);
 }
 
-void arch_send_call_function_ipi(cpumask_t mask)
+void arch_send_call_function_ipi_mask(const struct cpumask *mask)
 {
 	unsigned int cpu;
 
-	for_each_cpu_mask(cpu, mask)
+	for_each_cpu(cpu, mask)
 		smp_ops->message_pass(cpu, PPC_MSG_CALL_FUNCTION);
 }
 

commit 757cbd46d11cfa7506b7dd5dd6657ae645bf6a17
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Tue Sep 8 17:38:52 2009 +0000

    powerpc/85xx: Fix SMP compile error and allow NULL for smp_ops
    
    The following commit introduced a compile error since it removed
    the implementation of smp_85xx_basic_setup:
    
    commit 77c0a700c1c292edafa11c1e52821ce4636f81b0
    Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Date:   Fri Aug 28 14:25:04 2009 +1000
    
        powerpc: Properly start decrementer on BookE secondary CPUs
    
    Make it so that smp_ops probe() and setup_cpu() can be set to NULL.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 96f107cc0160..d387b3937ccc 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -269,7 +269,10 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	cpu_callin_map[boot_cpuid] = 1;
 
 	if (smp_ops)
-		max_cpus = smp_ops->probe();
+		if (smp_ops->probe)
+			max_cpus = smp_ops->probe();
+		else
+			max_cpus = NR_CPUS;
 	else
 		max_cpus = 1;
  
@@ -493,7 +496,8 @@ int __devinit start_secondary(void *unused)
 	preempt_disable();
 	cpu_callin_map[cpu] = 1;
 
-	smp_ops->setup_cpu(cpu);
+	if (smp_ops->setup_cpu)
+		smp_ops->setup_cpu(cpu);
 	if (smp_ops->take_timebase)
 		smp_ops->take_timebase();
 
@@ -556,7 +560,7 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	old_mask = current->cpus_allowed;
 	set_cpus_allowed(current, cpumask_of_cpu(boot_cpuid));
 	
-	if (smp_ops)
+	if (smp_ops && smp_ops->setup_cpu)
 		smp_ops->setup_cpu(boot_cpuid);
 
 	set_cpus_allowed(current, old_mask);

commit 6776426320e151051a16bc7bf86f12d310c9e8ca
Author: Gautham R Shenoy <ego@in.ibm.com>
Date:   Tue Jun 23 23:26:37 2009 +0000

    powerpc/pseries: Reduce the polling interval in __cpu_up()
    
    Time time taken for a single cpu online operation on a pseries machine
    is as follows:
    Dedicated LPAR (POWER6): ~220ms.
    Shared LPAR (POWER5)   : ~240ms.
    
    Of this time, approximately 200ms is taken up by __cpu_up(). This is because
    we poll every 200ms to check if the new cpu has notified it's presence
    through the cpu_callin_map. We repeat this operation until the new cpu sets
    the value in cpu_callin_map or 5 seconds elapse, whichever comes earlier.
    
    However, using completion_structs instead of polling loops,
    the time taken by the new processor to indicate it's presence has
    found to be less than 1ms on pseries. This method however may not
    work on all powerpc platforms due to the time-base synchronization code.
    
    Keeping this in mind, we could reduce msleep polling interval from
    200ms to 1ms while retaining the 5 second timeout.
    
    With this, the time taken for a cpu online operation changes as follows:
    Dedicated LPAR (POWER6): 20-25ms.
    Shared LPAR (POWER5)   : 60-80ms.
    
    In both these cases, it was found that the code polls through the loop
    only once indicating that 1ms is a reasonable value, atleast on pseries.
    
    The code needs testing on other powerpc platforms.
    
    Signed-off-by: Gautham R Shenoy <ego@in.ibm.com>
    Acked-by: Joel Schopp <jschopp@austin.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 0b47de07302d..96f107cc0160 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -412,9 +412,8 @@ int __cpuinit __cpu_up(unsigned int cpu)
 		 * CPUs can take much longer to come up in the
 		 * hotplug case.  Wait five seconds.
 		 */
-		for (c = 25; c && !cpu_callin_map[cpu]; c--) {
-			msleep(200);
-		}
+		for (c = 5000; c && !cpu_callin_map[cpu]; c--)
+			msleep(1);
 #endif
 
 	if (!cpu_callin_map[cpu]) {

commit 7ccbe504b5ee766d33211a507189a06f3079b29b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jun 18 23:30:07 2009 +0000

    powerpc/pmac: Fix issues with PowerMac "PowerSurge" SMP
    
    The old PowerSurge SMP (ie, dual or quad 604 machines) code has
    numerous issues in modern world.
    
    One is cpu_possible_map is set too late (the device-tree is bogus)
    so we fail to allocate the interrupt stacks and crash. Another
    problem is the fact the timebase is frozen by the bringup of the
    second CPU so the delays in the generic code will hang, we need
    to move some of the calling procedure to inside the powermac code.
    
    This makes it boot again for me
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 65484b2200b3..0b47de07302d 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -68,7 +68,8 @@ EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 /* SMP operations for this machine */
 struct smp_ops_t *smp_ops;
 
-static volatile unsigned int cpu_callin_map[NR_CPUS];
+/* Can't be static due to PowerMac hackery */
+volatile unsigned int cpu_callin_map[NR_CPUS];
 
 int smt_enabled_at_boot = 1;
 

commit b840d79631c882786925303c2b0f4fefc31845ed
Merge: 597b0d21626d c3d80000e3a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 2 11:44:09 2009 -0800

    Merge branch 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (66 commits)
      x86: export vector_used_by_percpu_irq
      x86: use logical apicid in x2apic_cluster's x2apic_cpu_mask_to_apicid_and()
      sched: nominate preferred wakeup cpu, fix
      x86: fix lguest used_vectors breakage, -v2
      x86: fix warning in arch/x86/kernel/io_apic.c
      sched: fix warning in kernel/sched.c
      sched: move test_sd_parent() to an SMP section of sched.h
      sched: add SD_BALANCE_NEWIDLE at MC and CPU level for sched_mc>0
      sched: activate active load balancing in new idle cpus
      sched: bias task wakeups to preferred semi-idle packages
      sched: nominate preferred wakeup cpu
      sched: favour lower logical cpu number for sched_mc balance
      sched: framework for sched_mc/smt_power_savings=N
      sched: convert BALANCE_FOR_xx_POWER to inline functions
      x86: use possible_cpus=NUM to extend the possible cpus allowed
      x86: fix cpu_mask_to_apicid_and to include cpu_online_mask
      x86: update io_apic.c to the new cpumask code
      x86: Introduce topology_core_cpumask()/topology_thread_cpumask()
      x86: xen: use smp_call_function_many()
      x86: use work_on_cpu in x86/kernel/cpu/mcheck/mce_amd_64.c
      ...
    
    Fixed up trivial conflict in kernel/time/tick-sched.c manually

commit b2ea25b958968c152c6fac0594f2c9aa8b59eb8d
Author: Nathan Lynch <ntl@pobox.com>
Date:   Wed Dec 10 20:16:07 2008 +0000

    powerpc: Convert cpu_to_l2cache() to of_find_next_cache_node()
    
    The smp code uses cache information to populate cpu_core_map; change
    it to use common code for cache lookup.
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index ffcb1779a220..8ac3f721d235 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -466,8 +466,7 @@ int cpu_to_core_id(int cpu)
 static struct device_node *cpu_to_l2cache(int cpu)
 {
 	struct device_node *np;
-	const phandle *php;
-	phandle ph;
+	struct device_node *cache;
 
 	if (!cpu_present(cpu))
 		return NULL;
@@ -476,13 +475,11 @@ static struct device_node *cpu_to_l2cache(int cpu)
 	if (np == NULL)
 		return NULL;
 
-	php = of_get_property(np, "l2-cache", NULL);
-	if (php == NULL)
-		return NULL;
-	ph = *php;
+	cache = of_find_next_cache_node(np);
+
 	of_node_put(np);
 
-	return of_find_node_by_phandle(ph);
+	return cache;
 }
 
 /* Activate a secondary processor. */

commit 13a9801eb669d567ab2c8f8db5e50557fef5f636
Author: Nathan Lynch <ntl@pobox.com>
Date:   Wed Dec 10 14:28:41 2008 +0000

    powerpc: Move smp_hw_index to 32-bit code
    
    smp_hw_index isn't used on 64-bit, so move it from smp.c to
    setup_32.c.
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index a59d8d72bb97..ffcb1779a220 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -57,7 +57,6 @@
 #define DBG(fmt...)
 #endif
 
-int smp_hw_index[NR_CPUS];
 struct thread_info *secondary_ti;
 
 cpumask_t cpu_possible_map = CPU_MASK_NONE;

commit 98a79d6a50181ca1ecf7400eda01d5dc1bc0dbf0
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sat Dec 13 21:19:41 2008 +1030

    cpumask: centralize cpu_online_map and cpu_possible_map
    
    Impact: cleanup
    
    Each SMP arch defines these themselves.  Move them to a central
    location.
    
    Twists:
    1) Some archs (m32, parisc, s390) set possible_map to all 1, so we add a
       CONFIG_INIT_ALL_POSSIBLE for this rather than break them.
    
    2) mips and sparc32 '#define cpu_possible_map phys_cpu_present_map'.
       Those archs simply have phys_cpu_present_map replaced everywhere.
    
    3) Alpha defined cpu_possible_map to cpu_present_map; this is tricky
       so I just manipulate them both in sync.
    
    4) IA64, cris and m32r have gratuitous 'extern cpumask_t cpu_possible_map'
       declarations.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Grant Grundler <grundler@parisc-linux.org>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Mike Travis <travis@sgi.com>
    Cc: ink@jurassic.park.msu.ru
    Cc: rmk@arm.linux.org.uk
    Cc: starvik@axis.com
    Cc: tony.luck@intel.com
    Cc: takata@linux-m32r.org
    Cc: ralf@linux-mips.org
    Cc: grundler@parisc-linux.org
    Cc: paulus@samba.org
    Cc: schwidefsky@de.ibm.com
    Cc: lethal@linux-sh.org
    Cc: wli@holomorphy.com
    Cc: davem@davemloft.net
    Cc: jdike@addtoit.com
    Cc: mingo@redhat.com

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index ff9f7010097d..d1165566f064 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -60,13 +60,9 @@
 int smp_hw_index[NR_CPUS];
 struct thread_info *secondary_ti;
 
-cpumask_t cpu_possible_map = CPU_MASK_NONE;
-cpumask_t cpu_online_map = CPU_MASK_NONE;
 DEFINE_PER_CPU(cpumask_t, cpu_sibling_map) = CPU_MASK_NONE;
 DEFINE_PER_CPU(cpumask_t, cpu_core_map) = CPU_MASK_NONE;
 
-EXPORT_SYMBOL(cpu_online_map);
-EXPORT_SYMBOL(cpu_possible_map);
 EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
 EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 

commit 25ddd738c2ebffb6c2d3cf29c91b986d1bb39c99
Author: Milton Miller <miltonm@bga.com>
Date:   Fri Nov 14 20:11:49 2008 +0000

    powerpc: Provide a separate handler for each IPI action
    
    With the new generic smp call function helpers, I noticed the code in
    smp_message_recv was a single function call in many cases.  While
    getting the message number from the ipi data is easy, we can reduce
    the path length by a function and data-dependent switch by registering
    seperate IPI actions for these simple calls.
    
    Originally I left the ipi action array exposed, but then I realized the
    registration code should be common too.
    
    The three users each had their own name array, so I made a fourth
    to convert all users to use a common one.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index ff9f7010097d..a59d8d72bb97 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -123,6 +123,65 @@ void smp_message_recv(int msg)
 	}
 }
 
+static irqreturn_t call_function_action(int irq, void *data)
+{
+	generic_smp_call_function_interrupt();
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t reschedule_action(int irq, void *data)
+{
+	/* we just need the return path side effect of checking need_resched */
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t call_function_single_action(int irq, void *data)
+{
+	generic_smp_call_function_single_interrupt();
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t debug_ipi_action(int irq, void *data)
+{
+	smp_message_recv(PPC_MSG_DEBUGGER_BREAK);
+	return IRQ_HANDLED;
+}
+
+static irq_handler_t smp_ipi_action[] = {
+	[PPC_MSG_CALL_FUNCTION] =  call_function_action,
+	[PPC_MSG_RESCHEDULE] = reschedule_action,
+	[PPC_MSG_CALL_FUNC_SINGLE] = call_function_single_action,
+	[PPC_MSG_DEBUGGER_BREAK] = debug_ipi_action,
+};
+
+const char *smp_ipi_name[] = {
+	[PPC_MSG_CALL_FUNCTION] =  "ipi call function",
+	[PPC_MSG_RESCHEDULE] = "ipi reschedule",
+	[PPC_MSG_CALL_FUNC_SINGLE] = "ipi call function single",
+	[PPC_MSG_DEBUGGER_BREAK] = "ipi debugger",
+};
+
+/* optional function to request ipi, for controllers with >= 4 ipis */
+int smp_request_message_ipi(int virq, int msg)
+{
+	int err;
+
+	if (msg < 0 || msg > PPC_MSG_DEBUGGER_BREAK) {
+		return -EINVAL;
+	}
+#if !defined(CONFIG_DEBUGGER) && !defined(CONFIG_KEXEC)
+	if (msg == PPC_MSG_DEBUGGER_BREAK) {
+		return 1;
+	}
+#endif
+	err = request_irq(virq, smp_ipi_action[msg], IRQF_DISABLED|IRQF_PERCPU,
+			  smp_ipi_name[msg], 0);
+	WARN(err < 0, "unable to request_irq %d for %s (rc %d)\n",
+		virq, smp_ipi_name[msg], err);
+
+	return err;
+}
+
 void smp_send_reschedule(int cpu)
 {
 	if (likely(smp_ops))

commit 6dc6472581f693b5fc95aebedf67b4960fb85cf0
Merge: ee673eaa72d8 8acd3a60bcca
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Oct 15 11:31:54 2008 +1100

    Merge commit 'origin'
    
    Manual fixup of conflicts on:
    
            arch/powerpc/include/asm/dcr-regs.h
            drivers/net/ibm_newemac/core.h

commit 22d660ffd0db8d136b122751287d186e869ca474
Author: Milton Miller <miltonm@bga.com>
Date:   Fri Oct 10 01:56:45 2008 +0000

    powerpc/smp: No need to set_need_resched when getting a resched IPI
    
    The comment in the code was asking "Do we have to do this?", and according
    to x86 and s390 the answer is no, the scheduler will do it before calling
    the arch hook.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 5337ca7bb649..3ee736fa8b1d 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -101,8 +101,7 @@ void smp_message_recv(int msg)
 		generic_smp_call_function_interrupt();
 		break;
 	case PPC_MSG_RESCHEDULE:
-		/* XXX Do we have to do this? */
-		set_need_resched();
+		/* we notice need_resched on exit */
 		break;
 	case PPC_MSG_CALL_FUNC_SINGLE:
 		generic_smp_call_function_single_interrupt();

commit e545a6140b698b2494daf0b32107bdcc5e901390
Author: Manfred Spraul <manfred@colorfullife.com>
Date:   Sun Sep 7 16:57:22 2008 +0200

    kernel/cpu.c: create a CPU_STARTING cpu_chain notifier
    
    Right now, there is no notifier that is called on a new cpu, before the new
    cpu begins processing interrupts/softirqs.
    Various kernel function would need that notification, e.g. kvm works around
    by calling smp_call_function_single(), rcu polls cpu_online_map.
    
    The patch adds a CPU_STARTING notification. It also adds a helper function
    that sends the message to all cpu_chain handlers.
    
    Tested on x86-64.
    All other archs are untested. Especially on sparc, I'm not sure if I got
    it right.
    
    Signed-off-by: Manfred Spraul <manfred@colorfullife.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 5337ca7bb649..c27b10a1bd79 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -453,6 +453,7 @@ int __devinit start_secondary(void *unused)
 	secondary_cpu_time_init();
 
 	ipi_call_lock();
+	notify_cpu_starting(cpu);
 	cpu_set(cpu, cpu_online_map);
 	/* Update sibling maps */
 	base = cpu_first_thread_in_core(cpu);

commit e9efed3b80a83e44b98fc626f3268ae072550b84
Author: Nathan Lynch <ntl@pobox.com>
Date:   Sun Jul 27 15:24:54 2008 +1000

    powerpc: Make core id information available to userspace
    
    Existing Open Firmware practice is to report each processor core as a
    separate node in the device tree.  Report the value of the "reg" OF
    property corresponding to a logical CPU's device node as the core_id
    attribute in /sys/devices/system/cpu/cpu*/topology/core_id.
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index f7a2f81b5b7d..5337ca7bb649 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -380,6 +380,29 @@ int __cpuinit __cpu_up(unsigned int cpu)
 	return 0;
 }
 
+/* Return the value of the reg property corresponding to the given
+ * logical cpu.
+ */
+int cpu_to_core_id(int cpu)
+{
+	struct device_node *np;
+	const int *reg;
+	int id = -1;
+
+	np = of_get_cpu_node(cpu, NULL);
+	if (!np)
+		goto out;
+
+	reg = of_get_property(np, "reg", NULL);
+	if (!reg)
+		goto out;
+
+	id = *reg;
+out:
+	of_node_put(np);
+	return id;
+}
+
 /* Must be called when no change can occur to cpu_present_map,
  * i.e. during cpu online or offline.
  */

commit 440a0857e32a05979fb01fc59ea454a723e80e4b
Author: Nathan Lynch <ntl@pobox.com>
Date:   Sun Jul 27 15:24:53 2008 +1000

    powerpc: Make core sibling information available to userspace
    
    Implement the notion of "core siblings" for powerpc.  This makes
    /sys/devices/system/cpu/cpu*/topology/core_siblings present sensible
    values, indicating online CPUs which share an L2 cache.
    
    BenH: Made cpu_to_l2cache() use of_find_node_by_phandle() instead
    of IBM-specific open coded search
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 3c4d07e5e06a..f7a2f81b5b7d 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -63,10 +63,12 @@ struct thread_info *secondary_ti;
 cpumask_t cpu_possible_map = CPU_MASK_NONE;
 cpumask_t cpu_online_map = CPU_MASK_NONE;
 DEFINE_PER_CPU(cpumask_t, cpu_sibling_map) = CPU_MASK_NONE;
+DEFINE_PER_CPU(cpumask_t, cpu_core_map) = CPU_MASK_NONE;
 
 EXPORT_SYMBOL(cpu_online_map);
 EXPORT_SYMBOL(cpu_possible_map);
 EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
+EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 
 /* SMP operations for this machine */
 struct smp_ops_t *smp_ops;
@@ -230,6 +232,7 @@ void __devinit smp_prepare_boot_cpu(void)
 
 	cpu_set(boot_cpuid, cpu_online_map);
 	cpu_set(boot_cpuid, per_cpu(cpu_sibling_map, boot_cpuid));
+	cpu_set(boot_cpuid, per_cpu(cpu_core_map, boot_cpuid));
 #ifdef CONFIG_PPC64
 	paca[boot_cpuid].__current = current;
 #endif
@@ -377,11 +380,36 @@ int __cpuinit __cpu_up(unsigned int cpu)
 	return 0;
 }
 
+/* Must be called when no change can occur to cpu_present_map,
+ * i.e. during cpu online or offline.
+ */
+static struct device_node *cpu_to_l2cache(int cpu)
+{
+	struct device_node *np;
+	const phandle *php;
+	phandle ph;
+
+	if (!cpu_present(cpu))
+		return NULL;
+
+	np = of_get_cpu_node(cpu, NULL);
+	if (np == NULL)
+		return NULL;
+
+	php = of_get_property(np, "l2-cache", NULL);
+	if (php == NULL)
+		return NULL;
+	ph = *php;
+	of_node_put(np);
+
+	return of_find_node_by_phandle(ph);
+}
 
 /* Activate a secondary processor. */
 int __devinit start_secondary(void *unused)
 {
 	unsigned int cpu = smp_processor_id();
+	struct device_node *l2_cache;
 	int i, base;
 
 	atomic_inc(&init_mm.mm_count);
@@ -410,7 +438,26 @@ int __devinit start_secondary(void *unused)
 			continue;
 		cpu_set(cpu, per_cpu(cpu_sibling_map, base + i));
 		cpu_set(base + i, per_cpu(cpu_sibling_map, cpu));
+
+		/* cpu_core_map should be a superset of
+		 * cpu_sibling_map even if we don't have cache
+		 * information, so update the former here, too.
+		 */
+		cpu_set(cpu, per_cpu(cpu_core_map, base +i));
+		cpu_set(base + i, per_cpu(cpu_core_map, cpu));
 	}
+	l2_cache = cpu_to_l2cache(cpu);
+	for_each_online_cpu(i) {
+		struct device_node *np = cpu_to_l2cache(i);
+		if (!np)
+			continue;
+		if (np == l2_cache) {
+			cpu_set(cpu, per_cpu(cpu_core_map, i));
+			cpu_set(i, per_cpu(cpu_core_map, cpu));
+		}
+		of_node_put(np);
+	}
+	of_node_put(l2_cache);
 	ipi_call_unlock();
 
 	local_irq_enable();
@@ -448,6 +495,7 @@ void __init smp_cpus_done(unsigned int max_cpus)
 #ifdef CONFIG_HOTPLUG_CPU
 int __cpu_disable(void)
 {
+	struct device_node *l2_cache;
 	int cpu = smp_processor_id();
 	int base, i;
 	int err;
@@ -464,7 +512,23 @@ int __cpu_disable(void)
 	for (i = 0; i < threads_per_core; i++) {
 		cpu_clear(cpu, per_cpu(cpu_sibling_map, base + i));
 		cpu_clear(base + i, per_cpu(cpu_sibling_map, cpu));
+		cpu_clear(cpu, per_cpu(cpu_core_map, base +i));
+		cpu_clear(base + i, per_cpu(cpu_core_map, cpu));
+	}
+
+	l2_cache = cpu_to_l2cache(cpu);
+	for_each_present_cpu(i) {
+		struct device_node *np = cpu_to_l2cache(i);
+		if (!np)
+			continue;
+		if (np == l2_cache) {
+			cpu_clear(cpu, per_cpu(cpu_core_map, i));
+			cpu_clear(i, per_cpu(cpu_core_map, cpu));
+		}
+		of_node_put(np);
 	}
+	of_node_put(l2_cache);
+
 
 	return 0;
 }

commit e2075f79a99b45a6cc10de021c93f07212098a84
Author: Nathan Lynch <ntl@pobox.com>
Date:   Sun Jul 27 15:24:52 2008 +1000

    powerpc: Update cpu_sibling_maps dynamically
    
    Rather doing one initialization pass over all the per-cpu
    cpu_sibling_maps at boot, update the maps at cpu online/offline time.
    
    This is a behavior change -- the thread_siblings attribute now
    reflects only online siblings, whereas it would display offline
    siblings before.  The new behavior matches that of x86, and is
    arguably more useful.
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index f5ae9fa222ea..3c4d07e5e06a 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -41,6 +41,7 @@
 #include <asm/smp.h>
 #include <asm/time.h>
 #include <asm/machdep.h>
+#include <asm/cputhreads.h>
 #include <asm/cputable.h>
 #include <asm/system.h>
 #include <asm/mpic.h>
@@ -228,6 +229,7 @@ void __devinit smp_prepare_boot_cpu(void)
 	BUG_ON(smp_processor_id() != boot_cpuid);
 
 	cpu_set(boot_cpuid, cpu_online_map);
+	cpu_set(boot_cpuid, per_cpu(cpu_sibling_map, boot_cpuid));
 #ifdef CONFIG_PPC64
 	paca[boot_cpuid].__current = current;
 #endif
@@ -380,6 +382,7 @@ int __cpuinit __cpu_up(unsigned int cpu)
 int __devinit start_secondary(void *unused)
 {
 	unsigned int cpu = smp_processor_id();
+	int i, base;
 
 	atomic_inc(&init_mm.mm_count);
 	current->active_mm = &init_mm;
@@ -400,6 +403,14 @@ int __devinit start_secondary(void *unused)
 
 	ipi_call_lock();
 	cpu_set(cpu, cpu_online_map);
+	/* Update sibling maps */
+	base = cpu_first_thread_in_core(cpu);
+	for (i = 0; i < threads_per_core; i++) {
+		if (cpu_is_offline(base + i))
+			continue;
+		cpu_set(cpu, per_cpu(cpu_sibling_map, base + i));
+		cpu_set(base + i, per_cpu(cpu_sibling_map, cpu));
+	}
 	ipi_call_unlock();
 
 	local_irq_enable();
@@ -437,10 +448,25 @@ void __init smp_cpus_done(unsigned int max_cpus)
 #ifdef CONFIG_HOTPLUG_CPU
 int __cpu_disable(void)
 {
-	if (smp_ops->cpu_disable)
-		return smp_ops->cpu_disable();
+	int cpu = smp_processor_id();
+	int base, i;
+	int err;
 
-	return -ENOSYS;
+	if (!smp_ops->cpu_disable)
+		return -ENOSYS;
+
+	err = smp_ops->cpu_disable();
+	if (err)
+		return err;
+
+	/* Update sibling maps */
+	base = cpu_first_thread_in_core(cpu);
+	for (i = 0; i < threads_per_core; i++) {
+		cpu_clear(cpu, per_cpu(cpu_sibling_map, base + i));
+		cpu_clear(base + i, per_cpu(cpu_sibling_map, cpu));
+	}
+
+	return 0;
 }
 
 void __cpu_die(unsigned int cpu)

commit 84c3d4aaec3338201b449034beac41635866bddf
Merge: 43d2548bb2ef fafa3a3f1672
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Jul 16 11:07:59 2008 +1000

    Merge commit 'origin/master'
    
    Manual merge of:
    
            arch/powerpc/Kconfig
            arch/powerpc/kernel/stacktrace.c
            arch/powerpc/mm/slice.c
            arch/ppc/kernel/smp.c

commit 8691e5a8f691cc2a4fda0651e8d307aaba0e7d68
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Jun 6 11:18:06 2008 +0200

    smp_call_function: get rid of the unused nonatomic/retry argument
    
    It's never used and the comments refer to nonatomic and retry
    interchangably. So get rid of it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 37a5ab410dcc..5191b46a611e 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -168,7 +168,7 @@ static void stop_this_cpu(void *dummy)
 
 void smp_send_stop(void)
 {
-	smp_call_function(stop_this_cpu, NULL, 0, 0);
+	smp_call_function(stop_this_cpu, NULL, 0);
 }
 
 extern struct gettimeofday_struct do_gtod;

commit b7d7a2404f80386307ccc0cde63d8d2a5e3bc85c
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jun 26 11:22:13 2008 +0200

    powerpc: convert to generic helpers for IPI function calls
    
    This converts ppc to use the new helpers for smp_call_function() and
    friends, and adds support for smp_call_function_single().
    
    ppc loses the timeout functionality of smp_call_function_mask() with
    this change, as the generic code does not provide that.
    
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 1457aa0a08f1..37a5ab410dcc 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -72,12 +72,8 @@ struct smp_ops_t *smp_ops;
 
 static volatile unsigned int cpu_callin_map[NR_CPUS];
 
-void smp_call_function_interrupt(void);
-
 int smt_enabled_at_boot = 1;
 
-static int ipi_fail_ok;
-
 static void (*crash_ipi_function_ptr)(struct pt_regs *) = NULL;
 
 #ifdef CONFIG_PPC64
@@ -99,12 +95,15 @@ void smp_message_recv(int msg)
 {
 	switch(msg) {
 	case PPC_MSG_CALL_FUNCTION:
-		smp_call_function_interrupt();
+		generic_smp_call_function_interrupt();
 		break;
 	case PPC_MSG_RESCHEDULE:
 		/* XXX Do we have to do this? */
 		set_need_resched();
 		break;
+	case PPC_MSG_CALL_FUNC_SINGLE:
+		generic_smp_call_function_single_interrupt();
+		break;
 	case PPC_MSG_DEBUGGER_BREAK:
 		if (crash_ipi_function_ptr) {
 			crash_ipi_function_ptr(get_irq_regs());
@@ -128,6 +127,19 @@ void smp_send_reschedule(int cpu)
 		smp_ops->message_pass(cpu, PPC_MSG_RESCHEDULE);
 }
 
+void arch_send_call_function_single_ipi(int cpu)
+{
+	smp_ops->message_pass(cpu, PPC_MSG_CALL_FUNC_SINGLE);
+}
+
+void arch_send_call_function_ipi(cpumask_t mask)
+{
+	unsigned int cpu;
+
+	for_each_cpu_mask(cpu, mask)
+		smp_ops->message_pass(cpu, PPC_MSG_CALL_FUNCTION);
+}
+
 #ifdef CONFIG_DEBUGGER
 void smp_send_debugger_break(int cpu)
 {
@@ -154,215 +166,9 @@ static void stop_this_cpu(void *dummy)
 		;
 }
 
-/*
- * Structure and data for smp_call_function(). This is designed to minimise
- * static memory requirements. It also looks cleaner.
- * Stolen from the i386 version.
- */
-static  __cacheline_aligned_in_smp DEFINE_SPINLOCK(call_lock);
-
-static struct call_data_struct {
-	void (*func) (void *info);
-	void *info;
-	atomic_t started;
-	atomic_t finished;
-	int wait;
-} *call_data;
-
-/* delay of at least 8 seconds */
-#define SMP_CALL_TIMEOUT	8
-
-/*
- * These functions send a 'generic call function' IPI to other online
- * CPUS in the system.
- *
- * [SUMMARY] Run a function on other CPUs.
- * <func> The function to run. This must be fast and non-blocking.
- * <info> An arbitrary pointer to pass to the function.
- * <nonatomic> currently unused.
- * <wait> If true, wait (atomically) until function has completed on other CPUs.
- * [RETURNS] 0 on success, else a negative status code. Does not return until
- * remote CPUs are nearly ready to execute <<func>> or are or have executed.
- * <map> is a cpu map of the cpus to send IPI to.
- *
- * You must not call this function with disabled interrupts or from a
- * hardware interrupt handler or from a bottom half handler.
- */
-static int __smp_call_function_map(void (*func) (void *info), void *info,
-				   int nonatomic, int wait, cpumask_t map)
-{
-	struct call_data_struct data;
-	int ret = -1, num_cpus;
-	int cpu;
-	u64 timeout;
-
-	if (unlikely(smp_ops == NULL))
-		return ret;
-
-	data.func = func;
-	data.info = info;
-	atomic_set(&data.started, 0);
-	data.wait = wait;
-	if (wait)
-		atomic_set(&data.finished, 0);
-
-	/* remove 'self' from the map */
-	if (cpu_isset(smp_processor_id(), map))
-		cpu_clear(smp_processor_id(), map);
-
-	/* sanity check the map, remove any non-online processors. */
-	cpus_and(map, map, cpu_online_map);
-
-	num_cpus = cpus_weight(map);
-	if (!num_cpus)
-		goto done;
-
-	call_data = &data;
-	smp_wmb();
-	/* Send a message to all CPUs in the map */
-	for_each_cpu_mask(cpu, map)
-		smp_ops->message_pass(cpu, PPC_MSG_CALL_FUNCTION);
-
-	timeout = get_tb() + (u64) SMP_CALL_TIMEOUT * tb_ticks_per_sec;
-
-	/* Wait for indication that they have received the message */
-	while (atomic_read(&data.started) != num_cpus) {
-		HMT_low();
-		if (get_tb() >= timeout) {
-			printk("smp_call_function on cpu %d: other cpus not "
-				"responding (%d)\n", smp_processor_id(),
-				atomic_read(&data.started));
-			if (!ipi_fail_ok)
-				debugger(NULL);
-			goto out;
-		}
-	}
-
-	/* optionally wait for the CPUs to complete */
-	if (wait) {
-		while (atomic_read(&data.finished) != num_cpus) {
-			HMT_low();
-			if (get_tb() >= timeout) {
-				printk("smp_call_function on cpu %d: other "
-					"cpus not finishing (%d/%d)\n",
-					smp_processor_id(),
-					atomic_read(&data.finished),
-					atomic_read(&data.started));
-				debugger(NULL);
-				goto out;
-			}
-		}
-	}
-
- done:
-	ret = 0;
-
- out:
-	call_data = NULL;
-	HMT_medium();
-	return ret;
-}
-
-static int __smp_call_function(void (*func)(void *info), void *info,
-			       int nonatomic, int wait)
-{
-	int ret;
-	spin_lock(&call_lock);
-	ret =__smp_call_function_map(func, info, nonatomic, wait,
-				       cpu_online_map);
-	spin_unlock(&call_lock);
-	return ret;
-}
-
-int smp_call_function(void (*func) (void *info), void *info, int nonatomic,
-			int wait)
-{
-	/* Can deadlock when called with interrupts disabled */
-	WARN_ON(irqs_disabled());
-
-	return __smp_call_function(func, info, nonatomic, wait);
-}
-EXPORT_SYMBOL(smp_call_function);
-
-int smp_call_function_single(int cpu, void (*func) (void *info), void *info,
-			     int nonatomic, int wait)
-{
-	cpumask_t map = CPU_MASK_NONE;
-	int ret = 0;
-
-	/* Can deadlock when called with interrupts disabled */
-	WARN_ON(irqs_disabled());
-
-	if (!cpu_online(cpu))
-		return -EINVAL;
-
-	cpu_set(cpu, map);
-	if (cpu != get_cpu()) {
-		spin_lock(&call_lock);
-		ret = __smp_call_function_map(func, info, nonatomic, wait, map);
-		spin_unlock(&call_lock);
-	} else {
-		local_irq_disable();
-		func(info);
-		local_irq_enable();
-	}
-	put_cpu();
-	return ret;
-}
-EXPORT_SYMBOL(smp_call_function_single);
-
 void smp_send_stop(void)
 {
-	int nolock;
-
-	/* It's OK to fail sending the IPI, since the alternative is to
-	 * be stuck forever waiting on the other CPU to take the interrupt.
-	 *
-	 * It's better to at least continue and go through reboot, since this
-	 * function is usually called at panic or reboot time in the first
-	 * place.
-	 */
-	ipi_fail_ok = 1;
-
-	/* Don't deadlock in case we got called through panic */
-	nolock = !spin_trylock(&call_lock);
-	__smp_call_function_map(stop_this_cpu, NULL, 1, 0, cpu_online_map);
-	if (!nolock)
-		spin_unlock(&call_lock);
-}
-
-void smp_call_function_interrupt(void)
-{
-	void (*func) (void *info);
-	void *info;
-	int wait;
-
-	/* call_data will be NULL if the sender timed out while
-	 * waiting on us to receive the call.
-	 */
-	if (!call_data)
-		return;
-
-	func = call_data->func;
-	info = call_data->info;
-	wait = call_data->wait;
-
-	if (!wait)
-		smp_mb__before_atomic_inc();
-
-	/*
-	 * Notify initiating CPU that I've grabbed the data and am
-	 * about to execute the function
-	 */
-	atomic_inc(&call_data->started);
-	/*
-	 * At this point the info structure may be out of scope unless wait==1
-	 */
-	(*func)(info);
-	if (wait) {
-		smp_mb__before_atomic_inc();
-		atomic_inc(&call_data->finished);
-	}
+	smp_call_function(stop_this_cpu, NULL, 0, 0);
 }
 
 extern struct gettimeofday_struct do_gtod;
@@ -596,9 +402,9 @@ int __devinit start_secondary(void *unused)
 
 	secondary_cpu_time_init();
 
-	spin_lock(&call_lock);
+	ipi_call_lock();
 	cpu_set(cpu, cpu_online_map);
-	spin_unlock(&call_lock);
+	ipi_call_unlock();
 
 	local_irq_enable();
 

commit 1c21a2937b1f342a8f5d580203c3396557d53b6e
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu May 8 14:27:19 2008 +1000

    [POWERPC] Fix sparse warnings in arch/powerpc/kernel
    
    Make a few things static in lparcfg.c
    Make init and exit routines static in rtas_flash.c
    Make things static in rtas_pci.c
    Make some functions static in rtas.c
    Make fops static in rtas-proc.c
    Remove unneeded extern for do_gtod in smp.c
    Make clocksource_init() static in time.c
    Make last_tick_len and ticklen_to_xs static in time.c
    Move the declaration of the pvr per-cpu into smp.h
    Make kexec_smp_down() and kexec_stack static in machine_kexec_64.c
    Don't return void in arch_teardown_msi_irqs() in msi.c
    Move declaration of GregorianDay()into asm/time.h
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 1457aa0a08f1..ba7989ffaeee 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -365,12 +365,8 @@ void smp_call_function_interrupt(void)
 	}
 }
 
-extern struct gettimeofday_struct do_gtod;
-
 struct thread_info *current_set[NR_CPUS];
 
-DECLARE_PER_CPU(unsigned int, pvr);
-
 static void __devinit smp_store_cpu_info(int id)
 {
 	per_cpu(pvr, id) = mfspr(SPRN_PVR);

commit 3b5750644b2ffa2a76fdfe7b4e00e4af2ecf3539
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri May 2 14:29:12 2008 +1000

    [POWERPC] Bolt in SLB entry for kernel stack on secondary cpus
    
    This fixes a regression reported by Kamalesh Bulabel where a POWER4
    machine would crash because of an SLB miss at a point where the SLB
    miss exception was unrecoverable.  This regression is tracked at:
    
    http://bugzilla.kernel.org/show_bug.cgi?id=10082
    
    SLB misses at such points shouldn't happen because the kernel stack is
    the only memory accessed other than things in the first segment of the
    linear mapping (which is mapped at all times by entry 0 of the SLB).
    The context switch code ensures that SLB entry 2 covers the kernel
    stack, if it is not already covered by entry 0.  None of entries 0
    to 2 are ever replaced by the SLB miss handler.
    
    Where this went wrong is that the context switch code assumes it
    doesn't have to write to SLB entry 2 if the new kernel stack is in the
    same segment as the old kernel stack, since entry 2 should already be
    correct.  However, when we start up a secondary cpu, it calls
    slb_initialize, which doesn't set up entry 2.  This is correct for
    the boot cpu, where we will be using a stack in the kernel BSS at this
    point (i.e. init_thread_union), but not necessarily for secondary
    cpus, whose initial stack can be allocated anywhere.  This doesn't
    cause any immediate problem since the SLB miss handler will just
    create an SLB entry somewhere else to cover the initial stack.
    
    In fact it's possible for the cpu to go quite a long time without SLB
    entry 2 being valid.  Eventually, though, the entry created by the SLB
    miss handler will get overwritten by some other entry, and if the next
    access to the stack is at an unrecoverable point, we get the crash.
    
    This fixes the problem by making slb_initialize create a suitable
    entry for the kernel stack, if we are on a secondary cpu and the stack
    isn't covered by SLB entry 0.  This requires initializing the
    get_paca()->kstack field earlier, so I do that in smp_create_idle
    where the current field is initialized.  This also abstracts a bit of
    the computation that mk_esid_data in slb.c does so that it can be used
    in slb_initialize.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index be35ffae10f0..1457aa0a08f1 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -386,6 +386,8 @@ static void __init smp_create_idle(unsigned int cpu)
 		panic("failed fork for CPU %u: %li", cpu, PTR_ERR(p));
 #ifdef CONFIG_PPC64
 	paca[cpu].__current = p;
+	paca[cpu].kstack = (unsigned long) task_thread_info(p)
+		+ THREAD_SIZE - STACK_FRAME_OVERHEAD;
 #endif
 	current_set[cpu] = task_thread_info(p);
 	task_thread_info(p)->cpu = cpu;

commit e057d985fd8aad83d07376c5c36f2c8a6c5411be
Author: Olof Johansson <olof@lixom.net>
Date:   Fri Dec 28 15:11:09 2007 +1100

    [POWERPC] Make smp_send_stop() handle panic and xmon reboot
    
    smp_send_stop() will send an IPI to all other cpus to shut them down.
    However, for the case of xmon-based reboots (as well as potentially some
    panics), the other cpus are (or might be) spinning with interrupts off,
    and won't take the IPI.
    
    Current code will drop us into the debugger when the IPI fails, which
    means we're in an infinite loop that we can't get out of without an
    external reset of some sort.
    
    Instead, make the smp_send_stop() IPI call path just print the warning
    about being unable to send IPIs, but make it return so the rest of the
    shutdown sequence can continue. It's not perfect, but the lesser of
    two evils.
    
    Also move the call_lock handling outside of smp_call_function_map so we
    can avoid deadlocks in smp_send_stop().
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index cefeee81c52e..be35ffae10f0 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -76,6 +76,8 @@ void smp_call_function_interrupt(void);
 
 int smt_enabled_at_boot = 1;
 
+static int ipi_fail_ok;
+
 static void (*crash_ipi_function_ptr)(struct pt_regs *) = NULL;
 
 #ifdef CONFIG_PPC64
@@ -204,8 +206,6 @@ static int __smp_call_function_map(void (*func) (void *info), void *info,
 	if (wait)
 		atomic_set(&data.finished, 0);
 
-	spin_lock(&call_lock);
-
 	/* remove 'self' from the map */
 	if (cpu_isset(smp_processor_id(), map))
 		cpu_clear(smp_processor_id(), map);
@@ -232,7 +232,8 @@ static int __smp_call_function_map(void (*func) (void *info), void *info,
 			printk("smp_call_function on cpu %d: other cpus not "
 				"responding (%d)\n", smp_processor_id(),
 				atomic_read(&data.started));
-			debugger(NULL);
+			if (!ipi_fail_ok)
+				debugger(NULL);
 			goto out;
 		}
 	}
@@ -259,15 +260,18 @@ static int __smp_call_function_map(void (*func) (void *info), void *info,
  out:
 	call_data = NULL;
 	HMT_medium();
-	spin_unlock(&call_lock);
 	return ret;
 }
 
 static int __smp_call_function(void (*func)(void *info), void *info,
 			       int nonatomic, int wait)
 {
-	return __smp_call_function_map(func, info, nonatomic, wait,
+	int ret;
+	spin_lock(&call_lock);
+	ret =__smp_call_function_map(func, info, nonatomic, wait,
 				       cpu_online_map);
+	spin_unlock(&call_lock);
+	return ret;
 }
 
 int smp_call_function(void (*func) (void *info), void *info, int nonatomic,
@@ -293,9 +297,11 @@ int smp_call_function_single(int cpu, void (*func) (void *info), void *info,
 		return -EINVAL;
 
 	cpu_set(cpu, map);
-	if (cpu != get_cpu())
+	if (cpu != get_cpu()) {
+		spin_lock(&call_lock);
 		ret = __smp_call_function_map(func, info, nonatomic, wait, map);
-	else {
+		spin_unlock(&call_lock);
+	} else {
 		local_irq_disable();
 		func(info);
 		local_irq_enable();
@@ -307,7 +313,22 @@ EXPORT_SYMBOL(smp_call_function_single);
 
 void smp_send_stop(void)
 {
-	__smp_call_function(stop_this_cpu, NULL, 1, 0);
+	int nolock;
+
+	/* It's OK to fail sending the IPI, since the alternative is to
+	 * be stuck forever waiting on the other CPU to take the interrupt.
+	 *
+	 * It's better to at least continue and go through reboot, since this
+	 * function is usually called at panic or reboot time in the first
+	 * place.
+	 */
+	ipi_fail_ok = 1;
+
+	/* Don't deadlock in case we got called through panic */
+	nolock = !spin_trylock(&call_lock);
+	__smp_call_function_map(stop_this_cpu, NULL, 1, 0, cpu_online_map);
+	if (!nolock)
+		spin_unlock(&call_lock);
 }
 
 void smp_call_function_interrupt(void)

commit b616de5ef928ac1914348ff6a42521ca6b83112e
Author: Olof Johansson <olof@lixom.net>
Date:   Fri Dec 28 15:08:36 2007 +1100

    [POWERPC] Make smp_call_function_map static
    
    smp_call_function_map should be static, and for consistency prepend it
    with __ like other local helper functions in the same file.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 338950aeb6f6..cefeee81c52e 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -181,12 +181,13 @@ static struct call_data_struct {
  * <wait> If true, wait (atomically) until function has completed on other CPUs.
  * [RETURNS] 0 on success, else a negative status code. Does not return until
  * remote CPUs are nearly ready to execute <<func>> or are or have executed.
+ * <map> is a cpu map of the cpus to send IPI to.
  *
  * You must not call this function with disabled interrupts or from a
  * hardware interrupt handler or from a bottom half handler.
  */
-int smp_call_function_map(void (*func) (void *info), void *info, int nonatomic,
-			int wait, cpumask_t map)
+static int __smp_call_function_map(void (*func) (void *info), void *info,
+				   int nonatomic, int wait, cpumask_t map)
 {
 	struct call_data_struct data;
 	int ret = -1, num_cpus;
@@ -265,7 +266,8 @@ int smp_call_function_map(void (*func) (void *info), void *info, int nonatomic,
 static int __smp_call_function(void (*func)(void *info), void *info,
 			       int nonatomic, int wait)
 {
-	return smp_call_function_map(func,info,nonatomic,wait,cpu_online_map);
+	return __smp_call_function_map(func, info, nonatomic, wait,
+				       cpu_online_map);
 }
 
 int smp_call_function(void (*func) (void *info), void *info, int nonatomic,
@@ -278,8 +280,8 @@ int smp_call_function(void (*func) (void *info), void *info, int nonatomic,
 }
 EXPORT_SYMBOL(smp_call_function);
 
-int smp_call_function_single(int cpu, void (*func) (void *info), void *info, int nonatomic,
-			int wait)
+int smp_call_function_single(int cpu, void (*func) (void *info), void *info,
+			     int nonatomic, int wait)
 {
 	cpumask_t map = CPU_MASK_NONE;
 	int ret = 0;
@@ -292,7 +294,7 @@ int smp_call_function_single(int cpu, void (*func) (void *info), void *info, int
 
 	cpu_set(cpu, map);
 	if (cpu != get_cpu())
-		ret = smp_call_function_map(func,info,nonatomic,wait,map);
+		ret = __smp_call_function_map(func, info, nonatomic, wait, map);
 	else {
 		local_irq_disable();
 		func(info);

commit d5a7430ddcdb598261d70f7eb1bf450b5be52085
Author: Mike Travis <travis@sgi.com>
Date:   Tue Oct 16 01:24:05 2007 -0700

    Convert cpu_sibling_map to be a per cpu variable
    
    Convert cpu_sibling_map from a static array sized by NR_CPUS to a per_cpu
    variable.  This saves sizeof(cpumask_t) * NR unused cpus.  Access is mostly
    from startup and CPU HOTPLUG functions.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: "Siddha, Suresh B" <suresh.b.siddha@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index d30f08fa0297..338950aeb6f6 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -61,11 +61,11 @@ struct thread_info *secondary_ti;
 
 cpumask_t cpu_possible_map = CPU_MASK_NONE;
 cpumask_t cpu_online_map = CPU_MASK_NONE;
-cpumask_t cpu_sibling_map[NR_CPUS] = { [0 ... NR_CPUS-1] = CPU_MASK_NONE };
+DEFINE_PER_CPU(cpumask_t, cpu_sibling_map) = CPU_MASK_NONE;
 
 EXPORT_SYMBOL(cpu_online_map);
 EXPORT_SYMBOL(cpu_possible_map);
-EXPORT_SYMBOL(cpu_sibling_map);
+EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
 
 /* SMP operations for this machine */
 struct smp_ops_t *smp_ops;

commit d831d0b83f205888f4be4dee0a074ad67ef809b3
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Fri Sep 21 13:26:03 2007 +1000

    [POWERPC] Implement clockevents driver for powerpc
    
    This registers a clock event structure for the decrementer and turns
    on CONFIG_GENERIC_CLOCKEVENTS, which means that we now don't need
    most of timer_interrupt(), since the work is done in generic code.
    For secondary CPUs, their decrementer clockevent is registered when
    the CPU comes up (the generic code automatically removes the
    clockevent when the CPU goes down).
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index b24dcbaeecaa..d30f08fa0297 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -569,6 +569,8 @@ int __devinit start_secondary(void *unused)
 	if (system_state > SYSTEM_BOOTING)
 		snapshot_timebase();
 
+	secondary_cpu_time_init();
+
 	spin_lock(&call_lock);
 	cpu_set(cpu, cpu_online_map);
 	spin_unlock(&call_lock);

commit 8fd7675c092f79f240246c76728477ec4e7f7f09
Author: Satyam Sharma <satyam@infradead.org>
Date:   Tue Sep 18 09:43:40 2007 +1000

    [POWERPC] Avoid pointless WARN_ON(irqs_disabled()) from panic codepath
    
    > ------------[ cut here ]------------
    > Badness at arch/powerpc/kernel/smp.c:202
    
    comes when smp_call_function_map() has been called with irqs disabled,
    which is illegal. However, there is a special case, the panic() codepath,
    when we do not want to warn about this -- warning at that time is pointless
    anyway, and only serves to scroll away the *real* cause of the panic and
    distracts from the real bug.
    
    * So let's extract the WARN_ON() from smp_call_function_map() into all its
      callers -- smp_call_function() and smp_call_function_single()
    
    * Also, introduce another caller of smp_call_function_map(), namely
      __smp_call_function() (and make smp_call_function() a wrapper over this)
      which does *not* warn about disabled irqs
    
    * Use this __smp_call_function() from the panic codepath's smp_send_stop()
    
    We also end having to move code of smp_send_stop() below the definition
    of __smp_call_function().
    
    Signed-off-by: Satyam Sharma <satyam@infradead.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 1ea43160f543..b24dcbaeecaa 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -152,11 +152,6 @@ static void stop_this_cpu(void *dummy)
 		;
 }
 
-void smp_send_stop(void)
-{
-	smp_call_function(stop_this_cpu, NULL, 1, 0);
-}
-
 /*
  * Structure and data for smp_call_function(). This is designed to minimise
  * static memory requirements. It also looks cleaner.
@@ -198,9 +193,6 @@ int smp_call_function_map(void (*func) (void *info), void *info, int nonatomic,
 	int cpu;
 	u64 timeout;
 
-	/* Can deadlock when called with interrupts disabled */
-	WARN_ON(irqs_disabled());
-
 	if (unlikely(smp_ops == NULL))
 		return ret;
 
@@ -270,10 +262,19 @@ int smp_call_function_map(void (*func) (void *info), void *info, int nonatomic,
 	return ret;
 }
 
+static int __smp_call_function(void (*func)(void *info), void *info,
+			       int nonatomic, int wait)
+{
+	return smp_call_function_map(func,info,nonatomic,wait,cpu_online_map);
+}
+
 int smp_call_function(void (*func) (void *info), void *info, int nonatomic,
 			int wait)
 {
-	return smp_call_function_map(func,info,nonatomic,wait,cpu_online_map);
+	/* Can deadlock when called with interrupts disabled */
+	WARN_ON(irqs_disabled());
+
+	return __smp_call_function(func, info, nonatomic, wait);
 }
 EXPORT_SYMBOL(smp_call_function);
 
@@ -283,6 +284,9 @@ int smp_call_function_single(int cpu, void (*func) (void *info), void *info, int
 	cpumask_t map = CPU_MASK_NONE;
 	int ret = 0;
 
+	/* Can deadlock when called with interrupts disabled */
+	WARN_ON(irqs_disabled());
+
 	if (!cpu_online(cpu))
 		return -EINVAL;
 
@@ -299,6 +303,11 @@ int smp_call_function_single(int cpu, void (*func) (void *info), void *info, int
 }
 EXPORT_SYMBOL(smp_call_function_single);
 
+void smp_send_stop(void)
+{
+	__smp_call_function(stop_this_cpu, NULL, 1, 0);
+}
+
 void smp_call_function_interrupt(void)
 {
 	void (*func) (void *info);

commit 17aa3a82aa2173a22405f862c4444656f0494a3f
Author: Kevin Corry <kevcorry@us.ibm.com>
Date:   Wed Aug 1 06:19:46 2007 +1000

    [POWERPC] Fix num_cpus calculation in smp_call_function_map()
    
    In smp_call_function_map(), num_cpus is set to the number of online
    CPUs minus one.  However, if the CPU mask does not include all CPUs
    (except the one we're running on), the routine will hang in the first
    while() loop until the 8 second timeout occurs.
    
    The num_cpus should be set to the number of CPUs specified in the mask
    passed into the routine, after we've made any modifications to the
    mask.  With this change, we can also get rid of the call to
    cpus_empty() and avoid adding another pass through the bitmask.
    
    Signed-off-by: Kevin Corry <kevcorry@us.ibm.com>
    Signed-off-by: Carl Love <carll@us.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 087c92f2a3eb..1ea43160f543 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -212,11 +212,6 @@ int smp_call_function_map(void (*func) (void *info), void *info, int nonatomic,
 		atomic_set(&data.finished, 0);
 
 	spin_lock(&call_lock);
-	/* Must grab online cpu count with preempt disabled, otherwise
-	 * it can change. */
-	num_cpus = num_online_cpus() - 1;
-	if (!num_cpus)
-		goto done;
 
 	/* remove 'self' from the map */
 	if (cpu_isset(smp_processor_id(), map))
@@ -224,7 +219,9 @@ int smp_call_function_map(void (*func) (void *info), void *info, int nonatomic,
 
 	/* sanity check the map, remove any non-online processors. */
 	cpus_and(map, map, cpu_online_map);
-	if (cpus_empty(map))
+
+	num_cpus = cpus_weight(map);
+	if (!num_cpus)
 		goto done;
 
 	call_data = &data;

commit adff093d6c545c882f1503607f6af14ddd90bb89
Author: Avi Kivity <avi@qumranet.com>
Date:   Fri Jul 20 01:33:48 2007 +1000

    [POWERPC] Allow smp_call_function_single() to current cpu
    
    This removes the requirement for callers to get_cpu() to check in simple
    cases.  i386 and x86_64 already received a similar treatment.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index d577b71db375..087c92f2a3eb 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -284,7 +284,7 @@ int smp_call_function_single(int cpu, void (*func) (void *info), void *info, int
 			int wait)
 {
 	cpumask_t map = CPU_MASK_NONE;
-	int ret = -EBUSY;
+	int ret = 0;
 
 	if (!cpu_online(cpu))
 		return -EINVAL;
@@ -292,6 +292,11 @@ int smp_call_function_single(int cpu, void (*func) (void *info), void *info, int
 	cpu_set(cpu, map);
 	if (cpu != get_cpu())
 		ret = smp_call_function_map(func,info,nonatomic,wait,map);
+	else {
+		local_irq_disable();
+		func(info);
+		local_irq_enable();
+	}
 	put_cpu();
 	return ret;
 }

commit d3fdaed9e973687f088c9c156a6e20870386e0b7
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat May 19 02:47:01 2007 +1000

    [POWERPC] Fix smp_call_function to be preempt-safe
    
    smp_call_function_map() was not safe against preemption to another
    cpu: its test for removing self from map was outside the spinlock.
    Rearrange it a little to fix that.
    
    smp_call_function_single() was also wrong: now get_cpu() before
    excluding self, as other architectures do.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 22f1ef1b3100..d577b71db375 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -201,13 +201,6 @@ int smp_call_function_map(void (*func) (void *info), void *info, int nonatomic,
 	/* Can deadlock when called with interrupts disabled */
 	WARN_ON(irqs_disabled());
 
-	/* remove 'self' from the map */
-	if (cpu_isset(smp_processor_id(), map))
-		cpu_clear(smp_processor_id(), map);
-
-	/* sanity check the map, remove any non-online processors. */
-	cpus_and(map, map, cpu_online_map);
-
 	if (unlikely(smp_ops == NULL))
 		return ret;
 
@@ -222,10 +215,17 @@ int smp_call_function_map(void (*func) (void *info), void *info, int nonatomic,
 	/* Must grab online cpu count with preempt disabled, otherwise
 	 * it can change. */
 	num_cpus = num_online_cpus() - 1;
-	if (!num_cpus || cpus_empty(map)) {
-		ret = 0;
-		goto out;
-	}
+	if (!num_cpus)
+		goto done;
+
+	/* remove 'self' from the map */
+	if (cpu_isset(smp_processor_id(), map))
+		cpu_clear(smp_processor_id(), map);
+
+	/* sanity check the map, remove any non-online processors. */
+	cpus_and(map, map, cpu_online_map);
+	if (cpus_empty(map))
+		goto done;
 
 	call_data = &data;
 	smp_wmb();
@@ -263,6 +263,7 @@ int smp_call_function_map(void (*func) (void *info), void *info, int nonatomic,
 		}
 	}
 
+ done:
 	ret = 0;
 
  out:
@@ -282,16 +283,17 @@ EXPORT_SYMBOL(smp_call_function);
 int smp_call_function_single(int cpu, void (*func) (void *info), void *info, int nonatomic,
 			int wait)
 {
-	cpumask_t map=CPU_MASK_NONE;
+	cpumask_t map = CPU_MASK_NONE;
+	int ret = -EBUSY;
 
 	if (!cpu_online(cpu))
 		return -EINVAL;
 
-	if (cpu == smp_processor_id())
-		return -EBUSY;
-
 	cpu_set(cpu, map);
-	return smp_call_function_map(func,info,nonatomic,wait,map);
+	if (cpu != get_cpu())
+		ret = smp_call_function_map(func,info,nonatomic,wait,map);
+	put_cpu();
+	return ret;
 }
 EXPORT_SYMBOL(smp_call_function_single);
 

commit 44755d11a3c054adf7eb974a4720936563cf7dcf
Author: will schmidt <will_schmidt@vnet.ibm.com>
Date:   Thu May 3 03:12:34 2007 +1000

    [POWERPC] Add smp_call_function_map and smp_call_function_single
    
    Add a new function named smp_call_function_single().  This matches a generic
    prototype from include/linux/smp.h.
    
    Add a function smp_call_function_map().  This is, for the most part, a rename
    of smp_call_function, with some added cpumask support.  smp_call_function and
    smp_call_function_single call into smp_call_function_map.
    
    Lightly tested on 970mp (blade), power4 and power5.
    
    Signed-off-by: Will Schmidt <will_schmidt@vnet.ibm.com>
    cc: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index d8e503b2e1af..22f1ef1b3100 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -176,10 +176,10 @@ static struct call_data_struct {
 #define SMP_CALL_TIMEOUT	8
 
 /*
- * This function sends a 'generic call function' IPI to all other CPUs
- * in the system.
+ * These functions send a 'generic call function' IPI to other online
+ * CPUS in the system.
  *
- * [SUMMARY] Run a function on all other CPUs.
+ * [SUMMARY] Run a function on other CPUs.
  * <func> The function to run. This must be fast and non-blocking.
  * <info> An arbitrary pointer to pass to the function.
  * <nonatomic> currently unused.
@@ -190,18 +190,26 @@ static struct call_data_struct {
  * You must not call this function with disabled interrupts or from a
  * hardware interrupt handler or from a bottom half handler.
  */
-int smp_call_function (void (*func) (void *info), void *info, int nonatomic,
-		       int wait)
-{ 
+int smp_call_function_map(void (*func) (void *info), void *info, int nonatomic,
+			int wait, cpumask_t map)
+{
 	struct call_data_struct data;
-	int ret = -1, cpus;
+	int ret = -1, num_cpus;
+	int cpu;
 	u64 timeout;
 
 	/* Can deadlock when called with interrupts disabled */
 	WARN_ON(irqs_disabled());
 
+	/* remove 'self' from the map */
+	if (cpu_isset(smp_processor_id(), map))
+		cpu_clear(smp_processor_id(), map);
+
+	/* sanity check the map, remove any non-online processors. */
+	cpus_and(map, map, cpu_online_map);
+
 	if (unlikely(smp_ops == NULL))
-		return -1;
+		return ret;
 
 	data.func = func;
 	data.info = info;
@@ -213,40 +221,42 @@ int smp_call_function (void (*func) (void *info), void *info, int nonatomic,
 	spin_lock(&call_lock);
 	/* Must grab online cpu count with preempt disabled, otherwise
 	 * it can change. */
-	cpus = num_online_cpus() - 1;
-	if (!cpus) {
+	num_cpus = num_online_cpus() - 1;
+	if (!num_cpus || cpus_empty(map)) {
 		ret = 0;
 		goto out;
 	}
 
 	call_data = &data;
 	smp_wmb();
-	/* Send a message to all other CPUs and wait for them to respond */
-	smp_ops->message_pass(MSG_ALL_BUT_SELF, PPC_MSG_CALL_FUNCTION);
+	/* Send a message to all CPUs in the map */
+	for_each_cpu_mask(cpu, map)
+		smp_ops->message_pass(cpu, PPC_MSG_CALL_FUNCTION);
 
 	timeout = get_tb() + (u64) SMP_CALL_TIMEOUT * tb_ticks_per_sec;
 
-	/* Wait for response */
-	while (atomic_read(&data.started) != cpus) {
+	/* Wait for indication that they have received the message */
+	while (atomic_read(&data.started) != num_cpus) {
 		HMT_low();
 		if (get_tb() >= timeout) {
 			printk("smp_call_function on cpu %d: other cpus not "
-			       "responding (%d)\n", smp_processor_id(),
-			       atomic_read(&data.started));
+				"responding (%d)\n", smp_processor_id(),
+				atomic_read(&data.started));
 			debugger(NULL);
 			goto out;
 		}
 	}
 
+	/* optionally wait for the CPUs to complete */
 	if (wait) {
-		while (atomic_read(&data.finished) != cpus) {
+		while (atomic_read(&data.finished) != num_cpus) {
 			HMT_low();
 			if (get_tb() >= timeout) {
 				printk("smp_call_function on cpu %d: other "
-				       "cpus not finishing (%d/%d)\n",
-				       smp_processor_id(),
-				       atomic_read(&data.finished),
-				       atomic_read(&data.started));
+					"cpus not finishing (%d/%d)\n",
+					smp_processor_id(),
+					atomic_read(&data.finished),
+					atomic_read(&data.started));
 				debugger(NULL);
 				goto out;
 			}
@@ -262,8 +272,29 @@ int smp_call_function (void (*func) (void *info), void *info, int nonatomic,
 	return ret;
 }
 
+int smp_call_function(void (*func) (void *info), void *info, int nonatomic,
+			int wait)
+{
+	return smp_call_function_map(func,info,nonatomic,wait,cpu_online_map);
+}
 EXPORT_SYMBOL(smp_call_function);
 
+int smp_call_function_single(int cpu, void (*func) (void *info), void *info, int nonatomic,
+			int wait)
+{
+	cpumask_t map=CPU_MASK_NONE;
+
+	if (!cpu_online(cpu))
+		return -EINVAL;
+
+	if (cpu == smp_processor_id())
+		return -EBUSY;
+
+	cpu_set(cpu, map);
+	return smp_call_function_map(func,info,nonatomic,wait,map);
+}
+EXPORT_SYMBOL(smp_call_function_single);
+
 void smp_call_function_interrupt(void)
 {
 	void (*func) (void *info);

commit a741e67969577163a4cfc78d7fd2753219087ef1
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Apr 10 17:09:37 2007 +1000

    [POWERPC] Make tlb flush batch use lazy MMU mode
    
    The current tlb flush code on powerpc 64 bits has a subtle race since we
    lost the page table lock due to the possible faulting in of new PTEs
    after a previous one has been removed but before the corresponding hash
    entry has been evicted, which can leads to all sort of fatal problems.
    
    This patch reworks the batch code completely. It doesn't use the mmu_gather
    stuff anymore. Instead, we use the lazy mmu hooks that were added by the
    paravirt code. They have the nice property that the enter/leave lazy mmu
    mode pair is always fully contained by the PTE lock for a given range
    of PTEs. Thus we can guarantee that all batches are flushed on a given
    CPU before it drops that lock.
    
    We also generalize batching for any PTE update that require a flush.
    
    Batching is now enabled on a CPU by arch_enter_lazy_mmu_mode() and
    disabled by arch_leave_lazy_mmu_mode(). The code epects that this is
    always contained within a PTE lock section so no preemption can happen
    and no PTE insertion in that range from another CPU. When batching
    is enabled on a CPU, every PTE updates that need a hash flush will
    use the batch for that flush.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 924d692bc8f9..d8e503b2e1af 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -428,10 +428,6 @@ void generic_mach_cpu_die(void)
 	smp_wmb();
 	while (__get_cpu_var(cpu_state) != CPU_UP_PREPARE)
 		cpu_relax();
-
-#ifdef CONFIG_PPC64
-	flush_tlb_pending();
-#endif
 	cpu_set(cpu, cpu_online_map);
 	local_irq_enable();
 }

commit 775aeff44774c6933d8f9c14e1f325d8acd03136
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Feb 8 18:34:04 2007 +1100

    [POWERPC] Move MPIC smp routines into mpic.c
    
    Move a couple of MPIC smp routines into mpic.c, they're inside an SMP
    block in mpic.c - so they're still only built for SMP.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 0e8beca460af..924d692bc8f9 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -78,29 +78,6 @@ int smt_enabled_at_boot = 1;
 
 static void (*crash_ipi_function_ptr)(struct pt_regs *) = NULL;
 
-#ifdef CONFIG_MPIC
-int __init smp_mpic_probe(void)
-{
-	int nr_cpus;
-
-	DBG("smp_mpic_probe()...\n");
-
-	nr_cpus = cpus_weight(cpu_possible_map);
-
-	DBG("nr_cpus: %d\n", nr_cpus);
-
-	if (nr_cpus > 1)
-		mpic_request_ipis();
-
-	return nr_cpus;
-}
-
-void __devinit smp_mpic_setup_cpu(int cpu)
-{
-	mpic_setup_this_cpu();
-}
-#endif /* CONFIG_MPIC */
-
 #ifdef CONFIG_PPC64
 void __devinit smp_generic_kick_cpu(int nr)
 {

commit b282b6f8a8d1cf3e132ce3769d7d1cac81d9dd2d
Author: Gautham R Shenoy <ego@in.ibm.com>
Date:   Wed Jan 10 23:15:34 2007 -0800

    [PATCH] Change cpu_up and co from __devinit to __cpuinit
    
    Compiling the kernel with CONFIG_HOTPLUG = y and CONFIG_HOTPLUG_CPU = n
    with CONFIG_RELOCATABLE = y generates the following modpost warnings
    
    WARNING: vmlinux - Section mismatch: reference to .init.data: from
    .text between '_cpu_up' (at offset 0xc0141b7d) and 'cpu_up'
    WARNING: vmlinux - Section mismatch: reference to .init.data: from
    .text between '_cpu_up' (at offset 0xc0141b9c) and 'cpu_up'
    WARNING: vmlinux - Section mismatch: reference to .init.text:__cpu_up
    from .text between '_cpu_up' (at offset 0xc0141bd8) and 'cpu_up'
    WARNING: vmlinux - Section mismatch: reference to .init.data: from
    .text between '_cpu_up' (at offset 0xc0141c05) and 'cpu_up'
    WARNING: vmlinux - Section mismatch: reference to .init.data: from
    .text between '_cpu_up' (at offset 0xc0141c26) and 'cpu_up'
    WARNING: vmlinux - Section mismatch: reference to .init.data: from
    .text between '_cpu_up' (at offset 0xc0141c37) and 'cpu_up'
    
    This is because cpu_up, _cpu_up and __cpu_up (in some architectures) are
    defined as __devinit
    AND
    __cpu_up calls some __cpuinit functions.
    
    Since __cpuinit would map to __init with this kind of a configuration,
    we get a .text refering .init.data warning.
    
    This patch solves the problem by converting all of __cpu_up, _cpu_up
    and cpu_up from __devinit to __cpuinit. The approach is justified since
    the callers of cpu_up are either dependent on CONFIG_HOTPLUG_CPU or
    are of __init type.
    
    Thus when CONFIG_HOTPLUG_CPU=y, all these cpu up functions would land up
    in .text section, and when CONFIG_HOTPLUG_CPU=n, all these functions would
    land up in .init section.
    
    Tested on a i386 SMP machine running linux-2.6.20-rc3-mm1.
    
    Signed-off-by: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Vivek Goyal <vgoyal@in.ibm.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 9b28c238b6c0..0e8beca460af 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -468,7 +468,7 @@ static int __devinit cpu_enable(unsigned int cpu)
 	return -ENOSYS;
 }
 
-int __devinit __cpu_up(unsigned int cpu)
+int __cpuinit __cpu_up(unsigned int cpu)
 {
 	int c;
 

commit 36ca4ba4b9728f3c420a589a3322c2fbd7ec88b7
Author: Christian Krafft <krafft@de.ibm.com>
Date:   Tue Oct 24 18:39:45 2006 +0200

    [POWERPC] cell: add cpufreq driver for Cell BE processor
    
    This patch adds a cpufreq backend driver to enable frequency scaling on cell.
    
    Signed-off-by: Christian Krafft <krafft@de.ibm.com>
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 35c6309bdb76..9b28c238b6c0 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -65,6 +65,7 @@ cpumask_t cpu_sibling_map[NR_CPUS] = { [0 ... NR_CPUS-1] = CPU_MASK_NONE };
 
 EXPORT_SYMBOL(cpu_online_map);
 EXPORT_SYMBOL(cpu_possible_map);
+EXPORT_SYMBOL(cpu_sibling_map);
 
 /* SMP operations for this machine */
 struct smp_ops_t *smp_ops;

commit 7d12e780e003f93433d49ce78cfedf4b4c52adc5
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 5 14:55:46 2006 +0100

    IRQ: Maintain regs pointer globally rather than passing to IRQ handlers
    
    Maintain a per-CPU global "struct pt_regs *" variable which can be used instead
    of passing regs around manually through all ~1800 interrupt handlers in the
    Linux kernel.
    
    The regs pointer is used in few places, but it potentially costs both stack
    space and code to pass it around.  On the FRV arch, removing the regs parameter
    from all the genirq function results in a 20% speed up of the IRQ exit path
    (ie: from leaving timer_interrupt() to leaving do_IRQ()).
    
    Where appropriate, an arch may override the generic storage facility and do
    something different with the variable.  On FRV, for instance, the address is
    maintained in GR28 at all times inside the kernel as part of general exception
    handling.
    
    Having looked over the code, it appears that the parameter may be handed down
    through up to twenty or so layers of functions.  Consider a USB character
    device attached to a USB hub, attached to a USB controller that posts its
    interrupts through a cascaded auxiliary interrupt controller.  A character
    device driver may want to pass regs to the sysrq handler through the input
    layer which adds another few layers of parameter passing.
    
    I've build this code with allyesconfig for x86_64 and i386.  I've runtested the
    main part of the code on FRV and i386, though I can't test most of the drivers.
    I've also done partial conversion for powerpc and MIPS - these at least compile
    with minimal configurations.
    
    This will affect all archs.  Mostly the changes should be relatively easy.
    Take do_IRQ(), store the regs pointer at the beginning, saving the old one:
    
            struct pt_regs *old_regs = set_irq_regs(regs);
    
    And put the old one back at the end:
    
            set_irq_regs(old_regs);
    
    Don't pass regs through to generic_handle_irq() or __do_IRQ().
    
    In timer_interrupt(), this sort of change will be necessary:
    
            -       update_process_times(user_mode(regs));
            -       profile_tick(CPU_PROFILING, regs);
            +       update_process_times(user_mode(get_irq_regs()));
            +       profile_tick(CPU_PROFILING);
    
    I'd like to move update_process_times()'s use of get_irq_regs() into itself,
    except that i386, alone of the archs, uses something other than user_mode().
    
    Some notes on the interrupt handling in the drivers:
    
     (*) input_dev() is now gone entirely.  The regs pointer is no longer stored in
         the input_dev struct.
    
     (*) finish_unlinks() in drivers/usb/host/ohci-q.c needs checking.  It does
         something different depending on whether it's been supplied with a regs
         pointer or not.
    
     (*) Various IRQ handler function pointers have been moved to type
         irq_handler_t.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>
    (cherry picked from 1b16e7ac850969f38b375e511e3fa2f474a33867 commit)

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 6a9bc9ce54e0..35c6309bdb76 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -115,7 +115,7 @@ void __devinit smp_generic_kick_cpu(int nr)
 }
 #endif
 
-void smp_message_recv(int msg, struct pt_regs *regs)
+void smp_message_recv(int msg)
 {
 	switch(msg) {
 	case PPC_MSG_CALL_FUNCTION:
@@ -127,11 +127,11 @@ void smp_message_recv(int msg, struct pt_regs *regs)
 		break;
 	case PPC_MSG_DEBUGGER_BREAK:
 		if (crash_ipi_function_ptr) {
-			crash_ipi_function_ptr(regs);
+			crash_ipi_function_ptr(get_irq_regs());
 			break;
 		}
 #ifdef CONFIG_DEBUGGER
-		debugger_ipi(regs);
+		debugger_ipi(get_irq_regs());
 		break;
 #endif /* CONFIG_DEBUGGER */
 		/* FALLTHROUGH */

commit 8cffc6ac66a2b251df2490702923611aa4ac1fc5
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 4 14:09:36 2006 +1000

    [POWERPC] Fix non-MPIC CHRPs with CONFIG_SMP set
    
    Pseudo-CHRP machines like Pegasos without an MPIC would crash at boot if
    CONFIG_SMP was set because the "smp_ops" pointer was set to MPIC related
    ops unconditionally. This patch makes it NULL on machines that don't
    support SMP and provides proper default behaviour in the callers when
    smp_ops is NULL.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 46c56cfd1b2f..6a9bc9ce54e0 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -144,13 +144,15 @@ void smp_message_recv(int msg, struct pt_regs *regs)
 
 void smp_send_reschedule(int cpu)
 {
-	smp_ops->message_pass(cpu, PPC_MSG_RESCHEDULE);
+	if (likely(smp_ops))
+		smp_ops->message_pass(cpu, PPC_MSG_RESCHEDULE);
 }
 
 #ifdef CONFIG_DEBUGGER
 void smp_send_debugger_break(int cpu)
 {
-	smp_ops->message_pass(cpu, PPC_MSG_DEBUGGER_BREAK);
+	if (likely(smp_ops))
+		smp_ops->message_pass(cpu, PPC_MSG_DEBUGGER_BREAK);
 }
 #endif
 
@@ -158,7 +160,7 @@ void smp_send_debugger_break(int cpu)
 void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
 {
 	crash_ipi_function_ptr = crash_ipi_callback;
-	if (crash_ipi_callback) {
+	if (crash_ipi_callback && smp_ops) {
 		mb();
 		smp_ops->message_pass(MSG_ALL_BUT_SELF, PPC_MSG_DEBUGGER_BREAK);
 	}
@@ -220,6 +222,9 @@ int smp_call_function (void (*func) (void *info), void *info, int nonatomic,
 	/* Can deadlock when called with interrupts disabled */
 	WARN_ON(irqs_disabled());
 
+	if (unlikely(smp_ops == NULL))
+		return -1;
+
 	data.func = func;
 	data.info = info;
 	atomic_set(&data.started, 0);
@@ -357,7 +362,10 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	smp_store_cpu_info(boot_cpuid);
 	cpu_callin_map[boot_cpuid] = 1;
 
-	max_cpus = smp_ops->probe();
+	if (smp_ops)
+		max_cpus = smp_ops->probe();
+	else
+		max_cpus = 1;
  
 	smp_space_timers(max_cpus);
 
@@ -453,7 +461,7 @@ void generic_mach_cpu_die(void)
 
 static int __devinit cpu_enable(unsigned int cpu)
 {
-	if (smp_ops->cpu_enable)
+	if (smp_ops && smp_ops->cpu_enable)
 		return smp_ops->cpu_enable(cpu);
 
 	return -ENOSYS;
@@ -467,7 +475,8 @@ int __devinit __cpu_up(unsigned int cpu)
 	if (!cpu_enable(cpu))
 		return 0;
 
-	if (smp_ops->cpu_bootable && !smp_ops->cpu_bootable(cpu))
+	if (smp_ops == NULL ||
+	    (smp_ops->cpu_bootable && !smp_ops->cpu_bootable(cpu)))
 		return -EINVAL;
 
 	/* Make sure callin-map entry is 0 (can be leftover a CPU
@@ -568,7 +577,8 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	old_mask = current->cpus_allowed;
 	set_cpus_allowed(current, cpumask_of_cpu(boot_cpuid));
 	
-	smp_ops->setup_cpu(boot_cpuid);
+	if (smp_ops)
+		smp_ops->setup_cpu(boot_cpuid);
 
 	set_cpus_allowed(current, old_mask);
 

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jörn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jörn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index c5d179d4f818..46c56cfd1b2f 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -17,7 +17,6 @@
 
 #undef DEBUG
 
-#include <linux/config.h>
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/sched.h>

commit ee0339f205d60375c5ce1653c0dc318c6ec72668
Author: Jon Loeliger <jdl@jdl.com>
Date:   Sat Jun 17 17:52:44 2006 -0500

    [POWERPC] Add starting of secondary 86xx CPUs.
    
    Clear the high BATS during load_up_mmu if FTR_HAS_HIGH_BATS.
    Allow just a bit more time for secondary CPUs to phone home.
    
    Signed-off-by: Wei Zhang <Wei.Zhang@freescale.com>
    Signed-off-by: Haiying Wang <Haiying.Wang@freescale.com>
    Signed-off-by: Jon Loeliger <jdl@freescale.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 530f7dba0bd2..c5d179d4f818 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -492,7 +492,7 @@ int __devinit __cpu_up(unsigned int cpu)
 	 * -- Cort
 	 */
 	if (system_state < SYSTEM_RUNNING)
-		for (c = 5000; c && !cpu_callin_map[cpu]; c--)
+		for (c = 50000; c && !cpu_callin_map[cpu]; c--)
 			udelay(100);
 #ifdef CONFIG_HOTPLUG_CPU
 	else

commit 0e5519548fdc8eadc3eacb49b1908d44d347fb2b
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Mar 28 14:50:51 2006 -0800

    [PATCH] for_each_possible_cpu: powerpc
    
    for_each_cpu() actually iterates across all possible CPUs.  We've had mistakes
    in the past where people were using for_each_cpu() where they should have been
    iterating across only online or present CPUs.  This is inefficient and
    possibly buggy.
    
    We're renaming for_each_cpu() to for_each_possible_cpu() to avoid this in the
    future.
    
    This patch replaces for_each_cpu with for_each_possible_cpu.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 805eaedbc308..530f7dba0bd2 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -362,7 +362,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
  
 	smp_space_timers(max_cpus);
 
-	for_each_cpu(cpu)
+	for_each_possible_cpu(cpu)
 		if (cpu != boot_cpuid)
 			smp_create_idle(cpu);
 }

commit c6622f63db86fcbd41bf6fe05ddf2e00c1e51ced
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Feb 24 10:06:59 2006 +1100

    powerpc: Implement accurate task and CPU time accounting
    
    This implements accurate task and cpu time accounting for 64-bit
    powerpc kernels.  Instead of accounting a whole jiffy of time to a
    task on a timer interrupt because that task happened to be running at
    the time, we now account time in units of timebase ticks according to
    the actual time spent by the task in user mode and kernel mode.  We
    also count the time spent processing hardware and software interrupts
    accurately.  This is conditional on CONFIG_VIRT_CPU_ACCOUNTING.  If
    that is not set, we do tick-based approximate accounting as before.
    
    To get this accurate information, we read either the PURR (processor
    utilization of resources register) on POWER5 machines, or the timebase
    on other machines on
    
    * each entry to the kernel from usermode
    * each exit to usermode
    * transitions between process context, hard irq context and soft irq
      context in kernel mode
    * context switches.
    
    On POWER5 systems with shared-processor logical partitioning we also
    read both the PURR and the timebase at each timer interrupt and
    context switch in order to determine how much time has been taken by
    the hypervisor to run other partitions ("steal" time).  Unfortunately,
    since we need values of the PURR on both threads at the same time to
    accurately calculate the steal time, and since we can only calculate
    steal time on a per-core basis, the apportioning of the steal time
    between idle time (time which we ceded to the hypervisor in the idle
    loop) and actual stolen time is somewhat approximate at the moment.
    
    This is all based quite heavily on what s390 does, and it uses the
    generic interfaces that were added by the s390 developers,
    i.e. account_system_time(), account_user_time(), etc.
    
    This patch doesn't add any new interfaces between the kernel and
    userspace, and doesn't change the units in which time is reported to
    userspace by things such as /proc/stat, /proc/<pid>/stat, getrusage(),
    times(), etc.  Internally the various task and cpu times are stored in
    timebase units, but they are converted to USER_HZ units (1/100th of a
    second) when reported to userspace.  Some precision is therefore lost
    but there should not be any accumulating error, since the internal
    accumulation is at full precision.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 13595a64f013..805eaedbc308 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -541,7 +541,7 @@ int __devinit start_secondary(void *unused)
 		smp_ops->take_timebase();
 
 	if (system_state > SYSTEM_BOOTING)
-		per_cpu(last_jiffy, cpu) = get_tb();
+		snapshot_timebase();
 
 	spin_lock(&call_lock);
 	cpu_set(cpu, cpu_online_map);
@@ -573,6 +573,8 @@ void __init smp_cpus_done(unsigned int max_cpus)
 
 	set_cpus_allowed(current, old_mask);
 
+	snapshot_timebases();
+
 	dump_numa_cpu_topology();
 }
 

commit 7d4d61544a12333600bdb9b018a149868418692e
Author: Nathan Lynch <ntl@pobox.com>
Date:   Mon Feb 6 22:44:23 2006 -0600

    [PATCH] powerpc: avoid timer interrupt replay effect when onlining cpu
    
    When a cpu is hotplug-onlined, if we don't set per_cpu(last_jiffy) to
    something sane, timer_interrupt will execute its while loop for every
    tick missed since the cpu was last online (or since the system was
    booted, if we're adding a new cpu).  This can cause weird hangs, ssh
    sessions dropping, and we can even go xmon if we take a global IPI at
    the wrong time.
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index c8458c531b25..13595a64f013 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -540,6 +540,9 @@ int __devinit start_secondary(void *unused)
 	if (smp_ops->take_timebase)
 		smp_ops->take_timebase();
 
+	if (system_state > SYSTEM_BOOTING)
+		per_cpu(last_jiffy, cpu) = get_tb();
+
 	spin_lock(&call_lock);
 	cpu_set(cpu, cpu_online_map);
 	spin_unlock(&call_lock);

commit b5e2fc1c6259e6f26bc4ae4de697da1f8da0edec
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Thu Jan 12 01:06:01 2006 -0800

    [PATCH] powerpc: task_thread_info()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index d381ec90b759..c8458c531b25 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -338,8 +338,8 @@ static void __init smp_create_idle(unsigned int cpu)
 #ifdef CONFIG_PPC64
 	paca[cpu].__current = p;
 #endif
-	current_set[cpu] = p->thread_info;
-	p->thread_info->cpu = cpu;
+	current_set[cpu] = task_thread_info(p);
+	task_thread_info(p)->cpu = cpu;
 }
 
 void __init smp_prepare_cpus(unsigned int max_cpus)
@@ -375,7 +375,7 @@ void __devinit smp_prepare_boot_cpu(void)
 #ifdef CONFIG_PPC64
 	paca[boot_cpuid].__current = current;
 #endif
-	current_set[boot_cpuid] = current->thread_info;
+	current_set[boot_cpuid] = task_thread_info(current);
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit 4b703a231799f43f3414b62300b8ad6736a4aa9d
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Dec 13 06:56:47 2005 +1100

    [PATCH] ppc64: Add NUMA cpu summary at boot
    
    We used to print a NUMA cpu summary at boot before the hotplug cpu code
    was added. This has been useful for catching machine configuration as
    well as firmware bugs in the past.
    
    This patch restores that functionality. An example of the output is:
    
    Node 0 CPUs: 0-7
    Node 1 CPUs: 8-15
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 8e3ca674d359..d381ec90b759 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -31,6 +31,7 @@
 #include <linux/sysdev.h>
 #include <linux/cpu.h>
 #include <linux/notifier.h>
+#include <linux/topology.h>
 
 #include <asm/ptrace.h>
 #include <asm/atomic.h>
@@ -568,6 +569,8 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	smp_ops->setup_cpu(boot_cpuid);
 
 	set_cpus_allowed(current, old_mask);
+
+	dump_numa_cpu_topology();
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit cc53291521701f9c7c7265bbb3c140563174d8b2
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Sun Dec 4 18:39:43 2005 +1100

    [PATCH] powerpc: Add arch dependent basic infrastructure for Kdump.
    
    Implementing the machine_crash_shutdown which will be called by
    crash_kexec (called in case of a panic, sysrq etc.). Disable the
    interrupts, shootdown cpus using debugger IPI and collect regs
    for all CPUs.
    
    elfcorehdr= specifies the location of elf core header stored by
    the crashed kernel. This command line option will be passed by
    the kexec-tools to capture kernel.
    
    savemaxmem= specifies the actual memory size that the first kernel
    has and this value will be used for dumping in the capture kernel.
    This command line option will be passed by the kexec-tools to
    capture kernel.
    
    Signed-off-by: Haren Myneni <haren@us.ibm.com>
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index a90df6bf0940..8e3ca674d359 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -75,6 +75,8 @@ void smp_call_function_interrupt(void);
 
 int smt_enabled_at_boot = 1;
 
+static void (*crash_ipi_function_ptr)(struct pt_regs *) = NULL;
+
 #ifdef CONFIG_MPIC
 int __init smp_mpic_probe(void)
 {
@@ -123,11 +125,16 @@ void smp_message_recv(int msg, struct pt_regs *regs)
 		/* XXX Do we have to do this? */
 		set_need_resched();
 		break;
-#ifdef CONFIG_DEBUGGER
 	case PPC_MSG_DEBUGGER_BREAK:
+		if (crash_ipi_function_ptr) {
+			crash_ipi_function_ptr(regs);
+			break;
+		}
+#ifdef CONFIG_DEBUGGER
 		debugger_ipi(regs);
 		break;
-#endif
+#endif /* CONFIG_DEBUGGER */
+		/* FALLTHROUGH */
 	default:
 		printk("SMP %d: smp_message_recv(): unknown msg %d\n",
 		       smp_processor_id(), msg);
@@ -147,6 +154,17 @@ void smp_send_debugger_break(int cpu)
 }
 #endif
 
+#ifdef CONFIG_KEXEC
+void crash_send_ipi(void (*crash_ipi_callback)(struct pt_regs *))
+{
+	crash_ipi_function_ptr = crash_ipi_callback;
+	if (crash_ipi_callback) {
+		mb();
+		smp_ops->message_pass(MSG_ALL_BUT_SELF, PPC_MSG_DEBUGGER_BREAK);
+	}
+}
+#endif
+
 static void stop_this_cpu(void *dummy)
 {
 	local_irq_disable();

commit 404849bbd2bfd62e05b36f4753f6e1af6050a824
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Thu Nov 24 16:51:31 2005 +1100

    [PATCH] powerpc: Remove some unneeded fields from the paca
    
    This patch removes several unnecessary fields from the paca:
    
    - next_jiffy_update_tb was simply unused.  Remove trivially.
    
    - The exdsi exception save area was not used.  There were plans to use
      it, but they never seem to have gone anywhere.  If they ever do, we
      can put it back.  Remove from the paca, and from asm-offsets.c
    
    - The default_decr field was used from asm, but was only ever assigned
      the value of tb_ticks_per_jiffy.  Just access tb_ticks_per_jiffy from
      asm directly instead.
    
    Built and booted on POWER5 LPAR and iSeries RS64.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 30374d2f88e5..a90df6bf0940 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -452,10 +452,6 @@ int __devinit __cpu_up(unsigned int cpu)
 	if (smp_ops->cpu_bootable && !smp_ops->cpu_bootable(cpu))
 		return -EINVAL;
 
-#ifdef CONFIG_PPC64
-	paca[cpu].default_decr = tb_ticks_per_jiffy;
-#endif
-
 	/* Make sure callin-map entry is 0 (can be leftover a CPU
 	 * hotplug
 	 */

commit f9e4ec57c66586d0c165ed9373efaf9e329d5766
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Nov 15 15:16:38 2005 +1100

    [PATCH] powerpc: More debugging fixups
    
    Add a few more missing includes of udbg.h
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 62dfc5b8d765..30374d2f88e5 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -49,15 +49,16 @@
 #include <asm/paca.h>
 #endif
 
-int smp_hw_index[NR_CPUS];
-struct thread_info *secondary_ti;
-
 #ifdef DEBUG
+#include <asm/udbg.h>
 #define DBG(fmt...) udbg_printf(fmt)
 #else
 #define DBG(fmt...)
 #endif
 
+int smp_hw_index[NR_CPUS];
+struct thread_info *secondary_ti;
+
 cpumask_t cpu_possible_map = CPU_MASK_NONE;
 cpumask_t cpu_online_map = CPU_MASK_NONE;
 cpumask_t cpu_sibling_map[NR_CPUS] = { [0 ... NR_CPUS-1] = CPU_MASK_NONE };

commit a7f290dad32ee34d931561b7943c858fe2aae503
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Nov 11 21:15:21 2005 +1100

    [PATCH] powerpc: Merge vdso's and add vdso support to 32 bits kernel
    
    This patch moves the vdso's to arch/powerpc, adds support for the 32
    bits vdso to the 32 bits kernel, rename systemcfg (finally !), and adds
    some new (still untested) routines to both vdso's: clock_gettime() with
    support for CLOCK_REALTIME and CLOCK_MONOTONIC, clock_getres() (same
    clocks) and get_tbfreq() for glibc to retreive the timebase frequency.
    
    Tom,Steve: The implementation of get_tbfreq() I've done for 32 bits
    returns a long long (r3, r4) not a long. This is such that if we ever
    add support for >4Ghz timebases on ppc32, the userland interface won't
    have to change.
    
    I have tested gettimeofday() using some glibc patches in both ppc32 and
    ppc64 kernels using 32 bits userland (I haven't had a chance to test a
    64 bits userland yet, but the implementation didn't change and was
    tested earlier). I haven't tested yet the new functions.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index e28a139c29d0..62dfc5b8d765 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -44,7 +44,7 @@
 #include <asm/cputable.h>
 #include <asm/system.h>
 #include <asm/mpic.h>
-#include <asm/systemcfg.h>
+#include <asm/vdso_datapage.h>
 #ifdef CONFIG_PPC64
 #include <asm/paca.h>
 #endif
@@ -371,7 +371,7 @@ int generic_cpu_disable(void)
 
 	cpu_clear(cpu, cpu_online_map);
 #ifdef CONFIG_PPC64
-	_systemcfg->processorCount--;
+	vdso_data->processorCount--;
 	fixup_irqs(cpu_online_map);
 #endif
 	return 0;

commit 3ae0af12b458461f36dfddb26e54056be32928dd
Merge: 3b44f137b9a8 7c43ee40ec60
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Thu Nov 10 07:37:51 2005 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc-merge

commit 094fe2e712f38f49bf79ef93306c61b1b993b07b
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Nov 10 14:26:12 2005 +1100

    powerpc: Fixes for 32-bit powermac SMP
    
    A couple of bugs crept in with the merge of smp.c...
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 7fd530898bd1..2ffdc863bff3 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -369,11 +369,11 @@ int generic_cpu_disable(void)
 	if (cpu == boot_cpuid)
 		return -EBUSY;
 
+	cpu_clear(cpu, cpu_online_map);
 #ifdef CONFIG_PPC64
 	_systemcfg->processorCount--;
-#endif
-	cpu_clear(cpu, cpu_online_map);
 	fixup_irqs(cpu_online_map);
+#endif
 	return 0;
 }
 
@@ -391,9 +391,11 @@ int generic_cpu_enable(unsigned int cpu)
 	while (!cpu_online(cpu))
 		cpu_relax();
 
+#ifdef CONFIG_PPC64
 	fixup_irqs(cpu_online_map);
 	/* counter the irq disable in fixup_irqs */
 	local_irq_enable();
+#endif
 	return 0;
 }
 
@@ -422,7 +424,9 @@ void generic_mach_cpu_die(void)
 	while (__get_cpu_var(cpu_state) != CPU_UP_PREPARE)
 		cpu_relax();
 
+#ifdef CONFIG_PPC64
 	flush_tlb_pending();
+#endif
 	cpu_set(cpu, cpu_online_map);
 	local_irq_enable();
 }

commit 799d6046d3fb557006e6d7c9767fdb96479b0e0a
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Nov 10 13:37:51 2005 +1100

    [PATCH] powerpc: merge code values for identifying platforms
    
    This patch merges platform codes.  systemcfg->platform is no longer used,
    systemcfg use in general is deprecated as much as possible (and renamed
    _systemcfg before it gets completely moved elsewhere in a future patch),
    _machine is now used on ppc64 along as ppc32.  Platform codes aren't gone
    yet but we are getting a step closer. A bunch of asm code in head[_64].S
    is also turned into C code.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 5c330c3366e4..7fd530898bd1 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -44,6 +44,7 @@
 #include <asm/cputable.h>
 #include <asm/system.h>
 #include <asm/mpic.h>
+#include <asm/systemcfg.h>
 #ifdef CONFIG_PPC64
 #include <asm/paca.h>
 #endif
@@ -368,7 +369,9 @@ int generic_cpu_disable(void)
 	if (cpu == boot_cpuid)
 		return -EBUSY;
 
-	systemcfg->processorCount--;
+#ifdef CONFIG_PPC64
+	_systemcfg->processorCount--;
+#endif
 	cpu_clear(cpu, cpu_online_map);
 	fixup_irqs(cpu_online_map);
 	return 0;

commit e4d76e1c0b15590f2ad9bba89426c2520cd22ca6
Author: Andrew Morton <akpm@osdl.org>
Date:   Wed Nov 9 15:45:30 2005 -0800

    [PATCH] powerpc: sched fixups
    
    - Re-add a hunk lost during merge: ppc64 is missing the hunk that disables
      preempt on the secondary CPUs before they call cpu_idle().
    
    - ppc's cpu_idle() had the need_resched() test wrong.
    
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    CC: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Anton Blanchard <anton@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 5c330c3366e4..36d67a8d7cbb 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -510,6 +510,7 @@ int __devinit start_secondary(void *unused)
 
 	smp_store_cpu_info(cpu);
 	set_dec(tb_ticks_per_jiffy);
+	preempt_disable();
 	cpu_callin_map[cpu] = 1;
 
 	smp_ops->setup_cpu(cpu);

commit 570142ca37248291c03df9852a5a0ce97f756464
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Nov 7 19:05:31 2005 +1100

    [PATCH] ppc64: remove some direct xmon calls
    
    Even though we can enable and disable xmon at runtime now, there are a
    few places in the merge tree that call xmon and xmon_printf directly.
    
    In the case below we call die() which will call xmon if it is enabled.
    
    Also remove an unnecessary include of xmon.h in smp.c.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 1794a694a928..5c330c3366e4 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -40,7 +40,6 @@
 #include <asm/prom.h>
 #include <asm/smp.h>
 #include <asm/time.h>
-#include <asm/xmon.h>
 #include <asm/machdep.h>
 #include <asm/cputable.h>
 #include <asm/system.h>

commit 5ad570786158e327a1c5d32dd3d66f26d8de6340
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Nov 5 10:33:55 2005 +1100

    powerpc: Merge smp.c and smp.h
    
    This also moves setup_cpu_maps to setup-common.c (calling it
    smp_setup_cpu_maps) and uses it on both 32-bit and 64-bit.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
new file mode 100644
index 000000000000..1794a694a928
--- /dev/null
+++ b/arch/powerpc/kernel/smp.c
@@ -0,0 +1,565 @@
+/*
+ * SMP support for ppc.
+ *
+ * Written by Cort Dougan (cort@cs.nmt.edu) borrowing a great
+ * deal of code from the sparc and intel versions.
+ *
+ * Copyright (C) 1999 Cort Dougan <cort@cs.nmt.edu>
+ *
+ * PowerPC-64 Support added by Dave Engebretsen, Peter Bergner, and
+ * Mike Corrigan {engebret|bergner|mikec}@us.ibm.com
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#undef DEBUG
+
+#include <linux/config.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/smp.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/spinlock.h>
+#include <linux/cache.h>
+#include <linux/err.h>
+#include <linux/sysdev.h>
+#include <linux/cpu.h>
+#include <linux/notifier.h>
+
+#include <asm/ptrace.h>
+#include <asm/atomic.h>
+#include <asm/irq.h>
+#include <asm/page.h>
+#include <asm/pgtable.h>
+#include <asm/prom.h>
+#include <asm/smp.h>
+#include <asm/time.h>
+#include <asm/xmon.h>
+#include <asm/machdep.h>
+#include <asm/cputable.h>
+#include <asm/system.h>
+#include <asm/mpic.h>
+#ifdef CONFIG_PPC64
+#include <asm/paca.h>
+#endif
+
+int smp_hw_index[NR_CPUS];
+struct thread_info *secondary_ti;
+
+#ifdef DEBUG
+#define DBG(fmt...) udbg_printf(fmt)
+#else
+#define DBG(fmt...)
+#endif
+
+cpumask_t cpu_possible_map = CPU_MASK_NONE;
+cpumask_t cpu_online_map = CPU_MASK_NONE;
+cpumask_t cpu_sibling_map[NR_CPUS] = { [0 ... NR_CPUS-1] = CPU_MASK_NONE };
+
+EXPORT_SYMBOL(cpu_online_map);
+EXPORT_SYMBOL(cpu_possible_map);
+
+/* SMP operations for this machine */
+struct smp_ops_t *smp_ops;
+
+static volatile unsigned int cpu_callin_map[NR_CPUS];
+
+void smp_call_function_interrupt(void);
+
+int smt_enabled_at_boot = 1;
+
+#ifdef CONFIG_MPIC
+int __init smp_mpic_probe(void)
+{
+	int nr_cpus;
+
+	DBG("smp_mpic_probe()...\n");
+
+	nr_cpus = cpus_weight(cpu_possible_map);
+
+	DBG("nr_cpus: %d\n", nr_cpus);
+
+	if (nr_cpus > 1)
+		mpic_request_ipis();
+
+	return nr_cpus;
+}
+
+void __devinit smp_mpic_setup_cpu(int cpu)
+{
+	mpic_setup_this_cpu();
+}
+#endif /* CONFIG_MPIC */
+
+#ifdef CONFIG_PPC64
+void __devinit smp_generic_kick_cpu(int nr)
+{
+	BUG_ON(nr < 0 || nr >= NR_CPUS);
+
+	/*
+	 * The processor is currently spinning, waiting for the
+	 * cpu_start field to become non-zero After we set cpu_start,
+	 * the processor will continue on to secondary_start
+	 */
+	paca[nr].cpu_start = 1;
+	smp_mb();
+}
+#endif
+
+void smp_message_recv(int msg, struct pt_regs *regs)
+{
+	switch(msg) {
+	case PPC_MSG_CALL_FUNCTION:
+		smp_call_function_interrupt();
+		break;
+	case PPC_MSG_RESCHEDULE:
+		/* XXX Do we have to do this? */
+		set_need_resched();
+		break;
+#ifdef CONFIG_DEBUGGER
+	case PPC_MSG_DEBUGGER_BREAK:
+		debugger_ipi(regs);
+		break;
+#endif
+	default:
+		printk("SMP %d: smp_message_recv(): unknown msg %d\n",
+		       smp_processor_id(), msg);
+		break;
+	}
+}
+
+void smp_send_reschedule(int cpu)
+{
+	smp_ops->message_pass(cpu, PPC_MSG_RESCHEDULE);
+}
+
+#ifdef CONFIG_DEBUGGER
+void smp_send_debugger_break(int cpu)
+{
+	smp_ops->message_pass(cpu, PPC_MSG_DEBUGGER_BREAK);
+}
+#endif
+
+static void stop_this_cpu(void *dummy)
+{
+	local_irq_disable();
+	while (1)
+		;
+}
+
+void smp_send_stop(void)
+{
+	smp_call_function(stop_this_cpu, NULL, 1, 0);
+}
+
+/*
+ * Structure and data for smp_call_function(). This is designed to minimise
+ * static memory requirements. It also looks cleaner.
+ * Stolen from the i386 version.
+ */
+static  __cacheline_aligned_in_smp DEFINE_SPINLOCK(call_lock);
+
+static struct call_data_struct {
+	void (*func) (void *info);
+	void *info;
+	atomic_t started;
+	atomic_t finished;
+	int wait;
+} *call_data;
+
+/* delay of at least 8 seconds */
+#define SMP_CALL_TIMEOUT	8
+
+/*
+ * This function sends a 'generic call function' IPI to all other CPUs
+ * in the system.
+ *
+ * [SUMMARY] Run a function on all other CPUs.
+ * <func> The function to run. This must be fast and non-blocking.
+ * <info> An arbitrary pointer to pass to the function.
+ * <nonatomic> currently unused.
+ * <wait> If true, wait (atomically) until function has completed on other CPUs.
+ * [RETURNS] 0 on success, else a negative status code. Does not return until
+ * remote CPUs are nearly ready to execute <<func>> or are or have executed.
+ *
+ * You must not call this function with disabled interrupts or from a
+ * hardware interrupt handler or from a bottom half handler.
+ */
+int smp_call_function (void (*func) (void *info), void *info, int nonatomic,
+		       int wait)
+{ 
+	struct call_data_struct data;
+	int ret = -1, cpus;
+	u64 timeout;
+
+	/* Can deadlock when called with interrupts disabled */
+	WARN_ON(irqs_disabled());
+
+	data.func = func;
+	data.info = info;
+	atomic_set(&data.started, 0);
+	data.wait = wait;
+	if (wait)
+		atomic_set(&data.finished, 0);
+
+	spin_lock(&call_lock);
+	/* Must grab online cpu count with preempt disabled, otherwise
+	 * it can change. */
+	cpus = num_online_cpus() - 1;
+	if (!cpus) {
+		ret = 0;
+		goto out;
+	}
+
+	call_data = &data;
+	smp_wmb();
+	/* Send a message to all other CPUs and wait for them to respond */
+	smp_ops->message_pass(MSG_ALL_BUT_SELF, PPC_MSG_CALL_FUNCTION);
+
+	timeout = get_tb() + (u64) SMP_CALL_TIMEOUT * tb_ticks_per_sec;
+
+	/* Wait for response */
+	while (atomic_read(&data.started) != cpus) {
+		HMT_low();
+		if (get_tb() >= timeout) {
+			printk("smp_call_function on cpu %d: other cpus not "
+			       "responding (%d)\n", smp_processor_id(),
+			       atomic_read(&data.started));
+			debugger(NULL);
+			goto out;
+		}
+	}
+
+	if (wait) {
+		while (atomic_read(&data.finished) != cpus) {
+			HMT_low();
+			if (get_tb() >= timeout) {
+				printk("smp_call_function on cpu %d: other "
+				       "cpus not finishing (%d/%d)\n",
+				       smp_processor_id(),
+				       atomic_read(&data.finished),
+				       atomic_read(&data.started));
+				debugger(NULL);
+				goto out;
+			}
+		}
+	}
+
+	ret = 0;
+
+ out:
+	call_data = NULL;
+	HMT_medium();
+	spin_unlock(&call_lock);
+	return ret;
+}
+
+EXPORT_SYMBOL(smp_call_function);
+
+void smp_call_function_interrupt(void)
+{
+	void (*func) (void *info);
+	void *info;
+	int wait;
+
+	/* call_data will be NULL if the sender timed out while
+	 * waiting on us to receive the call.
+	 */
+	if (!call_data)
+		return;
+
+	func = call_data->func;
+	info = call_data->info;
+	wait = call_data->wait;
+
+	if (!wait)
+		smp_mb__before_atomic_inc();
+
+	/*
+	 * Notify initiating CPU that I've grabbed the data and am
+	 * about to execute the function
+	 */
+	atomic_inc(&call_data->started);
+	/*
+	 * At this point the info structure may be out of scope unless wait==1
+	 */
+	(*func)(info);
+	if (wait) {
+		smp_mb__before_atomic_inc();
+		atomic_inc(&call_data->finished);
+	}
+}
+
+extern struct gettimeofday_struct do_gtod;
+
+struct thread_info *current_set[NR_CPUS];
+
+DECLARE_PER_CPU(unsigned int, pvr);
+
+static void __devinit smp_store_cpu_info(int id)
+{
+	per_cpu(pvr, id) = mfspr(SPRN_PVR);
+}
+
+static void __init smp_create_idle(unsigned int cpu)
+{
+	struct task_struct *p;
+
+	/* create a process for the processor */
+	p = fork_idle(cpu);
+	if (IS_ERR(p))
+		panic("failed fork for CPU %u: %li", cpu, PTR_ERR(p));
+#ifdef CONFIG_PPC64
+	paca[cpu].__current = p;
+#endif
+	current_set[cpu] = p->thread_info;
+	p->thread_info->cpu = cpu;
+}
+
+void __init smp_prepare_cpus(unsigned int max_cpus)
+{
+	unsigned int cpu;
+
+	DBG("smp_prepare_cpus\n");
+
+	/* 
+	 * setup_cpu may need to be called on the boot cpu. We havent
+	 * spun any cpus up but lets be paranoid.
+	 */
+	BUG_ON(boot_cpuid != smp_processor_id());
+
+	/* Fixup boot cpu */
+	smp_store_cpu_info(boot_cpuid);
+	cpu_callin_map[boot_cpuid] = 1;
+
+	max_cpus = smp_ops->probe();
+ 
+	smp_space_timers(max_cpus);
+
+	for_each_cpu(cpu)
+		if (cpu != boot_cpuid)
+			smp_create_idle(cpu);
+}
+
+void __devinit smp_prepare_boot_cpu(void)
+{
+	BUG_ON(smp_processor_id() != boot_cpuid);
+
+	cpu_set(boot_cpuid, cpu_online_map);
+#ifdef CONFIG_PPC64
+	paca[boot_cpuid].__current = current;
+#endif
+	current_set[boot_cpuid] = current->thread_info;
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+/* State of each CPU during hotplug phases */
+DEFINE_PER_CPU(int, cpu_state) = { 0 };
+
+int generic_cpu_disable(void)
+{
+	unsigned int cpu = smp_processor_id();
+
+	if (cpu == boot_cpuid)
+		return -EBUSY;
+
+	systemcfg->processorCount--;
+	cpu_clear(cpu, cpu_online_map);
+	fixup_irqs(cpu_online_map);
+	return 0;
+}
+
+int generic_cpu_enable(unsigned int cpu)
+{
+	/* Do the normal bootup if we haven't
+	 * already bootstrapped. */
+	if (system_state != SYSTEM_RUNNING)
+		return -ENOSYS;
+
+	/* get the target out of it's holding state */
+	per_cpu(cpu_state, cpu) = CPU_UP_PREPARE;
+	smp_wmb();
+
+	while (!cpu_online(cpu))
+		cpu_relax();
+
+	fixup_irqs(cpu_online_map);
+	/* counter the irq disable in fixup_irqs */
+	local_irq_enable();
+	return 0;
+}
+
+void generic_cpu_die(unsigned int cpu)
+{
+	int i;
+
+	for (i = 0; i < 100; i++) {
+		smp_rmb();
+		if (per_cpu(cpu_state, cpu) == CPU_DEAD)
+			return;
+		msleep(100);
+	}
+	printk(KERN_ERR "CPU%d didn't die...\n", cpu);
+}
+
+void generic_mach_cpu_die(void)
+{
+	unsigned int cpu;
+
+	local_irq_disable();
+	cpu = smp_processor_id();
+	printk(KERN_DEBUG "CPU%d offline\n", cpu);
+	__get_cpu_var(cpu_state) = CPU_DEAD;
+	smp_wmb();
+	while (__get_cpu_var(cpu_state) != CPU_UP_PREPARE)
+		cpu_relax();
+
+	flush_tlb_pending();
+	cpu_set(cpu, cpu_online_map);
+	local_irq_enable();
+}
+#endif
+
+static int __devinit cpu_enable(unsigned int cpu)
+{
+	if (smp_ops->cpu_enable)
+		return smp_ops->cpu_enable(cpu);
+
+	return -ENOSYS;
+}
+
+int __devinit __cpu_up(unsigned int cpu)
+{
+	int c;
+
+	secondary_ti = current_set[cpu];
+	if (!cpu_enable(cpu))
+		return 0;
+
+	if (smp_ops->cpu_bootable && !smp_ops->cpu_bootable(cpu))
+		return -EINVAL;
+
+#ifdef CONFIG_PPC64
+	paca[cpu].default_decr = tb_ticks_per_jiffy;
+#endif
+
+	/* Make sure callin-map entry is 0 (can be leftover a CPU
+	 * hotplug
+	 */
+	cpu_callin_map[cpu] = 0;
+
+	/* The information for processor bringup must
+	 * be written out to main store before we release
+	 * the processor.
+	 */
+	smp_mb();
+
+	/* wake up cpus */
+	DBG("smp: kicking cpu %d\n", cpu);
+	smp_ops->kick_cpu(cpu);
+
+	/*
+	 * wait to see if the cpu made a callin (is actually up).
+	 * use this value that I found through experimentation.
+	 * -- Cort
+	 */
+	if (system_state < SYSTEM_RUNNING)
+		for (c = 5000; c && !cpu_callin_map[cpu]; c--)
+			udelay(100);
+#ifdef CONFIG_HOTPLUG_CPU
+	else
+		/*
+		 * CPUs can take much longer to come up in the
+		 * hotplug case.  Wait five seconds.
+		 */
+		for (c = 25; c && !cpu_callin_map[cpu]; c--) {
+			msleep(200);
+		}
+#endif
+
+	if (!cpu_callin_map[cpu]) {
+		printk("Processor %u is stuck.\n", cpu);
+		return -ENOENT;
+	}
+
+	printk("Processor %u found.\n", cpu);
+
+	if (smp_ops->give_timebase)
+		smp_ops->give_timebase();
+
+	/* Wait until cpu puts itself in the online map */
+	while (!cpu_online(cpu))
+		cpu_relax();
+
+	return 0;
+}
+
+
+/* Activate a secondary processor. */
+int __devinit start_secondary(void *unused)
+{
+	unsigned int cpu = smp_processor_id();
+
+	atomic_inc(&init_mm.mm_count);
+	current->active_mm = &init_mm;
+
+	smp_store_cpu_info(cpu);
+	set_dec(tb_ticks_per_jiffy);
+	cpu_callin_map[cpu] = 1;
+
+	smp_ops->setup_cpu(cpu);
+	if (smp_ops->take_timebase)
+		smp_ops->take_timebase();
+
+	spin_lock(&call_lock);
+	cpu_set(cpu, cpu_online_map);
+	spin_unlock(&call_lock);
+
+	local_irq_enable();
+
+	cpu_idle();
+	return 0;
+}
+
+int setup_profiling_timer(unsigned int multiplier)
+{
+	return 0;
+}
+
+void __init smp_cpus_done(unsigned int max_cpus)
+{
+	cpumask_t old_mask;
+
+	/* We want the setup_cpu() here to be called from CPU 0, but our
+	 * init thread may have been "borrowed" by another CPU in the meantime
+	 * se we pin us down to CPU 0 for a short while
+	 */
+	old_mask = current->cpus_allowed;
+	set_cpus_allowed(current, cpumask_of_cpu(boot_cpuid));
+	
+	smp_ops->setup_cpu(boot_cpuid);
+
+	set_cpus_allowed(current, old_mask);
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+int __cpu_disable(void)
+{
+	if (smp_ops->cpu_disable)
+		return smp_ops->cpu_disable();
+
+	return -ENOSYS;
+}
+
+void __cpu_die(unsigned int cpu)
+{
+	if (smp_ops->cpu_die)
+		smp_ops->cpu_die(cpu);
+}
+#endif
